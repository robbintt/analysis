---
ver: rpa2
title: 'ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities'
arxiv_id: '2409.19839'
source_url: https://arxiv.org/abs/2409.19839
tags:
- question
- questions
- scratchpad
- freeze
- forecast
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ForecastBench, a dynamic benchmark for evaluating
  AI forecasting capabilities that addresses the limitations of static benchmarks.
  The benchmark consists of 1,000 questions about future events that are automatically
  generated and regularly updated from nine sources including prediction markets and
  real-world time series.
---

# ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities

## Quick Facts
- arXiv ID: 2409.19839
- Source URL: https://arxiv.org/abs/2409.19839
- Reference count: 40
- LLMs achieve Brier score of 0.122 while expert forecasters achieve 0.096 on forecasting benchmark

## Executive Summary
This paper introduces ForecastBench, a dynamic benchmark for evaluating AI forecasting capabilities that addresses the limitations of static benchmarks. The benchmark consists of 1,000 questions about future events that are automatically generated and regularly updated from nine sources including prediction markets and real-world time series. The authors collected forecasts from expert human forecasters, the general public, and 17 different LLMs on a random subset of 200 questions. Results show that expert forecasters significantly outperform the top-performing LLM (Claude-3.5 Sonnet), achieving a Brier score of 0.096 compared to 0.122 for the best LLM (p-value < 0.001). While LLMs have achieved superhuman performance on many benchmarks, they perform less well on forecasting tasks, particularly on combination questions requiring reasoning about event dependencies.

## Method Summary
The authors developed a dynamic benchmark that automatically generates and updates 1,000 forecasting questions from nine different sources including prediction markets, economic indicators, and geopolitical events. They evaluated forecasts from expert human forecasters, general public participants, and 17 different large language models on a random subset of 200 questions. Forecasts were collected at a single time point and evaluated using Brier scores to measure accuracy. The benchmark includes both simple and combination questions, with the latter requiring reasoning about dependencies between multiple events.

## Key Results
- Expert forecasters achieved Brier score of 0.096, significantly outperforming best LLM (Claude-3.5 Sonnet) at 0.122 (p<0.001)
- LLMs struggled particularly with combination questions requiring reasoning about event dependencies
- Despite achieving superhuman performance on many benchmarks, current LLMs lag behind human experts in forecasting tasks

## Why This Works (Mechanism)
The benchmark's dynamic nature addresses key limitations of static forecasting benchmarks by automatically generating and updating questions from real-world data sources. This ensures that questions remain relevant and challenging as events unfold, preventing models from memorizing patterns or relying on outdated information. The inclusion of both simple and combination questions tests different aspects of forecasting capability, from direct prediction to complex reasoning about interdependent events.

## Foundational Learning
- **Brier Score**: Why needed - measures probabilistic forecast accuracy; Quick check - lower scores indicate better performance
- **Dynamic Benchmarking**: Why needed - prevents overfitting to static datasets; Quick check - questions update with new information
- **Combination Questions**: Why needed - test reasoning about event dependencies; Quick check - require multi-step logical inference
- **Prediction Markets**: Why needed - provide real-world forecasting data; Quick check - aggregate crowd wisdom
- **Time Series Analysis**: Why needed - captures temporal patterns in forecasting; Quick check - uses historical data to inform predictions
- **Statistical Significance Testing**: Why needed - validates performance differences; Quick check - p-values indicate meaningful differences

## Architecture Onboarding

**Component Map:**
Data Sources (9 sources) -> Question Generation Engine -> Benchmark Database -> Evaluation Framework -> Leaderboard

**Critical Path:**
Data Collection -> Question Generation -> Forecast Collection -> Evaluation -> Results Publication

**Design Tradeoffs:**
The benchmark prioritizes real-world relevance over controlled experimental conditions, sacrificing some experimental control for ecological validity. This means results may vary with changing world conditions but provide more realistic assessment of forecasting capabilities.

**Failure Signatures:**
Poor performance on combination questions indicates limitations in reasoning about event dependencies. Static benchmarks may show artificially inflated performance due to memorization or outdated information.

**First 3 Experiments:**
1. Compare performance across different question types (simple vs combination)
2. Analyze error patterns by data source to identify domain-specific challenges
3. Test temporal forecasting by collecting multiple forecasts over time for the same questions

## Open Questions the Paper Calls Out
None

## Limitations
- The benchmark's dynamic nature means results may shift as questions are updated and new data becomes available
- The relatively small sample size of 200 questions limits generalizability of findings
- Only nine specific sources were used, which may not capture all relevant forecasting domains
- Forecasts were only evaluated at a single time point, while real-world forecasting typically involves multiple updates

## Confidence

**High Confidence:** Expert forecasters outperform LLMs (p<0.001)
**Medium Confidence:** Combination questions are particularly challenging for LLMs
**Medium Confidence:** Current state-of-the-art LLMs lag behind human experts

## Next Checks

1. Test forecasting performance across multiple time points to assess temporal reasoning capabilities
2. Expand source diversity beyond the current nine domains to evaluate generalization
3. Investigate specific failure modes in combination questions through detailed error analysis