---
ver: rpa2
title: An AI Architecture with the Capability to Explain Recognition Results
arxiv_id: '2406.08740'
source_url: https://arxiv.org/abs/2406.08740
tags:
- explainable
- effectiveness
- explainability
- metric
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of improving explainability in
  AI systems while maintaining performance, particularly for handwritten character
  recognition. The authors propose a novel architecture that combines explainable
  and unexplainable neural network flows, introducing a new metric (EP ARS) to quantify
  explainability and improve decision-making.
---

# An AI Architecture with the Capability to Explain Recognition Results

## Quick Facts
- arXiv ID: 2406.08740
- Source URL: https://arxiv.org/abs/2406.08740
- Authors: Paul Whitten; Francis Wolff; Chris Papachristou
- Reference count: 22
- Primary result: New architecture combines explainable and unexplainable neural network flows, introducing EP ARS metric to quantify explainability while improving recognition accuracy (95.5% on MNIST vs 92% with previous methods)

## Executive Summary
This paper presents a novel AI architecture that addresses the challenge of maintaining high performance while providing explainable recognition results. The system combines property-based explainable models with high-performing but unexplainable neural networks, introducing a new metric (EP ARS) to balance performance and interpretability. The approach is demonstrated on handwritten character recognition tasks using MNIST and EMNIST datasets, showing significant improvements in accuracy while maintaining clear explanations for decisions.

## Method Summary
The method involves training multiple flows: property-based explainable models (using transforms like stroke detection, circle detection, etc.) and a high-performing unexplainable flow. Each flow makes predictions that are combined through weighted voting based on per-class effectiveness metrics. The EP ARS metric (product of Precision, Accuracy, Recall, and Specificity) is introduced to evaluate per-class effectiveness, while the Exd metric quantifies overall explainability by weighting each flow's contribution based on its explainability score.

## Key Results
- Adding unexplainable flows improves overall accuracy from 92% to 95.5% on MNIST
- Accuracy reaches up to 98% when combining explainable and unexplainable flows
- EP ARS metric outperforms previous methods in per-class effectiveness evaluation
- System provides clear explanations for decisions, as demonstrated in EMNIST character recognition examples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining explainable and unexplainable flows improves recognition accuracy while retaining some interpretability.
- Mechanism: The explainable flows provide interpretable rationales (via property-based explanations), while the unexplainable flow supplies a high-performing baseline that compensates for weak properties in certain classes. The system weights each flow's vote by its effectiveness metric and combines them into a global decision.
- Core assumption: Unexplainable flows still provide useful signal even if not interpretable; property-based transforms are relevant to most classes.
- Evidence anchors:
  - [abstract] "Results show that adding unexplainable flows to the system improves overall accuracy, with the EP ARS metric outperforming previous methods."
  - [section III-A] "The unexplainable flow contributes excellent recognition performance but deters from the explainability of the system since the IE is considered an opaque box..."
  - [corpus] Weak - the cited corpus neighbors do not directly support this mechanism; they discuss explainability but not this hybrid architecture.
- Break condition: If property-based transforms become irrelevant to many classes, the explainable flows add noise and degrade performance.

### Mechanism 2
- Claim: The EP ARS metric improves per-class effectiveness estimation over standard metrics.
- Mechanism: EP ARS multiplies Precision, Accuracy, Recall, and Specificity to balance sensitivity to false positives and negatives. This product form penalizes models that are precise but miss negatives (or vice versa), and it preserves TPs/TNs in the numerator, avoiding collapse when classes are imbalanced.
- Core assumption: Effectiveness at a per-class level must penalize both FP and FN errors, not just one type; product combination yields better discrimination.
- Evidence anchors:
  - [section III-C] "The combination of metrics found to have the best performance as per-class effectiveness was the product of class vs others Precision ( P ), Accuracy (ACC ), Recall ( R), and Specificity ( S) as shown in (8) as the new effectiveness metric, EP ARS."
  - [section IV-B] "With analysis of the EP ARS metric, one could speculate that larger data sets, especially with more classes, may not perform as well with EP ARS."
  - [corpus] Weak - neighbors do not discuss this specific metric formulation.
- Break condition: When class imbalance becomes extreme, the product may underflow or overamplify noise from small counts.

### Mechanism 3
- Claim: Explainability metric Exd quantifies how interpretable each decision is.
- Mechanism: Exd weights each flow's effectiveness Ej,d by its explainability Xj and normalizes by total effectiveness. This ensures that votes from explainable flows contribute fully to explainability, while unexplainable flows contribute zero.
- Core assumption: Each flow's contribution to explainability can be modeled as a binary property (0 for unexplainable, 1 for explainable) and linearly combined.
- Evidence anchors:
  - [section III-B] "The explainability of a decision for class d is given by Exd where the numerator is the sum of the product of the explainability metric and effectiveness for the flows that voted for class d..."
  - [section IV-C] Example tables show Exd values ranging from 1.0 for fully explainable to 0.0 for unexplainable.
  - [corpus] Weak - corpus does not contain evidence about this particular Exd formulation.
- Break condition: If flows have partial explainability (not purely 0 or 1), the binary Xj assumption fails and the metric no longer reflects true interpretability.

## Foundational Learning

- Concept: Per-class effectiveness vs. global metrics
  - Why needed here: The architecture makes decisions at the class level; using global accuracy hides per-class failures.
  - Quick check question: If class A has 90% TP but 90% FP, what would Recall show? What would EP ARS show?

- Concept: Product-form metrics and numerical stability
  - Why needed here: EP ARS multiplies four ratios; without careful scaling, it can underflow or lose precision.
  - Quick check question: How would you rewrite EP ARS to avoid underflow when TP counts are small?

- Concept: Explainability vs. interpretability
  - Why needed here: The system provides explanations only for explainable flows; unexplainable flows boost accuracy but reduce explainability.
  - Quick check question: In the example, why does the capital 'S' have high explainability even though it received a vote from the unexplainable flow?

## Architecture Onboarding

- Component map:
  Input → Transform phase (property transforms for explainable flows) → Property Inferencing (SVMs/MLPs) → Decision Making (weighted voting) → Explainability phase (Exd calculation) → Output (ranked classes, confidence, rationale, explainability)
  Unexplainable flow bypasses Transform and uses raw input

- Critical path:
  1. Apply property transforms to input
  2. Compute per-flow predictions
  3. Retrieve effectiveness Ej,d from KB
  4. Compute confidence Cd via weighted sum
  5. Compute Exd via effectiveness-weighted explainability
  6. Sort and return top classes

- Design tradeoffs:
  - Explainable flows: interpretable but potentially lower accuracy
  - Unexplainable flow: higher accuracy but zero explainability
  - Metric choice: Recall favors TP, EP ARS balances FP/FN

- Failure signatures:
  - Accuracy drops sharply if unexplainable flow fails
  - Explainability scores near 0 for most decisions → most explainable flows are weak
  - Confidence scores collapse to uniform → effectiveness weights become similar

- First 3 experiments:
  1. Replace SVM IEs with MLPs in all flows; measure change in Accuracy, EP ARS, and Exd
  2. Remove the unexplainable flow; observe drop in Accuracy and increase in Exd
  3. Swap EP ARS for Precision-only effectiveness; record impact on per-class ranking and overall accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the EP ARS metric scale when applied to larger datasets with more classes?
- Basis in paper: [explicit] The paper speculates that larger datasets with more classes may not perform as well with EP ARS due to the T P³ and T N² terms.
- Why unresolved: The paper only analyzes performance on MNIST and EMNIST datasets and does not test the metric on larger, more complex datasets.
- What evidence would resolve it: Empirical testing of EP ARS on datasets with significantly more classes and samples than MNIST/EMNIST to observe performance degradation or stability.

### Open Question 2
- Question: What is the optimal balance between explainable and unexplainable flows for maximizing both accuracy and explainability?
- Basis in paper: [explicit] The paper shows that adding unexplainable flows improves accuracy but reduces explainability, introducing the Exd metric to quantify this trade-off.
- Why unresolved: The paper does not systematically explore different ratios of explainable to unexplainable flows or provide guidelines for optimal configuration.
- What evidence would resolve it: Systematic experiments varying the number and performance of unexplainable flows while measuring both accuracy and explainability metrics.

### Open Question 3
- Question: Can the EP ARS metric be modified to maintain its performance advantages while being less sensitive to class imbalance?
- Basis in paper: [explicit] The paper notes that EP ARS balances T P and T N but acknowledges it may not scale well to larger, more imbalanced datasets.
- Why unresolved: The paper proposes EP ARS as effective but does not explore modifications to make it more robust to varying dataset characteristics.
- What evidence would resolve it: Development and testing of modified versions of EP ARS that adjust the weighting of TP and TN terms based on dataset size and class distribution.

## Limitations

- The effectiveness of property-based transforms is not fully validated across diverse datasets
- The binary explainability assumption (X_j = 0 or 1) oversimplifies real-world scenarios with partial interpretability
- EP ARS metric may suffer from numerical instability with extreme class imbalance
- Lack of comprehensive ablation studies to isolate individual component contributions

## Confidence

- **High Confidence**: The hybrid architecture concept (combining explainable and unexplainable flows) and its basic implementation are well-supported by experimental results showing improved accuracy when unexplainable flows are added.
- **Medium Confidence**: The EP ARS metric formulation and its claimed advantages over traditional metrics are supported by mathematical reasoning but lack extensive empirical validation across diverse datasets.
- **Low Confidence**: The explainability metric Exd's binary weighting scheme and its ability to truly capture nuanced interpretability of different flows needs further validation.

## Next Checks

1. **Numerical Stability Analysis**: Test EP ARS on highly imbalanced datasets (e.g., 1:100 class ratios) to identify underflow issues and propose scaling solutions.
2. **Partial Explainability Extension**: Modify the Exd metric to handle partial explainability values (0 < X_j < 1) and evaluate its impact on decision interpretability.
3. **Property Transform Ablation**: Systematically remove individual property transforms and measure the resulting accuracy and explainability to quantify their relative importance.