---
ver: rpa2
title: Mem-elements based Neuromorphic Hardware for Neural Network Application
arxiv_id: '2403.03002'
source_url: https://arxiv.org/abs/2403.03002
tags: []
core_contribution: This thesis presents a comprehensive Python framework for evaluating
  large-scale deep neural networks (DNN) on memristive and memcapacitive crossbar
  systems, addressing various non-idealities. The framework incorporates device-level
  considerations, including conductance, capacitance cycle-to-cycle, and device-to-device
  variations.
---

# Mem-elements based Neuromorphic Hardware for Neural Network Application

## Quick Facts
- arXiv ID: 2403.03002
- Source URL: https://arxiv.org/abs/2403.03002
- Authors: Ankur Singh
- Reference count: 40
- 90.02% training accuracy achieved with memristive VMM accelerator on CIFAR-10 dataset

## Executive Summary
This thesis presents a comprehensive Python framework for evaluating deep neural networks on memristive and memcapacitive crossbar systems, addressing various non-idealities at the device level. The framework was tested with an 8-layer VGG network on a 128×128 RRAM array, achieving high training accuracies of 90.02% and 91.03% for memristive and memcapacitive accelerators respectively on the CIFAR-10 dataset. The work also proposes a meminductor model using three OTAs and two MOS capacitors, validated through extensive testing in Cadence Virtuoso with TSMC 180 nm PDK, demonstrating operation up to 60 MHz with 0.337 mW power consumption.

## Method Summary
The research developed a Python framework to evaluate large-scale DNNs on memristive and memcapacitive crossbar systems, incorporating device-level considerations including conductance, capacitance variations, and device-to-device variations. Testing was performed using an 8-layer VGG network on a 128×128 RRAM array. For the meminductor emulator, the model was validated through extensive testing in diverse scenarios using Cadence Virtuoso with TSMC 180 nm PDK, with experimental validation performed using readily accessible components.

## Key Results
- Memristive VMM accelerator achieved 90.02% training accuracy on CIFAR-10 dataset
- Memcapacitive VMM accelerator achieved 91.03% training accuracy on CIFAR-10 dataset
- Meminductor emulator operates at frequencies up to 60 MHz with 0.337 mW power consumption

## Why This Works (Mechanism)
The approach works by creating a comprehensive evaluation framework that accounts for real-world device imperfections in memristive and memcapacitive systems. By modeling conductance and capacitance variations at both cycle-to-cycle and device-to-device levels, the framework can accurately predict how non-idealities affect DNN training performance. The meminductor emulator leverages a simple circuit design using only MOS transistors, enabling easy silicon fabrication while maintaining high operating frequencies and low energy consumption.

## Foundational Learning
- **Memristive crossbar arrays**: Why needed - Enable in-memory computing for vector-matrix multiplication; Quick check - Verify conductance states can be reliably programmed and read
- **Device variation modeling**: Why needed - Real-world devices exhibit cycle-to-cycle and device-to-device variations that impact accuracy; Quick check - Compare training accuracy with and without variation models
- **OTA-based meminductor design**: Why needed - Provides analog implementation of meminductive behavior using standard components; Quick check - Verify inductor behavior across frequency range
- **VMM acceleration**: Why needed - Vector-matrix multiplication is the core operation in neural network inference; Quick check - Measure throughput and energy per operation
- **Non-ideality compensation**: Why needed - Crossbar systems suffer from parasitic resistances and limited precision; Quick check - Quantify accuracy degradation from line resistance effects
- **Hardware-software co-design**: Why needed - Optimizes neural network architectures for specific hardware constraints; Quick check - Evaluate different network architectures on the crossbar system

## Architecture Onboarding

Component map: Input -> Crossbar array -> Output sensing -> Digital processing -> Training algorithm

Critical path: Data input → Crossbar VMM → Analog-to-digital conversion → Accumulation → Activation function → Next layer

Design tradeoffs: The framework balances between modeling accuracy (more detailed device models) and computational efficiency (faster simulations). The 128×128 array size represents a compromise between implementation complexity and network capacity.

Failure signatures: Accuracy degradation manifests as gradual performance loss during training, particularly visible in the convergence rate and final validation accuracy. Variations in device conductance lead to asymmetric weight updates, while line resistance causes signal attenuation in larger arrays.

3 first experiments:
1. Test single-layer network training with ideal devices to establish baseline performance
2. Evaluate impact of device-to-device variations by training identical networks with different random seeds
3. Measure power consumption and latency of VMM operations across different input patterns

## Open Questions the Paper Calls Out
None

## Limitations
- 8-layer VGG network on 128×128 array represents constrained proof-of-concept rather than large-scale deployment
- Accuracy metrics lack comparison against baseline implementations on standard digital hardware
- Meminductor emulator validation limited to TSMC 180nm technology node

## Confidence
- High confidence: Basic functionality of memristive and memcapacitive VMM accelerators
- Medium confidence: Accuracy results for CIFAR-10 with 8-layer VGG on 128×128 arrays
- Low confidence: Comparative performance claims and scalability projections

## Next Checks
1. Evaluate the same network architectures on standard GPU/CPU baselines to establish performance gaps and verify claimed advantages
2. Test accuracy degradation across multiple random seeds and network initializations to quantify robustness against variations
3. Implement the meminductor emulator in advanced technology nodes (e.g., 65nm or 28nm) to validate scalability claims and energy consumption projections