---
ver: rpa2
title: 'CoDream: Exchanging dreams instead of models for federated aggregation with
  heterogeneous models'
arxiv_id: '2402.15968'
source_url: https://arxiv.org/abs/2402.15968
tags:
- data
- clients
- knowledge
- codream
- dreams
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents CoDream, a novel framework for federated learning
  that aggregates knowledge instead of model parameters. CoDream collaboratively optimizes
  randomly initialized data ("dreams") in the input data space, enabling model-agnostic
  learning where clients can have heterogeneous model architectures.
---

# CoDream: Exchanging dreams instead of models for federated aggregation with heterogeneous models

## Quick Facts
- arXiv ID: 2402.15968
- Source URL: https://arxiv.org/abs/2402.15968
- Reference count: 27
- Key outcome: Novel framework that aggregates knowledge through shared input-space "dreams" instead of model parameters, enabling model-agnostic federated learning

## Executive Summary
CoDream introduces a novel federated learning framework that exchanges collaboratively optimized input-space representations ("dreams") instead of model parameters. This approach enables model-agnostic learning where clients with heterogeneous model architectures can participate in the same federated learning process. The framework demonstrates competitive performance compared to traditional federated learning methods while offering advantages in privacy preservation, scalability, and personalization capabilities.

## Method Summary
CoDream enables federated learning by having clients collaboratively optimize randomly initialized data representations in the input space rather than sharing model parameters. Each client maintains its own heterogeneous model architecture but works with shared "dreams" - optimized input data that captures the collective knowledge learned across the federation. The aggregation process occurs directly in the input space, making the framework independent of model size and architecture. This design naturally supports secure aggregation for privacy preservation and allows for personalized learning adaptations.

## Key Results
- Competitive performance compared to baseline federated learning methods despite not sharing model parameters
- Demonstrated compatibility with secure aggregation for privacy preservation
- Validated effectiveness for personalized learning scenarios
- Empirical validation on standard FL tasks shows promising results

## Why This Works (Mechanism)
The framework works by shifting the aggregation point from model parameters to input-space representations. By collaboratively optimizing shared "dreams" - randomly initialized data that clients work with - the system captures collective knowledge without requiring model homogeneity. This approach leverages the fact that input representations can encode shared patterns across heterogeneous architectures, while avoiding the communication overhead and privacy concerns associated with parameter sharing.

## Foundational Learning
- **Federated Learning**: Distributed learning where multiple clients collaborate without sharing raw data - needed to understand the collaborative optimization context
- **Secure Aggregation**: Privacy-preserving techniques that aggregate encrypted client updates - needed to understand privacy guarantees
- **Heterogeneous Model Architectures**: Different clients using different neural network structures - needed to grasp the model-agnostic requirement
- **Input-Space Optimization**: Optimizing data representations rather than model parameters - needed to understand the core innovation
- **Knowledge Aggregation**: Combining learned information from multiple sources - needed to understand the collective learning aspect
- **Model-Agnostic Learning**: Learning processes that work across different model architectures - needed to appreciate the framework's flexibility

## Architecture Onboarding
Component Map: Clients -> Shared Dreams -> Aggregation Server -> Updated Dreams
Critical Path: Client optimization of dreams → Secure aggregation → Server update → Distribution of new dreams
Design Tradeoffs: Model-agnostic flexibility vs. potential computational overhead of input-space optimization
Failure Signatures: Degraded performance when client data distributions are too heterogeneous or when dream optimization becomes unstable
First Experiments:
1. Single client dream optimization with fixed model architecture
2. Two-client federation with heterogeneous models sharing simple dreams
3. Multi-client federation with varying dream complexities

## Open Questions the Paper Calls Out
None

## Limitations
- Limited validation on diverse and large-scale federated learning benchmarks
- Computational overhead of optimizing shared dreams across multiple clients
- Scalability concerns for real-world applications with highly diverse client data distributions

## Confidence
- **High Confidence**: Theoretical framework for model-agnostic knowledge aggregation through shared input-space optimization
- **Medium Confidence**: Empirical results demonstrating competitive performance on standard benchmarks
- **Low Confidence**: Claims about scalability and real-world applicability without extensive large-scale validation

## Next Checks
1. Scale Validation: Evaluate CoDream on larger-scale federated learning benchmarks (e.g., FedScale or Leaf datasets) with thousands of clients and heterogeneous data distributions
2. Computational Overhead Analysis: Quantify computational and communication costs of dream optimization across different model sizes and input complexities
3. Robustness Testing: Assess CoDream's performance under realistic constraints including high client dropout rates, varying computational capabilities, and adversarial clients attempting to poison shared dreams