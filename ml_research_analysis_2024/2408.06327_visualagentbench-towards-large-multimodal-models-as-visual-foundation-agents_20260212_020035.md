---
ver: rpa2
title: 'VisualAgentBench: Towards Large Multimodal Models as Visual Foundation Agents'
arxiv_id: '2408.06327'
source_url: https://arxiv.org/abs/2408.06327
tags:
- task
- agents
- agent
- lmms
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces VisualAgentBench (VAB), the first comprehensive
  benchmark for training and evaluating large multimodal models as visual foundation
  agents across three key scenarios: embodied, GUI, and visual design. VAB provides
  five datasets with 746 test instances and 4,482 training trajectories, using interactive
  evaluation to assess agents'' performance.'
---

# VisualAgentBench: Towards Large Multimodal Models as Visual Foundation Agents

## Quick Facts
- arXiv ID: 2408.06327
- Source URL: https://arxiv.org/abs/2408.06327
- Reference count: 40
- Primary result: VAB benchmark reveals significant capability gaps between proprietary and open LMMs as visual foundation agents, with gpt-4o achieving 36.2% average success rate.

## Executive Summary
This paper introduces VisualAgentBench (VAB), the first comprehensive benchmark for training and evaluating large multimodal models as visual foundation agents across three key scenarios: embodied, GUI, and visual design. VAB provides five datasets with 746 test instances and 4,482 training trajectories, using interactive evaluation to assess agents' performance. Through extensive testing of nine proprietary LMM APIs and eight open models, the study reveals significant capability gaps, with top models like gpt-4o achieving 36.2% success rate on average. The results demonstrate that behavior cloning on VAB's training data substantially improves open LMM performance, though gaps remain between proprietary and open models. VAB offers standardized prompting and data formats, along with a hybrid data curation pipeline combining program-based solvers, LMM agent bootstrapping, and human demonstrations.

## Method Summary
VAB introduces a comprehensive benchmark for evaluating LMMs as visual foundation agents across embodied (OmniGibson), GUI (MineRL), and visual design (CSS) tasks. The benchmark uses interactive evaluation where agents must complete tasks in simulated environments with visual inputs. For training, a hybrid data curation pipeline generates trajectories through three methods: program-based solvers for deterministic tasks, LMM agent bootstrapping for more complex scenarios, and human demonstrations for quality control. Open LMMs are fine-tuned using behavior cloning on the collected trajectories with specific hyperparameters (5k steps, batch size 64). Proprietary models are evaluated directly using standardized system prompts without fine-tuning. The evaluation measures Success Rate (SR) across all environments with consistent metrics.

## Key Results
- Top proprietary model (gpt-4o) achieves 36.2% average success rate across all five VAB tasks
- Fine-tuning open LMMs on VAB training data significantly improves performance, though gaps remain with proprietary models
- LMMs show severe degradation when object labels are removed from OmniGibson tasks, indicating weak visual grounding capabilities
- Proprietary models outperform open models across all tasks, with the largest gap in CSS design tasks (1.6% vs 24.4% SR)

## Why This Works (Mechanism)
None

## Foundational Learning

**Interactive evaluation protocols**: Standardized success criteria across diverse environments ensure fair comparison. Why needed: Different tasks require different success definitions (task completion vs. reward thresholds). Quick check: Verify all environments use the same evaluation script with consistent success metrics.

**Behavior cloning for LMM fine-tuning**: Training open models on demonstration trajectories using supervised learning. Why needed: Open LMMs lack the instruction tuning that makes proprietary models effective. Quick check: Compare training loss curves for converged vs. underfitted models.

**Hybrid data curation pipeline**: Combining program-based solvers, LMM bootstrapping, and human demonstrations. Why needed: Pure programmatic approaches fail on complex tasks while pure human labeling is prohibitively expensive. Quick check: Analyze the distribution of data sources across all training trajectories.

**Visual grounding methods**: Object labels, Set-of-Marks (SoM), and direct bounding box outputs. Why needed: Different grounding approaches affect LMM ability to localize and interact with objects. Quick check: Test LMM performance with each grounding method on simple vs. complex scenes.

**Standardized prompting**: Consistent system prompts across all environments and models. Why needed: Prompt variability can mask true capability differences between models. Quick check: Verify prompt templates match exactly across all evaluation runs.

## Architecture Onboarding

**Component map**: LMM API → Prompt Generator → Environment Interface → State Monitor → Success Evaluator

**Critical path**: Environment State → Image Capture → LMM Inference → Action Selection → Environment Update → Success Check

**Design tradeoffs**: The benchmark prioritizes standardized evaluation over task complexity, using simplified environments that may not fully capture real-world challenges.

**Failure signatures**: LMMs commonly fail on tasks requiring fine-grained object manipulation, complex sequential reasoning, and when visual grounding information is incomplete or noisy.

**First experiments**:
1. Run baseline evaluation of gpt-4o on OmniGibson without any fine-tuning to establish reference performance
2. Test behavior cloning on a single environment (MineRL) with varying training data sizes to measure scaling effects
3. Compare performance degradation when removing object labels vs. using direct bounding box outputs in OmniGibson

## Open Questions the Paper Calls Out

**Open Question 1**: How does LMM performance as visual foundation agents scale with model size, particularly for open models that currently underperform compared to proprietary models? The paper notes limitations in computing resources prevented testing larger open LMMs, leaving this relationship unexplored.

**Open Question 2**: Can reinforcement learning effectively close the performance gap between proprietary and open LMMs as visual foundation agents beyond behavior cloning? The paper only explores behavior cloning and states future advancements will come from integrating reinforcement learning.

**Open Question 3**: How do different types of visual grounding (object labels vs. Set-of-Marks vs. direct bounding box output) impact LMM performance in visual agent tasks? The paper identifies grounding challenges but doesn't systematically compare different approaches.

## Limitations

- Proprietary model comparisons are constrained by API access variability, rate limiting, and potential version differences
- Success rate metric may not fully capture quality or safety of agent behaviors, particularly in embodied scenarios
- Data augmentation approach for VAB-CSS (duplicating training data three times) introduces potential bias that could inflate performance metrics

## Confidence

- **High confidence**: Success rate measurements across the five benchmark tasks are reliable given the standardized interactive evaluation protocol and clear success criteria
- **Medium confidence**: The performance gap between proprietary and open models after fine-tuning is well-supported, though the exact magnitude may vary with API access conditions
- **Low confidence**: The relative ranking of open models should be interpreted cautiously due to potential variability in training data overlap and implementation details

## Next Checks

1. **Cross-validation with alternative success criteria**: Re-run the evaluation using a multi-dimensional scoring system that accounts for action efficiency, safety constraints, and partial credit for intermediate achievements rather than binary success/failure.

2. **Ablation study on data augmentation**: Test the CSS performance gains by varying the duplication factor (1x, 3x, 5x) to determine whether observed improvements are due to data quantity versus potential overfitting effects.

3. **API access consistency verification**: Document and control for specific API versions, rate limits, and temperature settings across all proprietary model evaluations to ensure fair comparison conditions.