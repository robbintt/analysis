---
ver: rpa2
title: 'The correlation between nativelike selection and prototypicality: a multilingual
  onomasiological case study using semantic embedding'
arxiv_id: '2405.13529'
source_url: https://arxiv.org/abs/2405.13529
tags:
- harm
- shang
- language
- chinese
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an innovative framework to study the correlation
  between nativelike selection (NLS) and prototypicality. It employs semantic embedding
  and multilingual corpus analysis to automatically discover potential NLSs, then
  validates them using frame semantics.
---

# The correlation between nativelike selection and prototypicality: a multilingual onomasiological case study using semantic embedding

## Quick Facts
- arXiv ID: 2405.13529
- Source URL: https://arxiv.org/abs/2405.13529
- Authors: Huasheng Zhang
- Reference count: 25
- Primary result: Chinese native speakers prefer the verb shang 'harm' to describe skin concerns in dish soap reviews, due to its language-specific prototype of everyday self-care, supporting the onomasiological hypothesis that referents are more readily named by lexical items if they are salient members of the category denoted by that item.

## Executive Summary
This paper proposes an innovative framework to study the correlation between nativelike selection (NLS) and prototypicality using semantic embedding and multilingual corpus analysis. The study focuses on Chinese dish soap customer reviews and reveals that Chinese native speakers prefer the verb shang 'harm' to describe skin concerns, due to its language-specific prototype of everyday self-care. This finding supports the onomasiological hypothesis that "a referent is more readily named by a lexical item if it is a salient member of the category denoted by that item." The results demonstrate that NLS can be explained by semantic motivation rather than arbitrary collocation, enriching our understanding of this crucial language phenomenon.

## Method Summary
The study employs a multilingual approach using customer reviews for dish soap products in Chinese, English, and Japanese. It uses BERTopic with semantic embeddings (GTE-large models) to automatically discover potential NLSs through topic modeling and clustering. The identified NLSs are then validated using frame semantics, where a super frame (Skin_feel) with subframes (Negative_product_impact, etc.) is defined to map target lexical units across languages. Behavioral Profile analysis with correspondence analysis is conducted to uncover language-specific prototypes by measuring relative associations between words and semantic features.

## Key Results
- Chinese native speakers prefer the verb shang 'harm' to describe skin concerns in dish soap reviews
- NLS patterns correlate with semantic prototypes rather than arbitrary collocation
- Frame semantics provides a viable framework for cross-linguistic comparison of NLS patterns
- Behavioral Profile analysis reveals language-specific prototypes through relative associations

## Why This Works (Mechanism)

### Mechanism 1
BERTopic with semantic embeddings automatically discovers cross-linguistic NLSs more efficiently than manual topic design. Semantic embeddings convert sentences into vectors in high-dimensional space; clustering algorithms (UMAP + HDBSCAN) group semantically similar sentences into topics; c-TF-IDF identifies representative words for each topic. Core assumption: Sentences about the same topic cluster together in embedding space even across languages, and topic word extraction via c-TF-IDF reliably captures salient lexical items.

### Mechanism 2
Frame semantics enables meaningful cross-linguistic comparison of NLSs by abstracting away syntactic differences. Define a super frame (Skin_feel) with subframes (Negative_product_impact, etc.); map target lexical units (LUs) like shang, good, yasashii to subframes; compare frequencies of LUs within the same subframe across languages. Core assumption: Semantic frames provide a universal "interlingua" that abstracts away morphosyntactic differences, allowing fair comparison of semantically equivalent expressions across languages.

### Mechanism 3
Behavioral Profile (BP) analysis with correspondence analysis reveals language-specific prototypes by measuring relative associations between words and semantic features. Manually annotate 300 instances of shang, harm, kizutsukeru with semantic features (e.g., "everyday life", "self-care"); apply correspondence analysis (CA) to measure relative associations; visualize with moon plot to identify distinctive feature profiles. Core assumption: Semantic features capture the nuanced pragmatic contexts in which words are used; relative associations (rather than raw frequencies) better reflect prototypicality.

## Foundational Learning

- **Semantic Embeddings and Clustering**: Why needed here: To efficiently discover potential NLSs across multiple languages without manual topic design. Quick check question: How do UMAP and HDBSCAN work together to form coherent topic clusters from semantic embeddings?
- **Frame Semantics**: Why needed here: To provide a unified framework for comparing semantically equivalent expressions across languages. Quick check question: What is the difference between a super frame and a subframe, and why is this distinction useful for cross-linguistic analysis?
- **Correspondence Analysis (CA) and Behavioral Profiles**: Why needed here: To uncover the subtle pragmatic nuances that differentiate synonyms across languages. Quick check question: Why is CA preferred over raw frequency counts when analyzing prototypicality?

## Architecture Onboarding

- **Component map**: Data Collection -> Preprocessing -> BERTopic (Embedding -> Clustering -> Topic Extraction) -> Frame Semantics -> Behavioral Profile Analysis
- **Critical path**: Data Collection → Preprocessing → BERTopic → Frame Semantics → BP Analysis
- **Design tradeoffs**: Use of off-the-shelf multilingual embedding models (fast, but may miss domain-specific nuances); Manual annotation of behavioral profiles (accurate but time-consuming and potentially biased); Focus on negative forms of verbs (reduces data volume but may miss affirmative usage patterns)
- **Failure signatures**: BERTopic produces incoherent or uninterpretable topics → likely embedding model mismatch or insufficient data; Frame semantics mapping fails to align across languages → frame definitions may be too language-specific; BP analysis shows no clear associations → semantic features may be poorly chosen or annotation inconsistent
- **First 3 experiments**: 1) Run BERTopic on a small sample of Chinese/English/Japanese dish soap reviews; inspect topic coherence manually. 2) Manually map a few identified NLSs (e.g., shang, good, yasashii) to Skin_feel subframes; compare frequencies. 3) Annotate 50 instances each of shang, harm, kizutsukeru with 5-10 semantic features; perform simple chi-square test for feature associations.

## Open Questions the Paper Calls Out

### Open Question 1
How can the semantic motivation behind nativelike selection be systematically quantified and validated across different languages? The paper suggests that NLS can be explained by semantic motivation rather than arbitrary collocation, but calls for further exploration of other factors influencing NLS.

### Open Question 2
How does the presence of collocations interact with and potentially influence the nativelike selection of individual words? The paper acknowledges the presence of collocations in the analysis but emphasizes that NLS goes beyond merely formulaic language.

### Open Question 3
What are the cognitive mechanisms underlying the language-specific prototypes that drive nativelike selection? The paper reveals a language-specific prototype for the Chinese verb shang 'harm' related to everyday self-care, but does not delve into the cognitive processes that give rise to such prototypes.

## Limitations
- Manual annotation process for behavioral profiles introduces potential bias and may not scale well
- Study relies on a single product category (dish soap), limiting generalizability to other domains
- English and Japanese data sources remain unspecified, raising questions about data comparability

## Confidence

**High confidence**: The technical implementation of BERTopic and clustering methodology

**Medium confidence**: The correlation between NLS and prototypicality as demonstrated for the specific dish soap domain

**Medium confidence**: The cross-linguistic comparison framework using frame semantics

**Low confidence**: The generalizability of findings to other product categories or semantic domains

## Next Checks

1. Replicate the analysis using customer reviews from multiple product categories (e.g., cosmetics, food, electronics) to test domain generalizability
2. Conduct inter-annotator agreement studies on the behavioral profile annotations to quantify annotation consistency
3. Perform ablation studies by comparing results when using different semantic embedding models (e.g., GTE vs. multilingual BERT) to assess robustness of topic discovery