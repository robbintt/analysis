---
ver: rpa2
title: When Should We Prefer State-to-Visual DAgger Over Visual Reinforcement Learning?
arxiv_id: '2412.13662'
source_url: https://arxiv.org/abs/2412.13662
tags:
- visual
- tasks
- learning
- dagger
- state-to-visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares State-to-Visual DAgger and visual RL across
  16 tasks from three benchmarks (ManiSkill, DMControl, Adroit) to determine when
  the two-stage approach is preferable. State-to-Visual DAgger trains a state policy
  via RL first, then learns a visual policy through online imitation, while visual
  RL directly trains policies from visual inputs using Asymmetric Actor Critic.
---

# When Should We Prefer State-to-Visual DAgger Over Visual Reinforcement Learning?

## Quick Facts
- arXiv ID: 2412.13662
- Source URL: https://arxiv.org/abs/2412.13662
- Reference count: 37
- Primary result: State-to-Visual DAgger excels in hard tasks and reduces wall-clock time but offers no sample efficiency advantage for easier tasks

## Executive Summary
This study systematically compares State-to-Visual DAgger and visual RL across 16 tasks from three benchmarks to determine when the two-stage approach is preferable. State-to-Visual DAgger trains a state policy via RL first, then learns a visual policy through online imitation, while visual RL directly trains policies from visual inputs. Results show State-to-Visual DAgger excels in hard tasks with more consistent performance and significantly reduces wall-clock training time, but offers no clear advantage in sample efficiency for easier tasks. The method is recommended when visual RL struggles to solve tasks or when wall-clock efficiency is prioritized, while visual RL is preferable when state observations aren't available or for simpler tasks.

## Method Summary
The study compares two approaches: State-to-Visual DAgger and visual RL. State-to-Visual DAgger operates in two stages - first training a state policy using SAC, then learning a visual policy through online imitation learning (DAgger) using the state policy as an expert. Visual RL directly trains policies from visual inputs using Asymmetric Actor Critic with SAC. Experiments span 16 tasks across ManiSkill, DMControl, and Adroit benchmarks, evaluating asymptotic performance, sample efficiency (environment steps), and computational cost (wall-clock time).

## Key Results
- State-to-Visual DAgger provides more consistent performance at convergence, particularly excelling in hard tasks where visual RL struggles
- No sample efficiency advantage for State-to-Visual DAgger on easier tasks, with performance similar or slightly worse than visual RL
- Significant wall-clock time efficiency across all tasks, with State-to-Visual DAgger consistently outperforming visual RL
- State-to-Visual DAgger is recommended when visual RL struggles to solve tasks or when wall-clock efficiency is prioritized

## Why This Works (Mechanism)

### Mechanism 1
- Claim: State-to-Visual DAgger achieves more stable and consistent performance at convergence compared to visual RL.
- Mechanism: By isolating visual feature learning from policy learning into two distinct stages, State-to-Visual DAgger avoids the noisy gradient problem that occurs when visual RL must simultaneously learn visual features and action policies through exploration.
- Core assumption: The visual encoder can learn to extract relevant features from expert demonstrations without the compounding errors that occur during RL exploration.
- Evidence anchors:
  - [abstract]: "State-to-Visual DAgger does not universally outperform Visual RL but shows significant advantages in challenging tasks, offering more consistent performance."
  - [section]: "Our results also indicate that State-to-Visual DAgger delivers more consistent performance at convergence, as evidenced by the narrower confidence intervals observed across all benchmarks and difficulty levels."
  - [corpus]: Weak - related works focus on DAgger variants but don't directly address stability advantages over visual RL.
- Break condition: If the state policy fails to generalize to visual observations, or if visual encoder training requires exploration-based fine-tuning that State-to-Visual DAgger cannot provide.

### Mechanism 2
- Claim: State-to-Visual DAgger provides significant wall-clock time efficiency advantages over visual RL.
- Mechanism: State-to-Visual DAgger requires visual encoder training only in the second stage, while the first stage uses fast state-based RL. Visual RL must train the visual encoder throughout the entire training process while also handling visual rendering overhead.
- Core assumption: Visual encoder training is the primary computational bottleneck in visual RL, and state-based RL is significantly faster than visual RL per training step.
- Evidence anchors:
  - [abstract]: "it often reduces the overall wall-clock time required for training."
  - [section]: "Although State-to-Visual DAgger may not enhance sample efficiency, it excels in wall-clock time, consistently outperforming visual RL across all tasks as shown in Fig. 5."
  - [corpus]: Weak - corpus neighbors don't discuss computational efficiency comparisons between methods.
- Break condition: If state-based RL is significantly slower than visual RL per step, or if visual encoder training becomes negligible compared to policy training time.

### Mechanism 3
- Claim: State-to-Visual DAgger provides superior asymptotic performance on hard tasks where visual RL struggles.
- Mechanism: By leveraging privileged state information in the first stage to learn a strong policy, then transferring this knowledge to visual observations through imitation learning, State-to-Visual DAgger bypasses the exploration challenges that hinder visual RL on complex tasks.
- Core assumption: The state policy learned in stage one can effectively guide visual policy learning in stage two, and the privileged information provides sufficient advantage to overcome visual RL's exploration difficulties.
- Evidence anchors:
  - [abstract]: "State-to-Visual DAgger does not universally outperform Visual RL but shows significant advantages in challenging tasks"
  - [section]: "Specifically, State-to-Visual DAgger shows notable superiority in many tasks within the ManiSkill and Adroit benchmarks."
  - [corpus]: Weak - corpus neighbors discuss DAgger applications but not comparative performance on hard vs easy tasks.
- Break condition: If the state policy cannot effectively transfer to visual observations, or if visual RL with asymmetric critic can match state policy performance on the same tasks.

## Foundational Learning

- Concept: Imitation learning (specifically DAgger algorithm)
  - Why needed here: State-to-Visual DAgger uses DAgger in its second stage to learn visual policy from state policy demonstrations
  - Quick check question: What is the key difference between traditional DAgger and the off-policy version used in this work?

- Concept: Reinforcement learning algorithms (SAC and asymmetric actor-critic)
  - Why needed here: State-to-Visual DAgger uses SAC for state policy learning in stage one, while visual RL comparison uses asymmetric actor-critic
  - Quick check question: How does asymmetric actor-critic differ from standard actor-critic in terms of input observation spaces?

- Concept: Visual encoder architectures and their training characteristics
  - Why needed here: Understanding why visual encoder training is computationally expensive and how it differs from state-based policy training
  - Quick check question: What makes training visual encoders with RL more challenging than training them with supervised imitation learning?

## Architecture Onboarding

- Component map:
  - Stage 1: SAC-based state policy learner (MLP architecture, state observations as input) -> Stage 2: DAgger-based visual policy learner (CNN architecture, visual observations as input, state policy as expert) -> Visual RL baseline: Asymmetric Actor-Critic (CNN for actor, state observations for critic)

- Critical path:
  1. Train state policy using SAC until convergence
  2. Initialize visual policy and replay buffer
  3. Collect visual observations by executing visual policy
  4. Compute expert actions using state policy
  5. Train visual policy using imitation learning until loss threshold
  6. Repeat collection and training until convergence

- Design tradeoffs:
  - Off-policy vs on-policy DAgger: Off-policy with replay buffer provides better sample efficiency but requires more memory
  - Fixed vs early stopping updates: Early stopping prevents overfitting and improves wall-clock time but may miss some learning opportunities
  - State policy checkpoint selection: Using latest checkpoint vs best performance checkpoint affects visual policy quality

- Failure signatures:
  - Visual policy fails to learn: Check if state policy is sufficiently good and if imitation loss is decreasing
  - Slow visual policy learning: Verify CNN architecture matches task complexity and that early stopping threshold is appropriate
  - Inconsistent performance: Check if state policy checkpoint is stable and if replay buffer is large enough

- First 3 experiments:
  1. Train state policy on a simple task (e.g., DMControl Acrobot) and verify it achieves good performance
  2. Run State-to-Visual DAgger stage 2 on a task where state policy is known to work well, verify imitation loss decreases
  3. Compare wall-clock time of state-based SAC vs visual RL on a representative task to confirm computational advantage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of checkpoint selection strategy for the state policy in Stage 1 affect the performance and efficiency of State-to-Visual DAgger?
- Basis in paper: [explicit] The paper mentions that "the training of stage 1 is stopped upon convergence, and we save the latest checkpoint. Alternatively, the final checkpoint could be selected based on a predetermined computational budget."
- Why unresolved: The paper does not investigate the impact of different checkpoint selection strategies on the efficiency and performance of State-to-Visual DAgger.
- What evidence would resolve it: Experiments comparing different checkpoint selection strategies (e.g., latest checkpoint vs. best performance checkpoint vs. checkpoint at predetermined computational budget) and their effects on both asymptotic performance and sample efficiency.

### Open Question 2
- Question: What is the precise relationship between task difficulty and the relative performance of State-to-Visual DAgger versus visual RL?
- Basis in paper: [inferred] The paper categorizes tasks as "easy" if state-based RL converges within 4 million steps and finds that State-to-Visual DAgger excels at difficult tasks but performs similarly or slightly worse on easier tasks.
- Why unresolved: The classification of "easy" vs. "hard" tasks based on a 4 million step threshold is described as "not rigorous" and the underlying reasons for why State-to-Visual DAgger performs differently on these task categories remain unclear.
- What evidence would resolve it: A more granular analysis of task difficulty (e.g., based on reward density, state-action space complexity) and corresponding performance breakdowns that explain why State-to-Visual DAgger struggles with easier tasks.

### Open Question 3
- Question: How do different visual encoder architectures affect the relative performance of State-to-Visual DAgger versus visual RL?
- Basis in paper: [inferred] The paper uses different CNN architectures for different benchmarks and mentions that "we refrained from incorporating advanced techniques for image-based feature extraction... that have been recently introduced."
- Why unresolved: The paper does not systematically compare how different visual encoder architectures (including modern ones like Vision Transformers or pretrained backbones) would affect the performance gap between the two methods.
- What evidence would resolve it: Controlled experiments varying visual encoder architectures across both methods while keeping other components constant, to determine if architectural differences explain the performance gaps observed.

## Limitations

- Computational advantage depends on visual encoder training being the primary bottleneck, which may not hold for all task types
- Method requires availability of state observations for initial policy training, limiting applicability in purely visual environments
- Two-stage approach may struggle when state policies cannot effectively transfer knowledge to visual observations

## Confidence

- **High confidence**: Wall-clock time efficiency advantage is well-supported by consistent performance improvements across all tasks
- **Medium confidence**: More consistent performance at convergence, particularly on hard tasks, is supported but could benefit from additional variance analysis
- **Medium confidence**: Recommendation to use State-to-Visual DAgger when visual RL struggles or wall-clock efficiency is prioritized is reasonable but may not capture all practical considerations

## Next Checks

1. **Transferability validation**: Test State-to-Visual DAgger on tasks where state observations are highly abstract or processed features rather than raw state information to verify the method works when state policies are learned from non-privileged information.

2. **Architectural ablation study**: Compare different visual encoder architectures and sizes in both State-to-Visual DAgger and visual RL to quantify the impact of encoder complexity on computational efficiency claims.

3. **Real-world deployment analysis**: Evaluate both methods on physical robot platforms to assess whether the wall-clock advantages observed in simulation translate to real-world training scenarios with additional practical constraints.