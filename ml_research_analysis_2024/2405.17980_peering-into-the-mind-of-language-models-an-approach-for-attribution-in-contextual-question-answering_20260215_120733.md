---
ver: rpa2
title: 'Peering into the Mind of Language Models: An Approach for Attribution in Contextual
  Question Answering'
arxiv_id: '2405.17980'
source_url: https://arxiv.org/abs/2405.17980
tags:
- answer
- attribution
- tokens
- question
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of attributing text spans in
  large language model (LLM) generations to their source documents in contextual question
  answering. The authors propose a novel method that leverages the hidden state representations
  of LLMs to identify and map verbatim-copied tokens from generated answers back to
  their original positions in the document.
---

# Peering into the Mind of Language Models: An Approach for Attribution in Contextual Question Answering

## Quick Facts
- **arXiv ID**: 2405.17980
- **Source URL**: https://arxiv.org/abs/2405.17980
- **Reference count**: 32
- **Primary result**: A method using LLM hidden states to identify and map verbatim-copied tokens to source documents with F1 up to 0.96 and paragraph attribution accuracy up to 77%

## Executive Summary
This paper addresses the challenge of attributing text spans in LLM-generated answers back to their source documents in contextual question answering. The authors propose a novel approach that leverages the hidden state representations of LLMs to identify verbatim-copied tokens and map them to their original positions in the document. Their method operates at token-level granularity without requiring model retraining or retrieval model overhead. Experiments show the approach performs on par or better than GPT-4 baselines across different LLM architectures.

## Method Summary
The authors propose a token-level attribution method that uses hidden state representations from LLM forward passes to identify verbatim-copied text. For each token in the generated answer, the method computes cosine similarity between its hidden state representation and all document token representations. Tokens exceeding a similarity threshold are classified as copied. The method then maps these copied tokens to their source paragraphs using anchor tokens and window ranking. The approach is evaluated on two datasets - QuoteSum and the newly released Verifiability-Granular - across multiple LLM architectures including Llama, Mistral, and OPT models.

## Key Results
- Token identification F1 scores up to 0.96 across multiple model families and datasets
- Paragraph-level attribution accuracy up to 77% for copied spans
- Consistent performance across different LLM architectures without model-specific retraining
- VERIFIABILITY-GRANULAR dataset released with token-level annotations for LLM generations

## Why This Works (Mechanism)

### Mechanism 1
The LLM's hidden state representations encode contextual information about the origin of text spans during generation. During generation, hidden states capture semantic and syntactic features that distinguish between text copied from the document versus text generated by the model itself.

### Mechanism 2
Cosine similarity between hidden state representations effectively identifies verbatim-copied tokens. For each answer token, cosine similarity with document token representations indicates whether it was copied from the document.

### Mechanism 3
Contextual embeddings in later layers provide disambiguation capability for identifying which occurrence of a substring is the source. When substrings appear multiple times, later layers provide contextual embeddings that distinguish between occurrences based on surrounding context.

## Foundational Learning

- **Token-level attribution**: Needed for precise identification of which specific tokens were copied rather than working at sentence/paragraph level. Quick check: Can you explain why token-level attribution provides more precision than sentence-level attribution?

- **Hidden state representations**: Needed to leverage information captured during LLM forward pass without retraining. Quick check: What information is typically encoded in the hidden state representations of LLM tokens?

- **Cosine similarity for embedding comparison**: Needed to measure similarity between token representations for identifying copied text. Quick check: Why is cosine similarity an appropriate metric for comparing hidden state representations?

## Architecture Onboarding

- **Component map**: Document + Question + Answer -> LLM Forward Pass -> Hidden State Extraction -> Cosine Similarity Matrix -> Token Identification -> Span Attribution -> Output

- **Critical path**:
  1. Concatenate document, question, and answer into single prompt
  2. Pass prompt through LLM to obtain hidden state representations
  3. Compute cosine similarity matrix between document and answer token embeddings
  4. Identify tokens above similarity threshold as copied
  5. For each copied token, find best matching document span using average representation similarity

- **Design tradeoffs**:
  - Token-level vs. span-level attribution: Token-level provides more precision but requires more computation
  - Layer selection: Earlier layers capture surface features while later layers capture contextual information
  - Threshold selection: Higher thresholds reduce false positives but may miss copied tokens

- **Failure signatures**:
  - Low precision: High threshold or insufficient distinction between copied and generated tokens
  - Low recall: Low threshold or missing contextual information in hidden states
  - Disambiguation failures: Incorrect attribution when substrings appear multiple times

- **First 3 experiments**:
  1. Test token identification on simple dataset with clear verbatim copies and varying thresholds
  2. Compare attribution accuracy across different LLM layers to identify optimal layers
  3. Test performance on paraphrased vs. verbatim datasets to understand limitations

## Open Questions the Paper Calls Out

### Open Question 1
What is the exact mechanism by which hidden states capture verbatim copying awareness in LLMs? The paper claims hidden states encode copying awareness but doesn't explain specific neural mechanisms or features that enable this capability.

### Open Question 2
How does the performance of the proposed method vary with different prompt engineering approaches? The paper uses a specific prompt format but doesn't explore how variations in prompt engineering might affect attribution accuracy.

### Open Question 3
What is the upper bound of attribution accuracy achievable with this hidden state approach? The paper reports strong performance but doesn't establish theoretical limits or compare against human-level attribution accuracy.

## Limitations
- Limited generalization beyond LLMs to other model architectures like encoder-only transformers
- Performance heavily depends on optimal threshold selection which varies across models and datasets
- Doesn't explicitly handle cases where identical substrings appear in identical contexts multiple times

## Confidence
- **High confidence** in token-level attribution claims: Consistent F1 scores above 0.90 across multiple model families
- **Medium confidence** in cross-model generalization: Limited range of model sizes and architectures tested
- **Medium confidence** in practical applicability: Strong performance on curated datasets but real-world deployment challenges remain

## Next Checks
1. Test the method on encoder-only transformer models (BERT, RoBERTa) and retrieval-augmented generation systems
2. Systematically evaluate how performance degrades with threshold variations across different model sizes
3. Validate the method on documents exceeding 512 tokens to determine if attention limitations affect accuracy in longer contexts