---
ver: rpa2
title: Xmodel-LM Technical Report
arxiv_id: '2406.02856'
source_url: https://arxiv.org/abs/2406.02856
tags:
- language
- zhang
- wang
- training
- xmodel-lm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Xmodel-LM, a compact 1.1B parameter language
  model pretrained on 2 trillion tokens from a balanced Chinese-English corpus. It
  employs a custom tokenizer with 32K vocabulary, RMSNorm, SwiGLU activation, and
  grouped-query attention.
---

# Xmodel-LM Technical Report

## Quick Facts
- **arXiv ID**: 2406.02856
- **Source URL**: https://arxiv.org/abs/2406.02856
- **Authors**: Yichuan Wang; Yang Liu; Yu Yan; Qun Wang; Xucheng Huang; Ling Jiang
- **Reference count**: 9
- **One-line primary result**: Xmodel-LM achieves competitive performance on commonsense reasoning, problem-solving, and Chinese language tasks despite its small 1.1B parameter size.

## Executive Summary
Xmodel-LM is a compact 1.1B parameter language model pretrained on 2 trillion tokens from a balanced Chinese-English corpus. It employs a custom tokenizer with 32K vocabulary, RMSNorm, SwiGLU activation, and grouped-query attention. Trained using AdamW with a global batch size of 840, the model achieves competitive performance on benchmarks, outperforming similar-sized open-source models in commonsense reasoning, problem-solving, and Chinese language tasks.

## Method Summary
Xmodel-LM is trained on a self-built dataset (Xdata) balancing Chinese and English corpora based on downstream task optimization. The model uses a custom tokenizer with 32K vocabulary trained via the unigram algorithm, RMSNorm for normalization, SwiGLU activation, and grouped-query attention. Training employs AdamW optimizer with learning rate 6e-4, batch size 840, and 600k iterations across 7x H800 GPUs.

## Key Results
- Achieves 51.41 average accuracy on commonsense reasoning tasks (ARC, BoolQ, HellaSwag, etc.)
- Scores 40.95 on Chinese language tasks (ARC-zh, XCOPA-zh, XNLI-zh)
- Outperforms similar-sized open-source models on problem-solving benchmarks (BBH, GLUE, GSM8K, MMLU)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model achieves competitive performance despite its small size due to high-quality, balanced Chinese-English pretraining corpus.
- Mechanism: By carefully curating a corpus that balances Chinese and English data based on downstream task optimization, the model gains strong cross-lingual capabilities while maintaining efficiency.
- Core assumption: The quality and balance of the training data directly translate to better generalization and task performance, even with fewer parameters.
- Evidence anchors:
  - [abstract]: "Trained on our self-built dataset (Xdata), which balances Chinese and English corpora based on downstream task optimization, Xmodel-LM exhibits remarkable performance despite its smaller size."
  - [section]: "Our original dataset primarily consists of aggregated training data from other LLMs... To address deficiencies in the distribution of book and mathematical data within the training data distribution, we have also incorporated FanFics and OpenWebMath... Additionally, we have added the Chinese data source PTD and WanJuan to imbue our model with a certain level of proficiency in Chinese."
  - [corpus]: Evidence is present; the corpus composition and sampling weights are detailed in Table 1, showing explicit balancing of Chinese and English sources.
- Break condition: If the corpus curation process is compromised or if the balance between Chinese and English data is skewed, the model's cross-lingual performance may degrade.

### Mechanism 2
- Claim: The custom tokenizer with a small vocabulary (32K) achieves better compression and efficiency than larger vocabularies.
- Mechanism: The unigram algorithm and optimized tokenization strategy (e.g., splitting numbers into digits, setting maximum token length for Chinese phrases) allow the model to represent text more efficiently, leading to better compression rates and potentially faster inference.
- Core assumption: Smaller, more efficient tokenization directly improves model performance and efficiency without sacrificing representational power.
- Evidence anchors:
  - [abstract]: "It employs a custom tokenizer with 32K vocabulary, RMSNorm, SwiGLU activation, and grouped-query attention."
  - [section]: "We employ the unigram algorithm for data tokenization... In contrast to the extensive vocabularies used in prevailing open-source models, our tokenizer is trained on a mixed corpus of Chinese and English, with a vocabulary size of only 32,000... Despite its small size, our tokenizer demonstrates impressive compression rates on test data."
  - [corpus]: The corpus itself is used to train the tokenizer, but specific compression metrics are only provided in Table 2, not directly tied to corpus characteristics.
- Break condition: If the vocabulary size is too small to adequately represent the language nuances, tokenization errors may increase, leading to degraded model performance.

### Mechanism 3
- Claim: Architectural choices (RMSNorm, SwiGLU activation, grouped-query attention) contribute to the model's efficiency and performance.
- Mechanism: These architectural optimizations reduce computational overhead and improve training stability, allowing the model to achieve better performance per parameter compared to baseline models.
- Core assumption: The selected architectural components provide measurable benefits in terms of training efficiency and final model quality.
- Evidence anchors:
  - [abstract]: "It employs a custom tokenizer with 32K vocabulary, RMSNorm, SwiGLU activation, and grouped-query attention."
  - [section]: "To enhance training stability, we utilize the RMSNorm function to normalize the input of each transformer sub-layer... We replace the conventional ReLU non-linearity with the SwiGLU activation function to optimize performance... For efficient training and inference, we employ grouped-query attention (GQA), featuring 32 attention heads and 4 KV heads."
  - [corpus]: No direct evidence in the corpus; the architectural choices are stated as design decisions without empirical comparison to alternatives in the paper.
- Break condition: If the architectural optimizations do not provide the expected efficiency gains or if they introduce instability during training, the model's performance may not improve as intended.

## Foundational Learning

- Concept: Language modeling as unsupervised distribution estimation
  - Why needed here: Understanding that the model learns to predict the next token in a sequence is fundamental to grasping how pretraining works and why large-scale pretraining is effective.
  - Quick check question: What is the primary objective during the pretraining phase of a language model?

- Concept: Tokenization and its impact on model efficiency
  - Why needed here: The choice of tokenizer and vocabulary size directly affects the model's ability to represent language efficiently, which in turn impacts both training and inference performance.
  - Quick check question: How does the size and design of a tokenizer's vocabulary influence the overall efficiency of a language model?

- Concept: Attention mechanisms and their variants
  - Why needed here: Understanding grouped-query attention and its benefits over standard multi-head attention is crucial for appreciating the model's architectural efficiency.
  - Quick check question: What is the main advantage of using grouped-query attention compared to standard multi-head attention in transformer models?

## Architecture Onboarding

- Component map: Tokenizer -> Embedding -> Transformer layers (24 layers, 32 attention heads, 4 KV heads) -> RMSNorm -> SwiGLU activation -> Rotary Positional Embedding -> Grouped-query attention -> AdamW optimization

- Critical path:
  1. Data preprocessing and tokenization
  2. Model initialization with specified architecture
  3. Training loop with AdamW optimization
  4. Gradient accumulation and distributed training
  5. Evaluation on benchmark tasks

- Design tradeoffs:
  - Smaller vocabulary vs. coverage and compression
  - RMSNorm vs. LayerNorm for stability
  - SwiGLU vs. ReLU for activation efficiency
  - Grouped-query attention vs. standard attention for speed
  - Fewer parameters vs. performance on benchmarks

- Failure signatures:
  - Poor tokenization leading to high perplexity
  - Training instability due to RMSNorm or learning rate
  - Suboptimal performance on Chinese tasks if corpus balance is off
  - Memory issues during training due to large batch size

- First 3 experiments:
  1. Train the model for a few steps with a small subset of data to verify tokenization and forward pass work correctly.
  2. Evaluate the model on a held-out validation set to check for overfitting or underfitting early in training.
  3. Compare the model's performance on a few benchmark tasks against a baseline to ensure architectural choices are beneficial.

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset transparency: Specific dataset versions, exact sampling weights, and quality filtering criteria are not fully specified
- Evaluation methodology: Lacks detailed methodology for zero-shot evaluation, including prompt templates and statistical significance testing
- Architectural comparisons: Claims benefits from architectural choices but provides no ablation studies or direct comparisons to baseline configurations

## Confidence
**High confidence**: The basic architectural specification and training procedure are clearly described and internally consistent. The model's benchmark scores are presented transparently with specific numerical values.

**Medium confidence**: Claims about corpus quality and balancing improving performance are supported by qualitative descriptions but lack quantitative validation of the balancing process or its direct impact on downstream tasks.

**Low confidence**: Claims about the specific benefits of architectural choices (RMSNorm, SwiGLU, GQA) over alternatives are stated but not empirically validated through controlled experiments or ablation studies.

## Next Checks
1. Reproduce tokenizer compression metrics: Using the same corpus composition and tokenization parameters (32K vocab, unigram algorithm), measure compression rates on test data and compare against reported values in Table 2 to validate the efficiency claims.

2. Conduct ablation studies: Train baseline versions of Xmodel-LM with alternative architectural choices (LayerNorm vs. RMSNorm, ReLU vs. SwiGLU, standard attention vs. GQA) using identical training settings to empirically validate the claimed performance benefits.

3. Verify corpus balance impact: Create controlled experiments with deliberately imbalanced Chinese-English corpora (varying ratios) and measure downstream performance on Chinese vs. English tasks to quantify the relationship between corpus balance and cross-lingual capability.