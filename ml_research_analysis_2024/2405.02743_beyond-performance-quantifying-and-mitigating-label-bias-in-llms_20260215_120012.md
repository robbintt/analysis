---
ver: rpa2
title: 'Beyond Performance: Quantifying and Mitigating Label Bias in LLMs'
arxiv_id: '2405.02743'
source_url: https://arxiv.org/abs/2405.02743
tags:
- bias
- label
- performance
- tasks
- demonstrations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates label bias in large language models (LLMs)
  and proposes a novel calibration method to mitigate it. The authors evaluate ten
  LLMs across 279 classification tasks, measuring bias using probabilistic and outcomes-based
  metrics.
---

# Beyond Performance: Quantifying and Mitigating Label Bias in LLMs

## Quick Facts
- **arXiv ID**: 2405.02743
- **Source URL**: https://arxiv.org/abs/2405.02743
- **Reference count**: 18
- **Primary result**: Substantial label bias persists in LLMs even after scaling and instruction-tuning; proposed LOOC method reduces bias while improving performance.

## Executive Summary
This paper investigates label bias in large language models (LLMs) and proposes a novel calibration method to mitigate it. The authors evaluate ten LLM families across 279 classification tasks, measuring bias using probabilistic and outcomes-based metrics. They find substantial label bias persists even after scaling, instruction-tuning, and applying existing calibration methods. To address this, they propose Leave-One-Out Calibration (LOOC), which uses in-context demonstrations to estimate bias. LOOC outperforms existing methods in both reducing bias and improving performance, particularly for instruction-tuned models with 8-16 demonstrations.

## Method Summary
The authors evaluate 10 LLM families (Llama-2, Mistral, Falcon) across 279 classification tasks from SUPER-NATURAL INSTRUCTIONS. They measure label bias using two metrics: Relative Standard Deviation (RSD) and BiasScore. The study compares four calibration methods: contextual calibration (CC), domain-context calibration (DC), few-shot LoRA fine-tuning, and the proposed Leave-One-Out Calibration (LOOC). LOOC uses leave-one-out cross-validation on in-context demonstrations to estimate bias parameters, then applies post-hoc probability adjustment. Models are evaluated with k ∈ {0, 2, 4, 8, 16} demonstrations across three different demonstration sets per task.

## Key Results
- Substantial label bias persists across all model families and sizes, even after instruction tuning and existing calibration methods
- LOOC outperforms existing calibration methods, reducing bias while maintaining or improving performance, especially for instruction-tuned models with 8-16 demonstrations
- Bias is highly sensitive to demonstration choice, with different demonstration sets leading to significantly different bias measurements for the same task

## Why This Works (Mechanism)

### Mechanism 1
Label bias persists because output probabilities are dominated by heuristic shortcuts rather than true task understanding. LLMs assign higher probability to certain labels regardless of input, indicating reliance on spurious correlations in pretraining data rather than task-specific reasoning. The bias is present in the model's probability distribution before any calibration or fine-tuning.

### Mechanism 2
Calibration methods fail because they use inputs (content-free or random words) that are not representative of the instruction-tuned data distribution. Existing calibration methods like contextual calibration and domain-context calibration use placeholder or random word inputs to estimate bias, but instruction-tuned models are trained on high-quality, curated examples that differ significantly from these inputs.

### Mechanism 3
The choice of in-context demonstrations significantly impacts both performance and bias, with some demonstration sets exacerbating bias. Different sets of demonstrations lead to different model behaviors, with some sets causing the model to make more biased predictions. This suggests that the demonstrations themselves can introduce or amplify label bias.

## Foundational Learning

- **Concept: Calibration in machine learning**
  - Why needed here: Understanding how post-hoc probability adjustment works is crucial for grasping why existing methods fail and why LOOC succeeds.
  - Quick check question: What is the difference between confidence calibration and label bias calibration?

- **Concept: Leave-one-out cross-validation**
  - Why needed here: LOOC is inspired by leave-one-out cross-validation, so understanding this technique is essential for grasping how LOOC estimates bias.
  - Quick check question: How does leave-one-out cross-validation differ from k-fold cross-validation?

- **Concept: Relative Standard Deviation (RSD)**
  - Why needed here: RSD is one of the key metrics used to quantify label bias in this work, so understanding how it measures class-wise performance disparity is crucial.
  - Quick check question: If a model has an RSD of 0.5, what does this tell you about its performance across different classes?

## Architecture Onboarding

- **Component map**: Input (Task examples with labels) -> Model (LLM) -> Bias Estimation (BiasScore, RSD) -> Mitigation Methods (CC, DC, LoRA, LOOC) -> Output (Calibrated probabilities)

- **Critical path**: 1. Load LLM and prepare task demonstrations, 2. Compute initial bias metrics, 3. Apply mitigation method, 4. Recompute bias metrics to evaluate improvement, 5. Compare performance across different mitigation strategies

- **Design tradeoffs**: Using content-free inputs for calibration vs. using task-relevant demonstrations, Computational cost of LoRA fine-tuning vs. post-hoc calibration, Sensitivity to demonstration choice vs. robustness to bias

- **Failure signatures**: BiasScore remains high after calibration, indicating persistent probability distribution bias, RSD remains high after calibration, indicating persistent performance disparity across classes, Calibration methods that use content-free inputs perform worse than those using task-relevant demonstrations

- **First 3 experiments**: 1. Compare BiasScore and RSD for a base model vs. an instruction-tuned model on the same set of tasks, 2. Apply LOOC to an instruction-tuned model and measure the reduction in both BiasScore and RSD, 3. Test the sensitivity of bias to demonstration choice by prompting the same model with different sets of demonstrations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the extent of label bias in LLMs scale with model size beyond the 40B parameter range?
- Basis in paper: [explicit] The paper explicitly mentions limitations, stating "the models we use are all in the 7B–40B range" and "we chose not to experiment with very large LLMs such as Llama 70B due to limitations in computational resources".
- Why unresolved: The paper's experimental scope was limited to models up to 40B parameters due to computational constraints, leaving the behavior of larger models unexplored.
- What evidence would resolve it: Experiments evaluating label bias in LLMs with parameter counts exceeding 40B, particularly in the 70B+ range, using similar methodologies and metrics.

### Open Question 2
- Question: To what extent does data contamination from SUPER-NATURAL INSTRUCTIONS during instruction tuning affect the observed label bias in Llama-2 models?
- Basis in paper: [explicit] The paper acknowledges potential data contamination, stating "roughly 20% of Flan consists of examples from SUPER-NATURAL INSTRUCTIONS" and "our evaluation of Llama-2 instruction-tuned models is likely affected by data contamination".
- Why unresolved: The paper proceeds with evaluation despite potential contamination but does not quantify its specific impact on label bias measurements or attempt to mitigate it.
- What evidence would resolve it: Comparative studies measuring label bias in Llama-2 models with and without exposure to SUPER-NATURAL INSTRUCTIONS during training, or analysis of bias across tasks with varying levels of potential overlap.

### Open Question 3
- Question: How do different instruction phrasing variations impact label bias across the evaluated tasks?
- Basis in paper: [inferred] The paper mentions limitations regarding prompt format, stating "each task only has one human-written instruction" and suggests "we leave experimentation with more varied formats and examination of bias across different instruction phrasings to future work".
- Why unresolved: The evaluation used a single instruction format per task, limiting understanding of how instruction phrasing variations might influence label bias.
- What evidence would resolve it: Experiments evaluating label bias using multiple instruction phrasings for the same tasks, comparing bias metrics across different formulations to identify sensitivity to phrasing.

## Limitations
- Focus on classification tasks from a single dataset may limit generalizability to other task types and domains
- Choice of bias metrics (RSD and BiasScore) may miss other important dimensions of bias such as semantic or demographic biases
- Effectiveness of LOOC relies heavily on the quality and representativeness of in-context demonstrations, which may vary significantly across tasks and domains

## Confidence

- **High Confidence**: The empirical finding that substantial label bias persists across scaled and instruction-tuned LLMs is well-supported by the extensive evaluation across 279 tasks and multiple model families.

- **Medium Confidence**: The mechanism explanations, particularly regarding why existing calibration methods fail, are plausible but not definitively proven.

- **Low Confidence**: The generalizability of findings to other bias types and task domains remains uncertain, as does the long-term effectiveness of LOOC in real-world deployment scenarios.

## Next Checks
1. Apply LOOC to LLMs on diverse task types beyond classification (e.g., reasoning, generation) to verify its effectiveness across broader task domains.
2. Evaluate whether LOOC reduces other forms of bias (e.g., semantic, demographic) in addition to label bias, using appropriate metrics for these bias types.
3. Systematically test the sensitivity of LOOC to demonstration quality and diversity by varying demonstration sources and measuring the impact on bias reduction and performance.