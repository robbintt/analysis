---
ver: rpa2
title: 'Prompt Recursive Search: A Living Framework with Adaptive Growth in LLM Auto-Prompting'
arxiv_id: '2408.01423'
source_url: https://arxiv.org/abs/2408.01423
tags:
- prompt
- prompts
- language
- steps
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Prompt Recursive Search (PRS), a framework
  for automatically designing prompts in Large Language Models (LLMs) that dynamically
  adapts to problem complexity. Unlike static Expert-Designed Prompts (EDPs), which
  are fixed and can waste tokens on simple problems, or LLM-Derived Prompts (LDPs),
  which may accumulate errors on complex problems, PRS uses the LLM to recursively
  break down problems based on assessed complexity.
---

# Prompt Recursive Search: A Living Framework with Adaptive Growth in LLM Auto-Prompting

## Quick Facts
- arXiv ID: 2408.01423
- Source URL: https://arxiv.org/abs/2408.01423
- Reference count: 38
- Primary result: PRS achieves 44% accuracy on BBH benchmark, outperforming Chain of Thought by 22%

## Executive Summary
Prompt Recursive Search (PRS) introduces an adaptive framework for automatic prompt generation in large language models that dynamically adjusts to problem complexity. Unlike static Expert-Designed Prompts (EDPs) or potentially error-prone LLM-Derived Prompts (LDPs), PRS recursively decomposes problems based on self-assessed complexity scores. The framework assigns complexity ratings from 1-10 and determines solution steps accordingly, achieving significant performance gains over traditional Chain of Thought methods while reducing dependency on human expertise.

## Method Summary
PRS operates by having the LLM assess problem complexity and recursively break down complex tasks into manageable subproblems. The framework assigns each problem a complexity score between 1-10, then determines the appropriate number of solution steps based on this assessment. For simple problems, PRS avoids unnecessary token expenditure by directly solving without excessive decomposition. For complex problems, it recursively splits them into subproblems until reaching manageable complexity levels. The method was tested on the BBH dataset across multiple domains, demonstrating superior performance to both static EDPs and recursive LDP approaches.

## Key Results
- PRS achieves 44% accuracy on BBH benchmark compared to 36% for Chain of Thought with Llama3-7B
- Framework shows 9% accuracy improvement over baseline methods with Yi-34B model
- Demonstrates significant reduction in human expertise dependency while maintaining high performance

## Why This Works (Mechanism)
The adaptive complexity assessment mechanism allows PRS to allocate computational resources proportionally to problem difficulty. By having the model self-assess complexity before solving, PRS prevents both over-engineering simple problems and under-resourcing complex ones. The recursive decomposition ensures that problems are broken down to appropriate granularity levels, while the complexity-based step determination prevents token waste on straightforward tasks and ensures sufficient depth for challenging problems.

## Foundational Learning
- **Complexity Scoring (1-10)**: Self-assessment mechanism for problem difficulty; needed to adapt solution depth; quick check: verify consistency across similar problem types
- **Recursive Decomposition**: Breaking problems into subproblems until manageable; needed for handling complex reasoning; quick check: ensure termination conditions prevent infinite recursion
- **Adaptive Step Determination**: Linking complexity scores to solution steps; needed for resource efficiency; quick check: validate correlation between complexity and actual problem difficulty
- **LLM Self-Evaluation**: Model's ability to assess its own performance requirements; needed for autonomous adaptation; quick check: test on problems with known difficulty ratings

## Architecture Onboarding
- **Component Map**: Problem Input -> Complexity Assessor -> Recursive Solver -> Subproblem Generator -> Solution Aggregator
- **Critical Path**: Complexity assessment → recursive decomposition → solution aggregation
- **Design Tradeoffs**: Balancing recursion depth vs. token efficiency; complexity accuracy vs. assessment overhead
- **Failure Signatures**: Over-simplification of complex problems; unnecessary recursion on simple problems; incorrect complexity assessment leading to inadequate solution steps
- **First Experiments**: 1) Test complexity scoring consistency on varied problem sets; 2) Measure token usage efficiency across complexity levels; 3) Validate recursion termination conditions

## Open Questions the Paper Calls Out
None

## Limitations
- Performance claims based on single dataset (BBH) without broader validation
- Computational overhead from recursive calls not thoroughly analyzed
- Potential circularity in model's self-assessment of problem difficulty

## Confidence
- BBH benchmark performance claims: Medium confidence (single dataset, specific model)
- Adaptive complexity assessment mechanism: Low-Medium confidence (theoretical soundness, potential circularity)
- Framework generalizability across domains: Low confidence (limited experimental scope)

## Next Checks
1. Replicate experiments on at least two additional reasoning benchmarks with different problem distributions to verify the 22% improvement claim
2. Conduct ablation studies removing the complexity scoring component to quantify its specific contribution versus standard recursive prompting
3. Measure and report per-instance token usage and wall-clock time to assess practical deployment costs compared to CoT and other baselines