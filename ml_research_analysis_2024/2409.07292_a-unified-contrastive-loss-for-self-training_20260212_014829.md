---
ver: rpa2
title: A Unified Contrastive Loss for Self-Training
arxiv_id: '2409.07292'
source_url: https://arxiv.org/abs/2409.07292
tags:
- learning
- loss
- contrastive
- semi-supervised
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a semi-supervised learning framework that unifies
  supervised and unsupervised contrastive learning within self-training approaches.
  The authors propose a semi-supervised contrastive (SSC) loss that handles labeled,
  pseudo-labeled, and unconfident unlabeled examples simultaneously using class prototypes.
---

# A Unified Contrastive Loss for Self-Training

## Quick Facts
- arXiv ID: 2409.07292
- Source URL: https://arxiv.org/abs/2409.07292
- Reference count: 39
- Primary result: SSC loss improves FixMatch by 6.2-20.4% on CIFAR-100, STL-10, SVHN with 4-25 labels per class

## Executive Summary
This paper presents a semi-supervised learning framework that unifies supervised and unsupervised contrastive learning within self-training approaches. The authors propose a semi-supervised contrastive (SSC) loss that handles labeled, pseudo-labeled, and unconfident unlabeled examples simultaneously using class prototypes. When applied to FixMatch, their approach achieves significant performance improvements across CIFAR-100, STL-10, and SVHN datasets with limited labeled data (4-25 labels per class). The method demonstrates faster convergence, better transfer learning capabilities with pre-training, and improved stability to hyperparameters compared to standard self-training approaches.

## Method Summary
The method extends FixMatch by replacing the standard cross-entropy loss with a unified semi-supervised contrastive (SSC) loss. This loss incorporates three types of examples: labeled data with ground truth labels, unlabeled data with pseudo-labels above a confidence threshold, and unlabeled data below the threshold assigned unique labels. Class prototypes serve as trainable parameters in the embedding space that enable probability distribution recovery without requiring an additional cross-entropy phase. The approach theoretically unifies supervised and unsupervised contrastive learning, and empirically demonstrates superior performance on standard semi-supervised benchmarks.

## Key Results
- SSC loss improves FixMatch accuracy by 6.2% on CIFAR-100 with 4 labels per class
- Achieves 20.4% improvement on CIFAR-100 with 25 labels per class
- Demonstrates faster convergence and better stability to hyperparameters compared to baseline self-training methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The unified contrastive loss handles all unlabeled examples simultaneously, including those below the confidence threshold, by assigning unique pseudo-labels to unconfident examples.
- Mechanism: By concatenating supervised embeddings (Z_x), unsupervised embeddings (Z_u), and prototype embeddings (Z_c) into a single loss function, the method creates positive pairs across all data types. Unconfident examples receive unique labels shifted by K, ensuring they form self-pairs that contribute to representation learning.
- Core assumption: Unconfident examples still contain useful information for representation learning even when their predictions are unreliable.
- Evidence anchors:
  - [abstract] "our proposed approach is the only one which takes advantage of all labeled and unlabeled training data for learning in a fully contrastive framework"
  - [section 3.1] "rather than disregarding examples that have a posterior probability below the threshold τ, we assign unique labels to them using yu↓"

### Mechanism 2
- Claim: Class prototypes enable probability distribution recovery without requiring an additional cross-entropy loss phase.
- Mechanism: Prototypes act as trainable class centers in the embedding space. By computing softmax over cosine similarities between weakly augmented embeddings and prototypes, the method generates probability distributions for pseudo-labeling.
- Core assumption: The prototype-based probability distribution approximates the true class distribution sufficiently well for effective pseudo-labeling.
- Evidence anchors:
  - [section 3.1] "we propose the use of class prototypes... which lie directly in the embeddings space" and "We define the label prototypes as yc = [1, 2, ..., K]"
  - [section 3.3] "we establish a relationship between cross-entropy (CE) loss and our framework using contrastive learning loss using prototypes"

### Mechanism 3
- Claim: The theoretical equivalence between cross-entropy and supervised contrastive learning with prototypes provides theoretical justification for the approach.
- Mechanism: When temperature is set to T=1 and embeddings are normalized, cross-entropy loss with weight matrix W is equivalent to applying separate supervised contrastive losses where each example's only positive pair is its class prototype.
- Core assumption: The mathematical equivalence holds under the specified conditions (T=1, normalized embeddings, no bias).
- Evidence anchors:
  - [section 3.3] "If we set the temperature of the SupCon loss to T = 1, and ensure the normalization of all embeddings... we get: H(Z x, y) = 1/B Σ LSupCon"
  - [section 3.3] "Both losses aim fundamentally to learn prototypes Z c or equivalently weights W to be aligned with their corresponding feature vectors"

## Foundational Learning

- Concept: Contrastive learning principles (InfoNCE loss, positive/negative pair construction)
  - Why needed here: The entire framework builds on contrastive learning foundations, extending supervised and unsupervised contrastive losses to the semi-supervised setting
  - Quick check question: Can you explain how InfoNCE loss pulls positive pairs together while pushing negative pairs apart in embedding space?

- Concept: Self-training with pseudo-labeling and confidence thresholding
  - Why needed here: The framework operates within a self-training paradigm, generating pseudo-labels for unlabeled data based on model confidence
  - Quick check question: How does the confidence threshold τ affect which unlabeled examples receive pseudo-labels versus unique labels?

- Concept: Prototype-based classification and representation learning
  - Why needed here: Prototypes serve as both class centers for probability distribution generation and as trainable parameters within the contrastive loss
  - Quick check question: What is the relationship between prototype embeddings and traditional classifier weights in a neural network?

## Architecture Onboarding

- Component map: Input → Encoder f → Three embeddings (Z_x, Z_u, Z_w) → Prototype probability computation → Pseudo-label generation → Unified contrastive loss L_SSC → Parameter updates
- Critical path: Input → Encoder → Three embeddings (Z_x, Z_u, Z_w) → Prototype probability computation → Pseudo-label generation → Unified contrastive loss → Parameter updates
- Design tradeoffs:
  - Including unconfident examples improves data utilization but may introduce noise
  - Using prototypes avoids additional CE loss but requires careful initialization and training
  - Unified loss simplifies optimization but requires careful weighting of different example types
- Failure signatures:
  - Poor performance with very few labeled examples may indicate insufficient supervision
  - Degradation when including too many unconfident examples suggests noise accumulation
  - Instability during training could indicate improper temperature or weighting parameters
- First 3 experiments:
  1. Implement the unified contrastive loss with only labeled data (Z_x and Z_c) to verify the prototype-based probability generation
  2. Add unlabeled data with pseudo-labels above threshold (Z_u and yu↑) to test the basic semi-supervised framework
  3. Include unconfident examples with unique labels (yu↓) to verify the complete unified approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed SSC loss perform with noisy labels or corrupted data?
- Basis in paper: [inferred] The paper focuses on standard semi-supervised learning but doesn't address robustness to label noise
- Why unresolved: The experiments use clean datasets (CIFAR-100, STL-10, SVHN) without simulating label corruption
- What evidence would resolve it: Experiments comparing SSC performance on datasets with varying degrees of label noise or synthetic label corruption

### Open Question 2
- Question: What is the impact of different prototype initialization strategies on final performance?
- Basis in paper: [explicit] The paper mentions prototypes are "initialized randomly" but doesn't explore alternative initialization methods
- Why unresolved: The authors only use random initialization without comparing to other strategies like k-means or pre-trained embeddings
- What evidence would resolve it: Experiments testing various initialization schemes (random, k-means, pre-trained) and their effect on convergence and final accuracy

### Open Question 3
- Question: How does the framework scale to larger datasets and more complex architectures?
- Basis in paper: [inferred] Experiments are limited to relatively small datasets (CIFAR-100, STL-10, SVHN) and standard architectures (WRN-28-2, WRN-37-2)
- Why unresolved: No experiments on larger-scale datasets like ImageNet or with modern architectures like Vision Transformers
- What evidence would resolve it: Scaling experiments on larger datasets with more parameters, measuring computational efficiency and performance trends

## Limitations

- The approach may accumulate noise from unconfident examples, potentially degrading representation quality
- Performance relies on proper temperature and weighting parameter tuning, which may be dataset-dependent
- Theoretical equivalence between CE and SupCon requires strict conditions (T=1, normalized embeddings) that may not hold in practice

## Confidence

- High confidence: Basic SSC loss formulation and implementation on FixMatch
- Medium confidence: Claims about faster convergence and improved hyperparameter stability
- Medium confidence: Theoretical equivalence claims under specified conditions
- Low confidence: Generalizability to other domains beyond vision tasks

## Next Checks

1. **Ablation study on unconfident examples**: Systematically vary the number of unconfident examples included (by adjusting τ) to quantify the tradeoff between data utilization and noise introduction.

2. **Prototype quality analysis**: Visualize and analyze the learned prototypes across different datasets to verify they capture meaningful class distributions and don't collapse to degenerate solutions.

3. **Theoretical condition validation**: Experimentally verify how much the theoretical equivalence between CE and SupCon degrades when relaxing the assumptions (T≠1, non-normalized embeddings, inclusion of bias terms).