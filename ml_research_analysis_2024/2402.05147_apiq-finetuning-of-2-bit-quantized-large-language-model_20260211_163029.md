---
ver: rpa2
title: 'ApiQ: Finetuning of 2-Bit Quantized Large Language Model'
arxiv_id: '2402.05147'
source_url: https://arxiv.org/abs/2402.05147
tags:
- quantization
- apiq
- finetuning
- loftq
- apiq-bw
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ApiQ addresses the problem of inconsistent performance in memory-efficient
  finetuning of large language models (LLMs) across different bit-width quantizations
  and tasks, caused by quantization error leading to catastrophic forgetting. The
  core method idea is to restore lost information from quantization by concurrently
  initializing LoRA components and quantizing LLM weights, preserving the original
  LLM's activation precision while mitigating error propagation.
---

# ApiQ: Finetuning of 2-Bit Quantized Large Language Model

## Quick Facts
- arXiv ID: 2402.05147
- Source URL: https://arxiv.org/abs/2402.05147
- Reference count: 40
- Primary result: ApiQ consistently outperforms baselines across various bit-widths, achieving superior finetuning results on a spectrum of language tasks with different LLMs.

## Executive Summary
ApiQ addresses the problem of inconsistent performance in memory-efficient finetuning of large language models (LLMs) across different bit-width quantizations and tasks, caused by quantization error leading to catastrophic forgetting. The core method idea is to restore lost information from quantization by concurrently initializing LoRA components and quantizing LLM weights, preserving the original LLM's activation precision while mitigating error propagation. Primary results show ApiQ consistently outperforms baselines across various bit-widths, achieving superior finetuning results on a spectrum of language tasks with different LLMs. Specifically, ApiQ achieves the best perplexity in post-training quantization and the highest accuracy in finetuning tasks like GLUE, WikiText-2, GSM8K, arithmetic reasoning, and commonsense reasoning.

## Method Summary
ApiQ is a framework for memory-efficient fine-tuning of quantized large language models. It operates in two steps: 1) Quantization step: Initialize Q, A, B to preserve activation and mitigate quantization error propagation; 2) Finetuning step: Freeze Q, train A and B with LoRA modules integrated into all linear layers. ApiQ optimizes the quantization function and LoRA components jointly to minimize activation error, ensuring the quantized output closely matches the original output at each layer. This approach preserves activation precision across quantized layers, preventing error propagation from shallower to deeper layers.

## Key Results
- ApiQ achieves the best perplexity in post-training quantization, with 7.59 on WikiText-2 for 2-bit quantization of Llama-2-7B compared to 1000+ for LoftQ.
- ApiQ consistently outperforms baselines across various bit-widths, achieving superior finetuning results on a spectrum of language tasks with different LLMs.
- ApiQ demonstrates compatibility with a broader range of PEFT methods without necessitating adaptations for every linear layer.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ApiQ preserves activation precision across quantized layers, preventing error propagation from shallower to deeper layers.
- Mechanism: ApiQ optimizes Q, A, and B to minimize the activation error ||XW - X_q(Q + AB⊤)|| instead of the weight error. By enforcing that the quantized output closely matches the original output at each layer, it mitigates error accumulation through the network.
- Core assumption: Activation error is a better proxy for model degradation than weight error in quantized LLMs.
- Evidence anchors:
  - [abstract]: "ApiQ demonstrably minimizes activation error during quantization... consistently achieves superior finetuning results across various bit-widths."
  - [section]: "ApiQ has two primary advantages. Firstly, it ensures that the output from the quantized linear layer closely aligns with the original output... Secondly, it potentially mitigates the quantization error from shallower layers into deeper layers."
- Break condition: If the activation error metric does not correlate with downstream task performance, or if error propagation is dominated by other factors (e.g., architecture depth beyond compensation).

### Mechanism 2
- Claim: ApiQ’s joint optimization of Q, A, and B provides better initialization than weight-only preservation methods.
- Mechanism: ApiQ solves argmin ||XW - X_q(Q + AB⊤)||, jointly optimizing the quantized weight Q and the LoRA components A and B. This initialization leads to a more balanced distribution of quantization error across the weight and adaptation parameters, avoiding large outliers.
- Core assumption: A and B initialized to a Gaussian-like distribution (as in ApiQ) are more trainable than default B=0 or weight-only preservation.
- Evidence anchors:
  - [abstract]: "ApiQ designed to restore the lost information from quantization by concurrently initializing the LoRA components and quantizing the weights of LLMs."
  - [section]: "ApiQ addresses this by centering AB⊤ in this critical region. Additionally, the distribution span of ApiQ’sA and B is significantly narrower compared toW and LoftQ, suggesting the potential for further quantizing A and B."
- Break condition: If the distribution of A and B does not correlate with finetuning stability or if joint optimization overfits to calibration data.

### Mechanism 3
- Claim: ApiQ is compatible with a broader range of PEFT methods due to its block-wise quantization approach.
- Mechanism: ApiQ-bw optimizes entire transformer blocks rather than individual layers, allowing A and B to be any trainable parameters (not just LoRA matrices). This flexibility enables integration with DoRA, Adapter, etc., without modifying the algorithm.
- Core assumption: Block-wise quantization is compatible with PEFT methods that combine addition and multiplication operations.
- Evidence anchors:
  - [section]: "ApiQ-bw is compatible with a broader range of PEFT methods without necessitating adaptations for every linear layer. The matrices As and Bs do not have to be the low-rank matrices from LoRA; they can be trainable parameters from any PEFT method, such as DoRA, (IA)3, HiWi and Adapter."
  - [corpus]: Weak. The corpus neighbors focus on quantization and PEFT, but no direct evidence for block-wise compatibility with multiple PEFT methods.
- Break condition: If block-wise quantization introduces optimization instability or if the gradient flow through A and B is disrupted by the block structure.

## Foundational Learning

- Concept: Quantization error and its propagation through neural network layers.
  - Why needed here: Understanding why preserving activation precision is more effective than weight-only preservation requires knowing how quantization errors accumulate and affect downstream computations.
  - Quick check question: In a two-layer network, if the first layer has quantization error δW₀, how does this error affect the second layer’s activation error? (Answer: The second layer’s error includes terms like X₀W₀δW₁ + X₀δW₀W₁ - X₀δW₀δW₁, showing error propagation.)

- Concept: Low-Rank Adaptation (LoRA) and parameter-efficient fine-tuning.
  - Why needed here: ApiQ integrates LoRA initialization into quantization; understanding LoRA’s mechanics and limitations is crucial for grasping why joint optimization helps.
  - Quick check question: What is the primary advantage of LoRA over full fine-tuning in terms of memory usage? (Answer: LoRA introduces far fewer trainable parameters, drastically reducing optimizer state memory.)

- Concept: Straight-through estimator (STE) and gradient flow through quantization functions.
  - Why needed here: ApiQ uses STE to enable gradient updates through the round-to-nearest operation in quantization; understanding STE is key to debugging training stability.
  - Quick check question: Why can’t standard backpropagation be used through the round-to-nearest operation in quantization? (Answer: The round operation is non-differentiable; STE approximates the gradient as 1 for the forward value and 0 for the quantization error.)

## Architecture Onboarding

- Component map:
  - Calibration data → Sequential layer/block optimization → Trained Q, A, B matrices → Saved QLLM → Fine-tuning with fixed Q.
  - Key components: Quantization function f (with learnable γ, β), LoRA modules (A, B), STE for gradient flow.

- Critical path:
  1. Load calibration samples (128 sentences from WikiText-2 training).
  2. Sequentially optimize each linear layer (ApiQ-lw) or transformer block (ApiQ-bw) to minimize activation error.
  3. Save the quantized weights Q and initialized LoRA components A, B.
  4. During fine-tuning, freeze Q and train A, B with standard optimizer.

- Design tradeoffs:
  - ApiQ-lw: Lower memory usage during quantization (only one layer’s activations cached) but slower (sequential layer-by-layer).
  - ApiQ-bw: Faster quantization (block-wise) but higher memory usage (caching activations within a block).
  - Joint optimization vs. weight-only preservation: Better initialization and error distribution but requires calibration data and more computation.

- Failure signatures:
  - Poor finetuning performance despite low activation error → Activation error metric not predictive of downstream task performance.
  - Unstable training or NaNs during fine-tuning → Issues with gradient flow through quantization or improper initialization of A, B.
  - High memory usage during quantization → Block size too large or activations not properly freed in ApiQ-bw.

- First 3 experiments:
  1. Replicate Table 1: Compare ApiQ-lw vs. LoftQ vs. QLoRA on MNLI and WikiText-2 with different trainable LoRA positions to verify activation error reduction and position independence.
  2. Replicate Table 2: Evaluate ApiQ-bw as a pure PTQ method on WikiText-2 and C4 for 2, 3, and 4-bit quantization to confirm perplexity improvements.
  3. Replicate Table 6: Fine-tune ApiQ-bw on WikiText-2 and GSM8K with Llama-2-7B, 13B, and Mistral-7B to validate finetuning gains across bit-widths and models.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical limit of ApiQ's performance on extremely low-bit quantization (e.g., 1-bit or 0.5-bit) for LLMs?
- Basis in paper: [inferred] The paper demonstrates ApiQ's effectiveness at 2-bit and 3-bit quantization but doesn't explore lower bit-widths or establish theoretical bounds.
- Why unresolved: The paper only evaluates ApiQ up to 2-bit quantization. Lower bit-widths introduce additional challenges like severe information loss and may require novel techniques beyond ApiQ's current approach.
- What evidence would resolve it: Experiments comparing ApiQ's performance on 1-bit or 0.5-bit quantization against other methods, along with theoretical analysis of information preservation limits at extremely low bit-widths.

### Open Question 2
- Question: How does ApiQ's performance scale with increasing LLM size (e.g., 70B or 175B parameters) and what are the computational/memory bottlenecks?
- Basis in paper: [inferred] The paper tests ApiQ on models up to 13B parameters but doesn't explore scalability to frontier-scale LLMs or identify bottlenecks.
- Why unresolved: The paper doesn't evaluate ApiQ on the largest available LLMs, and computational/memory constraints may limit its applicability to these models without architectural modifications.
- What evidence would resolve it: Empirical results showing ApiQ's performance and resource usage on 70B+ parameter models, along with analysis of which components (quantization, LoRA, optimization) become bottlenecks.

### Open Question 3
- Question: Can ApiQ's activation preservation technique be extended to multimodal LLMs (text-image, text-video) and what modifications would be needed?
- Basis in paper: [inferred] ApiQ is only tested on text-based models, but the activation preservation concept could theoretically apply to other modalities where quantization causes information loss.
- Why unresolved: The paper focuses exclusively on text-based language models. Extending to multimodal settings would require addressing modality-specific quantization challenges and potentially different error metrics.
- What evidence would resolve it: Successful implementation of ApiQ on multimodal models showing improved finetuning performance, along with analysis of modality-specific adaptations needed for activation preservation.

## Limitations

- Calibration Data Dependency: ApiQ's performance hinges on calibration samples from the target domain. The paper uses 128 WikiText-2 sentences, but this choice is arbitrary. Generalization across diverse datasets or tasks without recalibration remains untested.
- STE Gradient Approximation: The straight-through estimator (STE) approximates gradients through the non-differentiable quantization function. While common in quantization literature, STE can introduce bias, especially in low-bit regimes (2-bit).
- Block-wise Quantization Compatibility: ApiQ-bw claims compatibility with PEFT methods beyond LoRA (e.g., DoRA, Adapter), but the evidence is weak. The paper only tests LoRA in fine-tuning, leaving open the question of whether block-wise quantization disrupts gradient flow or optimization for other PEFT methods.

## Confidence

- **High Confidence**: ApiQ's core mechanism of preserving activation precision during quantization is well-supported by experimental results (Tables 1, 2, 6). The activation error metric correlates with improved finetuning performance, and the initialization of A and B is demonstrably better than weight-only preservation methods.
- **Medium Confidence**: The claim that ApiQ-bw is compatible with a broader range of PEFT methods is plausible but under-validated. The paper's evidence is theoretical (algorithmic compatibility) rather than empirical (testing multiple PEFT methods).
- **Low Confidence**: The long-term stability of ApiQ-trained models under distribution shifts or in tasks requiring reasoning over long contexts (e.g., GSM8K) is not rigorously tested. The paper's ablation studies are limited to ablation of LoRA positions, not ablation of calibration data size or STE gradient quality.

## Next Checks

1. **Calibration Data Sensitivity**: Replicate ApiQ's quantization step with varying calibration data sizes (e.g., 32, 64, 256 sentences) and domains (e.g., C4, GLUE) to test whether activation error and finetuning performance degrade with domain mismatch or insufficient calibration data.

2. **STE Gradient Quality**: Implement a differentiable quantization proxy (e.g., soft quantization) and compare its gradient flow and finetuning performance against STE in ApiQ. This will validate whether STE's approximation error impacts convergence or leads to suboptimal minima.

3. **PEFT Method Compatibility**: Extend ApiQ-bw to integrate with Adapter or DoRA instead of LoRA, and evaluate finetuning performance on a subset of tasks (e.g., MNLI, WikiText-2). This will empirically validate the claim of broader PEFT compatibility.