---
ver: rpa2
title: 'STAIR: Spatial-Temporal Reasoning with Auditable Intermediate Results for
  Video Question Answering'
arxiv_id: '2401.03901'
source_url: https://arxiv.org/abs/2401.03901
tags:
- video
- question
- program
- neural
- module
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes STAIR, a video question answering model based
  on neural module networks that can perform compositional spatial-temporal reasoning
  on long and informative videos. STAIR decomposes a given question into a hierarchical
  combination of several sub-tasks using a program generator, and then dynamically
  assembles a neural module network to complete each sub-task using lightweight neural
  modules.
---

# STAIR: Spatial-Temporal Reasoning with Auditable Intermediate Results for Video Question Answering

## Quick Facts
- arXiv ID: 2401.03901
- Source URL: https://arxiv.org/abs/2401.03901
- Reference count: 27
- This paper proposes STAIR, a video question answering model based on neural module networks that can perform compositional spatial-temporal reasoning on long and informative videos.

## Executive Summary
STAIR introduces a novel approach to video question answering by decomposing questions into hierarchical sub-tasks and dynamically assembling neural module networks to execute them. Unlike previous works that return attention maps, STAIR's modules return intermediate outputs specific to their intended functions, improving interpretability and enabling collaboration with pre-trained models. The model demonstrates strong performance across multiple video QA datasets while providing auditable reasoning traces.

## Method Summary
STAIR uses a program generator to convert questions into executable programs that determine which neural modules to assemble and execute. The model contains 16 lightweight neural modules that perform specific sub-tasks like filtering objects, localizing events in time, and temporal reasoning. Each module returns outputs specific to its intended function rather than generic attention maps. The system uses intermediate supervision during training, where ground truth intermediate results from scene graph program execution are used to supervise corresponding modules. The model supports both RX and I3D video features and employs a two-stage training approach with program generator training followed by main model training with a combined classification and intermediate supervision loss.

## Key Results
- STAIR achieves 57.11% accuracy on AGQA, outperforming end-to-end baseline (STAIR-E2E) by 3.03%
- On AGQA2 with reduced language bias, STAIR reaches 56.05% accuracy
- STAIR's intermediate outputs (Filter, Localize, Temporal) achieve high recall and IoU scores, demonstrating accurate execution of sub-tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: STAIR's modular decomposition enables better compositional reasoning than end-to-end models
- Mechanism: The program generator decomposes questions into sub-tasks, and lightweight neural modules execute each sub-task. This explicit decomposition allows STAIR to handle temporal reasoning by localizing events and understanding their order.
- Core assumption: Video understanding requires different abilities than image understanding, and compositional decomposition helps address this.
- Evidence anchors:
  - [abstract]: "STAIR decomposes a given question into a hierarchical combination of several sub-tasks using a program generator, and then dynamically assembles a neural module network to complete each sub-task"
  - [section]: "We define a set of basic video-text sub-tasks for video question answering and design a set of lightweight modules to complete them"
  - [corpus]: Found 25 related papers with average neighbor FMR=0.36, indicating moderate relatedness. Titles suggest STAIR is addressing a gap in video reasoning.
- Break condition: If the program generator fails to correctly decompose questions, or if modules cannot handle the diversity of video reasoning tasks, the compositional approach would fail.

### Mechanism 2
- Claim: Returning intermediate results specific to module intentions improves interpretability and enables collaboration with pre-trained models
- Mechanism: Unlike previous NMNs that always return attention maps, STAIR's modules return outputs specific to their intended function (e.g., Filter returns object names, Localize returns frame indices). This makes the reasoning process auditable.
- Core assumption: Intermediate results that are human-interpretable can serve as prompts to improve pre-trained model performance.
- Evidence anchors:
  - [abstract]: "Different from most prior works, modules of STAIR return intermediate outputs specific to their intentions instead of always returning attention maps, which makes it easier to interpret and collaborate with pre-trained models"
  - [section]: "Take the module Filter(video,objects) as an example, it intends to find all objects that appear in the video. Instead of returning an attention map showing when the objects occur, in our implementation it must return a feature vector from which we can predict the names of all objects in the video"
  - [corpus]: Papers like "Bridging Vision Language Models and Symbolic Grounding for Video Question Answering" suggest a trend toward interpretable video reasoning.
- Break condition: If the intermediate outputs are not accurate or human-interpretable, the auditable property fails and collaboration with pre-trained models would not work.

### Mechanism 3
- Claim: Intermediate supervision improves the accuracy of module outputs
- Mechanism: Ground truth intermediate results are obtained by executing the scene graph program on video scene graphs, and used to supervise the corresponding modules during training.
- Core assumption: Modules may not behave as expected without direct supervision of their intermediate outputs.
- Evidence anchors:
  - [section]: "To mitigate this problem, we use intermediate supervision to induce supervision to intermediate modules"
  - [section]: "Given that nmn program is obtained by converting sg program using a rule-based approach, we can record the correspondence between functions in sg program and modules in nmn program. Then we execute sg program on the video scene graph and take the return value of functions as ground truth answers of corresponding modules"
  - [corpus]: Weak evidence - no directly relevant papers found in corpus.
- Break condition: If the scene graph program execution doesn't provide accurate ground truth intermediate results, or if intermediate supervision causes optimization difficulties, this mechanism would fail.

## Foundational Learning

- Concept: Neural Module Networks (NMNs) for compositional reasoning
  - Why needed here: STAIR builds on the NMN approach but extends it to video, which requires different abilities than image reasoning
  - Quick check question: What are the key differences between applying NMNs to videos versus images?

- Concept: Program generation and execution for question decomposition
  - Why needed here: STAIR uses a program generator to convert questions to programs that determine module layout, enabling compositional reasoning
  - Quick check question: How does STAIR handle cases where the program generator produces incorrect programs?

- Concept: Intermediate supervision for training neural modules
  - Why needed here: STAIR introduces intermediate supervision to ensure modules produce accurate intermediate outputs, which is crucial for interpretability
  - Quick check question: What are the different loss functions used for different types of module outputs in STAIR?

## Architecture Onboarding

- Component map: Video features (RX/I3D) -> Video Encoder (bi-LSTM) -> Program Generator (LSTM/FLAN-T5) -> Neural Module Network (16 modules) -> Classifier -> Answer
- Critical path: Question -> Program Generator -> Neural Module Network -> Classifier -> Answer
  The program generator determines which modules are assembled and executed, making it critical for the overall reasoning process.
- Design tradeoffs:
  - Modularity vs. end-to-end training: STAIR uses modular decomposition for interpretability but this may limit end-to-end optimization
  - Intermediate supervision: Improves module accuracy but adds complexity and requires ground truth intermediate results
  - Video feature choice: STAIR supports both RX and I3D features, trading off temporal resolution vs. pre-training
- Failure signatures:
  - Program generator produces incorrect programs -> wrong modules assembled
  - Modules return inaccurate intermediate results -> incorrect final answers despite correct program
  - Intermediate supervision causes optimization difficulties -> training instability
- First 3 experiments:
  1. Verify program generator produces correct programs on AGQA validation set
  2. Test individual modules with ground truth inputs to ensure they produce correct outputs
  3. Evaluate intermediate outputs (Filter, Localize, Temporal) on validation set to verify interpretability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does intermediate supervision impact the accuracy of intermediate outputs in STAIR across different types of video question answering tasks beyond AGQA?
- Basis in paper: [explicit] The paper mentions using intermediate supervision to improve the accuracy of intermediate results in STAIR modules, but also notes that it does not always improve model performance due to potential coordination problems among multi-task learning losses.
- Why unresolved: The paper provides limited experimental evidence on the impact of intermediate supervision across various datasets and task types. It mentions the potential benefits but does not explore the consistency of these benefits across different scenarios.
- What evidence would resolve it: Conducting a series of experiments across multiple video QA datasets with varying levels of complexity and types of questions would provide insight into the generalizability and effectiveness of intermediate supervision in improving intermediate outputs.

### Open Question 2
- Question: What are the limitations of using neural module networks for video question answering in terms of scalability and handling more complex reasoning tasks?
- Basis in paper: [inferred] While the paper demonstrates STAIR's effectiveness on AGQA and its applicability to other datasets, it does not address potential scalability issues or the model's ability to handle increasingly complex reasoning tasks that may require more sophisticated neural modules or program structures.
- Why unresolved: The paper focuses on the model's performance and explainability but does not delve into the scalability challenges or limitations when scaling up to more complex tasks or larger datasets.
- What evidence would resolve it: Testing STAIR on larger and more complex video QA datasets, and experimenting with the addition of more sophisticated neural modules or program structures, would help identify scalability limitations and the model's capacity to handle complex reasoning tasks.

### Open Question 3
- Question: How does the performance of STAIR compare to state-of-the-art video-text pre-trained models on datasets with high language bias, and what strategies can be employed to mitigate this bias?
- Basis in paper: [explicit] The paper notes that STAIR underperforms compared to pre-trained models like GPT-2 and Violet on AGQA, potentially due to the absence of pre-training and fewer parameters. However, it also mentions that the performance gap is smaller on AGQA2, which has reduced language bias.
- Why unresolved: While the paper acknowledges the impact of language bias on model performance, it does not explore specific strategies to mitigate this bias or how STAIR's performance compares to pre-trained models on datasets with high language bias.
- What evidence would resolve it: Conducting experiments to compare STAIR's performance against pre-trained models on datasets known for high language bias, and exploring techniques such as data augmentation or bias correction methods, would provide insights into the model's robustness and strategies to mitigate language bias.

## Limitations
- Reliance on rule-based conversion from AGQA scene graph programs introduces potential brittleness
- Evaluation primarily focuses on synthetic programs rather than natural language questions with human-annotated programs
- The claim about applicability without program annotations is only demonstrated through fine-tuning rather than truly unsupervised methods

## Confidence

**High Confidence**: The core mechanism of compositional decomposition using neural module networks is well-supported by the results showing improved performance over end-to-end baselines (STAIR vs STAIR-E2E). The claim that returning intermediate results specific to module intentions improves interpretability is strongly supported by the auditable intermediate outputs and the ability to collaborate with pre-trained models.

**Medium Confidence**: The effectiveness of intermediate supervision for improving module accuracy has moderate support - while the paper demonstrates this through experimental comparisons, the actual improvement magnitude and robustness across different module types could be further explored. The claim about handling long and informative videos is supported by the results but could benefit from more diverse video length evaluations.

**Low Confidence**: The claim that STAIR is readily applicable when program annotations are not available has the lowest confidence, as the evaluation only demonstrates this through a single fine-tuning approach rather than exploring the full space of unsupervised or weakly-supervised program generation methods.

## Next Checks

1. **Program Generator Robustness Test**: Evaluate STAIR's performance when the program generator produces incorrect programs on a held-out validation set, to quantify how much performance degradation occurs from program generation errors versus module execution errors.

2. **Cross-Dataset Generalization**: Test STAIR's intermediate outputs and final answers on a dataset with naturally occurring questions (rather than synthetically generated programs) to assess real-world applicability beyond controlled synthetic environments.

3. **Ablation on Intermediate Supervision**: Conduct a more detailed ablation study varying the weight Î»IS in the loss function across a wider range of values to understand the optimal balance between intermediate supervision and final answer supervision, and to identify any instability in training when intermediate supervision is applied.