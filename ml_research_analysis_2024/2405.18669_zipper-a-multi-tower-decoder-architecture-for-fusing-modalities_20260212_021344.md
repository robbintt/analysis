---
ver: rpa2
title: 'Zipper: A Multi-Tower Decoder Architecture for Fusing Modalities'
arxiv_id: '2405.18669'
source_url: https://arxiv.org/abs/2405.18669
tags:
- speech
- zipper
- text
- data
- backbone
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Zipper, a novel multi-tower decoder architecture
  for fusing unimodal generative models into a multimodal system. The key idea is
  to use cross-attention to flexibly compose multimodal generative models from independently
  pre-trained unimodal decoders.
---

# Zipper: A Multi-Tower Decoder Architecture for Fusing Modalities

## Quick Facts
- arXiv ID: 2405.18669
- Source URL: https://arxiv.org/abs/2405.18669
- Authors: Vicky Zayats, Peter Chen, Melissa Ferrari, Dirk Padfield
- Reference count: 40
- Key result: Achieves 40% relative WER reduction on text-to-speech with limited aligned data

## Executive Summary
Zipper introduces a novel multi-tower decoder architecture that fuses pre-trained unimodal generative models into a multimodal system using cross-attention mechanisms. The approach addresses the challenge of limited aligned multimodal data by allowing independent unimodal decoders to be composed without retraining, while maintaining their original capabilities. Experiments demonstrate competitive performance with only 1% of aligned data compared to traditional vocabulary expansion methods, achieving 12 absolute WER reduction on TTS tasks.

## Method Summary
Zipper uses cross-attention to flexibly compose multimodal generative models from independently pre-trained unimodal decoders. The architecture maintains separate unimodal towers (backbones) that can be selectively frozen to preserve their original performance, while cross-attention mechanisms enable information fusion between modalities. This design allows the system to leverage rich unimodal representations without requiring extensive aligned multimodal training data.

## Key Results
- 40% relative WER reduction (12 absolute points) on text-to-speech tasks compared to vocabulary expansion baselines
- Maintains unimodal backbone performance when selectively frozen (negligible degradation in text-to-text generation)
- Learns meaningful representations with as little as 1% of aligned data for cross-modal tasks

## Why This Works (Mechanism)
Zipper works by leveraging cross-attention to fuse independently pre-trained unimodal decoders without requiring their retraining. The architecture preserves the rich representations learned by unimodal models while enabling cross-modal information flow through attention mechanisms. By allowing selective freezing of unimodal backbones, the system maintains their original capabilities while adapting only the necessary components for multimodal fusion. The modular design enables efficient use of limited aligned data by transferring knowledge from abundantly available unimodal data.

## Foundational Learning
- **Cross-attention mechanisms**: Enables information fusion between modalities by allowing one modality's representations to attend to another's features; needed to create meaningful cross-modal connections without retraining backbones.
- **Unimodal pre-training transfer**: Leverages rich representations learned from abundant unimodal data; needed because aligned multimodal data is typically scarce.
- **Selective freezing**: Maintains original unimodal performance by freezing backbone parameters; needed to preserve capabilities while adapting for multimodal tasks.
- **Multi-tower architectures**: Separates unimodal processing from multimodal fusion; needed to maintain modularity and enable independent component optimization.

## Architecture Onboarding

**Component Map**: Unimodal Backbone 1 -> Cross-Attention Layer -> Decoder Output, Unimodal Backbone 2 -> Cross-Attention Layer -> Decoder Output

**Critical Path**: Input -> Unimodal Backbone -> Cross-Attention Fusion -> Output Generation

**Design Tradeoffs**: Modularity and preservation of unimodal capabilities vs. computational overhead and architectural complexity. Zipper trades increased parameter count and inference cost for flexibility and data efficiency.

**Failure Signatures**: 
- Degraded unimodal performance when backbones are not properly frozen
- Ineffective cross-modal fusion when cross-attention weights are poorly initialized
- Suboptimal performance with extremely limited aligned data (<1%)

**First Experiments**:
1. Verify unimodal backbone performance preservation with selective freezing
2. Test cross-attention fusion effectiveness on a simple aligned dataset
3. Measure computational overhead vs. single-tower baselines

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental validation limited to speech-text modality pair, generalizability to other modality combinations unverified
- Computational overhead of multi-tower design not quantified relative to traditional architectures
- Claims about 1% data learning effectiveness require verification across different modality pairs and tasks

## Confidence

**High confidence**: Core architectural design and unimodal backbone performance preservation when frozen
**Medium confidence**: Competitive TTS performance with limited data
**Low confidence**: Generalization claims to other modality pairs and 1% data learning assertion

## Next Checks

1. Implement and evaluate Zipper on at least two additional modality pairs (e.g., image-text, audio-text) to verify architectural generalizability beyond speech-text.

2. Systematically test the 1% data learning claim by conducting controlled experiments across multiple modality pairs with varying data percentages (1%, 5%, 10%, 50%, 100%) and comparing against baseline methods.

3. Quantify the inference time and memory requirements of Zipper relative to traditional single-tower architectures across different hardware configurations and batch sizes to characterize the modularity-efficiency trade-off.