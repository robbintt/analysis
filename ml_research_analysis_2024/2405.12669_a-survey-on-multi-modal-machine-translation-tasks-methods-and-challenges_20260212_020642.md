---
ver: rpa2
title: 'A Survey on Multi-modal Machine Translation: Tasks, Methods and Challenges'
arxiv_id: '2405.12669'
source_url: https://arxiv.org/abs/2405.12669
tags:
- image
- translation
- machine
- text
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively reviews 99 prior works on multi-modal
  machine translation (MMT), covering various tasks, methods, and challenges. The
  study provides a detailed analysis of model design, training strategies, and performance-boosting
  techniques for scene-image MMT, as well as other emerging types of MMT such as e-commerce
  product-oriented translation, text image translation, video-guided translation,
  and more.
---

# A Survey on Multi-modal Machine Translation: Tasks, Methods and Challenges

## Quick Facts
- arXiv ID: 2405.12669
- Source URL: https://arxiv.org/abs/2405.12669
- Reference count: 40
- 99 prior works reviewed comprehensively on multi-modal machine translation (MMT)

## Executive Summary
This survey provides a comprehensive review of 99 prior works on multi-modal machine translation (MMT), covering various tasks, methods, and challenges. The study systematically analyzes model design, training strategies, and performance-boosting techniques across different types of MMT, including scene-image translation, e-commerce product-oriented translation, text image translation, and video-guided translation. Key findings highlight the effectiveness of strong image encoders, contrastive learning, and multi-task learning in improving translation quality. The survey also emphasizes the critical need for high-quality datasets and task-specific evaluation metrics tailored to MMT applications.

## Method Summary
The survey employs a systematic literature review methodology, analyzing 99 prior works on multi-modal machine translation. The authors categorize MMT into five main types based on data modalities: scene-image, e-commerce product-oriented, text image, video-guided, and cross-lingual document image translation. For each category, the paper examines model architectures, training strategies, and performance-boosting techniques. The analysis includes detailed discussions on image/video encoders, fusion strategies, training objectives, and evaluation metrics. The survey also identifies key challenges and proposes future research directions, including integration with large language models and development of task-specific evaluation metrics.

## Key Results
- Strong image encoders significantly improve translation quality across all MMT tasks
- Contrastive learning and multi-task learning are effective performance-boosting techniques
- High-quality, task-specific datasets are crucial for advancing MMT research and applications
- Current evaluation metrics are insufficient, necessitating the development of multimodal-specific metrics

## Why This Works (Mechanism)
The effectiveness of MMT stems from the complementary information provided by visual and textual modalities. Visual information helps disambiguate word meanings, especially for polysemous words and culture-specific expressions. By incorporating visual context, MMT models can better understand the semantic relationships between source and target languages. The fusion of visual and textual information at different levels (word, phrase, sentence) allows the model to capture more comprehensive representations, leading to improved translation quality. Additionally, techniques like contrastive learning help align visual and textual representations, further enhancing the model's ability to leverage multimodal information.

## Foundational Learning
- Visual context integration: why needed - disambiguates polysemous words and culture-specific expressions; quick check - improved translation accuracy for ambiguous terms
- Cross-modal attention mechanisms: why needed - aligns visual and textual information; quick check - increased correlation between visual attention and translation output
- Contrastive learning in MMT: why needed - enhances visual-textual representation alignment; quick check - improved performance on cross-modal retrieval tasks
- Multi-task learning for MMT: why needed - leverages related tasks to improve translation quality; quick check - better performance on low-resource language pairs
- Task-specific dataset construction: why needed - ensures relevance and quality of training data; quick check - correlation between dataset quality and translation performance

## Architecture Onboarding

Component Map:
Image Encoder -> Text Encoder -> Fusion Module -> Decoder -> Translation Output

Critical Path:
The critical path in MMT architecture involves the image encoder extracting visual features, which are then fused with text encoder outputs through cross-modal attention or other fusion strategies. This fused representation is fed into the decoder to generate the final translation.

Design Tradeoffs:
1. Complexity vs. performance: More sophisticated image encoders and fusion strategies generally improve performance but increase model complexity and computational cost.
2. Dataset size vs. quality: Larger datasets may provide more diverse training examples, but high-quality, task-specific datasets are crucial for optimal performance.
3. Modality weighting: Balancing the contribution of visual and textual information is critical; overemphasizing either modality can lead to suboptimal translations.

Failure Signatures:
1. Poor visual feature extraction leading to irrelevant visual information being incorporated into translations
2. Ineffective fusion strategies resulting in modality conflicts or loss of important information
3. Over-reliance on visual context causing translations to deviate from source text meaning

First Experiments:
1. Ablation study on image encoder strength across different MMT tasks
2. Comparison of various fusion strategies (e.g., early fusion vs. late fusion) on translation quality
3. Evaluation of contrastive learning impact on cross-modal alignment and translation performance

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How can visual information be effectively integrated with large language models (LLMs) for multimodal machine translation?
- Basis in paper: The paper suggests this as a future research direction, noting that existing multi-modal LLMs may lose significant visual information when projecting visual representations into the textual modality space.
- Why unresolved: Current methods use simple linear models or query transformers, which may not effectively capture complex visual-textual relationships.
- What evidence would resolve it: Development and evaluation of new architectures that better preserve and utilize visual information in LLM-based MMT systems.

### Open Question 2
- Question: What are the most effective evaluation metrics for multimodal machine translation that go beyond text-only metrics?
- Basis in paper: The paper highlights the need for evaluation metrics that consider semantic matching between translations and images/videos, and mentions the potential of using LLMs for more comprehensive evaluation.
- Why unresolved: Current metrics like BLEU and METEOR focus only on textual similarity, ignoring the visual modality's contribution to translation quality.
- What evidence would resolve it: Creation and validation of new metrics that incorporate visual-semantic alignment and LLM-based evaluation methods.

### Open Question 3
- Question: How can high-quality multimodal datasets be efficiently constructed for low-resource languages and diverse domains?
- Basis in paper: The paper identifies data scarcity as a major challenge for MMT, especially for low-resource languages and specific domains like e-commerce and dialogues.
- Why unresolved: Existing datasets are limited in size, language coverage, and domain diversity, constraining the development of MMT models.
- What evidence would resolve it: Development of scalable data collection and annotation methods that can produce large, diverse, and high-quality multimodal datasets for various languages and domains.

## Limitations
- The survey's analysis depth varies across different MMT task types, with scene-image translation receiving more detailed treatment than emerging areas
- The findings are based on published works up to a certain cutoff date, potentially missing recent developments in this rapidly evolving field
- The effectiveness of certain techniques (e.g., contrastive learning, multi-task learning) may be context-dependent and not universally applicable across all MMT scenarios

## Confidence
- High: Effectiveness of strong image encoders across MMT tasks
- High: Importance of high-quality, task-specific datasets
- Medium: Benefits of contrastive learning and multi-task learning approaches
- Medium: Recommendations for future research directions
- Low: Claims about the generalizability of findings to all emerging MMT types

## Next Checks
1. Conduct a systematic benchmark study comparing different image encoder architectures across multiple MMT tasks to validate the claimed effectiveness of strong image encoders.
2. Design and execute experiments to test the generalizability of contrastive learning and multi-task learning approaches across diverse MMT scenarios, particularly in less-studied domains like video-guided translation.
3. Develop and evaluate new task-specific metrics for emerging MMT types, such as e-commerce product-oriented translation, to assess the adequacy of current evaluation methods and inform metric design.