---
ver: rpa2
title: Using RL to Identify Divisive Perspectives Improves LLMs Abilities to Identify
  Communities on Social Media
arxiv_id: '2406.00969'
source_url: https://arxiv.org/abs/2406.00969
tags: []
core_contribution: "This paper proposes to use Large Language Models (LLMs) for social\
  \ media community detection, especially for emerging news events. The key idea is\
  \ to train a smaller language model to generate \"focus areas\" \u2014 additional\
  \ prompt sentences that tell the LLM what topics and issues to focus on when grouping\
  \ users into communities."
---

# Using RL to Identify Divisive Perspectives Improves LLMs Abilities to Identify Communities on Social Media

## Quick Facts
- arXiv ID: 2406.00969
- Source URL: https://arxiv.org/abs/2406.00969
- Authors: Nikhil Mehta; Dan Goldwasser
- Reference count: 30
- Key outcome: Focus areas generated by a smaller language model improve LLM community detection by 3-5% and enhance downstream fake news/political bias detection

## Executive Summary
This paper proposes a novel approach to social media community detection using Large Language Models (LLMs) by generating "focus areas" - additional prompt sentences that guide the LLM to identify nuanced user perspectives during emerging news events. The method trains a smaller language model first with supervised learning using gold focus areas, then refines it with reinforcement learning using reward functions that encourage informative, entity-relevant, and community-detecting focus areas. Experiments on Reddit and Twitter data demonstrate improvements in community detection performance by 3-5%, with the method generalizing to unseen domains and future time periods. The approach also shows promise in boosting performance on downstream tasks like fake news and political bias detection in news media profiling.

## Method Summary
The approach introduces "focus areas" - additional prompt sentences that guide LLMs to identify communities based on nuanced user perspectives rather than general topics. A smaller language model is first trained with supervised learning using gold focus areas, then refined with reinforcement learning using novel reward functions. These rewards encourage focus areas that are informative, entity-relevant, and effective at detecting communities. The method is tested on Reddit and Twitter data from emerging news events, showing improvements in community detection performance and downstream task effectiveness. The approach demonstrates generalization across different LLM architectures and offers a practical way to leverage LLMs for social media analysis without retraining.

## Key Results
- Focus areas improve community detection performance by 3-5% compared to baseline methods
- The approach works effectively on unseen domains and future time periods
- Improved communities enhance downstream task performance for fake news and political bias detection

## Why This Works (Mechanism)
The mechanism works by using focus areas to guide LLMs toward identifying specific perspectives and issues within social media discussions, rather than grouping users by general topics. The smaller language model learns to generate these focus areas through a combination of supervised learning (using gold focus areas) and reinforcement learning (using reward functions that optimize for informativeness, entity relevance, and community detection capability). This approach allows the LLM to distinguish between users based on their nuanced opinions and perspectives, which is particularly valuable for emerging news events where traditional topic-based grouping would be insufficient.

## Foundational Learning
- Focus areas concept: Additional prompt sentences that guide LLM attention toward specific perspectives - needed to overcome limitations of general topic-based grouping
- Supervised learning phase: Training the smaller model on gold focus areas - needed to establish baseline capability before RL refinement
- Reinforcement learning with custom rewards: Optimizing focus area generation for informativeness, entity relevance, and community detection - needed to improve the quality of generated focus areas
- Community detection evaluation: Measuring how well focus areas help LLMs group users into meaningful communities - needed to validate the effectiveness of the approach
- Downstream task integration: Using detected communities to improve fake news and political bias detection - needed to demonstrate practical value beyond community detection

## Architecture Onboarding

Component map: Data collection -> Supervised learning -> Reinforcement learning -> Focus area generation -> LLM community detection -> Downstream task evaluation

Critical path: The most critical path is from focus area generation through LLM community detection to downstream task evaluation, as this demonstrates the complete pipeline from input data to practical application.

Design tradeoffs: The approach trades off computational efficiency (generating focus areas adds overhead) for improved community detection quality and downstream task performance. It also requires a two-phase training approach (supervised then RL) rather than a single training process.

Failure signatures: The system may fail when focus areas are too generic, miss key entities in the discussion, or when the reinforcement learning rewards don't properly capture what makes a focus area useful for community detection.

Three first experiments:
1. Test focus area generation on a small sample dataset with known community structures to verify basic functionality
2. Compare community detection quality with and without focus areas using a simple evaluation metric
3. Evaluate the impact of different reward function weights in the RL phase on focus area quality

## Open Questions the Paper Calls Out
None

## Limitations
- The generalizability to other social platforms or discussion types beyond Reddit and Twitter remains untested
- The reinforcement learning approach's specific reward functions and their relative weights are not fully explored
- The computational costs and scalability for large-scale social media platforms with millions of users are not discussed

## Confidence

High confidence: The core methodology and experimental results are sound, and the improvements over baselines are measurable.

Medium confidence: The claims about generalization to unseen domains and future time periods, as well as the impact on downstream tasks.

Low confidence: The scalability and potential biases of the approach, as well as its applicability to diverse social media platforms.

## Next Checks

1. Test the approach on a wider range of social media platforms (e.g., Facebook, Instagram, LinkedIn) and diverse discussion types (e.g., product reviews, professional networking).

2. Conduct an ablation study to understand the relative importance of each reward function in the RL training and explore different weightings.

3. Analyze the computational costs and scalability of the approach for large-scale social media platforms with millions of users and posts.