---
ver: rpa2
title: Privacy-Preserving Heterogeneous Federated Learning for Sensitive Healthcare
  Data
arxiv_id: '2406.10563'
source_url: https://arxiv.org/abs/2406.10563
tags:
- local
- data
- privacy
- learning
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses two key challenges in federated learning
  for healthcare: data privacy leakage when centralizing sensitive personal information
  and model heterogeneity that requires confidential collaboration of heterogeneous
  local models. The proposed Abstention-Aware Federated Voting (AAFV) framework integrates
  a novel abstention-aware voting mechanism with local differential privacy to enable
  collaborative and confidential training of heterogeneous models while protecting
  data privacy.'
---

# Privacy-Preserving Heterogeneous Federated Learning for Sensitive Healthcare Data

## Quick Facts
- arXiv ID: 2406.10563
- Source URL: https://arxiv.org/abs/2406.10563
- Authors: Yukai Xu; Jingfeng Zhang; Yujie Gu
- Reference count: 40
- This paper addresses two key challenges in federated learning for healthcare: data privacy leakage when centralizing sensitive personal information and model heterogeneity that requires confidential collaboration of heterogeneous local models.

## Executive Summary
This paper proposes Abstention-Aware Federated Voting (AAFV), a framework that enables collaborative training of heterogeneous models in federated learning while preserving data privacy. The framework integrates a novel abstention-aware voting mechanism with local differential privacy to protect sensitive healthcare data. By using prediction votes instead of model parameters, AAFV allows clients to maintain model confidentiality while benefiting from collaborative learning. The approach was evaluated on diabetes and in-hospital patient mortality prediction tasks, showing consistent accuracy improvements over traditional federated learning approaches.

## Method Summary
AAFV addresses privacy leakage and model heterogeneity in federated learning by implementing a novel abstention-aware voting mechanism combined with local differential privacy. The method works by having each client train their heterogeneous local model on private labeled data, generate predictions on a public unlabeled dataset, perturb these predictions using a piecewise mechanism for privacy protection, and convert them to votes with threshold-based abstention. The central server aggregates these votes to create pseudo-labels, which are then used by all clients to update their local models without sharing model parameters. The framework was evaluated on two healthcare datasets with varying model architectures including SVM, perceptron, logistic regression, and MLP.

## Key Results
- AAFV consistently outperforms typical FedAvg framework and non-federated scenarios in test accuracy while preserving the same privacy level
- Average accuracy improvement of 3% across diabetes and in-hospital patient mortality prediction tasks
- Successfully handles heterogeneous models (SVM, perceptron, logistic regression, MLP) without requiring model parameter sharing
- Maintains differential privacy guarantees with privacy budget ε=1.0 while enabling effective collaborative learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The abstention-aware voting mechanism improves model accuracy by filtering low-confidence predictions from heterogeneous models.
- Mechanism: Each local model makes predictions on an unlabeled public dataset. Predictions are converted to votes (0, 1, or abstention) based on a threshold. Only high-confidence votes (above threshold or below 1-threshold) are counted, while uncertain predictions are marked as abstention. The central server aggregates these votes using majority rule to create pseudo-labels for the unlabeled data.
- Core assumption: Local models can produce varying confidence levels that correlate with prediction quality, and filtering out uncertain predictions improves the quality of aggregated labels.
- Evidence anchors:
  - [abstract] "The proposed abstention-aware voting mechanism exploits a threshold-based abstention method to select high-confidence votes from heterogeneous local models, which not only enhances the learning utility but also protects model confidentiality."
  - [section] "To mitigate the effects of perturbation and to select high-confidence predictions from local votes, we introduce an abstention-aware voting mechanism described as: vk,i = 0 if ˜pk,i ≤ τ, ∗ if τ < ˜pk,i < 1 − τ, 1 if ˜pk,i ≥ 1 − τ"
- Break condition: If local models' confidence scores are poorly calibrated or do not correlate with prediction quality, the abstention threshold would filter out both good and bad predictions indiscriminately, degrading performance.

### Mechanism 2
- Claim: Local differential privacy protects individual client data while maintaining model utility.
- Mechanism: Each client perturbs their local prediction scores using the piecewise mechanism before sharing them. This adds calibrated noise to predictions such that the probability distribution of perturbed outputs depends only on the privacy budget ε, not the specific input value, satisfying local differential privacy.
- Core assumption: The piecewise mechanism introduces sufficient noise to protect privacy while maintaining enough signal for useful aggregation.
- Evidence anchors:
  - [section] "To enhance the privacy protection, the local predictions pk are perturbed to ˜pk using the piecewise mechanism... The piecewise mechanism is a simple, high-performance and less-biased mechanism achieving LDP by virtue of an ϵ-based piecewise function."
  - [section] "Furthermore, we implement AAFV on two practical prediction tasks of diabetes and in-hospital patient mortality. The experiments demonstrate the effectiveness and superiority of AAFV in practical applications, particularly in enhancing model utility while protecting data privacy and model confidentiality."
- Break condition: If the privacy budget ε is set too low (strict privacy), the noise added by the piecewise mechanism could overwhelm the useful signal, making aggregated predictions uninformative.

### Mechanism 3
- Claim: Using local votes instead of model parameters enables training heterogeneous models while protecting model intellectual property.
- Mechanism: Instead of sharing model parameters (weights), each client only shares their prediction votes on the public unlabeled dataset. The central server aggregates these votes to create pseudo-labels, which are then used by all clients to update their local models. This approach doesn't require model parameter synchronization or sharing of model architectures.
- Core assumption: Prediction votes contain sufficient information for collaborative learning without revealing the underlying model architecture or parameters.
- Evidence anchors:
  - [abstract] "This is achieved by integrating a novel abstention-aware voting mechanism and a differential privacy mechanism onto local models' predictions."
  - [section] "This work will exploit a novel abstention-aware federated voting mechanism, which collects high-confidence local votes to generate global votes (pseudo labels) for an auxiliary unlabeled public dataset for collaboration."
  - [section] "Throughout the training process of AAFV, the detailed architectures of local models remain undisclosed, thereby effectively protecting the confidentiality and intellectual property of these models."
- Break condition: If the prediction space is too limited or if model architectures differ significantly in their output spaces, the aggregation of votes may not provide meaningful gradients for model updates.

## Foundational Learning

- Concept: Local Differential Privacy
  - Why needed here: Protects individual client data in the federated learning setting where clients cannot trust the central server with raw predictions
  - Quick check question: What mathematical property must a mechanism satisfy to achieve ε-local differential privacy?

- Concept: Federated Averaging vs Federated Voting
  - Why needed here: Understanding why parameter averaging fails for heterogeneous models while voting-based aggregation succeeds
  - Quick check question: What assumption does FedAvg make about local models that prevents it from handling model heterogeneity?

- Concept: Threshold-based Abstention
  - Why needed here: Enables filtering of low-confidence predictions to improve the quality of aggregated pseudo-labels
  - Quick check question: How does the abstention threshold τ affect the trade-off between privacy and utility?

## Architecture Onboarding

- Component map:
  - Local clients: Each holds private labeled data and a heterogeneous model
  - Piecewise mechanism: Adds noise to predictions for privacy
  - Abstention-aware voting: Converts noisy predictions to votes with abstentions
  - Central server: Aggregates votes to create pseudo-labels
  - Model update loop: Clients retrain using combined private data and pseudo-labeled public data

- Critical path: Private labeled data → Local model training → Predictions on public data → Privacy-preserving perturbation → Vote generation with abstention → Vote aggregation → Pseudo-label distribution → Model retraining

- Design tradeoffs:
  - Privacy budget ε vs. model accuracy: Lower ε provides stronger privacy but adds more noise
  - Abstention threshold τ vs. participation rate: Higher τ reduces noise but also reduces the number of valid votes
  - Communication efficiency vs. information content: Voting requires less bandwidth than parameter sharing but may contain less information

- Failure signatures:
  - Performance collapse: If most votes are abstentions due to high threshold or low privacy budget
  - Privacy leakage: If noise is insufficient and models can infer training data from predictions
  - Model incompatibility: If heterogeneous models produce votes in incompatible spaces

- First 3 experiments:
  1. Implement the piecewise mechanism with varying ε values and measure the variance of perturbed outputs to verify LDP properties
  2. Test the abstention-aware voting with synthetic heterogeneous models to observe how threshold τ affects aggregation quality
  3. Run a simple end-to-end federated voting with two clients using logistic regression models on the diabetes dataset with ε=1.0

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the abstention threshold τ affect the tradeoff between accuracy and privacy in AAFV?
- Basis in paper: [explicit] The paper mentions that τ is a predetermined threshold to identify valid high-confidence predictions, but does not explore its impact on performance.
- Why unresolved: The paper does not conduct experiments varying τ to determine its optimal value or its effects on accuracy and privacy.
- What evidence would resolve it: Experiments showing accuracy and privacy levels across different τ values, identifying an optimal threshold.

### Open Question 2
- Question: How does AAFV compare to other privacy-preserving federated learning methods like secure aggregation or homomorphic encryption?
- Basis in paper: [inferred] The paper only compares AAFV to FedAvg and non-federated scenarios, but does not compare it to other privacy-preserving methods.
- Why unresolved: The paper does not provide a comprehensive comparison with other privacy-preserving techniques in the literature.
- What evidence would resolve it: Experiments comparing AAFV's accuracy, privacy level, and computational efficiency to other privacy-preserving methods.

### Open Question 3
- Question: How does AAFV perform on datasets with class imbalance or rare diseases?
- Basis in paper: [inferred] The paper only evaluates AAFV on balanced datasets (diabetes and MIMIC-III mortality prediction), but does not test its performance on imbalanced datasets.
- Why unresolved: The paper does not address how well AAFV handles class imbalance, which is common in healthcare datasets.
- What evidence would resolve it: Experiments on imbalanced datasets showing AAFV's performance metrics (accuracy, precision, recall, F1-score) compared to other methods.

## Limitations

- The threshold value τ for the abstention-aware voting mechanism is not specified in the paper, making exact reproduction difficult
- The empirical validation is limited to two healthcare datasets and three relatively simple model architectures (SVM, perceptron, logistic regression, MLP)
- The paper does not explore how AAFV performs on datasets with class imbalance or rare diseases, which are common in healthcare

## Confidence

Medium. The mechanism description is mathematically sound and the privacy guarantees are theoretically valid. However, the empirical validation is limited to specific datasets and model configurations. The claimed 3% accuracy improvement over FedAvg needs independent verification across different domains and model combinations.

## Next Checks

1. **Sensitivity Analysis**: Systematically vary the abstention threshold τ (0.1, 0.3, 0.5, 0.7, 0.9) on both datasets to quantify its impact on accuracy-privacy trade-offs and identify optimal operating points.

2. **Privacy Budget Calibration**: Conduct experiments with varying ε values (0.5, 1.0, 2.0, 5.0) to verify that the piecewise mechanism maintains differential privacy guarantees while measuring degradation in model utility as privacy increases.

3. **Model Heterogeneity Stress Test**: Replace the current heterogeneous models with architecturally diverse models (CNNs, transformers, decision trees) on synthetic data to test whether the voting mechanism can handle more extreme heterogeneity while maintaining privacy and accuracy benefits.