---
ver: rpa2
title: Masked AutoEncoder for Graph Clustering without Pre-defined Cluster Number
  k
arxiv_id: '2401.04741'
source_url: https://arxiv.org/abs/2401.04741
tags:
- graph
- clustering
- methods
- information
- autoencoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Graph Clustering with Masked Autoencoders
  (GCMA), a nonparametric deep learning framework that addresses the challenge of
  graph clustering without requiring a pre-defined number of clusters. The core idea
  involves a fusion autoencoder based on graph masking, which learns comprehensive
  graph embeddings by reconstructing both node features and graph structure.
---

# Masked AutoEncoder for Graph Clustering without Pre-defined Cluster Number k

## Quick Facts
- arXiv ID: 2401.04741
- Source URL: https://arxiv.org/abs/2401.04741
- Authors: Yuanchi Ma; Hui He; Zhongxiang Lei; Zhendong Niu
- Reference count: 13
- One-line primary result: Graph Clustering with Masked Autoencoders (GCMA) achieves state-of-the-art clustering performance without requiring pre-defined number of clusters k

## Executive Summary
This paper introduces Graph Clustering with Masked Autoencoders (GCMA), a nonparametric deep learning framework that addresses the challenge of graph clustering without requiring a pre-defined number of clusters. The core idea involves a fusion autoencoder based on graph masking, which learns comprehensive graph embeddings by reconstructing both node features and graph structure. A key innovation is the use of a masked graph mechanism that improves generalization and interpretability. The model also incorporates an improved density-based clustering algorithm as a second decoder to automatically determine the optimal number of clusters.

GCMA represents a significant advancement in graph clustering by combining masked autoencoding with multi-target reconstruction and an improved density-based clustering algorithm. The approach achieves superior performance compared to state-of-the-art baselines across multiple datasets while eliminating the need to specify the number of clusters in advance. The framework demonstrates strong clustering accuracy, normalized mutual information, and adjusted rand index scores, making it a promising solution for real-world graph clustering applications.

## Method Summary
GCMA employs a fusion autoencoder architecture that combines graph masking with multi-target reconstruction. The model first applies random masking to both graph edges and node features, creating a masked graph input. A GAT/GCN encoder processes this masked graph to learn structural embeddings, while a separate autoencoder encodes node features. These embeddings are fused through a linear combination with a learnable coefficient. The decoder then reconstructs multiple targets (GAE structural embeddings and AE feature embeddings) using mutual information-based loss functions. Finally, an improved density-based clustering algorithm (modified CFSFDP) automatically determines the optimal number of clusters, with a self-optimization module using t-SNE to refine the results.

## Key Results
- GCMA achieves 74.74% ACC, 59.16% NMI, and 55.41% ARI on the Cora dataset
- The method demonstrates strong performance across five datasets including Citeseer, DBLP, and Ogbn-arxiv
- Outperforms state-of-the-art baselines in clustering accuracy while eliminating the need for pre-defined cluster number k

## Why This Works (Mechanism)

### Mechanism 1
Masked graph autoencoders can learn more generalizable graph embeddings by reconstructing both masked node features and edges. The graph masking mechanism (Me, Mf) randomly masks portions of the adjacency matrix and node feature matrix, forcing the encoder to learn robust representations that can handle missing information. This is similar to how masked language modeling in BERT forces the model to understand context by predicting missing words.

### Mechanism 2
Multi-target reconstruction with mutual information loss improves clustering quality. The decoder reconstructs multiple targets (GAE structural embeddings and AE feature embeddings) simultaneously using MI-based loss functions. This captures both structural proximity and node feature information in the learned representations, with MI being preferred over L2-norm for its ability to capture relationships between multiple targets.

### Mechanism 3
Improved density-based clustering automatically determines optimal k without requiring pre-defined cluster number. The modified CFSFDP algorithm uses density (ρ) and minimum distance (δ) to identify cluster centers. The Gaussian function replaces the indicator function to ensure δ values are sufficiently separated from thresholds, making cluster centers more distinguishable and eliminating the need for manual threshold selection.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and their variants (GCN, GAT)
  - Why needed here: The encoder uses GNN architectures to process graph-structured data and learn node representations
  - Quick check question: What is the key difference between GCN and GAT in how they aggregate information from neighboring nodes?

- Concept: Autoencoder architecture and reconstruction loss functions
  - Why needed here: The model uses a fusion autoencoder structure with multiple reconstruction targets
  - Quick check question: Why might mutual information be preferred over L2 loss for multi-target reconstruction in this context?

- Concept: Density-based clustering algorithms (DBSCAN, CFSFDP)
  - Why needed here: The model uses an improved version of the density-based clustering algorithm to automatically determine the number of clusters
  - Quick check question: What are the two main criteria used by the CFSFDP algorithm to identify cluster centers?

## Architecture Onboarding

- Component map: Graph → Masking → Encoding → Fusion → Multi-target Reconstruction → CFSFDP Clustering
- Critical path: Graph → Masking → Encoding → Fusion → Multi-target Reconstruction → CFSFDP Clustering
- Design tradeoffs:
  - Masking ratio vs. reconstruction quality: Higher masking improves generalization but may reduce reconstruction accuracy
  - Fusion coefficient (ε) vs. modality importance: Balances structural vs. feature information
  - Number of reconstruction targets vs. computational complexity: More targets capture more information but increase training time
- Failure signatures:
  - Poor clustering performance on datasets with very different cluster densities
  - Memory issues with large graphs due to the quadratic complexity of adjacency matrix operations
  - Degraded performance if masking ratio is too high (>50%)
- First 3 experiments:
  1. Test different masking ratios (10%, 20%, 30%, 40%) on Cora dataset and measure clustering accuracy
  2. Compare L2 loss vs. MI-based loss for multi-target reconstruction on Citeseer dataset
  3. Evaluate the impact of the fusion coefficient ε on clustering performance across different datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of graph masking strategy (edge vs. node feature masking) affect the clustering performance and generalization ability of GCMA?
- Basis in paper: The paper mentions that "masking both modal information at the same time will have a negative impact on model learning" but does not provide a detailed comparison of different masking strategies.
- Why unresolved: The paper does not provide a detailed comparison of different masking strategies, such as edge masking, node feature masking, or a combination of both.
- What evidence would resolve it: Experiments comparing the clustering performance and generalization ability of GCMA using different graph masking strategies on various datasets.

### Open Question 2
- Question: What is the optimal number of layers in the graph masking autoencoder for achieving the best clustering performance?
- Basis in paper: The paper mentions that "the selection of the number of encoder layers" is explored in the ablation study but does not provide a definitive answer.
- Why unresolved: The paper does not provide a clear answer on the optimal number of layers in the graph masking autoencoder for achieving the best clustering performance.
- What evidence would resolve it: Experiments varying the number of layers in the graph masking autoencoder and comparing the clustering performance on various datasets.

### Open Question 3
- Question: How does the proposed improved density-based clustering algorithm compare to other clustering algorithms in terms of clustering performance and computational efficiency?
- Basis in paper: The paper mentions that the improved density-based clustering algorithm is used as a second decoder but does not provide a detailed comparison with other clustering algorithms.
- Why unresolved: The paper does not provide a detailed comparison of the improved density-based clustering algorithm with other clustering algorithms in terms of clustering performance and computational efficiency.
- What evidence would resolve it: Experiments comparing the clustering performance and computational efficiency of the improved density-based clustering algorithm with other clustering algorithms on various datasets.

## Limitations

- Unknown architectural specifics: Exact GAT/GCN layer configurations (number of layers, hidden dimensions) are not specified in the paper, which could significantly impact performance
- Implementation details: The improved CFSFDP algorithm's exact implementation, particularly the Gaussian function usage and threshold determination mechanism, lacks sufficient detail for faithful reproduction
- Dataset preprocessing: No information provided about how node features were normalized or whether any dimensionality reduction was applied before clustering

## Confidence

- **High Confidence**: The core concept of using masked graph autoencoders for clustering without pre-defined k, the multi-target reconstruction framework, and the overall experimental methodology
- **Medium Confidence**: The effectiveness of the MI-based loss function for multi-target reconstruction and the specific implementation details of the improved CFSFDP algorithm
- **Low Confidence**: The exact implementation details of the self-optimization module using t-SNE and how it integrates with the clustering process

## Next Checks

1. **Architecture Sensitivity Analysis**: Systematically vary GAT/GCN layer configurations (1-3 layers, hidden dimensions 64-256) on Cora dataset to determine optimal architecture for this task

2. **Loss Function Ablation Study**: Compare L2 loss vs. MI-based loss for multi-target reconstruction across all datasets, measuring impact on ACC, NMI, and ARI

3. **Masking Ratio Optimization**: Conduct a comprehensive grid search (5%-50%) to identify optimal masking ratios for both edge and feature masking on each dataset, analyzing the trade-off between generalization and reconstruction quality