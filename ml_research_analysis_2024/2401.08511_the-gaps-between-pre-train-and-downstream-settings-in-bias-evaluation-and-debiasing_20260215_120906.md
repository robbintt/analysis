---
ver: rpa2
title: The Gaps between Pre-train and Downstream Settings in Bias Evaluation and Debiasing
arxiv_id: '2401.08511'
source_url: https://arxiv.org/abs/2401.08511
tags:
- bias
- debiasing
- linguistics
- downstream
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates the effectiveness of debiasing methods on
  pre-trained language models (PLMs) in both fine-tuning (FT) and in-context learning
  (ICL) settings. It reveals that ICL-based debiasing methods exhibit higher correlation
  between intrinsic and extrinsic bias scores compared to FT-based methods.
---

# The Gaps between Pre-train and Downstream Settings in Bias Evaluation and Debiasing

## Quick Facts
- arXiv ID: 2401.08511
- Source URL: https://arxiv.org/abs/2401.08511
- Authors: Masahiro Kaneko; Danushka Bollegala; Timothy Baldwin
- Reference count: 15
- Key outcome: ICL-based debiasing methods show higher correlation between intrinsic and extrinsic bias scores and lower performance degradation compared to FT-based methods

## Executive Summary
This study investigates the effectiveness of debiasing methods on pre-trained language models (PLMs) in both fine-tuning (FT) and in-context learning (ICL) settings. The research reveals that ICL-based debiasing methods exhibit higher correlation between intrinsic and extrinsic bias scores compared to FT-based methods. Additionally, ICL settings result in lower performance degradation in downstream tasks due to debiasing. The findings highlight the importance of considering the distinct tendencies of FT and ICL settings when evaluating and mitigating social biases in PLMs.

## Method Summary
The study evaluates debiasing methods on eight LaMini PLMs using both fine-tuning and in-context learning approaches. Four debiasing methods were tested: Counterfactual Data Augmentation (CDA), All-Layer Token-level debiasing (ALT), Zero-Shot Debiasing (ZSD), and Few-Shot Debiasing (FSD). Bias was evaluated using three intrinsic datasets (Crowds-Pairs, StereoSet, Multilingual Bias Evaluation) and three extrinsic datasets (BBQ, BNLI, WB). Downstream task performance was measured on RACE (question answering), ANLI (natural language inference), and OntoNotes v5.0 (coreference resolution). The study compared correlation coefficients between intrinsic and extrinsic bias scores, performance degradation in downstream tasks, and cosine similarity between original and debiased model outputs.

## Key Results
- ICL-based debiasing methods show higher correlation (0.42) between intrinsic and extrinsic bias scores compared to FT-based methods (0.23)
- ICL settings result in lower performance degradation in downstream tasks compared to FT settings
- ICL induces smaller changes to PLM output distributions compared to FT-based debiasing methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ICL-based debiasing methods exhibit higher correlation between intrinsic and extrinsic bias scores compared to FT-based methods
- Mechanism: ICL methods preserve model parameters and learned representations during debiasing, maintaining the relationship between pre-training and downstream bias evaluations
- Core assumption: The smaller parameter changes in ICL compared to FT lead to better preservation of beneficial information from pre-training
- Evidence anchors: [abstract] "ICL-based debiasing methods show a higher correlation between intrinsic and extrinsic bias scores compared to FT-based methods", [section] "ICL settings have higher correlations than FT settings in every case"
- Break condition: If ICL-based methods introduce significant prompt-induced biases that weren't present in the original model

### Mechanism 2
- Claim: ICL settings result in lower performance degradation in downstream tasks due to debiasing
- Mechanism: By preserving model parameters during debiasing, ICL methods retain more of the beneficial information learned during pre-training
- Core assumption: The beneficial semantic information encoded during pre-training is more likely to be preserved when parameters aren't updated
- Evidence anchors: [abstract] "the performance degradation due to debiasing is also lower in the ICL case compared to that in the FT case", [section] "Figure 2 shows that the performance drop due to debiasing in both CDA and ATL to be higher than that of FSD and ZSD"
- Break condition: If the prompt engineering itself introduces significant noise or if the prompt-based approach is less effective at addressing biases

### Mechanism 3
- Claim: ICL debiasing methods induce smaller changes to PLM output distributions compared to FT-based methods
- Mechanism: The lack of parameter updates in ICL means the model's output state changes less dramatically during debiasing
- Core assumption: Smaller changes to model parameters lead to more similar output distributions between original and debiased models
- Evidence anchors: [abstract] "ICL induces smaller changes to PLMs compared to FT-based debiasing methods", [section] "we measure the average similarity between the model outputs for a fixed set of inputs" and "the cosine similarity is higher for the debiased models with ICL than with FT"
- Break condition: If ICL prompts cause significant shifts in output distributions despite no parameter updates

## Foundational Learning

- Concept: Correlation coefficients and their interpretation
  - Why needed here: The paper relies heavily on Pearson correlation coefficients to compare relationships between different bias evaluation methods
  - Quick check question: What does a Pearson correlation coefficient of 0.42 versus 0.23 tell us about the relationship between two variables?

- Concept: Parameter updates vs. in-context learning
  - Why needed here: Understanding the fundamental difference between methods that update model parameters (FT) versus those that don't (ICL) is crucial to grasping the paper's main argument
  - Quick check question: What is the key difference in how FT and ICL methods modify a PLM during debiasing?

- Concept: Downstream task performance metrics
  - Why needed here: The paper evaluates debiasing impact on downstream tasks (RACE, ANLI, OntoNotes), requiring understanding of these evaluation metrics
  - Quick check question: Why might debiasing cause a drop in performance on tasks like question answering or natural language inference?

## Architecture Onboarding

- Component map:
  Pre-trained Language Models (PLMs) → Debiasing Method (FT or ICL) → Bias Evaluation (Intrinsic/Extrinsic) → Downstream Task Performance
  Intrinsic bias evaluation: CP, SS, MBE datasets
  Extrinsic bias evaluation: BBQ, BNLI, WB datasets
  Downstream tasks: RACE (QA), ANLI (NLI), OntoNotes (Coreference)

- Critical path:
  1. Load PLM and select debiasing method (FT or ICL)
  2. Apply debiasing while tracking parameter changes
  3. Evaluate bias using both intrinsic and extrinsic methods
  4. Measure downstream task performance impact
  5. Compare correlation between pre-training and downstream bias evaluations

- Design tradeoffs:
  - FT methods offer potentially more thorough bias mitigation but risk losing beneficial information
  - ICL methods preserve more beneficial information but may be less effective at debiasing
  - Parameter preservation vs. debiasing effectiveness
  - Computational cost of fine-tuning vs. inference-time prompt engineering

- Failure signatures:
  - Low correlation between intrinsic and extrinsic bias scores despite debiasing
  - Significant performance drop in downstream tasks
  - Cosine similarity between original and debiased model outputs below expected thresholds
  - Inconsistent debiasing results across different datasets or tasks

- First 3 experiments:
  1. Compare bias scores between original PLM and debiased versions using both FT and ICL methods on CP dataset
  2. Measure downstream task performance (RACE dataset) for FT vs ICL debiasing with equalized bias mitigation levels
  3. Calculate cosine similarity between output states of original and debiased models for both FT and ICL methods on ANLI dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the results of ICL-based debiasing methods generalize to larger language models like LLaMa and Flan-T5?
- Basis in paper: [explicit] The paper mentions that the LaMini series was used due to the need for fine-tuning, but it would require verification in environments with rich computation resources to determine if larger PLMs exhibit the same tendencies.
- Why unresolved: The study used LaMini models, which are smaller than models like LLaMa and Flan-T5. It is unclear if the observed tendencies in ICL-based debiasing methods would hold for larger models.
- What evidence would resolve it: Experiments conducted on larger language models like LLaMa and Flan-T5 using ICL-based debiasing methods would provide evidence to determine if the observed tendencies generalize to these models.

### Open Question 2
- Question: How do ICL-based debiasing methods perform across a broader range of downstream tasks beyond QA, NLI, and coreference resolution?
- Basis in paper: [explicit] The paper states that only QA, NLI, and coreference resolution tasks were used for experiments, and as more evaluation data for assessing social biases in downstream tasks becomes available, the conclusions should be analyzed across a broader range of datasets.
- Why unresolved: The study focused on a limited set of downstream tasks, and it is uncertain how ICL-based debiasing methods would perform on other types of tasks.
- What evidence would resolve it: Conducting experiments using ICL-based debiasing methods on a diverse range of downstream tasks and evaluating their performance would provide evidence to determine their generalizability.

### Open Question 3
- Question: How do ICL-based debiasing methods handle social biases beyond gender, such as race and religion?
- Basis in paper: [explicit] The paper mentions that only gender bias was considered in the work, and future work plans to consider non-binary gender. It also states that social biases like race and religion exist in PLMs, which require further investigations.
- Why unresolved: The study focused solely on gender bias, and it is unclear how ICL-based debiasing methods would perform in mitigating other types of social biases.
- What evidence would resolve it: Conducting experiments using ICL-based debiasing methods to mitigate various types of social biases, such as race and religion, would provide evidence to determine their effectiveness in handling these biases.

## Limitations

- The study's conclusions about ICL methods showing higher correlation between intrinsic and extrinsic bias scores are based on relatively small effect sizes (correlation coefficients around 0.42 vs 0.23)
- The study only examines a specific set of LaMini PLMs, limiting generalizability to other model architectures or scales
- The lack of detailed implementation specifications for debiasing methods (CDA, ALT, ZSD, FSD) creates uncertainty about whether results would be reproducible with alternative implementations

## Confidence

- **High confidence**: The fundamental observation that FT and ICL methods induce different magnitudes of change to PLM parameters and outputs is well-established and directly measurable through cosine similarity metrics.
- **Medium confidence**: The finding that ICL preserves more downstream task performance is reasonably well-supported, though the magnitude of this effect varies significantly across different debiasing methods.
- **Medium confidence**: The correlation analysis between intrinsic and extrinsic bias scores shows consistent patterns across datasets, but the relatively small differences in correlation coefficients suggest these findings should be interpreted cautiously.

## Next Checks

1. **Replicate with alternative debiasing implementations**: Implement CDA, ALT, ZSD, and FSD methods independently to verify whether the observed differences in correlation and performance degradation persist across different codebases and hyperparameter settings.

2. **Test on larger model architectures**: Evaluate the same debiasing approaches on larger, more widely-used PLMs (e.g., BERT, GPT, T5 families) to determine whether the observed trends scale or change with model capacity.

3. **Conduct ablation studies on prompt engineering**: Systematically vary prompt complexity and structure in ICL methods to isolate how much of the observed performance preservation is due to the absence of parameter updates versus the specific prompt formulations used.