---
ver: rpa2
title: Incentivized Exploration of Non-Stationary Stochastic Bandits
arxiv_id: '2403.10819'
source_url: https://arxiv.org/abs/2403.10819
tags:
- regret
- algorithm
- reward
- time
- compensation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses incentivized exploration in multi-armed bandit
  (MAB) problems with non-stationary reward distributions. Players receive compensation
  for exploring arms other than the greedy choice and may provide biased feedback
  influenced by the compensation.
---

# Incentivized Exploration of Non-Stationary Stochastic Bandits

## Quick Facts
- arXiv ID: 2403.10819
- Source URL: https://arxiv.org/abs/2403.10819
- Authors: Sourav Chakraborty; Lijun Chen
- Reference count: 40
- Key outcome: Achieves sublinear regret and compensation in non-stationary MAB problems with biased feedback

## Executive Summary
This paper addresses the challenge of incentivized exploration in multi-armed bandit problems where reward distributions change over time and agents provide biased feedback influenced by compensation. The authors propose algorithms that use discounted UCB (DUCB) and sliding window UCB (SWUCB) for abruptly-changing environments, and a restarting technique for continuously-changing environments. These approaches achieve sublinear regret (Õ(√T) for abrupt changes, Õ(T^(2/3)) for continuous changes) and compensation over time, effectively incentivizing exploration despite non-stationarity and biased feedback.

## Method Summary
The paper proposes three main algorithmic approaches for non-stationary MAB problems with biased feedback. For abruptly-changing environments, DUCB uses discount factors to emphasize recent rewards while SWUCB employs a sliding window of recent observations. Both methods track abrupt reward distribution changes by focusing on current rather than historical data. For continuously-changing environments with bounded total variation, the algorithm divides time into fixed-size batches and restarts standard bandit algorithms (UCB1, ε-greedy, or Thompson Sampling) at each batch boundary. Compensation is calculated as the difference between rewards for optimal and chosen arms, with feedback biased by a Lipschitz continuous drift function of the compensation amount.

## Key Results
- Regret bounds of Õ(√T) for abruptly-changing environments with known breakpoint frequency
- Regret bounds of Õ(T^(2/3)) for continuously-changing environments with bounded variation budget
- Sublinear compensation bounds over time for both environment types
- Theoretical guarantees hold under Lipschitz continuity assumptions on drift functions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Compensation drives biased feedback that allows the algorithm to correctly identify the optimal arm despite non-stationarity
- Mechanism: When compensation χt is offered, agent feedback becomes biased by δt = ft(χt), where ft is Lipschitz continuous and non-decreasing. This bias counteracts non-stationary effects, maintaining accurate arm value estimates.
- Core assumption: Drift function ft is Lipschitz continuous and non-decreasing
- Evidence anchors: [abstract] and [section II-C] on biased feedback mechanisms

### Mechanism 2
- Claim: Restarting bandit algorithms at fixed intervals allows adaptation to continuously changing reward distributions
- Mechanism: Time horizon T is divided into batches of size σ, with standard bandit algorithms restarted at each batch beginning to prevent outdated information from corrupting current estimates.
- Core assumption: Variation budget VT bounds total reward changes, making batch size selection feasible
- Evidence anchors: [section III-B] and [section II-B] on batch-based restarting technique

### Mechanism 3
- Claim: Discounting or sliding window techniques enable tracking abrupt reward distribution changes
- Mechanism: DUCB uses discount factors to weight recent rewards more heavily, while SWUCB uses a sliding window of recent observations to focus on current reward distributions rather than historical data.
- Core assumption: Number of breakpoints βT is bounded
- Evidence anchors: [section II-B] and [section III-A] on DUCB and SWUCB implementations

## Foundational Learning

- Concept: Lipschitz continuity of drift functions
  - Why needed here: Ensures compensation-induced bias remains bounded and controllable
  - Quick check question: If a drift function satisfies |ft(x) - ft(y)| ≤ lt|x-y|, what is the maximum possible bias when compensation χt = 1?

- Concept: Variation budget in non-stationary environments
  - Why needed here: Provides mathematical framework for bounding regret in continuously changing environments
  - Quick check question: Given variation budget VT = 3 and K = 2 arms, what is the maximum possible sum of absolute changes in expected rewards across all time steps?

- Concept: Upper confidence bound (UCB) algorithms
  - Why needed here: Forms basis for both DUCB and SWUCB algorithms used to handle abrupt changes
  - Quick check question: In standard UCB, how does confidence radius term ctp, aq scale with number of times arm a has been pulled?

## Architecture Onboarding

- Component map: Principal agent -> Compensation calculator -> Bandit algorithm module -> Feedback processor -> Performance monitor
- Critical path: Principal recommends arm → Agent pulls arm → Agent receives reward with drift → Principal updates estimates → Principal computes next compensation
- Design tradeoffs:
  - DUCB vs SWUCB: DUCB uses memory-efficient discounting but requires tuning γ; SWUCB uses fixed window τ but needs more memory
  - Batch size σ: Larger batches reduce restart overhead but increase adaptation lag; smaller batches adapt faster but increase computational cost
  - Compensation amount: Higher compensation encourages exploration but increases cost; lower compensation reduces cost but may not incentivize exploration
- Failure signatures:
  - Compensation grows linearly with T: Likely indicates algorithm cannot adapt to non-stationarity
  - Regret plateaus above O(√T): Suggests breakpoints or variations too frequent for chosen method
  - Algorithm consistently selects suboptimal arms: May indicate incorrect drift function assumptions or insufficient exploration
- First 3 experiments:
  1. Implement Algorithm 3 with DUCB on synthetic data with known breakpoints; verify regret scales as O(√βTT)
  2. Test Algorithm 4 with UCB1 on sinusoidal reward patterns; confirm regret scales as O(V^(1/3)T^(2/3))
  3. Compare compensation growth rates between DUCB and SWUCB as breakpoint frequency increases

## Open Questions the Paper Calls Out

- Question: How do the proposed algorithms perform in more complex non-stationary environments beyond the abruptly-changing and continuously-changing models considered?
  - Basis in paper: [inferred] The paper focuses on two specific non-stationary models and does not explore other potential models or their impact on algorithm performance
  - Why unresolved: The paper does not provide empirical or theoretical analysis of the algorithms' performance in other non-stationary environments
  - What evidence would resolve it: Experimental results or theoretical bounds for the algorithms' performance in other non-stationary environments, such as piece-wise stationary or Markovian changing environments

- Question: What is the impact of different reward drift functions on the performance of the proposed algorithms?
  - Basis in paper: [inferred] The paper assumes a specific form of reward drift function but does not explore the impact of different functions on algorithm performance
  - Why unresolved: The paper does not provide empirical or theoretical analysis of the algorithms' performance under different reward drift functions
  - What evidence would resolve it: Experimental results or theoretical bounds for the algorithms' performance under various reward drift functions, such as linear, exponential, or polynomial drift

- Question: How do the proposed algorithms perform in scenarios with multiple agents and principals?
  - Basis in paper: [inferred] The paper focuses on a single agent-principal setup and does not consider scenarios with multiple agents and principals
  - Why unresolved: The paper does not provide empirical or theoretical analysis of the algorithms' performance in multi-agent and multi-principal scenarios
  - What evidence would resolve it: Experimental results or theoretical bounds for the algorithms' performance in scenarios with multiple agents and principals, considering factors such as competition, cooperation, and coordination

## Limitations
- Assumption of Lipschitz continuity for drift functions may not hold in real-world scenarios with non-smooth compensation effects
- Performance bounds assume perfect knowledge of environmental parameters (breakpoint frequency βT, variation budget VT)
- Compensation mechanism relies on accurate estimation of optimal arms, challenging in highly volatile environments

## Confidence
- High confidence: Sublinear regret bounds for abruptly-changing environments (Õ(√T)) given the DUCB/SWUCB framework
- Medium confidence: Compensation bounds being sublinear in T, as this depends heavily on accurate drift function modeling
- Medium confidence: Õ(T^(2/3)) regret bound for continuously-changing environments, as this assumes known variation budget bounds

## Next Checks
1. Implement synthetic experiments testing DUCB and SWUCB algorithms across varying breakpoint frequencies to empirically verify the O(√βTT) regret scaling
2. Design a stress test where drift functions violate Lipschitz continuity assumptions to identify algorithm failure modes
3. Compare compensation costs between theoretical bounds and empirical measurements across different non-stationary patterns