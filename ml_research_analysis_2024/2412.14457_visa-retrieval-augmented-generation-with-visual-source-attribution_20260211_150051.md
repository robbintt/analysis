---
ver: rpa2
title: 'VISA: Retrieval Augmented Generation with Visual Source Attribution'
arxiv_id: '2412.14457'
source_url: https://arxiv.org/abs/2412.14457
tags:
- bounding
- visa
- document
- answer
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VISA, a novel visual source attribution method
  for retrieval-augmented generation (RAG) systems. VISA leverages large vision-language
  models (VLMs) to generate answers and visually highlight supporting evidence within
  document screenshots using bounding boxes.
---

# VISA: Retrieval Augmented Generation with Visual Source Attribution

## Quick Facts
- arXiv ID: 2412.14457
- Source URL: https://arxiv.org/abs/2412.14457
- Authors: Xueguang Ma; Shengyao Zhuang; Bevan Koopman; Guido Zuccon; Wenhu Chen; Jimmy Lin
- Reference count: 12
- Primary result: VISA achieves 41.6% bounding box accuracy and 51.1% answer accuracy in multi-candidate settings using fine-tuned VLMs

## Executive Summary
This paper introduces VISA, a novel visual source attribution method for retrieval-augmented generation (RAG) systems that leverages large vision-language models to generate answers while visually highlighting supporting evidence within document screenshots using bounding boxes. The authors curated two datasets - Wiki-VISA from Wikipedia webpages and Paper-VISA from PubLayNet for the medical domain - to train and evaluate the system. Experimental results demonstrate that fine-tuning VLMs on these datasets significantly improves both bounding box accuracy and answer quality compared to zero-shot prompting, while highlighting the method's effectiveness in enhancing transparency and verifiability in RAG systems.

## Method Summary
VISA employs large vision-language models to process document screenshots and generate answers with visual evidence attribution. The system takes screenshots of retrieved documents, processes them through a VLM, and generates answers while simultaneously producing bounding boxes that highlight the supporting evidence. The approach is trained and evaluated on two curated datasets: Wiki-VISA (based on Wikipedia webpages) and Paper-VISA (derived from PubLayNet for the medical domain). The fine-tuning process improves both the accuracy of the generated answers and the precision of the visual evidence highlighting compared to zero-shot prompting baselines.

## Key Results
- 7B model achieves 41.6% bounding box accuracy in multi-candidate settings
- 7B model achieves 51.1% answer accuracy in multi-candidate settings
- Fine-tuned VLMs show significant improvement over zero-shot prompting baselines

## Why This Works (Mechanism)
VISA works by combining visual and textual understanding through VLMs to process document screenshots and generate both answers and visual evidence highlights. The fine-tuning on domain-specific datasets (Wiki-VISA and Paper-VISA) enables the model to better understand document layouts and identify relevant supporting evidence. The bounding box generation capability provides transparency by visually connecting answers to their sources within the document, addressing the verifiability challenge in RAG systems.

## Foundational Learning
- Vision-Language Models (VLMs): Combine visual and textual understanding to process document screenshots - needed for processing complex document layouts and generating visual evidence highlights; quick check: model can accurately describe document contents from screenshots
- Bounding Box Generation: Spatial annotation technique to highlight relevant document regions - needed to visually connect answers to supporting evidence; quick check: generated boxes accurately enclose relevant text or figures
- Fine-tuning on Domain-Specific Data: Adapting pre-trained models to specific document types - needed to improve performance on specialized layouts and content; quick check: model performance improves on domain-specific evaluation sets
- Retrieval-Augmented Generation (RAG): Framework combining document retrieval with text generation - needed to provide context for answer generation; quick check: retrieved documents contain relevant information for answering queries
- Document Layout Understanding: Ability to parse and interpret visual document structure - needed to navigate complex multi-page documents; quick check: model can correctly identify sections, figures, and tables in various layouts
- Visual Evidence Attribution: Linking generated answers to specific document regions - needed to enhance transparency and verifiability; quick check: highlighted evidence directly supports the generated answer

## Architecture Onboarding

**Component Map:** Document screenshots -> VLM processing -> Answer generation + Bounding box prediction

**Critical Path:** User query -> Document retrieval -> Screenshot capture -> VLM processing -> Answer + Evidence highlighting

**Design Tradeoffs:** Fine-tuning vs zero-shot performance, model size (7B) vs accuracy, single vs multi-page document handling, general vs domain-specific training data

**Failure Signatures:** Inaccurate bounding boxes when document layouts differ from training data, answer quality degradation on multi-page documents, poor performance on layouts not represented in training sets

**3 First Experiments:**
1. Test zero-shot VLM performance on Wiki-VISA dataset
2. Evaluate bounding box accuracy on single-page vs multi-page documents
3. Compare answer quality between general and fine-tuned models on Paper-VISA dataset

## Open Questions the Paper Calls Out
None

## Limitations
- Performance degrades significantly when handling multi-page documents and diverse layouts
- Evaluation relies on two curated datasets that may not capture full real-world diversity
- Limited to 7B parameter models without exploration of larger architectures
- No direct comparisons with other specialized RAG attribution methods

## Confidence
- Core claims about VISA's effectiveness: Medium
- Methodology soundness: High
- Generalizability of findings: Medium
- Technical implementation robustness: High

## Next Checks
1. Test VISA on additional datasets with more diverse document layouts, including mixed-media documents and varying page layouts
2. Evaluate performance with larger VLM models (e.g., 30B+ parameters) and different VLM architectures
3. Conduct user studies to assess the practical utility and interpretability of the visual evidence attribution for non-expert users in real-world RAG applications