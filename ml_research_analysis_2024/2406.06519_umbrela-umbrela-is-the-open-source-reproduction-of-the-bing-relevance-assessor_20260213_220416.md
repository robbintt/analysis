---
ver: rpa2
title: 'UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor'
arxiv_id: '2406.06519'
source_url: https://arxiv.org/abs/2406.06519
tags:
- relevance
- judgments
- trec
- passage
- umbrela
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "UMBRELA is an open-source toolkit that reproduces and extends\
  \ a Microsoft Bing study showing that large language models can effectively replace\
  \ human assessors for relevance judgments. Using GPT-4o and zero-shot DNA prompting,\
  \ UMBRELA labels passages with 0\u20133 relevance scores across five TREC Deep Learning\
  \ Tracks."
---

# UMBRELA: UMbrela is the (Open-Source Reproduction of the Bing RELevance Assessor)

## Quick Facts
- arXiv ID: 2406.06519
- Source URL: https://arxiv.org/abs/2406.06519
- Reference count: 26
- UMBRELA reproduces LLM-based relevance assessment with fair agreement (κ ≈ 0.36) and preserves system rankings (Spearman ρ up to 0.99)

## Executive Summary
UMBRELA is an open-source toolkit that reproduces Microsoft Bing's study demonstrating large language models can replace human assessors for relevance judgments. Using GPT-4o with zero-shot DNA prompting, UMBRELA labels passages with 0–3 relevance scores across five TREC Deep Learning Tracks. The toolkit achieves fair agreement with human labels while maintaining high correlations between nDCG@10 rankings, confirming that LLM assessments preserve retrieval system ordering. Additionally, UMBRELA patches missing judgments to improve evaluation completeness and will support TREC 2024 RAG Track evaluation.

## Method Summary
UMBRELA employs zero-shot DNA prompting with GPT-4o to generate relevance judgments following the TREC Deep Learning Track schema. The DNA prompt breaks the task into descriptive, narrative, and aspects sections to guide LLM reasoning about query intent, content matching, and trustworthiness. The system processes query-passage pairs to produce integer scores (0–3) and can fill missing judgments left by incomplete human assessments. Evaluation uses Cohen's kappa for agreement measurement and rank correlation metrics (Kendall's tau and Spearman's rho) to validate preservation of system rankings.

## Key Results
- LLM-human agreement: Cohen's κ ≈ 0.36 (fair agreement)
- Rank correlation: Spearman ρ up to 0.99 between human and LLM-based nDCG@10 rankings
- Coverage improvement: UMBRELA patches missing relevance judgments to enhance evaluation completeness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: UMBRELA leverages LLM zero-shot DNA prompting to reproduce relevance judgments with fair agreement to human labels (κ ≈ 0.36).
- Mechanism: The zero-shot DNA prompt breaks the relevance labeling task into descriptive, narrative, and aspects steps, guiding the LLM to reason through query intent, content matching, and trustworthiness before outputting an integer score.
- Core assumption: LLMs can understand searcher intent and apply a structured reasoning process without task-specific training data.
- Evidence anchors:
  - [abstract] "Using GPT-4o and zero-shot DNA prompting, UMBRELA labels passages with 0–3 relevance scores"
  - [section] "we utilize the DNA prompting technique... consists of three essential sections: descriptive, narrative, and aspects"
  - [corpus] Weak: neighbor papers do not directly cite DNA prompting effectiveness; only general LLM assessment claims
- Break condition: If the LLM fails to follow the stepwise reasoning or misunderstands the query intent, agreement scores will drop sharply.

### Mechanism 2
- Claim: UMBRELA's judgments preserve retrieval system rankings (Spearman ρ up to 0.99) even with fair per-label agreement.
- Mechanism: High rank correlation arises because systematic LLM biases affect all judgments consistently, so relative ordering between systems is maintained despite label-level noise.
- Core assumption: LLM labeling errors are systematic rather than random across queries, preserving relative system performance.
- Evidence anchors:
  - [abstract] "high correlations between nDCG@10 rankings confirm that LLM assessments preserve retrieval system ordering (Spearman ρ up to 0.99)"
  - [section] "Table 2 (right) shows the results of Kendall τ and Spearman ρ correlations between evaluations performed with human ground-truth judgments and judgments provided by the LLM"
  - [corpus] Missing: no neighbor paper directly validates systematic bias preserving rankings
- Break condition: If LLM errors become random per query, rank correlation will degrade even if per-label agreement stays fair.

### Mechanism 3
- Claim: UMBRELA can patch missing relevance judgments to improve evaluation completeness.
- Mechanism: The LLM is used to label previously unjudged passage-query pairs, filling gaps left by human assessors and increasing coverage.
- Core assumption: LLM judgments for missing pairs are as reliable as those for already-judged pairs.
- Evidence anchors:
  - [abstract] "The toolkit also patches missing judgments"
  - [section] "relevance judgments facilitated by LLMs can be employed to 'fill holes' left by incomplete judgments"
  - [corpus] Weak: only indirect neighbor paper on unjudged document problem, no direct LLM patching evidence
- Break condition: If LLM performance degrades on truly unseen queries, patched judgments will introduce noise rather than completeness.

## Foundational Learning

- Concept: Cohen's κ and rank correlation metrics
  - Why needed here: To quantify agreement between LLM and human judgments and validate that system rankings are preserved
  - Quick check question: What does a κ of 0.36 indicate about LLM-human agreement?
- Concept: Zero-shot prompting and DNA prompting structure
  - Why needed here: Core to how UMBRELA generates judgments without training data
  - Quick check question: What are the three sections of the DNA prompt and their purposes?
- Concept: TREC Deep Learning Track relevance labeling schema
  - Why needed here: UMBRELA must reproduce the exact 0–3 scale used by human assessors
  - Quick check question: What distinguishes a label of 2 from a label of 3 in TREC DL?

## Architecture Onboarding

- Component map: UMBRELA → Input: query + passages → LLM (GPT-4o, zero-shot DNA) → Output: relevance scores → Optional: patch missing judgments → Integration with evaluation pipeline
- Critical path: Query + passages → DNA prompt → LLM inference → Score aggregation → Correlation/evaluation output
- Design tradeoffs: Zero-shot avoids labeling cost but may underperform fine-tuned models; patching increases coverage but risks noise
- Failure signatures: Low κ scores, poor rank correlation, LLM refusing to output integer scores, high variance in per-label accuracy
- First 3 experiments:
  1. Run UMBRELA on a small TREC DL subset and compare per-label confusion matrix to human labels
  2. Evaluate a known TREC run with both human and LLM judgments and compute nDCG@10 correlation
  3. Test LLM patching on a dataset with artificially removed judgments and measure completeness vs. accuracy tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLM-based relevance judgments perform across different types of queries (e.g., factoid vs. complex informational queries)?
- Basis in paper: [inferred] The paper mentions ambiguity in some query-passage pairs but doesn't systematically analyze performance across query types.
- Why unresolved: The paper focuses on overall correlation metrics but doesn't break down performance by query complexity or type.
- What evidence would resolve it: A detailed analysis of LLM performance across different query categories, showing which types are most/least accurately assessed.

### Open Question 2
- Question: What is the impact of different prompt engineering techniques on LLM assessment accuracy?
- Basis in paper: [explicit] The paper uses DNA prompting but doesn't explore alternative prompting strategies.
- Why unresolved: Only one prompting technique was tested, leaving open whether better techniques could improve accuracy.
- What evidence would resolve it: Comparative experiments testing various prompting approaches (chain-of-thought, role-based, etc.) to determine optimal prompting for relevance assessment.

### Open Question 3
- Question: How do open-source LLMs compare to GPT-4o for relevance assessment tasks?
- Basis in paper: [inferred] The paper exclusively uses GPT-4o but recent work shows open-source models can be effective for annotation tasks.
- Why unresolved: The study only evaluates one proprietary model, despite the availability of open-source alternatives.
- What evidence would resolve it: Head-to-head comparisons of GPT-4o versus state-of-the-art open-source models (Llama, Mistral, etc.) on the same relevance assessment tasks.

## Limitations
- The reported Cohen's κ ≈ 0.36 indicates only fair agreement between LLM and human judgments
- The claim of preserving system rankings despite low per-label agreement may indicate systematic bias rather than true assessment quality
- The effectiveness of patching missing judgments lacks direct validation with only indirect evidence

## Confidence
- High confidence: UMBRELA successfully implements zero-shot DNA prompting and generates relevance scores
- Medium confidence: LLM judgments preserve system rankings despite fair agreement
- Low confidence: LLM patching reliably fills missing judgments without introducing noise

## Next Checks
1. Compute confusion matrices between LLM and human judgments to identify systematic error patterns
2. Apply UMBRELA to a held-out TREC dataset not used in the original study to test generalizability
3. Create synthetic datasets with known missing judgments, apply UMBRELA patching, and measure accuracy degradation compared to ground truth