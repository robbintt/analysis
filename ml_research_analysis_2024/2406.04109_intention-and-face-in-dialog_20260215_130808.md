---
ver: rpa2
title: Intention and Face in Dialog
arxiv_id: '2406.04109'
source_url: https://arxiv.org/abs/2406.04109
tags:
- face
- acts
- which
- dialog
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the relationship between communicative
  intentions and face acts in dialog. The authors train a generative model for face
  act classification, achieving state-of-the-art performance (73% F1, up from 69%).
---

# Intention and Face in Dialog

## Quick Facts
- arXiv ID: 2406.04109
- Source URL: https://arxiv.org/abs/2406.04109
- Authors: Adil Soubki; Owen Rambow
- Reference count: 0
- State-of-the-art performance: 73% F1 for face act classification

## Executive Summary
This paper investigates the relationship between communicative intentions and face acts in dialog. The authors train a generative model for face act classification, achieving state-of-the-art performance of 73% F1 (up from 69%). They then explore how dialog act information affects face act detection, finding that it helps for minority classes but hurts overall performance. Through error analysis, they identify annotation inconsistencies and suggest that utterances often perform multiple face acts simultaneously.

## Method Summary
The authors train a generative model for face act classification, leveraging dialog act information to improve performance. They evaluate their approach on standard datasets, comparing against previous work. The model architecture incorporates both face act and dialog act features, with specific attention to how dialog act information influences face act detection across different classes.

## Key Results
- Achieved state-of-the-art face act classification performance (73% F1, up from 69%)
- Dialog act information helps for minority classes but hurts overall performance
- Error analysis revealed annotation inconsistencies and simultaneous performance of multiple face acts

## Why This Works (Mechanism)
The generative model effectively captures the relationship between face acts and dialog acts by leveraging the semantic information encoded in dialog acts. The mechanism works because dialog acts provide contextual information about the speaker's communicative goals, which correlates with the face act being performed. The model's architecture allows it to learn these correlations while maintaining the ability to classify face acts independently when dialog act information is not beneficial.

## Foundational Learning

### Generative Modeling
- Why needed: Enables probabilistic modeling of face act sequences and dependencies
- Quick check: Model should output well-calibrated probabilities and handle uncertainty

### Face Act Classification
- Why needed: Core task of identifying the social function of utterances in dialog
- Quick check: Baseline accuracy should exceed random chance for balanced datasets

### Dialog Act Integration
- Why needed: Provides contextual information that can disambiguate face acts
- Quick check: Performance should improve when dialog act features are added to the model

## Architecture Onboarding

### Component Map
Input Features -> Generative Model -> Face Act Classifier -> Output Predictions

### Critical Path
The critical path is: Input Features → Generative Model → Face Act Classifier → Output Predictions. This path determines the overall latency and is most sensitive to computational bottlenecks.

### Design Tradeoffs
The authors chose a generative approach over discriminative methods to better model the sequential nature of face acts and their dependencies. This allows for richer modeling of uncertainty but increases computational complexity during inference.

### Failure Signatures
- Performance degradation on minority classes when dialog act features are included
- Inconsistent predictions across similar utterances suggesting annotation issues
- Low confidence scores on ambiguous utterances that humans find difficult to annotate

### First Experiments
1. Train baseline face act classifier without dialog act features
2. Train generative model with dialog act features on full dataset
3. Perform ablation study removing dialog act features for each class

## Open Questions the Paper Calls Out
None

## Limitations

- The 4 percentage point improvement over previous work, while significant, suggests the task remains challenging
- Mixed effects of dialog act information (helping minority classes but hurting overall performance) require further investigation
- Annotation inconsistencies and the phenomenon of multiple simultaneous face acts indicate fundamental challenges in task formulation

## Confidence

**Face act classification results:** Medium - The state-of-the-art performance is demonstrated, but the improvement is incremental and the task difficulty is not fully characterized.

**Dialog act integration effects:** Low - The mixed effects on performance are observed but not thoroughly explained or validated.

**Annotation scheme limitations:** High - The authors clearly identify issues, though their impact is not fully quantified.

## Next Checks

1. Conduct a detailed ablation study to understand which dialog act features contribute to improved performance on minority classes versus overall performance degradation.

2. Implement and evaluate an annotation scheme that explicitly allows for multiple simultaneous face acts, then retrain and test the model to measure performance changes.

3. Perform inter-annotator agreement studies on the current dataset to quantify the extent of annotation inconsistencies and their correlation with model errors.