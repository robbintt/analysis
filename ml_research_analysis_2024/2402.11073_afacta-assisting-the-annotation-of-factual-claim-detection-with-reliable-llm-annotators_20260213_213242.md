---
ver: rpa2
title: 'AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable
  LLM Annotators'
arxiv_id: '2402.11073'
source_url: https://arxiv.org/abs/2402.11073
tags:
- error
- afacta
- claim
- factual
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AFaCTA is a framework that assists in annotating factual claims
  for fact-checking by leveraging large language models. It addresses the high cost
  of manual annotation by calibrating LLM annotation reliability through consistency
  across three reasoning paths.
---

# AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators

## Quick Facts
- arXiv ID: 2402.11073
- Source URL: https://arxiv.org/abs/2402.11073
- Reference count: 40
- Primary result: AFaCTA achieves 98.49% accuracy when all three reasoning paths achieve perfect consistency

## Executive Summary
AFaCTA is a framework that assists in annotating factual claims for fact-checking by leveraging large language models. It addresses the high cost of manual annotation by calibrating LLM annotation reliability through consistency across three reasoning paths. When all reasoning paths achieve perfect consistency, AFaCTA's annotations match expert-level accuracy (98.49%) on political speech data. This enables automatic labeling of 48.78% of data, significantly reducing expert effort. The framework also produces high-quality training data for claim detection classifiers, with perfectly consistent annotations performing nearly as well as expert-annotated data.

## Method Summary
AFaCTA uses a three-step approach with large language models to annotate factual claims. First, it performs direct classification of claims as factual or not. Second, it uses chain-of-thought reasoning to break down claims and extract factual components. Third, it employs reasoning with debate to evaluate competing arguments. The framework calibrates annotation reliability by requiring agreement across all three reasoning paths. When all paths achieve perfect consistency, the annotations are accepted automatically; otherwise, expert supervision is required. This consistency-based calibration enables AFaCTA to match expert accuracy while reducing manual annotation effort.

## Key Results
- Achieves 98.49% accuracy on political speech data when all reasoning paths achieve perfect consistency
- Enables automatic labeling of 48.78% of data, significantly reducing expert annotation effort
- Produces high-quality training data for claim detection classifiers, with perfectly consistent annotations performing nearly as well as expert-annotated data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** AFaCTA achieves high annotation accuracy when all three reasoning paths achieve perfect consistency.
- **Mechanism:** The framework calibrates annotation reliability by requiring agreement across three predefined reasoning paths that examine claims from different angles (direct classification, fact extraction with chain of thought, and reasoning with debate).
- **Core assumption:** Consistency across diverse reasoning approaches indicates higher annotation quality than any single approach.
- **Evidence anchors:**
  - [abstract]: "When all reasoning paths achieve perfect consistency, AFaCTA's annotations match expert-level accuracy (98.49%) on political speech data."
  - [section]: "Our evaluation shows that AFaCTA outperforms experts by a large margin when all reasoning paths achieve perfect consistency but fails to achieve expert-level performance on inconsistent samples."
  - [corpus]: Weak - the corpus contains related papers about LLM fact-checking but none directly address multi-path consistency calibration.

### Mechanism 2
- **Claim:** Predefined reasoning paths outperform automatically sampled CoTs for self-consistency calibration.
- **Mechanism:** The framework uses three carefully designed reasoning paths that force critical thinking from different angles, rather than sampling random CoTs from the model itself.
- **Core assumption:** Expert-designed reasoning paths are more effective at capturing the complexity of claim verifiability than randomly sampled model-generated reasoning.
- **Evidence anchors:**
  - [section]: "we show that predefined reasoning paths with expertise outperform those automatically sampled by LLMs" and "the accuracy of GPT-4 tends to converge to 85%" with self-consistency CoT versus "98.49%" with AFaCTA.
  - [section]: "One possible explanation is that the predefined paths encourage critical thinking and reasoning from different angles, making the achieved self-consistency more comprehensive."
  - [corpus]: Weak - corpus papers discuss LLM fact-checking but none comparing predefined versus sampled reasoning paths.

### Mechanism 3
- **Claim:** AFaCTA can automatically label 48.78% of data when perfect consistency is achieved, significantly reducing expert effort.
- **Mechanism:** The framework identifies a subset of data where all reasoning paths agree perfectly, allowing automatic labeling without expert supervision.
- **Core assumption:** Perfect consistency across multiple reasoning paths indicates high confidence in the annotation, making human supervision unnecessary.
- **Evidence anchors:**
  - [abstract]: "This enables automatic labeling of 48.78% of data, significantly reducing expert effort."
  - [section]: "Takeaway: With AFaCTA's self-consistency calibration, auto-annotation of perfectly consistent samples can be reliably adopted to reduce manual effort" and "only 51.22% needs further supervision, while 48.78% of manual effort is saved."
  - [corpus]: Weak - corpus papers discuss LLM annotation but don't address consistency-based automatic labeling.

## Foundational Learning

- **Concept:** Verifiability as the core dimension of factual claims
  - Why needed here: The framework defines claims based on verifiability rather than check-worthiness to minimize subjectivity and maximize objectivity in annotation
  - Quick check question: What distinguishes a verifiable fact from a subjective opinion in political speech?

- **Concept:** Chain of thought reasoning in LLMs
  - Why needed here: Step 2 uses CoT to break down the claim analysis into analyzing objective/subjective parts, extracting factual parts, and reasoning about verifiability
  - Quick check question: How does step-by-step reasoning help LLMs better understand complex claim structures?

- **Concept:** Position bias in LLM-as-judge scenarios
  - Why needed here: Step 3 addresses position bias by running the judging step twice with positions swapped to ensure fair evaluation
  - Quick check question: Why might an LLM show bias when judging between two competing arguments?

## Architecture Onboarding

- **Component map:** Direct Classification -> Fact-Extraction CoT -> Reasoning with Debate -> Majority Voting
- **Critical path:** The most critical component is the consistency calibration - if reasoning paths don't achieve agreement, the framework falls back to requiring expert supervision for those samples.
- **Design tradeoffs:** The framework trades computational cost (multiple LLM calls) for higher annotation quality and reduced expert effort. The three-step approach is more expensive than single-call annotation but achieves better results.
- **Failure signatures:** If AFaCTA shows low consistency rates, it indicates either the task is too ambiguous for LLMs or the predefined paths need refinement. If accuracy is low even with perfect consistency, the reasoning paths may be flawed.
- **First 3 experiments:**
  1. Test AFaCTA on a small sample set with known gold labels to verify the consistency-accuracy relationship
  2. Compare predefined reasoning paths against self-consistency CoT on the same data to validate mechanism 2
  3. Run AFaCTA with different LLM models (GPT-3.5 vs GPT-4) to understand model-specific performance differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does AFaCTA's performance on domains beyond political speech and COVID-19 social media?
- Basis in paper: [inferred] The paper mentions limited exploration of social media domain and leaves extensive study on other domains to future work
- Why unresolved: Only tested on political speech and one small social media dataset due to annotation budget constraints
- What evidence would resolve it: Large-scale testing of AFaCTA on diverse domains like scientific claims, financial news, and health information

### Open Question 2
- Question: Can open-source LLMs achieve comparable performance to GPT-4 when using AFaCTA framework?
- Basis in paper: [explicit] The paper shows open-source models like Llama-2-chat-13b and Zephyr-7b-beta suffer from heavy position bias in Step 3
- Why unresolved: Current open-source models show significant position bias issues that prevent consistent self-consistency
- What evidence would resolve it: Fine-tuning open-source models specifically to mitigate position bias or developing new prompting strategies that work better with open-source LLMs

### Open Question 3
- Question: How much does the quality of manual annotations affect AFaCTA's performance calibration?
- Basis in paper: [explicit] AFaCTA calibrates performance based on self-consistency, and experiments show differences between gold, silver, and bronze datasets
- Why unresolved: The paper doesn't explore how variations in expert annotation quality impact the self-consistency calibration mechanism
- What evidence would resolve it: Comparative studies using annotations from experts with different levels of domain expertise or training on datasets with known annotation quality variations

## Limitations
- The framework's effectiveness depends heavily on the predefined reasoning paths, but exact prompt templates are not fully specified
- Evaluation only covers political speech and COVID-19 tweet domains, leaving questions about generalizability to other fact-checking domains
- The computational cost of multiple LLM calls may be prohibitive for some applications

## Confidence
- **High Confidence**: The claim that AFaCTA achieves 98.49% accuracy when all reasoning paths achieve perfect consistency is well-supported by the results on PoliClaimtest.
- **Medium Confidence**: The assertion that predefined reasoning paths outperform automatically sampled CoTs is supported but relies on indirect evidence comparing AFaCTA to baseline self-consistency approaches.
- **Medium Confidence**: The claim about automatically labeling 48.78% of data is supported by the presented statistics but lacks deeper analysis of whether this percentage varies across different data characteristics.

## Next Checks
1. Replicate AFaCTA on a small sample set with known gold labels to verify the consistency-accuracy relationship holds beyond the original dataset.
2. Compare AFaCTA's predefined reasoning paths against the self-consistency CoT approach on the same data to validate the claimed performance difference.
3. Test AFaCTA with different LLM models (GPT-3.5 vs GPT-4) to understand whether the performance gains are model-specific or generalizable across different LLM capabilities.