---
ver: rpa2
title: 'Meaningless is better: hashing bias-inducing words in LLM prompts improves
  performance in logical reasoning and statistical learning'
arxiv_id: '2411.17304'
source_url: https://arxiv.org/abs/2411.17304
tags:
- experiment
- prompt
- which
- hashed
- hashing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a "hashing" method that masks bias-inducing
  words in LLM prompts with meaningless identifiers to reduce cognitive biases and
  improve logical reasoning and statistical learning performance. Across three experiments
  (490 prompts), chi-square tests showed significant improvements.
---

# Meaningless is better: hashing bias-inducing words in LLM prompts improves performance in logical reasoning and statistical learning

## Quick Facts
- arXiv ID: 2411.17304
- Source URL: https://arxiv.org/abs/2411.17304
- Reference count: 2
- Primary result: Hashing bias-inducing words with meaningless identifiers improves logical reasoning and statistical learning performance in LLMs

## Executive Summary
This paper introduces a novel debiasing method for large language models (LLMs) by replacing bias-inducing words in prompts with meaningless hash-like identifiers. The approach aims to reduce cognitive biases, particularly the conjunction fallacy and representativeness heuristic, while improving performance on logical reasoning and statistical learning tasks. Across three experiments involving 490 prompts, the authors demonstrate that hashing significantly improves performance on the Linda problem and frequent itemset extraction tasks, with effects varying by model type and task complexity.

## Method Summary
The method involves preprocessing prompts to replace bias-inducing words with meaningless identifiers (e.g., "woman" → "X", "artist" → "b321") before sending them to LLMs. This hashing transformation was applied to three experimental setups: a modified Linda problem testing conjunction fallacy, a frequent itemset extraction task, and a tabular version of the Linda problem. The authors compared performance between hashed and unhashed prompts using chi-square tests for statistical significance, measuring fallacy rates, itemset extraction accuracy, and hallucination rates across multiple LLM models including GPT-3.5, GPT-4, Gemini, Llama 2, Llama 3.1, and Mixtral.

## Key Results
- Hashing reduced fallacy rates in the modified Linda problem, with chi-square tests showing p < 0.001 for most models
- Improved frequent itemset extraction accuracy when hashing prevented models from substituting prior knowledge for input data
- Benefits maintained when using tabular (CSV) format, with most models showing correct responses in hashed variants
- Inconsistent effects on hallucination rates across models, with some showing increased hallucinations (e.g., Llama 3.1)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hashing reduces the impact of representativeness heuristics by removing salient bias-inducing lexical cues from prompts.
- Mechanism: When words that strongly activate stereotype-related or knowledge-based associations are replaced with meaningless identifiers, the model can no longer rely on these superficial cues to bias its inference, forcing a more neutral or logical response.
- Core assumption: The representativeness heuristic in LLMs operates primarily through the presence of specific lexical items rather than their underlying semantic roles or syntactic positions.
- Evidence anchors:
  - [abstract] "...masking potentially bias-inducing words in large language models (LLMs) with hash-like meaningless identifiers to reduce cognitive biases..."
  - [section] "We do this by hiding parts of the text that might present stereotypes or lead the model to make biased decisions based on patterns observed in the training data."
- Break Condition: If the bias-inducing effect is instead caused by semantic or structural cues that survive hashing (e.g., ordering, role, or phrase structure), the mechanism fails.

### Mechanism 2
- Claim: Hashing mitigates reliance on external knowledge in statistical reasoning tasks by breaking the association between dataset entries and real-world knowledge.
- Mechanism: In tasks like frequent itemset extraction, replacing known entities (e.g., animal names, traits) with meaningless tokens prevents the model from using its pre-trained associations (e.g., "rabbit" = 4 legs) to shortcut the counting logic, forcing actual computation over the input data.
- Core assumption: LLMs will automatically substitute their own knowledge for actual input content when both are present, and that this substitution is driven by the presence of recognizable tokens.
- Evidence anchors:
  - [section] "We introduced the CSV-wrong dataset, which contained the same animals but was described by 'wrong' trait values; for example, a rabbit is specified to have six legs."
  - [section] "The CSV-hashed dataset was derived by replacing all occurrences of each value in the CSV-correct with a meaningless identifier."
- Break Condition: If the model instead performs correct reasoning based on explicit token frequencies or structural cues, hashing would not improve accuracy.

### Mechanism 3
- Claim: Changing the input representation from free text to structured tabular (CSV) format reduces susceptibility to the conjunction fallacy by reducing reliance on narrative-style heuristic processing.
- Mechanism: Tabular data may prompt the model to process inputs more formally (e.g., as key-value pairs or structured records), thereby engaging logical rules over intuitive heuristic matching, especially when combined with hashing.
- Core assumption: The model treats free text and structured data differently, engaging different reasoning pathways; structured input promotes rule-based over heuristic processing.
- Evidence anchors:
  - [section] "In the hashed variant of the tabular representation, the majority of answers for all models except Gemini was correct."
  - [section] "We conducted a separate set of experiments on each of the three dataset versions... The prompt for CSV-correct is shown in Figure 8..."
- Break Condition: If the model processes both formats using the same underlying reasoning mechanism, format change alone will not affect fallacy rates.

## Foundational Learning

- Concept: Conjunction fallacy and representativeness heuristic
  - Why needed here: The paper's debiasing work is motivated by and tested on tasks where these biases are known to appear; understanding them is essential to grasp the experimental design and results.
  - Quick check question: In the Linda problem, why is it logically incorrect to say "Linda is a bank teller and is active in the feminist movement" is more probable than "Linda is a bank teller"?

- Concept: Frequent itemset mining and support counting
  - Why needed here: Experiment 2 tests whether LLMs can perform a core data mining task without code generation, and whether hashing improves this capability by preventing shortcutting via prior knowledge.
  - Quick check question: What is the "support" of an itemset in frequent itemset mining?

- Concept: Chi-square statistical significance testing
  - Why needed here: The paper uses chi-square tests to evaluate whether hashing significantly improves performance; understanding this allows interpretation of the reported p-values and claims of improvement.
  - Quick check question: In a chi-square test for independence, what does a p-value below 0.05 indicate?

## Architecture Onboarding

- Component map: Prompt preprocessor -> Hashing transformation -> LLM API call -> Output parsing -> Evaluation layer
- Critical path: Prompt generation → Hashing transformation → LLM API call → Output parsing → Evaluation (fallacy detection, itemset recall/precision, hallucination check)
- Design tradeoffs: Hashing improves debiasing but may reduce naturalness or cause loss of context; hashing may also increase hallucination in some models (e.g., Llama 3.1) due to over-reliance on internal knowledge when identifiers are opaque.
- Failure signatures: No improvement in fallacy rates despite hashing (model still picks conjunction), increased hallucination rates, or inconsistent performance across model types (e.g., GPT models improve, others do not).
- First 3 experiments:
  1. Test hashing on the Linda problem with a small set of bias-inducing words replaced; compare fallacy rates between hashed and unhashed prompts.
  2. Test hashing on a small CSV-based frequent itemset task with known ground truth; compare recall and precision.
  3. Test the combined effect of tabular representation and hashing on the Linda problem; compare fallacy rates to free-text hashed and unhashed variants.

## Open Questions the Paper Calls Out

- Question: How does the effectiveness of the hashing method vary across different types of cognitive biases beyond the conjunction fallacy?
  - Basis in paper: [explicit] The paper demonstrates hashing's effectiveness for conjunction fallacy but notes the need for further research on other biases.
  - Why unresolved: The experiments focused specifically on conjunction fallacy and representativeness heuristics, with limited exploration of other cognitive biases.
  - What evidence would resolve it: Systematic testing of hashing across multiple cognitive bias types (priming, anchoring, availability heuristic, etc.) with controlled experiments and statistical analysis.

- Question: What is the optimal balance between information preservation and bias reduction in the hashing method?
  - Basis in paper: [explicit] The paper identifies "losing information due to hiding parts of the text behind meaningless identifiers" as a key limitation.
  - Why unresolved: The paper experimented with and without added neutral descriptions but didn't systematically explore the tradeoff between information retention and bias mitigation.
  - What evidence would resolve it: Comparative experiments testing various levels of information preservation while measuring both bias reduction and task performance across different domains.

- Question: Does the hashing method scale effectively to larger, more complex datasets and tasks?
  - Basis in paper: [inferred] The frequent itemset experiment used a small dataset (4 rows), and the paper notes uncertainty about how results would scale.
  - Why unresolved: All experiments used relatively small datasets and simple tasks, leaving scalability questions unanswered.
  - What evidence would resolve it: Testing hashing on progressively larger datasets and more complex reasoning tasks while measuring performance degradation or improvement compared to baseline methods.

## Limitations
- Hashing's effectiveness is model- and task-dependent, with inconsistent effects on hallucination rates across models
- The exact list of bias-inducing words for each experiment is not fully specified, limiting reproducibility
- The mechanism by which hashing reduces fallacies (lexical masking vs. structural change) is not definitively established

## Confidence
- **High Confidence**: The claim that hashing reduces fallacy rates in the Linda problem is supported by statistically significant chi-square results across multiple models.
- **Medium Confidence**: The claim that hashing improves frequent itemset extraction accuracy is supported by the experimental results, but the mechanism (preventing knowledge substitution) is inferred rather than directly validated.
- **Low Confidence**: The claim that hashing consistently reduces hallucination rates is weakly supported, as results show inconsistent effects across models.

## Next Checks
1. **Replicate Hashing Effectiveness**: Test hashing on a broader set of bias-inducing words and tasks to confirm whether improvements generalize beyond the specific prompts used in the paper.
2. **Mechanism Validation**: Design controlled experiments to isolate whether hashing's effect is due to lexical masking, structural change, or both. For example, compare hashing with synonym replacement or reordering of bias-inducing phrases.
3. **Hallucination Impact**: Systematically measure hallucination rates across a wider range of models and tasks to determine under what conditions hashing increases or decreases hallucination.