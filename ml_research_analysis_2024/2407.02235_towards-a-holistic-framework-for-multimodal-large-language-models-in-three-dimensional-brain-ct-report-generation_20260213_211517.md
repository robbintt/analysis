---
ver: rpa2
title: Towards a Holistic Framework for Multimodal Large Language Models in Three-dimensional
  Brain CT Report Generation
arxiv_id: '2407.02235'
source_url: https://arxiv.org/abs/2407.02235
tags:
- braingpt
- data
- report
- reports
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of generating radiology reports
  for 3D brain CT scans using multimodal large language models (MLLMs). The authors
  collected a 3D-BrainCT dataset (18,885 text-scan pairs) and applied clinical visual
  instruction tuning (CVIT) to train BrainGPT models based on the open-source Otter
  architecture.
---

# Towards a Holistic Framework for Multimodal Large Language Models in Three-dimensional Brain CT Report Generation

## Quick Facts
- arXiv ID: 2407.02235
- Source URL: https://arxiv.org/abs/2407.02235
- Reference count: 0
- This study presents BrainGPT, an MLLM framework achieving strong performance on 3D brain CT report generation with 74% Turing test indistinguishability from human reports.

## Executive Summary
This study addresses the challenge of generating radiology reports for 3D brain CT scans using multimodal large language models (MLLMs). The authors collected a 3D-BrainCT dataset (18,885 text-scan pairs) and applied clinical visual instruction tuning (CVIT) to train BrainGPT models based on the open-source Otter architecture. They evaluated the models using traditional metrics and proposed a novel Feature-Oriented Radiology Task Evaluation (FORTE) to assess clinical relevance. The BrainGPT models achieved strong performance on traditional metrics and external validation, with 74% of generated reports being indistinguishable from human-written ones in a Turing test with physicians.

## Method Summary
The authors developed BrainGPT by fine-tuning the Otter MLLM architecture (CLIP ViT-L/14 + LLaMA-7B) using clinical visual instruction tuning (CVIT) with four conditions: Plain, In-context Example, Template, and Keyword Instructions. They created the 3D-BrainCT dataset (18,885 text-scan pairs) and applied sentence pairing and negation removal preprocessing to adapt traditional metrics for radiology reports. The model was evaluated using traditional metrics (BLEU, METEOR, ROUGE-L, CIDEr-R) and the novel FORTE framework that assesses clinical keyword categories (degree, landmark, feature, impression).

## Key Results
- BrainGPT models achieved strong traditional metric scores: BLEU-1=44.35, BLEU-4=20.38, METEOR=30.13, ROUGE-L=47.6, CIDEr-R=211.77
- FORTE evaluation showed average F1-scores of 0.71 across clinical keyword categories
- External validation on CQ500 dataset achieved 0.91 accuracy for midline shift detection
- Turing test with 11 physicians found 74% of BrainGPT reports indistinguishable from human-written ones

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CVIT improves BrainGPT's clinical sensibility by injecting structured radiology knowledge into the instruction-tuning process.
- Mechanism: The Template and Keyword Instructions provide domain-specific context that guides the model to generate clinically relevant reports rather than generic image captions.
- Core assumption: Radiology report generation benefits from explicit domain knowledge integration beyond generic image captioning.
- Evidence anchors:
  - [abstract] "applied clinical visual instruction tuning (CVIT) to train BrainGPT models"
  - [section] "we designed four distinct fine-tuning conditions... clinical visual instruction tuning (CVIT): Template Instruction... and Keyword Instruction"
- Break condition: If clinical domain knowledge is not properly encoded in the instruction templates, or if the model overfits to specific keyword patterns without understanding clinical context.

### Mechanism 2
- Claim: Sentence pairing and negation removal adapt traditional metrics to the unique structure of brain CT reports.
- Mechanism: Brain CT reports use list-by-list differential diagnosis format with many negative findings, which traditional metrics don't handle well. Sentence pairing decomposes multi-sentence paragraphs into smaller semantic units, while negation removal focuses evaluation on positive findings.
- Core assumption: Traditional evaluation metrics are not suitable for radiology reports due to their unique structure and content.
- Evidence anchors:
  - [abstract] "traditional metrics appeared to measure only the surface text similarity and failed to gauge the information density of the diagnostic purpose"
  - [section] "we applied sentence pairing to decompose the multi-sentence paragraph into smaller semantic granularity"
- Break condition: If sentence pairing fragments semantic relationships too much, or if negation removal removes clinically important negative findings.

### Mechanism 3
- Claim: FORTE captures clinical relevance by evaluating keyword categories (degree, landmark, feature, impression) rather than surface text similarity.
- Mechanism: FORTE uses domain-specific keyword categories that map to clinical reporting requirements, providing a multi-faceted evaluation of clinical content density rather than just n-gram overlap.
- Core assumption: Radiology report quality should be measured by clinical content density and relevance, not just text similarity to reference reports.
- Evidence anchors:
  - [abstract] "proposed a novel Feature-Oriented Radiology Task Evaluation (FORTE) to estimate the clinical relevance (lesion feature and landmarks)"
  - [section] "We categorized radiology keywords into degree, landmark, feature, and impression subsets to offer a multi-faceted evaluation"
- Break condition: If the keyword categories don't adequately capture all clinically relevant aspects, or if the evaluation becomes too rigid and misses nuanced clinical descriptions.

## Foundational Learning

- Concept: Multimodal Large Language Models (MLLMs)
  - Why needed here: BrainGPT is an MLLM that processes both 3D brain CT images and text to generate radiology reports.
  - Quick check question: What distinguishes an MLLM from a standard language model in the context of medical imaging?

- Concept: Visual Instruction Tuning
  - Why needed here: BrainGPT is fine-tuned using CVIT, which requires understanding how visual instruction tuning differs from standard fine-tuning approaches.
  - Quick check question: How does visual instruction tuning incorporate both image and text data during the fine-tuning process?

- Concept: Traditional Evaluation Metrics (BLEU, METEOR, ROUGE-L, CIDEr)
  - Why needed here: These metrics are used to evaluate BrainGPT's performance but are shown to be inadequate for radiology reports, requiring adaptation.
  - Quick check question: What are the fundamental limitations of traditional text evaluation metrics when applied to clinical radiology reports?

## Architecture Onboarding

- Component map: CLIP ViT-L/14 vision encoder -> Perceiver resampler -> LLaMA-7B language model
- Critical path: Data preprocessing → Visual instruction tuning (CVIT) → Report generation → Evaluation (traditional metrics + FORTE + Turing test)
- Design tradeoffs: Using the open-source Otter architecture (7B parameters) instead of larger custom models enables faster training (12 hours vs 7 days) but may limit performance on highly complex cases.
- Failure signatures: Poor clinical keyword usage, off-target descriptions, excessive negative findings, hallucinated content, or failure to recognize multi-object scenarios in 3D CT scans.
- First 3 experiments:
  1. Test BrainGPT on a small sample of 3D brain CT scans with known diagnoses to verify basic functionality
  2. Compare traditional metric scores before and after sentence pairing and negation removal to validate preprocessing effectiveness
  3. Run FORTE evaluation on generated reports to assess clinical keyword usage patterns and identify areas for instruction tuning improvement

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions in the main text. However, the research presents several areas that warrant further investigation based on the findings and limitations discussed.

## Limitations

- The specific instruction templates and clinical keyword lists for CVIT and FORTE are not fully specified, limiting reproducibility.
- The FORTE evaluation framework may not capture all clinically relevant aspects due to its reliance on predefined keyword categories.
- The Turing test involved only 11 physicians evaluating 66 reports each, which may not be representative of broader clinical practice.

## Confidence

**High Confidence**: Strong performance on traditional metrics and external validation (CQ500 dataset accuracy=0.91 for midline shift).

**Medium Confidence**: CVIT significantly improves clinical sensibility, supported by superior FORTE scores but lacking detailed ablation studies.

**Low Confidence**: 74% Turing test indistinguishability claim has limited statistical power due to small sample size (11 physicians, 66 reports each).

## Next Checks

1. Conduct ablation studies to isolate the impact of each CVIT component (Template vs Keyword Instructions) on both traditional metrics and FORTE scores.

2. Perform a comprehensive audit of the FORTE keyword categories against a larger sample of ground truth reports to ensure complete coverage of clinically relevant terminology.

3. Expand the human evaluation study to include 50+ physicians and 200+ reports to establish statistical significance and assess generalizability.