---
ver: rpa2
title: Benchmarking Japanese Speech Recognition on ASR-LLM Setups with Multi-Pass
  Augmented Generative Error Correction
arxiv_id: '2408.16180'
source_url: https://arxiv.org/abs/2408.16180
tags:
- best
- systems
- speech
- hypotheses
- proposed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks generative error correction (GER) for Japanese
  automatic speech recognition (ASR) using large language models (LLMs). The authors
  present the first GER benchmark for Japanese ASR with 0.9-2.6k text utterances and
  introduce a new multi-pass augmented generative error correction (MPA GER) method.
---

# Benchmarking Japanese Speech Recognition on ASR-LLM Setups with Multi-Pass Augmented Generative Error Correction

## Quick Facts
- arXiv ID: 2408.16180
- Source URL: https://arxiv.org/abs/2408.16180
- Reference count: 0
- Primary result: Proposed MPA GER method achieved CER as low as 1.8% on CSJ data

## Executive Summary
This paper introduces the first benchmark for generative error correction (GER) in Japanese automatic speech recognition (ASR) using large language models (LLMs). The authors propose a multi-pass augmented generative error correction (MPA GER) method that integrates multiple ASR system hypotheses with corrections from multiple LLMs. Experiments on SPREDS-U1-ja and CSJ datasets demonstrate significant improvements in ASR quality, particularly for short utterances with CER below 10. The MPA GER approach effectively reduces hallucinations and repetitions by combining outputs from different LLMs.

## Method Summary
The study benchmarks GER for Japanese ASR using pre-trained models including Whisper-Large-v3, Meta MMS, and CMU OWSM v3.1. The approach uses N-best hypotheses from ASR systems as input to LLM GER models (ELYZA-7b and Qwen1.5-7b). The MPA GER method combines multiple ASR system hypotheses on the input side with corrections from multiple LLMs on the output side, then merges them using ROVER system combination. Training involves 15-20 epochs with learning rate 1e-4 and LoRA rank 4, using 8-bit training. The method was evaluated on SPREDS-U1-ja (900 training, 100 evaluation sentences) and CSJ datasets (600 hours training, three evaluation sets with 424 utterances each).

## Key Results
- MPA GER achieved CER as low as 1.8% on CSJ dataset, outperforming previous GER approaches
- The method is particularly effective for short utterances with CER < 10
- Combining different LLMs in MPA GER reduces hallucinations and repetitions in LLM outputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MPA GER reduces hallucinations and repetitions by combining outputs from multiple LLMs
- Mechanism: Different LLMs exhibit varying tendencies toward hallucinations and repetitions. By merging their outputs, common hallucinations and repetitions are filtered out while correct information is reinforced.
- Core assumption: Different LLMs will hallucinate or repeat differently on the same input, allowing cross-model correction
- Evidence anchors:
  - [abstract] "The proposed MPA GER method achieved a Character Error Rate (CER) as low as 1.8% on CSJ data, outperforming previous GER approaches"
  - [section] "From both Table 5 and 6, we see the especially worse CERs... We observed that the LLM GER error correction generated extended hallucinations or repetitions in those high CER systems. Example 4 in Table 7 shows that the single LLM GER system generates repetitions, but the repetitions were not included in the MPA GER results."
  - [corpus] Weak - corpus provides related work but no direct evidence about hallucination reduction

### Mechanism 2
- Claim: MPA GER is particularly effective for short utterances with CER < 10
- Mechanism: Short utterances have fewer contextual cues, making them more vulnerable to ASR errors. MPA GER's multi-model approach compensates for this by leveraging diverse hypotheses.
- Core assumption: Short utterances are more error-prone and benefit more from multi-model correction
- Evidence anchors:
  - [section] "Fig. 4 shows the averaged CERs of 1-best and proposed MPA GER method in each reference length in Eval3. From Fig. 4, we can conclude the reduction trends in CER for short outputs below 10 or 20 and little effect for other relatively long outputs"
  - [section] "This led to an overall improvement in the proposed MPA GER, especially in Eval3, which contains more relatively short outputs"
  - [corpus] Weak - corpus provides related work but no direct evidence about utterance length effects

### Mechanism 3
- Claim: Combining multiple ASR system hypotheses provides diverse input that improves LLM GER performance
- Mechanism: Different ASR systems make different types of errors. By providing multiple hypotheses as input, the LLM has more information to work with and can choose the most likely correction path.
- Core assumption: Different ASR systems produce complementary errors that can be resolved through LLM processing
- Evidence anchors:
  - [section] "The leap forward in performance is due to the diversity of different system hypotheses. Both LLM GER-based N-best rescoring and system combination work on SPREDS-U1-ja"
  - [section] "Compared to SPREDS-U1-ja, the CER scores on CSJ are much lower. This would cause a slight improvement with the proposed method on CSJ compared to SPREDS-U1-ja"
  - [corpus] Weak - corpus provides related work but no direct evidence about ASR system diversity benefits

## Foundational Learning

- Concept: N-best hypotheses generation
  - Why needed here: The paper relies on N-best hypotheses from ASR systems as input to the LLM GER process
  - Quick check question: What is the difference between 1-best and N-best hypotheses in ASR systems?

- Concept: Character Error Rate (CER) calculation
  - Why needed here: CER is the primary metric used to evaluate the effectiveness of the GER methods
  - Quick check question: How is CER calculated and what does it measure?

- Concept: ROVER (Recognizer Output Voting Error Reduction)
  - Why needed here: ROVER is used as a baseline system combination method and as part of the MPA GER approach
  - Quick check question: What is the basic principle behind ROVER and how does it differ from LLM-based correction?

## Architecture Onboarding

- Component map: ASR systems → N-best hypotheses generation → LLM GER models (Elyza, Qwen1.5) → ROVER combination → Final output
- Critical path: ASR output → LLM correction → ROVER merging → Evaluation
- Design tradeoffs: Multiple LLM models increase accuracy but also computational cost and complexity
- Failure signatures: Increased CER when adding low-performing ASR models, hallucinations in LLM outputs, over-correction of disfluencies
- First 3 experiments:
  1. Test LLM GER with single ASR system on SPREDS-U1-ja dataset to establish baseline
  2. Compare MPA GER with 3-system vs 4-system combinations on CSJ dataset
  3. Evaluate performance differences between short and long utterances using MPA GER

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed MPA GER method perform when applied to ASR systems with significantly different error rates or domains (e.g., conversational speech vs. read speech)?
- Basis in paper: [inferred] The paper shows improvements in CER for both SPREDS-U1-ja (higher error rates) and CSJ (lower error rates) datasets, but does not explore the method's robustness across significantly different ASR conditions or domains.
- Why unresolved: The experiments focus on two Japanese datasets with relatively controlled conditions, limiting insights into the method's generalizability across diverse ASR scenarios.
- What evidence would resolve it: Testing MPA GER on a wider range of ASR datasets with varying error rates, accents, and speaking styles to evaluate performance consistency and identify potential limitations.

### Open Question 2
- Question: What is the impact of using different LLM architectures or fine-tuning strategies on the performance of the GER task, and how do these choices affect hallucination and repetition issues?
- Basis in paper: [explicit] The paper mentions that combining different LLMs in MPA GER helped reduce hallucinations and repetitions, but does not explore the effects of different LLM architectures or fine-tuning approaches.
- Why unresolved: The experiments use a limited set of LLM models and do not investigate the influence of architectural choices or fine-tuning strategies on GER performance.
- What evidence would resolve it: Conducting experiments with a broader range of LLM architectures (e.g., different model sizes, training objectives) and fine-tuning strategies to assess their impact on GER accuracy and the mitigation of hallucinations and repetitions.

### Open Question 3
- Question: How does the proposed MPA GER method compare to other advanced ASR error correction techniques, such as those leveraging contextual information or external knowledge bases?
- Basis in paper: [inferred] The paper compares MPA GER to traditional ROVER and previous LLM GER methods but does not benchmark it against other advanced error correction techniques that utilize contextual or external knowledge.
- Why unresolved: The experiments focus on comparing the proposed method to baseline approaches, without exploring its performance relative to state-of-the-art techniques that leverage additional information sources.
- What evidence would resolve it: Evaluating MPA GER against other advanced error correction methods that incorporate contextual information or external knowledge bases, using the same ASR datasets and performance metrics.

## Limitations
- Relatively small evaluation dataset (0.9-2.6k utterances) may not provide statistically robust results
- Focus exclusively on Japanese language ASR, limiting generalizability to other languages
- Does not address computational efficiency or inference latency for practical deployment

## Confidence
**High Confidence Claims:**
- MPA GER method outperforms single LLM GER approaches (CER of 1.8% achieved on CSJ data)
- MPA GER is particularly effective for short utterances (CER < 10)
- Combining different LLM outputs reduces hallucinations and repetitions

**Medium Confidence Claims:**
- System combination using multiple ASR hypotheses improves LLM GER performance
- The proposed method generalizes well across different datasets (SPREDS-U1-ja and CSJ)
- ROVER-based merging is effective for combining LLM GER outputs

**Low Confidence Claims:**
- The exact impact of utterance length on GER effectiveness across all ranges
- The scalability of the approach to larger datasets or different domains
- The robustness of the method to different types of ASR errors

## Next Checks
1. **Statistical Significance Testing**: Conduct paired statistical tests (e.g., bootstrap resampling) on the CER improvements to verify that the observed performance gains are statistically significant given the small evaluation dataset sizes.

2. **Cross-Lingual Validation**: Test the MPA GER approach on a non-Japanese language dataset (such as English or Mandarin) to validate whether the multi-pass augmented approach generalizes beyond Japanese ASR systems.

3. **Computational Cost Analysis**: Measure and report the computational overhead (both memory and inference time) of the MPA GER system compared to baseline approaches to assess practical deployment feasibility.