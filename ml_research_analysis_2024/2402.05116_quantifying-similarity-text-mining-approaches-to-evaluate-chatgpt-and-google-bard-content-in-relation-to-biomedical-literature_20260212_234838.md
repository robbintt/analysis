---
ver: rpa2
title: 'Quantifying Similarity: Text-Mining Approaches to Evaluate ChatGPT and Google
  Bard Content in Relation to BioMedical Literature'
arxiv_id: '2402.05116'
source_url: https://arxiv.org/abs/2402.05116
tags:
- chatgpt
- bard
- similarity
- pubmed
- google
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study compared content generated by ChatGPT and Google Bard
  with real biomedical literature on prostate cancer using text-mining and network
  analysis. Documents were generated via prompt engineering, then analyzed using document
  similarity (Cosine and Jaccard), bigram frequency (TF-IDF), and network centrality
  (degree and closeness).
---

# Quantifying Similarity: Text-Mining Approaches to Evaluate ChatGPT and Google Bard Content in Relation to BioMedical Literature

## Quick Facts
- arXiv ID: 2402.05116
- Source URL: https://arxiv.org/abs/2402.05116
- Reference count: 40
- Primary result: ChatGPT-generated content showed higher similarity to biomedical literature than Google Bard across all text-mining metrics

## Executive Summary
This study employed text-mining and network analysis to compare AI-generated content from ChatGPT and Google Bard against real biomedical literature on prostate cancer. Using prompt engineering to generate documents, the researchers applied multiple similarity metrics including Cosine and Jaccard coefficients, bigram frequency analysis via TF-IDF, and network centrality measures. The analysis revealed that ChatGPT consistently outperformed Google Bard in terms of document similarity, bigram similarity, and network structural similarity to biomedical literature. These findings suggest ChatGPT may be more effective at generating content that aligns with established medical knowledge and could potentially inspire clinically relevant questions.

## Method Summary
The study generated documents using ChatGPT and Google Bard through systematic prompt engineering focused on prostate cancer topics. These AI-generated documents were then compared against a corpus of biomedical literature obtained from Web of Science using specific search keywords. Text-mining techniques were applied including document similarity calculations using Cosine and Jaccard metrics, bigram frequency analysis using TF-IDF weighting, and network analysis examining degree and closeness centrality. The structural similarity between AI-generated networks and biomedical literature networks was evaluated to assess how closely the AI content mirrored the conceptual organization of real medical research.

## Key Results
- ChatGPT showed 38% document similarity (Cosine) versus Google Bard's 34% when compared to biomedical literature
- Bigram similarity favored ChatGPT at 47% versus Google Bard's 41% using TF-IDF analysis
- Network analysis revealed ChatGPT's content had higher degree and closeness centrality, indicating better structural alignment with biomedical literature

## Why This Works (Mechanism)
The study's text-mining approach works by quantifying lexical and structural patterns in AI-generated content against established biomedical literature. Document similarity metrics capture how closely AI responses align with existing medical knowledge through shared terminology and phrase patterns. Network analysis reveals how concepts are interconnected in AI-generated content compared to the natural conceptual relationships found in biomedical research, with higher centrality scores indicating better integration with established medical knowledge structures.

## Foundational Learning
- Cosine similarity: Measures angle between document vectors to assess semantic overlap - needed for quantifying how closely AI content aligns with medical literature; quick check: values closer to 1 indicate higher similarity
- Jaccard coefficient: Calculates intersection over union of term sets - provides binary presence/absence similarity measurement; quick check: useful for comparing document sets without considering term frequency
- TF-IDF: Weights terms by importance across documents - identifies distinctive bigrams that characterize medical literature; quick check: higher weights indicate more significant medical concepts
- Network centrality: Measures node importance in conceptual networks - reveals how well AI content mirrors the structural organization of biomedical knowledge; quick check: degree and closeness centrality scores indicate network integration

## Architecture Onboarding
- Component map: Web of Science literature corpus -> Text preprocessing -> Document similarity (Cosine/Jaccard) -> Bigram analysis (TF-IDF) -> Network construction -> Centrality analysis -> Comparison
- Critical path: Prompt engineering -> AI content generation -> Text preprocessing -> All similarity and network analyses -> Results synthesis
- Design tradeoffs: Static literature corpus vs. dynamic updating; simple lexical similarity vs. semantic understanding; binary term presence vs. weighted frequency
- Failure signatures: Low similarity scores indicating misalignment with medical literature; network fragmentation suggesting poor conceptual integration; inconsistent results across different similarity metrics
- First experiments: 1) Replicate analysis with updated AI models to check performance consistency; 2) Apply semantic similarity methods (word embeddings) to validate lexical findings; 3) Test across different medical domains to assess generalizability

## Open Questions the Paper Calls Out
None

## Limitations
- Literature corpus limited to single Web of Science search with specific keywords, potentially introducing selection bias
- Text-mining methods may not fully capture semantic meaning or clinical relevance of complex biomedical concepts
- Network analysis assumes structural similarity correlates with clinical utility without direct validation

## Confidence
- High confidence in reproducibility of text-mining and network analysis methods
- Medium confidence in comparative performance metrics between ChatGPT and Google Bard
- Low confidence in clinical relevance interpretation of structural similarity scores

## Next Checks
1. Replicate analysis across multiple cancer types and medical specialties to assess generalizability
2. Implement semantic analysis methods (e.g., word embeddings, BERT-based approaches) alongside current similarity metrics to capture deeper meaning
3. Conduct expert clinician review of generated content to validate whether higher structural similarity translates to clinically useful questions and insights