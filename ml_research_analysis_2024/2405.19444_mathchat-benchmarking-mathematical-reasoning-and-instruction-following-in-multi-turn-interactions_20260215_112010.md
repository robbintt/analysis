---
ver: rpa2
title: 'MathChat: Benchmarking Mathematical Reasoning and Instruction Following in
  Multi-Turn Interactions'
arxiv_id: '2405.19444'
source_url: https://arxiv.org/abs/2405.19444
tags:
- problem
- uni00000013
- error
- llms
- uni00000011
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MathChat, a benchmark for evaluating LLMs
  on multi-turn mathematical reasoning and instruction-following tasks. It includes
  four novel tasks: follow-up QA, error correction, error analysis, and problem generation.'
---

# MathChat: Benchmarking Mathematical Reasoning and Instruction Following in Multi-Turn Interactions

## Quick Facts
- arXiv ID: 2405.19444
- Source URL: https://arxiv.org/abs/2405.19444
- Reference count: 40
- Math-specialized LLMs underperform on multi-turn dialogue tasks despite excelling at single-turn QA

## Executive Summary
This paper introduces MathChat, a benchmark for evaluating large language models on multi-turn mathematical reasoning and instruction-following tasks. The benchmark reveals that while math-specialized models excel at single-turn question answering, they significantly underperform on complex scenarios requiring sustained reasoning and dialogue understanding. To address this gap, the authors develop MathChatsync, a synthetic dialogue-based math dataset for fine-tuning. Experimental results show that models fine-tuned with MathChatsync demonstrate marked improvements on open-ended tasks without sacrificing problem-solving accuracy, highlighting the importance of training LLMs with diverse, conversational instruction tuning datasets.

## Method Summary
The authors introduce MathChat, a benchmark with four novel tasks: follow-up QA, error correction, error analysis, and problem generation. They evaluate base models on these tasks, then generate MathChatsync, a synthetic dialogue-based dataset using GPT-4/GPT-3.5-turbo to create math conversations from GSM8K test data. Models (Mistral-7B and Gemma-7B) are fine-tuned using LoRA with MathChatsync data combined with traditional math instruction datasets. The fine-tuning process uses 8 NVIDIA V100 GPUs with FP16 precision, batch size 32, learning rate 3e-5, and 3 epochs. Models are re-evaluated on MathChat tasks to measure improvement in multi-turn reasoning and instruction-following capabilities.

## Key Results
- Math-specialized LLMs show 20-50% accuracy reductions on follow-up QA tasks in Rounds 2 and 3 compared to single-turn QA
- Models fine-tuned with MathChatsync dataset show marked improvements on open-ended tasks in MathChat without sacrificing problem-solving accuracy
- Incorporating general-purpose instruction tuning data from sources like Alpaca and UltraChat improves mathematical task performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** MathChat reveals that math-specialized LLMs overfit to single-turn QA formats and struggle with multi-turn dialogue reasoning.
- **Mechanism:** By evaluating models on follow-up QA, error correction, error analysis, and problem generation tasks that require sustained reasoning and instruction following, the benchmark exposes gaps in dialogue understanding and error analysis capabilities that are not captured by single-turn QA benchmarks.
- **Core assumption:** Math-specialized models trained on extensive single-turn QA data have not developed the ability to maintain coherent reasoning across multiple dialogue turns or follow diverse instructions.
- **Evidence anchors:**
  - [abstract] "while these models excel in single turn question answering, they significantly underperform in more complex scenarios that require sustained reasoning and dialogue understanding"
  - [section 3.3] "Follow-up QA. In Rounds 2 and 3 of the FOLLOW-UP QA tasks, models face significant challenges in multi-round math reasoning, with accuracy reductions ranging from 20% to 50%"

### Mechanism 2
- **Claim:** MathChatsync fine-tuning improves multi-turn mathematical reasoning without sacrificing single-turn problem-solving accuracy.
- **Mechanism:** The synthetic dialogue-based dataset provides training examples that teach models to understand instructions, engage in conversations, and maintain reasoning across multiple turns while preserving their problem-solving capabilities through continued exposure to math problems.
- **Core assumption:** Models can learn to balance dialogue understanding and mathematical reasoning when trained on a dataset that combines both elements.
- **Evidence anchors:**
  - [abstract] "Experimental results emphasize the need for training LLMs with diverse, conversational instruction tuning datasets like MathChatsync"
  - [section 4.3] "Notably, incorporating general-purpose instruction tuning data from sources such as Alpaca and UltraChat can improve performance on mathematical tasks"

### Mechanism 3
- **Claim:** The three-stage training paradigm (pre-training, supervised fine-tuning, reinforcement learning) can be enhanced by adding math-dialogue data to address multi-turn reasoning limitations.
- **Mechanism:** MathChatsync serves as an augmentation to the supervised fine-tuning stage, providing the model with experience in handling multi-turn mathematical conversations that are not present in traditional math QA datasets.
- **Core assumption:** The supervised fine-tuning stage is the most effective point to introduce multi-turn dialogue capabilities without disrupting the foundational mathematical knowledge acquired during pre-training.
- **Evidence anchors:**
  - [section 4] "We utilize Mistral-7B and Gemma-7B as our backbone models and conduct fine-tuning using Low-Rank Adaptation (LoRA)"
  - [section 4.3] "the model fine-tuned with our MathChatsync dataset demonstrates markedly superior overall performance"

## Foundational Learning

- **Concept:** Multi-turn dialogue understanding
  - Why needed here: The benchmark requires models to maintain context and reasoning across multiple conversation turns, which is essential for real-world mathematical assistance
  - Quick check question: Can you explain the difference between a single-turn QA task and a multi-turn dialogue task in mathematical reasoning?

- **Concept:** Error identification and correction
  - Why needed here: Tasks like error analysis require models to not only solve problems but also recognize and explain errors in given solutions
  - Quick check question: How would you approach identifying an error in a mathematical solution provided by a user?

- **Concept:** Instruction following and adaptation
  - Why needed here: Problem generation and follow-up QA tasks require models to understand and execute diverse instructions beyond simple problem-solving
  - Quick check question: What strategies would you use to ensure a model correctly follows complex, multi-step instructions?

## Architecture Onboarding

- **Component map:** MathChat benchmark (4 tasks) -> MathChatsync dataset generation -> LoRA fine-tuning -> Re-evaluation on MathChat
- **Critical path:** 1) Evaluate base models on MathChat tasks, 2) Generate MathChatsync dataset, 3) Fine-tune models with MathChatsync data, 4) Re-evaluate on MathChat tasks to measure improvement
- **Design tradeoffs:** Using synthetic data allows for scalability but may introduce biases; balancing dialogue understanding with mathematical accuracy requires careful dataset curation
- **Failure signatures:** Models that perform well on single-turn tasks but fail to maintain accuracy across dialogue turns; models that can identify errors but cannot correct them; models that generate relevant problems but provide incorrect solutions
- **First 3 experiments:**
  1. Evaluate base models on all four MathChat tasks to establish performance baselines
  2. Generate a small sample of MathChatsync data and fine-tune a subset of models to test the fine-tuning approach
  3. Compare performance of models fine-tuned with MathChatsync versus those fine-tuned with general instruction-tuning data on the open-ended tasks

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the performance of MathChatsync fine-tuned models scale with the amount and quality of synthetic dialogue data used during training?
  - **Basis in paper:** The authors mention budget constraints limited generation to 16,132 dialogues using GPT-4 and 131,346 using GPT-3.5-turbo, and state that scaling up quality and amount can bring more performance improvement as future work.

- **Open Question 2:** How does the performance of MathChatsync fine-tuned models compare to models trained with other specialized math instruction-following datasets?
  - **Basis in paper:** The authors developed MathChatsync to improve models' interaction and instruction-following capabilities but do not compare its effectiveness to other specialized math instruction-following datasets.

- **Open Question 3:** How does the performance of MathChatsync fine-tuned models on multi-turn math reasoning tasks transfer to other domains or types of reasoning tasks?
  - **Basis in paper:** The authors demonstrate improvements on math-specific tasks but do not investigate whether these improvements transfer to other domains or types of reasoning.

## Limitations

- The evaluation relies on GPT-4 for scoring instruction-following tasks, creating dependency on another black-box model's judgments
- The exact prompt templates and generation process for MathChatsync are not fully specified, making it difficult to assess potential biases
- The focus on 7B parameter models limits applicability to larger models with potentially different scaling behaviors

## Confidence

- **High confidence:** The observation that math-specialized models underperform on multi-turn dialogue tasks compared to single-turn QA tasks
- **Medium confidence:** The claim that MathChatsync fine-tuning improves multi-turn reasoning without sacrificing single-turn accuracy
- **Low confidence:** The generalizability of the three-stage training paradigm enhancement across different model architectures and long-term effectiveness

## Next Checks

1. **Human evaluation validation:** Recruit human annotators to independently evaluate a subset of model outputs on MathChat tasks, particularly the instruction-following scores, to verify alignment with GPT-4's judgments.

2. **Ablation study on training data composition:** Systematically vary the proportion of MathChatsync data versus traditional math QA data in the fine-tuning process to identify the optimal balance between dialogue understanding and mathematical accuracy.

3. **Zero-shot generalization test:** Evaluate the fine-tuned models on multi-turn mathematical reasoning tasks from entirely different datasets (e.g., MWP-Bench, MATH) that were not used in either the MathChat benchmark or MathChatsync generation.