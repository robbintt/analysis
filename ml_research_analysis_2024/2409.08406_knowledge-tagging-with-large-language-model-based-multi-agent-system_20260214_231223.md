---
ver: rpa2
title: Knowledge Tagging with Large Language Model based Multi-Agent System
arxiv_id: '2409.08406'
source_url: https://arxiv.org/abs/2409.08406
tags:
- knowledge
- question
- llms
- tagging
- multi-agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a multi-agent system using large language models
  to automate knowledge tagging for educational questions, addressing limitations
  of prior methods in handling complex semantic and numerical constraints. The system
  decomposes tasks into specialized LLM agents (planner, solver, semantic and numerical
  judgers) that collaborate to produce binary judgments on knowledge-question matches.
---

# Knowledge Tagging with Large Language Model based Multi-Agent System

## Quick Facts
- arXiv ID: 2409.08406
- Source URL: https://arxiv.org/abs/2409.08406
- Reference count: 6
- Primary result: Multi-agent LLM system achieves F1 scores up to 81.75 on knowledge tagging, outperforming single LLM and baseline methods

## Executive Summary
This paper introduces a multi-agent system using large language models to automate knowledge tagging for educational questions. The system addresses limitations of prior methods in handling complex semantic and numerical constraints by decomposing tasks into specialized LLM agents (planner, solver, semantic and numerical judgers) that collaborate to produce binary judgments on knowledge-question matches. Experiments on the MathKnowCT dataset demonstrate consistent precision improvements over single LLM inference and traditional embedding similarity and fine-tuning baselines.

The approach is deployed at scale in Squirrel AI Learning, significantly reducing manual labeling costs and enhancing content quality and diagnostic accuracy in adaptive learning systems. The modular design allows for targeted use of tools (e.g., Python code execution for numerical checks) and prevents interference between types of reasoning, though it comes at the cost of increased communication overhead and potential recall reduction due to strict AND aggregation of intermediate results.

## Method Summary
The method employs a multi-agent system where knowledge tagging is reformulated as a collaboration between specialized LLM agents. The system takes a knowledge definition and question stem as input, with a task planner decomposing the knowledge into sub-constraints. Semantic and numerical judgers evaluate these constraints independently, with the numerical judger using LLM-generated Python code for verification. The final judgment is formed by AND-ing all intermediate results, prioritizing precision over recall. The approach uses few-shot learning and tool augmentation to handle both semantic and numerical aspects of educational knowledge matching.

## Key Results
- Multi-agent system achieves F1 scores up to 81.75 on MathKnowCT dataset
- Consistent precision improvements over single LLM inference and embedding similarity baselines
- System deployed at scale in Squirrel AI Learning, reducing manual labeling costs significantly
- Performance gap between base-sized and large-sized LLMs narrows with multi-agent approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task decomposition into specialized agents improves precision by isolating complex constraints.
- Mechanism: The system splits knowledge definitions into semantic and numerical sub-constraints, assigning each to a dedicated LLM agent. This modularization prevents interference between types of reasoning and enables targeted use of tools.
- Core assumption: LLMs can reliably decompose a knowledge definition into valid, non-overlapping sub-constraints that each map cleanly to an agent's capability.
- Evidence anchors: [abstract] "By reformulating the judging process into a collaboration between multiple LLM-agents on independent sub-problems, we simplify the whole task and enhance the reliability of the judgment generation process."
- Break condition: If the task planner fails to generate valid or complete sub-constraints, or if sub-tasks overlap, the final judgment may be incorrect or incomplete.

### Mechanism 2
- Claim: Numerical constraints are verified via tool-augmented LLM agents, overcoming LLM limitations with arithmetic.
- Mechanism: The numerical judger uses the LLM's emergent coding capability to generate Python code that encodes the numerical constraint and executes it with extracted arguments, returning a boolean result.
- Core assumption: LLMs can accurately translate natural language numerical constraints into executable code and extract correct arguments from question text.
- Evidence anchors: [abstract] "We draw inspiration from the recently emerged Tool-use LLMs and leverage the LLMsâ€™ emergent coding capabilities to verify constraints through code execution."
- Break condition: If the LLM generates incorrect code or fails to extract correct numerical arguments, the boolean judgment will be wrong, potentially causing false negatives.

### Mechanism 3
- Claim: Multi-agent system achieves higher precision at the cost of recall by enforcing stricter verification.
- Mechanism: The final judgment is formed by AND-ing intermediate agent outputs. This reduces false positives (higher precision) but increases false negatives (lower recall) because any agent failure blocks a positive judgment.
- Core assumption: False positives are more harmful in educational contexts than false negatives, justifying the recall drop.
- Evidence anchors: [abstract] "We find that the introduction of planning and numerical agents leads to substantial improvements in precision."
- Break condition: If the strictness is too high, the system may become unusable due to excessive false negatives, outweighing precision gains.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: Enables LLMs to simulate human reasoning steps, improving performance on complex reasoning tasks like knowledge tagging.
  - Quick check question: Can the LLM solve a multi-step arithmetic word problem correctly when given a CoT prompt versus without?

- Concept: In-context learning (ICL)
  - Why needed here: Allows the system to adapt to new knowledge definitions without fine-tuning, using few-shot demonstrations to guide the LLM's behavior.
  - Quick check question: Does the LLM correctly classify a new (knowledge, question) pair after being shown 2 relevant examples in the prompt?

- Concept: Tool-use augmentation
  - Why needed here: Compensates for LLM weaknesses in numerical reasoning by delegating constraint verification to external code execution.
  - Quick check question: Can the numerical judger generate and execute Python code that correctly verifies a constraint like "sum of digits equals 10"?

## Architecture Onboarding

- Component map: Task Planner -> Semantic Judger, Numerical Judger -> Final Judgment Module
- Critical path:
  1. Input: (knowledge definition, question stem)
  2. Task Planner produces sub-constraint plan
  3. Semantic Judger and Numerical Judger (if applicable) process each sub-constraint
  4. Numerical Judger generates and executes code, returns boolean
  5. Final judgment is AND of all intermediate results
  6. Output: binary label

- Design tradeoffs:
  - Precision vs. recall: Multi-agent design boosts precision but drops recall due to strict AND aggregation.
  - Modularity vs. latency: Decomposing tasks improves reliability but adds communication overhead between agents.
  - Tool-use vs. simplicity: Numerical judger's code execution improves accuracy but adds dependency on external execution environment.

- Failure signatures:
  - False negatives: Likely due to task planner errors or numerical judger code bugs.
  - False positives: Likely due to semantic judger misinterpretation or missing constraints in decomposition.
  - System hang: Possible if numerical judger's code execution is blocked or slow.

- First 3 experiments:
  1. Replace task planner with a fixed rule-based decomposition and measure impact on precision/recall.
  2. Remove numerical judger and let semantic judger handle all constraints; compare F1 scores.
  3. Change final judgment aggregation from AND to OR; observe precision/recall shift.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the multi-agent system scale when applied to knowledge domains beyond primary school mathematics, such as advanced sciences or humanities?
- Basis in paper: [explicit] The paper focuses on primary school mathematics knowledge tagging and mentions potential deployment in broader educational contexts but does not validate performance on other domains.
- Why unresolved: The system was only tested on MathKnowCT dataset covering elementary math concepts, leaving its effectiveness on more complex or diverse subjects unexplored.
- What evidence would resolve it: Experiments applying the MAS to datasets from different educational domains (e.g., physics, literature) with comparative performance metrics would clarify domain generalizability.

### Open Question 2
- Question: What is the impact of agent model size on the cost-effectiveness tradeoff in real-world deployments, particularly for base-sized versus large-sized LLMs?
- Basis in paper: [explicit] The paper notes that base-sized LLMs show significant improvements with the multi-agent approach, while benefits for large-sized LLMs are less pronounced, but doesn't quantify cost-performance tradeoffs.
- Why unresolved: While the paper discusses cost savings from automation, it doesn't provide detailed analysis of computational costs versus performance gains across different model sizes.
- What evidence would resolve it: A comprehensive cost-benefit analysis comparing inference costs, deployment expenses, and performance metrics across model sizes in production environments.

### Open Question 3
- Question: How does the multi-agent system handle knowledge definitions that involve complex interdependencies between multiple constraints, rather than independent sub-tasks?
- Basis in paper: [inferred] The system decomposes knowledge definitions into independent sub-tasks, but the paper doesn't explore scenarios where constraints are mutually dependent or hierarchical.
- Why unresolved: The experimental examples focus on relatively simple constraint structures, leaving the system's capability to handle complex logical relationships unexamined.
- What evidence would resolve it: Testing the system on knowledge definitions with explicitly interdependent constraints and analyzing failure modes or performance degradation would reveal limitations in constraint handling.

## Limitations
- The system's reliability depends heavily on the task planner's ability to generate valid, non-overlapping sub-constraints from complex knowledge definitions.
- No validation is provided for the numerical constraint extraction and code generation steps in the numerical judger.
- Performance metrics are reported only for the full dataset without ablation studies showing the isolated impact of each agent type.

## Confidence
- **High confidence**: The multi-agent system improves precision over single LLM inference and embedding similarity baselines, as evidenced by F1 scores up to 81.75 and clear error reduction in false positives.
- **Medium confidence**: The claim that task decomposition and tool-use for numerical constraints drive the precision gains, since the paper does not provide direct ablation or validation of the decomposition or code generation steps.
- **Low confidence**: The generalizability of the approach to other educational domains or languages, as all experiments are conducted on a single math-focused dataset with Chinese questions.

## Next Checks
1. **Ablation study on task planner reliability**: Replace the LLM-based task planner with a fixed rule-based decomposition and measure the impact on precision and recall.
2. **Numerical judger robustness test**: Systematically evaluate the numerical judger on a set of questions with known correct and incorrect numerical constraints, checking for both false negatives and false positives.
3. **Generalization to new knowledge types**: Apply the multi-agent system to a new set of knowledge concepts not present in the training data, measuring whether precision and F1 scores remain consistent.