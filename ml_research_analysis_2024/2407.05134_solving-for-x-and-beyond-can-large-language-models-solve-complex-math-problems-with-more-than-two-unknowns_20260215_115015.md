---
ver: rpa2
title: 'Solving for X and Beyond: Can Large Language Models Solve Complex Math Problems
  with More-Than-Two Unknowns?'
arxiv_id: '2407.05134'
source_url: https://arxiv.org/abs/2407.05134
tags:
- problems
- solution
- problem
- equations
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BeyondX, the first benchmark for evaluating
  large language models (LLMs) on algebra problems with multiple unknowns (three to
  five variables). Existing math benchmarks mostly contain simple problems with one
  or two unknowns, masking LLMs' true capabilities.
---

# Solving for X and Beyond: Can Large Language Models Solve Complex Math Problems with More-Than-Two Unknowns?

## Quick Facts
- **arXiv ID**: 2407.05134
- **Source URL**: https://arxiv.org/abs/2407.05134
- **Reference count**: 40
- **Primary result**: Current LLMs show significant performance drops (up to 70%) on math problems with more than two unknowns, which the authors address with a novel prompting strategy achieving 86.7% accuracy.

## Executive Summary
This paper introduces BeyondX, the first benchmark for evaluating large language models on algebra problems with multiple unknowns (three to five variables). The authors demonstrate that existing math benchmarks primarily contain simple problems with one or two unknowns, masking LLMs' true capabilities. Through automated generation using a progressive expansion pipeline, they create a challenging dataset revealing significant performance drops in current models. To address this limitation, they propose Formulate-and-Solve, a prompting strategy that automatically generates demonstrations and leverages external symbolic solvers, achieving substantially better performance than traditional approaches.

## Method Summary
The authors develop BeyondX through a progressive expansion pipeline that incrementally adds unknowns to existing simpler problems, generating 480 problems across three, four, and five unknowns. They evaluate multiple LLMs and prompting methods on this benchmark, revealing significant performance degradation with increasing unknowns. To overcome this limitation, they propose Formulate-and-Solve, which combines automatic demonstration generation with symbolic solver integration. The method iteratively selects demonstrations, formulates problems as equation systems, solves them externally, and finalizes responses, achieving 86.7% average accuracy compared to 19.6-40.1% for other methods.

## Key Results
- Current LLMs experience performance drops up to 70% as the number of unknowns increases beyond two
- BeyondX reveals that existing benchmarks overstate LLMs' algebraic capabilities
- Formulate-and-Solve achieves 86.7% average accuracy versus 19.6-40.1% for baseline methods
- Math-specific fine-tuned models still struggle significantly with multi-unknown problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The progressive expansion pipeline enables automated generation of multi-unknown problems by incrementally building complexity from simpler problems.
- Mechanism: The pipeline starts with existing problems containing 1-2 unknowns, then iteratively introduces one new unknown at a time while maintaining solvability constraints. This divide-and-conquer approach breaks the problem generation into manageable steps: understanding the source problem, introducing a new unknown with an oracle value, expanding equation sets with a program verifier, adding equations to the problem statement, and final refinement.
- Core assumption: LLMs can reliably follow structured instructions to generate valid equations when problems are decomposed into smaller steps.
- Evidence anchors: [section] "This pipeline operates on three key ideas: (1). Scenario Expansion... (2). Progressive Extrapolation: We add unknowns incrementally â€” one at a time... (3). Decomposed Problem Generation: Instead of creating an entire problem at once..."
- Break condition: If LLMs cannot reliably maintain solvability constraints when introducing new variables, or if the step-by-step approach introduces cumulative errors that compound over multiple expansion steps.

### Mechanism 2
- Claim: Formulate-and-Solve overcomes LLMs' limitations with multi-unknown problems by combining automated demonstration generation with external symbolic solvers.
- Mechanism: The method first generates demonstrations through iterative prompting (approximately five demonstrations selected from ten sets), then uses these demonstrations to guide the LLM in translating problems into systems of equations. An external symbolic solver (e.g., SymPy) handles the actual equation solving, while the LLM focuses on problem formulation. The finalize module handles cases where the solver fails or the response format is incorrect.
- Core assumption: LLMs are better at understanding problem structure and generating equations than at solving complex equation systems, making the division of labor effective.
- Evidence anchors: [section] "This strategy refines current approaches by integrating general math-solving principles to automatically craft relevant multi-unknown in-context examples for LLMs."
- Break condition: If the generated demonstrations are consistently poor quality, or if the symbolic solver cannot handle the complexity of the equation systems generated by the LLM.

### Mechanism 3
- Claim: The performance drop in LLMs with increasing unknowns is primarily due to inadequate prompting strategies rather than inherent model limitations.
- Mechanism: By providing better structured prompts that include automatic demonstrations and leveraging external tools, the performance gap between single/double unknown problems and multi-unknown problems can be significantly reduced. The instruction component guides the LLM through the problem-solving process, while demonstrations provide concrete examples of successful approaches.
- Core assumption: Current prompting strategies fail because they don't account for the complexity of systems of equations and don't provide appropriate demonstrations for multi-unknown scenarios.
- Evidence anchors: [section] "Our findings suggest that while the inherent limitations of LLMs contribute to their underperformance on complex problems, inadequate prompting strategies are a substantial bottleneck."
- Break condition: If the performance improvement from better prompting strategies plateaus at a level still far below what would be expected for the model's inherent capabilities, suggesting that fundamental limitations remain.

## Foundational Learning

- Concept: System of equations formulation
  - Why needed here: Multi-unknown problems require expressing relationships between variables as mathematical equations. The ability to correctly identify unknowns, assign appropriate symbols, and translate problem statements into equations is fundamental to solving these problems.
  - Quick check question: Given a problem statement like "John has twice as many apples as Mary, and together they have 15 apples," can you correctly identify the unknowns and write the corresponding system of equations?

- Concept: External tool integration
  - Why needed here: LLMs struggle with complex mathematical computations, especially solving systems of equations. External symbolic solvers like SymPy can handle these computations reliably, allowing the LLM to focus on problem understanding and equation formulation.
  - Quick check question: If an LLM generates the system of equations 2x + 3y = 12 and x - y = 1, can you use an external solver to find the values of x and y?

- Concept: Prompt engineering and demonstration generation
  - Why needed here: Effective prompting requires providing the LLM with clear instructions and relevant examples. For multi-unknown problems, demonstrations must show how to handle the complexity of multiple variables and their relationships.
  - Quick check question: Given a simple two-unknown problem, can you generate a clear, step-by-step demonstration that shows how to solve it, including identifying unknowns, setting up equations, and finding the solution?

## Architecture Onboarding

- Component map: Problem generation pipeline -> Evaluation framework -> Formulate-and-Solve method
- Critical path: 1. Generate multi-unknown problems using progressive expansion pipeline 2. Evaluate existing LLMs and prompting methods on these problems 3. Implement Formulate-and-Solve method 4. Compare performance of Formulate-and-Solve against baselines 5. Analyze error cases and iterate on the approach
- Design tradeoffs: Manual vs. automated problem generation: Automated generation allows for larger datasets but may introduce quality issues that require human verification; Zero-shot vs. few-shot prompting: Few-shot methods with demonstrations generally perform better but require more effort to create demonstrations; LLM-only vs. tool-integrated approaches: Tool integration improves accuracy but adds complexity to the system
- Failure signatures: Performance drops significantly as the number of unknowns increases beyond two; Generated demonstrations are of poor quality, leading to incorrect equation formulation; Symbolic solver fails to solve the equation systems generated by the LLM; Response format errors prevent extraction of the system of equations
- First 3 experiments: 1. Evaluate the progressive expansion pipeline by generating a small set of multi-unknown problems and manually verifying their quality and solvability 2. Test the Formulate-and-Solve method on a single multi-unknown problem to ensure the instruction, demonstration generation, and solver integration components work together correctly 3. Compare the performance of Formulate-and-Solve against a baseline method (e.g., zero-shot CoT) on a small dataset of multi-unknown problems to validate the approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs on BeyondX generalize to real-world multi-unknown problems outside the generated dataset?
- Basis in paper: [inferred] The paper generates BeyondX from existing simpler problems and acknowledges limitations in scope and real-world applicability.
- Why unresolved: The benchmark is artificially generated and may not capture the full complexity and diversity of real-world problems with multiple unknowns.
- What evidence would resolve it: Testing current LLMs on a diverse collection of real-world multi-unknown problems from various domains (engineering, economics, physics, etc.) and comparing performance to BeyondX.

### Open Question 2
- Question: What is the optimal balance between instruction, demonstrations, and symbolic solvers in the Formulate-and-Solve approach?
- Basis in paper: [explicit] The ablation study shows all components contribute to performance, but doesn't explore optimal combinations or alternatives.
- Why unresolved: The current formulation uses a specific combination of instruction, automatic demonstrations, and symbolic solvers, but other configurations might yield better results.
- What evidence would resolve it: Systematic experimentation varying the number of demonstrations, types of instructions, alternative solving methods (e.g., neural solvers), and hybrid approaches to find optimal configurations.

### Open Question 3
- Question: Can LLMs be trained or fine-tuned specifically to improve performance on multi-unknown problems?
- Basis in paper: [explicit] The paper notes that mathematically fine-tuned models still struggle with multi-unknown problems, suggesting limitations in current fine-tuning approaches.
- Why unresolved: The paper focuses on prompting strategies rather than training methods, leaving open whether specialized training could address the performance gap.
- What evidence would resolve it: Training LLMs on large-scale datasets of multi-unknown problems using various techniques (curriculum learning, synthetic data generation, specialized architectures) and evaluating performance on BeyondX and real-world problems.

## Limitations

- The automated generation pipeline may introduce quality issues in problem creation, as LLMs may produce unsolvable or poorly constructed problems during expansion
- Performance improvements from Formulate-and-Solve may be partially attributed to demonstration quality rather than the method itself
- The benchmark is artificially generated and may not fully represent the complexity of real-world multi-unknown problems

## Confidence

- **High Confidence**: The core observation that LLMs struggle significantly with multi-unknown problems (3+ unknowns) is well-supported by empirical results showing performance drops up to 70% compared to single/double unknown problems
- **Medium Confidence**: The effectiveness of the Formulate-and-Solve method is demonstrated on the proposed benchmark, but results may be influenced by specific demonstration generation processes
- **Medium Confidence**: The conclusion that prompting strategies are a substantial bottleneck is supported by performance improvements, but relative contribution of prompting versus inherent model limitations requires further investigation

## Next Checks

1. Conduct a controlled study where the same problems are solved using different demonstration generation strategies to isolate the impact of demonstration quality versus the Formulate-and-Solve framework
2. Perform ablation studies on the progressive expansion pipeline by comparing problems generated with and without human validation to quantify the impact of the quality control process
3. Test the generalizability of findings by applying Formulate-and-Solve to problems with more than five unknowns or to different problem domains (e.g., physics word problems) to assess whether the method scales beyond the current scope