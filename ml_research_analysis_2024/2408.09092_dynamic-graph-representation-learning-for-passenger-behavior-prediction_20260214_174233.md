---
ver: rpa2
title: Dynamic Graph Representation Learning for Passenger Behavior Prediction
arxiv_id: '2408.09092'
source_url: https://arxiv.org/abs/2408.09092
tags:
- passenger
- station
- time
- prediction
- passengers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses passenger behavior prediction in subway systems
  by modeling passenger-station interactions as a continuous-time dynamic graph. The
  proposed DyGPP model captures both long-term periodic patterns and short-term abrupt
  changes in passenger travel behavior by extracting historical interaction sequences
  and encoding temporal patterns using MLP-based encoders.
---

# Dynamic Graph Representation Learning for Passenger Behavior Prediction

## Quick Facts
- arXiv ID: 2408.09092
- Source URL: https://arxiv.org/abs/2408.09092
- Authors: Mingxuan Xie; Tao Zou; Junchen Ye; Bowen Du; Runhe Huang
- Reference count: 37
- Key outcome: DyGPP achieves 0.9699 AP and 0.9591 AUC on BJSubway-40K dataset for passenger behavior prediction

## Executive Summary
This paper addresses passenger behavior prediction in subway systems by modeling passenger-station interactions as a continuous-time dynamic graph. The proposed DyGPP model captures both long-term periodic patterns and short-term abrupt changes in passenger travel behavior by extracting historical interaction sequences and encoding temporal patterns using MLP-based encoders. Experiments on real-world Beijing subway datasets demonstrate that DyGPP outperforms existing methods, achieving 0.9699 AP and 0.9591 AUC scores on the BJSubway-40K dataset. The model effectively handles continuously growing interaction records while maintaining prediction accuracy, offering a robust solution for smart city transportation planning and risk management.

## Method Summary
The DyGPP model treats passengers and stations as heterogeneous nodes in a continuous-time dynamic graph, where interactions are timestamped boarding and alighting events. The method extracts historical interaction sequences for each passenger and station independently, applies time encoding (cosine transformation of time intervals) and co-occurrence frequency encoding to capture both separate and cross-temporal patterns, then uses MLP-based encoders to learn these patterns. For prediction, the model concatenates learned passenger and station embeddings and performs link prediction. Time-based batch partitioning ensures temporal consistency during training, with the Adam optimizer used for model optimization.

## Key Results
- DyGPP achieves 0.9699 AP and 0.9591 AUC on BJSubway-40K dataset
- The model effectively handles continuously growing interaction records while maintaining prediction accuracy
- MLP-based temporal encoding outperforms Transformer, LSTM, and Mamba alternatives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model captures long-term periodicity and short-term abrupt changes in passenger behavior by separately encoding temporal patterns for passengers and stations and then correlating them.
- Mechanism: The DyGPP model extracts historical interaction sequences for each passenger and station independently, applies time encoding (cosine transformation of time intervals) and co-occurrence frequency encoding to capture both separate and cross-temporal patterns, then uses MLP-based encoders to learn these patterns.
- Core assumption: Passenger travel behavior exhibits both long-term periodicity (e.g., regular commutes) and short-term variability (e.g., weather or mood-driven changes), and these patterns can be effectively captured by modeling the historical interaction sequences separately for passengers and stations.
- Evidence anchors:
  - [abstract] "capture both long-term periodic patterns and short-term abrupt changes in passenger travel behavior"
  - [section 4.1.2] "We designed a module to consider both separate and cross-temporal patterns in these two sequences"
  - [corpus] Weak - no corpus papers directly discuss this specific temporal pattern separation mechanism
- Break condition: If passenger behavior does not exhibit clear periodicity or if short-term changes are too random to be captured by historical sequence patterns.

### Mechanism 2
- Claim: Dynamic graph representation allows the model to handle continuously growing interaction records while maintaining prediction accuracy.
- Mechanism: By treating passengers and stations as heterogeneous nodes in a continuous-time dynamic graph, the model can incorporate new interaction events as they occur without retraining on the entire history, using time-based batch partitioning to ensure temporal consistency within batches.
- Core assumption: Real-world passenger interaction data is continuously growing, and the model needs to adapt to new data without complete retraining while maintaining temporal coherence.
- Evidence anchors:
  - [abstract] "effectively handles continuously growing interaction records while maintaining prediction accuracy"
  - [section 4.4] "we adopted a method of partitioning batches based on time intervals" and "ensures the stability of the number of batches across different data scales"
  - [corpus] Weak - corpus papers focus on different aspects of dynamic graphs but don't specifically address continuous growth handling
- Break condition: If the rate of new interactions becomes too high relative to model update frequency, causing temporal inconsistencies.

### Mechanism 3
- Claim: The MLP-based encoder outperforms other sequence modeling methods (Transformer, LSTM, Mamba) for temporal pattern learning in this specific task.
- Mechanism: The paper demonstrates through ablation studies that MLP-based temporal encoding achieves better AP/AUC scores (0.9699/0.9591) compared to alternatives, while also being more computationally efficient.
- Core assumption: For passenger behavior prediction, the complexity of attention mechanisms (Transformer) or sequential processing (LSTM) is unnecessary, and a simpler MLP architecture can capture the required temporal patterns more effectively.
- Evidence anchors:
  - [section 5.7] "The AP and AUC results show that MLP performed better than the other encoders, demonstrating the effectiveness of using MLP for temporal learning"
  - [section 5.7 Table 7] Comparison showing DyGPP (MLP) outperforms Transformer (0.9692/0.9578), LSTM (0.9693/0.9580), and Mamba (0.9690/0.9581)
  - [corpus] Weak - corpus papers don't compare MLP to other sequence models for this specific task
- Break condition: If the temporal patterns in passenger behavior become more complex and require the expressive power of attention mechanisms or gated recurrent units.

## Foundational Learning

- Concept: Continuous-time dynamic graphs (CTDG)
  - Why needed here: The passenger behavior prediction task involves timestamped interactions that occur continuously over time, requiring a graph structure that can represent events at arbitrary time points rather than discrete snapshots
  - Quick check question: What is the key difference between continuous-time dynamic graphs and discrete-time dynamic graphs in terms of how they represent temporal information?

- Concept: Temporal pattern encoding with time intervals
  - Why needed here: Passenger behavior exhibits periodicity related to time of day, day of week, etc., which requires encoding the temporal distance between current prediction time and historical interaction times
  - Quick check question: How does the time encoder transform time intervals into high-dimensional vectors, and why is this transformation useful for capturing temporal patterns?

- Concept: Co-occurrence frequency encoding
  - Why needed here: The relationship between passengers and stations is bidirectional and complex - a station's characteristics affect passenger choices, and passenger patterns affect station flow, requiring a mechanism to capture these mutual influences
  - Quick check question: What does the co-occurrence frequency between a passenger and station sequence represent, and how does it help in predicting future interactions?

## Architecture Onboarding

- Component map: Historical Interaction Sequences -> Temporal Sequence Construction -> Dynamic Representation Learning -> Future Behavior Prediction
- Critical path: Temporal Sequence Construction → Dynamic Representation Learning → Future Behavior Prediction
- Design tradeoffs:
  - Fixed-length sequences vs. variable-length sequences: Fixed length simplifies batching but may lose information for nodes with many interactions
  - MLP vs. attention mechanisms: MLP is more efficient but may miss complex long-range dependencies
  - First-order neighbors only: Simplifies computation but may miss higher-order station relationships
- Failure signatures:
  - Poor performance on stations with few interactions: Indicates insufficient data for learning station-specific patterns
  - Degraded accuracy during peak hours: Suggests time-based batching may not capture rapid flow changes
  - High variance in predictions: May indicate overfitting to specific temporal patterns
- First 3 experiments:
  1. Ablation study removing the co-occurrence encoder to measure its impact on prediction accuracy
  2. Comparison of different sequence lengths (N neighbors) to find the optimal tradeoff between information and efficiency
  3. Time-based batch partitioning effectiveness test by comparing with random batching on peak vs. off-peak data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model performance change when incorporating spatial location information (e.g., geographical coordinates of stations and public transportation routes) into the station feature representation?
- Basis in paper: [inferred] The paper suggests that integrating spatial location information could improve station feature representation and prediction accuracy, but does not test this approach.
- Why unresolved: The paper focuses on temporal patterns and does not explore the impact of spatial features on model performance.
- What evidence would resolve it: Experimental results comparing model performance with and without spatial features integrated into station representations.

### Open Question 2
- Question: What is the impact of using second-order neighbors (station-station connections) instead of only first-order neighbors (passenger-station connections) for station feature extraction?
- Basis in paper: [inferred] The paper mentions that using second-order neighbors could better capture logical similarities between stations, but does not implement or test this approach.
- Why unresolved: The paper uses only first-order neighbors for feature extraction, leaving the potential benefits of second-order neighbors unexplored.
- What evidence would resolve it: Comparative experiments showing prediction accuracy differences between models using first-order versus second-order neighbor information.

### Open Question 3
- Question: How does the model's performance scale with increasing dataset sizes beyond the tested 1M interaction dataset?
- Basis in paper: [inferred] The paper tests on datasets up to 1M interactions but does not examine performance on larger-scale datasets or discuss scalability limitations.
- Why unresolved: The experiments are limited to relatively small-scale subway datasets, and the paper does not address how the model would perform with substantially larger datasets.
- What evidence would resolve it: Experimental results showing model performance and computational efficiency across multiple dataset sizes, including much larger than 1M interactions.

## Limitations
- The model's generalizability to other transportation systems or cities remains untested
- The superiority of MLP over other sequence models is demonstrated but not thoroughly explained mechanistically
- The co-occurrence encoding mechanism lacks detailed specification in the implementation

## Confidence
- High confidence in the overall model architecture and performance claims on tested datasets
- Medium confidence in the specific temporal pattern separation mechanism
- Low confidence in the generalizability and scalability claims beyond the tested Beijing subway data

## Next Checks
1. Conduct cross-city validation by testing DyGPP on subway datasets from different cities with varying passenger volumes and station networks to assess generalizability
2. Perform ablation studies specifically isolating the contribution of co-occurrence encoding to verify its necessity and effectiveness
3. Test the model's performance under different temporal resolutions and batch sizes to identify optimal parameters for real-world deployment