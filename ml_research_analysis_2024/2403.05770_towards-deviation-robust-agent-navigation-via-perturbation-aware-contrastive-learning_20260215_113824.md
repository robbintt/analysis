---
ver: rpa2
title: Towards Deviation-Robust Agent Navigation via Perturbation-Aware Contrastive
  Learning
arxiv_id: '2403.05770'
source_url: https://arxiv.org/abs/2403.05770
tags:
- trajectory
- navigation
- learning
- agent
- proper
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to improve the robustness of vision-and-language
  navigation agents by training them to handle unexpected route deviations. The core
  idea is to introduce path perturbations during training using a simple edge deletion
  strategy, requiring the agent to navigate successfully following the original instruction
  despite the deviation.
---

# Towards Deviation-Robust Agent Navigation via Perturbation-Aware Contrastive Learning

## Quick Facts
- arXiv ID: 2403.05770
- Source URL: https://arxiv.org/abs/2403.05770
- Reference count: 40
- Primary result: A method to improve VLN robustness by training agents to handle unexpected route deviations using progressive perturbation-aware contrastive learning.

## Executive Summary
This paper addresses the challenge of making vision-and-language navigation (VLN) agents robust to unexpected route deviations caused by real-world disturbances. The proposed method, PROPER, introduces path perturbations during training using an edge deletion strategy and employs progressive perturbation-aware contrastive learning to help agents distinguish between perturbation-free and perturbation-based scenarios. Experimental results show that PROPER significantly improves navigation performance in both standard and perturbation-based environments, outperforming state-of-the-art baselines.

## Method Summary
The method introduces path perturbations during training by deleting edges in the navigation connectivity graph, forcing agents to navigate successfully following the original instruction despite deviations. A progressively perturbed trajectory augmentation strategy is employed, where perturbed trajectories are introduced gradually based on the agent's performance. Additionally, a perturbation-aware contrastive learning mechanism is developed to help the agent distinguish between perturbation-free and perturbation-based scenarios by contrasting trajectory encodings.

## Key Results
- PROPER improves navigation success rate by 3.8% and SPL by 5.1% on the Path-Perturbed R2R dataset compared to state-of-the-art baselines.
- The progressive perturbation-aware contrastive learning mechanism contributes to better adaptation to route deviations.
- The method demonstrates improved robustness in both perturbation-free and perturbation-based environments.

## Why This Works (Mechanism)

### Mechanism 1
The agent is first trained on perturbation-free trajectories, then progressively introduced to perturbed trajectories based on its performance. This ensures the agent learns to navigate under perturbation at the right pace, avoiding both under-training and over-training on difficult perturbed data. The core assumption is that the agent's GT match rate is a reliable indicator of its readiness to handle more complex perturbed scenarios.

### Mechanism 2
Trajectory encodings at the end of navigation are used as representations. In perturbation-free scenarios, the positive sample is the GT trajectory encoding, and the negative samples are perturbation-aware GT trajectories. In perturbation-based scenarios, the positive sample is the perturbation-aware GT trajectory encoding, and the negative samples are perturbation-aware GT trajectories with different perturbation positions. This forces the agent to learn the characteristics of each environment.

### Mechanism 3
The edge deletion perturbation strategy creates realistic route deviations that the agent must overcome to navigate successfully. The perturbation-aware GT trajectory is constructed to be as short as possible while still following the original instruction. This ensures that the agent learns to navigate back to the instruction-correlated trajectory.

## Foundational Learning

- **Concept**: Reinforcement learning (RL) for sequential decision-making.
  - **Why needed here**: The VLN agent needs to learn a policy that maps observations to actions in order to reach the goal.
  - **Quick check question**: What is the difference between on-policy and off-policy RL algorithms, and which one is more suitable for training a VLN agent?

- **Concept**: Contrastive learning for representation learning.
  - **Why needed here**: The agent needs to learn representations of trajectories that capture the difference between perturbation-free and perturbation-based environments.
  - **Quick check question**: What is the InfoNCE loss, and how does it encourage the model to learn discriminative representations?

- **Concept**: Curriculum learning for efficient training.
  - **Why needed here**: The agent needs to learn to navigate under perturbation, which is a more difficult task than navigating without perturbation.
  - **Quick check question**: What is the difference between teacher-forcing and student-forcing in curriculum learning, and when would you use each one?

## Architecture Onboarding

- **Component map**: Instruction encoder -> Visual encoder -> Action decoder -> Trajectory encoder -> Contrastive loss module
- **Critical path**: Instruction encoder -> Visual encoder -> Action decoder -> Trajectory encoder -> Contrastive loss module
- **Design tradeoffs**: Using a simpler trajectory encoder vs. a more complex one; using a hard negative sampling strategy vs. a soft one; using a progressive training strategy vs. training on all data from the start.
- **Failure signatures**: If the agent's success rate does not improve over time, it may indicate that the progressive training strategy is not working correctly. If the agent's success rate on perturbation-free environments decreases when training with perturbation, it may indicate that the perturbation is too difficult or that the contrastive loss is not working correctly.
- **First 3 experiments**:
  1. Train the agent on perturbation-free data only and evaluate its success rate on both perturbation-free and perturbation-based environments.
  2. Train the agent on perturbation-free data and then fine-tune it on perturbation-based data, and evaluate its success rate on both environments.
  3. Train the agent using the full PROPER method and evaluate its success rate on both environments.

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of perturbation distance threshold (r) impact the trade-off between navigation robustness and instruction relevance in PROPER? The paper mentions setting r to be the average distance between neighbor nodes but does not provide empirical results showing the impact of varying r on navigation performance or instruction relevance.

### Open Question 2
How does the progressive perturbation-aware contrastive learning mechanism in PROPER compare to other contrastive learning approaches in VLN, such as those based on image or language augmentations? The paper introduces a perturbation-aware contrastive learning mechanism and compares it to a non-progressive variant in ablation studies, but it does not compare it to other contrastive learning approaches in VLN.

### Open Question 3
How does the performance of PROPER on the PP-R2R dataset generalize to other real-world VLN scenarios with different types of disturbances, such as sensor errors or visual ambiguity? The PP-R2R dataset only includes path perturbations, and it is unclear how well the learned robustness generalizes to other types of disturbances that may occur in real-world VLN scenarios.

## Limitations
- The effectiveness of progressive augmentation is inferred from ablation studies rather than direct comparisons, creating moderate uncertainty about its contribution relative to non-progressive approaches.
- The contrastive learning mechanism's specific implementation details (temperature parameter τ, sampling strategy) are not fully specified, limiting reproducibility.
- The edge deletion perturbation strategy's realism and frequency in real-world scenarios is not empirically validated beyond the constructed PP-R2R dataset.

## Confidence
- **High**: The overall framework combining perturbation training with contrastive learning improves robustness (supported by quantitative results across multiple baselines).
- **Medium**: The progressive augmentation strategy improves learning efficiency (supported by ablation but lacking direct comparative evidence).
- **Medium**: The contrastive learning mechanism effectively distinguishes perturbation-free vs. perturbation-based scenarios (mechanism described but not independently validated).

## Next Checks
1. Implement and test a non-progressive baseline (same contrastive learning but without progressive augmentation) to isolate the contribution of progressive training.
2. Conduct an ablation study varying the temperature parameter τ in the contrastive loss to optimize performance and assess sensitivity.
3. Validate the perturbation scheme by measuring how often real-world navigation scenarios would encounter similar edge deletions in the connectivity graph.