---
ver: rpa2
title: A Brief Introduction to Causal Inference in Machine Learning
arxiv_id: '2405.08793'
source_url: https://arxiv.org/abs/2405.08793
tags:
- causal
- learning
- have
- distribution
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces causal inference in machine learning, targeting
  students with a basic ML background but no prior exposure to causal reasoning. The
  paper explains how causal inference helps with out-of-distribution generalization,
  a key issue in ML where models fail to perform well on new data distributions.
---

# A Brief Introduction to Causal Inference in Machine Learning

## Quick Facts
- arXiv ID: 2405.08793
- Source URL: https://arxiv.org/abs/2405.08793
- Reference count: 0
- This paper introduces causal inference concepts for machine learning students, focusing on how causal reasoning enables out-of-distribution generalization.

## Executive Summary
This paper introduces causal inference to machine learning students with basic ML background but no prior exposure to causal reasoning. It explains how causal inference addresses out-of-distribution generalization - a key ML challenge where models fail on new data distributions. The paper covers probabilistic graphical models, structural causal models, causal quantities like ATE and CATE, and various estimation techniques from RCTs to instrumental variables. The central argument is that causal reasoning is essential for building models that generalize well to new distributions and provides frameworks for incorporating causal thinking into ML approaches.

## Method Summary
The paper synthesizes existing causal inference concepts for a machine learning audience, covering both theoretical foundations and practical estimation methods. It explains how to represent data generating processes using probabilistic graphical models and structural causal models, then demonstrates how to identify and estimate causal effects using various techniques depending on which confounders are observed. The paper bridges causal inference and machine learning by discussing connections like out-of-distribution generalization, the principle of invariance, and preference-based learning for language models.

## Key Results
- Causal reasoning through graphical models enables identification of stable correlations that generalize across distributions
- Randomized controlled trials provide unbiased causal estimates by breaking dependence between treatment and confounders
- Instrumental variables can recover causal effects from observational data when valid instruments exist
- Causal inference frameworks provide principled approaches to out-of-distribution generalization in machine learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The paper bridges machine learning and causal inference by showing how probabilistic graphical models encode data generating processes, enabling out-of-distribution generalization.
- Mechanism: Causal graphical models distinguish spurious from stable correlations; stable (invariant) correlations generalize better to new distributions.
- Core assumption: Causal relationships are invariant across environments, while spurious correlations are not.
- Evidence anchors:
  - [abstract] "causal reasoning is essential for building models that generalize well to new distributions"
  - [section] "invariance is a strong assumption. Can we relax this assumption to identify a more flexible notion of causal prediction?"
  - [corpus] Weak: neighbor papers focus on ML/DL introductions, not causal inference.
- Break condition: When the underlying causal structure itself changes across environments, invariance no longer guarantees generalization.

### Mechanism 2
- Claim: Randomized controlled trials (RCTs) enable causal inference even with unobserved confounders by enforcing independence between treatment and covariates.
- Mechanism: By randomly assigning treatment, the action becomes independent of confounders, so observed outcome differences reflect causal effects.
- Core assumption: The causal effect is stationary during and after the trial, and the population covariate distribution is stable.
- Evidence anchors:
  - [section] "we severed the edge from the confounder x to the action a" and "RCT avoids the issue of unobserved, or even unknown, confounders."
  - [corpus] Weak: neighbor papers focus on ML/DL introductions, not causal inference.
- Break condition: If the treatment assignment becomes dependent on the covariate (e.g., subconscious bias) or if the underlying causal effect drifts over time.

### Mechanism 3
- Claim: Instrumental variables (IV) recover causal effects from observational data by using a variable that affects treatment but is independent of confounders.
- Mechanism: IV acts as a proxy for the treatment component uncorrelated with confounders; regression on IV-predicted treatment isolates the causal pathway.
- Core assumption: The instrument is strongly predictive of treatment, has low variance, and is independent of confounders.
- Evidence anchors:
  - [section] "the instrument must be selected to be highly predictive of the action (the third term) but also exhibit a low variance in its prediction"
  - [corpus] Weak: neighbor papers focus on ML/DL introductions, not causal inference.
- Break condition: When the instrument correlates with confounders or is weakly predictive of treatment, bias dominates.

## Foundational Learning

- Concept: Probabilistic graphical models and structural causal models
  - Why needed here: They formalize the data generating process, enabling reasoning about interventions and counterfactuals.
  - Quick check question: In a DAG, if you observe a confounder between treatment and outcome, does the causal path remain open or closed?
- Concept: Average treatment effect (ATE) and conditional average treatment effect (CATE)
  - Why needed here: These quantify causal effects overall and conditional on observed covariates.
  - Quick check question: How does CATE differ from ATE mathematically?
- Concept: Inverse probability weighting (IPW) and instrumental variables
  - Why needed here: They adjust for confounding when some or all confounders are unobserved.
  - Quick check question: What does IPW weight each observation by?

## Architecture Onboarding

- Component map:
  - Data generating process (PGM/SCM) → Causal estimand (ATE/CATE) → Estimation method (regression, RCT, IV, IPW) → Generalization analysis
- Critical path:
  1. Specify DAG/SCM from domain knowledge
  2. Identify target causal effect (ATE/CATE)
  3. Choose appropriate estimator given confounder observability
  4. Validate assumptions (positivity, stationarity, instrument relevance)
  5. Evaluate generalization via invariance or distributional robustness
- Design tradeoffs:
  - RCT: unbiased but expensive/ethical constraints
  - IV: handles unobserved confounding but biased if instrument weak
  - IPW: balances bias-variance but sensitive to propensity score estimation
  - Invariant prediction: generalizes but requires multiple environments
- Failure signatures:
  - High variance in causal estimates → small effective sample size or weak instruments
  - Systematic bias → unmeasured confounding, violated stationarity, or collider bias
  - Poor out-of-distribution performance → reliance on spurious correlations
- First 3 experiments:
  1. Simulate a simple DAG with known ATE; estimate via regression, IV, and IPW; compare bias and variance.
  2. Vary the strength of an instrument; observe impact on IV estimator performance.
  3. Generate data from multiple environments; test invariant feature learning for out-of-distribution accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we identify unobserved confounders in causal inference when only observational data is available?
- Basis in paper: [explicit] The paper discusses challenges when confounders were not collected (Chapter 4), mentioning instrumental variables as a technique but acknowledging the bias issues with this approach.
- Why unresolved: Unobserved confounders create spurious correlations that are difficult to detect without additional information or assumptions. The paper notes that instrumental variables can help but introduces bias.
- What evidence would resolve it: A method to reliably detect and quantify the impact of unobserved confounders from observational data alone, or a framework for determining when such detection is impossible.

### Open Question 2
- Question: How can we scale causal discovery methods to handle high-dimensional data with millions of variables, as mentioned in the conclusion?
- Basis in paper: [explicit] The conclusion mentions "Scalable causal discovery" as an interesting open area, noting the challenge of inferring causal relationships among many variables.
- Why unresolved: Traditional causal discovery methods don't scale well to high-dimensional settings. The curse of dimensionality makes it difficult to identify true causal relationships among millions of variables.
- What evidence would resolve it: An algorithm that can reliably discover causal structures in high-dimensional data, or theoretical bounds on when such discovery is possible.

### Open Question 3
- Question: How can we relax the invariance assumption in out-of-distribution generalization to allow for more flexible causal prediction?
- Basis in paper: [explicit] Section 5.3 mentions "Beyond invariance" as an area for future work, noting that invariant predictors may be suboptimal for prediction under any environment.
- Why unresolved: The principle of invariance assumes that stable correlations are causal, but this may be too restrictive in many real-world scenarios where relationships change across environments.
- What evidence would resolve it: A framework for identifying causal relationships that allows for controlled changes across environments, or empirical results showing improved performance over invariant approaches.

## Limitations
- The paper presents theoretical frameworks without extensive empirical validation across diverse real-world datasets
- Some advanced concepts like the principle of invariance are discussed conceptually but lack concrete implementation guidelines
- The effectiveness of proposed methods heavily depends on correct specification of causal graphs, which is often challenging in practice

## Confidence
- **High confidence**: Basic causal inference concepts (PGMs, ATE/CATE, confounding) and their relevance to out-of-distribution generalization
- **Medium confidence**: The practical applicability of advanced techniques (IV, double ML) given the assumptions required and potential for assumption violations
- **Medium confidence**: The connection between causal inference and preference-based learning for language models, as this is an emerging area with limited empirical evidence

## Next Checks
1. Implement a simple simulation comparing invariant feature learning versus standard supervised learning across multiple environments to empirically validate out-of-distribution generalization claims
2. Create a synthetic dataset with known causal structure where some confounders are unobserved, then compare IV estimation performance against different instrument strengths
3. Test the sensitivity of IPW estimators to misspecification of propensity scores by varying the complexity of the propensity model on a benchmark dataset