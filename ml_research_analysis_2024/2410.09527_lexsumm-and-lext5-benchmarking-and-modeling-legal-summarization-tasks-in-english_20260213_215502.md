---
ver: rpa2
title: 'LexSumm and LexT5: Benchmarking and Modeling Legal Summarization Tasks in
  English'
arxiv_id: '2410.09527'
source_url: https://arxiv.org/abs/2410.09527
tags:
- legal
- summary
- language
- pages
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LexSumm, a benchmark for legal summarization
  in English across eight datasets from the US, UK, EU, and India. It also presents
  LexT5, a legal-oriented sequence-to-sequence model pre-trained on the LeXFiles corpus.
---

# LexSumm and LexT5: Benchmarking and Modeling Legal Summarization Tasks in English

## Quick Facts
- arXiv ID: 2410.09527
- Source URL: https://arxiv.org/abs/2410.09527
- Reference count: 40
- Primary result: LexT5 pre-trained on LeXFiles outperforms T5 and zero-shot LLMs on legal summarization benchmarks when used with SLED or Unlimiformer frameworks

## Executive Summary
This paper introduces LexSumm, a comprehensive benchmark for legal summarization in English spanning eight datasets from the US, UK, EU, and India. It also presents LexT5, a legal-oriented sequence-to-sequence model pre-trained on the LeXFiles corpus. The benchmark includes datasets with varying input lengths, compression ratios, and levels of abstraction. Evaluation shows that LexT5, when integrated into long-range frameworks like SLED and Unlimiformer, outperforms other models including zero-shot LLMs. The study reveals that legal summarization remains challenging, with issues in abstraction and faithfulness even in model outputs. LexSumm and LexT5 are publicly released to support future research.

## Method Summary
The authors developed LexSumm by curating eight English legal summarization datasets from diverse jurisdictions. They pre-trained LexT5 by continuing T5-base pre-training on the LeXFiles corpus using span denoising. For handling long legal documents, they evaluated LexT5 within SLED and Unlimiformer frameworks. Models were fine-tuned on individual LexSumm datasets and evaluated using ROUGE-1/2/L and BERTScore metrics. The study compared LexT5 against T5, LED, PRIMERA, LongT5, and zero-shot LLMs.

## Key Results
- LexT5 outperforms T5 and zero-shot LLMs on LegalLAMA probing tasks
- SLED-LexT5 and Unlimiformer-LexT5 achieve the best performance across all LexSumm datasets
- Legal summarization remains challenging, particularly for high-abstraction tasks like MultiLexSumm
- Models struggle with faithfulness and abstraction even when achieving reasonable ROUGE scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training LexT5 on LeXFiles improves legal domain knowledge over T5.
- Mechanism: Continuing pre-training on domain-specific corpora with span denoising allows the model to learn legal terminology and document structure, improving performance on legal tasks.
- Core assumption: The LeXFiles corpus contains sufficient legal domain diversity to transfer effectively to downstream legal summarization tasks.
- Evidence anchors:
  - [abstract]: "We assess its capabilities through zero-shot probing on LegalLAMA and fine-tuning on LexSumm."
  - [section]: "To evaluate the legal knowledge acquired by LexT5, we compare its to T5 on LegalLAMA (Chalkidis et al., 2023), a zero-shot legal probing suite."
- Break condition: If LeXFiles lacks representation of target jurisdiction(s) or document types, the benefit over T5 diminishes.

### Mechanism 2
- Claim: Wrapping LexT5 in long-range frameworks (SLED, Unlimiformer) enables competitive performance on long legal inputs without expensive long-sequence pre-training.
- Mechanism: Chunking + fusion (SLED) or retrieval + limited attention (Unlimiformer) allow short-range models to approximate full attention over long inputs efficiently.
- Core assumption: Local context plus cross-chunk fusion or top-k retrieval captures most relevant information for summarization.
- Evidence anchors:
  - [abstract]: "Our LexT5 models, pre-trained on legal corpora using random span masking strategies without specific long-range or summarization pre-training, when plugged into SLED and Unlimiformer consistently outperform all others across all datasets."
- Break condition: If critical information is consistently missed due to chunking or retrieval limitations, model performance degrades sharply.

### Mechanism 3
- Claim: LexSumm benchmark's diversity in input length, compression ratio, and abstraction level makes it a challenging evaluation platform.
- Mechanism: Varying dataset characteristics stress different model capabilities (e.g., handling long inputs, abstractive vs. extractive summarization), exposing weaknesses in current approaches.
- Core assumption: Datasets with different granularities require distinct summarization strategies.
- Evidence anchors:
  - [abstract]: "It comprises eight English legal summarization datasets, from diverse jurisdictions, such as the US, UK, EU and India."
- Break condition: If all datasets reduce to a single dominant summarization pattern, the benchmark loses discriminative power.

## Foundational Learning

- Concept: Sequence-to-sequence (seq2seq) pre-training with span denoising
  - Why needed here: Enables generation of legal summaries conditioned on long input documents
  - Quick check question: How does span denoising differ from masked language modeling, and why is it suited for summarization?

- Concept: Legal domain-specific corpora
  - Why needed here: Provides legal terminology, structure, and context absent in general-domain corpora
  - Quick check question: What are the key document types in LeXFiles, and how do they map to downstream summarization tasks?

- Concept: Long-context processing via chunking and retrieval
  - Why needed here: Legal documents often exceed model input limits; chunking/fusion or retrieval-based methods allow efficient approximation
  - Quick check question: What are the trade-offs between SLED's fusion-in-decoder and Unlimiformer's retrieval-based attention?

## Architecture Onboarding

- Component map: LexT5 (T5-base → LeXFiles pre-training) → SLED/Unlimiformer (chunk → encode/fuse or retrieve → decode) → LexSumm datasets (BillSum, EurLexSum, GovReport, MultiLexSumm variants, INAbs, UKAbs)
- Critical path: LexT5 pre-training → fine-tune on LexSumm via SLED/Unlimiformer → evaluate with ROUGE/BERTScore
- Design tradeoffs:
  - Long-range pre-training (LED/PRIMERA) vs. short-range + adaptation (SLED/Unlimiformer)
  - Full attention vs. retrieval-based approximation
  - Legal pre-training (LexT5) vs. general domain (T5)
- Failure signatures:
  - LexT5: Poor performance on legal terminology tasks → insufficient legal pre-training
  - SLED: Inconsistent chunk fusion → poor cross-chunk context handling
  - Unlimiformer: Retrieval misses critical content → ineffective k-NN indexing
  - All: Low ROUGE scores on MultiLexSumm → failure to handle high abstraction
- First 3 experiments:
  1. Compare T5 vs. LexT5 on LegalLAMA to confirm legal knowledge gain
  2. Evaluate SLED-LexT5 vs. Unlimiformer-LexT5 on a representative dataset to assess chunking vs. retrieval trade-off
  3. Test zero-shot Claude/GPT-4 on a small LexSumm subset to benchmark LLM performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the legal summarization datasets differ in their suitability for different summarization approaches (e.g., extractive vs. abstractive)?
- Basis in paper: The paper analyzes dataset characteristics including compression ratio, coverage, and density to assess the extractive vs. abstractive nature of each dataset.
- Why unresolved: While the paper identifies trends in these characteristics, it doesn't directly link them to the performance of different summarization approaches.
- What evidence would resolve it: Comparative analysis of summarization model performance (extractive vs. abstractive) on each dataset, correlating results with dataset characteristics.

### Open Question 2
- Question: What are the key factors influencing the performance of legal-specific pre-trained models like LexT5 compared to general-domain models?
- Basis in paper: The paper introduces LexT5 and evaluates its performance on legal tasks, showing improvements over general-domain models like T5.
- Why unresolved: The paper doesn't provide a detailed analysis of the specific aspects of legal text that benefit from domain-specific pre-training.
- What evidence would resolve it: Ablation studies on LexT5 to isolate the impact of legal-specific pre-training on different aspects of performance, such as handling legal terminology or complex sentence structures.

### Open Question 3
- Question: How do long-context models and retrieval-based approaches compare in their ability to handle the unique challenges of legal summarization?
- Basis in paper: The paper benchmarks various long-context models (LED, PRIMERA, LongT5) and retrieval-based approaches (SLED, Unlimiformer) on legal summarization tasks.
- Why unresolved: While the paper compares these approaches, it doesn't provide a detailed analysis of their strengths and weaknesses in addressing specific challenges of legal text.
- What evidence would resolve it: In-depth analysis of model outputs to identify specific strengths and weaknesses of each approach in handling different aspects of legal text.

## Limitations
- LeXFiles corpus lacks detailed characterization (size, diversity, domain coverage)
- Missing fine-tuned LLM baselines (GPT-4, Claude-2.1) for comprehensive comparison
- Chunking and retrieval mechanisms may miss cross-chunk dependencies in high-abstraction tasks
- ROUGE and BERTScore may not fully capture legal reasoning quality or faithfulness

## Confidence
- **High confidence**: LexSumm is a comprehensive and challenging benchmark for legal summarization
- **Medium confidence**: LexT5 pre-trained on LeXFiles improves legal domain performance over T5
- **Medium confidence**: SLED and Unlimiformer frameworks enable competitive long-input summarization
- **Low confidence**: LexT5 outperforms zero-shot LLMs on legal summarization without fine-tuned LLM baselines

## Next Checks
1. Obtain and characterize the LeXFiles corpus (size, document types, jurisdiction coverage) to validate the legal domain transfer
2. Fine-tune GPT-4 or Claude-2.1 on LexSumm datasets and compare against LexT5 + SLED/Unlimiformer
3. Conduct qualitative analysis of LexT5 outputs on high-abstraction datasets to identify systematic failures in abstraction or faithfulness