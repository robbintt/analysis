---
ver: rpa2
title: 'MindSearch: Mimicking Human Minds Elicits Deep AI Searcher'
arxiv_id: '2407.20183'
source_url: https://arxiv.org/abs/2407.20183
tags:
- arxiv
- search
- information
- mindsearch
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MindSearch, a multi-agent framework that
  mimics human cognitive processes for complex web information seeking and integration.
  The core innovation is modeling problem-solving as dynamic graph construction, where
  a WebPlanner decomposes queries into sub-questions and coordinates multiple WebSearchers
  to retrieve information in parallel.
---

# MindSearch: Mimicking Human Minds Elicits Deep AI Searcher

## Quick Facts
- **arXiv ID:** 2407.20183
- **Source URL:** https://arxiv.org/abs/2407.20183
- **Reference count:** 9
- **Primary result:** Multi-agent framework achieving 76.8% accuracy on Bamboogle, 60.0% on Musique, and 59.8% on HotpotQA tasks while processing 300+ web pages in 3 minutes

## Executive Summary
MindSearch introduces a novel multi-agent framework that mimics human cognitive processes for complex web information seeking. The system employs a WebPlanner to decompose queries into sub-questions and coordinate multiple WebSearchers that perform hierarchical retrieval in parallel. This approach enables processing over 300 web pages in under 3 minutes while improving both depth and breadth of responses. In human evaluations, MindSearch responses were preferred over ChatGPT-Web and Perplexity.ai, demonstrating competitive performance with open-source models.

## Method Summary
MindSearch uses a two-component architecture: WebPlanner decomposes user queries into sub-questions using dynamic graph construction via code generation, while multiple WebSearchers execute hierarchical information retrieval in parallel. The WebPlanner builds a directed acyclic graph representing the reasoning structure, distributing search tasks to specialized agents. WebSearchers employ a coarse-to-fine strategy, first generating multiple related queries to broaden search coverage, then filtering and selecting the most valuable pages for detailed reading. The system integrates results from over 300 pages in under 3 minutes using either GPT-4o or InternLM2.5-7B-Chat models.

## Key Results
- Achieved 76.8% accuracy on Bamboogle, 60.0% on Musique, and 59.8% on HotpotQA tasks
- Human evaluators preferred MindSearch responses over ChatGPT-Web and Perplexity.ai on depth, breadth, and factuality
- Processed more than 300 web pages in less than 3 minutes, compared to approximately 3 hours for human experts
- Demonstrated substantial improvements over baseline retrieval-augmented generation methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hierarchical retrieval with query rewriting improves recall and precision for multi-hop questions.
- **Mechanism:** WebSearcher first generates multiple related queries, executes them across search engines, aggregates results, then performs fine-grained page selection before detailed reading.
- **Core assumption:** Initial broad queries capture relevant pages that narrow queries would miss; subsequent filtering doesn't discard critical information.
- **Evidence anchors:** [section] "Initially, the LLM generates several similar queries based on the assigned questions from the WebPlanner to broaden the search content and thus improve the recall of relevant information."

### Mechanism 2
- **Claim:** Dynamic graph construction via code generation enables adaptive decomposition of complex queries.
- **Mechanism:** WebPlanner uses predefined code functions to add nodes and edges to a directed acyclic graph, where each node represents a web search and edges encode reasoning relationships between sub-questions.
- **Core assumption:** LLMs can effectively translate natural language planning steps into executable code that correctly models the reasoning structure.
- **Evidence anchors:** [section] "WebPlanner models the human mind of multi-step information seeking as a dynamic graph construction process."

### Mechanism 3
- **Claim:** Multi-agent parallel execution reduces context computation and enables processing of large-scale web pages.
- **Mechanism:** WebPlanner distributes search tasks to multiple WebSearchers that operate independently, each focusing on its sub-query without being distracted by other content.
- **Core assumption:** Separating planning from execution allows each agent to maintain a focused context, improving efficiency and reducing the computational burden on any single agent.
- **Evidence anchors:** [section] "MindSearch collects and integrates related information from more than 300 pages in less than 3 minute."

## Foundational Learning

- **Concept:** Directed Acyclic Graphs (DAGs) for representing reasoning paths
  - **Why needed here:** To model the sequential and parallel relationships between sub-questions in complex query decomposition
  - **Quick check question:** What distinguishes a DAG from a general graph in the context of representing reasoning steps?

- **Concept:** Hierarchical information retrieval strategies
  - **Why needed here:** To handle the massive volume of web pages by first broadening search then narrowing to most relevant content
  - **Quick check question:** What is the primary benefit of using a coarse-to-fine approach in web information retrieval?

- **Concept:** Context management in multi-agent systems
  - **Why needed here:** To ensure each agent maintains the necessary context for its task while preventing context overload
  - **Quick check question:** How does prefixing responses from parent nodes help maintain context across WebSearcher agents?

## Architecture Onboarding

- **Component map:** WebPlanner -> WebSearcher (multiple) -> Search APIs -> Python Interpreter
- **Critical path:**
  1. User query arrives at WebPlanner
  2. WebPlanner decomposes query into sub-questions and builds reasoning graph via code generation
  3. WebPlanner dispatches sub-questions to WebSearchers
  4. WebSearchers perform hierarchical retrieval and return results
  5. WebPlanner integrates results and generates final response

- **Design tradeoffs:**
  - Parallel vs sequential execution: Parallel reduces time but increases resource usage
  - Query breadth vs precision: Broader initial queries improve recall but require more filtering
  - Context length vs information completeness: Longer contexts capture more information but increase computational cost

- **Failure signatures:**
  - Incomplete decomposition leading to missing information
  - Overly aggressive filtering discarding relevant pages
  - Context loss between agents causing disconnected reasoning
  - Excessive parallel execution causing resource exhaustion

- **First 3 experiments:**
  1. Test single WebSearcher with hierarchical retrieval on a simple multi-hop question to validate the retrieval mechanism
  2. Implement basic graph construction with WebPlanner for a question requiring 2-3 decomposition steps
  3. Run end-to-end with both components on a controlled dataset to measure improvement over baseline retrieval-augmented generation

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the performance of MindSearch scale when increasing the number of WebSearcher agents beyond the current configuration?
- **Basis in paper:** [inferred] The paper mentions "more than 300" web pages are processed in 3 minutes, suggesting potential for further scaling
- **Why unresolved:** The paper does not systematically explore performance beyond the current agent configuration or establish scaling laws
- **What evidence would resolve it:** Controlled experiments varying the number of WebSearcher agents while measuring accuracy, response time, and cost-efficiency

### Open Question 2
- **Question:** What is the optimal strategy for context management between WebPlanner and WebSearcher agents to maximize information retention while minimizing computational overhead?
- **Basis in paper:** [explicit] The paper discusses context management challenges and mentions that "simply focusing the decomposed query from the Planner may lose useful information"
- **Why unresolved:** The paper only presents one empirical approach to context handling without exploring alternative strategies or conducting comparative analysis
- **What evidence would resolve it:** Systematic comparison of different context management strategies across various query types and evaluation of information retention vs. computational cost

### Open Question 3
- **Question:** How can the hallucination issues in MindSearch be effectively addressed, particularly when dealing with detailed search results?
- **Basis in paper:** [explicit] The paper notes that "more detailed search results may distract the concentration of the model on the initial problem" and identifies this as a "natural future work"
- **Why unresolved:** The paper acknowledges the hallucination problem but does not propose or test specific solutions
- **What evidence would resolve it:** Comparative experiments testing different hallucination mitigation techniques (such as confidence scoring, fact-checking modules, or fine-tuning approaches) on the same evaluation tasks

## Limitations
- Limited discussion of scalability beyond tested 300-page processing capacity
- No systematic exploration of performance across different LLM model families
- Potential brittleness of code-generation-based graph construction with ambiguous or complex query structures
- No analysis of biases introduced by hierarchical filtering process

## Confidence

**High confidence:** Core claims about MindSearch's effectiveness in complex web information seeking based on human evaluation results and closed-set task performance metrics.

**Medium confidence:** Hierarchical retrieval with query rewriting mechanism due to theoretical justification but limited empirical validation against alternatives.

**Medium confidence:** Multi-agent parallel execution claim based on strong theoretical rationale but limited discussion of failure modes or resource constraints.

## Next Checks

1. Conduct ablation studies comparing hierarchical retrieval against flat retrieval with the same total number of pages to isolate the benefit of the coarse-to-fine approach.

2. Test the system's performance on queries requiring longer reasoning chains (5+ hops) to identify potential limitations in the graph construction mechanism.

3. Evaluate resource usage and execution time as a function of parallel agent count to establish practical scalability boundaries.