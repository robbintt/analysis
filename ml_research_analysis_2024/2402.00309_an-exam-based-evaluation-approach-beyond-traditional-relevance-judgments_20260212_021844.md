---
ver: rpa2
title: An Exam-based Evaluation Approach Beyond Traditional Relevance Judgments
arxiv_id: '2402.00309'
source_url: https://arxiv.org/abs/2402.00309
tags:
- exam
- question
- evaluation
- system
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EXAM (EXAm Answerability Metric), a new approach
  for evaluating retrieval/generation systems without traditional relevance judgments.
  Instead of manual relevance labeling, EXAM uses a question bank to test whether
  system responses contain relevant information.
---

# An Exam-based Evaluation Approach Beyond Traditional Relevance Judgments

## Quick Facts
- arXiv ID: 2402.00309
- Source URL: https://arxiv.org/abs/2402.00309
- Authors: Naghmeh Farzi; Laura Dietz
- Reference count: 31
- One-line primary result: EXAM achieves Spearman rank correlation up to 0.96 and Kendall's Ï„ up to 0.84 with traditional TREC leaderboards

## Executive Summary
This paper introduces EXAM (EXAm Answerability Metric), a novel approach for evaluating retrieval/generation systems without traditional relevance judgments. Instead of manual relevance labeling, EXAM uses a question bank to test whether system responses contain relevant information. The method supports both human-in-the-loop question refinement and fully automated grading using LLM-based graders. Experimental results on TREC CAR Y3 and TREC DL datasets show strong correlation with official TREC rankings, outperforming traditional relevance labeling approaches.

## Method Summary
EXAM evaluates retrieval systems by generating exam questions for each query and measuring how many of these questions can be answered using the system's response passages. The approach uses either manually created question banks or automatically generated questions via ChatGPT prompts. System responses are segmented into 400-token passages and graded using FLAN-T5-large to determine answerability. The method produces two metrics: EXAM Cover (recall-oriented, counting answerable questions) and EXAM Qrels (precision-oriented, compatible with trec_eval). The process involves question generation, passage grading, and metric computation without requiring traditional relevance judgments.

## Key Results
- EXAM Qrels achieves Spearman correlation of 0.96 with TREC DL 2020 official leaderboard
- EXAM Cover shows Kendall's Ï„ of 0.84 with official CAR-Y3 rankings
- The method maintains strong correlation across both TREC CAR Y3 and TREC DL datasets
- EXAM produces reusable test collections that can be expanded post-hoc

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EXAM Cover metric correlates strongly with traditional leaderboard rankings
- Mechanism: By counting how many exam questions can be answered from any passage in a system's response, EXAM measures the breadth of relevant information coverage
- Core assumption: The exam questions adequately represent the information needs expressed in the queries
- Evidence anchors:
  - [abstract] "Experimental results on TREC CAR Y3 and TREC DL datasets show strong leaderboard rank correlation (Spearman up to 0.96, Kendall's ðœ up to 0.84) with official TREC rankings"
  - [section] "The EXAM Cover leaderboard generated using the TQA-based EXAM Cover approach is very similar to the official CAR-Y3 leaderboard (Table 3). With the exception of the ECNU_ReRank1 method, the top 10 of the leaderboard is in exactly the same order. The Spearman rank measure is 0.94, Kendall ðœ is 0.84"
- Break condition: If exam questions fail to capture the essential information needs of the queries, the correlation with traditional metrics would degrade significantly

### Mechanism 2
- Claim: LLM-based self-rating can effectively evaluate answerability without gold answer keys
- Mechanism: The LLM evaluates how well a question can be answered given a passage context, producing ratings from 0-5 based on relevance and completeness
- Core assumption: LLMs can accurately self-assess their ability to answer questions based on given context
- Evidence anchors:
  - [abstract] "We propose two evaluation measures, the recall-oriented EXAM Cover metric, and the precision-oriented EXAM Qrels metric, the latter which can be implemented with trec_eval"
  - [section] "We find that in the vast majority of cases, FLAN-T5 indeed responds with a numerical code between 0 and 5"
- Break condition: If LLM self-ratings become inconsistent or biased toward certain question types, the evaluation reliability would decrease

### Mechanism 3
- Claim: EXAM Qrels metric provides compatibility with existing evaluation tools while maintaining correlation with traditional metrics
- Mechanism: Converts exam question answerability into binary or multi-graded relevance labels that can be processed by trec_eval
- Core assumption: Binary relevance based on question answerability correlates with traditional relevance judgments
- Evidence anchors:
  - [abstract] "The latter which can be implemented with trec_eval"
  - [section] "The EXAM Qrels method correlates exceptionally well with the official leaderboard, obtaining spearman rank correlations of 0.93 in TREC DL 2019 and 0.96 on TREC DL 2020"
- Break condition: If the binary conversion loses too much nuance in question answerability, the correlation with traditional metrics would weaken

## Foundational Learning

- Concept: Question answering evaluation metrics
  - Why needed here: The EXAM approach fundamentally relies on question answering as the evaluation mechanism
  - Quick check question: What is the difference between recall-oriented and precision-oriented evaluation metrics in the context of question answering?

- Concept: LLM prompting and prompt engineering
  - Why needed here: The approach uses carefully crafted prompts to generate questions and evaluate answerability
  - Quick check question: Why is it important to include the instruction "based on the context" in the question answering prompt?

- Concept: Test collection construction and expansion
  - Why needed here: The EXAM approach creates reusable test collections that can be expanded post-hoc
  - Quick check question: How does the ability to expand question banks post-hoc improve the long-term utility of test collections?

## Architecture Onboarding

- Component map:
  Query processor -> Question generator -> Passage preprocessor -> Grading engine -> Evaluation module -> Integration layer

- Critical path:
  1. Receive queries and system responses
  2. Generate exam questions for each query
  3. Grade each passage against all exam questions
  4. Compute EXAM metrics
  5. Output results in desired format

- Design tradeoffs:
  - Question generation vs manual creation: Balance between coverage and quality
  - Answer verification vs self-rating: Trade-off between accuracy and practicality
  - EXAM Cover vs EXAM Qrels: Recall vs precision focus
  - Single vs multi-graded relevance: Simplicity vs nuance

- Failure signatures:
  - Low inter-annotator agreement with traditional relevance judgments (expected)
  - Poor correlation with traditional leaderboard rankings (critical failure)
  - Inconsistent self-ratings from LLM grader
  - Questions that are too easy or too difficult to distinguish system quality

- First 3 experiments:
  1. Run EXAM Cover evaluation on TREC CAR Y3 with TQA questions to verify baseline correlation
  2. Switch to generated question banks and compare results to baseline
  3. Apply EXAM Qrels to TREC DL 2019 and verify correlation with official leaderboard

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does human involvement in creating and refining exam question banks affect the quality and reliability of EXAM-based evaluation compared to fully automated approaches?
- Basis in paper: [explicit] The paper discusses human-in-the-loop potential but leaves user studies for future work, and focuses on empirical evaluation of automated approaches
- Why unresolved: The paper only tests fully manual and fully automated question generation, not intermediate human-involved approaches
- What evidence would resolve it: Comparative studies measuring evaluation quality (leaderboard correlation, inter-annotator agreement) across different levels of human involvement in question creation

### Open Question 2
- Question: What is the optimal self-rating threshold for EXAM Qrels that balances precision and recall while maintaining strong leaderboard correlation?
- Basis in paper: [explicit] The paper uses rating â‰¥ 4 as "strict" relevance but notes this is an arbitrary choice, and explores rating â‰¥ 1 as "lenient"
- Why unresolved: The paper doesn't systematically explore the trade-offs at different threshold levels
- What evidence would resolve it: Systematic evaluation of EXAM Qrels performance at various self-rating thresholds against official leaderboards

### Open Question 3
- Question: How well does the EXAM approach generalize to different types of queries and domains beyond the tested TREC datasets?
- Basis in paper: [inferred] The paper tests only TREC CAR Y3 and TREC DL datasets, which are specific to certain domains and query types
- Why unresolved: Limited scope of evaluation datasets prevents generalization claims
- What evidence would resolve it: Evaluation on diverse datasets spanning different domains, query types, and information needs

### Open Question 4
- Question: What is the impact of question bank size and diversity on EXAM evaluation effectiveness?
- Basis in paper: [inferred] The paper uses 10 questions per query but doesn't explore how varying this number affects results
- Why unresolved: The paper fixes question bank size without exploring optimal configuration
- What evidence would resolve it: Systematic studies varying question bank size and diversity while measuring evaluation quality metrics

## Limitations
- The method relies heavily on the quality and representativeness of the exam question banks, but the paper doesn't provide systematic analysis of question difficulty distribution or coverage completeness
- LLM-based grading introduces potential bias that may not be fully characterized - while the paper reports numerical consistency (0-5 ratings), it doesn't analyze grader reliability across different question types or domains
- The correlation results, while strong, are based on specific TREC datasets (CAR-Y3, DL 2019-2020) and may not generalize to other retrieval tasks or domains

## Confidence
- High confidence: The EXAM Qrels correlation results with traditional metrics (Spearman 0.96, Kendall's Ï„ 0.84) are well-supported by experimental data
- Medium confidence: The general methodology and approach are sound, but real-world applicability depends on question bank quality which varies by domain
- Medium confidence: The practical advantages (no manual relevance labeling, reusable collections) are demonstrated but would benefit from additional real-world deployment case studies

## Next Checks
1. **Generalization Test**: Apply EXAM evaluation to a non-TREC retrieval dataset (e.g., academic search or e-commerce) and measure correlation with traditional relevance judgments to assess domain transferability
2. **Question Quality Analysis**: Systematically analyze the generated exam questions for coverage gaps, difficulty balance, and potential bias patterns across different query types
3. **Efficiency Benchmark**: Measure and optimize the computational overhead of LLM-based grading compared to traditional relevance labeling for realistic system evaluation workloads