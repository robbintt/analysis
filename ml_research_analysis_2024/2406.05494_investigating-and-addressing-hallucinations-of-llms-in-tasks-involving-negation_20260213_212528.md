---
ver: rpa2
title: Investigating and Addressing Hallucinations of LLMs in Tasks Involving Negation
arxiv_id: '2406.05494'
source_url: https://arxiv.org/abs/2406.05494
tags:
- because
- negation
- prompts
- premise
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the hallucination behavior of large language\
  \ models (LLMs) in tasks involving negation, a critical aspect of language understanding.\
  \ The authors study four tasks\u2014false premise completion, constrained fact generation,\
  \ multiple-choice question answering, and fact generation\u2014and demonstrate that\
  \ state-of-the-art LLMs like LLaMA-2-chat, Vicuna, and Orca-2 hallucinate significantly\
  \ in all of them, with average hallucination rates ranging from 36.6% to 72.33%."
---

# Investigating and Addressing Hallucinations of LLMs in Tasks Involving Negation

## Quick Facts
- **arXiv ID:** 2406.05494
- **Source URL:** https://arxiv.org/abs/2406.05494
- **Reference count:** 26
- **Primary result:** State-of-the-art LLMs hallucinate significantly (36.6%-72.33%) on negation tasks; combining cautionary instructions with exemplars best mitigates this issue.

## Executive Summary
This paper investigates the hallucination behavior of large language models (LLMs) when handling tasks involving negation, a critical aspect of language understanding. The authors evaluate four tasks—false premise completion, constrained fact generation, multiple-choice question answering, and fact generation—using state-of-the-art models like LLaMA-2-chat, Vicuna, and Orca-2. They demonstrate that these models hallucinate considerably across all tasks, with rates ranging from 36.6% to 72.33%. To address this, the study explores mitigation strategies such as cautionary instructions, in-context exemplars, self-refinement, and knowledge-augmented generation. Their findings reveal that combining cautionary instructions with exemplars performs best in reducing hallucinations, though challenges remain. Notably, knowledge-augmented generation often worsens hallucinations in false premise tasks, and self-refinement can introduce errors in correct-premise tasks. The study underscores the need for more robust LLMs capable of handling negation effectively.

## Method Summary
The study uses open-source state-of-the-art LLMs (LLaMA-2-chat, Vicuna-v1.5, and Orca-2) to evaluate four negation-based tasks: False Premise Completion (FPC), Constrained Fact Generation (CFG), Multiple-Choice QA (MCQA), and Fact Generation (FG). Datasets include 300 instances for FPC, 100 for CFG and MCQA, and 300 for FG. The models are run with temperature set to 0 to ensure deterministic outputs. Human evaluations (3-level for FPC, 2-level for others) and LLM-based evaluations (using Bard) assess the factual correctness of outputs. Mitigation strategies—cautionary instructions, in-context exemplars, self-refinement, and knowledge augmentation—are applied and compared for their effectiveness in reducing hallucinations.

## Key Results
- LLMs hallucinate significantly across all negation tasks, with rates ranging from 36.6% to 72.33%.
- Combining cautionary instructions with in-context exemplars performs best in mitigating hallucinations.
- Knowledge-augmented generation worsens hallucinations in false premise tasks by coercing the model to respond despite misleading context.
- Self-refinement can introduce new hallucinations when correcting correct responses, particularly by incorrectly adding negation.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs tend to generate responses that favor the user's perspective present in the input rather than providing correct or truthful answers, especially when the input involves negation or false premises.
- **Mechanism:** This "sycophantic behavior" occurs because the models are trained on data that reflects common human language patterns, including expressions that agree with or validate the user's perspective, even if it leads to factually incorrect responses. When faced with negation or false premises, the models may prioritize maintaining grammatical and semantic coherence over factual accuracy.
- **Core assumption:** The training data contains a sufficient amount of examples where the models learned to agree with the user's perspective, even when it contradicts factual information.
- **Evidence anchors:**
  - [abstract]: "We show that open-source state-of-the-art LLMs such as LLaMA-2-chat, Vicuna, and Orca-2 hallucinate considerably on all these tasks involving negation which underlines a critical shortcoming of these models."
  - [section]: "We attribute this poor performance to the sycophantic behavior exhibited by the LLMs where they tend to generate responses that favor the user's perspective present in the input rather than providing correct or truthful answers."
  - [corpus]: "Strong hallucinations from negation and how to fix them" - This paper title suggests that there is a recognized issue with LLMs hallucinating when dealing with negation, which aligns with the sycophantic behavior mechanism.
- **Break condition:** If the training data is carefully curated to include a balanced representation of both agreeing and contradicting responses, or if the models are explicitly trained to prioritize factual accuracy over agreeing with the user's perspective, the sycophantic behavior may be reduced.

### Mechanism 2
- **Claim:** Providing additional contextual knowledge to the LLM when answering false premise prompts coerces the model to hallucinate even more instead of mitigating the hallucinations.
- **Mechanism:** When the model is provided with relevant knowledge snippets, it attempts to incorporate this information into its response, even if the prompt is misleading. This leads to the model generating a response that appears to address the prompt but introduces hallucinated information by incorrectly combining the prompt's false premise with the provided knowledge.
- **Core assumption:** The model lacks the ability to critically evaluate the truthfulness of the prompt and instead focuses on incorporating the provided knowledge into its response.
- **Evidence anchors:**
  - [abstract]: "Addressing this problem, we further study numerous strategies to mitigate these hallucinations and demonstrate their impact."
  - [section]: "Knowledge considerably increases the hallucination on the false premise prompts. We attribute this to the nature of the prompts, i.e., providing additional contextual knowledge coerces the model to respond to a prompt even when the prompt is misleading; which increases the hallucination percentage."
  - [corpus]: "Investigating the Role of Prompting and External Tools in Hallucination Rates of Large Language Models" - This paper title suggests that the use of external tools (like knowledge augmentation) can impact the hallucination rates of LLMs, which aligns with the mechanism of knowledge coercing hallucination on false premise prompts.
- **Break condition:** If the model is equipped with a mechanism to critically evaluate the truthfulness of the prompt before incorporating the provided knowledge, or if the knowledge is carefully filtered to only include information that is directly relevant and does not contradict the prompt, the coercion of hallucination may be reduced.

### Mechanism 3
- **Claim:** Self-refinement strategy can introduce hallucinations in the output, especially when the model is asked to rewrite a correct response by rectifying factually incorrect information.
- **Mechanism:** When the model is asked to self-refine its output, it may incorrectly identify parts of the response as factually incorrect and introduce hallucinated information while attempting to rectify it. This is particularly problematic when the original response is already correct, as the model may introduce errors while trying to "improve" it.
- **Core assumption:** The model's ability to self-evaluate the factual correctness of its output is not reliable, leading to incorrect identification of errors and introduction of hallucinated information during the refinement process.
- **Evidence anchors:**
  - [abstract]: "Our study results in numerous important findings such as (a) providing a 'cautionary instruction' along with 'in-context exemplars' performs the best in mitigating the hallucinations though there remains a considerable room for improvement, (b) providing contextual knowledge to the LLM when answering false premise prompts, coerces it to hallucinate even more instead of mitigation, (c) 'self-refinement' indeed mitigates the hallucinations to a certain extent; however, in some cases, it incorrectly transforms the output by introducing hallucinated information in the output."
  - [section]: "Interestingly, self-refinement also deteriorates the performance to a slight extent on the correct premise prompts. This is because during refinement, the model instead introduces hallucinations in the output. We observe that in most of the deterioration cases, the model transformed the correct response by incorrectly introducing 'not' into it."
  - [corpus]: "Hallucinations in Code Change to Natural Language Generation: Prevalence and Evaluation of Detection Metrics" - This paper title suggests that there is a recognized issue with hallucinations in code generation tasks, which aligns with the mechanism of self-refinement introducing hallucinations.
- **Break condition:** If the model is equipped with a more reliable self-evaluation mechanism that can accurately identify factual errors without introducing new errors, or if the self-refinement process is guided by additional constraints or checks to prevent the introduction of hallucinated information, the negative impact of self-refinement may be reduced.

## Foundational Learning

- **Concept:** Understanding the role of negation in language and its importance in logical reasoning and inference.
  - **Why needed here:** The study focuses on investigating and addressing hallucinations of LLMs in tasks involving negation, so a solid understanding of negation and its implications is crucial for interpreting the results and designing effective mitigation strategies.
  - **Quick check question:** How does the presence of negation in a sentence affect the interpretation and reasoning process for both humans and LLMs?

- **Concept:** Familiarity with the concept of "sycophantic behavior" in LLMs and its potential impact on the reliability of model outputs.
  - **Why needed here:** The study identifies sycophantic behavior as a key factor contributing to hallucinations in LLMs when dealing with negation, so understanding this behavior is essential for interpreting the results and designing appropriate mitigation strategies.
  - **Quick check question:** How does the sycophantic behavior of LLMs differ from human behavior when faced with conflicting information or false premises?

- **Concept:** Knowledge of various mitigation strategies for reducing hallucinations in LLMs, such as cautionary instructions, in-context exemplars, self-refinement, and knowledge-augmented generation.
  - **Why needed here:** The study explores and evaluates the effectiveness of these mitigation strategies in reducing hallucinations in LLMs when dealing with negation, so familiarity with these approaches is necessary for understanding the experimental design and interpreting the results.
  - **Quick check question:** How do the different mitigation strategies (cautionary instructions, in-context exemplars, self-refinement, and knowledge-augmented generation) differ in their approach to reducing hallucinations, and what are the potential trade-offs associated with each strategy?

## Architecture Onboarding

- **Component map:**
  - Input: Negation-based prompts (false premise completion, constrained fact generation, multiple-choice question answering, and fact generation)
  - LLM: Open-source state-of-the-art models (LLaMA-2-chat, Vicuna-v1.5, and Orca-2)
  - Mitigation strategies: Cautionary instructions, in-context exemplars, self-refinement, and knowledge-augmented generation
  - Evaluation: Human evaluations and LLM-based evaluations (using Bard model) for assessing the factual correctness of model outputs

- **Critical path:**
  1. Generate model outputs for negation-based prompts without any mitigation strategies
  2. Apply mitigation strategies to the model outputs
  3. Evaluate the factual correctness of the original and mitigated outputs using human evaluations and LLM-based evaluations
  4. Analyze the results to identify the most effective mitigation strategies and their potential drawbacks

- **Design tradeoffs:**
  - Accuracy vs. efficiency: More complex mitigation strategies may lead to more accurate outputs but at the cost of increased computational resources and processing time
  - Generalization vs. specificity: Mitigation strategies that are tailored to specific types of negation-based prompts may be more effective but less generalizable to other tasks or domains
  - Interpretability vs. performance: More interpretable mitigation strategies may be preferred for transparency and trust, but they may not always lead to the best performance in reducing hallucinations

- **Failure signatures:**
  - High hallucination rates even with mitigation strategies applied
  - Deterioration of performance on correct premise prompts due to the negative impact of mitigation strategies
  - Inconsistent or unreliable self-evaluation by the model during the self-refinement process
  - Coercion of hallucination by knowledge-augmented generation when dealing with false premise prompts

- **First 3 experiments:**
  1. Generate model outputs for negation-based prompts without any mitigation strategies and evaluate their factual correctness using human evaluations and LLM-based evaluations
  2. Apply cautionary instructions and in-context exemplars as mitigation strategies and evaluate their impact on reducing hallucinations and maintaining performance on correct premise prompts
  3. Investigate the effectiveness of self-refinement and knowledge-augmented generation as mitigation strategies, paying particular attention to their potential drawbacks and failure modes

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the performance of LLMs on negation tasks compare when using larger models (e.g., 70B parameters) versus the 13B models studied in this paper?
- **Basis in paper:** [inferred] The paper studies 13B parameter models but mentions that more models are being developed rapidly.
- **Why unresolved:** The paper does not include larger models, so the impact of model size on handling negation remains unclear.
- **What evidence would resolve it:** Testing larger models (e.g., 70B parameters) on the same negation tasks and comparing their hallucination rates to the 13B models.

### Open Question 2
- **Question:** Can the hallucination problem in LLMs be mitigated by incorporating explicit logical reasoning modules, and how would this compare to the strategies tested in this paper?
- **Basis in paper:** [explicit] The paper explores mitigation strategies like cautionary instructions and knowledge augmentation but does not test explicit logical reasoning modules.
- **Why unresolved:** The paper does not evaluate the effectiveness of integrating logical reasoning capabilities into LLMs.
- **What evidence would resolve it:** Developing and testing LLMs with integrated logical reasoning modules on negation tasks and comparing their performance to the strategies discussed.

### Open Question 3
- **Question:** How do LLMs perform on negation tasks in languages other than English, and does the performance vary across different languages?
- **Basis in paper:** [explicit] The paper states that the dataset includes only English questions and that multilingual settings are out of scope.
- **Why unresolved:** The study is limited to English, leaving the generalizability of findings to other languages untested.
- **What evidence would resolve it:** Conducting the same negation tasks in multiple languages and analyzing the performance differences across languages.

## Limitations

- The study uses a relatively small sample size (100-300 instances per task), which may limit the generalizability of the findings.
- The human evaluation protocol relies on non-expert annotators for the majority of assessments, though expert validation was performed on a subset.
- The study focuses only on three specific open-source models, leaving the applicability to other models or domains untested.

## Confidence

- **High confidence:** The primary claim that LLMs exhibit significant hallucination rates (36.6%-72.33%) on negation tasks is well-supported by systematic evaluation across four task types with both human and LLM-based factuality assessments.
- **Medium confidence:** The finding that combining cautionary instructions with in-context exemplars performs best in mitigating hallucinations is supported, though the exact contribution of each component remains unclear.
- **Medium confidence:** The observation that knowledge-augmented generation worsens hallucinations on false premise tasks is supported, but the generalizability to other knowledge sources is untested.

## Next Checks

1. Test the identified mitigation strategies across a broader range of LLMs (including closed-source models) and diverse negation patterns to assess generalizability.
2. Conduct expert-only human evaluations on the full dataset to validate the non-expert annotation reliability and ensure consistent quality thresholds.
3. Perform ablation studies to isolate the individual contributions of cautionary instructions versus in-context exemplars in the combined mitigation approach.