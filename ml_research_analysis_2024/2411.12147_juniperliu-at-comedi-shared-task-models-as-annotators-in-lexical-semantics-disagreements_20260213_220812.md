---
ver: rpa2
title: 'JuniperLiu at CoMeDi Shared Task: Models as Annotators in Lexical Semantics
  Disagreements'
arxiv_id: '2411.12147'
source_url: https://arxiv.org/abs/2411.12147
tags:
- subtask
- language
- methods
- layer
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a system for predicting majority votes and
  annotator disagreements in lexical semantic tasks. The approach treats individual
  models as virtual annotators and simulates the annotation process using ensemble
  strategies with MLP-based and threshold-based methods.
---

# JuniperLiu at CoMeDi Shared Task: Models as Annotators in Lexical Semantics Disagreements

## Quick Facts
- arXiv ID: 2411.12147
- Source URL: https://arxiv.org/abs/2411.12147
- Reference count: 18
- This paper presents a system for predicting majority votes and annotator disagreements in lexical semantic tasks using model ensembling and anisotropy removal techniques.

## Executive Summary
This paper addresses the challenge of predicting annotator disagreements in lexical semantic tasks by treating individual models as virtual annotators. The authors employ ensemble strategies with MLP-based and threshold-based methods, using anisotropy removal techniques to improve model performance. Their approach achieves competitive results, particularly excelling at subtask 2 (disagreement prediction) by showing that standard deviation on continuous relatedness scores correlates better with human disagreement annotations than metrics based on discrete labels.

## Method Summary
The method treats individual models as virtual annotators and simulates the annotation process using ensemble strategies. The system extracts contextual representations from pretrained models (XLM-RoBERTa, BERT, Llama), applies anisotropy removal techniques (centering, standardization, all-but-the-top), and uses threshold-based methods for discrete labels and MLP-based methods for continuous scores. Model ensembles are created using homogeneous, heterogeneous, and mixed strategies, with disagreement measured via standard deviation on continuous relatedness scores or mean pairwise differences on discrete labels.

## Key Results
- Standard deviation on continuous relatedness scores correlates better with human disagreement annotations than metrics based on discrete labels
- Anisotropy removal techniques significantly improve model performance by reducing inflated similarity scores between unrelated words
- Homogeneous model ensembles outperform heterogeneous combinations, though continuous score aggregation shows better disagreement estimation across ensemble types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Treating models as virtual annotators enables effective ensemble disagreement estimation.
- Mechanism: Each model configuration acts as an independent annotator, and disagreement is measured via standard deviation of continuous relatedness scores.
- Core assumption: Model variations produce statistically independent judgment distributions analogous to human annotators.
- Evidence anchors: [abstract] "treating individual models as virtual annotators"; [section 3.3] "treat each model or its manipulation as an annotator"
- Break condition: If model manipulations are highly correlated, the independence assumption fails and ensemble disagreement becomes uninformative.

### Mechanism 2
- Claim: Anisotropy removal improves discriminative power of contextual embeddings for semantic tasks.
- Mechanism: Removing directional biases reduces inflated similarity scores between unrelated words.
- Core assumption: Pretrained embeddings have systematic geometric biases that harm semantic discrimination.
- Evidence anchors: [section 3.2] "Contextual representations are known to be anisotropic... This inflates similarity scores"; [section 5.1] "The large gap between removal and non-removal emphasizes the importance of this technique"
- Break condition: If the embedding space is already approximately isotropic, additional removal techniques may degrade performance.

### Mechanism 3
- Claim: Continuous relatedness scores capture disagreement better than discrete labels.
- Mechanism: Standard deviation on continuous scores shows stronger correlation with human disagreement annotations than metrics based on aggregated discrete labels.
- Core assumption: Continuous scores preserve more information about semantic relationships than discretized labels.
- Evidence anchors: [abstract] "standard deviation on continuous relatedness scores... correlates with human disagreement annotations compared to metrics on aggregated discrete labels"; [section 5.2] "STD on a continuous similarity score outperforms the others"
- Break condition: If the task requires only ordinal classification, the additional information in continuous scores may not justify the complexity.

## Foundational Learning

- Concept: Gaussian distribution properties (mean and variance)
  - Why needed here: The paper frames both subtasks as parameter estimation problems - mean for consensus similarity, variance for disagreement
  - Quick check question: If human judgments follow N(µ, σ²), what do Subtask 1 and Subtask 2 estimate respectively?

- Concept: Anisotropy in high-dimensional spaces
  - Why needed here: Understanding why embeddings cluster in narrow regions and how this affects semantic similarity
  - Quick check question: Why does anisotropy cause inflated similarity scores between unrelated words?

- Concept: Ensemble methods and uncertainty estimation
  - Why needed here: The disagreement estimation relies on treating model variations as sources of uncertainty
  - Quick check question: How does model ensemble uncertainty relate to aleatoric and epistemic uncertainty?

## Architecture Onboarding

- Component map: Input: Word pairs in context -> Representation layer: Pretrained models with layer extraction -> Anisotropy removal: Centering/standardization/all-but-the-top -> Scoring: Threshold-based/MLP-based methods -> Aggregation: Ensemble strategies with STD/MPD/VR measures -> Output: Discrete label/continuous disagreement score

- Critical path: Pretrained model → anisotropy removal → scoring → aggregation → evaluation
- Design tradeoffs:
  - Model complexity vs. performance: MLP-based methods add parameters but improve results
  - Continuous vs. discrete: Continuous scores better capture disagreement but require regression
  - Homogeneous vs. heterogeneous ensembles: Homogeneous performs better but heterogeneous explores more model diversity

- Failure signatures:
  - Poor performance on Subtask 1: Likely issues with threshold selection or anisotropy removal
  - Weak correlation on Subtask 2: Check independence assumption of model variations or measure choice
  - Inconsistent language results: May need language-specific model configurations

- First 3 experiments:
  1. Compare anisotropy removal methods (centering, standardization, all-but-the-top) on development set to identify optimal transformation
  2. Test different layer indices for representation extraction to find sweet spot between shallow and deep contextualization
  3. Evaluate homogeneous vs. heterogeneous ensemble strategies with STD measure to determine best model combination approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal layer index for Llama-7B across different languages and tasks, and how does this vary between encoder-only and decoder-only models?
- Basis in paper: [explicit] The paper mentions that the optimal layer index for Llama-7B is 25, but this is used as an upper bound and not as the final choice for all languages.
- Why unresolved: The paper does not provide a systematic analysis of layer selection across different languages and tasks, leaving the question of optimal layer indices open.
- What evidence would resolve it: A comprehensive study comparing the performance of different layer indices across multiple languages and tasks would provide clarity on the optimal layer selection.

### Open Question 2
- Question: How do different anisotropy removal techniques (centering, standardization, all-but-the-top) affect model performance across various languages and tasks?
- Basis in paper: [explicit] The paper applies three anisotropy removal techniques and notes that standardization consistently performs best across all layers.
- Why unresolved: The paper focuses on the effectiveness of anisotropy removal in general but does not explore the specific impact of each technique on model performance across different languages and tasks.
- What evidence would resolve it: An ablation study comparing the performance of models with different anisotropy removal techniques across various languages and tasks would clarify their relative effectiveness.

### Open Question 3
- Question: What are the implications of using different measures (STD, MPD, VR) for capturing annotator disagreement, and how do these measures perform in heterogeneous vs. homogeneous model ensembles?
- Basis in paper: [explicit] The paper compares three measures (STD, MPD, VR) and finds that STD on continuous similarity scores outperforms the others.
- Why unresolved: The paper does not explore the implications of using different measures in detail, nor does it provide a thorough analysis of their performance in heterogeneous vs. homogeneous model ensembles.
- What evidence would resolve it: A detailed analysis comparing the performance of different measures in heterogeneous and homogeneous model ensembles across various tasks would provide insights into their effectiveness.

## Limitations
- The independence assumption for model variations as virtual annotators lacks direct validation
- The paper relies on pretrained models without fine-tuning, potentially missing task-specific nuances
- Language-specific biases in model ensembles are not addressed

## Confidence
The confidence in the proposed mechanisms is Medium.

## Next Checks
1. Validate the independence assumption by computing pairwise correlations between model outputs and assessing whether ensemble disagreement measures improve with truly independent models.
2. Conduct ablation studies to quantify the impact of anisotropy removal techniques on downstream semantic tasks across different embedding architectures.
3. Compare continuous disagreement measures against alternative uncertainty quantification methods (e.g., Bayesian approaches) to determine if the standard deviation metric is optimal.