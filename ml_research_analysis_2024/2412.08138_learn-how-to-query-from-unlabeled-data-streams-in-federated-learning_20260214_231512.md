---
ver: rpa2
title: Learn How to Query from Unlabeled Data Streams in Federated Learning
arxiv_id: '2412.08138'
source_url: https://arxiv.org/abs/2412.08138
tags:
- data
- clients
- training
- samples
- global
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of data querying in federated learning
  when unlabeled data arrive in a streaming fashion. It proposes LeaDQ, a multi-agent
  reinforcement learning algorithm that enables decentralized clients to collaboratively
  select informative samples for labeling while considering global model training
  objectives.
---

# Learn How to Query from Unlabeled Data Streams in Federated Learning

## Quick Facts
- arXiv ID: 2412.08138
- Source URL: https://arxiv.org/abs/2412.08138
- Reference count: 24
- Primary result: Achieves up to 6% higher accuracy on CIFAR-100 compared to state-of-the-art baselines

## Executive Summary
This paper addresses the challenge of data querying in federated learning when unlabeled data arrives in streaming fashion. The authors propose LeaDQ, a multi-agent reinforcement learning algorithm that enables decentralized clients to collaboratively select informative samples for labeling while considering global model training objectives. By formulating the problem as a decentralized POMDP and using centralized training with decentralized execution, LeaDQ learns local policies that are globally beneficial without compromising data privacy. The method demonstrates superior performance on both image and text tasks, achieving up to 6% higher accuracy on CIFAR-100 while consistently outperforming baselines across various non-IID data distributions.

## Method Summary
LeaDQ formulates the data querying problem as a decentralized POMDP where each client acts as an autonomous agent. The algorithm uses a centralized training, decentralized execution (CTDE) paradigm where local Q-networks learn to select informative samples based on global feedback. During centralized training, a mixing network aggregates local Q-values while maintaining the monotonic property to ensure stable learning of cooperative strategies. Clients observe prediction logits from the global model, select samples using their learned policies, and receive global feedback in the form of accuracy improvements on a held-out dataset. The method employs experience replay and temporal difference learning to train the policy networks, enabling clients to discover non-myopic querying strategies that consider future model improvements.

## Key Results
- Achieves up to 6% higher accuracy on CIFAR-100 compared to state-of-the-art baselines
- Consistently outperforms uncertainty-based methods across various non-IID data distributions
- Demonstrates effective coordination among multiple clients while preserving data privacy through decentralized execution

## Why This Works (Mechanism)

### Mechanism 1
The centralized training decentralized execution (CTDE) paradigm enables clients to make locally optimal querying decisions that are globally beneficial. By using a centralized mixing network to aggregate local Q-values while maintaining decentralized execution, the algorithm can implicitly incorporate global model state into local decision-making without violating data privacy constraints. The global state (model accuracy on held-out data) serves as a sufficient statistic to guide local querying policies toward globally beneficial samples. This mechanism could break if the held-out dataset distribution differs significantly from the actual target distribution, providing misleading guidance.

### Mechanism 2
The multi-agent reinforcement learning framework allows clients to learn collaborative querying strategies that outperform individual uncertainty-based approaches. Each client learns a policy that maximizes discounted cumulative reward based on global feedback, enabling discovery of non-myopic querying strategies that consider future model improvements. The reward signal (difference in model accuracy before and after model update) provides sufficient learning signal for clients to identify beneficial samples. However, if the reward signal is too sparse or noisy, learning may fail to converge to effective policies.

### Mechanism 3
The monotonic value function factorization in QMIX enables effective coordination among clients while preserving individual decision-making autonomy. The mixing network combines local Q-values using a monotonic function family, ensuring that the joint Q-value increases when any local Q-value increases, facilitating stable learning of cooperative strategies. The monotonic constraint is sufficient to capture the complex interactions between clients' querying decisions and their impact on global model performance. This could fail if client interactions are non-monotonic, such as when one client's good choice reduces another's options.

## Foundational Learning

- Concept: Reinforcement Learning with Sparse Rewards
  - Why needed here: The algorithm must learn from delayed global feedback (model accuracy improvements) that only occurs after multiple querying and training iterations.
  - Quick check question: What exploration strategy would help agents discover effective querying policies when rewards are sparse and delayed?

- Concept: Federated Learning with Non-IID Data
  - Why needed here: The algorithm operates in a setting where clients have different data distributions, making local optimization potentially misaligned with global objectives.
  - Quick check question: How does data heterogeneity across clients affect the ability of local policies to learn globally beneficial querying strategies?

- Concept: Multi-Agent Systems and Decentralized Decision-Making
  - Why needed here: Each client acts as an autonomous agent making decisions based on local observations while contributing to a shared global objective.
  - Quick check question: What challenges arise when coordinating multiple agents with partial observability and different local objectives?

## Architecture Onboarding

- Component map: Agent networks -> Mixing network -> Replay buffer -> Global state tracker -> Reward calculator
- Critical path:
  1. New unlabeled data arrives at clients
  2. Clients compute local observations (prediction logits)
  3. Agents select samples using greedy policy on Q-values
  4. Oracle provides labels for selected samples
  5. Clients perform local training and upload gradients
  6. Server aggregates gradients and updates global model
  7. Global state and reward are computed
  8. Experience is stored in replay buffer
  9. Policy networks are trained using TD loss minimization
- Design tradeoffs:
  - Centralized training vs. privacy: The mixing network requires global state but doesn't access raw data
  - Exploration vs. exploitation: Agents must balance trying new queries against using known effective strategies
  - Model complexity vs. training stability: More complex mixing networks may capture better interactions but could destabilize learning
- Failure signatures:
  - Policy collapse: All clients start selecting the same samples, indicating poor exploration
  - Negative reward: Model accuracy decreases after querying, suggesting poor sample selection
  - Convergence to local optima: Performance plateaus below baseline methods
- First 3 experiments:
  1. Run with single client and small dataset to verify basic Q-learning works before adding multi-agent complexity
  2. Test with synthetic data where optimal querying strategy is known to validate algorithm learns correct behavior
  3. Compare against uncertainty-based baseline on a simple image classification task to establish performance improvements

## Open Questions the Paper Calls Out

### Open Question 1
How does LeaDQ's performance scale with the number of clients beyond 50, particularly in terms of communication overhead and policy convergence? The paper shows results with 50 clients but does not explore larger numbers or analyze communication overhead explicitly. Experiments with varying numbers of clients (e.g., 100, 500, 1000) measuring model accuracy, communication rounds, and convergence time would provide insights into scalability.

### Open Question 2
What is the impact of different types of non-IID data distributions (e.g., label skew, feature skew) on LeaDQ's performance compared to other federated active learning methods? While the paper addresses some aspects of non-IID data, it does not comprehensively evaluate how different types of data heterogeneity affect performance. Experiments comparing LeaDQ's performance across various non-IID data distribution types would clarify its robustness to different data heterogeneity scenarios.

### Open Question 3
How does LeaDQ perform under varying label noise levels, and can it adapt to noisy oracle responses during the data querying process? The paper assumes perfect oracle responses but does not address scenarios with label noise, which is a practical concern in real-world applications. Experiments introducing varying levels of label noise and evaluating LeaDQ's performance would reveal its robustness to noisy oracle responses.

## Limitations
- Architecture details such as exact neural network specifications and mixing network implementation are not fully specified
- Experiments only demonstrate performance with up to 10 clients, leaving scalability to larger deployments unclear
- The method assumes perfect oracle responses and does not address robustness to label noise

## Confidence
- Algorithm Effectiveness (6% accuracy gain on CIFAR-100): Medium confidence - experimental results are presented but lack statistical significance testing and ablation studies
- Privacy Preservation through CTDE: High confidence - the mechanism of centralized training with decentralized execution is theoretically sound and well-established in MARL literature
- Scalability to Multiple Clients: Low confidence - experiments only demonstrate up to 10 clients, which may not reflect performance at realistic scale

## Next Checks
1. Ablation Study on Architecture Components: Remove the mixing network and compare against independent Q-learning per client to quantify the benefit of coordinated querying strategies
2. Robustness to Held-Out Dataset Quality: Systematically vary the similarity between held-out dataset and target distribution to test whether global state remains a reliable signal when distributions diverge
3. Generalization Across Data Modalities: Test the algorithm on structured data (tabular) and sequential data (time series) beyond image and text to verify the method's broad applicability claims