---
ver: rpa2
title: Feed-Forward Neural Networks as a Mixed-Integer Program
arxiv_id: '2402.06697'
source_url: https://arxiv.org/abs/2402.06697
tags:
- layer
- relu
- network
- formulation
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores mixed-integer programming (MIP) formulations
  for deep neural networks (DNNs), specifically focusing on ReLU-based feedforward
  neural networks (FF-DNNs). It presents various MIP formulations for trained DNNs,
  including big-M, convex hull, extended, and disjunctive programming approaches,
  along with valid inequalities to strengthen the formulations.
---

# Feed-Forward Neural Networks as a Mixed-Integer Program

## Quick Facts
- arXiv ID: 2402.06697
- Source URL: https://arxiv.org/abs/2402.06697
- Authors: Navid Aftabi; Nima Moradi; Fatemeh Mahroo
- Reference count: 9
- This study explores mixed-integer programming (MIP) formulations for deep neural networks (DNNs), specifically focusing on ReLU-based feedforward neural networks (FF-DNNs).

## Executive Summary
This study presents various MIP formulations for trained ReLU-based feedforward neural networks, including big-M, convex hull, extended, and disjunctive programming approaches. The research demonstrates that while MIP formulations are effective for smaller networks, they face scalability challenges with larger, deeper architectures. Computational experiments on handwritten digit classification using the MNIST dataset show that the extended and disjunctive formulations show promise for narrower networks, but broader networks require more computational resources.

## Method Summary
The paper trains two FF-DNN architectures on MNIST using TensorFlow with ReLU activations, early stopping, and Adam optimizer. It then formulates the trained networks as MIPs using four different approaches: big-M, convex hull, extended, and disjunctive programming. The formulations incorporate constraints for ReLU units and max-pooling layers, and are solved using Gurobi to generate adversarial examples by minimizing L1-norm while enforcing misclassification into a specific incorrect digit.

## Key Results
- MIP formulations can exactly represent ReLU-based feedforward neural networks using binary variables for activation states
- Valid inequalities significantly strengthen the LP relaxation of MIP formulations for ReLU networks
- MIP formulations enable exact adversarial example generation for trained neural networks
- While effective for smaller networks, MIP formulations face scalability challenges with larger, deeper architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ReLU-based feedforward neural networks can be exactly represented as mixed-integer programs by introducing binary variables for activation states.
- Mechanism: The ReLU activation function is piece-wise linear, which can be reformulated as a set of linear constraints using big-M, convex hull, extended, or disjunctive programming formulations. Each formulation uses binary variables to indicate whether a neuron is active (ReLU output is positive) or inactive (ReLU output is zero).
- Core assumption: The network weights and biases are fixed and known, so the ReLU behavior is deterministic given an input.
- Evidence anchors:
  - [abstract] "In scenarios like max pooling, where multiple input values are involved, a fixed-parameter DNN can be modeled as a mixed-integer program (MIP)."
  - [section 3.1.1] "By introducing a binary variable zl i, for i = 1, . . . , nl and l = 1, . . . , L, that takes 1 if the neuron is activated...ReLU is formulated as the set of linear inequalities in (8)."
- Break condition: If the ReLU formulation is overly conservative (big-M bounds too loose), the LP relaxation becomes weak and solver performance degrades.

### Mechanism 2
- Claim: Valid inequalities can significantly strengthen the LP relaxation of MIP formulations for ReLU networks.
- Mechanism: When ReLU bounds are known (e.g., a neuron is always active or always inactive), constraints can be simplified or removed. Additional inequalities based on network structure (e.g., if bias is negative, a neuron can only be active if some preceding neuron is active) tighten the formulation.
- Core assumption: Bounds on ReLU inputs/outputs can be computed efficiently via interval arithmetic or LP maximization/minimization.
- Evidence anchors:
  - [section 3.1.2] "If Ll i ≥ 0 or U l i ≤ 0, the output of xl i is known...Hence, if M l+ i < 0, for some i in layer l, the unit is always inactive...Similarly, if M l- i < 0...the unit is always active..."
  - [section 3.1.2] "Serra and Ramalingam [2020] proposed valid inequalities...for each l = 2, . . . , L and i = 1, . . . , nl, for which, wl 0i ≤ 0...they proposed the valid inequality in (19)."
- Break condition: If bound propagation is too conservative or expensive to compute, valid inequalities cannot be derived or are too weak.

### Mechanism 3
- Claim: MIP formulations enable exact adversarial example generation for trained neural networks.
- Mechanism: By modeling the trained ReLU network as a MIP, an optimization problem can be formulated to find minimal perturbations to an input that cause misclassification, subject to constraints that enforce the desired incorrect output class.
- Core assumption: The trained network is fixed and the goal is to find worst-case inputs within a bounded perturbation space.
- Evidence anchors:
  - [section 4.2] "We introduce an intentional misclassification...To enforce this condition, a constraint is imposed in the final layer...xL d+1 ≥ 1.2xL j ∀j ∈ {0, . . . ,9} \ {d}"
  - [section 4.2] "To minimize the L1-norm between x0 and ˜x0, the objective function Pn0 j=1 dj is used."
- Break condition: If the network is too large or deep, the MIP becomes intractable and solver cannot find solutions within time limits.

## Foundational Learning

- Concept: Mixed-integer programming (MIP) formulation of piecewise linear functions
  - Why needed here: ReLU activations are piecewise linear, so understanding how to encode them exactly in MIP is fundamental to modeling trained neural networks.
  - Quick check question: How does the big-M formulation enforce the ReLU constraint xl = max(0, wlxl-1) using binary variables?

- Concept: Valid inequalities and bound propagation
  - Why needed here: Strengthening MIP formulations via valid inequalities derived from network structure is critical for scalability; understanding interval arithmetic for bound computation is key.
  - Quick check question: What is the condition under which a ReLU unit can be declared "stably inactive" in the extended formulation?

- Concept: Adversarial example generation as optimization
  - Why needed here: The paper uses MIP to find minimal perturbations that cause misclassification, which is a concrete application of MIP for neural network analysis.
  - Quick check question: What constraint enforces that the target incorrect class has activation at least 20% higher than all other classes in the adversarial example formulation?

## Architecture Onboarding

- Component map: Data preprocessing -> Network training -> MIP formulation -> Solver optimization -> Adversarial example generation
- Critical path: For a new engineer: (a) Understand ReLU-MIP formulations, (b) Implement bound computation routines, (c) Code a basic MIP model for a small trained network, (d) Add valid inequalities, (e) Test on a simple classification task.
- Design tradeoffs: Big-M is simple but can have weak LP relaxations; convex hull is tight but has exponential constraints; extended formulation adds variables but is often more tractable; disjunctive formulation balances size and strength.
- Failure signatures: Solver reports "infeasible" likely due to overly tight big-M bounds; solver runs out of memory/time on wider networks due to exponential growth in variables/constraints; poor solution quality if bounds are not computed accurately.
- First 3 experiments:
  1. Implement and test the big-M formulation on a tiny ReLU network (e.g., 784-20-20-10-10-10-10-10) with known weights, verify exact representation.
  2. Add bound propagation and valid inequalities, compare LP relaxation gap before/after.
  3. Use the extended formulation to generate an adversarial example on the same tiny network, measure minimal perturbation found.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can MIP formulations be scaled to handle larger and deeper neural network architectures efficiently?
- Basis in paper: [explicit] The paper discusses the limitations of MIP formulations for larger networks, noting that as networks get wider and deeper, the number of constraints and variables grows exponentially, making it computationally expensive to find optimal solutions.
- Why unresolved: Current MIP solvers struggle with the exponential growth in complexity as network size increases, and there is a lack of efficient methods to handle this scaling issue.
- What evidence would resolve it: Development of new MIP formulations or optimization techniques that can handle larger networks efficiently, possibly through innovative constraint reduction methods or parallel processing strategies.

### Open Question 2
- Question: What are the potential benefits and drawbacks of using MIP formulations for training neural networks compared to traditional gradient-based methods?
- Basis in paper: [explicit] The paper mentions that MIP-based training eliminates the need for extensive hyperparameter tuning and could theoretically find optimal solutions, but it is not well-suited for training due to becoming bilinear in this context.
- Why unresolved: There is limited research on the practical application of MIP for training neural networks, and the trade-offs between MIP and gradient-based methods are not fully understood.
- What evidence would resolve it: Empirical studies comparing the performance, convergence, and scalability of MIP-based training versus gradient-based methods on various neural network architectures and datasets.

### Open Question 3
- Question: How can valid inequalities be further developed and integrated into MIP formulations to improve the efficiency of neural network verification and adversarial example generation?
- Basis in paper: [explicit] The paper discusses the role of valid inequalities in strengthening MIP formulations for ReLU-based neural networks, which can enhance verification and adversarial example generation.
- Why unresolved: While valid inequalities have been proposed, there is a need for more comprehensive and universally applicable inequalities that can be integrated into MIP formulations to improve their efficiency and effectiveness.
- What evidence would resolve it: Identification and implementation of new valid inequalities that significantly reduce the computational complexity of MIP formulations in neural network verification and adversarial example generation tasks.

## Limitations
- Scalability limitations for deeper and wider networks due to exponential growth in constraints and variables
- Effectiveness of valid inequalities depends heavily on accurate and computationally efficient bound propagation
- Choice of MIP formulation involves fundamental tradeoffs between LP relaxation strength and computational tractability

## Confidence
- High confidence: The fundamental mechanisms for encoding ReLU activations as MIP constraints (Mechanism 1) are well-established and directly supported by the paper's equations and formulations.
- Medium confidence: The effectiveness of valid inequalities for strengthening LP relaxations (Mechanism 2) is supported by theoretical discussion but limited computational validation across diverse network architectures.
- Medium confidence: The application to adversarial example generation (Mechanism 3) is demonstrated on small networks but the generalizability to more complex scenarios and larger networks remains uncertain.

## Next Checks
1. Test bound propagation algorithms (Cheng et al. [2017] vs. Tjeng et al. [2018]) on a wider range of network architectures to quantify their impact on MIP solver performance and solution quality.
2. Implement and evaluate the disjunctive formulation with different partitioning strategies (equal-size vs. equal-range) to determine which approach yields better computational efficiency for networks of varying widths.
3. Apply the MIP formulations to a different dataset (e.g., CIFAR-10) with multiple network architectures to assess scalability and generalization beyond the MNIST benchmark used in the paper.