---
ver: rpa2
title: Boundary Exploration for Bayesian Optimization With Unknown Physical Constraints
arxiv_id: '2402.07692'
source_url: https://arxiv.org/abs/2402.07692
tags:
- design
- optimization
- function
- constraints
- evaluations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces BE-CBO, a new method for Bayesian optimization
  with unknown physical constraints, where the optimal solution often lies on the
  boundary between feasible and infeasible regions. The key idea is to use an ensemble
  of neural networks to model the constraint boundary and explore it efficiently.
---

# Boundary Exploration for Bayesian Optimization With Unknown Physical Constraints

## Quick Facts
- arXiv ID: 2402.07692
- Source URL: https://arxiv.org/abs/2402.07692
- Authors: Yunsheng Tian; Ane Zuniga; Xinwei Zhang; Johannes P. Dürholt; Payel Das; Jie Chen; Wojciech Matusik; Mina Konaković Luković
- Reference count: 40
- Primary result: BE-CBO outperforms state-of-the-art methods on 12 benchmark problems for Bayesian optimization with unknown physical constraints.

## Executive Summary
This paper introduces BE-CBO, a novel method for Bayesian optimization with unknown physical constraints where optimal solutions typically lie on the boundary between feasible and infeasible regions. The key innovation is using an ensemble of neural networks to model the constraint boundary and explore it efficiently. BE-CBO achieves superior performance compared to state-of-the-art methods (CEI, SCBO, SVM-CBO) across 12 benchmark problems including synthetic functions and real-world engineering design problems. The method demonstrates particular effectiveness for practical problems with complex constraints and shows robustness across different problem dimensions.

## Method Summary
BE-CBO uses Deep Ensembles (5 MLPs with 64⌊log2(d)⌋ neurons per hidden layer) to model constraint boundaries and a dynamic boundary constraint l(x) = 0.5 - σ_E(x) for exploration. The method combines a Gaussian Process surrogate for the objective function with an ensemble classifier for constraints, using Expected Improvement with constrained optimization. The algorithm runs for 200 iterations with 10 initial random samples, dynamically adjusting the exploration region based on ensemble uncertainty to focus on the boundary between feasible and infeasible regions.

## Key Results
- BE-CBO outperforms CEI, SCBO, and SVM-CBO on 12 benchmark problems (3 synthetic, 9 real-world engineering)
- Achieves superior objective values and feasibility ratios across varying problem dimensions (2D to 30D)
- Particularly effective for practical problems with complex constraints where optimal solutions lie on feasibility boundaries
- Demonstrates robustness and consistent performance gains over baselines

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Deep ensembles outperform Gaussian Processes for modeling complex constraint boundaries.
- **Mechanism:** Deep ensembles leverage multiple independently trained neural networks to capture epistemic uncertainty, providing a more accurate approximation of complex, non-convex feasibility regions than Gaussian Processes.
- **Core assumption:** Complex constraint boundaries in real-world problems are not well-approximated by smooth Gaussian Process kernels.
- **Evidence anchors:** Deep ensembles outperform standard Gaussian Processes for capturing complex boundaries.
- **Break condition:** When constraints are simple and convex, Gaussian Processes may be sufficient and more computationally efficient.

### Mechanism 2
- **Claim:** The dynamic boundary constraint l(x) = 0.5 - σ_E(x) effectively balances exploration of infeasible regions while preventing excessive sampling of truly infeasible designs.
- **Mechanism:** By incorporating the ensemble's prediction uncertainty into the lower bound, the algorithm adaptively widens the exploration region around the boundary when uncertainty is high, and narrows it as the model becomes more confident.
- **Core assumption:** The uncertainty in the constraint model directly correlates with the model's accuracy in classifying the feasibility boundary.
- **Evidence anchors:** Dynamic bound formulation incorporates classifier uncertainty into selection strategy.
- **Break condition:** When ensemble uncertainty is consistently low, the dynamic bound becomes too restrictive and may miss important infeasible regions.

### Mechanism 3
- **Claim:** Focusing the search around the boundary between feasible and infeasible regions leads to better objective values than exploring the entire design space.
- **Mechanism:** Since optimal solutions in problems with unknown physical constraints often lie on the boundary, explicitly exploring this region with a constrained acquisition function increases the likelihood of finding better designs.
- **Core assumption:** In problems with unknown physical constraints, the global optimum is typically located on the boundary between feasible and infeasible regions.
- **Evidence anchors:** Optimal solution typically lies on the boundary between feasible and infeasible regions.
- **Break condition:** When the optimal solution is in the interior of the feasible region, this boundary-focused approach may miss it.

## Foundational Learning

- **Concept:** Bayesian Optimization fundamentals (surrogate modeling, acquisition functions, exploration-exploitation tradeoff)
  - Why needed here: BE-CBO builds upon standard BO framework with modifications for constraint handling
  - Quick check question: What is the purpose of the acquisition function in Bayesian Optimization?

- **Concept:** Gaussian Processes and their limitations for classification
  - Why needed here: Understanding why GPs are insufficient for complex constraint boundaries
  - Quick check question: What are the main limitations of Gaussian Processes for modeling non-convex classification boundaries?

- **Concept:** Deep Ensembles and uncertainty quantification
  - Why needed here: Deep ensembles are the core mechanism for modeling constraints in BE-CBO
  - Quick check question: How do Deep Ensembles capture epistemic uncertainty differently from Bayesian Neural Networks?

## Architecture Onboarding

- **Component map:** Data → Train Deep Ensemble classifier → Optimize acquisition function with boundary constraints → Evaluate sample → Update surrogates → Repeat

- **Critical path:** Initial random samples → Deep Ensemble training → Constrained EI optimization → Sample evaluation → Surrogate updates → Boundary exploration

- **Design tradeoffs:**
  - Deep Ensembles vs GPs: Better accuracy for complex boundaries vs computational efficiency
  - Boundary exploration vs safe exploration: Better objective values vs higher risk of infeasible samples
  - Dynamic bounds vs fixed bounds: Adaptive exploration vs simpler implementation

- **Failure signatures:**
  - Poor constraint classification accuracy → incorrect boundary exploration → suboptimal solutions
  - Ensemble uncertainty consistently low → overly restrictive exploration → missing important regions
  - Objective surrogate model poorly trained → acquisition function optimization fails

- **First 3 experiments:**
  1. Run BE-CBO on 2D Townsend function and visualize constraint boundary classification accuracy over iterations
  2. Compare BE-CBO with GP-based constraint modeling on a simple 2D problem to demonstrate accuracy differences
  3. Test different numbers of MLPs in the ensemble (3, 5, 8) on a benchmark problem to find optimal ensemble size

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the accuracy of constraint boundary modeling by Deep Ensembles compare to other state-of-the-art constraint modeling methods in BO, such as Bayesian neural networks or Gaussian processes with different kernel functions?
- **Basis in paper:** The paper states that Deep Ensembles outperform Gaussian Processes for capturing complex boundaries, but it does not compare to other methods like Bayesian neural networks or alternative GP kernels.
- **Why unresolved:** The paper only compares Deep Ensembles to Gaussian Processes, leaving the relative performance of other methods unexplored.
- **What evidence would resolve it:** Comparative experiments evaluating Deep Ensembles against Bayesian neural networks and GPs with various kernel functions on the same benchmark problems.

### Open Question 2
- **Question:** What is the impact of incorporating domain-specific knowledge or prior information about the constraint structure into the Deep Ensemble classifier?
- **Basis in paper:** The paper uses a generic Deep Ensemble architecture without leveraging any domain-specific information or prior constraints.
- **Why unresolved:** The paper does not explore whether incorporating domain knowledge could improve constraint modeling and optimization performance.
- **What evidence would resolve it:** Experiments comparing the performance of BE-CBO with and without domain-specific knowledge incorporated into the Deep Ensemble architecture or training process.

### Open Question 3
- **Question:** How does the performance of BE-CBO scale with the dimensionality of the design space, and what are the limitations of the method for high-dimensional problems?
- **Basis in paper:** The paper tests BE-CBO on problems up to 30 dimensions but does not provide a systematic analysis of its scalability or identify limitations for high-dimensional problems.
- **Why unresolved:** The paper does not investigate the scaling behavior of BE-CBO or provide insights into its limitations for high-dimensional problems.
- **What evidence would resolve it:** Experiments evaluating BE-CBO on problems with varying dimensions, beyond 30, and analyzing its performance trends and limitations.

### Open Question 4
- **Question:** How sensitive is the performance of BE-CBO to the choice of hyperparameters, such as the number of MLPs in the ensemble, the number of hidden layers, and the learning rate?
- **Basis in paper:** The paper conducts ablation studies on some hyperparameters but does not provide a comprehensive sensitivity analysis.
- **Why unresolved:** The paper does not explore the full range of hyperparameters or their interactions, leaving the sensitivity of BE-CBO's performance unclear.
- **What evidence would resolve it:** A systematic sensitivity analysis varying multiple hyperparameters simultaneously and evaluating their impact on BE-CBO's performance across different benchmark problems.

## Limitations

- Computational overhead from training 5 neural networks per iteration may be prohibitive for high-dimensional problems or real-time applications
- Performance relies on the assumption that optimal solutions typically lie on the boundary between feasible and infeasible regions, which may not hold for all problem types
- The method uses fixed hyperparameters (5 MLPs, specific architecture) without systematic sensitivity analysis or optimization

## Confidence

**High confidence:** BE-CBO outperforms CEI, SCBO, and SVM-CBO on benchmark problems based on experimental results
**Medium confidence:** Deep Ensembles are superior to Gaussian Processes for modeling complex constraint boundaries
**Low confidence:** The dynamic boundary constraint l(x) = 0.5 - σ_E(x) is the optimal formulation for balancing exploration and exploitation

## Next Checks

1. **Boundary assumption validation:** Test BE-CBO on problems with optimal solutions in both interior and boundary regions to quantify performance degradation when the boundary assumption is violated
2. **Ensemble size sensitivity analysis:** Systematically vary the number of MLPs (3, 5, 8, 10) across multiple benchmarks to identify optimal ensemble size and understand accuracy-efficiency trade-offs
3. **Computational efficiency comparison:** Measure wall-clock time per iteration for BE-CBO versus baselines across problems of varying dimensions to quantify practical computational overhead