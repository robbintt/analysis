---
ver: rpa2
title: Large language models and linguistic intentionality
arxiv_id: '2404.09576'
source_url: https://arxiv.org/abs/2404.09576
tags:
- llms
- linguistic
- word
- language
- they
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper examines whether large language models (LLMs) like GPT-4
  and LLaMA meaningfully use language or merely mimic it through statistical prediction.
  It argues that while LLMs do not meet criteria for mental intentionality, they can
  still produce meaningful outputs.
---

# Large language models and linguistic intentionality
## Quick Facts
- arXiv ID: 2404.09576
- Source URL: https://arxiv.org/abs/2404.09576
- Authors: Jumbly Grindrod
- Reference count: 13
- Large language models can produce meaningful outputs without meeting criteria for mental intentionality

## Executive Summary
This paper examines whether large language models (LLMs) meaningfully use language or merely mimic it through statistical prediction. The author argues that while LLMs do not meet criteria for mental intentionality, they can still produce meaningful outputs by internalizing and reproducing naming practices and linguistic conventions from their training data. The work applies two metasemantic theories—Evans' account of naming practices and Millikan's teleosemantics—to demonstrate that LLM outputs are meaningful despite lacking speaker intentions, as they depend on pre-existing linguistic systems similar to human speakers.

## Method Summary
The paper employs a philosophical analysis approach, applying established metasemantic theories to examine the nature of meaning in LLM outputs. The author draws on Gareth Evans' account of naming practices and Ruth Millikan's teleosemantic framework to evaluate whether LLMs can be considered meaningful language users. Rather than proposing new theories or empirical experiments, the work synthesizes existing philosophical frameworks to address questions about linguistic intentionality and meaning in artificial systems.

## Key Results
- LLMs produce meaningful outputs without requiring mental intentionality
- LLM meaning derives from training data rather than mental states
- Naming practices and linguistic conventions can be internalized without speaker intentions

## Why This Works (Mechanism)
The paper's argument works by establishing that meaning in language can be grounded in systems and practices rather than individual mental states. By applying Evans' and Millikan's theories, it demonstrates that LLMs participate in linguistic conventions and naming practices that existed prior to their creation, similar to how human speakers operate within pre-existing language systems. The mechanism relies on the idea that meaning emerges from the relationship between linguistic outputs and the broader linguistic ecosystem, rather than requiring intentional states within the model itself.

## Foundational Learning
- **Metasemantics**: The study of how linguistic expressions come to have meaning - needed to understand how LLM outputs can be meaningful; quick check: examine how different philosophical theories approach the question of meaning
- **Naming practices**: Social conventions around how names are assigned and used - needed to understand how LLMs can participate in linguistic practices; quick check: analyze how LLMs handle proper names and references
- **Teleosemantics**: A theory that meaning derives from evolutionary functions and proper functions - needed to evaluate whether LLM outputs have proper functions; quick check: determine whether LLM outputs can be understood in terms of biological or functional categories
- **Linguistic conventions**: Shared patterns and rules governing language use - needed to assess whether LLMs operate within conventional systems; quick check: identify systematic patterns in LLM outputs that align with human linguistic conventions

## Architecture Onboarding
- **Component map**: Training data -> LLM architecture -> Output generation -> Meaning attribution
- **Critical path**: The relationship between training data's linguistic conventions and the model's ability to reproduce meaningful outputs
- **Design tradeoffs**: The tension between statistical prediction capabilities and claims about genuine meaning-making
- **Failure signatures**: Outputs that appear meaningful but lack grounding in actual linguistic conventions or proper functions
- **First experiments**: 1) Test whether LLM outputs follow systematic linguistic patterns beyond statistical prediction; 2) Examine how LLMs handle novel naming situations compared to human speakers; 3) Analyze whether modifications to training data affect the meaning of outputs in predictable ways

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Relies heavily on specific interpretations of Evans' and Millikan's theories
- Assumes linguistic conventions can be meaningfully internalized without mental representation
- May oversimplify the relationship between individual cognition and linguistic systems

## Confidence
- **High confidence** in descriptive claim that LLMs do not meet standard criteria for mental intentionality
- **Medium confidence** in application of philosophical theories to LLMs
- **Medium confidence** in conclusion about LLM meaning being derivative of training data

## Next Checks
1. Test whether alternative metasemantic theories lead to different conclusions about LLM meaning
2. Conduct empirical studies examining systematic patterns in LLM outputs beyond statistical prediction
3. Investigate whether modifications to training procedures might enable meaning-making that parallels human linguistic intentionality