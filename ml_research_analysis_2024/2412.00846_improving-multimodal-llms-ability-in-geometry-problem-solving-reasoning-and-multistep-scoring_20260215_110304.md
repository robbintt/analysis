---
ver: rpa2
title: Improving Multimodal LLMs Ability In Geometry Problem Solving, Reasoning, And
  Multistep Scoring
arxiv_id: '2412.00846'
source_url: https://arxiv.org/abs/2412.00846
tags: []
core_contribution: This paper introduces GPSM4K, a multimodal geometry dataset designed
  to enhance the problem-solving capabilities of Large Vision Language Models (LVLMs).
  GPSM4K includes 2,157 manually extracted question-answer pairs from mathematics
  textbooks, covering grades 7-12, and is augmented to 5,340 problems featuring both
  numerical and theorem-proving questions.
---

# Improving Multimodal LLMs Ability In Geometry Problem Solving, Reasoning, And Multistep Scoring

## Quick Facts
- **arXiv ID**: 2412.00846
- **Source URL**: https://arxiv.org/abs/2412.00846
- **Reference count**: 40
- **Primary result**: GPSM4K dataset significantly improves geometry problem-solving in LLaVA and G-LLaVA models

## Executive Summary
This paper introduces GPSM4K, a multimodal geometry dataset designed to enhance the problem-solving capabilities of Large Vision Language Models (LVLMs). GPSM4K includes 2,157 manually extracted question-answer pairs from mathematics textbooks, covering grades 7-12, and is augmented to 5,340 problems featuring both numerical and theorem-proving questions. The dataset provides detailed step-by-step solutions, offering a structured approach for comprehensive evaluation. Experiments demonstrate that fine-tuning LVLMs on GPSM4K significantly improves their performance in geometry problem-solving compared to existing datasets like PGPS9K. The study also evaluates the effectiveness of techniques such as image captioning and Retrieval-Augmented Generation (RAG), showing improvements in model accuracy and reasoning capabilities.

## Method Summary
The authors created GPSM4K by manually extracting geometry problems from textbooks (grades 7-12) and augmenting them to 5,340 problems with step-by-step solutions. They fine-tuned LLaVA and G-LLaVA models on this dataset using a learning rate of 3e-5, batch size of 4 per GPU, and 2 epochs. The evaluation involved testing on a held-out set of 150 problems, using both final answer accuracy and chain-of-thought step-by-step evaluation through the Gemini Pro API. The study also compared performance against existing datasets (PGPS9K, Geometry3K) and evaluated image captioning techniques and RAG approaches for improving geometric reasoning.

## Key Results
- Fine-tuning LLaVA and G-LLaVA on GPSM4K significantly improves geometry problem-solving accuracy compared to baseline models
- GPSM4K outperforms existing datasets (PGPS9K, Geometry3K) in geometry problem-solving tasks
- Image captioning and RAG techniques further enhance model performance and reasoning capabilities
- Chain-of-thought evaluation strategy provides deeper insights into model reasoning compared to final answer evaluation alone

## Why This Works (Mechanism)
The effectiveness of GPSM4K stems from its comprehensive coverage of geometry problems spanning grades 7-12, with both numerical and theorem-proving questions. The dataset's step-by-step solutions provide structured learning pathways that help models understand the logical progression of geometric reasoning. By including detailed intermediate steps, the dataset enables models to learn not just the final answers but the complete reasoning process, which is crucial for geometry problem-solving. The manual extraction from textbooks ensures high-quality, curriculum-aligned problems that reflect real educational content.

## Foundational Learning
- **Geometry problem types**: Understanding the distinction between numerical and theorem-proving problems is essential for effective dataset design and model evaluation. Quick check: Verify dataset contains balanced representation of both problem types.
- **Step-by-step reasoning**: Geometric problem-solving requires sequential logical steps, making intermediate solution steps critical for model learning. Quick check: Confirm step-by-step solutions follow logical progression without gaps.
- **Multimodal integration**: Geometry problems require combining visual diagram interpretation with textual mathematical reasoning. Quick check: Evaluate model's ability to correctly interpret geometric diagrams and extract relevant information.

## Architecture Onboarding
**Component map**: Textbook problems -> Manual extraction -> Dataset augmentation -> Model fine-tuning -> Evaluation (final answer + chain-of-thought)
**Critical path**: GPSM4K dataset creation → LLaVA/G-LLaVA fine-tuning → Chain-of-thought evaluation with Gemini Pro
**Design tradeoffs**: Manual extraction ensures quality but limits dataset size; step-by-step solutions provide comprehensive learning but increase annotation complexity
**Failure signatures**: Poor performance indicates inadequate geometric concept understanding; image captioning failures suggest insufficient visual feature extraction
**First experiments**: 1) Fine-tune base LLaVA on GPSM4K subset 2) Compare chain-of-thought vs final answer evaluation 3) Test RAG integration with baseline model

## Open Questions the Paper Calls Out
None

## Limitations
- Manual dataset extraction introduces potential selection bias toward certain problem types or difficulty levels
- Limited evaluation of model robustness to variations in problem presentation and diagram complexity
- Focus on accuracy metrics without extensive analysis of long-term retention and generalization capabilities

## Confidence
- **High confidence**: Effectiveness of fine-tuning LLaVA and G-LLaVA models on GPSM4K for geometry problem-solving accuracy improvements
- **Medium confidence**: Superiority of GPSM4K over existing datasets (PGPS9K, Geometry3K) for geometry problem-solving tasks
- **Medium confidence**: Effectiveness of image captioning and RAG techniques in improving model performance, though specific implementation details would strengthen this claim

## Next Checks
1. Conduct ablation studies to isolate the contribution of GPSM4K's unique features (step-by-step solutions, theorem-proving problems) from general fine-tuning benefits
2. Test model performance on geometry problems with varying diagram complexity and presentation styles not present in the training dataset
3. Evaluate the long-term retention and generalization capabilities of models fine-tuned on GPSM4K by testing after extended periods and on novel problem types