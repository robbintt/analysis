---
ver: rpa2
title: A Taxonomy of Ambiguity Types for NLP
arxiv_id: '2403.14072'
source_url: https://arxiv.org/abs/2403.14072
tags:
- ambiguity
- language
- neutral
- types
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new taxonomy of eleven ambiguity types
  for natural language processing, refining and extending previous work. The authors
  propose a framework covering lexical, syntactic, scopal, elliptical, collective/distributive,
  implicative, presuppositional, idiomatic, coreferential, generic/non-generic, and
  type/token ambiguity.
---

# A Taxonomy of Ambiguity Types for NLP

## Quick Facts
- arXiv ID: 2403.14072
- Source URL: https://arxiv.org/abs/2403.14072
- Authors: Margaret Y. Li; Alisa Liu; Zhaofeng Wu; Noah A. Smith
- Reference count: 6
- Primary result: Proposes a taxonomy of 11 ambiguity types for NLP with systematic application to the AMBI ENT dataset

## Executive Summary
This paper introduces a comprehensive taxonomy of eleven ambiguity types specifically designed for natural language processing applications. The taxonomy refines and extends previous linguistic work by providing a framework that covers lexical, syntactic, scopal, elliptical, collective/distributive, implicative, presuppositional, idiomatic, coreferential, generic/non-generic, and type/token ambiguity. The authors demonstrate the taxonomy's utility by applying it to categorize examples from the AMBI ENT dataset, showing how different ambiguity types yield characteristic entailment label distributions. This enables more fine-grained analysis of ambiguity in datasets and models, facilitating the development of nuanced benchmarks that better capture the diversity of ambiguity types in natural language.

## Method Summary
The authors propose a taxonomy of eleven distinct ambiguity types and apply it to annotate examples from the AMBI ENT dataset, which captures ambiguities through entailment relations. Each example in AMBI ENT has ambiguity in either the premise or hypothesis and multiple possible labels depending on how the ambiguity is resolved. The method involves defining clear criteria for each ambiguity type, manually annotating a subset of AMBI ENT examples, and analyzing how different ambiguity types affect entailment label distributions. The ultimate goal is to create a more balanced dataset by adding examples for underrepresented ambiguity types and to analyze model performance across different types to identify the most challenging categories for targeted interventions.

## Key Results
- The taxonomy successfully categorizes examples from AMBI ENT into distinct ambiguity types with different characteristic entailment label distributions
- Different ambiguity types pose varying levels of difficulty for language models, suggesting opportunities for targeted evaluation formats
- The framework enables systematic analysis of ambiguity that was previously unavailable in modern NLP benchmarks
- Lexical, syntactic, and scopal ambiguities are well-represented in existing datasets, while presuppositional, implicative, and type/token ambiguities are underrepresented

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The taxonomy enables fine-grained analysis by mapping ambiguous phenomena to distinct entailment outcomes.
- Mechanism: Each ambiguity type produces characteristic entailment label distributions when disambiguated, allowing systematic performance breakdowns.
- Core assumption: Disambiguated variants of ambiguous examples will consistently yield different entailment labels across all instances of a given type.
- Evidence anchors:
  - [abstract] "The taxonomy's utility by categorizing examples from the AMBI ENT dataset, showing how different ambiguity types yield different entailment labels."
  - [section 4] "We also will analyze how models perform across different types. For the most difficult types, we hope to create targeted tasks or evaluation formats."
  - [corpus] Weak evidence - no direct citation of entailment-based evaluation in related papers, though AmbigNLG mentions ambiguity taxonomy.
- Break condition: If a particular ambiguity type produces inconsistent or overlapping entailment label distributions across examples, the taxonomy's discriminative power diminishes.

### Mechanism 2
- Claim: The taxonomy fills a gap in modern NLP by providing systematic coverage of ambiguity phenomena beyond semantic parsing.
- Mechanism: By covering 11 distinct types spanning lexical, syntactic, scopal, and pragmatic dimensions, the framework enables comprehensive ambiguity analysis previously unavailable in benchmark datasets.
- Core assumption: Modern NLP benchmarks have systematically under-represented certain ambiguity types (particularly presuppositional, implicative, and type/token).
- Evidence anchors:
  - [abstract] "no comprehensive taxonomy of ambiguity types has been created for or applied to analyzing ambiguity in the modern NLP context."
  - [section 1] "Recent work suggests that NLP systems may struggle to grasp certain elements of human language understanding because they may not handle ambiguities at the level that humans naturally do in communication."
  - [corpus] Moderate evidence - AmbigNLG also addresses task ambiguity in NLG, suggesting this is an active research area, but with different focus.
- Break condition: If downstream evaluation shows that the proposed types don't capture meaningful distinctions in model performance or dataset construction.

### Mechanism 3
- Claim: The taxonomy supports targeted model improvement by identifying specific failure modes.
- Mechanism: By categorizing examples, researchers can identify which ambiguity types most challenge current models and design targeted interventions or evaluation formats for those types.
- Core assumption: Different ambiguity types will produce varying difficulty levels for current language models, enabling prioritization of research efforts.
- Evidence anchors:
  - [abstract] "For the most difficult types, we hope to create targeted tasks or evaluation formats."
  - [section 4] "We also will analyze how models perform across different types."
  - [corpus] No direct evidence - the corpus contains related work on taxonomies but none specifically on targeted model improvement based on ambiguity type.
- Break condition: If all ambiguity types prove equally difficult or easy for models, eliminating the need for type-specific interventions.

## Foundational Learning

- Concept: Entailment relationships and NLI task structure
  - Why needed here: The taxonomy is demonstrated through examples from AMBI ENT, which uses natural language inference format where ambiguity affects entailment labels.
  - Quick check question: Given "The speaker is at the front of the room" (premise) and "There is a loudspeaker in the room" (hypothesis), what would the correct label be if "speaker" means the person vs. the device?

- Concept: Scoping and quantifier scope ambiguity
  - Why needed here: Scopal ambiguity is explicitly called out as a distinct category requiring different resolution approaches.
  - Quick check question: In "Every student read two poems," what's the difference between "every > two" vs. "two > every" scope interpretations?

- Concept: Pragmatic implicature and presupposition
  - Why needed here: Implicative and presuppositional ambiguities involve pragmatic meaning beyond literal content, requiring understanding of conversational principles.
  - Quick check question: What presupposition does "too" trigger in "Jane left early too" depending on context?

## Architecture Onboarding

- Component map: Taxonomy definition module -> Annotation pipeline -> Evaluation framework -> Targeted task generator
- Critical path: Taxonomy definition → Example annotation → Performance analysis → Targeted intervention development
- Design tradeoffs: Comprehensive coverage vs. practical annotability; linguistic precision vs. NLP applicability; fine-grained distinctions vs. statistical significance in small datasets
- Failure signatures: Inconsistent labeling across annotators; negligible performance differences across types; inability to create sufficient examples for certain types
- First 3 experiments:
  1. Annotate AMBI ENT examples with the 11-type taxonomy and verify distinct entailment label distributions
  2. Measure baseline model performance across types to identify most challenging categories
  3. Create minimal type-specific evaluation formats for top 2 most difficult ambiguity types and evaluate again

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we create a systematic method for annotating the frequency of different ambiguity types in natural language datasets like AMBI ENT?
- Basis in paper: [explicit] The authors propose applying their taxonomy to annotate AMBI ENT to estimate relative frequencies of ambiguity types.
- Why unresolved: The paper describes the intention to annotate but does not provide a concrete methodology for how to systematically identify and categorize ambiguities in text.
- What evidence would resolve it: A detailed annotation protocol or guidelines for identifying and labeling different ambiguity types in natural language data.

### Open Question 2
- Question: What specific challenges do different ambiguity types pose for language models, and how can we measure these differences in model performance?
- Basis in paper: [explicit] The authors state that different types of ambiguity may pose different challenges and require different approaches for resolution.
- Why unresolved: While the taxonomy distinguishes different ambiguity types, the paper does not investigate how language models perform across these types or what specific challenges each type presents.
- What evidence would resolve it: Empirical studies comparing language model performance on examples from different ambiguity types, identifying specific failure modes for each type.

### Open Question 3
- Question: How can we create targeted evaluation formats for the most difficult ambiguity types identified in language models?
- Basis in paper: [explicit] The authors mention creating "targeted tasks or evaluation formats" for the most difficult types after analyzing model performance.
- Why unresolved: The paper does not specify what these targeted evaluation formats would look like or how they would differ from existing evaluation methods.
- What evidence would resolve it: Concrete proposals for new evaluation tasks or formats specifically designed to test language models' ability to handle the most challenging ambiguity types.

## Limitations
- The taxonomy's practical utility depends on consistent annotation across examples, but inter-annotator agreement statistics are limited
- Some ambiguity types may be difficult to distinguish in practice (e.g., lexical vs. syntactic ambiguity, or generic/non-generic vs. type/token ambiguity)
- The framework's downstream impact on model performance improvement remains to be empirically validated

## Confidence
- **High confidence**: The claim that ambiguity types can produce different entailment label distributions when disambiguated. This is directly demonstrated in the paper's AMBI ENT analysis and follows from basic entailment theory.
- **Medium confidence**: The claim that modern NLP benchmarks systematically under-represent certain ambiguity types. While plausible given the focus on semantic parsing in prior work, the paper doesn't provide comprehensive benchmarking data to quantify this gap.
- **Medium confidence**: The claim that type-specific interventions can improve model performance. The mechanism is sound, but empirical validation is limited to planned future work rather than demonstrated results.

## Next Checks
1. Conduct inter-annotator agreement study on a sample of AMBI ENT examples to quantify annotation consistency across the 11 types and identify which distinctions are most challenging.
2. Perform ablation analysis to determine whether all 11 types contribute unique information to model performance prediction, or whether some can be merged without loss of discriminative power.
3. Implement a pilot targeted evaluation for the two most challenging ambiguity types identified in the AMBI ENT analysis, measuring whether type-specific evaluation formats reveal performance differences not captured by standard benchmarks.