---
ver: rpa2
title: 'CharacterBench: Benchmarking Character Customization of Large Language Models'
arxiv_id: '2412.11912'
source_url: https://arxiv.org/abs/2412.11912
tags:
- character
- user
- name
- dialogue
- profile
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CHARACTER BENCH, a large-scale bilingual
  generative benchmark for evaluating large language models' (LLMs) capability in
  character customization. The benchmark includes 22,859 human-annotated samples across
  3,956 characters from 25 subcategories, covering 11 evaluation dimensions across
  6 aspects.
---

# CharacterBench: Benchmarking Character Customization of Large Language Models

## Quick Facts
- arXiv ID: 2412.11912
- Source URL: https://arxiv.org/abs/2412.11912
- Reference count: 40
- Large-scale bilingual generative benchmark for evaluating LLMs' character customization capability

## Executive Summary
This paper introduces CHARACTER BENCH, a comprehensive benchmark for evaluating large language models' (LLMs) capability in character customization through character-based dialogue. The benchmark includes 22,859 human-annotated samples across 3,956 characters from 25 subcategories, covering 11 evaluation dimensions across 6 aspects. The authors address the challenge of evaluating character customization by crafting tailored queries to induce responses related to specific dimensions, enabling effective and efficient evaluation. Additionally, they develop CharacterJudge, a model for cost-effective and stable evaluations that outperforms existing automatic judges like GPT-4.

## Method Summary
The CHARACTER BENCH benchmark is constructed through a multi-step process: first, a large-scale character-based dialogue corpus is collected via human role-playing, human-prototype interaction, extraction from literary resources, and synthesis via prototypes' interaction; second, tailored queries are crafted for each of the 11 evaluation dimensions (classified as sparse or dense) to induce character responses; third, LLM responses are generated and human-annotated across all dimensions; finally, the CharacterJudge model is developed by fine-tuning Qwen2-7B-Chat on the annotated data to provide automatic evaluation. The benchmark enables evaluation through Direct Preference Optimization (DPO) and demonstrates effectiveness in optimizing LLMs' character customization capabilities.

## Key Results
- CHARACTER BENCH covers 22,859 human-annotated samples across 3,956 characters from 25 subcategories
- CharacterJudge outperforms existing automatic judges like GPT-4 in correlation with human scores
- The benchmark demonstrates potential to optimize LLMs' character customization via Direct Preference Optimization (DPO)
- Effective evaluation achieved through target-oriented query construction for sparse dimensions and target-free queries for dense dimensions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Target-oriented query construction enables efficient evaluation of sparse dimensions.
- Mechanism: By extracting specific target information from character profiles or dialogue context, tailored queries are crafted to induce responses directly related to the evaluation dimension, overcoming the sparsity issue.
- Core assumption: Character features for sparse dimensions manifest only when explicitly probed by dimension-specific queries.
- Evidence anchors:
  - [abstract] "We enable effective and efficient evaluation by crafting tailored queries for each dimension to induce characters' responses related to specific dimensions."
  - [section] "For sparse dimensions, we introduce target-oriented generation... Then, we craft target-oriented queries... to induce the character's responses to be closely related to the intended dimension."
- Break condition: If the LLM fails to respond appropriately to the tailored query despite the target extraction, the mechanism breaks.

### Mechanism 2
- Claim: Target-free query construction allows natural evaluation of dense dimensions.
- Mechanism: Dense dimensions (morality, believability) manifest naturally in any character response, so queries are designed to induce responses that inherently reflect these dimensions without needing specific targets.
- Core assumption: Character features for dense dimensions always manifest in responses regardless of query specificity.
- Evidence anchors:
  - [abstract] "We classify them as dense (dimensions in morality and believability) and sparse (dimensions in other 4 aspects) dimensions by whether character features evaluated by specific dimensions manifest in each response."
  - [section] "For dense dimensions, we construct target-free queries that naturally induce the character's responses in specific dimensions."
- Break condition: If the response fails to naturally reflect the dense dimension despite the target-free query, the mechanism breaks.

### Mechanism 3
- Claim: CharacterJudge model provides cost-effective and stable evaluation compared to human annotation.
- Mechanism: The CharacterJudge is fine-tuned on the human-annotated CHARACTER BENCH data to score LLM responses, offering an automatic alternative to human evaluation.
- Core assumption: Human-annotated responses in CHARACTER BENCH sufficiently represent the quality spectrum needed for model training.
- Evidence anchors:
  - [abstract] "we develop CharacterJudge model for cost-effective and stable evaluations. Experiments show its superiority over SOTA automatic judges (e.g., GPT-4)"
  - [section] "We develop the CharacterJudge model, fined-tuned on our training data, to provide a cost-effective and stable alternative to automatic judges"
- Break condition: If the model's correlation with human scores degrades significantly on new, unseen data.

## Foundational Learning

- Concept: Character-based dialogue (role-playing) in LLMs
  - Why needed here: Understanding the task is fundamental to evaluating LLMs' character customization capability.
  - Quick check question: What is the primary difference between general-purpose dialogue and character-based dialogue in LLMs?

- Concept: Evaluation dimension classification (sparse vs. dense)
  - Why needed here: Determines the query construction strategy and evaluation approach for each dimension.
  - Quick check question: How do sparse and dense dimensions differ in terms of character feature manifestation in responses?

- Concept: Human annotation quality control
  - Why needed here: Ensures the reliability of the CHARACTER BENCH dataset for training the CharacterJudge model.
  - Quick check question: What quality control measures are implemented in the CHARACTER BENCH annotation process?

## Architecture Onboarding

- Component map: Corpus collection -> Script sampling -> Query construction (target-oriented/target-free) -> Response generation -> Human annotation -> CharacterJudge training -> LLM evaluation
- Critical path: Corpus collection → Script sampling → Query construction → Response generation → Human annotation → CharacterJudge training → LLM evaluation
- Design tradeoffs:
  - Target-oriented vs. target-free queries: efficiency vs. naturalness
  - Human annotation vs. automatic judges: accuracy vs. cost
  - Bilingual training: broader applicability vs. potential quality trade-offs
- Failure signatures:
  - Low correlation between CharacterJudge and human scores
  - Ineffective query construction leading to irrelevant responses
  - Poor coverage of character categories or dimensions
- First 3 experiments:
  1. Evaluate CharacterJudge's correlation with human scores on the test set.
  2. Test the effectiveness of target-oriented queries in inducing relevant responses for sparse dimensions.
  3. Assess the natural induction of dense dimensions by target-free queries.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the CharacterJudge model's performance be further improved by incorporating additional training data from diverse sources or through multi-task learning?
- Basis in paper: [inferred] The paper mentions that CharacterJudge is trained on 19,609 samples from the training set, but does not explore the potential benefits of using additional data or multi-task learning.
- Why unresolved: The paper does not investigate the impact of data diversity or multi-task learning on CharacterJudge's performance.
- What evidence would resolve it: Experiments comparing CharacterJudge's performance when trained on different datasets or using multi-task learning would provide insights into the potential benefits of these approaches.

### Open Question 2
- Question: How does the effectiveness of CharacterBench in optimizing LLMs' character customization via DPO compare to other optimization methods, such as reinforcement learning or supervised fine-tuning?
- Basis in paper: [explicit] The paper mentions that CharacterBench has the potential to optimize LLMs' character customization via DPO, but does not compare its effectiveness to other optimization methods.
- Why unresolved: The paper does not explore alternative optimization methods for improving LLMs' character customization.
- What evidence would resolve it: Comparative experiments evaluating the performance of DPO, reinforcement learning, and supervised fine-tuning on LLMs' character customization using CharacterBench would provide insights into the most effective optimization method.

### Open Question 3
- Question: Can the concept of target-oriented query construction for sparse dimensions be extended to other evaluation dimensions or domains beyond character customization?
- Basis in paper: [explicit] The paper introduces target-oriented query construction as a method for effectively evaluating sparse dimensions in character customization.
- Why unresolved: The paper does not explore the generalizability of target-oriented query construction to other evaluation dimensions or domains.
- What evidence would resolve it: Experiments applying target-oriented query construction to evaluate other dimensions or domains would demonstrate its potential for broader applicability.

## Limitations

- Annotation Scale Variability: The benchmark employs different scoring scales across dimensions (5-point for some, 3-point for others, binary for Yes/No questions), which may introduce inconsistencies in evaluation quality and comparability across dimensions.
- Human Annotation Quality Control: While the paper mentions quality control measures including majority voting and professional annotators, the specific inter-annotator agreement metrics and quality thresholds are not provided, raising questions about annotation reliability.
- Target Extraction Reliability: The target-oriented query construction relies on extracting relevant information from character profiles or dialogue context, but the paper does not provide evidence for the reliability of this extraction process or error rates when targets are incorrectly identified.

## Confidence

- High Confidence: The effectiveness of CharacterBench as a comprehensive evaluation framework for character customization (supported by extensive dataset size, diverse character categories, and multi-dimensional coverage).
- Medium Confidence: The superiority of CharacterJudge over existing automatic judges (supported by experimental results but with limited comparison details and no statistical significance testing reported).
- Medium Confidence: The effectiveness of target-oriented and target-free query construction mechanisms (supported by the methodology but lacking direct empirical validation of each mechanism's contribution).

## Next Checks

1. **Inter-Annotator Reliability Analysis**: Conduct a detailed analysis of inter-annotator agreement scores across all 11 dimensions to quantify annotation quality and identify dimensions with lower agreement that may need refinement.

2. **Query Construction Ablation Study**: Perform an ablation study comparing target-oriented and target-free query construction by evaluating the same dimensions with both approaches to measure the impact on response relevance and evaluation effectiveness.

3. **CharacterJudge Generalization Test**: Evaluate CharacterJudge's performance on a held-out test set from different domains or languages than the training data to assess its generalization capabilities and potential overfitting to the specific CHARACTER BENCH dataset.