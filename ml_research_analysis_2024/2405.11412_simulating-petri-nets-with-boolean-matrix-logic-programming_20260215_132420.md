---
ver: rpa2
title: Simulating Petri nets with Boolean Matrix Logic Programming
arxiv_id: '2405.11412'
source_url: https://arxiv.org/abs/2405.11412
tags:
- boolean
- places
- matrix
- nets
- petri
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of efficiently simulating Petri
  nets using logic programming, particularly when dealing with extensive networks.
  The authors introduce a novel approach called Boolean Matrix Logic Programming (BMLP)
  that leverages low-level boolean matrices for computing logic programs.
---

# Simulating Petri nets with Boolean Matrix Logic Programming

## Quick Facts
- arXiv ID: 2405.11412
- Source URL: https://arxiv.org/abs/2405.11412
- Reference count: 18
- One-line primary result: BMLP methods achieve 40x speedup over tabled Prolog and ASP systems for simulating one-bounded elementary nets

## Executive Summary
This paper introduces Boolean Matrix Logic Programming (BMLP), a novel approach for efficiently simulating Petri nets using logic programming. The authors address the challenge of scaling Petri net simulation to extensive networks by leveraging boolean matrices as an alternative computation mechanism for Prolog. By compiling linear and immediately recursive datalog programs into boolean incidence matrices and processing them using boolean matrix algebra, BMLP achieves significant runtime improvements over traditional tabled Prolog systems and ASP solvers.

## Method Summary
The paper presents two BMLP algorithms for simulating one-bounded elementary nets (OENs): iterative extension (BMLP-IE) and repeated matrix squaring (BMLP-RMS). The approach involves transforming OENs into equivalent datalog programs, then compiling these programs into boolean incidence matrices. BMLP-IE computes reachability through iterative bitwise operations on sparse matrices, while BMLP-RMS achieves logarithmic time complexity through repeated boolean matrix squaring. Both methods are implemented in SWI-Prolog and evaluated against established tabled Prolog systems (B-Prolog, SWI-Prolog, XSB-Prolog) and ASP system Clingo.

## Key Results
- BMLP methods demonstrate 40× speedup compared to tabled Prolog systems and ASP solvers
- BMLP-IE performs close to O(n³) bitwise operations when transition probability is small, showing slower runtime growth
- BMLP-RMS runtime reduces when transition probability exceeds 0.01 due to fewer required squaring operations
- Experimental evaluation on airplane flight routes and genome-scale metabolic networks validates the approach's effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BMLP reduces runtime by replacing symbolic logic evaluation with boolean matrix operations.
- Mechanism: The paper compiles linear and immediately recursive datalog programs into boolean incidence matrices. These matrices are then processed using boolean matrix algebra (AND for multiplication, OR for addition) to compute transitive closures. This shifts computation from high-level symbolic inference to low-level bit operations.
- Core assumption: The logic program can be represented as boolean matrices without loss of semantics, and matrix multiplication captures the recursive inference.
- Evidence anchors:
  - [abstract]: "utilising boolean matrices as an alternative computation mechanism for Prolog to evaluate logic programs"
  - [section 5]: "We compile a datalog program P, we first create mappings between constant symbols and integer numbers... Each v(i, bi) is the i-th row of the boolean matrix and bi is a binary code"
  - [corpus]: Weak; no direct neighbor paper confirms this exact mechanism, though GPU-based BMLP is mentioned.
- Break condition: If the datalog program contains recursion patterns not expressible as linear recurrences, the boolean matrix compilation fails or produces incorrect results.

### Mechanism 2
- Claim: Iterative extension (BMLP-IE) efficiently computes reachability from a specific marking by traversing the boolean matrix row-by-row.
- Mechanism: BMLP-IE represents the initial marking as a 1×n vector and the program's ground facts as two k×n matrices (R1 for first arguments, R2 for second). It repeatedly performs bitwise AND to find derivable facts and OR to accumulate results until transitive closure is reached.
- Core assumption: The sparsity of the boolean matrix (low probability of transitions) keeps the number of iterations small, bounding runtime.
- Evidence anchors:
  - [section 5.2]: "We use the bitwise AND to locate rows in R1 that contain all the 1 elements in v... We iterate this process until we find the transitive closure"
  - [section 6.1]: "When the probability pt is small, Proposition 1 says that BMLP-IE performs close to O(n3) bitwise operations. This is shown by BMLP-IE's slower runtime growth"
  - [corpus]: No direct confirmation; neighboring work focuses on GPU acceleration, not this traversal method.
- Break condition: If the graph is dense (high transition probability), the number of iterations approaches n, making BMLP-IE slower than matrix squaring.

### Mechanism 3
- Claim: Repeated matrix squaring (BMLP-RMS) achieves logarithmic time complexity for computing all-pairs reachability.
- Mechanism: BMLP-RMS compiles the program into a single n×n boolean matrix and repeatedly squares it using boolean algebra. By exploiting the identity (I+R)^2k = (I+R)^(2k), it reduces the number of multiplications from O(n) to O(log n).
- Core assumption: The transitive closure can be computed by repeated squaring because boolean matrix multiplication is associative and idempotent.
- Evidence anchors:
  - [section 5.2]: "Equation (3) performs boolean matrix operations on the same elements... we take a similar approach to the logarithmic technique in Equation (2) to skip computations by repeatedly squaring matrix products"
  - [section 6.1]: "BMLP-RMS runtime even reduces when pt > 0.01... fewer squaring operations are needed to reach transitive closure"
  - [corpus]: Weak; no neighbor paper verifies this exact approach, though the general idea of matrix-based reachability is common.
- Break condition: If the matrix becomes too large (n > few thousand), the O(n³ log n) cost dominates, and memory becomes a bottleneck.

## Foundational Learning

- Concept: Boolean matrix algebra (AND for multiplication, OR for addition).
  - Why needed here: The entire speedup relies on replacing logical inference with matrix multiplication over {0,1}.
  - Quick check question: What is the result of multiplying two boolean matrices A and B where A[i][k]=1 and B[k][j]=1?

- Concept: Transitive closure and its equivalence to least Herbrand model for linear recursive programs.
  - Why needed here: The paper's correctness proof (Theorem 1) hinges on this equivalence.
  - Quick check question: In a linear recursive program, what does the transitive closure of the incidence matrix represent?

- Concept: Tabling in Prolog and its role in preventing non-termination.
  - Why needed here: The baseline comparisons use tabled Prolog; understanding tabling explains why those systems terminate.
  - Quick check question: How does tabling avoid infinite recursion in left-recursive clauses?

## Architecture Onboarding

- Component map:
  - Compiler: Maps OEN → linear recursive datalog → boolean matrices
  - BMLP-IE engine: Iterative bitwise traversal for single-source reachability
  - BMLP-RMS engine: Logarithmic repeated squaring for all-pairs reachability
  - Runtime harness: SWI-Prolog interface, timing collection, result serialization

- Critical path:
  1. Parse OEN → generate datalog clauses
  2. Compile clauses → boolean matrices (incidence + vector)
  3. Execute BMLP algorithm (IE or RMS)
  4. Deserialize matrix → Prolog facts for output

- Design tradeoffs:
  - BMLP-IE: Lower memory (k×n matrices) but potentially more iterations; best for sparse graphs
  - BMLP-RMS: Higher memory (n×n matrix) but fewer iterations; best for dense graphs
  - Compilation overhead: Writing matrices as Prolog facts adds fixed cost; more significant for small problems

- Failure signatures:
  - MemoryError: n too large for RMS (n³ log n space/time)
  - Infinite loop: Incorrect matrix compilation (e.g., missing identity)
  - Wrong results: Non-linear recursion in input program not handled

- First 3 experiments:
  1. Verify compilation: Input OEN with 3 places, 2 transitions → check generated matrices match expected boolean codes
  2. Baseline comparison: Run BMLP-IE on sparse OEN vs. SWI-Prolog with tabling; confirm 40× speedup claim
  3. Scaling test: Vary n from 1000 to 5000 with pt=0.001; plot runtime vs. n for BMLP-IE and BMLP-RMS to observe crossover point

## Open Questions the Paper Calls Out
- Can the BMLP framework be extended to handle more complex classes of Petri nets beyond one-bounded elementary nets (OENs)?
- How does the performance of BMLP methods scale with extremely large datasets containing millions of nodes and transitions?
- Can BMLP be effectively combined with parallel processing techniques to further enhance its performance?
- How can BMLP be adapted to handle probabilistic Petri nets and uncertain transitions?

## Limitations
- The boolean matrix compilation only handles linear recursive datalog programs, excluding more complex recursion patterns
- BMLP-RMS has O(n³ log n) time and memory requirements, limiting scalability for very large Petri nets
- The approach is specifically designed for one-bounded elementary nets and may not generalize to arbitrary Petri net types

## Confidence

- High: Runtime speedup claims (40× improvement)
- Medium: Correctness of compilation process from OEN to boolean matrices
- Low: General applicability to arbitrary logic programs beyond one-bounded elementary nets

## Next Checks

1. Test BMLP with a non-linear recursive datalog program to verify that it fails gracefully or reports an error
2. Run BMLP-RMS on an OEN with n > 10,000 to observe the memory and time constraints empirically
3. Compare the boolean matrix compilation output against a manually constructed correct matrix for a small OEN to verify the compiler's accuracy