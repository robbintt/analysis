---
ver: rpa2
title: 'Attack On Prompt: Backdoor Attack in Prompt-Based Continual Learning'
arxiv_id: '2406.19753'
source_url: https://arxiv.org/abs/2406.19753
tags:
- backdoor
- learning
- continual
- page
- trigger
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses backdoor attacks on prompt-based continual
  learning in multi-data supplier scenarios, where the attacker aims to manipulate
  the model to misclassify poisoned samples as a target class while maintaining normal
  performance on clean samples. The proposed AOP framework tackles three key challenges:
  ensuring transferability to unknown data via surrogate dataset manipulation, maintaining
  resilience against incremental learning through dynamic trigger optimization, and
  preventing adversarial noise via binary cross-entropy loss.'
---

# Attack On Prompt: Backdoor Attack in Prompt-Based Continual Learning

## Quick Facts
- arXiv ID: 2406.19753
- Source URL: https://arxiv.org/abs/2406.19753
- Authors: Trang Nguyen; Anh Tran; Nhat Ho
- Reference count: 40
- One-line primary result: Achieves up to 100% attack success rate with negligible impact on clean accuracy in prompt-based continual learning

## Executive Summary
This paper addresses backdoor attacks on prompt-based continual learning in multi-data supplier scenarios. The proposed AOP framework tackles three key challenges: ensuring transferability to unknown data via surrogate dataset manipulation, maintaining resilience against incremental learning through dynamic trigger optimization, and preventing adversarial noise via binary cross-entropy loss. The attack exploits visual prompting's label mapping property to manipulate which prompts are selected for triggered samples, effectively remapping labels between datasets.

## Method Summary
The AOP framework consists of three stages: (1) static prompt tuning on a surrogate dataset to capture label mapping relationships, (2) trigger optimization using binary cross-entropy loss with dynamic rounds to align with incremental learning states, and (3) transition and dynamic surrogate stages to enhance trigger resilience. The attack uses a pre-trained ViT-B/16 backbone and tests various prompt-based continual learners (L2P, DualPrompt, HiDe-Prompt, CODA-Prompt, PGP) across datasets like ImageNet-R variants, CUB200, and TinyImageNet.

## Key Results
- Achieves up to 100% attack success rate on triggered samples
- Maintains negligible impact on clean sample accuracy
- Effective across various datasets and continual learning algorithms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transferability of backdoor knowledge is achieved through surrogate dataset manipulation and prompt selection.
- Mechanism: The attack exploits visual prompting's label mapping property by manipulating which prompts are selected for triggered samples, effectively remapping labels between datasets.
- Core assumption: Prompts act as universal input perturbation templates that can map labels from source to target datasets.
- Evidence anchors:
  - [abstract]: "employ a surrogate dataset and manipulate prompt selection to transfer backdoor knowledge to data from other suppliers"
  - [section]: "prompts function as universal input perturbation templates, enabling the mapping of labels from a source dataset to a target dataset"
- Break condition: If the surrogate dataset is completely disjoint from the victim's data distribution, the label mapping may fail to transfer effectively.

### Mechanism 2
- Claim: Backdoor trigger resilience is maintained through static-dynamic optimization that simulates the victim's incremental learning states.
- Mechanism: The attack alternates between static stages (preparing label mappings) and dynamic stages (aligning with continuous learning updates) to ensure the trigger remains effective throughout the learning process.
- Core assumption: The surrogate learner's static and dynamic states can adequately simulate the victim's behavior during incremental learning.
- Evidence anchors:
  - [abstract]: "simulate static and dynamic states of the victim to ensure the backdoor trigger remains robust during intense incremental learning processes"
  - [section]: "static state that reflects how prompts learn label mappings... and a dynamic state that reflects the continuous learning procedure"
- Break condition: If the victim's learning rate or architecture significantly differs from the surrogate, the dynamic alignment may fail.

### Mechanism 3
- Claim: Authenticity of the backdoor trigger is preserved by using binary cross-entropy loss with sigmoid to prevent adversarial noise transformation.
- Mechanism: BCE with sigmoid avoids the competition between classes induced by softmax, independently optimizing target class scores without suppressing others.
- Core assumption: Softmax with cross-entropy introduces target class score bias that can transform triggers into adversarial perturbations.
- Evidence anchors:
  - [abstract]: "apply binary cross-entropy loss as an anti-cheating factor to prevent the backdoor trigger from devolving into adversarial noise"
  - [section]: "Softmax introduces competition between classes... the trigger to act like adversarial noise"
- Break condition: If BCE optimization is too weak to overcome the model's natural classification tendencies, the backdoor may not be effective.

## Foundational Learning

- Concept: Prompt-based continual learning
  - Why needed here: Understanding how prompts are selected and used for instruction is critical to the attack mechanism
  - Quick check question: How does the top-K cosine similarity mechanism in prompt selection work?

- Concept: Continual learning and catastrophic forgetting
  - Why needed here: The attack must work despite the victim's ability to forget previously learned knowledge
  - Quick check question: What distinguishes class-incremental learning from other continual learning settings?

- Concept: Backdoor attacks and trigger optimization
  - Why needed here: The attack leverages traditional backdoor principles but adapts them for the continual learning context
  - Quick check question: What is the difference between clean-label and dirty-label backdoor attacks?

## Architecture Onboarding

- Component map: Surrogate dataset → Static stage (label mapping) → Trigger optimization → Dynamic stage (continuous alignment) → Backdoored model
- Critical path: Data poisoning → Prompt selection manipulation → Static-dynamic optimization → BCE loss application
- Design tradeoffs: High transferability vs. potential adversarial noise generation; resilience vs. computational cost of dynamic optimization
- Failure signatures: Trigger losing effectiveness over time; model achieving high accuracy but low ASR; trigger being detected as adversarial noise
- First 3 experiments:
  1. Test backdoor effectiveness with varying surrogate dataset choices
  2. Evaluate trigger resilience across different numbers of dynamic optimization rounds
  3. Compare BCE vs. CE loss impact on trigger authenticity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of AOP vary when using different pre-trained model architectures beyond ViT, such as ConvNets or other transformer variants?
- Basis in paper: [inferred] The paper uses ViT-B/16 as the backbone for all experiments and mentions that HiDe-Prompt uses a different pre-trained model (iBOT-1K), but does not systematically explore other architectures.
- Why unresolved: The paper only tests one type of transformer architecture and one alternative pre-trained model, leaving open whether AOP's effectiveness generalizes to other architectures.
- What evidence would resolve it: Systematic experiments testing AOP on a diverse set of pre-trained models (ConvNets, different transformer variants) with quantitative comparison of ASR and ACC.

### Open Question 2
- Question: What is the minimum poisoning rate required for AOP to maintain high ASR across long sequences of tasks, and how does this vary with different surrogate dataset qualities?
- Basis in paper: [explicit] The paper tests poisoning rates up to 0.5% and notes that maintaining backdoor effectiveness with low poisoning rates is essential, but does not explore the minimum threshold or surrogate dataset quality impact in depth.
- Why unresolved: The paper demonstrates effectiveness at 0.1% poisoning rate but does not systematically explore lower bounds or the relationship between surrogate dataset quality and minimum poisoning rate.
- What evidence would resolve it: Experiments varying poisoning rates below 0.01% and systematically comparing surrogate datasets of different quality/distribution similarity to the target data.

### Open Question 3
- Question: Can AOP be adapted to work effectively in online continual learning scenarios where data arrives in a streaming fashion rather than in discrete task batches?
- Basis in paper: [inferred] The paper focuses on class-incremental learning with discrete task arrivals and does not address streaming/online scenarios where the model must update continuously without clear task boundaries.
- Why unresolved: The static-dynamic optimization framework relies on discrete task boundaries and may not translate directly to continuous streaming scenarios where prompt updates and trigger optimization need to be more fluid.
- What evidence would resolve it: Implementation and testing of AOP in an online continual learning framework with continuous data streams, measuring ASR and ACC performance compared to the discrete task setting.

## Limitations
- Exact prompt pool sizes and configurations vary across learner variants without complete specification
- The query function q(x), cosine similarity threshold γ, and projection operation for trigger constraint are not fully detailed
- While BCE loss is claimed to prevent adversarial noise transformation, the paper doesn't extensively validate this against sophisticated defense mechanisms

## Confidence

- **High Confidence**: The framework's three-component structure (surrogate manipulation, dynamic optimization, BCE loss) is clearly articulated and experimentally validated
- **Medium Confidence**: The mechanism for prompt selection manipulation and label mapping is logically sound but relies on assumptions about prompt universality that may not hold across all dataset pairs
- **Medium Confidence**: The dynamic optimization approach for maintaining trigger resilience shows promise but may struggle with significant architectural differences between surrogate and victim models

## Next Checks

1. Test backdoor effectiveness with completely disjoint surrogate datasets to verify the limits of label mapping transferability
2. Evaluate trigger resilience when the victim's learning rate or architecture differs substantially from the surrogate model's configuration
3. Conduct adversarial defense testing to confirm that BCE loss truly prevents the trigger from devolving into adversarial noise that could be detected by STRIP-style entropy analysis