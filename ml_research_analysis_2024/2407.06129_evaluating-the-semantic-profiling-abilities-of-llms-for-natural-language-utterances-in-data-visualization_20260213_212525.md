---
ver: rpa2
title: Evaluating the Semantic Profiling Abilities of LLMs for Natural Language Utterances
  in Data Visualization
arxiv_id: '2407.06129'
source_url: https://arxiv.org/abs/2407.06129
tags:
- data
- llms
- utterances
- visualization
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates four LLMs (GPT-4, Gemini-Pro, Llama3, Mixtral)
  for their ability to extract semantic information from natural language utterances
  for data visualization generation. The authors compiled a corpus of 500 utterances
  from two existing datasets and manually annotated them for uncertainties, data attributes/transformations,
  and visualization tasks.
---

# Evaluating the Semantic Profiling Abilities of LLMs for Natural Language Utterances in Data Visualization

## Quick Facts
- arXiv ID: 2407.06129
- Source URL: https://arxiv.org/abs/2407.06129
- Reference count: 40
- Primary result: Evaluates four LLMs for extracting semantic information from natural language utterances for data visualization, finding LLMs are sensitive to uncertainties, can identify data columns/transformations reasonably well, but struggle with inferring visualization tasks.

## Executive Summary
This paper evaluates four large language models (GPT-4, Gemini-Pro, Llama3, Mixtral) for their ability to extract semantic information from natural language utterances for data visualization generation. The authors compiled a corpus of 500 utterances from two existing datasets and manually annotated them for uncertainties, data attributes/transformations, and visualization tasks. Through systematic evaluation using few-shot prompting, the study reveals that LLMs are sensitive to uncertainties in utterances, can identify relevant data columns and transformations reasonably well, but struggle with inferring visualization tasks. The research highlights the current strengths and limitations of LLMs for visualization generation and suggests future research directions.

## Method Summary
The study compiled a corpus of 500 natural language utterances from NLV-Corpus and Quda datasets, manually annotating them for three dimensions: uncertainties, data attributes/transformations, and visualization tasks. Four LLMs were evaluated using few-shot prompting with manually annotated samples. The evaluation measured LLM performance across the three dimensions by comparing LLM outputs to human annotations. The authors assessed accuracy in identifying uncertainties, relevant data columns, data transformations, and visualization tasks through a systematic comparison process.

## Key Results
- LLMs are sensitive to uncertainties in natural language utterances, detecting more ambiguities than human annotators
- LLMs can identify relevant data columns and transformations reasonably well, but often produce code with errors
- LLMs struggle with inferring visualization tasks, with 50.4% of visual tasks in total disagreement with human annotations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs are sensitive to uncertainties in natural language utterances, detecting more ambiguities than human annotators.
- Mechanism: LLMs interpret utterances at a finer level of abstraction, flagging potential ambiguities and under-specifications that humans overlook due to contextual inference.
- Core assumption: Human annotators rely on contextual knowledge and domain understanding to resolve ambiguities, while LLMs treat each word/phrase as a potential source of uncertainty.
- Evidence anchors:
  - [abstract] "Our findings reveal that LLMs are sensitive to uncertainties in utterances."
  - [section] "We observe that all LLMs identified a higher proportion of uncertainty in the utterances than those identified by the human annotators."
- Break condition: If LLMs are prompted with explicit context or domain knowledge, they may start resolving ambiguities similarly to humans.

### Mechanism 2
- Claim: LLMs can identify relevant data columns and transformations reasonably well but often produce code with errors.
- Mechanism: LLMs use pattern matching and statistical correlations in training data to map natural language to data column names and generate transformation code, but lack deep semantic understanding of data schemas.
- Core assumption: The training data includes examples of natural language paired with data column names and transformation operations, allowing LLMs to learn approximate mappings.
- Evidence anchors:
  - [abstract] "they are able to extract the relevant data context."
  - [section] "LLMs are able to correctly infer relevant data columns for most utterances."
- Break condition: If the dataset schema is highly unusual or the utterance uses non-standard terminology, LLMs may fail to identify correct columns or transformations.

### Mechanism 3
- Claim: LLMs struggle to infer visualization tasks because they lack explicit task taxonomies in their training data.
- Mechanism: LLMs generate responses based on statistical patterns in text but do not have an inherent understanding of visualization task taxonomies, leading to mismatches between inferred and intended tasks.
- Core assumption: Visualization task inference requires explicit knowledge of task taxonomies (e.g., Amar et al., Munzner) which may not be well-represented in general LLM training data.
- Evidence anchors:
  - [abstract] "However, LLMs struggle with inferring visualization tasks."
  - [section] "50.4% of the visual tasks were in total disagreement."
- Break condition: If the utterance explicitly mentions a visualization type or task, LLMs may perform better due to pattern matching.

## Foundational Learning

### Concept: Natural Language Interfaces (NLIs) for data visualization
- Why needed here: Understanding how NLIs translate natural language into visualization specifications is crucial for evaluating LLM performance.
- Quick check question: What are the key challenges NLIs face when interpreting natural language utterances for visualization generation?

### Concept: Data transformation and wrangling
- Why needed here: LLMs must correctly identify and generate code for data transformations to prepare data for visualization.
- Quick check question: What are common data transformation operations needed for visualization tasks (e.g., grouping, filtering, pivoting)?

### Concept: Visualization task taxonomies
- Why needed here: Proper evaluation of LLM performance requires understanding standard visualization task classifications (e.g., Amar et al., Munzner).
- Quick check question: How do established visualization task taxonomies (e.g., Amar et al., Munzner) classify user intents in data analysis?

## Architecture Onboarding

### Component map
Utterance → LLM Prompt → LLM Response (JSON) → Annotation Evaluation → Performance Metrics

### Critical path
Prompt design → LLM response generation → Annotation comparison → Performance analysis

### Design tradeoffs
Few-shot prompting allows complex task learning but increases prompt size; open-source LLMs offer flexibility but may have lower performance than proprietary models.

### Failure signatures
High uncertainty detection but low task inference accuracy; code generation errors; inconsistent JSON formatting.

### First 3 experiments
1. Evaluate LLM performance on a subset of utterances with clear, unambiguous tasks to establish baseline accuracy.
2. Test different prompt engineering strategies (e.g., few-shot examples, explicit task taxonomies) to improve task inference.
3. Analyze error patterns in generated code to identify common failure modes and develop mitigation strategies.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLMs' sensitivity to uncertainties in natural language utterances be leveraged to facilitate deeper data exploration and analysis in visualization systems?
- Basis in paper: [explicit] The paper notes that LLMs identify more uncertainties than human annotators and suggests this sensitivity could be used to pose questions to analysts and help them think deeply about their analysis questions or approach.
- Why unresolved: While the paper identifies this potential, it does not explore specific methods or evaluate how this approach would work in practice.
- What evidence would resolve it: Empirical studies demonstrating improved data exploration outcomes when LLMs' uncertainty identification is incorporated into visualization systems, along with specific techniques for leveraging these uncertainties.

### Open Question 2
- Question: What techniques can improve LLMs' ability to correctly infer visualization tasks from natural language utterances?
- Basis in paper: [explicit] The paper finds that LLMs struggle to correctly infer visualization tasks and suggests this is important as tasks inform visualization design choices.
- Why unresolved: The paper identifies the problem but does not propose or test solutions for improving task inference.
- What evidence would resolve it: Development and evaluation of improved prompting strategies, fine-tuning approaches, or task-specific training that demonstrate enhanced task inference accuracy in LLMs.

### Open Question 3
- Question: How can the generation of relevant code for data transformations in visualization contexts be improved in LLMs?
- Basis in paper: [explicit] The paper observes that while LLMs can identify relevant data columns and transformations, many generated transformations contain errors or violate instructions.
- Why unresolved: Current approaches of prompting for multiple scripts and filtering errors are mentioned but not evaluated or improved upon in the paper.
- What evidence would resolve it: Comparative studies of different error-reduction techniques for LLM-generated visualization code, including feedback mechanisms and prompt engineering strategies that show measurable improvements in code accuracy.

## Limitations

- The study relies on manual annotation for ground truth, which introduces potential subjectivity and bias in the evaluation.
- The evaluation uses a fixed dataset schema (Basketball and IMDB) that may not generalize to more complex or diverse data structures.
- The study does not address the computational cost and latency of using LLMs for visualization generation in real-time applications.

## Confidence

**High Confidence**: The findings on LLMs' sensitivity to uncertainties in utterances are well-supported by the comparison with human annotations and the systematic evaluation across multiple LLMs.

**Medium Confidence**: The results showing LLMs' reasonable ability to identify relevant data columns and transformations are supported by the quantitative metrics, but the high rate of code execution errors suggests limitations in practical applicability.

**Low Confidence**: The claim about LLMs struggling with visualization task inference is based on the high disagreement rate, but the study does not provide sufficient analysis of why this occurs or how it might be mitigated.

## Next Checks

1. Conduct a user study comparing human interpretations of utterance ambiguities with LLM-identified uncertainties to validate the claimed sensitivity difference and explore potential false positives.

2. Test the LLMs on a broader range of dataset schemas and utterance types, including domain-specific examples and highly nested data transformations, to assess generalization beyond the Basketball and IMDB datasets.

3. Implement a prototype system that integrates LLM-generated visualization specifications with actual visualization libraries (e.g., Altair, Matplotlib) to measure end-to-end success rates and identify failure patterns in the visualization generation pipeline.