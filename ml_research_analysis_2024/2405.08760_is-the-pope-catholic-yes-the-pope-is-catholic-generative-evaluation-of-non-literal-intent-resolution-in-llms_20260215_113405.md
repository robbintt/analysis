---
ver: rpa2
title: Is the Pope Catholic? Yes, the Pope is Catholic. Generative Evaluation of Non-Literal
  Intent Resolution in LLMs
arxiv_id: '2405.08760'
source_url: https://arxiv.org/abs/2405.08760
tags:
- language
- intention
- llms
- response
- non-literal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a generative evaluation framework for assessing
  large language models' ability to understand and respond to non-literal language.
  The framework compares model-generated responses to counterfactual dialogues based
  on true and literal interpretations, using similarity metrics to measure pragmatic
  understanding.
---

# Is the Pope Catholic? Yes, the Pope is Catholic. Generative Evaluation of Non-Literal Intent Resolution in LLMs

## Quick Facts
- arXiv ID: 2405.08760
- Source URL: https://arxiv.org/abs/2405.08760
- Authors: Akhila Yerukola; Saujas Vaduguru; Daniel Fried; Maarten Sap
- Reference count: 36
- Models achieve only 50-55% accuracy in generating contextually appropriate responses to non-literal language

## Executive Summary
This paper introduces a novel generative evaluation framework for assessing large language models' ability to understand and respond to non-literal language. Unlike prior work that focused on discriminative intention detection, this approach evaluates whether models can generate contextually appropriate responses by comparing their outputs to counterfactual dialogues based on true and literal interpretations. The framework uses similarity metrics to measure pragmatic understanding across four non-literal language phenomena: indirect speech, irony, flouted conversational maxims, and metaphor.

The experiments reveal a substantial gap between models' ability to detect intentions and their capacity to use them in pragmatic response generation. While explicitly providing oracle intentions significantly improves performance (up to 75% for Mistral-Instruct), this still indicates challenges in leveraging given intentions to produce appropriate responses. The findings highlight the need for better mechanisms to model and leverage communicative intentions in language models, particularly for complex phenomena like irony and flouted conversational maxims.

## Method Summary
The framework generates counterfactual gold dialog chains using GPT-4 based on true and incorrect literal intentions, then evaluates LLMs' responses using similarity metrics comparing generated responses to these reference dialogues. The method tests several state-of-the-art open-source LLMs on four non-literal language phenomena through chain-of-thought prompting with varying levels of oracle cues, from no oracle information to full oracle intention and phenomenon specification.

## Key Results
- Models achieve only 50-55% accuracy in generating contextually appropriate responses to non-literal language
- Explicitly providing oracle intentions significantly improves performance (75% for Mistral-Instruct)
- Chain-of-thought prompting provides marginal improvements, suggesting fundamental limitations in current LLMs' ability to leverage explicit reasoning for pragmatic tasks
- Models struggle particularly with irony and flouted conversational maxims

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generative evaluation framework enables measuring pragmatic response accuracy beyond discriminative detection
- Mechanism: By comparing model-generated responses to counterfactual dialogues based on true and literal interpretations using similarity metrics, the framework assesses whether models respond in line with the speaker's true intention rather than literal meaning
- Core assumption: Contextual appropriateness of responses can be captured through similarity comparisons between generated responses and reference dialogues
- Evidence anchors:
  - [abstract] "Our primary focus on pragmatic response generation marks a departure from prior work...which has predominantly measured intention understanding through discriminative contrastive multiple-choice classification"
  - [section 2] "Ideally, if LLMs can accurately infer and use the intent to generate cooperative responses using direct language, they should respond as if the non-literal utterance was instead communicated literally"
- Break condition: If similarity metrics fail to capture nuanced pragmatic appropriateness or if models exploit superficial patterns in reference responses

### Mechanism 2
- Claim: Explicitly providing oracle intentions significantly improves pragmatic response generation
- Mechanism: When models receive true intentions directly, they can leverage this information to generate more contextually appropriate responses, achieving 75% accuracy for Mistral-Instruct
- Core assumption: Models have sufficient capability to utilize provided intentions effectively in response generation
- Evidence anchors:
  - [abstract] "While explicitly providing oracle intentions significantly improves performance (e.g., 75% for Mistral-Instruct), this still indicates challenges in leveraging given intentions to produce appropriate responses"
  - [section 4] "Specifying the type of non-literal language used along with the speaker's true intent (Prompt 6) significantly improves the model's ability to generate appropriate responses"
- Break condition: If models continue to struggle with utilizing provided intentions even when explicitly given, indicating deeper architectural limitations

### Mechanism 3
- Claim: Chain-of-thought prompting with varying levels of oracle cues systematically improves pragmatic response generation
- Mechanism: By incrementally providing oracle information, the framework tests how much hand-holding is needed for models to generate appropriate responses
- Core assumption: Models can learn to better disentangle linguistic strategies from communicative intent through structured reasoning
- Evidence anchors:
  - [section 4] "We observe a slight improvement in performance (on average) when no oracle information is provided (Prompt 0) or when prompted for counterfactual reasoning regarding the non-literal expression (Prompt 1)"
  - [section 4] "Providing explicit cues about the phenomenon (e.g., 'Kelly is being ironic' vs. 'Is Kelly being ironic?') help slightly (Prompts 2-4)"
- Break condition: If incremental oracle information does not show consistent improvement patterns, suggesting models cannot effectively utilize structured reasoning

## Foundational Learning

- Concept: Pragmatic inference and Gricean maxims
  - Why needed here: Understanding how humans interpret non-literal language through contextual and conventional factors is crucial for evaluating models' pragmatic understanding
  - Quick check question: What are the four conversational maxims proposed by Grice that speakers typically follow, and how do flouted maxims lead to implicatures?

- Concept: Discriminative vs. generative evaluation settings
  - Why needed here: The paper contrasts models' performance in multiple-choice intention detection versus open-ended pragmatic response generation, highlighting different challenges
  - Quick check question: Why might models perform better in discriminative settings compared to generative settings for non-literal language understanding?

- Concept: Chain-of-thought prompting and oracle information
  - Why needed here: The paper experiments with CoT prompting and varying levels of oracle cues to understand how much explicit guidance models need for pragmatic generation
  - Quick check question: How does providing true intentions as part of chain-of-thought prompting differ from models inferring intentions themselves?

## Architecture Onboarding

- Component map:
  - Context C → Non-literal utterance U_N1 → Reference dialogues (U_L1, U_L2 for literal; U_T1, U_T2 for true intention)
  - LLMs generate response U_N2
  - GPT-4 contextual similarity compares U_N2 to reference responses
  - Chain-of-thought module generates inferred intentions before response generation

- Critical path:
  1. Generate counterfactual reference dialogues using GPT-4
  2. Generate model response U_N2 given context and non-literal utterance
  3. Compare U_N2 to reference responses using GPT-4 similarity evaluation
  4. (Optional) Generate inferred intention via chain-of-thought, then response

- Design tradeoffs:
  - Using GPT-4 for evaluation vs. human evaluation: Higher throughput but potential bias
  - Providing oracle intentions vs. requiring model inference: Simpler task but less realistic
  - Temperature settings for generation: Higher temperature increases diversity but may reduce coherence

- Failure signatures:
  - Low similarity between generated responses and both reference responses indicates poor pragmatic understanding
  - High similarity with literal reference but low with true intention reference indicates literal interpretation
  - Inconsistent performance across different non-literal phenomena suggests limited generalization

- First 3 experiments:
  1. Evaluate baseline models on all four non-literal phenomena using the generative framework
  2. Implement chain-of-thought prompting with no oracle information and measure improvements
  3. Test models with full oracle intention and phenomenon information to establish upper performance bounds

## Open Questions the Paper Calls Out
- How do different levels of context (short vs. long stories, number of dialog turns) affect models' ability to generate appropriate responses to non-literal language?
- How does providing different types of additional context (social dynamics, emotional states, external knowledge) impact models' pragmatic understanding and response generation?
- How do models' abilities to detect intentions in a discriminative setting translate to their ability to generate appropriate responses in a generative setting across different non-literal phenomena?
- What are the specific linguistic strategies and patterns that models need to learn to effectively generate appropriate responses to different types of non-literal language?
- How does the inclusion of non-literal language phenomena beyond the four studied affect models' pragmatic understanding and response generation?
- What is the impact of improving models' ability to handle non-literal language on other aspects of their performance, such as factuality and faithfulness?

## Limitations
- The evaluation framework's reliance on GPT-4 similarity metrics introduces potential bias, as the same model serves both as evaluator and reference generator
- The reported 50-55% baseline accuracy raises questions about whether metrics truly capture pragmatic appropriateness or if models have learned to game the evaluation criteria
- The modest improvements from chain-of-thought prompting suggest fundamental limitations in current LLMs' ability to leverage explicit reasoning for pragmatic tasks
- The performance gap between discriminative intention detection and generative response generation highlights that models may understand intentions without being able to act on them appropriately

## Confidence
- High confidence: Core finding that current models struggle with pragmatic response generation
- Medium confidence: Specific accuracy numbers given the GPT-4 evaluation methodology
- Low confidence: Claimed improvements from chain-of-thought prompting due to marginal nature of gains observed

## Next Checks
1. Replicate the experiments using human evaluation alongside GPT-4 similarity metrics to verify the reliability of the automated assessment
2. Test the framework with newer, larger models to determine if the 50-55% baseline represents a fundamental ceiling or model-specific limitation
3. Conduct ablation studies removing individual components of the chain-of-thought prompts to isolate which specific elements drive the modest improvements observed