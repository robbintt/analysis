---
ver: rpa2
title: 'CrisisViT: A Robust Vision Transformer for Crisis Image Classification'
arxiv_id: '2401.02838'
source_url: https://arxiv.org/abs/2401.02838
tags:
- image
- crisis
- classi
- cation
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study addresses the challenge of rapidly classifying crisis-related
  images from social media to support emergency response efforts. It introduces CrisisViT,
  a transformer-based model pre-trained on the Incidents1M dataset of crisis imagery,
  designed to improve automatic classification across four key tasks: disaster type,
  informativeness, humanitarian categories, and damage severity.'
---

# CrisisViT: A Robust Vision Transformer for Crisis Image Classification

## Quick Facts
- arXiv ID: 2401.02838
- Source URL: https://arxiv.org/abs/2401.02838
- Authors: Zijun Long; Richard McCreadie; Muhammad Imran
- Reference count: 6
- Primary result: Transformer-based model pre-trained on crisis imagery achieves 1.25% average accuracy gain over CNN baselines for crisis image classification tasks

## Executive Summary
This paper addresses the challenge of rapidly classifying crisis-related images from social media to support emergency response efforts. The authors introduce CrisisViT, a transformer-based model pre-trained on the Incidents1M dataset of crisis imagery, designed to improve automatic classification across four key tasks: disaster type, informativeness, humanitarian categories, and damage severity. Experiments show that CrisisViT significantly outperforms existing convolutional neural network and baseline transformer models, achieving an average absolute accuracy gain of 1.25%. The findings demonstrate that in-domain pre-training on crisis-specific data substantially enhances model performance and robustness for crisis image classification.

## Method Summary
The authors develop a Vision Transformer (ViT-Base) architecture pre-trained on the Incidents1M dataset using various strategies including binary classification, incident type categorization, place type categorization, and combined approaches. The pre-trained model is then fine-tuned on four downstream tasks using the Crisis Image Benchmark dataset: disaster type detection, informativeness classification, humanitarian category classification, and damage severity estimation. The method involves crawling images from Incidents1M URLs, implementing the ViT-Base architecture with 12 layers and 768 hidden size, and evaluating classification accuracy across all four tasks.

## Key Results
- CrisisViT achieves an average absolute accuracy gain of 1.25% over CNN baselines across four crisis image classification tasks
- Pre-training on place category labels from Incidents1M outperforms incident type labels for downstream classification
- Vision Transformer architecture outperforms ResNet101, EfficientNet, and VGG16 on disaster type, humanitarian category, and damage severity classification tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training on a large-scale crisis-specific dataset improves downstream classification accuracy for crisis image tasks.
- Mechanism: The transformer model learns rich, domain-specific visual representations from crisis imagery, enabling better generalization to unseen crisis images during fine-tuning.
- Core assumption: Crisis images contain unique visual patterns (e.g., disaster scenes, damage severity) that are not well captured by general-purpose pre-training datasets like ImageNet-1k.
- Evidence anchors:
  - [abstract] "Experiments show that CrisisViT significantly outperforms existing convolutional neural network and baseline transformer models, achieving an average absolute accuracy gain of 1.25%."
  - [section] "Through experimentation over the Crisis Image Benchmark dataset, we show that pre-training on the Incidents1M dataset can lead to significant improvements in accuracy, with an average absolute gain of 1.25% over the four crisis image classification tasks tested."
- Break condition: If the pre-training dataset lacks sufficient diversity or contains mislabeled examples, the learned representations may not generalize, and accuracy gains may disappear.

### Mechanism 2
- Claim: Vision transformer architectures outperform convolutional neural networks for crisis image classification when pre-trained on crisis imagery.
- Mechanism: Transformers' self-attention mechanism allows global feature extraction across the entire image, capturing complex relationships in crisis scenes that CNNs might miss due to local receptive fields.
- Core assumption: Crisis images require understanding of spatial relationships and global context (e.g., extent of damage, presence of rescue efforts) that are better modeled by transformers than CNNs.
- Evidence anchors:
  - [section] "ViT outperforms the other three CNN-based models... The ViT transformer model appears to be primarily advantaged when used for Disaster Type classification, Humanitarian category classification, and Damage Severity estimation."
  - [section] "In the remainder of this paper, we use ViT as our primary comparison point."
- Break condition: If the crisis images are small or contain only localized features, the global attention of transformers may not provide significant advantages over CNNs.

### Mechanism 3
- Claim: Place category labels from Incidents1M are more effective for pre-training than incident type labels.
- Mechanism: Place categories capture environmental context (e.g., building outdoor, highway, forest) that is more broadly applicable to crisis image classification tasks than specific incident types.
- Core assumption: Environmental context is more transferable across different crisis scenarios than specific incident types, which may be too narrow or domain-specific.
- Evidence anchors:
  - [section] "Comparing the Incident and Place labels provided by Incidents1M, the Place labels result in the best-performing downstream models in all cases, while the Incident labels provide a comparably smaller benefit."
  - [section] "Furthermore, we notice that when combining the Incident and Place labels together, performance does not improve over using the Place labels alone, indicating that the Incident labels are redundant when the Place labels are available."
- Break condition: If future crisis image tasks require specific incident type recognition rather than environmental context, the place category pre-training may be less effective.

## Foundational Learning

- Concept: Vision transformers (ViT) and their self-attention mechanism
  - Why needed here: Understanding how ViT processes images and why it can capture global context is crucial for appreciating the model's advantages in crisis image classification.
  - Quick check question: How does the self-attention mechanism in ViT differ from the convolutional layers in CNNs, and why might this difference be beneficial for crisis image analysis?

- Concept: Transfer learning and pre-training strategies
  - Why needed here: The paper relies heavily on pre-training models on crisis-specific data before fine-tuning on downstream tasks, making it essential to understand how pre-training affects model performance.
  - Quick check question: What are the potential benefits and risks of pre-training on a large, domain-specific dataset like Incidents1M versus a general dataset like ImageNet-1k?

- Concept: Crisis image classification tasks and their applications
  - Why needed here: Understanding the four main tasks (disaster type detection, informativeness, humanitarian categories, damage severity) helps contextualize why the model's performance improvements matter for emergency response.
  - Quick check question: How might each of the four crisis image classification tasks contribute to effective emergency response, and why would accuracy improvements in these tasks be valuable?

## Architecture Onboarding

- Component map: Input 224x224 pixel RGB images -> Vision Transformer (ViT-Base) -> Pre-training on Incidents1M -> Fine-tuning on Crisis Image Benchmark tasks -> Classification probabilities

- Critical path:
  1. Load pre-trained ViT-Base model (or initialize from scratch)
  2. Pre-train on Incidents1M using chosen strategy (binary, incident-only, place-only, or dual)
  3. Fine-tune on each of the four downstream tasks
  4. Evaluate accuracy on test sets

- Design tradeoffs:
  - Computational cost: ViT requires more resources than CNNs but offers better performance
  - Dataset choice: In-domain pre-training (Incidents1M) vs. general pre-training (ImageNet-1k)
  - Pre-training strategy: Binary classification vs. multi-class vs. self-supervised

- Failure signatures:
  - Overfitting to Incidents1M: Poor performance on downstream tasks despite good pre-training accuracy
  - Catastrophic forgetting: Loss of general image recognition capabilities when pre-trained only on Incidents1M
  - Class imbalance: Poor performance on minority classes in downstream tasks

- First 3 experiments:
  1. Reproduce baseline ViT performance on Crisis Image Benchmark without pre-training
  2. Pre-train ViT on Incidents1M using place category labels, then fine-tune on all four tasks
  3. Compare performance of place-only vs. incident-only vs. dual pre-training strategies on the humanitarian category task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do transformer-based models perform on crisis image classification compared to CNNs?
- Basis in paper: [explicit] The paper states that ViT outperforms CNN baselines like ResNet101, EÔ¨ÉNet, and VGG16 on disaster type, humanitarian category, and damage severity classification tasks.
- Why unresolved: While the paper shows ViT is superior, it does not explore other transformer architectures or newer models that could potentially perform even better.
- What evidence would resolve it: Experiments comparing ViT to other transformer-based models like MAE, SimMIM, or Data2vec on the same crisis image classification tasks.

### Open Question 2
- Question: What is the optimal pre-training strategy for crisis image classification using the Incidents1M dataset?
- Basis in paper: [explicit] The paper experiments with different pre-training strategies using Incidents1M, including binary classification, incident vs. place categorization, and combined training, finding varying levels of performance.
- Why unresolved: The paper does not explore all possible pre-training strategies, such as self-supervised learning techniques like masked image modeling or contrastive learning.
- What evidence would resolve it: A comprehensive comparison of various pre-training strategies, including self-supervised methods, on the crisis image classification tasks using the Incidents1M dataset.

### Open Question 3
- Question: Does combining ImageNet-1k with Incidents1M provide significant advantages over using only in-domain training for crisis image classification?
- Basis in paper: [explicit] The paper finds that combining ImageNet-1k and Incidents1M does not consistently improve performance over using Incidents1M alone, but there are some task-specific benefits.
- Why unresolved: The paper does not explore the long-term benefits of combining general and in-domain pre-training, such as improved robustness or generalization to new crisis types.
- What evidence would resolve it: Longitudinal studies comparing the performance and robustness of models trained with and without ImageNet-1k pre-training on new, unseen crisis image datasets.

## Limitations
- The 1.25% average accuracy improvement, while statistically significant, may not justify the computational overhead of transformer-based models for all crisis response scenarios
- The Incidents1M dataset may contain noise or bias from social media sources, potentially affecting the robustness of pre-trained representations
- The study focuses on 224x224 pixel images, limiting applicability to higher-resolution imagery that may be available in emergency response contexts

## Confidence

- **High Confidence**: The superiority of ViT over CNN baselines for crisis image classification is well-supported by direct comparisons within the paper. The mechanism of in-domain pre-training improving downstream performance is also well-established.
- **Medium Confidence**: The claim that place category labels are more effective than incident type labels for pre-training, while supported by experiments, may be task-dependent and requires further validation across different crisis scenarios.
- **Low Confidence**: The generalization of these findings to real-world emergency response systems, where computational resources, data quality, and response time constraints may differ significantly from controlled experimental conditions.

## Next Checks

1. **Computational Efficiency Analysis**: Evaluate the trade-off between accuracy gains and computational costs (training time, inference latency, hardware requirements) to determine practical applicability for emergency response scenarios.

2. **Cross-Domain Generalization**: Test CrisisViT on crisis image datasets from different geographic regions and time periods to assess whether the pre-training benefits generalize beyond the Incidents1M distribution.

3. **Label Ablation Study**: Conduct a more granular analysis of which specific place categories contribute most to downstream performance, potentially informing more efficient pre-training strategies.