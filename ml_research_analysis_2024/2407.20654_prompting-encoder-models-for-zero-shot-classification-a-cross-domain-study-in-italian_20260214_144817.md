---
ver: rpa2
title: 'Prompting Encoder Models for Zero-Shot Classification: A Cross-Domain Study
  in Italian'
arxiv_id: '2407.20654'
source_url: https://arxiv.org/abs/2407.20654
tags:
- domain
- verbalizer
- classification
- calibration
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of applying language models
  in specialized domains and low-resource languages, specifically Italian bureaucratic
  and legal language. The authors propose using smaller, domain-specific encoder models
  with prompting techniques for zero-shot classification tasks.
---

# Prompting Encoder Models for Zero-Shot Classification: A Cross-Domain Study in Italian

## Quick Facts
- **arXiv ID**: 2407.20654
- **Source URL**: https://arxiv.org/abs/2407.20654
- **Reference count**: 38
- **Primary result**: Domain-specialized encoder models with calibration and knowledgeable verbalizers achieve superior zero-shot classification performance in Italian bureaucratic and legal domains

## Executive Summary
This study addresses the challenge of applying language models in specialized domains and low-resource languages, specifically Italian bureaucratic and legal language. The authors propose using smaller, domain-specific encoder models with prompting techniques for zero-shot classification tasks. They compare three models (UmBERTo, Ita-Legal-BERT, and BureauBERTo) on document classification and entity typing tasks in PA and legal domains. The results show that domain-specialized models outperform generic models, especially when combined with calibration techniques and knowledgeable verbalizers. BureauBERTo achieved the best overall performance, demonstrating the effectiveness of domain adaptation.

## Method Summary
The study employs prompt-based classification using encoder models (UmBERTo, Ita-Legal-BERT, BureauBERTo) on Italian bureaucratic and legal texts. The approach involves three verbalizer types (base, manual, knowledgeable) and two calibration methods (contextual and batch calibration). The models are evaluated on document classification and entity typing tasks using macro F1-score, precision, and recall as metrics. The experimental setup includes the ATTO corpus, InformedPA corpus, and legal judgments, with performance comparisons across different model configurations and calibration approaches.

## Key Results
- BureauBERTo, the domain-specialized model, achieved the best overall performance across all tasks and settings
- Calibration techniques significantly improved model performance, with batch calibration showing particular effectiveness
- Knowledgeable verbalizers enhanced classification accuracy compared to base and manual verbalizers
- Domain-specialized models consistently outperformed generic models in zero-shot classification tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Domain-specific encoder models achieve higher accuracy in specialized language tasks than generic models due to domain-adapted vocabulary and training.
- **Mechanism**: BureauBERTo was further pre-trained on Italian bureaucratic texts and expanded its vocabulary with 8,305 domain tokens, allowing it to better capture and process specialized terminology.
- **Core assumption**: Additional pre-training on domain-specific data and vocabulary expansion directly improve model performance on in-domain tasks.
- **Evidence anchors**:
  - [abstract]: "BureauBERTo achieved the best overall performance, demonstrating the effectiveness of domain adaptation."
  - [section]: "BureauBERTo has proved to be particularly accurate in a fill-mask task, in which the model had to predict both random and in-domain masked words."
  - [corpus]: Weak - no direct citations for vocabulary expansion claims, inferred from methodology.
- **Break condition**: If domain vocabulary doesn't capture sufficient linguistic variation or if domain texts are too limited to provide meaningful training signals.

### Mechanism 2
- **Claim**: Prompt-based classification using encoder models is effective for zero-shot tasks when combined with appropriate verbalizers and calibration techniques.
- **Mechanism**: Encoder models use cloze tasks where the model assigns probabilities to label words in a [MASK] position, with performance improved by domain-specific verbalizers and calibration to mitigate biases.
- **Core assumption**: The probability of label words being the "correct" token in the [MASK] position determines classification accuracy.
- **Evidence anchors**:
  - [abstract]: "Furthermore, the application of calibration techniques and in-domain verbalizers significantly enhances the efficacy of encoder models."
  - [section]: "The choice of the verbalizer greatly influences the model performance... We tested the models using three types of verbalizers: i.) a base verbalizer, ii.) a manually constructed verbalizer, and iii.) a verbalizer automatically generated by each model."
  - [corpus]: Weak - general prompting literature cited but not specific to encoder models in zero-shot settings.
- **Break condition**: If prompt templates are poorly designed or if verbalizer label words don't capture task semantics effectively.

### Mechanism 3
- **Claim**: Calibration methods (contextual and batch) significantly improve model performance by mitigating biases in prompt-based classification.
- **Mechanism**: Calibration adjusts model predictions by either using content-free strings (contextual) or stratified samples from the dataset (batch) to estimate and correct contextual biases.
- **Core assumption**: Models exhibit biases like recency bias, majority label bias, and common token bias that can be estimated and corrected through calibration.
- **Evidence anchors**:
  - [abstract]: "The application of calibration techniques and in-domain verbalizers significantly enhances the efficacy of encoder models."
  - [section]: "We addressed this issue by calibrating the models following the contextual and batch calibration approaches."
  - [corpus]: Moderate - Zhao et al. (2021) cited for contextual calibration, but limited direct evidence for batch calibration effectiveness.
- **Break condition**: If calibration data introduces new biases or if the content-free strings used for contextual calibration don't represent the actual bias patterns.

## Foundational Learning

- **Concept**: Masked Language Modeling (MLM)
  - **Why needed here**: Encoder models like BERT and its variants are trained using MLM, which forms the basis for their ability to perform cloze-style prompt-based classification tasks.
  - **Quick check question**: How does MLM training differ from autoregressive language modeling, and why is it better suited for prompt-based classification?

- **Concept**: Zero-shot learning
  - **Why needed here**: The study focuses on performing classification tasks without any task-specific training examples, relying solely on the model's pre-trained knowledge.
  - **Quick check question**: What distinguishes zero-shot learning from few-shot learning, and what are the main challenges of zero-shot approaches?

- **Concept**: Domain adaptation
  - **Why needed here**: Understanding how further pre-training on specialized corpora and vocabulary expansion improves model performance on domain-specific tasks.
  - **Quick check question**: What are the key differences between training from scratch on domain data versus further pre-training a general-purpose model on domain data?

## Architecture Onboarding

- **Component map**: Encoder models (UmBERTo, Ita-Legal-BERT, BureauBERTo) → Prompt templates → Verbalizers → Calibration modules → Classification Output

- **Critical path**: Model → Prompt Template → Verbalizer → Calibration → Classification Output

- **Design tradeoffs**:
  - Model size vs. domain specialization: Smaller specialized models vs. larger general-purpose models
  - Verbalizer complexity: Simple base vs. manual vs. knowledgeable verbalizers
  - Calibration approach: Contextual (content-free) vs. Batch (dataset-based)

- **Failure signatures**:
  - Low performance across all models: Likely issue with prompt template design
  - Specialized models underperforming generic: Potential vocabulary mismatch or insufficient domain training
  - Calibration not improving results: Calibration method may not match bias patterns

- **First 3 experiments**:
  1. Test all three models with base verbalizer and no calibration to establish baseline performance
  2. Apply contextual calibration to all models with base verbalizer to assess calibration impact
  3. Test specialized models with knowledgeable verbalizer and batch calibration to evaluate optimal configuration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of BureauBERTo on zero-shot classification tasks compare to other specialized domain models like those in the biomedical or legal fields?
- Basis in paper: [inferred] The paper compares BureauBERTo to general models and legal-specific models, but does not provide a comprehensive comparison to other specialized domain models.
- Why unresolved: The study focuses on Italian bureaucratic and legal domains and does not explore BureauBERTo's performance against models from other specialized fields.
- What evidence would resolve it: Conducting experiments comparing BureauBERTo to specialized models from various domains like biomedical, financial, or technical fields would provide a clearer understanding of its relative performance.

### Open Question 2
- Question: What is the impact of different calibration methods on the performance of specialized models when applied to zero-shot classification tasks?
- Basis in paper: [explicit] The paper explores the effects of contextual calibration (CC) and batch calibration (BC) on model performance, noting improvements but also highlighting differences based on the calibration method used.
- Why unresolved: While the paper demonstrates the benefits of calibration, it does not provide a comprehensive analysis of how different calibration methods specifically impact specialized models in zero-shot settings.
- What evidence would resolve it: Systematic experimentation with various calibration methods across different specialized models and tasks would clarify their individual and combined effects on zero-shot classification performance.

### Open Question 3
- Question: How does the size of the external knowledge base influence the effectiveness of knowledgeable verbalizers in zero-shot classification tasks?
- Basis in paper: [inferred] The paper constructs knowledgeable verbalizers using an external knowledge base but does not investigate the relationship between the size of this base and the resulting model performance.
- Why unresolved: The study does not explore how varying the size of the external knowledge base affects the performance of knowledgeable verbalizers in zero-shot classification.
- What evidence would resolve it: Experiments that systematically vary the size of the external knowledge base and measure the impact on classification accuracy would provide insights into the optimal knowledge base size for effective verbalizers.

## Limitations

- Reliance on three specific Italian encoder models limits generalizability to other languages or domains
- Effectiveness demonstrated primarily on bureaucratic and legal text classification tasks, leaving uncertainty about performance in other specialized domains
- Relatively small datasets (e.g., ATTO corpus with 600 PA texts and 300 legal judgments) may not fully represent the complexity of real-world language processing

## Confidence

- **High Confidence**: The finding that domain-specialized models outperform generic models in specialized language tasks is well-supported by direct experimental evidence showing BureauBERTo's superior performance across multiple metrics.
- **Medium Confidence**: The effectiveness of calibration techniques is supported by results, but the study provides limited comparison with alternative calibration methods or baseline approaches without calibration.
- **Medium Confidence**: The claim about knowledgeable verbalizers significantly enhancing performance is supported, but the manual construction process lacks detailed reproducibility guidelines.

## Next Checks

1. **Cross-Lingual Validation**: Test whether the prompting and calibration techniques developed for Italian models generalize to other low-resource languages by applying the methodology to encoder models trained on non-English languages with limited resources.

2. **Domain Transferability Assessment**: Evaluate BureauBERTo's performance on other specialized domains (e.g., medical, technical) to determine whether domain adaptation benefits transfer beyond bureaucratic and legal contexts.

3. **Dataset Size Sensitivity Analysis**: Systematically vary the size of training datasets used for calibration and verbalizer construction to identify minimum data requirements for maintaining performance gains, particularly important for truly low-resource scenarios.