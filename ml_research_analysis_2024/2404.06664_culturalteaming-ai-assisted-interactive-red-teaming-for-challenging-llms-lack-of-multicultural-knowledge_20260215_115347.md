---
ver: rpa2
title: 'CulturalTeaming: AI-Assisted Interactive Red-Teaming for Challenging LLMs''
  (Lack of) Multicultural Knowledge'
arxiv_id: '2404.06664'
source_url: https://arxiv.org/abs/2404.06664
tags:
- question
- users
- questions
- cultural
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'CulturalTeaming is an interactive red-teaming system that uses
  human-AI collaboration to create challenging datasets for evaluating LLMs'' multicultural
  knowledge. The system has two modes: Verifier-Only, which uses an LLM for in-the-loop
  testing, and AI-Assisted, which provides LLM-generated hints and question formulation
  assistance.'
---

# CulturalTeaming: AI-Assisted Interactive Red-Teaming for Challenging LLMs' (Lack of) Multicultural Knowledge

## Quick Facts
- arXiv ID: 2404.06664
- Source URL: https://arxiv.org/abs/2404.06664
- Authors: Yu Ying Chiu; Liwei Jiang; Maria Antoniak; Chan Young Park; Shuyue Stella Li; Mehar Bhatia; Sahithya Ravi; Yulia Tsvetkov; Vered Shwartz; Yejin Choi
- Reference count: 40
- One-line primary result: AI-assisted red-teaming significantly improves users' ability to create challenging cultural questions and enhances perceived creativity

## Executive Summary
CulturalTeaming is an interactive red-teaming system that leverages human-AI collaboration to create challenging evaluation datasets for assessing LLMs' multicultural knowledge. The system operates in two modes: Verifier-Only, which uses an LLM for in-the-loop testing, and AI-Assisted, which provides LLM-generated hints and question formulation assistance. Users iteratively create multiple-choice questions about cultural norms, revising them based on LLM feedback until they become sufficiently challenging. The resulting CULTURALBENCH-V0.1 dataset contains 252 questions across 34 cultures, revealing that modern LLMs achieve accuracies ranging from 37.7% to 72.2%, indicating a notable gap in their multicultural proficiency.

## Method Summary
The CulturalTeaming system implements a three-step workflow: Question Formulation (users create cultural scenarios and questions), Question Verification & Revision (LLM verifies and suggests improvements), and Feedback Collection (users provide input on the process). The system uses GPT-3.5-turbo as the verifier and GPT-4 for hint generation in AI-Assisted mode. Users participate in 1-hour workshop sessions where they are randomly assigned to either Verifier-Only or AI-Assisted conditions. The iterative process continues until the LLM fails to answer the question correctly, creating challenging test cases for evaluating multicultural knowledge gaps.

## Key Results
- AI assistance significantly improved users' ability to create difficult questions (Verifier-Only accuracy dropped from 72.2% to 37.7%)
- Users reported enhanced perceived creativity when using AI assistance
- The CULTURALBENCH-V0.1 dataset covers 34 diverse cultures with 252 questions total
- Modern LLMs show a notable gap in multicultural proficiency with accuracies ranging from 37.7% to 72.2%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AI assistance in question revision improves difficulty of questions for LLMs
- Mechanism: LLM-generated hints provide users with revision strategies that they might not have considered, enabling them to create more challenging questions through iterative refinement
- Core assumption: Users experience "idea depletion" after multiple revision attempts and benefit from external suggestions
- Evidence anchors:
  - [abstract] "the increased level of AI assistance (e.g., LLM-generated revision hints) empowers users to create more difficult questions with enhanced perceived creativity of themselves"
  - [section 3.1.3] "LLM assistance on Hints helps users to construct more challenging questions after several rounds of revision" and "LLM suggestions can therefore serve as an idea pool, providing diverse editing ideas"
  - [corpus] Weak - no direct evidence in corpus about this specific mechanism, but related works on AI-assisted data collection support this general approach
- Break condition: If users become overly dependent on AI suggestions and stop applying their own cultural knowledge, the quality and authenticity of cultural questions may suffer

### Mechanism 2
- Claim: Human-AI collaboration produces higher quality cultural evaluation datasets than purely automated or purely human approaches
- Mechanism: Humans provide authentic cultural knowledge and creativity while AI handles structured formatting and provides standardized revision assistance, combining strengths of both approaches
- Core assumption: Purely human approaches are too slow and inconsistent; purely automated approaches lack authentic cultural knowledge
- Evidence anchors:
  - [abstract] "To synergize the creativity and expert cultural knowledge of human annotators and the scalability and standardizability of LLM-based automation"
  - [section 1] "investigate alternative data annotation strategies that harness the valuable cultural insights of humans while streamlining mundane tasks with LLMs"
  - [corpus] Moderate - the corpus contains related works on AI-augmented data collection and cultural bias evaluation, supporting this hybrid approach
- Break condition: If the AI assistance becomes too directive, it may suppress human creativity or introduce its own biases into the cultural questions

### Mechanism 3
- Claim: Iterative red-teaming with confidence scoring enables effective question refinement
- Mechanism: Users can gauge the difficulty of their questions based on LLM confidence scores and iteratively refine until the LLM fails, creating challenging test cases
- Core assumption: LLM confidence scores correlate with actual difficulty for other models
- Evidence anchors:
  - [section 3.1.2] "Our LLM Verifier is able to create challenging questions for all tested models" and "LLM performance decreases with the number of revisions"
  - [section 2.2] "The platform assists them in revising the question and the options to make it more challenging by providing descriptions of various common revision strategies"
  - [corpus] Weak - no direct evidence in corpus about this specific confidence-based red-teaming mechanism
- Break condition: If the confidence scoring doesn't correlate well with actual difficulty for different model architectures, users may waste effort refining questions that aren't actually challenging

## Foundational Learning

- Concept: Cultural knowledge representation in LLMs
  - Why needed here: Understanding how LLMs encode and retrieve cultural knowledge is essential for creating effective test cases
  - Quick check question: How do LLMs typically represent cultural knowledge - through explicit cultural embeddings, learned patterns from training data, or both?

- Concept: Red-teaming methodology
  - Why needed here: The paper uses red-teaming techniques to find weaknesses in LLMs' cultural knowledge
  - Quick check question: What distinguishes red-teaming from traditional benchmarking approaches in terms of methodology and goals?

- Concept: Human-AI collaboration in data annotation
  - Why needed here: The paper's innovation relies on understanding how humans and AI can effectively collaborate in dataset creation
  - Quick check question: What are the key design considerations when creating systems that require human-AI collaboration for data annotation?

## Architecture Onboarding

- Component map:
  User Interface -> LLM Verifier (GPT-3.5-turbo) -> LLM Assistance (GPT-4) -> Iterative Refinement -> Dataset Compilation -> Cross-model Testing

- Critical path: User creates question → Question is verified by LLM → User revises based on confidence score → (AI-Assisted) User receives hints → Question is finalized → Feedback is collected

- Design tradeoffs:
  - Human creativity vs. AI standardization: More AI assistance improves efficiency but may reduce authentic cultural expression
  - Question difficulty vs. usability: Very challenging questions may be harder for users to create and evaluate
  - Model coverage vs. depth: Testing multiple models provides broader insights but may dilute focus on specific weaknesses

- Failure signatures:
  - Low attack success rate: Indicates questions aren't challenging enough or AI assistance is too directive
  - High user drop-off: Suggests the interface is too complex or the task is too cognitively demanding
  - Poor correlation between verifier and other models: Indicates the verifier isn't an effective estimator of difficulty

- First 3 experiments:
  1. Test the correlation between verifier confidence scores and actual difficulty for different model families
  2. Compare question quality (difficulty, cultural authenticity) between AI-Assisted and Verifier-Only conditions
  3. Evaluate whether AI assistance improves user satisfaction and perceived creativity without sacrificing question quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CulturalTeaming's AI-assisted system compare to other LLM-based benchmark generation approaches for multicultural knowledge assessment?
- Basis in paper: [inferred] The paper discusses the challenges of LLM-generated benchmarks and their tendency to propagate biases, but does not directly compare CulturalTeaming's performance to other LLM-based benchmark generation approaches.
- Why unresolved: The paper focuses on comparing the two variants of CulturalTeaming (Verifier-Only and AI-Assisted) rather than comparing CulturalTeaming to other LLM-based benchmark generation approaches.
- What evidence would resolve it: A comparative study evaluating CulturalTeaming's performance against other LLM-based benchmark generation approaches for multicultural knowledge assessment would provide evidence to resolve this question.

### Open Question 2
- Question: How does the performance of CulturalTeaming's AI-assisted system vary across different cultural groups, and what factors contribute to these variations?
- Basis in paper: [explicit] The paper mentions that the CULTURAL BENCH-V0.1 dataset covers 34 diverse cultures, but does not provide a detailed analysis of the system's performance across different cultural groups or the factors contributing to variations in performance.
- Why unresolved: The paper does not delve into a granular analysis of the system's performance across different cultural groups or explore the factors that may influence these variations.
- What evidence would resolve it: A comprehensive analysis of CulturalTeaming's performance across different cultural groups, along with an investigation of the factors contributing to variations in performance, would provide evidence to resolve this question.

### Open Question 3
- Question: How can CulturalTeaming be adapted to support multilingual datasets and evaluate LLMs' cultural awareness across different languages?
- Basis in paper: [explicit] The paper mentions that the CULTURAL BENCH-V0.1 dataset is in English, but does not explore the possibility of adapting CulturalTeaming to support multilingual datasets or evaluate LLMs' cultural awareness across different languages.
- Why unresolved: The paper does not discuss the potential for adapting CulturalTeaming to support multilingual datasets or evaluate LLMs' cultural awareness across different languages, which could be a valuable extension of the system's capabilities.
- What evidence would resolve it: An investigation into the feasibility and effectiveness of adapting CulturalTeaming to support multilingual datasets and evaluate LLMs' cultural awareness across different languages would provide evidence to resolve this question.

## Limitations

- Cultural representation bias: The diversity and authenticity of cultural knowledge captured depends heavily on the annotator pool, which isn't specified in the paper
- AI dependency risk: Users may become overly reliant on AI-generated hints, potentially suppressing authentic cultural knowledge and creativity
- Confidence score reliability: The effectiveness of GPT-3.5-turbo's confidence scores as a proxy for question difficulty across different model architectures remains unverified

## Confidence

- High confidence: The core finding that AI-assisted annotation improves question difficulty and user creativity is well-supported by the experimental results
- Medium confidence: The claim that CulturalTeaming reveals "a notable gap in LLMs' multicultural proficiency" is supported by the 37.7-72.2% accuracy range
- Low confidence: The generalizability of the CULTURALBENCH-V0.1 dataset to broader cultural contexts is uncertain without information about annotator diversity

## Next Checks

1. **Confidence score validation**: Test the correlation between GPT-3.5-turbo's confidence scores and actual difficulty across multiple model families (including non-GPT models) to verify the red-teaming mechanism's effectiveness

2. **Cultural authenticity audit**: Conduct a blind review of the CULTURALBENCH-V0.1 questions by cultural experts from the represented cultures to assess the authenticity and accuracy of the cultural knowledge captured

3. **AI assistance impact study**: Run a controlled experiment comparing question quality (difficulty, cultural authenticity) between AI-Assisted and Verifier-Only conditions with a larger, more diverse annotator pool to determine if AI assistance consistently improves outcomes without introducing bias