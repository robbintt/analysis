---
ver: rpa2
title: 'From human experts to machines: An LLM supported approach to ontology and
  knowledge graph construction'
arxiv_id: '2403.08345'
source_url: https://arxiv.org/abs/2403.08345
tags:
- ontology
- knowledge
- pipeline
- llms
- answers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores the use of open-source large language models
  to automate ontology and knowledge graph construction, aiming to reduce human effort
  while maintaining quality. The approach involves generating competency questions,
  creating an ontology, constructing a knowledge graph, and evaluating results with
  minimal human involvement.
---

# From human experts to machines: An LLM supported approach to ontology and knowledge graph construction

## Quick Facts
- arXiv ID: 2403.08345
- Source URL: https://arxiv.org/abs/2403.08345
- Reference count: 26
- Automated pipeline generates competency questions, ontology, and knowledge graph using open-source LLMs, achieving 142 correlated individuals out of 203 total

## Executive Summary
This work presents an automated pipeline for ontology and knowledge graph construction using open-source large language models, significantly reducing human effort while maintaining quality. The approach generates competency questions, creates ontologies, and constructs knowledge graphs with minimal human involvement, demonstrated on deep learning methodologies in biodiversity research. The pipeline achieved an average evaluation score above 60% and successfully correlated 142 individuals out of 203 total, showing promise for LLM-assisted knowledge engineering.

## Method Summary
The pipeline automates ontology and knowledge graph construction through four main steps: generating competency questions from input documents using LLMs, creating an ontology from these questions, constructing a knowledge graph from the ontology and source documents, and evaluating results with automated scoring. The process employs GPT-3.5-turbo and Llama-2-70b-chat models, with manual validation used sparingly for quality assurance. The approach emphasizes minimal human intervention while maintaining reasonable accuracy, validated through both automated metrics and manual evaluation against ground truth data.

## Key Results
- Successfully correlated 142 individuals out of 203 total in the knowledge graph
- Achieved average evaluation score above 60% across automated and manual metrics
- Demonstrated reduced human effort in ontology and knowledge graph construction while maintaining quality

## Why This Works (Mechanism)
LLMs serve as effective co-pilots in ontology engineering by automating the generation of competency questions and knowledge graph construction from domain-specific documents. The approach leverages the language understanding capabilities of LLMs to extract relevant concepts and relationships, reducing the traditionally labor-intensive process of manual ontology creation. By minimizing human involvement while maintaining quality through automated evaluation and targeted human validation, the pipeline enables more efficient knowledge graph development.

## Foundational Learning
- Competency Questions: Domain-specific queries that guide ontology development - needed to ensure ontologies capture relevant domain knowledge and can be automatically generated by LLMs to reduce human effort
- Knowledge Graph Construction: Process of representing entities and relationships as interconnected nodes - needed to create structured knowledge representations from unstructured text
- Ontology Evaluation: Methods to assess ontology quality and completeness - needed to ensure automatically generated ontologies meet domain requirements and maintain accuracy

## Architecture Onboarding

**Component Map:** Document Input -> Competency Question Generation -> Ontology Creation -> Knowledge Graph Construction -> Evaluation

**Critical Path:** The sequence from document input through competency question generation to final knowledge graph evaluation represents the core workflow, with each step dependent on the previous one's output quality.

**Design Tradeoffs:** Minimal human intervention versus quality control, automation speed versus accuracy, and computational cost versus model performance.

**Failure Signatures:** Hallucinations in LLM outputs leading to incorrect ontology elements, prompt sensitivity causing inconsistent results, and evaluation metric limitations failing to capture semantic correctness.

**3 First Experiments:**
1. Test competency question generation accuracy with varying prompt formulations
2. Validate ontology creation from automatically generated questions
3. Measure knowledge graph construction accuracy against ground truth data

## Open Questions the Paper Calls Out
None

## Limitations
- Reliability of LLM-generated competency questions and their impact on ontology quality
- Extent to which automated knowledge graph construction can match human-expert curated results in complex domains
- Sensitivity to prompt engineering and risk of model hallucination affecting reproducibility

## Confidence
- Pipeline demonstrates measurable success in automating ontology and knowledge graph construction: Medium
- Claims about LLM effectiveness as co-pilots in ontology engineering are supported but not yet proven for broader domains: Medium

## Next Checks
1. Test the pipeline on a larger, more diverse knowledge domain to assess scalability and robustness against domain complexity
2. Conduct a comparative evaluation with human-expert curated ontologies to measure semantic accuracy and practical utility
3. Implement and test automated hallucination detection mechanisms to improve reliability of LLM-generated outputs