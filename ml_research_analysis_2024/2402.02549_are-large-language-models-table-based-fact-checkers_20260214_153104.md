---
ver: rpa2
title: Are Large Language Models Table-based Fact-Checkers?
arxiv_id: '2402.02549'
source_url: https://arxiv.org/abs/2402.02549
tags:
- llms
- prompt
- table
- in-context
- table-based
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the capability of large language models
  (LLMs) to perform table-based fact verification (TFV), a task requiring extraction
  of entailment relations between statements and structured tables. It compares zero-shot,
  few-shot, and instruction-tuned settings for LLMs, focusing on ChatGPT and LLaMA
  variants.
---

# Are Large Language Models Table-based Fact-Checkers?

## Quick Facts
- arXiv ID: 2402.02549
- Source URL: https://arxiv.org/abs/2402.02549
- Authors: Hanwen Zhang; Qingyi Si; Peng Fu; Zheng Lin; Weiping Wang
- Reference count: 28
- Primary result: Instruction-tuned LLaMA models significantly outperform zero-shot/few-shot baselines on table-based fact verification

## Executive Summary
This paper investigates whether large language models (LLMs) can perform table-based fact verification (TFV) by extracting entailment relations between statements and structured tables. The authors compare zero-shot, few-shot, and instruction-tuned settings across multiple LLMs including ChatGPT and LLaMA variants. Their experiments demonstrate that while ChatGPT achieves reasonable accuracy under zero-shot and few-shot settings, LLaMA models require instruction tuning to significantly improve performance. Instruction-tuned LLaMA models outperform many smaller table-specific models but still lag behind the best task-specific approaches.

## Method Summary
The study evaluates LLMs on the TabFact dataset using three prompting strategies: zero-shot (no examples), few-shot (1-4 in-context examples), and instruction-tuning (fine-tuning with LoRA on constructed instruction data). Zero-shot and few-shot experiments use diverse prompt formats including sentence-based, paragraph, and dialogue styles. For instruction tuning, the authors construct instruction datasets following the Alpaca format and fine-tune LLaMA-1 and LLaMA-2 models. Performance is measured using accuracy on the TabFact-small-test set containing 1998 samples.

## Key Results
- ChatGPT achieves comparable accuracy to early baseline methods under zero-shot setting without requiring instruction tuning
- LLaMA-2 models show significant performance improvement after instruction tuning, outperforming certain program-based methods and encoder-only general PLMs
- Instruction-tuned LLaMA models still lag behind the best task-specific approaches despite showing substantial gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-shot LLMs can perform table-based fact verification when provided with carefully engineered prompts that clearly describe the task and constrain the output format.
- Mechanism: The prompt acts as a bridge between the LLM's general language understanding capabilities and the specific requirements of table reasoning. By explicitly stating the task ("Give you a statement and a table, please tell me whether the statement is supported or refuted by the table") and providing the linearized table and statement, the LLM can leverage its pre-trained knowledge to extract relevant information and make a judgment.
- Core assumption: The LLM has been exposed to enough tabular data during pre-training to understand table structure and perform basic reasoning operations.
- Evidence anchors:
  - [abstract]: "Experimental results demonstrate that LLMs can achieve acceptable results on zero-shot and few-shot TFV with prompt engineering"
  - [section]: "ChatGPT demonstrates an acceptable performance under the zero-shot setting, which is comparable to early baseline methods"
  - [corpus]: Weak evidence - corpus neighbors focus on table understanding but don't directly address zero-shot TFV performance.
- Break condition: The LLM lacks sufficient exposure to tabular data during pre-training, or the table structure is too complex for the prompt format to capture effectively.

### Mechanism 2
- Claim: Few-shot learning with in-context examples improves LLM performance on TFV by providing concrete examples of the input-output pattern the model should follow.
- Mechanism: By including 1-2 examples of statement-table pairs with their correct labels in the prompt, the LLM can learn the mapping from input to output without parameter updates. This demonstrates the expected reasoning process and output format.
- Core assumption: The LLM can effectively learn from a small number of in-context examples without overfitting or being confused by contradictory examples.
- Evidence anchors:
  - [abstract]: "We design diverse prompts to explore how the in-context learning can help LLMs in TFV, i.e., zero-shot and few-shot TFV capability"
  - [section]: "Compared with the zero-shot setting, ChatGPT gains better accuracy with the assistance of in-context examples"
  - [corpus]: Weak evidence - corpus neighbors discuss in-context learning generally but not specifically for TFV with few examples.
- Break condition: Too many in-context examples exceed the LLM's context window, or the examples are not representative of the test distribution.

### Mechanism 3
- Claim: Instruction tuning significantly improves LLM performance on TFV by fine-tuning the model on task-specific data with explicit instructions.
- Mechanism: By constructing instruction datasets that pair table-statement pairs with explicit instructions (e.g., "Is this statement supported or refuted by the table?"), and fine-tuning the LLM with parameter-efficient methods like LoRA, the model learns to better understand table structure and perform the verification task.
- Core assumption: The LLM can learn to specialize in table reasoning tasks when fine-tuned on a sufficient amount of task-specific data, even with parameter-efficient methods.
- Evidence anchors:
  - [abstract]: "instruction-tuning can stimulate the TFV capability significantly"
  - [section]: "Results show that fine-tuned LLaMA models gain a significant improvement. They outperform certain program-based methods and encoder-only general PLMs"
  - [corpus]: Weak evidence - corpus neighbors focus on table understanding but don't directly address instruction tuning for TFV.
- Break condition: Insufficient instruction-tuning data or the parameter-efficient fine-tuning method (LoRA) cannot capture the full complexity of the TFV task.

## Foundational Learning

- Concept: Table structure understanding
  - Why needed here: TFV requires extracting and reasoning about information from structured tables, which is fundamentally different from processing unstructured text.
  - Quick check question: Can the LLM correctly identify the relevant columns and rows in a table given a statement to verify?

- Concept: In-context learning
  - Why needed here: Few-shot TFV relies on the LLM's ability to learn from a small number of examples provided in the prompt without parameter updates.
  - Quick check question: Does the LLM's performance on TFV improve when provided with 1-2 relevant in-context examples?

- Concept: Instruction following
  - Why needed here: Instruction tuning requires the LLM to understand and follow explicit instructions for the TFV task, which is crucial for achieving high performance.
  - Quick check question: After instruction tuning, does the LLM consistently follow the instruction format when generating responses for TFV?

## Architecture Onboarding

- Component map: Statement and Table -> Prompt Generator -> LLM -> Output Parser -> Accuracy Evaluation
- Critical path: Prepare statement and table → Generate appropriate prompt → Send prompt to LLM → Parse LLM response → Compare predicted label to ground truth → Calculate accuracy
- Design tradeoffs:
  - Prompt engineering vs. instruction tuning: Prompt engineering is cheaper but less effective, while instruction tuning requires more resources but yields better performance.
  - Context window size: Larger context windows allow for more in-context examples but increase computational cost.
  - Parameter-efficient fine-tuning: LoRA reduces fine-tuning cost but may not capture the full complexity of the TFV task.
- Failure signatures:
  - Zero-shot: LLM generates seemingly reasonable responses but lacks reasoning ability on tables.
  - Few-shot: LLM's accuracy declines as the number of in-context examples increases beyond 2.
  - Instruction tuning: Fine-tuned LLM outperforms some small-scaled models but still lags behind the best task-specific approaches.
- First 3 experiments:
  1. Compare zero-shot performance of ChatGPT and LLaMA-2-chat on TabFact-small-test with different prompt formats (Sentence, Sentence+word, Paragraph, Dialogue-simple, Dialogue-complex).
  2. Evaluate few-shot performance of LLaMA-2, LLaMA-2-chat, and ChatGPT with 1, 2, and 4 in-context examples on TabFact-small-test.
  3. Fine-tune LLaMA-1 and LLaMA-2 on TabFact train set using LoRA and compare their performance to existing LLM-free baselines on TabFact-small-test.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs on table-based fact verification scale with model size, and is there a threshold beyond which additional parameters yield diminishing returns?
- Basis in paper: [explicit] The paper compares ChatGPT and LLaMA models, noting that ChatGPT's larger parameter count contributes to its better zero-shot performance, while LLaMA-2 (7B) requires instruction tuning to improve.
- Why unresolved: The study only uses specific models (ChatGPT and LLaMA variants) and does not systematically explore how performance changes with different model sizes or parameter counts.
- What evidence would resolve it: Conducting experiments with a range of LLM sizes (e.g., 1B, 7B, 13B, 70B parameters) on the same TFV tasks, analyzing accuracy trends and identifying performance saturation points.

### Open Question 2
- Question: What is the impact of instruction-tuning LLMs on out-of-domain and cross-domain table-based fact verification tasks?
- Basis in paper: [explicit] The paper discusses the potential of instruction-tuned LLMs for cross-domain and out-of-domain settings but does not evaluate this empirically.
- Why unresolved: The experiments focus on in-domain performance using the TabFact dataset, without testing generalization to unseen domains or table formats.
- What evidence would resolve it: Testing instruction-tuned LLMs on diverse TFV datasets from different domains (e.g., financial tables, scientific tables) and measuring performance degradation or robustness.

### Open Question 3
- Question: How effective are chain-of-thought (CoT) prompting strategies for improving the reasoning and accuracy of LLMs on complex table-based fact verification tasks?
- Basis in paper: [explicit] The paper suggests that specifying inference procedures, such as CoT, could enhance LLM performance but does not implement or test this approach.
- Why unresolved: The study focuses on direct prompting and instruction tuning without exploring intermediate reasoning steps or CoT methodologies.
- What evidence would resolve it: Designing and evaluating CoT prompts for TFV, comparing accuracy and interpretability against standard prompting methods, and analyzing the quality of generated reasoning steps.

### Open Question 4
- Question: What are the most effective table decomposition strategies for improving LLM performance on long or complex tables in fact verification tasks?
- Basis in paper: [explicit] The paper identifies handling long inputs as a key challenge and suggests table decomposition as a potential solution but does not explore specific strategies.
- Why unresolved: The study does not implement or compare different table decomposition methods, such as sub-table extraction or hierarchical processing.
- What evidence would resolve it: Experimenting with various decomposition techniques (e.g., semantic relevance filtering, row/column grouping) and measuring their impact on LLM accuracy and efficiency for TFV.

## Limitations

- The study is limited to English-language Wikipedia tables and may not generalize to other domains or languages
- Zero-shot and few-shot approaches rely heavily on prompt engineering quality, which may not scale to more complex table structures
- Instruction tuning experiments use parameter-efficient methods (LoRA) that may not fully capture the complexity of table-based reasoning

## Confidence

High confidence: The general finding that LLMs can perform table-based fact verification with appropriate prompting strategies, and that instruction tuning significantly improves performance over zero-shot approaches.

Medium confidence: The specific performance numbers and rankings of different models, as these are sensitive to prompt engineering quality and the relatively small test set size.

Low confidence: Any claims about generalizability to other domains or table types beyond the Wikipedia-based TabFact dataset.

## Next Checks

1. **Robustness to Table Complexity**: Test the best-performing instruction-tuned LLaMA model on tables with varying structural complexity (e.g., nested tables, tables with merged cells, tables with extensive footnotes) to validate whether the performance gains generalize beyond simple Wikipedia tables.

2. **Cross-Domain Transfer**: Evaluate the zero-shot and few-shot capabilities of ChatGPT and LLaMA-2-chat on a different table-based fact verification dataset from a non-Wikipedia domain (e.g., scientific tables, financial reports) to assess domain generalization.

3. **Instruction Tuning Data Efficiency**: Systematically vary the amount of instruction-tuning data (using subsets of the TabFact training set) to determine the minimum effective dataset size for achieving significant performance improvements, which would inform practical deployment considerations.