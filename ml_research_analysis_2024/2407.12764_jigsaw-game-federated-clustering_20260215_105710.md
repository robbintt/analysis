---
ver: rpa2
title: 'Jigsaw Game: Federated Clustering'
arxiv_id: '2407.12764'
source_url: https://arxiv.org/abs/2407.12764
tags:
- centroids
- algorithm
- federated
- clustering
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses federated clustering, specifically federated
  k-means, by proposing a one-shot algorithm called FeCA (Federated Centroid Aggregation).
  The key insight is exploiting the structured nature of local solutions in k-means,
  where centroids are either one-fit-many (near multiple true centers) or one/many-fit-one
  (near a single true center).
---

# Jigsaw Game: Federated Clustering

## Quick Facts
- arXiv ID: 2407.12764
- Source URL: https://arxiv.org/abs/2407.12764
- Reference count: 40
- Primary result: One-shot federated k-means algorithm (FeCA) that recovers global centroids by exploiting structural properties of local solutions

## Executive Summary
This paper addresses federated clustering by proposing FeCA (Federated Centroid Aggregation), a one-shot algorithm for federated k-means that exploits the structured nature of local solutions. The key insight is that local k-means solutions contain either one-fit-many centroids (near multiple true centers) or one/many-fit-one centroids (near a single true center). FeCA works by having clients refine their solutions to remove spurious centroids, then sending them to a central server which aggregates them using adaptive radii to recover the global solution. The authors extend this approach to representation learning with DeepFeCA, combining FeCA with DeepCluster for federated unsupervised feature learning.

## Method Summary
FeCA operates in three main phases: client refinement using Lloyd's algorithm with one-fit-many centroid removal, radius assignment based on minimum pairwise distances, and server-side aggregation that groups centroids by radius. The algorithm exploits the geometric structure of local k-means solutions under the Stochastic Ball Model, where centroids can be categorized as either near multiple true centers or a single true center. DeepFeCA extends this framework to representation learning by iteratively updating model parameters using local DeepCluster objectives and refining pseudo-labels through FeCA aggregation.

## Key Results
- FeCA consistently achieves lower ℓ2-distance to true centers than baselines (M-Avg, k-FED, FFCM) across synthetic datasets under IID and non-IID scenarios
- On real datasets (CIFAR-10/100, Tiny-ImageNet), FeCA outperforms baselines with higher Purity and NMI scores
- DeepFeCA demonstrates superior performance in federated representation learning compared to existing methods, particularly under non-IID data distributions
- The algorithm shows robust performance across varying numbers of clients (M=5 to 20) and maintains effectiveness with different data heterogeneity levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Local k-means solutions contain structured information about global centroids, enabling reconstruction through aggregation
- Mechanism: Each local solution consists of either one-fit-many centroids (near multiple true centers) or one/many-fit-one centroids (near single true center). By removing one-fit-many centroids and aggregating the remaining ones, the global solution can be recovered
- Core assumption: Local solutions under Stochastic Ball Model satisfy the one-fit-many/many-fit-one structure with high probability
- Evidence anchors:
  - [abstract]: "We outline the challenge posed by its non-convex objective and data heterogeneity in the federated framework. To tackle these challenges, we adopt a new perspective by studying the structures of local solutions ink-means and propose a one-shot algorithm called FeCA (Federated Centroid Aggregation)."
  - [section]: "Recent theoretical works by Qian et al. (2021); Chen et al. (2024) have established a positive result that under certain separation conditions, all the local solutions share a common geometric structure."
- Break condition: If the separation conditions between true centers are too small, the structural properties of local solutions may not hold, making aggregation ineffective

### Mechanism 2
- Claim: Adaptive radius assignment enables effective grouping of centroids from different clients on the server
- Mechanism: Each client assigns a radius to its centroids based on the minimum pairwise distance between centroids. The server then groups centroids within these radii, prioritizing larger radii first to ensure correct clustering
- Core assumption: The radius assignment effectively bounds the maximum pairwise distance between centroids associated with the same true center
- Evidence anchors:
  - [abstract]: "FeCA adaptively refines local solutions on clients, then aggregates these refined solutions to recover the global solution of the entire dataset in a single round."
  - [section]: "The server then groups all received centroids based on these radii, prioritizing the largest ones first. This ensures that the smaller radii associated with many-fit-one centroids minimally impact the aggregation process."
- Break condition: If the radius assignment fails to bound the maximum pairwise distance between centroids of the same true center, incorrect grouping may occur

### Mechanism 3
- Claim: One-shot federated clustering outperforms iterative approaches by leveraging multiple local solutions
- Mechanism: Instead of iteratively averaging models, FeCA uses one communication round where clients refine their local solutions and send them to the server for aggregation. This reduces communication overhead while potentially achieving better global solutions
- Core assumption: Multiple local solutions contain more information about the global solution than a single averaged model
- Evidence anchors:
  - [abstract]: "By incorporating multiple clients' solutions, the central server could potentially recover the global optimal solution in one shot, akin to assembling a jigsaw puzzle of clients' solutions."
  - [section]: "This challenge is amplified in the federated setting, where each client's data is a distinct subset of the entire dataset. Even under the IID data sample scenario, each client's clustering results might be suboptimal local solutions containing spurious centroids far from the true global centroids."
- Break condition: If all clients converge to the same local solution, the one-shot approach loses its advantage and becomes equivalent to having a single client

## Foundational Learning

- Concept: Stochastic Ball Model
  - Why needed here: Provides the theoretical foundation for understanding the structure of local solutions in k-means clustering
  - Quick check question: What are the key assumptions of the Stochastic Ball Model, and how do they relate to the behavior of local k-means solutions?

- Concept: Federated Learning and Data Heterogeneity
  - Why needed here: Explains why standard federated approaches may fail in clustering tasks and motivates the need for FeCA
  - Quick check question: How does data heterogeneity across clients affect the performance of federated clustering algorithms, and what challenges does it introduce?

- Concept: Local Solutions in Non-convex Optimization
  - Why needed here: Understanding why local solutions in k-means can be suboptimal and how they relate to the global solution is crucial for designing effective aggregation strategies
  - Quick check question: Why are local solutions in k-means clustering problematic, and how can the structure of these solutions be exploited to recover the global solution?

## Architecture Onboarding

- Component map: ClientUpdate -> RadiusAssign -> ServerAggregation
- Critical path:
  1. Each client performs k-means on local data
  2. Clients refine solutions by removing one-fit-many centroids
  3. Clients assign radii to remaining centroids
  4. Centroids and radii are sent to the server
  5. Server aggregates centroids using radii to recover global solution
- Design tradeoffs:
  - One-shot vs. iterative approaches: One-shot reduces communication but requires effective local refinement
  - Radius assignment methods: Theoretical vs. empirical variants balance theoretical guarantees with practical applicability
  - Centroid elimination: Removing one-fit-many centroids simplifies aggregation but may reduce output centroid count in extreme cases
- Failure signatures:
  - All clients converging to the same local solution
  - Radius assignment failing to bound pairwise distances within groups
  - Insufficient number of output centroids when k' < k*
- First 3 experiments:
  1. Evaluate ℓ2-distance between recovered centroids and true centers on S-sets under IID and non-IID scenarios
  2. Test radius assignment effectiveness by measuring maximum pairwise distance within groups on synthetic data
  3. Compare FeCA performance against baselines (M-Avg, k-FED, FFCM) on CIFAR-10/100 using Purity and NMI metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does FeCA's performance scale with increasing numbers of clients beyond M=20?
- Basis in paper: [explicit] The paper discusses varying client numbers up to M=20 and observes consistent performance improvements, but does not explore beyond this range
- Why unresolved: The paper's experimental evaluation is limited to M=20, leaving the scaling behavior for larger client populations unexplored
- What evidence would resolve it: Conducting experiments with M>20 clients while maintaining the same data distribution per client would demonstrate whether FeCA's performance gains continue, plateau, or degrade with larger client counts

### Open Question 2
- Question: How robust is FeCA to non-spherical or non-Gaussian cluster distributions in the underlying data?
- Basis in paper: [inferred] The theoretical analysis assumes the Stochastic Ball Model, and empirical evaluation focuses on Gaussian clusters, suggesting potential limitations for other distributions
- Why unresolved: The paper does not test FeCA on datasets with arbitrary cluster shapes or distributions, leaving its applicability to real-world clustering scenarios uncertain
- What evidence would resolve it: Evaluating FeCA on datasets with varied cluster shapes (e.g., moon-shaped, concentric circles) and non-Gaussian distributions would demonstrate its robustness to realistic clustering scenarios

### Open Question 3
- Question: What is the impact of data dimensionality on FeCA's performance and computational efficiency?
- Basis in paper: [inferred] The paper evaluates FeCA on 2D synthetic data and pre-trained neural network features, but does not systematically explore high-dimensional scenarios
- Why unresolved: High-dimensional clustering poses unique challenges, and the paper does not investigate FeCA's behavior in such settings
- What evidence would resolve it: Conducting experiments on synthetic and real-world datasets with varying dimensions (e.g., 10D, 100D, 1000D) would reveal FeCA's scalability and performance in high-dimensional spaces

### Open Question 4
- Question: How does FeCA perform in federated settings with partial client participation or stragglers?
- Basis in paper: [inferred] The paper assumes full client participation in each round, which may not be realistic in federated learning scenarios
- Why unresolved: Real-world federated systems often face issues with client availability and reliability, which are not addressed in the current formulation
- What evidence would resolve it: Implementing FeCA with partial client participation and simulating client dropouts or delays would demonstrate its robustness to real-world federated learning challenges

## Limitations

- The algorithm's effectiveness depends on specific geometric structures in local solutions that may not generalize beyond controlled synthetic settings
- Theoretical guarantees rely on the Stochastic Ball Model assumptions, which may not hold in real-world federated scenarios with complex data distributions
- The paper doesn't adequately address potential privacy concerns beyond the standard federated learning framework

## Confidence

- High Confidence: Claims about FeCA's effectiveness in controlled synthetic settings (S-sets) with clear separation between clusters
- Medium Confidence: Claims regarding DeepFeCA's performance on real-world datasets
- Low Confidence: Theoretical guarantees about FeCA's performance under non-IID data distributions

## Next Checks

1. **Real-world data heterogeneity test**: Evaluate FeCA on a federated dataset with known non-IID characteristics (e.g., LEAF or Shakespeare dataset) to assess performance degradation compared to synthetic settings

2. **Privacy amplification analysis**: Quantify the privacy-utility tradeoff by implementing differential privacy mechanisms and measuring the impact on clustering quality across multiple privacy budgets

3. **Scalability benchmark**: Measure FeCA's computational complexity and communication costs on datasets with increasing dimensionality (d > 100) and client count (M > 100) to identify practical deployment limits