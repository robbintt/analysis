---
ver: rpa2
title: 'TEDDY: Trimming Edges with Degree-based Discrimination strategY'
arxiv_id: '2402.01261'
source_url: https://arxiv.org/abs/2402.01261
tags:
- graph
- teddy
- performance
- sparsity
- edges
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces TEDDY, a novel one-shot edge sparsification\
  \ framework for Graph Neural Networks (GNNs) that leverages structural information\
  \ to preserve principal information flow. The method uses degree-based discrimination\
  \ to identify important low-degree edges, followed by parameter sparsification via\
  \ projected gradient descent on the \u21130 ball."
---

# TEDDY: Trimming Edges with Degree-based Discrimination strategY

## Quick Facts
- arXiv ID: 2402.01261
- Source URL: https://arxiv.org/abs/2402.01261
- Reference count: 27
- Primary result: One-shot edge sparsification framework achieving 73.20% accuracy at 43.12% graph sparsity on Citeseer dataset with GIN

## Executive Summary
TEDDY introduces a novel one-shot edge sparsification framework for Graph Neural Networks (GNNs) that leverages structural information to preserve principal information flow. The method uses degree-based discrimination to identify important low-degree edges, followed by parameter sparsification via projected gradient descent on the ℓ0 ball. TEDDY achieves efficient and rapid realization of Graph Lottery Tickets (GLT) within a single training process. Experimental results demonstrate that TEDDY significantly outperforms conventional iterative approaches in generalization, even when conducting one-shot sparsification without considering node features. The method consistently achieves state-of-the-art performance across diverse benchmark datasets and architectures, with notable improvements in multi-layer GNNs like GAT and GIN.

## Method Summary
TEDDY is a one-shot edge sparsification framework for GNNs that operates in two stages: edge pruning and parameter sparsification. First, it computes edge-wise scores using degree-based discrimination, which measures the importance of edges based on their contribution to message passing and the degrees of their endpoints. Edges are then pruned based on these scores. Second, the sparse GNN is trained using projected gradient descent on the ℓ0 ball, which encourages sparsity in the model parameters. The method can be applied to any GNN architecture and achieves efficient and rapid realization of Graph Lottery Tickets (GLT) within a single training process, without the need for iterative pruning and retraining.

## Key Results
- TEDDY achieves 73.20% accuracy at 43.12% graph sparsity on the Citeseer dataset with GIN, a 5.4% improvement over vanilla GIN.
- The method consistently outperforms conventional iterative approaches in generalization across diverse benchmark datasets and architectures.
- TEDDY demonstrates notable improvements in multi-layer GNNs like GAT and GIN, highlighting its effectiveness in preserving information flow in deeper architectures.

## Why This Works (Mechanism)
TEDDY leverages structural information to preserve principal information flow in GNNs during edge sparsification. The degree-based discrimination mechanism identifies important low-degree edges that are crucial for maintaining the graph's connectivity and information propagation. By focusing on these edges, TEDDY ensures that the pruned graph retains its essential structure while reducing redundancy. The projected gradient descent on the ℓ0 ball further enforces sparsity in the model parameters, leading to efficient and compact representations. This combination of structural preservation and parameter sparsification enables TEDDY to achieve superior performance compared to conventional iterative approaches.

## Foundational Learning
- Graph Neural Networks (GNNs): Deep learning models designed to operate on graph-structured data, capturing node features and graph topology through message passing.
  - Why needed: Understanding GNNs is crucial for grasping the problem of edge sparsification and the motivation behind TEDDY.
  - Quick check: Review the message passing mechanism and common GNN architectures like GCN, GAT, and GIN.

- Graph Sparsification: The process of reducing the number of edges in a graph while preserving its essential properties and structure.
  - Why needed: Edge sparsification is the core problem addressed by TEDDY, and understanding its challenges is key to appreciating the method's contributions.
  - Quick check: Explore different graph sparsification techniques and their applications in various domains.

- Projected Gradient Descent on the ℓ0 Ball: An optimization technique that enforces sparsity in model parameters by projecting the gradients onto the ℓ0 ball during training.
  - Why needed: This technique is used in TEDDY for parameter sparsification, enabling efficient and compact representations.
  - Quick check: Study the mathematical formulation and implementation details of projected gradient descent on the ℓ0 ball.

## Architecture Onboarding

Component Map:
Pre-trained Dense GNN -> Edge-wise Score Computation -> Edge Pruning -> Sparse GNN Training (Projected GD on ℓ0 ball)

Critical Path:
The critical path in TEDDY involves computing edge-wise scores using degree-based discrimination, pruning edges based on these scores, and training the sparse GNN using projected gradient descent on the ℓ0 ball. This path ensures that the pruned graph retains its essential structure while promoting sparsity in the model parameters.

Design Tradeoffs:
- Computational efficiency vs. sparsification quality: TEDDY trades off some sparsification quality for computational efficiency by performing one-shot edge pruning instead of iterative pruning and retraining.
- Structural preservation vs. parameter sparsity: The method balances the need to preserve important graph structure with the goal of achieving sparse model parameters.

Failure Signatures:
- Poor performance: If TEDDY's edge pruning fails to identify important edges or if the projected gradient descent on the ℓ0 ball leads to unstable training, the method may exhibit poor performance compared to baselines.
- Over-pruning: Aggressive edge pruning may result in a graph that is too sparse, leading to information loss and reduced model performance.

First Experiments:
1. Evaluate TEDDY on the Cora dataset with a simple GNN architecture (e.g., GCN) to assess its basic functionality and performance.
2. Compare TEDDY's performance with conventional iterative edge pruning methods on the Citeseer dataset using a more complex GNN architecture (e.g., GAT) to highlight its advantages.
3. Investigate the impact of different edge pruning ratios on TEDDY's performance across multiple datasets and architectures to understand its sensitivity to the pruning aggressiveness.

## Open Questions the Paper Calls Out
None

## Limitations
- The paper does not provide exact hyperparameter values for learning rates, regularization coefficients, and sparsity targets, which may affect the reproducibility of the results.
- The implementation details for the Sinkhorn iterations in the WD-GLT baseline are not provided, which could lead to discrepancies in the comparison.
- The generalizability of the method to other graph datasets and architectures beyond those tested in the paper remains to be investigated.

## Confidence
- High: TEDDY consistently achieves state-of-the-art performance across diverse benchmark datasets and architectures, demonstrating its effectiveness in edge sparsification for GNNs.
- Medium: The generalizability of the method to other graph datasets and architectures beyond those tested in the paper is uncertain and requires further investigation.

## Next Checks
1. Perform ablation studies to evaluate the impact of individual components of the TEDDY framework, such as the degree-based discrimination and the projected gradient descent on the ℓ0 ball.
2. Investigate the scalability of the method to larger graph datasets and more complex architectures, such as graph transformers and graph attention networks with multiple attention heads.
3. Compare the performance of TEDDY with other state-of-the-art graph sparsification methods, such as those based on spectral graph theory or graph signal processing, to provide a more comprehensive evaluation of the method's effectiveness.