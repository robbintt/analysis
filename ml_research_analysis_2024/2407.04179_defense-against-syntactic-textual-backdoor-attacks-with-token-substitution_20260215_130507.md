---
ver: rpa2
title: Defense Against Syntactic Textual Backdoor Attacks with Token Substitution
arxiv_id: '2407.04179'
source_url: https://arxiv.org/abs/2407.04179
tags:
- hidden
- killer
- tokens
- attacks
- syntactic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of defending against syntactic
  textual backdoor attacks, which are more concealed than special token-based attacks.
  The proposed method identifies and replaces semantically meaningful words in sentences
  while preserving syntactic templates or special tokens, then compares predicted
  labels before and after substitution to detect poisoned samples.
---

# Defense Against Syntactic Textual Backdoor Attacks with Token Substitution

## Quick Facts
- **arXiv ID**: 2407.04179
- **Source URL**: https://arxiv.org/abs/2407.04179
- **Reference count**: 40
- **Key outcome**: Defends against syntactic and special token-based backdoor attacks with F1-scores above 94% (syntactic) and 98% (insertion-based)

## Executive Summary
This paper proposes a defense algorithm against syntactic textual backdoor attacks by identifying and replacing semantically meaningful words while preserving syntactic templates and special tokens. The method compares predicted labels before and after substitution to detect poisoned samples. The algorithm effectively defends against both syntactic backdoor attacks (like Hidden Killer) and special token-based attacks (like BadNet), achieving high F1-scores across three datasets (SST-2, AG News, DBpedia) while maintaining clean accuracy.

## Method Summary
The defense algorithm works by constructing sets of special tokens (based on POS tags and punctuation), low-frequency tokens, and a substitution dictionary mapping POS tags and labels to candidate replacement tokens. For each sentence, non-special and non-low-frequency tokens are substituted with alternatives from the dictionary, and predicted labels are compared before and after substitution. The process is repeated multiple times to reduce randomness. The method assumes backdoor triggers are embedded in non-semantic tokens and leverages label consistency after substitution to detect poisoned sentences.

## Key Results
- Achieves F1-scores above 94% on average against syntactic backdoor attacks
- Achieves F1-scores above 98% against insertion-based backdoor attacks
- Successfully defends against both Hidden Killer (syntactic) and BadNet (special token) attack methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The algorithm detects poisoned sentences by substituting semantic content while preserving triggers, then comparing label consistency.
- Core assumption: Backdoor triggers are embedded in non-semantic tokens that don't affect meaning when substituted.
- Evidence: If syntactic template stays unaltered, prediction label persists even with word substitution.

### Mechanism 2
- Claim: The algorithm constructs substitution dictionary mapping POS tags and labels to candidate tokens.
- Core assumption: High-confidence tokens for each class can serve as effective substitutes.
- Evidence: Tokens with fl(t, θ*) > 0.95 are considered highly associated with class l.

### Mechanism 3
- Claim: Low-frequency tokens and special tokens are preserved during substitution.
- Core assumption: Backdoor triggers lurk in tokens that don't contribute to semantic meaning and are rarely seen in natural text.
- Evidence: Triggers are more likely in words like "if", "however", "though", etc., and low-frequency tokens.

## Foundational Learning

- **Concept**: Part-of-Speech (POS) tagging and its role in identifying function words vs content words
  - Why needed: Algorithm relies on POS tagging to separate trigger candidates from substitutable content words
  - Quick check: Can you explain why "if", "however", and punctuation marks are more likely trigger candidates than nouns or adjectives?

- **Concept**: Threshold-based percentile selection for building frequency-based token sets
  - Why needed: Algorithm uses 80th percentile threshold to separate high-frequency from low-frequency tokens
  - Quick check: With frequency distribution [5, 10, 15, 20, 25, 30] and 80th percentile, what frequency threshold separates high/low frequency tokens?

- **Concept**: Statistical hypothesis testing through repeated sampling
  - Why needed: Algorithm repeats substitution process Niter times and uses threshold ζ to determine statistical significance
  - Quick check: If 8 out of 10 substitution trials maintain same label, what proportion would you compare against threshold ζ?

## Architecture Onboarding

- **Component map**: Input sentence processor -> POS tagger -> Token classifier (special/low-frequency vs substitutable) -> Substitution dictionary lookup -> Label comparison engine -> Decision output
- **Critical path**: Sentence → Token classification → Substitution generation → Label comparison → Poison detection decision
- **Design tradeoffs**: Niter vs. detection accuracy (higher Niter improves reliability but increases computation time); percentile thresholds vs. sensitivity (lower thresholds capture more tokens but risk false positives)
- **Failure signatures**: High false negatives (check if triggers use semantic words); high false positives (check if substitution candidates are too semantically similar)
- **First 3 experiments**:
  1. Run algorithm on clean SST-2 sentences with Niter=1, then Niter=10, then Niter=20 to observe detection rate changes
  2. Test algorithm on sentences containing Hidden Killer trigger "when ..., ..." by substituting semantic words and verifying label consistency
  3. Compare algorithm performance against BadNet vs. Hidden Killer to validate dual-defense capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the defense algorithm perform against syntactic backdoor attacks that violate the assumption of not altering semantic meaning?
- Basis: The algorithm's intuition is built on the assumption that backdoor attacks don't alter semantic meaning, but performance when this assumption is violated is untested
- Resolution: Experimental results showing performance against attacks that do alter semantic meaning

### Open Question 2
- Question: Can the defense algorithm be extended to handle dynamically constructed triggers, as seen in BITE attacks?
- Basis: BITE uses dynamically constructed triggers that disable the word substitution dictionary, leading to unsatisfactory performance
- Resolution: Development and evaluation of enhanced algorithm that can handle dynamically constructed triggers

### Open Question 3
- Question: How does the defense algorithm compare to other state-of-the-art backdoor defense methods in terms of computational efficiency and scalability?
- Basis: While ONION is mentioned as baseline, detailed analysis of computational efficiency and scalability is lacking
- Resolution: Comprehensive comparison of computational efficiency and scalability with other defense methods

## Limitations

- The method assumes backdoor triggers are embedded in non-semantic tokens, making it vulnerable to attacks using semantic triggers
- Performance heavily depends on threshold choices (80th percentile for frequency, 95th percentile for label association) without thorough exploration
- Evaluation is limited to two specific attack methods and three datasets, limiting generalizability

## Confidence

- **High confidence**: Basic mechanism of substituting semantic content while preserving trigger integrity is sound and technically feasible
- **Medium confidence**: Effectiveness against real-world attacks and generalizability across datasets and attack types is uncertain
- **Low confidence**: Claims about defending against sophisticated attacks using semantic triggers are not supported

## Next Checks

1. Test against semantic trigger attacks by applying defense algorithm to sentences poisoned with triggers embedded in semantically meaningful words (nouns, verbs, adjectives) rather than function words
2. Perform hyperparameter sensitivity analysis by systematically varying frequency threshold (70th-90th percentile) and label association threshold (90th-98th percentile) across all three datasets
3. Conduct cross-dataset transferability test by training defense algorithm on SST-2 and evaluating performance on AG News and DBpedia without retraining