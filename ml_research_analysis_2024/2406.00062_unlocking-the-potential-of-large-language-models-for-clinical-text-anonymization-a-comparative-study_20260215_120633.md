---
ver: rpa2
title: 'Unlocking the Potential of Large Language Models for Clinical Text Anonymization:
  A Comparative Study'
arxiv_id: '2406.00062'
source_url: https://arxiv.org/abs/2406.00062
tags:
- anonymization
- clinical
- text
- data
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes six new evaluation metrics tailored to the
  challenges of generative anonymization with LLMs and compares their performance
  against two baseline techniques on MIMIC III clinical notes. The results establish
  LLM-based models, especially fine-tuned generative models, as a reliable alternative
  to common approaches, demonstrating superior anonymization sensitivity and clinical
  information retention.
---

# Unlocking the Potential of Large Language Models for Clinical Text Anonymization: A Comparative Study

## Quick Facts
- arXiv ID: 2406.00062
- Source URL: https://arxiv.org/abs/2406.00062
- Reference count: 15
- LLM-based models, especially fine-tuned generative models, achieve high recall rates for direct and quasi-identifiers while preserving critical clinical information better than traditional methods like Presidio and KNEO.

## Executive Summary
This study proposes six new evaluation metrics tailored to the challenges of generative anonymization with large language models (LLMs) and compares their performance against two baseline techniques on MIMIC III clinical notes. The results establish LLM-based models, especially fine-tuned generative models, as a reliable alternative to common approaches, demonstrating superior anonymization sensitivity and clinical information retention. The study highlights the potential of LLM-based models to enable trustworthy anonymization of clinical text, facilitating broader sharing of sensitive health data for secondary usage while maintaining patient privacy.

## Method Summary
The study compares LLM-based methods with traditional approaches for clinical text anonymization, proposing new evaluation metrics. It uses the MIMIC III dataset (66,645 discharge summary notes) and pre-trained LLMs (BERT variants, Phi-2, Llama-3) with INCOGNITUS toolbox baselines. The objective is to evaluate anonymization sensitivity and clinical information retention using six proposed metrics (ALID, LR, LRDI, LRQI, JSC, NSDCG, SMR). The method involves fine-tuning BERT and generative LLMs on NER tasks with MIMIC III training data and evaluating on test sets using the proposed metrics.

## Key Results
- Fine-tuned LLM models achieve high recall rates for direct and quasi-identifiers, significantly outperforming traditional methods like Presidio and KNEO.
- The Levenshtein-based metrics enable fair evaluation of generative anonymization by measuring string similarity without requiring token-label mapping.
- Clinical information retention is better preserved with fine-tuned LLMs compared to baseline methods, as measured by JSC and NSDCG metrics.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Levenshtein-based metrics enable fair evaluation of generative anonymization methods by measuring string similarity without requiring token-label mapping.
- Mechanism: The Levenshtein Ratio (LRa) computes similarity between sensitive entities in original text and windows of equal length in anonymized text, allowing comparison even when entities are replaced or removed.
- Core assumption: Sensitive entities in anonymized text can be detected through maximum similarity windows even if spelling is altered.
- Evidence anchors:
  - [abstract] The paper proposes six new evaluation metrics tailored to generative anonymization challenges, including Levenshtein-based metrics.
  - [section 3.1] "We first propose two metrics based on the concept of LRa: the Average Levenshtein Index of Dissimilarity (ALID) and the Levenshtein Recall (LR)."
  - [corpus] Weak evidence - corpus neighbors don't directly address Levenshtein-based evaluation methods.
- Break condition: When sensitive entities are completely removed without replacement, making similarity detection impossible.

### Mechanism 2
- Claim: Fine-tuning open-source LLMs on clinical text anonymization tasks significantly improves performance compared to zero-shot approaches.
- Mechanism: Domain-specific fine-tuning adapts the model's understanding of clinical text structure and sensitive entity patterns, enabling more precise anonymization while retaining clinical information.
- Core assumption: Clinical text has specific structural patterns that can be learned through fine-tuning.
- Evidence anchors:
  - [section 4.3] "Looking specifically at the generative methods, all of them were consistent in terms of recall. However, one can notice that an increase in the number of parameters of the model has a slight positive impact across all metrics."
  - [section 5] "Fine-tuned models have a better understanding of the structure of the clinical text and are able to retain crucial information while anonymizing sensitive entities."
  - [corpus] Weak evidence - corpus neighbors don't provide direct evidence about fine-tuning effectiveness.
- Break condition: When fine-tuning data is insufficient or doesn't capture the diversity of clinical text patterns.

### Mechanism 3
- Claim: The combination of Levenshtein-based privacy metrics and BioBERT-based clinical information retention metrics provides a comprehensive evaluation framework for anonymization methods.
- Mechanism: Privacy metrics (ALID, LR, LRDI, LRQI) assess whether sensitive information is properly masked, while clinical retention metrics (JSC, NSDCG) measure information preservation using a pre-trained ICD-10 classification model.
- Core assumption: A BioBERT model trained on ICD-10 classification can effectively measure clinical information retention after anonymization.
- Evidence anchors:
  - [section 3.2] "To assess the impact of anonymization on the preservation of clinical concepts, two new metrics were developed... leveraging an openly available BioBERT model pre-trained on a hierarchical classification task of ICD-10 code categories."
  - [section 5] "Looking specifically at the generative methods, all of them were consistent in terms of recall. However, one can notice that an increase in the number of parameters of the model has a slight positive impact across all metrics."
  - [corpus] Weak evidence - corpus neighbors don't directly address the comprehensive evaluation framework.
- Break condition: When the BioBERT model fails to generalize to anonymized text structures or when Levenshtein-based metrics miss subtle re-identification patterns.

## Foundational Learning

- Concept: Named Entity Recognition (NER)
  - Why needed here: NER forms the basis for identifying sensitive entities in clinical text that need to be anonymized.
  - Quick check question: What are the main types of entities typically identified in clinical text anonymization?

- Concept: Levenshtein Distance
  - Why needed here: Levenshtein Distance enables comparison of strings when entities are altered or removed during anonymization.
  - Quick check question: How does Levenshtein Distance differ from simple string matching in evaluating anonymization effectiveness?

- Concept: Transformer-based language models
  - Why needed here: Transformer architectures power the LLMs used for both text encoding and generative anonymization approaches.
  - Quick check question: What is the key architectural difference between BERT (encoder-only) and GPT (decoder-only) transformers?

## Architecture Onboarding

- Component map:
  - MIMIC III dataset -> preprocessing -> train/validation/test splits
  - Baseline methods: Presidio + Spacy, KNEO
  - LLM methods: ClinicalBERT, Phi-2, Llama-3 (fine-tuned and zero-shot)
  - Evaluation framework: Six proposed metrics + SMR
  - Infrastructure: GPU resources for model training and inference

- Critical path:
  1. Data preparation and entity insertion
  2. Baseline method implementation and evaluation
  3. LLM fine-tuning and zero-shot evaluation
  4. Comprehensive metric calculation across all methods
  5. Comparative analysis and result interpretation

- Design tradeoffs:
  - Privacy vs. utility: More aggressive anonymization increases privacy but reduces clinical information retention
  - Model size vs. performance: Larger models generally perform better but require more computational resources
  - Fine-tuning vs. zero-shot: Fine-tuned models are more precise but require labeled training data

- Failure signatures:
  - Low LR scores with high JSC: Model is preserving clinical information but failing to mask sensitive entities
  - High LR scores with low JSC: Model is masking entities but over-anonymizing, removing useful clinical information
  - Inconsistent metric results: Potential issues with threshold selection or metric implementation

- First 3 experiments:
  1. Run all baseline methods on a small subset of data and verify metric calculations
  2. Fine-tune a small LLM (Phi-2) on a limited dataset and compare performance to zero-shot baseline
  3. Test Levenshtein-based metrics on known anonymization cases to validate implementation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed evaluation metrics (ALID, LR, LRDI, LRQI, JSC, NSDCG) perform across different clinical note types and languages beyond MIMIC III discharge summaries?
- Basis in paper: [explicit] The paper evaluates metrics on MIMIC III discharge summaries but acknowledges limitations regarding generalizability to other note types and languages.
- Why unresolved: The study is limited to one dataset type (discharge summaries) and language (English), preventing conclusions about metric performance in other clinical contexts.
- What evidence would resolve it: Testing the metrics across multiple clinical note types (nursing notes, radiology reports, ECGs) and languages, comparing performance consistency and identifying potential metric adjustments needed for different clinical contexts.

### Open Question 2
- Question: What is the optimal threshold value for each metric across different clinical text anonymization scenarios?
- Basis in paper: [explicit] The paper mentions that LR, LRDI, LRQI, and JSC are dependent on thresholds, which may require adjustments for each case study, and determining optimal thresholds poses a challenge.
- Why unresolved: The paper uses fixed threshold values (0.85 for LR-based metrics, 0.05 for JSC) based on experimental findings but doesn't provide a systematic method for threshold selection across different scenarios.
- What evidence would resolve it: Developing a framework for threshold optimization that considers different clinical text characteristics, privacy requirements, and information retention needs, validated across multiple clinical datasets.

### Open Question 3
- Question: How can the information retention metrics (JSC, NSDCG) be improved to better handle highly anonymized clinical text where the BioBERT model fails?
- Basis in paper: [explicit] The paper acknowledges that using BioBERT to compare logit distributions can be faulty when clinical notes are highly anonymized, as the text classifier was not specifically fine-tuned on anonymized text.
- Why unresolved: The current information retention metrics rely on a BioBERT model pre-trained on ICD-10 classification, which may not capture the semantic content of highly anonymized text and introduces potential biases.
- What evidence would resolve it: Developing alternative ground truth models specifically trained on anonymized clinical text, or creating new information retention metrics that don't rely on text classification models, validated against human expert judgments of clinical information preservation.

## Limitations

- The evaluation framework relies on synthetic data augmentation through entity insertion, which may not fully capture real clinical text anonymization complexity.
- The Levenshtein-based metrics depend on threshold selection (λ) that may affect results and requires careful calibration.
- The clinical information retention metrics are based on a BioBERT model trained for ICD-10 classification, which may not comprehensively capture all clinically relevant information.

## Confidence

**High Confidence**: The superiority of fine-tuned LLM-based methods over zero-shot approaches and traditional methods for clinical text anonymization is well-supported by experimental results across multiple evaluation metrics.

**Medium Confidence**: The effectiveness of the proposed Levenshtein-based metrics for evaluating generative anonymization methods is supported by theoretical justification and implementation details, but optimal threshold selection remains an area for further validation.

**Low Confidence**: The generalizability of findings to clinical text types beyond discharge summaries and to real-world deployment scenarios with naturally occurring sensitive information patterns requires additional investigation.

## Next Checks

1. **Threshold Sensitivity Analysis**: Systematically vary the λ threshold parameter in the Levenshtein-based metrics and assess its impact on evaluation outcomes across different LLM sizes and fine-tuning conditions.

2. **Real Data Validation**: Apply the proposed framework to a dataset of clinical notes with naturally occurring sensitive information (not synthetically inserted) to validate performance claims in realistic conditions.

3. **Clinical Utility Assessment**: Conduct a blinded evaluation with clinical experts to assess whether the anonymized text generated by top-performing LLM models maintains sufficient clinical utility for secondary usage scenarios.