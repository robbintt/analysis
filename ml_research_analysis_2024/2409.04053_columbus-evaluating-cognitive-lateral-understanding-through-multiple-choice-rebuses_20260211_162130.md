---
ver: rpa2
title: 'COLUMBUS: Evaluating COgnitive Lateral Understanding through Multiple-choice
  reBUSes'
arxiv_id: '2409.04053'
source_url: https://arxiv.org/abs/2409.04053
tags:
- puzzle
- puzzles
- rebus
- rules
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces COLUMBUS, a synthetic benchmark for evaluating
  visual lateral thinking in vision-language models (VLMs). The authors define visual
  lateral thinking as a multiple-choice question-answering task using rebus puzzles
  and propose a three-step taxonomy-driven methodology for generating such puzzles.
---

# COLUMBUS: Evaluating COgnitive Lateral Understanding through Multiple-choice reBUSes

## Quick Facts
- arXiv ID: 2409.04053
- Source URL: https://arxiv.org/abs/2409.04053
- Reference count: 40
- Primary result: Substantial performance gap between humans and VLMs on visual lateral thinking tasks

## Executive Summary
This paper introduces COLUMBUS, a synthetic benchmark designed to evaluate visual lateral thinking in vision-language models (VLMs) through rebus puzzles. The benchmark addresses the challenge of measuring creative, out-of-the-box thinking in models by providing a structured methodology for generating over 1,000 multiple-choice rebus puzzles. The authors define visual lateral thinking as the ability to interpret visual-spatial relationships and abstract symbolic representations, and demonstrate that while state-of-the-art VLMs achieve decent accuracy, they lag significantly behind human performance.

## Method Summary
COLUMBUS employs a three-step taxonomy-driven methodology for generating rebus puzzles. The process begins with natural language phrases (idioms, compounds), which are transformed into structured graphs using a rule taxonomy that encodes visual-spatial and linguistic relationships. These graphs are then rendered into images, and distractor options are sampled based on orthographic and semantic similarity to the correct answer. The benchmark includes both text-based and icon-based puzzles, with evaluation showing that models benefit from human-curated descriptions but struggle to self-generate appropriate representations at the right level of abstraction.

## Key Results
- Significant performance gap between humans and VLMs on COLUMBUS benchmark
- VLMs benefit from human-curated graph descriptions, showing 11.91-14.9% performance improvement
- State-of-the-art VLMs achieve decent accuracy but lag substantially behind human performance
- Models struggle with self-generating appropriate abstractions for rebus interpretation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph-based puzzle generation constrains the abstraction level required for visual lateral thinking
- Mechanism: The three-step taxonomy-driven pipeline transforms natural language phrases into structured graphs, which are then rendered into rebus puzzles. This structured representation explicitly encodes both individual and relational rules (e.g., color, size, positioning), ensuring that the visual cues are interpretable and logically connected to the answer
- Core assumption: A graph structure is sufficient to capture all the visual-spatial and linguistic relationships needed for a rebus puzzle to be solvable
- Evidence anchors:
  - [abstract] "The puzzle rendering step leverages this taxonomy to create a graph representation for a puzzle answer and generate an image for the graph."
  - [section 3.2] "We generate a directed, attributed graph whose nodes are elements that will be rendered into a puzzle image."
  - [corpus] Found 25 related papers; average neighbor FMR=0.213. Neighbor papers include "Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint" suggesting this is a recognized challenge area
- Break condition: If the graph structure fails to encode a necessary visual or relational attribute, the puzzle may become unsolvable or ambiguous, leading to performance degradation

### Mechanism 2
- Claim: Human-curated descriptions significantly improve model performance by bridging the abstraction gap
- Mechanism: Providing models with additional context (graph nodes, edges, or puzzle nature) supplements their inherent limitations in inferring implicit visual-spatial relationships. This acts as a scaffold, enabling the model to map the visual elements to the correct answer more effectively
- Core assumption: The abstraction level of the puzzle is too high for the model to infer without explicit guidance
- Evidence anchors:
  - [abstract] "VLMs benefit from human-curated descriptions but struggle to self-generate such representations at the right level of abstraction."
  - [section 6.3] "Adding a description of the graph nodes (prompt 3) increases the model performance by 11.91% and 14.9% for non-icon and icon puzzles, respectively."
  - [corpus] "Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint" indicates models struggle with rebus puzzles, aligning with this mechanism
- Break condition: If the human-curated descriptions are incomplete or misleading, they may introduce noise or bias, potentially harming performance

### Mechanism 3
- Claim: The choice of distractors based on orthographic and semantic similarity increases task difficulty and tests true lateral thinking
- Mechanism: Distractors are sampled by identifying compounds or phrases that overlap with or are semantically similar to the puzzle's answer. This forces the model to distinguish between plausible but incorrect answers based on subtle visual or semantic cues, rather than simple keyword matching
- Core assumption: Semantic and orthographic similarity creates plausible but incorrect alternatives that require genuine reasoning to reject
- Evidence anchors:
  - [section 3.3] "Distractor sampling (blue part in Figure 2) selects the three most similar compounds or phrases to the input semi-automatically."
  - [section 4] "The similarity uses a λ-weighted combination of Jaccard word overlap (Leskovec, Rajaraman, and Ullman 2014) and cosine similarity using Sentence-BERT embeddings (Reimers and Gurevych 2019)."
  - [corpus] Weak corpus evidence; the focus is more on benchmark construction than distractor design analysis
- Break condition: If distractors are too similar or too dissimilar, the task may become either trivially easy or impossibly hard, skewing the evaluation of lateral thinking ability

## Foundational Learning

- Concept: Abstract Visual Reasoning (AVR)
  - Why needed here: COLUMBUS tests the ability to interpret visual-spatial relationships and abstract symbolic representations, which are core components of AVR
  - Quick check question: Can you explain how a rebus puzzle requires interpreting both the visual arrangement of elements and their symbolic meaning?

- Concept: Graph Representation of Visual Data
  - Why needed here: The puzzle generation process relies on converting phrases into attributed graphs, which then guide image rendering. Understanding graph structures is essential for both generating and solving these puzzles
  - Quick check question: How would you represent the phrase "big deal" as a graph, given the rules for size and relational positioning?

- Concept: Semantic Similarity and Embedding Models
  - Why needed here: Distractor sampling uses Sentence-BERT embeddings to find semantically similar phrases, ensuring that distractors are plausible but incorrect
  - Quick check question: What is the difference between Jaccard similarity and cosine similarity in the context of distractor selection?

## Architecture Onboarding

- Component map: Input phrases -> Graph generation -> Image rendering -> Distractor sampling -> Multiple-choice output
- Critical path:
  1. Parse input phrase into graph using rule taxonomy
  2. Render graph into rebus image
  3. Generate distractors based on similarity scoring
  4. Evaluate model performance on the resulting puzzles
- Design tradeoffs:
  - Manual vs. automatic puzzle generation: Manual generation ensures quality but lacks scalability; automatic generation enables large-scale benchmarking but may introduce noise
  - Complexity of graph structures: More complex graphs increase puzzle difficulty but may also increase generation and solving complexity
  - Choice of distractors: Balancing orthographic and semantic similarity ensures plausible distractors without making the task too easy or too hard
- Failure signatures:
  - Low model accuracy despite high-quality puzzles: Indicates a gap in the model's lateral thinking ability or insufficient context
  - Inconsistent performance across puzzle types (text vs. icon): Suggests perceptual biases or weaknesses in handling specific visual attributes
  - High variance in distractor quality: Points to issues in the similarity scoring or sampling process
- First 3 experiments:
  1. Evaluate model performance on COLUMBUS with and without graph descriptions to quantify the impact of additional context
  2. Compare model accuracy on puzzles with different rule combinations (e.g., individual vs. relational) to identify specific weaknesses
  3. Test the effect of varying the λ weight in distractor sampling to optimize the balance between orthographic and semantic similarity

## Open Questions the Paper Calls Out
None

## Limitations
- Automatic puzzle generation may produce puzzles that differ systematically from human-created ones in ways that affect difficulty
- Reliance on publicly available phrase collections limits diversity and cultural specificity of puzzles
- Benchmark construction focuses on English compounds and common phrases, potentially underestimating model capabilities on more varied lateral thinking tasks

## Confidence
- High: Primary claim of substantial human-VLM performance gap is supported by direct empirical comparison
- Medium: Mechanism linking graph-based generation to abstraction level shows correlation but not definitive causation
- Low-Medium: Distractor similarity mechanism has limited direct evidence from corpus analysis

## Next Checks
1. Conduct a human evaluation comparing model-generated puzzles to hand-crafted puzzles from "Puzzled by Puzzles" to identify systematic differences in difficulty and solvability
2. Test COLUMBUS across multiple cultural contexts by translating puzzles and evaluating whether performance gaps persist, validating the benchmark's cultural specificity assumptions
3. Perform ablation studies varying the λ weight in distractor sampling across a broader range to identify optimal similarity thresholds and their impact on lateral thinking assessment