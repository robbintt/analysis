---
ver: rpa2
title: Adapting Large Language Models for Document-Level Machine Translation
arxiv_id: '2401.06468'
source_url: https://arxiv.org/abs/2401.06468
tags:
- translation
- docmt
- language
- computational
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study adapts moderately-sized large language models (7B parameters)
  for document-level machine translation across nine language pairs. The researchers
  investigate prompt strategies and fine-tuning methods (parameter-efficient fine-tuning
  vs.
---

# Adapting Large Language Models for Document-Level Machine Translation

## Quick Facts
- arXiv ID: 2401.06468
- Source URL: https://arxiv.org/abs/2401.06468
- Authors: Minghao Wu; Thuy-Trang Vu; Lizhen Qu; George Foster; Gholamreza Haffari
- Reference count: 24
- Primary result: Parameter-efficient fine-tuning (LoRA) outperforms full fine-tuning for LLM-based document-level translation, but faces challenges with error propagation during decoding

## Executive Summary
This study adapts moderately-sized large language models (7B parameters) for document-level machine translation across nine language pairs. The researchers investigate prompt strategies and fine-tuning methods, comparing parameter-efficient fine-tuning with LoRA against full fine-tuning approaches. Results show that specialized LLM-based models can outperform GPT-4 in certain translation tasks but face challenges with off-target translations due to error propagation during decoding. The study highlights both the strengths and limitations of LLM-based document-level translation models, particularly the trade-offs between parameter efficiency and multilingual capability.

## Method Summary
The study employs a two-stage training approach: first fine-tuning LLMs on monolingual documents from the CulturaX corpus, then fine-tuning on parallel documents from IWSLT2017. Three LLM backbones (Llama 2-7B, BLOOM-7B, VICUNA-7B) are evaluated using both parameter-efficient fine-tuning with LoRA (updating ~8M parameters or 0.1% of total) and full fine-tuning approaches. The models are evaluated using Prompt 4 with REUSE and REGEN inference strategies, and compared against baselines including GPT-4, NLLB, and DOCMT models using sBLEU, dBLEU, and COMET metrics.

## Key Results
- Parameter-efficient fine-tuning with LoRA outperforms full fine-tuning overall for bilingual document-level translation tasks
- LLM-based models show better generalization on out-of-domain text compared to conventional translation models
- Base LLMs demonstrate superior zero-shot cross-lingual transfer performance compared to instruction-tuned models
- Error propagation during decoding causes high rates of off-target translations, particularly when using REUSE strategy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Parameter-efficient fine-tuning (PEFT) with LoRA outperforms full fine-tuning (FFT) for bilingual document-level machine translation tasks.
- Mechanism: LoRA modifies only a small subset of model parameters (about 8M parameters or 0.1% of total), allowing the model to retain pre-trained linguistic knowledge while adapting to specific translation contexts. This selective parameter update prevents catastrophic forgetting and maintains the model's general language understanding capabilities.
- Core assumption: The linguistic knowledge encoded in the pre-trained LLM backbone is sufficient for translation tasks when properly adapted through LoRA.
- Evidence anchors:
  - [abstract] "Our research indicates that the PEFT approach outperforms the FFT approach overall"
  - [section 5] "models using LORA generally outperform fully fine-tuned (FFT) LLMs in bilingual translations"
  - [corpus] Weak - no direct corpus evidence, but related papers discuss LoRA effectiveness for NLP tasks
- Break condition: When translation tasks require extensive parameter updates beyond what LoRA can provide, such as multilingual translation tasks or highly specialized domains.

### Mechanism 2
- Claim: Error propagation during decoding causes high rates of off-target translations in LLM-based document-level translation.
- Mechanism: When using previous translations as context (REUSE strategy), errors in earlier sentences propagate to subsequent translations, leading to increasingly degraded output quality and potential language drift.
- Core assumption: Translation errors in context sentences directly influence the model's ability to generate accurate subsequent translations.
- Evidence anchors:
  - [abstract] "face issues like off-target translation due to error propagation in decoding"
  - [section 6] "the high off-target translation rate in Table 3 is due to error propagation during the decoding stage"
  - [corpus] Weak - corpus shows related work on error propagation but no direct evidence
- Break condition: When the REGEN strategy (re-generating all context translations) is used, which significantly reduces off-target translation rates.

### Mechanism 3
- Claim: Fine-tuning LLMs on monolingual documents before parallel document fine-tuning improves multilingual translation performance.
- Mechanism: Initial fine-tuning on monolingual documents addresses the English-centric nature of pre-trained LLMs, improving their ability to handle target languages before being trained on translation tasks.
- Core assumption: Pre-trained LLMs have insufficient multilingual representation that can be remedied through monolingual fine-tuning.
- Evidence anchors:
  - [section 3.1] "Our initial step involves fine-tuning all the parameters of LLMs using monolingual data from the target languages"
  - [section 6] "B-7B-L ORA consistently maintains low off-target rates, likely due to BLOOM-7B's multilingual pre-training"
  - [corpus] Weak - corpus shows related work on multilingual fine-tuning but no direct evidence
- Break condition: When the base LLM already has strong multilingual capabilities (like BLOOM), making monolingual fine-tuning less critical.

## Foundational Learning

- Concept: Document-level context in machine translation
  - Why needed here: The study focuses on document-level translation, which requires understanding discourse phenomena across multiple sentences
  - Quick check question: How does document-level translation differ from sentence-level translation in terms of context requirements?

- Concept: Parameter-efficient fine-tuning methods (LoRA)
  - Why needed here: The study compares PEFT vs full fine-tuning approaches for adapting LLMs to translation tasks
  - Quick check question: What percentage of parameters does LoRA typically update compared to full fine-tuning?

- Concept: Zero-shot cross-lingual transfer
  - Why needed here: The study investigates whether translation capabilities learned from one language pair can transfer to other languages
  - Quick check question: What is the difference between zero-shot and few-shot cross-lingual transfer in the context of translation?

## Architecture Onboarding

- Component map: LLM backbone (Llama 2-7B, BLOOM-7B, VICUNA-7B) → Monolingual fine-tuning stage → Parallel document fine-tuning stage → Inference with context handling (REUSE/REGEN)
- Critical path: Monolingual fine-tuning → Parallel document fine-tuning → Inference with context handling
- Design tradeoffs: PEFT (LoRA) vs FFT - parameter efficiency vs. multilingual capability; REUSE vs REGEN - inference efficiency vs. translation accuracy
- Failure signatures: High off-target translation rates (>90% in some cases), failure to converge during fine-tuning, significant performance drops on out-of-domain text
- First 3 experiments:
  1. Test monolingual fine-tuning effectiveness by comparing base LLM performance vs. monolingually fine-tuned LLM on simple translation tasks
  2. Compare PEFT vs FFT on bilingual translation tasks with limited training data to observe data efficiency differences
  3. Test REUSE vs REGEN strategies on a small document translation task to quantify error propagation effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the REGEN inference strategy consistently improve translation quality across all language pairs and model architectures?
- Basis in paper: [explicit] The paper mentions that REGEN significantly improves translation quality compared to REUSE, with a four-fold increase in inference cost, but only provides detailed results for one language pair (Arabic-English).
- Why unresolved: The study only tested REGEN on Arabic-English translation, leaving uncertainty about its effectiveness across different language pairs and model types.
- What evidence would resolve it: Systematic testing of REGEN across all 18 translation tasks with each model architecture (L-7B-LORA, B-7B-LORA, V-7B-LORA, L-7B-FFT, B-7B-FFT, V-7B-FFT) would clarify whether the quality improvement generalizes.

### Open Question 2
- Question: What is the optimal training strategy (one-stage, two-stage, or three-stage) for LLM-based DOCMT models across different language resource scenarios?
- Basis in paper: [explicit] The paper compares three training strategies but finds them all suboptimal, noting that results vary between high-performing languages (Dutch, Romanian) and low-performing languages (Arabic, Chinese).
- Why unresolved: The study only tested these strategies on a limited set of languages and didn't explore intermediate approaches or adaptive strategies based on resource availability.
- What evidence would resolve it: Comprehensive testing across all language pairs with varying amounts of parallel data, including exploration of hybrid strategies that adapt based on language pair characteristics.

### Open Question 3
- Question: How can error propagation during decoding be effectively mitigated without incurring prohibitive computational costs?
- Basis in paper: [explicit] The paper identifies error propagation during decoding as the primary cause of off-target translations and poor performance, noting that REGEN helps but increases inference cost fourfold.
- Why unresolved: The study only tested two extreme inference strategies (REUSE vs REGEN) without exploring intermediate solutions or architectural modifications to address error propagation.
- What evidence would resolve it: Development and testing of novel decoding strategies that balance quality and efficiency, such as selective re-generation, error detection mechanisms, or architectural modifications to improve error resilience.

## Limitations
- Limited model and language coverage with only three 7B parameter LLM backbones and nine language pairs tested
- Lack of comparison with state-of-the-art document-level translation systems beyond IWSLT2017 dataset
- Absence of ablation studies isolating the effects of monolingual fine-tuning on final translation quality
- Focus on 7B parameter models raises questions about scalability to larger models and different architectural families

## Confidence
- **High confidence**: Error propagation during decoding causes high off-target translation rates (directly observed across multiple experiments)
- **Medium confidence**: LoRA-based PEFT outperforms FFT for bilingual translation (based on limited model and language pair comparisons)
- **Low confidence**: Base LLMs perform better than instruction-tuned models for zero-shot cross-lingual transfer (limited sample size and comparisons)

## Next Checks
1. **Error propagation mitigation**: Conduct controlled experiments comparing REUSE vs REGEN strategies on documents with known translation errors to quantify the exact reduction in off-target rates and identify thresholds where error propagation becomes critical.

2. **Model generalization**: Test the PEFT and FFT models on out-of-domain document translation tasks (e.g., different domains than IWSLT2017) to verify the claimed generalization advantages over conventional translation models.

3. **Zero-shot transfer validation**: Expand the zero-shot cross-lingual transfer experiments to include more language pairs and model variations, particularly comparing base vs. instruction-tuned versions of the same model families to isolate the effect of instruction tuning on transfer performance.