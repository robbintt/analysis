---
ver: rpa2
title: 'AffineQuant: Affine Transformation Quantization for Large Language Models'
arxiv_id: '2403.12544'
source_url: https://arxiv.org/abs/2403.12544
tags:
- uni00000013
- quantization
- uni00000018
- matrix
- uni00000014
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AffineQuant introduces affine transformation for post-training
  quantization of LLMs, expanding the optimization space beyond simple scaling to
  reduce quantization errors, especially in low-bit configurations. It employs a gradual
  mask method based on the Levy-Desplanques theorem to ensure matrix invertibility
  during optimization, starting with diagonal elements and progressively extending
  to others.
---

# AffineQuant: Affine Transformation Quantization for Large Language Models

## Quick Facts
- **arXiv ID**: 2403.12544
- **Source URL**: https://arxiv.org/abs/2403.12544
- **Reference count**: 25
- **Primary result**: State-of-the-art post-training quantization for LLMs using affine transformations, achieving 2.26 PPL reduction on LLaMA2-7B and 1.98% accuracy improvement on LLaMA-30B zero-shot tasks

## Executive Summary
AffineQuant introduces a novel post-training quantization method for large language models that employs affine transformations to expand the optimization space beyond simple scaling. By left-multiplying weight matrices with a learnable affine matrix and right-multiplying activations with its inverse, the method better aligns weight distributions with quantization levels, reducing quantization errors. A gradual mask method based on the Levy-Desplanques theorem ensures matrix invertibility during optimization. The approach achieves state-of-the-art performance, particularly in low-bit configurations, with significant improvements in perplexity and accuracy across multiple model sizes and datasets.

## Method Summary
AffineQuant optimizes post-training quantization for LLMs by introducing affine transformations that extend beyond simple scaling to reduce quantization errors. The method uses a learnable affine matrix applied to weight matrices, combined with a gradual mask approach that ensures matrix invertibility during optimization by maintaining strict diagonal dominance. The gradual mask starts with diagonal elements and progressively extends to others, following the Levy-Desplanques theorem. The approach is combined with translation transformations and optimized using mean square error between pre- and post-quantization outputs. The method is evaluated on OPT and LLaMA models using WikiText2, PTB, and C4 datasets.

## Key Results
- Reduces perplexity on LLaMA2-7B by 2.26 in 4/4-bit quantization
- Improves accuracy on LLaMA-30B zero-shot tasks by 1.98% in 4/4-bit quantization
- Achieves state-of-the-art performance across multiple datasets and model sizes
- Demonstrates effectiveness particularly in low-bit quantization configurations

## Why This Works (Mechanism)

### Mechanism 1: Affine Transformation Optimization Space
The affine transformation extends optimization beyond simple scaling by repositioning weight vectors through a learnable matrix multiplication. This allows arbitrary repositioning of weight distributions to better align with quantization levels, enabling smaller quantization errors. The transformation guarantees convergence of all dimensions to quantized fixed points while preserving output equivalence.

### Mechanism 2: Gradual Mask for Invertibility
The gradual mask ensures strict diagonal dominance throughout optimization by starting with diagonal elements and progressively unfreezing elements near the diagonal. This aligns with the Levy-Desplanques theorem, theoretically ensuring matrix invertibility and preventing training collapse. The stability factor α controls the rate of unfreezing while maintaining diagonal dominance.

### Mechanism 3: Combined Affine and Translation Transformations
The affine transformation handles rotation and scaling while translation (bias adjustment) modifies the baseline of the quantized distribution. These operations are orthogonal and can be optimized jointly without interference, covering more degrees of freedom in quantization optimization and further reducing quantization error.

## Foundational Learning

- **Strictly diagonally dominant matrices and Levy-Desplanques theorem**: Essential for ensuring the affine transformation matrix remains invertible during optimization. Quick check: What condition must a matrix satisfy to be strictly diagonally dominant, and why does this guarantee invertibility?

- **Post-training quantization vs. quantization-aware training**: Understanding the tradeoffs between data requirements, computational cost, and optimization approaches. Quick check: What are the key differences between PTQ and QAT in terms of data requirements and computational cost?

- **Quantization error metrics**: Mean square error of activations serves as the optimization objective and relates to model perplexity. Quick check: How does mean square error of activations relate to model perplexity in language models?

## Architecture Onboarding

- **Component map**: Affine transformation matrix A (learnable, diagonal initialization) -> Gradual mask matrix (controls trainable elements) -> Quantization function Q(x) (discrete mapping) -> Inverse matrix A⁻¹ (applied to activations)

- **Critical path**: 1) Initialize A with diagonal dominance, 2) Apply gradual mask to freeze/unfreeze elements near diagonal over epochs, 3) Optimize A to minimize mean square error between pre- and post-quantization activations, 4) Merge A into linear layers for inference

- **Design tradeoffs**: Memory vs. precision (double precision for A improves stability but increases memory usage), training time vs. performance (gradual mask slows convergence but ensures invertibility), bit-width vs. performance

- **Failure signatures**: Training collapse due to matrix losing invertibility, suboptimal performance from inappropriate α values, NaN or inf values during training

- **First experiments**: 1) Monitor loss during training and check for NaN/inf values, 2) Experiment with different α values and observe impact on quantization loss, 3) Test matrix invertibility throughout training across different configurations

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the text provided.

## Limitations

- The gradual mask method requires careful tuning of the stability factor α based on model size, quantization bits, and group size, with no specific values or guidelines provided
- The effectiveness of the gradual mask in maintaining invertibility is theoretically justified but lacks direct empirical validation across edge cases and non-standard architectures
- The orthogonal relationship between affine transformation and translation is asserted but not rigorously proven or empirically tested through ablation studies

## Confidence

**High confidence**: The fundamental claim that affine transformations expand the optimization space beyond simple scaling is well-supported by mathematical framework and experimental results showing consistent perplexity improvements.

**Medium confidence**: The effectiveness of the gradual mask method in maintaining matrix invertibility during optimization is supported by theoretical grounding but lacks direct empirical validation.

**Low confidence**: The orthogonal relationship between affine transformation and translation is asserted but not rigorously proven or empirically tested, with no ablation studies or interference metrics provided.

## Next Checks

1. **Invertibility robustness test**: Systematically evaluate matrix invertibility during training across different α values and model configurations, measuring frequency and severity of near-singular matrices or training instability.

2. **Ablation of gradual mask components**: Compare AffineQuant performance with variations that (a) remove the gradual mask entirely, (b) use fixed vs. adaptive α schedules, and (c) start with different initialization patterns to isolate each component's contribution.

3. **Generalization to non-LLM architectures**: Apply AffineQuant to transformer-based models outside the LLM domain (e.g., vision transformers or BERT variants) to test whether performance gains and stability claims hold across different model types and training objectives.