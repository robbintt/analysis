---
ver: rpa2
title: 'RLHF Workflow: From Reward Modeling to Online RLHF'
arxiv_id: '2405.07863'
source_url: https://arxiv.org/abs/2405.07863
tags:
- preference
- arxiv
- learning
- rlhf
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a detailed workflow for online iterative Reinforcement
  Learning from Human Feedback (RLHF) in large language models. The authors address
  the limitations of offline RLHF by constructing a preference model using open-source
  datasets as a proxy for human feedback, enabling iterative policy updates.
---

# RLHF Workflow: From Reward Modeling to Online RLHF

## Quick Facts
- arXiv ID: 2405.07863
- Source URL: https://arxiv.org/abs/2405.07863
- Authors: Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, Tong Zhang
- Reference count: 40
- Key outcome: Achieves 31.3% length-control win rate on AlpacaEval-2, outperforming larger models through iterative RLHF

## Executive Summary
This work presents a detailed workflow for online iterative Reinforcement Learning from Human Feedback (RLHF) in large language models. The authors address the limitations of offline RLHF by constructing a preference model using open-source datasets as a proxy for human feedback, enabling iterative policy updates. The method combines theoretical insights from online exploration with practical implementation details, including temperature tuning and rejection sampling for exploration. The resulting model achieves state-of-the-art performance on benchmarks like AlpacaEval-2 (31.3% length-control win rate), MT-Bench, and Chat-Arena-Hard, outperforming larger models and demonstrating the effectiveness of iterative RLHF in improving conversation quality while maintaining academic capabilities.

## Method Summary
The proposed workflow implements online iterative RLHF by first constructing a preference model trained on open-source datasets to serve as a proxy for human feedback. The system then performs iterative policy updates using reinforcement learning, incorporating temperature tuning and rejection sampling mechanisms to enable effective exploration. This approach addresses the key limitation of offline RLHF where the policy cannot adapt beyond the initial feedback data. The iterative nature allows the model to continuously improve through repeated cycles of policy generation, evaluation, and refinement based on the learned preference model.

## Key Results
- Achieves 31.3% length-control win rate on AlpacaEval-2 benchmark
- Outperforms larger models on MT-Bench and Chat-Arena-Hard evaluations
- Demonstrates effective balance between conversation quality improvement and academic capability maintenance

## Why This Works (Mechanism)
The method works by leveraging the preference model trained on open-source datasets as a scalable proxy for human feedback, enabling continuous policy refinement without requiring constant human annotation. The online iterative approach allows the model to explore and adapt its policy based on current performance rather than being constrained by initial training data. Temperature tuning and rejection sampling provide controlled exploration that balances exploitation of learned preferences with discovery of potentially better responses. The iterative cycles create a feedback loop where each policy update informs subsequent preference modeling, leading to progressive improvement in conversational quality.

## Foundational Learning

**Reinforcement Learning from Human Feedback (RLHF)**: The fundamental framework for aligning language models with human preferences through reward modeling and policy optimization. Needed because direct human feedback is expensive and limited, requiring scalable alternatives. Quick check: Verify understanding of policy gradient methods and reward modeling basics.

**Preference Modeling**: The process of training a reward model to predict human preferences between pairs of responses. Essential for creating the proxy feedback mechanism. Quick check: Understand pairwise ranking loss and its implementation.

**Online vs Offline RL**: Online RL allows policy updates based on current interactions, while offline RL is constrained by fixed datasets. Critical distinction for understanding the iterative advantage. Quick check: Compare convergence properties and exploration capabilities.

**Temperature Tuning in RL**: Adjusting the softmax temperature parameter to control exploration-exploitation trade-off during policy optimization. Needed for balancing stability and discovery. Quick check: Verify temperature affects action probability distributions.

**Rejection Sampling**: A technique for selective acceptance of samples based on probability thresholds. Used here for controlled exploration. Quick check: Understand acceptance probability calculations.

## Architecture Onboarding

**Component Map**: Open-source datasets -> Preference Model -> Policy Network -> Temperature Tuner -> Rejection Sampler -> Updated Policy

**Critical Path**: The core loop follows: Preference Model evaluation → Policy update via RL → Temperature adjustment → Rejection sampling → New policy generation

**Design Tradeoffs**: Using open-source datasets as proxy feedback trades potential domain mismatch for scalability and cost-effectiveness. The iterative approach requires more computational resources but enables continuous improvement versus one-time offline training.

**Failure Signatures**: 
- Poor performance indicates preference model misalignment with actual human preferences
- Unstable training suggests temperature tuning issues or insufficient exploration
- Convergence to suboptimal policies may result from inadequate rejection sampling thresholds

**Three First Experiments**:
1. Validate preference model accuracy on held-out preference pairs from training datasets
2. Test temperature sensitivity by running policy updates across different temperature values
3. Evaluate rejection sampling effectiveness by measuring diversity and quality of accepted samples

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on open-source datasets as proxy feedback may introduce distribution shifts and domain mismatches
- Temperature tuning and rejection sampling require extensive hyperparameter optimization affecting reproducibility
- Iterative workflow assumes continuous computational resources and feedback data availability
- Long-term stability and safety implications of iterative updates remain underexplored

## Confidence
- **High confidence**: Theoretical framework combining online exploration with preference modeling is well-established
- **Medium confidence**: Empirical results on benchmarks are promising but require independent replication
- **Medium confidence**: Practical implementation details are comprehensive but may need adaptation for different architectures

## Next Checks
1. Conduct ablation studies removing the open-source dataset proxy to quantify impact of distribution shift on final performance
2. Perform cross-domain evaluation on specialized datasets (medical, legal, technical) to test generalization beyond standard benchmarks
3. Implement and test workflow on multiple model sizes (7B, 13B, 33B) to establish scaling relationships and identify optimal sizes for different use cases