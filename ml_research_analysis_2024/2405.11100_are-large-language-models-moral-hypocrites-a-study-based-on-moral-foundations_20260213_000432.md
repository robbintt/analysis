---
ver: rpa2
title: Are Large Language Models Moral Hypocrites? A Study Based on Moral Foundations
arxiv_id: '2405.11100'
source_url: https://arxiv.org/abs/2405.11100
tags:
- moral
- values
- llms
- human
- foundations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) struggle to align their abstract moral
  values with concrete moral judgments, displaying hypocritical behavior. This study
  used the Moral Foundations Questionnaire (MFQ) and Moral Foundations Vignettes (MFVs)
  to test GPT-4 and Claude 2.1.
---

# Are Large Language Models Moral Hypocrites? A Study Based on Moral Foundations

## Quick Facts
- arXiv ID: 2405.11100
- Source URL: https://arxiv.org/abs/2405.11100
- Authors: José Luiz Nunes; Guilherme F. C. F. Almeida; Marcelo de Araujo; Simone D. J. Barbosa
- Reference count: 22
- Primary result: Large language models display moral hypocrisy, showing internal consistency within abstraction levels but failing to align abstract values with concrete moral judgments.

## Executive Summary
This study investigates whether large language models (LLMs) exhibit moral hypocrisy by testing their consistency between abstract moral values and concrete moral judgments. Using the Moral Foundations Questionnaire (MFQ) and Moral Foundations Vignettes (MFVs), the researchers evaluated GPT-4 and Claude 2.1 across two experimental conditions. While both models demonstrated reasonable internal consistency within each instrument comparable to humans, they failed to maintain coherence between abstract values and concrete evaluations, revealing contradictory and hypocritical moral behavior.

## Method Summary
The study employed Moral Foundations Theory's two primary instruments - the Moral Foundations Questionnaire (MFQ) for abstract values and Moral Foundations Vignettes (MFVs) for concrete moral violations. Researchers tested GPT-4 and Claude 2.1 using two conditions (QV and VQ) with different prompt structures and temperature settings. Data was collected through API calls, parsed and validated for correct answer formats, then analyzed using Cronbach's alpha for consistency and regression models for coherence. Human response data served as a benchmark for comparison.

## Key Results
- Both models showed reasonable internal consistency within each instrument (MFQ and MFV) comparable to human responses
- No significant correlation was found between foundations expressed in abstract (MFQ) and concrete (MFV) instruments
- GPT-4 displayed the "correct answer effect" at higher temperature settings, producing single answers for multiple items

## Why This Works (Mechanism)

### Mechanism 1
- Models maintain internal consistency within single abstraction levels but fail to map values across levels
- LLMs build coherent latent representations within each instrument but break cross-abstraction mapping due to different question formats and semantic contexts
- Assumes LLMs memorize statistical patterns tied to surface forms rather than learning true conceptual understanding
- Evidence: Both models showed reasonable consistency within instruments, but no significant correlation between foundations across instruments

### Mechanism 2
- Prompt-induced "correct answer effect" masks variability and reinforces inconsistency between abstraction levels
- Temperature settings and prompt wording cause models to lock onto single canonical responses, eliminating variability needed for cross-level alignment
- Assumes LLMs are brittle to small prompt variations and overfit to surface patterns
- Evidence: GPT-4 presented single answers to 5 items at temperature 1.2; Claude generated single answers to all 30 items at temperature 0.85

### Mechanism 3
- Hypocrisy arises because moral reasoning in LLMs is context-bound rather than principle-based
- Abstract moral statements (MFQ) and concrete vignettes (MFV) are processed as separate contexts with no shared reasoning bridge
- Assumes moral foundations are not embedded as explicit, transportable concepts but as emergent patterns tied to specific linguistic cues
- Evidence: No informative relations found in cross-instrument regression; related papers on persona-driven moral shifts support context dependency

## Foundational Learning

- **Moral Foundations Theory (MFT)**
  - Why needed: Provides theoretical lens for decomposing moral judgments into six stable dimensions, enabling cross-instrument comparison
  - Quick check: Can you list the six moral foundations and give one concrete example for each?

- **Internal consistency (Cronbach's α)**
  - Why needed: Quantifies whether items within an instrument reliably measure the same latent trait; low α flags incoherent moral judgments
  - Quick check: What α threshold is commonly accepted for "acceptable" internal consistency in psychology?

- **Cross-abstraction coherence**
  - Why needed: Tests whether abstract moral values predict concrete moral judgments; failure indicates hypocrisy
  - Quick check: If MFQ Care strongly predicts MFV Care ratings in humans, what does it mean if it does not in an LLM?

## Architecture Onboarding

- **Component map**: Prompt generator → API client (OpenAI/Gemini/Anthropic) → Response parser (regex extraction) → Validation filter (scale bounds + attention checks) → Statistical analyzer (α, regression)
- **Critical path**: Prompt → API call → Parse → Validate → Store → Analyze
- **Design tradeoffs**: Higher temperature increases variability but risks invalid responses; lower temperature reduces noise but can trigger "correct answer effect"
- **Failure signatures**: (1) Invalid token count → discard; (2) Out-of-scale values → discard; (3) Zero variance within an instrument → suspect correct answer effect
- **First 3 experiments**:
  1. Rerun with temperature=0 for all models and compare α and regression coefficients
  2. Swap the order of MFQ and MFV in the same session to see if context priming improves cross-abstraction mapping
  3. Use chain-of-thought prompting to force the model to explicitly connect abstract values to concrete vignettes before answering

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the minimum temperature setting required for LLMs to reliably avoid the "correct answer effect" in moral reasoning tasks?
- Basis: The paper notes GPT-4 displayed the "correct answer effect" at temperature 1.2, and Claude 2.1 at 0.85
- Why unresolved: The study used unsystematic preliminary testing to choose temperature settings
- What evidence would resolve it: Systematic testing of multiple temperature settings across different models

### Open Question 2
- Question: Does the order of presenting moral foundations questionnaire (MFQ) versus vignettes (MFVs) systematically affect LLM moral reasoning consistency?
- Basis: The study found a significant difference in Cronbach's alpha between conditions for GPT-4 (p < 0.001) but not for Claude 2.1
- Why unresolved: The mechanism behind this order effect and whether it generalizes to other models remains unexplored
- What evidence would resolve it: Controlled experiments testing multiple model families with systematic variation in presentation order

### Open Question 3
- Question: Can reinforcement learning from human feedback (RLHF) or constitutional AI approaches reduce moral hypocrisy in LLMs?
- Basis: The authors suggest exploring how foundational models, RLHF models, and constitutional AI models differ in their moral reasoning consistency
- Why unresolved: The study only tested base models without alignment fine-tuning
- What evidence would resolve it: Direct comparison of moral reasoning consistency between base models and their aligned counterparts

## Limitations

- Unknown exact prompt structures and temperature settings for each model, critical for reproducibility
- Criteria for excluding invalid responses, especially for Gemini Pro, are not detailed
- Assumes MFQ and MFV are structurally parallel, which may not fully account for complexity of human moral reasoning
- Reliance on statistical measures like Cronbach's alpha and regression models may not capture nuanced nature of moral judgments

## Confidence

- **High Confidence**: LLMs display internal consistency within individual instruments (MFQ and MFV), evidenced by reasonable Cronbach's alpha values comparable to humans
- **Medium Confidence**: LLMs display moral hypocrisy through lack of correlation between abstract and concrete moral judgments, supported by regression analysis
- **Low Confidence**: Mechanisms explaining observed behavior (correct answer effect, context-bound reasoning) are inferred from related literature and require further validation

## Next Checks

1. Reproduce with exact parameters: Rerun the experiment with the exact prompt structures and temperature settings specified in the original study to confirm consistency and hypocrisy findings

2. Context priming experiment: Swap the order of MFQ and MFV in the same session to test if context priming affects cross-abstraction mapping

3. Chain-of-thought prompting: Use chain-of-thought prompting to force models to explicitly connect abstract values to concrete vignettes, assessing if this improves cross-level coherence