---
ver: rpa2
title: Evaluating Large Language Models for automatic analysis of teacher simulations
arxiv_id: '2407.20360'
source_url: https://arxiv.org/abs/2407.20360
tags:
- characteristics
- teacher
- llama
- performance
- few-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated Large Language Models (LLMs) for automatic
  analysis of teacher simulation responses in teacher education. The researchers tested
  DeBERTaV3 and Llama 3 with zero-shot, few-shot, and fine-tuning configurations on
  a dataset of 4,822 response-characteristic pairs from a teacher simulation.
---

# Evaluating Large Language Models for automatic analysis of teacher simulations

## Quick Facts
- arXiv ID: 2407.20360
- Source URL: https://arxiv.org/abs/2407.20360
- Reference count: 40
- Primary result: Llama 3 outperforms DeBERTaV3 for identifying new characteristics in teacher simulation responses, with fine-tuning and few-shot examples providing the best performance

## Executive Summary
This study evaluates Large Language Models (LLMs) for automatic analysis of teacher simulation responses in teacher education. The researchers tested DeBERTaV3 and Llama 3 with zero-shot, few-shot, and fine-tuning configurations on a dataset of 4,822 response-characteristic pairs from a teacher simulation. They found significant variation in model performance depending on the characteristic being identified, with some characteristics proving much harder to detect than others. Llama 3 outperformed DeBERTaV3 in detecting new (unseen) characteristics and showed more stable performance across different scenarios. The fine-tuned few-shot configuration of Llama 3 generally provided the best results, particularly for identifying new characteristics.

## Method Summary
The study compares DeBERTaV3 and Llama 3 models using zero-shot, few-shot, and fine-tuning configurations on three different dataset splits from a teacher simulation dataset containing 4,822 response-characteristic pairs. The dataset includes responses from 494 participants to 3 open-ended questions, with 14 characteristics manually labeled by expert educators. The three experimental splits test performance on low-sample characteristics, random characteristics, and performance-matched characteristics. Models are evaluated using balanced accuracy, F1 score, precision, and recall metrics.

## Key Results
- Llama 3 consistently outperformed DeBERTaV3 in detecting new (unseen) characteristics
- Fine-tuning with few-shot examples significantly improved performance across all model configurations
- Performance varied dramatically by characteristic, with some being much harder to identify than others
- DeBERTaV3's performance degraded significantly when identifying new characteristics compared to Llama 3

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM performance for identifying educational characteristics varies significantly depending on the specific characteristic being analyzed.
- Mechanism: Different characteristics require different levels of contextual understanding, ambiguity tolerance, and semantic depth. Some characteristics (like "not_well") are straightforward observations that can be detected through surface-level text patterns, while others (like "jeremy_mental" or "rejects_policy") require deeper inference about underlying states or policy interpretations that may not be explicitly stated.
- Core assumption: The characteristics themselves have varying degrees of explicitness and semantic complexity in how they manifest in teacher responses.
- Evidence anchors:
  - [abstract] "We discovered a significant variation in the LLMs' performance depending on the characteristic to identify"
  - [section] "some characteristics are far more frequently observed than others. This could be due to the inherent nature of the characteristics or the specific context of the simulation"
  - [corpus] Weak - no direct corpus evidence about characteristic-specific performance variation

### Mechanism 2
- Claim: Fine-tuning with few-shot examples significantly improves LLM performance for both seen and unseen characteristics compared to zero-shot approaches.
- Mechanism: Fine-tuning adapts the model's weights to the specific distribution and patterns of the educational dataset, while few-shot examples provide explicit demonstrations of how to map responses to characteristics. This combination allows the model to learn both general patterns and specific label mappings that aren't present in the pre-training data.
- Core assumption: The educational characteristics and response patterns are sufficiently distinct from general language patterns that specialized adaptation improves performance.
- Evidence anchors:
  - [abstract] "Llama 3 outperformed DeBERTaV3 in detecting new (unseen) characteristics and showed more stable performance"
  - [section] "fine-tuning with few-shot examples significantly improves the capability of the model to generalize to unseen characteristics"
  - [corpus] Weak - no direct corpus evidence about fine-tuning effectiveness, but related papers suggest LoRA and fine-tuning are common approaches

### Mechanism 3
- Claim: DeBERTaV3's performance degrades significantly when identifying new characteristics compared to Llama 3.
- Mechanism: DeBERTaV3 is an encoder-only model designed for classification tasks, which limits its ability to adapt to new classification schemas compared to decoder-based models like Llama 3 that can follow instructions and understand new contexts through few-shot examples.
- Core assumption: The architectural differences between encoder-only and decoder-based models create fundamental differences in their ability to handle novel classification tasks.
- Evidence anchors:
  - [abstract] "DeBERTaV3 significantly reduced its performance when it had to identify new characteristics. In contrast, Llama 3 performed better than DeBERTaV3 in detecting new characteristics"
  - [section] "DeBERTaV3 is an encoder-only model, which fundamentally changes how it can be used. Encoder-only models like DeBERTaV3 are designed for tasks such as classification or sentence encoding, but they cannot generate text or follow instructions in the same way as decoder or encoder-decoder models like Llama 3 do"
  - [corpus] Weak - no direct corpus evidence about architectural limitations, but the paper explicitly states this limitation

## Foundational Learning

- Concept: Binary classification with imbalanced datasets
  - Why needed here: The dataset has characteristics with highly imbalanced positive/negative labels (e.g., "not_well" has 597 negative vs 16 positive), requiring appropriate evaluation metrics like balanced accuracy and F1 score
  - Quick check question: Why might accuracy be misleading for evaluating this model's performance?

- Concept: Few-shot learning vs zero-shot learning
  - Why needed here: The study compares zero-shot, few-shot, and fine-tuned configurations to understand how additional examples impact performance
  - Quick check question: What's the key difference between few-shot learning and fine-tuning in terms of model adaptation?

- Concept: Stratified sampling for dataset splitting
  - Why needed here: The dataset is split using stratified 70/30 method to maintain characteristic distributions across training and evaluation sets
  - Quick check question: Why is stratified sampling important when splitting this dataset compared to random sampling?

## Architecture Onboarding

- Component map: Data preprocessing -> Dataset splitting (3 experiments) -> Model inference/fine-tuning (Llama 3 with vLLM, DeBERTaV3 with Transformers) -> Evaluation (balanced accuracy, F1, precision, recall) -> Analysis of characteristic-specific performance
- Critical path: Data preprocessing → Dataset splitting → Model inference/fine-tuning → Evaluation → Analysis of characteristic-specific performance
- Design tradeoffs: The choice between Llama 3 (decoder-based, instruction-following) and DeBERTaV3 (encoder-only, classification-focused) represents a fundamental tradeoff between flexibility for new characteristics versus specialization for known classifications
- Failure signatures: Poor performance on new characteristics indicates model limitations in generalization; high variance across characteristics suggests the need for better characteristic definitions or more training data for specific cases
- First 3 experiments:
  1. Low-sample experiment: Test performance on characteristics with few training examples to understand minimum data requirements
  2. Random selection experiment: Test performance on randomly selected characteristics to understand general model capability
  3. Performance-matched experiment: Test characteristics where zero-shot performance is similar to understand the impact of selection bias

## Open Questions the Paper Calls Out
1. How do the LLMs' performance characteristics vary when identifying educational characteristics in different subject areas beyond the English context used in this study?
2. What is the optimal balance between fine-tuning and few-shot learning for LLM-based characteristic identification in educational simulations?
3. How do LLM-based characteristic identification systems impact teacher candidate learning outcomes compared to traditional human evaluation methods?

## Limitations
- Dataset imbalance with some characteristics having very few positive examples affects reliability of performance estimates
- Focus on a single teacher simulation scenario limits generalizability to other educational contexts
- Comparison conflates architectural differences with instruction-following capabilities

## Confidence
- High confidence: The overall finding that LLM performance varies significantly by characteristic
- Medium confidence: The superiority of Llama 3 for identifying new characteristics
- Medium confidence: The effectiveness of fine-tuning with few-shot examples

## Next Checks
1. Cross-simulation validation: Test the best-performing model (fine-tuned Llama 3) on teacher responses from a different simulation scenario
2. Characteristic-specific ablation study: Systematically vary the number of training examples for each characteristic to determine minimum data requirements
3. Human-in-the-loop evaluation: Have expert educators review model predictions to assess whether errors reflect model limitations or ambiguous characteristic definitions