---
ver: rpa2
title: 'Hint-before-Solving Prompting: Guiding LLMs to Effectively Utilize Encoded
  Knowledge'
arxiv_id: '2402.14310'
source_url: https://arxiv.org/abs/2402.14310
tags:
- prompting
- reasoning
- hints
- answer
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Hint-before-Solving Prompting (HSP), a method
  to enhance large language models (LLMs) by guiding them to generate hints (specific
  knowledge or key ideas) before solving problems. HSP is applied to four popular
  prompting methods (CoT, Least-to-Most, Plan-and-Solve, Standard) across six reasoning
  benchmarks (mathematical and commonsense reasoning) and four open-source LLMs.
---

# Hint-before-Solving Prompting: Guiding LLMs to Effectively Utilize Encoded Knowledge

## Quick Facts
- **arXiv ID**: 2402.14310
- **Source URL**: https://arxiv.org/abs/2402.14310
- **Reference count**: 40
- **One-line primary result**: HSP improves reasoning accuracy by guiding LLMs to generate hints before solving problems, with up to 9.7-point improvements on GSM8K.

## Executive Summary
This paper introduces Hint-before-Solving Prompting (HSP), a method that enhances large language models' reasoning by prompting them to generate hints before attempting problem-solving. HSP is designed to help LLMs better utilize their encoded knowledge by clarifying the problem's target and guiding the application of relevant knowledge. The method is applied to four popular prompting approaches across six reasoning benchmarks, demonstrating consistent performance improvements, particularly with high-quality hints.

## Method Summary
HSP works by inserting a hint generation step before the problem-solving process. For a given problem, the model first generates a hint that identifies key knowledge or insights needed to solve it. This hint then guides the subsequent reasoning steps. The method is applied to four prompting approaches (Chain-of-Thought, Least-to-Most, Plan-and-Solve, and Standard) and evaluated on six reasoning benchmarks using four open-source LLMs. The authors also create a fine-tuned model (Llemma-7B on HSPMATH) that demonstrates strong performance on mathematical reasoning tasks.

## Key Results
- Llama2-70B-Chat achieves 9.7-point improvement on GSM8K with HSP-enhanced CoT prompting
- Llemma-7B fine-tuned on HSPMATH reaches 64.3% accuracy, surpassing GPT-3.5 and WizardMath-13B
- HSP shows consistent performance improvements across four LLMs and six reasoning benchmarks
- High-quality external hints (GPT-4 generated) provide even greater improvements, especially for lower-capability models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Providing hints before problem-solving improves reasoning accuracy by clarifying the problem's target.
- **Mechanism**: Hints guide the model to identify the correct knowledge and apply it logically, avoiding misapplication of unrelated knowledge.
- **Core assumption**: The model has the relevant knowledge encoded but struggles to retrieve and apply it without explicit guidance.
- **Evidence anchors**: Abstract shows LLM cannot solve calculus problem without hints, but succeeds with hint; section states hints influence answer logic.

### Mechanism 2
- **Claim**: HSP allows models to generate their own hints, enabling self-directed knowledge utilization.
- **Mechanism**: By prompting the model to generate hints first, it activates relevant knowledge pathways before attempting the solution, improving reasoning coherence.
- **Core assumption**: The model can generate useful hints based on its encoded knowledge, even for challenging tasks.
- **Evidence anchors**: Abstract describes HSP as enabling explicit hint generation; section notes HSP is orthogonal to prompting methods.

### Mechanism 3
- **Claim**: Higher-quality hints lead to greater performance improvements, especially for lower-capability models.
- **Mechanism**: External high-quality hints provide clearer guidance, allowing even smaller models to outperform larger models without such hints.
- **Core assumption**: Hint quality directly correlates with reasoning accuracy improvement.
- **Evidence anchors**: Section shows high-quality hints make open-source models outperform ChatGPT; demonstrates greater improvements in lower-capability models.

## Foundational Learning

- **Concept**: Chain-of-Thought (CoT) prompting
  - **Why needed here**: HSP builds upon CoT by adding a hint generation step, so understanding CoT is essential.
  - **Quick check question**: What is the primary purpose of CoT prompting in enhancing LLM reasoning?

- **Concept**: Prompt engineering
  - **Why needed here**: HSP is a form of prompt engineering that modifies the input format to guide the model's reasoning process.
  - **Quick check question**: How does HSP differ from standard prompt engineering techniques?

- **Concept**: Self-consistency
  - **Why needed here**: Self-consistency is used to evaluate the robustness of HSP-enhanced reasoning across multiple sampled paths.
  - **Quick check question**: What role does self-consistency play in assessing the effectiveness of HSP?

## Architecture Onboarding

- **Component map**: Problem statement → Hint generation → Solution generation → Final answer
- **Critical path**: Problem → Hint generation → Solution generation → Answer
- **Design tradeoffs**:
  - Hint quality vs. generation cost (internal vs. external hints)
  - Model size vs. performance gain (larger models benefit more)
  - Task difficulty vs. HSP effectiveness (challenging tasks may limit gains)
- **Failure signatures**:
  - Hints are irrelevant or misleading
  - Model lacks the knowledge to generate useful hints
  - HSP disrupts the planning process in methods like PS and LtM
- **First 3 experiments**:
  1. Apply HSP to CoT prompting on GSM8K and compare accuracy with standard CoT.
  2. Introduce high-quality external hints (e.g., from GPT-4) and measure performance gains across different model sizes.
  3. Test HSP on a challenging dataset (e.g., MATH) to assess its effectiveness on difficult tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the upper bound of performance improvement achievable through high-quality external hints in HSP prompting?
- **Basis in paper**: Explicit - The paper demonstrates that introducing high-quality hints generated by GPT-4 into HSP2 (HSP2G) leads to significant performance improvements across all four evaluated LLMs on six datasets. It also shows that Mixtral-8x7B with HSP2G outperforms ChatGPT on GSM8K, MultiArith, and AQUA.
- **Why unresolved**: While the paper shows improvements with GPT-4 generated hints, it doesn't explore the theoretical or practical limits of how much further performance could be enhanced with even higher quality hints, or by integrating knowledge from other external sources.
- **What evidence would resolve it**: Experiments systematically varying hint quality (e.g., human expert hints, hints from specialized domain models) and measuring corresponding performance improvements on diverse reasoning tasks.

### Open Question 2
- **Question**: How does the effectiveness of HSP prompting scale with increasing model size beyond the evaluated 70B parameter models?
- **Basis in paper**: Explicit - The paper observes that larger model sizes tend to show more significant performance improvements with HSP across multiple prompting methods. However, the largest model evaluated is Llama2-70B.
- **Why unresolved**: The paper doesn't evaluate HSP prompting on models significantly larger than 70B parameters (e.g., GPT-4, Claude 3, or future models), leaving uncertainty about whether the observed scaling trends continue or plateau.
- **What evidence would resolve it**: Systematic evaluation of HSP prompting across a wide range of model sizes, including frontier models, measuring performance improvements on standardized reasoning benchmarks.

### Open Question 3
- **Question**: What is the optimal strategy for integrating HSP with other advanced prompting techniques like program-aided language models or tree-of-thoughts?
- **Basis in paper**: Inferred - The paper establishes that HSP is orthogonal to existing prompting methods and shows improvements when combined with CoT, LtM, PS, and standard prompting. However, it doesn't explore integration with more recent techniques like PAL (Program-Aided Language Models) or tree-of-thoughts.
- **Why unresolved**: The paper focuses on integrating HSP with four established prompting methods but doesn't investigate whether HSP could enhance more sophisticated reasoning frameworks that emerged after its primary experiments.
- **What evidence would resolve it**: Empirical studies evaluating HSP integration with advanced prompting techniques, measuring performance improvements on complex reasoning tasks that require both structured reasoning and knowledge utilization.

## Limitations
- HSP shows limited gains on challenging datasets like MATH, suggesting reduced effectiveness for complex mathematical reasoning tasks.
- The comparison between internal hint generation and external high-quality hints is not fully explored, leaving questions about cost-effectiveness.
- Exact prompt templates and hyperparameters for HSP implementation remain unspecified, creating uncertainty about faithful reproduction.

## Confidence

- **High confidence**: The core mechanism of HSP - that providing hints before problem-solving improves reasoning accuracy - is well-supported by experimental results, particularly on GSM8K.
- **Medium confidence**: The claim that HSP is orthogonal to prompting methods and can be applied universally needs more validation across diverse datasets and model architectures.
- **Medium confidence**: The assertion that higher-quality hints lead to greater performance improvements is supported but requires more rigorous quantitative analysis across different hint quality levels.

## Next Checks
1. **Reproduce results on challenging datasets**: Test HSP on MATH and other difficult reasoning benchmarks to assess its effectiveness beyond GSM8K, particularly for complex mathematical reasoning tasks.
2. **Benchmark against strong baselines**: Compare HSP-enhanced models against state-of-the-art approaches like GPT-4 and GPT-3.5-turbo across all six benchmarks to establish relative performance.
3. **Analyze hint quality impact**: Conduct controlled experiments varying hint quality (internal vs. GPT-4 generated) across different model sizes to quantify the relationship between hint quality and performance gains.