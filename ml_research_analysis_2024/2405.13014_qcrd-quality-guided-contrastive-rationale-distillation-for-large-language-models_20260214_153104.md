---
ver: rpa2
title: 'QCRD: Quality-guided Contrastive Rationale Distillation for Large Language
  Models'
arxiv_id: '2405.13014'
source_url: https://arxiv.org/abs/2405.13014
tags:
- rationales
- negative
- smaller
- distillation
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently deploying large
  language models (LLMs) by distilling their reasoning capabilities into smaller models.
  The authors propose Quality-guided Contrastive Rationale Distillation (QCRD), a
  novel approach that leverages contrastive learning to improve knowledge transfer.
---

# QCRD: Quality-guided Contrastive Rationale Distillation for Large Language Models

## Quick Facts
- **arXiv ID**: 2405.13014
- **Source URL**: https://arxiv.org/abs/2405.13014
- **Reference count**: 11
- **Primary result**: Quality-guided Contrastive Rationale Distillation (QCRD) achieves up to 69.0% accuracy on SV AMP and 54.0% on ANLI1, outperforming existing distillation methods

## Executive Summary
This paper addresses the challenge of efficiently deploying large language models (LLMs) by distilling their reasoning capabilities into smaller models. The authors propose Quality-guided Contrastive Rationale Distillation (QCRD), a novel approach that leverages contrastive learning to improve knowledge transfer. QCRD uses temperature sampling to generate diverse rationales from LLMs, self-consistency for denoising, and a self-play mechanism to create negative rationales from previous iterations of smaller models. A contrastive loss with an online-updating discriminator assigns quality scores to rationales, optimizing the training process. Experiments on four reasoning tasks with T5-base and T5-small models show that QCRD consistently outperforms existing distillation methods, achieving up to 69.0% accuracy on SV AMP and 54.0% on ANLI1. The approach demonstrates robustness and effectiveness in producing higher-quality rationales while using fewer resources.

## Method Summary
QCRD addresses efficient LLM deployment by distilling reasoning capabilities into smaller models through a multi-stage process. The method uses temperature sampling to generate diverse rationales from LLMs, then applies self-consistency to denoise and select high-quality positive samples. Negative rationales are generated through a self-play mechanism that samples from previous iterations of the smaller model itself. A discriminator is trained to assess rationale quality and assign appropriate weights to the contrastive loss. The smaller model learns from both positive and negative rationales using this quality-guided contrastive loss. The approach is evaluated on four reasoning tasks using T5-base and T5-small models, demonstrating consistent improvements over baseline distillation methods.

## Key Results
- QCRD achieves 69.0% accuracy on SV AMP, outperforming baseline methods
- The approach reaches 54.0% accuracy on ANLI1, demonstrating effectiveness on natural language inference
- QCRD shows consistent improvements across four reasoning tasks compared to existing distillation approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using temperature sampling and self-consistency to denoise LLM rationales improves the quality of positive samples.
- Mechanism: Temperature sampling generates diverse rationales, and self-consistency selects the most consistent ones, filtering out noisy or incorrect reasoning paths.
- Core assumption: Correct reasoning paths tend to have greater agreement in their final answers than incorrect ones.
- Evidence anchors:
  - [abstract] "For the learning of positive knowledge, we collect positive rationales through self-consistency to denoise the LLM rationales generated by temperature sampling."
  - [section] "We thus choose the rationales with the most consistent label in all outputs of the LLM as positive rationale samples"
  - [corpus] Weak evidence - related papers do not explicitly mention self-consistency for denoising rationales.
- Break condition: If incorrect reasoning paths coincidentally produce the same final answer, self-consistency might incorrectly select them as positive samples.

### Mechanism 2
- Claim: Generating negative rationales from previous iterations of smaller models creates effective contrastive samples.
- Mechanism: Sampling from previous iterations of the smaller model produces low-quality rationales that are treated as negative samples, encouraging the model to learn from its own weaknesses.
- Core assumption: The rationale quality of LLMs is higher than that of smaller models.
- Evidence anchors:
  - [abstract] "For the negative knowledge, we propose an innovative self-adversarial approach that generates low-quality rationales by sampling previous iterations of smaller language models"
  - [section] "Moreover, we generate additional lower-quality rationales by iterating the smaller model itself"
  - [corpus] Weak evidence - related papers mention knowledge distillation but do not discuss generating negative samples from model iterations.
- Break condition: If the smaller model's performance improves significantly, the generated negative rationales might become too similar to positive ones, reducing their effectiveness.

### Mechanism 3
- Claim: The quality-guided contrastive loss with an online-updating discriminator optimizes the training process.
- Mechanism: A discriminator judges the quality of rationales and assigns weights to the contrastive loss, ensuring that higher-quality rationales have appropriate influence on the training.
- Core assumption: A trained discriminator can effectively distinguish between positive and negative rationales and assign accurate quality scores.
- Evidence anchors:
  - [abstract] "A contrastive loss is developed to distill both positive and negative knowledge into smaller language models, where an online-updating discriminator is integrated to assess qualities of rationales and assign them appropriate weights"
  - [section] "For better knowledge learning, we present a contrastive loss to distill both the positive and negative rationales into the smaller language model. And a discriminator is adopted to assess the qualities of the rationales and assign them appropriate weights"
  - [corpus] Weak evidence - related papers do not discuss using discriminators for quality assessment in knowledge distillation.
- Break condition: If the discriminator is not updated frequently enough, it may not accurately reflect the evolving quality of rationales as the model trains.

## Foundational Learning

- Concept: Temperature sampling
  - Why needed here: Generates diverse rationales from LLMs, providing a broader set of reasoning paths for selection and denoising.
  - Quick check question: How does temperature sampling differ from greedy decoding in generating LLM outputs?

- Concept: Self-consistency
  - Why needed here: Filters out noisy or incorrect rationales by selecting those with the most consistent final answers.
  - Quick check question: Why might rationales with correct final answers still be considered noisy or incorrect?

- Concept: Contrastive learning
  - Why needed here: Enables the model to learn from both positive and negative rationales, improving reasoning capabilities by distinguishing between them.
  - Quick check question: How does contrastive learning differ from traditional supervised learning in this context?

## Architecture Onboarding

- Component map:
  - LLM (Teacher) -> Temperature sampling -> Diverse rationales
  - LLM -> Self-consistency -> Positive rationales
  - Student Model -> Self-play -> Negative rationales
  - Discriminator -> Quality assessment -> Weight assignment
  - Contrastive loss -> Knowledge distillation -> Student model

- Critical path:
  1. Generate diverse rationales using temperature sampling
  2. Denoise rationales using self-consistency
  3. Generate negative rationales using self-play
  4. Train discriminator to assess rationale quality
  5. Apply quality-guided contrastive loss to distill knowledge

- Design tradeoffs:
  - Temperature sampling vs. greedy decoding: Diversity vs. consistency in generated rationales
  - Number of sampling iterations: Quality of negative rationales vs. computational cost
  - Discriminator update frequency: Accuracy of quality assessment vs. training time

- Failure signatures:
  - Poor performance on reasoning tasks: Indicates issues with rationale quality or contrastive learning
  - Model overfitting to specific rationales: Suggests need for more diverse negative samples
  - Discriminator unable to distinguish rationales: Indicates need for better quality assessment training

- First 3 experiments:
  1. Compare performance with and without temperature sampling to assess impact on rationale diversity
  2. Test different numbers of sampling iterations for negative rationale generation to find optimal balance
  3. Evaluate the effect of discriminator update frequency on overall model performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed self-play mechanism for generating negative rationales compare in effectiveness to alternative approaches like using static pre-generated negative examples or adversarial training methods?
- Basis in paper: [inferred] The paper mentions using self-play to generate negative rationales from previous iterations of smaller models, but doesn't compare this approach to other potential methods for generating negative examples.
- Why unresolved: The paper focuses on their proposed method without conducting direct comparisons to alternative negative sample generation techniques.
- What evidence would resolve it: A systematic comparison study between self-play negative generation and other methods like pre-generated negative sets or adversarial approaches on the same tasks and datasets.

### Open Question 2
- Question: What is the optimal sampling temperature for the self-play mechanism across different types of reasoning tasks, and how does this optimal value change as the student model trains?
- Basis in paper: [explicit] The paper shows that Ï„=1.5 works best for SV AMP but doesn't explore how this optimal value varies across tasks or training stages.
- Why unresolved: The paper only provides results for one task and one fixed temperature, leaving open the question of generalizability and adaptation.
- What evidence would resolve it: A comprehensive ablation study testing different temperature values across multiple tasks at different training epochs to identify patterns in optimal settings.

### Open Question 3
- Question: How does the quality discriminator's performance impact the final distillation results, and what happens if the discriminator is trained on imperfect or biased rationales?
- Basis in paper: [explicit] The paper introduces a quality discriminator but doesn't analyze its sensitivity to training data quality or its direct impact on distillation performance.
- Why unresolved: The paper assumes the discriminator works effectively but doesn't investigate scenarios where it might fail or be misled by poor training data.
- What evidence would resolve it: Experiments testing distillation performance when the discriminator is trained on deliberately corrupted or biased rationale sets, along with analysis of discriminator accuracy versus distillation quality.

## Limitations
- The study focuses primarily on T5-base and T5-small models, limiting generalizability to other model architectures
- Evaluation is limited to specific benchmark datasets, raising questions about performance on more diverse reasoning tasks
- Reliance on GPT-3.5-turbo as the teacher model may not represent scenarios where such powerful models are unavailable or cost-prohibitive

## Confidence

**High Confidence**: The experimental results demonstrating improved performance over baseline methods on the tested datasets. The ablation studies showing the contribution of individual components are well-designed and provide strong evidence for the effectiveness of the approach.

**Medium Confidence**: The claims about the efficiency gains from using smaller models. While the paper mentions resource efficiency, detailed comparisons of computational costs and inference times are not provided.

**Low Confidence**: The robustness of the method to different reasoning task types and model architectures. The evaluation is limited to specific datasets and model sizes, and the paper does not provide evidence for performance on out-of-domain tasks or with different teacher models.

## Next Checks
1. **Generalization to Other Model Architectures**: Test QCRD with other transformer-based models (e.g., BERT, RoBERTa) and different sizes to assess the method's effectiveness beyond T5 models and evaluate its scalability.

2. **Evaluation on Diverse Reasoning Tasks**: Apply QCRD to a broader range of reasoning tasks, including those with different complexity levels and domains (e.g., mathematical reasoning, commonsense reasoning) to validate the method's robustness and versatility.

3. **Efficiency Analysis**: Conduct a comprehensive analysis of the computational costs associated with QCRD, including training time, inference time, and resource usage, to quantify the practical benefits of deploying smaller models over their larger counterparts.