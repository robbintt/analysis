---
ver: rpa2
title: 'Can Vision-Language Models Replace Human Annotators: A Case Study with CelebA
  Dataset'
arxiv_id: '2410.09416'
source_url: https://arxiv.org/abs/2410.09416
tags:
- annotation
- human
- annotations
- celeba
- attributes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated Vision-Language Models (VLMs) for image annotation
  tasks using the CelebA dataset. The LLaVA-NeXT model achieved 79.5% agreement with
  original human annotations on 1000 images.
---

# Can Vision-Language Models Replace Human Annotators: A Case Study with CelebA Dataset

## Quick Facts
- arXiv ID: 2410.09416
- Source URL: https://arxiv.org/abs/2410.09416
- Reference count: 26
- Primary result: VLMs achieved 79.5% agreement with human annotations, reducing costs to less than 1% of manual annotation

## Executive Summary
This study evaluates Vision-Language Models (VLMs) as alternatives to human annotators for image classification tasks using the CelebA dataset. The LLaVA-NeXT model demonstrated 79.5% agreement with original human annotations on 1000 images, with re-annotation of disagreed cases increasing consistency to 89.1%. Cost analysis revealed AI annotation costs were less than 1% of manual annotation costs for the entire CelebA dataset. The results indicate VLMs can effectively replace human annotators for specific image classification tasks, particularly excelling at objective attributes while significantly reducing both financial burden and ethical concerns associated with large-scale manual data annotation.

## Method Summary
The study used the LLaVA-NeXT-8B model to generate annotations for 1000 randomly selected CelebA images across 40 binary attributes. AI-generated annotations were compared with original human annotations to calculate agreement percentages. Disagreed cases were re-annotated by humans, and a majority vote was used to determine final consensus. Cost analysis compared GPU inference costs against estimated human annotation expenses for the entire CelebA dataset (200K images). The evaluation focused on both quantitative agreement metrics and qualitative assessment of annotation quality across different attribute types.

## Key Results
- LLaVA-NeXT achieved 79.5% agreement with original human annotations on 1000 CelebA images
- Re-annotation of disagreed cases increased agreement to 89.1%, with even higher consistency for objective labels
- AI annotation costs were less than 1% of manual annotation costs for the entire CelebA dataset
- VLMs showed particular strength in annotating objective attributes like eyeglasses and wearing hat

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vision-Language Models can achieve similar annotation quality to humans for image classification tasks with objective attributes.
- Mechanism: VLMs like LLaVA-NeXT leverage their learned visual and linguistic representations to accurately classify images based on clear, unambiguous features without requiring human interpretation.
- Core assumption: Objective attributes have clear visual markers that VLMs can reliably detect.
- Evidence anchors:
  - [abstract] "Annotations from the state-of-the-art LLaVA-NeXT model on 1000 CelebA images are in 79.5% agreement with the original human annotations."
  - [section] "Incorporating re-annotations of disagreed cases into a majority vote boosts AI annotation consistency to 89.1% and even higher for more objective labels."
- Break condition: When attributes become more subjective or require nuanced human judgment beyond clear visual markers.

### Mechanism 2
- Claim: VLMs can significantly reduce annotation costs compared to manual methods.
- Mechanism: Once trained, VLMs can process images and generate annotations at computational cost rather than human labor cost, leveraging GPU inference efficiency.
- Core assumption: GPU inference costs scale favorably compared to human annotation costs for large datasets.
- Evidence anchors:
  - [section] "Cost assessments demonstrate that AI annotation significantly reduces expenditures compared to traditional manual methods—representing less than 1% of the costs for manual annotation in the CelebA dataset."
  - [section] "The cost estimation is based on constructing the entire CelebA dataset—generating 40 labels on each of the 200K images."
- Break condition: When dataset complexity requires expensive model fine-tuning or when human oversight becomes necessary for quality control.

### Mechanism 3
- Claim: Re-annotation of disagreed cases can improve overall annotation quality and align with VLM judgments.
- Mechanism: By having humans re-annotate only the cases where VLMs disagreed with original annotations, we can identify and correct human errors while validating VLM judgments, creating a more accurate consensus.
- Core assumption: Some original human annotations contain errors, and VLMs can correctly identify these cases.
- Evidence anchors:
  - [abstract] "Incorporating re-annotations of disagreed cases into a majority vote boosts AI annotation consistency to 89.1%."
  - [section] "The re-annotation showed an equal preference for AI and human annotations, with each method favored more in exactly 20 attributes."
- Break condition: When VLMs consistently disagree with humans due to model bias or when re-annotation costs approach manual annotation costs.

## Foundational Learning

- Concept: Vision-Language Models (VLMs) architecture and capabilities
  - Why needed here: Understanding how VLMs process visual information and generate annotations is crucial for evaluating their annotation performance.
  - Quick check question: What are the key components of VLMs like LLaVA-NeXT that enable image classification?

- Concept: Annotation quality metrics and evaluation methods
  - Why needed here: The study relies on agreement percentages and majority voting to assess annotation quality, requiring understanding of evaluation methodology.
  - Quick check question: How is annotation agreement calculated, and what does 79.5% agreement mean in practical terms?

- Concept: Cost calculation for AI vs human annotation
  - Why needed here: The cost comparison forms a key argument for VLM adoption, requiring understanding of both computational and human labor costs.
  - Quick check question: How are GPU inference costs calculated, and what factors influence the 1% cost ratio?

## Architecture Onboarding

- Component map: Image input → LLaVA-NeXT model → binary classification output → majority voting mechanism
- Critical path: Image preprocessing → VLM inference → annotation generation → quality assessment
- Design tradeoffs: Model accuracy vs. inference speed, annotation detail level vs. cost, human oversight vs. full automation
- Failure signatures: Low agreement percentages, high disagreement rates, cost ratios exceeding manual annotation, model bias in subjective attributes
- First 3 experiments:
  1. Test VLM annotation accuracy on a small subset of objective CelebA attributes (e.g., eyeglasses, wearing hat)
  2. Compare single VLM annotation vs. majority vote approach on subjective attributes
  3. Scale cost analysis from 1000 images to full CelebA dataset to validate 1% cost claim

## Open Questions the Paper Calls Out
- How does VLM annotation performance generalize to more complex image classification tasks beyond binary attributes?
- What specific interaction mechanisms with VLMs could improve annotation quality beyond majority voting?
- How do VLMs perform across different cultural contexts and demographic groups?
- What is the optimal trade-off between model size, inference cost, and annotation quality for VLMs?

## Limitations
- Study focuses exclusively on binary classification of objective attributes, limiting applicability to more nuanced tasks
- Long-term model performance and cost-effectiveness over time are not addressed
- Results may vary with different image subsets or VLM models beyond LLaVA-NeXT-8B

## Confidence
- High confidence: Cost analysis showing AI annotation costs at less than 1% of manual annotation costs
- Medium confidence: 79.5% agreement baseline and 89.1% improvement through re-annotation
- Low confidence: Generalization to other datasets or more complex annotation tasks

## Next Checks
1. Test VLM performance on a broader range of attribute types including more subjective or complex annotations to establish boundaries of applicability
2. Conduct cost analysis using different GPU pricing models and include model fine-tuning costs to validate the 1% cost ratio under varying conditions
3. Evaluate annotation consistency across multiple VLM models and compare their agreement rates to establish whether LLaVA-NeXT's performance is representative or exceptional