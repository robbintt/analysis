---
ver: rpa2
title: 'LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and
  Adaptive Quantization'
arxiv_id: '2403.01136'
source_url: https://arxiv.org/abs/2403.01136
tags:
- quantization
- layer
- memory
- llm-pq
- serving
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents LLM-PQ, a system designed to improve large
  language model (LLM) inference on heterogeneous GPU clusters. The system introduces
  phase-aware model partitioning and adaptive quantization to address the inefficiencies
  of existing solutions in heterogeneous environments.
---

# LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization

## Quick Facts
- **arXiv ID**: 2403.01136
- **Source URL**: https://arxiv.org/abs/2403.01136
- **Reference count**: 40
- **Key outcome**: LLM-PQ achieves up to 2.88× (2.26× on average) throughput improvement for LLM inference on heterogeneous GPU clusters through phase-aware partitioning and adaptive quantization.

## Executive Summary
This paper introduces LLM-PQ, a system for efficient serving of large language models on heterogeneous GPU clusters. The core innovation lies in jointly optimizing quantization precision, layer partition, and micro-batch sizing while accounting for the different computational characteristics of prefill and decode phases. By using adaptive mixed-precision quantization and phase-aware model partitioning, LLM-PQ addresses the inefficiencies of existing solutions in heterogeneous environments, achieving significant throughput improvements while maintaining user-specified model quality targets.

## Method Summary
LLM-PQ uses an iterative algorithm that explores device topology orderings and micro-batch size pairs, then solves an integer linear programming (ILP) problem to jointly determine bitwidth assignment, layer partition, and micro-batch sizing. The system includes an offline assigner with cost models for memory and latency estimation, a variance indicator for quantization sensitivity, and a distributed runtime with a master engine and worker processes. The approach carefully considers the interdependencies between quantization precision, layer placement, and batch sizing rather than optimizing each factor independently.

## Key Results
- Achieves up to 2.88× throughput improvement compared to state-of-the-art approaches
- Demonstrates 2.26× average throughput improvement across 11 different cluster configurations
- Maintains user-specified model quality targets while optimizing inference performance
- Effective across various workload settings including different prompt lengths, batch sizes, and token generation lengths

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Phase-aware model partitioning improves inference throughput by accounting for differing computational characteristics of prefill and decode phases across heterogeneous GPUs.
- **Mechanism**: LLM-PQ models the execution time of both prefill and decode phases separately, recognizing that prefill is compute-bound while decode is memory-bound. By considering these phase-specific behaviors, the system can optimally partition model layers across heterogeneous GPUs to balance workloads and minimize pipeline bubbles.
- **Core assumption**: The execution time ratio between different GPU types varies significantly between prefill and decode phases, making uniform partitioning suboptimal.
- **Evidence anchors**: [abstract], [section], [corpus]
- **Break condition**: If GPU performance characteristics become more uniform across phases, the benefits of phase-aware partitioning would diminish.

### Mechanism 2
- **Claim**: Adaptive mixed-precision quantization improves both model accuracy and inference speed compared to uniform quantization in heterogeneous clusters.
- **Mechanism**: LLM-PQ assigns different quantization bitwidths to model layers based on GPU memory availability and layer sensitivity. Higher precision is used on GPUs with more memory to preserve accuracy, while lower precision is used where memory is constrained, avoiding waste and OOM errors.
- **Core assumption**: Different layers exhibit varying sensitivities to quantization, and GPU memory capacities vary significantly in heterogeneous clusters.
- **Evidence anchors**: [abstract], [section], [corpus]
- **Break condition**: If all GPUs in the cluster have similar memory capacities, uniform quantization might perform comparably.

### Mechanism 3
- **Claim**: Joint optimization of quantization, partitioning, and micro-batch sizing yields superior performance compared to optimizing each factor independently.
- **Mechanism**: LLM-PQ uses an ILP formulation that simultaneously determines bitwidth assignment, layer partition, and micro-batch sizes, considering their interdependencies. The optimizer explores device orderings and micro-batch size pairs to find the global optimum.
- **Core assumption**: The optimal solution requires considering the interaction between quantization precision, layer placement, and batch sizing rather than sequential optimization.
- **Evidence anchors**: [abstract], [section], [corpus]
- **Break condition**: If the solution space becomes too large to explore efficiently, approximation methods might be necessary.

## Foundational Learning

- **Concept**: Transformer architecture and attention mechanism
  - Why needed here: Understanding how LLMs process sequences and generate tokens is essential for grasping phase-aware partitioning and KV cache management.
  - Quick check question: What are the two main phases in LLM inference, and how do they differ in terms of computational characteristics?

- **Concept**: Quantization techniques and precision trade-offs
  - Why needed here: Knowledge of quantization methods (symmetric vs asymmetric, deterministic vs stochastic rounding) is crucial for understanding adaptive mixed-precision strategies.
  - Quick check question: How does the choice of quantization precision affect both model accuracy and inference speed?

- **Concept**: Pipeline parallelism and micro-batch processing
  - Why needed here: Understanding how pipeline parallelism works, including pipeline bubbles and micro-batch sizing, is necessary to grasp the optimization of inference throughput.
  - Quick check question: How does micro-batch size affect pipeline utilization and memory consumption in a heterogeneous cluster?

## Architecture Onboarding

- **Component map**: Offline Assigner -> Distributed Runtime -> Worker Processes
- **Critical path**: 
  1. Offline Assigner generates optimized plan (bitwidth assignment, layer partition, micro-batch sizing)
  2. Plan is loaded by distributed runtime
  3. Master engine handles preprocessing and postprocessing
  4. Workers execute pipeline stages with quantized weights
- **Design tradeoffs**: Precision vs. speed, Batch size vs. latency, Search space exploration vs. optimization time
- **Failure signatures**: OOM errors (insufficient memory), Underutilization (imbalanced partitioning), Accuracy degradation (aggressive quantization)
- **First 3 experiments**: 
  1. Run LLM-PQ on a simple heterogeneous cluster (1xV100 + 1xT4) with OPT-13b to verify basic functionality
  2. Compare inference throughput with baseline approaches (PipeEdge, Uniform) on the same cluster
  3. Test different micro-batch sizes to observe their impact on pipeline utilization and latency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between inference latency and model quality degradation in heterogeneous LLM serving, and how does it vary across different cluster configurations and workload characteristics?
- Basis in paper: [explicit] The paper discusses the trade-off between inference latency and model quality, with users specifying a "quality scalar" (θ) to weight their concern for model quality degradation.
- Why unresolved: The paper uses a heuristic approach to solve the optimization problem and explores the impact of θ on model quality and throughput, but does not provide a definitive answer on the optimal balance for different scenarios.
- What evidence would resolve it: Empirical studies comparing different θ values across various cluster configurations and workload characteristics, measuring both inference latency and model quality degradation.

### Open Question 2
- Question: How does the effectiveness of LLM-PQ's variance indicator compare to other quantization sensitivity metrics, such as Hessian-based approaches, across different LLM architectures and quantization schemes?
- Basis in paper: [explicit] The paper introduces a variance indicator to quantify the model performance perturbation introduced by quantization and compares it with random assignment and Hessian-based approaches.
- Why unresolved: The comparison is limited to specific LLM models (BLOOM-3b and OPT-1.3b) and quantization schemes (weight-only quantization).
- What evidence would resolve it: Extensive experiments evaluating the variance indicator's performance across various LLM architectures and quantization schemes.

### Open Question 3
- Question: How does the inclusion of tensor parallelism in LLM-PQ impact its performance in heterogeneous clusters, and what are the optimal strategies for combining tensor and pipeline parallelism?
- Basis in paper: [explicit] The paper mentions that tensor parallelism is not incorporated in the current implementation due to the favorable characteristics of pipeline parallelism in heterogeneous settings.
- Why unresolved: The paper does not explore the potential benefits or challenges of incorporating tensor parallelism in LLM-PQ.
- What evidence would resolve it: Experimental evaluations comparing LLM-PQ with and without tensor parallelism in various heterogeneous cluster configurations.

## Limitations

- **Heterogeneity Assumptions**: The claimed 2.88× improvement heavily depends on significant performance variation between prefill and decode phases across different GPU types, which may not hold for newer GPU architectures.
- **Cost Model Accuracy**: The ILP formulation relies on accurate latency and memory cost models, but the paper doesn't provide error bounds or validation against real-world measurements.
- **Benchmark Representativeness**: Experiments focus on OPT and BLOOM model families, and generalizability to other architectures or tasks remains uncertain.

## Confidence

- **High Confidence**: The basic premise that heterogeneous clusters benefit from differentiated treatment of prefill vs decode phases, and that adaptive quantization can improve memory utilization.
- **Medium Confidence**: The claimed 2.88× throughput improvement and 2.26× average improvement, as these depend heavily on specific cluster configurations and workload characteristics.
- **Low Confidence**: The optimality of the ILP formulation approach, as no comparison is provided against alternative optimization strategies or heuristic methods.

## Next Checks

1. **Cost Model Validation**: Implement a measurement framework to collect actual latency and memory usage data for different quantization levels and partitioning schemes on a heterogeneous cluster. Compare these measurements against the predicted values from LLM-PQ's cost models to quantify prediction error and its impact on performance.

2. **Architecture Generalization Test**: Apply LLM-PQ to at least two additional LLM architectures (e.g., LLaMA and GPT-2) and evaluate whether the phase-aware partitioning and adaptive quantization strategies transfer effectively. Measure both throughput improvement and quality degradation compared to baseline approaches.

3. **Small-Scale Reproduction**: Set up a minimal heterogeneous cluster (e.g., 1xV100 + 1xT4) and implement a simplified version of LLM-PQ's core algorithms. Run controlled experiments varying prompt lengths, batch sizes, and token generation lengths to verify that the basic mechanisms produce expected behavior and measure the actual throughput improvements against uniform partitioning and quantization baselines.