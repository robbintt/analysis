---
ver: rpa2
title: 'Rapidly Developing High-quality Instruction Data and Evaluation Benchmark
  for Large Language Models with Minimal Human Effort: A Case Study on Japanese'
arxiv_id: '2403.03690'
source_url: https://arxiv.org/abs/2403.03690
tags:
- data
- instruction
- japanese
- self
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study proposes an efficient self-instruct method to develop
  high-quality instruction data and evaluation benchmarks for Japanese large language
  models with minimal human effort. Instead of translating existing English resources,
  the authors first translate a small amount of English instructions into Japanese,
  post-edit them for native-level quality, and then use GPT-4 to generate diverse
  Japanese instruction data.
---

# Rapidly Developing High-quality Instruction Data and Evaluation Benchmark for Large Language Models with Minimal Human Effort: A Case Study on Japanese

## Quick Facts
- arXiv ID: 2403.03690
- Source URL: https://arxiv.org/abs/2403.03690
- Reference count: 0
- Primary result: GPT-4 self-instruct method generates high-quality Japanese instruction data that significantly outperforms translated data

## Executive Summary
This paper presents an efficient method for developing high-quality instruction data and evaluation benchmarks for Japanese large language models with minimal human effort. Instead of translating existing English resources, the authors use a small set of post-edited Japanese seed tasks to guide GPT-4 in generating diverse Japanese instruction data. They construct an 80-question evaluation benchmark and demonstrate that models fine-tuned on their GPT-4 self-instruct data significantly outperform those fine-tuned on translated data, achieving a 54.37% win-rate against GPT-3.5.

## Method Summary
The authors first translate 175 English instruction seeds into Japanese and post-edit them for native-level quality. They then use GPT-4 to generate 52K diverse Japanese instruction examples, filtering them using ROUGE-L similarity to ensure diversity. Models are fine-tuned using LoRA on this self-instruct data, and evaluation is performed using GPT-4 as a reference-free judge on a newly constructed 80-question Japanese Vicuna benchmark. The approach demonstrates that a small amount of high-quality instruction data can dramatically improve LLM performance compared to large amounts of lower-quality translated data.

## Key Results
- Models fine-tuned on GPT-4 self-instruct data achieve 54.37% win-rate against GPT-3.5
- Just 5K self-instruct examples achieve competitive performance to 52K translated Alpaca examples
- Human evaluation confirms consistency between GPT-4's assessments and human preferences
- Self-instruct data quality is significantly higher than machine-translated Alpaca data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 self-instruction data generation yields higher quality Japanese instruction data than machine translation of English Alpaca
- Mechanism: Uses high-quality Japanese seed tasks to guide GPT-4 in generating diverse, fluent Japanese instructions
- Evidence: Empirical results show significantly higher win-rates; ROUGE-L filtering ensures diversity
- Break condition: GPT-4 generation quality degrades or seed tasks lack diversity

### Mechanism 2
- Claim: GPT-4 can reliably evaluate Japanese LLM responses in a reference-free manner
- Mechanism: GPT-4 judges pairwise comparisons or assigns scores based on helpfulness, relevance, accuracy, depth, creativity, and detail
- Evidence: Human evaluation confirms consistency with GPT-4 assessments
- Break condition: GPT-4 develops systematic bias toward certain response styles

### Mechanism 3
- Claim: A small amount of high-quality instruction data can dramatically improve LLM performance
- Mechanism: Quality over quantity; 5K examples achieve competitive results to 52K examples
- Evidence: Fine-tuned models show significant performance gains with limited high-quality data
- Break condition: Downstream tasks require more diverse examples than seed set provides

## Foundational Learning

- Concept: Supervised Fine-Tuning (SFT)
  - Why needed here: Adapts pre-trained LLMs to follow instructions using labeled instruction-response pairs
  - Quick check question: What is the difference between SFT and unsupervised pre-training?

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: Understanding CoT helps in designing prompts that generate high-quality instruction data
  - Quick check question: How does CoT improve the quality of generated instructions?

- Concept: Low-Rank Adaptation (LoRA)
  - Why needed here: Efficiently fine-tunes large models with fewer parameters, reducing computational cost
  - Quick check question: What are the trade-offs between LoRA and full fine-tuning?

## Architecture Onboarding

- Component map: Seed translation -> Post-editing -> GPT-4 generation -> Quality filtering -> SFT training -> GPT-4 evaluation
- Critical path: Seed task preparation -> GPT-4 data generation -> Filtering -> Fine-tuning -> Evaluation
- Design tradeoffs: Quality vs. quantity of instruction data; native post-editing cost vs. translation quality; GPT-4 API cost vs. manual annotation cost
- Failure signatures: Low diversity in generated instructions; poor alignment between seed tasks and outputs; inconsistent GPT-4 evaluations
- First 3 experiments:
  1. Generate 1K instruction examples and manually inspect quality vs. baseline translated data
  2. Run SFT with 5K examples and compare performance to 52K baseline
  3. Validate GPT-4 evaluation consistency with human judgments on a small subset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GPT-4 self-instructed Japanese instruction data compare to human-annotated Japanese instruction data?
- Basis: The paper demonstrates GPT-4 self-instructed data outperforms machine-translated data but doesn't compare to human-annotated data
- Why unresolved: Only compared to machine-translated data, not human-annotated Japanese instruction data
- What evidence would resolve it: Direct comparison study between GPT-4 self-instructed and human-annotated Japanese instruction data

### Open Question 2
- Question: What is the impact of instruction data quality versus quantity on the performance of Japanese LLMs?
- Basis: The paper concludes quality is more significant than quantity
- Why unresolved: Lacks detailed analysis of specific impact of each factor on model performance
- What evidence would resolve it: Systematic study varying both quality and quantity of instruction data

### Open Question 3
- Question: How do the proposed evaluation benchmarks perform across different Japanese LLM architectures?
- Basis: The paper constructs an evaluation benchmark but doesn't extensively test across various architectures
- Why unresolved: Focuses on limited set of models without exploring different architectures
- What evidence would resolve it: Evaluating the benchmark with diverse range of Japanese LLM architectures

## Limitations
- Relies heavily on GPT-4's generation and evaluation capabilities without independent validation
- Translation and post-editing process lacks detailed quality metrics or validation procedures
- Focuses exclusively on Japanese, limiting generalizability to other low-resource languages
- Exact prompt engineering details remain underspecified, affecting reproducibility

## Confidence

- High Confidence: GPT-4 self-instruct data outperforms translated data; GPT-4 evaluation consistency with human preferences
- Medium Confidence: Quality over quantity claim; GPT-4 evaluation methodology effectiveness
- Low Confidence: Generalizability to other languages; long-term effectiveness of reference-free evaluation

## Next Checks

1. Reproduce the methodology by generating 1,000 instruction examples and comparing quality metrics against translated data using both automated metrics and human evaluation

2. Apply the same self-instruct approach to another low-resource language (e.g., Korean or Turkish) to verify language-agnostic effectiveness

3. Create a smaller human-annotated evaluation set for Japanese and compare GPT-4's judgments against human preferences across different response dimensions to validate the reference-free evaluation approach