---
ver: rpa2
title: 'NewTerm: Benchmarking Real-Time New Terms for Large Language Models with Annual
  Updates'
arxiv_id: '2410.20814'
source_url: https://arxiv.org/abs/2410.20814
tags:
- terms
- llms
- term
- question
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NewTerm, a highly automated benchmark for
  evaluating large language models' (LLMs) ability to understand real-time new terms.
  The benchmark addresses the challenge of LLMs' knowledge cutoffs by constructing
  tasks based on newly added dictionary terms.
---

# NewTerm: Benchmarking Real-Time New Terms for Large Language Models with Annual Updates

## Quick Facts
- arXiv ID: 2410.20814
- Source URL: https://arxiv.org/abs/2410.20814
- Authors: Hexuan Deng; Wenxiang Jiao; Xuebo Liu; Min Zhang; Zhaopeng Tu
- Reference count: 40
- Key outcome: Introduces NewTerm, an automated benchmark showing over 20% performance reduction in LLMs when they don't understand new terms

## Executive Summary
This paper introduces NewTerm, a highly automated benchmark for evaluating large language models' ability to understand real-time new terms. The benchmark addresses the challenge of LLMs' knowledge cutoffs by constructing tasks based on newly added dictionary terms. The authors design three open-domain tasks—Choice of Multiple Alternatives (COMA), Choice of Similar Terms (COST), and Common Sense Judgment (CSJ)—to comprehensively assess LLM performance. The benchmark construction pipeline uses LLMs to generate questions and filter them with human verification, ensuring high-quality results with minimal effort. Empirical results show over 20% performance reduction in LLMs when they do not understand new terms. The study also reveals that updates to LLMs' knowledge cutoffs do not fully generalize to more distant new terms. NewTerm 2022 and 2023 are released, with annual updates planned.

## Method Summary
The method involves collecting new terms from online dictionaries (Cambridge, Collins, Oxford) for target years, using LLMs to automatically generate questions for three tasks (COMA, COST, CSJ) with related term generation for incorrect choices, applying LLM filtering to remove inconsistent questions, then human filtering for final quality verification. The benchmark evaluates LLM performance under Base and Gold settings to measure the impact of understanding new terms.

## Key Results
- Over 20% performance reduction in LLMs when they do not understand new terms
- Limited overlap in learned new terms between different LLM series
- Updates to LLMs' knowledge cutoffs do not fully generalize to more distant new terms
- NewTerm 2022 and 2023 benchmarks successfully constructed and released

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Automated LLM-based generation of high-quality benchmark questions with minimal human effort.
- Mechanism: The pipeline uses LLMs to generate questions, related terms, and incorrect choices, then filters with LLM and human verification to ensure quality.
- Core assumption: LLMs can generate semantically related incorrect choices and filter out low-quality questions reliably.
- Evidence anchors: [abstract] "We design a highly automated construction method to ensure high-quality benchmark construction with minimal human effort"; [section 3.4] "The high consistency with human annotations demonstrates that we can automatically construct and update benchmarks"; [corpus] Found 25 related papers; average neighbor FMR=0.404 suggests moderate relevance but limited direct citations, indicating this is a novel automation approach.
- Break condition: LLM-generated incorrect choices become too weakly correlated or too easy to eliminate, reducing benchmark difficulty.

### Mechanism 2
- Claim: Tracking real-time LLM knowledge evolution by benchmarking annually updated new terms.
- Mechanism: Collect new terms from dictionaries each year, construct benchmarks, and evaluate multiple LLM versions to measure knowledge cutoff updates.
- Core assumption: Dictionary updates reflect genuine emergence of new language usage and knowledge that LLMs should learn.
- Evidence anchors: [abstract] "We construct NewTerm 2022 and 2023 to evaluate the new terms updated each year and will continue updating annually"; [section 3.2] "We collect these terms from the update logs of three prominent online dictionaries: Cambridge, Collins, and Oxford"; [corpus] Average neighbor citations=0.0 suggests this annual tracking approach is novel in the field.
- Break condition: Dictionary update frequency becomes too sparse or new terms become dominated by domain-specific jargon not reflecting general language evolution.

### Mechanism 3
- Claim: Performance degradation reveals LLM's inability to understand new terms, creating a measurable gap between base and gold settings.
- Mechanism: Compare LLM performance on questions with and without new term definitions provided, isolating the impact of unknown terms.
- Core assumption: Providing definitions removes the knowledge cutoff effect, isolating pure language understanding capability.
- Evidence anchors: [abstract] "Empirical results on various LLMs demonstrate over 20% performance reduction caused by new terms"; [section 4.2] "Results under Gold settings can be seen as the score for LLMs when they understand every term"; [corpus] Weak corpus support (no citations), but the claim is directly supported by the paper's experimental results.
- Break condition: LLMs develop meta-learning capabilities that allow them to infer meanings of new terms from context without explicit definitions.

## Foundational Learning

- Concept: Knowledge cutoff in LLMs
  - Why needed here: Understanding why LLMs struggle with new terms requires grasping the temporal limitation of their training data.
  - Quick check question: If an LLM was trained on data up to 2021, would it understand a term first added to dictionaries in 2023?

- Concept: Automated data generation using LLMs
  - Why needed here: The benchmark construction relies on using LLMs to generate questions, choices, and perform filtering.
  - Quick check question: What are the advantages and risks of using LLMs to generate evaluation data versus human annotation?

- Concept: Term frequency and deducing difficulty
  - Why needed here: The paper analyzes which types of new terms are more challenging based on these features.
  - Quick check question: Why would a new word with low frequency and high deducing difficulty be more challenging for LLMs than a new phrase with similar characteristics?

## Architecture Onboarding

- Component map: Dictionary scraping -> Term selection -> Categorization -> Question generation -> Related term generation -> Incorrect choice generation -> LLM filtering -> Human filtering -> Benchmark release
- Critical path: Term collection -> Question generation -> LLM filtering -> Human filtering -> Benchmark release
  The filtering stages are critical because errors compound through the pipeline.
- Design tradeoffs:
  - Automation vs. quality: More automation reduces cost but risks lower quality
  - Coverage vs. depth: More terms provide broader coverage but may dilute difficulty
  - Frequency of updates vs. stability: Annual updates capture trends but may introduce volatility
- Failure signatures:
  - Low inter-annotator agreement indicates ambiguous questions
  - Performance gap between base and gold settings that's too small suggests insufficient challenge
  - High variance across LLM versions suggests unstable evaluation metrics
- First 3 experiments:
  1. Run the pipeline end-to-end with a small set of terms (5-10) to verify all components work
  2. Compare LLM filtering results with human filtering on the same small dataset to validate automation
  3. Test the complete benchmark on a single LLM version to verify evaluation metrics work correctly

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs perform on new terms from different sources beyond dictionaries (e.g., social media, technical documents)?
- Basis in paper: [explicit] The paper mentions the method could be extended to new terms from broader sources but hasn't validated it yet.
- Why unresolved: The authors state they haven't validated the approach across a wider variety of new term sources due to budget constraints.
- What evidence would resolve it: Conducting experiments using the same framework to generate benchmarks from social media posts, technical documents, or other non-dictionary sources and comparing LLM performance across these different sources.

### Open Question 2
- Question: Does the performance gap between Base and Gold settings change when using different prompting strategies or few-shot learning approaches?
- Basis in paper: [explicit] The authors mention that few-shot settings didn't show obvious improvements in preliminary experiments but don't explore this further.
- Why unresolved: The paper only tests zero-shot settings and acknowledges potential improvements from prompting methods but doesn't investigate them.
- What evidence would resolve it: Running experiments with various prompting strategies (chain-of-thought, role-playing, etc.) and few-shot examples to measure changes in the Base-Gold performance gap across different LLM models.

### Open Question 3
- Question: How does the overlap of learned new terms between different LLM series change when controlling for knowledge cutoff periods?
- Basis in paper: [explicit] The authors found limited overlap in learned terms between different LLM series but note that knowledge cutoff periods vary among them.
- Why unresolved: The paper acknowledges that varying knowledge cutoff spans contribute to limited overlap but doesn't control for this variable in their analysis.
- What evidence would resolve it: Comparing learned term overlap between LLM series with matched knowledge cutoff periods or normalizing the analysis to account for different time windows covered by each model series.

## Limitations
- The automated construction pipeline may introduce bias through LLM-generated content and filtering
- The evaluation relies on the assumption that dictionary updates accurately reflect language evolution LLMs should track
- Limited corpus support for the automated benchmark generation approach suggests it may be novel but unproven

## Confidence

- **High confidence**: The performance degradation findings (>20% reduction) are directly supported by experimental results and the mechanism of comparing base vs gold settings is well-documented
- **Medium confidence**: The automated pipeline's effectiveness is demonstrated through human verification consistency, but long-term quality assurance across multiple annual updates remains unproven
- **Low confidence**: The assumption that dictionary updates comprehensively capture language evolution that LLMs should track, as this depends on dictionary selection criteria and update frequency

## Next Checks
1. Replicate the automated pipeline with a small subset of terms (5-10) and compare LLM filtering results against human annotations to verify the claimed high consistency
2. Test cross-model stability by evaluating the benchmark on multiple LLM versions with different knowledge cutoffs to assess whether the >20% performance degradation claim holds consistently across models
3. Analyze term selection criteria by examining whether the frequency and deducing difficulty metrics from Google Search results and LLM meaning deduction actually predict question difficulty as claimed