---
ver: rpa2
title: Why Not Together? A Multiple-Round Recommender System for Queries and Items
arxiv_id: '2412.10787'
source_url: https://arxiv.org/abs/2412.10787
tags:
- magus
- user
- items
- item
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes MAGUS, a multiple-round recommender system that
  jointly considers queries and items to enhance recommendation performance. The core
  idea is to construct a relational graph connecting queries and items through shared
  categorical words, enabling the system to leverage dependencies between them.
---

# Why Not Together? A Multiple-Round Recommender System for Queries and Items

## Quick Facts
- arXiv ID: 2412.10787
- Source URL: https://arxiv.org/abs/2412.10787
- Authors: Jiarui Jin; Xianyu Chen; Weinan Zhang; Yong Yu; Jun Wang
- Reference count: 40
- Primary result: MAGUS significantly improves recommendation accuracy through joint query-item relational graph and multi-round interactions

## Executive Summary
MAGUS introduces a novel multiple-round recommender system that jointly considers queries and items through a relational graph framework. The system constructs connections between queries and items via shared categorical words, enabling it to capture dependencies that single-round systems miss. By combining offline-tuned initialization with iterative label propagation, MAGUS progressively refines recommendations based on user feedback across multiple interaction rounds. Experimental results demonstrate substantial improvements over both single-round and conversational approaches across three real-world datasets.

## Method Summary
MAGUS operates by first constructing a relational graph where nodes represent queries and items, connected through shared categorical attributes. An offline-tuned recommendation model initializes node scores based on historical data. During multiple rounds of interaction, the system employs label propagation to update scores based on user feedback, progressively refining the recommendation list. The framework leverages the dual perspective of queries and items to address data sparsity and user preference uncertainty inherent in single-round systems. The iterative process allows the system to learn from each interaction round, building a more comprehensive understanding of user intent than traditional approaches.

## Key Results
- Significant improvement in session-wise accuracy (SA@3 and SA@5) over single-round and conversational baselines
- Enhanced round-wise accuracy (RA@3) demonstrating effective progressive refinement across multiple interaction rounds
- Outperforms 12 different recommendation methods across Amazon, Movielens, and Yelp datasets

## Why This Works (Mechanism)
The core mechanism leverages the relational dependencies between queries and items through shared categorical attributes. By constructing a unified graph structure, MAGUS can propagate information bidirectionally between queries and items, addressing the inherent data sparsity when either is considered in isolation. The multiple-round interaction framework allows the system to progressively refine recommendations based on user feedback, mimicking natural conversational patterns while maintaining computational efficiency. The offline initialization provides a strong starting point, while label propagation adapts recommendations in real-time based on user interactions.

## Foundational Learning
- Relational graph construction: Connecting queries and items through shared attributes creates a unified space for recommendation
  - Why needed: Single-round systems treat queries and items separately, missing valuable interaction patterns
  - Quick check: Verify graph density and connectivity metrics across different datasets

- Label propagation dynamics: Information flows bidirectionally between queries and items during each interaction round
  - Why needed: Enables the system to leverage user feedback to refine both query understanding and item recommendations
  - Quick check: Track score distribution changes across rounds to verify meaningful propagation

- Multi-round interaction framework: Progressive refinement through sequential user feedback
  - Why needed: Single interactions provide limited information about user preferences and intent
  - Quick check: Measure convergence rate of recommendation accuracy across rounds

## Architecture Onboarding

Component map: Offline model -> Graph construction -> Label propagation -> User interaction loop

Critical path: User query → Graph lookup → Initial scoring → Label propagation → Recommendation output → User feedback → Score update → Next round

Design tradeoffs: The system balances between computational efficiency (offline initialization) and adaptability (online label propagation). The relational graph approach requires upfront construction overhead but enables more efficient information propagation during interactions.

Failure signatures: Poor performance may manifest as stagnant scores across rounds, indicating ineffective label propagation, or highly polarized score distributions suggesting initialization issues.

First experiments:
1. Measure graph connectivity metrics (average path length, clustering coefficient) across datasets
2. Compare convergence rates of recommendation accuracy across different label propagation parameters
3. Evaluate sensitivity of results to offline model initialization quality

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on synthetic interactions rather than real user behavior
- Performance gains measured primarily through accuracy metrics without examining computational efficiency trade-offs
- Label propagation mechanism scalability concerns for larger graphs with millions of nodes

## Confidence
High confidence in core methodology - relational graph construction and multi-round framework appear technically sound
Medium confidence in performance claims - derived from controlled experiments on specific datasets that may not generalize
Low confidence in practical deployment implications - computational overhead and real-world user adoption remain uncertain

## Next Checks
1. Deploy MAGUS in a controlled user study with real-time interactions to measure actual user satisfaction and engagement
2. Conduct stress testing on larger datasets (millions of items/queries) to evaluate computational scalability and runtime performance
3. Implement ablation studies isolating each component (graph construction, label propagation, offline initialization) to quantify individual contributions