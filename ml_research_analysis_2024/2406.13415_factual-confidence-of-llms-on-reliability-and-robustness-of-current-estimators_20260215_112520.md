---
ver: rpa2
title: 'Factual Confidence of LLMs: on Reliability and Robustness of Current Estimators'
arxiv_id: '2406.13415'
source_url: https://arxiv.org/abs/2406.13415
tags:
- confidence
- methods
- factual
- language
- trained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper surveys and compares existing methods for estimating
  the factual confidence of large language models (LLMs), i.e., how likely a model
  is to know or accurately state a fact. It introduces an experimental framework covering
  both fact-verification and question answering setups, and compares five method categories:
  trained probes, sequence probability, verbalization, surrogate token probability,
  and consistency.'
---

# Factual Confidence of LLMs: on Reliability and Robustness of Current Estimators

## Quick Facts
- **arXiv ID**: 2406.13415
- **Source URL**: https://arxiv.org/abs/2406.13415
- **Reference count**: 11
- **Primary result**: Trained hidden-state probes outperform prompting-based methods for factual confidence estimation, though requiring model access.

## Executive Summary
This paper presents a systematic evaluation of methods for estimating factual confidence in large language models. The authors introduce a unified experimental framework and compare five distinct approaches across eight LLMs from the Falcon and Mistral families. Their findings demonstrate that trained probes on hidden states consistently outperform prompting-based methods for factual confidence estimation. The study also reveals significant instability in LLM confidence across semantically equivalent inputs, suggesting that models do not always robustly encode facts as abstractions over linguistic variations.

## Method Summary
The paper compares five groups of methods for factual confidence estimation: trained probes (neural networks trained on LLM hidden states), sequence probability (likelihood of factual statements), verbalization (LLM generates confidence as text), surrogate token probability (using special tokens as confidence indicators), and consistency (multiple generations for stability). The experimental framework uses two datasets - Lama T-REx for fact-verification (predicting P(True)) and PopQA for question answering (predicting P(I Know)). Evaluation is performed using area under the precision-recall curve (AUPRC) as the primary metric, with eight publicly available LLMs from Falcon and Mistral families serving as test subjects.

## Key Results
- Trained probes achieve the highest AUPRC scores across both datasets, outperforming all prompting-based methods
- Sequence probability and verbalization methods show moderate performance but remain sensitive to prompt variations
- LLM confidence exhibits significant instability across semantically equivalent inputs
- The reliability gap between trained probes and prompting-based methods persists even with larger models

## Why This Works (Mechanism)
Trained probes work by directly accessing the internal representations of LLMs, capturing factual knowledge encoded in hidden states before it's transformed into language-specific outputs. This bypasses the linguistic variability that causes prompting-based methods to fail on semantically equivalent inputs.

## Foundational Learning
- **Hidden state representations**: Internal model states containing encoded knowledge (why needed: probes analyze these directly; quick check: verify hidden state dimension compatibility)
- **Precision-recall curve**: Evaluation metric for ranking quality (why needed: measures ability to distinguish true from false facts; quick check: plot curves for visual inspection)
- **Prompt engineering**: Crafting inputs to elicit desired model behavior (why needed: verbalization and surrogate methods rely on prompts; quick check: test multiple prompt variations)
- **Semantic equivalence**: Different phrasings conveying same meaning (why needed: reveals model instability; quick check: create paraphrases of test facts)
- **Fact verification vs QA**: Different tasks for confidence estimation (why needed: validates methods across task types; quick check: compare performance across both datasets)

## Architecture Onboarding

**Component Map**
Input Dataset -> Preprocessing -> Method Application -> Confidence Score Generation -> AUPRC Evaluation

**Critical Path**
Dataset → Method Implementation → Confidence Score Calculation → Metric Computation

**Design Tradeoffs**
- Trained probes: High accuracy but requires model access and training data
- Prompting methods: Easy to deploy via API but sensitive to prompt variations
- Consistency method: Simple to implement but computationally expensive

**Failure Signatures**
- Low AUPRC scores indicate method implementation issues or poor prompt quality
- High variance across semantically equivalent inputs suggests model instability
- Poor out-of-domain generalization reveals overfitting in trained probes

**First Experiments**
1. Implement all five methods on a small subset of Lama T-REx
2. Test trained probe training with reduced hidden state dimensions
3. Compare consistency method with varying number of generations

## Open Questions the Paper Calls Out
- **Open Question 1**: How do trained probe methods perform when applied to non-atomic facts or more complex reasoning tasks? The paper focuses on atomic facts and simple QA tasks but did not study more complex aspects of factual knowledge.
- **Open Question 2**: What is the impact of prompt variations on the reliability of verbalized confidence and surrogate token probability methods? The paper mentions sensitivity to prompt variations but lacks detailed analysis of how different prompts affect performance.
- **Open Question 3**: Can the reliability gap between trained probe methods and prompting-based methods be reduced with larger, more capable LLMs? The paper suggests this might be possible but lacks empirical evidence.

## Limitations
- Results may not generalize beyond Falcon and Mistral model families
- Trained probe method requires access to model weights and training data
- Analysis focuses on atomic facts, not complex reasoning or non-factual knowledge

## Confidence
- **High confidence**: Relative ranking of method performance across tested models and datasets
- **Medium confidence**: Claim that trained probes are most reliable method for factual confidence estimation
- **Medium confidence**: Observation about instability across semantically equivalent inputs

## Next Checks
1. Test the proposed methods on additional LLM families (e.g., GPT, Claude, LLaMA) to assess generalizability
2. Evaluate performance on more diverse factual domains and specialized knowledge bases
3. Conduct ablation studies on trained probe architecture to identify minimal effective configuration