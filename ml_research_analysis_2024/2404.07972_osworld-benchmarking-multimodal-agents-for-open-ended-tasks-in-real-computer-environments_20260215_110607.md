---
ver: rpa2
title: 'OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer
  Environments'
arxiv_id: '2404.07972'
source_url: https://arxiv.org/abs/2404.07972
tags:
- pyautogui
- tasks
- click
- action
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OSWorld introduces the first real computer environment for benchmarking
  multimodal agents on open-ended tasks involving arbitrary applications across operating
  systems. It supports task setup, execution-based evaluation, and interactive learning,
  enabling reliable assessment of complex workflows spanning multiple apps and interfaces.
---

# OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments

## Quick Facts
- arXiv ID: 2404.07972
- Source URL: https://arxiv.org/abs/2404.07972
- Reference count: 40
- State-of-the-art agents achieve only 12.24% task completion on real-world computer tasks

## Executive Summary
OSWorld introduces the first real computer environment benchmark for evaluating multimodal agents on open-ended tasks across operating systems. The benchmark addresses the gap in assessing agent performance on complex workflows involving multiple applications, requiring both GUI understanding and operational knowledge. It provides 369 real-world tasks derived from actual user scenarios, complete with initial state configurations and custom evaluation scripts. The benchmark reveals significant performance gaps between humans and current AI agents, with humans completing 72.2% of tasks while the best model achieves only 12.24%, highlighting the need for improved GUI grounding and multimodal reasoning capabilities.

## Method Summary
OSWorld establishes a real computer environment that enables comprehensive evaluation of multimodal agents on open-ended tasks. The benchmark provides 369 real-world tasks derived from actual user scenarios, each with detailed initial state configurations and custom evaluation scripts. The environment supports task setup, execution-based evaluation, and interactive learning, enabling reliable assessment of complex workflows spanning multiple applications and interfaces. Agents are evaluated through execution-based methods that measure whether they can successfully complete entire workflows rather than isolated actions.

## Key Results
- Human performance baseline: 72.2% task completion rate
- Best AI model performance: 12.24% task completion rate
- Significant performance gap highlights limitations in GUI understanding and operational knowledge
- Agents struggle with complex workflows spanning multiple applications

## Why This Works (Mechanism)
OSWorld works by creating a controlled yet realistic environment that captures the complexity of real-world computer tasks. The benchmark's execution-based evaluation methodology ensures that agents must complete entire workflows rather than just individual actions, providing a more accurate assessment of their practical capabilities. By including tasks that span multiple applications and require both GUI understanding and operational knowledge, OSWorld exposes the limitations of current multimodal agents in handling real-world scenarios.

## Foundational Learning
- GUI grounding: Agents must understand and interact with graphical user interfaces - needed for visual task comprehension, quick check: can the agent locate and click on specific UI elements
- Multimodal reasoning: Combining visual and textual information for decision-making - needed for understanding complex workflows, quick check: can the agent interpret instructions while navigating GUIs
- Operational knowledge: Understanding of system operations and file management - needed for executing multi-step tasks, quick check: can the agent perform file operations across different applications
- Cross-application workflows: Managing tasks that span multiple software programs - needed for real-world computer use, quick check: can the agent coordinate actions across different applications
- Initial state configuration: Setting up environments to match task requirements - needed for reproducible evaluation, quick check: can the environment consistently recreate test conditions

## Architecture Onboarding

**Component Map:** Task Generator -> Environment Setup -> Agent Execution -> Evaluation Script -> Performance Metrics

**Critical Path:** The benchmark execution follows a linear flow from task definition through environment configuration to agent execution and evaluation. Each task begins with initial state setup, proceeds through agent interaction with the environment, and concludes with automated evaluation against success criteria.

**Design Tradeoffs:** The benchmark prioritizes ecological validity by using real applications and workflows over controlled synthetic tasks. This increases realism but reduces controllability compared to artificial environments. The execution-based evaluation provides more accurate performance measurement but requires more computational resources than action-based metrics.

**Failure Signatures:** Common failure modes include inability to locate GUI elements, confusion between similar interface components, failure to maintain context across application boundaries, and incorrect sequence of operations. Agents particularly struggle with tasks requiring memory of previous steps or coordination across multiple windows.

**3 First Experiments:**
1. Baseline human performance measurement to establish reference point
2. Single-application task execution to test basic GUI interaction capabilities
3. Multi-application workflow completion to assess cross-application reasoning

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability limited to 369 tasks derived from real user scenarios, potentially missing broader workflow space
- Custom evaluation scripts per task may introduce inconsistency and subjectivity in scoring
- Human baseline of 72.2% measured under specific conditions with potentially skilled participants
- Linux environment reliance with Xvfb raises questions about cross-platform compatibility and real-world applicability

## Confidence
- High confidence in technical implementation of benchmark infrastructure
- Medium confidence in reported performance gap between humans and AI agents
- Medium confidence in generalizability of findings to broader computer-use scenarios
- Low confidence in ecological validity of evaluation setup compared to real-world deployment conditions

## Next Checks
1. Conduct cross-platform validation testing on Windows and macOS environments to assess OS dependency
2. Perform inter-rater reliability testing on human task completion to verify the 72.2% baseline
3. Implement blind validation with independent task creation and scoring to verify benchmark objectivity