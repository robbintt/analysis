---
ver: rpa2
title: Understanding the Generalization Benefits of Late Learning Rate Decay
arxiv_id: '2401.11600'
source_url: https://arxiv.org/abs/2401.11600
tags:
- learning
- loss
- training
- rate
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates why training neural networks with large
  learning rates for extended periods improves generalization. Through experiments
  on CIFAR-10 with VGG-11, the authors visualize training and testing loss landscapes,
  revealing that training trajectories navigate a flat minima manifold while testing
  loss has an isolated minimum.
---

# Understanding the Generalization Benefits of Late Learning Rate Decay

## Quick Facts
- arXiv ID: 2401.11600
- Source URL: https://arxiv.org/abs/2401.11600
- Authors: Yinuo Ren, Chao Ma, Lexing Ying
- Reference count: 40
- One-line primary result: Late learning rate decay improves generalization by navigating through training loss minima manifold toward testing loss minimum

## Executive Summary
This paper investigates why training neural networks with large learning rates for extended periods improves generalization. Through experiments on CIFAR-10 with VGG-11, the authors visualize training and testing loss landscapes, revealing that training trajectories navigate a flat minima manifold while testing loss has an isolated minimum. They introduce a nonlinear overparameterized model with reparameterization y = ||w||^γ w^T x that mirrors these observed landscape behaviors. The training process is characterized in three phases: (I) initial large learning rate drives trajectory toward minima manifold, (II) extended large learning rate navigates through manifold to minimum L2-norm solution, and (III) small learning rate refines convergence. Theoretical analysis proves Phase II convergence to minimum L2-norm solution with high probability, explaining the empirical benefits of late learning rate decay for improved generalization.

## Method Summary
The paper combines empirical experiments with theoretical analysis to understand generalization benefits of late learning rate decay. Empirically, VGG-11 is trained on CIFAR-10 with different learning rate schedules, and parameter trajectories are collected and visualized using PCA to reveal loss landscape structure. Theoretically, a nonlinear overparameterized model is introduced with reparameterization y = ||w||^γ w^T x, and convergence to minimum L2-norm solution is analyzed using gradient flow equations. The three-phase training process is characterized and proven mathematically for the nonlinear model.

## Key Results
- Training loss landscape exhibits a minima manifold with open level sets, while testing loss has an isolated minimum with closed level sets
- Extended training with large learning rates drives models toward minimum L2-norm solution of training loss, improving generalization
- Theoretical proof shows Phase II convergence to minimum L2-norm solution with probability at least 1 - e^(-0.1γ²) for the nonlinear model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training trajectories with large learning rates navigate through the minima manifold of the training loss toward the testing loss minimum.
- Mechanism: The minima manifold is a low-dimensional subspace of parameters with near-zero training loss. SGD with large learning rates explores this manifold, moving closer to the testing loss minimum despite not directly minimizing testing loss.
- Core assumption: The training loss landscape contains a connected manifold of global minima, while the testing loss has an isolated minimum with closed level sets.
- Evidence anchors:
  - [abstract]: "the training trajectory with a large learning rate navigates through the minima manifold of the training loss, finally nearing the neighborhood of the testing loss minimum"
  - [section]: "The training loss landscape displays a minima manifold characterized by open level sets, while the testing loss landscape presents an isolated minimum with closed level sets"
  - [corpus]: Weak evidence - related papers discuss learning rate decay but not this specific manifold-navigation mechanism
- Break condition: If the training and testing loss landscapes become aligned (e.g., through data augmentation or architecture changes), the manifold navigation may no longer provide generalization benefits.

### Mechanism 2
- Claim: Extended training with large learning rates drives the model toward the minimum L2-norm solution of the training loss.
- Mechanism: The nonlinear overparameterized model introduces a reparameterization that creates a "shrinking" landscape. Large learning rates enable the trajectory to escape local minima and find the minimum norm solution through the manifold.
- Core assumption: The minimum L2-norm solution of overparameterized models corresponds to near-optimal generalization.
- Evidence anchors:
  - [abstract]: "an extended phase with a large learning rate steers our model towards the minimum L2-norm solution of the training loss, which may achieve near-optimal generalization"
  - [section]: "Phase II propels the model towards the minimum L2-norm solution of the training loss"
  - [corpus]: Weak evidence - related papers discuss weight decay and regularization but not this specific L2-norm mechanism
- Break condition: If the data distribution changes or the model becomes underparameterized, the minimum norm solution may no longer generalize well.

### Mechanism 3
- Claim: The nonlinear reparameterization introduced by neural network depth creates the observed landscape structure.
- Mechanism: Starting from an overparameterized linear regression model, the testing loss yields an isolated minimum while the training loss has a manifold due to the null space. The reparameterization y = ||w||^γ w^T x warps the parameter space, creating the observed "shrinking" training loss landscape.
- Core assumption: The depth of neural networks introduces nonlinearity that transforms the quadratic loss landscapes into the observed structure.
- Evidence anchors:
  - [abstract]: "a nonlinear model whose loss landscapes mirror those observed for real neural networks"
  - [section]: "the flatness of the training loss landscape near the testing loss minimum is intrinsically linked to the depth of the neural networks"
  - [corpus]: Weak evidence - related papers discuss neural network depth but not this specific reparameterization mechanism
- Break condition: If the network architecture lacks sufficient depth or uses different activation functions, the reparameterization may not create the desired landscape structure.

## Foundational Learning

- Concept: Minima manifold in overparameterized models
  - Why needed here: Understanding why large learning rates navigate through the manifold is central to the paper's explanation
  - Quick check question: What distinguishes a minima manifold from isolated minima in loss landscapes?

- Concept: Implicit regularization through optimization dynamics
  - Why needed here: The paper argues that SGD with large learning rates implicitly regularizes toward minimum norm solutions
  - Quick check question: How does the optimization trajectory implicitly determine which minimum to converge to?

- Concept: Landscape visualization through PCA
  - Why needed here: The paper's key insight comes from visualizing high-dimensional loss landscapes in 2D
  - Quick check question: What are the limitations of using PCA for visualizing neural network loss landscapes?

## Architecture Onboarding

- Component map:
  CIFAR-10 image classification task -> VGG-11 neural network architecture -> SGD optimizer with learning rate scheduling -> PCA-based loss landscape visualization -> Nonlinear overparameterized model with reparameterization

- Critical path:
  1. Train VGG-11 on CIFAR-10 with different learning rate schedules
  2. Collect parameter trajectories at each epoch
  3. Apply PCA to extract principal components
  4. Visualize training and testing loss landscapes
  5. Implement and analyze the nonlinear model
  6. Characterize the three-phase training process

- Design tradeoffs:
  - Using VGG-11 vs simpler architectures: More realistic but computationally expensive
  - PCA dimensionality reduction: Intuitive visualization but may miss important landscape features
  - Continuous-time analysis: Mathematically tractable but may not capture discrete effects

- Failure signatures:
  - Training loss doesn't stabilize on the manifold
  - Testing loss doesn't improve with delayed learning rate decay
  - PCA doesn't capture sufficient variance in parameter trajectories
  - Nonlinear model doesn't reproduce observed landscape behaviors

- First 3 experiments:
  1. Train VGG-11 with different learning rate decay timings and record testing performance
  2. Visualize loss landscapes using PCA on collected parameter trajectories
  3. Implement the nonlinear model and verify it reproduces the observed landscape structure

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model's parameter norm ∥w∥ influence the convergence speed to the minimum L2-norm solution in Phase II, and can this relationship be quantified more precisely?
- Basis in paper: [explicit] The paper mentions that the convergence rate depends on the parameter γ, which represents the depth of the neural network, but the exact relationship is not fully characterized.
- Why unresolved: The paper provides an upper bound on the convergence rate as γ approaches infinity but does not provide a precise characterization for finite γ values.
- What evidence would resolve it: A more detailed analysis of the convergence rate for different values of γ, potentially through simulations or further theoretical analysis.

### Open Question 2
- Question: How does the choice of learning rate scheduling (e.g., cyclic learning rates) affect the generalization benefits observed in the study?
- Basis in paper: [inferred] The paper focuses on late learning rate decay but does not explore other learning rate scheduling strategies.
- Why unresolved: The study's focus on late learning rate decay limits the generalizability of the findings to other scheduling strategies.
- What evidence would resolve it: Experiments comparing the generalization performance of different learning rate schedules on the same models and datasets.

### Open Question 3
- Question: How does the proposed nonlinear model with reparametrization y = ||w||^γ w^T x compare to other overparameterized models in terms of capturing the loss landscape behavior observed in neural networks?
- Basis in paper: [explicit] The paper introduces this model to explain the loss landscape behavior observed in neural networks but does not compare it to other models.
- Why unresolved: The paper does not provide a comprehensive comparison of the proposed model with other overparameterized models.
- What evidence would resolve it: A comparative study of the proposed model with other overparameterized models, evaluating their ability to capture the loss landscape behavior.

## Limitations
- Experimental results are limited to CIFAR-10 dataset and VGG-11 architecture, limiting generalizability
- Theoretical analysis relies on simplifying assumptions about loss landscape structure that may not hold in practice
- PCA-based visualization technique may miss important high-dimensional features of the loss landscape

## Confidence
- High confidence: The empirical observations of improved generalization with late learning rate decay and the characterization of the three-phase training process are well-supported by experimental results on CIFAR-10
- Medium confidence: The theoretical analysis of the nonlinear model provides a plausible explanation for the observed phenomena, but the assumptions required for the proofs may not hold in practical scenarios
- Low confidence: The generalizability of these findings to other architectures, datasets, or optimization algorithms remains uncertain without additional empirical validation

## Next Checks
1. **Architecture Transferability**: Replicate the experiments with different neural network architectures (ResNet, DenseNet) on CIFAR-10 to test whether the late learning rate decay benefits are architecture-dependent.

2. **Dataset Generalization**: Apply the same experimental protocol to other image classification datasets (ImageNet, SVHN) to determine if the observed phenomena extend beyond CIFAR-10.

3. **Optimization Algorithm Robustness**: Test whether similar benefits can be achieved using adaptive optimizers (Adam, RMSprop) with appropriate learning rate scheduling strategies, or if the effects are specific to SGD dynamics.