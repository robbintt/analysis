---
ver: rpa2
title: Context Quality Matters in Training Fusion-in-Decoder for Extractive Open-Domain
  Question Answering
arxiv_id: '2403.14197'
source_url: https://arxiv.org/abs/2403.14197
tags:
- context
- quality
- training
- passages
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how context quality and quantity during
  training affect the performance of Fusion-in-Decoder (FiD) models in extractive
  open-domain question answering tasks. The authors demonstrate that FiD models tend
  to overfit to context quality during training, resulting in degraded performance
  when evaluated on contexts with different qualities.
---

# Context Quality Matters in Training Fusion-in-Decoder for Extractive Open-Domain Question Answering

## Quick Facts
- arXiv ID: 2403.14197
- Source URL: https://arxiv.org/abs/2403.14197
- Reference count: 40
- Primary result: FiD models overfit to context quality during training, but temperature-based cross-attention adjustment can mitigate this effect

## Executive Summary
This paper investigates how context quality and quantity during training affect Fusion-in-Decoder (FiD) model performance in extractive open-domain question answering. The authors demonstrate that FiD models overfit to the context quality distribution used during training, resulting in suboptimal performance when evaluated on contexts with different quality levels. Through experimental analysis, they reveal that FiD models trained with different context qualities exhibit distinct cross-attention distribution patterns, with higher quality contexts leading to more uniform attention across passages. Based on these findings, the authors propose a method to mitigate overfitting by introducing temperature-based bias to the cross-attention distribution, which is shown to effectively improve model performance when deployed in environments with different context quality than those used during training.

## Method Summary
The authors conducted experiments using the Fusion-in-Decoder (FiD) architecture on the NaturalQuestions dataset. They trained multiple FiD models with varying context quality conditions by creating two sets of contexts: one enriched with gold answer passages (n+) and one without (n-). They evaluated these models on both same-quality and cross-quality conditions. The cross-attention patterns were analyzed using Integrated Gradients to visualize how models distribute attention across passages. To address the overfitting issue, they introduced a temperature parameter T to control the sharpness of cross-attention probabilities during inference, allowing the model to adjust its attention distribution based on context quality.

## Key Results
- FiD models trained on lower context quality (n-) showed performance degradation when evaluated on higher quality contexts (n+), and vice versa
- Context quality during training had a stronger impact on model performance than context quantity
- Temperature-based intervention on cross-attention distribution effectively mitigated overfitting, with optimal temperatures found between 1.5-2.0
- Cross-attention patterns varied systematically with training context quality, with higher quality training leading to more uniform attention across passages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FiD models overfit to context quality during training, causing performance degradation when evaluated on contexts with different quality.
- Mechanism: During training, the model learns attention patterns that are highly selective to relevant passages when context quality is low. When evaluated on high-quality contexts (mostly relevant), this excessive selectivity causes the model to overlook necessary information in less prominent relevant passages, leading to suboptimal performance.
- Core assumption: The attention mechanism learned during training becomes too specialized to the training context quality distribution.
- Evidence anchors:
  - [abstract]: "FiD models overfit to context quality during training and show suboptimal performance when evaluated on different context quality."
  - [section]: "Models trained with lower context quality attended more strongly to relevant passages, especially at higher layers that are closer to the output layer."
  - [corpus]: Weak - no direct citations found supporting this specific overfitting mechanism.

### Mechanism 2
- Claim: FiD models trained with different context qualities show different cross-attention distribution patterns.
- Mechanism: Higher quality contexts during training lead to more uniform attention across passages, while lower quality contexts lead to more selective attention focused on relevant passages. This difference in attention patterns explains the overfitting behavior.
- Core assumption: The attention mechanism adapts to the training context quality by changing its selectivity pattern.
- Evidence anchors:
  - [abstract]: "FiD models trained with different context quality have different cross-attention distribution patterns. Specifically, as context quality during training increases, the trained models tend to attend more uniformly to each passage in context."
  - [section]: "Models trained with lower context quality showed more long-tailed distribution for relevant passages, and there was a more significant difference between the distribution for relevant and irrelevant passages."
  - [corpus]: Weak - no direct citations found supporting this specific attention pattern finding.

### Mechanism 3
- Claim: Introducing temperature-based bias to cross-attention distribution mitigates overfitting to context quality.
- Mechanism: By adjusting the temperature parameter, the sharpness of the attention distribution can be controlled. Higher temperatures lead to more uniform attention (simulating high-quality training), while lower temperatures lead to more selective attention (simulating low-quality training).
- Core assumption: The temperature parameter can effectively simulate the attention patterns learned during different quality training.
- Evidence anchors:
  - [abstract]: "based on these observations, we propose a method to mitigate overfitting to specific context quality by introducing bias to the cross-attention distribution, which we demonstrate to be effective in improving the performance of FiD models on different context quality."
  - [section]: "We propose to change sharpness of distribution of cross-attention probability during inference. More specifically, we introduce temperature parameter T (T > 0) and compute total cross-attention probability from the l-th decoder token to the i-th passage at the k-th layer."
  - [corpus]: Weak - no direct citations found supporting this specific temperature-based mitigation approach.

## Foundational Learning

- Concept: Cross-attention mechanism in Transformer decoders
  - Why needed here: Understanding how FiD models aggregate information from multiple passages through cross-attention is crucial for grasping the overfitting mechanism.
  - Quick check question: How does the cross-attention mechanism in FiD differ from standard Transformer attention?

- Concept: Context quality vs. context quantity distinction
  - Why needed here: The paper makes a key distinction between how context quality and quantity affect model training differently, which is central to the findings.
  - Quick check question: Why does context quality have a stronger impact on training than context quantity according to the paper?

- Concept: Overfitting in machine learning
  - Why needed here: The core finding is that FiD models overfit to context quality, which requires understanding what overfitting means and how it manifests in model behavior.
  - Quick check question: What is the difference between overfitting to data distribution vs. overfitting to specific features?

## Architecture Onboarding

- Component map: Question → Retriever → Passage Encoding → Cross-attention → Answer Generation
- Critical path: Question → Retriever → Passage Encoding → Cross-attention → Answer Generation
- Design tradeoffs:
  - Encoding passages independently vs. jointly (memory efficiency vs. cross-passage interaction)
  - Fixed vs. adaptive context quality during training (simpler training vs. robustness to deployment conditions)
  - Temperature adjustment during inference vs. retraining (computational cost vs. performance)
- Failure signatures:
  - Large performance drop when evaluation context quality differs from training
  - Attention becoming too selective (overfitting to low-quality contexts)
  - Attention becoming too uniform (underutilizing high-quality contexts)
- First 3 experiments:
  1. Train FiD models with varying context qualities (n+, n-) and evaluate on same vs. different qualities
  2. Analyze cross-attention patterns across layers for models trained on different qualities
  3. Apply temperature-based intervention to validate attention patterns cause overfitting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does context quality overfitting generalize to other model architectures beyond FiD, such as RAG or Internet-augmented language models?
- Basis in paper: [explicit] The paper states "it is unclear whether different architectures such as RAG (Lewis et al., 2020) and Internet-augmented language models (Lazaridou et al., 2022) produce similar results."
- Why unresolved: The authors only investigated FiD models and did not test other architectures.
- What evidence would resolve it: Experimental results comparing context quality effects on training for multiple model architectures (RAG, Internet-augmented models, etc.) would show if the overfitting phenomenon is architecture-specific or general.

### Open Question 2
- Question: How does model scale (size) affect the degree of overfitting to context quality during training?
- Basis in paper: [explicit] The authors note "it is unclear how scaling model size changes the behavior of overfitting to context quality."
- Why unresolved: Experiments were conducted only with t5-base sized models.
- What evidence would resolve it: Training and evaluating FiD models of different sizes (small, base, large, XL) on varying context qualities would reveal if model scale influences overfitting behavior.

### Open Question 3
- Question: Can we develop more sophisticated methods to predict optimal temperature parameters for individual questions rather than using a single temperature per fold?
- Basis in paper: [explicit] The authors mention "Predicting the optimal temperature parameter for each input question is an interesting direction of future works."
- Why unresolved: The current method uses a single temperature parameter for all questions within a fold.
- What evidence would resolve it: Developing and evaluating methods that predict individual temperature parameters per question, and measuring performance improvements compared to single temperature approaches.

## Limitations

- Context quality quantification relies primarily on gold answer passage presence rather than a robust, generalizable metric
- Cross-attention pattern analysis is primarily based on visualization rather than quantitative metrics
- Temperature-based mitigation requires access to context quality indicators during inference, which may not be available in practical deployment scenarios

## Confidence

**High Confidence**:
- FiD models show performance degradation when evaluation context quality differs from training context quality
- Context quality has a stronger impact on model performance than context quantity
- Temperature-based attention modification provides measurable performance improvements

**Medium Confidence**:
- The cross-attention distribution patterns differ systematically across training conditions
- Higher context quality during training leads to more uniform attention patterns
- Lower context quality during training leads to more selective attention patterns

**Low Confidence**:
- The proposed temperature-based mitigation method generalizes beyond the specific experimental setup
- The relationship between attention patterns and overfitting is fully understood
- The temperature parameter can be effectively tuned without quality indicators

## Next Checks

1. **Generalization Test**: Evaluate the temperature-based mitigation approach on a diverse set of open-domain QA datasets (e.g., NaturalQuestions, TriviaQA) with varying context quality distributions to assess generalizability beyond the current experimental setup.

2. **Ablation Study**: Conduct a systematic ablation study removing the temperature intervention at different layers to determine whether the effect is consistent across all layers or concentrated in specific attention layers.

3. **Quality Detection Integration**: Develop and evaluate methods for automatically detecting context quality during inference, then test whether quality-aware temperature tuning provides additional performance gains compared to fixed temperature settings.