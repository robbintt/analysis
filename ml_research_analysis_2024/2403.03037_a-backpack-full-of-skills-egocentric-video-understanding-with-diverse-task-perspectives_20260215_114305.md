---
ver: rpa2
title: 'A Backpack Full of Skills: Egocentric Video Understanding with Diverse Task
  Perspectives'
arxiv_id: '2403.03037'
source_url: https://arxiv.org/abs/2403.03037
tags:
- tasks
- task
- egopack
- learning
- temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EgoPack is a framework for learning egocentric video understanding
  tasks that builds a "backpack" of knowledge from multiple task perspectives. It
  uses a shared temporal graph-based architecture to model different tasks with minimal
  task-specific overhead, learning prototypes that abstract knowledge gained from
  each task.
---

# A Backpack Full of Skills: Egocentric Video Understanding with Diverse Task Perspectives

## Quick Facts
- arXiv ID: 2403.03037
- Source URL: https://arxiv.org/abs/2403.03037
- Reference count: 40
- Primary result: State-of-the-art on Long Term Anticipation (LTA) and competitive results on other Ego4D tasks

## Executive Summary
EgoPack introduces a framework for egocentric video understanding that learns multiple tasks through a shared temporal graph-based architecture with minimal task-specific overhead. The key innovation is building a "backpack" of knowledge through task-specific prototypes that abstract insights from each task, which can then be leveraged when learning novel tasks. By using prototypes as a source of additional insights and enabling cross-task interactions through message passing, EgoPack achieves state-of-the-art performance on Long Term Anticipation and competitive results on other Ego4D benchmarks, outperforming both single-task and multi-task learning baselines.

## Method Summary
EgoPack uses a shared temporal graph backbone where video segments form nodes connected by edges to temporally close segments. Each task has minimal task-specific heads, and during multi-task pretraining, task-specific features are aggregated by verb/noun labels to form prototypes. For novel task learning, these prototypes are matched using k-NN and refined through message passing to enrich task-specific features. The method is evaluated on four Ego4D benchmarks: Action Recognition, Long Term Anticipation, Object State Change Classification, and Point of No Return, using pre-extracted Omnivore Swin-L features and TRN temporal aggregation.

## Key Results
- Achieves state-of-the-art performance on Long Term Anticipation (edit distance 1.7)
- Outperforms single-task learning baselines on AR, OSCC, and LTA tasks
- Competitive results on Point of No Return task (temporal localization error 2.5 seconds)
- Demonstrates effective cross-task knowledge transfer through prototype-based interaction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-task knowledge transfer is enabled by prototypes that summarize task-specific perspectives and can be queried at inference time
- Mechanism: Task-specific heads produce features that are aggregated by verb/noun labels to form prototypes. During novel task learning, these prototypes are matched using k-NN and refined via message passing to enrich task-specific features
- Core assumption: Different egocentric tasks share semantic relationships that can be abstracted into reusable prototypes
- Evidence anchors:
  - [abstract]: "EgoPack builds a collection of task perspectives... used as a potential source of additional insights"
  - [section 3.3]: "We propose to explicitly model the perspectives of the different tasks as a set of task-specific prototypes that abstract the knowledge gained"
  - [corpus]: Weak - no direct mention of prototype-based transfer, but related works on MTL suggest this is novel
- Break condition: If semantic relationships between tasks are weak or inconsistent, prototypes may not capture transferable knowledge

### Mechanism 2
- Claim: Shared temporal graph architecture enables efficient multi-task learning with minimal task-specific overhead
- Mechanism: Videos are modeled as temporal graphs where nodes represent segments and edges connect temporally close segments. A shared GNN backbone processes all tasks with minimal task-specific heads
- Core assumption: Different egocentric vision tasks can be modeled using the same temporal graph structure with minimal task-specific modifications
- Evidence anchors:
  - [section 3.2]: "We present a unified temporal architecture to learn multiple egocentric vision tasks with minimal task-specific overhead"
  - [section 4.1]: "The task-specific perspectives are collected in a single pretraining step of a novel multi-task network under the form of prototypes"
  - [corpus]: Weak - most related works use separate architectures per task
- Break condition: If tasks require fundamentally different temporal resolutions or modeling approaches, shared architecture may be insufficient

### Mechanism 3
- Claim: Message passing between task features and prototypes enables effective cross-task interaction
- Mechanism: Task features are used as queries to select nearest neighbor prototypes. Message passing updates features by aggregating prototype information while keeping prototypes frozen
- Core assumption: Task features can effectively query relevant prototypes to improve novel task learning
- Evidence anchors:
  - [section 3.3]: "Task features and their neighbouring prototypes form a graph-like structure, on which message passing can be used to enrich the task-specific features"
  - [section 4.2]: "We observe that EgoPack is quite robust to the number of GNN layers in the interaction stage"
  - [corpus]: Weak - message passing for cross-task transfer is novel approach
- Break condition: If k-NN selection fails to find relevant prototypes or message passing causes feature distortion

## Foundational Learning

- Concept: Graph Neural Networks for temporal modeling
  - Why needed here: Videos are naturally sequential data where temporal relationships between segments matter for understanding actions and their consequences
  - Quick check question: Can you explain how the graph connectivity pattern differs between Action Recognition and Long Term Anticipation?

- Concept: Multi-Task Learning with shared backbone
  - Why needed here: Multiple egocentric vision tasks are related and can benefit from shared representation learning while maintaining task-specific outputs
  - Quick check question: What are the risks of negative transfer in MTL and how does EgoPack mitigate them?

- Concept: Prototype-based knowledge abstraction
  - Why needed here: Need to capture task-specific knowledge that can be reused across different tasks without requiring supervision for all tasks simultaneously
  - Quick check question: How are prototypes aggregated from verb/noun labels and why is this aggregation strategy effective?

## Architecture Onboarding

- Component map: Video features → Temporal Graph Backbone → Task-specific Heads → Prototypes → Cross-task Interaction → Novel Task Head
- Critical path: Feature extraction → Temporal graph construction → Shared GNN processing → Task-specific projection → Prototype matching and message passing
- Design tradeoffs: Shared backbone vs task-specific models (efficiency vs specialization), prototype aggregation strategy (coverage vs specificity), k-NN parameter (performance vs computational cost)
- Failure signatures: Poor performance on novel tasks (inadequate prototype matching), task competition (imbalanced losses), temporal modeling issues (incorrect graph connectivity)
- First 3 experiments:
  1. Verify temporal graph construction with simple node classification task
  2. Test prototype generation quality by visualizing nearest neighbors
  3. Validate cross-task interaction by comparing with and without message passing

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does EgoPack's performance change when using features extracted from backbones pretrained on Ego4D instead of Omnivore features?
- Basis in paper: [inferred] The paper mentions that EgoPack uses Omnivore features pre-trained on Kinetics-400, while other methods pretrain their backbones on Ego4D data, leading to better performance on certain tasks.
- Why unresolved: The paper focuses on demonstrating EgoPack's effectiveness using Omnivore features, but does not explore the impact of using Ego4D-pretrained features.
- What evidence would resolve it: An experiment comparing EgoPack's performance using Omnivore features versus Ego4D-pretrained features for the different tasks would provide insights into the importance of feature extraction for the method's effectiveness.

### Open Question 2
- Question: How does the number of prototypes in each task-specific set affect EgoPack's performance?
- Basis in paper: [explicit] The paper mentions that prototypes are created by aggregating features from AR samples based on verb and noun labels, and that the number of unique (verb, noun) pairs determines the number of prototypes.
- Why unresolved: While the paper provides an analysis of the impact of k-NN parameter on performance, it does not explore the effect of varying the number of prototypes in each task-specific set.
- What evidence would resolve it: An experiment varying the number of prototypes in each task-specific set and evaluating EgoPack's performance on the different tasks would shed light on the optimal number of prototypes for effective knowledge abstraction.

### Open Question 3
- Question: Can EgoPack be extended to tasks beyond egocentric video understanding?
- Basis in paper: [explicit] The paper mentions that EgoPack is designed for egocentric video understanding tasks and demonstrates its effectiveness on Ego4D benchmarks.
- Why unresolved: The paper does not explore the potential application of EgoPack to other domains or task types.
- What evidence would resolve it: Applying EgoPack to other task domains, such as general video understanding or other multimodal tasks, and evaluating its performance would determine its generalizability and potential for broader application.

## Limitations
- Evaluation limited to Ego4D benchmarks may not generalize to other egocentric video domains
- Assumes semantic relationships between tasks are strong enough for effective prototype transfer
- Modest performance gains (0.4% on LTA) compared to added complexity

## Confidence

**High confidence** in the technical implementation details and MTL baseline comparisons. The shared temporal graph architecture and prototype generation process are well-specified and reproducible.

**Medium confidence** in the claimed benefits of cross-task interaction through prototypes. While improvements are demonstrated, the mechanism by which prototypes enable transfer could be more thoroughly validated through ablation studies.

**Low confidence** in generalizability beyond Ego4D benchmarks. The evaluation is limited to four specific tasks on one dataset, making it difficult to assess performance on different egocentric video domains or task combinations.

## Next Checks

1. **Ablation on k-NN parameter**: Systematically vary k in the cross-task interaction stage to quantify sensitivity and identify optimal values for different task pairs.

2. **Temporal graph ablation**: Test different temporal graph connectivity patterns (varying τ) and compare performance to understand the impact of temporal modeling choices on task performance.

3. **Generalization test**: Apply EgoPack to a non-Ego4D egocentric video dataset (e.g., EPIC-KITCHENS) with different task combinations to validate cross-dataset transfer capability.