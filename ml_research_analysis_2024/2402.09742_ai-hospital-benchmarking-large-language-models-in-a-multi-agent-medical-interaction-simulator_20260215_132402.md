---
ver: rpa2
title: 'AI Hospital: Benchmarking Large Language Models in a Multi-agent Medical Interaction
  Simulator'
arxiv_id: '2402.09742'
source_url: https://arxiv.org/abs/2402.09742
tags:
- medical
- doctors
- intern
- examination
- diagnostic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AI Hospital, a multi-agent framework for
  simulating doctor-patient interactions to benchmark large language models (LLMs)
  in clinical diagnosis. The framework includes agents for doctors, patients, examiners,
  and a medical director, using high-quality medical records to guide interactions.
---

# AI Hospital: Benchmarking Large Language Models in a Multi-agent Medical Interaction Simulator

## Quick Facts
- arXiv ID: 2402.09742
- Source URL: https://arxiv.org/abs/2402.09742
- Reference count: 29
- Primary result: AI Hospital framework simulates doctor-patient interactions for LLM clinical diagnosis benchmarking; reveals significant gaps in multi-turn interactions versus one-step approaches.

## Executive Summary
This paper introduces AI Hospital, a multi-agent framework for benchmarking large language models (LLMs) in clinical diagnosis through simulated doctor-patient interactions. The framework employs high-quality Chinese medical records to ground agent behavior and introduces a Multi-View Medical Evaluation (MVME) benchmark assessing symptom collection, examination recommendations, and diagnostic quality. A dispute resolution-based collaborative mechanism is proposed to improve diagnostic accuracy through iterative discussion. Experiments show that while collaboration improves accuracy, LLMs still struggle with multi-turn interactions, highlighting the need for further research in clinical diagnostic capabilities.

## Method Summary
AI Hospital simulates medical interactions using four agent roles (patient, examiner, medical director, intern doctor) guided by partitioned Chinese medical records. Agents are implemented as LLMs using carefully crafted prompts that reflect their medical role-specific knowledge. The framework evaluates diagnostic performance through both LLM-based scoring (GPT-4 reference) and link-based matching against ICD-10 terminology. A dispute resolution collaboration mechanism iteratively refines diagnoses by having the medical director synthesize intern doctors' reports and guide discussions toward key disagreements.

## Key Results
- Collaboration improves diagnostic accuracy but significant gaps remain in multi-turn interactions compared to one-step approaches
- Interactive diagnosis is more challenging for LLMs than single-step generation of complete diagnostic reports
- Dispute resolution collaboration mechanism effectively reduces diagnostic variance through iterative consensus building

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using high-quality medical records as input to simulate patient, examiner, and medical director agents grounds the interaction in real clinical knowledge, enabling more realistic multi-turn consultations.
- Mechanism: Medical records are parsed into three role-specific knowledge partitions (Basic Information → Patient, Professional Medical Examination → Examiner, Diagnosis and Treatment → Medical Director). These partitions populate prompts that guide LLM agent behavior, ensuring that responses are anchored in actual clinical data.
- Core assumption: The structure and content of medical records accurately reflect real-world doctor-patient interactions and can be faithfully mapped to agent roles without loss of critical context.
- Evidence anchors:
  - [abstract] "To address the issue of medical roles lacking medical knowledge, we collect high-quality Chinese medical records, utilizing professional medical data as inputs for large language models to effectively simulate the agents."
  - [section] "We categorize the information of medical record into three types... These categories are correspondingly allocated to the patient, examiner, and medical director, then employed in prompts of LLMs to facilitate the medical knowledge of distinct roles."
  - [corpus] Weak evidence: CP-Env and related works also use structured clinical data but focus on pathway modeling rather than role-based agent prompting.
- Break condition: If medical records contain incomplete or inconsistent data, the prompts will misguide agent behavior, leading to unrealistic or erroneous interactions.

### Mechanism 2
- Claim: The dispute resolution-focused collaborative mechanism improves diagnostic accuracy by iteratively aligning intern doctors' assessments around clinically significant disagreements.
- Mechanism: After each discussion round, the medical director synthesizes intern doctors' diagnostic reports, identifies key points of contention, and guides the next round of discussion toward those issues. This iterative refinement reduces diagnostic variance and accelerates consensus.
- Core assumption: Highlighting specific disagreements is more effective than general discussion for reaching diagnostic consensus in multi-agent settings.
- Evidence anchors:
  - [abstract] "Additionally, a dispute resolution collaborative mechanism is proposed to enhance diagnostic accuracy through iterative discussions."
  - [section] "To enhance LLMs' diagnostic accuracy, drawing on previous research... we delve into the collaborative mechanism... our framework introduces a dispute resolution strategy that effectively engages the medical director to guide discussions, clarify issues, and steer the interns towards a more structured and efficient collaboration..."
  - [corpus] Weak evidence: While DoctorAgent-RL and PatientSim use multi-agent dialogue, none explicitly formalize dispute resolution as a guiding principle.
- Break condition: If the medical director fails to accurately identify or prioritize disagreements, the mechanism may waste cycles on irrelevant issues or stall without convergence.

### Mechanism 3
- Claim: Link-based evaluation against ICD-10 terminology provides a more objective and standardized assessment of diagnostic accuracy than purely LLM-based evaluation.
- Mechanism: Diagnostic results are matched to ICD-10 disease entities via fuzzy string matching, and precision, recall, and F1 scores are computed to measure alignment with gold-standard diagnoses.
- Core assumption: ICD-10 terminology is sufficiently granular and consistent to serve as a universal reference for evaluating diverse diagnostic outputs from LLMs.
- Evidence anchors:
  - [section] "To enhance the scientific rigor and validity of our evaluations... we employ the International Classification of Diseases (ICD-10) as the authoritative source, and link standardized disease terminologies with natural language based diagnostic results."
  - [section] "Subsequently, we implement a fuzzy matching process with a predefined threshold of 0.5 to link these disease entities with ICD-10 terminology, building a normalized disease sets."
  - [corpus] Weak evidence: None of the corpus papers explicitly use ICD-10 linkage for automated evaluation; most rely on LLM or human scoring.
- Break condition: If diagnostic outputs use highly non-standard terminology or complex multi-disease descriptions, the fuzzy matching may fail to capture the correct entities, undermining evaluation reliability.

## Foundational Learning

- Concept: Role-based prompt engineering
  - Why needed here: Different medical roles require different knowledge and conversational behaviors; prompts must reflect those distinctions.
  - Quick check question: If you assign the same prompt to both patient and examiner, what type of unrealistic behavior would you expect?

- Concept: Iterative consensus building in multi-agent systems
  - Why needed here: Clinical diagnosis benefits from collaborative reasoning; the mechanism must systematically reduce disagreement.
  - Quick check question: What happens if the medical director's dispute summary is too vague—will agents converge faster or slower?

- Concept: Evaluation with external knowledge bases
  - Why needed here: LLM self-evaluation can be biased; using standardized medical ontologies (e.g., ICD-10) provides an objective reference.
  - Quick check question: If you only use LLM-based evaluation, what kind of bias might creep into your assessment?

## Architecture Onboarding

- Component map:
  - Data layer: Chinese medical records → partitioned by role
  - Agent layer: Patient, Examiner, Medical Director, Intern Doctor agents
  - Interaction engine: Multi-turn dialogue manager coordinating turns and enforcing role-specific behavior
  - Evaluation layer: LLM-based scoring + ICD-10 link-based scoring
  - Collaboration module: Dispute resolution loop with medical director synthesis

- Critical path: Record ingestion → Role-based prompt generation → Multi-turn simulation → Diagnostic report → Evaluation → Optional collaboration loop

- Design tradeoffs:
  - Larger medical record dataset improves realism but increases parsing complexity and latency.
  - More discussion rounds can improve consensus but risk diminishing returns and higher computational cost.
  - ICD-10 matching improves objectivity but may misalign with nuanced or rare diagnoses.

- Failure signatures:
  - Agents producing inconsistent or contradictory statements across turns → prompt or parsing errors.
  - No convergence in collaboration → dispute identification or synthesis failures.
  - Evaluation scores mismatch between LLM and ICD-10 methods → coverage or terminology gaps.

- First 3 experiments:
  1. Single-agent interaction: Run one intern doctor against a static patient/examiner pair and verify symptom collection accuracy.
  2. Two-agent collaboration: Enable two intern doctors with dispute resolution disabled; compare diagnostic consistency to single-agent baseline.
  3. Full collaboration: Enable three intern doctors with dispute resolution; measure reduction in discussion rounds and improvement in diagnostic F1 over single-agent.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do smaller-scale and open-source models compare to OpenAI's APIs in terms of environmental impact when used for clinical diagnosis in the AI Hospital framework?
- Basis in paper: [explicit] The paper mentions that widespread use of OpenAI's APIs for LLMs could escalate environmental burdens and suggests that adopting smaller-scale and open-source models could mitigate this impact.
- Why unresolved: The paper acknowledges this as a potential risk but does not provide any comparative analysis or evidence on the environmental impact of different model types.
- What evidence would resolve it: Comparative studies measuring the carbon footprint and energy consumption of OpenAI's APIs versus smaller-scale and open-source models in the context of the AI Hospital framework.

### Open Question 2
- Question: What are the specific mechanisms by which the dispute resolution process in the collaborative mechanism improves diagnostic accuracy beyond the benefits of having more agents involved?
- Basis in paper: [explicit] The paper introduces a dispute resolution strategy to enhance collaborative diagnosis, but it does not detail the specific mechanisms by which this strategy improves diagnostic accuracy.
- Why unresolved: While the paper demonstrates that the collaborative mechanism with dispute resolution improves diagnostic accuracy, it does not explain the underlying reasons for this improvement.
- What evidence would resolve it: Detailed analysis of the decision-making process in the collaborative mechanism, comparing cases with and without dispute resolution to identify specific improvements in diagnostic reasoning and consensus-building.

### Open Question 3
- Question: How does the AI Hospital framework handle cases where there is no clear gold standard diagnosis, and how does this affect the evaluation of LLMs' diagnostic capabilities?
- Basis in paper: [explicit] The paper mentions using medical records as the gold standard for evaluation, but it does not address situations where a clear gold standard diagnosis may not exist.
- Why unresolved: The paper does not discuss the limitations of using medical records as the sole benchmark for evaluating diagnostic accuracy, particularly in complex or ambiguous cases.
- What evidence would resolve it: Case studies and analysis of the AI Hospital framework's performance in scenarios with ambiguous or multiple possible diagnoses, along with expert evaluations of the framework's diagnostic reasoning in these cases.

## Limitations
- Chinese medical records dataset not publicly available, limiting reproducibility
- Dispute resolution mechanism implementation details not fully specified
- Evaluation relies on ICD-10 matching which may miss nuanced or rare diagnoses

## Confidence
- Role-based prompt engineering with medical records: Medium
- Dispute resolution collaboration mechanism: Low
- ICD-10 link-based evaluation: Medium

## Next Checks
1. Reconstruct agent prompts using the described role partitions and run a single-turn simulation to verify symptom extraction and examination request accuracy.
2. Implement a simplified dispute resolution loop with two intern doctors and measure convergence speed and diagnostic consistency versus single-agent baseline.
3. Apply the ICD-10 matching pipeline to a small set of diagnostic outputs and assess false positive/negative rates against a manually annotated gold standard.