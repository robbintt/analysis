---
ver: rpa2
title: Explaining the Impact of Training on Vision Models via Activation Clustering
arxiv_id: '2411.19700'
source_url: https://arxiv.org/abs/2411.19700
tags:
- nave
- explanations
- training
- vision
- concepts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Neuro-Activated Vision Explanations (NAVE),
  a method to visualize and extract internal representations from vision model encoders
  without fine-tuning. NAVE clusters feature activations across layers to produce
  interpretable segmentations aligned with image semantics.
---

# Explaining the Impact of Training on Vision Models via Activation Clustering

## Quick Facts
- arXiv ID: 2411.19700
- Source URL: https://arxiv.org/abs/2411.19700
- Authors: AhcÃ¨ne Boubekki; Samuel G. Fadel; Sebastian Mair
- Reference count: 40
- One-line primary result: NAVE achieves AP@50% scores of 62.7-68.7% on VOC and COCO datasets with ViT-small/16 models

## Executive Summary
This paper introduces Neuro-Activated Vision Explanations (NAVE), a method to visualize and extract internal representations from vision model encoders without fine-tuning. NAVE clusters feature activations across layers to produce interpretable segmentations aligned with image semantics. The method is evaluated using object localization as a proxy task, achieving AP@50% scores of 62.7-68.7% on VOC and COCO datasets with ViT-small/16 models, comparable to state-of-the-art baselines. The study shows that training dataset choice, training scheme, and model architecture significantly impact encoder representation quality, with ImageNet1K training and ViT-small outperforming larger models.

## Method Summary
NAVE extracts feature activations from pre-trained vision models at multiple encoder layers, upsamples them to match input resolution, concatenates across layers, and applies k-means clustering to produce semantic segmentations. The method requires no fine-tuning and can be applied to any pre-trained encoder. For evaluation, NAVE generates explanation maps that are converted to bounding boxes for object localization using an inner-box calculation, with performance measured via Average Precision at 50% IoU (AP@50%) on VOC07, VOC12, and COCO20k datasets.

## Key Results
- NAVE achieves AP@50% scores of 62.7-68.7% on VOC and COCO datasets using ViT-small/16 models
- Training on ImageNet1K significantly outperforms STL-10 and Chest-X-Ray for object localization tasks
- Surprisingly, ViT-small outperforms ViT-base despite being smaller, suggesting better inductive biases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NAVE can produce semantically meaningful segmentations by clustering feature activations across multiple layers without fine-tuning.
- Mechanism: The encoder maps similar images close in embedding space, so feature activations from similar regions across layers should reflect shared semantics. K-means clustering groups these activations into coherent spatial regions (clusters), each representing a concept aligned with image semantics.
- Core assumption: The encoder's feature activations encode semantic information in a way that is spatially localized and clusterable across layers.
- Evidence anchors:
  - [abstract] "By clustering feature activations, NAVE provides insights into learned semantics without fine-tuning."
  - [section 3] "The rationale builds upon the fact that similar images are projected close to each other in the embedding space... it should thus be possible to identify them in the feature activations along with an approximate spatial localization."
  - [corpus] Weak: Related papers focus on interpretability via gradients or activation maximization but not layer-wise activation clustering for unsupervised segmentation.
- Break condition: If the encoder's activations are not spatially coherent or not semantically discriminative, the clustering will produce noisy or meaningless segments.

### Mechanism 2
- Claim: Training dataset choice significantly affects the semantic concepts NAVE can extract.
- Mechanism: The encoder learns to recognize concepts present in the training data. NAVE reveals these learned concepts through clustering; if a concept (e.g., "person") is absent from training, NAVE cannot extract it.
- Core assumption: The encoder's feature space is shaped by the training data distribution, and NAVE reflects that learned representation.
- Evidence anchors:
  - [section 4.4] "The person does not appear in images produced by a randomly initialized ResNet18. For a ResNet18 trained on STL-10, the pink silhouette is visible only for K = 10... When trained on ImageNet1K, a human pink shape stands for K = 5 and 10."
  - [abstract] "The study shows that training dataset choice... significantly impact encoder representation quality."
  - [corpus] Weak: Related work focuses on concept attribution but not on the effect of training data absence on concept extraction.
- Break condition: If the encoder uses very general or dataset-agnostic features, or if the clustering is not sensitive to dataset-specific concepts, this mechanism fails.

### Mechanism 3
- Claim: Model architecture choice (ViT-small vs. ViT-base) affects NAVE's ability to extract coherent concepts, with smaller models sometimes outperforming larger ones.
- Mechanism: Larger models may overfit or subdivide objects into too many small concepts, making them harder for NAVE to cluster meaningfully. Smaller models with better inductive biases produce more generalizable concepts that cluster cleanly.
- Core assumption: Model capacity and inductive biases influence how concepts are encoded in activations and thus how well they can be recovered by clustering.
- Evidence anchors:
  - [section 4.4] "Surprisingly, ViTB performs poorly despite being the largest model... the ViTS significantly outperforms a ResNet50... The better inductive biases afforded by the architectural choices of ViTS allow it to learn more generalizable concepts."
  - [abstract] "the study shows that... model architecture significantly impact encoder representation quality, with... ViT-small outperforming larger models."
  - [corpus] Weak: No direct corpus evidence; this is a novel empirical finding.
- Break condition: If the clustering algorithm can handle high-dimensional or subdivided concepts, or if the larger model is regularized to produce coarser concepts, the performance gap may disappear.

## Foundational Learning

- Concept: K-means clustering
  - Why needed here: NAVE uses k-means to group feature activations into semantic clusters without supervision.
  - Quick check question: What does k-means optimize for, and why is it suitable for grouping activations?
- Concept: Feature activation maps
  - Why needed here: NAVE operates on intermediate layer outputs (activations) to extract spatial concepts.
  - Quick check question: How do activation maps relate to the spatial structure of the input image?
- Concept: Intersection over Union (IoU)
  - Why needed here: Object localization evaluation uses IoU to measure overlap between NAVE segments and ground-truth boxes.
  - Quick check question: What IoU threshold is commonly used to count a prediction as correct in weakly supervised localization?

## Architecture Onboarding

- Component map: Input image -> Encoder (L layers) -> Feature activations (per layer) -> Upsample to (h,w) -> Reshape & normalize -> Concatenate across layers -> K-means clustering -> Segmentation mask
- Critical path: Feature extraction -> Layer-wise upsampling -> Concatenation -> Clustering -> Segmentation
- Design tradeoffs:
  - Resolution (h,w): Higher resolution gives finer details but more computation and noise.
  - Number of clusters K: Too few clusters oversimplify; too many may split semantics unnecessarily.
  - Clustering algorithm: k-means is fast but stochastic; hierarchical is deterministic but expensive.
- Failure signatures:
  - Random-looking segments -> Poor feature coherence or wrong K.
  - Over-segmentation of objects -> Too many clusters or too many layers used.
  - Background-dominated clusters -> Architecture or training scheme not capturing foreground semantics.
- First 3 experiments:
  1. Run NAVE with K=3,5,7 on a trained ViTS/16 and visualize results to see how cluster count affects segmentation quality.
  2. Compare NAVE output using activations from only the last layer vs. multiple layers to see effect on detail.
  3. Apply NAVE to an untrained model and a model trained on a dataset missing certain classes to confirm training data dependency.

## Open Questions the Paper Calls Out
None explicitly stated in the provided material.

## Limitations
- Architecture generalization: Findings based primarily on ViT variants may not extend to other vision architectures
- Proxy task evaluation: Object localization may not fully capture semantic richness of extracted concepts
- Hyperparameter sensitivity: Clustering-based method's performance depends heavily on K and layer selection

## Confidence

- **High confidence**: NAVE can extract meaningful segmentations from pre-trained encoders without fine-tuning, as demonstrated by AP@50% scores of 62.7-68.7% on standard benchmarks.
- **Medium confidence**: Training dataset choice significantly affects extracted concepts, based on controlled experiments with STL-10 vs ImageNet1K training.
- **Medium confidence**: Model architecture impacts representation quality, particularly the surprising finding that ViT-small outperforms ViT-base, though this needs validation across more architectures.

## Next Checks

1. Test NAVE across diverse architectures including CNNs, Swin transformers, and MLP-Mixers to verify architecture-general findings about representation quality.
2. Evaluate NAVE on semantic segmentation benchmarks (rather than just object localization) to better assess the quality of extracted semantic concepts.
3. Conduct a systematic hyperparameter study varying K and layer selection across multiple datasets to determine optimal configurations for different scenarios.