---
ver: rpa2
title: An Evaluation of Estimative Uncertainty in Large Language Models
arxiv_id: '2405.15185'
source_url: https://arxiv.org/abs/2405.15185
tags:
- weps
- llms
- probability
- gpt-4
- uncertainty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates how well large language models (LLMs) like
  GPT-4 and ERNIE-4 understand and express uncertainty using words of estimative probability
  (WEPs) compared to humans. The researchers compared LLM probability estimates for
  17 WEPs across different contexts, including gender and language (English vs.
---

# An Evaluation of Estimative Uncertainty in Large Language Models

## Quick Facts
- arXiv ID: 2405.15185
- Source URL: https://arxiv.org/abs/2405.15185
- Reference count: 40
- LLMs like GPT-4 diverge from human estimates for most WEPs except extreme certainty expressions

## Executive Summary
This study evaluates how well large language models (LLMs) understand and express uncertainty using words of estimative probability (WEPs) compared to humans. The researchers compared LLM probability estimates for 17 WEPs across different contexts, including gender and language (English vs. Chinese). Results showed that GPT-3.5 and GPT-4 diverged from human estimates for 11 and 12 out of 17 WEPs, respectively. While alignment was strong for WEPs expressing extreme certainty, significant gaps remained for most others. LLMs showed minimal sensitivity to gender in prompts and only minor differences between English and Chinese prompts. Additionally, GPT-4 performed significantly better than random in mapping statistical to estimative uncertainty, though consistency remained below 100%.

## Method Summary
The study compared human and LLM probability estimates for 17 WEPs using prompts in English and Chinese, with both gender-specific and neutral contexts. Researchers used KL divergence and Mann-Whitney U tests to compare distributions between humans and GPT-3.5, GPT-4, Llama-2, and ERNIE-4.0. They also evaluated GPT-4's ability to map statistical uncertainty to WEPs using four consistency metrics, testing both standard and Chain-of-Thought prompting approaches.

## Key Results
- GPT-3.5 and GPT-4 diverged from human estimates for 11 and 12 out of 17 WEPs, respectively
- LLMs showed strong alignment only for WEPs expressing extreme certainty (e.g., "almost certain")
- GPT-4 outperformed random chance in mapping statistical to estimative uncertainty, but consistency remained imperfect
- Chain-of-Thought prompting failed to improve performance on statistical-to-verbal mapping tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit humanlike calibration for WEPs at the extremes (high certainty) but diverge in intermediate probability ranges.
- Mechanism: LLMs trained on large corpora capture statistical patterns of WEPs usage, aligning with human interpretations where language use is unambiguous (e.g., "almost certain" â‰ˆ 95-100%). In ambiguous ranges (e.g., "likely," "probably"), human variability and cultural context lead to wider probability distributions that LLMs do not fully mirror.
- Core assumption: Training data includes sufficient examples of WEPs in varied contexts, allowing models to infer distributional semantics.
- Evidence anchors:
  - [abstract] "alignment was strong for WEPs expressing extreme certainty, significant gaps remained for most others"
  - [section] "humans and GPT models have statistically indistinguishable distributions for WEPs with high positive certainty, such as 'almost certain'"
- Break condition: If training data lacks sufficient diversity in context or if WEPs are used inconsistently, model calibration fails for those terms.

### Mechanism 2
- Claim: GPT-4 can map statistical to estimative uncertainty but consistency remains imperfect.
- Mechanism: GPT-4 leverages its statistical reasoning to map numeric intervals to WEPs, outperforming random chance across metrics, yet lacks full calibration due to limited training on explicit statistical-to-verbal mappings.
- Core assumption: LLMs possess latent reasoning capabilities that can be activated via prompting, even without explicit statistical training.
- Evidence anchors:
  - [abstract] "GPT-4 performed significantly better than random in mapping statistical to estimative uncertainty, though consistency remained below 100%"
  - [section] "GPT-4 does perform better under the 3 choices condition" when evaluated using empirical consistency
- Break condition: If prompts do not clearly frame the statistical task or if the WEP choice set is too narrow, mapping accuracy degrades.

### Mechanism 3
- Claim: Chain-of-Thought prompting does not reliably improve WEP mapping over standard prompting.
- Mechanism: CoT prompting fails to add value for statistical uncertainty tasks because the reasoning steps required are domain-specific and not generalizable from typical reasoning tasks (e.g., math word problems).
- Core assumption: CoT effectiveness is task-dependent; it works for procedural reasoning but not for aligning statistical and linguistic uncertainty.
- Evidence anchors:
  - [section] "Chain-of-Thought (CoT) prompting... failed to realize the improvements in performance that have been observed for more traditional natural language understanding problems"
  - [section] "the little, or even negative, improvement yielded by Chain-of-Thought prompting suggests the inherent difference between this 'task' and other natural language processing tasks"
- Break condition: If the statistical task is reformulated to explicitly require stepwise probability reasoning, CoT might gain utility.

## Foundational Learning

- Concept: Words of Estimative Probability (WEPs)
  - Why needed here: Understanding WEPs is essential to interpret how LLMs express uncertainty in natural language.
  - Quick check question: What is the difference between "probable" and "likely" in terms of implied probability?

- Concept: KL divergence and Mann-Whitney U test
  - Why needed here: These statistical tests are used to compare probability distributions between humans and LLMs.
  - Quick check question: When would you use KL divergence instead of Mann-Whitney U test to compare two distributions?

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: CoT is tested as a method to improve LLM reasoning, but its limitations for statistical-to-verbal mapping are revealed.
  - Quick check question: Why might CoT prompting improve performance on arithmetic tasks but not on WEP calibration?

## Architecture Onboarding

- Component map: Prompt generation -> LLM inference (GPT-3.5, GPT-4, ERNIE-4.0) -> Probability distribution extraction -> Statistical comparison (KL divergence, Mann-Whitney U) -> Consistency evaluation (pair-wise, monotonicity, empirical metrics)
- Critical path: Prompt construction -> LLM response -> Post-processing into probability bins -> Statistical test execution -> Metric calculation
- Design tradeoffs: Tradeoff between prompt richness (context vs. conciseness) and model interpretability; choice of WEP set size affects calibration granularity
- Failure signatures: Collapse of LLM output to a single point (low variability), divergence from human distributions for intermediate WEPs, poor CoT performance
- First 3 experiments:
  1. Compare human vs. GPT-4 probability distributions for "almost certain" and "likely" using KL divergence.
  2. Test GPT-4 with and without CoT on statistical-to-WEP mapping for 5-choice vs. 3-choice sets.
  3. Evaluate impact of gender-specific vs. neutral prompts on LLM WEP estimates.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do LLMs exhibit consistent bias patterns when interpreting WEPs across different cultural contexts beyond English and Chinese?
- Basis in paper: [explicit] The authors note divergence in WEP interpretation when comparing English and Chinese prompts, and suggest this as a promising future direction for cross-linguistic studies with more languages.
- Why unresolved: The study only tested English and Chinese, leaving uncertainty about how WEPs are interpreted in other languages and cultures.
- What evidence would resolve it: Systematic testing of multiple LLMs with WEP prompts across diverse languages (e.g., Arabic, Hindi, Spanish) with human benchmark data from native speakers of each language.

### Open Question 2
- Question: Can prompting strategies like Chain-of-Thought be adapted to improve LLM performance on mapping statistical to estimative uncertainty?
- Basis in paper: [explicit] The authors found that CoT prompting failed to improve performance on this task, despite success in other NLP problems, and suggest this gap warrants further investigation.
- Why unresolved: The study only tested one variant of CoT prompting, and the underlying reasons for its ineffectiveness remain unclear.
- What evidence would resolve it: Testing alternative prompting techniques (e.g., few-shot examples, structured reasoning templates) on the same statistical-to-estimative uncertainty task with comprehensive performance metrics.

### Open Question 3
- Question: Do LLMs adjust their WEP probability estimates differently when processing gender-specific versus gender-neutral prompts in contexts beyond the simple narratives tested?
- Basis in paper: [explicit] The authors found minimal divergence from human estimates for most WEPs when using gender-specific prompts, but noted this warrants investigation in more complex scenarios.
- Why unresolved: The study used simple, controlled narratives, which may not reflect the complexity of real-world gender-context interactions.
- What evidence would resolve it: Testing LLMs with gender-specific prompts embedded in complex, real-world scenarios (e.g., professional domains, cultural narratives) and comparing the resulting WEP interpretations to human benchmarks.

## Limitations

- The training data may lack sufficient diversity in WEP usage contexts, leading to calibration gaps for intermediate probability ranges
- The study only tested a specific set of 17 WEPs, which may not capture the full spectrum of uncertainty expressions in natural language
- Chain-of-Thought prompting's ineffectiveness for statistical uncertainty tasks may be task-dependent, requiring further investigation with different formulations

## Confidence

- **High Confidence**: LLMs exhibit strong calibration for WEPs expressing extreme certainty, with significant divergence observed for intermediate probability ranges
- **Medium Confidence**: GPT-4's ability to map statistical to estimative uncertainty, outperforming random chance but with imperfect consistency
- **Low Confidence**: The generalizability of Chain-of-Thought prompting's ineffectiveness for statistical uncertainty tasks

## Next Checks

1. Expand WEP Set and Context Diversity: Validate the findings by testing LLMs on a broader set of WEPs and contexts, including domain-specific or culturally nuanced expressions of uncertainty.

2. Test Alternative Prompting Strategies: Investigate whether alternative prompting strategies, such as step-by-step probability reasoning or explicit statistical framing, can improve LLM performance on statistical-to-verbal mapping tasks.

3. Cross-Linguistic and Cultural Validation: Evaluate the performance of LLMs on WEPs in additional languages and cultural contexts beyond English and Chinese.