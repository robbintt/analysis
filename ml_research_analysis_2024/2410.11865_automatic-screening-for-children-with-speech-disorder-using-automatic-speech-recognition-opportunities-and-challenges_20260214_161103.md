---
ver: rpa2
title: 'Automatic Screening for Children with Speech Disorder using Automatic Speech
  Recognition: Opportunities and Challenges'
arxiv_id: '2410.11865'
source_url: https://arxiv.org/abs/2410.11865
tags:
- speech
- children
- language
- automated
- some
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This position paper surveys techniques for automating speech-language
  assessments (SLAs) for children with speech disorders, focusing on adapting automatic
  speech recognition (ASR) models for children''s speech. The paper presents three
  main contributions: surveying existing ASR adaptation techniques for children''s
  speech, demonstrating automated SLA prototypes, and discussing practical concerns
  around accessibility and privacy.'
---

# Automatic Screening for Children with Speech Disorder using Automatic Speech Recognition: Opportunities and Challenges

## Quick Facts
- arXiv ID: 2410.11865
- Source URL: https://arxiv.org/abs/2410.11865
- Authors: Dancheng Liu; Jason Yang; Ishan Albrecht-Buehler; Helen Qin; Sophie Li; Yuting Hu; Amir Nassereldine; Jinjun Xiong
- Reference count: 13
- Primary result: This position paper surveys techniques for automating speech-language assessments (SLAs) for children with speech disorders, focusing on adapting automatic speech recognition (ASR) models for children's speech.

## Executive Summary
This position paper surveys techniques for automating speech-language assessments (SLAs) for children with speech disorders, focusing on adapting automatic speech recognition (ASR) models for children's speech. The paper presents three main contributions: surveying existing ASR adaptation techniques for children's speech, demonstrating automated SLA prototypes, and discussing practical concerns around accessibility and privacy. Key findings include: ASR models like Whisper show significant performance degradation on children's speech compared to adults (e.g., WER increasing from 3.3% to 16.34% on one children's dataset), though fine-tuning can improve performance at the cost of generalization; automated versions of SLA tests like Redmond Sentence Recall achieve 90.4% accuracy compared to human scoring; and privacy concerns necessitate edge deployment of ASR models, which presents challenges around model size, quantization, and fairness across different speaker groups.

## Method Summary
The paper surveys existing ASR adaptation techniques for children's speech, including fine-tuning, voice activity detection adaptation, and context-aware processing. It demonstrates automated SLA prototypes using fine-tuned ASR models like Whisper on children's speech datasets, achieving performance improvements through targeted adaptations. The research also explores edge deployment strategies for privacy-preserving ASR implementations, addressing challenges around model size, quantization, and fairness across different speaker groups.

## Key Results
- ASR models like Whisper show significant performance degradation on children's speech compared to adults (WER increasing from 3.3% to 16.34% on one children's dataset)
- Fine-tuning can improve ASR performance on children's speech but reduces generalization capabilities
- Automated versions of SLA tests like Redmond Sentence Recall achieve 90.4% accuracy compared to human scoring

## Why This Works (Mechanism)
ASR models are pretrained on adult speech corpora, which differ significantly from children's speech in terms of acoustic characteristics, pronunciation patterns, and linguistic development. Children's speech typically has higher pitch, greater variability in pronunciation, and different grammatical structures. By adapting ASR models through fine-tuning on children's speech data and implementing context-aware processing, these models can better handle the unique characteristics of children's speech. Edge deployment addresses privacy concerns while maintaining acceptable performance through model quantization and optimization techniques.

## Foundational Learning
- Children's speech characteristics: Children's speech differs from adult speech in pitch, pronunciation variability, and linguistic development, requiring specialized ASR adaptation techniques.
- Why needed: Standard ASR models trained on adult speech perform poorly on children's speech due to fundamental acoustic and linguistic differences.
- Quick check: Compare WER of baseline ASR on adult vs. children's speech datasets.

- Fine-tuning trade-offs: Fine-tuning improves children's speech recognition but reduces model generalization across different speaker groups and domains.
- Why needed: Balancing performance gains against loss of generalization is crucial for practical deployment.
- Quick check: Evaluate model performance across multiple datasets before and after fine-tuning.

- Edge deployment constraints: Privacy concerns necessitate running ASR models locally on devices rather than cloud servers, requiring model optimization.
- Why needed: Healthcare applications involving children's speech data require strict privacy protections.
- Quick check: Measure inference latency and memory usage of quantized models on target hardware.

## Architecture Onboarding

Component map: Data Collection -> Preprocessing -> ASR Model -> Fine-tuning -> Edge Deployment -> SLA Assessment

Critical path: The critical path for automated SLA systems involves collecting children's speech data, preprocessing it for ASR input, running it through a fine-tuned ASR model, and generating assessment scores. Edge deployment adds additional steps for model quantization and optimization.

Design tradeoffs: The main tradeoff is between ASR performance and privacy - cloud deployment offers better performance but raises privacy concerns, while edge deployment protects privacy but may sacrifice accuracy. Fine-tuning improves performance on target domains but reduces generalization. Model size and quantization affect both accuracy and inference speed on edge devices.

Failure signatures: Common failure modes include poor recognition of children's speech characteristics (high pitch, pronunciation variability), overfitting to specific speaker groups during fine-tuning, and degraded performance from aggressive model quantization. Privacy failures occur when data is transmitted to cloud servers or when models leak sensitive information.

First experiments:
1. Benchmark baseline ASR performance on adult vs. children's speech datasets to quantify degradation
2. Evaluate fine-tuning effectiveness by testing model performance across multiple children's speech datasets
3. Compare quantized edge models against full models to measure accuracy vs. privacy trade-offs

## Open Questions the Paper Calls Out
The paper acknowledges but does not fully resolve privacy and accessibility trade-offs in ASR deployment for children's speech applications. Key open questions include how to balance model performance with privacy requirements, how to ensure fairness across different demographic groups, and how to scale fine-tuning approaches across different languages and dialects.

## Limitations
- Real-world performance of automated SLA systems in diverse clinical settings remains unverified
- Long-term reliability of edge-deployed models across different usage patterns is unknown
- Scalability of fine-tuning approaches across different languages and dialects has not been demonstrated

## Confidence

High confidence claims:
- ASR performance degradation on children's speech compared to adults
- Privacy concerns necessitate edge deployment for children's speech applications
- Edge deployment presents challenges around model size, quantization, and fairness

Medium confidence claims:
- Fine-tuning effectiveness for children's speech recognition
- Automated SLA accuracy claims of 90.4% on Redmond Sentence Recall
- Generalization trade-offs from fine-tuning approaches

## Next Checks

1. Conduct independent field testing of automated SLA systems across multiple clinical sites to verify reported accuracy rates
2. Evaluate the performance of quantized edge models against full models to quantify accuracy trade-offs
3. Test model fairness across different demographic groups, including various ages, accents, and socioeconomic backgrounds