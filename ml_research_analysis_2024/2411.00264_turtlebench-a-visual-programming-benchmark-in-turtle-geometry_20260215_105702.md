---
ver: rpa2
title: 'TurtleBench: A Visual Programming Benchmark in Turtle Geometry'
arxiv_id: '2411.00264'
source_url: https://arxiv.org/abs/2411.00264
tags:
- code
- tasks
- turtle
- image
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TurtleBench, a benchmark to evaluate large
  multimodal models (LMMs) on visual programming tasks using turtle geometry. TurtleBench
  contains 260 tasks requiring LMMs to interpret geometric patterns from images and/or
  text and generate corresponding Python Turtle code.
---

# TurtleBench: A Visual Programming Benchmark in Turtle Geometry

## Quick Facts
- arXiv ID: 2411.00264
- Source URL: https://arxiv.org/abs/2411.00264
- Reference count: 40
- Primary result: LMMs show poor performance on visual programming tasks requiring code generation and editing from images/text using turtle geometry

## Executive Summary
This paper introduces TurtleBench, a benchmark designed to evaluate large multimodal models (LMMs) on visual programming tasks using turtle geometry. The benchmark contains 260 tasks requiring models to interpret geometric patterns from images and/or text and generate corresponding Python Turtle code. Tasks are divided into scratch (code generation from input) and tweak (modifying existing code) types. Evaluation on GPT-4o and Gemini 1.5 Flash reveals significant challenges, with models achieving only 19% accuracy on simple scratch tasks and less than 20% on tweak tasks, indicating substantial difficulties in integrating visual reasoning with code generation.

## Method Summary
TurtleBench provides a benchmark for evaluating LMMs on visual programming tasks using turtle geometry. The dataset contains 260 tasks with images of geometric patterns and/or textual descriptions, expecting Python Turtle code outputs. Models are evaluated using zero-shot and visual chain-of-thought prompting approaches. The evaluation pipeline runs generated code in a sandbox environment and compares the resulting shapes with ground truth using bitwise similarity (>95% = correct). The benchmark tests both code generation (scratch tasks) and code editing (tweak tasks) capabilities across different input modalities.

## Key Results
- GPT-4o achieved only 19% accuracy on simple scratch tasks and less than 20% on tweak tasks
- Visual chain-of-thought prompting improved performance over basic prompting across all tasks
- Models performed significantly worse with visual-only instructions compared to text-only instructions
- When tested with a custom library mimicking Python Turtle but using different command names, models showed substantial performance drops

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Turtle geometry provides a simplified, structured visual-programming interface that reduces cognitive load compared to general programming tasks.
- Mechanism: The turtle module's constrained API (forward, right, circle) and predictable coordinate system allow models to map visual shapes to discrete geometric operations without requiring complex syntax parsing or arbitrary code generation.
- Core assumption: Visual patterns in turtle geometry are algorithmic and repeatable, making them more amenable to pattern matching than arbitrary real-world code tasks.
- Evidence anchors:
  - [abstract]: "TurtleBench features tasks with patterned shapes that have underlying algorithmic logic."
  - [section 2]: "The intuitive nature and learnability of turtle geometry, along with its ability to generate patterns of diverse complexities, make it a compelling concept upon which to base a benchmark for LMMs."
- Break condition: If shapes involve irregular or non-algorithmic patterns, the visual-to-code mapping becomes ambiguous and harder for models to resolve.

### Mechanism 2
- Claim: Chain-of-Thought prompting improves visual reasoning by forcing structured decomposition of the task into observation and step-by-step reasoning.
- Mechanism: By explicitly asking models to describe relevant visual artifacts before generating code, CoT encourages intermediate reasoning steps that help bridge the gap between raw pixel input and procedural code generation.
- Core assumption: Models can decompose visual information into logical components if prompted to do so explicitly.
- Evidence anchors:
  - [abstract]: "We conduct an evaluation of leading LMMs on TurtleBench code generation and code editing tasks, utilizing zero-shot and visual chain-of-thought approach."
  - [section 4.1]: "A comparison between CoT and basic prompting within Table 1 illustrates that CoT prompting outperforms basic prompting on the same models."
- Break condition: If the visual input is too complex or ambiguous, even structured reasoning may not resolve the correct code mapping.

### Mechanism 3
- Claim: Task modality (image vs. text) significantly impacts model performance due to differences in how visual and linguistic information are processed.
- Mechanism: Textual descriptions provide explicit, unambiguous cues about shape attributes, while images require implicit pattern recognition and spatial reasoning, which current LMMs handle less effectively.
- Core assumption: LMMs have stronger text-processing capabilities than pure visual reasoning.
- Evidence anchors:
  - [section 4.5]: "Interestingly, for both GPT-4o and Gemini 1.5 Flash, the model performed worse when the task was presented only in the image, compared to the other modes."
  - [section 4.4]: "This outcome suggests a disparity in the models' ability to process visual versus textual instructions, revealing that their reasoning abilities may not align closely with human-like understanding."
- Break condition: If future models improve their visual reasoning modules, the modality gap may narrow.

## Foundational Learning

- Concept: Visual abstraction and pattern recognition
  - Why needed here: Models must infer geometric rules from visual input and translate them into procedural code.
  - Quick check question: Given a square image, what sequence of turtle commands would recreate it?

- Concept: Geometric reasoning with constraints
  - Why needed here: Tasks require understanding angles, symmetry, and shape relationships under turtle geometry constraints.
  - Quick check question: How would you modify turtle code to rotate a shape 90 degrees without changing the shape's structure?

- Concept: Code generation from high-level descriptions
  - Why needed here: Models must map visual or textual instructions into executable turtle code.
  - Quick check question: What is the turtle code for drawing a circle inscribed within a square?

## Architecture Onboarding

- Component map: Input parsing -> visual decomposition -> code generation -> sandbox execution -> shape comparison -> accuracy scoring
- Critical path: Input parsing → visual decomposition → code generation → sandbox execution → shape comparison → accuracy scoring
- Design tradeoffs: Simple turtle API vs. expressiveness; rigid shape comparison vs. flexible geometric tolerance; benchmark diversity vs. task clarity.
- Failure signatures: High runnable code but low shape similarity (poor visual reasoning); low runnable code (syntax or API errors); inconsistent performance across modalities (visual reasoning gap).
- First 3 experiments:
  1. Test basic prompt vs. v-CoT on a small subset of scratch tasks with image input only.
  2. Run same tasks with text input only to compare modality effects.
  3. Modify turtle API slightly (like Rabbit class) and measure generalization drop.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do LMMs struggle significantly more with visual-only tweak instructions compared to textual instructions, even when the visual instructions are directly showing the desired outcome?
- Basis in paper: [explicit] The paper reports a significant decline in accuracy when instructions were provided visually rather than textually in tweak code edit tasks, despite the visual instructions explicitly showing the desired outcome.
- Why unresolved: The paper notes this disparity but doesn't provide a definitive explanation for why models perform worse with visual instructions when the desired outcome is directly shown. This suggests a fundamental limitation in how models process and reason with visual information compared to text.
- What evidence would resolve it: Comparative experiments testing different visual representation formats, ablation studies on vision model components, or analysis of attention patterns during visual vs textual instruction processing could reveal the underlying cause.

### Open Question 2
- Question: Can fine-tuning techniques significantly improve LMM performance on TurtleBench tasks, and if so, which fine-tuning approaches are most effective?
- Basis in paper: [inferred] The authors acknowledge they did not experiment with fine-tuning techniques and suggest this as future work, noting that poor performance may be related to vision components rather than syntax unfamiliarity.
- Why unresolved: The paper only evaluates zero-shot performance of existing models without exploring whether targeted fine-tuning could bridge the performance gap. The effectiveness of different fine-tuning strategies remains unknown.
- What evidence would resolve it: Experiments comparing various fine-tuning approaches (supervised fine-tuning, instruction tuning, curriculum learning) on TurtleBench tasks, measuring performance improvements and identifying which aspects of visual reasoning benefit most from fine-tuning.

### Open Question 3
- Question: How do probabilistic program induction methods like LAPS compare to LMMs on TurtleBench, and what architectural differences account for performance gaps?
- Basis in paper: [explicit] The paper references related work showing probabilistic program induction methods like LAPS achieving 82% accuracy on similar turtle geometry tasks, compared to LMMs' much lower performance on TurtleBench.
- Why unresolved: While the paper mentions these alternative approaches, it doesn't directly compare their performance on TurtleBench or analyze what architectural or methodological differences enable their superior performance.
- What evidence would resolve it: Direct benchmarking of probabilistic program induction methods on TurtleBench, followed by systematic comparison of their architectures, training approaches, and reasoning mechanisms against LMMs to identify key differentiators.

## Limitations

- Benchmark's narrow domain specificity in turtle geometry may not generalize to broader visual programming tasks
- Reliance on bitwise similarity for shape comparison may be overly strict for graphics rendering with floating-point imprecision
- Only evaluated two commercial LMMs and one open model, limiting applicability to broader model landscape
- Synthetic nature of tasks may not capture full complexity and ambiguity of real-world visual programming scenarios

## Confidence

**High Confidence**: The empirical results showing poor performance on both scratch and tweak tasks are well-supported by the data. The methodology for task creation, evaluation pipeline, and comparison between basic prompting and visual chain-of-thought are clearly specified and reproducible.

**Medium Confidence**: The claim that turtle geometry provides an appropriate difficulty level for benchmarking LMMs is reasonable but depends on the assumption that visual-to-code mapping in this domain represents a fundamental challenge rather than just a syntax familiarity issue.

**Low Confidence**: The generalizability of these findings to other visual programming domains remains uncertain. The benchmark's synthetic nature and specific focus on turtle geometry may limit how well results translate to broader programming or reasoning tasks.

## Next Checks

1. **Human Performance Baseline**: Administer the TurtleBench tasks to human programmers with varying experience levels (novice, intermediate, expert) to establish a performance baseline and determine whether the LMM performance gap reflects fundamental visual reasoning limitations or simply unfamiliarity with the task format.

2. **Cross-Benchmark Validation**: Test the same LMMs on complementary visual programming benchmarks (like ScratchEval or Tangram) to determine whether the observed limitations are specific to turtle geometry or represent more general challenges in visual-to-code translation across different programming paradigms.

3. **Syntax Generalization Test**: Create a systematic ablation study where the turtle API is modified incrementally (changing command names, parameter orders, or coordinate systems) to quantify exactly how much performance degradation stems from visual reasoning limitations versus syntactic/API familiarity, helping isolate the true source of the observed difficulties.