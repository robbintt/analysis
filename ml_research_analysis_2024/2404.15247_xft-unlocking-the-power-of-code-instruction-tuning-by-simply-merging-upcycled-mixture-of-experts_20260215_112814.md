---
ver: rpa2
title: 'XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging Upcycled
  Mixture-of-Experts'
arxiv_id: '2404.15247'
source_url: https://arxiv.org/abs/2404.15247
tags: []
core_contribution: 'This paper introduces XFT, a novel training scheme for code instruction
  tuning that unlocks the performance limits of instruction-tuned code LLMs. XFT consists
  of two key steps: upcycling and merging.'
---

# XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging Upcycled Mixture-of-Experts

## Quick Facts
- arXiv ID: 2404.15247
- Source URL: https://arxiv.org/abs/2404.15247
- Reference count: 23
- Primary result: XFT improves code instruction tuning by upcycling dense models to MoE and learning to merge them back to dense models

## Executive Summary
XFT introduces a novel training scheme for code instruction tuning that overcomes limitations of existing approaches. The method involves upcycling pre-trained dense language models into sparse Mixture-of-Experts (MoE) models with shared expert mechanisms and routing weight normalization, then learning to merge these back into dense models. This approach achieves up to 22.8% improvement over standard fine-tuning on HumanEval benchmarks while maintaining computational efficiency comparable to dense models.

## Method Summary
XFT consists of two key steps: upcycling and merging. First, a pre-trained dense model is converted to a sparse MoE model with a shared expert mechanism that ensures each expert receives sufficient training data. Routing weights are normalized to prevent scale mismatch between MoE and dense layers. After instruction tuning the upcycled MoE model, a learnable merging mechanism learns mixing coefficients to compile the MoE back to a dense model, preserving or enhancing MoE-level performance while reducing parameters.

## Key Results
- Achieves 22.8% improvement over standard fine-tuning on HumanEval
- Maintains dense-model compute requirements while delivering MoE-level performance
- Effective across multiple base models (1.3B, 3B, 1.1B parameters) and instruction datasets
- Provides consistent improvements across multiple programming languages

## Why This Works (Mechanism)

### Mechanism 1
Vanilla sparse upcycling fails for instruction tuning because experts are under-trained due to token routing imbalance. In traditional sparse upcycling, each expert processes only a subset of tokens, so each expert sees less training data than the original dense model. This under-training hurts performance during instruction tuning.

### Mechanism 2
Shared expert with normalized routing weights fixes scale mismatch and ensures balanced learning. XFT isolates one expert as shared (always active) and uses normalized routing weights for other experts. This prevents scale mismatch between MoE and dense layers while ensuring the shared expert learns general knowledge and others learn specific knowledge.

### Mechanism 3
Learnable model merging preserves or enhances MoE performance while reducing parameters. XFT learns mixing coefficients to average expert weights into a dense layer, allowing the final model to maintain MoE-level performance with dense-model compute.

## Foundational Learning

- **Mixture-of-Experts (MoE) architecture**: Understanding how MoE works is essential to grasp why XFT introduces shared experts and routing normalization. Quick check: In an MoE layer with 8 experts and top-6 activation, how many experts are active for each token?

- **Sparse upcycling**: XFT builds upon sparse upcycling, so understanding its limitations is crucial. Quick check: What is the main limitation of vanilla sparse upcycling for instruction tuning?

- **Model merging techniques**: XFT uses a learnable merging mechanism, which is a key differentiator from other approaches. Quick check: How does XFT's learnable merging differ from simple weight averaging?

## Architecture Onboarding

- **Component map**: Dense base model → Upcycling (MoE conversion with shared expert) → Instruction tuning → Learnable merging → Final dense model
- **Critical path**: 1) Convert dense model to MoE with shared expert 2) Fine-tune on instruction dataset for 4 epochs 3) Learn mixing coefficients for merging 4) Merge MoE back to dense model
- **Design tradeoffs**: Shared expert vs. fully distributed experts (shared expert ensures each token sees all experts but may create bottleneck); Learnable merging vs. fixed averaging (learnable allows optimization but adds complexity); Parameter efficiency vs. performance (XFT aims to maintain MoE performance with fewer parameters)
- **Failure signatures**: Scale mismatch between MoE and dense layers → performance degradation; Under-trained experts → poor performance; Improper mixing coefficients → loss of expert knowledge
- **First 3 experiments**: 1) Compare vanilla sparse upcycling vs. XFT's shared expert on HumanEval+ 2) Test effect of removing routing weight normalization 3) Compare learnable merging vs. initialized mixing coefficients

## Open Questions the Paper Calls Out

- **Language handling optimization**: How does the shared expert mechanism affect the model's ability to handle different programming languages, and can the model be further optimized to specialize in specific languages?

- **Scalability to larger models**: Can the learnable model merging mechanism be extended to handle larger models or more complex architectures, and what are the potential challenges and benefits of doing so?

- **Comparison with parameter-efficient fine-tuning**: How does XFT compare to other parameter-efficient fine-tuning techniques like LoRA or adapters, and what are the trade-offs between these approaches?

## Limitations

- Critical evidence gaps exist for proposed mechanisms, particularly token routing imbalance and scale mismatch hypotheses
- Lack of statistical significance reporting for empirical results
- Missing detailed ablation studies to isolate contribution of each component
- No direct comparison with parameter-efficient fine-tuning techniques

## Confidence

**High Confidence (3/3 claims supported):**
- XFT achieves improved performance on code instruction tuning benchmarks compared to standard fine-tuning
- The overall two-step approach (upcycling → merging) produces a working dense model
- XFT is applicable to multiple base models and instruction datasets

**Medium Confidence (1/3 claims supported):**
- The shared expert mechanism improves upon vanilla sparse upcycling for instruction tuning
- Routing weight normalization contributes to performance gains
- Learnable merging coefficients outperform static alternatives

**Low Confidence (0/3 claims supported):**
- Detailed mechanistic explanations for why vanilla sparse upcycling fails (token routing imbalance)
- Scale mismatch between MoE and dense layers as the primary failure mode
- The specific routing weight normalization strategy is optimal or necessary

## Next Checks

1. **Expert activation analysis**: Conduct a detailed study of expert activation patterns during instruction tuning for both vanilla sparse upcycling and XFT's shared expert approach. Quantify the distribution of tokens across experts and demonstrate whether under-training occurs in vanilla approaches.

2. **Routing normalization ablation**: Systematically test different routing normalization strategies (no normalization, softmax normalization, learned scaling factors) and their impact on performance. Include experiments with varying numbers of shared experts to isolate the normalization effect.

3. **Mixing coefficient analysis**: Analyze the learned mixing coefficients across different layers and experts. Compare against fixed averaging strategies and perform ablation studies where coefficients are initialized differently or trained with different objectives to understand what knowledge is being preserved.