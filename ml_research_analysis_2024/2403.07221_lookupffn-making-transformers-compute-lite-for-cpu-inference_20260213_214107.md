---
ver: rpa2
title: 'LookupFFN: Making Transformers Compute-lite for CPU inference'
arxiv_id: '2403.07221'
source_url: https://arxiv.org/abs/2403.07221
tags:
- hash
- lookupffn
- flop
- performance
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational intensity of GEMM-based
  Feed Forward Networks (FFNs) in Transformers, which poses a significant challenge
  for CPU inference. To make FFNs more compute-lite, the authors propose LookupFFN,
  an alternative formulation that recasts most essential operations as memory lookups.
---

# LookupFFN: Making Transformers Compute-lite for CPU inference

## Quick Facts
- **arXiv ID**: 2403.07221
- **Source URL**: https://arxiv.org/abs/2403.07221
- **Reference count**: 33
- **Primary result**: LookupFFN achieves 6× FLOP reduction on RoBERTa pretraining with near-identical accuracy

## Executive Summary
This paper introduces LookupFFN, a novel approach to make Feed Forward Networks in Transformers more compute-efficient for CPU inference. By recasting most operations as memory lookups inspired by Locality Sensitive Hashing (LSH), LookupFFN dramatically reduces the computational complexity while maintaining comparable performance to traditional GEMM-based FFNs. The method uses learnable hash tables and hash functions that can be directly optimized without the costly rehashing step, achieving significant FLOP reductions while preserving model accuracy.

## Method Summary
LookupFFN replaces traditional dense matrix multiplications in FFNs with a system of learnable hash tables and hash functions. The method partitions input vectors using hash functions and performs weighted lookups in hash tables to approximate the FFN computation. A fast Hadamard transform (BH4) is used for efficient projections, reducing computational complexity from O(hτd) to O(hτ log d). The softmax operation is approximated using a structured matrix that enables O(τ) computation instead of exponential complexity. The entire system is trained end-to-end using backpropagation, with hash tables and functions directly optimized rather than reconstructed.

## Key Results
- Achieves 6× (or more) reduction in FLOPs compared to vanilla FFN on RoBERTa pretraining
- Maintains almost identical accuracy with log perplexity comparable to baseline
- Demonstrates 2.51× faster inference on modern CPU hardware
- Successfully scales from small experiments to RoBERTa-base architecture

## Why This Works (Mechanism)

### Mechanism 1
The decoupled dependency between hash tables and hash functions enables direct learnable updates without rehashing. By parameterizing hash tables (Tk) and hash functions (fk) separately, updates to weights no longer require reconstructing hash tables, eliminating the costly rehashing step. This is possible because hash tables are treated as coarse representations that can be directly optimized.

### Mechanism 2
The use of a fast Hadamard transform (BH4) reduces the computational complexity of the projection step while maintaining representational power. BH4 replaces dense matrix multiplication with block diagonal matrices followed by Hadamard transforms, reducing complexity from O(hτd) to O(hτ log d) while the block structure preserves expressiveness.

### Mechanism 3
The specialized softmax approximation using structured matrix S enables efficient computation with minimal loss in accuracy. The structured matrix S allows the denominator of the softmax to be computed in O(τ) time, and the numerator can be efficiently sampled based on the ℓ0 difference to sign(zk), dramatically reducing computation from O(2^τ) to O(τ) or O(1) for the single-largest-term approximation.

## Foundational Learning

- **Concept**: Locality Sensitive Hashing (LSH)
  - Why needed here: Understanding how LSH partitions high-dimensional space to enable approximate nearest neighbor search is fundamental to grasping why LookupFFN works.
  - Quick check question: What property must a hash function have to be considered "locality sensitive" and why is this property important for approximating FFNs?

- **Concept**: Fast Hadamard Transform
  - Why needed here: The BH4 projection relies on Hadamard transforms for computational efficiency, and understanding its properties is essential for grasping the efficiency gains.
  - Quick check question: How does the complexity of a Hadamard transform compare to standard matrix multiplication, and why does this difference matter for large-scale models?

- **Concept**: Differentiable approximations of non-differentiable operations
  - Why needed here: The transition from arg max to softmax and the relaxation of the sampling process require understanding how non-differentiable operations can be approximated with differentiable ones.
  - Quick check question: What is the mathematical relationship between arg max and softmax, and how does this relationship enable gradient-based learning?

## Architecture Onboarding

- **Component map**: Input embedding (X) → Projection layer (R or BH4) → Hash function computation (g(zk) or sampling from N(zk)) → Hash tables (Tk) → lookup and weighted accumulation → Output aggregation

- **Critical path**: Input → Projection → Hash computation → Hash table lookup → Weighted accumulation → Output
  The projection and hash table lookups are the primary computational bottlenecks.

- **Design tradeoffs**:
  - Hash table size vs. accuracy: Larger tables (higher τ) improve approximation but increase memory requirements
  - Number of hash tables (h) vs. FLOP: More tables improve accuracy but increase computation
  - Block size in BH4 vs. approximation quality: Larger blocks improve approximation but reduce computational efficiency

- **Failure signatures**:
  - Degraded model performance despite correct implementation suggests hash table parameterization is insufficient
  - High LLC miss rates indicate working set exceeds cache capacity
  - Poor scaling with sequence length suggests hash computation is not properly parallelized

- **First 3 experiments**:
  1. Implement a single hash table with dense projection and verify it can approximate a simple FFN on a small dataset
  2. Replace dense projection with BH4 and measure the impact on both accuracy and FLOP count
  3. Scale to multiple hash tables and measure the trade-off between FLOP reduction and performance degradation on RoBERTa-small

## Open Questions the Paper Calls Out
- How does LookupFFN's performance scale to even larger models beyond RoBERTa-base?
- What is the impact of reducing precision (e.g., from float32 to float16) on LookupFFN's performance and accuracy?
- How does LookupFFN compare to other efficient transformer architectures, such as Longformer or Big Bird, in terms of both performance and efficiency?

## Limitations
- Limited evaluation to a single task (RoBERTa pretraining) and model architecture
- Exact implementation details of fast Hadamard transform and specific hyperparameters are not fully specified
- Long-term stability of hash table parameterization during extended training is not adequately explored

## Confidence
- **High Confidence**: The fundamental mechanism of using learnable hash tables to replace dense matrix multiplications is sound and well-supported by the theoretical framework and empirical results.
- **Medium Confidence**: The efficiency claims for the BH4 projection and structured softmax approximation are plausible but the approximation quality across diverse workloads is not thoroughly validated.
- **Low Confidence**: The long-term stability of the hash table parameterization during extended training and the method's robustness to different data distributions are not adequately explored.

## Next Checks
1. **Ablation study on hash table parameters**: Systematically vary the number of hash tables (h), hash table size (τ), and block size in BH4 across a range of values on RoBERTa-small to quantify the accuracy-efficiency trade-off and identify optimal configurations for different memory constraints.

2. **Cross-model generalization test**: Implement LookupFFN in both BERT and GPT-style architectures, then evaluate performance on GLUE benchmarks and causal language modeling tasks respectively, comparing both accuracy retention and FLOP reduction against baseline FFN implementations.

3. **Hardware sensitivity analysis**: Profile LookupFFN on CPUs with different cache hierarchies (varying LLC sizes) and SIMD capabilities to measure how cache misses, memory bandwidth utilization, and instruction-level parallelism affect the theoretical vs. actual speedup, particularly focusing on the impact of working set size exceeding cache capacity.