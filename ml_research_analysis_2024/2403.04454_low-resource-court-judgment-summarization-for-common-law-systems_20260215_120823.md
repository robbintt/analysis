---
ver: rpa2
title: Low-Resource Court Judgment Summarization for Common Law Systems
arxiv_id: '2403.04454'
source_url: https://arxiv.org/abs/2403.04454
tags:
- summarization
- court
- data
- summaries
- legal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of generating high-quality summaries
  of court judgment documents in common law systems under low data and computing resource
  conditions. The authors propose a solution that includes building a new dataset
  (CLSum) covering multi-jurisdictional judgments, employing large language models
  (LLMs) for data augmentation and summary generation, and designing a legal knowledge
  enhanced evaluation metric (LTScore).
---

# Low-Resource Court Judgment Summarization for Common Law Systems

## Quick Facts
- arXiv ID: 2403.04454
- Source URL: https://arxiv.org/abs/2403.04454
- Reference count: 40
- Primary result: LLM-based methods demonstrate strong few-shot and zero-shot performance on the CLSum dataset for court judgment summarization

## Executive Summary
This paper addresses the challenge of generating high-quality summaries of court judgment documents in common law systems under low data and computing resource conditions. The authors propose a comprehensive solution including building a new multi-jurisdictional dataset (CLSum), employing large language models for data augmentation and summary generation, and designing a legal knowledge enhanced evaluation metric (LTScore). The experimental results demonstrate that the proposed approach achieves strong performance in few-shot and zero-shot settings, with the knowledge-constrained data augmentation and two-stage summarization framework effectively addressing the challenges of limited labeled data and long document processing.

## Method Summary
The authors propose a two-stage summarization framework that first selects salient content from long court judgment documents and then generates abstractive summaries using large language models. The approach employs Vicuna and LLaMA models, which are fine-tuned on the CLSum dataset with memory-efficient training techniques and sparse attention mechanisms to handle long documents. Knowledge-constrained rephrasing is used for data augmentation to ensure accurate usage of legal terms, and the LTScore metric evaluates summaries based on legal term accuracy. The method is evaluated across four common law jurisdictions (Canada, Australia, UK, Hong Kong SAR) under few-shot and zero-shot settings.

## Key Results
- LLM-based summarization methods perform well in few-shot and zero-shot settings on the CLSum dataset
- Knowledge-constrained data augmentation effectively mitigates the impact of insufficient labeled data
- The two-stage summarization framework improves efficiency in processing long documents under limited computing resources
- LTScore metric effectively evaluates the accurate usage of legal terms in generated summaries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning large language models on small labeled datasets produces effective court judgment summaries.
- Mechanism: Pre-trained language models adapt to legal domain with minimal task-specific examples through supervised fine-tuning.
- Core assumption: Pre-training data contains sufficient diversity to cover legal domain.
- Evidence anchors: Abstract states LLM methods perform well in few-shot/zero-shot settings; section 6.3 shows fine-tuning on few examples brings performance gains.

### Mechanism 2
- Claim: Knowledge-constrained rephrasing improves data augmentation quality for legal text generation.
- Mechanism: Constraining LLM to preserve and accurately use legal terms maintains domain-specific accuracy while increasing dataset size.
- Core assumption: Legal terms have precise meanings and must be used accurately.
- Evidence anchors: Section 5.1 emphasizes accurate use of legal terms; section 7.4 confirms constraints ensure accurate use in synthesized data.

### Mechanism 3
- Claim: Two-stage summarization with content selection followed by abstractive summarization improves efficiency for long legal documents.
- Mechanism: First stage compresses long documents while preserving key information, reducing computational burden on abstractive model.
- Core assumption: Key information can be identified and preserved during compression.
- Evidence anchors: Section 5.2 describes maximizing recall of essential content during compression; section 5.3 mentions reducing context length neural models need to model.

## Foundational Learning

- Concept: Legal domain knowledge and terminology
  - Why needed here: Court judgment summarization requires understanding legal concepts and terminology specific to common law systems.
  - Quick check question: Can you explain the difference between common law and civil law systems, and why this distinction matters for judgment summarization?

- Concept: Few-shot learning and transfer learning
  - Why needed here: Approach relies on adapting pre-trained models to new task with minimal labeled examples.
  - Quick check question: What are the key differences between few-shot learning, zero-shot learning, and traditional supervised learning, and when would each be most appropriate?

- Concept: Text summarization techniques (extractive vs. abstractive)
  - Why needed here: System employs both extractive content selection and abstractive summary generation.
  - Quick check question: When would you choose extractive summarization over abstractive summarization, and what are the trade-offs between these approaches?

## Architecture Onboarding

- Component map: Data collection → Data cleaning → Content selection → Abstractive summarization → Evaluation
- Critical path: Content selection → Abstractive summarization
- Design tradeoffs:
  - Model size vs. computational efficiency: Larger models perform better but require more resources
  - Data augmentation vs. data quality: More data can help but may introduce noise if not properly constrained
  - Abstractiveness vs. accuracy: More abstract summaries may lose important details
- Failure signatures:
  - Redundant or repetitive content: Indicates issues with divide-and-conquer approach or model capacity
  - Missing key legal terms: Suggests problems with knowledge-constrained rephrasing or model understanding
  - Poor performance on new jurisdictions: Indicates lack of generalizability in model
- First 3 experiments:
  1. Test zero-shot performance of different LLMs on CLSum to establish baseline capabilities
  2. Compare different content selection methods (TextRank, LexRank, truncation) on sample documents
  3. Evaluate impact of data augmentation with and without knowledge constraints on small training sets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of knowledge-constrained rephrasing compare to other data augmentation techniques when labeled data is extremely limited (fewer than 10 samples)?
- Basis in paper: [explicit] Paper discusses effectiveness of knowledge-constrained rephrasing but lacks specific results for very low data scenarios.
- Why unresolved: Paper doesn't provide experimental results for training sets with fewer than 10 examples.
- What evidence would resolve it: Experiments with training sets containing fewer than 10 examples comparing knowledge-constrained rephrasing to other data augmentation methods.

### Open Question 2
- Question: How does the two-stage summarization framework with sparse attention compare to other efficient methods like sliding window or hierarchical attention?
- Basis in paper: [inferred] Paper mentions sparse attention and two-stage framework but doesn't compare to other efficient methods.
- Why unresolved: Paper lacks direct comparison between proposed framework and other efficient summarization methods.
- What evidence would resolve it: Experiments comparing proposed two-stage framework with sparse attention to methods using sliding window or hierarchical attention.

### Open Question 3
- Question: How does LTScore compare to other legal domain-specific evaluation metrics like those based on legal expert judgment or domain-specific language models?
- Basis in paper: [explicit] Paper introduces LTScore but doesn't compare it to other legal domain-specific metrics.
- Why unresolved: Paper doesn't provide comparison between LTScore and other legal domain-specific evaluation metrics.
- What evidence would resolve it: Experiments comparing LTScore to metrics based on legal expert judgment or domain-specific language models.

## Limitations
- Novel components (knowledge-constrained rephrasing, LTScore, two-stage framework) lack direct validation through comparison with established baselines
- Generalizability across legal systems is claimed but not systematically evaluated through cross-system ablation studies
- Specific computational requirements and scalability for very long documents or extremely limited resources are not thoroughly characterized

## Confidence
- High Confidence: Effectiveness of fine-tuning large language models for court judgment summarization in few-shot and zero-shot settings
- Medium Confidence: Overall effectiveness of two-stage summarization framework and memory-efficient training techniques
- Medium Confidence: Value of knowledge-constrained rephrasing for data augmentation

## Next Checks
1. Conduct component ablation study to systematically evaluate contribution of each major component
2. Perform cross-system generalization test by training and evaluating on subsets from different jurisdictions
3. Execute scalability benchmark by systematically varying document length and computational resources to characterize performance envelope