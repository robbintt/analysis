---
ver: rpa2
title: 'Efficient Learning in Chinese Checkers: Comparing Parameter Sharing in Multi-Agent
  Reinforcement Learning'
arxiv_id: '2405.18733'
source_url: https://arxiv.org/abs/2405.18733
tags:
- game
- policy
- checkers
- chinese
- board
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of efficient learning in the competitive
  multi-agent game of Chinese Checkers using reinforcement learning. The authors develop
  a new PettingZoo environment for Chinese Checkers, implementing the full game rules
  including chaining jumps.
---

# Efficient Learning in Chinese Checkers: Comparing Parameter Sharing in Multi-Agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2405.18733
- Source URL: https://arxiv.org/abs/2405.18733
- Authors: Noah Adhikari; Allen Gu
- Reference count: 15
- This paper develops a new PettingZoo environment for Chinese Checkers and demonstrates that full parameter sharing in MARL achieves 100% win rate against random opponents six times faster than independent training.

## Executive Summary
This paper addresses efficient learning in the competitive multi-agent game of Chinese Checkers using reinforcement learning. The authors develop a new PettingZoo environment implementing full game rules including chaining jumps, and propose a submove architecture to reduce the large action space. They compare three parameter sharing variations in MARL: independent, shared-encoder, and fully-shared architectures. The primary finding is that full parameter sharing outperforms the other architectures in both win rate and game length, achieving 100% win rate against random opponents most efficiently while learning six times faster than the independent approach.

## Method Summary
The authors implement a Chinese Checkers environment in PettingZoo with a 3D binary observation space inspired by AlphaGo and a submove architecture that decomposes complex actions into branching submoves. They train PPO agents using three parameter sharing architectures: independent (separate agents), shared-encoder (shared encoder with separate policy/value heads), and fully-shared (all parameters shared). The training procedure involves self-play for 100 iterations of 4000 timesteps each, with evaluation against random opponents and head-to-head matches between architectures.

## Key Results
- Full parameter sharing achieves 100% win rate against random opponents
- Fully shared model learns six times faster than independent approach in homogeneous environment
- Full parameter sharing outperforms both independent and shared-encoder architectures in head-to-head matches
- Game length is minimized with full parameter sharing approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Full parameter sharing accelerates learning in homogeneous multi-agent environments by allowing all agents to jointly update a single policy.
- Mechanism: In a homogeneous environment, all agents share the same state and action space, so a single policy can learn optimal behavior for all agents simultaneously. This increases sample efficiency by 6x compared to independent training.
- Core assumption: The environment is truly homogeneous, with all agents having identical roles, state spaces, and action spaces.
- Evidence anchors:
  - [abstract] "full parameter sharing outperforms independent and partially shared architectures in the competitive perfect-information homogenous game of Chinese Checkers"
  - [section] "We attribute the increase in performance to the fact that the fully shared model is able to take advantage of the homogenous environment to effectively learn six times as quickly compared to the independent approach"
  - [corpus] Weak evidence - no direct citations in corpus papers supporting this specific claim
- Break condition: If the environment becomes heterogeneous (different agent types, roles, or state/action spaces), parameter sharing would hurt performance.

### Mechanism 2
- Claim: The submove architecture (branching actions) reduces the action space complexity while preserving game fidelity.
- Mechanism: Instead of enumerating all possible jump sequences as separate actions, the environment breaks each turn into submoves (individual jumps or movements). This allows agents to learn move composition rather than memorizing long action sequences.
- Core assumption: The game can be decomposed into meaningful sub-moves without losing strategic depth.
- Evidence anchors:
  - [section] "To prevent this, we adopt the concept of branching actions from complex action spaces in other RL environments, where a move consists of one or more submoves that may not end the player's turn immediately"
  - [section] "In Chinese Checkers, a submove is a single jump, adjacent-space movement, or end-turn action"
  - [corpus] No direct evidence in corpus papers about this specific submove mechanism
- Break condition: If submoves become too granular and lose strategic coherence, or if the state transitions between submoves become too complex to learn.

### Mechanism 3
- Claim: The 3D binary observation space inspired by AlphaGo provides sufficient state representation for Chinese Checkers.
- Mechanism: The 8-layer binary representation captures all relevant game state information (current player's pieces, other players' pieces, jump history) in a format that neural networks can process efficiently.
- Core assumption: Binary encoding is sufficient to capture all strategic information needed for optimal play.
- Evidence anchors:
  - [section] "Our observation space is inspired by AlphaGo with many binary game boards stacked in a 3D array to encode information"
  - [section] "The observation space is a flattened representation of 8 different layers of the board, where each layer provides unique information about the game state"
  - [corpus] No direct evidence in corpus papers about this specific observation encoding
- Break condition: If critical strategic information is lost in the binary encoding, or if the representation becomes too sparse for effective learning.

## Foundational Learning

- Concept: Proximal Policy Optimization (PPO) with clipped surrogate objective
  - Why needed here: PPO provides stable learning in multi-agent settings by limiting policy updates and preventing catastrophic divergence
  - Quick check question: How does the clipped surrogate objective prevent large policy updates that could destabilize training?

- Concept: Multi-agent parameter sharing architectures
  - Why needed here: Understanding when and why to share parameters between agents is crucial for designing efficient MARL systems
  - Quick check question: What are the key differences between fully-independent, shared-encoder, and fully-shared parameter architectures?

- Concept: Action masking for legality constraints
  - Why needed here: Chinese Checkers has complex movement rules that require filtering illegal actions in real-time
  - Quick check question: How does action masking integrate with PPO's action sampling process?

## Architecture Onboarding

- Component map: PPO agent -> encoder network -> policy head + value head -> action masking -> environment step
- Critical path: Environment step -> PPO training loop -> parameter update -> policy evaluation
- Design tradeoffs: Full parameter sharing gives 6x speedup but loses agent-specific specialization; submove architecture simplifies learning but may miss long-term strategies
- Failure signatures: Poor win rates indicate architectural mismatch; long game lengths suggest suboptimal strategy; distributional shift indicates exploration problems
- First 3 experiments:
  1. Run independent vs. full parameter sharing on N=2 board to verify 6x speedup claim
  2. Test different observation space encodings to find optimal representation
  3. Vary entropy coefficient to assess exploration impact on distributional shift

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does parameter sharing in MARL provide similar performance gains in heterogeneous environments as it does in homogeneous environments like Chinese Checkers?
- Basis in paper: [explicit] The authors state "We do not expect the same performance gains in heterogenous environments, but similar methods in stochastic environments may prove effective."
- Why unresolved: The paper only tests parameter sharing in a homogeneous environment, leaving the question of performance in heterogeneous settings unanswered.
- What evidence would resolve it: Conducting experiments with parameter sharing in MARL across various heterogeneous environments (e.g., games with different rules, player types, or objectives) and comparing the results to independent learning approaches.

### Open Question 2
- Question: How does the entropy coefficient affect the exploration and performance of parameter sharing approaches in MARL?
- Basis in paper: [explicit] The authors mention "Attempts to increase exploration through changing PPOâ€™s entropy coefficient led to inconclusive results and did not noticeably improve performance."
- Why unresolved: The paper does not provide a clear understanding of the relationship between the entropy coefficient and the performance of parameter sharing in MARL, as the results were inconclusive.
- What evidence would resolve it: Conducting a more extensive hyperparameter sweep on the entropy coefficient and analyzing its effect on exploration and performance in parameter sharing approaches across different MARL tasks.

### Open Question 3
- Question: Can the distributional shift problem in self-play be mitigated through improved exploration strategies in parameter sharing approaches?
- Basis in paper: [inferred] The authors raise concerns about distributional shift in self-play and attribute the robustness of their trained policies to fixating on their own pieces rather than the global board state.
- Why unresolved: The paper does not provide a definitive solution to the distributional shift problem in self-play for parameter sharing approaches, as attempts to increase exploration through changing the entropy coefficient were inconclusive.
- What evidence would resolve it: Developing and testing novel exploration strategies specifically designed to address the distributional shift problem in self-play for parameter sharing approaches, and evaluating their effectiveness across various MARL tasks.

## Limitations

- Results are validated only in a specific domain (Chinese Checkers with 6 players) against random opponents
- The distributional shift concern is raised but not empirically validated with experiments
- The 6x learning speedup claim is based on a single environment and may not generalize

## Confidence

- Full parameter sharing outperforms alternatives in Chinese Checkers: **High**
- 6x learning speedup from parameter sharing: **Medium**
- Submove architecture preserves strategic depth: **Low**
- Binary observation space is optimal: **Low**

## Next Checks

1. Test the three parameter sharing architectures against self-play opponents to measure distributional shift effects on performance
2. Evaluate performance on larger board sizes (N > 2) to assess scalability of the learning speed advantage
3. Compare the 3D binary observation space against alternative encodings (e.g., coordinate-based, one-hot) to verify representational adequacy