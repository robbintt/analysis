---
ver: rpa2
title: 'SAFES: Sequential Privacy and Fairness Enhancing Data Synthesis for Responsible
  AI'
arxiv_id: '2411.09178'
source_url: https://arxiv.org/abs/2411.09178
tags:
- data
- fairness
- privacy
- race
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SAFES is a method that combines differential privacy (DP) and fairness
  constraints to synthesize privacy-preserving and fair datasets. It sequentially
  applies DP data synthesis and fairness-aware pre-processing transformations, allowing
  control over the privacy-fairness-utility trade-off.
---

# SAFES: Sequential Privacy and Fairness Enhancing Data Synthesis for Responsible AI

## Quick Facts
- arXiv ID: 2411.09178
- Source URL: https://arxiv.org/abs/2411.09178
- Reference count: 40
- Primary result: SAFES achieves strong DP guarantees and significantly improved fairness metrics with relatively low utility loss compared to baseline methods.

## Executive Summary
SAFES is a method that combines differential privacy (DP) and fairness constraints to synthesize privacy-preserving and fair datasets. It sequentially applies DP data synthesis and fairness-aware pre-processing transformations, allowing control over the privacy-fairness-utility trade-off. Experiments on the Adult and COMPAS datasets show that SAFES can achieve strong DP guarantees and significantly improved fairness metrics with relatively low utility loss, compared to baseline methods.

## Method Summary
SAFES combines AIM (Adaptive and Iterative Mechanism) for DP data synthesis with TOT (Triple-constrained Transformation) for fairness-aware preprocessing. AIM uses marginal-based DP synthesis with Gaussian mechanism, adaptively selecting high-value marginals within a privacy budget to preserve joint distributions. TOT solves a constrained optimization problem balancing discrimination control, distortion limits, and utility preservation. The method processes data sequentially: first applying DP synthesis to generate a synthetic dataset, then applying fairness transformation that operates solely on the synthetic data without accessing the original, ensuring DP guarantees are preserved through post-processing immunity.

## Key Results
- SAFES achieves significantly improved fairness metrics (COD, SPD, AOD) compared to DP-only synthesis
- The method demonstrates robustness to changes in privacy loss for small fairness parameters
- Utility loss remains relatively low, with KS statistics and TVD remaining comparable to DP-only baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sequential composition of DP and fairness transforms yields DP guarantees with improved fairness metrics.
- Mechanism: SAFES applies a DP data synthesizer first, then a fairness-aware transformation that operates solely on the DP synthetic data. DP immunity to post-processing ensures that the transformation does not further leak privacy.
- Core assumption: The fairness transformation can improve fairness without significantly degrading utility, and its effect is independent of DP noise injection.
- Evidence anchors:
  - [abstract] "SAFES allows full control over the privacy-fairness-utility trade-off via tunable privacy and fairness parameters."
  - [section] "Proposition 1. SAFES satisfies DP guarantees. The proof is straightforward. Since S satisfies DP with parameters Θ1, D* achieves Θ1-DP guarantees. Since the fairness data transformation T only operates on D* and never accesses the original data D, the post-processing theorem for DP ensures the same DP guarantees on D~."
  - [corpus] Weak—no directly comparable sequential DP+fairness frameworks found; neighboring works treat these separately or combine them in-task.

### Mechanism 2
- Claim: Marginal-based DP synthesis (AIM) maintains sufficient utility while preserving privacy, enabling effective fairness post-processing.
- Mechanism: AIM learns a graphical model from sanitized marginals, adaptively selecting high-value marginals within a privacy budget. This preserves joint distributions better than naive marginals, allowing fairness transforms to act on a richer synthetic dataset.
- Core assumption: The graphical model approximation remains faithful enough to the original data structure so that fairness-aware transformations can meaningfully reduce bias.
- Evidence anchors:
  - [section] "AIM makes several improvements... adaptively updated with higher-order marginals selected with replacement from W in order to optimize the utility of the graphical model."
  - [section] "Empirical evaluations on the Adult and COMPAS datasets demonstrate that for reasonable privacy loss, SAFES-generated synthetic data achieves significantly improved fairness metrics with relatively low utility loss."
  - [corpus] Weak—neighboring works focus on DP-only or fairness-only pipelines, not marginal-based synthesis paired with fairness post-processing.

### Mechanism 3
- Claim: Fairness constraints (η) can be tuned independently of privacy parameters (ε) because of sequential composition.
- Mechanism: The fairness transformation enforces bounds on outcome disparities (e.g., statistical parity difference ≤ η) via convex optimization, without further privacy loss. This allows separate calibration of privacy and fairness.
- Core assumption: The fairness optimization remains feasible under the noise introduced by DP, and the solution space is not overly constrained by privacy noise.
- Evidence anchors:
  - [section] "TOT learns a randomized mapping T in the form of a conditional distribution... by solving an optimization problem that balances controlling discrimination between groups (group fairness), limiting distortion of individual observations (individual fairness), and maintaining a data distribution similar to the one before the transformation (utility preservation)."
  - [section] "In both experiments, we convert ϕA and ϕC into a binary ϕ using threshold values... So that a few minor changes to each record are permissible with small probability, but large changes or changes to too many attributes are strongly discouraged."
  - [corpus] Weak—no direct evidence of independent ε/η tuning in neighboring literature; most works couple these in-task.

## Foundational Learning

- Concept: Differential Privacy (ε,δ) guarantees and the post-processing theorem.
  - Why needed here: SAFES relies on the fact that transformations applied after a DP mechanism do not incur further privacy loss.
  - Quick check question: If M is (ε,δ)-DP and T is any function, is T(M(D)) also (ε,δ)-DP?

- Concept: Fairness metrics (statistical parity, equalized odds, conditional outcome difference).
  - Why needed here: SAFES aims to improve these metrics in the synthetic data, so understanding their definitions and relationships is essential.
  - Quick check question: If SPD = 0.1, does that mean the privileged group is favored or disfavored?

- Concept: Marginal-based DP data synthesis and graphical models.
  - Why needed here: SAFES uses AIM, which builds a graphical model from sanitized marginals; understanding how marginals capture joint distributions is key to reasoning about utility.
  - Quick check question: If a 2-way marginal is sanitized, can you recover the exact joint distribution of those two variables?

## Architecture Onboarding

- Component map: Data source → AIM (DP synthesizer) → TOT (fairness transform) → Synthetic output
- Critical path: Generate marginals → inject DP noise → learn graphical model → sample synthetic data → apply fairness transformation → output
- Design tradeoffs:
  - Higher privacy (smaller ε) → more noise → potentially infeasible fairness constraints
  - Tighter fairness (smaller η) → stricter optimization → potential loss of utility or infeasibility
  - Larger marginal workload → better utility but higher computational and privacy cost
- Failure signatures:
  - Optimization fails: TOT cannot find a solution satisfying η and distortion bounds
  - Utility collapse: KS statistic or TVD much larger than baseline
  - Fairness non-improvement: SPD, AOD remain close to original or worse
- First 3 experiments:
  1. Run AIM alone on Adult dataset with ε=1, δ=1e-9; measure utility (KS, TVD) and fairness (COD, SPD).
  2. Run TOT alone on original Adult data with η=0.1; measure distortion and fairness improvement.
  3. Run SAFES with ε=1, η=0.1 on Adult; compare utility and fairness to steps 1 and 2.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SAFES perform when applied to datasets with mixed categorical and continuous variables?
- Basis in paper: [inferred] The paper explicitly states that both AIM and TOT are limited to categorical and discretized numeric variables, and mentions this as a future research direction.
- Why unresolved: The current experiments only used datasets with categorical variables or discretized continuous variables.
- What evidence would resolve it: Experiments applying SAFES to real-world datasets containing continuous variables without discretization, comparing performance metrics to the current approach.

### Open Question 2
- Question: What is the optimal strategy for balancing privacy loss parameter ε and fairness parameter η to achieve the best utility-fairness trade-off?
- Basis in paper: [inferred] The paper recommends choosing the smallest possible ε for acceptable utility and smallest possible η for a solvable optimization problem, but doesn't provide a systematic approach for finding the optimal balance.
- Why unresolved: The experiments show that the relationship between ε, η, and utility/fairness metrics is complex and non-linear, but no optimization framework is provided.
- What evidence would resolve it: A systematic study varying ε and η across multiple datasets to develop guidelines or an algorithm for selecting these parameters based on desired utility and fairness outcomes.

### Open Question 3
- Question: How does SAFES perform on machine learning tasks beyond binary classification, such as multi-class classification, regression, or clustering?
- Basis in paper: [explicit] The discussion section explicitly identifies extending SAFES to other machine learning tasks as an important future direction.
- Why unresolved: The current experiments only evaluated logistic regression for binary classification tasks.
- What evidence would resolve it: Applying SAFES to various machine learning tasks on multiple datasets, measuring how the privacy-fairness-utility trade-offs differ across task types.

## Limitations
- Sequential composition framework relies on assumption that fairness post-processing doesn't require original data access, with no explicit mechanism preventing indirect leakage via correlated synthetic features
- Marginal-based DP synthesis claims to preserve joint distributions, yet no formal bound on approximation error is provided
- Fairness parameter tuning is described as independent of privacy, but empirical evidence of feasibility across all ε-η combinations is incomplete

## Confidence
- **High**: SAFES satisfies DP guarantees via post-processing (Proposition 1)
- **Medium**: Sequential composition of AIM + TOT yields practical privacy-fairness-utility trade-offs
- **Low**: Independence of ε and η tuning across all operating regimes

## Next Checks
1. Test TOT feasibility under varying noise levels by fixing ε and sweeping η to identify break points
2. Measure approximation error of the graphical model by comparing learned marginals to sanitized ground truth
3. Verify that fairness improvements persist when applying TOT to AIM-synthetic data versus original data to rule out indirect leakage