---
ver: rpa2
title: LLM-based feature generation from text for interpretable machine learning
arxiv_id: '2409.07132'
source_url: https://arxiv.org/abs/2409.07132
tags:
- features
- feature
- dataset
- rules
- high
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work demonstrates the use of Large Language Models (LLMs)
  for generating interpretable text features to enable explainable machine learning,
  addressing the high dimensionality and low interpretability of traditional text
  representations like embeddings and bag-of-words. Two workflows are proposed: one
  where LLMs automatically propose and compute feature values, and another where users
  define features and LLMs compute values.'
---

# LLM-based feature generation from text for interpretable machine learning

## Quick Facts
- arXiv ID: 2409.07132
- Source URL: https://arxiv.org/abs/2409.07132
- Reference count: 9
- Primary result: LLM-generated features achieve predictive performance similar to embeddings but with far fewer, interpretable features

## Executive Summary
This work presents a novel approach to generating interpretable text features using Large Language Models (LLMs) for explainable machine learning. Traditional text representations like embeddings and bag-of-words suffer from high dimensionality and low interpretability, making it difficult to understand model decisions. The proposed method leverages LLMs to automatically generate human-readable, meaningful features from text that can be used in interpretable machine learning models. Two workflows are introduced: one where LLMs automatically propose and compute feature values, and another where users define features and LLMs compute values.

Evaluated on five diverse datasets including scientific articles and intent classification tasks, the approach demonstrates that LLM-generated features can achieve predictive performance comparable to state-of-the-art SciBERT embeddings while using far fewer dimensions (e.g., 62 vs. 768). The generated features are statistically significant, often relevant for prediction tasks, and enable mining of well-interpretable action rules that provide actionable insights. This work addresses a critical gap in explainable AI by providing a method to convert high-dimensional, opaque text representations into sparse, interpretable features that maintain predictive power while enabling human understanding of model decisions.

## Method Summary
The method proposes two complementary workflows for generating interpretable text features using LLMs. In the first workflow, LLMs automatically propose candidate features based on the dataset and then compute feature values for each text instance. In the second workflow, users define specific features they want to extract, and LLMs compute the corresponding values. The approach involves prompting LLMs with task-specific instructions and text samples to generate both the feature definitions and their values. The generated features are then used as inputs to interpretable machine learning models. The evaluation compares the predictive performance of LLM-generated features against traditional embeddings (SciBERT) across five diverse datasets, including scientific articles and intent classification tasks. Statistical significance of features is assessed, and action rules are mined from the interpretable features to demonstrate practical utility.

## Key Results
- LLM-generated features achieved predictive performance similar to SciBERT embeddings on five tested datasets
- Generated features used far fewer dimensions (62 vs. 768) while maintaining comparable accuracy
- Features were statistically significant and enabled mining of interpretable action rules providing actionable insights
- The approach worked across diverse domains including scientific articles and intent classification

## Why This Works (Mechanism)
The approach works because LLMs can understand context and semantics in text, allowing them to identify meaningful patterns and concepts that humans can interpret. By leveraging the LLM's ability to process natural language, the method transforms high-dimensional, opaque text representations into sparse, human-readable features that capture the essential information needed for prediction tasks. The LLM acts as an intelligent feature engineering system that can both propose relevant features and compute their values, bridging the gap between raw text and interpretable machine learning models.

## Foundational Learning
- **Text Embeddings**: Dense vector representations of text that capture semantic meaning but are high-dimensional and uninterpretable - needed because traditional approaches sacrifice interpretability for predictive power
- **Feature Engineering**: The process of creating meaningful features from raw data - needed because raw text cannot be directly used by interpretable ML models
- **Interpretability in ML**: The ability to understand and explain model decisions - needed because black-box models lack transparency for critical applications
- **Action Rules**: If-then rules derived from interpretable features that suggest actionable changes - needed to demonstrate practical utility of interpretable features
- **LLM Prompting**: The technique of crafting instructions for LLMs to generate desired outputs - needed to control feature generation process
- **Statistical Significance Testing**: Methods to determine if features are meaningful predictors - needed to validate the quality of generated features

## Architecture Onboarding

**Component Map**: Raw Text -> LLM Feature Generator -> Interpretable Features -> ML Model -> Predictions

**Critical Path**: The pipeline from raw text through LLM-based feature generation to interpretable ML model represents the critical path, as each step depends on the successful completion of the previous one.

**Design Tradeoffs**: The approach trades computational cost (LLM inference) for interpretability and potentially reduced dimensionality. While embeddings are computationally cheaper, LLM-generated features provide human-readable insights at the expense of inference time.

**Failure Signatures**: Poor feature quality may manifest as low predictive performance, lack of statistical significance, or features that don't align with domain knowledge. Computational bottlenecks may occur during LLM inference for large datasets.

**3 First Experiments**:
1. Run the feature generation workflow on a small text dataset with known interpretable features to validate the basic functionality
2. Compare predictive performance of LLM-generated features against bag-of-words on a simple classification task
3. Test the statistical significance of generated features using a small validation dataset

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to five relatively small datasets with limited domain diversity
- Computational cost of using LLMs for feature generation at scale not fully addressed
- Comparison focuses on predictive performance rather than computational efficiency or robustness to noisy input

## Confidence
- High confidence: LLM-generated features can achieve predictive performance comparable to embeddings on tested datasets
- Medium confidence: Generated features are meaningfully more interpretable, based on qualitative evidence
- Low confidence: Scalability and robustness claims due to limited testing conditions and lack of systematic evaluation

## Next Checks
1. Evaluate the approach on substantially larger datasets with greater domain diversity and document computational costs at scale
2. Conduct systematic human studies to validate that generated features are more interpretable and useful for decision-making compared to traditional embeddings
3. Test robustness of LLM-generated features to noisy or out-of-distribution text and investigate potential biases in the feature generation process