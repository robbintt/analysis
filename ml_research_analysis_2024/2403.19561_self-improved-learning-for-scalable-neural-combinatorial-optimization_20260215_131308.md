---
ver: rpa2
title: Self-Improved Learning for Scalable Neural Combinatorial Optimization
arxiv_id: '2403.19561'
source_url: https://arxiv.org/abs/2403.19561
tags:
- node
- training
- learning
- attention
- nodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a self-improved learning (SIL) method for scalable
  neural combinatorial optimization (NCO). SIL enables direct training of NCO models
  on large-scale problems without labeled data by iteratively generating better solutions
  as pseudo-labels via a local reconstruction approach.
---

# Self-Improved Learning for Scalable Neural Combinatorial Optimization

## Quick Facts
- arXiv ID: 2403.19561
- Source URL: https://arxiv.org/abs/2403.19561
- Reference count: 29
- This paper proposes SIL method enabling direct training of NCO models on large-scale problems without labeled data

## Executive Summary
This paper introduces Self-Improved Learning (SIL), a novel approach for training neural combinatorial optimization models on large-scale problems without requiring labeled data. SIL iteratively generates high-quality pseudo-labels through local reconstruction of partial solutions, enabling the model to improve itself through successive training cycles. The method achieves state-of-the-art performance on TSP and CVRP problems with up to 100K nodes while maintaining linear computational complexity.

## Method Summary
SIL employs an iterative training cycle consisting of local reconstruction to generate enhanced solutions and model training using these solutions as pseudo-labels. The method uses a linear complexity attention mechanism with representative points (starting and destination nodes) to efficiently handle large-scale instances. The model architecture follows an encoder-decoder structure where the decoder features stacked linear attention modules. Training begins with warm-up on small instances using RL-based methods, then transitions to direct training on large-scale instances through the SIL process.

## Key Results
- Achieves state-of-the-art performance on TSP and CVRP with up to 100K nodes
- Linear attention mechanism reduces computational complexity from O(N²) to O(N)
- Outperforms transfer learning approaches by training directly on large-scale instances
- Demonstrates superior scalability compared to existing NCO methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-Improved Learning enables direct training on large-scale problems without labeled data by iteratively generating better solutions as pseudo-labels
- Mechanism: SIL uses local reconstruction to sample and improve partial solutions, then uses these enhanced solutions as pseudo-labels to train the model iteratively
- Core assumption: The model's bias in decoding can be exploited to generate better partial solutions through local reconstruction
- Evidence anchors:
  - [abstract]: "we develop an efficient self-improved mechanism that enables direct model training on large-scale problem instances without any labeled data."
  - [section]: "The local reconstruction step produces enhanced solutions to guide model training, while the improved model further strengthens the local reconstruction approach to generate even better solutions."
- Break condition: If the model cannot generate better partial solutions through local reconstruction, the pseudo-label quality degrades and training stalls

### Mechanism 2
- Claim: Linear complexity attention mechanism enables efficient handling of large-scale instances with low computation overhead
- Mechanism: Instead of quadratic attention over all nodes, representative points aggregate key information and broadcast it to all nodes, achieving linear complexity
- Core assumption: Representative points (starting and destination nodes) capture most critical information needed for solution construction
- Evidence anchors:
  - [abstract]: "we design a linear complexity attention mechanism for the model to efficiently handle large-scale combinatorial problem instances with low computation overhead."
  - [section]: "Our proposed model also uses an encoder-decoder structure... its decoder features L stacked linear attention modules."
- Break condition: If representative points fail to capture sufficient graph information, solution quality degrades despite computational efficiency

### Mechanism 3
- Claim: Training on large-scale instances directly rather than transferring from small-scale models provides better scalability
- Mechanism: The model is trained from scratch on large-scale instances using SIL, avoiding the generalization gap from small-to-large scale transfer
- Core assumption: Direct training on target scale problem instances yields better performance than transfer learning approaches
- Evidence anchors:
  - [abstract]: "SIL has a novel iterative cycle that contains 1) a local reconstruction step to produce enhanced solutions for model training, and 2) a model training step to further strengthen the local reconstruction performance."
  - [section]: "Our contributions... We develop a novel and efficient self-improved learning method that allows NCO models to be directly trained on large-scale CO problem instances without any labeled data."
- Break condition: If computational resources become prohibitive, transfer learning from smaller scales might become necessary despite performance trade-offs

## Foundational Learning

- Concept: Local reconstruction for solution improvement
  - Why needed here: Enables generation of high-quality pseudo-labels without labeled data, crucial for scaling to large instances
  - Quick check question: How does sampling partial solutions from the circular representation of the complete solution enable diverse reconstruction opportunities?

- Concept: Attention mechanisms and their computational complexity
  - Why needed here: Understanding why quadratic attention fails on large-scale problems and how linear attention overcomes this limitation
  - Quick check question: What is the computational complexity of standard self-attention versus the proposed linear attention mechanism per decoding step?

- Concept: Markov Decision Process formulation of constructive NCO
  - Why needed here: Provides the theoretical foundation for viewing solution construction as sequential decision-making
  - Quick check question: How does the autoregressive nature of solution construction relate to the MDP formulation in constructive NCO?

## Architecture Onboarding

- Component map:
  Encoder -> Linear projection of node features to embeddings
  Decoder -> L stacked linear attention modules with representative points
  Training loop -> Warm-up on small instances → SIL training on target scale
  Inference -> Parallel local reconstruction with random insertion initialization

- Critical path:
  1. Initialize solution via random insertion or model's greedy search
  2. Perform parallel local reconstruction to generate enhanced solutions
  3. Use enhanced solutions as pseudo-labels for model training
  4. Repeat until convergence or budget exhausted

- Design tradeoffs:
  - Linear attention vs quadratic attention: computational efficiency vs potential information loss
  - Local reconstruction vs complete solution generation: scalability vs solution quality
  - Direct training vs transfer learning: resource requirements vs generalization performance

- Failure signatures:
  - Training plateau: local reconstruction fails to generate better solutions
  - Memory overflow: attention mechanism complexity exceeds available resources
  - Generalization failure: model overfits to training scale and fails on different sizes

- First 3 experiments:
  1. Verify linear attention reduces memory usage compared to standard attention on TSP1K
  2. Test local reconstruction improvement rate on TSP5K with varying lmax values
  3. Compare SIL-trained model performance against transfer learning baseline on TSP10K

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the linear attention model scale with extremely large problem sizes (e.g., 1 million nodes) compared to quadratic attention models?
- Basis in paper: [explicit] The paper discusses the linear complexity attention mechanism's efficiency for large-scale problems but does not provide empirical results for extremely large instances.
- Why unresolved: The paper's experiments only cover up to 100,000 nodes, leaving a gap in understanding performance at much larger scales.
- What evidence would resolve it: Experimental results comparing linear and quadratic attention models on problem instances with 1 million nodes or more.

### Open Question 2
- Question: What are the specific impacts of different partial solution sizes (ω) on the effectiveness of the local reconstruction approach?
- Basis in paper: [explicit] The paper mentions using a partial solution size range [4, lmax] but does not explore how varying ω affects performance.
- Why unresolved: The paper does not provide a detailed analysis of how different partial solution sizes influence the model's ability to generate improved solutions.
- What evidence would resolve it: A study examining the performance of the local reconstruction approach with different partial solution sizes on various problem scales.

### Open Question 3
- Question: How does the proposed method perform on combinatorial optimization problems beyond TSP and CVRP, such as scheduling or bin packing problems?
- Basis in paper: [inferred] The paper focuses on TSP and CVRP, suggesting potential applicability to other CO problems, but does not test it.
- Why unresolved: The method's effectiveness on a broader range of CO problems remains untested, limiting understanding of its generalizability.
- What evidence would resolve it: Experimental results demonstrating the method's performance on other CO problems like job scheduling or bin packing.

## Limitations

- Limited empirical validation on problem sizes beyond 100K nodes despite theoretical claims of scalability
- Lack of comprehensive comparison with all relevant NCO baselines across different problem types
- Uncertainty about computational overhead of maintaining representative points in linear attention mechanism

## Confidence

**High Confidence**: The theoretical foundation of using self-improved learning for NCO without labeled data is sound and well-grounded in existing RL literature. The linear attention mechanism design follows established principles for reducing computational complexity in sequence models.

**Medium Confidence**: The claim that direct training on large-scale instances outperforms transfer learning from small-scale models needs more empirical validation. While the paper presents strong results, the comparison is limited to specific baselines and problem instances.

**Low Confidence**: The assertion that the proposed method achieves state-of-the-art performance across all tested scales and problem types is difficult to verify without access to the exact implementation details and comprehensive benchmarking against all relevant baselines.

## Next Checks

1. **Scalability Validation**: Test the linear attention mechanism's memory and time complexity on TSP instances ranging from 1K to 100K nodes, measuring actual resource consumption versus theoretical predictions.

2. **Pseudo-label Quality Assessment**: Evaluate the effectiveness of local reconstruction by measuring solution improvement rates across different sampling strategies and partial solution sizes (lmax values) on CVRP instances.

3. **Transfer Learning Comparison**: Implement a direct transfer learning baseline from small-scale (1K nodes) to large-scale (100K nodes) instances and compare solution quality and training efficiency against the proposed SIL approach.