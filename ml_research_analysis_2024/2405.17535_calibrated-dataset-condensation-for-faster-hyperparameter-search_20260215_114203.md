---
ver: rpa2
title: Calibrated Dataset Condensation for Faster Hyperparameter Search
arxiv_id: '2405.17535'
source_url: https://arxiv.org/abs/2405.17535
tags: []
core_contribution: This paper tackles the challenge of accelerating hyperparameter/architecture
  search by proposing a novel dataset condensation approach called HCDC. The core
  idea is to preserve the validation-performance rankings of models with different
  hyperparameters when using condensed data, which is achieved by aligning hypergradients
  computed via implicit differentiation and efficient inverse Hessian approximation.
---

# Calibrated Dataset Condensation for Faster Hyperparameter Search

## Quick Facts
- arXiv ID: 2405.17535
- Source URL: https://arxiv.org/abs/2405.17535
- Reference count: 10
- Primary result: Achieves Spearman's rank correlation of 0.74±0.21 on CIFAR-10 for preserving architecture rankings between condensed and original datasets

## Executive Summary
This paper addresses the computational bottleneck in hyperparameter and architecture search by proposing HCDC, a novel dataset condensation approach that preserves validation-performance rankings across different model configurations. Traditional dataset condensation methods focus on retaining training performance, but HCDC uniquely targets the preservation of ranking relationships when evaluating multiple architectures. The method uses hypergradient alignment through implicit differentiation and inverse Hessian approximation to create condensed datasets that maintain relative performance orderings. Experimental results demonstrate significant improvements in search efficiency, with HCDC achieving much higher correlation in architecture rankings compared to random sampling and K-Center coresets.

## Method Summary
HCDC works by aligning hypergradients computed via implicit differentiation to ensure that condensed data preserves the relative validation performance of different models. The approach uses efficient inverse Hessian approximation to make the hypergradient computation tractable. Unlike traditional dataset condensation that optimizes for training accuracy, HCDC specifically optimizes for maintaining the ranking of models when evaluated on the condensed dataset versus the original dataset. This calibration ensures that hyperparameter search results on condensed data translate effectively to full dataset performance. The method is tested across multiple image and graph datasets, showing consistent improvements in search efficiency and accuracy.

## Key Results
- HCDC achieves Spearman's rank correlation of 0.74±0.21 on CIFAR-10, significantly outperforming random sampling (-0.12±0.07) and K-Center coresets (-0.21±0.15)
- The method preserves architecture rankings more effectively than existing dataset condensation approaches when searching over neural architectures
- HCDC demonstrates consistent improvements across both image and graph datasets, validating its generalizability beyond a single domain

## Why This Works (Mechanism)
HCDC works by aligning hypergradients through implicit differentiation, which captures how small changes in the dataset affect validation performance across different model configurations. By preserving these gradient relationships, the condensed dataset maintains the relative performance ordering of architectures. The inverse Hessian approximation makes this computationally feasible by providing an efficient way to compute the second-order information needed for accurate hypergradient alignment. This mechanism ensures that the condensed data captures the essential structure needed for reliable ranking preservation rather than just maximizing training accuracy on a single model.

## Foundational Learning

1. **Hypergradient computation** - needed to understand how dataset changes affect validation performance across different models; quick check: verify that hypergradients capture ranking relationships by testing on synthetic ranking-preserving datasets.

2. **Implicit differentiation** - required for efficient computation of hypergradients without explicit optimization of the inner loop; quick check: confirm that implicit differentiation yields similar results to unrolled optimization for small problems.

3. **Inverse Hessian approximation** - necessary to make hypergradient computation tractable at scale; quick check: compare approximation accuracy against exact Hessian computation on small datasets.

4. **Dataset condensation objectives** - understanding that traditional condensation optimizes for training accuracy while HCDC optimizes for ranking preservation; quick check: verify that traditional objectives fail to preserve rankings by testing on simple ranking tasks.

## Architecture Onboarding

**Component Map:** Data → Hypergradient Calculator → Inverse Hessian Approximator → Ranking Preservation Optimizer → Condensed Dataset

**Critical Path:** The hypergradient computation via implicit differentiation is the computational bottleneck that determines overall efficiency. The inverse Hessian approximation directly impacts the accuracy of ranking preservation.

**Design Tradeoffs:** Accuracy vs. computational cost in hypergradient computation; dataset size vs. search effectiveness; ranking preservation vs. absolute performance on individual models.

**Failure Signatures:** Low Spearman correlation indicates failure to preserve rankings; high variance in results suggests sensitivity to initialization; poor performance on out-of-distribution architectures suggests limited generalizability.

**First Experiments:** 1) Verify hypergradient alignment on synthetic datasets with known rankings, 2) Compare condensed dataset size vs. correlation trade-offs systematically, 3) Test on diverse model architectures beyond those used in the paper to assess generalizability.

## Open Questions the Paper Calls Out

None

## Limitations
- High variance in results (±0.21) suggests sensitivity to initialization or hyperparameter settings
- Computational overhead of hypergradient computation may offset some time savings
- Limited testing on non-image datasets and larger-scale problems like ImageNet-1K

## Confidence

| Claim | Confidence |
|-------|------------|
| Improved ranking preservation vs. baselines | Medium |
| Generalizability across dataset types | Low |
| Time savings from faster hyperparameter search | Medium |

## Next Checks
1. Test HCDC on larger-scale datasets (ImageNet-1K) and diverse model families (Vision Transformers, ResNets) to assess scalability
2. Measure actual wall-clock time savings from HCDC-compressed searches versus baseline methods, including hypergradient computation overhead
3. Conduct ablation studies on different condensed dataset sizes to identify optimal compression ratio for various hyperparameter search budgets