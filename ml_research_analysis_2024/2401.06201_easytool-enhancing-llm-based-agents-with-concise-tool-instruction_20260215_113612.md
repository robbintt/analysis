---
ver: rpa2
title: 'EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction'
arxiv_id: '2401.06201'
source_url: https://arxiv.org/abs/2401.06201
tags: []
core_contribution: The paper introduces EASYTOOL, a framework that transforms diverse
  and lengthy tool documentation into concise, unified tool instructions for large
  language model (LLM)-based agents. EASYTOOL addresses the challenges of inconsistency,
  redundancy, and incompleteness in tool documentation by generating standardized
  tool descriptions and functionality guidelines.
---

# EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction

## Quick Facts
- arXiv ID: 2401.06201
- Source URL: https://arxiv.org/abs/2401.06201
- Reference count: 8
- The paper introduces EASYTOOL, a framework that transforms diverse and lengthy tool documentation into concise, unified tool instructions for large language model (LLM)-based agents.

## Executive Summary
This paper introduces EASYTOOL, a framework that addresses the challenge of inconsistent, redundant, and incomplete tool documentation by transforming it into concise, standardized tool instructions. The method leverages LLMs to generate structured tool descriptions and functionality guidelines, significantly reducing token consumption (up to 97.35%) while improving LLM performance in tool utilization. Experiments on ToolBench, RestBench, and FuncQA demonstrate substantial improvements in success rates and generalization to open-source LLMs without prior documentation.

## Method Summary
EASYTOOL transforms raw tool documentation through a two-stage process: first generating concise tool descriptions that summarize functionality, then creating functionality guidelines with parameters and usage examples. The framework uses LLMs (ChatGPT/GPT-4) to distill essential information from diverse documentation sources, producing structured instructions that maintain critical details while dramatically reducing token consumption. The generated instructions follow a standardized format with tool descriptions and functionality guidelines, enabling consistent tool usage across different LLMs.

## Key Results
- EASYTOOL reduces token consumption by up to 97.35% compared to raw documentation
- Success rates on ToolBench increase by up to 70% when using EASYTOOL-enhanced instructions
- The framework generalizes to open-source LLMs like Vicuna and Mistral-Instruct-7B, enabling effective tool usage even without prior documentation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transforming tool documentation into concise instructions improves LLM tool utilization.
- Mechanism: The EASYTOOL framework distills essential tool information by removing redundant content and organizing key details into structured tool descriptions and functionality guidelines. This reduces token consumption and helps LLMs focus on core functionality.
- Core assumption: LLMs benefit from reduced context size and clearer structure when selecting and using tools.
- Evidence anchors:
  - [abstract] "EASYTOOL can significantly reduce token consumption (up to 97.35%) and improve LLM performance in tool utilization."
  - [section 3.2] "tool documentation usually includes plenty of irrelevant information that makes it difficult to understand practical usage for LLMs."
  - [corpus] The related work highlights challenges in API documentation complexity and inconsistency, which EASYTOOL addresses through structured summarization.
- Break condition: If the distilled instructions lose critical details necessary for tool execution, performance could degrade.

### Mechanism 2
- Claim: Providing tool usage examples in instructions enhances LLM parameter prediction accuracy.
- Mechanism: The framework includes functionality guidelines with example scenarios and parameter values, enabling LLMs to learn proper tool invocation patterns through demonstration.
- Core assumption: LLMs can effectively learn from few-shot examples within instructions to predict correct parameters.
- Evidence anchors:
  - [section 4.2] "EASY TOOL provides detailed information for tool usage (e.g., its parameters with demonstrations generated by ChatGPT)"
  - [section 5.1] "lower tool error rate of models withEASY TOOL indicates that compared to few-shot learning with demonstrations, concise and effective tool instructions can better guide models to select correct tools and pass valid parameters."
  - [corpus] The corpus includes related work on tool documentation enhancement and zero-shot learning, supporting the effectiveness of example-based instruction.
- Break condition: If examples don't cover the parameter space adequately, LLMs may still fail to predict correct parameters.

### Mechanism 3
- Claim: Unified tool instruction format enables consistent tool usage across diverse LLMs.
- Mechanism: By standardizing tool descriptions and functionality guidelines into a consistent format, EASYTOOL allows different LLMs to use tools effectively without model-specific adaptations.
- Core assumption: LLMs with varying architectures can benefit from standardized instruction formats.
- Evidence anchors:
  - [abstract] "The framework also generalizes to open-source LLMs, enabling effective tool usage even without prior documentation."
  - [section 5.1] "Vicuna and Mistral-Instruct-7B fail to pass any instruction (pass rate and win rate = 0). This could be due to their lack of training on data formatted with function calls. However, tool Instructions generated by EASY TOOL can endow these models with tool utilization ability."
  - [corpus] The corpus shows that documentation standardization is a key challenge in tool utilization research.
- Break condition: If the standardized format doesn't align with an LLM's natural language understanding patterns, effectiveness may decrease.

## Foundational Learning

- Concept: Token consumption and context window limitations in LLMs
  - Why needed here: The paper emphasizes reducing token consumption from tool documentation to fit within LLM context limits and improve efficiency.
  - Quick check question: How does reducing token consumption from thousands to hundreds of tokens impact LLM performance?

- Concept: Few-shot learning through examples in prompts
  - Why needed here: The functionality guidelines include examples to help LLMs understand proper tool usage, which is a key mechanism for improving parameter prediction.
  - Quick check question: Why are examples more effective than plain descriptions for helping LLMs learn tool usage patterns?

- Concept: Instruction-following capabilities in LLMs
  - Why needed here: The framework relies on LLMs' ability to follow structured instructions and examples to use tools effectively.
  - Quick check question: What makes some LLMs better at following instructions than others, and how does this affect tool utilization performance?

## Architecture Onboarding

- Component map: Raw Documentation → Description Generation → Functionality Guidelines → Tool Instructions → LLM Usage → Performance Evaluation
- Critical path: Documentation → Description Generation → Functionality Guidelines → Tool Instructions → LLM Usage → Performance Evaluation
- Design tradeoffs:
  - Conciseness vs completeness: Balancing token reduction with retaining essential information
  - Example diversity vs specificity: Providing enough examples to cover parameter space without being overly verbose
  - Standardization vs flexibility: Creating uniform format while accommodating tool-specific nuances
- Failure signatures:
  - Performance degradation when instructions become too brief and lose critical details
  - Inconsistent results across different LLMs due to varying instruction-following capabilities
  - Token overflow issues if some tool instructions remain too lengthy
- First 3 experiments:
  1. Compare LLM performance using original documentation vs EASYTOOL instructions on a simple tool selection task
  2. Test instruction quality by having annotators evaluate accuracy of generated descriptions and examples
  3. Measure token reduction achieved by EASYTOOL across different documentation sources and tool types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does EASYTOOL handle tool dependencies and relationships between different tools when generating instructions?
- Basis in paper: Inferred from the "Limitations" section which mentions that the method only works on single documentation and neglects dependencies among tools
- Why unresolved: The paper doesn't provide any experimental results or methodology for handling tool dependencies, despite acknowledging this as a limitation
- What evidence would resolve it: Experimental results showing EASYTOOL's performance on datasets requiring multi-tool coordination, or a proposed methodology for incorporating tool dependencies into the instruction generation process

### Open Question 2
- Question: What is the performance impact of using EASYTOOL on LLMs with different context window sizes?
- Basis in paper: Inferred from the token reduction statistics and discussion of context length limitations
- Why unresolved: While the paper shows significant token reduction, it doesn't analyze how this reduction affects different models with varying context window capacities
- What evidence would resolve it: Comparative experiments showing EASYTOOL's effectiveness across LLMs with different context window sizes (e.g., 4K vs 32K tokens)

### Open Question 3
- Question: How robust is EASYTOOL's instruction generation to variations in tool documentation quality and format?
- Basis in paper: Inferred from the "Analysis" section discussing issues with tool documentation inconsistency and incompleteness
- Why unresolved: The paper doesn't provide systematic testing across diverse documentation qualities or formats
- What evidence would resolve it: Experiments showing EASYTOOL's performance degradation (or lack thereof) when processing poorly formatted or incomplete documentation, or documentation from non-standard sources

## Limitations
- The framework only works on single documentation and neglects dependencies among tools
- Performance may degrade if distilled instructions lose critical details necessary for tool execution
- Evaluation is limited to specific datasets and may not generalize to all tool types

## Confidence

**Major Uncertainties and Limitations:**
The evaluation primarily focuses on success rates and token consumption metrics without deeply exploring edge cases where tool instructions might fail due to overly aggressive summarization. The framework's reliance on LLM-generated instructions introduces potential biases based on the underlying model's training data and capabilities. Additionally, the generalizability to extremely complex or domain-specific tools with intricate parameter dependencies remains unclear.

**Confidence Labels:**
- High confidence: The core claim that EASYTOOL significantly reduces token consumption (up to 97.35%) is well-supported by quantitative results and aligns with the fundamental principle of reducing context size.
- Medium confidence: The improvement in LLM performance through concise instructions is supported by success rate increases (up to 70% on ToolBench), but the exact contribution of each mechanism (description quality vs example quality) is not isolated.
- Medium confidence: The generalization to open-source LLMs is demonstrated, but the evaluation is limited to two models (Vicuna and Mistral-Instruct-7B), which may not represent the broader landscape of LLM capabilities.

## Next Checks
1. Conduct ablation studies to isolate the impact of tool descriptions versus functionality guidelines on LLM performance, determining which component contributes more to success rate improvements.
2. Test EASYTOOL's performance on a broader range of open-source LLMs with varying instruction-following capabilities to better understand generalization limits.
3. Evaluate edge cases where tool parameters have complex dependencies or require multi-step reasoning to ensure the framework doesn't lose critical information during summarization.