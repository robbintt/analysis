---
ver: rpa2
title: Accelerating Depthwise Separable Convolutions on Ultra-Low-Power Devices
arxiv_id: '2406.12478'
source_url: https://arxiv.org/abs/2406.12478
tags:
- memory
- data
- fusion
- kernels
- transfers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of accelerating depthwise separable
  convolutions on ultra-low-power devices by proposing a novel approach to fuse depthwise
  and pointwise convolution kernels. The core method involves designing six new fused
  kernels that leverage different data layouts and processing patterns to maximize
  data reuse and minimize memory transfers.
---

# Accelerating Depthwise Separable Convolutions on Ultra-Low-Power Devices

## Quick Facts
- arXiv ID: 2406.12478
- Source URL: https://arxiv.org/abs/2406.12478
- Reference count: 19
- Primary result: Reduces latency of end-to-end network execution by up to 11.40% and activation data movements between L2 and L1 memories by up to 52.97% on GAP8 SoC

## Executive Summary
This paper addresses the challenge of accelerating depthwise separable convolutions on ultra-low-power devices by proposing a novel approach to fuse depthwise and pointwise convolution kernels. The core method involves designing six new fused kernels that leverage different data layouts and processing patterns to maximize data reuse and minimize memory transfers. When implemented on the GreenWaves GAP8 SoC, the proposed approach significantly reduces both latency and memory movements for mobile network inference on edge devices.

## Method Summary
The authors propose six new fused kernels that combine depthwise and pointwise convolutions in different sequences (DW-PW and PW-DW) with varying tiling strategies (row-wise and channel-wise). These kernels are implemented by extending the DORY compiler with a fusion engine, new tiler, and post-processing optimizer. The approach is validated on three DNN architectures (MobileNetV1, MobileNetV2, DSCNN) on the GAP8 SoC, achieving significant reductions in both latency and memory transfers through elimination of intermediate tensor storage and optimized data layouts.

## Key Results
- 11.40% reduction in end-to-end network execution latency
- 52.97% reduction in activation data movements between L2 and L1 memories
- Successful deployment on three DNN architectures (MobileNetV1, MobileNetV2, DSCNN) with varying input sizes

## Why This Works (Mechanism)

### Mechanism 1
Fusing depthwise and pointwise convolutions reduces memory transfers by eliminating intermediate tensor storage. In standard execution, the depthwise layer produces intermediate activations that must be written to memory and then read back for the pointwise layer. Fusing them allows the pointwise computation to directly consume depthwise outputs without intermediate memory staging. Core assumption: The fused kernel can fit both operations in the L1 scratchpad without exceeding memory constraints. Break condition: When the intermediate buffer size exceeds L1 capacity, forcing external memory accesses that negate the benefit.

### Mechanism 2
Row-wise tiling in depthwise-pointwise fusion avoids recomputation while maximizing input reuse. By processing entire rows at once, the fused kernel can reuse input data across multiple output channels without recalculating partial results, unlike block-wise tiling which requires overlap. Core assumption: Input reuse in pointwise convolution is sufficient to amortize the cost of loading more input rows per tile. Break condition: When filter size is large, requiring excessive input rows per tile that exceed L1 capacity.

### Mechanism 3
Channel-wise tiling in pointwise-depthwise fusion enables flexible memory partitioning while avoiding recomputation. Processing F_D channels per tile with a buffer containing all spatial locations eliminates the need to recompute intermediate activations between adjacent channel tiles. Core assumption: The intermediate buffer size remains manageable even for large spatial dimensions. Break condition: When feature map spatial dimensions are large enough that F_D · IX · IY exceeds available L1 memory.

## Foundational Learning

- Concept: Depthwise separable convolution factorization
  - Why needed here: Understanding how DW and PW layers interact is fundamental to designing effective fusion strategies
  - Quick check question: What is the computational complexity reduction from standard convolution to depthwise separable convolution?

- Concept: Memory hierarchy and data movement costs
  - Why needed here: The primary benefit of fusion is reducing expensive L2-L1 transfers, which requires understanding memory access patterns
  - Quick check question: Why is data movement between memory levels more expensive than computation in ultra-low-power devices?

- Concept: Tiling strategies and data locality
  - Why needed here: Different tiling approaches affect data reuse patterns and intermediate buffer requirements in fused kernels
  - Quick check question: How does row-wise tiling differ from block-wise tiling in terms of input data reuse?

## Architecture Onboarding

- Component map: Fused kernels -> DORY compiler -> GAP8 SoC (RISC-V cluster + L1/L2 memories + DMA)
- Critical path: Input data -> DMA transfer to L1 -> Fused kernel execution -> Output DMA to L2
- Design tradeoffs: Larger intermediate buffers reduce recomputation but limit tile size; different data layouts affect memory access patterns and computational efficiency
- Failure signatures: Excessive memory transfers indicate buffer overflow; poor computational efficiency suggests suboptimal data layout choice; core underutilization indicates tiling misalignment with parallelization scheme
- First 3 experiments:
  1. Profile unfused DW and PW kernels separately to establish baseline performance and identify memory transfer bottlenecks
  2. Test each fused kernel variant with different F_D values to find optimal intermediate buffer size for various layer geometries
  3. Deploy a small network (DSCNN) with different fusion strategies to validate end-to-end performance improvements

## Open Questions the Paper Calls Out

### Open Question 1
How do the proposed fused kernels perform on other ultra-low-power SoCs beyond GAP8, such as STM32H7 or NXP LPC4300? The study is limited to GAP8, and the performance may vary significantly on other architectures due to differences in memory hierarchy, core count, and SIMD capabilities.

### Open Question 2
What is the impact of different data layout combinations (CHW vs HWC) on the performance of fused kernels for various layer geometries? The paper mentions using different data layouts but only considers specific combinations deemed optimal.

### Open Question 3
How does the performance of fused kernels scale with larger input resolutions and more channels, such as 512x512 inputs or 1024 channels? The study is limited to relatively small input sizes and channel counts.

### Open Question 4
What is the energy consumption reduction achieved by the fused kernels when considering the entire memory hierarchy (L1, L2, and L3)? The paper provides only an estimate of energy savings and does not measure the actual energy consumption.

## Limitations

- Fusion benefits are highly dependent on specific hardware characteristics of the GAP8 SoC and may not generalize to other architectures
- The study does not provide sensitivity analysis showing how performance varies with different input sizes, channel counts, or filter dimensions
- Intermediate buffer sizing constraints may limit the applicability of certain kernels for arbitrary network configurations

## Confidence

- **High confidence**: The fundamental mechanism of reducing memory transfers through kernel fusion is well-established
- **Medium confidence**: The specific performance numbers are tied to particular network configurations and hardware parameters that may not generalize
- **Low confidence**: The comparison against alternative approaches is limited, making it difficult to assess relative effectiveness

## Next Checks

1. Implement the fused kernels on a different ultra-low-power platform (e.g., ARM Cortex-M) to test generalizability beyond GAP8
2. Systematically vary the intermediate buffer sizes and document the performance trade-offs to establish clear guidelines for kernel selection
3. Benchmark the fused kernels against alternative optimization strategies such as quantized inference, sparsity exploitation, or specialized hardware accelerators