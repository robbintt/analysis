---
ver: rpa2
title: 'MME-Finance: A Multimodal Finance Benchmark for Expert-level Understanding
  and Reasoning'
arxiv_id: '2411.03314'
source_url: https://arxiv.org/abs/2411.03314
tags:
- mllms
- financial
- arxiv
- preprint
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MME-Finance is a bilingual multimodal benchmark designed to evaluate
  large multimodal language models on financial image understanding tasks. It includes
  1,171 English and 1,103 Chinese questions covering six types of financial images,
  with annotations from experts with over 10 years of financial industry experience.
---

# MME-Finance: A Multimodal Finance Benchmark for Expert-level Understanding and Reasoning

## Quick Facts
- arXiv ID: 2411.03314
- Source URL: https://arxiv.org/abs/2411.03314
- Reference count: 40
- MME-Finance is a bilingual multimodal benchmark designed to evaluate large multimodal language models on financial image understanding tasks.

## Executive Summary
MME-Finance is a bilingual multimodal benchmark designed to evaluate large multimodal language models on financial image understanding tasks. It includes 1,171 English and 1,103 Chinese questions covering six types of financial images, with annotations from experts with over 10 years of financial industry experience. The benchmark tests three levels of ability: perception (e.g., OCR, spatial awareness), reasoning (e.g., numerical calculations), and cognition (e.g., risk warning, investment advice). Evaluation is performed using an LLM-based evaluator with image input, achieving high consistency with human judgment. Experiments on 19 models show that even the best-performing models, Qwen2VL-72B (65.69%) and GPT-4o (63.18%), struggle with financial tasks, particularly candlestick charts, technical indicators, and mobile photographs.

## Method Summary
The benchmark was constructed by first collecting and categorizing financial images, then annotating them with expert-generated questions and answers. A hierarchical taxonomy was developed to organize questions into three levels of ability: perception, reasoning, and cognition. To ensure high-quality data, experts with over 10 years of financial industry experience were involved in the annotation process. An LLM-based evaluator was used to assess model performance, with its outputs compared against human judgments to ensure consistency. The benchmark was tested on 19 large multimodal language models, with results indicating significant room for improvement in financial image understanding.

## Key Results
- MME-Finance includes 1,171 English and 1,103 Chinese questions covering six types of financial images.
- Best-performing models, Qwen2VL-72B (65.69%) and GPT-4o (63.18%), struggle with financial tasks, particularly candlestick charts, technical indicators, and mobile photographs.
- Evaluation using an LLM-based evaluator achieves high consistency with human judgment.

## Why This Works (Mechanism)
The benchmark works by providing a structured framework for evaluating multimodal models on financial image understanding tasks. It leverages expert annotations to ensure high-quality data and uses a hierarchical taxonomy to categorize questions by difficulty. The LLM-based evaluator allows for scalable and consistent assessment, while the bilingual nature of the benchmark ensures broader applicability.

## Foundational Learning
- Financial image understanding: Why needed - Essential for interpreting real-world financial data; Quick check - Ability to extract meaningful information from candlestick charts or financial reports.
- Multimodal reasoning: Why needed - Combines visual and textual information for comprehensive analysis; Quick check - Solving numerical problems based on financial graphs.
- Expert annotation: Why needed - Ensures high-quality and contextually accurate data; Quick check - Consistency between expert and LLM-based evaluations.

## Architecture Onboarding
**Component Map:**
MME-Finance -> Expert Annotation -> LLM-based Evaluator -> Model Testing

**Critical Path:**
Image collection -> Categorization -> Expert annotation -> Question generation -> LLM-based evaluation -> Model testing

**Design Tradeoffs:**
- Bilingual support vs. annotation complexity
- Expert involvement vs. scalability
- LLM-based evaluation vs. human judgment accuracy

**Failure Signatures:**
- Poor performance on candlestick charts and technical indicators
- Difficulty with mobile photographs due to varied contexts
- Inconsistent results across different financial image types

**First Experiments:**
1. Test model performance on candlestick charts vs. technical indicators.
2. Evaluate model accuracy on mobile photographs compared to static financial images.
3. Compare results across different cultural contexts (e.g., English vs. Chinese questions).

## Open Questions the Paper Calls Out
None

## Limitations
- Potential dataset bias due to expert annotations from a specific region and professional background.
- Reliance on a single LLM-based evaluator may not capture all nuances of financial image understanding.
- Performance gaps between models on different image types suggest limited generalizability.

## Confidence
- High confidence in the benchmark's effectiveness for evaluating expert-level financial understanding.
- Medium confidence in the claim that current models universally struggle with financial tasks.

## Next Checks
1. Conduct cross-cultural validation by testing the benchmark with financial experts from diverse regions to assess generalizability.
2. Expand the dataset to include more varied financial images, such as those from different markets or time periods, to test model robustness.
3. Compare the LLM-based evaluator's performance against a broader range of human evaluators to ensure consistency and reduce potential bias.