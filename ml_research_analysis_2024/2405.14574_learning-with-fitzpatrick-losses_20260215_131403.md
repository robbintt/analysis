---
ver: rpa2
title: Learning with Fitzpatrick Losses
arxiv_id: '2405.14574'
source_url: https://arxiv.org/abs/2405.14574
tags:
- fitzpatrick
- loss
- losses
- function
- convex
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Fitzpatrick losses, a new family of convex
  loss functions for machine learning that are tighter lower bounds of Fenchel-Young
  losses while maintaining the same link function for predictions. The losses are
  derived from the Fitzpatrick function, a well-known tool in monotone operator theory.
---

# Learning with Fitzpatrick Losses

## Quick Facts
- arXiv ID: 2405.14574
- Source URL: https://arxiv.org/abs/2405.14574
- Reference count: 40
- Introduces Fitzpatrick losses - a new family of convex loss functions that are tighter lower bounds of Fenchel-Young losses while maintaining the same link function for predictions

## Executive Summary
This paper introduces Fitzpatrick losses, a new family of convex loss functions for machine learning derived from the Fitzpatrick function in monotone operator theory. The key innovation is that these losses provide tighter lower bounds than Fenchel-Young losses while preserving the same prediction link function. The authors provide two concrete instances - Fitzpatrick logistic loss and Fitzpatrick sparsemax loss - and demonstrate their effectiveness on label proportion estimation tasks across 11 datasets.

## Method Summary
The method involves constructing loss functions using the Fitzpatrick function of the subdifferential operator of a convex function Ω. The loss is defined as LF[∂Ω](y,θ) = F[∂Ω](y,θ) - ⟨y,θ⟩, where F[∂Ω] is the Fitzpatrick function. The authors show that these losses can be computed either directly using the Fitzpatrick function or equivalently as Fenchel-Young losses with a target-dependent generating function Ωy(y′) = Ω(y′) + DΩ(y,y′), where DΩ is the generalized Bregman divergence. For differentiable Ω, the losses have a particularly simple Mahalanobis-like form involving the Hessian of Ω.

## Key Results
- Fitzpatrick losses provide tighter lower bounds than Fenchel-Young losses while maintaining the same link function for prediction
- Experiments on 11 datasets show Fitzpatrick sparsemax loss is competitive with sparsemax loss, particularly for achieving sparse predictions
- Logistic and Fitzpatrick logistic losses show comparable performance in most cases
- The theoretical framework connects monotone operator theory with machine learning loss functions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fitzpatrick losses provide tighter lower bounds than Fenchel-Young losses while preserving the same prediction link function.
- Mechanism: The Fitzpatrick function F[∂Ω] refines the Fenchel-Young inequality by incorporating the generalized Bregman divergence DΩ, leading to LF[∂Ω](y,θ) = F[∂Ω](y,θ) - ⟨y,θ⟩. This construction ensures LF[∂Ω] ≤ LΩ⊕Ω∗ while maintaining the identity of indiscernibles (zero loss iff y = byΩ(θ)).
- Core assumption: The operator ∂Ω is maximal monotone, ensuring the Fitzpatrick function is well-defined and jointly convex.
- Evidence anchors: [abstract], [section], [corpus]

### Mechanism 2
- Claim: Fitzpatrick losses can be computed as Fenchel-Young losses with a target-dependent generating function Ωy(y′) = Ω(y′) + DΩ(y,y′).
- Mechanism: By Proposition 7, LF[∂Ω](y,θ) = LΩy⊕Ω∗y(y,θ), where Ωy incorporates the Bregman divergence from y to y′. This reformulation allows using existing Fenchel-Young machinery while achieving tighter bounds.
- Core assumption: The generalized Bregman divergence DΩ is convex in its second argument, ensuring Ωy is convex.
- Evidence anchors: [section], [section], [corpus]

### Mechanism 3
- Claim: For differentiable Ω, Fitzpatrick losses have a simple Mahalanobis-like form involving the Hessian of Ω.
- Mechanism: Proposition 2 shows that when Ω is twice differentiable, LF[∂Ω](y,θ) = ⟨y⋆ − y, ∇²Ω(y⋆)(y⋆ − y)⟩, where y⋆ solves ∇²Ω(y⋆)(y⋆ − y) = θ − ∇Ω(y⋆). This provides an efficient computational form.
- Core assumption: Ω is twice differentiable on its domain, ensuring the Hessian exists and is positive semidefinite.
- Evidence anchors: [section], [section], [corpus]

## Foundational Learning

- Concept: Fenchel-Young losses as primal-dual Bregman divergences
  - Why needed here: Understanding Fitzpatrick losses requires recognizing they are a refinement of Fenchel-Young losses, sharing the same link function but providing tighter bounds.
  - Quick check question: What is the key difference between primal-primal and primal-dual Bregman divergences, and how does this relate to Fenchel-Young losses?

- Concept: Fitzpatrick function and maximal monotone operators
  - Why needed here: The Fitzpatrick function is the core building block of Fitzpatrick losses, and understanding its properties (joint convexity, refinement of Fenchel-Young inequality) is essential.
  - Quick check question: How does the Fitzpatrick function F[∂Ω](y,θ) relate to the Fenchel conjugate Ω*(θ), and what inequality connects them?

- Concept: Generalized Bregman divergences and their role in loss functions
  - Why needed here: Fitzpatrick losses incorporate the generalized Bregman divergence DΩ, which modifies the generating function to achieve tighter bounds.
  - Quick check question: Express DΩ(y,y′) in terms of the supremum over θ′ ∈ ∂Ω(y′), and explain how this relates to the Fenchel-Young loss.

## Architecture Onboarding

- Component map: Input θ ∈ R^k -> Link function byΩ(θ) mapping to predictions y ∈ R^k -> Loss computation LF[∂Ω](y,θ) -> Gradient ∂θLF[∂Ω](y,θ) = y⋆F[∂Ω](y,θ) - y

- Critical path:
  1. Compute byΩ(θ) for predictions
  2. Evaluate LF[∂Ω](y,θ) using the appropriate formula (direct or via Ωy)
  3. Compute gradient ∂θLF[∂Ω](y,θ) for optimization
  4. Update model parameters using gradient descent or other optimizer

- Design tradeoffs:
  - Tightness vs. computational cost: Fitzpatrick losses are tighter but may require solving additional equations (e.g., Lambert W for logistic case)
  - Differentiability: Fitzpatrick losses are differentiable when Ω is strictly convex and DΩ is convex in y′, but this may not always hold
  - Memory: Computing y⋆F[∂Ω] requires storing intermediate values, potentially increasing memory usage

- Failure signatures:
  - Non-convergence during training: May indicate issues with differentiability or convexity of the loss
  - NaN or infinite values: Could result from numerical instability in solving equations (e.g., Lambert W) or from y not being in dom Ω
  - Poor performance: Might suggest the tighter bound of Fitzpatrick loss is not beneficial for the specific task or dataset

- First 3 experiments:
  1. Implement Fitzpatrick logistic loss for multiclass classification and compare convergence and accuracy against standard logistic loss on a small dataset (e.g., Iris)
  2. Test Fitzpatrick sparsemax loss for multi-label classification on a benchmark dataset (e.g., Scene) and evaluate sparsity of predictions compared to sparsemax loss
  3. Analyze the computational overhead of Fitzpatrick losses by measuring training time per epoch against Fenchel-Young losses on a medium-sized dataset (e.g., Cal500)

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical advantage of tighter bounds not clearly demonstrated to translate into practical performance gains across all tasks
- Experimental validation limited to label proportion estimation, leaving questions about performance on other tasks like standard classification or regression
- Performance comparison between logistic and Fitzpatrick logistic losses showing "comparable" results needs broader validation

## Confidence

- **High Confidence**: The theoretical foundation connecting Fitzpatrick functions to Fenchel-Young losses is mathematically rigorous. The claims about tightness of bounds are well-supported by convex analysis theory.
- **Medium Confidence**: The computational methods for evaluating Fitzpatrick losses (using Proposition 7 or the Mahalanobis form) are correctly stated, though implementation details may affect numerical stability.
- **Low Confidence**: The claim that Fitzpatrick losses are "a serious contender" to sparsemax loss is based on a single experimental setting.

## Next Checks

1. **Broader Task Evaluation**: Test Fitzpatrick losses on standard classification tasks (e.g., CIFAR-10) and regression tasks to verify if tightness advantages transfer beyond label proportion estimation.

2. **Ablation on Differentiability**: Systematically evaluate performance when Ω is non-differentiable (e.g., using total variation regularization) to understand when the Mahalanobis form breaks down.

3. **Computational Overhead Analysis**: Measure wall-clock training time and memory usage for Fitzpatrick vs. Fenchel-Young losses across varying dataset sizes to quantify the practical cost of tighter bounds.