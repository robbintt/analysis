---
ver: rpa2
title: 'I Could''ve Asked That: Reformulating Unanswerable Questions'
arxiv_id: '2407.17469'
source_url: https://arxiv.org/abs/2407.17469
tags:
- questions
- question
- unanswerable
- span
- reformulation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of reformulating unanswerable
  questions in document-grounded question answering. When users ask questions that
  cannot be answered by the documents, existing models identify them but do not assist
  in reformulating, limiting their utility.
---

# I Could've Asked That: Reformulating Unanswerable Questions

## Quick Facts
- **arXiv ID:** 2407.17469
- **Source URL:** https://arxiv.org/abs/2407.17469
- **Reference count:** 10
- **Primary result:** Current models fail to reformulate unanswerable questions effectively, with GPT-4 and Llama2-7B succeeding only 26% and 12% of the time respectively

## Executive Summary
This paper addresses a critical gap in document-grounded question answering systems: when users ask questions that cannot be answered by the provided documents, existing models identify them as unanswerable but fail to help users reformulate these questions. The authors introduce COULD ASK, a benchmark combining existing and newly collected datasets to study question reformulation in the presence of presupposition errors. Through comprehensive evaluation of state-of-the-art models and prompting methods, the paper reveals that current approaches achieve limited success rates (26% for GPT-4, 12% for Llama2-7B) and that 62% of failures involve merely rephrasing or repeating the original questions. The work highlights the need for improved methods to assist users in reformulating questions, which could significantly enhance user-LLM interactions.

## Method Summary
The authors introduce COULD ASK, a benchmark designed to evaluate question reformulation capabilities in document-grounded question answering systems. The benchmark combines existing datasets with newly collected data specifically targeting presupposition errors where questions contain assumptions not supported by the documents. The evaluation methodology employs human annotation to assess reformulation quality, with models like GPT-4 and Llama2-7B tested on their ability to generate valid reformulations. Error analysis is conducted to categorize failure modes, revealing that a majority of unsuccessful attempts simply rephrase or repeat the original unanswerable questions rather than addressing the underlying presupposition errors.

## Key Results
- GPT-4 successfully reformulates unanswerable questions only 26% of the time
- Llama2-7B achieves even lower success rate of 12% on question reformulation
- 62% of unsuccessful reformulations involve merely rephrasing or repeating the original questions
- Current models struggle significantly with addressing presupposition errors in unanswerable questions

## Why This Works (Mechanism)
None

## Foundational Learning
1. **Document-grounded question answering**: Understanding how QA systems retrieve and generate answers from document collections - needed to contextualize the reformulation problem within existing QA frameworks; quick check: verify understanding of retriever-generator architectures in document QA
2. **Presupposition errors**: Recognizing when questions contain assumptions not supported by source documents - crucial for identifying why questions are unanswerable; quick check: identify presupposition errors in sample unanswerable questions
3. **Question reformulation benchmarks**: Familiarity with evaluation methodologies for natural language generation tasks - necessary to assess the validity of the COULD ASK benchmark; quick check: understand benchmark construction and evaluation metrics
4. **Human annotation in NLP**: Knowledge of annotation guidelines and reliability measures - important for interpreting the reported success rates and error analysis; quick check: review inter-annotator agreement calculation methods
5. **LLM prompting strategies**: Understanding how different prompts affect model performance on reformulation tasks - relevant for interpreting why certain models perform better than others; quick check: compare prompt engineering techniques for generation tasks
6. **Error analysis in NLP**: Skills for categorizing and analyzing model failures - essential for understanding the 62% rephrasing failure mode; quick check: practice categorizing different types of generation errors

## Architecture Onboarding

**Component map:**
User Question -> Document Retrieval -> Unanswerability Detection -> Question Reformulation Model -> Reformulated Question

**Critical path:**
The critical path flows from the user question through document retrieval to identify unanswerability, then to the reformulation model which must generate a valid reformulation. The reformulation model represents the bottleneck, as evidenced by the low success rates (26% for GPT-4, 12% for Llama2-7B), indicating this component requires significant improvement to enhance overall system performance.

**Design tradeoffs:**
The benchmark focuses specifically on presupposition errors rather than all types of unanswerable questions, which provides focused evaluation but may limit generalizability. Human annotation is used for quality assessment, trading automation for potentially more nuanced evaluation, though this introduces subjectivity and reliability concerns. The use of both existing and new datasets balances resource efficiency with the need for targeted evaluation of reformulation capabilities.

**Failure signatures:**
Primary failure modes include 62% of cases where models merely rephrase or repeat the original question rather than addressing the presupposition error, indicating a fundamental misunderstanding of the reformulation task. Models struggle to identify and correct the unsupported assumptions within questions, often producing semantically equivalent but still unanswerable reformulations. The error analysis suggests models lack the reasoning capability to determine what information would make a question answerable given the document context.

**Three first experiments:**
1. Run the benchmark evaluation with multiple independent human annotators to calculate inter-annotator agreement scores and establish reliability of reformulation quality assessments
2. Test evaluated models on additional types of unanswerable questions beyond presupposition errors (e.g., temporal constraints, document coverage limitations) to assess generalizability
3. Implement and evaluate simple rule-based reformulation approaches as baselines to understand the complexity of the reformulation task

## Open Questions the Paper Calls Out
None

## Limitations
- Human annotation for reformulation quality introduces potential subjectivity and inter-annotator variability, with reliability metrics not fully disclosed
- Benchmark focuses specifically on presupposition errors, potentially limiting generalizability to other unanswerable question types
- Error analysis methodology may not capture the full complexity of reformulation challenges, affecting interpretation of the 62% rephrasing failure rate

## Confidence
- High confidence in the identification of reformulation problem as significant limitation in current document-grounded QA systems
- Medium confidence in specific reformulation success rates (26% GPT-4, 12% Llama2-7B) due to potential annotation subjectivity
- Medium confidence in characterization of reformulation failure modes based on available error analysis

## Next Checks
1. Replicate the human annotation process with multiple independent raters and calculate inter-annotator agreement scores to establish reliability of reformulation quality assessments

2. Test the evaluated models on additional types of unanswerable questions beyond presupposition errors to assess generalizability of the findings

3. Conduct a user study where actual end-users attempt to reformulate questions with and without model assistance, measuring both success rates and user satisfaction to validate the practical utility claims