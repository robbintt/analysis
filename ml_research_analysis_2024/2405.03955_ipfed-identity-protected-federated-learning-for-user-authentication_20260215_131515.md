---
ver: rpa2
title: 'IPFed: Identity protected federated learning for user authentication'
arxiv_id: '2405.03955'
source_url: https://arxiv.org/abs/2405.03955
tags:
- learning
- class
- data
- ipfed
- server
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: IPFed solves privacy vs. accuracy trade-off in federated face recognition.
---

# IPFed: Identity protected federated learning for user authentication

## Quick Facts
- arXiv ID: 2405.03955
- Source URL: https://arxiv.org/abs/2405.03955
- Authors: Yosuke Kaga; Yusei Suzuki; Kenta Takahashi
- Reference count: 22
- Primary result: IPFed achieves same accuracy as FedFace on LFW (99.22%), IJB-A (75.70% TAR@FAR=0.1%), and IJB-C (80.73% TAR@FAR=0.1%) while protecting class embeddings

## Executive Summary
IPFed introduces a privacy-preserving federated learning framework for user authentication that addresses the trade-off between privacy and accuracy in federated face recognition. The method uses random orthonormal projection to transform class embeddings, enabling spreadout regularization without leaking identity information to the learning server. Experiments on CASIA-WebFace demonstrate that IPFed achieves equivalent accuracy to state-of-the-art FedFace while providing stronger privacy guarantees.

## Method Summary
IPFed protects class embeddings by multiplying them with random orthonormal matrices before sending to the learning server. The learning server performs spreadout regularization on these transformed embeddings, which is mathematically equivalent to performing spreadout on the original embeddings when the transformation matrix is orthonormal. Secure aggregation prevents the server from recovering individual client updates, while the transformation parameters themselves are randomly generated and independent of personal data, making them useless for privacy attacks.

## Key Results
- IPFed achieves same accuracy as FedFace on LFW (99.22%), IJB-A (75.70% TAR@FAR=0.1%), and IJB-C (80.73% TAR@FAR=0.1%)
- Compared to fixed-class-embedding baseline, IPFed improves accuracy by ~2-3% on IJB-A, confirming spreadout benefits
- The method adds negligible computation/communication overhead, making it practical for privacy-preserving federated user authentication

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Random orthonormal projection preserves the geometric structure needed for spreadout regularization while hiding identity information
- Mechanism: By multiplying class embeddings by a random orthonormal matrix, the Euclidean distances between embeddings are preserved, allowing the server to optimize spreadout without knowing the actual identity-related information
- Core assumption: The transformation matrix is orthonormal (rt^T rt = I) and kept secret from the learning server
- Evidence anchors:
  - [abstract] "It protects class embeddings with random orthonormal projection, enabling equivalent spreadout regularization without leaking identity information"
  - [section III-A] "IPFed is equivalent to FedFace in terms of spreadout if the transformation parameter rt is an orthonormal matrix"
  - [corpus] No direct evidence found in corpus papers, but this is a well-established mathematical property
- Break condition: If the transformation matrix is not orthonormal or is leaked to the learning server

### Mechanism 2
- Claim: Secure aggregation prevents the learning server from recovering individual client updates
- Mechanism: By aggregating client updates before sending to the learning server, individual contributions are hidden, making it difficult to perform gradient inversion attacks
- Core assumption: The aggregation process is performed securely and the learning server only receives aggregated updates
- Evidence anchors:
  - [section III-B] "it is known that an attack to estimate training data from the updated global parameter θi t can be made difficult by using secure aggregation [11] to calculate θt+1 while keeping θi t secret"
  - [abstract] "Experiments on face image datasets show that IPFed can protect the privacy of personal data while maintaining the accuracy of the state-of-the-art method"
  - [corpus] No direct evidence found in corpus papers
- Break condition: If secure aggregation is not properly implemented or if individual updates are leaked

### Mechanism 3
- Claim: Transformation parameters are independent of personal data, making them useless for privacy attacks
- Mechanism: Since the transformation parameters are randomly generated and independent of the actual data, even if an attacker obtains them, they cannot be used to infer personal information
- Core assumption: The transformation parameters are truly random and independent of the data
- Evidence anchors:
  - [section III-B] "Since the transformation parameter is randomly generated data independent of the personal data, it is difficult to obtain any personal data"
  - [abstract] "Experiments on face image datasets show that IPFed can protect the privacy of personal data while maintaining the accuracy of the state-of-the-art method"
  - [corpus] No direct evidence found in corpus papers
- Break condition: If the transformation parameters are not truly random or are somehow correlated with the data

## Foundational Learning

- Concept: Random projection and orthonormal matrices
  - Why needed here: Understanding how random orthonormal projection preserves distances while hiding information is crucial for understanding the privacy mechanism
  - Quick check question: If a random orthonormal matrix R is applied to a vector v, how does ||Rv|| relate to ||v||?

- Concept: Secure aggregation in federated learning
  - Why needed here: Understanding how secure aggregation prevents the server from seeing individual client updates is important for understanding the privacy guarantees
  - Quick check question: What is the main difference between standard federated averaging and secure federated averaging?

- Concept: Spreadout regularization and class embedding optimization
  - Why needed here: Understanding how spreadout regularization works and why class embedding optimization improves accuracy is important for understanding the accuracy mechanism
  - Quick check question: In the context of face recognition, what is the purpose of spreadout regularization?

## Architecture Onboarding

- Component map: Parameter server -> Clients -> Learning server -> Shuffle server (optional)
- Critical path:
  1. Parameter server generates random orthonormal matrix rt
  2. Clients receive rt and transform their class embeddings
  3. Clients train on local data and send transformed class embeddings and model updates to learning server
  4. Learning server aggregates model updates and performs spreadout on transformed class embeddings
  5. Learning server sends back optimized transformed class embeddings
  6. Clients apply inverse transformation to get optimized class embeddings
- Design tradeoffs:
  - Privacy vs. accuracy: IPFed achieves the same accuracy as FedFace while providing better privacy
  - Communication overhead: Adding random projection adds negligible overhead
  - Computational overhead: Inner product computation on clients adds minimal overhead
- Failure signatures:
  - Privacy breach: If transformation parameters are leaked or not orthonormal
  - Accuracy degradation: If the random projection significantly distorts the class embedding space
  - Communication failure: If any of the three entities (clients, learning server, parameter server) cannot communicate properly
- First 3 experiments:
  1. Verify that random orthonormal projection preserves distances by applying random matrices to test vectors and measuring distance changes
  2. Test the complete IPFed pipeline on a small face dataset to verify both privacy and accuracy
  3. Perform a gradient inversion attack simulation to verify that individual client updates cannot be recovered from aggregated updates

## Open Questions the Paper Calls Out
No specific open questions were called out in the paper.

## Limitations
- Limited dataset diversity: Only tested on CASIA-WebFace with 1000 clients, lacking broader evaluation across different face recognition datasets
- Missing statistical validation: No confidence intervals or statistical significance tests provided for the reported accuracy improvements
- Unaddressed practical concerns: Does not discuss implementation vulnerabilities, side-channel attacks, or real-world deployment challenges

## Confidence
- Mathematical framework: High confidence - well-established techniques in secure multiparty computation
- Empirical results: Medium confidence - limited dataset diversity and lack of statistical validation
- Practical deployment: Low confidence - no discussion of real-world failure modes or implementation vulnerabilities

## Next Checks
1. **Statistical validation**: Re-run experiments across multiple random seeds and compute confidence intervals for all reported accuracy metrics to verify the claimed ~2-3% improvement over the fixed-class-embedding baseline is statistically significant.

2. **Privacy robustness testing**: Implement a simulated attack where an adversary attempts to recover client embeddings through gradient inversion on the aggregated updates, measuring how much information can be extracted despite the random projection.

3. **Scalability evaluation**: Test IPFed with varying numbers of clients (e.g., 100, 500, 2000) and dataset sizes to verify that the claimed "negligible computation/communication overhead" holds across different scales of federated learning deployments.