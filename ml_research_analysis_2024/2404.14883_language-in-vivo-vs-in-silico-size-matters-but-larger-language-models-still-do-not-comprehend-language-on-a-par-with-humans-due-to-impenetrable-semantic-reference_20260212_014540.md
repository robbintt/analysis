---
ver: rpa2
title: 'Language in Vivo vs. in Silico: Size Matters but Larger Language Models Still
  Do Not Comprehend Language on a Par with Humans Due to Impenetrable Semantic Reference'
arxiv_id: '2404.14883'
source_url: https://arxiv.org/abs/2404.14883
tags:
- language
- llms
- humans
- accuracy
- chatgpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates whether scaling up Large Language Models (LLMs)
  improves their ability to perform grammaticality judgments on par with humans. Testing
  three LLMs (Bard, ChatGPT-3.5, ChatGPT-4) on a validated task featuring anaphora,
  center embedding, comparatives, and negative polarity, the authors collected 1,200
  judgments and compared results with 80 human participants.
---

# Language in Vivo vs. in Silico: Size Matters but Larger Language Models Still Do Not Comprehend Language on a Par with Humans Due to Impenetrable Semantic Reference

## Quick Facts
- arXiv ID: 2404.14883
- Source URL: https://arxiv.org/abs/2404.14883
- Reference count: 40
- Key outcome: Scaling improves LLM grammaticality judgment accuracy but does not bridge the qualitative gap with human linguistic competence

## Executive Summary
This study evaluates whether increasing model size improves Large Language Models' (LLMs) ability to perform grammaticality judgments on par with humans. Testing three LLMs (Bard, ChatGPT-3.5, ChatGPT-4) on a validated task featuring anaphora, center embedding, comparatives, and negative polarity items, the authors collected 1,200 judgments and compared results with 80 human participants. While ChatGPT-4 outperformed smaller models and achieved higher accuracy than humans for grammatical sentences (93.5% vs. 76%), it still underperformed on ungrammatical sentences and showed less stability, especially with repeated prompts. The findings suggest that scaling improves performance but does not bridge the qualitative gap in linguistic competence between LLMs and humans, highlighting fundamental differences in language learning in silico versus in vivo.

## Method Summary
The study collected 1,200 model judgments (400 per model) and 80 human judgments on grammaticality using a validated task with four linguistic phenomena (anaphora, center embedding, comparatives, and negative polarity items). Each sentence was presented 10 times with binary yes/no prompts asking if the sentence was grammatically correct. Researchers measured accuracy (correct/incorrect judgments) and stability (oscillations and deviations across repetitions). Statistical analysis used generalized linear mixed-effects models with random intercepts for items nested within phenomena. Human judgments came from 80 participants, while model judgments were collected between April and June 2023.

## Key Results
- ChatGPT-4 achieved 93.5% accuracy on grammatical sentences versus 76% for humans, but only 43.8% on ungrammatical sentences versus 82% for humans
- ChatGPT-4 showed higher instability than humans, with 12.5% oscillating answers versus 9.6% for humans
- All three LLMs provided less stable answers for ungrammatical than grammatical sentences
- ChatGPT-4 reached above-chance performance for ungrammatical sentences, while smaller models did not

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger parameter counts improve LLM accuracy on grammatical sentences but not on ungrammatical ones.
- Mechanism: Increased model capacity allows better encoding of structural dependencies in well-formed input, but without theory formation or reference grounding, the model cannot reliably detect structural violations.
- Core assumption: The model's performance on grammatical sentences reflects better next-word prediction for lawful patterns, but performance on ungrammatical sentences is capped by the absence of explicit negative evidence during pre-training and lack of true semantic reference.
- Evidence anchors:
  - [abstract] "ChatGPT-4 outperforms humans only in one task condition, namely on grammatical sentences."
  - [section] "ChatGPT-4 reaches a very high albeit not perfect level of accuracy for grammatical sentences (93.5%)... the only LLM that performs above chance level for ungrammatical sentences."
- Break condition: If the model receives explicit negative evidence during training, the ungrammatical sentence performance could improve, breaking the assumed ceiling.

### Mechanism 2
- Claim: LLM response instability is higher on ungrammatical sentences and does not decrease with repetitions.
- Mechanism: Without a stable internalized grammar, the model's predictions for ill-formed inputs are driven by surface-level statistics, leading to fluctuating judgments that are not corrected by repeated exposure.
- Core assumption: Instability reflects the model's lack of robust syntactic knowledge; repeated exposure to the same ill-formed prompt does not create convergence because the model lacks a stable reference point for "ungrammaticality."
- Evidence anchors:
  - [abstract] "ChatGPT-4 wavers more than humans in its answers (12.5% vs. 9.6% likelihood of an oscillating answer)."
  - [section] "All three LLMs provide less stable/more oscillating answers for ungrammatical than grammatical sentences."
- Break condition: If the model is fine-tuned with stability objectives or negative examples, response consistency could improve, breaking the assumed instability.

### Mechanism 3
- Claim: Scaling alone cannot close the qualitative gap between LLM and human linguistic competence.
- Mechanism: The gap stems from fundamental differences in learning regimes (positive vs. positive+negative evidence, amount of data, theory formation vs. statistical prediction), which parameter scaling cannot resolve.
- Core assumption: Human language acquisition relies on innate biases, limited positive evidence, and theory formation, whereas LLMs depend on massive data and statistical pattern matching; these divergent foundations cannot be equalized by size alone.
- Evidence anchors:
  - [abstract] "It seems possible but unlikely that scaling alone can fix this issue."
  - [section] "scaling indeed matters... however, alternative explanations... are possible."
- Break condition: If future architectures incorporate mechanisms for theory formation or grounded reference, scaling could become sufficient, breaking the assumed irreconcilability.

## Foundational Learning

- Concept: Statistical learning vs. theory formation
  - Why needed here: The study contrasts LLM reliance on pattern matching with human reliance on forming linguistic theories; understanding this distinction is critical to interpreting why LLMs fail on ungrammaticality despite scaling.
  - Quick check question: Does a model that only memorizes co-occurrence statistics have the same competence as one that can form and test hypotheses about language rules?

- Concept: Positive vs. negative evidence in language acquisition
  - Why needed here: Humans learn from positive input alone; LLMs in this study also receive explicit negative examples via RLHF, yet still underperform, highlighting the limits of negative evidence without true comprehension.
  - Quick check question: If a model is explicitly told which sentences are ungrammatical, should it always correctly identify them, or can other factors still cause failure?

- Concept: Poverty of the stimulus
  - Why needed here: The paper invokes arguments about how humans generalize from limited data due to innate mechanisms, while LLMs require vast datasets; this frames why even huge models may not match human robustness.
  - Quick check question: If a child can learn grammar from sparse input, why does a model with internet-scale data still struggle with basic grammaticality judgments?

## Architecture Onboarding

- Component map: Sentence input → LLM prompt interface → Binary judgment output → Accuracy/stability recording → Statistical analysis pipeline
- Critical path: Input sentence → LLM prediction (yes/no) → Record accuracy and stability → Aggregate per condition → Compare with human baselines
- Design tradeoffs: Using only prompting avoids probability measurement complexities but limits insight into model internals; using probability would require access not available for GPT-4
- Failure signatures: Inconsistent judgments on repeated prompts, systematic bias toward "grammatical" responses, accuracy plateau despite parameter increase
- First 3 experiments:
  1. Replicate accuracy comparison across all four phenomena with a fixed prompt style.
  2. Measure stability by repeating each sentence multiple times and tracking oscillation/deviation rates.
  3. Test the effect of explicit negative evidence by prompting with "ungrammatical" labels and observing changes in accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can negative evidence alone (without positive evidence) enable LLMs to develop human-like language competence?
- Basis in paper: [explicit] The paper notes that while humans have access only to positive evidence, LLMs are exposed to both positive and negative evidence during training, yet still struggle with grammaticality judgments
- Why unresolved: Despite having access to negative evidence, ChatGPT-4 still underperforms humans on ungrammatical sentences and shows instability in responses
- What evidence would resolve it: A systematic study comparing LLM performance on grammaticality tasks when trained with only positive evidence versus both positive and negative evidence, controlling for model size and architecture

### Open Question 2
- Question: What is the minimum effective training data size for LLMs to achieve human-like language competence?
- Basis in paper: [explicit] The paper notes that humans can develop language competence with limited exposure, while LLMs require massive datasets that still don't bridge the performance gap
- Why unresolved: The relationship between training data size and language competence in LLMs remains unclear, as even the largest model tested (ChatGPT-4) shows significant differences from human performance
- What evidence would resolve it: Controlled experiments varying training data size while holding other factors constant, measuring performance on standardized linguistic tasks

### Open Question 3
- Question: How does the inability to form theories from data limit LLM language learning compared to humans?
- Basis in paper: [inferred] The paper suggests that humans learn language through forming hypotheses about input, while LLMs only perform data-based predictions without theory formation
- Why unresolved: The fundamental difference in learning mechanisms between humans and LLMs is hypothesized but not empirically tested
- What evidence would resolve it: Comparative studies examining whether LLMs can develop predictive theories from linguistic data in ways analogous to human hypothesis formation

## Limitations

- Temporal confounding due to LLM interface updates between testing periods, with ChatGPT-4 accuracy shifting from 96% to 93.5% within months
- Binary judgment task may not fully capture nuanced linguistic competence underlying human grammaticality judgments
- Cannot definitively isolate whether performance differences stem from architectural limitations versus training data composition or prompting strategies

## Confidence

- **High confidence**: The finding that ChatGPT-4 outperforms smaller models on grammatical sentences but still underperforms on ungrammatical sentences compared to humans.
- **Medium confidence**: The claim that scaling alone cannot bridge the qualitative gap between LLM and human linguistic competence.
- **Low confidence**: The assertion that models lack "true semantic reference" as the primary limiting factor.

## Next Checks

1. **Temporal validation**: Re-run the grammaticality judgment task with current versions of all tested models using identical prompts to assess whether observed performance differences persist or have shifted due to interface updates.

2. **Prompt sensitivity analysis**: Systematically vary prompt wording, formatting, and context across a subset of test sentences to determine whether performance differences between models are robust to prompting variations or reflect sensitivity to surface-level presentation.

3. **Intermediate scaling investigation**: Test additional models at parameter scales between GPT-3.5 and GPT-4 (e.g., GPT-3.5-turbo, Claude-2) to better characterize the relationship between model size and grammaticality judgment performance, particularly for ungrammatical sentences.