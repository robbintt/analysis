---
ver: rpa2
title: Comparing Knowledge Sources for Open-Domain Scientific Claim Verification
arxiv_id: '2402.02844'
source_url: https://arxiv.org/abs/2402.02844
tags:
- claims
- evidence
- claim
- retrieval
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates open-domain scientific claim verification
  using three knowledge sources (PubMed, Wikipedia, Google) and two retrieval techniques
  (BM25, semantic search). It keeps the evidence selection and verdict prediction
  components constant while varying the knowledge source and retrieval method.
---

# Comparing Knowledge Sources for Open-Domain Scientific Claim Verification
## Quick Facts
- arXiv ID: 2402.02844
- Source URL: https://arxiv.org/abs/2402.02844
- Reference count: 25
- Primary result: Wikipedia excels for everyday health claims, PubMed for specialized biomedical claims; semantic search improves recall, BM25 improves precision

## Executive Summary
This paper investigates open-domain scientific claim verification by systematically varying knowledge sources (PubMed, Wikipedia, Google) and retrieval techniques (BM25, semantic search) while keeping evidence selection and verdict prediction components constant. The study evaluates performance across four biomedical datasets: SCIFACT, PUBMEDQA, HEALTH FC, and COVERT. Results reveal that Wikipedia is superior for everyday health claims while PubMed excels for specialized biomedical claims, with semantic search providing better recall and BM25 offering higher precision.

## Method Summary
The authors evaluate three knowledge sources (PubMed, Wikipedia, Google) and two retrieval methods (BM25, semantic search) for open-domain scientific claim verification. They maintain constant evidence selection and verdict prediction components while varying only the knowledge source and retrieval method. The evaluation spans four biomedical datasets: SCIFACT for scientific claims, PUBMEDQA for biomedical questions, HEALTH FC for everyday health claims, and COVERT for COVID-related misinformation. Performance is measured across multiple metrics including precision, recall, and F1-score to comprehensively assess retrieval effectiveness.

## Key Results
- Wikipedia outperforms PubMed for everyday health claims while PubMed excels for specialized biomedical claims
- Semantic search achieves higher recall compared to BM25
- BM25 demonstrates superior precision over semantic search
- Performance varies significantly across different datasets and claim types

## Why This Works (Mechanism)
The performance differences between knowledge sources stem from their inherent characteristics: PubMed's peer-reviewed biomedical literature provides authoritative evidence for specialized claims, while Wikipedia's broader coverage better addresses general health queries. The retrieval method differences reflect fundamental trade-offs - semantic search captures conceptual similarity improving recall, while BM25's term-matching precision reduces noise.

## Foundational Learning
- **Knowledge source bias**: Understanding that PubMed and Wikipedia have different coverage biases is crucial for selecting appropriate sources. Quick check: Compare source coverage statistics for different claim types.
- **Retrieval method trade-offs**: BM25 vs semantic search represents the classic precision-recall trade-off. Quick check: Plot precision-recall curves for both methods across datasets.
- **Open-domain verification complexity**: Unlike closed-domain settings, open-domain verification requires handling diverse claim types and knowledge sources. Quick check: Analyze claim distribution across knowledge sources.

## Architecture Onboarding
- **Component map**: Claim -> Retrieval Method -> Knowledge Source -> Evidence Selection -> Verdict Prediction
- **Critical path**: The evidence retrieval and selection components form the critical path, as their output directly impacts verdict prediction accuracy.
- **Design tradeoffs**: Knowledge source selection involves trade-offs between domain expertise (PubMed) and general coverage (Wikipedia), while retrieval methods balance precision (BM25) and recall (semantic search).
- **Failure signatures**: Poor performance indicates either inappropriate knowledge source selection for the claim type or suboptimal retrieval method configuration.
- **First experiments**: 1) Compare single knowledge source performance across all datasets, 2) Ablate retrieval method to isolate source effects, 3) Analyze evidence quality differences between sources

## Open Questions the Paper Calls Out
None

## Limitations
- Knowledge source selection may introduce systematic biases based on dataset composition rather than inherent superiority
- Retrieval method evaluation lacks parameter tuning analysis and implementation details
- Dataset representation adequacy for real-world claim distributions is not assessed

## Confidence
- **High Confidence**: PubMed outperforms Wikipedia for specialized biomedical claims due to domain-specific indexing
- **Medium Confidence**: Semantic search improves recall while BM25 enhances precision, though magnitude varies
- **Low Confidence**: Wikipedia universally better for everyday health claims lacks sufficient evidence and may be dataset-dependent

## Next Checks
1. Cross-dataset validation using held-out datasets to verify generalizability of knowledge source performance differences
2. Knowledge source overlap analysis to determine if performance differences reflect complementary coverage or systematic biases
3. Retrieval method ablation study to isolate impact of method choice from implementation specifics