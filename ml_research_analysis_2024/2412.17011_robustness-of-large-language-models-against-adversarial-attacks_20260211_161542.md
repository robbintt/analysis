---
ver: rpa2
title: Robustness of Large Language Models Against Adversarial Attacks
arxiv_id: '2412.17011'
source_url: https://arxiv.org/abs/2412.17011
tags:
- prompts
- llms
- jailbreak
- robustness
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates the robustness of GPT-family models against
  adversarial attacks. Two methods are used: character-level text attacks, which introduce
  random character deletions in prompts across three sentiment classification datasets,
  and jailbreak prompts, which test the ability of models to bypass safety mechanisms
  using 1405 curated adversarial prompts.'
---

# Robustness of Large Language Models Against Adversarial Attacks

## Quick Facts
- arXiv ID: 2412.17011
- Source URL: https://arxiv.org/abs/2412.17011
- Reference count: 23
- Large language models show varying robustness to adversarial attacks, with character-level perturbations causing significant performance drops and jailbreak prompts exposing safety mechanism vulnerabilities

## Executive Summary
This paper evaluates the robustness of GPT-family models against two types of adversarial attacks: character-level text attacks and jailbreak prompts. The study finds that character-level perturbations cause significant drops in sentiment classification accuracy across all models, with GPT-4o achieving the highest original accuracy but also experiencing the largest performance decline. For jailbreak attacks, GPT-4o demonstrates superior safety detection capabilities at 95.7%, while GPT-3.5-turbo shows substantial vulnerability with only 48.9% detection rate. These findings highlight the need for improved adversarial training and enhanced safety mechanisms to ensure robust and secure LLM deployment.

## Method Summary
The study employs two distinct attack methodologies to evaluate LLM robustness. First, character-level text attacks introduce random character deletions with a 0.05 probability and maximum 10 deletions per sentence across three sentiment classification datasets (StanfordNLP/IMDB, Yelp Reviews, and SST-2). Second, jailbreak prompt evaluation tests the models' ability to detect and block 1405 curated adversarial prompts from JAILBREAKHUB. The research evaluates four GPT models (GPT-4o, GPT-4, GPT-4-turbo, and GPT-3.5-turbo) using classification accuracy on sentiment datasets and jailbreak prompt detection rates as primary metrics.

## Key Results
- Character-level perturbations cause significant accuracy drops across all models, with GPT-4o showing highest baseline performance but largest decline
- GPT-4o achieves 95.7% jailbreak prompt detection rate, while GPT-3.5-turbo detects only 48.9% of adversarial prompts
- All models demonstrate varying degrees of vulnerability to both attack types, highlighting the need for enhanced safety mechanisms

## Why This Works (Mechanism)
The effectiveness of these attacks stems from exploiting fundamental vulnerabilities in LLM processing. Character-level perturbations disrupt the tokenization process and semantic understanding by introducing noise that confuses the model's ability to correctly parse and interpret text. Jailbreak prompts work by crafting semantically complex inputs that attempt to bypass safety filters through carefully constructed language patterns that exploit the model's instruction-following capabilities while avoiding trigger phrases that typically activate safety mechanisms.

## Foundational Learning
- **Character-level perturbation attacks**: Why needed - to test model resilience against input noise; Quick check - implement random character deletion with specified parameters
- **Jailbreak prompt engineering**: Why needed - to evaluate safety mechanism effectiveness; Quick check - test model responses to known adversarial prompts
- **Sentiment classification evaluation**: Why needed - provides quantifiable metric for model performance under attack; Quick check - measure accuracy drop between clean and perturbed inputs
- **Model safety detection mechanisms**: Why needed - critical for understanding vulnerability to malicious inputs; Quick check - record detection rates for curated adversarial prompts
- **Tokenization process understanding**: Why needed - perturbations affect how text is converted to model input; Quick check - analyze tokenization changes with character deletions
- **API interaction configuration**: Why needed - affects model behavior and safety responses; Quick check - document specific API settings used in evaluation

## Architecture Onboarding
Component map: Sentiment datasets -> Character-level attack function -> GPT models -> Classification accuracy metrics
Critical path: Data preprocessing -> Attack implementation -> Model evaluation -> Result analysis
Design tradeoffs: Character-level attacks are simple but may not capture sophisticated adversarial techniques; Jailbreak evaluation provides safety assessment but uses a specific curated dataset
Failure signatures: Large accuracy drops indicate vulnerability to input perturbations; Low jailbreak detection rates suggest inadequate safety mechanisms
First experiments: 1) Test character-level attack parameters on a small dataset sample, 2) Verify jailbreak prompt detection with one model before full evaluation, 3) Compare results across different model versions to identify consistency patterns

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the robustness of LLMs to character-level perturbations scale with the size of the model?
- Basis in paper: [explicit] The paper evaluates four GPT models of varying sizes and notes differences in their robustness to character-level text attacks, with GPT-4o showing the highest original accuracy but also the largest performance drop.
- Why unresolved: The paper provides a comparative analysis but does not explore a direct correlation between model size and robustness, nor does it suggest how scaling might affect adversarial resilience.
- What evidence would resolve it: Systematic experiments varying model sizes and analyzing the relationship between size, baseline performance, and robustness to character-level perturbations.

### Open Question 2
- Question: What are the most effective strategies for enhancing the safety mechanisms of LLMs against jailbreak prompts?
- Basis in paper: [explicit] The paper highlights the varying effectiveness of safety mechanisms across different GPT models, with GPT-3.5-turbo showing significant vulnerability to jailbreak prompts.
- Why unresolved: While the paper identifies the need for improved safety mechanisms, it does not propose specific strategies or methods to enhance these mechanisms against sophisticated adversarial prompts.
- What evidence would resolve it: Development and testing of novel safety mechanisms or adversarial training techniques specifically designed to counter jailbreak prompts, followed by empirical evaluation of their effectiveness.

### Open Question 3
- Question: How do LLMs perform in real-world applications where inputs may contain both character-level perturbations and semantically complex adversarial prompts?
- Basis in paper: [inferred] The paper evaluates robustness separately for character-level attacks and jailbreak prompts, but does not address the combined impact of these adversarial techniques in practical scenarios.
- Why unresolved: The study focuses on isolated attack methods, leaving open the question of how models handle simultaneous or sequential exposure to different types of adversarial inputs.
- What evidence would resolve it: Experiments testing LLM performance in environments where inputs are subjected to both character-level perturbations and semantically complex adversarial prompts, assessing their combined effect on model robustness.

## Limitations
- Character-level perturbation methodology may not capture full spectrum of real-world adversarial techniques
- Evaluation limited to three sentiment classification datasets, potentially limiting generalizability
- Jailbreak attack assessment uses specific curated dataset (JAILBREAKHUB), results may vary with different prompt collections

## Confidence
- Model robustness measurements: Medium
- Jailbreak detection rates: Medium
- Generalizability of findings: Low

## Next Checks
1. Replicate character-level attack results using different perturbation strategies (insertions, substitutions) to test robustness across attack vectors
2. Evaluate the same models on additional task types beyond sentiment classification to assess generalizability
3. Test multiple model versions and API configurations to understand the stability of jailbreak detection rates across different deployment scenarios