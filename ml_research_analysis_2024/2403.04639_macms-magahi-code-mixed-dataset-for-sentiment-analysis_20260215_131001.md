---
ver: rpa2
title: 'MaCmS: Magahi Code-mixed Dataset for Sentiment Analysis'
arxiv_id: '2403.04639'
source_url: https://arxiv.org/abs/2403.04639
tags: []
core_contribution: This paper introduces MaCmS, the first sentiment analysis dataset
  for Magahi-Hindi-English code-mixed language. The dataset consists of 5,000 sentences
  and 2,642 language-specific spans annotated for sentiment polarity at both sentence
  and span levels.
---

# MaCmS: Magahi Code-mixed Dataset for Sentiment Analysis

## Quick Facts
- arXiv ID: 2403.04639
- Source URL: https://arxiv.org/abs/2403.04639
- Reference count: 13
- Primary result: First sentiment analysis dataset for Magahi-Hindi-English code-mixed language with 5,000 sentences and 2,642 language-specific spans

## Executive Summary
This paper introduces MaCmS, the first sentiment analysis dataset for Magahi-Hindi-English code-mixed language. The dataset consists of 5,000 sentences and 2,642 language-specific spans annotated for sentiment polarity at both sentence and span levels. The authors conduct linguistic and statistical analyses to understand code-mixing patterns and language preferences across sentiments. Baseline experiments using multilingual models (mBERT, XLM-R) and GenMA achieve F1-scores of 0.75 and 0.53 for sentence and span-level sentiment analysis respectively, demonstrating the dataset's utility and quality for low-resource code-mixed settings.

## Method Summary
The authors collected 5,000 code-mixed comments from social media, removed URLs, and annotated them for sentiment polarity at both sentence and span levels. They used Krippendorff's alpha (0.78) to measure inter-annotator agreement. The dataset was split into 70% training, 20% validation, and 10% test sets. Baseline models included multilingual transformers (mBERT, XLM-R) and GenMA, a generative morpheme model. Models were fine-tuned using standard hyperparameters with Hugging Face implementations.

## Key Results
- MaCmS is the first sentiment analysis dataset for Magahi-Hindi-English code-mixed language
- Multilingual transformers (mBERT, XLM-R) achieve F1-score of 0.75 for sentence-level sentiment analysis
- GenMA model achieves F1-score of 0.53 for span-level sentiment analysis
- Language preferences vary by sentiment: Magahi for positive/neutral, Hindi for negative sentiments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual transformer models (mBERT, XLM-R) perform better than monolingual models on code-mixed sentiment tasks because they capture cross-lingual representations useful for mixed language patterns.
- Mechanism: These models are pre-trained on diverse multilingual corpora, allowing them to align representations across languages and handle code-switching patterns even without explicit code-mixing data.
- Core assumption: The cross-lingual alignment in multilingual models is sufficiently robust to transfer to unseen code-mixed settings with related languages.
- Evidence anchors:
  - [abstract]: Baseline experiments using multilingual models (mBERT, XLM-R) and GenMA achieve F1-scores of 0.75 and 0.53 for sentence and span-level sentiment analysis respectively.
  - [section]: "While looking at the distribution of the tags across the language, we can deduce the following points" - indicates multilingual models can handle the language preferences.
  - [corpus]: Weak - corpus shows code-mixing between Magahi and Hindi, but no explicit evidence of multilingual model training data containing this specific pair.
- Break condition: If the related languages (Magahi and Hindi) share insufficient lexical or structural overlap, or if the code-mixing patterns are highly idiosyncratic to the low-resource context.

### Mechanism 2
- Claim: Language-specific span-level annotation improves model understanding of sentiment by isolating linguistic units within code-mixed text.
- Mechanism: Tokenizing code-mixed text into language-specific spans allows models to focus on the sentiment contribution of each language segment independently before aggregating for overall sentiment.
- Core assumption: Sentiment-bearing words and phrases are often language-specific and their meaning is preserved when isolated from mixed context.
- Evidence anchors:
  - [abstract]: The dataset provides 2,642 language-specific spans annotated for sentiment polarity at both sentence and span levels.
  - [section]: "We tokenised the comments into language-specific spans and tagged these spans with their polarity" - explicit span-level annotation process.
  - [corpus]: Moderate - the dataset includes span-level annotations, but evaluation shows lower F1-scores at span level (0.53) compared to sentence level (0.75), suggesting challenges.
- Break condition: If sentiment is heavily context-dependent and requires understanding of code-mixed discourse structure rather than isolated spans.

### Mechanism 3
- Claim: Generative morpheme models (GenMA) can capture code-mixed patterns by generating artificial character sequences that represent mixed language features.
- Mechanism: GenMA creates artificial morphemes that combine character-level information from multiple languages, allowing it to model code-switching at a sub-word level.
- Core assumption: Code-mixing patterns can be effectively captured through character-level representations rather than word or span-level features.
- Evidence anchors:
  - [abstract]: "We also provide some baseline models for sentiment analysis at the sentence and language-specific span levels" including GenMA.
  - [section]: "Generative Morphemes with Attention (GenMA) model... This model combines two convolution layers with one max pooling layer, a BiLSTM layer and an attention layer" - detailed model architecture.
  - [corpus]: Weak - no explicit evidence in corpus that GenMA's character-level approach is superior for this specific code-mixed dataset.
- Break condition: If the code-mixing patterns are too complex for character-level modeling or if morphological boundaries don't align across the mixed languages.

## Foundational Learning

- **Code-mixing and code-switching**: Why needed here: The dataset contains Magahi-Hindi-English code-mixed text, requiring understanding of how multiple languages interact within single utterances. Quick check: What distinguishes code-mixing from borrowing, and why does this distinction matter for sentiment analysis?
- **Multilingual transformer architectures**: Why needed here: The baseline experiments use mBERT and XLM-R, which require understanding of cross-lingual representation learning. Quick check: How do multilingual transformers handle languages they weren't explicitly trained on, and what limitations exist?
- **Inter-annotator agreement metrics**: Why needed here: The dataset quality depends on annotation reliability, measured using Krippendorff's alpha. Quick check: What does a Krippendorff's alpha of 0.78 indicate about annotation quality, and what thresholds are considered acceptable?

## Architecture Onboarding

- **Component map**: Data collection -> Preprocessing (URL removal) -> Train/Val/Test split (70/20/10) -> Model training (mBERT, XLM-R, GenMA) -> Evaluation (F1, Precision, Recall)
- **Critical path**: Data annotation quality -> Model selection -> Training configuration -> Evaluation metrics
- **Design tradeoffs**: Span-level vs sentence-level annotation (span-level has lower F1 but provides more granular data), model complexity vs performance (GenMA more complex but lower performance than XLM-R)
- **Failure signatures**: Low inter-annotator agreement, significant performance gap between sentence and span-level models, poor generalization to unseen code-mixing patterns
- **First 3 experiments**:
  1. Train XLM-R on full dataset with standard hyperparameters, evaluate on test set
  2. Train mBERT with different learning rates (1e-5 vs 2e-5) to find optimal configuration
  3. Compare GenMA performance with different kernel sizes in convolution layers to optimize character-level feature extraction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific linguistic features or patterns in Magahi-Hindi-English code-mixing are most predictive of sentiment polarity?
- Basis in paper: [explicit] The authors mention conducting linguistic analysis to understand code-mixing structure and identify patterns like interjection insertion, cultural references, and code-switching during strong emotions.
- Why unresolved: The paper provides examples but doesn't systematically quantify which linguistic features most strongly correlate with specific sentiments.
- What evidence would resolve it: A statistical analysis correlating specific linguistic features (e.g., interjections, cultural terms, specific code-switching patterns) with sentiment labels across the dataset.

### Open Question 2
- Question: How does the performance of multilingual models vary across different sentiment categories in this low-resource code-mixed setting?
- Basis in paper: [inferred] The authors report overall F1-scores but don't provide breakdown by sentiment category, and note poor performance particularly at span level due to limited training data.
- Why unresolved: The evaluation focuses on overall metrics without examining whether models perform differently across positive, negative, neutral, and mixed categories.
- What evidence would resolve it: Detailed per-category precision, recall, and F1-scores for each model, particularly comparing performance across the four sentiment labels.

### Open Question 3
- Question: What is the relationship between language choice and sentiment strength/intensity in Magahi-Hindi-English code-mixed communication?
- Basis in paper: [explicit] The authors note speakers use more Magahi for positive/neutral sentiments and more Hindi for negative sentiments, but don't measure intensity.
- Why unresolved: The statistical analysis identifies language preferences but doesn't examine whether certain languages correlate with stronger sentiment expressions.
- What evidence would resolve it: Analysis of intensity markers (exclamation points, intensifiers, capitalization) across different language spans and their correlation with specific sentiment categories.

## Limitations
- Span-level sentiment analysis performs significantly worse (F1=0.53) than sentence-level (F1=0.75), suggesting the task may exceed current model capacity with limited training data
- GenMA model underperforms compared to transformer baselines despite its more complex architecture
- Small dataset size (750 sentences for training) limits statistical power of linguistic pattern observations

## Confidence

**High Confidence**: The dataset introduces the first Magahi-Hindi-English code-mixed sentiment analysis resource, and the baseline transformer model performances (mBERT and XLM-R achieving F1=0.75 for sentence-level) are well-supported by the experimental results. The inter-annotator agreement (Krippendorff's alpha=0.78) provides reasonable validation of annotation quality.

**Medium Confidence**: The claim that multilingual transformers perform well on code-mixed data due to cross-lingual representation learning is plausible but not directly tested against monolingual alternatives in this work. The linguistic patterns observed across sentiments are interesting but require larger sample sizes for robust generalization.

**Low Confidence**: The effectiveness of GenMA for code-mixed sentiment analysis is questionable given its substantially lower performance (F1=0.53) compared to transformer baselines, and the paper provides limited analysis of why this architecture underperforms.

## Next Checks
1. **Model Architecture Comparison**: Conduct controlled experiments comparing mBERT and XLM-R against monolingual models (e.g., Hindi BERT) to isolate whether the multilingual advantage stems from cross-lingual capabilities or simply better general language modeling.
2. **Data Augmentation Study**: Evaluate whether techniques like back-translation, synthetic code-mixing generation, or data augmentation can improve span-level performance, given the current 0.53 F1 score suggests the task exceeds current model capacity.
3. **Linguistic Pattern Validation**: Expand the linguistic analysis by collecting additional data to test whether the observed language preferences across sentiments (e.g., Hindi dominance in certain sentiment categories) hold across larger samples and different domains of Magahi-Hindi-English code-mixed communication.