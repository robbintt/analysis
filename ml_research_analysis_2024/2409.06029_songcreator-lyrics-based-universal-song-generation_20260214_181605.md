---
ver: rpa2
title: 'SongCreator: Lyrics-based Universal Song Generation'
arxiv_id: '2409.06029'
source_url: https://arxiv.org/abs/2409.06029
tags:
- song
- accompaniment
- generation
- songcreator
- vocals
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SongCreator introduces a dual-sequence language model (DSLM) to
  separately model vocals and accompaniment within songs, using a bidirectional cross-attention
  module to capture their mutual influence. An attention mask strategy enables universal
  song generation tasks including lyrics-to-song, vocals-to-song, and song editing.
---

# SongCreator: Lyrics-based Universal Song Generation

## Quick Facts
- arXiv ID: 2409.06029
- Source URL: https://arxiv.org/abs/2409.06029
- Reference count: 40
- Key outcome: SongCreator achieves state-of-the-art performance across eight song generation tasks using a dual-sequence language model with bidirectional cross-attention, trained on 8,500 hours of song data.

## Executive Summary
SongCreator introduces a dual-sequence language model (DSLM) that separately models vocals and accompaniment within songs, using bidirectional cross-attention to capture their mutual influence. The model supports universal song generation tasks including lyrics-to-song, vocals-to-song, and song editing through an attention mask strategy. Trained on 8,500 hours of song data, SongCreator achieves state-of-the-art performance across eight tasks, surpassing baselines by significant margins in lyrics-to-song and lyrics-to-vocals. The architecture enables independent control of vocals and accompaniment through prompts and demonstrates robust zero-shot voice cloning capabilities.

## Method Summary
SongCreator employs a dual-sequence language model (DSLM) with separate decoders for vocals and accompaniment, connected through bidirectional cross-attention layers. The model uses an attention mask strategy to enable multiple generation tasks within a unified framework. It is trained on 8,500 hours of song data (270,000 songs) segmented into 30-second clips, with vocals and accompaniment extracted using Demucs source separation. The system processes semantic tokens from BEST-RQ through the DSLM and generates final audio using a latent diffusion model.

## Key Results
- Achieves state-of-the-art performance across eight song generation tasks
- Significant improvements over baselines in lyrics-to-song and lyrics-to-vocals tasks
- Supports independent control of vocals and accompaniment through prompts
- Demonstrates robust zero-shot voice cloning capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dual-sequence language model (DSLM) enables independent modeling of vocals and accompaniment while capturing their mutual influence through bidirectional cross-attention.
- Mechanism: DSLM uses separate decoders for vocals and accompaniment with bidirectional cross-attention (BCA) layers that allow each sequence to attend to the other's context, enabling harmonic coordination.
- Core assumption: Treating vocals and accompaniment as separate but interrelated sequences is more effective than modeling them as a single entity.
- Evidence anchors:
  - [abstract] "SongCreator features a novel dual-sequence language model (DSLM), which utilizes two decoders to separately model vocals and accompaniment information, and employs a dynamic bidirectional cross-attention module to capture the influences between these two sequences."
  - [section 3.2] "To understand and model this interrelationship, we introduce a bidirectional cross-attention (BCA) layer, which consists of two symmetrical cross-attention mechanisms."
- Break condition: If the mutual influence between vocals and accompaniment is too weak or too strong, the bidirectional attention may not improve performance.

### Mechanism 2
- Claim: The attention mask strategy enables universal song generation by allowing different task configurations within a single model.
- Mechanism: Different attention mask configurations (causal, non-causal, bidirectional, A2V, V2A, none) control what tokens can attend to each other, enabling tasks like lyrics-to-song, vocals-to-song, and song editing.
- Core assumption: A single model can handle multiple generation tasks through appropriate attention mask configurations.
- Evidence anchors:
  - [abstract] "we design a special attention mask strategy for DSLM, which enables SongCreator to complete song generation tasks of various forms, such as editing, understanding and generation in a unified manner."
  - [section 3.3] "By employing different mask strategy for SA and BCA, as well as the input format, a single SongCreator can achieve competitive performance on multiple song generation tasks."
- Break condition: If task-specific architectures outperform the unified approach significantly, the mask strategy may not be optimal.

### Mechanism 3
- Claim: Multi-task training improves the model's composition, arrangement, and comprehension abilities across different song generation tasks.
- Mechanism: Training on multiple tasks (song generation from lyrics, song generation from pre-determined accompaniment/vocals, and song editing) allows the model to learn shared information and relationships.
- Core assumption: Different song generation tasks share underlying compositional and arrangement knowledge that can be learned simultaneously.
- Evidence anchors:
  - [section 3.4] "we investigate a multi-task training setup, in which the model is trained on several tasks to enhance its composition, arrangement, and comprehension abilities."
  - [section 4.3] "Through previous experiments, we can find that multi-task training significant improves most tasks, especially in lyrics-to-song."
- Break condition: If tasks conflict too strongly, multi-task training may hurt performance on individual tasks.

## Foundational Learning

- Concept: Self-attention and cross-attention mechanisms in transformer architectures
  - Why needed here: These mechanisms form the foundation for how the model processes and relates different parts of the song (vocals, accompaniment, lyrics)
  - Quick check question: How does self-attention differ from cross-attention in terms of what it attends to?

- Concept: Bidirectional versus causal attention patterns
  - Why needed here: Different attention patterns (causal for generation, bidirectional for understanding) enable different capabilities within the same model
  - Quick check question: What is the key difference between causal and non-causal attention masks?

- Concept: Vector quantization and discrete representation learning
  - Why needed here: The model uses BEST-RQ with vector quantization to create semantic tokens from audio, bridging continuous audio and discrete language modeling
  - Quick check question: Why is vector quantization necessary when working with audio in a language model framework?

## Architecture Onboarding

- Component map:
  Lyrics Encoder (4-layer Transformer) → Vocal Decoder (8-layer DSLM) ↔ Accompaniment Decoder (8-layer DSLM) → Song Decoder (4-layer Transformer) → Latent Diffusion Model

- Critical path: Lyrics → Lyrics Encoder → Dual Decoders with BCA → Song Decoder → LDM → Generated Song
- Design tradeoffs: Separate decoders for vocals/accompaniment vs. single decoder for full song; multi-task training vs. task-specific specialization
- Failure signatures: Poor harmony between vocals and accompaniment suggests BCA issues; low quality suggests LDM problems; task-specific failures suggest mask strategy issues
- First 3 experiments:
  1. Test DSLM with only self-attention (no BCA) on lyrics-to-song to measure impact of bidirectional cross-attention
  2. Test with only causal masks vs. mixed masks to measure impact of non-causal context
  3. Test single-task training vs. multi-task training on lyrics-to-song to measure knowledge transfer benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the mutual influence between vocals and accompaniment affect the perceived naturalness and harmony of generated songs compared to models that treat them as a single sequence?
- Basis in paper: [explicit] The paper highlights that SongCreator uses a dual-sequence language model (DSLM) with bidirectional cross-attention to capture the mutual influence between vocals and accompaniment, addressing the limitation of previous models that treat them as a single entity.
- Why unresolved: The paper provides subjective evaluations for harmony but does not directly compare the mutual influence modeling against models that ignore this interaction in terms of naturalness and harmony.
- What evidence would resolve it: A direct comparison between SongCreator and a baseline model without bidirectional cross-attention, specifically evaluating naturalness and harmony metrics.

### Open Question 2
- Question: What is the impact of the attention mask strategy on the model's ability to generalize across different song generation tasks?
- Basis in paper: [explicit] The paper describes the attention mask strategy as enabling universal song generation tasks and supporting multi-task training, but does not provide quantitative evidence of its impact on generalization.
- Why unresolved: While the paper demonstrates performance across multiple tasks, it does not isolate the contribution of the attention mask strategy to task generalization.
- What evidence would resolve it: An ablation study comparing SongCreator's performance with and without the attention mask strategy across all supported tasks, measuring task-specific metrics.

### Open Question 3
- Question: How does the quality of the BEST-RQ semantic tokens affect the overall fidelity and musicality of the generated songs?
- Basis in paper: [inferred] The paper relies on BEST-RQ for extracting semantic tokens, which are then used by the latent diffusion model, but does not evaluate the impact of token quality on the final output.
- Why unresolved: The paper assumes BEST-RQ provides sufficient semantic information but does not validate this assumption or explore alternative tokenization methods.
- What evidence would resolve it: A comparison between SongCreator and a variant using different tokenization methods (e.g., raw audio, alternative VQ methods) measuring FAD, MCD, and subjective musicality scores.

## Limitations
- Dataset composition and genre/language coverage remain unspecified, raising generalization concerns
- Several critical implementation details are missing or insufficiently specified
- Evaluation focuses on controlled tests rather than real-world applications

## Confidence

**High Confidence (★★★☆☆):**
- The dual-sequence architecture with separate vocals and accompaniment decoders is technically sound and well-implemented
- The attention mask strategy effectively enables multiple generation tasks within a unified framework
- The overall approach to separating vocals and accompaniment modeling represents a valid technical direction

**Medium Confidence (★★☆☆☆):**
- The claimed SOTA performance improvements over baselines, particularly for lyrics-to-song and lyrics-to-vocals tasks
- The effectiveness of multi-task training in improving compositional and arrangement abilities
- The robustness of zero-shot voice cloning capabilities

**Low Confidence (★☆☆☆☆):**
- Generalization claims to "all music styles" without empirical validation across diverse genres
- The model's ability to maintain musical coherence in extended compositions
- Claims about "universal" applicability across all song generation scenarios

## Next Checks
1. **Cross-Genre Performance Validation:** Conduct systematic testing of SongCreator across at least 5-7 distinct musical genres (pop, classical, jazz, rock, electronic, folk, hip-hop) to verify the claimed universal applicability. Measure performance degradation patterns and identify genre-specific limitations.

2. **Ablation Study on Bidirectional Cross-Attention:** Implement and evaluate a controlled ablation where the BCA module is progressively disabled or simplified. Compare lyrics-to-song performance with: (a) full DSLM with BCA, (b) DSLM with only self-attention, (c) traditional single-sequence modeling. This will quantify the actual contribution of the bidirectional mechanism.

3. **Long-Form Generation Coherence Test:** Evaluate the model's ability to maintain musical coherence and quality over extended durations (5+ minutes) rather than just 30-second clips. Measure metrics like harmonic consistency, thematic development, and listener fatigue across progressively longer generated pieces.