---
ver: rpa2
title: Critical Phase Transition in Large Language Models
arxiv_id: '2406.05335'
source_url: https://arxiv.org/abs/2406.05335
tags:
- propn
- noun
- phase
- correlation
- sequences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides numerical evidence that a critical phase transition
  occurs in large language models (LLMs) when varying the temperature parameter. The
  authors analyze statistical properties of texts generated by GPT-2 at different
  temperatures, focusing on correlations between part-of-speech (POS) tags.
---

# Critical Phase Transition in Large Language Models

## Quick Facts
- **arXiv ID**: 2406.05335
- **Source URL**: https://arxiv.org/abs/2406.05335
- **Reference count**: 40
- **Primary result**: LLMs exhibit critical phase transitions when varying temperature parameter

## Executive Summary
This paper presents compelling numerical evidence that large language models undergo a critical phase transition when the temperature parameter is varied. Through systematic analysis of part-of-speech tag correlations in GPT-2-generated text, the authors identify distinct behavioral regimes below and above a critical temperature T_c ≈ 1. Below T_c, texts exhibit long-range correlations and repetitive structures, while above T_c, correlations decay to zero and repetitive patterns disappear. At the critical point, the correlation follows power-law decay and shows critical slowing down, drawing strong parallels with natural language phenomena.

## Method Summary
The authors analyze statistical properties of text generated by GPT-2 across different temperature settings. They focus on correlations between part-of-speech (POS) tags to characterize the statistical structure of generated sequences. By systematically varying temperature from low to high values, they observe transitions in correlation behavior, identifying a critical temperature where power-law decay emerges. The analysis includes measurements of correlation functions, detection of repetitive structures, and comparison with natural language datasets to establish the relevance of the observed phenomena.

## Key Results
- Below critical temperature T_c ≈ 1, generated texts show long-range correlations and repetitive structures
- Above T_c, correlations decay to zero and repetitive structures disappear
- At critical point, correlation follows power-law decay and system exhibits critical slowing down, similar to natural languages

## Why This Works (Mechanism)
The critical phase transition emerges from the competition between entropy and energy in the language model's probability distribution. At low temperatures, the model strongly favors high-probability sequences, leading to repetitive patterns and long-range correlations as the system gets trapped in local energy minima. At high temperatures, the entropy term dominates, producing more diverse outputs with shorter-range correlations. At the critical temperature, the system balances these competing forces, resulting in scale-invariant behavior characterized by power-law correlations.

## Foundational Learning

**Statistical Physics Concepts**: Understanding phase transitions, critical phenomena, and correlation functions is essential for interpreting the observed behavior. These concepts provide the theoretical framework for identifying critical points and characterizing transitions.

*Why needed*: The critical point identification and interpretation of power-law correlations directly rely on statistical physics principles.

*Quick check*: Verify understanding of second-order phase transitions and critical exponents through simple Ising model examples.

**Information Theory**: Concepts like entropy, mutual information, and information bottlenecks help explain why certain temperature ranges produce qualitatively different text properties.

*Why needed*: The temperature parameter effectively controls the entropy-energy tradeoff in the model's output distribution.

*Quick check*: Calculate entropy changes in probability distributions when varying temperature in simple generative models.

**Linguistics and POS Tagging**: Understanding part-of-speech structures and their role in language organization provides context for why POS correlations are meaningful statistical indicators.

*Why needed*: POS tags serve as proxies for linguistic structure, making them interpretable measures of text coherence and organization.

*Quick check*: Examine POS tag sequences in natural text to identify typical correlation patterns.

## Architecture Onboarding

**Component Map**: GPT-2 architecture -> Temperature parameter -> Softmax sampling -> Generated text sequence -> POS tag extraction -> Correlation analysis

**Critical Path**: Temperature setting → Softmax temperature scaling → Token probability distribution → Sampling mechanism → Output sequence → POS tagging → Correlation computation

**Design Tradeoffs**: The study uses POS tags as interpretable statistical measures, trading computational simplicity for potential loss of semantic information. Temperature serves as a single control parameter, providing clear experimental control but potentially missing multi-dimensional phase behavior.

**Failure Signatures**: Non-monotonic correlation behavior, inconsistent critical temperature identification across runs, or failure to observe power-law decay at proposed critical point would indicate problems with the analysis or model behavior.

**First Experiments**:
1. Generate text sequences at multiple temperature points around T_c ≈ 1 and compute POS tag correlations
2. Vary model size while keeping temperature constant to test scaling behavior
3. Compare correlation functions for different POS tag categories (nouns, verbs, adjectives) to identify universal vs. category-specific behavior

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis focuses exclusively on GPT-2 architecture, limiting generalizability to other models
- Part-of-speech tags as proxy measures may not capture all relevant linguistic structures
- Critical temperature identification is empirical without theoretical derivation from fundamental model parameters

## Confidence

**Major Claims Confidence Assessment**:
- **High confidence**: Existence of distinct behavioral regimes below and above critical temperature
- **Medium confidence**: Power-law correlation decay at critical point indicating true criticality
- **Medium confidence**: Critical slowing down phenomenon observed in LLM generation

## Next Checks

1. Replicate analysis across multiple model architectures (GPT-3, LLaMA, OPT) to test universality of critical behavior
2. Apply alternative statistical measures (entropy rate, permutation entropy, information bottleneck) to verify robustness of findings beyond POS tag correlations
3. Conduct ablation studies varying model size and training data to determine how these factors influence critical temperature location and transition sharpness