---
ver: rpa2
title: Handling Ontology Gaps in Semantic Parsing
arxiv_id: '2406.19537'
source_url: https://arxiv.org/abs/2406.19537
tags:
- symbols
- hallucination
- detection
- dataset
- ontology
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces the Hallucination Simulation Framework (HSF)
  to study and mitigate hallucinations in Neural Semantic Parsing (NSP) models. The
  framework forces models to hallucinate by excluding certain ontology symbols during
  training, enabling controlled analysis of model behavior under ontology gaps.
---

# Handling Ontology Gaps in Semantic Parsing

## Quick Facts
- arXiv ID: 2406.19537
- Source URL: https://arxiv.org/abs/2406.19537
- Reference count: 9
- Key outcome: HDM with activations and confidence score achieves up to 21% improvement in F1-score for ontology gap detection

## Executive Summary
This work introduces the Hallucination Simulation Framework (HSF) to study and mitigate hallucinations in Neural Semantic Parsing (NSP) models. The framework forces models to hallucinate by excluding certain ontology symbols during training, enabling controlled analysis of model behavior under ontology gaps. To detect these hallucinations, the authors propose the Hallucination Detection Model (HDM), which leverages multiple signals including model activations, confidence scores, and Monte Carlo Dropout. Experiments on the KQA-Pro dataset show that the HDM with activations and confidence score achieves up to 21% improvement in F1-score for ontology gap detection, 1% for NSP error detection, and 24% for out-of-domain detection compared to baselines.

## Method Summary
The Hallucination Simulation Framework (HSF) induces hallucinations by training NSP models with only a subset of ontology symbols (O_known) while reserving others (O_unknown) for testing. This creates controlled ontology gaps where the model must predict symbols it hasn't seen during training. The Hallucination Detection Model (HDM) is then trained to detect these hallucinations using three key features: model activations from the NSP decoder, confidence scores from the prediction distribution, and Monte Carlo Dropout uncertainty estimates. The HDM is trained on a Hallucination Detection Dataset (HDD) constructed by splitting ontology symbols into disjoint sets for training, development, and testing.

## Key Results
- HDM with activations and confidence score achieves 21% improvement in F1-score for ontology gap detection
- HDM achieves 1% improvement in F1-score for NSP error detection
- HDM achieves 24% improvement in F1-score for out-of-domain detection
- Monte Carlo Dropout contributes to uncertainty estimation but is less effective than activations and confidence score
- HDM outperforms simple thresholding baselines across all three detection tasks

## Why This Works (Mechanism)
The approach works by creating a systematic way to study how NSP models behave when they encounter symbols outside their training ontology. By forcing the model to hallucinate through the HSF, the researchers can observe and analyze the specific patterns that emerge when models make predictions on unseen symbols. The HDM leverages these patterns by extracting meaningful signals from the model's internal representations (activations), its confidence in predictions, and its uncertainty estimates (Monte Carlo Dropout). This multi-signal approach allows the HDM to distinguish between legitimate predictions within the known ontology and hallucinations when encountering unknown symbols.

## Foundational Learning

**Neural Semantic Parsing (NSP)**
*Why needed:* The core task being addressed, where natural language questions are converted to logical forms
*Quick check:* Verify understanding of how NSP differs from general semantic parsing by focusing on the closed-ontology constraint

**Hallucination in NSP**
*Why needed:* Understanding the specific problem of models generating symbols not present in the ontology
*Quick check:* Can identify examples of hallucinations in NSP outputs versus correct predictions

**Monte Carlo Dropout**
*Why needed:* Technique for estimating model uncertainty through multiple stochastic forward passes
*Quick check:* Understand how dropout at inference time creates a distribution of predictions rather than a single point estimate

**Ontology Gap Detection**
*Why needed:* The specific problem of identifying when a model is predicting symbols outside its known ontology
*Quick check:* Can explain why ontology gaps are problematic in closed-ontology NSP tasks

**Feature Extraction from NLP Models**
*Why needed:* Understanding how to extract meaningful signals (activations, confidence) from transformer-based models
*Quick check:* Can describe what decoder activations represent and how they might indicate uncertainty

## Architecture Onboarding

**Component Map:** BART NSP Model -> Feature Extractor -> HDM -> Hallucination Detection

**Critical Path:** The HSF trains the NSP model on O_known symbols, then the HDM is trained using features extracted from this NSP model's predictions on the HDD. The HDM uses activations, confidence scores, and Monte Carlo Dropout outputs as input features to classify whether predictions are hallucinations.

**Design Tradeoffs:** The approach trades off between training complexity (multiple models) and detection accuracy. Using multiple features (activations, confidence, dropout) increases detection performance but also increases computational overhead and model complexity.

**Failure Signatures:** The HDM may fail when ontology symbols in O_unknown have similar representations to O_known symbols, making it difficult to distinguish hallucinations. Performance may degrade if the confidence score threshold is not optimally selected or if activation pooling loses critical information.

**First Experiments:**
1. Train HDM with only confidence scores as features to establish baseline performance
2. Add Monte Carlo Dropout features to assess contribution to detection accuracy
3. Add decoder activations to evaluate the full HDM architecture

## Open Questions the Paper Calls Out
None

## Limitations
- Restricted generalizability to NSP tasks with different domain characteristics or ontology structures
- Assumes fixed ontology size, not reflecting real-world scenarios with evolving or large ontologies
- Does not thoroughly explore optimal configurations across different error types
- Confidence score threshold selection process not fully detailed

## Confidence
High: The core methodology of using SFS to induce hallucinations and HDM for detection is well-justified and technically sound.
Medium: The generalizability of the approach to other NSP tasks and real-world applications with larger, dynamic ontologies remains uncertain.
Low: The practical impact of the 1% improvement in NSP error detection is questionable given the potential noise in the metric.

## Next Checks
1. Test HDM on additional NSP datasets with different domain characteristics to assess generalizability beyond KQA-Pro and TOP v2.
2. Evaluate HDM performance when ontology sizes are scaled up to hundreds or thousands of symbols, as this represents more realistic NSP scenarios.
3. Conduct ablation studies with different confidence score thresholds and activation pooling methods to identify optimal configurations for each error type.