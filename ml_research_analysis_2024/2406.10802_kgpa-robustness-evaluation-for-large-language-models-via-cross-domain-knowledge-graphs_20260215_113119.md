---
ver: rpa2
title: 'KGPA: Robustness Evaluation for Large Language Models via Cross-Domain Knowledge
  Graphs'
arxiv_id: '2406.10802'
source_url: https://arxiv.org/abs/2406.10802
tags:
- robustness
- adversarial
- language
- prompts
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a knowledge graph-based framework, KGPA, to
  evaluate the robustness of large language models under adversarial attacks. The
  framework generates prompts from knowledge graph triplets, creates adversarial prompts
  by poisoning, and assesses model robustness through adversarial attack results.
---

# KGPA: Robustness Evaluation for Large Language Models via Cross-Domain Knowledge Graphs

## Quick Facts
- **arXiv ID:** 2406.10802
- **Source URL:** https://arxiv.org/abs/2406.10802
- **Reference count:** 40
- **Primary result:** KGPA framework shows GPT-4-turbo has highest adversarial robustness among ChatGPT family models, with domain-specific variations observed across knowledge graphs

## Executive Summary
This paper introduces KGPA (Knowledge Graph-based Prompt Attack), a framework for evaluating the robustness of large language models (LLMs) under adversarial attacks using knowledge graphs. The framework generates original prompts from knowledge graph triplets, creates adversarial prompts through poisoning, and assesses model robustness through adversarial attack results. Experiments demonstrate that among ChatGPT family models, GPT-4-turbo exhibits the highest adversarial robustness, followed by GPT-4o and GPT-3.5-turbo. The study also reveals that LLM robustness varies across different knowledge graph domains, with performance differences observed between general and specialized knowledge graphs.

## Method Summary
The KGPA framework employs a multi-module architecture to evaluate LLM robustness. It begins by generating original prompts from knowledge graph triplets using either template-based or LLM-based strategies. The framework then creates adversarial prompts through a few-shot attack approach, where example prompts are generated to satisfy semantic similarity while achieving different classification outcomes. A Prompt Refinement Engine (PRE) filters generated prompts based on an LLMScore metric, ensuring semantic similarity and quality. The framework evaluates robustness using metrics including Natural Response Accuracy (NRA), Robust Response Accuracy (RRA), and Adversarial Success Rate (ASR). Experiments were conducted across multiple knowledge graph datasets (T-REx, Google-RE, UMLS, WikiBio) using ChatGPT family models.

## Key Results
- GPT-4-turbo demonstrates highest adversarial robustness, followed by GPT-4o and GPT-3.5-turbo
- LLM robustness varies significantly across different knowledge graph domains
- Few-shot attack strategies show modest improvements in adversarial success rates
- Template-based prompt generation achieves comparable results to LLM-based approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework effectively evaluates LLM robustness by leveraging knowledge graphs to generate adversarial prompts that test model performance across different domains.
- Mechanism: KGPA generates original prompts from knowledge graph triplets and creates adversarial prompts by poisoning these triplets. The framework then assesses LLM robustness through the results of these adversarial attacks, using metrics like ASR, NRA, and RRA.
- Core assumption: Knowledge graphs provide a rich source of domain-specific knowledge that can be used to create diverse and challenging prompts for evaluating LLM robustness.
- Evidence anchors:
  - [abstract] "Our framework generates original prompts from the triplets of knowledge graphs and creates adversarial prompts by poisoning, assessing the robustness of LLMs through the results of these adversarial attacks."
  - [section 3.2] "This module randomly selects some of the original prompts generated by the T2P module and uses an APGP_prompt, similar to the KGB-FSA_prompt, to guide the large language model in modifying these original prompts."
  - [corpus] Found 25 related papers, indicating active research in this area. The related papers discuss various approaches to evaluating LLM robustness, including domain-constrained knowledge guidelines and refined adversarial prompts.

### Mechanism 2
- Claim: The use of few-shot attack strategies improves the effectiveness of adversarial prompt generation.
- Mechanism: The KGB-FSA module generates example prompts that satisfy two conditions: semantic similarity to original prompts and different classification outcomes. These examples are then used in the APGP module to guide the generation of more effective adversarial prompts.
- Core assumption: Providing the LLM with examples of successful adversarial prompts helps it generate more effective attacks.
- Evidence anchors:
  - [section 3.2] "These example prompts must satisfy the following two conditions: their semantics should be sufficiently similar to the original prompts, and their classification by the large language model should differ from that of the original prompts."
  - [section 4.3] "The graphs show that the KGPA framework achieves higher ASR values with FSA than without."
  - [corpus] The related papers discuss various attack strategies, including few-shot attacks and ensemble methods, indicating the importance of this approach.

### Mechanism 3
- Claim: The Prompt Refinement Engine (PRE) improves the quality of adversarial prompts by filtering out low-quality or semantically dissimilar prompts.
- Mechanism: The PRE module uses an LLMScore metric to evaluate the semantic similarity and quality of generated prompts compared to the original prompts. Prompts that meet a certain threshold (tau_llm) are considered high-quality and are used in the robustness evaluation.
- Core assumption: Filtering out low-quality prompts ensures that the adversarial prompts used in the evaluation are both effective and of high quality.
- Evidence anchors:
  - [section 3.2] "The PRE uses this score to filter generated sentences, comparing the LLMScore to a threshold, tau_llm. If the LLMScore meets or exceeds this threshold, the sentence is considered semantically similar and of high quality."
  - [section 4.3] "Figure 12 shows that with tau_llm below 0.9, ASR differences between GPT-3.5-turbo and GPT-4.0-turbo are minimal. Raising tau_llm above 0.9 widens the ASR gap, stabilizing ASR of GPT-3.5-turbo and decreasing GPT-4-turbo."

## Foundational Learning

- Concept: Knowledge graphs
  - Why needed here: Knowledge graphs provide the structured knowledge that is used to generate original and adversarial prompts for evaluating LLM robustness.
  - Quick check question: What are the main components of a knowledge graph, and how are they used in the KGPA framework?

- Concept: Adversarial attacks
  - Why needed here: Adversarial attacks are used to test the robustness of LLMs by attempting to fool them with carefully crafted prompts.
  - Quick check question: What are the different types of adversarial attacks, and how does the KGPA framework implement them?

- Concept: Few-shot learning
  - Why needed here: Few-shot learning is used in the KGB-FSA module to generate example prompts that help guide the creation of more effective adversarial prompts.
  - Quick check question: How does the few-shot attack strategy in the KGPA framework work, and what are its benefits?

## Architecture Onboarding

- Component map: T2P module -> KGB-FSA module -> APGP module -> PRE module -> Robustness evaluation
- Critical path: T2P module -> KGB-FSA module -> APGP module -> PRE module -> Robustness evaluation
- Design tradeoffs:
  - Template-based vs. LLM-based strategies for generating original prompts
  - Use of few-shot attack strategies vs. no few-shot attacks
  - Threshold setting for the PRE module (tau_llm)
- Failure signatures:
  - Low ASR values across all LLMs, indicating that the adversarial prompts are not effective
  - High ASR values across all LLMs, indicating that the LLMs are too robust and the prompts are not challenging enough
  - Inconsistent results across different domains, indicating that the knowledge graphs may not be representative
- First 3 experiments:
  1. Compare the ASR values of LLMs when using template-based vs. LLM-based strategies for generating original prompts
  2. Evaluate the impact of using few-shot attack strategies on the ASR values
  3. Analyze the effect of different tau_llm threshold settings on the ASR values and the quality of adversarial prompts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the robustness of LLMs vary across different knowledge graph domains, and what underlying factors contribute to these variations?
- Basis in paper: Explicit
- Why unresolved: The paper mentions that the robustness of LLMs is influenced by the professional domains in which they operate, but it does not delve into the specific factors that cause these variations. Understanding the underlying reasons for domain-specific robustness differences could provide insights into how LLMs learn and generalize knowledge.
- What evidence would resolve it: Comparative studies analyzing the training data composition, domain-specific language patterns, and the frequency of domain-specific knowledge in the training corpora of different LLMs could shed light on the factors contributing to robustness variations.

### Open Question 2
- Question: How does the performance of the KGB-FSA module compare to other few-shot attack strategies in terms of generating effective adversarial prompts?
- Basis in paper: Inferred
- Why unresolved: While the paper introduces the KGB-FSA module as a method for generating adversarial prompts, it does not compare its performance against other few-shot attack strategies. A comparative analysis could determine the relative effectiveness of KGB-FSA in creating adversarial prompts.
- What evidence would resolve it: Empirical studies comparing the KGB-FSA module's adversarial prompt generation with other state-of-the-art few-shot attack strategies, using metrics like ASR and prompt quality, would provide insights into its effectiveness.

### Open Question 3
- Question: What are the long-term effects of adversarial training on LLM robustness, and how do these effects manifest across different domains?
- Basis in paper: Inferred
- Why unresolved: The paper focuses on evaluating LLM robustness using adversarial prompts but does not explore the impact of adversarial training on improving robustness. Understanding how adversarial training affects long-term robustness could inform strategies for enhancing model resilience.
- What evidence would resolve it: Longitudinal studies tracking LLM performance on adversarial tasks before and after adversarial training, across various domains, would reveal the effects of such training on robustness.

## Limitations

- Limited evaluation to three ChatGPT family models, restricting generalizability to other LLM architectures
- Few-shot attack strategy shows only modest improvements, suggesting limited effectiveness
- Lack of detailed specification for LLMScore metric used in prompt refinement

## Confidence

- **High confidence**: The relative ranking of GPT-4-turbo > GPT-4o > GPT-3.5-turbo in adversarial robustness is well-supported by consistent experimental results across multiple knowledge graphs and metrics.
- **Medium confidence**: The domain-specific robustness variations (general vs. professional knowledge graphs) are observed but may be influenced by dataset size and prompt distribution rather than inherent model differences.
- **Low confidence**: The claimed benefits of the few-shot attack strategy (KGB-FSA module) are not strongly demonstrated, as ASR improvements are marginal compared to baseline methods.

## Next Checks

1. **Threshold sensitivity analysis**: Systematically vary the tau_llm threshold in the PRE module across its full range (0.0 to 1.0) to quantify its impact on ASR values and determine if current settings are optimal or arbitrary.

2. **Cross-architecture generalization**: Apply the KGPA framework to non-ChatGPT models (e.g., Claude, LLaMA) to test whether the observed robustness patterns hold across different model families and training approaches.

3. **Semantic diversity evaluation**: Design prompts that deliberately violate the semantic similarity constraint while maintaining adversarial effectiveness to test whether the framework's filtering mechanism may be overly conservative.