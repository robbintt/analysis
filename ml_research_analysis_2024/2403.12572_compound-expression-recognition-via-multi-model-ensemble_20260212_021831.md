---
ver: rpa2
title: Compound Expression Recognition via Multi Model Ensemble
arxiv_id: '2403.12572'
source_url: https://arxiv.org/abs/2403.12572
tags:
- expression
- recognition
- compound
- expressions
- surprised
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The authors tackle compound expression recognition by training\
  \ three distinct architectures\u2014Vision Transformer, MANet, and ResNet50\u2014\
  and fusing their predictions via late fusion. Each model emphasizes different facial\
  \ feature aspects: ViT captures global context, MANet combines multi-scale and local\
  \ attention, and ResNet focuses on local features."
---

# Compound Expression Recognition via Multi Model Ensemble

## Quick Facts
- arXiv ID: 2403.12572
- Source URL: https://arxiv.org/abs/2403.12572
- Reference count: 28
- Primary result: 80.86% accuracy and 74.60% F1 score on RAF-DB compound expression validation set using ViT + MANet + ResNet50 ensemble

## Executive Summary
This paper addresses compound expression recognition by combining three distinct neural network architectures through late fusion. The authors propose that different models capture complementary aspects of facial expressions - global context, multi-scale features, and local details. Their ensemble approach achieves significant performance improvements over individual models, particularly for the most challenging expression class, demonstrating that architectural diversity can enhance complex multi-label facial expression recognition.

## Method Summary
The authors train three separate models on the same RAF-DB dataset: a Vision Transformer for global feature capture, a MANet for multi-scale and local attention features, and a ResNet50 for local feature extraction. Each model is trained independently with its own learning rate and epoch settings. During inference, the models' predictions are combined through a weighted average based on their validation performance, where better-performing models receive higher weights. This late fusion approach leverages the complementary strengths of each architecture without requiring architectural modifications or joint training.

## Key Results
- Ensemble achieves 80.86% accuracy and 74.60% F1 score on RAF-DB validation set
- Outperforms individual models, particularly showing improvement on the hardest expression class
- Demonstrates that combining complementary models enhances recognition of complex, multi-label facial expressions
- Each architecture contributes different facial feature aspects: global context (ViT), multi-scale/local attention (MANet), and local features (ResNet50)

## Why This Works (Mechanism)
The ensemble approach works by leveraging complementary strengths of different architectures. Vision Transformer captures global contextual relationships across the entire face, MANet combines multi-scale processing with local attention mechanisms to handle varying feature sizes, and ResNet50 excels at extracting detailed local features. By combining these diverse feature extraction strategies through weighted late fusion, the system benefits from each model's strengths while compensating for individual weaknesses. This architectural diversity is particularly valuable for compound expressions where multiple facial action units interact in complex ways that may be better captured by different modeling approaches.

## Foundational Learning
- **Vision Transformer (ViT)**: Uses self-attention to capture global contextual relationships across entire facial images. Needed because compound expressions involve complex spatial relationships beyond local features. Quick check: Verify attention maps show global face region interactions.
- **Multi-scale attention networks (MANet)**: Processes features at different scales with attention mechanisms to handle varying facial component sizes. Needed because facial expressions involve features of different scales (eyes vs. mouth). Quick check: Confirm multi-scale feature maps capture both fine and coarse facial details.
- **ResNet50**: Employs residual connections for deep feature extraction focusing on local facial regions. Needed for extracting detailed local features that indicate specific expression components. Quick check: Validate local feature extraction captures individual facial action units.
- **Late fusion ensemble**: Combines model predictions using weighted averaging based on validation performance. Needed to leverage complementary strengths without architectural complexity. Quick check: Verify ensemble weights reflect individual model validation accuracies.
- **Compound expression annotation**: Multi-label classification where faces can display multiple expressions simultaneously. Needed because real facial expressions often combine basic emotions. Quick check: Confirm dataset contains samples with multiple valid expression labels.

## Architecture Onboarding

**Component Map:**
Input images -> ViT model -> Predictions
Input images -> MANet model -> Predictions
Input images -> ResNet50 model -> Predictions
Individual predictions -> Weighted averaging -> Final ensemble prediction

**Critical Path:**
Image preprocessing → Three independent model inference → Prediction probability extraction → Weighted averaging based on validation performance → Final compound expression classification

**Design Tradeoffs:**
- **Independent training vs. joint training**: Authors chose independent training for simplicity and flexibility, avoiding complex joint optimization but potentially missing cross-model feature correlations
- **Late fusion vs. intermediate fusion**: Late fusion requires no architectural modifications and allows easy model swapping, but may miss opportunities for feature-level complementarity
- **Three-model ensemble vs. larger ensemble**: Three models balance diversity and computational efficiency, though more models might capture additional expression aspects

**Failure Signatures:**
- Poor performance on specific expression classes suggests individual model weakness in that category
- Degradation with out-of-distribution data indicates limited generalization beyond RAF-DB
- Suboptimal weighting could indicate validation set bias or overfitting
- Similar failure patterns across all three models suggest dataset-specific artifacts rather than architectural complementarity

**First 3 Experiments:**
1. Validate individual model performance on RAF-DB validation set to establish baseline for ensemble comparison
2. Test ensemble performance with uniform weights vs. performance-based weights to quantify benefit of weighted fusion
3. Evaluate model performance breakdown by expression class to identify which expressions benefit most from ensemble approach

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow experimental scope limited to RAF-DB dataset without cross-dataset validation
- Performance metrics show substantial room for improvement given compound expression complexity
- Equal contribution assumption in late fusion may not be optimal for all expression categories
- No ablation studies quantifying individual model contributions to ensemble performance

## Confidence

**High confidence:**
- Ensemble approach demonstrates measurable improvement over individual models on RAF-DB validation set
- Architectural descriptions of ViT, MANet, and ResNet50 are technically sound and well-established

**Medium confidence:**
- Claim that combining complementary models enhances recognition of complex expressions is supported but would benefit from additional cross-dataset validation

**Low confidence:**
- Assertion that this approach will generalize to other compound expression datasets or real-world applications without further testing

## Next Checks
1. Test the ensemble model on additional compound expression datasets (e.g., FERPlus, AffectNet) to verify generalizability across different data distributions and annotation schemes
2. Conduct ablation studies to quantify the individual contribution of each model (ViT, MANet, ResNet50) to the ensemble performance and determine optimal weighting strategies
3. Evaluate model performance on out-of-distribution samples and in varying lighting/pose conditions to assess real-world robustness beyond the controlled RAF-DB validation set