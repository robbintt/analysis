---
ver: rpa2
title: 'Gold Panning in Vocabulary: An Adaptive Method for Vocabulary Expansion of
  Domain-Specific LLMs'
arxiv_id: '2410.01188'
source_url: https://arxiv.org/abs/2410.01188
tags:
- vocabulary
- vegad
- domain
- words
- domain-specific
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the influence of adding domain-specific
  words and the generation of domain vocabulary for large language models (LLMs).
  It reveals that expanding vocabulary with only a subset of the entire supplementary
  domain vocabulary may lead to superior performance over using the whole vocabulary.
---

# Gold Panning in Vocabulary: An Adaptive Method for Vocabulary Expansion of Domain-Specific LLMs

## Quick Facts
- **arXiv ID**: 2410.01188
- **Source URL**: https://arxiv.org/abs/2410.01188
- **Reference count**: 40
- **Primary result**: VEGAD outperforms other vocabulary expansion methods, improving domain-specific task performance while mitigating catastrophic forgetting on general tasks.

## Executive Summary
This paper addresses the challenge of vocabulary expansion for domain-specific LLMs by proposing VEGAD, a gradient-based method for selecting optimal subsets of domain vocabulary. The key insight is that adding only a carefully selected subset of domain words (rather than the entire vocabulary) leads to better performance. VEGAD uses gradients to identify high-value words that are crucial for domain-specific tasks, making it a plug-and-play solution that integrates with various fine-tuning techniques. Experiments on three Chinese datasets demonstrate superior performance compared to other vocabulary expansion methods.

## Method Summary
VEGAD is a plug-and-play task-adaptive vocabulary selection method that uses gradients to identify the most valuable words from a candidate vocabulary for optimal LLM expansion. The method builds a Trie from domain-specific candidate words, calculates gradients for each word by tracing sub-sequences in token streams, and selects the top-K words with highest gradients. It then resizes the embedding and LM head layers, initializes new tokens (typically by averaging sub-token weights), and fine-tunes the model on domain data. The approach is tokenizer-agnostic and aims to mitigate catastrophic forgetting while improving domain-specific performance.

## Key Results
- VEGAD achieves superior performance on both domain-specific tasks and general tasks compared to other vocabulary expansion methods
- Selective vocabulary expansion (2500-3000 words) outperforms using the entire vocabulary, with general task performance degrading significantly when using full vocabulary
- The method mitigates catastrophic forgetting, showing less performance degradation on general tasks (ALPACA, GSM8K, SafetyPrompts) compared to direct fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding only a subset of domain vocabulary improves LLM performance more than using the entire vocabulary.
- Mechanism: Selective vocabulary expansion based on gradient magnitude prioritizes tokens that are most influential for domain-specific tasks, avoiding overfitting to noise or low-impact tokens.
- Core assumption: Gradients correlate with token importance for domain adaptation.
- Evidence anchors:
  - [abstract] "Our pilot study reveals that expansion with only a subset of the entire vocabulary may lead to superior performance."
  - [section 4.3] Results show best performance at ~2500-3000 words, degradation with full vocabulary.
  - [corpus] Neighboring papers show similar vocabulary expansion methods, supporting the general idea of selective expansion.
- Break condition: If gradient magnitude no longer correlates with domain relevance, or if the vocabulary selection process becomes biased toward overfitting.

### Mechanism 2
- Claim: VEGAD mitigates catastrophic forgetting by selectively expanding vocabulary.
- Mechanism: By adding domain-specific words and initializing them with sub-token averaging, the model retains representations for general vocabulary while adapting domain-specific tokens.
- Core assumption: Representation shift from SFT is shared across new and old tokens, preserving general knowledge.
- Evidence anchors:
  - [abstract] "The selection of an optimal subset for expansion has shown to enhance performance on both domain-specific tasks and general tasks, showcasing the potential of VEGAD."
  - [section 4.2] General task performance (e.g., ALPACA, GSM8K) degrades less with VEGAD than with full vocabulary.
  - [corpus] Neighboring papers cite catastrophic forgetting as a key concern in domain adaptation.
- Break condition: If initialization method fails to preserve general representations, or if the added vocabulary disrupts tokenization patterns too much.

### Mechanism 3
- Claim: Gradient-based word selection is tokenizer-agnostic and plug-and-play.
- Mechanism: The method computes gradients over token sequences without altering the underlying tokenizer, making it compatible with any tokenization algorithm.
- Core assumption: Gradients can be calculated independently of the tokenizer's specific segmentation rules.
- Evidence anchors:
  - [abstract] "VEGAD is a plug-and-play task-adaptive vocabulary selection method, seamlessly integrating with diverse techniques utilized in supervised fine-tuning."
  - [section 3.3] "In contrast to previous methods... which might alter the intrinsic behaviors of current tokenizers... VEGAD is tokenizer-agnostic."
  - [corpus] Neighboring papers discuss tokenizer-agnostic approaches as desirable for flexibility.
- Break condition: If the tokenizer's segmentation rules change fundamentally, or if gradient calculation becomes computationally prohibitive.

## Foundational Learning

- **Concept**: Gradient-based optimization
  - Why needed here: VEGAD uses gradients to rank candidate words for vocabulary expansion.
  - Quick check question: Can you explain how gradients indicate the importance of a token for a given task?

- **Concept**: Trie data structure
  - Why needed here: Efficiently maps candidate words to their constituent tokens for gradient aggregation.
  - Quick check question: How does a Trie help in retrieving all occurrences of candidate words in token sequences?

- **Concept**: Catastrophic forgetting
  - Why needed here: VEGAD aims to mitigate forgetting of general capabilities while adapting to domain-specific tasks.
  - Quick check question: What causes catastrophic forgetting in fine-tuning, and how does selective vocabulary expansion help?

## Architecture Onboarding

- **Component map**: Trie construction from candidate vocabulary -> Gradient calculation module (embedding + LM head) -> Top-K word selection -> Embedding and LM head resizing -> Fine-tuning pipeline

- **Critical path**: 1. Build Trie from candidate vocabulary 2. Compute gradients for each candidate word 3. Select top-K words by gradient magnitude 4. Resize embedding and LM head layers 5. Fine-tune on domain data

- **Design tradeoffs**:
  - Full vocabulary vs. selective expansion: trade-off between coverage and overfitting.
  - Gradient-based vs. frequency-based selection: trade-off between relevance and simplicity.
  - Tokenizer-agnostic vs. tokenizer-specific methods: trade-off between flexibility and performance.

- **Failure signatures**:
  - Performance degradation on general tasks: indicates overfitting or forgetting.
  - Poor domain task performance: indicates ineffective word selection or initialization.
  - Slow convergence: indicates inefficient gradient computation or Trie traversal.

- **First 3 experiments**:
  1. Compare performance with full vocabulary vs. VEGAD-selected subset.
  2. Test different initialization methods for new vocabulary tokens.
  3. Vary K (top-K words) to find optimal vocabulary size.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the optimal strategies for initializing the weights of newly added vocabulary tokens in domain-specific LLMs?
- Basis in paper: [explicit] The paper explicitly states that current initialization methods (averaging sub-token weights, attention-based methods, and external knowledge retrieval) do not provide stable improvements and highlights the necessity of developing effective initialization approaches, especially under low-resource scenarios.
- Why unresolved: While the paper investigates various initialization strategies, it finds that none consistently improve performance across different domains and tasks. The challenge lies in finding a method that properly initializes new token embeddings without disrupting the pre-trained knowledge.
- What evidence would resolve it: Comparative experiments showing consistent performance improvements across multiple domains and tasks when using a novel initialization method, particularly in low-resource settings where traditional approaches fail.

### Open Question 2
- Question: How does the size of domain-specific vocabulary affect catastrophic forgetting of general capabilities in LLMs?
- Basis in paper: [explicit] The paper demonstrates through experiments that increasing vocabulary size doesn't always improve performance and can lead to significant decreases in general task performance (e.g., math accuracy dropping by 50% when using the full Jieba vocabulary).
- Why unresolved: While the paper shows that VEGAD helps mitigate catastrophic forgetting compared to direct fine-tuning, the relationship between vocabulary size, domain performance, and preservation of general capabilities remains unclear. The optimal vocabulary size may vary across domains and tasks.
- What evidence would resolve it: Systematic experiments varying vocabulary sizes across multiple domains while measuring both domain-specific performance and general task retention, identifying patterns or thresholds where vocabulary expansion becomes detrimental.

### Open Question 3
- Question: Can VEGAD be effectively adapted for non-segmented languages (e.g., English) without relying on specialized segmentation tools?
- Basis in paper: [explicit] The paper mentions that for English experiments, VEGAD was adapted to work with SentencePiece instead of Jieba, but provides limited analysis of this adaptation's effectiveness.
- Why unresolved: The paper focuses primarily on Chinese datasets and tools like Jieba, leaving open questions about VEGAD's performance and limitations when applied to languages without clear word boundaries or when using different tokenization approaches.
- What evidence would resolve it: Comprehensive experiments comparing VEGAD's performance across multiple languages using different tokenization strategies, particularly demonstrating effectiveness in languages where word segmentation is ambiguous or unnecessary.

## Limitations

- **Vocabulary Selection Reliability**: The finding that selective expansion outperforms full expansion is primarily validated on Chinese domain datasets, with unclear generalizability to other languages and domains.
- **Computational Efficiency Claims**: The paper mentions using Aho-Corasick algorithm for efficiency but doesn't provide runtime comparisons or complexity analysis to validate claimed benefits.
- **Gradient Correlation Assumption**: Limited theoretical justification for why gradients should be the optimal metric for vocabulary selection versus alternatives like frequency or mutual information.

## Confidence

- **High Confidence**: The empirical observation that selective vocabulary expansion (2500-3000 words) outperforms full vocabulary expansion on tested Chinese datasets.
- **Medium Confidence**: The claim that VEGAD mitigates catastrophic forgetting, though comparison baseline is limited to full vocabulary expansion.
- **Low Confidence**: The generalizability of the gradient-based selection mechanism across different languages, domains, and model architectures.

## Next Checks

1. **Cross-Domain Generalization Test**: Apply VEGAD to English medical/legal datasets and non-medical/non-legal domains to verify if the 2500-3000 word optimal range holds across different domains and languages.

2. **Gradient Correlation Analysis**: Conduct ablation studies comparing VEGAD's gradient-based selection against frequency-based, attention-based, and random selection methods to quantify whether gradients provide superior vocabulary selection.

3. **Computational Overhead Measurement**: Measure wall-clock time and memory usage for VEGAD's gradient calculation across different vocabulary sizes and compare against baseline vocabulary expansion methods to validate claimed efficiency benefits.