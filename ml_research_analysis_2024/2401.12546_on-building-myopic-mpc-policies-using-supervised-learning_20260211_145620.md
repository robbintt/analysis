---
ver: rpa2
title: On Building Myopic MPC Policies using Supervised Learning
arxiv_id: '2401.12546'
source_url: https://arxiv.org/abs/2401.12546
tags:
- function
- learning
- data
- myopic
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational burden of online model predictive
  control (MPC) by proposing a supervised learning approach to approximate the optimal
  value function. Instead of directly approximating the MPC policy, the authors learn
  the optimal cost-to-go function offline using optimal state-action-value tuples.
---

# On Building Myopic MPC Policies using Supervised Learning

## Quick Facts
- arXiv ID: 2401.12546
- Source URL: https://arxiv.org/abs/2401.12546
- Reference count: 2
- One-line primary result: Supervised learning of cost-to-go function enables myopic MPC with reduced online computation and improved handling of online parameter variations

## Executive Summary
This paper addresses the computational burden of online model predictive control (MPC) by proposing a supervised learning approach to approximate the optimal value function. Instead of directly approximating the MPC policy, the authors learn the optimal cost-to-go function offline using optimal state-action-value tuples. This cost-to-go function is then used in a myopic MPC with a short prediction horizon, significantly reducing online computation without compromising performance. The key novelty is encoding the descent property constraint during the learning process, ensuring that the successor state's cost-to-go is lower than the current state's cost-to-go.

## Method Summary
The proposed approach involves learning the optimal cost-to-go function offline from optimal state-action-value tuples generated by high-horizon MPC. This learned function is then incorporated into a myopic MPC formulation with a short prediction horizon. The critical innovation is enforcing the descent property during learning, which ensures that the cost-to-go of successor states is lower than the current state's cost-to-go. This constraint is shown to improve closed-loop performance compared to standard mean squared error loss approaches. The myopic MPC can handle online variations in controller parameters such as tightened state constraints and sampling time changes, which is not possible with direct policy approximation methods.

## Key Results
- Learning the cost-to-go function with descent property constraint outperforms MSE-only approaches on CSTR benchmark
- Myopic MPC with learned cost-to-go achieves comparable performance to full-horizon MPC with significantly reduced online computation
- The approach can handle online parameter variations (constraint tightening, sampling time changes) that direct policy approximation cannot

## Why This Works (Mechanism)
The approach works by learning the optimal value function offline, which captures the long-term cost structure of the control problem. By enforcing the descent property during learning, the method ensures that the myopic MPC with short horizon can still make progress toward the optimal solution. This is because the learned cost-to-go function encodes information about future costs beyond the short prediction horizon, effectively extending the foresight of the controller. The descent property constraint ensures that each step reduces the estimated total cost, maintaining the optimality guarantees of the original MPC.

## Foundational Learning
- **Model Predictive Control (MPC)**: Optimal control strategy that solves a finite-horizon optimization problem online. Needed because the proposed method is a variant of MPC. Quick check: Can formulate basic MPC problem with stage cost and terminal cost.
- **Value Function / Cost-to-go Function**: The expected cumulative cost from a given state to the end of the planning horizon. Needed because this is what the paper learns offline instead of the policy. Quick check: Can compute value function for simple linear quadratic regulator.
- **Descent Property**: Constraint that successor state's cost-to-go must be lower than current state's cost-to-go. Needed because this is the key innovation that ensures myopic MPC makes progress. Quick check: Can verify descent property holds for Bellman equation.
- **State-Action Value Function (Q-function)**: The expected cumulative cost from taking a specific action in a given state. Needed because the paper uses optimal Q-values for training data. Quick check: Can compute Q-values for simple grid-world MDP.

## Architecture Onboarding

### Component Map
High-horizon MPC -> Optimal trajectory generation -> State-action-value dataset -> Cost-to-go learning (with descent constraint) -> Myopic MPC (short horizon)

### Critical Path
1. Generate optimal trajectories using high-horizon MPC on system dynamics
2. Extract state-action-value tuples from optimal trajectories
3. Train cost-to-go function with descent property constraint
4. Deploy myopic MPC using learned cost-to-go with short prediction horizon

### Design Tradeoffs
- Longer offline learning time vs. reduced online computation time
- Accuracy of learned cost-to-go vs. computational complexity of learning
- Strictness of descent constraint vs. flexibility of learned function
- Length of myopic horizon vs. performance degradation

### Failure Signatures
- Oscillations or divergence in closed-loop behavior indicate poor learning of cost-to-go
- Degradation in performance when parameters change suggests insufficient generalization
- High computational cost during online MPC suggests inefficient myopic formulation
- Poor performance on validation trajectories indicates overfitting to training data

### 3 First Experiments
1. Compare myopic MPC performance with and without descent constraint on simple linear system
2. Vary the myopic horizon length and measure impact on closed-loop performance
3. Test robustness to model mismatch by introducing parameter variations in validation

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes accurate system dynamics models are available (as with traditional MPC)
- Offline learning phase requires extensive optimal trajectories from high-horizon MPC
- Method's scalability to high-dimensional state spaces remains unproven
- Experimental validation limited to single CSTR example, restricting generalizability

## Confidence
- **High confidence**: The mathematical formulation of encoding descent properties during learning is sound and clearly presented
- **Medium confidence**: The computational complexity claims are reasonable but not rigorously proven
- **Medium confidence**: Performance improvements over direct policy approximation are demonstrated but may be problem-specific

## Next Checks
1. Test the approach on multiple benchmark control problems with varying complexity, particularly higher-dimensional systems
2. Evaluate robustness to model mismatch by introducing parametric uncertainties in the system dynamics
3. Conduct ablation studies to quantify the specific contribution of the descent property constraint versus standard MSE loss