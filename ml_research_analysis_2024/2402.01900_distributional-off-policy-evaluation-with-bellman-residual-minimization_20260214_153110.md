---
ver: rpa2
title: Distributional Off-policy Evaluation with Bellman Residual Minimization
arxiv_id: '2402.01900'
source_url: https://arxiv.org/abs/2402.01900
tags:
- following
- where
- pmin
- which
- bound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies distributional off-policy evaluation, aiming
  to estimate the return distribution for a target policy using offline data. Existing
  methods rely on supremum-extended statistical distances, which are difficult to
  estimate.
---

# Distributional Off-policy Evaluation with Bellman Residual Minimization

## Quick Facts
- arXiv ID: 2402.01900
- Source URL: https://arxiv.org/abs/2402.01900
- Reference count: 40
- Proposes Energy Bellman Residual Minimizer (EBRM) for distributional off-policy evaluation without completeness assumptions

## Executive Summary
This paper addresses distributional off-policy evaluation, which aims to estimate return distributions for target policies using offline data. The authors challenge the prevailing approach of using supremum-extended statistical distances by proposing expectation-extended statistical distances instead. They develop the Energy Bellman Residual Minimizer (EBRM) method, which provides finite-sample error bounds under realizability assumptions without requiring the completeness assumption that previous methods depend on. The work includes both theoretical analysis and simulation studies demonstrating improved performance over existing methods.

## Method Summary
The authors propose using expectation-extended statistical distances instead of supremum-extended ones for distributional off-policy evaluation. They develop the Energy Bellman Residual Minimizer (EBRM) algorithm that minimizes Bellman residuals in distributional space. The method establishes finite-sample error bounds under realizability assumptions and introduces a multi-step extension for non-realizable settings. The approach relaxes the completeness assumption required by prior work while maintaining theoretical guarantees through careful analysis of Bellman residual minimization in distributional space.

## Key Results
- EBRM provides finite-sample error bounds without requiring completeness assumptions
- Theoretical guarantees established under realizability assumption
- Multi-step extension developed for non-realizable settings
- Simulation studies show superior performance compared to existing methods
- First method to achieve theoretical guarantees without completeness assumptions

## Why This Works (Mechanism)
The method works by minimizing Bellman residuals in distributional space using expectation-extended statistical distances. This approach captures the temporal structure of the problem while being more tractable than supremum-based methods. The expectation extension allows for more stable estimation while maintaining theoretical guarantees. The Bellman residual minimization framework naturally incorporates the recursive structure of value distributions, and the relaxation of completeness assumptions broadens the method's applicability.

## Foundational Learning
- Distributional off-policy evaluation: Estimating return distributions for target policies using offline data. Needed to understand the problem setting and motivation.
- Bellman residuals: Measures of error in the Bellman equation. Critical for understanding how the method evaluates and minimizes distributional errors.
- Realizability assumption: Assumption that the true distribution lies in the function class. Key for establishing theoretical guarantees.
- Completeness assumption: Previous methods required this assumption for convergence. Understanding its relaxation is central to the contribution.
- Statistical distances: Metrics for comparing probability distributions. The choice between supremum and expectation extensions is fundamental to the approach.

## Architecture Onboarding

**Component map:** Data -> Bellman residual computation -> Expectation-extended distance minimization -> EBRM estimator -> Return distribution estimate

**Critical path:** Data collection -> Bellman residual calculation -> Distributional distance computation -> Parameter update -> Final distribution estimate

**Design tradeoffs:** The choice of expectation vs. supremum extensions trades theoretical tightness for practical tractability. Realizability assumption enables stronger guarantees but limits applicability. Multi-step extension increases complexity but handles non-realizable settings.

**Failure signatures:** Poor performance when realizability assumption violated, instability with insufficient data, degraded accuracy when Bellman operator has large variance, potential bias from expectation extension.

**3 first experiments:** 1) Validate theoretical error bounds on synthetic data with known ground truth distributions. 2) Compare EBRM vs. baseline methods on simple tabular environments. 3) Test multi-step extension on environments where realizability is violated.

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Realizability assumption may be restrictive in practical settings
- Simulation studies limited to relatively simple environments
- Multi-step extension adds complexity that may affect practical performance
- Theoretical analysis focused on distributional evaluation rather than policy optimization

## Confidence
- Theoretical claims: High
- Simulation results: Medium
- Practical applicability: Medium
- Comparison to existing methods: Medium

## Next Checks
1. Validate error bounds empirically on synthetic problems with varying sample sizes
2. Test performance on more complex, high-dimensional environments
3. Investigate sensitivity to the choice of expectation-extended statistical distance metrics