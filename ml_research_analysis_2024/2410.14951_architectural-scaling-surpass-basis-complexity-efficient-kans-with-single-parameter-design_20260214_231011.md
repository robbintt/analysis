---
ver: rpa2
title: Architectural Scaling Surpass Basis Complexity? Efficient KANs with Single-Parameter
  Design
arxiv_id: '2410.14951'
source_url: https://arxiv.org/abs/2410.14951
tags:
- parameter
- functions
- accuracy
- lss-skan
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the Efficient KAN Expansion (EKE) Hypothesis
  and introduces Single-Parameter KANs (SKANs) as a novel approach to Kolmogorov-Arnold
  Networks (KANs). The EKE Hypothesis posits that allocating parameters to expand
  network scale rather than increasing basis function complexity yields superior performance.
---

# Architectural Scaling Surpass Basis Complexity? Efficient KANs with Single-Parameter Design

## Quick Facts
- arXiv ID: 2410.14951
- Source URL: https://arxiv.org/abs/2410.14951
- Reference count: 21
- Primary result: LShifted Softplus-based SKAN (LSS-SKAN) improves MNIST F1 scores by up to 6.51% and reduces test loss by 93.1% compared to existing KAN variants

## Executive Summary
This paper introduces the Efficient KAN Expansion (EKE) Hypothesis and Single-Parameter KANs (SKANs) as a novel approach to Kolmogorov-Arnold Networks. The EKE Hypothesis posits that allocating parameters to expand network scale rather than increasing basis function complexity yields superior performance. SKANs use basis functions with only one learnable parameter, with the LShifted Softplus-based variant demonstrating optimal accuracy. Experiments on MNIST show LSS-SKAN improves F1 scores by up to 6.51% and reduces test loss by 93.1% compared to existing KAN variants, while achieving up to 6x faster training speeds.

## Method Summary
The paper proposes Single-Parameter KANs (SKANs) that use basis functions with only one learnable parameter, contrasting with traditional KANs that use multi-parameter spline-based functions. The key insight is that by reducing basis function complexity, more computational resources can be allocated to expanding network scale while maintaining the same total parameter budget. The LShifted Softplus function is identified as optimal through systematic experimentation. The method is validated on MNIST using Adam optimizer with learning rates between 0.0001-0.01, training for 10-30 epochs with batch size of 64. The paper also introduces the Universal KAN (Uni-KAN) framework, providing a formal unification of all KAN-style networks.

## Key Results
- LSS-SKAN improves MNIST F1 scores by up to 6.51% compared to existing KAN variants
- Test loss reduced by 93.1% compared to baseline KAN implementations
- Training speed improved by up to 6x compared to other pure KAN implementations
- LELU, LLeaky ReLU, and LHard Sigmoid failed to train due to unstable gradients

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing complex basis functions with single-parameter learnable functions allows network scale expansion without increasing total parameter count
- Mechanism: By reducing the parameter count per basis function from grid-size-dependent spline coefficients to a single learnable parameter, more computational resources can be allocated to increasing hidden layer dimensions while maintaining the same total parameter budget
- Core assumption: The representational power of KANs depends more on network scale (number of nodes) than on basis function complexity when total parameters are constrained
- Evidence anchors: Abstract states "allocating parameters to expand network scale rather than increasing basis function complexity yields superior performance"; preliminary experiments show "the network should focus more on capturing the expressions of these interactions"

### Mechanism 2
- Claim: Single-parameter basis functions with smooth, non-zero derivatives enable more stable training dynamics than piecewise or discontinuous alternatives
- Mechanism: Functions like LShifted Softplus maintain smoothness and non-zero gradients across their domain, preventing training instability that occurs with basis functions having discontinuous derivatives or regions of zero gradient
- Core assumption: Training stability in KANs is primarily determined by the smoothness and gradient properties of basis functions rather than their functional form
- Evidence anchors: Abstract notes "provide the first strong empirical validation for the theoretical necessity of basis function smoothness for stable training"; LELU, LLeaky ReLU, and LHard Sigmoid "failed to generate data, indicating these SKANs were unable to train"

### Mechanism 3
- Claim: The LShifted Softplus basis function provides an optimal balance between expressiveness and computational efficiency for MNIST classification
- Mechanism: The LShifted Softplus combines the smoothness of softplus with learnable shift parameters, enabling adaptive activation ranges while maintaining computational efficiency through its simple analytical form
- Core assumption: For image classification tasks like MNIST, the specific choice of single-parameter function matters less than its smoothness and gradient properties
- Evidence anchors: LShifted Softplus "demonstrates superior performance at optimal learning rates compared to other functions while maintaining moderate training times"; LSS-SKAN "achieved higher test set accuracy and F1 scores compared to other pure KAN implementations"

## Foundational Learning

- Concept: Kolmogorov-Arnold representation theorem
  - Why needed here: The entire KAN architecture is derived from this theorem's claim that multivariate functions can be decomposed into univariate functions
  - Quick check question: What is the mathematical statement of the Kolmogorov-Arnold representation theorem and how does it justify the KAN architecture?

- Concept: Basis function parameterization tradeoffs
  - Why needed here: Understanding why reducing basis function complexity enables network scale expansion requires knowledge of parameter allocation in neural networks
  - Quick check question: How does the total parameter count in a KAN layer depend on both the number of nodes and the complexity of the basis function?

- Concept: Smoothness and gradient properties in neural networks
  - Why needed here: The stability of KAN training depends critically on basis function smoothness, which affects gradient flow during backpropagation
  - Quick check question: What are the mathematical conditions for a function to provide stable gradients throughout its domain?

## Architecture Onboarding

- Component map: Input layer (784 nodes) -> Hidden layer(s) -> Output layer (10 nodes) -> Basis functions (Single-parameter learnable functions)
- Critical path:
  1. Data preprocessing and batching
  2. Forward pass through learnable basis functions
  3. Summation at nodes
  4. Loss computation
  5. Backward pass through basis functions
  6. Parameter update
- Design tradeoffs:
  - Single-parameter vs. multi-parameter basis functions
  - Network depth vs. width given fixed parameter budget
  - Computational efficiency vs. representational capacity
  - Training stability vs. model expressiveness
- Failure signatures:
  - NaN values during training (typically from unstable basis functions)
  - Vanishing gradients (from basis functions with zero derivatives)
  - Oscillating loss (from discontinuous basis functions)
  - Slow convergence (from overly complex basis functions)
- First 3 experiments:
  1. Compare LShifted Softplus vs. ReLU as basis functions on a small dataset to verify training stability
  2. Test different hidden layer sizes with fixed total parameters to validate the EKE hypothesis
  3. Evaluate training dynamics with different learning rates to identify optimal optimization settings

## Open Questions the Paper Calls Out
- What specific mathematical properties of basis functions (e.g., Lipschitz continuity, bounded derivatives) determine their suitability for SKAN training stability?
- Does the EKE Principle hold across different network architectures and task domains beyond MNIST digit classification?
- What is the theoretical relationship between parameter count, basis function complexity, and representational capacity in KAN networks?

## Limitations
- Limited empirical validation to a single dataset (MNIST) without testing generalization to other domains
- Lack of theoretical justification for why expanding network scale would be more efficient than complex basis functions
- Missing systematic analysis of mathematical properties that determine basis function stability

## Confidence
- EKE Hypothesis validity: Medium - supported by MNIST results but lacks broader empirical validation
- LShifted Softplus superiority: Medium - optimal within tested single-parameter functions but not compared to multi-parameter alternatives
- Training stability mechanism: Low - empirical observation without rigorous theoretical justification

## Next Checks
1. Test LSS-SKAN on CIFAR-10/100 and natural language processing tasks to evaluate domain generalization
2. Conduct ablation studies comparing training stability across different basis function families with matched parameter counts
3. Implement theoretical analysis of gradient flow properties for different basis function smoothness levels