---
ver: rpa2
title: Partially Frozen Random Networks Contain Compact Strong Lottery Tickets
arxiv_id: '2402.14029'
source_url: https://arxiv.org/abs/2402.14029
tags:
- search
- network
- networks
- frozen
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a method to reduce the search space for finding
  strong lottery tickets (SLTs) by partially freezing a randomly weighted neural network
  at initialization. Freezing is achieved by randomly pruning and locking parameters,
  which are excluded from the search space.
---

# Partially Frozen Random Networks Contain Compact Strong Lottery Tickets

## Quick Facts
- arXiv ID: 2402.14029
- Source URL: https://arxiv.org/abs/2402.14029
- Reference count: 29
- One-line primary result: Partially frozen random networks contain strong lottery tickets with better accuracy-to-search-space and accuracy-to-model-size trade-offs than dense or sparsely pruned source networks.

## Executive Summary
This paper proposes a method to reduce the search space for finding strong lottery tickets (SLTs) by partially freezing a randomly weighted neural network at initialization. Freezing is achieved by simultaneously pruning and locking a subset of parameters, which are excluded from both the search space and supermask. The authors theoretically prove the existence of SLTs in such frozen networks using an extended subset-sum approximation lemma. Experiments on image and graph node classification tasks demonstrate that frozen SLTs achieve better accuracy-to-search space and accuracy-to-model size trade-offs compared to SLTs found in dense or sparsely pruned source networks.

## Method Summary
The method involves freezing a random subset of parameters in a randomly initialized neural network by simultaneously applying pruning and locking masks. These frozen parameters are excluded from the SLT search space, reducing computational cost and model size. The SLT search is performed using an extension of the Edge-Popup algorithm that updates weight scores instead of weights. The freezing masks are compressed using ternary encoding (-1 for pruning, +1 for locking, 0 for supermask inclusion) and can be regenerated from their seed, further compressing the model. Theoretical analysis proves that SLTs exist in frozen networks by extending the subset-sum approximation lemma to handle locked parameters.

## Key Results
- ImageNet experiments show frozen SLTs achieve 3.3Ã— compression and up to 14.12 points higher accuracy than counterparts from dense or sparsely pruned source networks
- CIFAR-10 experiments demonstrate 0.22-0.26 points higher accuracy than sparse source networks
- OGBN-Arxiv graph node classification shows 0.12-0.29 points higher accuracy than sparse source networks
- Freezing up to 90% of parameters still yields comparable accuracy while significantly reducing search space

## Why This Works (Mechanism)

### Mechanism 1
Random parameter freezing at initialization reduces the SLT search space without restricting the sparsity range. Freezing is implemented by simultaneously pruning a subset of parameters and locking another subset as permanently active in the SLT. These frozen parameters are excluded from both the search space and the supermask, reducing training cost and model size. The core assumption is that optimal SLT sparsity is not always in the high-sparsity region, requiring a method to reduce search space while preserving ability to explore lower sparsities.

### Mechanism 2
Strong lottery tickets exist within frozen networks, as proven by extending the subset-sum approximation to handle locked parameters. The proof replaces the standard subset-sum approximation lemma with a version that accounts for parameters that are either unavailable (pruned) or must be used (locked). This extended lemma is then applied to the SLT existence theorem from Gadhikar et al. (2023). The core assumption is that source network width must be sufficiently larger than target network width to satisfy subset-sum approximation requirement, even after freezing.

### Mechanism 3
Freezing not only reduces search space but also improves the accuracy-to-model size trade-off of found SLTs. By excluding frozen parameters from the supermask, the SLT can be reconstructed from a smaller set of information (weights + supermask). Additionally, the freezing pattern itself can be regenerated from its seed, further compressing the model. The core assumption is that proper compression of freezing masks can be achieved through ternary encoding.

## Foundational Learning

- Concept: Strong Lottery Ticket Hypothesis (SLTH)
  - Why needed here: The paper builds on SLTH to show that SLTs can exist in frozen networks, extending previous work that proved SLTs in dense and sparse networks
  - Quick check question: What is the key difference between a strong lottery ticket (SLT) and a weak lottery ticket (WLT)?

- Concept: Subset-sum approximation
  - Why needed here: The theoretical proof of SLT existence in frozen networks relies on extending the subset-sum approximation lemma to handle locked parameters
  - Quick check question: In the context of SLTs, what does it mean to "approximate" a target network using a subset-sum of source weights?

- Concept: Edge-Popup algorithm
  - Why needed here: Experiments use an extension of Edge-Popup to find SLTs within frozen networks by updating weight scores instead of weights
  - Quick check question: How does Edge-Popup differ from traditional weight training in terms of what is being optimized?

## Architecture Onboarding

- Component map: Source network -> Freezing masks (pruning + locking) -> SLT search (Edge-Popup variant) -> Supermask + Freezing mask compression

- Critical path:
  1. Initialize source network with random weights
  2. Generate pruning and locking masks based on desired ratios
  3. Apply masks to create frozen network
  4. Run SLT search (Edge-Popup variant) on non-frozen parameters
  5. Generate supermask and freezing mask from results
  6. Compress model using freezing mask encoding

- Design tradeoffs:
  - Freezing ratio vs. search space reduction: Higher freezing reduces search space more but may hurt accuracy if too aggressive
  - Pruning ratio vs. locking ratio: Determines bounds of search space; optimal positioning depends on desired SLT sparsity
  - Width expansion: Wider networks can compensate for freezing but increase memory usage

- Failure signatures:
  - Accuracy collapse: Freezing ratio too high, leaving insufficient capacity
  - No improvement over sparse networks: Locking ratio too low; not enough reduction in search space
  - Slow convergence: Search space still too large; consider increasing freezing ratio

- First 3 experiments:
  1. Vary freezing ratio (0%, 50%, 80%) on Conv6 with CIFAR-10, measure accuracy and search space size
  2. Compare accuracy-to-model size trade-off between frozen, dense, and sparse source networks on ResNet-18
  3. Test different prune:lock proportions for fixed freezing ratio (e.g., 80%) to find optimal search space positioning

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed method's accuracy improvement in frozen networks vary across different neural network architectures and tasks beyond image and graph node classification? The paper demonstrates effectiveness on image classification (CIFAR-10) and node classification (OGBN-Arxiv) using Conv6, ResNet-18, and GIN architectures, but does not explore other domains or architectures.

### Open Question 2
What is the theoretical limit of the freezing ratio that can be applied to a source network before the accuracy of the found SLT starts to degrade significantly? The paper shows freezing up to 90% can still lead to comparable accuracy in some cases, but does not investigate the upper limit of freezing before significant accuracy degradation occurs.

### Open Question 3
How does the proposed method's performance compare to other SLT search algorithms when applied to frozen networks? The paper focuses on Edge-Popup algorithm and does not compare its performance with other algorithms (e.g., SNIP, GraSP) when applied to frozen networks.

## Limitations
- Theoretical analysis limited to specific width expansion conditions that may not hold for all architectures
- Subset-sum approximation proof generalizability to deeper networks and non-vision tasks remains uncertain
- Practical benefits depend on specific task and architecture, with more modest improvements in smaller-scale experiments

## Confidence
- High Confidence: The mechanism of reducing search space through parameter freezing is well-established and directly observable in experimental results
- Medium Confidence: Practical benefit of frozen SLTs over dense or sparsely pruned alternatives depends on specific task and architecture
- Low Confidence: Generalizability of subset-sum approximation proof to deeper networks and non-vision tasks remains uncertain

## Next Checks
1. Systematically vary freezing ratios beyond tested 50-90% range to identify optimal points and potential failure modes
2. Test frozen SLT approach on architectures not covered in paper (e.g., transformers, recurrent networks) to assess generalizability
3. Measure actual storage savings from ternary freezing mask encoding versus claimed compression benefits