---
ver: rpa2
title: Private Fine-tuning of Large Language Models with Zeroth-order Optimization
arxiv_id: '2401.04343'
source_url: https://arxiv.org/abs/2401.04343
tags:
- dp-zo
- privacy
- private
- laplace
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new framework for private fine-tuning of
  large language models using zeroth-order optimization (DP-ZO). The key idea is to
  privatize only the scalar step size in the zeroth-order update, which is memory-efficient.
---

# Private Fine-tuning of Large Language Models with Zeroth-order Optimization

## Quick Facts
- arXiv ID: 2401.04343
- Source URL: https://arxiv.org/abs/2401.04343
- Reference count: 40
- Key outcome: DP-ZO achieves just 1.86% performance degradation due to privacy at (1, 10^-5)-DP when fine-tuning OPT-66B on 1000 training samples from SQuAD, outperforming DP-SGD.

## Executive Summary
This paper introduces a novel approach to private fine-tuning of large language models using zeroth-order optimization (DP-ZO). The key innovation lies in privatizing only the scalar step size in the zeroth-order update, which is memory-efficient compared to traditional gradient-based methods. DP-ZO can be instantiated with either Laplace or Gaussian noise, providing strong privacy-utility tradeoffs. The framework achieves just 1.86% performance degradation due to privacy at (1, 10^-5)-DP when fine-tuning OPT-66B on 1000 training samples from SQuAD, outperforming DP-SGD. DP-ZO also provides the first non-trivial pure ε-DP result by using the Laplace mechanism.

## Method Summary
The method introduces DP-ZO, which leverages zeroth-order optimization for private fine-tuning of large language models. Instead of computing gradients directly, DP-ZO estimates the gradient direction using random perturbations and then scales this estimate by a privatized step size. The critical innovation is that only the scalar step size needs to be privatized, rather than the entire gradient vector, making it memory-efficient for large models. The framework can be instantiated with either Laplace or Gaussian noise mechanisms, providing flexibility in the privacy-utility tradeoff. The approach requires access to public data for generating random directions, which are used to estimate the gradient in the optimization process.

## Key Results
- DP-ZO achieves 1.86% performance degradation at (1, 10^-5)-DP when fine-tuning OPT-66B on SQuAD with 1000 samples
- Outperforms DP-SGD in privacy-utility tradeoff on the tested configuration
- Provides the first non-trivial pure ε-DP result using Laplace mechanism for fine-tuning LLMs

## Why This Works (Mechanism)
The effectiveness of DP-ZO stems from its memory-efficient privatization strategy. By privatizing only the scalar step size rather than the entire gradient vector, the method significantly reduces the noise required to achieve differential privacy. This is particularly advantageous for large language models where gradients are high-dimensional. The zeroth-order approach estimates gradients through random perturbations, and the privatization of the step size provides the necessary privacy guarantee while maintaining utility. The ability to use either Laplace or Gaussian noise mechanisms allows for flexibility in achieving different privacy-utility tradeoffs.

## Foundational Learning

**Zeroth-order Optimization**: Optimization method that estimates gradients through function evaluations at perturbed points, rather than computing gradients directly.
- Why needed: Enables gradient estimation without explicit computation, crucial for memory-efficient private fine-tuning
- Quick check: Verify gradient estimates converge as number of perturbations increases

**Differential Privacy**: Mathematical framework for quantifying privacy guarantees in data analysis.
- Why needed: Provides rigorous privacy guarantees for fine-tuning LLMs on sensitive data
- Quick check: Verify privacy accountant correctly accumulates privacy loss

**Laplace Mechanism**: Privacy mechanism that adds Laplace noise for achieving pure ε-DP.
- Why needed: Enables the first non-trivial pure ε-DP result for LLM fine-tuning
- Quick check: Verify noise scale matches theoretical privacy requirements

**Gaussian Mechanism**: Privacy mechanism that adds Gaussian noise for achieving (ε,δ)-DP.
- Why needed: Provides alternative privacy instantiation with potentially better utility
- Quick check: Verify noise parameters satisfy moments accountant bounds

## Architecture Onboarding

**Component Map**: Public data -> Random direction generator -> Perturbation function -> Zeroth-order estimator -> Step size privatizer -> Model updater

**Critical Path**: The core optimization loop where random directions are generated, perturbations are applied to the model, function values are evaluated, gradient estimates are computed, step size is privatized, and model parameters are updated.

**Design Tradeoffs**: Memory efficiency vs. gradient accuracy (ZO vs. first-order), pure ε-DP vs. (ε,δ)-DP utility tradeoffs, public data dependency vs. privacy guarantees.

**Failure Signatures**: Degradation in gradient estimation quality with insufficient perturbations, privacy budget exhaustion from poor noise calibration, model collapse from excessive noise in step size privatization.

**First Experiments**:
1. Verify gradient estimation quality with varying numbers of random perturbations
2. Test privacy-utility tradeoff with different noise scales in step size privatization
3. Evaluate sensitivity to public data quality and quantity for random direction generation

## Open Questions the Paper Calls Out
None

## Limitations
- Requires access to public data for generating random directions, which may not always be available
- Assumes knowledge of Lipschitz constants for gradient and zeroth-order operators, which can be challenging to estimate accurately
- Theoretical analysis relies on specific assumptions about loss landscape and model properties that may not hold universally

## Confidence
- Main SQuAD experiments on OPT-66B: High
- Pure ε-DP claims: Medium
- Theoretical privacy guarantees: High

## Next Checks
1. Test DP-ZO on larger datasets and model sizes to verify if claimed privacy-utility tradeoffs hold at scale
2. Investigate sensitivity of DP-ZO's performance to the quality and quantity of public data used for generating random directions
3. Evaluate DP-ZO on a diverse set of NLP tasks beyond SQuAD to assess general applicability