---
ver: rpa2
title: On the growth of the parameters of approximating ReLU neural networks
arxiv_id: '2406.14936'
source_url: https://arxiv.org/abs/2406.14936
tags:
- lemma
- relu
- parameters
- network
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the asymptotic growth of parameters in ReLU
  neural networks as they approximate smooth functions. While most literature focuses
  on approximation accuracy given fixed architecture, the authors analyze how the
  parameters of optimal networks scale with width and depth.
---

# On the growth of the parameters of approximating ReLU neural networks

## Quick Facts
- arXiv ID: 2406.14936
- Source URL: https://arxiv.org/abs/2406.14936
- Reference count: 16
- One-line primary result: Deep ReLU networks can achieve polynomial parameter growth for smooth function approximation, with parameters bounded by O(max(N^{(6q-3)/d} L^{(6q-2)/d}, N² L³))

## Executive Summary
This paper investigates the asymptotic growth of parameters in neural networks as they approximate smooth functions, focusing on how parameters scale with network width and depth rather than just approximation accuracy. While shallow networks with Gaussian or logistic activations exhibit exponential parameter growth, the authors show that deep ReLU networks can achieve polynomial parameter growth by modifying the architecture. Specifically, they construct networks with width O(N log N) and depth O(L² log L) that approximate C^q([0,1]^d) functions with error O(N^{-2q/d} L^{-2q/d}) while maintaining parameters growing as O(max(N^{(6q-3)/d} L^{(6q-2)/d}, N² L³)). The ReQU activation function achieves uniformly bounded parameters but with worse approximation rates. These findings provide crucial insights into parameter scaling in neural network approximation and have significant implications for understanding training dynamics and error analysis.

## Method Summary
The authors analyze fully connected feed forward ReLU neural networks approximating C^q([0,1]^d) functions by following the construction from [12] Theorem 1.1, which provides explicit proofs for smooth function approximation. The method involves analyzing subnetworks for projections, multinomial approximations, derivative fitting, and binomial approximation. By modifying the depth to O(L² log L) while maintaining width at O(N log N), they achieve polynomial parameter growth bounds. The analysis traces parameter complexity through each subnetwork component, combining bounds to establish the overall growth rate of O(max(N^{(6q-3)/d} L^{(6q-2)/d}, N² L³)). The approach also examines negative results for shallow networks with Gaussian and logistic activations, showing exponential parameter growth, and compares these findings with ReQU networks that achieve uniformly bounded parameters.

## Key Results
- Deep ReLU networks can achieve polynomial parameter growth O(max(N^{(6q-3)/d} L^{(6q-2)/d}, N² L³)) for C^q([0,1]^d) function approximation
- Shallow networks with Gaussian or logistic activations exhibit exponential parameter growth under mild assumptions
- ReQU networks achieve uniformly bounded parameters but with worse approximation rates compared to ReLU networks
- The polynomial parameter growth for deep ReLU networks represents an improvement over existing results, especially for high-dimensional inputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ReLU neural networks can achieve polynomial growth of parameters when approximating smooth functions by modifying the depth.
- Mechanism: By increasing the depth to O(L² log L) and maintaining width at O(N log N), the network can approximate C^q([0,1]^d) functions with error O(N^{-2q/d} L^{-2q/d}) while keeping parameters bounded by O(max(N^{(6q-3)/d} L^{(6q-2)/d}, N² L³)).
- Core assumption: The construction in [12] can be modified to maintain approximation accuracy while improving parameter growth bounds.
- Evidence anchors:
  - [abstract] states that for deep ReLU networks, the authors modify the architecture to achieve polynomial parameter growth.
  - [section] describes how the main result shows that for ReLU architecture with state of the art approximation error, the realizing parameters grow at most polynomially.
- Break condition: If the depth modification significantly increases the approximation error beyond O(N^{-2q/d} L^{-2q/d}), the polynomial parameter growth claim would fail.

### Mechanism 2
- Claim: Shallow networks with Gaussian or logistic activations can have exponentially growing parameters.
- Mechanism: The single-hidden-layer networks constructed in [13] for smooth function approximation have parameters that grow at least exponentially under mild assumptions.
- Core assumption: The specific construction in [13] for Gaussian and logistic activation functions leads to exponential parameter growth.
- Evidence anchors:
  - [abstract] states that for single-hidden-layer networks with Gaussian or logistic activations, the authors show parameters can grow exponentially.
  - [section] provides a negative result showing that for the Gaussian and logistic activation function, the parameters realizing the approximating neural networks grow at least exponentially under mild assumptions.
- Break condition: If the approximation can be achieved with bounded parameters for these activation functions, the exponential growth claim would be invalid.

### Mechanism 3
- Claim: ReQU networks achieve uniformly bounded parameters for smooth function approximation.
- Mechanism: The ReQU activation function σ_ReQU(x) = (x ∨ 0)² allows for approximation with uniformly bounded weights due to its ability to represent piecewise polynomials exactly.
- Core assumption: The ReQU activation function's properties enable uniform boundedness of parameters in the approximation of smooth functions.
- Evidence anchors:
  - [abstract] mentions that ReQU networks achieve uniformly bounded parameters.
  - [section] explains that the choice of ReQU in [1] is crucial as it can represent piecewise polynomials exactly, thus, in particular also the identity mapping and products.
- Break condition: If the ReQU activation function cannot represent piecewise polynomials exactly or if the approximation error increases significantly, the uniformly bounded parameters claim would fail.

## Foundational Learning

- Concept: Universal approximation property of neural networks
  - Why needed here: Understanding the universal approximation property is crucial for grasping why the paper focuses on parameter growth rather than approximation accuracy.
  - Quick check question: What is the main difference between universal approximation and the focus of this paper?

- Concept: Sobolev and Hölder spaces
  - Why needed here: The paper compares results for functions in different function spaces (C^q([0,1]^d), Sobolev-regular, Hölder-smooth), so understanding these spaces is essential for interpreting the results.
  - Quick check question: How do the regularity requirements for C^q([0,1]^d) functions compare to those for Sobolev-regular functions?

- Concept: Parameter complexity in neural networks
  - Why needed here: The paper's main focus is on the asymptotic growth of parameters in approximating networks, so understanding how parameter complexity is measured and analyzed is crucial.
  - Quick check question: What is the significance of analyzing the supremum norm of parameters in neural network approximation?

## Architecture Onboarding

- Component map:
  - Single-hidden-layer networks (shallow) -> Activation functions: Gaussian, logistic, ReQU -> Parameter growth: Exponential, exponential, bounded
  - Deep ReLU networks -> Architecture: Width O(N log N), depth O(L² log L) -> Parameter growth: Polynomial O(max(N^{(6q-3)/d} L^{(6q-2)/d}, N² L³))

- Critical path:
  1. Understand the universal approximation property
  2. Analyze the function spaces and regularity requirements
  3. Compare parameter growth rates for different activation functions and architectures
  4. Examine the specific constructions for shallow and deep networks
  5. Evaluate the implications for error analysis and training consistency

- Design tradeoffs:
  - Shallow vs. deep networks: Shallow networks may have simpler structures but can suffer from exponential parameter growth, while deep networks offer polynomial parameter growth at the cost of increased depth.
  - Activation functions: Different activation functions (Gaussian, logistic, ReQU) lead to varying parameter growth rates, with ReQU achieving uniformly bounded parameters but potentially requiring more complex constructions.

- Failure signatures:
  - If the approximation error significantly increases when modifying the depth for polynomial parameter growth
  - If the exponential parameter growth in shallow networks cannot be avoided for certain activation functions
  - If the ReQU activation function fails to provide uniformly bounded parameters for smooth function approximation

- First 3 experiments:
  1. Implement the single-hidden-layer network construction from [13] for Gaussian and logistic activation functions and measure the parameter growth rate for smooth function approximation.
  2. Modify the deep ReLU network construction from [12] to achieve polynomial parameter growth and verify the approximation accuracy and parameter bounds.
  3. Compare the parameter growth rates and approximation errors for ReQU networks with other activation functions for smooth function approximation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the polynomial parameter growth bounds for ReLU networks be further improved beyond the O(max(N^{(6q-3)/d} L^{(6q-2)/d}, N² L³)) rate?
- Basis in paper: [explicit] The paper establishes that parameters grow at most polynomially with the given rate, but does not prove this is optimal
- Why unresolved: The authors explicitly state they do not make any statement on optimality of the asymptotic parameter growth
- What evidence would resolve it: Either a matching lower bound proof showing the polynomial rate is tight, or construction of networks with better parameter scaling

### Open Question 2
- Question: How do the parameter growth bounds change for ReLU networks when approximating functions with less than C^q smoothness?
- Basis in paper: [inferred] The current analysis focuses on C^q functions and the parameter bounds depend on q through the (6q-3)/d and (6q-2)/d exponents
- Why unresolved: The paper does not explore the case when q is not an integer or when functions have lower regularity
- What evidence would resolve it: Analysis of parameter scaling for specific function classes with fractional smoothness or Sobolev regularity

### Open Question 3
- Question: Can the ReQU activation function's uniformly bounded parameters be maintained while achieving better approximation rates than the current O(N^{-q/d})?
- Basis in paper: [explicit] The paper notes ReQU networks achieve uniformly bounded parameters but with worse approximation rates compared to ReLU networks
- Why unresolved: The trade-off between parameter boundedness and approximation quality for ReQU remains unexplored
- What evidence would resolve it: Construction of ReQU networks that simultaneously achieve better approximation rates while maintaining parameter bounds

## Limitations
- The analysis is primarily theoretical and may not directly translate to practical implementation scenarios
- Exact constants for width and depth specifications are not provided, potentially affecting precise parameter scaling
- The subnetwork constructions rely on referenced techniques that are not fully detailed in this paper

## Confidence
- High Confidence: The polynomial parameter growth result for deep ReLU networks (O(max(N^{(6q-3)/d} L^{(6q-2)/d}, N² L³))) is well-supported by the theoretical analysis and construction modifications
- Medium Confidence: The exponential parameter growth claim for shallow networks with Gaussian/logistic activations is supported but relies on constructions from [13]
- Medium Confidence: The uniformly bounded parameters claim for ReQU networks is theoretically sound but may face practical implementation challenges

## Next Checks
1. Implement the modified deep ReLU network construction and empirically verify the polynomial parameter growth against the theoretical bounds
2. Conduct numerical experiments comparing parameter growth rates across different activation functions (ReLU, Gaussian, logistic, ReQU) for smooth function approximation
3. Analyze the impact of depth modifications on approximation accuracy in practice, particularly for high-dimensional input spaces