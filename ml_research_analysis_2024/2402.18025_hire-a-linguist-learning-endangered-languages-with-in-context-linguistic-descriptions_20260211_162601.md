---
ver: rpa2
title: 'Hire a Linguist!: Learning Endangered Languages with In-Context Linguistic
  Descriptions'
arxiv_id: '2402.18025'
source_url: https://arxiv.org/abs/2402.18025
tags:
- languages
- language
- lingo
- translation
- grammar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LINGO LLM enables large language models to process and translate
  endangered languages by incorporating linguistic descriptions such as dictionaries,
  grammar books, and morphological analyzers into prompts. This training-free approach
  significantly improves translation quality from near-zero BLEU scores to 10.5 BLEU
  for 10 language directions, and also enhances performance on mathematical reasoning,
  response selection, and word reordering tasks across 8 endangered or low-resource
  languages.
---

# Hire a Linguist!: Learning Endangered Languages with In-Context Linguistic Descriptions

## Quick Facts
- arXiv ID: 2402.18025
- Source URL: https://arxiv.org/abs/2402.18025
- Reference count: 16
- Primary result: LINGO LLM improves translation quality from near-zero BLEU to 10.5 BLEU for 10 language directions using linguistic descriptions

## Executive Summary
This paper presents LINGO LLM, a training-free approach that enables large language models to process and translate endangered languages by incorporating linguistic descriptions such as dictionaries, grammar books, and morphological analyzers into prompts. The method significantly improves translation quality for 8 endangered or low-resource languages, achieving up to 10.5 spBLEU for 10 language directions. LINGO LLM also demonstrates improvements across multiple tasks including mathematical reasoning, response selection, and word reordering, addressing the challenge of limited training data for endangered languages.

## Method Summary
LINGO LLM uses morphological analysis, dictionary lookup, and grammar knowledge integration into LLM prompts to enable processing of endangered languages without requiring model fine-tuning. The approach takes linguistic descriptions (morphological analyzers, dictionaries, grammar books) as inputs and prepares annotated input for the LLM by splitting words into morphemes, mapping morphemes to dictionary glosses, and integrating relevant grammar features. The method was evaluated across 5 tasks (translation, mathematical reasoning, response selection, word reordering, keyword-to-text) using 8 endangered or low-resource languages.

## Key Results
- Translation quality improved from near-zero BLEU scores to 10.5 BLEU for 10 language directions
- Significant improvements across 8 endangered/low-resource languages including Manchu, Gitksan, Uspanteko, Natugu, Tsez, Wolof, Arapaho, and Bribri
- Enhanced performance on mathematical reasoning, response selection, and word reordering tasks beyond just translation

## Why This Works (Mechanism)
LINGO LLM works by providing LLMs with structured linguistic knowledge that compensates for the lack of large-scale training data in endangered languages. By incorporating morphological analyzers to break down complex word structures, dictionaries to provide word meanings, and grammar books to capture syntactic patterns, the approach gives LLMs the contextual information needed to understand and generate text in languages they haven't been trained on extensively. The in-context learning paradigm allows the model to leverage this linguistic knowledge at inference time without requiring expensive fine-tuning.

## Foundational Learning
- Morphological analysis - Breaking words into constituent morphemes; needed for understanding complex word structures in morphologically rich languages; quick check: verify analyzer correctly segments sample words
- Dictionary lookup - Mapping morphemes to their meanings; essential for translation accuracy; quick check: measure dictionary coverage percentage
- Grammar integration - Incorporating syntactic rules from grammar books; required for proper sentence structure; quick check: test with and without grammar excerpts
- In-context learning - Providing information within prompts rather than fine-tuning; enables zero-shot adaptation; quick check: compare with fine-tuned baseline
- Parallel text alignment - Matching source and target language sentences; foundation for translation evaluation; quick check: verify alignment quality

## Architecture Onboarding
- Component map: Linguistic descriptions (morphological analyzer, dictionary, grammar book) -> Preprocessing pipeline -> LLM prompt -> Output generation
- Critical path: Morphological analysis → Dictionary lookup → Grammar integration → LLM processing
- Design tradeoffs: Training-free approach vs. fine-tuning (speed/efficiency vs. potentially higher accuracy); manual effort in preparing linguistic resources vs. automated data collection
- Failure signatures: Low dictionary coverage leading to poor translation; LLM failing to locate relevant grammar sections in long contexts
- Three first experiments: 1) Test translation quality with 50%, 70%, and 90% dictionary coverage to identify minimum viable threshold; 2) Compare performance with and without grammar book inclusion to measure grammar knowledge impact; 3) Evaluate system robustness when morphological analyzer fails to correctly segment words

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy dependence on availability and quality of linguistic descriptions, which may be incomplete or inaccessible for many endangered languages
- Dictionary coverage emerged as a significant bottleneck, with performance degrading when coverage drops below 90%
- Requires substantial manual effort to prepare grammar book excerpts and morphological analyzer outputs, limiting scalability
- Performance could vary significantly with different base models, and reliance on instruction-tuned LLMs may not be accessible to all researchers

## Confidence
- High Confidence: Core methodology of using in-context linguistic descriptions is well-established; reported translation improvements (from near-zero to 10.5 spBLEU) are robust across multiple language pairs
- Medium Confidence: Performance gains on non-translation tasks are promising but may be less consistent due to varying task complexities and language-specific challenges
- Low Confidence: Scalability to additional endangered languages remains uncertain due to dependency on availability of comprehensive linguistic resources

## Next Checks
1. Systematically vary dictionary coverage percentages (e.g., 50%, 70%, 90%) and measure corresponding impact on translation quality to quantify minimum viable coverage threshold
2. Test performance impact of varying length and specificity of grammar book excerpts included in prompts to identify optimal context size for different language complexities
3. Evaluate system performance when using different morphological analyzers or when analyzer fails to correctly segment words to assess approach's resilience to analyzer errors