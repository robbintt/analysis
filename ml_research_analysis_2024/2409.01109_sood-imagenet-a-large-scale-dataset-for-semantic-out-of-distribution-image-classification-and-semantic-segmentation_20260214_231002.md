---
ver: rpa2
title: 'SOOD-ImageNet: a Large-Scale Dataset for Semantic Out-Of-Distribution Image
  Classification and Semantic Segmentation'
arxiv_id: '2409.01109'
source_url: https://arxiv.org/abs/2409.01109
tags:
- semantic
- dataset
- data
- segmentation
- conf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SOOD-ImageNet, a large-scale dataset for
  semantic out-of-distribution (SOOD) generalization in computer vision. SOOD-ImageNet
  addresses limitations of existing OOD benchmarks by focusing on semantic shifts
  and providing a larger scale dataset suitable for modern deep learning models.
---

# SOOD-ImageNet: a Large-Scale Dataset for Semantic Out-Of-Distribution Image Classification and Semantic Segmentation

## Quick Facts
- arXiv ID: 2409.01109
- Source URL: https://arxiv.org/abs/2409.01109
- Reference count: 40
- Introduces SOOD-ImageNet, a large-scale dataset for semantic out-of-distribution generalization

## Executive Summary
This paper introduces SOOD-ImageNet, a large-scale dataset specifically designed for semantic out-of-distribution (SOOD) generalization in computer vision. The dataset addresses limitations of existing OOD benchmarks by focusing on semantic shifts rather than just covariate shifts, providing a larger scale dataset suitable for modern deep learning models. SOOD-ImageNet comprises around 1.6 million images across 56 classes, designed for both image classification and semantic segmentation tasks. The dataset is divided into in-distribution training data and out-of-distribution test data with varying degrees of semantic shift ("Easy" and "Hard" splits).

## Method Summary
The SOOD-ImageNet dataset was created using a novel data engine that combines vision-language models (VLMs) with human verification to ensure scalability and quality. The process begins with identifying related synsets from WordNet hierarchies, then uses VLMs for visual filtering and scoring, followed by targeted human checks on the most promising candidates. A scoring function based on CLIP embeddings was developed to separate in-distribution from out-of-distribution samples. The dataset is split into in-distribution training data (1M images) and out-of-distribution test data ("Easy" and "Hard" splits) based on semantic similarity scores.

## Key Results
- Even state-of-the-art deep learning models struggle with SOOD generalization on SOOD-ImageNet
- Convolutional architectures and SwinV2 outperform ViT architectures, suggesting exploitation of visual priors (hierarchy, translation invariance, locality)
- Extensive experiments highlight the need for specialized solutions to address semantic OOD generalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The proposed data engine successfully creates large-scale SOOD datasets by combining vision-language model filtering with human verification
- Mechanism: The pipeline uses hierarchical semantic relationships from WordNet to identify related synsets, then employs VLMs for visual filtering and scoring, followed by targeted human checks on the most promising candidates
- Core assumption: VLMs can effectively distinguish between visually related and unrelated synsets, and human verification of filtered candidates is more efficient than full manual annotation
- Evidence anchors: [abstract] "ensured the necessary scalability and quality by developing an innovative data engine that leverages the capabilities of modern vision-language models, complemented by accurate human checks"

### Mechanism 2
- Claim: The scoring function based on CLIP embeddings effectively separates in-distribution from out-of-distribution samples
- Mechanism: By computing average cosine similarity between CLIP embeddings of images and class representations, the system creates a probability-like score that determines whether a sub-class is representative of its super-class
- Core assumption: CLIP embeddings capture semantic-visual relationships that align with human perception of class membership
- Evidence anchors: [section 3.1] "we employed CLIP embeddings to measure the correlation"

### Mechanism 3
- Claim: The two-stage filtering process (automatic VLM filtering followed by human verification) achieves both scalability and quality
- Mechanism: Step A removes obvious outliers using automated VLM analysis, reducing the human verification workload in Step B to manageable levels while maintaining quality standards
- Core assumption: Most filtering can be automated without significant quality loss, and human verification is most effective when focused on pre-filtered candidates
- Evidence anchors: [section 3.1] "we performed two filtering steps, A and B. Step A aimed to remove as many spurious synsets as possible"

## Foundational Learning

- Concept: Semantic shift and its distinction from covariate shift
  - Why needed here: The dataset specifically targets semantic shift (OOD samples from different classes) rather than just covariate shift (same classes, different contexts)
  - Quick check question: If a model trained on "car" should classify "wheelchair" as "chair" class, is this semantic shift or covariate shift?

- Concept: Vision-Language Models and their capabilities
  - Why needed here: The data engine heavily relies on VLMs for both filtering and scoring, requiring understanding of their strengths and limitations
  - Quick check question: Can a VLM trained on general images reliably distinguish between "seed" and "fruit" when both are hyponyms of the same parent class?

- Concept: Hierarchical semantic relationships and taxonomy
  - Why needed here: The construction process uses WordNet hierarchies to identify related classes and measure semantic distance
  - Quick check question: How would you determine if "Golden Delicious" is a valid sub-class of "fruit" versus "seed" in the ImageNet-21K-P hierarchy?

## Architecture Onboarding

- Component map: WordNet hierarchy processor → VLM filter → Human verifier → CLIP scorer → Dataset splitter → Training pipeline → Evaluation framework
- Critical path: WordNet hierarchy extraction → VLM filtering → Human verification → CLIP scoring → Dataset partitioning
- Design tradeoffs: Scale vs quality (more automation reduces quality but increases scale), granularity vs manageability (more partitions enable finer evaluation but increase complexity)
- Failure signatures: Poor separation between Easy/Hard splits (thresholds inappropriate), human verification bottleneck (VLM filtering insufficient), or quality issues in segmentation masks (CLIPSeg or SAM2 failures)
- First 3 experiments:
  1. Test VLM filtering accuracy on a small subset with ground truth labels to establish baseline performance
  2. Validate CLIP scoring by checking if semantically closer sub-classes receive higher scores than distant ones
  3. Verify segmentation quality by comparing CLIPSeg outputs against human annotations on a sample of Easy/Hard test images

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the SOOD-ImageNet dataset performance change if a more sophisticated seed class selection process was used to capture greater diversity from ImageNet-21K-P?
- Basis in paper: [explicit] The paper mentions that the dataset currently covers less than 15% of ImageNet-21K-P and suggests that a more deliberate selection of seed classes could improve coverage.
- Why unresolved: The current dataset uses 106 super-classes selected from other datasets, which may not fully capture the diversity of ImageNet-21K-P. A more sophisticated selection process could potentially lead to better performance.
- What evidence would resolve it: Conducting experiments with a revised seed class selection process and comparing the performance of models trained on the new dataset with those trained on the current SOOD-ImageNet.

### Open Question 2
- Question: How does the performance of models on SOOD-ImageNet change when using a finer-grained partition of the OOD test set beyond the current "Easy" and "Hard" splits?
- Basis in paper: [explicit] The paper states that studying the influence of coarse and fine-grained partitions would be interesting but limits the number of partitions to three for this exploratory work.
- Why unresolved: The current partitioning into "Easy" and "Hard" OOD test sets provides a general measure of performance, but a finer-grained partitioning could reveal more nuanced insights into model behavior across different degrees of semantic shift.
- What evidence would resolve it: Creating additional partitions in the OOD test set and evaluating model performance across these finer-grained splits to observe changes in generalization capabilities.

### Open Question 3
- Question: What is the impact of incorporating additional visual priors beyond hierarchy, translation invariance, and locality on SOOD generalization performance?
- Basis in paper: [inferred] The paper suggests that convolutional architectures and SwinV2 perform better than ViT architectures, possibly due to their exploitation of visual priors. This implies that exploring other visual priors could be beneficial.
- Why unresolved: While the paper identifies some visual priors that contribute to better performance, it does not explore the full range of potential visual priors that could be leveraged for improved SOOD generalization.
- What evidence would resolve it: Designing and training models that incorporate additional visual priors and comparing their performance on SOOD-ImageNet with existing architectures to assess the impact of these priors on generalization.

## Limitations

- The quality of automatically generated segmentation masks may introduce errors that propagate through the dataset
- The VLM filtering stage may exclude valid semantic relationships that are visually dissimilar
- The reliance on a single VLM (PaliGemma) for filtering raises questions about generalizability to different model architectures

## Confidence

- High confidence: The dataset construction methodology and evaluation framework are clearly defined and reproducible
- Medium confidence: The effectiveness of the VLM filtering and human verification pipeline in maintaining quality at scale
- Low confidence: The absolute performance numbers of models on the SOOD-ImageNet benchmark, as they may be influenced by dataset-specific characteristics

## Next Checks

1. Conduct ablation studies comparing different VLM models for the filtering stage to assess robustness to model choice
2. Perform detailed error analysis on the segmentation masks to quantify the quality degradation introduced by the automated pipeline
3. Test the dataset's sensitivity to the 60th percentile threshold by creating alternative splits with different threshold values and comparing model performance trends