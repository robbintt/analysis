---
ver: rpa2
title: Asking Multimodal Clarifying Questions in Mixed-Initiative Conversational Search
arxiv_id: '2402.07742'
source_url: https://arxiv.org/abs/2402.07742
tags:
- questions
- retrieval
- multimodal
- clarifying
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a multimodal query clarification (MQC) task
  to improve conversational search by adding relevant images to clarifying questions.
  The authors collect a new dataset Melon containing 4k multimodal clarifying questions
  and 14k images, and propose a model Marto based on the VLT5 architecture.
---

# Asking Multimodal Clarifying Questions in Mixed-Initiative Conversational Search

## Quick Facts
- arXiv ID: 2402.07742
- Source URL: https://arxiv.org/abs/2402.07742
- Reference count: 40
- Key outcome: Adding images to clarifying questions improves retrieval performance by up to 90%

## Executive Summary
This paper addresses the challenge of query ambiguity in conversational search by proposing a multimodal query clarification (MQC) task. The authors introduce the concept of adding relevant images to clarifying questions to help users disambiguate their information needs. To support this task, they collect a new dataset called Melon containing 4k multimodal clarifying questions and 14k images. They also propose Marto, a model based on the VLT5 architecture, which uses multi-task fine-tuning to perform multimodal-enhanced question classification, image selection, and document retrieval. Experiments demonstrate that incorporating images significantly improves retrieval performance, and Marto outperforms both unimodal and other multimodal baselines.

## Method Summary
The authors propose a multimodal query clarification (MQC) task to improve conversational search by adding relevant images to clarifying questions. They collect a new dataset called Melon containing 4k multimodal clarifying questions and 14k images. The proposed Marto model is based on the VLT5 architecture and uses a multi-task fine-tuning strategy to perform multimodal-enhanced question classification, image selection, and document retrieval. The model is trained to predict the most appropriate clarifying question, select relevant images, and retrieve relevant documents based on the user's query and conversation context.

## Key Results
- Adding images to clarifying questions improves retrieval performance by up to 90%
- Marto outperforms both unimodal and other multimodal baselines in terms of effectiveness and efficiency
- The multi-task fine-tuning strategy contributes to the model's performance

## Why This Works (Mechanism)
The paper proposes a multimodal query clarification (MQC) task to improve conversational search by adding relevant images to clarifying questions. The authors collect a new dataset Melon containing 4k multimodal clarifying questions and 14k images, and propose a model Marto based on the VLT5 architecture. Marto uses a multi-task fine-tuning strategy to perform multimodal-enhanced question classification, image selection, and document retrieval. Experiments show that adding images significantly improves retrieval performance by up to 90%, and that Marto outperforms both unimodal and other multimodal baselines in terms of effectiveness and efficiency.

## Foundational Learning
- Multimodal query clarification (MQC): A task that involves adding relevant images to clarifying questions in conversational search to help users disambiguate their information needs.
  - Why needed: To address the challenge of query ambiguity in conversational search and improve user satisfaction.
  - Quick check: Ensure that the collected clarifying questions and images genuinely reflect real user needs in conversational search scenarios.

- VLT5 architecture: A multimodal encoder-decoder model that can handle both text and image inputs.
  - Why needed: To enable the model to process and generate both text and image content for the MQC task.
  - Quick check: Verify that the VLT5 architecture is suitable for the specific requirements of the MQC task.

- Multi-task fine-tuning: A training strategy that involves optimizing the model for multiple tasks simultaneously.
  - Why needed: To improve the model's performance on the MQC task by leveraging shared representations across tasks.
  - Quick check: Evaluate the contribution of each task to the overall performance and identify potential bottlenecks.

## Architecture Onboarding

Component map: User query -> Conversation context -> Marto model -> Multimodal clarifying question + Images -> Document retrieval

Critical path: The critical path involves processing the user's query and conversation context through the Marto model to generate a multimodal clarifying question and select relevant images, which are then used for document retrieval.

Design tradeoffs: The paper does not explicitly discuss design tradeoffs, but potential tradeoffs include the balance between the number of clarifying questions and images, the computational cost of processing multimodal content, and the trade-off between model complexity and interpretability.

Failure signatures: Potential failure modes include generating irrelevant or unhelpful clarifying questions, selecting inappropriate images, or failing to retrieve relevant documents due to poor query understanding.

Three first experiments:
1. Evaluate the impact of adding images to clarifying questions on user satisfaction and search task completion rates through a user study.
2. Test the Marto model on a different conversational search dataset to assess its generalization capabilities beyond the Melon collection.
3. Perform ablation studies to isolate the contribution of each component in the multi-task fine-tuning strategy and identify the key drivers of performance improvements.

## Open Questions the Paper Calls Out
None

## Limitations
- The dataset construction relies on Amazon Mechanical Turk workers, raising concerns about whether the collected clarifying questions and images genuinely reflect real user needs in conversational search scenarios.
- The evaluation primarily focuses on retrieval effectiveness metrics (Recall, MRR) without measuring actual user satisfaction or interaction quality.
- The experiments compare against relatively few baselines, and the absolute performance numbers suggest room for improvement even with the proposed method.

## Confidence
- **High confidence**: Images significantly improve retrieval performance (supported by 90% improvement claim with experimental evidence)
- **Medium confidence**: Marto architecture is effective and efficient (supported by ablation studies but limited baseline comparisons)
- **Medium confidence**: Multi-task fine-tuning strategy contributes to performance (supported by ablation but could benefit from more detailed analysis)

## Next Checks
1. Conduct a user study to evaluate whether the multimodal clarifying questions actually improve user satisfaction and search task completion rates compared to text-only clarifications.
2. Test the Marto model on a different conversational search dataset to assess generalization beyond the Melon collection.
3. Perform ablation studies specifically isolating the contribution of each component in the multi-task fine-tuning strategy to better understand which aspects drive performance improvements.