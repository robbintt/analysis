---
ver: rpa2
title: 'Multi-BERT: Leveraging Adapters and Prompt Tuning for Low-Resource Multi-Domain
  Adaptation'
arxiv_id: '2404.02335'
source_url: https://arxiv.org/abs/2404.02335
tags:
- domains
- domain
- each
- parameters
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of adapting Named Entity Recognition
  (NER) models to multiple domains, particularly in low-resource settings like Persian
  language. The proposed Multi-BERT approach utilizes a single pre-trained model with
  multiple sets of domain-specific parameters, leveraging prompt tuning and adapter
  techniques.
---

# Multi-BERT: Leveraging Adapters and Prompt Tuning for Low-Resource Multi-Domain Adaptation

## Quick Facts
- arXiv ID: 2404.02335
- Source URL: https://arxiv.org/abs/2404.02335
- Authors: Parham Abed Azad; Hamid Beigy
- Reference count: 2
- One-line primary result: Multi-BERT achieves state-of-the-art NER performance across multiple Persian domains using parameter-efficient fine-tuning with adapters and prompt tuning

## Executive Summary
This paper addresses the challenge of adapting Named Entity Recognition (NER) models to multiple domains, particularly in low-resource settings like Persian language. The proposed Multi-BERT approach utilizes a single pre-trained model with multiple sets of domain-specific parameters, leveraging prompt tuning and adapter techniques. This allows the model to perform comparably to individual domain-specific models while being more efficient in terms of training and storage. Experimental results on various Persian NER datasets demonstrate that Multi-BERT significantly outperforms existing practical models, achieving state-of-the-art results in some cases. The paper also introduces a document-based domain detection pipeline for scenarios with unknown text domains, enhancing the model's adaptability in real-world applications.

## Method Summary
The Multi-BERT approach uses a frozen ParsBERT core with domain-specific LoRA adapters and prompt tuning parameters. Each domain has its own set of parameters while sharing the core model. The training follows a two-step procedure: first pre-training adapters on all domains except the target, then fine-tuning on the target domain data. For inference with unknown domains, a document-based domain detection pipeline classifies groups of sentences to select the appropriate domain-specific parameters. The model uses 18 prompt tokens per layer based on grid search optimization and demonstrates parameter efficiency while maintaining strong NER performance across formal, informal, and multi-domain Persian datasets.

## Key Results
- Multi-BERT outperforms existing practical models on Persian NER datasets, achieving state-of-the-art results in some cases
- The document-based domain detection pipeline achieves 100% accuracy for formality classification using 8-sentence groups
- Parameter-efficient fine-tuning with adapters and prompt tuning enables multi-domain adaptation without catastrophic forgetting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-BERT achieves domain adaptation without catastrophic forgetting by using separate adapter parameters per domain while sharing the core BERT model.
- Mechanism: The model freezes a pre-trained ParsBERT core and attaches domain-specific LoRA adapters and prompt tuning parameters. Each domain uses its own set of parameters, so updates to one domain's parameters don't interfere with others.
- Core assumption: Domain-specific data distributions are sufficiently distinct that separate parameter sets are better than fine-tuning the entire model.
- Evidence anchors:
  - [abstract] "This enables the model to perform comparably to individual models for each domain."
  - [section 3] "each set of these parameters and layers are only used for certain domains."
- Break condition: If domains have highly overlapping data distributions, separate adapters may waste parameters without performance gain.

### Mechanism 2
- Claim: Prompt tuning with 18 tokens per layer provides optimal adaptation efficiency for Persian NER.
- Mechanism: Continuous prompt vectors are added to each layer's input, influencing token representations without modifying the core model. 18 tokens were found via grid search to balance performance and parameter count.
- Core assumption: A small set of learned prompt tokens can effectively shift model behavior for domain-specific NER tasks.
- Evidence anchors:
  - [section 4.3] "we came to the final conclusion that the best number of tokens to add to our prompt is eighteen."
  - [section 3] "Prefix tuning... has been shown to achieve comparable performance to full model fine-tuning while requiring the tuning of only a tiny fraction of the parameters."
- Break condition: If the optimal token count is domain-dependent or dataset-specific, the fixed 18-token choice may underperform.

### Mechanism 3
- Claim: Document-based domain detection enables inference without prior domain labels by classifying groups of sentences.
- Mechanism: Groups of 8 sentences are concatenated (up to 512 tokens) and passed through a classification adapter to predict the domain, then the corresponding NER parameters are applied.
- Core assumption: Short text groups contain enough contextual clues to reliably determine the domain.
- Evidence anchors:
  - [section 5] "we introduce an independent context model by fine-tuning a new set of parameters to our core model."
  - [section 6] "if we use the model for sets of 8, it could understand the formality of the inputs completely with a 100% accuracy."
- Break condition: If individual sentences are too short or ambiguous, the group-based classifier may misclassify domains.

## Foundational Learning

- Concept: Parameter-efficient fine-tuning (PEFT)
  - Why needed here: The paper uses adapters and prompt tuning instead of full fine-tuning to adapt to multiple domains efficiently.
  - Quick check question: What are the key differences between LoRA adapters and prompt tuning in terms of parameter count and training speed?

- Concept: Domain adaptation in NER
  - Why needed here: The model must handle multiple Persian NER datasets with different entity schemas and noise levels.
  - Quick check question: How does entity label mismatch between domains affect model design choices?

- Concept: Text classification for domain detection
  - Why needed here: The document-based pipeline uses a classifier to determine which NER parameters to apply.
  - Quick check question: Why does the paper use groups of 8 sentences rather than single sentences for domain classification?

## Architecture Onboarding

- Component map: ParsBERT core -> Domain-specific LoRA adapters -> Domain-specific prompt parameters -> Output layers (shared/separate) -> Domain classifier adapter (optional)

- Critical path:
  1. Pre-train each adapter on all domains except its target
  2. Fine-tune adapter on target domain data
  3. During inference, classify document domain → select corresponding adapter/prompt → generate predictions

- Design tradeoffs:
  - Separate adapters per domain give better specialization but use more memory
  - Shared output layers save parameters but require compatible label schemas
  - Group-based domain detection improves accuracy but adds preprocessing complexity

- Failure signatures:
  - Poor performance on low-resource domains → check adapter initialization and fine-tuning duration
  - Domain classifier confusion → check group size and token truncation strategy
  - Memory overflow → check adapter and prompt parameter counts

- First 3 experiments:
  1. Train a single adapter on all domains and measure performance drop vs domain-specific adapters
  2. Vary prompt token count (10, 18, 25) to find optimal balance for Persian NER
  3. Test domain classifier accuracy with different group sizes (4, 8, 12 sentences)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Multi-BERT compare to fine-tuning a model on all domains combined, in terms of F1 score, for both formal and informal Persian NER datasets?
- Basis in paper: [explicit] The paper mentions that the general model underperforms in every domain, while Multi-BERT with prompt-tuning outperforms fine-tuning a model on multiple instances by a small margin.
- Why unresolved: The paper does not provide a direct comparison of F1 scores between Multi-BERT and a model fine-tuned on all domains combined for both formal and informal datasets.
- What evidence would resolve it: Conducting experiments to compare the F1 scores of Multi-BERT and a model fine-tuned on all domains combined for both formal and informal Persian NER datasets.

### Open Question 2
- Question: What is the impact of using a larger number of domain-specific parameters on the performance of Multi-BERT?
- Basis in paper: [inferred] The paper mentions that each set of parameters belongs to exactly one domain, and that there are multiple sets of domain-specific parameters in Multi-BERT.
- Why unresolved: The paper does not explore the impact of using a larger number of domain-specific parameters on the performance of Multi-BERT.
- What evidence would resolve it: Conducting experiments with varying numbers of domain-specific parameters to assess their impact on Multi-BERT's performance.

### Open Question 3
- Question: How does the document-based domain detection pipeline perform in terms of accuracy when the number of samples per document is increased?
- Basis in paper: [explicit] The paper introduces a document-based domain detection pipeline that uses 8 samples per document and achieves 100% accuracy for formal and informal datasets and 97% accuracy for the ParsNER dataset.
- Why unresolved: The paper does not investigate the performance of the pipeline when the number of samples per document is increased.
- What evidence would resolve it: Conducting experiments with varying numbers of samples per document to evaluate the accuracy of the document-based domain detection pipeline.

## Limitations

- Parameter hyperparameter sensitivity: The reported performance gains may be highly dependent on specific hyperparameter choices that may not generalize across different languages or domain distributions.
- Dataset representativeness: The evaluation uses three Persian NER datasets, but the domain diversity and label schema compatibility across these datasets isn't thoroughly characterized.
- Real-world domain detection performance: The document-based domain detection pipeline's 100% accuracy on formality classification doesn't directly translate to NER domain classification performance.

## Confidence

**High Confidence**: The core architectural design of using separate adapters and prompts per domain while sharing a frozen BERT core is technically sound and well-grounded in existing parameter-efficient fine-tuning literature.

**Medium Confidence**: The reported F1 score improvements and state-of-the-art results are plausible given the strong baseline (ParsBERT) and the effectiveness of adapter-based approaches, though exact magnitudes require more detailed analysis.

**Low Confidence**: Claims about the robustness of the 18-token prompt choice and the universal applicability of the document-based domain detection pipeline lack sufficient validation across different domain distributions and dataset sizes.

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Conduct experiments varying prompt token counts (10, 15, 18, 22, 25) and adapter configurations across all three datasets to determine the stability of performance gains and identify optimal settings for different domain types.

2. **Domain Detection Pipeline Validation**: Implement the document-based domain detection pipeline and evaluate its accuracy specifically on the NER domains used in the paper. Measure the end-to-end impact on NER performance when domain classification is incorrect, including error propagation analysis.

3. **Cross-Domain Transfer Evaluation**: Test Multi-BERT's performance when adapting to completely new domains not seen during pre-training. This would validate the model's ability to generalize beyond the specific Persian datasets used in the paper and assess the true utility of the parameter-efficient approach for low-resource scenarios.