---
ver: rpa2
title: Interpreting Attention Layer Outputs with Sparse Autoencoders
arxiv_id: '2406.17759'
source_url: https://arxiv.org/abs/2406.17759
tags:
- features
- feature
- attention
- induction
- head
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to interpret the outputs of attention
  layers in transformers by applying sparse autoencoders (SAEs) to their activation
  vectors. The authors train SAEs on the pre-linear attention outputs (z vectors)
  across all heads, enabling decomposition into interpretable feature directions.
---

# Interpreting Attention Layer Outputs with Sparse Autoencoders

## Quick Facts
- arXiv ID: 2406.17759
- Source URL: https://arxiv.org/abs/2406.17759
- Reference count: 40
- Primary result: Applying sparse autoencoders to attention layer outputs enables interpretable decomposition of attention computations in transformers.

## Executive Summary
This paper introduces a method to interpret the outputs of attention layers in transformers by applying sparse autoencoders (SAEs) to their activation vectors. The authors train SAEs on the pre-linear attention outputs (z vectors) across all heads, enabling decomposition into interpretable feature directions. They develop techniques for head attribution (via decoder weight norms) and direct feature attribution (via linear transformations), and extend this to Recursive Direct Feature Attribution (RDFA) for tracing feature computation upstream through the model.

The method is validated across multiple model families (including GPT-2 Small and Gemma-2B) and layers, achieving sparsity (L0 < 20), high fidelity (80%+ cross-entropy loss recovered), and interpretability (>80% of sampled features interpretable). The authors identify interpretable feature families in attention outputs including induction, local context, and high-level context features. Key findings include the discovery of "long prefix" vs "short prefix" induction heads and revealing that positional signals in the IOI circuit are determined by position relative to "and" tokens rather than absolute or relative position between subject tokens.

## Method Summary
The authors apply sparse autoencoders to the pre-linear attention outputs (z vectors) of transformers to decompose attention computations into interpretable features. They train SAEs on activations across all heads simultaneously, enabling feature discovery at the layer level rather than head level. The method includes head attribution through decoder weight norms, direct feature attribution via linear transformations, and Recursive Direct Feature Attribution (RDFA) for tracing feature computation upstream through the model. The approach is validated across multiple model families and layers, demonstrating sparsity, high fidelity, and interpretability of discovered features.

## Key Results
- 90% of GPT-2 Small attention heads are polysemantic, with only 14 plausibly monosemantic heads identified
- Discovery of "long prefix" vs "short prefix" induction heads, explaining apparent redundancy in induction mechanisms
- Application to the IOI circuit reveals that positional signals are determined by position relative to "and" tokens rather than absolute or relative position between subject tokens

## Why This Works (Mechanism)
The method works by decomposing the high-dimensional attention output vectors into sparse, interpretable feature directions using sparse autoencoders. Since attention outputs are computed as linear combinations of value vectors weighted by attention scores, they contain rich semantic information about what the model is computing. By applying SAEs to these activations, the authors can identify specific feature directions that correspond to interpretable computational patterns, such as induction, local context tracking, or high-level semantic processing. The head attribution technique leverages the fact that SAE decoder weights indicate which heads contribute most to each feature, while RDFA traces how features are computed through upstream layers.

## Foundational Learning

**Sparse Autoencoders (SAEs)**: Neural networks trained to reconstruct their input while enforcing sparsity on the hidden layer. Why needed: To decompose high-dimensional activation vectors into interpretable, sparse feature directions. Quick check: Verify that the trained SAE can accurately reconstruct inputs while maintaining low L0 norm on activations.

**Attention Output Vectors (z vectors)**: The result of the attention computation before the final linear projection, computed as z = ∑ᵢ aₖᵥᵥᵥᵥ where aₖ are attention weights. Why needed: These vectors contain the raw attention computation results before final processing, making them rich targets for interpretation. Quick check: Confirm that z vectors have the expected dimensionality (num_heads × head_dim).

**Decoder Weight Norms for Head Attribution**: Using the L2 norm of SAE decoder weights to determine which attention heads contribute most to each feature. Why needed: To attribute feature importance back to specific attention heads for circuit analysis. Quick check: Verify that high-norm decoder weights correspond to heads that actually attend to relevant tokens for the feature.

## Architecture Onboarding

Component map: Input tokens -> Embedding layer -> Transformer blocks (LayerNorm, Attention, MLP) -> Output logits. Critical path: Input embedding -> Attention computation (Q, K, V matrices, attention weights) -> Attention output (z vectors) -> SAE decomposition -> Feature interpretation.

Design tradeoffs: The authors chose to apply SAEs at the attention output level rather than individual heads to capture layer-level computation patterns, sacrificing some head-level granularity for richer feature discovery. They use a single SAE across all heads to enable cross-head feature discovery, rather than training separate SAEs per head.

Failure signatures: Poor fidelity (low cross-entropy recovery) indicates the SAE is missing important features. Low sparsity (high L0) suggests the features aren't well-separated. Poor interpretability (>20% uninterpretable features) suggests the SAE isn't capturing meaningful computational patterns.

First experiments:
1. Train SAE on attention outputs and verify fidelity >80% on held-out data
2. Sample and manually interpret 50 features to establish baseline interpretability
3. Apply head attribution to identify top-contributing heads for interpretable features

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis is primarily focused on smaller models (GPT-2 Small, Gemma-2B), raising questions about scalability to frontier models
- Feature attribution methods are heuristic and may not capture all aspects of feature computation
- Interpretability analysis relies heavily on manual sampling and labeling, which could introduce bias or miss less obvious features

## Confidence
High confidence in the basic SAE methodology and fidelity results. Medium confidence in the feature attribution methods and their ability to trace computations through multiple layers. Medium confidence in the interpretability analysis, given its reliance on manual inspection.

## Next Checks
1. Apply the method to larger models (GPT-3, LLaMA) to test scalability and validate findings about polysemanticity at scale
2. Conduct automated feature discovery and validation using techniques like causal interventions or controlled generation to complement manual interpretability analysis
3. Test the robustness of RDFA by comparing its traced computations against ground truth from synthetic circuits or carefully constructed toy models