---
ver: rpa2
title: Uncertainty quantification for probabilistic machine learning in earth observation
  using conformal prediction
arxiv_id: '2401.06421'
source_url: https://arxiv.org/abs/2401.06421
tags: []
core_contribution: This paper assesses the current state of uncertainty quantification
  (UQ) in Earth Observation (EO) and introduces conformal prediction as a robust,
  model-agnostic framework for quantifying uncertainty in machine learning models
  applied to EO data. The authors find that only 20% of reviewed Google Earth Engine
  (GEE) datasets include UQ, with commonly used methods lacking validity guarantees.
---

# Uncertainty quantification for probabilistic machine learning in earth observation using conformal prediction

## Quick Facts
- **arXiv ID:** 2401.06421
- **Source URL:** https://arxiv.org/abs/2401.06421
- **Reference count:** 40
- **Primary result:** Only 20% of GEE datasets include UQ; conformal prediction provides statistically valid uncertainty estimates

## Executive Summary
This paper evaluates the current state of uncertainty quantification (UQ) in Earth Observation (EO) and demonstrates conformal prediction as a robust framework for quantifying uncertainty in machine learning models applied to EO data. The authors conducted a systematic review of Google Earth Engine datasets, finding that only 20% include any form of UQ, with commonly used methods lacking statistical validity guarantees. Through three case studies covering local invasive tree species mapping, continental canopy height estimation, and global land cover classification, they show that conformal prediction can provide computationally efficient, statistically valid uncertainty estimates at pixel level, with empirical coverage closely matching nominal confidence levels (e.g., 95.15% coverage for 95% confidence in canopy height estimation).

## Method Summary
The authors employed conformal prediction, a distribution-free framework that provides statistical guarantees on uncertainty estimates without requiring model retraining. They used split-conformal prediction with quantile regression to generate prediction intervals, implementing both full conformal (which computes nonconformity scores for all training samples) and lightweight conformal (which uses a random subset) variants. The methodology was applied across three diverse EO tasks using Google Earth Engine for computation, with uncertainty quantified through prediction intervals that maintain valid coverage probability. Python and JavaScript implementations were provided as open-source tools to facilitate broader adoption in the EO community.

## Key Results
- Only 20% of reviewed GEE datasets include any form of uncertainty quantification
- Conformal prediction achieved empirical coverage of 95.15% for 95% confidence level in canopy height estimation
- Lightweight conformal prediction reduced computation time by 92% compared to full conformal method while maintaining similar coverage accuracy

## Why This Works (Mechanism)
Conformal prediction works by constructing prediction intervals that maintain statistical validity regardless of the underlying data distribution or model architecture. The method leverages the exchangeability assumption of data points and uses nonconformity scores to determine the size of uncertainty intervals needed to achieve the desired coverage level. By calibrating on a separate validation set, conformal prediction provides finite-sample coverage guarantees, ensuring that the true value falls within the prediction interval at the specified confidence level across the entire dataset.

## Foundational Learning

**Exchangeability assumption**: Data points must be exchangeable (joint distribution invariant to permutations) for conformal prediction validity. *Why needed:* Ensures statistical guarantees hold across the dataset. *Quick check:* Verify data collection methods don't introduce systematic ordering biases.

**Nonconformity scores**: Measures of how different a new prediction is from existing predictions. *Why needed:* Quantifies uncertainty by comparing new instances to training data. *Quick check:* Ensure score distributions are stable across different data subsets.

**Coverage probability**: The proportion of times true values fall within prediction intervals. *Why needed:* Primary metric for validating uncertainty quantification quality. *Quick check:* Calculate empirical coverage on held-out test sets.

## Architecture Onboarding

**Component map:** EO data inputs -> Machine learning model -> Nonconformity scoring -> Prediction intervals -> Uncertainty quantification output

**Critical path:** Data preprocessing → Model inference → Nonconformity score calculation → Interval calibration → Uncertainty output

**Design tradeoffs:** Full conformal provides better coverage but requires O(n) computation vs. O(k) for lightweight conformal (where k << n); split-conformal requires separate calibration data which reduces training data but ensures validity.

**Failure signatures:** Undercoverage occurs when exchangeability assumption violated or calibration set not representative; computational bottlenecks appear with large training sets in full conformal method; overconfident intervals result from insufficient calibration data.

**First experiments:** 1) Run conformal prediction on simple regression task with synthetic data to verify coverage guarantees, 2) Compare full vs. lightweight conformal computation times on representative EO dataset, 3) Test exchangeability assumption by checking prediction interval coverage across different geographic regions.

## Open Questions the Paper Calls Out
None

## Limitations
- Empirical coverage validation performed on relatively small test sets (n=200) may not capture full geographic diversity
- Computational efficiency gains demonstrated only on GEE platform, may not translate to other cloud environments
- Case studies focused on specific tasks (tree species, canopy height, land cover) limiting generalizability

## Confidence
- **High:** Systematic review findings showing limited UQ adoption in existing EO datasets
- **Medium:** Validity guarantees of conformal prediction methods under controlled experimental conditions
- **Low:** Long-term generalization across all EO applications given limited task diversity

## Next Checks
1. Test conformal prediction across broader range of EO tasks including time-series analysis and change detection to verify generalizability
2. Conduct extensive validation on larger, more diverse test sets representing various biomes and sensor types to ensure coverage guarantees hold in production settings
3. Compare computational efficiency against alternative UQ methods (like Bayesian approaches) across different cloud platforms and model architectures to establish consistent performance benefits