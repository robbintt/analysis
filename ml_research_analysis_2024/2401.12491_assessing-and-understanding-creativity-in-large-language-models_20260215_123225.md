---
ver: rpa2
title: Assessing and Understanding Creativity in Large Language Models
arxiv_id: '2401.12491'
source_url: https://arxiv.org/abs/2401.12491
tags:
- creativity
- llms
- task
- language
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study introduces a framework for evaluating the creativity
  of large language models (LLMs) using adapted Torrance Tests of Creative Thinking.
  It constructs a dataset of 700 questions across seven tasks and employs GPT-4 as
  an evaluator across four criteria: Fluency, Flexibility, Originality, and Elaboration.'
---

# Assessing and Understanding Creativity in Large Language Models

## Quick Facts
- arXiv ID: 2401.12491
- Source URL: https://arxiv.org/abs/2401.12491
- Reference count: 40
- Key outcome: Model type matters more than parameter scale for LLM creativity; GPT-4 evaluator shows LLMs excel in Elaboration but lag in Originality across 700 questions and 7 tasks.

## Executive Summary
This study develops a comprehensive framework for evaluating LLM creativity using adapted Torrance Tests of Creative Thinking. The authors construct a dataset of 700 questions across seven tasks and employ GPT-4 as an automated evaluator across four creativity criteria: Fluency, Flexibility, Originality, and Elaboration. Through systematic testing of six diverse LLMs, they demonstrate that model architecture and prompt engineering significantly influence creative output, with instructive prompts and scientist roles enhancing creativity while multi-agent collaboration particularly improves originality.

## Method Summary
The researchers adapted Torrance Tests of Creative Thinking to create a dataset of 700 questions across seven tasks using GPT-4 for question generation. Six LLMs (GPT-3.5, LLaMA-2-13b/70b, Qwen, Vicuna-7b/13b) generated responses to these questions, which were then evaluated by GPT-4 using Likert scales across four creativity criteria. The study analyzed creativity across different model types, tasks, prompt formats (Instructive, Chain-of-Thought, Role-play), and explored correlations with personality traits. Multi-agent collaboration was tested by having multiple models sequentially refine responses.

## Key Results
- Model type matters more than parameter scale for creativity, with architecture differences driving performance gaps
- LLMs generally excel in Elaboration but lag significantly in Originality compared to human benchmarks
- Instructive prompts boost Flexibility and Originality while Chain-of-Thought prompts improve Elaboration
- Multi-agent collaboration enhances Originality but not Fluency or Flexibility
- Scientist role-play setting yields higher creativity scores than other professional roles

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4's evaluation accuracy depends on task-specific prompt framing.
- Mechanism: By instructing GPT-4 to follow explicit evaluation criteria (e.g., "Rate on a 1-5 scale for Fluency"), the model's responses become consistent and comparable across diverse LLMs.
- Core assumption: GPT-4's own reasoning capacity is sufficient to reliably score open-ended creative outputs if the prompt is explicit and constrained.
- Evidence anchors:
  - [abstract]: "we utilized GPT-4 as an evaluator to assess each answer, as GPT-4 is capable of effectively assessing the openness of responses and identifying their shortcomings and errors."
  - [section]: "Under proper prompt engineering, GPT-4 can efficiently and effectively complete the evaluation of the entire results of the dataset."
- Break condition: If GPT-4's own creative limitations bias its scoring, or if task framing is ambiguous, evaluator reliability degrades.

### Mechanism 2
- Claim: Prompt type (Instructive, CoT, Role-play) modulates creativity criteria differently.
- Mechanism: Instructive prompts boost Flexibility and Originality by directing the model toward divergent thinking, while CoT prompts improve Elaboration by encouraging stepwise expansion.
- Core assumption: LLMs respond predictably to prompt structure, and specific prompts can activate distinct cognitive modes analogous to human creative strategies.
- Evidence anchors:
  - [section]: "Instruct prompts significantly enhance both flexibility and originality but do not increase elaboration. On the other hand, CoT prompts slightly improve elaboration."
  - [corpus]: "MuseScorer: Idea Originality Scoring At Scale" (demonstrates that statistical infrequency-based originality scoring can be automated, supporting the idea that model prompts can be engineered for this).
- Break condition: If the model's training data or architecture biases it away from the desired mode, prompt effects may be muted or reversed.

### Mechanism 3
- Claim: Multi-agent collaboration improves Originality but not Fluency or Flexibility.
- Mechanism: Sequential review and refinement by multiple agents exposes each response to fresh perspectives, increasing the chance of novel ideas while risking redundancy in volume.
- Core assumption: Each agent can independently generate ideas without being constrained by the prior agent's style, so novelty accumulates over rounds.
- Evidence anchors:
  - [section]: "When the number of rounds is one, an increase in the number of agents leads to a decrease in the level of creativity across the four criteria. Additionally, in the cases of originality, flexibility, and elaboration, an increase in both rounds and agents enhances the level of creativity, with the most significant improvement observed in originality."
  - [corpus]: "Curiosity-Driven LLM-as-a-judge for Personalized Creative Judgment" (supports the idea that iterative, multi-agent processes can surface more creative judgments).
- Break condition: If agents overfit to each other's styles or if diminishing returns occur after a few rounds.

## Foundational Learning

- Concept: Divergent thinking in psychology.
  - Why needed here: Creativity assessment in this work is adapted from Torrance Tests of Creative Thinking, which rely on divergent thinking tasks.
  - Quick check question: What are the four main criteria used in Torrance Tests for Creative Thinking?
- Concept: Prompt engineering principles.
  - Why needed here: The paper demonstrates that different prompt types (Instructive, CoT, Role-play) yield different creativity outcomes.
  - Quick check question: Which prompt type improves both Flexibility and Originality?
- Concept: LLM-based evaluation methodology.
  - Why needed here: GPT-4 is used as an automated judge, requiring understanding of how to construct reliable evaluation prompts.
  - Quick check question: What is the key difference between using human judges and LLM-based evaluators in this framework?

## Architecture Onboarding

- Component map: Dataset generator (GPT-4-based) → Test dataset (700 questions across 7 tasks) → LLM under test (6 models) → Generates responses → GPT-4 evaluator → Scores responses on 4 criteria → Optional: Multi-agent collaboration layer → Further refines responses
- Critical path: Dataset generation → LLM response generation → GPT-4 evaluation → Statistical analysis
- Design tradeoffs:
  - Automated evaluation (GPT-4) vs. human judgment: Speed and scalability vs. nuanced judgment
  - Broad task coverage vs. depth in individual tasks: 7 tasks provide balance but may dilute focus
  - Single-round vs. multi-round collaboration: Trade-off between simplicity and creativity gains
- Failure signatures:
  - Low inter-model score variance → Dataset may be too easy or evaluator not sensitive enough
  - Inconsistent scoring across tasks → Evaluator prompt not uniform enough
  - Originality scores plateau despite collaboration → Agents are converging on same ideas
- First 3 experiments:
  1. Run the same 700-question dataset through all 6 models with basic prompts; record all four creativity scores.
  2. Repeat with instructive prompts only; compare changes in Flexibility and Originality.
  3. Set up a two-agent collaboration on a subset of tasks; measure impact on Originality vs. Fluency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can creativity assessment frameworks be adapted for large multimodal models (LMMs) that accept inputs beyond text?
- Basis in paper: [explicit] The authors explicitly identify this as a future research direction, noting that current methods rely on verbal tasks and suggesting the need for diverse task formats for LMMs.
- Why unresolved: The paper focuses on text-based evaluation and does not explore multimodal input scenarios or propose specific assessment methodologies for LMMs.
- What evidence would resolve it: Development and validation of creativity assessment tasks incorporating images, audio, or video inputs, along with experimental results comparing LMM performance across modalities.

### Open Question 2
- Question: To what extent does LLM creativity represent genuine creative capability versus sophisticated imitation of human creativity patterns?
- Basis in paper: [explicit] The authors acknowledge this fundamental question, stating that "whether AI models possess true creativity from a human cognitive perspective remains an open question" and noting that LLM creativity "is likely to be an imitation of human creativity through a large number of learning."
- Why unresolved: The paper demonstrates that LLMs can produce creative outputs but does not investigate the underlying cognitive mechanisms or distinguish between genuine creativity and pattern replication.
- What evidence would resolve it: Neuroscientific or cognitive studies comparing human and LLM creative processes, or experiments testing whether LLMs can produce truly novel concepts outside their training distribution.

### Open Question 3
- Question: How does the integration of LLMs with external systems (plugins, agents) affect their creative capabilities compared to standalone models?
- Basis in paper: [explicit] The authors identify this as a future research direction, noting that "LLM's creativity in these cases is bound to be different and needs to be investigated" when considering plugins and agent systems.
- Why unresolved: The paper evaluates only standalone LLMs and does not explore how system integration might enhance or alter creative output.
- What evidence would resolve it: Comparative studies measuring creativity scores of integrated LLM systems versus standalone models across the same tasks, with analysis of how external tools influence creative performance.

## Limitations

- Reliance on GPT-4 as both evaluator and question generator may introduce circular dependency and bias
- Personality trait correlations based on model self-assessment rather than objective measurement
- Focus on text-only tasks limits applicability to multimodal models
- Limited exploration of multi-agent collaboration configurations

## Confidence

**High Confidence**: Model type matters more than parameter scale; LLMs excel at Elaboration but lag in Originality
**Medium Confidence**: Prompt engineering effects; multi-agent collaboration benefits
**Low Confidence**: Personality trait correlations

## Next Checks

1. **Cross-evaluator validation**: Run the same evaluation pipeline using Claude-3.5 or another independent LLM as the judge to verify that GPT-4's scoring patterns aren't systematically biased toward certain model types or training approaches.

2. **Human-LLM agreement study**: Have human experts score a subset of model responses and calculate inter-rater reliability between human judges and GPT-4, particularly for the Originality criterion which showed the largest gap.

3. **Zero-shot vs. few-shot comparison**: Generate a parallel dataset using zero-shot prompts for question creation and compare the resulting creativity assessments to the few-shot approach to test whether the evaluation framework itself introduces bias.