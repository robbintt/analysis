---
ver: rpa2
title: 'NeurCAM: Interpretable Neural Clustering via Additive Models'
arxiv_id: '2408.13361'
source_url: https://arxiv.org/abs/2408.13361
tags:
- clustering
- interpretable
- neurcam
- shape
- cluster
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NeurCAM, an interpretable clustering approach
  using neural Generalized Additive Models (GAMs). The key innovation is combining
  GAMs with neural networks to produce fuzzy cluster memberships while providing additive
  explanations.
---

# NeurCAM: Interpretable Neural Clustering via Additive Models

## Quick Facts
- arXiv ID: 2408.13361
- Source URL: https://arxiv.org/abs/2408.13361
- Authors: Nakul Upadhya; Eldan Cohen
- Reference count: 26
- Key outcome: NeurCAM achieves clustering performance comparable to black-box methods on tabular and text datasets while maintaining interpretability through additive explanations and feature selection gates.

## Executive Summary
NeurCAM introduces an interpretable clustering approach using neural Generalized Additive Models (GAMs) that produces fuzzy cluster memberships with additive explanations. The method combines GAMs with neural networks to create disentangled representations - interpretable features for clustering decisions and transformed features for distance calculations. By employing feature selection gates to enforce sparsity and using self-supervised regularization, NeurCAM achieves clustering performance comparable to black-box methods while providing human-understandable explanations in the original feature space. Experiments demonstrate that NeurCAM significantly outperforms other interpretable baselines on text data, achieving over 92% of black-box model performance with limited feature usage.

## Method Summary
NeurCAM uses neural GAMs to create interpretable clustering by learning additive shape functions for each selected feature or interaction. The method employs entmaxα-based selection gates to limit the number of features and pairwise interactions used in explanations, with temperature annealing forcing hard selection during training. A self-supervised regularization term maintains consistency between the annealed model and its warm-up relaxation. The model separates interpretable features (x) used for cluster assignment from transformed representations (˜x) used in the clustering loss, allowing deep representations to guide clustering while preserving human-understandable explanations. The architecture includes initialization via Mini-Batch K-Means, a warm-up phase with relaxed selection gates, and an annealing phase with KL regularization to prevent degradation.

## Key Results
- NeurCAM achieves clustering performance comparable to black-box methods on tabular datasets while remaining interpretable
- On text data, NeurCAM significantly outperforms other interpretable baselines, achieving over 92% of black-box model performance with limited feature usage
- The selection gate mechanism successfully enforces sparsity, limiting feature usage while maintaining clustering quality
- Including both the fuzzy clustering loss and KL divergence regularization consistently results in lower inertia values across metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NeurCAM achieves clustering performance comparable to black-box methods while maintaining interpretability through disentangled representations.
- Mechanism: The model separates the interpretable feature space (x) used for cluster assignment from a transformed representation (˜x) used in the clustering loss, allowing deep representations to guide clustering while preserving human-understandable explanations.
- Core assumption: The transformed representation captures essential clustering structure while the interpretable representation remains meaningful for explanations.
- Evidence anchors:
  - [abstract] "NeurCAM achieves performance comparable to black-box methods on tabular datasets while remaining interpretable"
  - [section 4.2.3] "We decouple the representations of the data in our loss function into the interpretable representation x and the transformed representation ˜x ∈ RR"
  - [corpus] Weak evidence - corpus does not discuss disentangled representations specifically
- Break condition: If the transformed representation loses clustering-relevant information or the interpretable representation becomes too sparse to be meaningful.

### Mechanism 2
- Claim: Selection gates enforce sparsity in feature usage, making explanations more interpretable by limiting the number of features and interactions.
- Mechanism: The entmaxα-based selection gates learn to select specific features for each shape function, with temperature annealing forcing hard selection as training progresses.
- Core assumption: Users can determine an appropriate number of features (C) that balances interpretability and performance for their specific problem.
- Evidence anchors:
  - [abstract] "we introduce selection gates that explicitly limit the number of features and pairwise interactions leveraged"
  - [section 4.1.2] "We modify the NBM to include these selection gates to obtain the additive model architecture used by NeurCAM"
  - [section 5.5.1] "NeurCAM allows users to explicitly control the number of features and interactions used to construct the clusters through our selection gate mechanism"
- Break condition: If the selection mechanism converges to poor local optima or fails to select truly relevant features.

### Mechanism 3
- Claim: Self-supervised regularization stabilizes training by maintaining consistency between the annealed model and its warm-up relaxation.
- Mechanism: After selection gate annealing begins, a KL-divergence term between the current soft assignments and the warm-up assignments prevents degradation in clustering quality.
- Core assumption: The warm-up phase discovers a reasonable clustering structure that can guide subsequent optimization.
- Evidence anchors:
  - [section 4.2.3] "we add a regularization term that penalizes the KL-divergence between the current mapping w(xn) and the mapping w∗(xn) discovered at the end of the warm-up phase"
  - [section 5.4] "Including both terms in our loss function consistently results in lower values across both metrics"
  - [corpus] Weak evidence - corpus does not discuss KL regularization specifically
- Break condition: If the warm-up solution is poor or the KL term prevents necessary exploration of better clusterings.

## Foundational Learning

- Generalized Additive Models (GAMs)
  - Why needed here: GAMs provide the interpretable foundation that allows NeurCAM to explain clustering decisions through additive shape functions.
  - Quick check question: How do GAMs decompose predictions into additive contributions from individual features?

- Neural Network Backbones for Shape Functions
  - Why needed here: Neural networks allow GAMs to learn flexible, non-linear relationships while maintaining interpretability.
  - Quick check question: What properties must a neural network have to serve as an interpretable shape function?

- Fuzzy Clustering Concepts
  - Why needed here: NeurCAM uses fuzzy assignments rather than hard cluster assignments, requiring understanding of membership weighting.
  - Quick check question: How does the fuzziness parameter m affect cluster assignment uncertainty?

## Architecture Onboarding

- Component map:
Input (x, ˜x) -> Selection gates (entmaxα) -> Shape functions (MLPs) -> Fuzzy clustering loss + KL regularization -> Output (fuzzy memberships with additive explanations)

- Critical path:
1. Preprocess data and create transformed representation
2. Initialize centroids using Mini-Batch K-Means
3. Warm-up phase: Train with relaxed selection gates
4. Annealing phase: Gradually harden selection gates while maintaining warm-up consistency via KL regularization
5. Inference: Use final model to assign clusters and extract explanations

- Design tradeoffs:
- Number of shape functions (C) vs interpretability: Higher C allows more complex explanations but reduces sparsity
- Temperature annealing schedule: Faster annealing risks poor local optima; slower annealing increases training time
- Transformed representation choice: Autoencoder vs SpectralNet embeddings affect clustering quality differently
- Fuzziness parameter (m): Higher values create softer boundaries but may reduce cluster separation

- Failure signatures:
- Selection gates fail to converge to sparse solutions (many features remain selected)
- KL regularization prevents convergence to better clusterings
- Transformed representation loses critical clustering information
- Pairwise interactions overwhelm single-feature effects in high-dimensional data

- First 3 experiments:
1. Test on tabular dataset with original features only (no transformed representation) to validate basic GAM clustering
2. Add denoising autoencoder transformed representation and compare clustering performance
3. Enable pairwise interactions (Neur2CAM) and measure impact on both performance and interpretability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does NeurCAM's interpretability performance scale with the number of clusters (K) in the dataset?
- Basis in paper: [inferred] The paper mentions that NeurCAM was tested on datasets with varying numbers of clusters (2-26) but does not analyze the relationship between cluster count and interpretability.
- Why unresolved: The experiments were conducted on datasets with different K values but no systematic study was performed to determine if interpretability degrades as K increases.
- What evidence would resolve it: A controlled experiment varying K while holding dataset characteristics constant, measuring interpretability metrics (like explanation complexity, feature usage sparsity) as K increases.

### Open Question 2
- Question: What is the impact of using more expressive basis functions (like wavelets or splines) instead of MLPs for the shape functions in NeurCAM?
- Basis in paper: [explicit] The paper mentions that the choice of basis functions for shape functions is an open direction for future work.
- Why unresolved: The current implementation uses MLPs for the shape functions, but the paper suggests that other basis functions could be explored for potentially better interpretability or performance.
- What evidence would resolve it: A comparative study of NeurCAM using different basis function types (MLPs, wavelets, splines) on the same datasets, measuring both performance and interpretability metrics.

### Open Question 3
- Question: How does NeurCAM perform on high-dimensional datasets where the number of features significantly exceeds the number of samples?
- Basis in paper: [inferred] The paper demonstrates NeurCAM on tabular datasets with moderate feature counts but does not test scenarios with high dimensionality relative to sample size.
- Why unresolved: The experiments were conducted on datasets with reasonable feature-to-sample ratios, but no analysis was provided for the high-dimensional setting.
- What evidence would resolve it: Experiments on high-dimensional datasets (e.g., genomics, hyperspectral imaging) where D >> N, comparing NeurCAM's performance and interpretability against other methods in this regime.

## Limitations

- The disentangled representation mechanism relies on transformed embeddings that are not fully detailed in the paper, creating uncertainty about whether performance gains are primarily driven by the neural GAM architecture or the quality of pre-trained embeddings.
- The selection gate mechanism's behavior in high-dimensional settings (particularly with pairwise interactions) is not thoroughly validated, raising concerns about scalability and potential for over-selection in feature-rich domains.
- Claims about significant performance improvements on text data relative to interpretable baselines may be influenced by the quality of pre-trained embeddings rather than the proposed method itself.

## Confidence

- **High Confidence**: NeurCAM's basic architecture combining GAMs with neural networks for interpretable clustering is technically sound and well-motivated.
- **Medium Confidence**: The selection gate mechanism and KL regularization improve clustering stability, though empirical evidence is somewhat limited.
- **Low Confidence**: Claims about significant performance improvements on text data relative to interpretable baselines may be influenced by the quality of pre-trained embeddings rather than the proposed method itself.

## Next Checks

1. **Ablation on Embedding Quality**: Compare NeurCAM performance using random embeddings versus pre-trained embeddings to isolate the contribution of the neural GAM architecture from the transformed representation quality.

2. **Selection Gate Behavior Analysis**: Conduct a systematic study of selection gate sparsity across different dimensionality and feature interaction settings to validate the mechanism's effectiveness and identify failure modes.

3. **Interpretability Validation**: Implement human evaluation studies to assess whether the additive explanations provided by NeurCAM actually improve user understanding of clustering decisions compared to black-box alternatives.