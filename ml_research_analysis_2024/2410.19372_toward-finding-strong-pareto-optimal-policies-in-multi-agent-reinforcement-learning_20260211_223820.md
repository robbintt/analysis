---
ver: rpa2
title: Toward Finding Strong Pareto Optimal Policies in Multi-Agent Reinforcement
  Learning
arxiv_id: '2410.19372'
source_url: https://arxiv.org/abs/2410.19372
tags:
- pareto
- mgda
- optimal
- agent
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper tackles the problem of achieving Pareto optimal policies
  in multi-agent reinforcement learning (MARL) with cooperative reward structures.
  The core issue is that when agents optimize only their own rewards, they may converge
  to suboptimal policies.
---

# Toward Finding Strong Pareto Optimal Policies in Multi-Agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2410.19372
- Source URL: https://arxiv.org/abs/2410.19372
- Authors: Bang Giang Le; Viet Cuong Ta
- Reference count: 6
- Primary result: Proposes MGDA++ to achieve strong Pareto optimal solutions in MARL with cooperative rewards

## Executive Summary
This paper addresses the challenge of achieving Pareto optimal policies in multi-agent reinforcement learning with cooperative reward structures. The authors identify that standard multi-objective optimization methods like MGDA only converge to weak Pareto optimal solutions, which can be significantly suboptimal. They propose MGDA++, an improved algorithm that filters out small gradient norms to converge to strong Pareto optimal solutions. The method is theoretically proven for convex, smooth bi-objective problems and empirically validated on Gridworld benchmarks, demonstrating superior convergence compared to existing methods.

## Method Summary
The authors frame multi-agent reinforcement learning with cooperative rewards as a multi-objective optimization problem where each agent's reward is an objective to be optimized. They propose MGDA++, an extension of the Multiple Gradient Descent Algorithm that filters out small gradient norms (below threshold ϵ) to prevent premature convergence to weak Pareto optimal solutions. The method combines MGDA++ with trust region policy optimization (TRPO) and uses a multi-head value function to estimate returns for each agent separately. During centralized training, agents optimize a convex combination of all agents' rewards rather than just their own, enabling altruistic learning behavior.

## Key Results
- MGDA++ achieves strong Pareto optimal convergence in convex, smooth bi-objective settings where standard MGDA only reaches weak Pareto optimality
- Empirical results on Gridworld Door scenario show MGDA++ achieves near-optimal returns for both agents (4.8 each) compared to other methods
- MGDA++ demonstrates superior convergence properties with smaller variance in final returns across agents

## Why This Works (Mechanism)

### Mechanism 1
Standard MGDA converges only to weak Pareto optimal solutions because it stops at stationary points where gradient norms are small, even if the solution is dominated. In MGDA, the algorithm stops when the convex combination of gradient vectors equals zero. For convex objectives, this condition is sufficient for weak Pareto optimality but not strong Pareto optimality. The stopping criterion is triggered too early when some gradients become small, leaving the solution dominated by other feasible points.

### Mechanism 2
MGDA++ achieves strong Pareto optimality by filtering out small gradient norms and only considering gradients above a threshold ϵ in the optimization. By removing the influence of small gradient norms (which correspond to already-converged objectives), MGDA++ prevents premature termination at weakly optimal points. The algorithm continues optimizing only the non-converged objectives until all gradients are below ϵ, ensuring the final solution is ε-Pareto optimal.

### Mechanism 3
In MARL with cooperative reward structures, agents need to consider other agents' rewards to achieve Pareto optimality, which MGDA++ facilitates through multi-objective optimization. When each agent only optimizes its own reward, it cannot influence other agents' rewards, leading to suboptimal convergence. By treating the multi-agent problem as a multi-objective optimization where each agent's objective is considered, MGDA++ finds policies that improve all agents' returns simultaneously.

## Foundational Learning

- Concept: Multi-objective optimization and Pareto optimality
  - Why needed here: The paper frames MARL with cooperative rewards as a multi-objective optimization problem where agents must find Pareto optimal policies rather than Nash equilibria.
  - Quick check question: What is the difference between weak and strong Pareto optimality?

- Concept: Gradient-based optimization methods (MGDA)
  - Why needed here: MGDA is the baseline algorithm that the paper improves upon, and understanding its convergence properties is crucial for understanding why MGDA++ is needed.
  - Quick check question: How does MGDA find descent directions for multiple objectives simultaneously?

- Concept: Markov games and multi-agent reinforcement learning
  - Why needed here: The paper applies multi-objective optimization techniques to MARL settings, specifically cooperative environments with different reward functions for each agent.
  - Quick check question: What is the difference between fully cooperative and cooperative reward structure settings in MARL?

## Architecture Onboarding

- Component map: Policy network -> Multi-head value function -> Advantage estimation -> MGDA++ optimizer -> Trust region constraint
- Critical path: 1) Collect trajectories from all agents 2) Compute multi-head value function estimates 3) Calculate advantage estimates for each agent 4) Apply MGDA++ to find common descent direction 5) Update policies within trust region
- Design tradeoffs:
  - Single vs. separate policy networks: Single network reduces parameters but may limit expressiveness
  - ϵ threshold in MGDA++: Higher values prevent weak Pareto convergence but may skip important objectives
  - Trust region coefficient: Higher values allow larger updates but may cause instability
- Failure signatures:
  - Only one agent improves while others stagnate: Likely weak Pareto convergence issue
  - All agents improve equally but suboptimally: May indicate insufficient exploration or poor reward shaping
  - High variance in agent performance: Could indicate instability in the trust region updates
- First 3 experiments:
  1. Implement MGDA++ on a simple bi-objective convex optimization problem to verify strong Pareto convergence
  2. Apply MGDA++ to the Gridworld Door scenario to verify cooperative behavior emergence
  3. Test MGDA++ on the Two Corridors scenario to verify handling of asymmetric task difficulties

## Open Questions the Paper Calls Out

### Open Question 1
Does MGDA++ guarantee convergence to strong Pareto optimal solutions for more than two objectives (n > 2) in convex, smooth settings? The authors state that the theoretical analysis is limited to bi-objective problems and that generalizing to general n > 2 is challenging due to the complexity of convergence analysis for multiple objectives.

### Open Question 2
How does the choice of the threshold parameter ϵ in MGDA++ affect the convergence properties and the quality of the final solutions? The paper mentions that MGDA++ removes the effect of small gradient norms by considering only gradients whose norm is greater than a certain threshold, but doesn't provide a systematic study of how different values of ϵ affect the convergence and the quality of solutions.

### Open Question 3
Can the theoretical guarantees of MGDA++ be extended to non-convex, non-smooth settings common in deep reinforcement learning? The authors assume convex and smooth objective functions in their theoretical analysis, but the analysis relies on properties that hold only for convex, smooth functions, such as the monotonicity of the sum of objectives.

## Limitations
- Theoretical guarantees are limited to convex, smooth bi-objective problems and may not extend to non-convex, multi-objective settings common in RL
- The choice of ϵ threshold and its sensitivity to problem characteristics is not thoroughly explored
- Limited comparison with other multi-objective optimization methods specifically designed for RL applications

## Confidence
- Theoretical convergence proof: High (for the specific assumptions made)
- Algorithmic improvement over standard MGDA: Medium
- Empirical superiority in cooperative settings: Medium
- Generalizability to complex MARL environments: Low

## Next Checks
1. Test MGDA++ on non-convex, multi-objective problems to verify convergence properties beyond the theoretical guarantees
2. Conduct sensitivity analysis on the ϵ threshold parameter across different cooperative MARL environments
3. Compare MGDA++ against other state-of-the-art multi-objective optimization methods specifically designed for RL applications