---
ver: rpa2
title: Exploring Explainability in Video Action Recognition
arxiv_id: '2404.09067'
source_url: https://arxiv.org/abs/2404.09067
tags:
- concepts
- video
- action
- recognition
- concept
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores explainability in video action recognition
  by addressing the limitations of existing feature attribution methods like Grad-CAM
  when extended to video tasks. The authors introduce Video-TCAV, an extension of
  the TCAV method for quantifying the importance of specific concepts in the decision-making
  process of video action recognition models.
---

# Exploring Explainability in Video Action Recognition

## Quick Facts
- arXiv ID: 2404.09067
- Source URL: https://arxiv.org/abs/2404.09067
- Authors: Avinab Saha; Shashank Gupta; Sravan Kumar Ankireddy; Karl Chahine; Joydeep Ghosh
- Reference count: 18
- Primary result: Dynamic spatiotemporal concepts are significantly more important than static spatial concepts in later layers of Video Swin Transformer for "playing tennis" class (p < 0.05)

## Executive Summary
This paper addresses the limitations of existing feature attribution methods like Grad-CAM when extended to video tasks, which ignore temporal information and provide only local explanations. The authors introduce Video-TCAV, an extension of the TCAV method for quantifying the importance of specific concepts in video action recognition models. Using YOLO-v7 for object detection and tracking, they generate spatial and spatiotemporal concepts and demonstrate that dynamic concepts are significantly more important than static ones, particularly in deeper layers of the network, with statistical significance.

## Method Summary
Video-TCAV extends the TCAV method to video action recognition by learning Concept Activation Vectors (CAVs) that separate concept-specific video embeddings from random videos in the model's embedding space. The method generates spatial concepts (static object crops) and spatiotemporal concepts (tracked object movements) using YOLO-v7 object detector. For each concept, multiple CAVs are learned and used to compute directional derivatives that measure concept importance. Statistical significance is validated through hypothesis testing with multiple random concept sets using a two-sided t-test with Bonferroni correction.

## Key Results
- Dynamic spatiotemporal concepts show significantly higher importance than static spatial concepts in later layers of Video Swin Transformer
- Temporal concepts dominate over static ones in most layers, particularly the final layer (p < 0.05)
- The importance gap between temporal and static concepts increases with network depth
- Findings align with human motion perception and demonstrate statistical robustness

## Why This Works (Mechanism)

### Mechanism 1
Video-TCAV quantifies concept importance by measuring directional derivatives of activations along concept activation vectors (CAVs) in the embedding space. The method generates CAVs by learning hyperplanes that separate embeddings of concept-specific videos from random videos. For any input video, the directional derivative along the CAV indicates how sensitive the model's predictions are to that concept. This works under the assumption that the learned hyperplane between concept and random videos represents a meaningful axis of variation corresponding to the human-interpretable concept.

### Mechanism 2
Dynamic spatiotemporal concepts are more important than static spatial concepts because later network layers increasingly encode motion information. YOLO-v7 detects and tracks objects across frames to generate spatiotemporal concept videos (e.g., a moving tennis racket). When tested with Video-TCAV, these concepts show higher relative importance scores than static concept videos (same object repeated across frames). The importance gap widens in deeper layers, suggesting that motion features become more discriminative for action recognition as information propagates through the network.

### Mechanism 3
Video-TCAV provides statistically significant, robust concept importance measurements validated through hypothesis testing with random concept sets. For each concept, Video-TCAV generates multiple CAVs using different random concept sets. The relative TCAV scores are computed and tested against 10 random sets using a two-sided t-test with Bonferroni correction. Statistically significant results (p < 0.05) indicate that the concept importance is not due to random chance, validating the robustness of the findings.

## Foundational Learning

- **Gradient-based feature attribution methods (Grad-CAM) vs. concept-based methods (TCAV)**: Understanding Grad-CAM's limitations in videos (ignoring temporal information, being local explanations) motivates the need for Video-TCAV as an alternative that quantifies concept importance globally. Quick check: Why does Grad-CAM fail to capture temporal information when extended to videos by treating each frame independently?

- **Concept activation vectors (CAVs) and directional derivatives in embedding spaces**: Video-TCAV's core mechanism relies on learning CAVs that represent concepts and measuring directional derivatives to quantify concept importance in model predictions. Quick check: How does the directional derivative along a CAV relate to the importance of that concept for a specific model prediction?

- **Statistical hypothesis testing with multiple random baselines**: Validating the robustness and significance of Video-TCAV results requires comparing concept importance scores against multiple random concept sets using appropriate statistical tests. Quick check: Why is it necessary to use multiple random concept sets rather than just one when testing the significance of concept importance scores?

## Architecture Onboarding

- **Component map**: Kinetics-400 Dataset -> YOLO-v7 Object Detector -> Concept Video Generation Pipeline -> Video Swin Transformer Model -> Video-TCAV Framework

- **Critical path**: 
  1. Run YOLO-v7 on Kinetics-400 test videos to detect objects
  2. Generate concept videos (spatial: static object crops; spatiotemporal: tracked object crops)
  3. Extract embeddings from Video Swin Transformer for concept and random videos
  4. Learn CAVs by training classifiers to separate concept vs. random embeddings
  5. Compute relative TCAV scores for each concept across test videos
  6. Perform statistical testing with multiple random concept sets

- **Design tradeoffs**: 
  - Spatial vs. Spatiotemporal concepts: Spatial concepts are easier to generate but miss motion information; spatiotemporal concepts capture motion but require object tracking which may introduce errors
  - Concept granularity: Fine-grained concepts (e.g., specific tennis strokes) may be more discriminative but harder to generate; coarse concepts (e.g., tennis racket) are easier but less specific
  - Statistical validation: Using more random concept sets increases robustness but also computational cost

- **Failure signatures**:
  - CAVs show no separation between concept and random embeddings (flat importance scores)
  - Statistical tests show non-significant results (p > 0.05) despite manual inspection suggesting importance
  - Temporal concepts show similar importance to static concepts across all layers (suggesting network doesn't encode motion hierarchically)
  - YOLO-v7 fails to detect key objects, leading to missing or incorrect concept videos

- **First 3 experiments**:
  1. Reproduce Grad-CAM results on Video Swin Transformer for playing tennis class to establish baseline limitations
  2. Generate spatial concepts (tennis racket, sports ball, person) and compute their relative TCAV scores across all layers
  3. Generate spatiotemporal concepts (moving tennis racket, moving person) and compare their relative TCAV scores to spatial concepts in later layers

## Open Questions the Paper Calls Out
- **Open Question 1**: How robust are Video-TCAV results to different concept generation methods, such as using text-to-video diffusion models instead of YOLO-v7? The authors suggest this as a future research direction but only used YOLO-v7 in their study.
- **Open Question 2**: How vulnerable is Video-TCAV to adversarial attacks that could artificially raise or lower the importance of specific concepts? The authors mention this potential weakness but do not test for it.
- **Open Question 3**: How generalizable are the findings that temporal concepts are more important than spatial concepts across different video action recognition models and datasets? The study only used Video Swin Transformer on Kinetics-400, limiting generalizability.

## Limitations
- Method effectiveness heavily depends on quality of object detection and tracking by YOLO-v7, which could introduce errors
- Statistical significance testing relies on multiple random concept sets, but methodology for generating these sets remains unclear
- Paper doesn't address potential correlations between different concepts or scalability to other action classes

## Confidence
- **High Confidence**: The mechanism explaining how Video-TCAV quantifies concept importance through directional derivatives along CAVs is well-supported by the paper's theoretical framework and mathematical formulation.
- **Medium Confidence**: The assumption that the observed concept importance gap reflects hierarchical motion encoding in the Video Swin Transformer architecture is reasonable but not conclusively proven.
- **Low Confidence**: The claim that Video-TCAV findings "correlate well with how the human brain perceives motion" is more speculative and lacks direct experimental validation.

## Next Checks
1. Implement ablation studies removing motion information from spatiotemporal concepts (e.g., using optical flow magnitude thresholds) to verify that observed importance is specifically due to motion rather than other factors.
2. Test Video-TCAV on additional action classes with varying motion characteristics (e.g., "playing basketball" vs. "watching TV") to assess generalizability of the spatiotemporal vs. spatial concept importance findings.
3. Conduct human perception studies comparing concept importance rankings from Video-TCAV against human judgments of concept relevance for action recognition, to validate the claimed correlation with human motion perception.