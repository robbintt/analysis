---
ver: rpa2
title: Differentially Private Fair Binary Classifications
arxiv_id: '2402.15603'
source_url: https://arxiv.org/abs/2402.15603
tags:
- classi
- algorithm
- dp-fermi
- privacy
- fair
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a differentially private fair binary classification
  algorithm using decoupled classifiers and post-processing. The method first trains
  separate classifiers for each demographic group with differential privacy, then
  applies a private post-processing step to achieve statistical parity with minimal
  prediction changes.
---

# Differentially Private Fair Binary Classifications

## Quick Facts
- **arXiv ID**: 2402.15603
- **Source URL**: https://arxiv.org/abs/2402.15603
- **Reference count**: 40
- **Key outcome**: Differentially private fair binary classification using decoupled classifiers and post-processing achieves better statistical parity than DP-FERMI while maintaining similar accuracy.

## Executive Summary
This paper proposes a differentially private fair binary classification algorithm that decouples classifier training by demographic group and applies post-processing to achieve statistical parity. The method trains separate DP classifiers for each group, then adjusts predictions to minimize disparate impact while maintaining privacy guarantees. Experiments on Adult and Credit Card datasets demonstrate improved fairness metrics compared to DP-FERMI while preserving accuracy.

## Method Summary
The method trains decoupled DP classifiers for each demographic group using DP-SGD, then applies a post-processing step using Laplace mechanism to privately estimate prediction proportions. These estimates drive a randomized adjustment that enforces statistical parity with minimal prediction changes. The algorithm provides both expected and high-probability bounds on fairness and utility, with privacy guarantees through composition theorems.

## Key Results
- Outperforms DP-FERMI in statistical parity gap on Adult and Credit Card datasets
- Maintains similar accuracy levels while achieving better fairness
- Provides theoretical guarantees for both fairness and utility under differential privacy
- Shows robustness across different privacy budget allocations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoupling classifiers by sensitive attribute groups and post-processing can reduce disparate impact while maintaining utility.
- Mechanism: Separate DP classifiers are trained for each demographic group, then a post-processing step adjusts the combined classifier to satisfy statistical parity with minimal prediction changes.
- Core assumption: Privacy noise does not eliminate the ability to distinguish between group prediction distributions in a way that supports fairness adjustments.
- Evidence anchors:
  - [abstract] "This paper proposes a differentially private fair binary classification algorithm using decoupled classifiers and post-processing."
  - [section] "Our pipeline is structured to first apply DP and subsequently address fairness using a post-processing step."
  - [corpus] Weak support: related papers focus on DPSGD impacts on fairness but not on decoupled post-processing; no direct corroboration.
- Break condition: If privacy noise overwhelms group-specific signal, the post-processing step cannot achieve fairness without large utility loss.

### Mechanism 2
- Claim: Laplace mechanism applied to subgroup prediction proportion estimates preserves differential privacy while allowing fairness adjustment.
- Mechanism: Private estimates of the proportion of positive predictions in each group (α and β) are obtained by adding Laplace noise, then projected to [0,1]. These noisy proportions drive the randomized fairness adjustment.
- Core assumption: The ℓ1-sensitivity of the proportion query is bounded by 1/n for each group, making Laplace noise sufficient for ε-differential privacy.
- Evidence anchors:
  - [section] "Algorithm 2 is designed to privately estimate these quantities using the Laplace mechanism."
  - [section] "Let L0 ∼ Lap(1/(n0 ε2)) and L1 ∼ Lap(1/(n1 ε3))."
  - [corpus] No direct support; related work uses Gaussian noise for DP-SGD but not Laplace for proportion estimation.
- Break condition: If group sizes are very small, the Laplace noise dominates the estimate, making fairness adjustment unreliable.

### Mechanism 3
- Claim: Statistical parity can be achieved with bounded utility loss even under differential privacy.
- Mechanism: The post-processing step trades off between perfect statistical parity and minimal prediction changes, bounded by the difference in original group prediction proportions minus γ.
- Core assumption: The optimal fair classifier can be approximated by a randomized scheme that interpolates between group predictions based on noisy proportions.
- Evidence anchors:
  - [section] "Our post-processing method aims to derive a fair classifier... while attaining the lower bound in Proposition 1 for γ = 0."
  - [section] "Theorem 3... (Fairness guarantee) ∆SP (hε,δ,Fair) ≤ ..."
  - [corpus] Weak: no direct evidence of utility-fairness trade-off bounds in related work; assumption inferred from proof structure.
- Break condition: If the privacy noise is too large relative to the original group proportion difference, the utility bound becomes vacuous.

## Foundational Learning

- **Concept: Differential Privacy (ε, δ)-DP**
  - Why needed here: Provides the privacy guarantee for both the decoupled classifiers and the post-processing step.
  - Quick check question: What is the composition theorem used to combine the privacy budgets of the two decoupled classifiers and the Laplace mechanism?

- **Concept: Statistical Parity**
  - Why needed here: Defines the fairness metric that the post-processing step aims to satisfy.
  - Quick check question: How is the statistical parity gap calculated from the marginal distributions of the sensitive groups?

- **Concept: Laplace Mechanism**
  - Why needed here: Used to privately estimate the proportion of positive predictions in each group.
  - Quick check question: What is the ℓ1-sensitivity of the proportion query for a group of size n?

## Architecture Onboarding

- **Component map**: Dataset split by sensitive attribute (D0, D1) -> Train DP classifiers hε0,δ0 and hε1,δ1 -> Estimate group proportions with Laplace noise -> Randomized post-processing -> Fair and private classifier hε,δ,Fair

- **Critical path**:
  1. Train hε0,δ0 and hε1,δ1 (DP-SGD)
  2. Compute noisy α, β via Laplace mechanism
  3. Apply randomized post-processing based on α, β
  4. Evaluate fairness and accuracy on test set

- **Design tradeoffs**:
  - Privacy budget allocation: How to split ε, δ among the two classifiers and the Laplace mechanism
  - Noise level: Trade-off between privacy and accuracy of proportion estimates
  - Randomization: Balancing fairness and utility via the post-processing probabilities

- **Failure signatures**:
  - Accuracy drops significantly while fairness gap remains large
  - Privacy budget exhausted before post-processing
  - Numerical instability when projected proportions are near 0 or 1

- **First 3 experiments**:
  1. Run Algorithm 2 on Adult dataset with (ε=3, δ=1e-5) and compare to DP-FERMI; check statistical parity gap.
  2. Vary ε2, ε3 (Laplace noise parameters) to observe trade-off between privacy and fairness.
  3. Test with synthetic data where group proportions are known; verify theoretical bounds empirically.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the choice of decoupling technique affect the fairness-utility tradeoff in differentially private settings?
  - Basis in paper: [explicit] The paper discusses using decoupled classifiers for each demographic group and then applying a post-processing step to achieve statistical parity, building on previous work [9], [10].
  - Why unresolved: While the paper proposes a method using decoupled classifiers, it does not empirically compare different decoupling techniques or analyze how the choice of decoupling method impacts the fairness-utility tradeoff under differential privacy constraints.
  - What evidence would resolve it: Empirical studies comparing various decoupling techniques (e.g., group-specific feature selection, different training objectives) on multiple datasets, measuring their impact on both fairness metrics (statistical parity gap) and utility metrics (accuracy, prediction changes) under different privacy budgets.

- **Open Question 2**: What is the impact of dataset size imbalance between demographic groups on the performance of differentially private fair classification algorithms?
  - Basis in paper: [inferred] The paper mentions that "when the demographic subgroups are imbalanced in the overall population, relying only on prediction changes across the combined distribution can be misleading" and that their approach examines "the sum of the prediction changes within each subgroup's distribution." However, it does not provide a detailed analysis of how imbalance affects performance.
  - Why unresolved: The paper does not conduct experiments or provide theoretical analysis on how varying degrees of imbalance between groups affect the fairness and utility guarantees of the proposed algorithm.
  - What evidence would resolve it: Experiments systematically varying the size ratio between demographic groups, measuring changes in statistical parity gap, accuracy, and utility gap. Theoretical analysis of how these metrics scale with group size ratios under differential privacy constraints.

- **Open Question 3**: How does the proposed algorithm perform on datasets with continuous sensitive attributes (e.g., age) rather than binary ones?
  - Basis in paper: [explicit] The paper explicitly states it considers "binary classification" and uses "binary sensitive attributes" in experiments with Adult and Credit Card datasets.
  - Why unresolved: The proposed algorithm is designed and tested only for binary sensitive attributes, with no discussion or experimentation on how it might extend to continuous or multi-valued sensitive attributes.
  - What evidence would resolve it: Experiments applying the algorithm to datasets with continuous sensitive attributes (e.g., age in Adult dataset, credit limit in Credit Card dataset), with appropriate modifications to handle non-binary attributes, measuring fairness and utility metrics.

## Limitations

- The decoupled classifier approach may fail when privacy noise overwhelms group-specific signal, preventing effective fairness adjustment
- Laplace mechanism sensitivity analysis assumes fixed group sizes, which could break down with imbalanced datasets
- Comparison with DP-FERMI is difficult to verify without full implementation details of the baseline method

## Confidence

- **High Confidence**: The overall methodology of decoupled classifiers with post-processing is sound and follows established DP principles.
- **Medium Confidence**: The theoretical bounds for fairness and utility are mathematically derived but require empirical validation on real-world datasets.
- **Low Confidence**: The comparison with DP-FERMI is difficult to verify without full implementation details of the baseline method.

## Next Checks

1. **Sensitivity Analysis**: Test the algorithm across different privacy budgets (ε) and group size imbalances to identify when the post-processing step fails to achieve statistical parity.
2. **Baseline Reproduction**: Attempt to reproduce DP-FERMI results using available code and documentation to ensure fair comparison.
3. **Ablation Study**: Run experiments with and without the post-processing step to isolate the contribution of each component to the final fairness-accuracy trade-off.