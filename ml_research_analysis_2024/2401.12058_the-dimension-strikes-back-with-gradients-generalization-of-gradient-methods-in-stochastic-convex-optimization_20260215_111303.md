---
ver: rpa2
title: 'The Dimension Strikes Back with Gradients: Generalization of Gradient Methods
  in Stochastic Convex Optimization'
arxiv_id: '2401.12058'
source_url: https://arxiv.org/abs/2401.12058
tags:
- every
- lemma
- then
- loss
- holds
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes tight lower bounds on the sample complexity
  of gradient methods in the stochastic convex optimization (SCO) setting, with a
  focus on the dimension dependence. The authors construct learning problems in polynomial
  dimension where standard gradient descent (GD) and one-pass stochastic gradient
  descent (SGD) converge to solutions with poor generalization despite optimal empirical
  risk performance.
---

# The Dimension Strikes Back with Gradients: Generalization of Gradient Methods in Stochastic Convex Optimization

## Quick Facts
- arXiv ID: 2401.12058
- Source URL: https://arxiv.org/abs/2401.12058
- Authors: Matan Schliserman; Uri Sherman; Tomer Koren
- Reference count: 40
- Key outcome: Establishes tight Ω(√d) lower bounds on sample complexity for GD and SGD in SCO, resolving dimension dependence question

## Executive Summary
This paper establishes tight lower bounds on the sample complexity of gradient methods in stochastic convex optimization (SCO), with a focus on dimension dependence. The authors construct learning problems where standard gradient descent and one-pass stochastic gradient descent converge to solutions with poor generalization despite optimal empirical risk performance. For gradient descent, they show an Ω(√d) lower bound on samples needed to achieve non-trivial test error, resolving an open question about the necessity of dimension dependence. For SGD, they prove a similar Ω(√d) lower bound for achieving non-trivial empirical risk, despite its ability to achieve optimal test performance. These results are tight up to logarithmic factors and improve exponentially over previous work.

## Method Summary
The paper constructs hard instances for gradient methods in SCO using a subspace-based approach. For GD, the construction involves T orthogonal subspaces, each containing a candidate "bad ERM" vector. A carefully designed loss function forces successive GD steps to move in round-robin fashion across subspaces, ensuring all subspaces make progress toward their respective bad ERMs. For SGD, a similar subspace construction is adapted for sequential data exposure, where in each iteration the algorithm is guided to take a step toward a vector present in all previous samples but absent in future ones. The encoding mechanism tracks the training set incrementally, allowing identification of the target vector at each step. Both constructions rely on nearly orthogonal vector sets and precise control over gradient dynamics to establish the lower bounds.

## Key Results
- Proves Ω(√d) lower bound on sample complexity for GD to achieve non-trivial test error in SCO
- Establishes Ω(√d) lower bound for SGD to achieve non-trivial empirical risk, despite optimal test performance
- Shows these bounds are tight up to logarithmic factors, matching existing upper bounds
- Resolves open question about necessity of dimension dependence in SCO sample complexity
- Demonstrates exponential improvement over previous work on SCO lower bounds

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GD converges to a "bad ERM" with constant probability in dimension polynomial in n
- Mechanism: The construction uses T orthogonal subspaces, each containing a candidate bad ERM vector. In each subspace, a single small GD step (of size η) moves toward a vector u0 that is not present in any training sample, making it an overfitted solution. The loss function is designed so that successive GD steps are forced to move in round-robin fashion across subspaces, ensuring all subspaces make progress toward their respective bad ERMs.
- Core assumption: The candidate vectors {u0} can be identified from the training data and encoded into the GD iterate such that subgradients point toward them
- Evidence anchors:
  - [abstract]: "construct a learning problem in polynomial dimension where GD...converges to solutions with poor generalization"
  - [section 3.2]: "we introduce a loss function h that resembles Feldman's function...in T orthogonal subspaces"
  - [corpus]: Weak - related papers discuss risk bounds and gradient estimation but do not construct similar lower bounds
- Break condition: If the encoding/decoding mechanism fails or if the vectors in U are not sufficiently orthogonal, the algorithm may not reach the bad ERM

### Mechanism 2
- Claim: SGD also converges to a solution with poor empirical risk despite optimal population risk
- Mechanism: Similar subspace construction as GD, but adapted for sequential data exposure. In each iteration t, SGD is guided to take a step toward a vector ut that is present in all previous samples but absent in future ones. The encoding mechanism tracks the training set incrementally, allowing identification of ut at each step.
- Core assumption: The probability of finding a vector present in all previous samples but absent in future ones remains constant
- Evidence anchors:
  - [abstract]: "for standard one-pass SGD...provides a similar Ω(√d) lower bound for the sample complexity of SGD to reach a non-trivial empirical error"
  - [section 3.6]: "our goal within every subspace W(t) is to take a single gradient step towards a vector ut present only in sets up to that point"
  - [corpus]: Weak - related work on SGD convergence does not address this specific underfitting phenomenon
- Break condition: If the probability of finding suitable vectors ut becomes too low, the construction fails

### Mechanism 3
- Claim: The dimension dependence is tight up to logarithmic factors
- Mechanism: The lower bounds Ω(√d) match upper bounds from Bassily et al. (2020), establishing that the polynomial dimension dependence is unavoidable for these algorithms
- Core assumption: The upper bounds from existing literature are tight
- Evidence anchors:
  - [abstract]: "Both of the results above are tight (up to logarithmic factors) in view of existing matching upper bounds"
  - [section 1.1]: "tight lower bounds for the population loss of GD and for the empirical loss of SGD"
  - [corpus]: Weak - related papers discuss bounds but do not establish tightness
- Break condition: If new upper bounds are discovered that improve on the existing ones, the tightness claim would need revision

## Foundational Learning

- Concept: Stochastic Convex Optimization (SCO) framework
  - Why needed here: The paper's lower bounds are established within the SCO setting, which is fundamental to understanding the problem context
  - Quick check question: What is the difference between population risk and empirical risk in SCO?

- Concept: Uniform convergence and its failure in high dimensions
- Concept: Algorithmic stability and its relationship to generalization
  - Why needed here: The paper's results rely on understanding why uniform convergence fails and how lack of algorithmic stability leads to overfitting/underfitting
  - Quick check question: How does the lack of algorithmic stability allow GD to converge to a bad ERM?

- Concept: Gradient dynamics in high-dimensional spaces
  - Why needed here: The construction relies on precise control over how gradients move in high-dimensional subspaces
  - Quick check question: How do nearly orthogonal vectors in high dimensions affect gradient directions?

## Architecture Onboarding

- Component map: Loss function components (ℓ1, ℓ2, ℓ3, ℓ4 for GD; analogous for SGD) -> Encoding/decoding mechanism -> Subspace structure (T orthogonal subspaces of dimension d') -> Subgradient oracle interface

- Critical path:
  1. Construct set of nearly orthogonal vectors U
  2. Design loss function with components that force GD/SGD toward bad ERMs
  3. Implement encoding mechanism to store training data in iterates
  4. Prove dynamics under "good event" conditions
  5. Establish lower bounds on population/empirical risk

- Design tradeoffs:
  - Higher dimension d allows better control over algorithm dynamics but increases computational cost
  - More complex encoding schemes provide stronger guarantees but are harder to analyze
  - Trade-off between tightness of bounds and simplicity of construction

- Failure signatures:
  - If ∥wt∥ exceeds 1, the analysis breaks down
  - If collisions occur in the encoding (same j for different samples), the construction fails
  - If the orthogonality of vectors in U degrades, the gradient steps may interfere

- First 3 experiments:
  1. Verify the dynamics lemma (Lemma 4) for a small instance (n=2, T=2)
  2. Check that the encoding correctly recovers training sets in a simple case
  3. Validate that the loss function is indeed convex and Lipschitz as claimed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the dimension dependence of gradient methods in SCO be reduced below √d?
- Basis in paper: The paper establishes a Ω(√d) lower bound for both GD and SGD to achieve non-trivial generalization or empirical risk.
- Why unresolved: While the paper proves this bound is tight up to logarithmic factors, it doesn't explore whether techniques like adaptive step sizes, variance reduction, or momentum could further improve the dimension dependence.
- What evidence would resolve it: Constructing specific SCO problems or proving impossibility results showing whether the Ω(√d) bound can be improved or not.

### Open Question 2
- Question: Does the lack of algorithmic stability in gradient methods inevitably lead to overfitting in SCO?
- Basis in paper: The paper demonstrates that GD can overfit the training data due to lack of algorithmic stability, while SGD can underfit despite achieving optimal test performance.
- Why unresolved: The paper doesn't explore the precise relationship between algorithmic stability and generalization performance, or investigate techniques to improve stability without sacrificing convergence rates.
- What evidence would resolve it: Theoretical analysis of the stability-generalization tradeoff in SCO, and empirical studies of how different algorithmic modifications affect both stability and generalization.

### Open Question 3
- Question: Can the "benign underfitting" phenomenon of SGD in SCO be exploited for better generalization in practice?
- Basis in paper: The paper shows that SGD can achieve optimal test performance despite underfitting the training data, suggesting that a model with low empirical risk may not be necessary for good generalization.
- Why unresolved: The paper doesn't explore the practical implications of this finding, such as how to choose the optimal step size or number of passes for SGD to balance underfitting and generalization.
- What evidence would resolve it: Empirical studies comparing the generalization performance of SGD with different step sizes and number of passes, and theoretical analysis of the optimal tradeoff between underfitting and generalization in SCO.

## Limitations

- The construction relies heavily on worst-case scenarios that may not reflect practical machine learning applications
- The encoding/decoding mechanism requires precise parameter tuning and careful implementation to ensure theoretical guarantees hold
- The tightness claims depend on existing upper bounds that may not be the tightest possible

## Confidence

- **High Confidence**: The theoretical framework and lower bound proofs are mathematically rigorous and build on established results in convex optimization and statistical learning theory. The dimension dependence claim (Ω(√d)) is well-supported by the analysis.
- **Medium Confidence**: The construction of the hard distribution and loss functions is complex and requires precise parameter tuning. While the theoretical guarantees are strong, practical implementation may reveal additional challenges not captured in the analysis.
- **Low Confidence**: The applicability of these worst-case results to practical machine learning scenarios, where data distributions and optimization landscapes are typically more benign, remains unclear.

## Next Checks

1. Implement and test the encoding/decoding mechanism on small-scale instances to verify the "good event" conditions hold as claimed.
2. Conduct experiments comparing the proposed hard instances against standard convex optimization benchmarks to understand the practical impact of the dimension dependence.
3. Investigate whether regularization techniques or modified optimization algorithms can mitigate the identified lower bounds, potentially leading to new algorithmic insights.