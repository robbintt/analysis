---
ver: rpa2
title: A Distributionally Robust Optimisation Approach to Fair Credit Scoring
arxiv_id: '2402.01811'
source_url: https://arxiv.org/abs/2402.01811
tags:
- credit
- fairness
- data
- scoring
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of Distributionally Robust Optimization
  (DRO) with fairness constraints for credit scoring, aiming to improve fairness in
  both training and out-of-sample data. The proposed method, Distributionally Robust
  Fair Logistic Regression (DRFLR), combines DRO with a fairness penalty to optimize
  over the worst possible distribution within a Wasserstein ball, considering changes
  in sensitive attributes.
---

# A Distributionally Robust Optimisation Approach to Fair Credit Scoring

## Quick Facts
- arXiv ID: 2402.01811
- Source URL: https://arxiv.org/abs/2402.01811
- Authors: Pablo Casas; Christophe Mues; Huan Yu
- Reference count: 21
- Primary result: DRFLR achieves lower fairness metrics (LEO and SP) with minimal ROC loss compared to standard and regularized logistic regression models

## Executive Summary
This paper introduces Distributionally Robust Fair Logistic Regression (DRFLR) for credit scoring, combining DRO with fairness constraints to improve fairness in both training and out-of-sample data. The method optimizes over the worst-case distribution within a Wasserstein ball while penalizing unfairness between protected and non-protected groups. Experiments on five credit scoring datasets demonstrate that DRFLR consistently achieves better fairness metrics (lower LEO and SP) with minimal loss in predictive performance compared to standard and regularized logistic regression approaches.

## Method Summary
The paper proposes DRFLR, which combines DRO with a fairness penalty to optimize over the worst possible distribution within a Wasserstein ball. The method uses logistic regression with a fairness constraint based on log probabilistic equalised opportunities (LEO). The optimization problem includes hyperparameters: radius of Wasserstein ball (ρ), fairness penalty (η), and ground metric weights (κs, κy). Five credit scoring datasets (German Credit, Give Me Some Credit, Home Credit, Taiwan Credit, PAKDD) are preprocessed with one-hot encoding, median imputation, and Lasso feature selection, then evaluated using k-fold cross-validation with ROC for performance and LEO/SP for fairness.

## Key Results
- DRFLR consistently achieves lower fairness metrics (LEO and SP) across all five credit scoring datasets
- Minimal loss in predictive performance (ROC) compared to standard and regularized logistic regression models
- DRFLR demonstrates superior fairness performance, suggesting a potential connection between robustness and fairness in credit scoring applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DRO-based classifiers improve fairness by optimizing over worst-case distribution within Wasserstein ball
- Mechanism: Minimizes expected loss under worst possible distribution within Wasserstein ball around empirical distribution, accounting for distributional shifts in target and sensitive attributes
- Core assumption: True underlying distribution lies within Wasserstein ball of radius ρ around empirical distribution
- Evidence anchors:
  - [abstract]: "Distributionally Robust Fair Logistic Regression (DRFLR)... optimizes over the worst possible distribution within a Wasserstein ball, considering changes in sensitive attributes"
  - [section 3]: "DRO extends both methods since DRO problem can be converted into RO or SO by enlarging parameter ρ"
  - [corpus]: Found related papers on DRO for fairness, but no direct evidence on specific Wasserstein ball approach

### Mechanism 2
- Claim: Fairness penalty within DRO framework ensures fairness under distributional shifts
- Mechanism: Fairness penalty measures difference in log-probability of default between protected and non-protected groups under worst-case distribution
- Core assumption: Log probabilistic equalised opportunities is valid convex relaxation of desired fairness criterion
- Evidence anchors:
  - [abstract]: "The proposed method... combines DRO with a fairness penalty to optimize over the worst possible distribution within a Wasserstein ball"
  - [section 3]: "The resulting problem is a regularised logistic regression, where ρ is the regularisation penalty"
  - [corpus]: Found papers on DRO for fairness, but no direct evidence on specific LEO metric

### Mechanism 3
- Claim: DRO-based classifiers achieve similar or better predictive performance while improving fairness
- Mechanism: Wasserstein-based regularization implicitly shrinks coefficients and reduces model complexity, improving generalization and fairness
- Core assumption: Implicit regularization from Wasserstein ball is beneficial for credit scoring datasets
- Evidence anchors:
  - [abstract]: "Experiments on five credit scoring datasets show that DRFLR consistently achieves lower fairness metrics... with minimal loss in predictive performance"
  - [section 6.2]: "In terms of SP, there is no clear leader. However, DRFLR, DRLR and FLR are the top performers"
  - [corpus]: Found related papers on DRO for credit scoring, but no direct evidence on specific performance claims

## Foundational Learning

- Concept: Distributionally Robust Optimization (DRO)
  - Why needed here: Provides principled way to handle distributional shifts in credit scoring data for fairness in out-of-sample predictions
  - Quick check question: What is the difference between Stochastic Optimization (SO) and Distributionally Robust Optimization (DRO)?

- Concept: Wasserstein Distance
  - Why needed here: Defines ambiguity set in DRO, allowing model to account for feature and sensitive attribute shifts
  - Quick check question: How does Wasserstein distance differ from other metrics used to measure distribution similarity?

- Concept: Fairness Metrics for Credit Scoring
  - Why needed here: Traditional fairness metrics may not be suitable due to dependence on classification thresholds; LEO is threshold-independent
  - Quick check question: Why might separation measure (SP) be problematic for credit scoring applications?

## Architecture Onboarding

- Component map: Data preprocessing -> DRO model optimization -> Evaluation with ROC/LEO/SP
- Critical path: Preprocess data → Define DRO model with hyperparameters (ρ, η, κs, κy) → Train using conic optimization solver → Evaluate on held-out test set
- Design tradeoffs:
  - Larger ρ provides more robustness but may harm performance if too large
  - Larger η enforces stronger fairness but may harm performance if too large
  - Higher κs/κy gives more weight to sensitive attribute/target differences in ground metric
- Failure signatures:
  - Poor performance on training data: Likely overfitting or poor hyperparameter choice
  - Unfairness on test data: Likely insufficient robustness or fairness penalty
  - Very low performance on both: Likely too much regularization or poor model specification
- First 3 experiments:
  1. Compare DRFLR vs. standard logistic regression on single dataset with default hyperparameters
  2. Vary ρ to understand tradeoff between robustness and performance
  3. Vary η to understand tradeoff between fairness and performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does choice of ground metric impact fairness and robustness trade-offs in DRO models?
- Basis in paper: [explicit] Mentions using ground metric with sensitive attribute information and discusses effects of κs and κy
- Why unresolved: Paper provides insights on κs and κy effects but doesn't thoroughly explore different ground metric formulations
- What evidence would resolve it: Experiments comparing DRO models with different ground metric formulations across datasets

### Open Question 2
- Question: How does computational complexity scale with dataset size and dimensionality?
- Basis in paper: [explicit] Mentions high computational requirements, exponential increase with sample size, lack of parallelization
- Why unresolved: Paper doesn't provide detailed analysis of computational complexity scaling or explore mitigation strategies
- What evidence would resolve it: Empirical studies comparing computational performance with increasing dataset size, investigations into approximation techniques

### Open Question 3
- Question: How do DRO models perform in scenarios with significant concept drift?
- Basis in paper: [explicit] Mentions DRO handles population drift but doesn't explicitly address concept drift
- Why unresolved: Paper doesn't provide experiments on concept drift performance
- What evidence would resolve it: Experiments evaluating DRO performance in synthetic/real datasets with induced concept drift

## Limitations
- Performance gains vary across datasets, with some showing minimal improvement
- High computational requirements limit practical applicability to large-scale problems
- Results specific to age as sensitive attribute; generalizability to other attributes unknown

## Confidence

- High confidence in mathematical formulation and implementation approach
- Medium confidence in empirical results showing fairness improvements
- Medium confidence in robustness-fairness connection as general principle

## Next Checks

1. **Cross-attribute sensitivity analysis**: Apply DRFLR to datasets with different sensitive attributes (e.g., gender, race) to assess performance beyond age-based fairness

2. **Stress test with extreme distributional shifts**: Simulate severe concept drift scenarios to evaluate whether Wasserstein-based robustness degrades gracefully or fails catastrophically

3. **Ablation study on fairness metric choice**: Compare LEO against other fairness metrics (e.g., demographic parity, equalized odds) to determine if observed improvements are specific to log probabilistic framework