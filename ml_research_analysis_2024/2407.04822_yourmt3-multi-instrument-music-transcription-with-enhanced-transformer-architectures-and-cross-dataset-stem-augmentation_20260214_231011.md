---
ver: rpa2
title: 'YourMT3+: Multi-instrument Music Transcription with Enhanced Transformer Architectures
  and Cross-dataset Stem Augmentation'
arxiv_id: '2407.04822'
source_url: https://arxiv.org/abs/2407.04822
tags:
- music
- dataset
- transcription
- training
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: YourMT3+ improves multi-instrument music transcription by enhancing
  the MT3 model with a hierarchical attention transformer encoder, mixture of experts,
  and a multi-channel decoder. It introduces intra- and cross-stem augmentation for
  training with incomplete annotations and mixed datasets.
---

# YourMT3+: Multi-instrument Music Transcription with Enhanced Transformer Architectures and Cross-dataset Stem Augmentation

## Quick Facts
- **arXiv ID:** 2407.04822
- **Source URL:** https://arxiv.org/abs/2407.04822
- **Reference count:** 0
- **Primary result:** YourMT3+ achieves competitive or superior performance on ten public multi-instrument transcription datasets compared to existing models, with direct vocal transcription capabilities without voice separation preprocessing.

## Executive Summary
This paper presents YourMT3+, an enhanced transformer architecture for multi-instrument music transcription that improves upon the MT3 model through hierarchical attention mechanisms, mixture of experts, and a multi-channel decoder. The model introduces innovative data augmentation techniques including intra-stem and cross-dataset stem augmentation to address the limitations of incomplete annotations and mixed datasets. Extensive experiments on ten public datasets demonstrate that YourMT3+ achieves state-of-the-art or competitive performance, with particular success in vocal transcription without requiring separate voice separation preprocessing. However, the model shows significant performance limitations when applied to commercial pop music recordings, highlighting the need for more diverse training data.

## Method Summary
YourMT3+ enhances the MT3 architecture through several key modifications: replacing the encoder with hierarchical attention transformer blocks using PerceiverTF with spectral cross-attention, incorporating mixture of experts (MoE) layers to increase model capacity without proportional computational cost, and implementing a multi-channel decoder that can handle incomplete annotations through task-query based training. The model introduces novel data augmentation strategies including intra-stem augmentation (randomly dropping instruments within tracks) and cross-dataset stem augmentation (mixing stems from multiple datasets to create synthetic multi-instrument combinations). The model is trained on a combination of ten public datasets with log-magnitude spectrograms or mel-spectrograms, using AdamWScale optimizer for 300K steps with cosine scheduler.

## Key Results
- YourMT3+ achieves state-of-the-art performance on nine out of ten evaluated datasets, with particular improvements in vocal transcription
- The hierarchical attention transformer and MoE components provide consistent performance gains across different datasets
- Cross-dataset stem augmentation enables effective training on incomplete annotations without requiring voice separation preprocessing
- Performance on commercial pop music recordings remains significantly lower than on synthetic datasets, indicating generalization limitations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical attention transformer in the time-frequency domain improves multi-instrument transcription by better modeling spectral cross-attention and temporal dependencies.
- Mechanism: The model replaces the MT3 encoder with PerceiverTF blocks, which use a learnable latent array to attend to both spectral (c × f′) and temporal (t) dimensions separately before combining them. This separation allows the model to first focus on spectral content across instruments, then model temporal evolution within each instrument.
- Core assumption: Spectral information across instruments is sufficiently separable in the latent space to allow targeted attention, and temporal dependencies within instruments are better captured after spectral processing.
- Evidence anchors:
  - [abstract]: "We enhance its encoder by adopting a hierarchical attention transformer in the time-frequency domain"
  - [section]: "The PTF block ( ♠, Figure 1) performs three iterations. Initially,⭑acts as the query in SCA during the first iteration. In the second and third iterations,⭐serves as the query."
- Break condition: If the latent array size k is too small, the model cannot distinguish between instrument groups; if too large, the computational cost outweighs benefits.

### Mechanism 2
- Claim: Mixture of Experts (MoE) improves transcription performance with minimal parameter increase.
- Mechanism: The model replaces the feed-forward network (FFN) in the latent and temporal transformer sub-blocks with a mixture of experts layer. This layer routes each input to two of eight experts, allowing the model to learn specialized feature transformations for different types of musical content.
- Core assumption: Different musical signals (e.g., harmonic vs percussive instruments) benefit from different processing pathways within the transformer blocks.
- Evidence anchors:
  - [section]: "We propose an augmentation method, denoted by a plus (+) sign, that formalizes the earlier stem mixing approach [5] within an online multi-dataset pipeline."
  - [section]: "MoE: Our models, designated as YPTF.MoE, replace the FFNs in latent and temporal transformer blocks with mixture of experts (MoE) layers from [7], routing attention output to two out of eight experts"
- Break condition: If expert specialization is too fine-grained or routing is suboptimal, performance may not improve and additional parameters are wasted.

### Mechanism 3
- Claim: Cross-dataset stem augmentation enables training on incomplete annotations and improves generalization to unseen instrument combinations.
- Mechanism: The model randomly samples segments from multiple datasets and mixes their stems according to a policy, creating synthetic multi-instrument mixtures not present in any single dataset. This exposes the model to diverse instrument combinations during training.
- Core assumption: The model can learn to separate and transcribe instruments even when they were never seen together in the training data, as long as the individual instrument characteristics are learned from other examples.
- Evidence anchors:
  - [abstract]: "To address data limitations, we introduce a new multi-channel decoding method for training with incomplete annotations and propose intra- and cross-stem augmentation for dataset mixing."
  - [section]: "The concept of cross-dataset stem augmentation, as discussed in Section 4.2, draws inspiration from PerceiverTF [5]. It aims to create a new mixture of stems from multiple datasets."
- Break condition: If the mixing policy creates unrealistic instrument combinations or the model overfits to synthetic mixtures, performance on real music may degrade.

## Foundational Learning

- Concept: Transformer attention mechanisms
  - Why needed here: The model relies on spectral cross-attention and self-attention in both latent and temporal transformer blocks to process audio spectrograms and generate note sequences.
  - Quick check question: Can you explain the difference between self-attention and cross-attention in the context of the PerceiverTF encoder?

- Concept: Mixture of Experts (MoE)
  - Why needed here: The model uses MoE to increase model capacity without proportional computational cost by routing inputs to specialized expert networks.
  - Quick check question: How does the Top-k routing mechanism in MoE work, and why is it more efficient than using all experts?

- Concept: Data augmentation for music
  - Why needed here: The model uses intra-stem and cross-dataset stem augmentation to address the lack of fully annotated multi-instrument data and improve generalization.
  - Quick check question: What is the difference between intra-stem augmentation (dropping stems within a track) and cross-dataset augmentation (mixing stems from different datasets)?

## Architecture Onboarding

- Component map: Audio input → 2.048-second segment → log-magnitude spectrogram → hierarchical attention transformer encoder with MoE → multi-channel T5 decoder → MIDI-like event tokens
- Critical path: Audio input → spectrogram → encoder (with attention and MoE) → decoder → token sequence
- Design tradeoffs:
  - Single vs multi-channel decoder: Single is simpler but cannot mask unannotated instruments; multi allows training with incomplete labels but increases sequence length.
  - Encoder choice: T5 encoder is faster but less expressive; PerceiverTF with MoE is more powerful but slower and more complex.
  - Data augmentation: More aggressive augmentation improves generalization but may introduce unrealistic mixtures.
- Failure signatures:
  - Low onset F1 but high frame F1: Model is predicting notes but with poor timing accuracy.
  - High performance on synthetic data but low on real music: Model overfits to synthetic instrument combinations.
  - Training instability with MoE: Expert routing is unbalanced or gradients are exploding.
- First 3 experiments:
  1. Compare YMT3 vs YPTF on a small multi-instrument dataset to validate encoder improvement.
  2. Test single vs multi-channel decoder on a dataset with incomplete annotations to validate the masking approach.
  3. Evaluate the impact of cross-dataset stem augmentation by training with and without it on MIR-ST500.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the mixture of experts (MoE) layer in the YPTF.MoE model affect the performance of music transcription, particularly in handling pitch-shifting augmentation?
- Basis in paper: [explicit] The paper states that the MoE layer improved performance across various datasets and helped prevent performance degradation when pitch shifting was applied during training.
- Why unresolved: The paper does not provide detailed analysis of which expert FFN is active on specific datasets or types of tokens, nor does it discuss the effect of weight initialization on MoE performance.
- What evidence would resolve it: Detailed experiments analyzing the activation patterns of different experts across various datasets and token types, along with ablation studies on weight initialization methods, would provide insights into the specific contributions of MoE to the model's performance.

### Open Question 2
- Question: What are the limitations of the current multi-instrument music transcription models in transcribing pop music recordings, and how can these limitations be addressed?
- Basis in paper: [explicit] The paper highlights that the model significantly underperformed on commercial pop music recordings, particularly for non-main instruments, suggesting that relying solely on synthetic datasets is insufficient for modeling the diverse timbres of pop music.
- Why unresolved: The paper does not provide a detailed analysis of the specific challenges posed by pop music recordings or propose concrete solutions to improve transcription performance on this genre.
- What evidence would resolve it: Comparative studies analyzing the differences between synthetic and pop music recordings, along with experiments incorporating more diverse and real-world datasets, would help identify the specific challenges and potential solutions for improving pop music transcription.

### Open Question 3
- Question: How does the proposed cross-dataset stem augmentation method impact the performance of music transcription, and what are the optimal parameters for this augmentation strategy?
- Basis in paper: [explicit] The paper introduces the cross-dataset stem augmentation method and demonstrates its effectiveness in training models without the need for singing voice separation. However, the paper does not provide a detailed analysis of the impact of different parameters on the augmentation method's performance.
- Why unresolved: The paper presents the cross-dataset stem augmentation method but does not thoroughly investigate the effects of varying parameters such as the probability of retaining singing stems, the maximum number of iterations, or the stem mixing policy on the model's performance.
- What evidence would resolve it: Systematic experiments varying the parameters of the cross-dataset stem augmentation method and analyzing their impact on the model's performance across different datasets would provide insights into the optimal configuration of this augmentation strategy.

## Limitations
- Significant performance degradation on commercial pop music recordings compared to synthetic datasets
- Model may overfit to characteristics of curated datasets rather than learning generalizable transcription capabilities
- Cross-dataset augmentation may introduce unrealistic instrument combinations not reflective of real musical arrangements

## Confidence
- **High Confidence**: Architectural improvements and baseline comparisons are technically sound with clear implementation details
- **Medium Confidence**: Data augmentation methodology and generalization claims, though well-motivated, show limitations on real-world music
- **Low Confidence**: Model's ability to handle diverse commercial pop music recordings and real-world deployment scenarios

## Next Checks
1. **Cross-dataset Generalization Test**: Evaluate the trained model on a separate held-out test set containing instrument combinations never seen during training to verify true generalization capabilities.
2. **Commercial Music Robustness Analysis**: Test the model on a diverse set of 20-30 commercial pop music tracks spanning different genres, decades, and production styles to identify specific failure patterns and quantify performance variance.
3. **Ablation Study on Augmentation Policy**: Systematically vary the stem mixing policy parameters to determine which aspects of the augmentation strategy contribute most to performance gains versus potential overfitting.