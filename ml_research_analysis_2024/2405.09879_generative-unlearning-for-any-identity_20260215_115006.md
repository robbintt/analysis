---
ver: rpa2
title: Generative Unlearning for Any Identity
arxiv_id: '2405.09879'
source_url: https://arxiv.org/abs/2405.09879
tags:
- identity
- unlearning
- latent
- image
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces the problem of generative identity unlearning,
  which aims to prevent a pre-trained generative model from synthesizing images of
  a specific identity using only a single exemplar image. The proposed method, GUIDE,
  achieves this by first finding a target latent code that represents an anonymous
  identity using an extrapolation technique called UFO, then optimizing the generator
  to shift from the source identity to the target identity using three novel loss
  functions: local unlearning loss, adjacency-aware unlearning loss, and global preservation
  loss.'
---

# Generative Unlearning for Any Identity

## Quick Facts
- arXiv ID: 2405.09879
- Source URL: https://arxiv.org/abs/2405.09879
- Reference count: 40
- One-line primary result: Achieves strong identity removal (ID scores drop from 0.19 to 0.06) while preserving generation quality using only a single exemplar image.

## Executive Summary
This paper introduces generative identity unlearning, a novel approach to prevent pre-trained generative models from synthesizing images of specific identities using only a single exemplar image. The proposed GUIDE framework achieves this by first finding a target latent code that represents an anonymous identity using an extrapolation technique called UFO, then optimizing the generator to shift from the source identity to the target identity using three novel loss functions. Experiments demonstrate significant reduction in identity similarity metrics while maintaining generation quality across various scenarios.

## Method Summary
The GUIDE framework operates by first inverting a source image to obtain its latent code, then using the UFO method to extrapolate to a target latent representing an anonymous identity. The generator is then optimized using three loss functions: local unlearning loss (to remove the source identity), adjacency-aware unlearning loss (to prevent identity leakage through latent perturbations), and global preservation loss (to maintain generation quality on unrelated inputs). The method uses Adam optimization with learning rate 1e-4 and hyperparameters Na=Ng=2, d=30.

## Key Results
- Achieves significant identity removal: ID scores drop from 0.19 to 0.06 on average
- Maintains generation quality: FIDpre scores remain close to pre-trained model
- Robust performance across scenarios: Random, In-Domain, and Out-of-Domain identities
- Generalizes to unseen images: IDothers scores improve for same identity images not used in training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: UFO reliably identifies target latent codes identity-distinct from source while maintaining image quality
- Mechanism: Subtracts average latent from source latent to extract "identity direction," then extrapolates away from average
- Core assumption: Identity differences are aligned along certain directions in latent space
- Evidence anchors: Empirical findings show UFO identifies promising targets robustly
- Break condition: Source identity close to average latent produces degraded quality

### Mechanism 2
- Claim: Adjacency-aware unlearning loss prevents identity leakage through latent perturbations
- Mechanism: Samples and optimizes latent codes in neighborhood of both source and target
- Core assumption: Images of same identity form cluster in latent space
- Evidence anchors: Prevents reconstruction of similar identities through marginal perturbations
- Break condition: Large neighborhood size causes overgeneralization

### Mechanism 3
- Claim: Global preservation loss prevents catastrophic forgetting of other identities
- Mechanism: Constrains generator on random latents far from source and target
- Core assumption: Unlearning only needs to affect localized region in latent space
- Evidence anchors: Regularizes generator to retain performance on distant latents
- Break condition: Insufficient preservation effect with too few random latents

## Foundational Learning

- Concept: GAN latent space geometry and disentanglement
  - Why needed: Method relies on understanding identity distribution in latent space
  - Quick check: What happens when interpolating between two random latent codes?

- Concept: GAN inversion and latent code extraction
  - Why needed: Approach uses inversion networks to map images to latents
  - Quick check: Difference between optimization-based and learning-based GAN inversion?

- Concept: Perceptual and identity loss functions
  - Why needed: Unlearning losses use perceptual and identity-based comparisons
  - Quick check: How does ArcFace compute similarity between face embeddings?

## Architecture Onboarding

- Component map: Source image -> GAN inversion -> UFO -> Target latent -> LTU optimizer -> Generator

- Critical path:
  1. Invert source image → source latent
  2. Apply UFO to get target latent
  3. Sample adjacent and random latents
  4. Optimize generator with three losses
  5. Evaluate identity removal and generation quality

- Design tradeoffs:
  - Distance `d` in UFO: Larger → better identity separation but worse image quality
  - `αmax` in adjacency loss: Larger → more thorough unlearning but risk of overgeneralization
  - `Na` and `Ng`: Balance between unlearning effectiveness and preservation

- Failure signatures:
  - Identity leakage: ID similarity between source and unlearned images remains high
  - Generation collapse: FIDpre or ∆FIDreal spikes dramatically
  - Mode collapse: Generated images become overly uniform or lose diversity

- First 3 experiments:
  1. Run baseline (no UFO, only Llocal) on single FFHQ image; record ID and FIDpre
  2. Enable UFO with d=30; check if ID drops significantly while FIDpre stays stable
  3. Add adjacency-aware loss (Na=2, αmax=15); verify IDothers improves on unseen same-identity images

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does choice of d (distance parameter in UFO) affect quality and identity of target image across different scenarios?
- Basis: Paper mentions d=30 was chosen empirically but lacks detailed analysis
- Why unresolved: Only brief mention of d effect, no comprehensive study across scenarios
- What evidence would resolve: Detailed ablation study showing effects of different d values on ID scores, FID scores, and visual quality across all scenarios

### Open Question 2
- Question: How does number of latent codes (Na and Ng) affect effectiveness of identity unlearning and preservation of generation quality?
- Basis: Paper uses Na=Ng=2 based on empirical results but doesn't explore impact of varying
- Why unresolved: No thorough analysis of how latent code numbers affect balance between unlearning and preservation
- What evidence would resolve: Ablation study varying Na and Ng while measuring ID scores, FID scores, and visual quality

### Open Question 3
- Question: Can GUIDE framework extend to other generative models beyond GANs, such as diffusion models?
- Basis: Paper focuses on GANs but mentions privacy concerns in diffusion models
- Why unresolved: Doesn't explore applicability to other generative model architectures
- What evidence would resolve: Applying GUIDE to different generative model architectures and comparing unlearning performance

## Limitations

- Scalability and robustness of UFO method across diverse identities is uncertain
- Adjacency-aware loss hyperparameter αmax could lead to overgeneralization
- Lack of ablation studies on relative importance of each loss component
- Method assumes availability of well-trained GAN inversion network

## Confidence

- **High confidence**: Identity removal effectiveness (ID score reduction) is well-supported by quantitative results
- **Medium confidence**: Generation quality preservation (FID scores) is maintained but sensitive to hyperparameters
- **Low confidence**: UFO method's generalizability beyond faces is not empirically tested

## Next Checks

1. Cross-domain robustness test: Apply GUIDE to non-face domains (e.g., AFHQv2-Cat) to validate UFO's generalizability

2. Hyperparameter sensitivity analysis: Systematically vary d and αmax to map trade-off curve between identity removal and generation quality

3. Ablation study of loss components: Run experiments with individual loss components to quantify incremental benefits on ID score reduction and FID maintenance