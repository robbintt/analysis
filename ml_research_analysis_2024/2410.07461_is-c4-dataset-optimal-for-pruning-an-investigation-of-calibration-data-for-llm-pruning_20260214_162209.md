---
ver: rpa2
title: Is C4 Dataset Optimal for Pruning? An Investigation of Calibration Data for
  LLM Pruning
arxiv_id: '2410.07461'
source_url: https://arxiv.org/abs/2410.07461
tags:
- data
- pruning
- calibration
- datasets
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the impact of calibration data selection
  on large language model (LLM) pruning performance. While prior work universally
  uses the C4 dataset for pruning score calculation, the authors systematically evaluate
  four pre-training datasets (C4, Pile, OSCAR, RedPajama) and nine downstream datasets
  across three task categories.
---

# Is C4 Dataset Optimal for Pruning? An Investigation of Calibration Data for LLM Pruning

## Quick Facts
- arXiv ID: 2410.07461
- Source URL: https://arxiv.org/abs/2410.07461
- Authors: Abhinav Bandari; Lu Yin; Cheng-Yu Hsieh; Ajay Kumar Jaiswal; Tianlong Chen; Li Shen; Ranjay Krishna; Shiwei Liu
- Reference count: 12
- Key outcome: C4 dataset is not optimal for LLM pruning; Pile dataset consistently outperforms it, with downstream datasets also showing strong performance

## Executive Summary
This paper systematically investigates how calibration data selection impacts large language model pruning performance, challenging the widespread assumption that C4 is optimal. Through experiments with four pre-training datasets and nine downstream datasets across three task categories, the authors demonstrate that Pile consistently outperforms C4 for pruning score calculation. The study also reveals that ICL format broadly benefits pruning performance while CoT format only helps specific tasks, and that downstream datasets like arithmetic reasoning tasks can be as effective or better than pre-training data for calibration.

## Method Summary
The authors conduct pruning experiments using Llama 2-Chat 7B and Llama 7B models with Wanda and SparseGPT methods at 50% and 70% sparsity. They evaluate four pre-training datasets (C4, Pile, OSCAR, RedPajama) and nine downstream datasets across arithmetic reasoning, natural language inference, and commonsense reasoning tasks. Calibration data is tested in three formats: zero-shot, in-context learning (ICL), and ICL with chain-of-thought (CoT) prompts. The primary metric is accuracy on downstream tasks after pruning.

## Key Results
- Pile dataset consistently outperforms C4 across multiple pruning methods and evaluation tasks
- Downstream datasets, particularly arithmetic reasoning tasks like SV AMP, achieve comparable or better performance than pre-training datasets
- ICL calibration data broadly benefits pruning performance across all task categories
- ICL w/ CoT is only advantageous for arithmetic reasoning tasks, not for other task categories

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Calibration data quality and domain alignment significantly impact pruning effectiveness
- Mechanism: Pruning algorithms calculate parameter importance scores based on how much each weight contributes to preserving task performance on calibration data. When calibration data better represents the target task distribution, the pruning process more accurately identifies and preserves critical parameters for that task.
- Core assumption: The pruning algorithm's sensitivity to input data distribution means that calibration data that better matches the evaluation task distribution will produce more task-relevant sparse models.
- Evidence anchors:
  - [abstract] "C4 is not the optimal choice for LLM pruning, even among commonly used pre-training datasets"
  - [section] "Our analysis reveals that the average accuracy of Pile consistently outperforms the C4 dataset"
  - [corpus] Weak evidence - related papers focus on pruning algorithms but not calibration data selection

### Mechanism 2
- Claim: In-Context Learning (ICL) format in calibration data broadly benefits pruning performance across all task categories
- Mechanism: ICL format provides structured examples that help the pruning algorithm better understand the task structure and reasoning patterns needed for downstream tasks, leading to more effective parameter preservation.
- Core assumption: The structured nature of ICL examples in calibration data provides stronger signals about task requirements than unstructured text.
- Evidence anchors:
  - [abstract] "ICL calibration data broadly benefits all data categories"
  - [section] "Our findings, detailed in Table 4, reveal that ICL consistently enhances performance across all data categories compared to the baseline zero-shot approach"
  - [corpus] Weak evidence - no direct mention of ICL format effects in related pruning papers

### Mechanism 3
- Claim: Arithmetic reasoning datasets as calibration data perform on par or better than pre-training datasets for LLM pruning
- Mechanism: Arithmetic reasoning tasks require precise computation and step-by-step reasoning, which creates stronger gradients and more distinct parameter importance signals during pruning score calculation.
- Core assumption: The structured, logical nature of arithmetic problems creates clearer patterns for pruning algorithms to identify important parameters.
- Evidence anchors:
  - [abstract] "arithmetic datasets—when used as calibration data—performs on par or even better than pre-training datasets"
  - [section] "We find no clear relationship between the number of steps of CoT in calibration data and the performance of the sparse LLM" (suggesting arithmetic reasoning structure itself matters)
  - [corpus] Weak evidence - related papers don't discuss arithmetic dataset performance in pruning context

## Foundational Learning

- Concept: Pruning score calculation and parameter importance ranking
  - Why needed here: The paper investigates how different calibration data affects pruning scores, which directly determine which parameters get removed
  - Quick check question: What mathematical operation do pruning algorithms typically use to calculate parameter importance from calibration data?

- Concept: In-Context Learning (ICL) vs Chain-of-Thought (CoT) prompting
  - Why needed here: The paper evaluates how different prompt formats in calibration data affect pruning outcomes, with ICL showing broad benefits and CoT only helping specific tasks
  - Quick check question: How does including Chain-of-Thought rationale in calibration data affect the pruning algorithm's parameter importance calculations?

- Concept: Unstructured vs structured sparsity in LLM pruning
  - Why needed here: The experiments target 50% unstructured sparsity, meaning individual weights are pruned rather than entire channels or neurons
  - Quick check question: What's the key difference between unstructured and structured pruning in terms of how parameters are selected for removal?

## Architecture Onboarding

- Component map: Calibration data → Pruning algorithm (Wanda/SparseGPT) → Parameter importance scores → Sparse model → Evaluation on downstream tasks
- Critical path: Calibration data selection → Pruning score computation → Parameter ranking → Weight removal → Model evaluation
- Design tradeoffs: Using task-specific calibration data vs. general pre-training data (task alignment vs. preservation of general knowledge)
- Failure signatures: Calibration data too short → poor pruning scores; Calibration data too long → computational inefficiency; Mismatched task distribution → poor downstream performance
- First 3 experiments:
  1. Run pruning with C4 calibration data vs. Pile calibration data on Llama 2-Chat 7B, measure average accuracy across all tasks
  2. Compare ICL vs. zero-shot calibration data format on arithmetic reasoning tasks, measure performance difference
  3. Test different numbers of CoT steps (3, 4, 5) in calibration data, measure impact on specific task performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does the Pile dataset consistently outperform C4 as calibration data for LLM pruning across different pruning methods and evaluation tasks?
- Basis in paper: [explicit] The paper demonstrates that Pile consistently outperforms C4 in pruning experiments, but does not rigorously investigate the underlying reasons for this superiority.
- Why unresolved: The authors conjecture that Pile's superior performance may be due to its more diverse and higher quality examples compared to C4, but this remains untested.
- What evidence would resolve it: A detailed analysis comparing the characteristics of Pile and C4 datasets (diversity, quality metrics, domain coverage) and their correlation with pruning performance would provide insights into why Pile is more effective.

### Open Question 2
- Question: How does the effectiveness of calibration data for LLM pruning correlate with its effectiveness for LLM pretraining?
- Basis in paper: [inferred] The authors mention this as a future direction, noting that Pile is designed for improved downstream generalization compared to C4, but they have not investigated the correlation between pretraining and pruning effectiveness.
- Why unresolved: This relationship between pretraining and pruning effectiveness has not been systematically studied.
- What evidence would resolve it: A comprehensive study comparing the performance of various datasets as both pretraining and calibration data for pruning would reveal any correlations between their effectiveness in these different contexts.

### Open Question 3
- Question: What is the optimal number of CoT steps in calibration data for maximizing pruning performance across different task categories?
- Basis in paper: [explicit] The authors investigated different numbers of CoT steps (3, 4, 5) but found no clear relationship between step count and performance, suggesting the optimal number may vary by task.
- Why unresolved: The study was limited to arithmetic reasoning tasks and did not explore a wider range of task categories or more granular step counts.
- What evidence would resolve it: Systematic experiments varying CoT step counts across multiple task categories with finer granularity would identify optimal configurations for different task types.

## Limitations

- Limited generalizability across different LLM architectures and sizes beyond Llama 2-Chat 7B and Llama 7B models
- Focus only on unstructured sparsity at 50% and 70% rates, not exploring structured pruning or different sparsity targets
- No investigation of temporal dynamics - whether optimal calibration data changes as models continue to train or adapt over time

## Confidence

**High Confidence**: The finding that Pile consistently outperforms C4 across multiple pruning methods and task categories is well-supported by systematic experimentation.

**Medium Confidence**: The observation that arithmetic reasoning datasets perform on par or better than pre-training datasets is supported by the data, but the mechanism behind this effect requires further investigation.

**Low Confidence**: The specific recommendations about optimal calibration data for individual downstream tasks should be interpreted cautiously due to high task-dependency and lack of a definitive decision framework.

## Next Checks

1. **Cross-architecture validation**: Test whether Pile remains the optimal calibration dataset for pruning when applied to different LLM families (e.g., GPT-3.5, Mistral, or other transformer variants) to assess the generalizability of findings beyond Llama models.

2. **Scaling experiment**: Evaluate calibration data effectiveness across multiple model scales (1B, 7B, 13B, 34B parameters) to determine whether the relative performance of different calibration datasets remains consistent as model capacity changes.

3. **Temporal stability test**: Conduct follow-up pruning experiments after model fine-tuning or continued pre-training to assess whether optimal calibration data shifts over time, and whether dynamic calibration data selection might be necessary for production deployments.