---
ver: rpa2
title: 'HoTPP Benchmark: Are We Good at the Long Horizon Events Forecasting?'
arxiv_id: '2406.14341'
source_url: https://arxiv.org/abs/2406.14341
tags:
- events
- prediction
- t-map
- event
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces HoTPP, the first benchmark for evaluating
  long-horizon event forecasting in marked temporal point processes. The authors identify
  limitations in existing evaluation metrics and propose a new metric called T-mAP
  (Temporal mean Average Precision), inspired by object detection methods.
---

# HoTPP Benchmark: Are We Good at the Long Horizon Events Forecasting?

## Quick Facts
- **arXiv ID**: 2406.14341
- **Source URL**: https://arxiv.org/abs/2406.14341
- **Reference count**: 40
- **Primary result**: Introduction of HoTPP benchmark with T-mAP metric for long-horizon event forecasting evaluation

## Executive Summary
The HoTPP benchmark addresses a critical gap in evaluating long-horizon event forecasting in marked temporal point processes. The authors identify fundamental limitations in existing metrics like Optimal Transport Distance (OTD) and propose a theoretically grounded T-mAP metric inspired by object detection methods. Through comprehensive experiments on five diverse datasets, they reveal that modern MTPP approaches often underperform simple statistical baselines and exhibit mode collapse. The work provides efficient implementations of popular models and establishes new evaluation standards for the field.

## Method Summary
The HoTPP benchmark introduces T-mAP (Temporal mean Average Precision) as a new evaluation metric for long-horizon event forecasting, addressing limitations in OTD by evaluating full label distributions rather than fixed-size prefixes. The benchmark includes implementations of statistical baselines, intensity-based models (RMTPP, NHP), intensity-free approaches (MAE-CE), Next-K variants, and HYPRO reranking methods. All models are implemented in PyTorch Lightning with Hydra configuration, supporting multi-seed evaluation for stability assessment. The framework processes five datasets with different characteristics and provides GPU-accelerated T-mAP computation using the Jonker-Volgenant algorithm for optimal matching.

## Key Results
- Modern MTPP approaches often underperform simple statistical baselines across multiple datasets
- Most methods exhibit mode collapse, showing decreasing entropy in predicted sequences
- T-mAP metric successfully evaluates long-horizon prediction quality where OTD fails
- Optimal predicted sequence length is typically less than the maximum horizon due to confidence degradation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: T-mAP metric addresses the fundamental limitations of OTD by evaluating the full label distribution rather than just the most probable label.
- **Mechanism**: T-mAP compares predicted and ground truth sequences within a time horizon using an assignment problem solver (Jonker-Volgenant algorithm) to find optimal matching between predictions and ground truth events. It then constructs a precision-recall curve across all thresholds to compute average precision.
- **Core assumption**: The optimal matching for any threshold can be derived from the optimal matching of the full sequence (Theorem 4.1).
- **Evidence anchors**:
  - [abstract]: "We identify shortcomings in widely used evaluation metrics, propose a theoretically grounded T-mAP metric"
  - [section]: "For any threshold h, we can count true positives (TP), false positives (FP), and false negatives (FN) across all predicted and ground truth sequences"
  - [corpus]: Weak - no direct evidence in corpus neighbors about T-mAP metric specifics

### Mechanism 2
- **Claim**: Next-K models can outperform autoregressive models for long-horizon prediction because they avoid error accumulation from previous predictions.
- **Mechanism**: Next-K models predict multiple future events simultaneously rather than sequentially, eliminating the dependency on potentially erroneous previous predictions. This allows for better calibration of confidence across the prediction horizon.
- **Core assumption**: Predicting multiple events independently maintains or improves overall prediction quality compared to sequential autoregressive generation.
- **Evidence anchors**:
  - [abstract]: "we analyze the impact of autoregression and intensity-based losses on prediction quality"
  - [section]: "In contrast, Next-K models demonstrate increasing entropy, which is expected as future events become harder to predict"
  - [corpus]: Weak - no direct evidence in corpus neighbors about Next-K model performance

### Mechanism 3
- **Claim**: The optimal sequence length for prediction is typically less than the maximum horizon, as confidence degrades for events further in the future.
- **Mechanism**: As prediction horizon extends, the model's confidence estimation for distant events degrades, making it beneficial to limit the number of predictions. T-mAP is the only metric capable of evaluating this optimal predicted sequence length.
- **Core assumption**: Prediction confidence naturally degrades with increasing time horizon, requiring manual sequence length limitation.
- **Evidence anchors**:
  - [abstract]: "we analyze the diversity of predicted sequences and find that most methods exhibit mode collapse"
  - [section]: "The optimal number of predicted events is usually less than the maximum number of events in the horizon"
  - [corpus]: Weak - no direct evidence in corpus neighbors about sequence length optimization

## Foundational Learning

- **Concept**: Temporal Point Processes (TPP)
  - **Why needed here**: Understanding the probabilistic framework for modeling event sequences with irregular time intervals is fundamental to grasping the HoTPP benchmark's approach.
  - **Quick check question**: How does the intensity function in a TPP differ from a probability density function, and why is this distinction important for event prediction?

- **Concept**: Optimal Transport Distance (OTD)
  - **Why needed here**: OTD serves as the baseline comparison metric that T-mAP aims to improve upon, so understanding its formulation and limitations is crucial.
  - **Quick check question**: What are the key limitations of OTD when evaluating long-horizon event forecasting, particularly regarding fixed-size prefix comparisons?

- **Concept**: Assignment Problem and Jonker-Volgenant Algorithm
  - **Why needed here**: These concepts underpin the efficient computation of T-mAP, making them essential for understanding how the metric achieves practical performance.
  - **Quick check question**: How does the Jonker-Volgenant algorithm solve the assignment problem, and why is this approach suitable for matching predicted and ground truth events in T-mAP?

## Architecture Onboarding

- **Component map**: Data preprocessing → Model training → Evaluation metrics → Analysis
  - Data preprocessing transforms raw event sequences into standardized format
  - Model training optimizes parameters for prediction tasks
  - Evaluation computes both OTD and T-mAP metrics
  - Analysis compares performance across methods and datasets

- **Critical path**: Data → Model → Training → Evaluation → Analysis
  - Data preprocessing transforms raw event sequences into standardized format
  - Model training optimizes parameters for prediction tasks
  - Evaluation computes both OTD and T-mAP metrics
  - Analysis compares performance across methods and datasets

- **Design tradeoffs**:
  - T-mAP vs OTD: T-mAP provides more nuanced evaluation but requires more complex computation
  - Next-K vs Autoregression: Next-K avoids error accumulation but cannot predict arbitrary sequence lengths
  - Statistical baselines vs Neural models: Simple baselines provide reference points but lack flexibility

- **Failure signatures**:
  - OTD showing low values even for simple baselines indicates metric limitations
  - T-mAP showing inconsistent results across different δ parameters suggests calibration issues
  - Entropy decreasing in autoregressive models indicates error accumulation

- **First 3 experiments**:
  1. Compare OTD and T-mAP metrics on a simple dataset with known ground truth to validate T-mAP computation
  2. Test statistical baselines (MostPopular, Last-K) against neural models on Transactions dataset to establish performance baselines
  3. Evaluate Next-K vs Autoregression performance on Retweet dataset to verify the claim about avoiding error accumulation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does intensity-free modeling consistently outperform intensity-based approaches across all MTPP datasets and tasks?
- Basis in paper: [explicit] The authors compare MAE-CE (intensity-free) with RMTPP and NHP (intensity-based) and note that MAE-CE excels in next-event MAE prediction but results vary between datasets.
- Why unresolved: The comparison is limited to a specific set of methods and datasets. Other intensity-free or intensity-based methods might yield different results.
- What evidence would resolve it: Comprehensive benchmarking of a wider range of intensity-free and intensity-based methods across diverse MTPP datasets and tasks, including long-horizon forecasting.

### Open Question 2
- Question: What is the optimal predicted sequence length for long-horizon event forecasting in MTPP, and how does it vary across datasets and methods?
- Basis in paper: [explicit] The authors observe that long-horizon prediction quality depends on the maximum number of allowed predictions and recommend careful tuning of this hyperparameter.
- Why unresolved: The optimal sequence length is dataset and method-dependent, and the paper does not provide a general guideline for determining it.
- What evidence would resolve it: Systematic analysis of the impact of predicted sequence length on long-horizon forecasting performance across a wide range of datasets and methods, leading to a general guideline or heuristic.

### Open Question 3
- Question: How can label distribution estimation and confidence calibration be improved in long-horizon event forecasting?
- Basis in paper: [explicit] The authors highlight the importance of label distribution estimation and emphasize the need to improve confidence estimation and calibration.
- Why unresolved: The paper does not propose specific solutions for improving label distribution estimation and confidence calibration.
- What evidence would resolve it: Development and evaluation of novel methods for improving label distribution estimation and confidence calibration in long-horizon event forecasting, leading to more reliable and interpretable predictions.

## Limitations

- The theoretical assumption underlying T-mAP (Theorem 4.1) requires empirical validation across diverse datasets
- Mode collapse in MTPP methods may represent either a fundamental limitation or a modeling artifact that needs deeper investigation
- The comparison between Next-K and autoregressive approaches doesn't fully address scenarios with strong temporal dependencies

## Confidence

- **High Confidence**: The empirical finding that statistical baselines often outperform sophisticated MTPP methods (confirmed across 5 datasets)
- **Medium Confidence**: The theoretical foundation of T-mAP and its computational efficiency claims (limited by lack of external validation)
- **Medium Confidence**: The analysis of mode collapse in MTPP methods (based on observed entropy patterns, but requires deeper investigation)

## Next Checks

1. **Validate Theorem 4.1 empirically**: Test whether the optimal matching assumption holds across datasets with different temporal characteristics by comparing computed T-mAP scores with exhaustive threshold-by-threshold matching

2. **Investigate mode collapse mechanisms**: Analyze the latent space representations of MTPP models to determine whether mode collapse is a training issue or an inherent limitation of the prediction task formulation

3. **Benchmark Next-K generalization**: Evaluate Next-K models on datasets with strong temporal dependencies (like MIMIC-IV) to determine if simultaneous prediction truly outperforms autoregressive approaches in all scenarios