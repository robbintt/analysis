---
ver: rpa2
title: A Two-Stage Algorithm for Cost-Efficient Multi-instance Counterfactual Explanations
arxiv_id: '2403.01221'
source_url: https://arxiv.org/abs/2403.01221
tags:
- counterfactual
- multi-instance
- counterfactuals
- instances
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the problem of generating cost-efficient counterfactual
  explanations for groups of instances, which is important in scenarios where a single
  explanation needs to satisfy multiple instances simultaneously (e.g., customer satisfaction
  or employee attrition). The authors propose a two-stage algorithm: first, clustering
  instances based on the direction of their individual counterfactuals using cosine
  similarity, and second, computing multi-instance counterfactuals for each cluster
  using an evolutionary algorithm.'
---

# A Two-Stage Algorithm for Cost-Efficient Multi-instance Counterfactual Explanations

## Quick Facts
- arXiv ID: 2403.01221
- Source URL: https://arxiv.org/abs/2403.01221
- Authors: André Artelt; Andreas Gregoriades
- Reference count: 27
- Primary result: Proposed algorithm achieves superior or competitive performance (correctness 0.95-1.0, cost 0.55-0.73) compared to existing methods

## Executive Summary
This paper addresses the problem of generating cost-efficient counterfactual explanations for groups of instances, where a single explanation must satisfy multiple instances simultaneously. The authors propose a two-stage algorithm that first clusters instances based on the direction of their individual counterfactuals using cosine similarity, then computes multi-instance counterfactuals for each cluster using an evolutionary algorithm. The method is evaluated on two datasets (IBM HR attrition and Law school admissions) and compared against two existing methods, demonstrating excellent performance in terms of correctness and cost efficiency.

## Method Summary
The authors propose a two-stage algorithm for generating cost-efficient multi-instance counterfactual explanations. First, instances are clustered based on the direction of their individual counterfactuals using cosine similarity, grouping together instances that require similar feature changes. Second, an evolutionary algorithm (specifically a (μ + λ) genetic algorithm) is used to compute multi-instance counterfactuals for each cluster, optimizing for both cost minimization and correctness. The method is evaluated on two real-world datasets and compared against existing approaches, showing superior or competitive performance.

## Key Results
- The proposed algorithm achieves correctness scores of 0.95-1.0 across both datasets
- Cost efficiency is measured at 0.55-0.73 percentage of changed features
- The two-stage approach outperforms or matches two existing methods in all evaluation metrics
- Clustering by counterfactual direction yields better results than spatial clustering

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Clustering based on counterfactual direction (cosine similarity) yields more cost-efficient multi-instance explanations than spatial clustering
- Mechanism: When individual counterfactuals point in similar directions, their multi-instance counterpart has lower cost (Lemma 1). Clustering by counterfactual direction groups instances that require similar feature changes, reducing the maximum individual cost needed for the group
- Core assumption: The local behavior of the classifier near each instance can be approximated as linear, making counterfactual directions comparable across instances
- Evidence anchors:
  - [abstract]: "The method is evaluated on two datasets... and compared against two existing methods. The proposed algorithm achieves superior or competitive performance in terms of correctness (0.95-1.0) and cost (0.55-0.73 percentage of changed features)"
  - [section]: "Lemma 1 states that if the individual counterfactuals all have the same direction, then a multi-instance counterfactual not only exists but we can also state a tight upper bound on its cost"
  - [corpus]: Weak evidence - corpus contains papers on counterfactual explanations but lacks specific studies on clustering by counterfactual direction
- Break condition: When classifier behavior is highly non-linear in the region of interest, making local linear approximations invalid

### Mechanism 2
- Claim: Evolutionary algorithms can effectively optimize multi-objective counterfactual problems
- Mechanism: The (μ + λ) genetic algorithm searches the space of possible counterfactuals, using mutation based on feasible feature change ranges and combining multiple objectives (cost minimization and correctness) into a single fitness function
- Core assumption: The search space of counterfactuals is sufficiently continuous and smooth for evolutionary optimization to be effective
- Evidence anchors:
  - [section]: "Our evolutionary method is an instance of the classic (μ + λ) genetic algorithm... and can handle all types of variables"
  - [section]: "we propose an evolutionary method for solving Definition 2. This not only constitutes a model/domain-agnostic method but also a very flexible solution since additional constraints can be easily introduced"
  - [corpus]: Weak evidence - corpus mentions various counterfactual methods but lacks specific evaluation of evolutionary approaches for this problem
- Break condition: When the objective landscape is too rugged or discontinuous for evolutionary algorithms to make meaningful progress

### Mechanism 3
- Claim: The two-stage approach (clustering then optimization) reduces outlier effects on multi-instance counterfactual cost
- Mechanism: By grouping instances first based on similar counterfactual directions, outliers are isolated rather than contaminating the counterfactual generation for the entire set, leading to more cost-efficient explanations for the majority
- Core assumption: Real-world datasets contain some outlier instances whose counterfactuals would significantly increase the cost if included in the same group as the majority
- Evidence anchors:
  - [section]: "Stage 1) Finding a grouping of instances and then Stage 2) Computing multi-instance counterfactual explanations for each of those groups – by this, we aim to reduce the effect of outliers on the cost of the final multi-instance counterfactuals"
  - [section]: "From the results, we observe that our proposed method achieves excellent performance... The results demonstrate the merits of our proposed two-stage algorithm"
  - [corpus]: Weak evidence - corpus contains related work but lacks direct comparison of two-stage vs single-stage approaches
- Break condition: When the dataset has no meaningful outliers or when all instances naturally require very similar counterfactuals

## Foundational Learning

- Concept: Counterfactual explanations
  - Why needed here: The entire method builds on the foundation of what counterfactual explanations are and how they work
  - Quick check question: What is the key difference between a counterfactual explanation and a feature importance explanation?

- Concept: Multi-objective optimization
  - Why needed here: The core problem involves balancing cost efficiency against correctness across multiple instances
  - Quick check question: How does the Pareto optimality concept apply to multi-instance counterfactual explanations?

- Concept: Clustering by similarity metrics
  - Why needed here: The first stage relies on grouping instances based on the similarity of their counterfactual directions
  - Quick check question: Why might cosine similarity be preferred over Euclidean distance when clustering counterfactual directions?

## Architecture Onboarding

- Component map: Instance processing → Counterfactual generation → Direction clustering → Multi-instance optimization → Result aggregation
- Critical path: Instance → Individual counterfactual → Clustering assignment → Group counterfactual → Evaluation
- Design tradeoffs: Clustering granularity vs. computational cost; exploration vs. exploitation in evolutionary algorithm; single vs. multiple objectives
- Failure signatures: High variance in cluster costs; low correctness scores; computational timeouts; feature change distribution anomalies
- First 3 experiments:
  1. Run the full pipeline on a small synthetic dataset where ground truth counterfactuals are known
  2. Compare clustering by counterfactual direction vs. spatial clustering on a simple dataset
  3. Evaluate the impact of the number of clusters on overall cost and correctness metrics

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in a dedicated section.

## Limitations

- The method relies on the assumption that classifier behavior can be approximated as linear near each instance, which may not hold for complex models
- The comparison is limited to only two existing methods, which may not fully represent the state of the art
- Implementation details of the evolutionary algorithm (population size, mutation rates) are not fully specified, affecting reproducibility

## Confidence

- **High confidence**: The core two-stage framework (clustering then optimization) is well-justified and the experimental setup is clearly described
- **Medium confidence**: The effectiveness of cosine similarity for clustering counterfactual directions, as this depends on classifier linearity assumptions
- **Medium confidence**: The superiority over existing methods, given the limited comparison set and lack of ablation studies

## Next Checks

1. Conduct sensitivity analysis on the number of clusters to determine optimal granularity for different dataset sizes and characteristics
2. Test the method on datasets with known non-linear classifier behavior to validate the linear approximation assumption
3. Implement and compare against additional state-of-the-art multi-instance counterfactual methods to strengthen the comparative analysis