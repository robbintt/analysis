---
ver: rpa2
title: 'VOODOO XP: Expressive One-Shot Head Reenactment for VR Telepresence'
arxiv_id: '2405.16204'
source_url: https://arxiv.org/abs/2405.16204
tags:
- head
- neural
- facial
- expression
- reenactment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VOODOO XP is a 3D-aware one-shot head reenactment system for VR
  telepresence. It transfers facial expressions from a driver video to a source portrait
  in real-time while preserving identity and producing view-consistent renderings
  for novel camera poses.
---

# VOODOO XP: Expressive One-Shot Head Reenactment for VR Telepresence

## Quick Facts
- arXiv ID: 2405.16204
- Source URL: https://arxiv.org/abs/2405.16204
- Reference count: 40
- One-line primary result: VOODOO XP is a 3D-aware one-shot head reenactment system for VR telepresence that transfers facial expressions from a driver video to a source portrait in real-time while preserving identity and producing view-consistent renderings for novel camera poses.

## Executive Summary
VOODOO XP introduces a novel 3D-aware one-shot head reenactment system for VR telepresence. The method transfers facial expressions from a driver video to a source portrait in real-time while preserving identity and producing view-consistent renderings for novel camera poses. Using a transformer-based architecture, it directly modifies the source's 3D lifting module through cross-attention layers, enabling highly expressive facial reenactment without relying on parametric models. The system is integrated into an end-to-end VR telepresence setup, allowing instant avatar creation from a single photo and real-time communication using Meta Quest Pro headsets.

## Method Summary
VOODOO XP uses a transformer-based architecture that directly modifies the source's 3D lifting module through cross-attention layers. The system extracts expression vectors from the driver image using a vision transformer and injects these vectors into intermediate transformer blocks of the source's 3D lifting module. This modifies the tri-plane representation directly, allowing fine-scale and asymmetric expressions to be synthesized. A novel multi-stage training approach combines explicit face neutralization and 3D lifted frontalization in the initial stage with fine-scale training and global fine-tuning. The method is trained on CelebVHQ and Nersemble datasets and integrated into a VR telepresence setup for real-time avatar creation and communication.

## Key Results
- Superior expressiveness and identity preservation compared to state-of-the-art methods
- Improved quantitative metrics including FID, LPIPS, and CSIM scores
- Real-time performance with instant avatar creation from a single photo
- View-consistent renderings across novel camera poses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Direct expression transfer into the 3D lifting module enables highly expressive facial reenactment.
- Mechanism: The system uses a vision transformer to extract expression vectors from the driver image and injects these vectors into intermediate transformer blocks of the source's 3D lifting module via cross-attention layers. This modifies the tri-plane representation directly, allowing fine-scale and asymmetric expressions to be synthesized without relying on parametric models.
- Core assumption: Low-level features in the tri-plane representation can be meaningfully altered by cross-attention to change expression while preserving identity.
- Evidence anchors:
  - [abstract]: "Our method uses a transformer-based architecture that directly modifies the source's 3D lifting module through cross-attention layers."
  - [section]: "we use another transformer block ğµğ‘’ğ‘¥ğ‘ ğ‘– to modify its output: ğ¹ğ‘– = ğµğ‘™ğ‘œğ‘¤ ğ‘– (ğ¹ğ‘– âˆ’1) + ğµğ‘’ğ‘¥ğ‘ ğ‘– (ğ¹ğ‘– âˆ’1, ğ‘’ğ‘‘ )"
- Break condition: If identity leakage occurs or the expression transfer does not generalize to unseen expressions, the core assumption fails.

### Mechanism 2
- Claim: Multi-stage training with explicit face neutralization and 3D lifted frontalization prevents identity leakage while enabling expressiveness.
- Mechanism: Stage 1 uses driver frontalization, strong augmentations, and a novel neutralizing loss to align the identity of the neutralized source and cross-reenacted image. Stage 2 relaxes constraints to improve expressiveness using synthetic drivers. Stage 3 adds GAN loss to enhance high-frequency details and identity preservation.
- Core assumption: Frontalization and neutralization effectively disentangle identity from expression in the volumetric representation.
- Evidence anchors:
  - [abstract]: "We show that highly effective disentanglement is possible using an innovative multi-stage self-supervision approach, which is based on a coarse-to-fine strategy, combined with an explicit face neutralization and 3D lifted frontalization during its initial training stage."
  - [section]: "Lğ‘›ğ‘’ğ‘¢ (ğ‘¥ğ‘ 1, ğ‘¥ğ‘‘2 ) = âˆ¥N (ğ‘¥ğ‘ 1â†’ğ‘‘2 ) âˆ’ N (ğ‘¥ğ‘ 1 ) âˆ¥"
- Break condition: If the neutralizer fails to produce consistent neutral tri-plane representations or if identity leakage persists after training, the mechanism breaks.

### Mechanism 3
- Claim: The novel view synthesis is view-consistent and preserves identity across extreme poses and expressions.
- Mechanism: The tri-plane based NeRF representation inherently supports view-consistent rendering. The disentanglement in training ensures that identity features are not corrupted by expression modifications, and the volumetric renderer samples from the modified tri-plane to produce consistent outputs from arbitrary viewpoints.
- Core assumption: The tri-plane representation and volumetric rendering preserve geometric and appearance consistency across views when properly trained.
- Evidence anchors:
  - [abstract]: "Our solution is real-time, view-consistent, and can be instantly used without calibration or fine-tuning."
  - [section]: "Given an input portrait, Lp3D first extracts low-level features using a hybrid transformer model and high-frequency details using a convolution-based network."
- Break condition: If novel view renderings show identity drift or expression artifacts, the assumption fails.

## Foundational Learning

- Concept: Tri-plane based NeRF and volumetric rendering
  - Why needed here: Provides a 3D-aware representation that supports view-consistent rendering and expressive facial synthesis from a single image.
  - Quick check question: How does a tri-plane representation enable efficient and consistent novel view synthesis compared to dense voxel grids?

- Concept: Transformer-based feature injection and cross-attention
  - Why needed here: Allows direct modification of low-level features in the 3D lifting module to transfer expressions without parametric constraints.
  - Quick check question: What is the role of cross-attention in transferring driver expression features to the source's tri-plane representation?

- Concept: Multi-stage self-supervised training with disentanglement
  - Why needed here: Enables effective separation of identity and expression in a fully volumetric setting, preventing identity leakage while allowing expressive reenactment.
  - Quick check question: Why is explicit face neutralization necessary in the first training stage, and how does it prevent identity leakage?

## Architecture Onboarding

- Component map:
  - Driver image â†’ Vision Transformer (DINO) â†’ Expression vector
  - Source image â†’ Lp3D 3D lifting module â†’ Tri-plane representation
  - Expression vector + Tri-plane â†’ Expression transfer module (transformer blocks with cross-attention) â†’ Modified tri-plane
  - Modified tri-plane â†’ Volumetric renderer â†’ Output image
  - Optional: 2Ã— super-resolution network for higher resolution

- Critical path: Driver â†’ Expression vector â†’ Expression transfer â†’ Tri-plane â†’ Rendering

- Design tradeoffs:
  - Using tri-plane NeRF vs. explicit mesh: Better expressiveness and view-consistency, but higher computational cost.
  - Direct expression transfer vs. parametric models: More expressive and handles asymmetry, but requires complex disentanglement training.
  - Multi-stage training vs. end-to-end: Better disentanglement and identity preservation, but longer training time.

- Failure signatures:
  - Identity leakage: Driver's identity appears in the output (hair, skin color, etc.).
  - Expression artifacts: Unnatural mouth shapes, missing fine-scale details, or asymmetric errors.
  - View inconsistency: Identity or expression changes across novel views.

- First 3 experiments:
  1. Verify expression transfer: Input a neutral source and an expressive driver; check if the output matches the driver's expression while preserving source identity.
  2. Test identity preservation: Cross-reenact with different identities; ensure no identity leakage from the driver.
  3. Validate view consistency: Render the same reenactment from multiple views; confirm identity and expression consistency.

## Open Questions the Paper Calls Out
- Question: How does the performance of VOODOO XP compare to 2D-based methods like EMOPortraits when generating extreme expressions and views?
  - Basis in paper: [explicit] The paper mentions that EMOPortraits is a 2D method not designed for 3D novel-view synthesis, and provides qualitative comparisons showing VOODOO XP handles extreme cases better.
  - Why unresolved: The paper only provides qualitative comparisons, not quantitative metrics comparing VOODOO XP to 2D methods like EMOPortraits on extreme expressions and views.
  - What evidence would resolve it: Quantitative metrics (e.g., FID, LPIPS, CSIM) comparing VOODOO XP to 2D methods like EMOPortraits on a dataset with extreme expressions and views.

- Question: How does the multi-stage training approach affect the expressiveness of VOODOO XP compared to a single-stage approach?
  - Basis in paper: [explicit] The paper introduces a multi-stage training approach with explicit neutralization and 3D lifted frontalization in the first stage, followed by fine-scale training and global fine-tuning. It mentions this improves expressiveness compared to previous works.
  - Why unresolved: The paper doesn't provide ablation studies comparing the multi-stage approach to a single-stage approach.
  - What evidence would resolve it: Quantitative metrics (e.g., FID, LPIPS, CSIM) comparing VOODOO XP with and without the multi-stage training approach on a dataset with extreme expressions.

- Question: How does VOODOO XP perform on out-of-distribution data, such as faces with extreme makeup or animal faces?
  - Basis in paper: [explicit] The paper mentions limitations of VOODOO XP, including struggles with out-of-distribution data, side views, and aliasing artifacts. It provides examples of VOODOO XP hallucinating human features on animal faces and extreme makeup.
  - Why unresolved: The paper only provides a few examples of out-of-distribution data, not a comprehensive evaluation.
  - What evidence would resolve it: A comprehensive evaluation of VOODOO XP on a dataset with out-of-distribution data, such as faces with extreme makeup, animal faces, or other non-human subjects.

## Limitations
- Reliance on a multi-stage training pipeline introduces complexity and may limit scalability to new datasets
- Performance on extreme facial expressions or occlusions (e.g., glasses, facial hair) is not thoroughly evaluated
- Computational cost of transformer-based expression transfer may limit real-time performance on less powerful hardware

## Confidence

- **High Confidence**: The core mechanism of using cross-attention to modify the tri-plane representation for expression transfer is well-supported by the method description and experimental results. The multi-stage training approach is clearly outlined and justified.
- **Medium Confidence**: The effectiveness of the neutralizing loss in preventing identity leakage is demonstrated qualitatively and via CSIM scores, but the exact architecture of the neutralizer network remains unspecified. The view consistency claims are supported by qualitative results but lack extensive quantitative validation across diverse viewpoints.
- **Low Confidence**: The real-time performance claims in a VR telepresence setup are based on integration with Meta Quest Pro headsets, but specific latency measurements and hardware requirements are not provided. The method's robustness to extreme poses or occlusions is not thoroughly tested.

## Next Checks
1. **Identity Leakage Test**: Cross-reenact with a diverse set of driver identities and measure CSIM scores between source and reenacted outputs to ensure identity preservation.
2. **View Consistency Evaluation**: Render reenacted outputs from multiple novel viewpoints (e.g., Â±30Â° yaw/pitch) and quantify identity and expression consistency using geometric and photometric metrics.
3. **Extreme Expression Robustness**: Test the system on highly expressive or occluded driver videos (e.g., wide smiles, open mouth, glasses) and assess output quality and identity preservation.