---
ver: rpa2
title: 'A Survey on Deep Clustering: From the Prior Perspective'
arxiv_id: '2406.19602'
source_url: https://arxiv.org/abs/2406.19602
tags: []
core_contribution: 'This survey systematically reviews deep clustering methods through
  the lens of prior knowledge, identifying six categories: structure prior, distribution
  prior, augmentation invariance, neighborhood consistency, pseudo-labeling, and external
  knowledge. The paper traces the evolution from early structure-based approaches
  like spectral clustering to modern methods leveraging data augmentation and external
  information.'
---

# A Survey on Deep Clustering: From the Prior Perspective

## Quick Facts
- **arXiv ID**: 2406.19602
- **Source URL**: https://arxiv.org/abs/2406.19602
- **Reference count**: 29
- **Primary result**: Systematic review of deep clustering methods through prior knowledge lens, categorizing approaches into six types and benchmarking them across five datasets

## Executive Summary
This survey provides a comprehensive framework for understanding deep clustering methods by categorizing them according to the types of prior knowledge they leverage. The authors identify six categories: structure prior, distribution prior, augmentation invariance, neighborhood consistency, pseudo-labeling, and external knowledge. By tracing the evolution from early structure-based approaches to modern methods using data augmentation and external information, the paper reveals how deep clustering has shifted from mining to constructing priors and from internal to external knowledge sources. The survey benchmarks representative methods across five image datasets, demonstrating performance improvements as methods incorporate more sophisticated priors, with external knowledge approaches achieving state-of-the-art results.

## Method Summary
The survey systematically categorizes deep clustering methods based on the prior knowledge they exploit. Structure prior methods rely on assumptions about cluster structure, distribution prior methods assume specific data distributions, augmentation invariance methods leverage data augmentation techniques, neighborhood consistency methods maintain local neighborhood relationships, pseudo-labeling methods use self-supervised training with generated labels, and external knowledge methods incorporate information from outside the dataset. The authors implement and evaluate representative methods from each category using five benchmark datasets (CIFAR-10, CIFAR-100, STL-10, ImageNet-10, ImageNet-Dogs) with clustering accuracy (ACC), normalized mutual information (NMI), and adjusted rand index (ARI) as evaluation metrics.

## Key Results
- Deep clustering methods show consistent performance improvements as they progress from structure-based to external knowledge approaches
- External knowledge methods, particularly those incorporating semantic information, achieve state-of-the-art clustering performance
- The six-category framework effectively explains the evolution and relationships between different deep clustering approaches
- Recent methods that combine multiple prior knowledge types demonstrate superior performance compared to single-prior approaches

## Why This Works (Mechanism)
The effectiveness of deep clustering methods stems from their ability to incorporate domain knowledge through different types of priors. Structure priors work by assuming specific geometric properties of data clusters, distribution priors leverage statistical assumptions about data generation, and augmentation invariance priors exploit the fact that semantically similar data should maintain cluster membership under transformations. Neighborhood consistency priors preserve local relationships, pseudo-labeling priors enable self-supervised learning, and external knowledge priors bring in semantic information from outside the dataset. By combining these priors, modern deep clustering methods can overcome the limitations of traditional approaches that rely on single assumptions.

## Foundational Learning
- **Prior Knowledge in Clustering**: Domain-specific assumptions that guide the clustering process and help avoid trivial solutions. Needed to provide structure when no ground truth labels exist. Quick check: Verify that the chosen priors align with the dataset's characteristics.

- **Spectral Clustering Foundations**: Traditional method using graph Laplacian to capture cluster structure. Needed as the basis for many structure prior deep clustering methods. Quick check: Ensure the graph construction properly reflects data similarities.

- **Data Augmentation Invariance**: Assumption that transformed versions of the same instance belong to the same cluster. Needed for leveraging large amounts of unlabeled data. Quick check: Confirm that augmentations preserve semantic content while introducing variation.

- **Self-supervised Learning**: Training framework that generates pseudo-labels for unsupervised learning. Needed to provide supervisory signals in the absence of ground truth. Quick check: Verify pseudo-label quality and consistency across training iterations.

- **External Knowledge Integration**: Incorporating semantic information from pretrained models or knowledge bases. Needed to overcome limitations of internal priors. Quick check: Ensure external knowledge aligns with the target clustering task.

## Architecture Onboarding

**Component Map**: Input Data -> Encoder f(·) -> Feature Extraction -> Cluster Head h(·) -> Cluster Assignment

**Critical Path**: The essential components are the encoder network for feature extraction and the clustering head for generating assignments. The encoder must produce discriminative features while the clustering head must effectively group these features according to the chosen prior.

**Design Tradeoffs**: Structure prior methods prioritize computational efficiency but may lack flexibility, while external knowledge methods achieve higher accuracy at the cost of increased complexity and dependency on external resources. Augmentation invariance methods balance performance and generalization but require careful augmentation selection.

**Failure Signatures**: Common failure modes include degenerate solutions where most instances are assigned to a single cluster, poor feature discriminability leading to mixed clusters, and instability during training due to improper hyperparameter settings.

**First Experiments**:
1. Implement a basic structure prior method (e.g., spectral clustering-based approach) to establish baseline performance
2. Add augmentation invariance using standard image transformations to evaluate performance gains
3. Incorporate external knowledge from a pretrained model to test the impact of semantic priors

## Open Questions the Paper Calls Out
### Open Question 1
- **Question**: How can deep clustering methods be designed to handle fine-grained clustering tasks where categories exhibit high semantic similarity?
- **Basis in paper**: The paper explicitly identifies fine-grained clustering as a future challenge, noting that traditional priors like color and shape augmentations become ineffective when distinctions lie in subtle characteristics.
- **Why unresolved**: Fine-grained clustering requires capturing nuanced differences that existing priors may miss, and there's limited research on how to adapt augmentation invariance or other priors for such subtle distinctions.
- **What evidence would resolve it**: Empirical demonstrations of deep clustering methods successfully distinguishing between highly similar categories (like biological subspecies) on challenging datasets, showing significant performance improvements over current approaches.

### Open Question 2
- **Question**: What is the most effective approach for determining the optimal number of clusters in non-parametric deep clustering without introducing excessive computational costs?
- **Basis in paper**: The paper identifies non-parametric clustering as a future challenge, noting that existing methods relying on global similarity calculations introduce huge computational costs, especially for large-scale datasets.
- **Why unresolved**: While some works like DeepDPM introduce Dirichlet Process Gaussian Mixture Models, determining the optimal cluster number efficiently at scale remains an open problem that often requires human priors.
- **What evidence would resolve it**: A scalable algorithm that can accurately determine the number of clusters in large-scale datasets with minimal computational overhead, validated on challenging benchmarks like full ImageNet-1K.

### Open Question 3
- **Question**: How can external knowledge from large language models like ChatGPT or GPT-4V(ision) be effectively incorporated into deep clustering frameworks?
- **Basis in paper**: The paper suggests that exploring external pretrained models might be a promising avenue for future deep clustering research, particularly given the success of external knowledge methods like SIC and TAC.
- **Why unresolved**: While some external knowledge methods exist, the integration of sophisticated language-vision models into clustering frameworks is largely unexplored, and the optimal ways to leverage such knowledge remain unclear.
- **What evidence would resolve it**: Concrete implementations showing significant clustering performance improvements by incorporating outputs from large language-vision models, with clear demonstrations of how these models enhance feature discriminability or guide cluster assignments.

## Limitations
- The survey focuses primarily on image datasets without exploring other data modalities like text, audio, or graphs, which may exhibit different clustering characteristics
- Some modern approaches combine multiple prior knowledge types simultaneously, making strict categorization into six categories challenging
- The rapid development of deep clustering methods means some recent approaches may not be fully captured in the survey

## Confidence
- **Methodological Framework**: High - The categorization system effectively explains method evolution and relationships
- **Completeness**: Medium - The survey covers major approaches but may miss some recent developments
- **Benchmarking Claims**: Medium - Performance comparisons depend on implementation details and hyperparameter tuning
- **Generalizability**: Low - Focus on image data limits applicability to other domains

## Next Checks
1. **Cross-dataset validation**: Test the six prior categories on non-image datasets (text, audio) to verify the framework's generalizability beyond visual data.

2. **Prior combination analysis**: Systematically evaluate methods that combine multiple prior knowledge types to determine whether performance gains are additive or synergistic.

3. **Reproducibility audit**: Implement and benchmark at least one representative method from each of the six categories using the specified datasets and metrics to verify the reported performance ranges.