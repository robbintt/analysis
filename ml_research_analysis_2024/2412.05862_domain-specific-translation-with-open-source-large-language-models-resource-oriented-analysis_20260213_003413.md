---
ver: rpa2
title: 'Domain-Specific Translation with Open-Source Large Language Models: Resource-Oriented
  Analysis'
arxiv_id: '2412.05862'
source_url: https://arxiv.org/abs/2412.05862
tags:
- translation
- one-shot
- zero-shot
- llama-3
- baseline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper compares open-source large language models (LLMs) with
  traditional machine translation models for domain-specific translation, focusing
  on the medical domain. The study evaluates four language directions with varying
  resource availability: English-to-French, English-to-Portuguese, English-to-Swahili,
  and Swahili-to-English.'
---

# Domain-Specific Translation with Open-Source Large Language Models: Resource-Oriented Analysis

## Quick Facts
- **arXiv ID**: 2412.05862
- **Source URL**: https://arxiv.org/abs/2412.05862
- **Reference count**: 40
- **Primary result**: Task-oriented MT models (NLLB-200 3.3B) outperform open-source LLMs (7-8B) in 3 of 4 medical domain language pairs tested

## Executive Summary
This paper compares open-source large language models with traditional task-oriented machine translation models for domain-specific translation in the medical domain. The study evaluates four language directions with varying resource availability: English-to-French, English-to-Portuguese, English-to-Swahili, and Swahili-to-English. Results show that task-oriented machine translation models, specifically NLLB-200 3.3B, outperform all evaluated LLMs in the 7-8B parameter range across three out of four language directions. While fine-tuning improves LLM performance, they still underperform compared to fine-tuned NLLB-200 3.3B models. Larger LLMs like Llama-3.1 405B achieve better results but pose deployment challenges.

## Method Summary
The study evaluates open-source LLMs (Mistral 7B, Llama-3 8B, Llama-3.1 8B, Gemma 7B, Qwen models, Llama-3 70B, Llama-3.1 405B, DeepSeek) against task-oriented NLLB-200 3.3B for domain-specific medical translation. Four language directions are tested: English-to-French, English-to-Portuguese, English-to-Swahili, and Swahili-to-English, representing different resource settings. Experiments include zero-shot and one-shot translation using fuzzy match retrieval, with fine-tuning performed via QLoRA on selected models. Translation quality is measured using BLEU, chrF++, TER, and COMET metrics. Training sets of 10,000 (small) and 100,000 (medium) segments are used per language pair, with evaluation on separate test sets.

## Key Results
- NLLB-200 3.3B outperforms all 7-8B parameter LLMs in 3 of 4 language directions tested
- Fine-tuning improves LLM performance but they still underperform fine-tuned NLLB-200 3.3B
- Llama-3.1 405B achieves the best results among LLMs but presents deployment challenges
- Task-oriented MT models remain superior for domain-specific translation, especially in medium-resource and low-resource settings

## Why This Works (Mechanism)
The mechanism behind task-oriented MT models' superior performance lies in their architectural specialization for translation tasks. NLLB-200 3.3B was specifically designed and trained for multilingual translation with domain-specific capabilities, while LLMs are general-purpose models that require fine-tuning to adapt to translation tasks. The task-oriented models have optimized attention mechanisms and training objectives tailored for translation, whereas LLMs must learn these capabilities through adaptation. Additionally, the resource-oriented analysis reveals that task-oriented models maintain efficiency advantages even when compared to much larger LLMs in terms of deployment costs and inference speed.

## Foundational Learning

**Multilingual Neural Machine Translation (NMT)**
*Why needed*: Understanding the architecture differences between general LLMs and specialized NMT models is crucial for interpreting the performance gap observed in the experiments.
*Quick check*: Verify that NLLB-200 uses a standard encoder-decoder architecture with attention mechanisms optimized for translation, unlike transformer-based LLMs that may use different positional encoding or attention patterns.

**Domain Adaptation in Translation**
*Why needed*: The medical domain presents unique terminology and style requirements that affect model performance.
*Quick check*: Confirm that domain-specific datasets (medical corpora) were used for both training and evaluation to ensure fair comparison between models.

**Fine-tuning vs. Task-Oriented Training**
*Why needed*: Understanding the difference between adapting a general model versus training a specialized model from scratch explains the observed performance differences.
*Quick check*: Compare the number of parameters updated during fine-tuning versus the total parameters in task-oriented models to understand the adaptation efficiency.

## Architecture Onboarding

**Component Map**
Data preparation -> Model selection -> Zero-shot/one-shot translation -> Fine-tuning (if applicable) -> Evaluation

**Critical Path**
Dataset curation and preprocessing → Model inference setup → Baseline evaluation → Fine-tuning (for selected models) → Fine-tuned model evaluation → Comparative analysis

**Design Tradeoffs**
- Task-oriented models: Lower parameter count (3.3B) but specialized architecture yields better performance for translation tasks
- LLMs: Higher parameter flexibility but require more resources for fine-tuning and still underperform in domain-specific tasks
- Resource efficiency: Task-oriented models are more computationally efficient for translation despite smaller size
- Deployment considerations: Larger LLMs (405B) achieve better results but are impractical for most production scenarios

**Failure Signatures**
- Poor fuzzy match retrieval quality leading to suboptimal one-shot performance
- Inefficient GPU utilization during inference causing longer translation times
- Overfitting during fine-tuning on small datasets (10K segments)
- BLEU score degradation when switching between zero-shot and fine-tuned modes

**First Experiments**
1. Test zero-shot translation on a held-out validation set to establish baseline performance
2. Evaluate fuzzy match retrieval quality by checking semantic similarity scores on sample source sentences
3. Compare inference times and memory usage between NLLB-200 3.3B and Llama-3.1 405B on the same hardware

## Open Questions the Paper Calls Out
None

## Limitations
- Results may not generalize to proprietary LLMs or models with different architectural choices
- Resource-oriented analysis focuses on specific settings (10K and 100K segments) which may not represent all practical scenarios
- Semantic filtering criteria for dataset curation are not fully specified, potentially affecting evaluation data quality

## Confidence

**High confidence**: Experimental methodology and reproducibility steps are clearly documented with specific model versions, hyperparameters, and evaluation metrics.

**Medium confidence**: Comparative results between LLMs and NLLB-200 3.3B are robust within tested parameter range but may not extend to larger or differently architected models.

**Medium confidence**: Conclusion about ongoing need for specialized MT models is supported by experimental results but depends on future LLM efficiency developments.

## Next Checks
1. Replicate experiments with additional open-source LLMs in the 7-8B parameter range to verify consistency across different architectures
2. Conduct experiments with same models on different domains (e.g., legal or technical) to assess performance patterns across various specialized translation tasks
3. Test resource-efficiency trade-off by evaluating whether smaller, domain-specific models trained from scratch can achieve comparable performance to fine-tuned LLMs in low-resource settings