---
ver: rpa2
title: 'More Room for Language: Investigating the Effect of Retrieval on Language
  Models'
arxiv_id: '2404.10939'
source_url: https://arxiv.org/abs/2404.10939
tags:
- retrieval
- pretraining
- language
- patch
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how retrieval augmentation during pretraining
  affects standalone language model behavior. Using a controlled "ideal retrieval"
  setup with paraphrased contexts, the authors pretrain models of three sizes (9M,
  28M, 98M parameters) with and without retrieval augmentation.
---

# More Room for Language: Investigating the Effect of Retrieval on Language Models

## Quick Facts
- arXiv ID: 2404.10939
- Source URL: https://arxiv.org/abs/2404.10939
- Reference count: 40
- Models with retrieval augmentation perform better on syntactic tasks but worse on global context comprehension

## Executive Summary
This paper investigates how retrieval augmentation during pretraining affects standalone language model behavior. Using a controlled "ideal retrieval" setup with paraphrased contexts, the authors pretrain models of three sizes (9M, 28M, 98M parameters) with and without retrieval augmentation. The primary finding is that retrieval-augmented pretraining leads to a separation between world knowledge (delegated to the retrieval module) and syntactic knowledge (strengthened in the language model). Models with retrieval augmentation perform better on syntactic tasks like dependency parsing and grammaticality judgment but worse on language understanding tasks requiring global context, such as multi-sentence comprehension and reading comprehension.

## Method Summary
The paper uses masked language models pretrained on paraphrased Wikipedia data with and without retrieval augmentation. Retrieval augmentation is implemented via cross-attention to paraphrased contexts, with an "ideal retrieval" setup where paraphrases are semantically similar to original text (cosine similarity > 0.88). Models are evaluated in standalone mode by replacing the cross-attention mechanism with a linear patch. The study uses three model sizes (8.5M, 27.7M, 98.2M parameters) and evaluates on benchmarks including LAMA for world knowledge, BLiMP and MSGS for syntactic knowledge, LAMBADA for global context resolution, GLUE for general language understanding, and SQuAD for reading comprehension.

## Key Results
- Retrieval-augmented models save substantially less world knowledge in their weights (LAMA performance degrades)
- Syntactic understanding consistently improves with retrieval augmentation across all noise levels
- Retrieval augmentation degrades performance on tasks requiring global context comprehension (LAMBADA, GLUE)
- Noisy retrieval (25-50% irrelevant context) during pretraining shows robust performance without degradation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval augmentation shifts parameter allocation from factual knowledge storage to syntactic rule encoding.
- Mechanism: When retrieval provides relevant context during pretraining, the language model no longer needs to store as much factual information in its weights, freeing capacity for syntactic representations.
- Core assumption: The retrieved context reliably contains the factual information needed for language modeling tasks.
- Evidence anchors:
  - [abstract] "these models: i) save substantially less world knowledge in their weights"
  - [section] "strongly suggests that a language model with retrieval does not allocate as many parameters to store world knowledge"
  - [corpus] Weak - no direct evidence about parameter allocation mechanisms.
- Break condition: If retrieved context is unreliable or irrelevant, the model must store more factual knowledge in parameters, negating the shift.

### Mechanism 2
- Claim: Retrieval augmentation degrades long-range context resolution abilities in the standalone language model.
- Mechanism: During pretraining, the language model learns to defer global context processing to the retrieval module, weakening its internal long-range dependency mechanisms.
- Core assumption: The language model optimizes for using retrieval when available rather than developing independent global context processing.
- Evidence anchors:
  - [abstract] "iii) are worse at comprehending global context"
  - [section] "language understanding gets worse with retrieval-augmented pretraining" and "the processing of global context is mainly delegated out of the language model itself to its retrieval augmentation"
  - [corpus] Weak - no evidence about how language models actually distribute processing between self and retrieval.
- Break condition: If retrieval is unavailable during inference, the weakened global context processing significantly degrades performance.

### Mechanism 3
- Claim: Retrieval augmentation improves syntactic knowledge encoding regardless of retrieval noise level.
- Mechanism: The presence of any retrieval context during pretraining shifts the learning objective toward better utilization of local syntactic patterns, with syntactic improvements persisting even with noisy retrieval.
- Core assumption: Even imperfect retrieval context provides enough syntactic signal to improve local pattern learning.
- Evidence anchors:
  - [abstract] "ii) are better at understanding local context and inter-word dependencies"
  - [section] "on the other hand, its syntactic understanding consistently improves" and "poor retrieval quality does not negatively impact pretraining"
  - [corpus] Weak - no evidence about how retrieval noise specifically affects syntactic learning mechanisms.
- Break condition: If retrieval noise exceeds a threshold where syntactic signal is overwhelmed by noise, improvements may degrade.

## Foundational Learning

- Concept: Masked Language Modeling Objective
  - Why needed here: The paper uses masked language models (MLMs) as the base architecture, so understanding how MLMs work is fundamental to interpreting results.
  - Quick check question: What is the difference between predicting masked tokens vs. next token prediction in terms of what linguistic information is learned?

- Concept: Linear Probing for Syntactic Knowledge
  - Why needed here: The paper evaluates syntactic knowledge using linear probing, which extracts dependency parsing information from frozen model representations.
  - Quick check question: Why use a linear classifier rather than a neural network for probing syntactic knowledge?

- Concept: Paraphrase Generation and Semantic Similarity
  - Why needed here: The "ideal retrieval" methodology relies on paraphrases as retrieved context, requiring understanding of how paraphrases preserve meaning while changing surface form.
  - Quick check question: How does semantic similarity (cosine > 0.88) ensure paraphrases provide useful retrieval context without leaking information?

## Architecture Onboarding

- Component map:
  - Encoder-Decoder Transformer with MLM head
  - Cross-attention mechanism for retrieval integration
  - Linear patch replacement for standalone evaluation
  - Paraphrase generation system for ideal retrieval

- Critical path:
  1. Generate paraphrases of training corpus
  2. Pretrain with cross-attention to paraphrases
  3. Replace cross-attention with linear patch
  4. Evaluate standalone language model performance

- Design tradeoffs:
  - Using paraphrases vs. real retrieval: Controllable but may not reflect real retrieval challenges
  - Linear patch vs. other separation methods: Simple but may introduce artifacts
  - MLM vs. causal LM: Better evaluation flexibility but different learning dynamics

- Failure signatures:
  - Poor LAMA performance: Indicates inadequate factual knowledge encoding
  - Low syntactic probing scores: Suggests retrieval augmentation not improving local pattern learning
  - High perplexity on LAMBADA: Indicates global context resolution deficits

- First 3 experiments:
  1. Train a base model with and without retrieval augmentation on a small corpus, evaluate on LAMA and syntactic probing
  2. Introduce 25% and 50% noise into retrieval, measure impact on world knowledge vs. syntactic knowledge
  3. Test retrieval-augmented model with and without retrieval during LAMBADA evaluation to measure global context dependency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the separation between linguistic knowledge and world knowledge scale with model size beyond 100M parameters?
- Basis in paper: [explicit] The paper states "the positive results on syntactic tasks suggest that retrieval-based pretraining can be promising... This separation becomes larger with scale" and "we decided to limit the size of the pretrained language models to 100M parameters."
- Why unresolved: The paper only evaluates models up to 100M parameters due to computational constraints, leaving the scaling behavior for larger models unexplored.
- What evidence would resolve it: Pretraining and evaluating retrieval-augmented models with sizes ranging from 100M to billions of parameters would reveal whether the separation between linguistic and world knowledge continues to grow or plateaus.

### Open Question 2
- Question: How does the performance of retrieval-augmented pretraining differ across languages with varying syntactic structures?
- Basis in paper: [inferred] The paper notes "we only evaluate the syntactic knowledge of an English knowledge model, and the results might differ for a typologically different language" and uses English Wikipedia for pretraining.
- Why unresolved: All experiments are conducted on English data, which may not generalize to languages with different syntactic properties or writing systems.
- What evidence would resolve it: Repeating the experiments with retrieval-augmented pretraining on typologically diverse languages (e.g., languages with rich morphology, free word order, or non-Latin scripts) and comparing syntactic performance would clarify the cross-linguistic applicability.

### Open Question 3
- Question: What is the impact of retrieval augmentation on autoregressive language models versus masked language models?
- Basis in paper: [explicit] The paper states "This study only evaluates the performance of masked language models... We believe that most of our findings hold for causal language models, too; and we look forward to future work that evaluates these (typically much larger) models."
- Why unresolved: The paper exclusively uses masked language models, while autoregressive models are more commonly used in practice and may exhibit different behavior with retrieval augmentation.
- What evidence would resolve it: Pretraining and evaluating autoregressive language models with and without retrieval augmentation on the same benchmarks would directly compare their behavior and validate the paper's hypothesis about generalizability.

## Limitations
- The controlled "ideal retrieval" setup with paraphrased contexts may not reflect real-world retrieval challenges
- Small model sizes (8M-98M parameters) may not capture behaviors of larger language models
- Linear patching method for standalone evaluation introduces potential confounding factors
- Limited noise range (25-50%) may not represent realistic retrieval failure scenarios

## Confidence

**High Confidence**: The observation that retrieval-augmented pretraining improves syntactic knowledge encoding (measured via dependency parsing and grammaticality judgment tasks) is well-supported by consistent results across multiple syntactic benchmarks. The finding that retrieval augmentation reduces world knowledge retention in model parameters (measured via LAMA) shows strong statistical significance across model sizes.

**Medium Confidence**: The claim that retrieval augmentation specifically degrades global context resolution abilities (measured via LAMBADA and GLUE) has supporting evidence but could be influenced by the linear patching procedure. The interpretation that retrieval-augmented models "delegate" global context processing to the retrieval module is plausible but not directly measured. The robustness finding for noisy retrieval (25-50% noise) is supported but tested only within a narrow noise range.

**Low Confidence**: The mechanistic explanation that parameter allocation shifts from factual to syntactic knowledge storage lacks direct evidence - this is inferred from performance patterns rather than measured through parameter analysis. The claim that retrieval augmentation "consistently" improves syntactic knowledge across all noise levels needs more extensive validation across different noise types and distributions.

## Next Checks
1. **Real Retrieval Validation**: Replace the idealized paraphrase retrieval with an actual retrieval system (e.g., dense passage retrieval on Wikipedia) to test whether the observed effects hold under realistic retrieval conditions with variable quality, latency, and noise patterns.

2. **Larger Model Scaling**: Replicate the core experiments with models in the 100M-1B parameter range to determine whether the world knowledge/syntactic knowledge separation effect scales with model capacity, and whether there's a threshold where models become better at maintaining both types of knowledge simultaneously.

3. **Parameter Analysis Study**: Conduct a direct analysis of parameter usage patterns in retrieval-augmented vs standard models, potentially using techniques like activation patching, probing classifiers, or parameter importance scores to verify whether the syntactic improvements correlate with reduced parameter allocation to factual knowledge storage as hypothesized.