---
ver: rpa2
title: 'GPT is Not an Annotator: The Necessity of Human Annotation in Fairness Benchmark
  Construction'
arxiv_id: '2405.15760'
source_url: https://arxiv.org/abs/2405.15760
tags:
- bias
- survey
- benchmark
- benchmarks
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) can
  replace human annotation in constructing bias benchmarks. The authors attempt to
  use GPT-3.5-Turbo to extract bias-related predicates from survey responses for two
  bias benchmarks (WinoQueer and WinoSemitism).
---

# GPT is Not an Annotator: The Necessity of Human Annotation in Fairness Benchmark Construction

## Quick Facts
- arXiv ID: 2405.15760
- Source URL: https://arxiv.org/abs/2405.15760
- Reference count: 10
- Primary result: GPT-3.5-Turbo exhibits significant hallucinations and misrepresentations in predicate extraction for bias benchmarks, failing to replace human annotation.

## Executive Summary
This paper investigates whether large language models can replace human annotation in constructing bias benchmarks. The authors attempt to use GPT-3.5-Turbo to extract bias-related predicates from survey responses for two bias benchmarks (WinoQueer and WinoSemitism). Their analysis reveals significant quality issues with model-extracted predicates, including high rates of hallucinations, misrepresentations, and incomplete extractions. Quantitative metrics and human evaluation show poor correlation between model- and human-created benchmarks.

The authors conclude that GPT-3.5-Turbo is not an appropriate substitute for human annotation in sensitive tasks related to social biases, and that its use negates the benefits of community-sourcing bias benchmarks. This finding highlights the continued necessity of human involvement in fairness evaluation tasks, particularly when dealing with sensitive social dimensions.

## Method Summary
The authors used GPT-3.5-Turbo to extract predicates from survey responses collected for two bias benchmarks (WinoQueer and WinoSemitism). They compared the model-generated predicates against human-annotated predicates using multiple quantitative metrics including precision, recall, and F1 scores. Human evaluators were also employed to assess the quality and accuracy of the extracted predicates. The study examined both the overall performance of the model and specific failure modes such as hallucinations and misrepresentations.

## Key Results
- GPT-3.5-Turbo showed high rates of hallucinations (incorrect information presented as fact) and misrepresentations when extracting predicates from survey responses
- Quantitative metrics (precision, recall, F1) demonstrated poor correlation between model- and human-created benchmarks
- Human evaluation confirmed that model-extracted predicates were frequently inaccurate, incomplete, or misleading
- The use of GPT-3.5-Turbo negated the benefits of community-sourcing bias benchmarks by introducing systematic errors

## Why This Works (Mechanism)
None

## Foundational Learning
- **Bias benchmark construction**: Understanding how bias benchmarks are created and validated through community-sourced annotations
  - Why needed: Provides context for why human annotation is traditionally required in fairness evaluation
  - Quick check: Can you explain the difference between model-generated and human-annotated predicates?

- **Predicate extraction**: The process of identifying key elements or relationships from text that indicate potential bias
  - Why needed: Core technical task being evaluated in the paper
  - Quick check: What distinguishes a hallucination from a misrepresentation in predicate extraction?

- **LLM hallucination**: When language models generate false or fabricated information presented as fact
  - Why needed: Primary failure mode identified in the study
  - Quick check: How do hallucinations differ from simply missing relevant information?

- **Human evaluation methodology**: Systematic approaches to assessing the quality of model outputs through human judgment
  - Why needed: Critical for validating the quantitative metrics used in the study
  - Quick check: What are the limitations of using human evaluators to assess model performance?

## Architecture Onboarding

**Component Map**: Survey Responses -> GPT-3.5-Turbo Predicate Extraction -> Quantitative Metrics (Precision, Recall, F1) + Human Evaluation -> Quality Assessment

**Critical Path**: The most important workflow is the comparison between model-extracted predicates and human-annotated predicates, as this directly determines whether LLMs can substitute for human annotation in bias benchmark construction.

**Design Tradeoffs**: The authors chose to use a relatively older model (GPT-3.5-Turbo) rather than more recent versions, which may limit generalizability but provides a conservative baseline for LLM capabilities in this task.

**Failure Signatures**: The primary failure modes identified are hallucinations (fabricating information), misrepresentations (distorting the original meaning), and incomplete extractions (missing relevant predicates).

**First Experiments**:
1. Replicate the predicate extraction task using GPT-4 to determine if newer models show improved performance
2. Test the methodology on a different type of bias benchmark (e.g., racial or disability bias) to assess generalizability
3. Conduct an ablation study to determine which types of predicates (explicit vs. implicit) are most problematic for LLMs

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses exclusively on GPT-3.5-Turbo, which may not generalize to other LLM architectures or more recent models like GPT-4
- Analysis is limited to two specific benchmark types (WinoQueer and WinoSemitism), potentially missing other forms of bias
- Human evaluation relies on a limited number of annotators, which may not capture the full spectrum of human judgment variability
- The paper does not explore potential mitigation strategies for LLM hallucinations or systematic errors

## Confidence
- **High Confidence**: GPT-3.5-Turbo exhibits significant hallucination and misrepresentation rates in predicate extraction tasks
- **Medium Confidence**: LLMs cannot serve as appropriate substitutes for human annotation in sensitive bias benchmark construction
- **Low Confidence**: Using GPT-3.5-Turbo "negates the benefits of community-sourcing bias benchmarks"

## Next Checks
1. **Cross-model validation**: Test the same predicate extraction task across multiple LLM architectures (GPT-4, Claude, LLaMA) to determine if the quality issues are specific to GPT-3.5-Turbo or representative of LLMs generally.

2. **Task generalization**: Apply the methodology to additional bias benchmark types beyond gender and religion (e.g., racial, disability, age-related biases) to assess whether the findings hold across diverse social dimensions.

3. **Annotation pipeline analysis**: Conduct a detailed error analysis to determine which types of predicates (complex vs. simple, explicit vs. implicit) are most problematic for LLMs, potentially identifying specific use cases where limited automation might still be viable.