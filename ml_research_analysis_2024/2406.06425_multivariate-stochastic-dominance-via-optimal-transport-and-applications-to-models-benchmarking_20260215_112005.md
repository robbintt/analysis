---
ver: rpa2
title: Multivariate Stochastic Dominance via Optimal Transport and Applications to
  Models Benchmarking
arxiv_id: '2406.06425'
source_url: https://arxiv.org/abs/2406.06425
tags:
- optimal
- theorem
- multivariate
- which
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for multivariate stochastic dominance
  testing using optimal transport. The authors define a multivariate first-order stochastic
  dominance violation ratio using entropic regularization, establish a central limit
  theorem and bootstrap consistency for the empirical statistic, and propose a hypothesis
  testing framework.
---

# Multivariate Stochastic Dominance via Optimal Transport and Applications to Models Benchmarking

## Quick Facts
- arXiv ID: 2406.06425
- Source URL: https://arxiv.org/abs/2406.06425
- Reference count: 40
- Primary result: A method for multivariate stochastic dominance testing using optimal transport with entropic regularization, applied to benchmark large language models

## Executive Summary
This paper introduces a framework for testing multivariate first-order stochastic dominance (FSD) between probability measures using entropic optimal transport (EOT). The authors define an entropic violation ratio that measures the degree of FSD violation and establish its statistical properties through a central limit theorem and bootstrap consistency. The method is particularly well-suited for comparing machine learning models evaluated on multiple metrics, capturing dependencies between metrics that univariate approaches miss. The framework is validated on both synthetic data and real-world LLM evaluation benchmarks.

## Method Summary
The method uses entropic optimal transport with cost functions compatible with multivariate FSD to define a violation ratio that quantifies the degree to which one distribution dominates another. The Sinkhorn algorithm efficiently computes the EOT, while a bootstrap procedure estimates the variance of the empirical violation ratio. A hypothesis testing framework with family-wise error rate (FWER) control enables statistically significant decisions about model comparisons. The approach extends univariate FSD testing to multivariate settings while accounting for metric dependencies through the optimal transport formulation.

## Key Results
- The entropic violation ratio provides a measure of multivariate FSD violation that captures dependencies between metrics
- Bootstrap consistency is established for the empirical violation ratio through the functional delta method
- Applications to LLM benchmarking demonstrate the method's ability to provide statistically significant model rankings that account for metric dependencies
- The approach outperforms univariate aggregation methods in capturing meaningful performance differences between models

## Why This Works (Mechanism)

### Mechanism 1
Entropic regularization stabilizes OT computation in high dimensions and improves statistical efficiency. By replacing the hard linear program with a smooth optimization problem, the Sinkhorn algorithm achieves O(N^2) complexity instead of O(N^3) for exact OT, reducing the curse of dimensionality in statistical estimation. The regularization parameter λ controls a bias-variance tradeoff where small λ preserves the OT structure while enabling computational tractability.

### Mechanism 2
The multivariate violation ratio captures dependencies between metrics that univariate aggregation approaches miss. By computing OT with cost functions compatible with multivariate FSD, the method accounts for joint behavior of multiple metrics rather than treating them independently through aggregation techniques. This captures meaningful information about model performance that is lost when reducing to univariate comparisons.

### Mechanism 3
The bootstrap procedure provides valid confidence intervals for the violation ratio in high-dimensional settings. The functional delta method establishes that the empirical violation ratio converges to a Gaussian distribution, and bootstrap resampling consistently estimates the variance of this limiting distribution. This relies on the smoothness condition (SCd) for optimal potentials under the OT problem.

## Foundational Learning

- **Optimal Transport theory and Wasserstein distances**: The entire method is built on characterizing multivariate stochastic dominance through OT problems with compatible cost functions. Quick check: What is the dual formulation of the OT problem and how does entropic regularization modify it?

- **Multivariate stochastic dominance theory**: The paper extends univariate FSD concepts to multivariate settings, requiring understanding of coupling-based characterizations. Quick check: How does the definition of multivariate FSD using couplings differ from univariate FSD based on quantile functions?

- **Bootstrap consistency and the functional delta method**: The statistical inference framework relies on establishing that bootstrap estimates converge to the correct limiting distribution. Quick check: What conditions must be satisfied for the functional delta method to establish bootstrap consistency for a statistic?

## Architecture Onboarding

- **Component map**: LLM evaluation metrics -> Entropic OT Engine (Sinkhorn) -> Violation Ratio Calculator -> Bootstrap Resampler -> Hypothesis Tester -> FWER Controller -> Ranked Model Comparisons

- **Critical path**: 1) Compute EOT costs between model metric distributions, 2) Calculate violation ratios using entropic OT with compatible costs, 3) Perform bootstrap resampling to estimate variance, 4) Apply hypothesis testing with FWER correction, 5) Aggregate pairwise results using Borda count

- **Design tradeoffs**: λ regularization parameter (smaller preserves OT structure but increases computational cost; larger improves speed but reduces accuracy), bootstrap iterations (more improve variance estimation but increase runtime), FWER correction method (Bonferroni is conservative but simple; Holm is tighter but more complex)

- **Failure signatures**: Violation ratios near 0 or 1 indicate extreme dominance relationships, bootstrap variance estimates that don't stabilize suggest insufficient samples, hypothesis test rejections that don't align with pairwise comparisons indicate FWER issues

- **First 3 experiments**: 1) Synthetic data experiment with controlled dominance relationships to verify method captures known structure, 2) Benchmark on univariate FSD reduction method to quantify dependency-capturing advantage, 3) Sensitivity analysis varying λ and bootstrap parameters to understand tradeoff behavior

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal choice of cost function in C≤ for multivariate FSD testing? The paper discusses various compatible cost functions including squared hinge loss and logistic function, but does not provide definitive recommendations for the best cost function.

### Open Question 2
How does the choice of entropic regularization parameter λ affect the statistical power and computational efficiency of the multivariate FSD test? The paper discusses the use of entropic regularization but does not provide specific guidance on choosing λ.

### Open Question 3
Can the multivariate FSD framework be extended to other types of stochastic orders beyond first-order stochastic dominance? The paper mentions potential extensions to other stochastic orders like µ-first order dominance and the multivariate Lorenz order, but does not explore these extensions.

## Limitations

- The theoretical framework relies heavily on the smoothness condition (SCd) for bootstrap consistency, but lacks concrete examples where this condition fails
- The entropic regularization parameter λ creates an inherent bias-variance tradeoff that is not fully characterized
- Computational complexity claims assume efficient GPU implementation of Sinkhorn, but real-world performance on large-scale datasets may face memory and runtime constraints

## Confidence

- Entropic OT improves computational tractability while preserving statistical properties: **Medium**
- Multivariate violation ratio captures metric dependencies: **Medium**
- Bootstrap procedure provides valid inference: **Low-Medium**

## Next Checks

1. **Robustness to smoothness condition violations**: Construct synthetic examples where optimal potentials exhibit irregular behavior and test whether the bootstrap inference remains valid or breaks down as predicted

2. **Hyperparameter sensitivity analysis**: Systematically vary λ across multiple orders of magnitude and measure the impact on violation ratio accuracy, computational time, and bootstrap variance estimates to characterize the bias-variance tradeoff

3. **Scalability benchmark**: Evaluate the method on LLM datasets with increasing numbers of metrics and samples to empirically verify the claimed O(N^2) complexity and identify memory/runtime bottlenecks in the Sinkhorn implementation