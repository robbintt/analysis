---
ver: rpa2
title: 'Automatic Metrics in Natural Language Generation: A Survey of Current Evaluation
  Practices'
arxiv_id: '2408.09169'
source_url: https://arxiv.org/abs/2408.09169
tags:
- linguistics
- association
- computational
- metrics
- papers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper surveys automatic evaluation metrics used in natural
  language generation (NLG) research, analyzing 110 papers from INLG 2023 and ACL
  Generation track. The study examines which metrics are used, their rationale, implementation
  details, and correlation with human evaluations.
---

# Automatic Metrics in Natural Language Generation: A Survey of Current Evaluation Practices

## Quick Facts
- **arXiv ID**: 2408.09169
- **Source URL**: https://arxiv.org/abs/2408.09169
- **Reference count**: 40
- **Primary result**: 94.23% of NLG papers use automatic metrics, with BLEU and ROUGE being most popular despite known limitations

## Executive Summary
This paper surveys automatic evaluation metrics used in natural language generation (NLG) research, analyzing 110 papers from INLG 2023 and ACL Generation track. The study examines which metrics are used, their rationale, implementation details, and correlation with human evaluations. The findings reveal significant shortcomings in current evaluation practices, including the lack of rationales for metric selection, insufficient implementation details, and minimal correlation between automatic and human evaluations. The paper provides recommendations for improving evaluation practices in NLG research.

## Method Summary
The authors conducted a systematic survey of 110 papers from INLG 2023 and ACL Generation track, independently annotating each paper for metrics used, rationales for metric selection, implementation details, correlation with human evaluations, and code availability. Multiple annotators participated with an iterative annotation scheme refinement process. Inter-annotator agreement was measured using Jaccard and MASI distance metrics. The analysis focused on quantifying usage patterns, identifying gaps in evaluation practices, and drawing conclusions about current state of the field.

## Key Results
- 94.23% of papers use automatic metrics, with BLEU and ROUGE being the most popular despite known limitations
- 76.9% of metrics lack any rationale for their use
- Only 12.0% of metrics correlate their results with human judgments
- 75% of INLG and 70.2% of ACL papers provide code, but documentation quality varies

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The survey identifies a significant gap in evaluation practices by revealing that most NLG papers use automatic metrics without providing rationales for their selection.
- **Mechanism**: The authors systematically annotate 110 papers from INLG 2023 and ACL Generation track, documenting which metrics are used, why they were chosen, and how their usage is reported. By quantifying the lack of rationales (76.9% of metrics lack any rationale), they highlight a widespread issue in the field.
- **Core assumption**: The assumption is that providing rationales for metric selection is essential for transparency and reproducibility in research.
- **Evidence anchors**:
  - [abstract] "Our findings from this survey reveal significant shortcomings, including inappropriate metric usage, lack of implementation details and missing correlations with human judgements."
  - [section 4.1] "The vast majority of annotated metrics (486, 76.9%) did not include a rationale for the use of a metric."
- **Break condition**: If the majority of researchers start providing clear rationales for their metric choices, the identified gap would diminish, reducing the impact of this mechanism.

### Mechanism 2
- **Claim**: The survey demonstrates that the predominance of BLEU and ROUGE metrics, despite known limitations, indicates a lack of critical evaluation in metric selection.
- **Mechanism**: By showing that BLEU and ROUGE are the most popular metrics across both INLG and ACL papers, the authors argue that researchers are not critically assessing the appropriateness of these metrics for their specific tasks.
- **Core assumption**: The assumption is that the continued use of BLEU and ROUGE, despite their known limitations, suggests a lack of awareness or consideration of better alternatives.
- **Evidence anchors**:
  - [abstract] "The study examines which metrics are used, their rationale, implementation details, and correlation with human evaluations."
  - [section 4.1] "Interestingly, as shown in Figure 1, the usage of BLEU and ROUGE is proportionately higher in INLG compared to the ACL Generation track. BLEU is the most popular metric in both INLG and ACL, despite the multiple concerns raised by researchers on its validity as an NLG metric."
- **Break condition**: If researchers begin to adopt and validate alternative metrics that address the limitations of BLEU and ROUGE, the reliance on these metrics would decrease, weakening this mechanism.

### Mechanism 3
- **Claim**: The survey reveals that the lack of correlation between automatic and human evaluations suggests a disconnect in how researchers interpret and validate their results.
- **Mechanism**: By annotating whether papers correlate their automatic metric results with human evaluations, the authors highlight that most papers do not establish this link, treating the evaluations as separate entities.
- **Core assumption**: The assumption is that correlating automatic and human evaluations is crucial for validating the effectiveness of automatic metrics as proxies for human judgment.
- **Evidence anchors**:
  - [abstract] "Our findings from this survey reveal significant shortcomings, including inappropriate metric usage, lack of implementation details and missing correlations with human judgements."
  - [section 4.2] "Interestingly, papers from the ACL generation track and INLG are very similar in terms of correlating with human evaluations... Authors who provided either a qualitative or quantitative analysis between their automatic and human evaluation results are very much in the minority."
- **Break condition**: If researchers consistently correlate their automatic and human evaluations, providing insights into the relationship between the two, the identified disconnect would diminish, reducing the impact of this mechanism.

## Foundational Learning

- **Concept**: Automatic evaluation metrics in NLG
  - Why needed here: Understanding the role and limitations of automatic metrics is crucial for evaluating their appropriateness in research.
  - Quick check question: Can you list three limitations of BLEU and ROUGE metrics?

- **Concept**: Human evaluation methods
  - Why needed here: Recognizing the importance of human evaluations in validating automatic metrics helps in understanding the need for correlations.
  - Quick check question: Why is it important to correlate automatic metric results with human evaluations?

- **Concept**: Metric selection and rationale
  - Why needed here: Knowing how to select appropriate metrics and justify their use is essential for transparent and reproducible research.
  - Quick check question: What are the key factors to consider when selecting an automatic metric for an NLG task?

## Architecture Onboarding

- **Component map**: Paper selection -> Independent annotation -> Inter-annotator agreement calculation -> Data analysis -> Conclusions and recommendations
- **Critical path**: The critical path includes paper selection, annotation, inter-annotator agreement, data analysis, and drawing conclusions and recommendations.
- **Design tradeoffs**: The tradeoff between comprehensiveness (covering many papers and metrics) and depth (detailed annotation of each paper) affects the survey's findings and recommendations.
- **Failure signatures**: If the annotation process is not thorough or consistent, the findings may not accurately reflect the state of evaluation practices. Similarly, if the paper selection is biased, the results may not be generalizable.
- **First 3 experiments**:
  1. **Reproduce the annotation process**: Select a small subset of papers and manually annotate them for metrics used, rationales, and correlations to understand the process and identify any ambiguities.
  2. **Analyze a specific metric**: Choose a commonly used metric (e.g., BLEU) and examine its usage across papers to understand its prevalence and any associated rationales or correlations.
  3. **Compare evaluation practices**: Select papers from different years or venues and compare their evaluation practices to identify trends or changes over time.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do new factuality-focused metrics like AlignScore, NegBleurt, and WeCheck compare to traditional metrics in real-world NLG applications?
- **Basis in paper**: [explicit] The paper mentions eight newly introduced metrics focused on factual correctness, which goes against the majority of currently used metrics.
- **Why unresolved**: These metrics are newly proposed and their adoption by the research community is uncertain. Their practical performance in diverse NLG tasks needs empirical validation.
- **What evidence would resolve it**: Comparative studies evaluating these new metrics against traditional metrics across various NLG tasks, showing their correlation with human judgments and practical utility.

### Open Question 2
- **Question**: What are the long-term trends in automatic metric usage for NLG tasks beyond 2023?
- **Basis in paper**: [inferred] The paper provides a snapshot of practices in 2023 but acknowledges that quantitatively capturing long-term trends was out of scope.
- **Why unresolved**: The paper focuses on a single year's data, and the field of NLG is rapidly evolving with new metrics and evaluation practices emerging regularly.
- **What evidence would resolve it**: Longitudinal studies tracking metric usage patterns over multiple years, analyzing shifts in metric popularity and emerging trends in evaluation practices.

### Open Question 3
- **Question**: How can the research community incentivize better documentation and reproducibility practices for NLG evaluation code and resources?
- **Basis in paper**: [explicit] The paper recommends releasing code with proper documentation and installation instructions, noting current variations in practices.
- **Why unresolved**: While the paper suggests improvements, it doesn't address systemic changes needed to enforce better practices across the research community.
- **What evidence would resolve it**: Implementation of mandatory code and resource submission policies in major conferences, with clear guidelines and validation processes, leading to improved reproducibility rates.

## Limitations

- Focus on only two conferences (INLG 2023 and ACL Generation track) may limit generalizability to broader NLG research community
- Unable to verify implementation details without access to actual code, potentially underestimating the quality of implementations
- Survey provides snapshot of 2023 practices but cannot capture evolving trends or emerging evaluation methodologies

## Confidence

Our confidence in the findings is **Medium-High**. The survey methodology is robust, with independent annotation by multiple authors and clear inter-annotator agreement metrics. However, limitations include the focus on only two conferences, potential selection bias in paper sampling, and the inability to verify implementation details without access to the actual code.

**Next validation checks:**
1. Replicate the annotation process on a broader sample including papers from additional venues (EMNLP, NAACL, AAAI) to test generalizability
2. Contact authors of papers using BLEU/ROUGE without rationales to understand their selection reasoning and validate the assumption that this indicates lack of critical evaluation
3. Implement a controlled experiment comparing metric performance with and without human correlation to quantify the practical impact of the identified gaps