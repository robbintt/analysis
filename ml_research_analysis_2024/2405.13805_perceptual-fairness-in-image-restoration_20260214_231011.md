---
ver: rpa2
title: Perceptual Fairness in Image Restoration
arxiv_id: '2405.13805'
source_url: https://arxiv.org/abs/2405.13805
tags:
- group
- asian
- image
- non-asian
- fairness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new fairness metric for image restoration
  tasks called Perceptual Fairness (PF). Existing definitions like Representation
  Demographic Parity (RDP) are overly restrictive, considering reconstructions correct
  only if they fall within the group's ground truth image set.
---

# Perceptual Fairness in Image Restoration

## Quick Facts
- **arXiv ID**: 2405.13805
- **Source URL**: https://arxiv.org/abs/2405.13805
- **Reference count**: 40
- **Primary result**: Introduces Perceptual Fairness (PF) metric that detects distributional bias in image restoration that existing metrics like RDP miss

## Executive Summary
This paper addresses fairness in image restoration tasks by introducing a new metric called Perceptual Fairness (PF). Existing fairness definitions like Representation Demographic Parity (RDP) are overly restrictive, only considering reconstructions correct if they fall within the ground truth image set for that group. PF instead measures the statistical distance between the distribution of ground truth images and their reconstructions for each group, achieving fairness when these distances are equal across groups. The authors prove theoretical properties of PF, including that perfect fairness across all groups is impossible under severe degradation and that perfect PF conflicts with perfect Perceptual Index (PI) when there's a majority group. Experiments on face image super-resolution demonstrate PF's superiority over RDP in detecting fairness bias, including bias injected via adversarial attacks.

## Method Summary
The method evaluates fairness in image restoration by computing the Group Perceptual Index (GPI) for each demographic group, defined as the statistical distance between the distribution of ground truth images and their reconstructions. The authors generate synthetic fairness groups using image-to-image translation on CelebA-HQ test images, applying degradation (average pooling down-sampling + additive white Gaussian noise + JPEG compression), and running state-of-the-art face image super-resolution algorithms on the degraded images. GPI is computed using statistical distance metrics (KID or FID) with features extracted from a FairFace classifier, and compared across groups to assess fairness.

## Key Results
- PF detects distributional bias that RDP misses by measuring statistical distance between ground truth and reconstruction distributions within each group
- Perfect GPI for all groups simultaneously is impossible under severe degradation due to distributional overlap constraints
- Perfect Perceptual Index (PI) conflicts with perfect Perceptual Fairness (PF) when there's a majority group
- Experiments on face super-resolution show PF's superiority over RDP in detecting fairness bias, including bias injected via adversarial attacks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Perceptual Fairness (PF) detects distributional bias that Representation Demographic Parity (RDP) misses by measuring statistical distance between ground truth and reconstruction distributions within each group.
- **Mechanism**: Instead of binary correctness (reconstruction in ground truth set or not), PF uses divergence measures (e.g., Wasserstein, Total Variation) to capture how well the entire distribution of reconstructions matches the ground truth distribution for each group. PF is achieved when these divergences are equal across groups.
- **Core assumption**: The statistical distance between distributions captures perceptual similarity in a way humans can distinguish, and that distribution mismatch reflects unfairness.
- **Evidence anchors**:
  - [abstract] "We define the Group Perceptual Index (GPI) to be the statistical distance between the distribution of the group's ground truth images and the distribution of their reconstructions."
  - [section 2.2.3] "We define the fairness of an image restoration algorithm as its ability to equally preserve the distribution pX|A(·|a) across all possible values of a."
  - [corpus] Weak - related papers focus on image restoration quality but don't directly address fairness metrics.

### Mechanism 2
- **Claim**: Perfect GPI for all groups simultaneously is impossible under severe degradation due to distributional overlap constraints.
- **Mechanism**: When degraded measurements of different groups overlap more than their ground truth images (condition in Theorem 2), at least one group must have non-zero GPI because perfect reconstruction for all groups would violate information constraints.
- **Core assumption**: The degradation process creates a fundamental information bottleneck that prevents perfect reconstruction for overlapping groups.
- **Evidence anchors**:
  - [section 3, Theorem 2] "Suppose that ∃a1, a2 ∈ supp pA such that P(X ∈ X a1 ∩ Xa2 |A = ai) < P(Y ∈ Y a1 ∩ Ya2 |A = ai), for both i = 1, 2... Then, GPId(a1) and GPId(a2) cannot both be equal to zero."
  - [section 2.2.2] "Our formulation is quite general, as it does not make any assumptions regarding the nature of the image distributions, whether they have overlapping supports or not, etc."
  - [corpus] Weak - no direct evidence about distributional overlap impossibility in restoration.

### Mechanism 3
- **Claim**: Perfect Perceptual Index (PI) conflicts with perfect Perceptual Fairness (PF) when there's a majority group.
- **Mechanism**: For algorithms with perfect PI (p̂X = pX), improving GPI for a minority group necessarily worsens GPI for the majority group (Theorem 4 relationship). Thus, perfect PF (equal GPIs) cannot coexist with perfect PI.
- **Core assumption**: The mathematical relationship between group divergences for perfect PI algorithms creates a zero-sum tradeoff.
- **Evidence anchors**:
  - [section 3, Theorem 3] "Suppose that A takes discrete values, ˆX attains perfect PId (p̂X = pX), and ∃a, am ∈ supp pA such that GPId(a) > 0 and P(A = am) > 0.5. Then, ˆX cannot achieve perfect PFdTV."
  - [section 3, Theorem 4] "GPIdTV(a) ≤ 1/P(A = a) Σa′≠a P(A = a′)GPIdTV(a′)" showing the inverse relationship.
  - [corpus] Weak - related papers don't discuss tradeoffs between PI and fairness.

## Foundational Learning

- **Concept**: Statistical distance/divergence measures (Wasserstein, Total Variation, Kullback-Leibler)
  - Why needed here: PF fundamentally relies on quantifying distributional differences between ground truth and reconstructions for fairness assessment.
  - Quick check question: Can you compute and interpret the Wasserstein distance between two empirical distributions?

- **Concept**: Markov chains and conditional independence in Bayesian inverse problems
  - Why needed here: The formulation assumes A → Y → ˆX forms a Markov chain, which is critical for the theoretical results and for understanding the group-based analysis.
  - Quick check question: What does it mean for X and ˆX to be statistically independent given Y in the context of image restoration?

- **Concept**: Group fairness definitions and their limitations (RDP, PR, CPR)
  - Why needed here: Understanding why existing definitions are "highly restrictive" motivates the need for PF and helps evaluate its advantages.
  - Quick check question: How does RDP differ from PF in terms of what constitutes a "correct" reconstruction?

## Architecture Onboarding

- **Component map**: Degraded image → Restoration algorithm → Group classification → Distribution estimation → Divergence calculation → Fairness evaluation
- **Critical path**: Degraded image → Restoration algorithm → Group classification → Distribution estimation → Divergence calculation → Fairness evaluation
- **Design tradeoffs**:
  - Choice of divergence measure: Wasserstein captures perceptual differences better but is computationally heavier than TV
  - Group granularity: Finer partitions (e.g., age+ethnicity) vs. coarser ones (ethnicity alone) affect RDP sensitivity
  - Feature extractor selection: General-purpose features vs. attribute-specific features for GPI computation
- **Failure signatures**:
  - GPIs are similar but groups have different semantic content (false positive for PF)
  - GPIs differ but human judges don't perceive the difference (false negative for PF)
  - High computational cost making GPI evaluation impractical for large datasets
- **First 3 experiments**:
  1. Implement PF evaluation on a simple restoration task (e.g., Gaussian denoising) with synthetic groups having known distributional differences
  2. Compare PF vs RDP detection on a face super-resolution dataset with injected bias
  3. Test Theorem 4 relationship by training a perfect PI algorithm and measuring GPI tradeoffs across groups

## Open Questions the Paper Calls Out

- **Question**: How severe is the tradeoff between perceptual fairness and overall perceptual quality for different degradation levels?
  - **Basis in paper**: [explicit] The paper proves that perfect PF often conflicts with perfect PI (Perceptual Index), but the practical severity of this tradeoff across different degradation scenarios is not explored.
  - **Why unresolved**: The paper focuses on theoretical properties of PF and does not experimentally investigate the tradeoff between PF and PI for different degradation levels (scaling factors, noise levels, etc.).
  - **What evidence would resolve it**: Experiments measuring both PF and PI for a range of degradation levels, showing how the tradeoff changes as degradation severity varies.

- **Question**: What are the characteristics of optimal estimators that achieve good perceptual fairness?
  - **Basis in paper**: [inferred] The paper discusses that common estimators (MMSE, posterior sampling) do not trivially achieve good PF, but does not characterize optimal estimators for PF.
  - **Why unresolved**: The paper focuses on defining PF and proving theoretical properties, but does not explore what types of estimators (deterministic vs. stochastic, etc.) or training objectives lead to good PF.
  - **What evidence would resolve it**: Analysis of different estimator types (deterministic, stochastic, hybrid) and their PF performance, potentially leading to new estimator designs optimized for PF.

- **Question**: How does the choice of statistical distance (e.g., Wasserstein, TV, KL) affect the detection of perceptual fairness bias?
  - **Basis in paper**: [explicit] The paper defines PF using a general statistical distance but uses Wasserstein and TV in experiments. It does not compare different distance metrics.
  - **Why unresolved**: The paper uses specific distance metrics (Wasserstein, TV) but does not investigate how the choice of distance affects bias detection or the practical utility of PF.
  - **What evidence would resolve it**: Experiments comparing bias detection using different statistical distances (Wasserstein, TV, KL, etc.) on the same datasets and algorithms.

## Limitations

- The theoretical impossibility results rely on idealized assumptions about group distributions and degradation processes that may not hold in practical scenarios with overlapping or correlated attributes.
- GPI computation depends heavily on the choice of divergence measure and feature extractor, with FairFace classifier features showing stronger bias detection than general-purpose extractors in ablation studies.
- The synthetic fairness groups created via image-to-image translation may not fully capture real-world demographic variations, potentially limiting generalizability.

## Confidence

- **High Confidence**: The mechanism by which PF detects distributional bias (Mechanism 1) is well-founded theoretically and empirically validated through experiments showing RDP's limitations.
- **Medium Confidence**: The impossibility theorems (Mechanism 2) are mathematically rigorous but depend on strong assumptions about group distributions that may not always apply in practice.
- **Medium Confidence**: The tradeoff between perfect PI and perfect PF (Mechanism 3) is theoretically proven but requires perfect PI algorithms which may not be achievable in practice.

## Next Checks

1. **Validate distributional assumptions**: Test Theorem 2 under relaxed conditions where group distributions have varying degrees of overlap to assess the robustness of the impossibility results.

2. **Cross-feature validation**: Compare PF results using FairFace classifier features versus general-purpose feature extractors (dinov2-vit-g-14, clip-vit-l-14) on real-world demographic datasets to verify consistency.

3. **Real-world bias detection**: Apply PF evaluation to actual demographic groups in CelebA-HQ (as opposed to synthetic groups) to test whether PF detects bias patterns that RDP misses in realistic scenarios.