---
ver: rpa2
title: 'Measuring Bargaining Abilities of LLMs: A Benchmark and A Buyer-Enhancement
  Method'
arxiv_id: '2402.15813'
source_url: https://arxiv.org/abs/2402.15813
tags:
- buyer
- bargaining
- seller
- price
- deal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a benchmark for evaluating LLM bargaining
  abilities in a formalized asymmetric incomplete information game, distinguishing
  buyer and seller roles with defined profit metrics. The authors construct a real-world
  dataset of 930 Amazon products with historical prices, and find that buyers struggle
  more than sellers, with no clear performance gain from larger models.
---

# Measuring Bargaining Abilities of LLMs: A Benchmark and A Buyer-Enhancement Method

## Quick Facts
- **arXiv ID**: 2402.15813
- **Source URL**: https://arxiv.org/abs/2402.15813
- **Reference count**: 29
- **Primary result**: OG-Narrator method improves buyer deal rates from 26.67% to 88.88% and multiplies profits ten times across all tested models

## Executive Summary
This paper introduces a benchmark for evaluating LLM bargaining abilities in a formalized asymmetric incomplete information game, distinguishing buyer and seller roles with defined profit metrics. The authors construct a real-world dataset of 930 Amazon products with historical prices, and find that buyers struggle more than sellers, with no clear performance gain from larger models. They propose OG-Narrator, a method that decouples deterministic offer pricing from LLM-generated dialogue, which significantly improves buyer success rates and profits across all tested models, even those not fine-tuned for chat.

## Method Summary
The method formalizes bargaining as an asymmetric incomplete information game where buyers have private budgets and sellers have private costs. The OG-Narrator approach separates price generation (using linear interpolation from 50% to 100% of budget) from dialogue generation, allowing LLMs to focus on natural language negotiation rather than numerical reasoning. The benchmark uses a dataset of 930 Amazon products with historical prices and evaluates performance through sum of profits and normalized profit metrics for both buyers and sellers.

## Key Results
- Buyers struggle significantly more than sellers, with many models achieving negative profits
- Larger model sizes benefit sellers more than buyers, correlating with better instruction-following
- OG-Narrator improves buyer deal rates from 26.67% to 88.88% and multiplies profits ten times across all tested models
- The method works even on models not fine-tuned for chat (e.g., phi-2)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OG-Narrator improves buyer performance by separating deterministic price offers from LLM-generated dialogue, allowing the LLM to focus on negotiation context without needing to reason about optimal prices.
- Mechanism: The Offer Generator (OG) uses a linear interpolation strategy to create a series of escalating price offers (starting at 50% of budget and increasing to 100% over the negotiation rounds). This ensures the buyer always starts with a low offer and incrementally moves up, while the LLM Narrator focuses on crafting natural language responses based on the offer and conversation history.
- Core assumption: LLMs struggle with numerical reasoning for optimal price generation but excel at generating contextually appropriate dialogue when given structured input.
- Evidence anchors: Experimental results show OG-Narrator improves buyer deal rates from 26.67% to 88.88% and brings ten times multiplication of profits on all baselines, even on models not aligned for chat.

### Mechanism 2
- Claim: The bargaining task formalization as an asymmetric incomplete information game with profit metrics provides a rigorous framework for evaluating LLM bargaining performance.
- Mechanism: By defining the bargaining task with explicit profit functions (Pb = B - D for buyer, Ps = D - C for seller) and normalized profits for comparison across MI and CI scenarios, the benchmark can quantitatively assess whether agents are making rational economic decisions rather than just reaching deals.
- Core assumption: Rational agents should maximize their expected profit and avoid negative profits, which can be measured through the defined profit metrics.
- Evidence anchors: The paper formally describes the Bargaining task as an asymmetric incomplete information game, defining the gains of the Buyer and Seller in multiple bargaining processes.

### Mechanism 3
- Claim: Larger model size benefits sellers more than buyers in bargaining tasks because sellers need better instruction-following and valid rate maintenance.
- Mechanism: Experimental results show that among models of the same series, larger models consistently achieve higher valid rates and deal rates as sellers, while buyer performance shows no clear correlation with model size.
- Core assumption: Seller performance requires more robust instruction-following capabilities and better handling of complex dialogue instructions, which correlates with model capacity.
- Evidence anchors: In contrast to the Buyer, the Seller's SNP is highly related to the valid rate. A high valid rate requires good instruction following capabilities.

## Foundational Learning

- Concept: Asymmetric incomplete information games
  - Why needed here: The bargaining task involves private information (buyer's budget, seller's cost) that each party must estimate from the other's actions and dialogue.
  - Quick check question: In a bargaining scenario where the buyer knows their budget is $100 and the seller knows their cost is $60, what type of information asymmetry exists?

- Concept: Profit maximization vs. deal completion
  - Why needed here: The benchmark evaluates agents not just on reaching deals but on achieving profitable deals, which requires understanding the difference between these objectives.
  - Quick check question: If a buyer's budget is $100 and a seller's cost is $80, what is the maximum profit the buyer can achieve in a successful negotiation?

- Concept: Normalized profit metrics
  - Why needed here: The benchmark uses normalized profits to compare performance across different price ranges and MI/CI scenarios, requiring understanding of how normalization works.
  - Quick check question: How does the normalized profit formula change the interpretation of performance when comparing a $10 product negotiation to a $1000 product negotiation?

## Architecture Onboarding

- Component map: Offer Generator (OG) → LLM Narrator → Action output (for OG-Narrator); LLM → Action output (for baseline)
- Critical path: Session initialization → Negotiation rounds → Profit calculation → Metric aggregation
- Design tradeoffs: Separating price generation from dialogue allows better control but adds complexity; using linear interpolation is simple but may not be optimal for all negotiation scenarios; real product data provides authenticity but introduces variability
- Failure signatures: Low valid rates indicate instruction-following issues; negative profits across all models suggest fundamental misunderstanding of the task; inconsistent performance across MI/CI scenarios indicates poor handling of different negotiation contexts
- First 3 experiments:
  1. Run OG-Narrator with phi-2 (unaligned model) to verify the dramatic improvement effect
  2. Test OG-Narrator with a medium-sized model (e.g., Llama-2-13b-chat) to confirm consistent improvement across model sizes
  3. Run the baseline benchmark with ChatGPT vs. itself to establish the difficulty of the buyer role and verify the negative profit finding

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the OG-Narrator method's success generalize to other negotiation domains beyond simple price bargaining, such as multi-issue negotiations or negotiations involving non-monetary factors?
- Basis in paper: The paper focuses on a specific bargaining scenario with fixed product prices and simple buy/sell actions. While the OG-Narrator shows significant improvement in this context, the paper does not explore its applicability to more complex negotiation scenarios.
- Why unresolved: The authors only tested OG-Narrator on their AmazonHistoryPrice dataset and did not experiment with other negotiation domains or more complex bargaining scenarios. The method's reliance on a deterministic offer generator might not translate well to negotiations with multiple interdependent issues or qualitative factors.
- What evidence would resolve it: Testing OG-Narrator on diverse negotiation datasets involving multi-issue bargaining, preference-based negotiations, or negotiations with non-price factors (e.g., delivery time, product features) would demonstrate its generalizability. Comparing its performance to other negotiation strategies in these contexts would provide further insights.

### Open Question 2
- Question: How does the OG-Narrator method's performance scale with the complexity of the negotiation environment, such as an increase in the number of products, product categories, or the introduction of dynamic pricing models?
- Basis in paper: The paper uses a fixed dataset of 930 products across 18 categories. While this provides a reasonable test bed, it does not explore how OG-Narrator handles significantly larger or more dynamic environments. The current implementation uses a simple linear interpolation for offer generation, which might not be optimal for complex environments.
- Why unresolved: The authors did not test OG-Narrator on datasets with varying sizes, different product distributions, or dynamic pricing models. The scalability and adaptability of the offer generation strategy to more complex environments remain unexplored.
- What evidence would resolve it: Conducting experiments with increasingly large datasets, diverse product categories, and dynamic pricing models would reveal how OG-Narrator's performance scales. Analyzing the impact of different offer generation strategies (e.g., non-linear interpolation, machine learning-based approaches) on performance in complex environments would provide valuable insights.

### Open Question 3
- Question: What is the impact of incorporating additional contextual information, such as product reviews, seller reputation, or market trends, on the bargaining performance of LLM agents?
- Basis in paper: The paper acknowledges that their dataset includes product descriptions, features, and images, but the experiments primarily focus on using product titles, descriptions, and list prices. The potential impact of additional contextual information on bargaining performance is not explored.
- Why unresolved: The authors did not experiment with incorporating additional contextual information into the bargaining process. While the dataset contains such information, its potential to enhance LLM agents' understanding of product value and negotiation strategies remains untested.
- What evidence would resolve it: Designing experiments that incorporate product reviews, seller reputation scores, or market trend data into the bargaining process would reveal the impact of contextual information. Comparing the performance of LLM agents using different combinations of contextual information would provide insights into which factors are most influential in improving bargaining outcomes.

## Limitations

- The paper's claims about LLM numerical reasoning limitations lack direct empirical validation from the bargaining literature
- The observed correlation between model size and seller performance may be confounded by architectural differences rather than pure scaling effects
- The controlled bargaining framework may not fully capture the complexity of real-world negotiations with multiple offers, market research, and concurrent negotiations

## Confidence

- **High Confidence**: The formalization of bargaining as an asymmetric incomplete information game with defined profit metrics is well-established in game theory and the experimental methodology for measuring deal rates and profits is sound.
- **Medium Confidence**: The OG-Narrator method's effectiveness is supported by strong experimental results, but the underlying claim about LLM numerical reasoning limitations requires additional validation.
- **Low Confidence**: The assertion that larger model sizes specifically benefit sellers more than buyers due to instruction-following capabilities, as this effect is observed but not thoroughly explained or tested across diverse model architectures.

## Next Checks

1. **Cross-Series Model Comparison**: Test OG-Narrator with models from different architectural families (e.g., transformer vs. other architectures) while controlling for parameter count to isolate whether size or architecture drives the observed performance differences.

2. **Numerical Reasoning Baseline**: Compare OG-Narrator against a variant where the LLM generates prices but receives explicit numerical calculation prompts to determine if the improvement comes from separation of concerns or simply from better prompting strategies.

3. **Market Complexity Extension**: Extend the benchmark to include multiple negotiation rounds with multiple sellers, time pressure, and imperfect information revelation to test whether OG-Narrator's benefits persist in more complex bargaining environments.