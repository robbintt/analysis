---
ver: rpa2
title: Which questions should I answer? Salience Prediction of Inquisitive Questions
arxiv_id: '2404.10917'
source_url: https://arxiv.org/abs/2404.10917
tags:
- questions
- question
- salience
- article
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces QSALIENCE, a salience predictor for inquisitive
  questions, which are open-ended questions generated while reading. The authors collected
  and annotated 1,766 question-context pairs with salience scores indicating how important
  answering each question is for understanding the text.
---

# Which questions should I answer? Salience Prediction of Inquisitive Questions

## Quick Facts
- arXiv ID: 2404.10917
- Source URL: https://arxiv.org/abs/2404.10917
- Reference count: 40
- Primary result: QSALIENCE, a fine-tuned model for predicting question salience, outperforms GPT-4 on this task

## Executive Summary
This paper introduces QSALIENCE, a salience predictor for inquisitive questions - open-ended questions generated while reading. The authors collected and annotated 1,766 question-context pairs with salience scores (1-5) indicating how important answering each question is for understanding the text. They found that highly salient questions are more likely to be answered in the article, connecting potential questions to Questions Under Discussion (QUDs). QSALIENCE, instruction-tuned from open-source LLMs, outperforms GPT-4 in predicting question salience even with much smaller models like Flan-T5. The model achieves moderate agreement with human annotations and shows practical utility in improving summary quality.

## Method Summary
The authors collected a dataset of 1,766 question-context pairs annotated with salience scores (1-5). They fine-tuned open-source LLMs including Mistral-7B-instruct, Llama-2-7B-chat, Flan-T5, and TinyLlama using instruction-tuning with QLoRA. The models were trained to predict salience ratings given an article context. They evaluated their approach against zero/few-shot GPT-4 baselines and tested the utility of predicted salience scores in a downstream summary expansion task.

## Key Results
- Fine-tuned models outperform GPT-4 on salience prediction across zero/few-shot, and Chain-of-Thought prompting
- Highly salient questions are significantly more likely to be answered in the article (Spearman's ρ = 0.65)
- Summaries answering more salient questions receive higher human rankings
- QSALIENCE achieves moderate agreement with human annotations (Krippendorff's α = 0.44)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuned models outperform zero/few-shot LLMs because salience prediction is a discourse task that recovers implicit information not readily grasped by LLMs.
- Mechanism: The task of predicting question salience requires understanding how answering a question enhances comprehension of the text. This involves implicit processing of discourse structure and reader expectations that are not explicitly captured in LLM training data.
- Core assumption: Question salience is a predictable property that can be learned from data, and it involves implicit information that isn't directly stated in the text.
- Evidence anchors:
  - [abstract]: "QSALIENCE is instruction-tuned from open-sourced LLMs that predict salience ratings given an article context. QSALIENCE is the first application-agnostic question salience model, and outperforms GPT-4 under zero-shot, few-shot, and Chain-of-Thought prompting"
  - [section 6.2]: "Table 6 shows that the fine-tuned models clearly outperform zero- or few-shot LLMs, even with stronger prompting techniques such as kNN-based in-context example retrieval and Chain-of-Thought"

### Mechanism 2
- Claim: Salient questions are more likely to be answered in the article because reader expectations align with QUDs (Questions Under Discussion).
- Mechanism: The authors found that questions with high salience scores correlate with answerability, suggesting that readers expect important questions to be answered later in the text. This creates a feedback loop where writers answer salient questions to meet reader expectations.
- Core assumption: There is a "common" notion of question salience that captures reader expectations and connects to answer utility, and that writer and reader expectations align to some degree.
- Evidence anchors:
  - [section 5]: "Table 4 presents the Spearman rank correlation coefficients between annotated salience and answerability. As comparison, we also compute a random correlation baseline between salience and the answerability of a random question, averaged across 10 trials. The annotated salience and answerability values have a significant Spearman's ρ of 0.65"
  - [section 5]: "Table 5 presents the salience distribution of 36 DCQA questions that are annotated QUDs. Similar to the previous analysis, we also take a random subset of 36 potential questions, averaging their scores over 10 trials and present their salience distribution. We see that QUDs, which are potential questions answered in later context, are overall much more salient compared to a random set of potential questions sampled from QSALIENCE -data"

### Mechanism 3
- Claim: Summaries that answer more salient questions are ranked higher by humans because answering important questions enhances understanding.
- Mechanism: The authors hypothesize that reader expectation aligns with QUDs in expanded summaries. A higher-quality summary answers more salient questions, providing a fuller understanding of the article and thus receiving higher human rankings.
- Core assumption: Human rankings of summary quality are based on how well the summary answers important questions and enhances understanding.
- Evidence anchors:
  - [section 7]: "The ranking results show an expected order of quality: GPT-4 (2.91 average ranking), Flan-T5 (1.73), GPT-4-Corrupted (1.35). This is the oracle ordering that we aim to reproduce by utilizing the salience of QUDs in the expanded summary."
  - [section 7]: "Table 7 shows the SummScr on the test set portion of DiverseSumm. All SummScr values derived from predicted salience values, using fine-tuned systems, reproduce the same system-level ranking as humans: GPT-4, Flan-T5, then GPT-4-Corrupted"

## Foundational Learning

- Concept: Question salience as a measure of importance for understanding
  - Why needed here: The entire paper is built around the concept of question salience and its prediction. Understanding what question salience means and how it's defined is crucial for understanding the rest of the work.
  - Quick check question: According to the paper, how is question salience defined and what scale is used to measure it?

- Concept: Potential questions and QUDs (Questions Under Discussion)
  - Why needed here: The paper connects question salience to the linguistic concepts of potential questions and QUDs. Understanding these concepts is important for understanding the theoretical background and the analysis in section 5.
  - Quick check question: What is the difference between a potential question and a QUD, and how does the paper connect these concepts to question salience?

- Concept: Fine-tuning and instruction-tuning of LLMs
  - Why needed here: The paper presents QSALIENCE, an instruction-tuned model for question salience prediction. Understanding the basics of fine-tuning and instruction-tuning is important for understanding how QSALIENCE works and why it outperforms zero/few-shot approaches.
  - Quick check question: What is the difference between zero-shot, few-shot, and instruction-tuned approaches to using LLMs, and why does instruction-tuning perform better in this case?

## Architecture Onboarding

- Component map: Data collection (context, questions) -> Annotation (salience scores) -> Fine-tuning (instruction-tuning) -> Salience prediction -> Downstream task (summary expansion) -> Evaluation

- Critical path: 1. Collect and annotate data (QSALIENCE -data) 2. Train fine-tuned models for salience prediction 3. Evaluate salience prediction models 4. Apply salience prediction to downstream task (summary expansion) 5. Evaluate downstream task performance

- Design tradeoffs:
  - Using LLM-generated questions vs. human-generated questions for training data
  - Fine-tuning smaller models vs. using larger models with prompting
  - Balancing label distribution in training data vs. maintaining natural distribution
  - Using instructions vs. truncating context for models with small context windows

- Failure signatures:
  - Poor performance on salience prediction (low correlation with human annotations)
  - Inability to distinguish between different levels of salience
  - Failure to generalize to new domains or types of questions
  - Poor performance on downstream task (summary expansion)
  - Human rankings not aligning with salience-based scores

- First 3 experiments:
  1. Evaluate baseline LLM performance (zero/few-shot) on salience prediction task to establish performance floor
  2. Fine-tune smaller models (e.g., Flan-T5, TinyLlama) and compare performance to baselines to test if fine-tuning helps
  3. Analyze confusion matrices to understand where models are making mistakes and which salience levels are most challenging to predict

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on LLM-generated questions rather than human-generated ones for the core dataset may affect generalization
- Moderate agreement with human annotations (Krippendorff's α = 0.44) indicates substantial disagreement remains
- Results may not generalize to domains outside news articles, scientific papers, and stories

## Confidence
- High confidence in the core finding that fine-tuned models outperform zero/few-shot LLMs for question salience prediction
- Medium confidence in the downstream application to summary expansion
- Low confidence in the generalizability of the results to domains outside the dataset's focus

## Next Checks
1. Test QSALIENCE on a held-out set of human-generated questions to assess generalization beyond LLM-generated data.
2. Conduct ablation studies removing the answerability and QUD analysis to determine whether these components are essential to the model's performance or if they simply serve as useful training signals.
3. Evaluate the model's performance on questions from different domains (e.g., technical documentation, legal texts) to assess domain transferability and identify potential limitations in the approach.