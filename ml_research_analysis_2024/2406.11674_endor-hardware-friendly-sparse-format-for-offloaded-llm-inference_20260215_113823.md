---
ver: rpa2
title: 'Endor: Hardware-Friendly Sparse Format for Offloaded LLM Inference'
arxiv_id: '2406.11674'
source_url: https://arxiv.org/abs/2406.11674
tags: []
core_contribution: This paper proposes Endor, a sparse format to compress unstructured
  sparse LLM weights for offloaded inference. Endor expresses the positions of non-zero
  elements with a bitmap, achieving high compression ratio and low decompression overhead.
---

# Endor: Hardware-Friendly Sparse Format for Offloaded LLM Inference

## Quick Facts
- arXiv ID: 2406.11674
- Source URL: https://arxiv.org/abs/2406.11674
- Reference count: 38
- Primary result: Endor achieves 1.70× and 1.78× speedup on OPT-66B and Llama2-70B respectively compared to dense offloaded inference

## Executive Summary
Endor introduces a novel sparse format for compressing pruned LLM weights to accelerate offloaded inference. The format uses bitmaps to efficiently represent sparse matrices, achieving high compression ratios while maintaining low decompression overhead on GPUs. By reducing the weight transfer latency from storage to GPU memory—identified as the critical bottleneck in offloaded inference—Endor significantly improves inference speed without requiring changes to the model architecture itself.

## Method Summary
Endor is a sparse format that compresses unstructured sparse LLM weights by storing only non-zero elements and using a bitmap to indicate their positions. This approach is designed specifically for offloaded inference scenarios where weight transfer from storage to GPU memory is the dominant bottleneck. The format is decompressed on the GPU during inference, allowing for parallel processing of non-zero elements. Endor is orthogonal to other optimization methods like quantization and activation sparsity, making it compatible with existing acceleration techniques.

## Key Results
- Achieves 1.70× speedup on OPT-66B compared to dense offloaded inference
- Achieves 1.78× speedup on Llama2-70B compared to dense offloaded inference
- When leveraging direct SSD-to-GPU transfer, achieves 2.25× and 2.37× speedup on OPT-66B and Llama2-70B respectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Storing only non-zero values and using a bitmap for positions achieves high compression ratio and low decompression overhead.
- Mechanism: The bitmap directly maps to weight positions, allowing parallel decompression by GPU threads that write non-zero values into their correct locations.
- Core assumption: The unstructured sparsity pattern can be expressed efficiently with a bitmap without incurring excessive indexing overhead.
- Evidence anchors:
  - [abstract] "Endor achieves this by expressing the positions of non-zero elements with a bitmap."
  - [section] "We save only the non-zero elements and express the location of non-zero elements as a binary array, or a bitmap."
- Break condition: If sparsity is too low (e.g., <10%), the bitmap overhead outweighs compression benefits.

### Mechanism 2
- Claim: Offloading bottleneck is weight transfer latency, not computation.
- Mechanism: By compressing weights before transfer, the reduced data volume directly shortens transfer time, which dominates end-to-end latency.
- Core assumption: Weight transfer time is proportional to data size and much larger than computation time.
- Evidence anchors:
  - [abstract] "due to the low bandwidth between storage devices and GPU, the latency of transferring large model weights... becomes the critical bottleneck with actual compute taking nearly 0% of runtime."
  - [section] "the latency of transferring large model weights from its offloaded location to GPU memory becomes the critical bottleneck with actual compute taking nearly 0% of runtime."
- Break condition: If storage bandwidth improves significantly or if compute becomes non-negligible.

### Mechanism 3
- Claim: Endor sparse format is compatible with quantization and activation sparsity.
- Mechanism: Since Endor preserves non-zero values and their positions, it can be applied after pruning regardless of bit-width or in conjunction with selective weight loading.
- Core assumption: Preserving exact non-zero values and their positions does not interfere with other optimizations.
- Evidence anchors:
  - [abstract] "Endor sparse format for pruned LLM is orthogonal to other optimization methods of offloaded inference, mainly quantization and activation sparsity."
  - [section] "Endor reduces the weight transfer latency of pruned LLM regardless of its bit-width."
- Break condition: If quantization or pruning method alters sparsity pattern incompatibly with bitmap indexing.

## Foundational Learning

- Concept: Sparse matrix representations (CSR, bitmap)
  - Why needed here: Understanding trade-offs between different sparse formats is key to seeing why bitmap works for Endor.
  - Quick check question: What is the main disadvantage of CSR format for unstructured sparsity?

- Concept: Model offloading and memory hierarchy
  - Why needed here: The effectiveness of Endor depends on the relative costs of weight transfer across SSD, CPU memory, and GPU memory.
  - Quick check question: In the described setup, which memory-to-memory transfer is the slowest?

- Concept: GPU parallel decompression
  - Why needed here: Endor's decompression must be fast enough to offset transfer gains; understanding GPU parallelism is essential.
  - Quick check question: Why is GPU decompression preferred over CPU in this context?

## Architecture Onboarding

- Component map: Model weights (pruned) -> Endor compression module (pre-inference) -> Storage (SSD) -> Host CPU memory -> GPU memory -> Endor decompression kernel (GPU runtime)

- Critical path: SSD → CPU memory → GPU memory (decompression) → computation
  The critical bottleneck is SSD→CPU transfer; Endor shortens this by compressing weights beforehand.

- Design tradeoffs:
  - Higher sparsity ratio → better compression but potentially less accurate model
  - GPU decompression overhead vs. transfer time savings
  - Compatibility with quantization (compression ratio drops with lower bit-width)

- Failure signatures:
  - Decompression overhead > transfer time savings
  - Insufficient sparsity → bitmap overhead dominates
  - Incompatible sparsity pattern with GPU thread mapping

- First 3 experiments:
  1. Measure SSD→CPU transfer time for dense vs. Endor-compressed weights at 50% sparsity.
  2. Benchmark GPU decompression kernel runtime for varying matrix sizes.
  3. Evaluate accuracy impact of pruning ratio vs. speedup gain.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Endor's sparse format compression perform with different sparsity ratios (e.g., 30%, 70%) compared to the 50% sparsity evaluated in the paper?
- Basis in paper: [inferred] The paper mentions that Endor can support any sparsity ratio, but only evaluates 50% sparsity.
- Why unresolved: The paper does not provide performance data for sparsity ratios other than 50%, which could impact compression ratios and speedup.
- What evidence would resolve it: Experimental results showing speedup and compression ratio for various sparsity ratios (e.g., 30%, 50%, 70%) applied to Endor.

### Open Question 2
- Question: How does Endor's performance compare to other compression formats like Gzip or LZMA in terms of decompression overhead and compression ratio?
- Basis in paper: [inferred] The paper only compares Endor to ZSTD compression format, leaving other common compression formats unevaluated.
- Why unresolved: Without comparison to other compression formats, it's unclear if Endor is the most efficient option for sparse LLM weights.
- What evidence would resolve it: A comprehensive comparison of Endor with other compression formats like Gzip and LZMA, including metrics for decompression time and compression ratio.

### Open Question 3
- Question: How does Endor's performance scale with larger model sizes (e.g., GPT-3 175B) compared to the evaluated OPT-66B and Llama2-70B?
- Basis in paper: [inferred] The paper only evaluates Endor on OPT-66B and Llama2-70B, but mentions that LLMs can be hundreds of gigabytes in size.
- Why unresolved: The paper does not provide performance data for larger models, which could impact the effectiveness of Endor's compression and speedup.
- What evidence would resolve it: Experimental results showing Endor's performance (speedup, compression ratio) on larger LLM models like GPT-3 175B or BLOOM-176B.

## Limitations
- Sparse Format Overhead: The bitmap indexing introduces non-trivial memory overhead, particularly at lower sparsity levels (<30%).
- Hardware Dependencies: Speedup claims depend on specific hardware configurations, particularly direct SSD-to-GPU transfer support.
- Compression-Decompression Balance: The actual GPU kernel implementation details are sparse, making it difficult to assess the true overhead of decompression.

## Confidence
**High Confidence**: The fundamental claim that weight transfer latency dominates offloaded inference performance is well-established and supported by multiple citations. The observation that Endor is orthogonal to quantization methods is also well-founded.

**Medium Confidence**: The specific speedup numbers (1.70×-1.78× for OPT-66B/Llama2-70B) are plausible but depend on exact hardware configurations and pruning implementations that aren't fully specified. The orthogonal nature to other optimizations is theoretically sound but needs empirical validation.

**Low Confidence**: The claim about GPU decompression being faster than CPU lacks sufficient justification. The paper doesn't adequately explain why GPU decompression overhead is negligible compared to transfer time savings.

## Next Checks
1. **Hardware Portability Test**: Validate Endor's performance across different GPU architectures (NVIDIA vs AMD) and configurations to assess hardware dependency claims.

2. **Sparsity Break-even Analysis**: Systematically measure the compression ratio and decompression overhead across sparsity levels from 10% to 90% to identify the optimal operating range.

3. **Quantization Compatibility Benchmark**: Test Endor's performance when combined with various quantization schemes (FP16, INT8, NF4) to verify the claimed orthogonality and identify any hidden incompatibilities.