---
ver: rpa2
title: Is Reference Necessary in the Evaluation of NLG Systems? When and Where?
arxiv_id: '2403.14275'
source_url: https://arxiv.org/abs/2403.14275
tags:
- metrics
- evaluation
- reference-free
- correlation
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates when and where reference-free metrics can
  replace reference-based ones for evaluating NLG systems. It conducts a comprehensive
  comparison of reference-free and reference-based metrics across summarization, data-to-text,
  and dialogue tasks using 8 datasets and 3 evaluation criteria.
---

# Is Reference Necessary in the Evaluation of NLG Systems? When and Where?

## Quick Facts
- **arXiv ID**: 2403.14275
- **Source URL**: https://arxiv.org/abs/2403.14275
- **Reference count**: 28
- **Primary result**: Reference-free metrics show higher correlation with human judgments than reference-based metrics across summarization, data-to-text, and dialogue tasks

## Executive Summary
This paper investigates whether reference-free metrics can replace reference-based ones for evaluating natural language generation systems. The authors conduct a comprehensive comparison across three NLG tasks (summarization, data-to-text, and dialogue) using 8 datasets and three evaluation criteria. They find that reference-free metrics generally show higher correlation with human judgments and better sensitivity to language quality defects compared to reference-based metrics. However, their effectiveness varies by task and depends on the quality of candidate text.

The study reveals that reference-free metrics are more suitable for summarization and data-to-text tasks, while dialogue tasks may require different approaches depending on reference availability. The authors emphasize the importance of assessing reference-free metrics before applying them to new tasks, particularly when inputs are in uncommon form or answer spaces are highly variable.

## Method Summary
The paper conducts a comprehensive evaluation comparing reference-free and reference-based metrics across three NLG tasks: summarization, data-to-text, and dialogue. Using 8 datasets and three evaluation criteria, the authors systematically assess metric performance through correlation with human judgments and sensitivity to language quality defects. The study evaluates multiple metrics including BERTScore, MoverScore, and UniEval, examining their behavior across different task types and quality ranges of generated text. The experimental design allows for task-specific analysis of when reference-free metrics outperform their reference-based counterparts.

## Key Results
- Reference-free metrics show higher correlation with human judgments than reference-based metrics across all three tasks
- Reference-free metrics demonstrate better sensitivity to language quality defects
- Effectiveness of reference-free metrics varies by task type and candidate text quality
- For practical applications, reference-free metrics are recommended for summarization and data-to-text tasks
- UniEval or BERTScore are suggested for dialogue tasks depending on reference availability

## Why This Works (Mechanism)
Reference-free metrics work by evaluating generated text against input content or general language quality criteria rather than comparing to reference outputs. This approach eliminates the need for human-written references while potentially providing more consistent evaluation across diverse outputs. The metrics typically use pre-trained language models to assess semantic similarity, fluency, and relevance without requiring ground truth comparisons.

## Foundational Learning
1. **Reference-free evaluation**: Why needed - eliminates dependency on human references; Quick check - metrics should correlate with human judgments without reference text
2. **Human judgment correlation**: Why needed - establishes metric reliability; Quick check - compute Pearson/Spearman correlation between metric scores and human ratings
3. **Sensitivity to language defects**: Why needed - measures metric's ability to detect quality issues; Quick check - introduce controlled errors and observe metric response
4. **Task-specific evaluation**: Why needed - different NLG tasks have different requirements; Quick check - evaluate metrics separately for each task type
5. **Candidate text quality impact**: Why needed - determines metric reliability ranges; Quick check - stratify results by quality tiers of generated text
6. **Cross-dataset generalization**: Why needed - ensures findings aren't dataset-specific; Quick check - test metrics across multiple datasets for each task type

## Architecture Onboarding

**Component map**: Input text -> Reference-free metric (BERTScore/MoverScore/UniEval) -> Quality score

**Critical path**: Generated text → Pre-trained language model → Semantic/feature extraction → Quality assessment → Human judgment correlation

**Design tradeoffs**: Reference-free metrics sacrifice task-specific alignment for broader applicability, trading precision in output matching for robustness to diverse generation patterns

**Failure signatures**: Poor performance on uncommon input forms, degraded correlation with human judgments for low-quality candidates, inconsistent results across tasks with variable answer spaces

**First experiments**: 
1. Test correlation with human judgments on a held-out dataset
2. Evaluate sensitivity to injected grammatical errors
3. Compare computational efficiency against reference-based alternatives

## Open Questions the Paper Calls Out
None

## Limitations
- Findings may not generalize to languages beyond English or specialized technical domains
- Performance gap depends heavily on candidate text quality but relationship isn't fully characterized
- Computational efficiency trade-offs between reference-free and reference-based metrics weren't fully explored
- Recommendations based primarily on correlation with human judgments without considering practical deployment constraints

## Confidence
- **High confidence**: Reference-free metrics generally show better correlation with human judgments across tested tasks
- **Medium confidence**: Task-specific effectiveness varies and depends on candidate text quality
- **Medium confidence**: Reference-free metrics are more sensitive to language quality defects

## Next Checks
1. Test the same reference-free metrics on low-resource languages and specialized technical domains to assess cross-lingual and cross-domain generalizability
2. Conduct ablation studies on candidate text quality ranges to establish quantitative thresholds for when reference-free metrics become unreliable
3. Evaluate computational efficiency trade-offs between reference-free and reference-based metrics in production environments with realistic batch sizes and latency requirements