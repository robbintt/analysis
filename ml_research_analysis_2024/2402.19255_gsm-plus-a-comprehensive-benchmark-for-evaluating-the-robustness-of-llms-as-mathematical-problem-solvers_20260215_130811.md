---
ver: rpa2
title: 'GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs
  as Mathematical Problem Solvers'
arxiv_id: '2402.19255'
source_url: https://arxiv.org/abs/2402.19255
tags: []
core_contribution: This paper introduces GSM-Plus, a benchmark designed to evaluate
  the robustness of large language models (LLMs) in mathematical problem-solving.
  GSM-Plus extends the GSM8K dataset with eight types of perturbations across five
  perspectives, including numerical variation, arithmetic variation, problem understanding,
  distractor insertion, and critical thinking.
---

# GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers

## Quick Facts
- arXiv ID: 2402.19255
- Source URL: https://arxiv.org/abs/2402.19255
- Reference count: 40
- Primary result: LLMs show significant performance degradation (average PDR of 16.88% for GPT-3.5-Turbo) when tested on perturbed versions of GSM8K

## Executive Summary
GSM-Plus is a benchmark designed to evaluate the robustness of large language models in mathematical problem-solving by extending the GSM8K dataset with eight types of perturbations across five perspectives. Experiments on 25 LLMs and four prompting techniques show that while LLMs perform well on GSM8K, their accuracy drops significantly on GSM-Plus, with an average performance drop rate of 16.88% for GPT-3.5-Turbo and 8.23% for GPT-4. The study introduces a compositional prompting method that iteratively generates and verifies reasoning steps, showing improved robustness compared to existing techniques. However, the performance gap between GSM8K and GSM-Plus remains substantial, highlighting the need for further research to enhance the robustness of LLMs in mathematical reasoning.

## Method Summary
The GSM-Plus benchmark extends GSM8K with eight perturbation types across five perspectives (numerical variation, arithmetic variation, problem understanding, distractor insertion, and critical thinking) to evaluate LLM robustness. The dataset was generated using GPT-4 for initial variations, followed by human annotation and quality control. The evaluation framework tests 25 LLMs with four prompting techniques (Chain-of-Thought, Program-of-Thought, Least-to-Most, and complexity-based CoT) and introduces a compositional prompting method (COMP) that iteratively generates and verifies reasoning steps. Performance is measured using Accuracy, Performance Drop Rate (PDR), and percentage of accurately solved pairs (ASP).

## Key Results
- LLMs show significant performance degradation on GSM-Plus compared to GSM8K, with average PDR of 16.88% for GPT-3.5-Turbo and 8.23% for GPT-4
- All models demonstrate inferior robustness compared to humans in critical thinking, arithmetic variations, and distractor insertion
- The compositional prompting method (COMP) improves performance across various variation types through iterative generation and verification
- Program-based models show particular sensitivity to integer-decimal-fraction conversions
- Complexity-based CoT shows superior performance for GPT-4, GPT-3.5-Turbo, and LLaMA-2-70B

## Why This Works (Mechanism)

### Mechanism 1
GSM-Plus reveals LLM fragility through systematic perturbations that expose shortcut reliance. By introducing perturbations across five perspectives (numerical, arithmetic, problem understanding, distractor insertion, critical thinking), GSM-Plus creates controlled variations that reveal whether LLMs truly understand mathematical concepts or merely memorize patterns from training data. The core assumption is that LLMs relying on pattern matching rather than genuine mathematical reasoning will fail when patterns are altered. Evidence shows FMR scores validate GSM-Plus's position in the math reasoning benchmark ecosystem, while experiments demonstrate significant performance drops across all models.

### Mechanism 2
Compositional prompting improves robustness by iteratively generating and verifying reasoning steps. The COMP method alternates between generating subgoals and calculations while self-verifying completion status, creating a feedback loop that catches errors mid-reasoning rather than at the final answer. The core assumption is that LLMs make errors mid-process that could be caught if verified before proceeding to subsequent steps. Experiments show COMP improves model performance across various variation types and demonstrates remarkable improvements over ensemble-based COT methods when combined with self-consistency.

### Mechanism 3
GSM-Plus enables fine-grained evaluation of specific mathematical reasoning skills through its eight perturbation types. By categorizing perturbations into eight distinct types across five perspectives, GSM-Plus allows researchers to isolate which mathematical skills LLMs struggle with most, rather than just measuring overall performance. The core assumption is that different mathematical skills require different types of reasoning, and LLMs may excel at some while failing at others. Results show all models demonstrate inferior robustness compared to humans in specific areas like critical thinking and arithmetic variations, enabling targeted analysis of LLM weaknesses.

## Foundational Learning

- **Concept**: Mathematical problem-solving taxonomy (Polya's principles)
  - Why needed here: The benchmark is designed based on Polya's four-stage problem-solving framework (identifying variables, representing structure, calculating, verifying), which provides the theoretical foundation for categorizing perturbations
  - Quick check question: Can you list the four stages of Polya's problem-solving framework and give an example of how each might be tested in a math problem?

- **Concept**: Prompt engineering techniques (Chain-of-Thought, Program-of-Thought, Least-to-Most)
  - Why needed here: The paper evaluates multiple prompting techniques to understand how different approaches affect robustness, with COMP building on insights from these existing methods
  - Quick check question: What is the key difference between Chain-of-Thought prompting and Program-of-Thought prompting in terms of how they guide LLM reasoning?

- **Concept**: Performance metrics (Accuracy, PDR, ASP)
  - Why needed here: Understanding these metrics is crucial for interpreting results, particularly PDR which measures relative performance decline and ASP which measures reasoning transferability
  - Quick check question: If a model achieves 80% accuracy on GSM8K and 60% on GSM-Plus, what is its PDR value?

## Architecture Onboarding

- **Component map**: Data generation pipeline (GPT-4 rewriting → human annotation → quality control) → Evaluation framework (25 LLMs + 4 prompting techniques across GSM8K vs GSM-Plus) → Analysis tools (PDR calculation, ASP computation, perturbation-specific performance tracking) → Prompting methods (COT, POT, LTM, complexity-based COT, COMP)

- **Critical path**: GSM8K question → perturbation generation → answer generation → human validation → evaluation on LLMs → performance analysis

- **Design tradeoffs**: Using GPT-4 for initial variation generation provides scale but requires human validation for quality; including 8 perturbations per question provides comprehensive coverage but increases dataset size significantly; focusing on GSM8K as seed ensures grade-appropriate difficulty but may limit generalizability to other math domains

- **Failure signatures**: Low inter-annotator agreement on question variations indicates ambiguity in perturbation definitions; inconsistent performance across perturbation types suggests model weaknesses in specific reasoning skills; high PDR values across all models indicate fundamental robustness issues rather than model-specific problems

- **First 3 experiments**: 1) Run baseline evaluation: Compare GSM8K vs GSM-Plus accuracy for a small set of models to establish performance gap; 2) Test perturbation sensitivity: Evaluate model performance on individual perturbation types to identify weakest skills; 3) Validate compositional prompting: Implement COMP method on a subset of questions to verify improvement claims before full-scale evaluation

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of LLMs on GSM-Plus vary across different grade levels of math problems (e.g., elementary, middle, high school)? The study is limited to grade school math problems, leaving open the question of how LLMs would perform on more advanced math problems. Conducting experiments on GSM-Plus with math problems from various grade levels would provide insights into their robustness at different educational stages.

### Open Question 2
What are the underlying reasons for the failures of LLMs in solving math word problems, particularly in cases of critical thinking and arithmetic variation? The paper evaluates the robustness of LLMs but does not delve into the specific causes of their failures in solving math word problems. Conducting detailed error analysis and examining the reasoning chains of LLMs when they fail would help identify the root causes of their failures.

### Open Question 3
How does the performance of LLMs on GSM-Plus change when using external tools or resources, such as calculators or knowledge bases, during the problem-solving process? The study focuses on the performance of LLMs using different prompting techniques but does not consider the potential benefits of incorporating external tools or resources. Conducting experiments with LLMs using external tools and comparing their performance would provide insights into the potential benefits of these resources.

## Limitations
- Perturbation design validity uncertainty: While GSM-Plus introduces systematic perturbations, there remains uncertainty about whether the perturbations truly isolate distinct reasoning skills or if there is significant overlap between perturbation types
- Compositional prompting generalizability: The COMP method shows promising results but was only tested on a subset of the benchmark, requiring broader validation across different mathematical domains
- Dataset representativeness: GSM-Plus is built on GSM8K as a foundation, which may limit its ability to assess LLM robustness in more advanced mathematical domains or real-world problem-solving scenarios

## Confidence

**High Confidence**: The core finding that LLMs show significant performance degradation (average PDR of 16.88% for GPT-3.5-Turbo) when tested on perturbed versions of GSM8K is well-supported by the experimental results across 25 models and four prompting techniques.

**Medium Confidence**: The compositional prompting method (COMP) shows consistent improvements, but the sample size and breadth of testing is limited. More extensive validation across different mathematical domains would strengthen these claims.

**Medium Confidence**: The analysis of perturbation-specific weaknesses provides valuable insights, though the interpretation of which skills LLMs struggle with most could benefit from additional validation with human performance baselines.

## Next Checks

1. **Cross-Domain Validation**: Test GSM-Plus perturbations on mathematical problems from higher education levels or different mathematical domains (algebra, geometry, calculus) to assess whether the robustness issues persist beyond grade school arithmetic.

2. **Human Performance Baseline**: Conduct human evaluations on GSM-Plus questions to establish performance baselines for each perturbation type, enabling more meaningful comparisons of LLM weaknesses versus human reasoning capabilities.

3. **Perturbation Independence Analysis**: Perform a statistical analysis of the correlation between performance on different perturbation types to determine whether the eight variations truly test distinct reasoning skills or if there is significant overlap that could confound the fine-grained analysis.