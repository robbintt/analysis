---
ver: rpa2
title: Autoregressive Pre-Training on Pixels and Texts
arxiv_id: '2404.10710'
source_url: https://arxiv.org/abs/2404.10710
tags:
- text
- data
- pixel
- language
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores autoregressive pre-training on both pixel-based
  and text-based language models. The authors propose PixelGPT, a tokenization-free
  model trained on rendered RGB images of text, and DualGPT, which integrates both
  visual and textual modalities during pre-training.
---

# Autoregressive Pre-Training on Pixels and Texts

## Quick Facts
- arXiv ID: 2404.10710
- Source URL: https://arxiv.org/abs/2404.10710
- Reference count: 31
- Key outcome: Pixel-based autoregressive models achieve competitive results compared to state-of-the-art bidirectional models on language understanding tasks

## Executive Summary
This paper explores autoregressive pre-training on both pixel-based and text-based language models. The authors propose PixelGPT, a tokenization-free model trained on rendered RGB images of text, and DualGPT, which integrates both visual and textual modalities during pre-training. The primary finding is that pixel-based autoregressive models can achieve competitive results compared to state-of-the-art bidirectional models on language understanding tasks. DualGPT, with its multimodal approach, shows improved performance over models trained on a single modality.

## Method Summary
The paper introduces PixelGPT, which renders text as RGB images and processes them as sequential patches using an autoregressive transformer decoder. The model learns to predict the next patch in sequence, capturing orthographic and typographic features directly from visual data. DualGPT extends this approach by incorporating both pixel and text modalities during pre-training, using paired image-text data to create complementary representations. The architecture uses standard transformer components with SwiGLU activations, RMSNorm, rotary position embeddings, and grouped query attention, trained on large-scale data including peS2o, English Wikipedia, C4, Common Crawl, and The Stack v1.

## Key Results
- PixelGPT achieves comparable performance to BERT on GLUE benchmark tasks
- DualGPT shows superior performance over PIXEL on cross-lingual tasks (XNLI)
- Pixel-based models overcome vocabulary bottlenecks in multilingual contexts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pixel-based autoregressive models can achieve competitive performance with bidirectional models on language understanding tasks.
- Mechanism: By rendering text as RGB images and using next patch prediction with regression head, the model captures orthographic and typographic features directly from visual data, bypassing tokenization bottlenecks.
- Core assumption: Visual representation of text contains sufficient linguistic information for understanding tasks.
- Evidence anchors:
  - [abstract]: "remarkably, we find that a unidirectional pixel-based model trained solely on visual data can achieve comparable results to state-of-the-art bidirectional models on several language understanding tasks."
  - [section 2.1]: "Pixel-based modeling also addresses the vocabulary bottleneck—a trade-off between input encoding granularity and the computational costs associated with vocabulary estimation in conventional language models."
  - [corpus]: Weak - corpus neighbors focus on visual-language integration but don't directly support pixel-based text modeling claims.
- Break condition: If rendered images lose critical linguistic features or if visual features don't correlate with language understanding metrics.

### Mechanism 2
- Claim: Dual-modality pre-training with paired image-text data improves multilingual understanding.
- Mechanism: Integrating visual and textual modalities during pre-training creates complementary representations that enhance cross-lingual generalization by leveraging orthographic similarities across languages.
- Core assumption: Visual representations provide language-agnostic features that benefit multilingual tasks.
- Evidence anchors:
  - [abstract]: "incorporating both visual and textual data significantly improves the performance of pixel-based language models."
  - [section 4.2]: "DualGPT demonstrates superior performance over PIXEL, showcasing the efficacy of exclusive pixel-based input modality in cross-lingual contexts."
  - [corpus]: Weak - corpus neighbors discuss multimodal integration but not specifically pixel-based multilingual learning.
- Break condition: If modality conflicts during training prevent effective knowledge transfer or if visual features don't generalize across languages.

### Mechanism 3
- Claim: Pixel-based models overcome vocabulary bottleneck in multilingual contexts.
- Mechanism: By processing text as images rather than tokens, the model avoids language-specific tokenization limitations and can generalize across languages using visual orthographic patterns.
- Core assumption: Visual patterns in text images are consistent enough across languages to enable cross-lingual transfer.
- Evidence anchors:
  - [section 2.1]: "Pixel-based representations, however, transcend this limitation by representing text in a modality that inherently supports unified processing—the visual domain of images."
  - [section 4.2]: "Pixel-based representations, however, transcend this limitation by representing text in a modality that inherently supports unified processing—the visual domain of images."
  - [corpus]: Weak - corpus doesn't provide direct evidence for vocabulary bottleneck claims.
- Break condition: If visual patterns are too language-specific or if model performance degrades significantly on languages with different scripts.

## Foundational Learning

- Concept: Visual representation learning for text
  - Why needed here: The entire approach depends on rendering text as images and extracting meaningful features from visual representations
  - Quick check question: How does rendering text as RGB images preserve linguistic information while avoiding tokenization issues?

- Concept: Multimodal pre-training strategies
  - Why needed here: DualGPT requires understanding how to effectively combine visual and textual modalities during training
  - Quick check question: What are the key differences between training on paired vs. separate image-text data?

- Concept: Autoregressive vs. bidirectional modeling
  - Why needed here: PixelGPT uses autoregressive approach while PIXEL uses bidirectional approach, requiring understanding of their trade-offs
  - Quick check question: How does the unidirectional nature of autoregressive models affect their ability to capture bidirectional context?

## Architecture Onboarding

- Component map:
  Text renderer -> Patchifier -> Embedding projection -> Transformer layers -> Prediction heads

- Critical path:
  1. Text rendering → patch segmentation → embedding projection
  2. Transformer layers processing sequential patches
  3. Prediction head generating next patch/token probabilities
  4. Loss computation (MSE for images, cross-entropy for text)

- Design tradeoffs:
  - RGB vs grayscale: RGB preserves color information but increases computational cost
  - Patch size (16x16): Balances local feature capture with global context
  - Global batch size: Larger batches improve stability but require more memory
  - Pre-training data ratio: Optimal balance between visual and textual data

- Failure signatures:
  - Low training loss but poor fine-tuning performance: Likely overfitting to pre-training task
  - High variance in fine-tuning results: Batch size too small for pixel modality
  - Language-specific performance gaps: Insufficient multilingual pre-training data
  - Slow convergence: Learning rate too low or data scale insufficient

- First 3 experiments:
  1. Train PixelGPT on English Wikipedia only, evaluate on GLUE tasks
  2. Compare RGB vs grayscale rendering performance on HatemojiBuild dataset
  3. Test different batch sizes (64, 128, 256, 512) on QQP fine-tuning task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can pixel-based autoregressive models like PixelGPT surpass text-based models like BERT and GPT-2 with increased training data and model scale?
- Basis in paper: [explicit] The paper states "Pixel-based autoregressive pretraining models exhibit an increased data demand" and "with the increase of training data, a critical volume threshold catalyzes a substantial rise in performance for PixelGPT... culminating in its overtaking of PIXEL at around 200B tokens/patches and nearing TextGPT with a less than 5-point performance differential, while still on an upward trend."
- Why unresolved: The paper notes computational constraints prevented exploration beyond 200B tokens/patches, leaving open whether scaling to larger models (7B, 13B, 70B, 100B+ parameters) and training data (>1,000 billion tokens/patches) would allow pixel-based models to surpass text-based models.
- What evidence would resolve it: Empirical results from training PixelGPT at larger scales (7B+ parameters) with significantly more data (>1,000B tokens/patches) showing superior performance compared to equivalent text-based models on benchmarks like GLUE and XNLI.

### Open Question 2
- Question: How does the integration of paired dual-modality data during pre-training improve multimodal learning compared to unimodal training, and what are the optimal ratios and strategies for combining visual and textual data?
- Basis in paper: [explicit] The paper states "incorporating dual-modality data during pre-training markedly enhances average performance" and "the addition of pixel-text paired data improved the model's multilingual interpretative proficiency."
- Why unresolved: The paper only explores one configuration of dual-modality pre-training (4:4:2 text:image:pair ratio) without investigating the impact of different ratios or strategies for combining modalities during pre-training.
- What evidence would resolve it: Systematic ablation studies varying the ratios of text, image, and paired data during pre-training, along with different strategies for combining modalities (e.g., alternating batches, joint training), to identify optimal configurations that maximize performance across various tasks.

### Open Question 3
- Question: What are the implications of using pixel-based representations for handling languages with non-Latin scripts, and how does this approach perform compared to traditional tokenization-based models on extremely low-resource languages?
- Basis in paper: [explicit] The paper notes PixelGPT "exhibits pronounced gains over BERT in languages that diverge significantly from English, such as Thai and Chinese" and suggests this is due to "the absence of PixelGPT's reliance on language-specific tokenization."
- Why unresolved: While the paper demonstrates advantages for Thai and Chinese, it doesn't extensively test performance on extremely low-resource languages or provide detailed analysis of how pixel-based representations handle the unique challenges of diverse writing systems.
- What evidence would resolve it: Comprehensive evaluation of PixelGPT and similar models on a wide range of low-resource languages with diverse scripts (e.g., indigenous languages, scripts like Arabic, Devanagari, Hangul), comparing performance against strong tokenization-based baselines and analyzing specific advantages/disadvantages of the pixel-based approach for each language family.

## Limitations

- Architecture Generalization: The approach relies heavily on specific rendering parameters (16x16 patches, RGB color depth, specific font settings), and robustness across different writing systems and typographic variations remains unclear.
- Multilingual Generalization: Evaluation is limited to XNLI languages, with insufficient testing on extremely low-resource languages and diverse script systems.
- Computational Efficiency: The pixel-based approach requires significantly more computational resources than text-based models, with no comprehensive efficiency analysis provided.

## Confidence

**High Confidence Claims**:
- PixelGPT can learn meaningful representations from pixel-based text data
- The autoregressive architecture with patch prediction is implementable and trainable at scale
- Dual-modality pre-training shows performance improvements over single-modality approaches

**Medium Confidence Claims**:
- Pixel-based models overcome vocabulary bottlenecks in multilingual contexts
- Visual representations provide language-agnostic features for cross-lingual transfer
- PixelGPT achieves comparable results to bidirectional models on language understanding tasks

**Low Confidence Claims**:
- Pixel-based representations are superior to tokenization for all multilingual scenarios
- The specific rendering parameters are optimal across all contexts
- The computational overhead is justified by the performance gains

## Next Checks

1. **Script Diversity Testing**: Evaluate PixelGPT on languages with non-Latin scripts (Arabic, Chinese, Devanagari) using carefully controlled rendering to assess whether visual representations truly provide language-agnostic benefits. Compare performance against strong multilingual text-based models.

2. **Rendering Parameter Sensitivity**: Conduct systematic ablation studies varying font size, DPI, patch dimensions, and color depth to quantify their impact on downstream task performance. This would validate whether the chosen parameters are optimal or merely sufficient.

3. **Efficiency Benchmarking**: Measure wall-clock training time, memory usage, and inference latency of pixel-based vs. text-based models at equivalent parameter counts. Calculate the performance-per-FLOP ratio to assess whether the computational overhead is justified by the gains.