---
ver: rpa2
title: Federated Class-Incremental Learning with New-Class Augmented Self-Distillation
arxiv_id: '2401.00622'
source_url: https://arxiv.org/abs/2401.00622
tags:
- learning
- classes
- fednasd
- federated
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of catastrophic forgetting in
  federated class-incremental learning (FCIL), where models suffer from degraded performance
  on previously learned classes when learning new classes. The proposed method, FedNASD,
  augments self-distillation with new-class information by combining current model
  predictions for new classes with historical model outputs for old classes.
---

# Federated Class-Incremental Learning with New-Class Augmented Self-Distillation

## Quick Facts
- arXiv ID: 2401.00622
- Source URL: https://arxiv.org/abs/2401.00622
- Reference count: 5
- Key outcome: FedNASD reduces average forgetting rates to 16.22-22.05% and improves global accuracy to 86.14-97.00% in federated class-incremental learning

## Executive Summary
This paper introduces FedNASD, a method for federated class-incremental learning that addresses catastrophic forgetting through new-class augmented self-distillation. The approach enriches historical model logits with new-class scores during knowledge transfer, enabling more comprehensive model updates. Theoretical analysis shows FedNASD models old-class scores as conditional probabilities and uses current model predictions to refine these probabilities. Experimental results demonstrate significant improvements over baseline methods across multiple datasets and class-incremental settings.

## Method Summary
FedNASD augments self-distillation by combining historical model predictions for old classes with current model predictions for new classes during knowledge transfer. The method extends the model architecture to accommodate new classes and uses a memory buffer to store representative samples from previous tasks. During training, clients perform augmented self-distillation using both old-class samples from memory and new-class samples from the current task, with the augmented knowledge transferred through federated aggregation.

## Key Results
- Reduces average forgetting rates to 16.22-22.05% across multiple datasets
- Improves global accuracy to 86.14-97.00% in federated class-incremental settings
- Outperforms four baseline methods in both accuracy and forgetting rate metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FedNASD reduces catastrophic forgetting by enriching historical model logits with new-class scores during self-distillation
- Mechanism: Combines old-class predictions from historical models with new-class predictions from current models to ensure comprehensive probability information
- Core assumption: Current model predictions accurately approximate what historical models would have predicted with new-class information
- Evidence anchors: [abstract], [section], [corpus] weak evidence
- Break condition: If approximation assumption fails, enriched knowledge transfer becomes inaccurate

### Mechanism 2
- Claim: FedNASD models old-class scores as conditional probabilities in absence of new classes
- Mechanism: Treats historical model outputs as conditional probabilities p(Cold|Sold)
- Core assumption: Historical outputs can be mathematically decomposed into conditional probabilities
- Evidence anchors: [abstract], [section], [corpus] weak evidence
- Break condition: If conditional probability decomposition doesn't hold mathematically

### Mechanism 3
- Claim: FedNASD uses current model predictions to refine conditional probabilities of historical scores
- Mechanism: Combines historical outputs with current predictions to create enriched knowledge
- Core assumption: Current predictions can adjust and refine historical understanding
- Evidence anchors: [abstract], [section], [corpus] no direct evidence
- Break condition: If current predictions are unreliable, refinement introduces noise

## Foundational Learning

- Concept: Federated Learning (FL) principles
  - Why needed here: Understanding federated aggregation while preserving privacy is fundamental to FedNASD's distributed nature
  - Quick check question: How does FedNASD modify standard FedAvg for class-incremental learning?

- Concept: Class-Incremental Learning (CIL) mechanisms
  - Why needed here: Core problem is catastrophic forgetting in class-incremental scenarios
  - Quick check question: What CIL components does FedNASD modify for federated settings?

- Concept: Knowledge Distillation fundamentals
  - Why needed here: FedNASD builds on self-distillation techniques for knowledge transfer
  - Quick check question: How does FedNASD's augmented self-distillation differ from standard self-distillation?

## Architecture Onboarding

- Component map: Client-side training with augmented self-distillation -> Server-side federated aggregation -> Global model distribution
- Critical path: Client receives global model → Loads memory buffer → Performs augmented self-distillation → Uploads updated model → Server aggregates updates → Broadcasts updated global model
- Design tradeoffs: Memory buffer size vs. forgetting rate, distillation coefficient vs. learning speed, client participation vs. communication efficiency
- Failure signatures: Increased forgetting rate indicates insufficient knowledge transfer, poor new-class performance suggests overemphasis on historical knowledge
- First 3 experiments: 1) Baseline comparison vs. FedAvg on two-task setup, 2) Parameter sensitivity for β and memory size, 3) Dataset generalization across CIFAR-10, EMNIST, SYN-NUM

## Open Questions the Paper Calls Out
1. How does FedNASD's performance scale with more than three incremental tasks and what are long-term effects on accuracy and forgetting rates?
2. What are the theoretical guarantees of FedNASD's performance trade-offs between learning new classes and retaining old class knowledge?
3. How does memory size choice impact FedNASD performance across different class-incremental learning scenarios?

## Limitations
- Lack of comprehensive ablation studies to isolate mechanism contributions
- No analysis of communication efficiency in federated settings
- Unclear memory buffer selection strategy and its impact on performance

## Confidence
- Medium-High confidence in empirical results showing performance improvements
- Medium-Low confidence in theoretical claims about conditional probability modeling
- Medium confidence in mechanism claims regarding knowledge refinement

## Next Checks
1. Conduct ablation study validation to isolate contribution of new-class augmentation
2. Measure communication efficiency overhead and assess real-world deployment impact
3. Test different memory selection strategies to identify optimal buffer management approaches