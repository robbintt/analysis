---
ver: rpa2
title: Text Serialization and Their Relationship with the Conventional Paradigms of
  Tabular Machine Learning
arxiv_id: '2406.13846'
source_url: https://arxiv.org/abs/2406.13846
tags:
- learning
- data
- tabular
- text
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates whether language models (LMs) can replace
  traditional machine learning methods for tabular data classification. The authors
  explore text serialization as a method to convert tabular data into natural language
  representations, which can then be used to fine-tune LMs for classification tasks.
---

# Text Serialization and Their Relationship with the Conventional Paradigms of Tabular Machine Learning

## Quick Facts
- arXiv ID: 2406.13846
- Source URL: https://arxiv.org/abs/2406.13846
- Authors: Kyoka Ono; Simon A. Lee
- Reference count: 40
- Primary result: Language models using text serialization do not consistently outperform traditional ML methods for tabular data classification.

## Executive Summary
This study investigates whether pre-trained language models can replace traditional machine learning approaches for tabular data classification through text serialization techniques. The authors convert tabular data into natural language representations and fine-tune language models on these serialized inputs, benchmarking against conventional methods like SVM, LightGBM, and XGBoost. Their systematic evaluation across eight diverse datasets reveals that while language models can achieve competitive performance in some cases, they do not consistently outperform traditional approaches. The research identifies feature selection as the most beneficial preprocessing step, while other techniques like scaling and missing value handling show mixed effects on model performance.

## Method Summary
The authors employ text serialization to transform tabular data into natural language representations, which are then used to fine-tune DistilBERT for classification tasks. They systematically evaluate the impact of various preprocessing techniques including feature selection (using SHAP values and ANOVA F-tests), scaling, and missing value handling on LM performance. The methodology includes 5-fold cross-validation across eight tabular datasets with different characteristics (class imbalance, distribution shift, high dimensionality). Performance is benchmarked against traditional ML models (SVM, LightGBM, XGBoost) and deep learning models (TabNet, TabPFN) using F1, accuracy, AUROC, and MCC metrics.

## Key Results
- Feature selection consistently improves LM performance across most evaluation datasets
- Text serialization with LMs achieves competitive but not superior performance compared to traditional methods
- Preprocessing techniques like scaling and missing value handling show mixed or negative effects on LM performance
- LMs struggle with smaller datasets (<1000 samples) and do not demonstrate clear advantages over conventional approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Text serialization enables language models to process tabular data by converting structured data into natural language representations.
- **Mechanism:** The conversion process uses template filling to transform tabular data into serialized text, which can then be input into language models for supervised fine-tuning.
- **Core assumption:** Language models can effectively learn classification tasks from natural language representations of tabular data.
- **Evidence anchors:**
  - [abstract] "This involves employing text serialization and supervised fine-tuning (SFT) techniques."
  - [section 3.1] "Text serialization is the process of transforming structured tabular data X with dimensions n Ã— m into textual representations."
  - [corpus] Weak evidence - the corpus focuses on fairness and performance evaluations but doesn't directly support the serialization mechanism.

### Mechanism 2
- **Claim:** Feature selection improves LM performance by reducing dimensionality and focusing on relevant features.
- **Mechanism:** By identifying and serializing only important features (using methods like SHAP values or ANOVA F-tests), the model can achieve better classification results.
- **Core assumption:** Not all features contribute equally to classification performance, and reducing noise improves model accuracy.
- **Evidence anchors:**
  - [section 5.2] "Our study reveals that feature selection appears to have a positive effect on both F1 score and AUROC in most evaluation datasets."
  - [table 2] Shows improved metrics with feature selection in most datasets.
  - [corpus] Weak evidence - corpus papers focus on fairness and benchmarking rather than feature selection mechanisms.

### Mechanism 3
- **Claim:** Pre-trained language models provide competitive performance through transfer learning, but may not consistently outperform traditional methods.
- **Mechanism:** The large number of pre-trained parameters in language models can be fine-tuned for specific tabular tasks, potentially achieving state-of-the-art results.
- **Core assumption:** Transfer learning from pre-trained language models can be effectively applied to tabular data classification tasks.
- **Evidence anchors:**
  - [abstract] "Our findings reveal current pre-trained models should not replace conventional approaches."
  - [section 6.3] "Therefore, while our TabLM model reached SOTA accuracy levels for specific tasks, other methodologies often yielded more robust results across the board."
  - [corpus] Weak evidence - corpus papers discuss benchmarking and fairness but don't provide direct evidence for this transfer learning mechanism.

## Foundational Learning

- **Concept:** Natural language processing and transformer architecture
  - Why needed here: Understanding how language models process and learn from text data is fundamental to understanding why text serialization works.
  - Quick check question: How do transformer models use attention mechanisms to process sequential data?

- **Concept:** Feature selection and importance ranking methods
  - Why needed here: Feature selection is a key preprocessing step that significantly impacts model performance, as shown in the experimental results.
  - Quick check question: What is the difference between SHAP values and ANOVA F-tests for feature importance?

- **Concept:** Traditional machine learning classification metrics
  - Why needed here: The paper uses multiple metrics (F1, AUROC, MCC) to evaluate model performance, requiring understanding of their differences and appropriate use cases.
  - Quick check question: When would you prefer MCC over accuracy for evaluating a classification model?

## Architecture Onboarding

- **Component map:** Tabular data -> Preprocessing (feature selection) -> Text serialization -> Language model fine-tuning -> Evaluation (multiple metrics) -> Benchmark comparison

- **Critical path:**
  1. Load and preprocess tabular data
  2. Apply feature selection if beneficial
  3. Serialize data into natural language format
  4. Fine-tune pre-trained language model
  5. Evaluate using multiple classification metrics
  6. Compare against baseline models

- **Design tradeoffs:**
  - Text serialization vs. direct tabular input: Serialization enables LM processing but may lose structural information
  - Feature selection vs. full feature use: Selection improves performance but requires additional computation
  - Pre-trained LM vs. traditional methods: LMs offer flexibility but are computationally expensive

- **Failure signatures:**
  - Poor performance on datasets with many missing values
  - Inconsistent results across different serialization strategies
  - High computational cost compared to traditional methods
  - Overfitting on small datasets during fine-tuning

- **First 3 experiments:**
  1. Test text serialization with a simple dataset (e.g., Iris) without any preprocessing to establish baseline LM performance
  2. Apply feature selection using SHAP values and compare performance against the baseline
  3. Test different serialization strategies (template vs. list readout) on a dataset with mixed data types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of text serialization depend on the specific characteristics of the tabular dataset (e.g., high dimensionality, class imbalance, distribution shift)?
- Basis in paper: [explicit] The authors evaluate LMs on datasets with high dimensionality, class imbalance, and distribution shift, finding that LMs do not consistently outperform traditional methods.
- Why unresolved: While the study tests LMs on datasets with various characteristics, it does not systematically isolate the impact of each characteristic on LM performance. Further investigation is needed to determine if certain dataset characteristics make text serialization more or less effective.
- What evidence would resolve it: Controlled experiments varying individual dataset characteristics (e.g., dimensionality, imbalance ratio, degree of distribution shift) while keeping other factors constant, followed by a statistical analysis of LM performance trends across these variations.

### Open Question 2
- Question: Are there specific serialization strategies or text template designs that consistently improve LM performance on tabular data?
- Basis in paper: [explicit] The authors note that "engineering the input text could significantly enhance or reduce the performance of various language models in classification tasks," suggesting sensitivity to serialization strategies.
- Why unresolved: The study uses a single text template approach but does not explore alternative serialization strategies or conduct a systematic comparison of different template designs. It remains unclear which specific strategies are most effective.
- What evidence would resolve it: A comprehensive comparison of multiple serialization strategies (e.g., list readouts, different text templates, alternative natural language representations) across various tabular datasets, followed by an analysis of performance patterns and best practices.

### Open Question 3
- Question: How does the performance of text serialization with LMs scale with increasing dataset size and feature count?
- Basis in paper: [inferred] The authors mention that "TabPFN is not suitable for datasets with training sizes above 1024 and feature sizes above 10," indicating potential scalability issues. They also note that LMs "require processing a substantially larger number of parameters."
- Why unresolved: The study primarily uses small to medium-sized datasets. The scalability of text serialization with LMs to larger datasets and higher-dimensional feature spaces remains unexplored, raising questions about practical applicability.
- What evidence would resolve it: Experiments evaluating LM performance with text serialization on progressively larger datasets (increasing both sample size and feature count), measuring computational efficiency, memory usage, and classification accuracy to identify scalability thresholds and limitations.

## Limitations
- The serialization templates used for different datasets are not fully specified, making exact reproduction challenging
- Only a limited set of preprocessing techniques were explored without considering other potentially important methods
- The evaluation focused on a relatively small number of datasets (8 total), which may not be representative of all tabular data scenarios
- Computational cost of fine-tuning large language models was not thoroughly discussed as a practical consideration
- The study did not explore ensemble approaches combining LMs with traditional methods

## Confidence

- **High Confidence:** The finding that feature selection improves LM performance across most datasets is well-supported by consistent experimental results across multiple evaluation metrics.
- **Medium Confidence:** The conclusion that current pre-trained LMs should not replace conventional approaches is supported by the benchmark comparisons, though the limited number of datasets and specific model choices (DistilBERT) may affect generalizability.
- **Low Confidence:** The assertion that scaling and missing value handling have mixed effects is based on the specific datasets tested, and different datasets or serialization strategies might yield different results.

## Next Checks

1. **Template Reproducibility Test:** Create and test the exact serialization templates for all eight datasets using the examples provided in the paper, then verify if the same performance patterns emerge when fine-tuning DistilBERT.

2. **Cross-Dataset Generalization:** Apply the complete methodology (feature selection + serialization + fine-tuning) to two additional tabular datasets not used in the original study, comparing performance against the same baseline models to test generalizability.

3. **Preprocessing Ablation Study:** Systematically test different combinations of preprocessing techniques (including methods not explored in the paper, such as outlier detection and feature engineering) on a subset of datasets to determine if the "feature selection only" finding holds across broader preprocessing strategies.