---
ver: rpa2
title: Continual Few-shot Event Detection via Hierarchical Augmentation Networks
arxiv_id: '2403.17733'
source_url: https://arxiv.org/abs/2403.17733
tags:
- event
- few-shot
- learning
- task
- types
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces continual few-shot event detection (CFED),
  a realistic scenario where models must learn new event types with few-shot samples
  while maintaining performance on previously learned types. The authors propose a
  memory-based framework called Hierarchical Augmentation Networks (HANet) that addresses
  two key challenges: memorizing previous event types with limited exemplars and learning
  new event types with few-shot samples.'
---

# Continual Few-shot Event Detection via Hierarchical Augmentation Networks

## Quick Facts
- arXiv ID: 2403.17733
- Source URL: https://arxiv.org/abs/2403.17733
- Reference count: 0
- This paper introduces continual few-shot event detection (CFED) and proposes HANet, achieving 7.27% and 8.44% improvements on micro F1 scores compared to state-of-the-art methods.

## Executive Summary
This paper addresses the challenging problem of continual few-shot event detection (CFED), where models must learn new event types with limited samples while maintaining performance on previously learned types. The authors propose HANet, a memory-based framework that incorporates prototypical augmentation to expand exemplar feature spaces, contrastive augmentation to prevent overfitting on new types, and knowledge distillation to preserve previous knowledge. HANet significantly outperforms existing methods on ACE and MAVEN benchmarks, demonstrating robust performance even in extreme few-shot scenarios.

## Method Summary
HANet is a hierarchical augmentation network that tackles CFED through three key components: prototypical augmentation, contrastive augmentation, and knowledge distillation. The method stores one exemplar per event type in memory and generates synthetic features from these exemplars to mitigate catastrophic forgetting. For learning new event types, contrastive augmentation creates multiple views of few-shot samples using techniques like dropout and token shuffle, then applies InfoNCE loss to encourage consistent representations. Knowledge distillation at both feature and prediction levels preserves information from previous tasks. The framework is trained using cross-entropy loss for new tasks, distillation losses for previous tasks, and exemplar replay loss to maintain overall performance.

## Key Results
- HANet achieves 7.27% improvement on micro F1 score in 4-way 5-shot MAVEN setting
- HANet achieves 8.44% improvement on micro F1 score in 2-way 5-shot ACE setting
- Outperforms both previous state-of-the-art methods and ChatGPT on continual few-shot event detection benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prototypical augmentation reconstructs the feature space of exemplars to mitigate catastrophic forgetting.
- Mechanism: Exemplars are treated as means of Gaussian distributions whose variances are computed from all training samples of the same category. Synthetic features are sampled to create a "prototypical space" that better characterizes the category.
- Core assumption: Exemplars are the most representative samples and can serve as Gaussian distribution means.
- Evidence anchors: [abstract] "we incorporate prototypical augmentation into the memory set"; [section 3.2.2] "We assume the pseudo feature space follows Gaussian Distribution."
- Break condition: If exemplars are not truly representative (e.g., in extreme few-shot scenarios), the Gaussian assumption breaks down.

### Mechanism 2
- Claim: Contrastive augmentation mitigates overfitting when learning new event types with few-shot samples.
- Mechanism: Data augmentation creates multiple views of each sample. Positive pairs come from different views of the same sentence/trigger, while negative pairs come from different sentences/triggers. InfoNCE loss encourages representations of positive pairs to be similar and negative pairs to be dissimilar.
- Core assumption: Augmented views of the same sample contain the same semantic information.
- Evidence anchors: [abstract] "we propose a contrastive augmentation module for token representations"; [section 3.3.2] "Representations originating from the same sentence are regarded as positive pairs."
- Break condition: If augmentation methods destroy semantic information or create views that are too dissimilar, contrastive loss may push apart representations that should be similar.

### Mechanism 3
- Claim: Knowledge distillation at feature and prediction levels preserves previous knowledge while learning new tasks.
- Mechanism: Feature-level distillation minimizes differences between normalized representations of previous and current models. Prediction-level distillation minimizes KL divergence between probability distributions over previous label sets.
- Core assumption: Previous model's representations and predictions contain valuable knowledge that should be preserved.
- Evidence anchors: [section 3.4] "Similar to Cao et al. (2020), we use Knowledge Distillation at feature-level and predict-level."
- Break condition: If previous model's knowledge is incorrect or outdated, distilling it may hinder learning of new, correct information.

## Foundational Learning

- Concept: Gaussian distribution assumptions in prototypical augmentation
  - Why needed here: The method assumes exemplars follow Gaussian distributions to generate synthetic features that better characterize the category's feature space.
  - Quick check question: If an exemplar has representation h_e, what distribution is used to generate synthetic features in prototypical augmentation?

- Concept: InfoNCE loss for contrastive learning
  - Why needed here: Contrastive augmentation uses InfoNCE loss to encourage representations of positive pairs (different views of same sample) to be similar and negative pairs (different samples) to be dissimilar.
  - Quick check question: In contrastive learning, what type of pairs are created from different views of the same sentence?

- Concept: Knowledge distillation for preserving previous knowledge
  - Why needed here: Feature-level and prediction-level distillation losses ensure the model doesn't forget previously learned information when learning new tasks.
  - Quick check question: What are the two types of knowledge distillation used in this method?

## Architecture Onboarding

- Component map: BERT encoder → Trigger extractor → Classifier + HANet (Prototypical Augmentation + Contrastive Augmentation + Knowledge Distillation)
- Critical path: Input sentence → BERT encoding → Trigger extraction → Classifier with HANet losses → Output event type predictions
- Design tradeoffs: Limited memory (1 exemplar per type) vs. computational efficiency; synthetic feature generation vs. storage of more exemplars; contrastive learning vs. potential overfitting to augmentation methods
- Failure signatures: Degraded performance on previous tasks (catastrophic forgetting); poor performance on new tasks (overfitting); unstable training due to conflicting loss objectives
- First 3 experiments:
  1. Verify prototypical augmentation generates diverse synthetic features by visualizing t-SNE plots of original vs. augmented exemplar features
  2. Test contrastive augmentation with different augmentation methods (dropout, shuffle, token replacement) and measure impact on few-shot learning performance
  3. Validate knowledge distillation effectiveness by comparing performance with and without distillation losses on previous tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does HANet's performance scale when the number of event types per task increases beyond 10-way classification?
- Basis in paper: [inferred] The paper conducts experiments with 10-way classification but does not explore scenarios with more event types per task.
- Why unresolved: The paper focuses on 10-way classification as the upper bound for task complexity, leaving open questions about performance in more extreme multi-class scenarios.
- What evidence would resolve it: Experimental results comparing HANet's performance on 15-way, 20-way, or even higher-way classification tasks would provide insights into scalability limits.

### Open Question 2
- Question: What is the minimum number of training samples per event type that HANet can effectively learn from while maintaining reasonable performance?
- Basis in paper: [explicit] The paper explores 1-shot and 2-shot scenarios but does not investigate even fewer samples.
- Why unresolved: While the paper demonstrates effectiveness at 1-shot and 2-shot levels, the absolute minimum sample requirement for functional performance remains unknown.
- What evidence would resolve it: Experiments testing HANet's performance on 0.5-shot (one sample for every two event types) or even less extreme few-shot scenarios would establish practical limits.

### Open Question 3
- Question: How does HANet's memory-based approach compare to parameter-efficient fine-tuning methods for continual few-shot learning?
- Basis in paper: [inferred] The paper compares against memory-based baselines but does not explore parameter-efficient methods like adapters or LoRA.
- Why unresolved: With the rise of parameter-efficient fine-tuning techniques, a direct comparison would clarify whether memory-based approaches remain superior.
- What evidence would resolve it: Head-to-head experiments comparing HANet against adapter-based or LoRA-based continual learning approaches on the same benchmarks would provide definitive answers.

## Limitations

- The Gaussian distribution assumption for prototypical augmentation may not accurately capture the true feature space of complex or ambiguous event types.
- The effectiveness of contrastive augmentation depends heavily on the choice of augmentation methods and their ability to preserve semantic information.
- The knowledge distillation approach assumes previous models contain accurate and relevant knowledge, which may not hold if earlier training data was limited or biased.

## Confidence

- High confidence: The experimental results showing HANet outperforming baseline methods on ACE and MAVEN benchmarks are well-supported by the reported metrics.
- Medium confidence: The mechanism explanations for how prototypical augmentation and contrastive augmentation address catastrophic forgetting and overfitting are plausible but lack direct empirical validation.
- Low confidence: The assumption that exemplars are the most representative samples for Gaussian distribution modeling may not hold in extreme few-shot scenarios.

## Next Checks

1. **Prototypical augmentation validation:** Conduct experiments with controlled synthetic feature generation to verify that the Gaussian assumption produces meaningful feature expansions. Test with different variance calculations and compare against alternative distribution assumptions.

2. **Augmentation method ablation:** Systematically evaluate different contrastive augmentation techniques (dropout, token shuffle, token replacement) to identify which methods provide the most benefit for few-shot event detection and which might harm performance.

3. **Knowledge distillation sensitivity analysis:** Test HANet's performance with varying strengths of knowledge distillation (adjusting λi parameters) and with intentionally corrupted previous knowledge to understand the robustness of the distillation approach.