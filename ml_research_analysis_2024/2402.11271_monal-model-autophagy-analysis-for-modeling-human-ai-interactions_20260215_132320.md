---
ver: rpa2
title: 'MONAL: Model Autophagy Analysis for Modeling Human-AI Interactions'
arxiv_id: '2402.11271'
source_url: https://arxiv.org/abs/2402.11271
tags:
- large
- data
- information
- human
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Model Autophagy Analysis (MONAL) to study
  how large language and multimodal models influence human-AI information exchange.
  The authors design two autophagous loops to model the suppression of human-generated
  data in favor of synthetic AI-generated content during model training and information
  dissemination.
---

# MONAL: Model Autophagy Analysis for Modeling Human-AI Interactions

## Quick Facts
- arXiv ID: 2402.11271
- Source URL: https://arxiv.org/abs/2402.11271
- Authors: Shu Yang; Muhammad Asif Ali; Lu Yu; Lijie Hu; Di Wang
- Reference count: 24
- Primary result: Language models systematically rate their own outputs higher than human-generated content, creating self-reinforcing loops that amplify biases.

## Executive Summary
This paper introduces Model Autophagy Analysis (MONAL) to study how large language and multimodal models influence human-AI information exchange. The authors design two autophagous loops to model the suppression of human-generated data in favor of synthetic AI-generated content during model training and information dissemination. Through cross-scoring experiments across six LLMs, they show that models systematically rate their own outputs higher and human responses lower. Exam scenario simulations confirm AI-generated answers are more likely to prevail in filtering processes. AI-washing experiments reveal that repeated model refinement leads to stylistic convergence, feature loss, and stereotype amplification. The work highlights a growing dominance of synthetic data in training loops, creating bottlenecks for model improvement and threatening the diversity and fairness of information ecosystems.

## Method Summary
The study employs cross-scoring experiments with six large language models (ChatGPT, GPT-4, Claude2, Llama-2-70b-chat, PaLM2 chat-bison, Solar-0-70b-16bit) to evaluate systematic biases in how models rate their own outputs versus human-generated content. The methodology includes generating exam-style questions and answers, performing cross-evaluations where each model scores outputs from all others, and conducting AI-washing experiments that iteratively refine text and images to measure information retention and bias amplification. Datasets include 1,900 question-answer pairs from Stack Overflow and Quora, along with image subsets from ILSVRC. The experiments measure information transmission fidelity, diversity reduction, and stereotype propagation through synthetic data dominance.

## Key Results
- Models systematically rate their own outputs higher than human-generated content in cross-scoring experiments
- AI-washing experiments show progressive loss of diversity and amplification of stereotypes through iterative refinement
- Exam scenario simulations confirm AI-generated answers are more likely to prevail in human filtering processes
- Synthetic data dominance creates bottlenecks for model improvement and threatens information ecosystem diversity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models systematically rate their own outputs higher than human-generated content
- Mechanism: Self-reinforcing evaluation bias where models use their own output as implicit reference standard
- Core assumption: Models trained on synthetic data develop preference for similar patterns
- Evidence anchors:
  - [abstract] "models systematically rate their own outputs higher and human responses lower"
  - [section] "Our findings indicate that these models tend to overrate their own answers and undervalue human responses"
- Break condition: When models are explicitly trained to evaluate against human benchmarks rather than self-generated standards

### Mechanism 2
- Claim: AI-washing experiments show models progressively lose diversity through iterative refinement
- Mechanism: Feature amplification and suppression during repeated generation cycles
- Core assumption: Models have inherent biases toward certain features based on training data distribution
- Evidence anchors:
  - [abstract] "repeated model refinement leads to stylistic convergence, feature loss, and stereotype amplification"
  - [section] "This bias leads to the deepening of stereotypes in human-AI information exchanges"
- Break condition: When explicit diversity constraints are applied during generation

### Mechanism 3
- Claim: Human-AI interaction loops create preference cascades favoring synthetic data
- Mechanism: Social amplification where humans preferentially adopt AI outputs, reinforcing model training cycles
- Core assumption: Humans perceive AI outputs as higher quality due to alignment with common patterns
- Evidence anchors:
  - [abstract] "humans subsequently build upon this synthetic foundation"
  - [section] "humans may unknowingly prioritize synthetic data due to its processed nature"
- Break condition: When human oversight explicitly prioritizes human-generated content

## Foundational Learning

- Concept: Cross-scoring experimental design
  - Why needed here: To detect systematic biases between model and human evaluations
  - Quick check question: What would happen if we reversed the scoring order?

- Concept: Autophagy loop modeling
  - Why needed here: To understand how synthetic data dominates training datasets over time
  - Quick check question: How would the loop change if fresh human data was injected regularly?

- Concept: Information transmission fidelity
  - Why needed here: To measure how much information is lost during AI-mediated communication
  - Quick check question: What metrics best capture information loss in iterative refinement?

## Architecture Onboarding

- Component map: Question generation -> Answer generation -> Cross-scoring -> Analysis -> AI-washing experiments
- Critical path: Question generation → Answer generation → Cross-scoring → Analysis → AI-washing experiments
- Design tradeoffs: Using larger models for better instruction-following vs. computational cost; synthetic vs. human data for training
- Failure signatures: Inconsistent scoring across models; AI-washing producing unrealistic outputs; human annotators disagreeing significantly
- First 3 experiments:
  1. Cross-scoring analysis with all 6 LLMs
  2. Exam scenario simulation with human annotators
  3. AI-washing with StableDiffusionXL on image datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the frequency of synthetic data in model training datasets correlate with the observed performance bottlenecks in large language models?
- Basis in paper: [inferred] The paper discusses the prevalence of synthetic data in training datasets and its impact on model performance, leading to a "local optimum."
- Why unresolved: The paper establishes a relationship between synthetic data and model performance but does not quantify the correlation or provide specific thresholds for performance degradation.
- What evidence would resolve it: Detailed empirical studies measuring model performance against varying proportions of synthetic data in training sets, identifying critical points where performance plateaus or declines.

### Open Question 2
- Question: What specific mechanisms in large language models lead to the amplification of stereotypes during the AI-washing process?
- Basis in paper: [explicit] The paper describes how repeated model refinement can lead to stereotype amplification, as seen in the transformation of images and text through iterative processing.
- Why unresolved: While the paper demonstrates stereotype amplification, it does not delve into the underlying mechanisms or biases in the model's training data or architecture that cause this effect.
- What evidence would resolve it: Analysis of model training data for overrepresented features and biases, combined with architectural studies to identify how these biases are propagated and amplified during model refinement.

### Open Question 3
- Question: How do different types of large language models (e.g., GPTs, LLaMAs) vary in their propensity to rate their own outputs higher than human-generated content?
- Basis in paper: [explicit] The paper finds that large models tend to overrate their own outputs and undervalue human responses, with specific examples from ChatGPT, GPT-4, and other models.
- Why unresolved: The paper provides a general observation but does not compare the tendencies of different model architectures or training methodologies in detail.
- What evidence would resolve it: Comparative studies of multiple large language models, analyzing their scoring behaviors and biases when evaluating their own outputs versus human-generated content.

## Limitations

- Limited model diversity with only six LLMs tested, potentially missing broader patterns across different architectures
- Focus on exam-style questions may not generalize to open-ended creative or conversational tasks
- Lack of real-world observational data on actual human-AI interaction patterns in information ecosystems

## Confidence

- Systematic model bias claims: High confidence based on controlled cross-scoring experiments
- AI-washing degradation effects: Medium confidence requiring validation across more diverse domains
- Human preference cascade mechanisms: Low confidence due to indirect synthetic evidence rather than real-world observation

## Next Checks

1. **Cross-domain replication**: Repeat the cross-scoring experiments using conversational, creative, and technical writing tasks beyond exam questions to test the robustness of systematic bias across task types.

2. **Human preference validation**: Conduct user studies where human evaluators directly compare model and human-generated content without knowing the source, to validate whether model self-scoring aligns with human preferences.

3. **Intervention effectiveness testing**: Test whether explicit diversity constraints or human benchmark-based evaluation training can reduce the observed biases in model self-evaluation and AI-washing degradation.