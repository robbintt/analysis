---
ver: rpa2
title: Scalable 3D Registration via Truncated Entry-wise Absolute Residuals
arxiv_id: '2404.00915'
source_url: https://arxiv.org/abs/2404.00915
tags:
- point
- tear
- registration
- bound
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of outlier-robust 3D point cloud
  registration when dealing with extremely large datasets containing over 99% outliers.
  The proposed method, TEAR (Truncated Entry-wise Absolute Residuals), formulates
  the problem using a novel robust loss function and solves it efficiently via a branch-and-bound
  approach.
---

# Scalable 3D Registration via Truncated Entry-wise Absolute Residuals

## Quick Facts
- arXiv ID: 2404.00915
- Source URL: https://arxiv.org/abs/2404.00915
- Authors: Tianyu Huang; Liangzu Peng; René Vidal; Yun-Hui Liu
- Reference count: 40
- Primary result: Proposes TEAR, a scalable branch-and-bound approach for 3D registration that handles >10 million points with >99% outliers

## Executive Summary
This paper addresses the challenge of outlier-robust 3D point cloud registration in extremely large-scale scenarios where traditional methods fail due to memory constraints. The authors propose TEAR (Truncated Entry-wise Absolute Residuals), which uses a novel robust loss function and efficient bound computation to enable branch-and-bound optimization on datasets with over 10 million point pairs and 99% outliers. By decomposing the 6D registration problem into lower-dimensional subproblems and developing O(N log N) bound computation algorithms, TEAR achieves state-of-the-art performance in both accuracy and runtime efficiency while using minimal memory.

## Method Summary
TEAR formulates 3D registration as minimizing truncated entry-wise absolute residuals, then solves this using branch-and-bound optimization. The key innovation is decomposing the 6D registration problem into two subproblems: TEAR-1 (3D subproblem for rotation and translation) and TEAR-2 (2D subproblem for rotation refinement). The method uses ℓ1 norm residuals instead of ℓ2, enabling separable bounds that can be computed in O(N log N) time. After solving both subproblems via branch-and-bound with custom bounds, the final rotation and translation are obtained through SVD on the identified inlier set. The approach demonstrates exceptional scalability, processing datasets that are infeasible for consistency graph-based methods due to their O(N²) memory requirements.

## Key Results
- Processes over 10 million point pairs with minimal memory usage while maintaining high accuracy
- Achieves state-of-the-art performance on synthetic data with 99.8% outliers
- Outperforms existing methods in both registration accuracy and runtime efficiency on large-scale problems
- Demonstrates superior scalability compared to consistency graph methods that fail due to memory constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing the 6D registration problem into two lower-dimensional subproblems significantly improves scalability.
- Mechanism: TEAR splits the original 6D problem into a 3D subproblem (TEAR-1) and a 2D subproblem (TEAR-2). This decomposition allows branch-and-bound to search over lower-dimensional spaces, reducing computational complexity from exponential in 6D to exponential in 3D and 2D respectively.
- Core assumption: The decomposition preserves the essential structure needed for accurate registration while reducing dimensionality.
- Evidence anchors:
  - [abstract]: "TEAR decomposes the original 6-dimensional problem into two subproblems of dimensions 3 and 2, respectively"
  - [section 2.2]: "we decompose TEAR into easier subproblems"
- Break condition: If the decomposition loses critical geometric information, accuracy would degrade significantly.

### Mechanism 2
- Claim: The Truncated Entry-wise Absolute Residuals (TEAR) loss function enables tighter bounds and more efficient computation than traditional robust losses.
- Mechanism: TEAR uses ℓ1 norm instead of ℓ2, making residuals separable and enabling O(N log N) bound computation. The truncation handles outliers while maintaining computational efficiency.
- Core assumption: ℓ1 residuals are computationally advantageous for branch-and-bound bound computation compared to ℓ2.
- Evidence anchors:
  - [abstract]: "our method involves minimizing an outlier-robust loss that computes Truncated Entry-wise Absolute Residuals"
  - [section 2.3]: "the derivative of U (t1) lies in {−N,..., −1, 0, 1,...,N}"
  - [section 2.4]: "TEAR-1 is more robust to outliers than CM-1"
- Break condition: If the ℓ1 assumption fails for specific point cloud geometries, bound tightness could degrade.

### Mechanism 3
- Claim: Novel bounding functions that are both tight and computationally efficient enable branch-and-bound to scale to millions of points.
- Mechanism: The paper develops specific algorithms (Algorithms 2, 3, 5, 6) that compute upper and lower bounds in O(N log N) time instead of O(N²), making branch-and-bound practical for large-scale problems.
- Core assumption: The specific mathematical structure of TEAR allows for these efficient bound computations.
- Evidence anchors:
  - [section 2.3]: "Theorem 1. We can solve (3) in O(N log N) time"
  - [section 2.4]: "computing bounds for TLS-1 is more time-consuming"
  - [corpus]: "KISS-Matcher: Fast and Robust Point Cloud Registration Revisited" - weak evidence for scalability claims
- Break condition: If the bound computation algorithms don't maintain tightness as problem size increases, branch-and-bound efficiency would collapse.

## Foundational Learning

- Concept: Branch-and-bound algorithm framework
  - Why needed here: TEAR uses branch-and-bound as the core optimization technique, so understanding the general framework is essential.
  - Quick check question: What are the two key components that must be provided to any branch-and-bound implementation?

- Concept: Robust loss functions for outlier handling
  - Why needed here: TEAR proposes a novel robust loss (TEAR) that differs from consensus maximization and truncated least-squares.
  - Quick check question: How does the ℓ1 norm in TEAR differ computationally from ℓ2 norm in terms of bound computation?

- Concept: 3D rotation parameterization and constraints
  - Why needed here: The decomposition requires understanding how rotations can be parameterized (e.g., using angles α, β) and how constraints like orthogonality are maintained.
  - Quick check question: Why does the constraint r₂ᵀr₁ = 0 in TEAR-2 reduce the degrees of freedom from 3 to 2?

## Architecture Onboarding

- Component map:
  - TEAR formulation (ℓ1 residuals with truncation)
  - Problem decomposition (TEAR-1 for r₁,t₁; TEAR-2 for r₂,t₂)
  - Branch-and-bound solver with custom bounds
  - SVD post-processing for final rotation/translation
  - Memory-efficient data structures for large N

- Critical path:
  1. Parse input point pairs and initialize bounds
  2. Solve TEAR-1 via branch-and-bound (Algorithm 2 for upper, Algorithm 3 for lower)
  3. Use solution to define inlier set Î₁
  4. Solve TEAR-2 via branch-and-bound (similar to TEAR-1 but 2D)
  5. Use solution to define inlier set Î₂
  6. Apply SVD to points in Î₂ for final rotation/translation

- Design tradeoffs:
  - Decomposition vs. direct 6D optimization: Trade accuracy for scalability
  - ℓ1 vs. ℓ2 residuals: Trade computational efficiency for bound tightness
  - Branch-and-bound vs. other methods: Trade theoretical guarantees for practical speed

- Failure signatures:
  - Memory overflow: Consistency graph methods fail with O(N²) storage
  - Slow convergence: Poor bound tightness leads to excessive branching
  - Accuracy degradation: Decomposition loses geometric information

- First 3 experiments:
  1. Verify O(N log N) bound computation on synthetic data with varying N
  2. Compare accuracy vs. state-of-the-art on 3DMatch dataset
  3. Test scalability on synthetic data with 10⁷ points and 99.8% outliers

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several areas remain unexplored based on the experimental methodology and scope of the work.

## Limitations
- Decomposition from 6D to 3D+2D lacks rigorous theoretical analysis of optimality preservation
- Performance primarily demonstrated on datasets with specific descriptor types (FPFH, ISS), limiting generalizability
- Assumes specific mathematical properties for O(N log N) bound computation that may not hold for all point cloud distributions

## Confidence
- High confidence: TEAR's scalability claims and memory efficiency (supported by clear computational complexity analysis)
- Medium confidence: Outperformance claims relative to state-of-the-art (based on synthetic data experiments with controlled parameters)
- Low confidence: Real-world applicability across diverse point cloud types (limited to specific descriptor-based datasets)

## Next Checks
1. Test TEAR on point clouds with non-uniform density distributions and varying geometric complexity beyond the current synthetic and descriptor-based datasets
2. Conduct ablation studies isolating the impact of the 3D+2D decomposition versus direct 6D branch-and-bound on registration accuracy
3. Benchmark against methods that use learned feature representations to assess TEAR's performance in the era of deep learning-based registration