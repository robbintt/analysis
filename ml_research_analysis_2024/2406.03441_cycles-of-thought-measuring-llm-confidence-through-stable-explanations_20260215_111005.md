---
ver: rpa2
title: 'Cycles of Thought: Measuring LLM Confidence through Stable Explanations'
arxiv_id: '2406.03441'
source_url: https://arxiv.org/abs/2406.03441
tags:
- confidence
- explanations
- arxiv
- uncertainty
- explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method for quantifying LLM uncertainty by
  analyzing the distribution of generated explanations for answers. The key idea is
  to sample multiple explanations for a given question-answer pair, measure the entailment
  probability of each explanation, and use this to reweight the distribution of explanations.
---

# Cycles of Thought: Measuring LLM Confidence through Stable Explanations

## Quick Facts
- arXiv ID: 2406.03441
- Source URL: https://arxiv.org/abs/2406.03441
- Reference count: 40
- Primary result: Method quantifies LLM uncertainty by analyzing explanation distributions, outperforming baselines on selective uncertainty tasks

## Executive Summary
This paper introduces a novel method for quantifying LLM uncertainty by examining the stability of generated explanations. The approach samples multiple explanations for each answer and uses entailment probabilities to reweight these explanations, producing a more principled confidence score than traditional token probability methods. The method demonstrates superior performance on selective uncertainty tasks across five datasets, particularly excelling on complex reasoning problems. The authors argue this represents a more robust way to assess uncertainty by considering the distribution of test-time classifiers rather than just output probabilities.

## Method Summary
The method generates multiple explanations for each question-answer pair and measures the entailment probability of each explanation. These entailment scores are used to reweight the distribution of explanations, and the final confidence score is computed as the posterior predictive distribution over answers marginalized over these weighted explanations. This approach treats confidence estimation as an aggregation problem over multiple explanations rather than relying solely on output token probabilities. The method is evaluated against baselines including token probabilities, linguistic confidence measures, and consistency-based approaches across five different datasets.

## Key Results
- Stable explanations method outperforms baselines on selective uncertainty tasks (AURC and AUROC metrics)
- Performance advantage is most pronounced on complex question-answering problems
- GPT-4 implementation shows particularly strong results compared to other models
- Method demonstrates well-principled uncertainty quantification by considering test-time classifier distributions

## Why This Works (Mechanism)
The method works by leveraging the intuition that high-confidence answers should produce consistent, high-quality explanations across multiple samples. When an LLM is uncertain about an answer, the generated explanations will vary more widely in quality and content. By measuring the entailment probability of each explanation and using these as weights, the method captures this variation in explanation quality as a proxy for answer uncertainty. This creates a more nuanced confidence estimate than simple probability thresholds, as it accounts for the semantic consistency of supporting explanations rather than just the likelihood of specific tokens.

## Foundational Learning

**Entailment Probability** - Measures how well one statement follows from another
*Why needed:* Provides a semantic measure of explanation quality beyond simple word matching
*Quick check:* Test with pairs where entailment is clear (A: "X is Y", B: "X is Y") versus pairs where it's not

**Posterior Predictive Distribution** - Probability distribution over possible answers given observed data
*Why needed:* Allows proper marginalization over multiple explanations to compute final confidence
*Quick check:* Verify that summing weighted probabilities equals 1 across all possible answers

**Selective Prediction** - Model abstains from answering when uncertainty is high
*Why needed:* The evaluation framework assumes models can choose not to answer
*Quick check:* Implement simple threshold-based abstention rule and measure coverage-accuracy trade-off

## Architecture Onboarding

**Component Map:** Question -> Multiple Explanations -> Entailment Scoring -> Weighted Aggregation -> Confidence Score

**Critical Path:** The entailment scoring step is the bottleneck, as it requires computing semantic similarity between each explanation and the ground truth. This step determines both computational cost and final confidence accuracy.

**Design Tradeoffs:** 
- Higher k (number of explanations) improves confidence estimate quality but increases computation linearly
- Using entailment vs. simpler similarity metrics affects both accuracy and runtime
- Choice of entailment model impacts both performance and inference speed

**Failure Signatures:** 
- Low entailment scores across all explanations suggest either poor question understanding or insufficient knowledge
- High variance in entailment scores indicates genuine uncertainty in the answer
- Consistently high entailment scores with low confidence suggest issues in the aggregation mechanism

**3 First Experiments:**
1. Vary k from 5 to 100 explanations and measure AURC improvement vs. computation time
2. Compare different entailment models (BERTScore, BLEURT, human judgment) on a validation set
3. Test on questions where the answer is known but explanations should vary (e.g., "What is 2+2?")

## Open Questions the Paper Calls Out
None

## Limitations
- Method effectiveness depends heavily on the quality of explanation generation, creating circular dependency
- Substantial computational overhead from generating multiple explanations per answer
- Evaluation focuses on selective prediction scenarios, not on whether confidence estimates improve actual decision-making

## Confidence

**Major Claim Confidence:**
- Method outperforms baselines on selective uncertainty tasks: High confidence
- Stable explanations provide more principled uncertainty quantification than token probabilities: Medium confidence
- Method scales to complex reasoning tasks: Low confidence

## Next Checks

1. Conduct ablation studies varying the number of explanation samples (k) to quantify the trade-off between computational cost and confidence estimate quality
2. Evaluate performance on multi-step reasoning tasks requiring chained logical inferences to test robustness on more complex problems
3. Test whether confidence estimates improve human decision-making in practical applications by conducting user studies comparing different uncertainty visualization methods