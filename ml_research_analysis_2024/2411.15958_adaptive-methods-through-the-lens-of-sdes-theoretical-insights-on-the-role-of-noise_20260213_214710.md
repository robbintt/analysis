---
ver: rpa2
title: 'Adaptive Methods through the Lens of SDEs: Theoretical Insights on the Role
  of Noise'
arxiv_id: '2411.15958'
source_url: https://arxiv.org/abs/2411.15958
tags:
- page
- loss
- lemma
- noise
- signsgd
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides the first formal SDE model for SignSGD, demonstrating
  its dynamics traverse three phases influenced by signal-to-noise ratio. The analysis
  reveals that noise inversely affects convergence speed of SignSGD, unlike SGD where
  noise only impacts asymptotic loss quadratically.
---

# Adaptive Methods through the Lens of SDEs: Theoretical Insights on the Role of Noise

## Quick Facts
- arXiv ID: 2411.15958
- Source URL: https://arxiv.org/abs/2411.15958
- Reference count: 40
- Primary result: First formal SDE model for SignSGD showing noise inversely affects convergence speed, unlike SGD; extends analysis to AdamW and RMSpropW with novel scaling rules

## Executive Summary
This paper introduces the first formal stochastic differential equation (SDE) model for SignSGD, demonstrating that its dynamics traverse three distinct phases influenced by signal-to-noise ratio. The analysis reveals that noise inversely affects SignSGD's convergence speed, contrasting with SGD where noise only impacts asymptotic loss quadratically. The framework is extended to AdamW and RMSpropW, deriving novel scaling rules and showing that decoupled weight decay plays a crucial stabilization role at high noise levels near the minimizer. Theoretical insights are validated through extensive experiments across multiple neural network architectures.

## Method Summary
The paper derives continuous-time SDE models for adaptive optimization methods by matching the first two moments of the discrete optimizer updates. Using Milstein's theorem, the authors verify that these SDEs provide order 1 weak approximations of the discrete optimizers. The analysis focuses on three key optimizers: SignSGD, AdamW, and RMSpropW. For each, the paper derives stationary distributions, analyzes convergence dynamics across three phases, and derives scaling rules for hyperparameters. Validation is performed through Euler-Maruyama numerical integration and empirical comparison on synthetic convex functions and real neural networks including MLPs, CNNs, ResNets, and Transformers.

## Key Results
- Novel SDE model for SignSGD reveals inverse relationship between noise and convergence speed, contrasting with SGD's quadratic noise impact on asymptotic loss
- AdamW's decoupled weight decay provides crucial stabilization against high noise levels, bounding stationary covariance while Adam without weight decay shows linear noise growth
- Novel scaling rules derived for batch size, learning rate, and weight decay for AdamW and RMSpropW, validated across multiple architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The SDE provides an order 1 weak approximation of the discrete optimizer updates, meaning its expected values match up to error O(η).
- Mechanism: The drift and diffusion terms of the SDE are constructed so that their first two moments match those of the optimizer's one-step difference, satisfying the conditions of Milstein's theorem.
- Core assumption: The loss function f, its gradients, and the gradient noise have sufficient regularity (Lipschitz, bounded derivatives, polynomial growth) and the noise density is smooth and integrable.
- Evidence anchors:
  - [abstract] "Our SDEs accurately track the behavior of the respective optimizers, especially when compared to previous SDEs derived for Adam and RMSprop."
  - [section] Theorem C.7 and the proof of Theorem C.16 explicitly verify the weak approximation conditions.
  - [corpus] Related work section notes that most SDE derivations lack empirical validation; this paper validates on multiple architectures.
- Break condition: If gradient noise lacks smoothness or has unbounded variance not handled by the model, the weak approximation fails and the SDE no longer tracks the optimizer.

### Mechanism 2
- Claim: Noise affects SignSGD's convergence speed inversely, unlike SGD where noise only affects asymptotic loss quadratically.
- Mechanism: In SignSGD, the sign operation makes the effective step size inversely proportional to the noise level; higher noise means smaller effective steps and slower convergence, while in SGD the noise only perturbs the direction but not the magnitude of the step.
- Core assumption: The noise covariance Σ is bounded and the gradient is Lipschitz; the analysis assumes strong convexity or PL condition for clean bounds.
- Evidence anchors:
  - [abstract] "Our novel analysis of SignSGD highlights a noteworthy and precise contrast to SGD in terms of convergence speed, stationary distribution, and robustness to heavy-tail noise."
  - [section] Lemma 3.5 and Lemma 3.6 explicitly compare convergence rates and asymptotic loss scaling between SignSGD and SGD.
  - [corpus] Related works like Jastrzebski et al. and Malladi et al. derive SDEs but do not highlight this inverse relationship.
- Break condition: If the noise is extremely heavy-tailed with infinite variance, the inverse relationship may break down unless the model is adjusted as in Corollary 3.11.

### Mechanism 3
- Claim: Decoupled weight decay in AdamW stabilizes the optimizer against high noise levels near the minimizer, unlike Adam without weight decay where loss grows linearly with noise.
- Mechanism: The weight decay term θXt in the SDE acts as a regularization force that counteracts the destabilizing effect of high gradient noise, bounding the stationary covariance and loss; without it, the noise term scales the loss unboundedly.
- Core assumption: The weight decay parameter θ > 0 and the noise covariance Σ are bounded; the analysis assumes strong convexity for clean stationary distribution formulas.
- Evidence anchors:
  - [abstract] "Due to an intricate interaction between noise, curvature, and regularization, decoupled weight decay plays a crucial stabilization role at high noise levels near the minimizer."
  - [section] Lemma 3.13 and Lemma 3.14 show that AdamW's asymptotic loss is bounded in σ while Adam's is linear in σ; Figure 6 confirms experimentally.
  - [corpus] Previous works on AdamW (Zhou et al. 2024, Zhou et al. 2022) do not emphasize this stabilization role explicitly.
- Break condition: If θ is set to zero or too small relative to noise level, the stabilizing effect disappears and loss grows unboundedly.

## Foundational Learning

- Concept: Stochastic Differential Equations (SDEs) and Itô calculus
  - Why needed here: The entire theoretical framework models optimizer dynamics as continuous-time stochastic processes; understanding drift, diffusion, and Itô's lemma is essential to derive and interpret the SDEs.
  - Quick check question: What is the difference between an Itô integral and a Riemann integral in the context of stochastic processes?

- Concept: Weak approximation and numerical integration of SDEs
  - Why needed here: The paper claims the SDEs are order 1 weak approximations; verifying this requires understanding moment matching and Euler-Maruyama discretization.
  - Quick check question: In the context of SDE approximation, what does it mean for two processes to have matching first two moments up to O(η²)?

- Concept: Stationary distributions and convergence analysis
  - Why needed here: The analysis derives stationary distributions for optimizers near minima and compares convergence speeds; this requires familiarity with ergodicity and invariant measures.
  - Quick check question: How does the presence of a diffusion term in an SDE affect the stationary distribution compared to a deterministic ODE?

## Architecture Onboarding

- Component map:
  Optimizer -> Discrete update equations (SignSGD, AdamW, RMSpropW) -> SDE -> Continuous-time model with drift and diffusion terms -> Validation -> Euler-Maruyama integration + empirical comparison on multiple architectures -> Scaling rules -> Hyperparameter adjustments preserving loss bounds or dynamics

- Critical path:
  1. Derive SDE from optimizer update (match moments)
  2. Verify weak approximation conditions (Milstein's theorem)
  3. Analyze SDE for convergence, stationary distribution, and noise effects
  4. Validate SDE against optimizer on toy landscapes and real DNNs
  5. Use insights to derive scaling rules for batch size/learning rate

- Design tradeoffs:
  - Simplicity vs accuracy: Simpler SDEs (e.g., ignoring gradient size) are easier to analyze but less accurate globally
  - Assumptions vs generality: Stronger regularity assumptions give cleaner theory but may exclude realistic noise structures
  - Empirical vs theoretical: Theoretical insights guide hyperparameter tuning but must be validated experimentally

- Failure signatures:
  - SDE diverges from optimizer trajectory on non-convex landscapes or large learning rates
  - Theoretical bounds on loss do not match empirical results (indicates wrong assumptions or missing terms)
  - Scaling rules derived from SDE do not improve generalization in large-scale experiments

- First 3 experiments:
  1. Run SignSGD and its SDE on a simple convex quadratic (e.g., f(x) = x⊤Hx/2) with Gaussian noise; compare trajectories and loss curves.
  2. Validate AdamW's SDE on a small MLP on MNIST; compare with Adam without weight decay to observe noise stabilization.
  3. Test the derived scaling rule for AdamW on a medium-sized transformer (e.g., 160M params) by scaling batch size by 4 and adjusting learning rate, β, and weight decay as prescribed.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the dynamics of SignSGD change when using non-stationary noise structures or time-varying learning rates?
- Basis in paper: [explicit] The paper analyzes SignSGD under Gaussian noise with constant covariance and discusses heavy-tailed noise with infinite variance, but doesn't explore time-varying noise structures.
- Why unresolved: The current analysis focuses on stationary noise assumptions, leaving open how adaptive methods behave under dynamic noise conditions common in real-world training scenarios.
- What evidence would resolve it: Experimental validation comparing SignSGD performance under time-varying noise distributions versus stationary assumptions, showing how convergence rates and asymptotic loss values change.

### Open Question 2
- Question: Can the scaling rules for AdamW and RMSpropW be generalized to non-convex loss functions beyond convex quadratics?
- Basis in paper: [explicit] The scaling rules are derived and validated for strongly convex functions, but the authors acknowledge that generalization to non-convex settings remains an open problem.
- Why unresolved: The current theoretical framework relies on convexity assumptions that don't hold for deep learning loss landscapes, limiting practical applicability.
- What evidence would resolve it: Empirical studies showing how batch size scaling affects convergence and generalization in non-convex settings across different neural network architectures.

### Open Question 3
- Question: What is the exact mechanism by which decoupled weight decay stabilizes AdamW and RMSpropW at high noise levels near the minimizer?
- Basis in paper: [explicit] The paper observes that decoupled weight decay plays a crucial stabilization role at high noise levels, but the precise mathematical mechanism remains unclear.
- Why unresolved: While the empirical observation is clear, the theoretical connection between weight decay regularization and noise resilience near minima requires deeper mathematical analysis.
- What evidence would resolve it: Detailed analysis of the interaction between curvature, noise, and regularization terms in the SDEs, possibly through higher-order approximations or alternative analytical techniques.

## Limitations
- The analysis relies on strong convexity and bounded gradient noise assumptions that may not hold in highly non-convex deep learning landscapes.
- The weak approximation conditions are formally verified but the practical relevance of O(η) errors in long training runs remains uncertain.
- The heavy-tail noise extensions use theoretical tools (Garsia-Rodemich-Rumsey) that may not fully capture real-world gradient noise distributions.

## Confidence
- **High Confidence**: The SDE derivation methodology and weak approximation proofs (Mechanism 1) are mathematically rigorous and empirically validated across multiple architectures.
- **Medium Confidence**: The inverse noise-convergence relationship for SignSGD (Mechanism 2) is supported by theory and experiments but may be sensitive to noise structure assumptions.
- **Medium Confidence**: The stabilization role of weight decay in AdamW (Mechanism 3) is theoretically proven and experimentally shown, though the practical impact depends on hyperparameter choices.

## Next Checks
1. **Non-Convex Validation**: Test the SDEs on more challenging non-convex tasks like language modeling or object detection to assess whether the theoretical insights hold beyond convex approximations.
2. **Noise Structure Sensitivity**: Conduct controlled experiments varying noise distribution (Gaussian vs heavy-tailed) to validate the robustness of the inverse noise-convergence relationship for SignSGD.
3. **Scaling Rule Transfer**: Apply the derived scaling rules for batch size and weight decay to much larger models (e.g., ViT-Huge) to verify that the theoretical guidance generalizes to state-of-the-art architectures.