---
ver: rpa2
title: Distilling Text Style Transfer With Self-Explanation From LLMs
arxiv_id: '2403.01106'
source_url: https://arxiv.org/abs/2403.01106
tags:
- text
- transfer
- style
- association
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes CoTeX, a framework that leverages large language
  models (LLMs) and chain-of-thought prompting to improve text style transfer (TST).
  CoTeX distills the rewriting and reasoning capabilities of LLMs into smaller models,
  enabling TST on both non-parallel and parallel data.
---

# Distilling Text Style Transfer With Self-Explanation From LLMs

## Quick Facts
- arXiv ID: 2403.01106
- Source URL: https://arxiv.org/abs/2403.01106
- Authors: Chiyu Zhang; Honglong Cai; Yuezhang; Li; Yuexin Wu; Le Hou; Muhammad Abdul-Mageed
- Reference count: 33
- Key outcome: CoTeX improves text style transfer performance by 5.5% BLEU on average, especially in low-resource settings, while providing interpretable chain-of-thought rationales

## Executive Summary
CoTeX introduces a novel approach to text style transfer (TST) that leverages large language models (LLMs) and chain-of-thought prompting to generate both transferred text and explicit reasoning paths. The method distills these capabilities into smaller, more efficient models, enabling high-quality style transfer even with limited parallel training data. By providing transparent explanations for the transformation process, CoTeX addresses both the performance and interpretability challenges in TST tasks.

## Method Summary
CoTeX uses chain-of-thought (CoT) prompting with LLMs to generate synthetic parallel data for text style transfer. The LLM is prompted with few-shot examples to produce both a CoT rationale explaining the style transformation and the final transferred text. This synthetic data (source + CoT + target) is then used to train smaller T5-based student models through supervised fine-tuning or knowledge distillation. The approach works in both target-blind (TB) and target-aware (TA) settings, with TB being more practical as it doesn't require gold target text during training.

## Key Results
- CoTeX outperforms traditional supervised fine-tuning by 5.5% BLEU on average across four TST datasets
- Achieves superior performance in low-resource settings (1K-20K examples) compared to other methods
- Provides interpretable CoT rationales, with 74% of formality transfer examples rated as acceptable (Rate A or B) by human evaluators

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CoTeX leverages chain-of-thought prompting to explicitly generate reasoning paths for text style transfer, which are then distilled into smaller models.
- Mechanism: The LLM generates rationales (CoT paths) explaining how to transform a source text into a target style while preserving content. These rationales are then used as additional supervision when fine-tuning smaller student models, providing explicit reasoning signals alongside the final transferred text.
- Core assumption: The reasoning paths capture the essential transformation steps and can be effectively learned by smaller models through distillation.
- Evidence anchors:
  - [abstract]: "CoTeX distinguishes itself by offering transparent explanations for its style transfer process"
  - [section 2.1]: "We employ CoT combined with instruction prompting to extract rationales from LLMs regarding the TST process"
  - [corpus]: Weak - corpus neighbors discuss style transfer but don't specifically address CoT-based reasoning

### Mechanism 2
- Claim: CoTeX improves data efficiency by combining few-shot in-context learning from LLMs with knowledge distillation to train compact models.
- Mechanism: The LLM generates synthetic parallel data (source + CoT + target) through few-shot prompting. This data is then used to train smaller models via supervised fine-tuning or distillation, bypassing the need for expensive human-annotated parallel corpora.
- Core assumption: LLM-generated synthetic data is high-quality enough to serve as effective training signals for smaller models.
- Evidence anchors:
  - [abstract]: "CoTeX distills the complex rewriting and reasoning capabilities of LLMs into more streamlined models"
  - [section 2.1]: "We employ CoT combined with instruction prompting to extract rationales from LLMs regarding the TST process"
  - [corpus]: Moderate - corpus mentions "Are Large Language Models Actually Good at Text Style Transfer?" which supports the premise that LLMs can handle TST tasks

### Mechanism 3
- Claim: CoTeX provides interpretability through the generation of explicit CoT rationales alongside transferred text.
- Mechanism: By prompting the LLM to generate step-by-step reasoning before producing the transferred text, CoTeX creates human-readable explanations of the style transfer process. These explanations can then be inspected to understand how the transformation was performed.
- Core assumption: The CoT rationales accurately reflect the actual reasoning behind the style transfer and are not just post-hoc rationalizations.
- Evidence anchors:
  - [abstract]: "CoTeX distinguishes itself by offering transparent explanations for its style transfer process"
  - [section 2.1]: "We employ CoT combined with instruction prompting to extract rationales from LLMs regarding the TST process"
  - [corpus]: Weak - corpus doesn't specifically address interpretability or explainability in TST

## Foundational Learning

- Concept: Chain-of-thought prompting
  - Why needed here: Enables extraction of explicit reasoning from LLMs for style transfer transformations
  - Quick check question: What is the purpose of adding "Let's break down the rewriting process step by step" to the LLM prompt?

- Concept: Knowledge distillation
  - Why needed here: Transfers capabilities from large LLMs to smaller, more efficient models while maintaining performance
  - Quick check question: How does knowledge distillation differ from standard supervised fine-tuning in this context?

- Concept: Few-shot in-context learning
  - Why needed here: Allows LLMs to generate high-quality synthetic data without parameter updates
  - Quick check question: Why does the paper use 3 manually crafted examples as few-shot prompts rather than training the LLM?

## Architecture Onboarding

- Component map:
  - LLM (teacher) -> Data generator -> Student model (T5) -> Evaluation pipeline

- Critical path:
  1. Few-shot prompt LLM with source text + style instruction
  2. Extract CoT path and transferred text from LLM output
  3. Format synthetic data (source + CoT + target)
  4. Fine-tune T5 model on synthetic data
  5. Evaluate on test set using BLEU and human evaluation

- Design tradeoffs:
  - LLM choice: Larger LLMs (like PaLM2 Unicorn) provide better synthetic data but increase generation cost
  - CoT vs. direct transfer: CoT provides interpretability but may increase generation time
  - Synthetic data quantity: More examples per source text improves performance but increases storage/compute requirements

- Failure signatures:
  - BLEU scores drop dramatically on test sets
  - Human evaluation shows low quality CoT rationales (Rate C/D)
  - Student model overfits to synthetic data patterns
  - Generated text preserves source style instead of target style

- First 3 experiments:
  1. Generate synthetic data for one source text and verify CoT structure and transferred text quality
  2. Train T5 on 100 synthetic examples and evaluate on a small validation set
  3. Compare performance of CoTeX-TB vs SFT on a low-resource dataset subset (1K examples)

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions but raises several through its discussion and limitations section, including: How does the quality of CoT rationales compare to human-written explanations? What is the optimal number of synthetic examples per source text? How does CoTeX perform on more complex style transfer tasks beyond formality, detoxification, and Shakespearean English? What is the relationship between student model size and distillation effectiveness?

## Limitations
- The exact instruction templates for CoT prompting are not fully specified, limiting reproducibility
- Evaluation relies primarily on BLEU scores, which may not fully capture style transfer quality or content preservation
- Human evaluation of CoT rationales lacks detailed criteria and inter-annotator agreement metrics

## Confidence

- **High confidence**: The core mechanism of using LLM-generated CoT paths as training signals for smaller models is technically sound and well-supported by the literature on knowledge distillation and chain-of-thought prompting.
- **Medium confidence**: The performance improvements reported (5.5% BLEU increase on average) are likely accurate for the specific datasets and evaluation setup used, though generalizability to other TST tasks remains uncertain.
- **Low confidence**: The interpretability claims regarding CoT rationales are difficult to verify without access to the actual generated examples and detailed human evaluation criteria.

## Next Checks

1. **Replicate synthetic data generation**: Generate CoT paths and transferred text for 50 source examples using the described few-shot prompting approach. Evaluate the quality and consistency of the CoT rationales using Rate A-D criteria from the paper.

2. **Ablation study on CoT supervision**: Train T5 models with identical hyperparameters but different training data: (a) synthetic data with CoT paths, (b) synthetic data without CoT paths, (c) original parallel data. Compare BLEU scores to quantify the contribution of CoT supervision.

3. **Cross-dataset generalization test**: Apply the best-performing CoTeX model (trained on GYAFC) to the Shakespeare-to-Modern English dataset without fine-tuning. Measure BLEU score and analyze failure patterns to assess model adaptability to new style transfer domains.