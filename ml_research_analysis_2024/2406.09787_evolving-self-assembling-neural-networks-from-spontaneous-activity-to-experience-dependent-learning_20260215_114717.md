---
ver: rpa2
title: 'Evolving Self-Assembling Neural Networks: From Spontaneous Activity to Experience-Dependent
  Learning'
arxiv_id: '2406.09787'
source_url: https://arxiv.org/abs/2406.09787
tags:
- neural
- networks
- learning
- nodes
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Lifelong Neural Developmental Programs (LNDPs),
  an extension of Neural Developmental Programs that enables self-assembling neural
  networks to learn and adapt throughout their lifetime through both synaptic and
  structural plasticity. The method combines graph transformer layers for self-organization
  with GRUs for modeling neural and synaptic dynamics, allowing networks to grow from
  randomly connected or empty initial states.
---

# Evolving Self-Assembling Neural Networks: From Spontaneous Activity to Experience-Dependent Learning

## Quick Facts
- arXiv ID: 2406.09787
- Source URL: https://arxiv.org/abs/2406.09787
- Reference count: 6
- Key outcome: LNDPs enable self-assembling neural networks to learn and adapt throughout lifetime through synaptic and structural plasticity, achieving strong performance on control tasks with particular advantages in non-stationary environments

## Executive Summary
This paper introduces Lifelong Neural Developmental Programs (LNDPs), an extension of Neural Developmental Programs that enables self-assembling neural networks to learn and adapt throughout their lifetime through both synaptic and structural plasticity. The method combines graph transformer layers for self-organization with GRUs for modeling neural and synaptic dynamics, allowing networks to grow from randomly connected or empty initial states. Pre-experience development is enabled through spontaneous activity of sensory neurons. Results demonstrate that LNDPs can solve control tasks (Cartpole, Acrobot, Pendulum, Foraging) and that structural plasticity particularly improves performance in environments requiring fast adaptation or with non-stationary rewards. The approach bridges the gap between indirect developmental encodings and meta-learned plasticity rules, showing promise for creating more adaptive artificial neural networks.

## Method Summary
LNDPs combine graph neural networks with developmental biology principles to create neural networks that can grow, adapt, and learn throughout their lifetime. The architecture uses graph transformer layers to enable nodes to perceive the complete network state and differentiate based on structural features, while GRUs model both node and edge dynamics. Structural plasticity is implemented through learnable synaptogenesis and pruning functions that probabilistically add or remove edges based on local node states and reward signals. A novel spontaneous activity phase allows for pre-experience development, where sensory neurons generate Ornstein-Uhlenbeck stochastic activity patterns that enable the network to self-organize before receiving environmental feedback. The entire system is optimized using CMA-ES, an evolutionary strategy well-suited for the non-differentiable components of the architecture.

## Key Results
- LNDPs successfully solve four control tasks (Cartpole, Acrobot, Pendulum, Foraging) with average returns meeting or exceeding optimal performance thresholds
- Structural plasticity significantly improves performance in environments requiring fast adaptation or with non-stationary rewards
- Spontaneous activity-driven pre-experience development enables LNDPs to start with better initial organization compared to empty or randomly initialized networks
- LNDPs demonstrate superior sample efficiency compared to traditional reinforcement learning methods, particularly when combining synaptic and structural plasticity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structural plasticity enables fast adaptation in non-stationary environments by allowing the network to modify its topology during lifetime.
- Mechanism: The model uses synaptogenesis and pruning functions (f^+ and f^-) that probabilistically add or remove edges based on local node states and reward signals, enabling the network to rewire itself in response to environmental changes.
- Core assumption: The local node states contain sufficient information about the global network utility to guide structural changes effectively.
- Evidence anchors:
  - [abstract] "structural plasticity particularly improves performance in environments requiring fast adaptation or with non-stationary rewards"
  - [section] "f^+_θ: H^2 → [0,1] is the synaptogenesis function which defines the probability of adding an edge... f^-_θ: E → [0,1] is the pruning function"
  - [corpus] Weak evidence - the corpus neighbors focus on synaptic plasticity in spiking networks but don't directly address structural plasticity in graph-based networks
- Break condition: If local node states become too correlated or homogeneous, the structural changes may not effectively target useful network modifications

### Mechanism 2
- Claim: Spontaneous activity-driven development allows pre-experience organization of neural networks, enabling better initial performance.
- Mechanism: A learnable Ornstein-Uhlenbeck stochastic process generates spontaneous activity patterns for input neurons during a pre-environmental phase, allowing the network to self-organize before receiving environmental feedback.
- Core assumption: Spontaneous activity patterns contain sufficient statistical structure to guide meaningful network organization without external supervision.
- Evidence anchors:
  - [abstract] "pre-experience development is enabled through spontaneous activity of sensory neurons"
  - [section] "We model SA with a simple learnable stochastic process of sensory neurons... Using spontaneous activity for pre-experience development enables the use of the same synaptic plasticity mechanisms"
  - [corpus] Moderate evidence - some corpus papers discuss spontaneous activity in developing biological networks, supporting the biological plausibility
- Break condition: If the spontaneous activity patterns are too random or lack sufficient structure, the pre-experience development may not provide meaningful organization

### Mechanism 3
- Claim: Graph transformer layers enable self-organization by allowing nodes to perceive the complete network state and differentiate based on structural features.
- Mechanism: Nodes use graph transformer layers to aggregate information from the entire graph, including structural features like degrees and node types, enabling them to make informed decisions about division, connections, and differentiation.
- Core assumption: The complete graph state contains sufficient information for nodes to make globally coherent decisions despite only having local computational capabilities.
- Evidence anchors:
  - [section] "Nodes are updated using Gated Recurrent Units (GRUs) and take as input the output of a GT layer... which enables nodes to 'perceive' the network and self-organize"
  - [section] "The graph transformer layer is used to model node-node interactions allowing them to self-organize and differentiate"
  - [corpus] Weak evidence - the corpus neighbors don't directly address graph transformer architectures in self-organizing networks
- Break condition: If the graph transformer layer becomes too shallow or the aggregation function is insufficient, nodes may not perceive enough global information to make effective decisions

## Foundational Learning

- Concept: Graph neural networks and message passing
  - Why needed here: The entire architecture is built on graph structures where nodes need to communicate and update their states based on neighborhood information
  - Quick check question: How does a graph transformer layer differ from a standard graph convolutional network in terms of information aggregation?

- Concept: Recurrent neural networks and gating mechanisms
  - Why needed here: Both node states and edge states are modeled using GRUs to capture temporal dynamics and maintain memory of past states
  - Quick check question: What advantage does using GRUs for edge states provide compared to simple feedforward updates?

- Concept: Evolutionary optimization strategies
  - Why needed here: The model parameters are optimized using CMA-ES, an evolutionary strategy that works well for non-differentiable or black-box optimization problems
- Quick check question: Why might evolutionary strategies be preferred over gradient-based methods for optimizing LNDP parameters?

## Architecture Onboarding

- Component map: Graph structure (nodes, edges, adjacency matrix) -> Node model (graph transformer layer + GRU) -> Edge model (GRU with reward input) -> Structural plasticity (synaptogenesis and pruning functions) -> Network dynamics (activation update function) -> Spontaneous activity (Ornstein-Uhlenbeck process)

- Critical path: Node state update → Edge state update → Structural plasticity decisions → Network activation update → Reward computation

- Design tradeoffs:
  - Using graph transformers vs simpler GNNs: Better self-organization capability vs higher computational cost
  - GRUs vs other RNN variants: Better temporal modeling vs more parameters
  - Evolutionary optimization vs gradient methods: Better for non-differentiable components vs slower convergence

- Failure signatures:
  - Network collapse: All nodes converge to similar states, losing differentiation
  - Structural instability: Constant rewiring without settling into functional patterns
  - Reward insensitivity: Structural changes don't correlate with reward improvements

- First 3 experiments:
  1. Verify basic graph transformer functionality by checking node state diversity after several updates
  2. Test spontaneous activity generation by visualizing input neuron activation patterns over time
  3. Validate structural plasticity by measuring edge creation/removal rates under different reward conditions

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the balance between structural plasticity and synaptic plasticity change across different environmental conditions?
  - Basis in paper: [explicit] The paper states "structural plasticity is advantageous in environments necessitating fast adaptation or with non-stationary rewards" but doesn't quantify this relationship
  - Why unresolved: The paper shows structural plasticity helps in specific cases but doesn't systematically analyze when and how much it's beneficial
  - What evidence would resolve it: Comparative studies varying the ratio of structural to synaptic plasticity across a wide range of environments with different adaptation requirements

- **Open Question 2**: What is the optimal duration and intensity of spontaneous activity phases for pre-experience development?
  - Basis in paper: [explicit] The paper mentions "T SA developmental steps" as a hyperparameter but doesn't explore its impact systematically
  - Why unresolved: The paper demonstrates spontaneous activity helps but doesn't investigate how different durations or patterns affect learning outcomes
  - What evidence would resolve it: Experiments varying T SA and spontaneous activity patterns across different environments and measuring resulting network capabilities

- **Open Question 3**: How do LNDPs scale to more complex environments with higher-dimensional observations and action spaces?
  - Basis in paper: [explicit] The paper acknowledges "future research must determine how they scale to more complex ones with higher dimensions"
  - Why unresolved: All experiments were conducted on relatively simple control tasks; scalability to complex domains remains untested
  - What evidence would resolve it: Demonstrating LNDP performance on challenging benchmarks like Atari games or continuous control tasks with high-dimensional states

## Limitations

- Scalability concerns: The evolutionary optimization approach may face computational challenges when scaling to larger networks or more complex domains with higher-dimensional observations and action spaces
- Limited environment diversity: All experiments were conducted on relatively simple control tasks, leaving questions about performance in more complex or real-world scenarios
- Uncertain biological plausibility: While inspired by developmental biology, the specific mechanisms and their biological relevance remain to be validated against empirical neuroscience findings

## Confidence

- **High confidence**: The basic framework combining graph transformers with GRUs for modeling neural dynamics is technically sound and well-implemented
- **Medium confidence**: The claim that structural plasticity improves performance in non-stationary environments is supported by experiments, but the causal mechanism could be better isolated
- **Medium confidence**: The spontaneous activity mechanism enables pre-experience development, though the extent of its contribution relative to other factors is unclear

## Next Checks

1. **Ablation study on spontaneous activity phase duration**: Systematically vary the number of developmental steps before environmental interaction to quantify the marginal benefit of pre-experience development across all tasks.

2. **Transfer learning experiment**: Train LNDPs on one environment, then evaluate their performance when deployed in a different but related environment (e.g., Cartpole to Acrobot) to test generalization of learned developmental programs.

3. **Non-stationary reward test**: Design a variant of the foraging task where reward locations change periodically, and measure how quickly LNDPs with structural plasticity adapt compared to those without.