---
ver: rpa2
title: Sharing Knowledge in Multi-Task Deep Reinforcement Learning
arxiv_id: '2401.09561'
source_url: https://arxiv.org/abs/2401.09561
tags:
- learning
- tasks
- task
- multi-task
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies multi-task reinforcement learning by sharing
  representations among tasks, proving theoretical benefits and showing empirical
  gains over single-task methods. The authors derive finite-time bounds for Approximate
  Value Iteration and Policy Iteration in the multi-task setting, building on results
  by Maurer et al.
---

# Sharing Knowledge in Multi-Task Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2401.09561
- Source URL: https://arxiv.org/abs/2401.09561
- Reference count: 40
- Key outcome: First general theoretical justification for representation sharing in multi-task RL, showing sample efficiency gains over single-task methods

## Executive Summary
This paper establishes theoretical foundations for sharing representations in multi-task reinforcement learning and demonstrates significant empirical gains. The authors extend Approximate Value Iteration and Policy Iteration frameworks to the multi-task setting, proving that the cost of learning shared representations decreases with the number of tasks. They propose a neural network architecture with task-specific input mappings, shared hidden layers, and task-specific output heads, implementing multi-task variants of DQN and DDPG. Experiments on OpenAI Gym and MuJoCo environments show substantial improvements in sample efficiency and performance compared to single-task baselines.

## Method Summary
The approach combines theoretical analysis with practical algorithm design. Theoretically, the authors extend AVI and API frameworks to multi-task RL, deriving finite-time bounds that show the approximation error cost decreases with O(1/√T) as the number of tasks increases. Practically, they implement a neural network architecture where each task maps its input to shared hidden layers that extract common features, which are then specialized through task-specific output heads. They create multi-task variants of DQN and DDPG that use this shared architecture, storing transitions separately for each task while learning a common representation across all tasks.

## Key Results
- Theoretical bounds show the cost of learning shared representations decreases with O(1/√T) as task count increases
- Multi-task DQN and DDPG variants achieve significant improvements in sample efficiency compared to single-task baselines
- Shared network architecture provides better feature extraction and transfer learning capabilities across tasks
- Performance gains demonstrated on challenging RL benchmarks including OpenAI Gym and MuJoCo environments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shared representations reduce approximation error propagation by learning a more general feature space
- Mechanism: The theoretical bounds show the cost of learning the shared representation decreases with a factor O(1/√T), where T is the number of tasks. As more tasks are added, the shared representation becomes more efficient at capturing common features, reducing approximation error at each iteration.
- Core assumption: Tasks share common properties that can be captured by a shared representation, and function approximator hypothesis classes have bounded Gaussian complexity
- Evidence anchors: [abstract] "learning from different tasks, sharing common properties, is helpful to generalize the knowledge of them resulting in a more effective feature extraction"; [section] "The bound in Theorem 2 shows that a small approximation error is critical to improve the convergence towards the optimal action-value function, and the bound in Theorem 3 shows that the cost of learning the shared representation at each AVI iteration is mitigated by using multiple tasks."

### Mechanism 2
- Claim: The proposed neural network architecture enables effective multi-task learning
- Mechanism: Task-specific input mappings transform inputs to common feature spaces, shared hidden layers extract common representations, and task-specific output heads specialize the shared representation for each task's specific needs.
- Core assumption: Tasks have inputs and outputs that can be mapped to a shared feature space, and the shared representation can effectively capture common properties
- Evidence anchors: [abstract] "A neural network architecture with task-specific input mappings, shared hidden layers, and task-specific output heads is proposed"; [section] "the network we propose extracts representations wt from inputs xt for each task µt, mapping them to common features in a set of shared layers h, specializing the learning of each task in respective separated layers ft"

### Mechanism 3
- Claim: Multi-task extensions of DQN and DDPG improve sample efficiency and performance
- Mechanism: By using the shared neural network architecture, these algorithms leverage common features extracted from multiple tasks to improve learning, with the shared representation enabling better feature extraction and transfer learning capabilities.
- Core assumption: Single-task variants can be effectively extended to the multi-task setting using the shared architecture, and tasks have common properties that can be leveraged
- Evidence anchors: [abstract] "Results demonstrate significant improvements in sample efficiency and performance, with the shared network providing better feature extraction and transfer learning capabilities compared to single-task baselines"; [section] "The outcome of the empirical analysis joins the theoretical results, showing significant performance improvements compared to the single-task version of the algorithms in various RL problems"

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and their extension to the multi-task setting
  - Why needed here: The paper studies multi-task reinforcement learning involving learning from multiple MDPs simultaneously
  - Quick check question: What is the difference between a single MDP and the multi-task MDP setting described in the paper?

- Concept: Approximate Value Iteration (AVI) and Approximate Policy Iteration (API) frameworks
  - Why needed here: The paper extends these frameworks to the multi-task setting and derives theoretical bounds
  - Quick check question: How do AVI and API frameworks differ from their exact counterparts, and why are they used in practice?

- Concept: Gaussian complexity and its role in measuring function class complexity
  - Why needed here: The theoretical analysis relies on Gaussian complexity to measure the complexity of representations learned by the shared neural network
  - Quick check question: What is the relationship between Gaussian complexity and Rademacher complexity, and how is Gaussian complexity used to measure function class complexity?

## Architecture Onboarding

- Component map: Task-specific input mappings (wt) -> Shared hidden layers (h) -> Task-specific output heads (ft) -> Replay memories (per task) -> Loss function (per task) -> Optimizer (shared)

- Critical path: 1) Collect transitions for each task using exploration policy 2) Store transitions in respective replay memories 3) Sample batch of transitions from each replay memory 4) Compute target outputs using Bellman equation 5) Forward propagate through neural network 6) Compute loss as sum of task discrepancies 7) Backpropagate gradients and update parameters 8) Periodically update target network parameters 9) Repeat for fixed iterations or convergence

- Design tradeoffs: Number of shared layers vs. task-specific layers (balance common feature extraction vs. task interference), size of shared representation (capacity vs. computational cost and overfitting risk), exploration strategy (transition distribution affects learned representation quality)

- Failure signatures: Poor individual task performance (shared representation not capturing common features or task-specific layers not specializing adequately), training instability or divergence (learning rate too high, ineffective exploration, or tasks too dissimilar), slow convergence (shared representation not reducing approximation error effectively or tasks too complex)

- First 3 experiments: 1) Implement shared network architecture with minimal shared layers and test on simple two-task problem with shared features 2) Compare multi-task DQN vs. single-task DQN on OpenAI Gym tasks with similar state/action spaces 3) Investigate impact of shared layer count and representation size on multi-task DDPG performance across MuJoCo tasks with similar dynamics

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided.

## Limitations
- Theoretical analysis relies on strong assumptions about task similarity and bounded Gaussian complexity that may not hold in diverse task settings
- Empirical evaluation is limited to specific OpenAI Gym and MuJoCo environments, may not generalize to all multi-task scenarios
- The approach's effectiveness for tasks with minimal shared structure or very different reward functions remains uncertain

## Confidence
- **High confidence**: The theoretical framework for multi-task AVI/API and the neural network architecture design
- **Medium confidence**: The empirical performance improvements on tested benchmarks
- **Low confidence**: Generalization to tasks with minimal shared structure or very different reward functions

## Next Checks
1. Test the approach on tasks with gradually decreasing similarity to identify the breaking point where shared representations no longer provide benefits
2. Evaluate the learned representations through ablation studies, measuring how much performance degrades when removing shared layers
3. Assess the approach's robustness to task imbalance, where some tasks have significantly more samples than others