---
ver: rpa2
title: Task-conditioned adaptation of visual features in multi-task policy learning
arxiv_id: '2402.07739'
source_url: https://arxiv.org/abs/2402.07739
tags:
- task
- tasks
- visual
- adapters
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses multi-task robotic policy learning by introducing
  task-conditioned visual adapters that modulate pre-trained vision features for specific
  downstream tasks. The method uses a single policy trained with behavior cloning,
  augmented with middle and top adapter modules conditioned on task embeddings.
---

# Task-conditioned adaptation of visual features in multi-task policy learning

## Quick Facts
- arXiv ID: 2402.07739
- Source URL: https://arxiv.org/abs/2402.07739
- Reference count: 40
- Achieves competitive performance to single-task policies on known tasks and demonstrates few-shot generalization to new tasks without fine-tuning

## Executive Summary
This paper introduces a novel approach for multi-task robotic policy learning using task-conditioned visual adapters. The method leverages pre-trained CLIP vision features and modulates them through task-specific adapter modules, enabling a single policy to handle multiple tasks effectively. The approach demonstrates both competitive performance on known tasks and the ability to generalize to unseen tasks using only a few demonstrations, addressing a key challenge in robotics where collecting extensive data for every possible task is impractical.

The core innovation lies in the use of task embeddings to condition both middle and top adapter modules, allowing the policy to adapt its visual processing based on the specific task requirements. This design enables the model to share knowledge across tasks while maintaining task-specific capabilities, resulting in improved sample efficiency and generalization performance compared to traditional single-task learning approaches.

## Method Summary
The approach employs a single policy trained with behavior cloning, augmented with middle and top adapter modules conditioned on task embeddings. These embeddings can be selected from a learned space for known tasks or optimized from few demonstrations for unseen tasks. The method uses CLIP pre-trained vision features as input, which are then modulated by the task-conditioned adapters to produce task-specific representations. During inference, for unseen tasks, the model optimizes task embeddings from demonstration data to enable zero-shot adaptation. The policy is trained on a diverse set of tasks and evaluated on both seen and unseen tasks to assess its generalization capabilities.

## Key Results
- Achieves competitive performance to single-task policies on known tasks from CortexBench and MetaWorld
- Demonstrates successful few-shot generalization to 15 unseen tasks with 33% average success rate using only 5 demonstrations
- Shows the importance of task-conditioned visual adaptation for effective multi-task learning

## Why This Works (Mechanism)
The method works by conditioning visual feature processing on task-specific embeddings, allowing the same policy network to adapt its behavior for different tasks. The adapter modules act as task-specific modulators that transform generic visual features into task-relevant representations. This conditioning enables the model to leverage shared visual understanding across tasks while maintaining the ability to perform task-specific actions. The approach effectively creates a continuous space of task embeddings that can be interpolated for novel tasks, enabling generalization beyond the training distribution.

## Foundational Learning
- **Behavior Cloning**: Why needed - to learn policies from expert demonstrations; Quick check - verify demonstration quality and coverage
- **Visual Feature Adaptation**: Why needed - to transform generic visual features into task-specific representations; Quick check - ensure adapter modules learn meaningful transformations
- **Task Embedding Space**: Why needed - to enable continuous interpolation between related tasks; Quick check - validate semantic similarity in embedding space
- **Few-shot Optimization**: Why needed - to adapt to unseen tasks with limited data; Quick check - test convergence speed and stability
- **Multi-task Policy Learning**: Why needed - to share knowledge across tasks while maintaining task-specific capabilities; Quick check - verify performance on both individual and multiple tasks
- **CLIP Feature Extraction**: Why needed - to leverage pre-trained visual representations; Quick check - ensure features capture relevant task information

## Architecture Onboarding

Component map: CLIP features -> Middle adapter -> Policy network -> Top adapter -> Action output

Critical path: Visual input → CLIP feature extraction → Middle adapter (task-conditioned) → Policy network → Top adapter (task-conditioned) → Action output

Design tradeoffs: The approach trades off between task-specific specialization and generalization capability. Using task-conditioned adapters allows sharing across tasks while maintaining specificity, but requires learning a meaningful task embedding space. The few-shot adaptation capability comes at the cost of optimization complexity during inference.

Failure signatures: Poor performance on unseen tasks may indicate inadequate task embedding space or insufficient demonstration data. Suboptimal performance on known tasks could suggest adapter modules are not properly conditioning features or that the task embedding space lacks resolution for fine-grained distinctions.

First experiments:
1. Verify adapter module contributions by testing with only middle adapter, only top adapter, and both adapters
2. Test task embedding space quality by interpolating between known task embeddings and evaluating performance
3. Evaluate the few-shot optimization process by varying the number of demonstrations and measuring adaptation speed

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding the scalability of the approach to larger task spaces, the robustness of the few-shot adaptation process to demonstration quality variations, and the potential for integrating this method with reinforcement learning to improve performance on challenging tasks. The authors also note the need for further investigation into the structure of the learned task embedding space and its ability to capture meaningful task relationships across diverse robotic domains.

## Limitations
- 33% average success rate on unseen tasks indicates limited reliability for real-world deployment
- Performance depends heavily on demonstration quality and distribution for unseen tasks
- Reliance on behavior cloning limits applicability when optimal demonstrations are scarce

## Confidence

| Claim | Confidence |
|-------|------------|
| Competitive performance on known tasks | High |
| Few-shot generalization to unseen tasks | Medium |
| Task-conditioned visual adaptation importance | High |
| CLIP feature adaptation effectiveness | Medium |

## Next Checks

1. Conduct ablation studies on the adapter modules to quantify their individual contributions to overall performance
2. Test the approach with varying numbers of demonstrations (1, 3, 5, 10) for unseen tasks to establish scaling relationships and identify optimal demonstration requirements
3. Evaluate the method on tasks with more diverse visual appearances to test the robustness and generalization capability of the CLIP feature adaptation mechanism