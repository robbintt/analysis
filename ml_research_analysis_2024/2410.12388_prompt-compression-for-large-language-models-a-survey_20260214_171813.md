---
ver: rpa2
title: 'Prompt Compression for Large Language Models: A Survey'
arxiv_id: '2410.12388'
source_url: https://arxiv.org/abs/2410.12388
tags:
- prompt
- methods
- compression
- language
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey reviews prompt compression methods for large language
  models (LLMs), which reduce lengthy prompts to improve memory usage and inference
  costs. It categorizes approaches into hard prompt methods (filtering or paraphrasing
  natural language) and soft prompt methods (learning continuous representations).
---

# Prompt Compression for Large Language Models: A Survey

## Quick Facts
- **arXiv ID**: 2410.12388
- **Source URL**: https://arxiv.org/abs/2410.12388
- **Reference count**: 33
- **Primary result**: Comprehensive survey of prompt compression methods for LLMs, categorizing approaches into hard prompt methods (filtering or paraphrasing natural language) and soft prompt methods (learning continuous representations)

## Executive Summary
This survey provides a comprehensive overview of prompt compression techniques for large language models, addressing the challenges of lengthy prompts that increase memory usage and inference costs. The paper systematically categorizes existing methods into hard prompt approaches that manipulate natural language through filtering or paraphrasing, and soft prompt approaches that learn continuous representations. Key techniques are analyzed across various dimensions including mechanisms, applications, and limitations, with specific attention to methods like SelectiveContext, LLMLingua, Nano-Capsulator for hard prompts, and GIST, AutoCompressor, ICAE, 500xCompressor, xRAG, and UniICL for soft prompts. The survey also identifies future research directions including optimizing compression encoders and combining hard and soft prompt strategies.

## Method Summary
The survey systematically categorizes prompt compression methods into two main approaches: hard prompt methods that directly manipulate natural language through techniques like filtering and paraphrasing, and soft prompt methods that learn continuous representations independent of natural language structure. Hard prompt methods include techniques such as SelectiveContext for context selection, LLMLingua for prompt paraphrasing, and Nano-Capsulator for token reduction. Soft prompt methods encompass approaches like GIST for learning task-specific continuous representations, AutoCompressor for automated compression, ICAE for iterative compression, 500xCompressor for extreme compression ratios, xRAG for retrieval-augmented generation, and UniICL for unified instruction following. The survey analyzes these methods across multiple dimensions including their compression mechanisms, downstream applications, efficiency gains, and inherent limitations.

## Key Results
- Comprehensive categorization of prompt compression methods into hard (natural language manipulation) and soft (continuous representation) approaches
- Identification of specific techniques: SelectiveContext, LLMLingua, and Nano-Capsulator for hard prompts; GIST, AutoCompressor, ICAE, 500xCompressor, xRAG, and UniICL for soft prompts
- Discussion of future research directions including optimization of compression encoders and integration of hard and soft prompt methods
- Analysis of limitations including fine-tuning challenges and variable efficiency gains across different scenarios

## Why This Works (Mechanism)
Prompt compression works by reducing the token count of input prompts while preserving essential information for model performance. Hard prompt methods achieve this by selectively filtering or paraphrasing natural language, maintaining human-readable text that the model can process efficiently. Soft prompt methods bypass natural language constraints entirely, learning compact continuous vector representations that encode task-relevant information directly. These approaches reduce memory footprint and computational overhead during inference, enabling faster processing and lower costs when dealing with lengthy prompts.

## Foundational Learning

**Prompt Compression** - The process of reducing input prompt length while maintaining task performance. Needed to address memory and computational constraints of processing long prompts in LLMs. Quick check: Compare token counts before and after compression.

**Hard Prompts** - Natural language-based compression methods that manipulate text through filtering or paraphrasing. Needed when maintaining human-readable prompts is important for interpretability or debugging. Quick check: Verify compressed prompts remain grammatically coherent.

**Soft Prompts** - Continuous vector representations learned independently of natural language. Needed for achieving higher compression ratios and bypassing token limitations. Quick check: Measure dimensionality reduction compared to original token sequences.

**Context Selection** - Identifying and retaining only the most relevant portions of a prompt. Needed to eliminate redundancy and focus model attention on essential information. Quick check: Evaluate task performance degradation when removing different prompt segments.

**Paraphrasing** - Rewriting prompts using more concise language while preserving meaning. Needed to reduce token count without losing critical information. Quick check: Compare semantic similarity between original and paraphrased versions.

## Architecture Onboarding

**Component Map**: Input Prompt -> Compression Method (Hard/Soft) -> Compressed Prompt -> LLM -> Output

**Critical Path**: The compression method must preserve task-relevant information while significantly reducing token count. The most critical step is ensuring the compressed prompt maintains sufficient context for the LLM to generate accurate outputs.

**Design Tradeoffs**: Hard prompts maintain interpretability but may have limited compression ratios; soft prompts achieve higher compression but lose human readability and may require additional training. The choice depends on application requirements for transparency versus efficiency.

**Failure Signatures**: Compression that removes critical context leads to degraded model performance; over-compression causes information loss; poor paraphrasing introduces ambiguity; continuous representations that are too compact lose task-specific details.

**First Experiments**:
1. Baseline: Measure original prompt length and inference time on target LLM
2. Compression ratio test: Apply different compression methods and quantify token reduction
3. Performance retention: Compare task accuracy before and after compression across multiple methods

## Open Questions the Paper Calls Out
The survey does not explicitly call out open questions, but implies several through its discussion of limitations and future directions. These include optimizing compression encoders for better efficiency, exploring combinations of hard and soft prompt methods, and investigating prompt compression in multimodal LLM contexts.

## Limitations
- May miss emerging techniques developed after the survey's writing period
- Effectiveness of different compression methods across various LLM architectures and domains is not thoroughly validated
- Lacks quantitative comparisons of compression ratios and performance trade-offs between methods

## Confidence

**High confidence**: Categorization of prompt compression into hard and soft prompt methods based on existing literature.

**Medium confidence**: Effectiveness claims of specific techniques like SelectiveContext, LLMLingua, and Nano-Capsulator for hard prompts, as empirical results are not provided.

**Medium confidence**: Claimed efficiency gains and limitations of soft prompt methods, as the survey relies on reported results from original papers without independent verification.

## Next Checks

1. Conduct a quantitative benchmark study comparing compression ratios, inference time reduction, and performance retention across multiple prompt compression methods on standardized datasets and LLM architectures.

2. Perform ablation studies to isolate the impact of different compression components (e.g., paraphrasing vs. filtering for hard prompts, token reduction vs. context preservation for soft prompts) on downstream task performance.

3. Evaluate the generalizability of compression methods across diverse domains (e.g., code generation, medical text analysis) and model sizes to identify domain-specific limitations and opportunities for improvement.