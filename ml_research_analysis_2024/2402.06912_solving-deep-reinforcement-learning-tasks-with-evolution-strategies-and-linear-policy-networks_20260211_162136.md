---
ver: rpa2
title: Solving Deep Reinforcement Learning Tasks with Evolution Strategies and Linear
  Policy Networks
arxiv_id: '2402.06912'
source_url: https://arxiv.org/abs/2402.06912
tags:
- linear
- methods
- policy
- learning
- gradient-based
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares evolution strategies (ES) to gradient-based
  deep reinforcement learning (DRL) methods on classic benchmarks like CartPole, LunarLander,
  MuJoCo robotics, and Atari games. The authors benchmark three ES methods (CSA-ES,
  sep-CMA-ES, CMA-ES), three gradient-based DRL methods (DQN, PPO, SAC), and Augmented
  Random Search (ARS).
---

# Solving Deep Reinforcement Learning Tasks with Evolution Strategies and Linear Policy Networks

## Quick Facts
- arXiv ID: 2402.06912
- Source URL: https://arxiv.org/abs/2402.06912
- Reference count: 40
- Evolution Strategies can find effective linear policies for many RL benchmark tasks, unlike deep RL methods that require much larger networks

## Executive Summary
This paper compares evolution strategies (ES) to gradient-based deep reinforcement learning (DRL) methods on classic benchmarks like CartPole, LunarLander, MuJoCo robotics, and Atari games. The authors benchmark three ES methods (CSA-ES, sep-CMA-ES, CMA-ES), three gradient-based DRL methods (DQN, PPO, SAC), and Augmented Random Search (ARS). They use both standard deep networks and linear networks for DRL methods, while only linear networks are trained for ES and ARS. Results show that ES can find effective linear policies for many tasks where DRL requires much larger networks, and that ES often outperforms ARS while achieving comparable results to DRL for more complex tasks.

## Method Summary
The paper evaluates three ES variants (CSA-ES, sep-CMA-ES, CMA-ES) against three gradient-based DRL methods (DQN, PPO, SAC) and ARS across classic RL benchmarks. All methods optimize policies mapping states to actions, with ES and ARS using only linear networks while DRL methods are tested with both linear and deep networks. The experiments cover classic control tasks (CartPole, LunarLander), continuous control tasks (MuJoCo locomotion), and Atari games. For Atari, ES methods are evaluated using RAM state input rather than pixel images to isolate policy optimization from perception challenges.

## Key Results
- ES methods can find effective linear policies for many RL benchmark tasks where DRL requires much larger networks
- ES outperforms ARS and achieves comparable results to DRL for higher-complexity tasks
- For Atari, ES using RAM state input can find policies competitive with DQN trained on pixel images

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Evolution Strategies can find effective linear policies for many RL benchmark tasks, while gradient-based DRL methods require much larger networks.
- Mechanism: ES performs global search in policy parameter space using isotropic mutations, avoiding local optima traps common in gradient-based methods. This global search is particularly effective for optimizing simple, low-dimensional linear policies.
- Core assumption: The optimization landscape for linear policies in these benchmark tasks is multimodal but not deceptive enough to trap ES in poor local optima.
- Evidence anchors:
  - [abstract] "Evolution Strategies can find effective linear policies for many reinforcement learning benchmark tasks, unlike deep reinforcement learning methods that can only find successful policies using much larger networks"
  - [section] "The ES can almost instantly sample the optimal policy, while the gradient-based methods have a much harder time"
  - [corpus] Weak - no direct corpus evidence comparing ES and DRL on linear policies
- Break Condition: If the optimization landscape becomes highly deceptive or if the linear policy structure cannot represent the optimal solution for a given task.

### Mechanism 2
- Claim: ES methods achieve comparable performance to gradient-based DRL algorithms for higher-complexity tasks.
- Mechanism: Advanced ES variants (CSA-ES, sep-CMA-ES, CMA-ES) adapt the mutation distribution parameters (step size and covariance matrix) to efficiently explore high-dimensional search spaces, allowing them to scale to more complex problems.
- Core assumption: The self-adaptation mechanisms in ES can effectively learn the structure of the optimization landscape in high-dimensional spaces.
- Evidence anchors:
  - [abstract] "Evolution Strategies also achieve results comparable to gradient-based deep reinforcement learning algorithms for higher-complexity tasks"
  - [section] "For our experiments on Atari, we find that by accessing the RAM memory of the Atari controller, ES methods can find a linear policy that is competitive with 'superhuman' DQN"
  - [corpus] Weak - no direct corpus evidence on ES performance on complex tasks
- Break Condition: If the dimensionality of the search space exceeds the computational limits of the ES self-adaptation mechanisms (typically around 100 dimensions for full CMA-ES).

### Mechanism 3
- Claim: ES methods are simpler in design, have fewer hyperparameters, and are trivially parallelizable compared to gradient-based DRL.
- Mechanism: ES only requires evaluating the fitness function (cumulative reward) for each candidate solution, without needing to compute gradients or maintain complex data structures like replay buffers.
- Core assumption: The computational overhead of gradient computation and backpropagation in DRL methods outweighs the additional fitness evaluations required by ES.
- Evidence anchors:
  - [abstract] "Evolution Strategies can be equally effective, are algorithmically simpler, allow smaller network architectures, and are thus easier to implement, understand, and tune"
  - [section] "ES can be more easily parallelized and scaled, offering the possibility for faster convergence in terms of wall-clock time"
  - [corpus] Weak - no direct corpus evidence comparing implementation complexity
- Break Condition: If the problem requires gradient information for efficient optimization or if the fitness evaluation is prohibitively expensive.

## Foundational Learning

- Concept: Reinforcement Learning (RL) as Markov Decision Processes (MDPs)
  - Why needed here: Understanding the RL problem formulation is crucial for interpreting how ES and DRL methods optimize policies.
  - Quick check question: What are the key components of an MDP and how do they relate to the RL problem?

- Concept: Neural network parameter optimization
  - Why needed here: Both ES and DRL methods optimize the parameters of neural network policies, so understanding optimization techniques is essential.
  - Quick check question: What is the difference between gradient-based and gradient-free optimization methods?

- Concept: Evolution Strategies (ES) and covariance matrix adaptation
  - Why needed here: ES methods are the primary focus of the paper, and understanding their mechanisms is crucial for interpreting the results.
  - Quick check question: How do ES methods use self-adaptation to improve their search efficiency?

## Architecture Onboarding

- Component map:
  - Environment (Gymnasium API) -> Policy Network (linear or deep) -> ES/DRL Method -> Cumulative Reward

- Critical path:
  1. Initialize policy network parameters
  2. Evaluate policy in environment to get cumulative reward
  3. Update policy parameters using ES or DRL method
  4. Repeat until convergence or maximum timesteps reached

- Design tradeoffs:
  - Linear vs. deep policy networks: Simplicity and interpretability vs. potential for better performance
  - ES vs. DRL: Global search vs. local gradient-based search, parallelization potential vs. sample efficiency
  - Self-adaptation complexity: Computational overhead vs. search efficiency in high dimensions

- Failure signatures:
  - ES: Premature convergence to suboptimal policies, failure to adapt step size/covariance matrix effectively
  - DRL: Local optima trapping, instability in training, sensitivity to hyperparameters
  - Linear policies: Inability to represent optimal policies for complex tasks

- First 3 experiments:
  1. CartPole-v1 with linear policy using ES and DRL methods to compare convergence speed and final performance
  2. LunarLander-v2 with linear policy to test ES effectiveness on slightly more complex tasks
  3. Atari RAM input with linear policy to evaluate ES performance on high-dimensional problems

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses on relatively simple benchmark environments; performance on more complex, real-world tasks remains untested
- Only linear policies are considered for ES methods, while the DRL methods could potentially benefit from deeper architectures
- Atari experiments use RAM state instead of pixel inputs, which may not reflect typical DRL applications
- No statistical significance testing is reported for performance differences between methods

## Confidence
- **High Confidence:** ES finding effective linear policies for classic control tasks (CartPole, LunarLander)
- **Medium Confidence:** ES performance on high-dimensional MuJoCo tasks with linear policies
- **Medium Confidence:** ES performance on Atari RAM input compared to DQN on pixel inputs
- **Low Confidence:** Generalization to more complex, real-world problems beyond the tested benchmarks

## Next Checks
1. Test ES performance on more complex, procedurally-generated environments to assess scalability beyond fixed benchmarks
2. Compare ES and DRL methods using identical network architectures (e.g., both using linear policies) to isolate the impact of the optimization method
3. Conduct statistical significance testing across multiple random seeds to validate performance differences between methods