---
ver: rpa2
title: Selecting Large Language Model to Fine-tune via Rectified Scaling Law
arxiv_id: '2402.02314'
source_url: https://arxiv.org/abs/2402.02314
tags:
- scaling
- fine-tuning
- arxiv
- language
- phase
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of selecting the most appropriate
  large language model (LLM) for fine-tuning given resource constraints. The authors
  formulate this as a resource-constrained prediction task and draw a natural connection
  to scaling law.
---

# Selecting Large Language Model to Fine-tune via Rectified Scaling Law

## Quick Facts
- **arXiv ID:** 2402.02314
- **Source URL:** https://arxiv.org/abs/2402.02314
- **Reference count:** 40
- **Primary result:** Proposes Rectified Scaling Law with "pre-power phase" discovery and achieves near-optimal LLM selection with hundreds of times less resource consumption

## Executive Summary
This paper addresses the challenge of selecting the most appropriate large language model (LLM) for fine-tuning under resource constraints. The authors formulate this as a resource-constrained prediction task and discover a previously unobserved "pre-power phase" in the fine-tuning scaling curve, in addition to the well-known "power phase." By introducing the concept of "pre-learned data size" into their Rectified Scaling Law, they develop a novel LLM selection algorithm that significantly outperforms existing methods in terms of resource efficiency and prediction accuracy.

## Method Summary
The authors reformulate the LLM selection problem as a resource-constrained prediction task and connect it to scaling law principles. They observe that fine-tuning curves exhibit two distinct phases: a pre-power phase and a power phase, which existing scaling laws fail to capture adequately. To address this, they introduce the concept of "pre-learned data size" into their scaling law formulation. Based on this rectified scaling law, they propose a novel algorithm for selecting the near-optimal LLM to fine-tune given resource constraints. The algorithm predicts full-fine-tuning performance with significantly higher accuracy (62.7% average Pearson correlation) compared to existing methods while consuming hundreds of times less computational resources.

## Key Results
- Discovers "pre-power phase" in fine-tuning scaling curves, complementing the well-known "power phase"
- Achieves average Pearson correlation of 62.7% between predicted and true full-fine-tuning performance
- Outperforms existing methods by hundreds of times in resource efficiency while maintaining prediction quality
- Other methods can provide negatively correlated predictions under demanding constraints

## Why This Works (Mechanism)
The key mechanism is the introduction of "pre-learned data size" into the scaling law formulation, which captures the pre-power phase behavior that previous scaling laws missed. By accounting for the initial performance plateau and subsequent power-law growth in fine-tuning curves, the Rectified Scaling Law provides a more accurate model of how LLMs improve with additional fine-tuning data. This improved modeling enables more reliable predictions of which LLM will perform best under given resource constraints.

## Foundational Learning

**Scaling Laws in Deep Learning**
*Why needed:* Provides the theoretical framework for understanding how model performance scales with data and compute
*Quick check:* Verify that the observed pre-power phase follows a different functional form than the power phase

**Fine-tuning Dynamics**
*Why needed:* Essential for understanding how pre-trained models adapt to new tasks and datasets
*Quick check:* Confirm that the pre-power phase represents a genuine phenomenon across multiple model families

**Resource-Constrained Optimization**
*Why needed:* The core problem being addressed - selecting models under limited compute budgets
*Quick check:* Validate that resource savings are substantial across diverse constraint scenarios

## Architecture Onboarding

**Component Map:** Pre-trained LLM Library -> Resource Estimator -> Scaling Law Model -> Candidate Ranker -> Selection Algorithm

**Critical Path:** Model selection and ranking depends on accurate resource estimation and scaling law predictions

**Design Tradeoffs:** Between prediction accuracy and computational overhead - more accurate predictions require more computation

**Failure Signatures:** Poor resource estimates leading to suboptimal model selection; scaling law mispredictions causing wrong candidate ranking

**3 First Experiments:**
1. Validate the pre-power phase observation across different model architectures
2. Test resource consumption comparison with baseline selection methods
3. Evaluate prediction accuracy under varying constraint tightness levels

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability of pre-power phase observation across diverse model architectures and domains remains uncertain
- Method for estimating pre-learned data size for arbitrary pre-trained models is not fully specified
- 62.7% average Pearson correlation represents moderate predictive capability that may not suffice for mission-critical applications

## Confidence
- Resource efficiency claims: Medium
- Prediction accuracy claims: Medium
- Generalizability claims: Low

## Next Checks
1. Test the algorithm's performance on a broader set of LLMs including different architectures (e.g., LLaMA, Mistral, specialized domain models) to assess architectural generalizability
2. Implement and evaluate the method on real-world constrained fine-tuning scenarios where compute budgets fluctuate dynamically
3. Conduct ablation studies to quantify the relative contribution of the "pre-learned data size" parameter versus other factors in the scaling law formulation