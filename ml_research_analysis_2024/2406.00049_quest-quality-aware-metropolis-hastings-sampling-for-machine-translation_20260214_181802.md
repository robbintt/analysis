---
ver: rpa2
title: 'QUEST: Quality-Aware Metropolis-Hastings Sampling for Machine Translation'
arxiv_id: '2406.00049'
source_url: https://arxiv.org/abs/2406.00049
tags:
- translation
- distribution
- quality
- language
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a quality-aware sampling method for machine
  translation using Metropolis-Hastings MCMC with a novel sentence-level proposal
  distribution. Instead of relying solely on model likelihood, it samples translations
  in proportion to an automatic quality metric (COMET or BLEURT) treated as an energy
  function in a Gibbs distribution.
---

# QUEST: Quality-Aware Metropolis-Hastings Sampling for Machine Translation

## Quick Facts
- **arXiv ID**: 2406.00049
- **Source URL**: https://arxiv.org/abs/2406.00049
- **Reference count**: 22
- **Primary result**: QUEST achieves up to 2-point XCOMET-XL improvements and better COMET-KIWI-XL scores than ancestral sampling

## Executive Summary
QUEST introduces a quality-aware Metropolis-Hastings sampling method for machine translation that generates higher-quality and more diverse translations than traditional ancestral sampling. Instead of relying solely on model likelihood, QUEST samples translations proportionally to an automatic quality metric (COMET or BLEURT) treated as an energy function. Experiments with two strong decoder-only LLMs (ALMA-7B, Tower-7B) on English↔German and English↔Russian demonstrate substantial quality improvements while maintaining computational feasibility.

## Method Summary
QUEST combines Metropolis-Hastings MCMC with a novel sentence-level proposal distribution for machine translation. The method uses model likelihood for proposals but accepts/rejects samples based on an automatic quality metric (COMET or BLEURT) as an energy function in a Gibbs distribution. This quality-aware approach allows the sampler to explore high-density regions that traditional likelihood-based sampling might miss, producing translations that balance quality and diversity through iterative refinement over multiple decoding steps.

## Key Results
- Up to 2 COMET-XL point improvements over ancestral sampling
- Better COMET-KIWI-XL scores across all tested language pairs
- QUEST samples from underexplored high-density regions
- Quality improvements continue with more decoding steps

## Why This Works (Mechanism)
QUEST works by redefining the sampling objective from pure likelihood to a quality-weighted distribution. By treating automatic quality metrics as energy functions, the Metropolis-Hastings algorithm can escape local optima in the likelihood landscape and explore translation options that score higher on human-aligned quality measures. The MCMC framework ensures samples converge to the desired quality-weighted distribution while maintaining theoretical guarantees about the stationary distribution.

## Foundational Learning
- **Metropolis-Hastings MCMC**: Needed for theoretical guarantees about stationary distribution; quick check: verify detailed balance condition
- **Automatic quality metrics (COMET/BLEURT)**: Needed as proxy for human judgment; quick check: correlation with human evaluations
- **Gibbs distribution**: Needed to convert quality scores to energy-based sampling; quick check: proper normalization across candidates
- **Ancestral sampling**: Needed as baseline proposal distribution; quick check: compare diversity metrics
- **Stationary distribution**: Needed to ensure convergence properties; quick check: trace convergence over decoding steps

## Architecture Onboarding

**Component Map**
Quality Metric -> Energy Function -> Acceptance Probability -> Metropolis-Hastings Sampler -> Translation Output

**Critical Path**
Proposal generation → Quality estimation → Acceptance/rejection decision → Output translation

**Design Tradeoffs**
- Quality vs. computational cost (more steps = better quality but slower)
- Metric reliability vs. exploration capability
- Sample diversity vs. convergence stability

**Failure Signatures**
- Poor quality metrics → degraded translations
- Insufficient decoding steps → incomplete convergence
- Proposal distribution bias → limited exploration

**First Experiments**
1. Compare COMET vs BLEURT as energy functions
2. Vary number of decoding steps for quality/diversity tradeoff
3. Test on low-resource language pairs for robustness

## Open Questions the Paper Calls Out
None

## Limitations
- Quality metric calibration may not transfer across all domains
- MCMC requires multiple decoding steps, limiting deployment speed
- Proposal distribution may still be biased toward high-likelihood regions
- Difficulty verifying claims about "underexplored regions" empirically

## Confidence

**Translation Quality Improvements (High)**: Substantial and consistent improvements across multiple metrics and model sizes
**Diversity Claims (Medium)**: Improvements observed but relationship to quality is indirect
**Sampling Distribution Claims (Medium)**: Theoretically sound but difficult to verify empirically

## Next Checks
1. Test QUEST on specialized domains (medical, legal, technical) where quality metrics may have different calibration
2. Conduct human evaluations to assess correlation between metric improvements and human-perceived quality
3. Measure wall-clock time and memory usage across different decoding step counts to quantify deployment costs