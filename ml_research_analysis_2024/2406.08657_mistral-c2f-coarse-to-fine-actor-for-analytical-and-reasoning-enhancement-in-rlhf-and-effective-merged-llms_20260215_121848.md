---
ver: rpa2
title: 'Mistral-C2F: Coarse to Fine Actor for Analytical and Reasoning Enhancement
  in RLHF and Effective-Merged LLMs'
arxiv_id: '2406.08657'
source_url: https://arxiv.org/abs/2406.08657
tags:
- flowers
- actor
- coarse
- have
- they
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces the Coarse-to-Fine Actor model, a novel
  approach to address limitations in conversational and analytical capabilities of
  small-scale Large Language Models (LLMs). The method involves a two-step process:
  the Coarse Actor, which uses "Continuous Maximization" to generate a knowledge-rich
  pool with enhanced analytical and reasoning capabilities, and the Fine Actor, which
  refines the extensive content from the Coarse Actor using a "Knowledge Residue Merger"
  approach to reduce redundancy and improve quality.'
---

# Mistral-C2F: Coarse to Fine Actor for Analytical and Reasoning Enhancement in RLHF and Effective-Merged LLMs

## Quick Facts
- arXiv ID: 2406.08657
- Source URL: https://arxiv.org/abs/2406.08657
- Reference count: 40
- Key outcome: State-of-the-art performance on 11 general language tasks and MT-Bench Dialogue task, surpassing similar-scale and larger models

## Executive Summary
This paper introduces the Coarse-to-Fine Actor model, a novel approach to address limitations in conversational and analytical capabilities of small-scale Large Language Models (LLMs). The method involves a two-step process: the Coarse Actor, which uses "Continuous Maximization" to generate a knowledge-rich pool with enhanced analytical and reasoning capabilities, and the Fine Actor, which refines the extensive content from the Coarse Actor using a "Knowledge Residue Merger" approach to reduce redundancy and improve quality. The model, Mistral-C2F, was evaluated on 11 general language tasks and the MT-Bench Dialogue task, achieving state-of-the-art performance and outperforming similar-scale models and even larger models with 13B and 30B parameters. The Coarse Actor effectively enhances the model's analytical and reasoning abilities, while the Fine Actor ensures precise and relevant responses, resulting in significant improvements in conversational and analytical reasoning abilities.

## Method Summary
The Coarse-to-Fine Actor model enhances LLM capabilities through a two-step process. First, the Coarse Actor employs Continuous Maximization, dynamically extending output length based on performance metrics like reward model scores and critic value losses. This generates a knowledge-rich pool with improved analytical reasoning. Second, the Fine Actor uses Knowledge Residue Merger to blend parameters from the Coarse Actor and an Instruction model, reducing redundancy while preserving analytical depth. The merged model, Mistral-C2F, is then evaluated on various tasks to verify enhanced conversational and analytical abilities.

## Key Results
- Achieved state-of-the-art performance on 11 general language tasks
- Outperformed similar-scale models and even larger models with 13B and 30B parameters
- Significant improvements in conversational and analytical reasoning abilities on MT-Bench Dialogue task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Coarse Actor improves analytical reasoning by dynamically extending output length based on performance metrics (reward scores and critic value losses) rather than fixed iteration steps.
- Mechanism: The Continuous Maximization strategy adjusts the output length limit `l(t)` in real-time based on the reward model score `Rt(st, at)` and critic value function `Vϕ(st)`. When these metrics stabilize, the output length increases, allowing the model to generate more detailed analytical content.
- Core assumption: Reward model scores and critic value function losses are reliable indicators of when the model is ready to generate more detailed content without degrading quality.
- Evidence anchors:
  - [abstract]: "it employs Continuous Maximization, a strategy that dynamically and adaptively extends the output length limit, enabling the generation of more detailed and analytical content"
  - [section]: "the output length limit adjusts dynamically based on these sampling actor performance indicators, allowing for a flexible and responsive enhancement of the model's analytical and reasoning capabilities"
  - [corpus]: Weak - No direct corpus evidence found supporting the specific claim about performance metrics driving length extension. This appears to be a novel contribution.
- Break condition: If performance metrics become unstable or oscillate significantly, the model should maintain current output length rather than extend it.

### Mechanism 2
- Claim: The Fine Actor eliminates redundancy from Coarse Actor output while preserving analytical depth through weighted parameter merging.
- Mechanism: The Knowledge Residue Merger approach blends parameters from the Coarse Actor (θcoarse) and Instruction model (θSFT) using a weighted sum: `θfine = γθcoarse + (1-γ)θSFT`, where γ controls the balance between analytical depth and precision.
- Core assumption: The Coarse Actor contains valuable analytical knowledge that can be selectively merged with the Instruction model's precise response capabilities without catastrophic interference.
- Evidence anchors:
  - [abstract]: "We introduce a 'Knowledge Residue Merger' approach, refining the content from the Coarse Actor and merging it with an existing Instruction model to improve quality, correctness, and reduce redundancies"
  - [section]: "The merged model parameters are computed as follows: θfine = γθcoarse + (1-γ)θSFT"
  - [corpus]: Weak - No direct corpus evidence found supporting this specific weighted merging approach. This appears to be a novel contribution.
- Break condition: If γ is set too high (>0.7), the model loses segmentation capability and produces less logical responses.

### Mechanism 3
- Claim: The two-step process addresses both depth and quality limitations that single-stage fine-tuning cannot resolve.
- Mechanism: The Coarse Actor generates knowledge-rich, analytical content through extended generation, while the Fine Actor refines this content by merging with an Instruction model that provides precision and context-specific capabilities.
- Core assumption: Single-stage approaches (either pure RLHF or pure SFT) cannot simultaneously achieve both analytical depth and conversational quality.
- Evidence anchors:
  - [abstract]: "Our approach begins with the Policy-based Coarse Actor... Subsequently, the Fine Actor refines this analytical content"
  - [section]: "Unlike Kim et al. (2023), which merges several versions of SFT to balance different in-domain tasks' effects, our goal is to refine the extensive, knowledge-rich pool and reduce redundancy inherited from the Coarse Actor"
  - [corpus]: Moderate - Related work on coarse-to-fine approaches exists in other domains (NLP, IR), but this specific application to LLM alignment appears novel.
- Break condition: If the Coarse Actor generates excessive redundancy that cannot be effectively filtered by the Fine Actor, the merged model may still contain repetitive content.

## Foundational Learning

- Concept: Proximal Policy Optimization (PPO) in RLHF
  - Why needed here: The entire Coarse-to-Fine approach is built on PPO-based RLHF, requiring understanding of actor-critic architecture, advantage functions, and policy gradient updates
  - Quick check question: What is the purpose of the clipping mechanism in PPO's objective function, and how does it prevent large policy updates?

- Concept: Reward modeling and human preference alignment
  - Why needed here: The Coarse Actor relies on reward model scores to guide the Continuous Maximization process, requiring understanding of how human preferences are encoded into reward functions
  - Quick check question: How does the reward model differentiate between "chosen" and "rejected" responses in the Anthropic dataset used for training?

- Concept: Knowledge distillation and parameter merging
  - Why needed here: The Fine Actor's Knowledge Residue Merger approach involves blending parameters from two models, requiring understanding of how parameter spaces interact during merging
  - Quick check question: What happens to the representational capacity of a model when parameters are merged using weighted averaging versus concatenation?

## Architecture Onboarding

- Component map:
  Base LLM (Mistral) -> Coarse Actor (with Continuous Maximization) -> Knowledge-rich pool -> Fine Actor (with Knowledge Residue Merger) -> Mistral-C2F
  Supporting components: Reward Model, Critic Model, PPO optimizer, System Prompt manager

- Critical path:
  1. Generate extensive analytical content using Coarse Actor with dynamic length extension
  2. Merge Coarse Actor parameters with Instruction model using optimal γ ratio (0.7:0.3 found in experiments)
  3. Evaluate merged model on general tasks and MT-Bench to verify analytical and conversational improvements

- Design tradeoffs:
  - Output length vs. redundancy: Longer generation enables more analysis but risks redundancy
  - γ ratio selection: Higher γ preserves analytical depth but may sacrifice precision; lower γ improves precision but may lose analytical capability
  - System Prompt modification: Removing EOS tokens enables extended generation but requires careful prompt engineering

- Failure signatures:
  - Excessive repetition in generated responses (indicates Coarse Actor extended too far)
  - Loss of logical flow or segmentation in responses (indicates γ ratio too high)
  - Decreased performance on general tasks despite MT-Bench improvements (indicates over-specialization to dialogue)

- First 3 experiments:
  1. Implement Coarse Actor with fixed output length (baseline) vs. dynamic length extension, measure MMLU performance difference
  2. Test different γ ratios (0.5, 0.6, 0.7, 0.8) on a validation set to find optimal balance between analytical depth and precision
  3. Compare merged model performance against both parent models on MT-Bench to verify conversational improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Coarse Actor's "Continuous Maximization" strategy improve analytical reasoning capabilities more effectively than simply increasing the output length limit by a fixed amount?
- Basis in paper: [explicit] The paper states that the Coarse Actor uses "Continuous Maximization" which "dynamically and adaptively extends the output length limit, based on the model's sampling performance metrics, such as reward model scores and critic value function losses, rather than fixed iteration steps."
- Why unresolved: The paper does not provide a direct comparison between the Continuous Maximization strategy and a fixed output length increase method.
- What evidence would resolve it: An ablation study comparing the performance of the Coarse Actor with Continuous Maximization to a version with a fixed output length increase on a set of analytical reasoning tasks.

### Open Question 2
- Question: What is the optimal ratio of Coarse Actor to Instruction model parameters in the Fine Actor's "Knowledge Residue Merger" approach?
- Basis in paper: [explicit] The paper mentions that a 0.7 vs. 0.3 ratio achieved the best results, but states that this was "contrary to the anticipated 0.5 vs. 0.5 distribution."
- Why unresolved: The paper does not explore a wide range of ratios to determine the true optimal balance.
- What evidence would resolve it: A comprehensive study testing various ratios of Coarse Actor to Instruction model parameters in the Fine Actor and evaluating their impact on analytical reasoning and conversational ability.

### Open Question 3
- Question: How does the Coarse-to-Fine Actor model's performance on analytical reasoning tasks compare to larger models like GPT-4 and Claude?
- Basis in paper: [inferred] The paper claims that Mistral-C2F "outperforms similar-scale models and even larger models with 13B and 30B parameters" but does not provide a direct comparison to state-of-the-art models like GPT-4 and Claude on analytical reasoning benchmarks.
- Why unresolved: The paper focuses on comparing Mistral-C2F to models of similar scale and does not include comparisons to the largest and most capable models.
- What evidence would resolve it: Benchmarking Mistral-C2F against GPT-4 and Claude on a suite of analytical reasoning tasks to quantify the performance gap.

## Limitations

- Novel contributions lack direct corpus validation for Continuous Maximization and Knowledge Residue Merger approaches
- Missing implementation details for critical components like the logistic regression function fCM and optimal γ ratio determination
- Evaluation focused on 11 general language tasks and MT-Bench, lacking comprehensive benchmarking against state-of-the-art models

## Confidence

- High confidence: The overall two-step architectural approach and the general problem statement (enhancing small LLM capabilities through Coarse-to-Fine refinement) are well-grounded in existing RLHF literature.
- Medium confidence: The theoretical framework connecting performance metrics to output length extension and the weighted parameter merging concept are reasonable, though novel.
- Low confidence: Specific quantitative claims about performance improvements (e.g., exact percentage gains, state-of-the-art rankings) cannot be independently verified without access to the full experimental setup and baseline comparisons.

## Next Checks

1. **Implementation replication test**: Implement the Coarse Actor with Continuous Maximization on a simplified task (e.g., mathematical reasoning) to verify that performance metrics reliably predict when to extend output length. Measure if the dynamic extension actually improves analytical depth without introducing redundancy.

2. **Parameter merging ablation study**: Systematically test different γ ratios (0.3, 0.5, 0.7, 0.9) on a validation set to empirically determine the optimal balance between analytical depth and conversational precision. Measure both task performance and qualitative response quality.

3. **Break case analysis**: Deliberately push the Coarse Actor to extreme output lengths and observe when the Fine Actor fails to adequately filter redundancy. Identify the threshold where the Knowledge Residue Merger approach breaks down, providing practical limits for the method.