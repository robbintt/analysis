---
ver: rpa2
title: LLMs can learn self-restraint through iterative self-reflection
arxiv_id: '2405.13022'
source_url: https://arxiv.org/abs/2405.13022
tags: []
core_contribution: This paper introduces ReSearch, an iterative self-reflection algorithm
  that enables large language models to learn self-restraint by generating synthetic
  data tailored to their internal knowledge. The method uses a utility function to
  encourage the model to produce responses only when confident, and employs iterative
  self-prompting and self-evaluation to improve generations.
---

# LLMs can learn self-restraint through iterative self-reflection

## Quick Facts
- arXiv ID: 2405.13022
- Source URL: https://arxiv.org/abs/2405.13022
- Authors: Alexandre Piché; Aristides Milios; Dzmitry Bahdanau; Chris Pal
- Reference count: 40
- Primary result: ReSearch algorithm enables LLMs to learn self-restraint through iterative self-reflection, reducing hallucinations and improving accuracy on biography and historical event generation tasks.

## Executive Summary
This paper introduces ReSearch, an iterative self-reflection algorithm that enables large language models to learn self-restraint by generating synthetic data tailored to their internal knowledge. The method uses a utility function to encourage the model to produce responses only when confident, and employs iterative self-prompting and self-evaluation to improve generations. Models trained on ReSearch-generated data show fewer hallucinations and higher accuracy on biography and historical event generation tasks compared to baseline methods, while also learning to abstain from answering when uncertain. The approach achieves these improvements without additional inference cost and provides a general framework for teaching LLMs to adapt their behavior based on their level of knowledge and uncertainty.

## Method Summary
ReSearch is an iterative self-reflection algorithm that generates synthetic training data for large language models. The process involves multiple iterations of self-prompting and self-evaluation, where the model generates initial responses, evaluates their accuracy using self-consistency checks against other generations, and uses the most likely true claims to construct improved prompts for subsequent iterations. A utility function rewards true claims and penalizes false ones, with a target accuracy threshold that determines when the model should abstain from answering. The synthetic data, consisting of queries, generations, and their expected utilities, is then used to fine-tune the model using various training algorithms (SFT, DPO, RLOO) to internalize the self-restraint behavior.

## Key Results
- Models trained with ReSearch-generated data achieve higher utility and accuracy compared to baseline methods on biography and historical event generation tasks
- The method effectively reduces hallucinations while maintaining completeness of responses
- RLOO training algorithm achieves the highest utility across all datasets for 7B parameter models
- ReSearch enables models to learn when to abstain from answering, achieving zero utility at the target accuracy threshold

## Why This Works (Mechanism)

### Mechanism 1
The iterative self-reflection algorithm enables LLMs to learn self-restraint by refining their generations through multiple iterations of self-evaluation and self-prompting. The model generates multiple initial responses, evaluates their accuracy by comparing claims against other generations, and then uses the most likely true claims to prompt itself for improved generations in subsequent iterations. Core assumption: The model's self-evaluation capability is sufficiently accurate to identify which claims are likely true or false based on internal consistency.

### Mechanism 2
The utility function U(x, y, λ) effectively encourages the model to produce accurate responses while learning when to abstain from answering. The utility function assigns positive utility for true claims and negative utility for false claims, with a target accuracy threshold ρ* that determines when the model should abstain to achieve zero utility. Core assumption: The model can accurately estimate the expected utility of its generations during the self-evaluation phase.

### Mechanism 3
Training on synthetic data generated by ReSearch enables LLMs to internalize self-restraint behavior without additional inference cost. The model learns from the synthetic dataset (x, Y, U) where Y contains generations with their expected utilities, allowing it to generalize the self-restraint behavior learned during the search process. Core assumption: The synthetic data distribution captures the essential patterns needed for the model to learn appropriate restraint behavior.

## Foundational Learning

- Concept: Self-evaluation and calibration
  - Why needed here: The model needs to accurately assess its own confidence in generated claims to determine which ones are likely true and when to abstain.
  - Quick check question: How does the model estimate the probability that a claim is true based on self-consistency with other generations?

- Concept: Utility maximization and decision thresholds
  - Why needed here: The utility function with target accuracy ρ* provides a clear optimization objective that balances accuracy against completeness and abstention.
  - Quick check question: How does changing the target accuracy ρ* affect the model's behavior in terms of claims generated versus accuracy?

- Concept: Iterative refinement and search algorithms
  - Why needed here: The iterative process allows the model to progressively improve its generations by incorporating feedback from previous iterations.
  - Quick check question: Why is iterative search more effective than naive wide search for generating high-quality synthetic data?

## Architecture Onboarding

- Component map: Generation component -> Self-evaluation component -> Self-prompting component -> Utility scoring component -> Training component
- Critical path:
  1. Generate initial responses (Pwrite prompt)
  2. Self-evaluate claim probabilities using self-consistency
  3. Select likely true claims based on threshold ρ*
  4. Construct improved prompt (Prewrite) with selected claims
  5. Generate refined responses
  6. Repeat steps 2-5 for K iterations
  7. Score all generations using expected utility
  8. Train model on synthetic data (SFT/DPO/RLOO)
- Design tradeoffs:
  - Number of search iterations vs. computational cost
  - Width of search (number of generations per iteration) vs. diversity of responses
  - Target accuracy threshold ρ* vs. desired behavior (accuracy vs. completeness)
  - Training algorithm choice (SFT vs. DPO vs. RLOO) vs. final model behavior
- Failure signatures:
  - Model continues to generate false claims despite training: Likely indicates poor self-evaluation calibration
  - Model abstains too frequently: Target accuracy ρ* may be set too high
  - Model generates very few claims: Utility function may be overly penalizing false claims
  - No improvement across iterations: Self-prompting may not be effectively incorporating true claims
- First 3 experiments:
  1. Compare single wide search vs. iterative search with varying iterations on utility achieved
  2. Test different target accuracy thresholds ρ* to observe behavior changes in generated data
  3. Evaluate different training algorithms (SFT, DPO, RLOO) on the same ReSearch-generated dataset to compare final model behaviors

## Open Questions the Paper Calls Out

### Open Question 1
How does ReSearch's iterative self-reflection algorithm scale to larger and more diverse datasets beyond biographies and historical events? The paper evaluates ReSearch on two specific tasks (biographies and historical events) and suggests that general conclusions can be deduced from this work, implying potential for broader application. This remains unresolved as the paper does not provide empirical evidence or theoretical analysis of ReSearch's performance on datasets outside of the evaluated domains. Experiments applying ReSearch to a wider variety of domains (e.g., scientific articles, news, literature) and comparing its performance to baseline methods in these domains would resolve this question.

### Open Question 2
Can the utility function design in ReSearch be extended to incorporate additional factors beyond accuracy and abstention, such as relevance, diversity, or ethical considerations? The paper discusses the challenges of designing a utility function for long-form factual content generation, balancing completeness and accuracy, and mentions the need for self-restraint. It also notes that future work could combine this approach with retrieval-augmented generation to teach LLMs to abstain from answering queries when the required information is not contained in the retrieved documents. This remains unresolved as the current utility function focuses primarily on accuracy and abstention, and the paper does not explore the incorporation of other factors. Experiments demonstrating the effectiveness of utility functions that incorporate additional factors, such as relevance, diversity, or ethical considerations, and comparing their performance to the current utility function would resolve this question.

### Open Question 3
What are the computational trade-offs between ReSearch's iterative approach and wider search strategies in terms of efficiency and effectiveness? The paper discusses the importance of iterative search over naive wide search, showing that iterative search is more effective and efficient in terms of maximizing utility and reducing computation cost. This remains unresolved as while the paper provides evidence for the effectiveness of iterative search, it does not conduct a comprehensive analysis of the computational trade-offs between different search strategies. A detailed analysis of the computational costs (e.g., number of tokens generated, model calls) and performance metrics (e.g., utility, accuracy) for different search strategies, including iterative search and wide search, across various tasks and datasets would resolve this question.

## Limitations

- Evaluation is limited to biography and historical event generation tasks, potentially limiting generalizability
- Reliance on model's internal consistency for self-evaluation may break down for poorly calibrated models
- Does not address potential biases introduced through iterative self-reflection process
- Computational cost of iterative process during training data generation phase may be substantial

## Confidence

**High Confidence Claims:**
- The ReSearch algorithm can generate synthetic data that leads to measurable improvements in accuracy and reduction in hallucinations for biography and historical event generation tasks
- The utility function with target accuracy ρ* effectively controls the trade-off between accuracy and the number of claims generated
- Different training algorithms (SFT, DPO, RLOO) can effectively train models on ReSearch-generated data, with RLOO showing the highest utility

**Medium Confidence Claims:**
- The iterative self-reflection process with self-consistency evaluation reliably identifies true claims
- The learned self-restraint behavior generalizes beyond the specific training tasks
- The method achieves these improvements without additional inference cost

**Low Confidence Claims:**
- The approach generalizes to domains beyond biography and historical event generation
- The self-reflection mechanism is robust to out-of-distribution queries and poor model calibration
- The method scales effectively to larger models and different architectures

## Next Checks

1. **Cross-domain Generalization Test**: Evaluate ReSearch-trained models on diverse task types including mathematical reasoning, code generation, and open-ended creative tasks to assess whether the self-restraint behavior generalizes beyond factual knowledge generation.

2. **Calibration and Robustness Analysis**: Systematically test model behavior on out-of-distribution queries, ambiguous prompts, and scenarios where the model's internal knowledge is limited to identify failure modes and evaluate the robustness of the self-evaluation mechanism.

3. **Scalability and Architecture Study**: Apply ReSearch to models of varying sizes (1B, 13B, 70B) and different architectures (decoder-only, encoder-decoder, multimodal) to understand how the method performs across the model spectrum and identify any architecture-specific considerations.