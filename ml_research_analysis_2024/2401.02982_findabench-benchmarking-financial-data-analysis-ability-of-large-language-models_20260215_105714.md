---
ver: rpa2
title: 'FinDABench: Benchmarking Financial Data Analysis Ability of Large Language
  Models'
arxiv_id: '2401.02982'
source_url: https://arxiv.org/abs/2401.02982
tags:
- data
- knowledge
- language
- arxiv
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces FinDABench, a comprehensive benchmark for
  evaluating large language models'' (LLMs) financial data analysis capabilities.
  It assesses models across three dimensions: foundational ability, reasoning ability,
  and technical skill, through 11 sub-tasks covering classification, extraction, and
  generation.'
---

# FinDABench: Benchmarking Financial Data Analysis Ability of Large Language Models

## Quick Facts
- arXiv ID: 2401.02982
- Source URL: https://arxiv.org/abs/2401.02982
- Authors: Shu Liu; Shangqing Zhao; Chenghao Jia; Xinlin Zhuang; Zhaoguang Long; Jie Zhou; Aimin Zhou; Man Lan; Qingquan Wu; Chong Yang
- Reference count: 15
- One-line primary result: FinDABench evaluates LLMs across three dimensions (Foundational Ability, Reasoning Ability, Technical Skill) through 11 sub-tasks covering classification, extraction, and generation

## Executive Summary
FinDABench is a comprehensive benchmark designed to evaluate large language models' financial data analysis capabilities across three cognitive dimensions: Foundational Ability (numerical reasoning and financial concepts), Reasoning Ability (textual comprehension and abnormal report analysis), and Technical Skill (real-world data analysis challenges). The benchmark includes 11 sub-tasks covering classification, extraction, and generation tasks, providing a structured evaluation framework for business intelligence applications. To enhance performance, the authors developed BIChat, a domain-specific chatbot fine-tuned on over 1.8 million examples using Chain of Thought and Self-Instruction methods.

## Method Summary
The method involves fine-tuning the Qwen-7B-Chat model using BIChat, a domain-specific dataset constructed with Chain of Thought and Self-Instruction methods, comprising 1.8 million examples. The training procedure uses LoRA (Low-Rank Adaptation) with 4-bit quantization, LoRA rank 64, and 3 training epochs. The FinDABench benchmark evaluates models across 11 sub-tasks categorized into classification, extraction, and generation types, structured around three cognitive dimensions: Foundational Ability, Reasoning Ability, and Technical Skill.

## Key Results
- The benchmark successfully defines a structured evaluation framework with three cognitive dimensions
- Domain-specific fine-tuning using BIChat improves financial data analysis performance
- The task categorization into classification, extraction, and generation provides comprehensive coverage of BI analysis requirements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The benchmark's multi-dimensional structure captures the full spectrum of BI analysis requirements
- Mechanism: By decomposing BI analysis into three distinct cognitive dimensions, the benchmark isolates and evaluates specific capabilities that general-purpose LLMs may lack
- Core assumption: Financial data analysis requires both domain knowledge and technical skills that can be meaningfully separated and measured
- Evidence anchors: [abstract] assessment across three dimensions; [section] BIBench structured with three critical dimensions; [corpus] found 25 related papers
- Break condition: If the three dimensions are not truly independent or if models can game the system by memorizing patterns

### Mechanism 2
- Claim: Domain-specific fine-tuning with 1.8M+ examples significantly improves financial data analysis performance
- Mechanism: Large-scale domain-specific fine-tuning on carefully curated data allows LLMs to internalize financial domain knowledge and develop specialized reasoning patterns
- Core assumption: Financial data analysis requires specialized knowledge that cannot be adequately acquired through general pre-training alone
- Evidence anchors: [abstract] BIChat fine-tuned on over 1.8 million examples; [section] constructed 400,000 examples using Chain of Thought and Self-Instruction
- Break condition: If the fine-tuning data is not sufficiently diverse or if the model overfits to specific patterns

### Mechanism 3
- Claim: The benchmark's task categorization provides comprehensive coverage of BI analysis requirements
- Mechanism: By including all three major NLP task types, the benchmark ensures that models are evaluated on their ability to understand, extract, and generate financial insights
- Core assumption: Real-world financial data analysis requires proficiency in all three task types, not just one or two
- Evidence anchors: [abstract] 11 sub-tasks covering classification, extraction, and generation; [section] BIBench composed of 11 sub-tasks in three categories
- Break condition: If the task categories are not representative of real-world BI analysis or if important task types are missing

## Foundational Learning

- Concept: Bloom's Taxonomy and cognitive dimension classification
  - Why needed here: The benchmark uses Bloom's Taxonomy to structure its evaluation dimensions
  - Quick check question: How does Bloom's Taxonomy classify cognitive abilities from lowest to highest level?

- Concept: Financial domain knowledge and terminology
  - Why needed here: The benchmark evaluates financial data analysis capabilities, requiring understanding of financial concepts, reports, and analysis methods
  - Quick check question: What are the key differences between financial statement analysis and financial ratio analysis?

- Concept: SQL and data manipulation languages
  - Why needed here: Several benchmark tasks involve SQL generation and data analysis, requiring knowledge of query languages and database concepts
  - Quick check question: What is the difference between INNER JOIN and LEFT JOIN in SQL?

## Architecture Onboarding

- Component map: FinDABench -> 11 sub-tasks -> 3 dimensions (Foundational, Reasoning, Technical) -> 3 task types (classification, extraction, generation)
- Critical path: Load benchmark configuration -> Load model to be evaluated -> Run each task in sequence -> Collect and aggregate results -> Generate comprehensive report
- Design tradeoffs: Task selection vs. coverage (more tasks provide better coverage but increase evaluation time); Dataset size vs. quality (larger datasets improve performance but may introduce noise); Open vs. closed source (open benchmarks allow broader participation but may be easier to game)
- Failure signatures: Inconsistent results across similar tasks; Poor performance on tasks requiring domain-specific knowledge; Overfitting to specific task formats or patterns
- First 3 experiments: 1) Run a single task (e.g., Financial Multiple Choice) to verify basic functionality; 2) Test the evaluation script with a simple baseline model; 3) Run a complete dimension (e.g., Foundational Ability) to check integration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of FinDABench change when evaluating models on multimodal data (e.g., combining text with charts or tables)?
- Basis in paper: [inferred] The paper mentions chart visualization as part of the technical skill dimension but does not evaluate multimodal inputs
- Why unresolved: The benchmark focuses on text-based analysis without explicitly testing models' ability to interpret and analyze visual data
- What evidence would resolve it: Incorporating multimodal evaluation tasks where models must analyze both textual and visual financial data

### Open Question 2
- Question: What is the impact of catastrophic forgetting when fine-tuning general-purpose LLMs on financial domain-specific data?
- Basis in paper: [explicit] The authors mention this as a limitation, noting they will use MoE to balance foundational and domain-specific knowledge
- Why unresolved: The current BIChat model is trained on a 7B parameter model without addressing catastrophic forgetting
- What evidence would resolve it: Comparative analysis of model performance before and after fine-tuning, measuring retention of general knowledge

### Open Question 3
- Question: How does the performance of FinDABench vary across different financial subdomains (e.g., banking vs. insurance vs. investment)?
- Basis in paper: [inferred] The benchmark includes general financial tasks but does not specifically evaluate performance across different financial sectors
- Why unresolved: The tasks are designed to be broad without focusing on specific financial subdomains
- What evidence would resolve it: Creating subdomain-specific versions of the benchmark tasks and comparing model performance across sectors

## Limitations
- Dataset construction methodology for BIChat lacks detailed documentation about data quality control and potential biases
- Evaluation metrics for the 11 sub-tasks are not fully specified, making scoring consistency difficult to assess
- Benchmark relies on structured tasks that may not adequately evaluate open-ended analytical questions or unexpected data formats

## Confidence
- High Confidence: The benchmark successfully defines a structured evaluation framework with three cognitive dimensions; task categorization is methodologically sound; domain-specific fine-tuning approach is valid
- Medium Confidence: Specific dataset construction methods will produce meaningful improvements; three cognitive dimensions are mutually exclusive and collectively exhaustive; 11 sub-tasks provide comprehensive coverage
- Low Confidence: Exact performance improvements from BIChat fine-tuning across all tasks; benchmark's ability to generalize to real-world financial analysis scenarios; reproducibility of results across different LLM architectures

## Next Checks
1. Conduct systematic analysis of the BIChat dataset to verify diverse financial scenarios, appropriate domain coverage, and absence of data leakage from pre-training corpora
2. Evaluate the benchmark using multiple LLM architectures to determine whether performance patterns are consistent across different model families and sizes
3. Compare benchmark tasks against actual financial analyst workflows to identify gaps between evaluated capabilities and practical business intelligence requirements, focusing on open-ended analytical reasoning and handling ambiguous or incomplete data