---
ver: rpa2
title: 'Ask LLMs Directly, "What shapes your bias?": Measuring Social Bias in Large
  Language Models'
arxiv_id: '2406.04064'
source_url: https://arxiv.org/abs/2406.04064
tags:
- bias
- persona
- social
- default
- american
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to measuring social bias
  in large language models (LLMs) by directly quantifying social perceptions from
  varied perspectives. Unlike previous methods that rely on indirect assessments or
  fixed stereotypes, the proposed method employs a question-answering format to measure
  how different personas perceive various target identities.
---

# Ask LLMs Directly, "What shapes your bias?": Measuring Social Bias in Large Language Models

## Quick Facts
- **arXiv ID**: 2406.04064
- **Source URL**: https://arxiv.org/abs/2406.04064
- **Reference count**: 40
- **Primary result**: Introduces a novel approach to measure social bias in LLMs by directly quantifying social perceptions from varied perspectives using persona assignments and a question-answering format.

## Executive Summary
This paper presents a novel methodology for measuring social bias in large language models by directly quantifying how different personas perceive various target identities. Unlike traditional bias measurement approaches that rely on indirect assessments or fixed stereotypes, the proposed method employs a question-answering format where incorrect responses are interpreted as indicators of social bias. The study introduces three novel metrics—TARGET BIAS, BIAS AMOUNT, and PERSONA BIAS—to evaluate the polarity, quantity, and variance of biases across personas. Experiments on the BBQ dataset demonstrate that the approach effectively captures multi-dimensional aspects of bias, revealing in-group favoritism patterns and variations in perception based on assigned personas.

## Method Summary
The proposed method quantifies social bias by assigning personas to LLMs and measuring their responses to carefully designed questions about target identities. The approach uses a question-answering format where selecting certain targets reveals implicit social perceptions, with reward and penalty scores assigned based on correctness. Three novel metrics are introduced: TARGET BIAS measures polarity, BIAS AMOUNT measures quantity, and PERSONA BIAS measures variance across personas. The method was evaluated using the BBQ dataset across five domains (age, race/ethnicity, religion, socioeconomic status, and sexual orientation) with five different LLMs including Llama-2 and GPT models.

## Key Results
- The proposed metrics effectively capture multi-dimensional aspects of bias, revealing in-group favoritism patterns across different personas
- Larger models generally exhibit stronger biases, while GPT-4 shows notably low bias and weak in-group favoritism
- The method successfully quantifies social perceptions without requiring additional measurement steps, demonstrating fine-grained analysis of bias in LLMs
- Results show significant variation in bias based on assigned personas, with some personas exhibiting stronger biases than others

## Why This Works (Mechanism)

### Mechanism 1
The method directly quantifies social perceptions by measuring LLMs' response selection in a QA format. Questions are designed so that selecting certain targets reveals the model's implicit social perception, with reward and penalty scores assigned based on correctness and selection. The core assumption is that incorrect responses to designed questions are driven by social bias rather than knowledge gaps.

### Mechanism 2
Assigning personas allows capturing different social perceptions toward the same targets. Personas are assigned via system prompts, and the model's responses to questions about targets are measured to reveal how each persona perceives different groups. The core assumption is that LLMs can effectively role-play personas and their responses will reflect persona-specific biases.

### Mechanism 3
Aggregating social perceptions across personas and targets yields three novel bias metrics. TARGET BIAS measures polarity, BIAS AMOUNT measures quantity, and PERSONA BIAS measures variance across personas. The core assumption is that social bias can be quantified as the aggregation of social perceptions, and these three dimensions capture different aspects of bias.

## Foundational Learning

- **Concept**: Understanding of social bias as accumulated social perceptions
  - Why needed here: The paper's approach is based on measuring social perceptions rather than stereotypes or sentiment, so understanding this distinction is crucial
  - Quick check question: How does the paper define social bias differently from traditional bias measurement approaches?

- **Concept**: Question-answering format for bias measurement
  - Why needed here: The method relies on designing specific questions that reveal bias when answered incorrectly, so understanding how to construct such questions is essential
  - Quick check question: What makes a question suitable for revealing social bias through incorrect responses?

- **Concept**: Persona assignment in LLMs
  - Why needed here: The method uses persona assignment to capture different perspectives, so understanding how to effectively assign and maintain personas is important
  - Quick check question: How does persona assignment influence the social perceptions captured by the model?

## Architecture Onboarding

- **Component map**: Persona assignment module → Question generation module → Response collection module → Scoring module → Aggregation module → Visualization module
- **Critical path**: Persona assignment → Question → Response → Scoring → Aggregation
- **Design tradeoffs**: Counter-scoring vs. direct scoring, persona variety vs. experimental complexity, reward/penalty weight selection
- **Failure signatures**: Low variance in PERSONA BIAS scores, high correlation between accuracy and BAMT, inability to distinguish between different personas
- **First 3 experiments**:
  1. Test with a single persona and simple binary targets to validate scoring mechanism
  2. Add multiple personas with the same target set to validate PERSONA BIAS metric
  3. Test with full dataset and all five domains to validate complete system

## Open Questions the Paper Calls Out

### Open Question 1
**Question**: How can we effectively mitigate the biases revealed by the proposed metrics in large language models?
**Basis in paper**: [inferred] The paper discusses measuring and analyzing biases in LLMs but does not address mitigation strategies
**Why unresolved**: The paper focuses on measurement and analysis rather than proposing or evaluating mitigation techniques
**What evidence would resolve it**: Research demonstrating successful bias reduction techniques in LLMs and their impact on the proposed metrics

### Open Question 2
**Question**: How do cultural differences and non-English language models impact the manifestation and measurement of biases?
**Basis in paper**: [explicit] The paper mentions that determining the reasons for bias in other languages is challenging due to multiple factors
**Why unresolved**: The study focuses on English language models and does not explore cross-cultural or multilingual aspects of bias
**What evidence would resolve it**: Comparative studies of bias in LLMs across different languages and cultures, including adaptations of the proposed metrics for non-English contexts

### Open Question 3
**Question**: What are the root causes of bias in large language models - is it primarily due to data quality, training algorithms, or model policies?
**Basis in paper**: [explicit] The paper acknowledges that the root of bias in language models could stem from various factors including performance issues, cultural dependency, or persona-assigning challenges
**Why unresolved**: The study focuses on measuring and analyzing biases but does not investigate their underlying causes
**What evidence would resolve it**: Empirical studies isolating and testing the impact of different factors (data quality, training algorithms, model policies) on the manifestation of biases in LLMs

## Limitations
- The method's validity hinges on the assumption that incorrect responses in designed questions stem from social bias rather than genuine knowledge gaps or ambiguity in the questions themselves
- The persona assignment mechanism raises questions about how deeply LLMs can embody social perspectives versus providing superficial role-play responses
- The core assumption that incorrect responses directly indicate social bias rather than other factors (knowledge gaps, ambiguity, or task misunderstanding) lacks empirical validation

## Confidence
- **High confidence**: The three proposed metrics (TARGET BIAS, BIAS AMOUNT, PERSONA BIAS) are mathematically well-defined and can be computed as described
- **Medium confidence**: The claim that this approach captures "multi-dimensional aspects of bias" is supported by the metric design but requires further validation
- **Low confidence**: The core assumption that incorrect responses directly indicate social bias rather than other factors (knowledge gaps, ambiguity, or task misunderstanding) lacks empirical validation

## Next Checks
1. **Ground truth validation**: Compare the method's bias scores against human-annotated social perceptions for the same questions to establish whether the model's "incorrect" responses actually reflect biased social perceptions versus factual errors
2. **Control question analysis**: Introduce a set of control questions where the correct answer is known to be independent of social bias, then verify that these questions do not produce significant bias scores
3. **Persona adherence test**: Conduct a separate evaluation where models must demonstrate consistent persona-specific knowledge (e.g., asking persona-relevant factual questions) before using their responses to bias questions