---
ver: rpa2
title: 'Manifold Integrated Gradients: Riemannian Geometry for Feature Attribution'
arxiv_id: '2405.09800'
source_url: https://arxiv.org/abs/2405.09800
tags: []
core_contribution: This paper tackles the problem of noisy and adversarial-attribution
  vulnerable feature attributions in Integrated Gradients (IG). It proposes Manifold
  Integrated Gradients (MIG), a novel method that integrates gradients along geodesics
  of a learned Riemannian data manifold instead of the linear path in IG.
---

# Manifold Integrated Gradients: Riemannian Geometry for Feature Attribution

## Quick Facts
- arXiv ID: 2405.09800
- Source URL: https://arxiv.org/abs/2405.09800
- Reference count: 40
- Primary result: Proposed method achieves lower maximum sensitivity (0.17 on Oxford IIT Pets, 0.21 on 102 Flowers) compared to IG variants (0.87 and 0.74 respectively)

## Executive Summary
This paper addresses the problem of noisy and adversarial-attribution vulnerable feature attributions in Integrated Gradients (IG) by proposing Manifold Integrated Gradients (MIG). MIG integrates gradients along geodesics of a learned Riemannian data manifold instead of the linear path used in IG. The approach leverages deep generative models (VAEs) to capture the curved geometry of the data manifold, resulting in more perceptually aligned feature visualizations and increased robustness to targeted attributional attacks.

## Method Summary
MIG extends Integrated Gradients by integrating gradients along geodesics on a learned Riemannian data manifold. The method uses a VAE with perceptual loss to learn the manifold structure, trains a classifier on VAE reconstructions, and computes geodesic paths between baseline and target inputs. Gradients are then integrated along these geodesic paths to generate feature attributions. The approach is evaluated on Oxford IIT Pets and 102 Flowers datasets, demonstrating significant improvements in maximum sensitivity, infidelity, and robustness to targeted attributional attacks compared to IG variants.

## Key Results
- MIG achieves maximum sensitivity scores of 0.17 and 0.21 on Oxford IIT Pets and 102 Flowers datasets respectively, compared to 0.87 and 0.74 for IG
- MIG shows significantly higher structural similarity index (SSI) under targeted attributional attacks, indicating better preservation of attribution map structure
- The method generates more perceptually intuitive explanations by conforming to the curved geometry of the Riemannian data manifold

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: MIG generates smoother paths by integrating gradients along geodesics on the learned Riemannian data manifold instead of the linear path in IG.
- **Mechanism**: The VAE learns a Riemannian metric over the latent space, and geodesics represent the shortest, smoothest paths between points on this manifold. Integrating along these geodesics accumulates gradients in a way that respects the curved geometry of the data, avoiding regions of high curvature and noise that occur with linear interpolation.
- **Core assumption**: The data manifold is approximately Riemannian, and the VAE's learned metric accurately captures its intrinsic geometry.
- **Evidence anchors**:
  - [abstract]: "IG along the geodesics conforms to the curved geometry of the Riemannian data manifold, generating more perceptually intuitive explanations."
  - [section 4.1]: "Geodesics are the shortest paths that represent the most seamless transitions possible between two points. When applying this to latent representation of images, it translates to a smooth interpolation between two images, staying faithful to the structure of the data manifold."
  - [corpus]: Found related work "Riemannian Integrated Gradients: A Geometric View of Explainable AI" suggesting geometric extensions to IG are being explored, but no direct evaluation of noise reduction.

### Mechanism 2
- **Claim**: By aligning the model's gradients with the data manifold, MIG makes feature attributions more robust to targeted attributional attacks.
- **Mechanism**: Adversarial attributional attacks often exploit model sensitivity to perturbations that move inputs off the data manifold. By integrating along geodesics that stay on the manifold, MIG accumulates gradients that are less sensitive to such perturbations, as they are more aligned with the natural structure of the data.
- **Core assumption**: Gradients aligned with the data manifold are inherently more robust to adversarial perturbations than gradients accumulated along linear paths that can deviate off-manifold.
- **Evidence anchors**:
  - [abstract]: "subsequently, substantially increasing robustness to targeted attributional attacks."
  - [section 4.3]: "Adversarial attributional attacks exploit model's sensitivity to imperceptible perturbations, which are usually off-manifold. Aligning the model's gradients with the data manifold, provides inherent robustness to such perturbations."
  - [section 5.2]: "MIG scores the highest SSI, indicating it preserves the structure and relevancy of attribution maps under targeted attributional attacks."
  - [corpus]: Found related work "Rethinking Robustness: A New Approach to Evaluating Feature Attribution Methods" suggesting robustness evaluation is an active area, but no specific findings on manifold alignment.

### Mechanism 3
- **Claim**: The VAE's denoising capabilities, combined with geodesic paths, project adversarial inputs closer to the data manifold, further enhancing robustness.
- **Mechanism**: VAEs can denoise inputs, including adversarial examples, by projecting them towards the learned data manifold. When MIG integrates gradients along geodesics on this denoised manifold, the resulting attributions are based on a cleaner, more representative version of the input, reducing the impact of adversarial perturbations.
- **Core assumption**: The VAE's denoising process effectively projects adversarial examples closer to the true data manifold, and this projected version is a better basis for attribution than the original adversarial input.
- **Evidence anchors**:
  - [section 4.3]: "VAEs possess denoising capabilities, allowing them to denoise input, including noisy or adversarial examples. This process can be seen as projecting noisy or adversarial inputs closer to the manifold of normal data."
  - [corpus]: No direct corpus evidence found for this specific denoising mechanism in the context of adversarial robustness.

## Foundational Learning

- **Concept: Riemannian geometry and geodesics**
  - Why needed here: Understanding how the curved geometry of the data manifold differs from Euclidean space, and how geodesics represent the shortest, smoothest paths on such manifolds.
  - Quick check question: What is the key difference between a geodesic on a Riemannian manifold and a straight line in Euclidean space, and why is this difference important for feature attribution?

- **Concept: Variational autoencoders (VAEs) and latent space structure**
  - Why needed here: Grasping how VAEs learn a probabilistic latent space with a structured geometry, and how this structure can be leveraged to define a Riemannian metric.
  - Quick check question: How does a VAE's latent space differ from a standard autoencoder's latent space, and why is this difference crucial for the approach in this paper?

- **Concept: Path-based feature attribution methods and their axioms**
  - Why needed here: Understanding the properties and limitations of existing methods like Integrated Gradients, and how MIG extends them to respect the data manifold.
  - Quick check question: What are the key axioms that path-based feature attribution methods aim to satisfy, and how does MIG's use of geodesics help satisfy these axioms while addressing the limitations of linear paths?

## Architecture Onboarding

- **Component map**: VAE -> Classifier -> Geodesic computation module -> MIG
- **Critical path**:
  1. Train VAE on image dataset to learn the data manifold.
  2. Use VAE to reconstruct all images in the dataset.
  3. Train classifier on VAE reconstructions and labels.
  4. For a given input and baseline, compute the geodesic path on the VAE's latent manifold.
  5. Integrate classifier gradients along this geodesic path to generate the MIG attribution.

- **Design tradeoffs**:
  - Using a VAE introduces additional complexity and training time compared to directly using IG, but provides a more accurate representation of the data manifold.
  - Computing geodesics can be computationally expensive, especially for high-dimensional latent spaces, but is necessary to respect the manifold's geometry.
  - The choice of VAE architecture and training procedure can significantly impact the quality of the learned manifold and, consequently, the effectiveness of MIG.

- **Failure signatures**:
  - Noisy or inaccurate geodesic paths, leading to poor-quality attributions.
  - Over-regularization of the VAE, causing the learned manifold to be too smooth and not capture important data variations.
  - Under-regularization of the VAE, resulting in a manifold that does not accurately represent the data structure.
  - Poor classifier performance on VAE reconstructions, indicating a mismatch between the manifold and the classifier's decision boundaries.

- **First 3 experiments**:
  1. Train a VAE on a simple 2D dataset (e.g., a spiral) and visualize the learned manifold and geodesics to verify that the VAE captures the data structure and that geodesics follow the expected paths.
  2. Apply MIG to a small image dataset (e.g., MNIST) and compare the attributions to IG, focusing on perceptual quality and robustness to simple adversarial perturbations.
  3. Analyze the impact of VAE architecture choices (e.g., latent space dimensionality, regularization strength) on the quality of MIG attributions and the learned manifold structure.

## Open Questions the Paper Calls Out
- None explicitly stated in the paper.

## Limitations
- The accuracy of the learned Riemannian data manifold and geodesic computation is critical, and any errors in these components can significantly impact MIG's performance.
- The paper lacks sufficient details on the VAE architecture and training hyperparameters, making it challenging to reproduce the results and assess the quality of the learned manifold.
- The computational cost of geodesic computation, especially for high-dimensional data, is not thoroughly discussed, raising concerns about the method's scalability.

## Confidence
- **High confidence**: The core idea of leveraging Riemannian geometry for feature attribution is sound, and the use of VAEs to learn the data manifold is a well-established approach. The reported improvements in maximum sensitivity and infidelity metrics are likely valid.
- **Medium confidence**: The claims of enhanced robustness to targeted attributional attacks are supported by the SSI metric but require further validation with diverse attack scenarios. The perceptual alignment of MIG attributions is subjective and needs qualitative assessment.
- **Low confidence**: The paper does not provide sufficient details on the VAE architecture and training process, making it challenging to assess the quality of the learned manifold and its impact on MIG performance. The computational efficiency of geodesic computation is not thoroughly discussed.

## Next Checks
1. **Manifold Quality Assessment**: Visualize the learned Riemannian data manifold using t-SNE or UMAP on a held-out test set. Assess the quality of the manifold by checking if similar images are grouped together and if the geodesic paths follow the expected structure.

2. **Adversarial Attack Robustness**: Evaluate MIG's robustness against a wider range of targeted attributional attacks, including both white-box and black-box attacks, with varying magnitudes and attack types. Compare the results with other state-of-the-art attribution methods.

3. **Computational Efficiency Analysis**: Measure the computational cost of MIG, including VAE training, geodesic computation, and attribution generation, on datasets of varying sizes and complexities. Compare the runtime with IG and other baseline methods to assess the practical feasibility of MIG for large-scale applications.