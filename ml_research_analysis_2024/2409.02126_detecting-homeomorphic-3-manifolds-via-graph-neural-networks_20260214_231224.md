---
ver: rpa2
title: Detecting Homeomorphic 3-manifolds via Graph Neural Networks
arxiv_id: '2409.02126'
source_url: https://arxiv.org/abs/2409.02126
tags:
- graph
- moves
- graphs
- nnconv
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of Graph Neural Networks (GNNs)
  to determine whether two graph-manifolds are homeomorphic. Graph-manifolds are a
  class of 3-manifolds that can be uniquely represented by plumbing graphs.
---

# Detecting Homeomorphic 3-manifolds via Graph Neural Networks

## Quick Facts
- arXiv ID: 2409.02126
- Source URL: https://arxiv.org/abs/2409.02126
- Reference count: 0
- Best accuracy achieved: 70.7% in determining whether two graph-manifolds are homeomorphic

## Executive Summary
This paper investigates using Graph Neural Networks (GNNs) to determine whether two graph-manifolds are homeomorphic, bypassing the super-polynomial complexity of von Neumann moves with polynomial-time GNN inference at the cost of accuracy. The authors create a dataset of 80,000 pairs of plumbing graphs (40,000 homeomorphic, 40,000 non-homeomorphic) and train six different GNN architectures using various combinations of convolutional layers (GEN, GCN, GAT, NNConv). The best-performing NNConv+NNConv architecture achieves 70.7% accuracy, demonstrating that GNNs can learn approximate decision boundaries for the homeomorphism problem, though the authors note that longer training times and larger datasets could further improve results.

## Method Summary
The method involves generating pairs of plumbing graphs labeled as homeomorphic or non-homeomorphic, then training GNN architectures to classify these pairs. The dataset consists of 80,000 pairs created using EquivPair (homeomorphic) and InEquivPair/TweakPair (non-homeomorphic) algorithms, with vertices labeled by (ei, gi, ri) and edges labeled with orientations ±1. Six GNN architectures with two convolutional layers (GEN, GCN, GAT, NNConv) are trained using Adam optimizer (learning rate 5×10⁻³) for 150 epochs with 80/20 train/validation split and Cross-Entropy loss. The models process pairs of graphs through convolutional layers, aggregate node features into embeddings, and classify the concatenated embeddings as homeomorphic or not.

## Key Results
- Best-performing NNConv+NNConv architecture achieved 70.7% accuracy on validation set
- GCN+GCN and NNConv+NNConv architectures performed best among the six tested combinations
- Edge-aware convolutional layers (NNConv) did not significantly outperform edge-agnostic GCN layers, suggesting edge orientations play a weaker role in homeomorphism classification than expected
- Loss function continued decreasing through 150 epochs, indicating potential for improved accuracy with longer training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GNNs can learn approximate decision boundaries for the homeomorphism problem in polynomial time, bypassing super-polynomial complexity of von Neumann moves
- Mechanism: GNN processes pairs of plumbing graphs, uses convolutional layers to transform node features based on graph structure, aggregates them into embeddings, and classifier outputs binary prediction
- Core assumption: Space of graph-manifold homeomorphism classes is approximately learnable by polynomial-sized neural network, and training dataset captures sufficient variation
- Evidence anchors: Abstract statement about polynomial-time approximation; section discussion of von Neumann moves' super-polynomial complexity

### Mechanism 2
- Claim: Choice of convolutional layer architecture affects network's sensitivity to edge labels critical for capturing homeomorphism equivalence
- Mechanism: GCN layers ignore edge labels while GEN, GAT, NNConv layers incorporate edge information into message passing
- Core assumption: Edge orientations not fully redundant due to R0 move; preserving orientation information helps distinguish homeomorphism classes
- Evidence anchors: Section noting GCN insensitivity to edge labels; discussion of R0 von Neumann move's effect on edge label importance

### Mechanism 3
- Claim: Accuracy improves with longer training time and larger datasets as loss function hasn't fully stabilized
- Mechanism: Training loss decreases over epochs, indicating network still learning; NNConv+NNConv reached peak accuracy at final epoch
- Core assumption: Training data distribution representative of problem space, network architecture has sufficient capacity
- Evidence anchors: Section noting best-performing GNNs have same architecture; discussion of loss function trends in Figure 3

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and their convolutional layers (GCN, GAT, GEN, NNConv)
  - Why needed here: GNNs are core tool used to transform graph-structured data into embeddings that can be classified for homeomorphism
  - Quick check question: What is the difference between spectral and spatial convolutional layers in GNNs, and why does it matter for edge-aware vs edge-agnostic processing?

- Concept: Plumbing graphs and von Neumann moves for graph-manifolds
  - Why needed here: Understanding mathematical structure of problem is essential to interpret what GNN is learning and why certain architectures perform better
  - Quick check question: How do von Neumann moves define equivalence classes of plumbing graphs, and what role do edge orientations play in this equivalence?

- Concept: Supervised learning and dataset construction for graph pairs
  - Why needed here: Accuracy of GNN depends on quality and representativeness of training data generated via algorithms like EquivPair, InEquivPair, TweakPair
  - Quick check question: How are homeomorphic and non-homeomorphic graph pairs generated for training, and what ensures dataset is balanced and diverse?

## Architecture Onboarding

- Component map: Input → Conv1 → Conv2 → Aggregation → Classification → Output
- Critical path: Input → Conv1 → Conv2 → Aggregation → Classification → Output
- Design tradeoffs:
  - Edge-aware vs edge-agnostic layers: GCN ignores edge labels, potentially missing information; edge-aware layers capture more detail but add complexity
  - Same vs mixed convolutional layers: Same-layer architectures perform better than mixed ones, possibly due to consistent feature transformation
  - Training time vs accuracy: Longer training can improve accuracy but increases computational cost; dataset size also affects performance
- Failure signatures:
  - Low accuracy (< 60%): Model not learning meaningful patterns; check data quality, label balance, and architecture capacity
  - High training accuracy but low validation accuracy: Overfitting; consider regularization, data augmentation, or simpler architecture
  - Slow convergence: Learning rate too low or architecture too deep; try adjusting optimizer or reducing layers
- First 3 experiments:
  1. Train GCN+GCN and NNConv+NNConv on same dataset; compare accuracy and loss curves to assess impact of edge-awareness
  2. Train with and without edge labels in input to see how much orientation information affects performance
  3. Vary number of training epochs (e.g., 50, 100, 150) for best architecture to determine if accuracy plateaus or continues improving

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical limit of accuracy achievable by Graph Neural Networks in determining homeomorphic graph-manifolds, and how does this compare to computational efficiency of von Neumann moves?
- Basis in paper: Explicit mention of 70.7% accuracy and suggestion that longer training/larger datasets could improve accuracy; discussion of super-polynomial vs polynomial complexity trade-off
- Why unresolved: Paper doesn't provide theoretical framework for maximum achievable accuracy or compare to exact methods' computational cost
- What evidence would resolve it: Theoretical analysis of GNN limitations in graph homeomorphism problems, comparative studies of accuracy vs computational cost for various graph sizes

### Open Question 2
- Question: How sensitive are Graph Neural Networks to edge labels in plumbing graphs when determining homeomorphic graph-manifolds?
- Basis in paper: Explicit note that GCN layers are insensitive to edge labels while NNConv layers are not, yet architectures perform similarly; suggestion this may be due to R0 von Neumann move
- Why unresolved: Paper doesn't quantify importance of edge labels in dataset or determine what percentage of pairs are identical modulo edge labels
- What evidence would resolve it: Detailed analysis of edge label importance in dataset, quantification of percentage identical modulo edge labels, studies on how accuracy differences correlate with edge label sensitivity

### Open Question 3
- Question: Can Graph Neural Networks be extended to predict not just homeomorphism but also number of protected operators in 3d N=2 supersymmetric quantum field theories derived from compactifying 6d SCFTs on graph-manifolds?
- Basis in paper: Explicit discussion of motivation to study BPS spectra and q-series invariants; suggestion GNNs could predict when graphs have same first N coefficients in q-series
- Why unresolved: Paper focuses on homeomorphism detection and doesn't explore predictions related to quantum field theory observables or BPS spectra
- What evidence would resolve it: Development of GNN architectures capable of predicting q-series coefficients, validation against known q-series for graph-manifolds

## Limitations

- The 70.7% accuracy represents only a heuristic approximation rather than reliable decision procedure for homeomorphism detection, with practical utility threshold unclear
- Dataset generation methods (EquivPair, InEquivPair, TweakPair) are not fully specified, making it difficult to assess whether training data captures full complexity or introduces biases
- Analysis of architectural differences is limited - counterintuitive finding that edge-aware layers don't significantly outperform edge-agnostic layers lacks deeper exploration

## Confidence

- **High Confidence**: Core claim that GNNs can achieve polynomial-time approximate solutions to homeomorphism problem at cost of accuracy (directly supported by 70.7% experimental results)
- **Medium Confidence**: Comparative analysis of different GNN architectures and performance characteristics (results presented but small differences and lack of deeper analysis create uncertainty)
- **Low Confidence**: Claim that longer training times and larger datasets could significantly improve accuracy (speculative, not empirically validated within study's constraints)

## Next Checks

1. **Extended Training Analysis**: Train NNConv+NNConv architecture for 300-500 epochs (or until convergence) to empirically determine if accuracy plateaus or continues improving, validating claim about training time effects

2. **Ablation Study on Edge Labels**: Systematically remove edge orientations from input data and retrain models to quantify exactly how much information edge labels contribute to performance, addressing counterintuitive finding about edge-aware vs edge-agnostic layers

3. **Dataset Diversity Analysis**: Generate and test on datasets with systematically varied graph sizes, label distributions, and complexity levels to identify whether performance degradation correlates with specific structural properties, helping understand generalization limits