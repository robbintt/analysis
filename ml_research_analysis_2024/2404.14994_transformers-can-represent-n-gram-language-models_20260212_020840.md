---
ver: rpa2
title: Transformers Can Represent $n$-gram Language Models
arxiv_id: '2404.14994'
source_url: https://arxiv.org/abs/2404.14994
tags:
- transformer
- attention
- n-gram
- symbol
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the representational capacity of transformer
  language models (LMs) in terms of their ability to represent probability distributions
  over strings, specifically focusing on the relationship between transformer LMs
  and n-gram LMs. The authors show that transformer LMs using hard or sparse attention
  mechanisms can exactly represent any n-gram LM, providing a concrete lower bound
  on their probabilistic representational capacity.
---

# Transformers Can Represent $n$-gram Language Models

## Quick Facts
- arXiv ID: 2404.14994
- Source URL: https://arxiv.org/abs/2404.14994
- Authors: Anej Svete; Ryan Cotterell
- Reference count: 40
- This paper shows that transformer LMs using hard or sparse attention can exactly represent any n-gram LM, providing a concrete lower bound on their probabilistic representational capacity.

## Executive Summary
This paper establishes that transformer language models have sufficient representational capacity to exactly simulate any n-gram language model. The authors prove this through three constructive mechanisms: using n-1 heads to uniquely attend to each position in the history, using n-1 layers to accumulate the history through shifting, or using a single head and layer with position-scaled encodings. These results provide the first concrete lower bound on transformer LMs' ability to represent probability distributions over strings, demonstrating that they can implement the n-gram assumption exactly rather than just approximating it.

## Method Summary
The authors define a formal framework for analyzing transformer representational capacity by considering transformers with hard or sparse attention mechanisms. They prove three theorems showing different architectural configurations (varying numbers of heads and layers) that can exactly represent any n-gram LM. The key insight is that transformers can implement the n-gram assumption - that only the last n-1 symbols matter for predicting the next symbol - by either isolating each symbol in the history through multiple heads, accumulating the history through multiple layers, or encoding the entire history into a single vector. The proofs constructively demonstrate how to design the query, key, value, and output transformations to achieve this simulation.

## Key Results
- Transformer LMs with hard or sparse attention can exactly represent any n-gram LM
- Three mechanisms demonstrated: n-1 heads, n-1 layers, or single head/single layer with complex transformations
- Establishes a concrete lower bound on transformer LMs' probabilistic representational capacity
- Shows trade-off between number of heads, layers, and complexity of non-linear transformations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hard attention transformer LMs can simulate n-gram LMs using n-1 heads, where each head uniquely attends to one position in the history.
- Mechanism: The transformer uses n-1 heads to identify each symbol in the preceding n-1 positions. Each head has a query transformation that encodes which position to attend to, and a key transformation that exposes positional information. The value transformation passes through the symbol identity. The attended symbols are then combined using a logical AND operation implemented by an MLP to form a one-hot encoding of the full history, which indexes the conditional probabilities.
- Core assumption: The scoring function (dot product) can be designed so that each head uniquely attends to exactly one position in the history, and the MLP can perform the logical AND operation to combine the one-hot encodings.
- Evidence anchors:
  - [abstract] "We show that transformer LMs using the hard or sparse attention mechanisms can exactly represent any n-gram LM"
  - [section] "Given an n-gram LM p, we can construct a weakly equivalent LM pT defined by a transformer T that looks back at the preceding n-1 positions using n-1 heads, each of them uniquely attending to exactly one position"
  - [corpus] "Can Transformers Learn $n$-gram Language Models?" - This is directly related work that extends the question beyond just representational capacity
- Break condition: If the attention mechanism cannot uniquely attend to a single position (e.g., with soft attention where all positions receive some weight), or if the MLP cannot perform the logical AND operation with sufficient precision.

### Mechanism 2
- Claim: Hard attention transformer LMs can simulate n-gram LMs using n-1 layers with a single head, where each layer shifts and accumulates the history.
- Mechanism: Instead of using n-1 heads, the transformer uses n-1 layers. Each layer attends to the immediately preceding position and copies that symbol forward, shifting it down one "slot" in the representation. After n-1 layers, the contextual representation contains the ordered history of the preceding n-1 symbols, which can then be used to index the conditional probabilities.
- Core assumption: The value transformation V can be designed to shift the representation of the symbol at position t-1 down by one slot in the contextual representation, and this shifting can be accumulated over n-1 layers to build up the full history.
- Evidence anchors:
  - [abstract] "We also study the role of the number of heads (Theorem 3.1) and the number of layers (Theorem 3.2), illustrating a trade-off between the number of heads, layers, and the complexity of the non-linear transformations"
  - [section] "Whereas the transformer LM constructed in Theorem 3.1 used n-1 heads to look at all the n-1 positions of interest, an n-1-layer transformer LM can use the n-1 layers to look back at the immediately preceding position and copy it forward n-1 times"
  - [corpus] "Lower Bounds on the Expressivity of Recurrent Neural Language Models" - This related work likely explores similar trade-offs between architectural components
- Break condition: If the shifting mechanism cannot be implemented precisely enough to maintain the correct ordering of symbols in the history, or if residual connections interfere with the accumulation process.

### Mechanism 3
- Claim: Hard attention transformer LMs can simulate n-gram LMs using a single head and a single layer by encoding the entire history into a vector using position-scaled one-hot encodings.
- Mechanism: A single head attends to all positions in the history (t-n+1 to t-1) with equal weight (score 0) and to all other positions with negative weight. The value transformation uses position-scaled one-hot encodings of the symbols, where the scale factor encodes the position. This creates a vector where each entry contains digits that encode the positions of that symbol in the history. An MLP then decodes this vector into a one-hot encoding of the history by extracting the position digits.
- Core assumption: The position-augmented symbol representations can be designed to encode both symbol identity and position in a way that can be decoded by an MLP, and the scoring function can be designed to attend only to the history positions.
- Evidence anchors:
  - [abstract] "Simulation is possible even with a single head and a single layer (Theorem 3.3) but might require a more elaborate set of non-linear transformations"
  - [section] "The bulk of this construction lies in the encoding yt-1t-n+1 in a vector that can be constructed by a single attention head in one layer"
  - [corpus] Weak - this is a novel construction not directly referenced in the related papers
- Break condition: If the position-scaled encodings cannot be decoded precisely enough by the MLP, or if the scoring function cannot be designed to attend only to the history positions.

## Foundational Learning

- Concept: n-gram language models and the n-gram assumption
  - Why needed here: The paper is about connecting transformer LMs to n-gram LMs, so understanding what n-gram LMs are and how they work is fundamental.
  - Quick check question: What is the n-gram assumption, and how does it simplify the computation of conditional probabilities in language models?

- Concept: Transformer architecture and attention mechanism
  - Why needed here: The paper analyzes how transformers can represent n-gram LMs, so understanding the transformer architecture and attention mechanism is essential.
  - Quick check question: How does the attention mechanism in a transformer work, and what role do the query, key, and value transformations play?

- Concept: Representation-based language models and the output matrix E
  - Why needed here: The paper uses the definition of a representation-based LM where the output matrix E indexes the conditional probabilities by the contextual representation, so understanding this formulation is important.
  - Quick check question: In a representation-based LM, how is the conditional probability p(yt|y< t) computed from the contextual representation and the output matrix E?

## Architecture Onboarding

- Component map:
  Input -> Position-augmented symbol representations r -> Attention (Q, K, V, O) -> Scoring function f -> Normalization (hardmax/sparsemax/softmax) -> Head combining H -> Final transformation F -> Output matrix E

- Critical path: For the n-1 heads, n-1 layers, or single head/single layer constructions, the critical path is identifying the history of interest (yt-1t-n+1) and using it to index the conditional probabilities in E.

- Design tradeoffs:
  - Number of heads vs. number of layers: More heads require fewer layers but increase the dimensionality of the value vectors, while more layers require fewer heads but increase the depth of the network.
  - Precision of positional encodings: More precise positional encodings allow for more accurate identification of positions but may require more bits to represent.
  - Complexity of MLP: More complex MLPs can perform more sophisticated operations (like the AND operation or decoding position-scaled encodings) but may be harder to train and may require more parameters.

- Failure signatures:
  - If the attention mechanism does not uniquely attend to the correct positions, the history will be incorrectly identified.
  - If the MLP cannot perform the required operations (AND, decoding) with sufficient precision, the one-hot encoding of the history will be incorrect.
  - If the positional encodings are not precise enough, the attention mechanism may not be able to distinguish between different positions.

- First 3 experiments:
  1. Implement the n-1 heads construction and verify that it correctly identifies the history and computes the conditional probabilities for a simple n-gram LM (e.g., a bigram or trigram model).
  2. Implement the n-1 layers construction and verify that it correctly accumulates the history and computes the conditional probabilities.
  3. Implement the single head/single layer construction and verify that it correctly encodes and decodes the history using position-scaled encodings.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can transformer language models with soft attention approximate n-gram language models over arbitrarily long strings?
- Basis in paper: Inferred - The paper explicitly notes that analyzing soft attention transformers requires a different type of analysis in terms of approximation of probabilities over arbitrarily long strings, and conjectures that soft attention transformers can approximate LMs whose average string length is finite.
- Why unresolved: The authors explicitly state that this is a difficult problem because Î£* is an infinite set, and simply scaling model parameters to a large constant is not sufficient for a complete study.
- What evidence would resolve it: A theoretical proof showing that soft attention transformers can approximate any n-gram LM to arbitrary precision over strings of finite average length, or a counterexample demonstrating a fundamental limitation.

### Open Question 2
- Question: Can transformer language models learn the specific mechanisms described in this paper (using multiple heads/layers to simulate n-gram models) from data, or do they employ different strategies?
- Basis in paper: Explicit - The authors state that the constructions are meant to showcase the existence of mechanisms and do not suggest that the same mechanisms will be employed by models used in practice.
- Why unresolved: The paper focuses on theoretical representational capacity rather than learnability. Empirical studies would be needed to determine whether trained transformers actually use these specific mechanisms.
- What evidence would resolve it: Empirical analysis of trained transformer language models showing whether they use similar head/layer specialization patterns to implement n-gram-like behavior, or evidence of alternative mechanisms.

### Open Question 3
- Question: What is the exact upper bound on the representational capacity of transformer language models beyond n-gram models?
- Basis in paper: Inferred - The authors note that their lower bounds are somewhat loose and that transformer LMs can likely represent much more than n-gram models, expecting many existing results on computational power to extend to the probabilistic setting.
- Why unresolved: The paper only establishes lower bounds and does not consider any upper bounds. The relationship between transformers and more expressive formal models of computation in the probabilistic setting remains unexplored.
- What evidence would resolve it: Theoretical proofs establishing upper bounds on the classes of probability distributions over strings that transformer LMs can represent, or empirical evidence showing transformer capabilities beyond n-gram models.

## Limitations

- The paper establishes representational capacity rather than learnability - it proves transformers can represent n-gram LMs but does not address whether gradient-based learning can discover these representations
- The constructions require non-differentiable hard attention mechanisms, raising questions about how such representations would be learned in practice
- The proofs assume idealized conditions with perfect positional encodings and precise MLP operations that may not hold in practice

## Confidence

**High confidence** in the theoretical claims: The proofs are mathematically rigorous and the three mechanisms for simulating n-gram LMs are constructively demonstrated.

**Medium confidence** in practical implications: While the theoretical results are sound, the gap between representational capacity and learnability remains significant.

**Low confidence** in scalability claims: The paper does not address how these constructions scale to large n or what happens when the representational requirements exceed practical computational limits.

## Next Checks

1. Implement a differentiable approximation of the hard attention mechanism and test whether gradient-based learning can discover n-gram-like representations when trained on n-gram data. Measure the KL divergence between the learned transformer distribution and the target n-gram distribution.

2. Systematically vary the precision of positional encodings and measure at what point the transformer fails to correctly identify symbol positions in the history. This would quantify the practical limits of the construction.

3. Train transformers with the exact architectures proven in the theorems on synthetic n-gram data, then analyze whether the learned attention patterns and transformations match the theoretical constructions. Use techniques like attention visualization and probing classifiers to verify the internal representations.