---
ver: rpa2
title: 'Chasing COMET: Leveraging Minimum Bayes Risk Decoding for Self-Improving Machine
  Translation'
arxiv_id: '2405.11937'
source_url: https://arxiv.org/abs/2405.11937
tags:
- translation
- decoding
- comet
- test
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a study on using Minimum Bayes Risk (MBR) decoding
  for self-improvement in machine translation. The approach involves fine-tuning models
  on their MBR-decoded forward translations, leveraging COMET as the MBR utility metric.
---

# Chasing COMET: Leveraging Minimum Bayes Risk Decoding for Self-Improving Machine Translation

## Quick Facts
- arXiv ID: 2405.11937
- Source URL: https://arxiv.org/abs/2405.11937
- Reference count: 24
- Key outcome: MBR decoding guided by COMET significantly improves translation quality across three language pairs

## Executive Summary
This paper presents a novel approach to self-improvement in machine translation using Minimum Bayes Risk (MBR) decoding with COMET as the utility metric. The method involves generating multiple translation candidates, selecting the best ones using MBR decoding with COMET, and fine-tuning the model on this synthetic data. Experiments across English-German, Czech-Ukrainian, and English-Hausa language pairs demonstrate significant quality improvements, particularly in domain adaptation and low-resource scenarios. The study also explores iterative application of this self-improvement process and the importance of language-specific utility metrics.

## Method Summary
The approach uses MBR decoding with COMET as the utility metric to select high-quality translations from candidate sets generated by beam search. The base Marian Transformer models are fine-tuned on synthetic datasets created from MBR-selected translations. The process can be applied iteratively for further improvements. Experiments investigate optimal sample sizes, domain adaptation scenarios, low-resource language pairs, and the impact of using language-specific metrics like AfriCOMET for English-Hausa.

## Key Results
- MBR decoding with COMET significantly improves translation quality across all tested language pairs
- Domain adaptation shows enhanced performance when fine-tuning on MBR-decoded synthetic data
- Iterative application of MBR self-improvement yields additional gains, though with diminishing returns after multiple iterations
- AfriCOMET performs comparably to WMT22 COMET for English-Hausa, suggesting language-specific metrics may not always be necessary

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MBR decoding guided by COMET selects translations that better align with human preferences, improving translation quality.
- Mechanism: MBR decoding reranks translation candidates by calculating expected utility scores against all other candidates, selecting the hypothesis with the highest expected score. COMET serves as the utility function, providing a learned metric that correlates well with human judgments.
- Core assumption: The COMET metric accurately captures translation quality aspects that matter to humans and can effectively distinguish better translations from worse ones.
- Evidence anchors:
  - [abstract] "By employing COMET as the MBR utility metric, we aim to achieve the reranking of translations that better aligns with human preferences."
  - [section 2] "Freitag et al. (2022) proposed using reference-based metrics, such as BLEURT (Sellam et al., 2020a) and Quality Estimation (QE) models, such as COMET-QE (Rei et al., 2021) for reranking the set of hypotheses produced by the NMT model."
  - [corpus] Weak evidence - related papers discuss MBR decoding but don't specifically validate COMET's effectiveness in this self-improvement context.
- Break condition: If COMET fails to capture important translation quality aspects or becomes biased toward specific linguistic patterns that don't reflect human preferences.

### Mechanism 2
- Claim: Fine-tuning on MBR-decoded synthetic data creates a feedback loop that improves the model's ability to generate high-quality translations.
- Mechanism: The model generates candidates, MBR selects the best ones based on COMET scores, and the model is fine-tuned on this filtered dataset. This process reinforces patterns that lead to higher COMET scores.
- Core assumption: The synthetic dataset created through MBR decoding contains higher-quality translations than the original training data, and fine-tuning on this data transfers these quality improvements to the model's generation process.
- Evidence anchors:
  - [section 3.1] "Step 3: Model Fine-tuning Fine-tune the base model on the synthetically created dataset."
  - [section 4.5] "We used the above data to train the following models" followed by multiple MBR-finetuned variations.
  - [corpus] Weak evidence - related papers discuss MBR decoding but don't specifically validate the self-improvement feedback loop.
- Break condition: If the MBR-decoded dataset doesn't contain genuinely better translations, or if fine-tuning overfits to the synthetic data patterns.

### Mechanism 3
- Claim: Iterative application of MBR-based self-improvement can lead to continuous quality gains, though with diminishing returns and potential overfitting.
- Mechanism: After initial self-improvement, the process is repeated: the improved model generates new candidates, MBR selects the best, and fine-tuning continues. This iterative refinement progressively enhances translation quality.
- Core assumption: Each iteration produces a meaningfully better synthetic dataset that further improves the model, and the gains don't diminish to zero or become negative due to overfitting.
- Evidence anchors:
  - [section 3.5] "Following the initial self-improvement through MBR decoding, we explored the possibility of applying it iteratively to further enhance the model's translation quality."
  - [section 5.3] "Tables 5 and 6 showcase the impact of iterative training with MBR decoding" showing that the second iteration improves scores but the third iteration shows decreases in some metrics.
  - [corpus] Weak evidence - related papers don't discuss iterative MBR self-improvement specifically.
- Break condition: When subsequent iterations show decreased performance on metrics other than the MBR utility metric, indicating overfitting to COMET scores rather than genuine quality improvements.

## Foundational Learning

- Concept: Minimum Bayes Risk (MBR) decoding
  - Why needed here: MBR decoding is the core technique that enables selecting high-quality translations from candidate sets based on a utility metric.
  - Quick check question: How does MBR decoding differ from beam search in terms of selection criteria?

- Concept: COMET metric and neural evaluation metrics
  - Why needed here: COMET serves as the utility function in MBR decoding, and understanding its properties is crucial for interpreting results and potential biases.
  - Quick check question: What makes COMET different from traditional metrics like BLEU in terms of correlation with human judgments?

- Concept: Domain adaptation and low-resource translation challenges
  - Why needed here: The paper specifically addresses these scenarios, and understanding their unique challenges helps interpret why MBR self-improvement is valuable in these contexts.
  - Quick check question: Why might self-improvement through MBR decoding be particularly beneficial for low-resource language pairs?

## Architecture Onboarding

- Component map: Base NMT model -> Beam search candidate generation -> MBR decoding engine -> COMET metric evaluation -> Fine-tuning pipeline -> Evaluation framework
- Critical path: 1. Generate candidates using beam search; 2. Apply MBR decoding with COMET as utility function; 3. Create synthetic dataset from selected translations; 4. Fine-tune base model on synthetic data; 5. Evaluate improvements across multiple metrics
- Design tradeoffs: Sample size vs. computational cost; Metric choice vs. generalizability; Iterative application vs. overfitting
- Failure signatures: Performance improvements only on COMET but not on other metrics; Decreased performance on general domain test sets after domain-specific fine-tuning; Iterative improvements plateau or reverse after several iterations
- First 3 experiments: 1. Test different sample sizes (10, 50, 100, 200) with beam search on a small validation set; 2. Compare MBR decoding with beam search decoding directly on the same validation set; 3. Run a single iteration of MBR self-improvement on a small subset of training data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of translation candidates for MBR decoding in self-improvement scenarios?
- Basis in paper: [explicit] The paper investigates various numbers of translation candidates (10, 25, 50, 100, 200, 300, 400, 500) for MBR decoding across different language pairs and decoding algorithms.
- Why unresolved: The paper finds that while initial increases in the number of samples show rapid gains, further increases do not result in such large improvements and may even deteriorate performance on n-gram metrics.
- What evidence would resolve it: Systematic experiments comparing translation quality across a wider range of candidate numbers and language pairs, potentially including adaptive candidate selection methods.

### Open Question 2
- Question: How does the choice of MBR utility metric impact the effectiveness of self-improvement for low-resource languages?
- Basis in paper: [explicit] The English-Hausa experiments compare MBR decoding using WMT22 COMET and AfriCOMET, finding that both models yield comparable and significant improvements.
- Why unresolved: While the paper shows that both metrics lead to improvements, it doesn't fully explore the potential need for language-specific metrics or the impact of metric choice on different low-resource language pairs.
- What evidence would resolve it: Comparative studies of MBR-guided self-improvement using various metrics (general vs. language-specific) across multiple low-resource language pairs.

### Open Question 3
- Question: What is the optimal fine-tuning strategy for models trained on MBR-decoded data?
- Basis in paper: [inferred] The Czech-Ukrainian experiments compare three fine-tuning approaches: standard fine-tuning, fine-tuning with a higher learning rate, and resuming training from the baseline model.
- Why unresolved: The paper finds that standard fine-tuning yields the smallest improvements, while the other two approaches achieve comparable performance, but doesn't fully explore the reasons behind these differences or potential alternative strategies.
- What evidence would resolve it: Detailed analysis of the learning dynamics and model behavior under different fine-tuning strategies, potentially including adaptive learning rate schedules or curriculum learning approaches.

## Limitations
- Findings are primarily validated on a limited set of language pairs and specific domains, limiting generalizability
- Computational cost of generating and evaluating 50 candidates per sentence may be prohibitive for production systems
- Long-term effects of iterative self-improvement beyond three iterations remain unexplored

## Confidence

- **High confidence**: The core mechanism of MBR decoding with COMET for translation quality improvement, supported by multiple validation metrics showing consistent gains across language pairs
- **Medium confidence**: The domain adaptation benefits and low-resource effectiveness, as these are demonstrated on specific datasets with limited validation across multiple domains or resource settings
- **Medium confidence**: The iterative self-improvement approach, particularly regarding the second iteration's effectiveness and the third iteration's potential overfitting issues

## Next Checks

1. **Cross-domain generalization test**: Evaluate MBR-finetuned models on out-of-domain test sets to assess whether domain-specific improvements come at the cost of general translation quality degradation.
2. **Sample size sensitivity analysis**: Systematically test MBR decoding with varying candidate set sizes (10, 25, 50, 100, 200) on a validation set to identify the optimal tradeoff between translation quality gains and computational cost.
3. **Alternative metric comparison**: Replace COMET with other reference-based metrics (e.g., BLEURT, COMET-QE) as the MBR utility function on the same datasets to determine if COMET's effectiveness is unique or if similar improvements can be achieved with other metrics.