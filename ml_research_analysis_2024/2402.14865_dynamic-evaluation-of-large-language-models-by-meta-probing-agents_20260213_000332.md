---
ver: rpa2
title: Dynamic Evaluation of Large Language Models by Meta Probing Agents
arxiv_id: '2402.14865'
source_url: https://arxiv.org/abs/2402.14865
tags:
- probing
- evaluation
- llms
- language
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces meta probing agents (MPA), a dynamic evaluation
  protocol for large language models (LLMs) inspired by psychometrics. MPA addresses
  the challenge of data contamination in LLM evaluation by using probing and judging
  agents to automatically transform existing problems into new ones, following three
  basic cognitive abilities: language understanding, problem solving, and domain knowledge.'
---

# Dynamic Evaluation of Large Language Models by Meta Probing Agents

## Quick Facts
- arXiv ID: 2402.14865
- Source URL: https://arxiv.org/abs/2402.14865
- Reference count: 24
- Key outcome: Most LLMs achieve poorer performance on MPA-generated benchmarks, indicating potential data contamination in standard evaluation.

## Executive Summary
This paper introduces Meta Probing Agents (MPA), a dynamic evaluation protocol for large language models inspired by psychometric theory. MPA addresses the critical challenge of data contamination in LLM evaluation by using probing and judging agents to automatically transform existing problems into new ones, thereby creating novel evaluation samples that reveal genuine capabilities rather than memorized patterns. Extensive experiments on popular benchmarks demonstrate that most LLMs perform worse on MPA-generated benchmarks compared to original benchmarks, confirming potential data contamination issues.

The protocol is built on three basic cognitive abilities—language understanding, problem solving, and domain knowledge—and can be flexibly combined for multifaceted analysis. MPA not only serves as an evaluation protocol but also as a data augmentation approach to improve LLM performance. The multifaceted analysis reveals strong correlations between basic abilities and an implicit Matthew effect on model size, where larger models tend to demonstrate stronger correlations across abilities.

## Method Summary
MPA uses LLMs as probing and judging agents to automatically generate new evaluation problems from existing benchmarks based on psychometric principles. The probing agent transforms questions according to five principles corresponding to three cognitive abilities (language understanding, problem solving, domain knowledge), while the judge agent validates the semantic consistency of transformations. This dynamic approach creates probing benchmarks that reveal genuine model capabilities rather than memorized patterns. MPA can also be used for data augmentation by fine-tuning models on the transformed samples.

## Key Results
- Most LLMs (GPT-4-Turbo, GPT-3.5-Turbo, Gemini-Pro, Llama2-70b-chat, Yi-34b-chat, Mixtral-8x7b-Instruct) perform worse on MPA-generated benchmarks than on original benchmarks
- Strong correlations exist between basic abilities (language understanding, problem solving, domain knowledge)
- Implicit Matthew effect observed: larger models show stronger correlations between abilities
- MPA-generated samples can improve LLM performance when used for data augmentation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Meta probing agents (MPA) mitigate data contamination by dynamically transforming existing evaluation problems into new ones using probing and judging agents.
- **Mechanism**: The probing agent transforms a given question into a new one based on psychometric principles (language understanding, problem solving, domain knowledge), while the judge agent validates the transformed question's consistency with the original.
- **Core assumption**: LLMs trained on static benchmarks have memorized specific problem patterns, and dynamic transformation can reveal genuine capabilities by creating novel problem variants.
- **Evidence anchors**:
  - [abstract]: "MPA addresses the challenge of data contamination in LLM evaluation by using probing and judging agents to automatically transform existing problems into new ones"
  - [section]: "Instead of relying on specific rules like DyVal (Zhu et al., 2023a), we employ LLMs as agents to automatically generate new problems based on the given evaluation samples"
  - [corpus]: Weak - related papers focus on AI judging AI but don't specifically address data contamination through dynamic transformation
- **Break condition**: If probing agents fail to maintain semantic equivalence while transforming questions, the judge agent will reject the transformation, preventing meaningful evaluation.

### Mechanism 2
- **Claim**: The modular design of MPA enables multifaceted analysis of LLMs' basic cognitive abilities by dynamically combining psychometric principles.
- **Mechanism**: MPA uses five principles corresponding to three cognitive abilities (language understanding: paraphrasing questions/choices, permuting choices; problem solving: adding extra context; domain knowledge: adding new choices) that can be flexibly combined.
- **Core assumption**: LLMs' performance varies across different cognitive abilities, and isolating these abilities through principled transformations reveals their strengths and weaknesses.
- **Evidence anchors**:
  - [abstract]: "These basic abilities are also dynamically configurable, allowing multifaceted analysis"
  - [section]: "Inspired by psychometric theory on the three basic cognitive abilities, our Meta Probing Agent (MPA) designs corresponding principles that transforms original benchmarks into a new one"
  - [corpus]: Weak - related papers discuss AI evaluation but don't specifically address modular psychometric analysis
- **Break condition**: If principles don't adequately isolate specific abilities, or if LLMs demonstrate similar performance across all ability combinations, the multifaceted analysis loses discriminatory power.

### Mechanism 3
- **Claim**: MPA-generated samples can serve as data augmentation to improve LLM performance by exposing models to diverse problem variants during fine-tuning.
- **Mechanism**: The transformed questions created by MPA are semantically similar to original questions but with structural variations, providing additional training data that helps models generalize better.
- **Core assumption**: Exposure to diverse problem formulations during training improves a model's ability to handle novel problem variations at test time.
- **Evidence anchors**:
  - [abstract]: "MPA can also be used as a data augmentation approach to enhance LLMs"
  - [section]: "The fine-tuning results demonstrated that MPA is not only an evaluation protocol, but a general data augmentation approach to improve the performance of LLMs"
  - [corpus]: Weak - related papers discuss AI evaluation and training but don't specifically address data augmentation through dynamic problem transformation
- **Break condition**: If augmented data introduces semantic inconsistencies or if models overfit to the specific transformation patterns rather than learning genuine problem-solving skills.

## Foundational Learning

- **Concept**: Psychometric theory of cognitive abilities
  - Why needed here: MPA is directly inspired by psychometric theory's categorization of cognitive abilities into language understanding, problem solving, and domain knowledge, which guides the design of evaluation principles.
  - Quick check question: What are the three basic cognitive abilities in psychometric theory that MPA evaluates?

- **Concept**: Adversarial validation using judge agents
  - Why needed here: The judge agent ensures transformed questions maintain semantic equivalence to original questions, preventing meaningless or incorrect transformations from being used in evaluation.
  - Quick check question: What is the binary response system used by the judge agent to validate transformed questions?

- **Concept**: Dynamic evaluation vs static benchmarks
  - Why needed here: Static benchmarks can be memorized by LLMs during training, while dynamic evaluation creates novel problem variants that reveal genuine capabilities rather than memorization.
  - Quick check question: Why does MPA use dynamic evaluation instead of static benchmarks?

## Architecture Onboarding

- **Component map**: Original benchmarks (MMLU, ARC-C, GSM8K, BBH) -> Probing agents (transform questions) -> Judge agents (validate consistency) -> Probing benchmarks -> Evaluated LLMs (GPT-4-Turbo, GPT-3.5-Turbo, Gemini-Pro, Llama2-70b-chat, Yi-34b-chat, Mixtral-8x7b-Instruct) -> Performance analysis

- **Critical path**: 
  1. Load original benchmark samples
  2. Apply probing principles to transform questions
  3. Judge agent validates each transformation
  4. Generate probing benchmark
  5. Evaluate target LLMs on probing benchmark
  6. Analyze performance across abilities and correlations

- **Design tradeoffs**:
  - Using powerful LLMs (GPT-4) as probing/judging agents ensures high-quality transformations but increases computational cost and dependency
  - Simpler transformation rules would be faster but might not adequately probe underlying abilities
  - More complex psychometric principles could provide better analysis but increase implementation complexity

- **Failure signatures**:
  - Judge agent consistently rejects probing agent transformations → probing agent prompts need refinement
  - All LLMs show similar performance drops across all probing benchmarks → transformations may be too difficult or not properly aligned with original abilities
  - No correlation between abilities → either the principles don't effectively isolate abilities or the evaluation lacks sensitivity

- **First 3 experiments**:
  1. Apply all five principles to MMLU dataset and verify judge agent acceptance rate is high (e.g., >90%)
  2. Compare performance of a single LLM on original vs probing MMLU benchmark to confirm degradation
  3. Evaluate the correlation between language understanding and problem solving abilities using transformed samples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of different LLMs on MPA-generated benchmarks correlate with their performance on other dynamic evaluation benchmarks like DyVal and NPHardEval?
- Basis in paper: [inferred] The paper demonstrates that most LLMs perform worse on MPA-generated benchmarks compared to original benchmarks, suggesting potential data contamination. It would be interesting to see if this trend holds true for other dynamic evaluation benchmarks as well.
- Why unresolved: The paper does not compare the performance of LLMs on MPA-generated benchmarks with their performance on other dynamic evaluation benchmarks.
- What evidence would resolve it: Conduct experiments evaluating the same LLMs on MPA-generated benchmarks and other dynamic evaluation benchmarks, and analyze the correlation between their performance on these benchmarks.

### Open Question 2
- Question: What is the impact of different prompting techniques, such as Chain-of-Thought and In-Context Learning, on the performance of LLMs on MPA-generated benchmarks?
- Basis in paper: [explicit] The paper mentions that prompt engineering techniques like Chain-of-Thought and In-Context Learning were explored, but they did not effectively boost the performance of LLMs on MPA-generated benchmarks.
- Why unresolved: The paper does not provide a detailed analysis of the impact of different prompting techniques on the performance of LLMs on MPA-generated benchmarks.
- What evidence would resolve it: Conduct experiments using different prompting techniques on MPA-generated benchmarks and compare the performance of LLMs with and without these techniques.

### Open Question 3
- Question: How does the performance of LLMs on MPA-generated benchmarks vary across different domains and tasks?
- Basis in paper: [explicit] The paper evaluates LLMs on four popular benchmarks (MMLU, ARC-C, GSM8K, and BBH) that cover a broad spectrum of computational challenges. However, it does not provide a detailed analysis of the performance of LLMs on MPA-generated benchmarks across different domains and tasks.
- Why unresolved: The paper does not provide a comprehensive analysis of the performance of LLMs on MPA-generated benchmarks across different domains and tasks.
- What evidence would resolve it: Conduct experiments evaluating LLMs on MPA-generated benchmarks across different domains and tasks, and analyze the performance trends.

### Open Question 4
- Question: How does the performance of LLMs on MPA-generated benchmarks relate to their size and training data?
- Basis in paper: [inferred] The paper mentions an implicit "Matthew effect" between model size and correlations of the basic abilities, suggesting that larger models tend to have stronger correlations. It would be interesting to see if this trend extends to the performance of LLMs on MPA-generated benchmarks.
- Why unresolved: The paper does not provide a detailed analysis of the relationship between the performance of LLMs on MPA-generated benchmarks and their size and training data.
- What evidence would resolve it: Conduct experiments evaluating LLMs of different sizes and with different training data on MPA-generated benchmarks, and analyze the relationship between their performance and these factors.

## Limitations
- The evaluation protocol's effectiveness depends heavily on the probing and judging agents' ability to maintain semantic equivalence while creating novel problem variants
- The data augmentation results primarily show improvement on original benchmarks rather than transfer to genuinely novel problems
- The paper doesn't extensively validate that judge rejections aren't overly conservative or that probing transformations consistently probe the intended cognitive abilities

## Confidence
- High confidence: MPA successfully creates novel evaluation samples that reduce performance compared to original benchmarks, indicating data contamination issues in standard evaluation.
- Medium confidence: The multifaceted analysis reveals meaningful correlations between basic abilities and an implicit Matthew effect on model size, though the psychometric validity of these correlations could be strengthened.
- Medium confidence: MPA serves as effective data augmentation, though results are primarily shown on original benchmarks rather than genuinely novel problems.

## Next Checks
1. Conduct ablation studies varying the judge agent's acceptance threshold to quantify its impact on probing benchmark quality and evaluate consistency.
2. Test whether models trained on MPA-augmented data show improved performance on genuinely novel problems (not MPA-generated variants of existing benchmarks).
3. Validate that judge agent rejections are appropriate by having human annotators assess whether rejected transformations maintain semantic equivalence to originals.