---
ver: rpa2
title: Document-level Claim Extraction and Decontextualisation for Fact-Checking
arxiv_id: '2406.03239'
source_url: https://arxiv.org/abs/2406.03239
tags: []
core_contribution: This paper tackles the challenge of extracting and decontextualizing
  check-worthy claims from documents for fact-checking. Existing methods focus on
  individual sentences, leading to redundancy and claims that are hard to understand
  out of context.
---

# Document-level Claim Extraction and Decontextualisation for Fact-Checking
## Quick Facts
- arXiv ID: 2406.03239
- Source URL: https://arxiv.org/abs/2406.03239
- Reference count: 11
- Key outcome: Novel framework for document-level claim extraction and decontextualization improves central sentence extraction (Precision@1 47.8, +10%) and evidence retrieval (+1.08 precision).

## Executive Summary
This paper addresses the challenge of extracting and decontextualizing check-worthy claims from documents for fact-checking. Current approaches typically operate at the sentence level, which leads to redundancy and claims that are difficult to understand out of context. The authors propose a novel framework that treats claim extraction as extractive summarization to identify central sentences, followed by a QA-based decontextualization process to rewrite these sentences for better standalone comprehension. Evaluation on the A VeriTeC dataset demonstrates that their method achieves a Precision@1 score of 47.8 for central sentence extraction, a 10% improvement over the previous best method. Additionally, decontextualized claims improve evidence retrieval by an average of 1.08 in precision.

## Method Summary
The authors propose a two-stage framework for document-level claim extraction and decontextualization. The first stage uses extractive summarization to identify central sentences from the document, which are likely to contain check-worthy claims. The second stage employs a QA-based approach to decontextualize these sentences, rewriting them to be standalone and more comprehensible. The method was evaluated on the A VeriTeC dataset, demonstrating significant improvements in both central sentence extraction (Precision@1 of 47.8, +10% over prior work) and downstream evidence retrieval performance (average +1.08 precision).

## Key Results
- Precision@1 for central sentence extraction: 47.8, a 10% improvement over previous best method.
- Decontextualized claims improve evidence retrieval by an average of 1.08 in precision.
- The framework significantly reduces redundancy and improves claim understandability compared to sentence-level approaches.

## Why This Works (Mechanism)
The approach leverages the strengths of extractive summarization to identify the most salient sentences within a document, ensuring that extracted claims are central and representative. The QA-based decontextualization step then rewrites these sentences to remove dependencies on the broader document context, making the claims standalone and easier to understand. This combination addresses the limitations of sentence-level methods, which often extract redundant or context-dependent claims that are less useful for fact-checking.

## Foundational Learning
- **Extractive Summarization**: Identifying central sentences from a document is crucial because claims must be representative and salient. Quick check: Are the selected sentences capturing the main topics of the document?
- **QA-based Decontextualization**: Rewriting sentences to be standalone is necessary to ensure claims are understandable without document context. Quick check: Can the decontextualized claim be understood without reading the original document?
- **Precision@1 Metric**: Measures the accuracy of the top-ranked extraction, important for evaluating the effectiveness of claim selection. Quick check: Does the highest-ranked claim accurately reflect the document's check-worthy content?
- **Evidence Retrieval Performance**: Assessing how well decontextualized claims aid in finding supporting evidence, a key step in fact-checking. Quick check: Do the decontextualized claims lead to more relevant evidence being retrieved?
- **Central Sentence Extraction**: The ability to identify sentences that contain the core information of the document, which is essential for effective claim extraction. Quick check: Are the extracted sentences truly central to the document's main points?
- **Check-worthy Claim Definition**: Understanding what makes a claim suitable for fact-checking, which guides the extraction and decontextualization process. Quick check: Does the framework align with established criteria for check-worthy claims?

## Architecture Onboarding
**Component Map**: Document -> Extractive Summarization -> Central Sentences -> QA-based Decontextualization -> Decontextualized Claims
**Critical Path**: Document is processed by extractive summarization to find central sentences, which are then rewritten by the QA-based decontextualization module to produce final claims.
**Design Tradeoffs**: The method trades off between the comprehensiveness of extractive summarization and the precision of QA-based rewriting. While extractive summarization may retain some contextual dependencies, the decontextualization step aims to resolve these for standalone comprehension.
**Failure Signatures**: Potential failures include extractive summarization selecting non-central or redundant sentences, and the QA-based decontextualization failing to fully remove context, resulting in claims that are still not fully standalone.
**First Experiments**:
1. Evaluate the full pipeline on a multilingual dataset (e.g., WiCE) to test generalizability.
2. Measure the impact of decontextualized claims on end-to-end fact-checking accuracy rather than just retrieval performance.
3. Conduct ablation studies to assess the relative contribution of extractive summarization versus QA-based decontextualization.

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on a single dataset (A VeriTeC), which is in English and may not generalize across domains or languages.
- The extractive summarization step may still retain contextual dependencies that the decontextualization step does not fully resolve.
- The claim selection step is unsupervised, which could introduce noise if the summarization model is not well-calibrated to the claim worthiness criteria.

## Confidence
- **High**: The framework improves central sentence extraction performance over prior methods, as Precision@1 and mAP metrics are reported with clear baselines.
- **Medium**: Decontextualization improves downstream evidence retrieval, since the improvement is measured on a single dataset and via a proxy task rather than full fact-checking accuracy.
- **Low**: Claims about cross-domain or multilingual applicability, due to lack of evaluation beyond A VeriTeC.

## Next Checks
1. Evaluate the full pipeline on a multilingual dataset (e.g., WiCE) to test generalizability.
2. Measure the impact of decontextualized claims on end-to-end fact-checking accuracy rather than just retrieval performance.
3. Conduct ablation studies to assess the relative contribution of extractive summarization versus QA-based decontextualization.