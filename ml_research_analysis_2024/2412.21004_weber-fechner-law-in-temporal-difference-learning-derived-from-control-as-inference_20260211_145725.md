---
ver: rpa2
title: Weber-Fechner Law in Temporal Difference learning derived from Control as Inference
arxiv_id: '2412.21004'
source_url: https://arxiv.org/abs/2412.21004
tags:
- learning
- page
- value
- which
- control
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper uncovers a nonlinear update rule in temporal difference
  (TD) learning derived from the Control as Inference framework, revealing the Weber-Fechner
  Law (WFL). WFL describes how the update magnitude in RL depends logarithmically
  on the value function scale: updates are more sensitive when values are small and
  less sensitive when values are large.'
---

# Weber-Fechner Law in Temporal Difference learning derived from Control as Inference

## Quick Facts
- **arXiv ID**: 2412.21004
- **Source URL**: https://arxiv.org/abs/2412.21004
- **Reference count**: 10
- **Key outcome**: TD learning derived from Control as Inference exhibits Weber-Fechner Law, with nonlinear updates that are more sensitive for small values and less sensitive for large values

## Executive Summary
This paper establishes a theoretical connection between Control as Inference and the Weber-Fechner Law in Temporal Difference learning. The authors derive a nonlinear TD update rule where the magnitude of updates depends logarithmically on the scale of the value function. This leads to updates that are more sensitive when values are small and less sensitive when values are large, following the Weber-Fechner Law. To make this practically implementable, the authors propose a reward-punishment framework with modified definitions of optimality that allows computation of the nonlinear terms.

## Method Summary
The authors start from the Control as Inference framework and derive a nonlinear TD update rule. They propose a reward-punishment framework where the agent receives both rewards and punishments, with the punishment mechanism incorporating the Weber-Fechner Law through a logarithmic term. The modified definitions of optimality include a reward threshold that determines when the agent is considered optimal. This allows the nonlinear update terms to be computed in practice. The update rule becomes more sensitive to changes when the value function estimates are small, and less sensitive when they are large, following the logarithmic relationship of the Weber-Fechner Law.

## Key Results
- WFL accelerates reward maximization in early stages of learning in pendulum control task
- WFL suppresses punishments during learning, leading to higher task success rates in robotic valve-turning task
- The proposed framework shows improved learning speed compared to conventional TD methods

## Why This Works (Mechanism)
The Weber-Fechner Law in TD learning works because the logarithmic update rule naturally balances exploration and exploitation. When value estimates are small (uncertain), the updates are larger, promoting exploration. When value estimates are large (confident), the updates are smaller, promoting exploitation. This adaptive learning rate based on value scale prevents overshooting in confident regions while maintaining sensitivity in uncertain regions. The reward-punishment framework with modified optimality definitions makes this theoretically sound and practically implementable by providing a mechanism to compute the nonlinear terms.

## Foundational Learning
- **Control as Inference**: Treats RL as Bayesian inference problem where optimal actions maximize probability of success
  - *Why needed*: Provides probabilistic foundation for deriving TD updates
  - *Quick check*: Verify understanding of how actions map to optimality in inference framework

- **Weber-Fechner Law**: Psychophysical law stating that perceived change is proportional to logarithm of stimulus magnitude
  - *Why needed*: Explains the nonlinear scaling relationship in TD updates
  - *Quick check*: Confirm logarithmic relationship between update magnitude and value scale

- **Temporal Difference Learning**: RL method that updates value estimates based on temporal differences
  - *Why needed*: Standard framework being modified to incorporate WFL
  - *Quick check*: Understand standard TD(λ) update equations

- **Reward-punishment framework**: Extension where agents receive both positive rewards and negative punishments
  - *Why needed*: Enables practical implementation of nonlinear updates
  - *Quick check*: Verify how punishment mechanism affects learning dynamics

- **Optimality definitions in RL**: Criteria for determining when an agent's behavior is optimal
  - *Why needed*: Modified definitions enable computation of nonlinear terms
  - *Quick check*: Understand threshold-based optimality criteria

## Architecture Onboarding

**Component Map**:
Value Function -> TD Update Rule -> Weber-Fechner Scaling -> Reward-Punishment Mechanism -> Optimality Threshold

**Critical Path**:
Value Function estimates → TD error calculation → Logarithmic scaling factor → Combined reward-punishment update → Updated value function

**Design Tradeoffs**:
The framework trades computational complexity for adaptive learning rates. The logarithmic scaling adds computation but provides automatic adjustment of learning sensitivity based on value scale. The reward-punishment mechanism doubles the feedback channels but enables the nonlinear behavior. Modified optimality definitions may complicate theoretical analysis but enable practical implementation.

**Failure Signatures**:
- Overshooting behavior indicates scaling factor is too large for confident regions
- Slow learning suggests scaling factor is too small for uncertain regions
- Unstable training may result from inappropriate reward-punishment balance
- Poor performance on simple tasks might indicate unnecessary complexity

**3 First Experiments**:
1. Compare learning curves on pendulum task with and without WFL scaling
2. Vary the reward-punishment balance to find optimal tradeoff
3. Test sensitivity to different value function initialization scales

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but implicit questions remain about the generality of the approach across different RL domains and the theoretical justification for the specific form of the reward-punishment framework.

## Limitations
- Theoretical derivation assumes specific reward-punishment framework that may not generalize
- Experimental validation limited to only two control tasks (pendulum and valve-turning)
- Modified definitions of optimality need more rigorous theoretical justification
- Practical implementation details for handling nonlinear terms could be more thoroughly explored

## Confidence
- **Theoretical claims linking Control as Inference to WFL in TD learning**: Medium
- **Generalization of results across diverse RL problems**: Low

## Next Checks
1. Test the proposed framework on a broader range of benchmark RL tasks, including discrete control and high-dimensional environments
2. Compare the sample efficiency and final performance against state-of-the-art TD learning variants with different update rules
3. Conduct ablation studies to isolate the contribution of individual components in the reward-punishment framework to observed improvements