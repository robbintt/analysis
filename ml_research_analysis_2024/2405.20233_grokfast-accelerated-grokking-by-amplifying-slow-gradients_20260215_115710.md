---
ver: rpa2
title: 'Grokfast: Accelerated Grokking by Amplifying Slow Gradients'
arxiv_id: '2405.20233'
source_url: https://arxiv.org/abs/2405.20233
tags:
- grokfast
- grokking
- train
- baseline
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the phenomenon of grokking, where deep learning
  models generalize long after overfitting to training data. The authors propose a
  method called Grokfast to accelerate this delayed generalization by amplifying slow-varying
  components of parameter updates during training.
---

# Grokfast: Accelerated Grokking by Amplifying Slow Gradients

## Quick Facts
- arXiv ID: 2405.20233
- Source URL: https://arxiv.org/abs/2405.20233
- Reference count: 19
- Primary result: Up to 50x acceleration of grokking phenomenon across multiple tasks

## Executive Summary
Grokfast is a novel method to accelerate the grokking phenomenon in deep learning, where models generalize long after overfitting. The approach works by amplifying slow-varying components of parameter updates during training while preserving fast-varying components. By treating parameter updates as discrete-time signals and applying low-pass filtering, Grokfast modifies the optimization trajectory to reach generalization states more quickly. The method is simple to implement and has been validated across diverse tasks including algorithmic data, MNIST image classification, QM9 molecule prediction, and IMDb sentiment analysis.

## Method Summary
Grokfast accelerates grokking by amplifying slow-varying components of gradients through low-pass filtering. The algorithm treats parameter updates as discrete-time signals and applies either moving average (MA) or exponential moving average (EMA) filters to emphasize slow-varying gradient components. The filtered gradients are then added back to the original gradients before optimization, preserving both fast (overfitting) and slow (generalization) dynamics. The method requires tuning of hyperparameters including the scalar amplification factor λ, momentum α for EMA, and window size w for MA filtering.

## Key Results
- Achieves up to 50x acceleration of grokking across algorithmic data, MNIST, QM9, and IMDb tasks
- Reduces parameter space distance between overfitting and generalization states by up to 16x
- Makes training more deterministic with hundredfold smaller variances in parameter trajectories
- Simple implementation that works across diverse model architectures without architectural modifications

## Why This Works (Mechanism)

### Mechanism 1
Grokfast accelerates generalization by amplifying slow-varying components of gradients while preserving fast-varying components. The algorithm treats parameter updates as discrete-time signals and applies low-pass filtering to emphasize slow-varying gradient components. By adding filtered gradients back to original gradients, it maintains both fast (overfitting) and slow (generalization) dynamics. The core assumption is that slow-varying components contribute to generalization while fast-varying components drive overfitting.

### Mechanism 2
The method creates a more direct optimization path to generalization by reducing the parameter space distance between overfitting and generalization states. By emphasizing slow gradients, the algorithm modifies the parameter trajectory to reach a state closer to the final generalized solution, shortening the B→C transition phase. The core assumption is that parameter space distance between overfitting and generalization states can be reduced through gradient modification.

### Mechanism 3
Grokfast introduces determinism into training dynamics while maintaining necessary stochasticity for generalization. Low-pass filtering reduces variance in parameter updates, creating more stable trajectories while the original gradient component maintains exploration capability. The core assumption is that reduced variance in parameter updates leads to more deterministic training without sacrificing generalization.

## Foundational Learning

- Concept: Signal processing and Fourier analysis
  - Why needed here: The algorithm treats parameter updates as discrete-time signals and applies spectral decomposition to separate fast and slow components
  - Quick check question: What is the relationship between time-domain filtering and frequency-domain amplification?

- Concept: Optimization algorithms and gradient descent mechanics
  - Why needed here: Understanding how first-order optimizers work is essential to grasp how gradient modification affects parameter updates
  - Quick check question: How do momentum terms in SGD relate to low-pass filtering of gradients?

- Concept: Grokking phenomenon and generalization dynamics
  - Why needed here: The algorithm specifically targets the delayed generalization phase characteristic of grokking
  - Quick check question: What distinguishes the parameter trajectories during overfitting versus generalization phases?

## Architecture Onboarding

- Component map: Gradient computation → Filtering and amplification → Modified gradient → Optimizer update → Parameter update
- Critical path: The low-pass filter (moving average or EMA) processes gradients before they reach the optimizer, with hyperparameters controlling filter characteristics and amplification strength
- Design tradeoffs: Memory vs. effectiveness (windowed MA requires O(w) memory while EMA requires O(1)), cutoff frequency selection (too low loses necessary information, too high fails to accelerate)
- Failure signatures: Training instability (if amplification is too high), no acceleration (if filter is too weak), overfitting without generalization (if fast components are removed)
- First 3 experiments:
  1. Apply Grokfast to a simple grokking task (modular multiplication) with varying λ and α to find optimal hyperparameters
  2. Compare memory usage and wall-clock time between windowed MA and EMA implementations
  3. Visualize parameter trajectories using PCA to verify reduced distance between overfitting and generalization states

## Open Questions the Paper Calls Out

### Open Question 1
What is the precise mechanism by which slow-varying gradient components contribute to generalization in grokking? While the paper demonstrates empirically that amplifying slow gradients accelerates grokking, it does not provide a theoretical explanation for why this occurs or what specific properties of the slow components enable generalization.

### Open Question 2
How do different architectures and dataset types affect the optimal hyperparameters (λ, α, window size) for Grokfast? The paper only explores a limited range of architectures and datasets, and the optimal hyperparameters are found through grid search rather than a principled approach.

### Open Question 3
What is the relationship between Grokfast and implicit regularization effects in optimization? The paper does not fully explore the connection between Grokfast's effects and known implicit regularization mechanisms in optimization, such as the effects of batch size or learning rate on generalization.

## Limitations
- The geometric interpretation of parameter trajectories lacks rigorous mathematical proof linking distance reduction to generalization speed
- The exponential moving average implementation uses fixed hyperparameters across all tasks, which may not be optimal
- The paper doesn't address potential overfitting to specific grokking tasks or whether acceleration generalizes to non-grokking learning scenarios

## Confidence
- **High Confidence**: Empirical demonstration of acceleration across multiple tasks with clear quantitative metrics
- **Medium Confidence**: Mechanistic claim about dual-component hypothesis lacks ablation studies validating necessity of both components
- **Low Confidence**: Geometric interpretation primarily supported by visualizations rather than quantitative analysis

## Next Checks
1. **Ablation Study**: Implement variants using only slow gradients and only fast gradients to determine which component is essential for acceleration
2. **Hyperparameter Sensitivity Analysis**: Conduct systematic grid searches for λ and α across all tasks to identify optimal configurations
3. **Non-Grokking Generalization**: Apply Grokfast to standard deep learning tasks without delayed generalization to test method's generality beyond grokking phenomenon