---
ver: rpa2
title: 'Generative Camera Dolly: Extreme Monocular Dynamic Novel View Synthesis'
arxiv_id: '2405.14868'
source_url: https://arxiv.org/abs/2405.14868
tags:
- video
- dynamic
- camera
- scene
- view
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for monocular dynamic novel view synthesis,
  where a video from one viewpoint is translated into a video from a different viewpoint.
  The core idea is to leverage large-scale video diffusion models by fine-tuning them
  on synthetic multi-view video data.
---

# Generative Camera Dolly: Extreme Monocular Dynamic Novel View Synthesis

## Quick Facts
- arXiv ID: 2405.14868
- Source URL: https://arxiv.org/abs/2405.14868
- Authors: Basile Van Hoorick; Rundi Wu; Ege Ozguroglu; Kyle Sargent; Ruoshi Liu; Pavel Tokmakov; Achal Dave; Changxi Columbia; Carl Vondrick
- Reference count: 40
- One-line primary result: Achieves state-of-the-art monocular dynamic novel view synthesis without depth input or explicit 3D modeling

## Executive Summary
This paper proposes Generative Camera Dolly (GCD), a method for translating videos from one viewpoint to another using monocular input. The approach leverages large-scale video diffusion models by fine-tuning them on synthetic multi-view data. GCD performs end-to-end video-to-video translation without requiring depth input or explicit 3D geometry modeling. Experiments demonstrate state-of-the-art performance on extreme camera transformations, including handling occlusions and generalizing to real-world videos from various domains.

## Method Summary
GCD fine-tunes a pretrained Stable Video Diffusion (SVD) model on synthetic multi-view video data to perform monocular dynamic novel view synthesis. The model conditions on input video frames and relative camera pose parameters (azimuth, elevation, radius in spherical coordinates) using a micro-conditioning mechanism. Training uses hybrid visual conditioning with CLIP embeddings and VAE-encoded frames. The method interpolates camera poses gradually during training to leverage SVD's tendency to start near the input viewpoint, enabling extreme viewpoint transformations while maintaining temporal coherence.

## Key Results
- Achieves state-of-the-art performance on monocular dynamic novel view synthesis tasks
- Handles extreme camera transformations including up to 180° azimuth rotation and 60° elevation change
- Demonstrates strong zero-shot generalization to real-world videos despite training only on synthetic data
- Successfully reconstructs occluded objects and maintains temporal synchronization in generated videos

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model learns dynamic novel view synthesis by leveraging large-scale video diffusion priors via fine-tuning
- Mechanism: Fine-tuning pretrained SVD on synthetic multi-view data enables translation between viewpoints guided by camera pose parameters. The diffusion model's learned priors about real-world dynamics and geometry allow inference of unseen regions.
- Core assumption: Pretrained diffusion model priors are sufficiently rich and generalizable to handle the task when fine-tuned on synthetic data.
- Evidence anchors: [abstract] "Our model does not require depth as input..." [section 3.1] "Since large-scale video diffusion models have been trained on hyper-scale data..."

### Mechanism 2
- Claim: Ability to handle extreme camera transformations enabled by gradual pose interpolation during training
- Mechanism: Linearly interpolating camera pose from source to destination viewpoint teaches gradual transitions. Exploits SVD's bias toward starting near input viewpoint while allowing significant changes.
- Core assumption: Pretrained SVD generation process is biased toward input viewpoint, enabling gradual pose changes.
- Evidence anchors: [section 5] "we wish to get to the destination camera pose 'as fast as possible'..." [section 5] "where should the output pose start..."

### Mechanism 3
- Claim: Zero-shot generalization enabled by richness of synthetic training data and diffusion model priors
- Mechanism: Training on diverse synthetic data covering complex occlusion patterns and dynamic scenes, combined with diffusion model's generalizable priors, enables transfer to real-world videos.
- Core assumption: Synthetic data is sufficiently rich and diverse, and diffusion priors are generalizable to real scenarios.
- Evidence anchors: [section 4] "we contribute two high-quality synthetic datasets..." [section 6] "our approach show surprisingly strong generalization skills..."

## Foundational Learning

- Concept: Camera extrinsics and intrinsics
  - Why needed here: Essential for defining dynamic novel view synthesis task and conditioning model on desired viewpoint transformations
  - Quick check question: What is the difference between camera extrinsics and intrinsics, and how are they used in this paper?

- Concept: Spherical coordinate system
  - Why needed here: Used to parameterize camera pose for defining bounds of camera transformations during training and sampling target viewpoints during inference
  - Quick check question: How does the spherical coordinate system used here differ from standard, and why is this difference important?

- Concept: Diffusion models and score distillation
  - Why needed here: Crucial for understanding training process and how model generates output videos conditioned on input and camera pose parameters
  - Quick check question: How do diffusion models work, and how is score distillation used for video-to-video translation?

## Architecture Onboarding

- Component map: Input video → VAE encoder → U-Net (with cross-attention and micro-conditioning) → Output video

- Critical path: Input video → VAE encoder → U-Net (with cross-attention and micro-conditioning) → Output video

- Design tradeoffs:
  - Using pretrained diffusion model vs. training from scratch: Enables faster convergence and better generalization but may introduce biases requiring careful management
  - Gradual vs. direct camera pose interpolation: Gradual exploits input viewpoint bias but may limit transformations; direct allows more extreme changes but risks distribution misalignment

- Failure signatures:
  - Blurry or low-quality output videos
  - Incorrect shape or appearance of objects, especially moving ones
  - Failure to respect desired camera trajectory or scene dynamics
  - Hallucination of non-existent objects or background regions

- First 3 experiments:
  1. Train with gradual camera pose interpolation and SVD finetuning, evaluate on Kubric-4D test set
  2. Compare gradual vs. direct interpolation approaches with and without SVD finetuning
  3. Evaluate zero-shot generalization to real-world videos from various domains and analyze failure modes

## Open Questions the Paper Calls Out

- Open Question 1: How does the model handle videos with significant camera motion in the input video?
  - Basis in paper: [inferred] Paper assumes static input camera but doesn't test scenarios with substantial input camera motion
  - Why unresolved: Performance on moving camera scenarios could significantly differ from static cases
  - What evidence would resolve it: Testing on datasets with varying input camera motion degrees and comparing to static camera results

- Open Question 2: What is the impact of using different diffusion model architectures on GCD framework performance?
  - Basis in paper: [explicit] Framework can generalize to any video generation approach but only experiments with SVD
  - Why unresolved: Different architectures may handle spatial/temporal features differently, influencing generated video quality
  - What evidence would resolve it: Implementing GCD with different diffusion architectures and evaluating on same datasets

- Open Question 3: How does the model perform on videos with multiple moving objects interacting in complex ways?
  - Basis in paper: [explicit] Evaluates on multi-object interactions but doesn't specifically address complex interactions or occlusions
  - Why unresolved: Complex interactions could challenge accurate scene reconstruction and temporal coherence
  - What evidence would resolve it: Testing on datasets with complex multi-object interactions and comparing to simpler scenes

- Open Question 4: What are the limitations when dealing with deformable objects or non-rigid body motions?
  - Basis in paper: [explicit] Mentions occasional struggles with deformable objects leading to blurriness or incorrect shapes
  - Why unresolved: Deformable objects introduce additional complexity in shape and appearance
  - What evidence would resolve it: Evaluating on datasets specifically including deformable objects and analyzing generated video errors

## Limitations
- Relies on synthetic training data which may not fully capture natural dynamic scene complexity
- Performance on out-of-distribution data (humans, animals) remains uncertain
- Evaluation metrics may not fully capture perceptual quality or temporal consistency
- Occasional struggles with deformable objects and non-rigid body motions

## Confidence
- **High Confidence**: Core methodology of fine-tuning diffusion models for view synthesis and general approach effectiveness
- **Medium Confidence**: Claims about zero-shot generalization to real-world videos supported by qualitative but needing more extensive quantitative evaluation
- **Medium Confidence**: Analysis of failure modes based on observations but could benefit from more systematic investigation

## Next Checks
1. Conduct comprehensive quantitative evaluation on diverse real-world videos including challenging scenarios with humans, animals, and complex occlusions
2. Perform ablation study to assess impact of different conditioning mechanisms on performance and generalization
3. Investigate temporal consistency using metrics like optical flow or video interpolation error, comparing with state-of-the-art video generation models