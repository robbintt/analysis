---
ver: rpa2
title: Navigating the OverKill in Large Language Models
arxiv_id: '2401.17633'
source_url: https://arxiv.org/abs/2401.17633
tags:
- safety
- question
- overkill
- system
- refusal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the "overkill" phenomenon in large language
  models (LLMs), where models excessively refuse to answer benign queries due to safety
  alignment. The authors analyze factors contributing to overkill by examining information
  flow within models and identify that models overly attend to harmful words regardless
  of context.
---

# Navigating the OverKill in Large Language Models

## Quick Facts
- arXiv ID: 2401.17633
- Source URL: https://arxiv.org/abs/2401.17633
- Authors: Chenyu Shi; Xiao Wang; Qiming Ge; Songyang Gao; Xianjun Yang; Tao Gui; Qi Zhang; Xuanjing Huang; Xun Zhao; Dahua Lin
- Reference count: 16
- Key outcome: Self-Contrastive Decoding reduces LLM refusal rates by ~20% while maintaining safety

## Executive Summary
This paper investigates the "overkill" phenomenon in large language models, where models excessively refuse benign queries due to safety alignment. The authors identify that models exhibit shortcut attention to harmful words regardless of context, leading to false refusals. They propose Self-Contrastive Decoding (Self-CD), a training-free method that reduces refusal rates by modulating the model's output distribution through contrastive decoding between safety-emphasized and non-safety prompts.

## Method Summary
The Self-Contrastive Decoding method generates two outputs per query: one with safety-emphasized system prompts and one without. It computes the distribution difference (∆yt) between these outputs and applies a weighted subtraction to the original distribution using hyperparameter α=2.5. This adjustment reduces the model's over-attention to harmful words while preserving safety alignment. The method is tested across seven models (LLaMa2-7B/13B/70B, Vicuna-7B/13B, Mistral-7B, Beaver-7B, InternLM-7B) on OKTest and XSTest-Safe datasets.

## Key Results
- Self-CD achieves an average 20% reduction in refusal rate across tested models
- On XSTest-Safe dataset: refusal rate decreases from 31.8% to 4.8%
- On OKTest dataset: refusal rate decreases from 29.1% to 6.7%
- Safety maintained with winning rate ≈ 50% vs raw models on unsafe query datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Self-CD exploits shortcut attention to harmful words by contrasting output distributions under different system prompts
- **Mechanism**: The difference in probability distributions (∆y_t) between safety-prompted and non-safety-prompted responses isolates tokens most influenced by shortcut attention. Subtracting a weighted version of ∆y_t reduces over-attention to harmful words
- **Core assumption**: Safety system prompts amplify attention to certain harmful words regardless of context, reflected in output distribution
- **Evidence anchors**: Information flow analysis shows more pronounced attention when safety prompts are present; experiments show 20% refusal reduction

### Mechanism 2
- **Claim**: Shortcut attention to harmful words causes models to refuse benign queries based on keyword presence
- **Mechanism**: Models over-weight harmful words in input through internal attention mechanisms, classifying benign queries as unsafe based on superficial keyword matching
- **Core assumption**: Safety alignment relies on keyword matching rather than deep semantic comprehension
- **Evidence anchors**: Refusal rates exceed 60% across tested models; information flow shows convergence in importance regardless of contextual safety

### Mechanism 3
- **Claim**: Adjusting output distribution via contrastive decoding reduces refusal of benign queries without compromising safety
- **Mechanism**: Down-weighting tokens associated with refusal responses (identified via ∆y_t) shifts sampling distribution toward non-refusal tokens while preserving underlying safety alignment
- **Core assumption**: Refusal-associated tokens are separable from safety-critical tokens
- **Evidence anchors**: Empirical results show 20% refusal reduction with minimal safety impact; winning rate ≈ 50% on unsafe query datasets

## Foundational Learning

- **Concept**: Information Flow Analysis
  - **Why needed here**: To identify how models prioritize certain words when making safety decisions
  - **Quick check question**: How does the information flow metric quantify token attention weights' importance to final prediction?

- **Concept**: Contrastive Decoding
  - **Why needed here**: To understand how contrasting output distributions isolates model behaviors related to safety decisions
  - **Quick check question**: What is the mathematical relationship between output distributions under safety-emphasized and non-emphasized prompts?

- **Concept**: Safety Alignment in LLMs
  - **Why needed here**: To understand mechanisms by which models are trained to refuse unsafe queries and how these might lead to overkill
  - **Quick check question**: What are primary safety alignment techniques (RLHF, prompt engineering) and their failure modes?

## Architecture Onboarding

- **Component map**: Input processing -> Base model -> Self-CD module -> Output sampling
- **Critical path**: 
  1. Receive user query with system prompt
  2. Generate two responses (safety-emphasized and non-safety)
  3. Compute distribution difference (∆yt)
  4. Apply weighted subtraction to original distribution
  5. Sample final token from adjusted distribution
  6. Repeat until sequence completion

- **Design tradeoffs**: Training-free vs. fine-tuning (no additional training but potentially less effective); model-agnostic vs. optimized (works with any transformer but may miss model-specific characteristics); simplicity vs. sophistication (easy to implement but may miss complex safety nuances)

- **Failure signatures**: Increased unsafe outputs (if adjustment removes too much safety attention); persistent overkill (if shortcut attention not fully captured); computational overhead (if two responses per token becomes too slow)

- **First 3 experiments**:
  1. Baseline comparison: Test refusal rates on OKTest with and without Self-CD across multiple models
  2. Safety validation: Evaluate outputs on unsafe query datasets to ensure safety not compromised
  3. Hyperparameter sensitivity: Test different α values to find optimal balance between refusal reduction and safety preservation

## Open Questions the Paper Calls Out
- How does training data quality influence overkill severity across different models?
- Can Self-CD be extended to address other safety issues beyond query refusal?
- How do different model architectures and sizes influence shortcuts leading to overkill?
- Can Self-CD be combined with other alignment techniques for improved safety?
- How does hyperparameter α affect the trade-off between reducing overkill and maintaining safety?

## Limitations
- Effectiveness across diverse safety alignment mechanisms beyond tested models is uncertain
- Potential safety degradation not captured by current evaluation datasets
- Generalizability of shortcut attention hypothesis to models with different safety training approaches

## Confidence
**High Confidence**: Self-CD reduces refusal rates by ~20% across seven models while maintaining safety on evaluated datasets
**Medium Confidence**: Models exhibit shortcut attention to harmful words leading to overkill, supported by information flow analysis
**Low Confidence**: Mechanism by which contrastive decoding isolates harmful word attention lacks rigorous empirical proof

## Next Checks
1. Conduct attention visualization and gradient-based attribution studies to directly observe harmful word attention patterns when safety prompts are present
2. Evaluate adjusted models on comprehensive safety benchmarks including adversarial examples to identify potential safety degradation
3. Test Self-CD on models with different safety alignment approaches to determine universal applicability of shortcut attention mechanism