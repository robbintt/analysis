---
ver: rpa2
title: The use of GPT-4o and Other Large Language Models for the Improvement and Design
  of Self-Assessment Scales for Measurement of Interpersonal Communication Skills
arxiv_id: '2409.14050'
source_url: https://arxiv.org/abs/2409.14050
tags:
- items
- llms
- scales
- communication
- scale
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates how Large Language Models (LLMs) like GPT-4o,
  Gemini 1.5 Pro, and Claude 3.5 Sonnet can assist researchers in evaluating, improving,
  and designing self-assessment scales for measuring interpersonal communication skills.
  The study shows that LLMs can effectively translate and simplify scale items, categorize
  items semantically to complement empirical factor analysis, and generate new items
  or entire scales for constructs like active listening, empathy, and verbal expressivity.
---

# The use of GPT-4o and Other Large Language Models for the Improvement and Design of Self-Assessment Scales for Measurement of Interpersonal Communication Skills

## Quick Facts
- **arXiv ID**: 2409.14050
- **Source URL**: https://arxiv.org/abs/2409.14050
- **Reference count**: 40
- **Primary result**: LLMs can translate, simplify, categorize, and generate items for interpersonal communication skills scales, with potential for automated interactive administration.

## Executive Summary
This paper explores the use of large language models (LLMs) such as GPT-4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet for improving and designing self-assessment scales to measure interpersonal communication skills. The study demonstrates that LLMs can effectively translate and simplify scale items, categorize items semantically to complement empirical factor analysis, and generate new items or entire scales for constructs like active listening, empathy, and verbal expressivity. Additionally, the paper shows that LLMs can contextualize items for different communication modalities and automate the administration and scoring of newly generated scales. While the results are promising, the paper emphasizes the need for expert review and careful prompting to ensure accuracy and validity.

## Method Summary
The study used several LLMs (GPT-4o, GPT-4, Copilot, Gemini 1.5 Pro, Claude 3.5 Sonnet) to perform tasks such as translating scale items, categorizing items semantically, improving existing scales, and generating new scale items. Unpublished ICCI scales and shortened versions were used as input data. The process involved crafting specific prompts for each task, with LLMs responding to zero-shot or few-shot instructions. Expert review and empirical validation were recommended but not conducted within the study itself.

## Key Results
- LLMs effectively translated and simplified scale items, aiding in cross-linguistic adaptation.
- LLMs categorized items semantically, aligning with or diverging from original construct designs, and could generate new items for specific interpersonal skills.
- LLMs demonstrated the ability to contextualize items for online communication and automate interactive scale administration with scoring and feedback.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can serve as automated semantic validators for scale items, grouping them into constructs without prior training.
- Mechanism: LLMs perform zero-shot or few-shot prompting to categorize items based on semantic similarity, generating construct labels that align with or differ from original design.
- Core assumption: LLMs possess sufficient latent knowledge of interpersonal communication constructs to infer semantic structure.
- Evidence anchors:
  - [abstract] "LLMs can effectively translate and simplify scale items, categorize items semantically to complement empirical factor analysis"
  - [section 6.2] "GPT-4o was not given the names of categories but was told to make the groupings according to the semantic similarity of items"
  - [corpus] weak; no direct corpus evidence for construct inference in communication skills
- Break condition: If semantic overlap between constructs is high, LLM categorization diverges significantly from theoretical design.

### Mechanism 2
- Claim: LLMs can generate new, valid scale items for interpersonal communication skills when given definitions and exemplar items.
- Mechanism: Prompting LLMs with construct definitions and exemplar items leads to generation of semantically consistent new items.
- Core assumption: LLMs can leverage internal training data to produce items matching specified construct definitions.
- Evidence anchors:
  - [abstract] "LLMs also perform well in contextualization tasks, such as adapting items from face-to-face interaction to online communication"
  - [section 6.4] "All the observed LLMs...were able to provide acceptable definitions of the selected interpersonal communication skills constructs"
  - [corpus] weak; corpus neighbors are VR/interaction but not item generation
- Break condition: If exemplar items are insufficient or poorly representative, generated items may drift semantically.

### Mechanism 3
- Claim: LLMs can automate the entire scale design and administration pipeline, including interactive application and scoring.
- Mechanism: Single prompts trigger LLMs to generate a scale, administer items one-by-one, score responses, and provide feedback.
- Core assumption: LLMs can maintain context over multi-turn interactions and perform structured scoring.
- Evidence anchors:
  - [abstract] "LLMs also perform well in contextualization tasks...Additionally, the paper demonstrates the potential for automated, interactive application of newly generated scales"
  - [section 6.5] "LLMs...can both design a single new self-assessment scale and apply its items one by one"
  - [corpus] weak; corpus neighbors involve VR training but not LLM interactive assessment
- Break condition: If context window or memory limits are exceeded, multi-turn accuracy degrades.

## Foundational Learning

- Concept: Semantic similarity and categorization in natural language processing
  - Why needed here: LLMs rely on semantic matching to group items into constructs; understanding this helps craft effective prompts.
  - Quick check question: If an LLM groups "empathy" and "comforting" items together, what does this suggest about semantic overlap?

- Concept: Construct validity and content validity
  - Why needed here: Evaluating whether LLM-generated or categorized items truly measure intended constructs is essential for scale quality.
  - Quick check question: How would you distinguish between face validity and content validity in LLM-categorized scales?

- Concept: Multi-turn dialogue and context management
  - Why needed here: Automated administration requires LLMs to remember prior responses and maintain coherent interaction.
  - Quick check question: What happens if an LLM loses context mid-administration of a 10-item scale?

## Architecture Onboarding

- **Component map**: Prompt → LLM → Item categorization/generation → (optional) back-translation → Feedback loop → Validation
- **Critical path**: Prompt design → LLM response → Human expert review → Empirical validation (if needed)
- **Design tradeoffs**: Zero-shot simplicity vs. few-shot accuracy; automated speed vs. expert oversight; generality vs. domain specificity
- **Failure signatures**: Misaligned categories, hallucinated items, context loss in multi-turn tasks, cultural bias in generated content
- **First 3 experiments**:
  1. Translate and simplify a short scale from Croatian to English using Copilot; compare with human translation.
  2. Use GPT-4o to categorize 16 items into 4 constructs without naming categories; compare to original design.
  3. Generate 5 new items for "Verbal Expressivity" using a construct definition and 3 exemplars; review for semantic fit.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific cognitive mechanisms enable LLMs to generate interpersonal communication skills items that are both semantically valid and distinct from training data?
- Basis in paper: [inferred]
- Why unresolved: The paper demonstrates LLMs can generate items but does not explore the underlying cognitive mechanisms that enable this capability or how they avoid reproducing training data.
- What evidence would resolve it: Comparative analysis of LLM-generated items versus existing scales, with cognitive process tracing or explainability techniques.

### Open Question 2
- Question: How do cultural biases manifest in LLM-generated interpersonal communication skills scales, and what methods can effectively mitigate these biases?
- Basis in paper: [explicit]
- Why unresolved: While the paper mentions cultural biases as a concern, it does not systematically investigate how these biases manifest in generated scales or evaluate mitigation strategies.
- What evidence would resolve it: Cross-cultural validation studies comparing LLM-generated scales across different cultural contexts.

### Open Question 3
- Question: What is the optimal prompting strategy for balancing construct coverage and item diversity when using LLMs to generate interpersonal communication skills scales?
- Basis in paper: [inferred]
- Why unresolved: The paper demonstrates various prompting approaches but does not empirically compare their effectiveness in achieving optimal construct coverage and item diversity.
- What evidence would resolve it: Systematic comparison of different prompting strategies with psychometric analysis of resulting scales.

## Limitations
- The study lacks empirical validation; LLM outputs are not tested with actual participants or compared to established scales.
- Expert review is called for but not demonstrated in the paper.
- Cultural generalizability and potential biases in LLM-generated items are not systematically addressed.

## Confidence
- **High**: LLMs can translate and simplify scale items; they can administer items interactively and score them.
- **Medium**: LLMs can semantically categorize items and contextualize them for new modalities; generated items are "acceptable" but not validated.
- **Low**: The claim that LLM-generated scales are ready for research use without further validation.

## Next Checks
1. **Empirical validation**: Administer LLM-generated and categorized scales to a sample; compare factor structure and reliability to original scales.
2. **Prompt sensitivity analysis**: Systematically vary prompt wording; quantify impact on item generation and categorization quality.
3. **Cultural bias audit**: Have items reviewed by diverse cultural experts; test LLM generation across different cultural contexts.