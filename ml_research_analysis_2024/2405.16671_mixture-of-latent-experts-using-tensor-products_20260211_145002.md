---
ver: rpa2
title: Mixture of Latent Experts Using Tensor Products
arxiv_id: '2405.16671'
source_url: https://arxiv.org/abs/2405.16671
tags:
- tensor
- routing
- learning
- arxiv
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TensorPoly, a novel modular language model
  that leverages tensor products to achieve parameter efficiency and effective multi-task
  transfer learning. The core method involves reparameterizing LoRA adapters using
  an entangled tensor structure (TLoRA) and implementing two routing functions (TensorPoly-I
  and TensorPoly-II) to selectively activate modules based on tensor rank or order.
---

# Mixture of Latent Experts Using Tensor Products

## Quick Facts
- arXiv ID: 2405.16671
- Source URL: https://arxiv.org/abs/2405.16671
- Reference count: 12
- Introduces TensorPoly, a modular language model using tensor products for parameter efficiency and multi-task transfer learning

## Executive Summary
This paper introduces TensorPoly, a novel modular language model architecture that leverages tensor products to achieve parameter efficiency and effective multi-task transfer learning. The core innovation involves reparameterizing LoRA adapters using an entangled tensor structure (TLoRA) combined with two routing functions (TensorPoly-I and TensorPoly-II) that selectively activate modules based on tensor rank or order. The approach aims to mitigate negative transfer in multi-task learning while improving compositional generalization across diverse tasks.

## Method Summary
The method introduces TensorPoly, a modular language model that uses tensor products to create parameter-efficient LoRA adapters. The approach involves two main components: TLoRA, which reparameterizes LoRA adapters using an entangled tensor structure, and two routing functions (TensorPoly-I and TensorPoly-II) that selectively activate modules based on tensor rank or order. The routing mechanisms allow for dynamic module selection during inference, enabling the model to compose different capabilities for different tasks. The architecture is designed to address the limitations of traditional dense approaches by providing better parameter efficiency and reduced negative transfer in multi-task scenarios.

## Key Results
- TensorPoly models outperform traditional dense approaches like LoRA and TLoRA on the T0 benchmark
- TensorPoly-I achieves state-of-the-art results while using fewer training parameters
- The tensor product routing mechanism effectively mitigates negative transfer in multi-task learning scenarios
- The approach demonstrates enhanced compositional generalization across diverse tasks

## Why This Works (Mechanism)
The paper claims tensor products provide an effective way to entangle LoRA adapters, creating a more structured and parameter-efficient representation. The routing functions (TensorPoly-I and TensorPoly-II) leverage tensor rank and order properties to selectively activate relevant modules for each task, enabling dynamic composition of capabilities. This modular approach allows the model to share parameters across tasks while maintaining task-specific specialization, reducing the risk of negative transfer that often occurs in multi-task learning. The tensor structure enables more efficient representation of task relationships compared to traditional dense or mixture-of-experts approaches.

## Foundational Learning
- **Tensor Products**: Mathematical operations that combine vectors/matrices to create higher-dimensional representations; needed for creating entangled LoRA adapters; quick check: verify basic tensor multiplication properties
- **Low-Rank Adaptation (LoRA)**: Parameter-efficient fine-tuning method that approximates weight updates with low-rank matrices; needed as baseline adapter structure; quick check: confirm rank reduction effects on model capacity
- **Modular Language Models**: Architectures that compose multiple specialized modules rather than using single dense parameters; needed for task-specific routing; quick check: compare module activation patterns across tasks
- **Routing Functions**: Mechanisms that determine which modules to activate for given inputs; needed for dynamic capability composition; quick check: analyze routing sparsity and accuracy trade-offs
- **Negative Transfer**: Phenomenon where training on multiple tasks degrades performance on some tasks; needed context for motivation; quick check: measure performance degradation in multi-task settings
- **Compositional Generalization**: Ability to combine learned skills to handle novel task combinations; needed for evaluating multi-task effectiveness; quick check: test on unseen task combinations

## Architecture Onboarding

**Component Map**
Input -> Tokenizer -> Base Model -> Tensor Product Layer -> Routing Function -> Module Ensemble -> Output

**Critical Path**
The critical path involves input processing through the base model, tensor product transformation, routing function evaluation, and module ensemble selection for final output generation.

**Design Tradeoffs**
The architecture trades increased routing complexity for parameter efficiency and reduced negative transfer. The tensor product structure adds computational overhead but enables more structured parameter sharing compared to dense approaches.

**Failure Signatures**
- Poor routing decisions leading to incorrect module activation
- Overfitting to specific task combinations in the tensor product space
- Computational bottlenecks in tensor operations during inference
- Insufficient module diversity causing performance degradation on novel tasks

**3 First Experiments**
1. Compare TensorPoly routing against standard MoE routing on identical task sets
2. Measure parameter efficiency gains versus performance on held-out tasks
3. Analyze routing activation patterns to verify task-specific module selection

## Open Questions the Paper Calls Out
None specified in the provided materials.

## Limitations
- Experimental validation restricted to T0 benchmark, leaving unclear how TensorPoly performs on other task distributions
- Limited ablation studies demonstrating why tensor products specifically outperform alternative modularization approaches
- No detailed computational overhead analysis of the routing mechanisms
- Scalability to very large models remains unproven

## Confidence
- **High Confidence**: Basic architecture and implementation of TLoRA with tensor product structure
- **Medium Confidence**: Reported performance improvements on T0 benchmarks
- **Medium Confidence**: Claims about mitigating negative transfer

## Next Checks
1. Conduct ablation studies comparing tensor product routing against alternative modular architectures on identical task sets to isolate the specific contribution of the tensor structure
2. Perform scalability testing by implementing TensorPoly on larger foundation models (e.g., 30B+ parameters) and measuring both performance and computational overhead across different tensor ranks
3. Test cross-task generalization by evaluating TensorPoly on tasks outside the T0 distribution, including low-resource languages and novel task types not seen during training, to assess true compositional generalization capabilities