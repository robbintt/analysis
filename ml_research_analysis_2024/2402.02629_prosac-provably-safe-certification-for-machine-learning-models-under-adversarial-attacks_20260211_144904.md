---
ver: rpa2
title: 'PROSAC: Provably Safe Certification for Machine Learning Models under Adversarial
  Attacks'
arxiv_id: '2402.02629'
source_url: https://arxiv.org/abs/2402.02629
tags:
- adversarial
- learning
- machine
- attack
- risk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces PROSAC, a framework for certifying machine\
  \ learning model robustness against adversarial attacks with population-level risk\
  \ guarantees. The key idea is to formulate a hypothesis testing problem where a\
  \ model is considered (\u03B1, \u03B6)-safe if the probability of declaring the\
  \ adversarial risk below \u03B1 while it is actually higher is less than \u03B6."
---

# PROSAC: Provably Safe Certification for Machine Learning Models under Adversarial Attacks

## Quick Facts
- arXiv ID: 2402.02629
- Source URL: https://arxiv.org/abs/2402.02629
- Authors: Chen Feng; Ziquan Liu; Zhuo Zhi; Ilija Bogunovic; Carsten Gerner-Beuerle; Miguel Rodrigues
- Reference count: 13
- Primary result: Introduces PROSAC framework providing provable safety guarantees for ML models against adversarial attacks using hypothesis testing and Bayesian optimization

## Executive Summary
PROSAC is a framework for certifying the safety of machine learning models against adversarial attacks with rigorous statistical guarantees. The approach formulates the problem as a hypothesis testing task where a model is considered (α, ζ)-safe if the probability of incorrectly declaring adversarial risk below threshold α is less than ζ. The framework uses Bayesian optimization (GP-UCB) to efficiently approximate p-values across different attack hyperparameters, enabling practical certification of model safety at the population level.

## Method Summary
The PROSAC framework establishes safety guarantees by treating adversarial risk as a random variable and computing p-values for hypothesis tests about whether the risk exceeds a threshold α. The key insight is to formulate this as a search problem over attack hyperparameters to find the most likely attack that violates the safety condition. To make this computationally tractable, the framework employs GP-UCB (Gaussian Process Upper Confidence Bound) Bayesian optimization to approximate the p-value. This allows the framework to provide statistical guarantees about model safety under adversarial attacks, going beyond empirical robustness measures to offer provable risk bounds.

## Key Results
- Vision transformers (ViTs) demonstrate greater robustness to adversarial attacks than ResNets on ImageNet-1k
- Larger model architectures show increased robustness compared to smaller counterparts
- The framework provides rigorous, provable safety guarantees suitable for regulatory compliance
- PROSAC achieves population-level risk guarantees beyond existing empirical approaches

## Why This Works (Mechanism)
The framework's effectiveness stems from its statistical foundation in hypothesis testing. By treating adversarial risk as a random variable and computing p-values, PROSAC can make probabilistic statements about model safety. The use of Bayesian optimization allows efficient exploration of the attack hyperparameter space to find the most likely adversarial examples that would violate safety guarantees. This combination of statistical rigor and computational efficiency enables practical certification of model safety under realistic adversarial conditions.

## Foundational Learning

**Hypothesis Testing**
- Why needed: Provides statistical framework for making decisions about model safety
- Quick check: Verify p-value computation correctly captures probability of safety violation

**Bayesian Optimization**
- Why needed: Efficiently searches high-dimensional attack hyperparameter space
- Quick check: Confirm GP-UCB convergence properties for p-value approximation

**Gaussian Processes**
- Why needed: Models uncertainty in adversarial risk estimation
- Quick check: Validate GP assumptions about smoothness and kernel choice

## Architecture Onboarding

Component Map: Attack space -> GP-UCB optimizer -> p-value computation -> Safety decision
Critical Path: The core computation involves evaluating model safety through hypothesis testing using adversarial examples generated via Bayesian optimization of attack hyperparameters.
Design Tradeoffs: The framework balances statistical rigor against computational efficiency by using GP-UCB to approximate p-values rather than exhaustive search.
Failure Signatures: Poor performance may arise from GP-UCB convergence issues, inadequate coverage of attack space, or violations of GP assumptions.
First Experiments:
1. Validate p-value computation on simple synthetic adversarial scenarios
2. Test GP-UCB convergence across varying dimensionalities of attack hyperparameter space
3. Compare PROSAC safety guarantees against empirical robustness measures on standard benchmarks

## Open Questions the Paper Calls Out
None

## Limitations
- Computational bottlenecks may arise from Bayesian optimization, especially in high-dimensional attack spaces
- The framework's assumptions about adversarial attacks as random variables may not capture worst-case scenarios
- Effectiveness heavily depends on choice of attack algorithms and hyperparameters
- Real-world deployment scenarios may reveal limitations not apparent in controlled experiments

## Confidence

**High confidence** in theoretical framework and mathematical formulation of safety guarantees
**Medium confidence** in experimental results comparing ViTs and ResNets
**Low confidence** in generalizability to real-world deployment scenarios

## Next Checks

1. Validate computational efficiency and convergence properties of GP-UCB algorithm across diverse attack hyperparameter spaces and model architectures
2. Conduct experiments using broader range of attack algorithms and strategies to assess framework's robustness against various adversarial techniques
3. Implement and test framework in real-world deployment scenario to evaluate practical effectiveness and identify limitations not captured in controlled experiments