---
ver: rpa2
title: SGW-based Multi-Task Learning in Vision Tasks
arxiv_id: '2410.03778'
source_url: https://arxiv.org/abs/2410.03778
tags:
- learning
- task
- tasks
- neural
- multi-task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of inter-task interference in
  multi-task learning (MTL), particularly the challenge of knowledge sharing in large-scale
  and complex tasks. The authors identify a flaw in the cross-attention mechanism,
  where noise from irrelevant tasks can interfere with the performance of other tasks.
---

# SGW-based Multi-Task Learning in Vision Tasks

## Quick Facts
- arXiv ID: 2410.03778
- Source URL: https://arxiv.org/abs/2410.03778
- Authors: Ruiyuan Zhang; Yuyao Chen; Yuchi Huo; Jiaxiang Liu; Dianbing Xi; Jie Liu; Chao Wu
- Reference count: 40
- Primary result: KEM and sKEM achieve state-of-the-art performance on NYUD-v2 and PASCAL VOC datasets by reducing inter-task interference through information bottleneck and ETF projection

## Executive Summary
This paper addresses inter-task interference in multi-task learning (MTL) by proposing the Knowledge Extraction Module (KEM), which compresses task knowledge into a shared bottleneck memory to filter noise. KEM operates through three steps—Retrieve, Write, Broadcast—reducing computational complexity while improving task performance. The authors also introduce Stable KEM (sKEM), which uses neural collapse via Equiangular Tight Frame (ETF) projection to enhance stability under imbalanced input distributions. Experiments demonstrate significant performance gains over existing methods on standard vision benchmarks.

## Method Summary
The authors propose KEM, a module that reduces inter-task interference in MTL by compressing task features into a fixed-size memory using attention-based retrieval. The module follows three steps: Retrieve (attend to task features), Write (compress into memory slots), and Broadcast (distribute compressed knowledge back to tasks). This approach filters noise through Top-K selection and reduces computational complexity from O(2n²·de) to O(3ns·d²e). sKEM extends KEM by projecting input features into an Equiangular Tight Frame (ETF) space using neural collapse principles, making the method robust to imbalanced input distributions. Both modules are integrated into a multi-task vision architecture with pre-trained Swin-Transformer encoders and task-specific decoders.

## Key Results
- KEM and sKEM achieve state-of-the-art performance on NYUD-v2 and PASCAL VOC datasets
- Significant reduction in inter-task interference compared to standard cross-attention methods
- Computational complexity reduced from O(2n²·de) to O(3ns·d²e)
- sKEM demonstrates improved stability under imbalanced input distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: KEM reduces inter-task interference by compressing task knowledge into a shared bottleneck memory and filtering noise through Top-K selection.
- Mechanism: Task features are compressed into a fixed-size memory using attention-based retrieval, which inherently filters out noise by only retaining the top K most relevant values before broadcasting back to each task.
- Core assumption: The cross-attention softmax operation in standard MTL creates noise by assigning non-zero weights to irrelevant task features, and constraining this flow via a bottleneck reduces interference.
- Evidence anchors:
  - [abstract] "KEM compresses the knowledge from tasks into an information-bottleneck memory and then distributes the memory to different tasks."
  - [section] "Due to the choice mechanism, KEM can eliminate ineffective noise information, retain common information, and resist interference between tasks."
- Break condition: If the Top-K selection is too aggressive or the memory size L is too small, useful information may be discarded, harming task performance.

### Mechanism 2
- Claim: Neural collapse is used to stabilize the Top-K selection process by projecting features into an Equiangular Tight Frame (ETF) space, making the method robust to imbalanced input distributions.
- Mechanism: Before applying KEM, input features are projected into ETF space via a fixed matrix W*, which aligns features from the same class and increases inter-class separability, reducing sensitivity to statistical imbalances in the input.
- Core assumption: Input features fed to KEM are not guaranteed to be balanced; ETF projection helps ensure Top-K remains stable regardless of input imbalance.
- Evidence anchors:
  - [section] "sKEM projects the input features into an Equiangular Tight Frame (ETF space), allowing memory to select even statistical-less features."
  - [section] "We have theoretically shown that sKEM can improve resistance within imbalanced input."
- Break condition: If the number of tasks does not match the ETF vertex count K, or if input data does not exhibit the class structure assumed by neural collapse, the projection may degrade rather than improve performance.

### Mechanism 3
- Claim: KEM reduces computational complexity from O(2n²·de) in standard cross-attention to O(3ns·d²e) by limiting memory size L << ns.
- Mechanism: By compressing task features into a fixed-size memory (L slots), KEM replaces the full ns × ns attention matrix with smaller L × ns operations, dramatically reducing the number of multiplications required.
- Core assumption: L is kept small relative to the sequence length ns and feature dimension de, so that the L × ns operations are cheaper than the full ns × ns attention.
- Evidence anchors:
  - [section] "The total computational complexity for cross-attention is O(2n²·de). Meanwhile, the total computational complexity for KEM is O(3ns·d²e), which implies that KEM has a lower computational complexity."
  - [section] "The memory size L is constant, and the accompanying benefit is that we can also reduce the algorithmic complexity to a linear level."
- Break condition: If L is increased to match ns (e.g., in extreme cases), the computational advantage disappears and the method reverts to cross-attention complexity.

## Foundational Learning

- Concept: Information Bottleneck Principle
  - Why needed here: Provides the theoretical justification for compressing task features into a smaller memory to filter noise while retaining task-relevant information.
  - Quick check question: What is the key trade-off the information bottleneck tries to balance in representation learning?

- Concept: Cross-Attention Mechanism in Transformers
  - Why needed here: Understanding how standard cross-attention in MTL works is essential to see why noise is introduced and how KEM modifies it.
  - Quick check question: In cross-attention, what does the softmax operation do to attention weights, and why can this introduce noise?

- Concept: Neural Collapse Phenomenon
  - Why needed here: Explains why projecting features into ETF space stabilizes Top-K selection in sKEM, especially under imbalanced input.
  - Quick check question: What happens to features of the same class during neural collapse in the later stages of training?

## Architecture Onboarding

- Component map: Image -> Shared Swin-Transformer encoder (frozen) -> Task-specific encoders (2-layer ViT) -> KEM (Retrieve-Write-Broadcast) -> Task-specific decoders -> Outputs
- Critical path: Image → Shared encoder → Task encoders → KEM module → Task decoders → Final predictions
- Design tradeoffs:
  - Memory size L vs. computational cost vs. information retention: smaller L = faster but more lossy.
  - Top-K choice K vs. noise filtering vs. information loss: larger K = less aggressive filtering but more noise.
  - Using sKEM vs. KEM: sKEM is more stable under imbalanced inputs but requires ETF projection, adding complexity.
- Failure signatures:
  - Degraded task performance with very small L or very small K.
  - Performance drops when input feature distributions are highly imbalanced without ETF projection.
  - Increased computational cost if L is not kept small relative to ns.
- First 3 experiments:
  1. Run KEM with varying L (e.g., 5, 10, 20, 30) on NYUD-v2 and measure mIoU/RMSE to find the optimal trade-off.
  2. Compare KEM vs. sKEM on a long-tailed version of Sort-of-CLEVER to validate robustness to imbalance.
  3. Profile computational cost (step/second and GPU memory) for KEM, sKEM, and standard cross-attention to confirm complexity reduction.

## Open Questions the Paper Calls Out
None

## Limitations
- Lack of precise architectural specifications for task-specific encoders/decoders
- Incomplete implementation details for Equiangular Tight Frame (ETF) projection
- Performance claims based on standard vision datasets may not generalize to more complex scenarios
- Theoretical analysis of computational complexity depends on parameter choices not fully specified

## Confidence

- **High Confidence**: The core mechanism of KEM (Retrieve-Write-Broadcast) and its information bottleneck approach for reducing inter-task interference.
- **Medium Confidence**: The stability improvements claimed for sKEM via neural collapse and ETF projection, as the theoretical justification is sound but practical implementation details are sparse.
- **Medium Confidence**: The computational complexity reduction claims, as the analysis is theoretically grounded but depends on parameter choices not fully specified in the paper.

## Next Checks

1. **Parameter Sensitivity Analysis**: Systematically vary memory size L (e.g., 5, 10, 20, 30) and Top-K value K on NYUD-v2 to identify optimal trade-offs between performance and computational efficiency.

2. **Robustness to Imbalanced Data**: Evaluate KEM vs. sKEM on a long-tailed version of Sort-of-CLEVER or similar benchmark to validate the ETF projection's effectiveness under severe class imbalance.

3. **Computational Profiling**: Measure actual GPU memory usage and step times during training for KEM, sKEM, and standard cross-attention on identical hardware to verify the claimed O(3ns · d²e) complexity reduction.