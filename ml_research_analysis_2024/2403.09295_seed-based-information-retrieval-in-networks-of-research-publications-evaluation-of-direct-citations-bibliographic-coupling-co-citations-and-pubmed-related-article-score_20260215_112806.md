---
ver: rpa2
title: 'Seed-based information retrieval in networks of research publications: Evaluation
  of direct citations, bibliographic coupling, co-citations and PubMed related article
  score'
arxiv_id: '2403.09295'
source_url: https://arxiv.org/abs/2403.09295
tags:
- publications
- approaches
- citation
- information
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares the performance of direct citation, bibliographic
  coupling, co-citation, and PubMed related article score for seed-based information
  retrieval in biomedical research publications. Using systematic reviews as a baseline
  and NIH Open Citation Collection data, the authors find that co-citation outperforms
  direct citation and bibliographic coupling.
---

# Seed-based information retrieval in networks of research publications: Evaluation of direct citations, bibliographic coupling, co-citations and PubMed related article score

## Quick Facts
- arXiv ID: 2403.09295
- Source URL: https://arxiv.org/abs/2403.09295
- Reference count: 11
- Primary result: Co-citation outperforms direct citation and bibliographic coupling for seed-based information retrieval in biomedical research publications

## Executive Summary
This study evaluates four citation-based approaches for seed-based information retrieval in biomedical research publications: direct citations, bibliographic coupling, co-citations, and PubMed related article score. Using systematic reviews as a baseline and NIH Open Citation Collection data, the authors systematically compare these methods' performance in retrieving relevant publications. The research provides empirical evidence for selecting citation similarity measures in information retrieval systems, particularly for researchers conducting literature reviews in the biomedical domain.

## Method Summary
The authors conducted a comparative evaluation using systematic reviews as a baseline for complete relevant publication sets. They utilized NIH Open Citation Collection data to analyze four citation-based approaches: direct citations, bibliographic coupling, co-citations, and PubMed related article score. The study measured recall and performance across these methods, examining both individual approach effectiveness and the impact of combining multiple approaches. The analysis focused on biomedical research publications and compared each method's ability to retrieve relevant publications from seed sets.

## Key Results
- Co-citation consistently outperformed direct citation and bibliographic coupling in retrieval performance
- Combining the three citation-based approaches significantly improved recall compared to any single method
- Adding PubMed related article score to citation-based approaches further enhanced retrieval performance

## Why This Works (Mechanism)
The superior performance of co-citation stems from its ability to capture semantic similarity between publications through shared citation patterns. When two papers cite similar sources, they likely address related research questions or methodologies, making co-citation a stronger indicator of topical relevance than direct citation relationships. The combination of multiple approaches leverages different aspects of citation networks: direct citations capture immediate influence, bibliographic coupling identifies contemporaneous research communities, and co-citations reveal conceptual similarity. The PubMed related article score adds content-based similarity, creating a multi-dimensional retrieval approach that captures both citation and textual relationships.

## Foundational Learning
1. **Citation network analysis** - Understanding how publications connect through citation relationships; needed to interpret the different similarity measures and their strengths
2. **Information retrieval metrics** - Familiarity with recall, precision, and related evaluation measures; needed to assess and compare method performance
3. **Systematic review methodology** - Knowledge of how systematic reviews are conducted and used as gold standards; needed to understand the baseline approach
4. **Biomedical literature structure** - Understanding of PubMed, NIH Open Citation Collection, and biomedical publication conventions; needed to contextualize the domain-specific findings
5. **Network similarity measures** - Knowledge of different ways to measure similarity in networks; needed to differentiate between direct citation, bibliographic coupling, and co-citation approaches

## Architecture Onboarding

Component map: Seed publications -> Citation analysis methods (direct, bibliographic coupling, co-citation, PubMed related) -> Performance evaluation -> Combined approach optimization

Critical path: Seed selection → Method application → Performance measurement → Combination strategy → Final retrieval output

Design tradeoffs: The study balances computational complexity against retrieval performance, choosing to combine multiple methods despite increased computational cost for better recall. The use of systematic reviews as ground truth provides methodological rigor but may introduce bias if reviews are incomplete.

Failure signatures: Poor performance occurs when citation networks are sparse (few connections between publications), when seed sets are too narrow or too broad, or when the domain has rapidly evolving terminology that citation patterns don't capture well.

First experiments:
1. Compare single method performance across different seed set sizes
2. Test combination strategies with varying weights for each method
3. Evaluate performance on non-biomedical domains to test generalizability

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis relies on NIH Open Citation Collection, potentially missing citations outside NIH-funded publications
- Systematic reviews used as baseline may themselves be incomplete due to publication bias
- Comparison limited to four specific methods, potentially overlooking other effective approaches
- Evaluation metrics and methodology details not fully specified in abstract

## Confidence
- High confidence in the relative performance ranking of the four compared methods
- Medium confidence in the improvement claims from combining methods
- Low confidence in generalizability to non-biomedical domains

## Next Checks
1. Replicate the analysis using alternative citation databases (e.g., Scopus, Web of Science) to assess robustness across different data sources
2. Conduct ablation studies to determine the specific contribution of each citation-based method to the combined approach's performance
3. Test the optimal combination strategies on a separate, independently curated set of systematic reviews to validate generalizability