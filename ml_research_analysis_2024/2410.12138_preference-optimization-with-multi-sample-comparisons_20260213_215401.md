---
ver: rpa2
title: Preference Optimization with Multi-Sample Comparisons
arxiv_id: '2410.12138'
source_url: https://arxiv.org/abs/2410.12138
tags:
- story
- arxiv
- mdpo
- mipo
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of current post-training methods
  like RLHF and DAP, which primarily use single-sample comparisons and fail to capture
  collective characteristics like generative diversity and bias. The authors propose
  Multi-sample Direct Preference Optimization (mDPO) and Multi-sample Identity Preference
  Optimization (mIPO), which extend DAP methods to utilize multi-sample comparisons
  for better optimization of group-wise characteristics.
---

# Preference Optimization with Multi-Sample Comparisons

## Quick Facts
- arXiv ID: 2410.12138
- Source URL: https://arxiv.org/abs/2410.12138
- Reference count: 40
- Multi-sample comparison improves optimization of collective characteristics like diversity and bias in generative models

## Executive Summary
This paper addresses the limitations of current post-training methods like RLHF and DAP, which primarily use single-sample comparisons and fail to capture collective characteristics like generative diversity and bias. The authors propose Multi-sample Direct Preference Optimization (mDPO) and Multi-sample Identity Preference Optimization (mIPO), which extend DAP methods to utilize multi-sample comparisons for better optimization of group-wise characteristics. Empirically, the authors demonstrate that multi-sample comparison is more effective in optimizing collective characteristics such as diversity and bias for generative models compared to single-sample comparison. The methods also provide a more robust optimization framework, particularly for datasets with label noise.

## Method Summary
The paper proposes extending Direct Preference Optimization (DPO) and Identity Preference Optimization (IPO) to multi-sample settings. The key innovation is replacing single-sample comparisons with group-wise comparisons, where k samples are drawn from each distribution being compared. The methods use a preference estimator that computes the difference between average scores of k samples from each group. This approach captures distributional properties that are missed by single-sample comparisons. The paper provides theoretical analysis showing variance reduction benefits and empirical validation across three domains: random number generation, text-to-image generation, and fiction writing. The methods are implemented using LoRA fine-tuning with specific hyperparameters for each experiment.

## Key Results
- Multi-sample comparison significantly improves diversity metrics in text-to-image generation (Simpson Diversity Index increased from 0.265 to 0.503)
- Random number generation becomes more uniform with multi-sample optimization, reducing KL divergence from 0.689 to 0.014
- Fiction generation shows improved lexical diversity (distinct-1 from 0.088 to 0.097) and genre distribution entropy (from 2.27 to 2.57)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Multi-sample comparisons improve the optimization of collective characteristics like diversity and bias.
- **Mechanism**: By evaluating groups of samples rather than individual ones, the method captures distributional properties that are missed by single-sample comparisons. This is especially important for assessing variability and diversity across multiple outputs.
- **Core assumption**: The distributional characteristics (e.g., diversity, bias) can only be accurately assessed through the joint distribution of multiple samples, not through marginal distributions.
- **Evidence anchors**:
  - [abstract]: "current post-training methods such as reinforcement learning from human feedback (RLHF) and direct alignment from preference methods (DAP) primarily utilize single-sample comparisons. These approaches often fail to capture critical characteristics such as generative diversity and bias, which are more accurately assessed through multiple samples."
  - [section 4.1]: "Consider the cases where we prompt the diffusion models to generate an image of 'a software engineer'. For such cases, there is no preference between 'a female software engineer' and 'a male software engineer'. Similarly, if a language model is prompted to generate an integer randomly and uniformly from {0, 1, ...,9}, there is also no preference between generating any two specific numbers, e.g., 3 or 5. Nonetheless, preferences can exist at a multi-sample or joint distribution level."
- **Break condition**: If the characteristics being optimized are not truly distributional in nature, or if the group sizes are too small to capture meaningful distributional differences.

### Mechanism 2
- **Claim**: The multi-sample estimator reduces variance compared to single-sample comparisons.
- **Mechanism**: By averaging over multiple samples from each group, the estimator for the preference comparison has lower variance. This is particularly beneficial when the underlying distributions have high variance.
- **Core assumption**: The variance of the estimator decreases with the square root of the sample size, making larger group comparisons more stable.
- **Evidence anchors**:
  - [section 4.2]: "Proposition 2. Let µp = Ex∼p[f(x)], µq = Ex∼q[f(x)], σ2p = Varx∼p[f(x)], σ2q = Varx∼q[f(x)] and n and m be the number of independent samples from distributions p and q, respectively. Then, the variance of the mini-batch estimator ˆℓ is given by Var(ˆℓ) = O (σ2p/n + σ2q/m) · (σ2p/n + σ2q/m + (µp − µq − c)2)!"
  - [section 4.2]: "Figure 2 Biased estimator vs. Unbiased estimator. We observe that the unbiased estimator results in lower error compared to the biased estimator at small sample sizes. However, as the sample size increases, the error of the biased estimator also decreases and performs similarly to the unbiased one."
- **Break condition**: If the number of samples per group is too small, the variance reduction benefit may not be realized.

### Mechanism 3
- **Claim**: Multi-sample comparisons are more robust to label noise in preference data.
- **Mechanism**: When comparing groups of samples, the law of large numbers ensures that the correct group preference is more likely to be identified, even if individual comparisons are noisy.
- **Core assumption**: The average quality of samples from one group is consistently better than the other group, making group-level comparison more reliable than individual comparisons.
- **Evidence anchors**:
  - [section 5.4]: "Remark 1. For two independent and bounded random variables X and Y, if E[X] − E[Y ] > 0, then the probability p(Pk i=1 Xi >Pk i=1 Yi) will (approximately) increase as the sample size k increases. Therefore, the multi-sample pairwise comparison is (approximately) more likely to be correct than the single-sample pairwise comparison."
  - [section 5.4]: "Our experiments reveal that without labeling noise, multi-sample comparison has no advantage over single-sample comparison... To confirm the impact of potentially noisy labels, we switched to the default labeling. In this case, both k = 2 and k = 3 outperformed k = 1."
- **Break condition**: If the noise is systematic or if the group averages are not reliably better, the robustness advantage may not materialize.

## Foundational Learning

- **Concept**: Bradley-Terry model for pairwise comparisons
  - **Why needed here**: The original DPO and IPO methods are built on the Bradley-Terry model, which assumes Gumbel noise in the reward function. Understanding this foundation is crucial for extending to multi-sample comparisons.
  - **Quick check question**: How does the Bradley-Terry model compute the probability of one sample being preferred over another?

- **Concept**: Direct Preference Optimization (DPO) and Identity Preference Optimization (IPO)
  - **Why needed here**: The paper extends these existing methods to multi-sample settings. Understanding their original formulations is essential for grasping the proposed modifications.
  - **Quick check question**: What is the key difference between DPO and IPO in how they model preferences?

- **Concept**: Variance and bias in estimators
  - **Why needed here**: The paper analyzes the variance and bias of the proposed multi-sample estimators. Understanding these concepts is crucial for evaluating the effectiveness of the approach.
  - **Quick check question**: How does increasing the sample size affect the variance and bias of an estimator?

## Architecture Onboarding

- **Component map**: Base generative model (LLM or diffusion model) -> Preference dataset with group comparisons -> Multi-sample optimization algorithms (mDPO/mIPO) -> Updated model parameters
- **Critical path**: Generate groups of samples for each prompt → Collect preference data at the group level → Apply mDPO or mIPO to update model parameters → Evaluate model on collective characteristics (diversity, bias)
- **Design tradeoffs**: Larger group sizes reduce variance but increase computational cost. The choice between mDPO and mIPO depends on whether low-variance or unbiased estimation is prioritized.
- **Failure signatures**: If the model fails to improve collective characteristics, it may indicate that the group sizes are too small or that the preference data is not capturing the true distributional differences.
- **First 3 experiments**:
  1. Implement mDPO and mIPO with small group sizes (k=2) on a toy dataset to verify basic functionality.
  2. Gradually increase group sizes and measure variance reduction in the estimator.
  3. Test the methods on a real generative task (e.g., random number generation) and evaluate improvements in collective characteristics.

## Open Questions the Paper Calls Out
No specific open questions were explicitly called out in the provided content.

## Limitations
- Computational overhead of multi-sample comparisons is not thoroughly analyzed
- Experiments are limited to synthetic and controlled settings rather than real-world scenarios
- The impact of group size choice on both performance and efficiency is not fully explored

## Confidence

- **Variance reduction mechanism**: High confidence - Well-established mathematical foundation with clear theoretical analysis
- **Label noise robustness**: Medium confidence - Demonstrated in controlled experiments but may not fully generalize to real-world noisy preference data
- **Generalizability to other domains**: Medium confidence - Results are promising but limited to specific task domains (RNG, text-to-image, fiction generation)

## Next Checks

1. **Scalability test**: Evaluate mDPO/mIPO on larger-scale generative models (e.g., GPT-4 level) to assess computational feasibility and performance gains

2. **Real-world preference noise**: Test robustness on preference datasets with naturally occurring noise rather than synthetically injected noise

3. **Human evaluation**: Conduct perceptual studies to validate whether improvements in diversity metrics correspond to meaningful quality improvements as judged by humans