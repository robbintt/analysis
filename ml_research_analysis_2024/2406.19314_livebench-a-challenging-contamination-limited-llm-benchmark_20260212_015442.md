---
ver: rpa2
title: 'LiveBench: A Challenging, Contamination-Limited LLM Benchmark'
arxiv_id: '2406.19314'
source_url: https://arxiv.org/abs/2406.19314
tags:
- questions
- livebench
- task
- tasks
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "LiveBench is a benchmark designed to evaluate LLMs using frequently-updated\
  \ questions from recent sources and objective ground-truth scoring, avoiding the\
  \ pitfalls of LLM judging and test set contamination. It contains 18 tasks across\
  \ six categories\u2014math, coding, reasoning, language, instruction following,\
  \ and data analysis\u2014with questions sourced from recent math competitions, arXiv\
  \ papers, news articles, and datasets."
---

# LiveBench: A Challenging, Contamination-Limited LLM Benchmark

## Quick Facts
- arXiv ID: 2406.19314
- Source URL: https://arxiv.org/abs/2406.19314
- Reference count: 40
- Primary result: LiveBench evaluates LLMs with fresh, objective-scored questions across 18 tasks; top models score below 70% accuracy

## Executive Summary
LiveBench is a novel benchmark designed to evaluate large language models using frequently-updated questions from recent sources and objective ground-truth scoring. It contains 18 tasks across six categories—math, coding, reasoning, language, instruction following, and data analysis—with questions sourced from recent math competitions, arXiv papers, news articles, and datasets. The benchmark is updated monthly, with 1/6 of questions replaced each cycle to maintain freshness and challenge. LiveBench is scored automatically via regex-based grading to ensure fairness, and it includes synthetic and harder versions of tasks from prior benchmarks like Big-Bench Hard, AMPS, and IFEval.

Evaluation across 40 models (0.5B to 405B parameters) shows top models achieving below 70% accuracy, demonstrating its difficulty. o1-preview-2024-09-12 is the highest performer overall, excelling in data analysis, language, and math. Open-source models like llama-3.1-405b-instruct and qwen2.5-72b-instruct perform competitively with leading proprietary models. LiveBench aims to provide a sustainable, contamination-resistant framework for future LLM evaluation.

## Method Summary
LiveBench employs a contamination-avoidance strategy by sourcing questions from recent publications and updating its dataset monthly, replacing 1/6 of questions each cycle. Questions are drawn from recent math competitions, arXiv papers, news articles, and curated datasets. The benchmark uses regex-based grading for objective, automated scoring without relying on LLM judges. It includes 18 tasks across six categories: math, coding, reasoning, language, instruction following, and data analysis. The benchmark features harder variants of tasks from existing benchmarks like Big-Bench Hard, AMPS, and IFEval, as well as synthetic questions designed to be more challenging than their originals.

## Key Results
- Top-performing models achieve below 70% accuracy, demonstrating benchmark difficulty
- o1-preview-2024-09-12 leads overall, excelling in data analysis, language, and math
- Open-source models like llama-3.1-405b-instruct and qwen2.5-72b-instruct perform competitively with leading proprietary models

## Why This Works (Mechanism)
LiveBench's contamination-avoidance strategy relies on three mechanisms: temporal freshness through monthly updates, objective scoring via regex matching, and diverse question sources. The monthly update cycle with 1/6 question replacement creates a moving target that reduces the likelihood of model exposure during training. Regex-based grading eliminates the variability and potential bias of LLM judges, providing consistent evaluation. The diverse sourcing from recent math competitions, arXiv papers, news articles, and synthetic datasets creates a broad challenge space that tests different capabilities while maintaining difficulty through recency.

## Foundational Learning
- **Contamination detection**: Understanding how to verify whether test questions have leaked into training data is crucial because widespread contamination undermines benchmark validity and leads to inflated performance metrics. Quick check: Analyze model responses for patterns suggesting prior exposure to source materials.
- **Objective scoring systems**: Implementing automated, non-LLM-based grading is essential to eliminate judge bias and ensure fair comparison across models. Quick check: Validate regex patterns against human expert grading on a sample set.
- **Temporal benchmarking**: Creating a sustainable evaluation framework that maintains relevance over time requires systematic question rotation and freshness protocols. Quick check: Monitor performance degradation on older questions versus newer ones.
- **Multi-task evaluation design**: Balancing diverse task categories while maintaining consistent difficulty levels ensures comprehensive capability assessment. Quick check: Analyze score distributions across categories for balance.
- **Synthetic question generation**: Developing harder variants of existing benchmark questions extends evaluation scope and maintains challenge levels. Quick check: Compare performance on original versus synthetic versions of the same task.

## Architecture Onboarding

Component map: Monthly update scheduler -> Question source curator -> Regex grader -> Model evaluator -> Results aggregator -> Leaderboard updater

Critical path: Question selection → Grading template creation → Model inference → Automated scoring → Performance analysis

Design tradeoffs: The benchmark prioritizes contamination resistance over absolute fairness in grading, accepting potential false negatives from strict regex matching to ensure questions remain fresh and unmemorized. This tradeoff favors long-term validity over short-term accuracy measurement.

Failure signatures: Models scoring unexpectedly high on specific categories may indicate contamination from recent training data sources. Inconsistent performance across monthly updates suggests instability in model capabilities rather than true knowledge. Poor performance on regex-scored questions where human experts would accept answers indicates grading system limitations.

First experiments:
1. Evaluate a diverse set of models (0.5B to 405B parameters) across all 18 tasks to establish baseline performance distributions
2. Compare regex-graded scores against human expert evaluation on a sample of 100 responses to quantify grading strictness
3. Test temporal stability by running the same evaluation suite after 30 days to assess question freshness effectiveness

## Open Questions the Paper Calls Out
None

## Limitations
- Regex-based scoring may produce false negatives by rejecting correct answers that deviate from expected patterns
- True contamination avoidance is difficult to guarantee given overlapping training data sources and widespread availability of recent publications
- Limited task categories may not fully represent real-world LLM applications and capabilities

## Confidence
- **Medium confidence**: Claim that benchmark avoids contamination through monthly updates, due to possibility of latent model knowledge of source materials
- **Medium confidence**: Assertion that regex scoring ensures fairness, as it may miss nuanced correct answers
- **High confidence**: Finding that top models score below 70% accuracy, based on systematic evaluation across 40 diverse models

## Next Checks
1. Conduct systematic analysis of false negative rates by having human experts review a sample of regex-rejected responses to quantify grading system strictness
2. Implement contamination detection protocol by analyzing model outputs for patterns suggesting prior exposure to source materials, particularly for competition mathematics and scientific content
3. Test benchmark's temporal stability by evaluating the same questions after different time intervals to assess whether difficulty is primarily due to recency effects versus intrinsic complexity