---
ver: rpa2
title: 'RLHF Workflow: From Reward Modeling to Online RLHF'
arxiv_id: '2405.07863'
source_url: https://arxiv.org/abs/2405.07863
tags:
- preference
- arxiv
- learning
- rlhf
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a detailed workflow for online iterative Reinforcement
  Learning from Human Feedback (RLHF) in large language models. The authors address
  the limitations of offline RLHF by constructing a preference model using open-source
  datasets as a proxy for human feedback, enabling iterative policy updates.
---

# RLHF Workflow: From Reward Modeling to Online RLHF

## Quick Facts
- arXiv ID: 2405.07863
- Source URL: https://arxiv.org/abs/2405.07863
- Authors: Hanze Dong; Wei Xiong; Bo Pang; Haoxiang Wang; Han Zhao; Yingbo Zhou; Nan Jiang; Doyen Sahoo; Caiming Xiong; Tong Zhang
- Reference count: 40
- Key outcome: Presents a detailed workflow for online iterative RLHF in LLMs using offline preference modeling to overcome data limitations

## Executive Summary
This paper addresses the limitations of offline RLHF in large language models by proposing a comprehensive online iterative RLHF workflow. The authors construct a preference model using open-source datasets as a proxy for human feedback, enabling iterative policy updates. Their approach combines theoretical insights from online exploration with practical implementation details, including temperature tuning and rejection sampling. The resulting model achieves state-of-the-art performance on multiple benchmarks while maintaining academic capabilities.

## Method Summary
The authors propose an iterative RLHF workflow that overcomes the limitations of offline approaches by using offline preference modeling with open-source datasets as a proxy for human feedback. The method incorporates temperature tuning and rejection sampling to balance exploration and exploitation during the online learning process. The workflow enables multiple rounds of policy updates, allowing the model to continuously improve through interaction with the preference model rather than relying on static human-labeled datasets.

## Key Results
- Achieves state-of-the-art performance on AlpacaEval-2 with 31.3% length-control win rate
- Outperforms larger models on MT-Bench and Chat-Arena-Hard benchmarks
- Demonstrates effective improvement in conversation quality while maintaining academic capabilities

## Why This Works (Mechanism)
The approach works by bridging the gap between offline preference modeling and online RLHF through iterative refinement. By using open-source datasets to construct a proxy preference model, the system can generate synthetic feedback that approximates human preferences without requiring expensive continuous human labeling. The temperature tuning mechanism provides controlled exploration, allowing the model to discover high-reward behaviors while avoiding catastrophic forgetting of learned capabilities.

## Foundational Learning
- **Preference modeling**: Learning to predict human preferences between text pairs; needed to generate synthetic feedback for online RLHF; quick check: validate model predictions against human judgments
- **Temperature tuning**: Adjusting sampling temperature to balance exploration and exploitation; needed to maintain diversity while optimizing for reward; quick check: monitor KL divergence between updates
- **Rejection sampling**: Selective acceptance of generated samples based on quality thresholds; needed to filter low-quality outputs during exploration; quick check: measure acceptance rate and quality correlation
- **Iterative policy optimization**: Repeated cycles of policy updates using current preference model; needed to progressively refine model behavior; quick check: track reward progression across iterations
- **Online exploration strategies**: Methods for discovering new high-reward behaviors; needed to escape local optima in reward landscape; quick check: diversity metrics of generated responses
- **Reward modeling**: Learning to predict scalar rewards from preference data; needed to provide gradient signals for RL optimization; quick check: correlation between predicted and actual human preferences

## Architecture Onboarding

**Component map:** Open-source dataset -> Preference model -> Temperature tuner -> Rejection sampler -> Policy optimizer -> Improved LLM

**Critical path:** Dataset preprocessing -> Preference model training -> Temperature parameter search -> Rejection sampling implementation -> Online RL loop -> Evaluation

**Design tradeoffs:** Offline preference modeling reduces human annotation costs but introduces approximation errors; temperature-based exploration provides practical control but requires careful hyperparameter tuning; iterative updates enable continuous improvement but increase computational requirements

**Failure signatures:** Preference model overfitting to synthetic data; temperature settings causing mode collapse or excessive randomness; rejection sampling becoming too restrictive; reward hacking through degenerate solutions

**First experiments:**
1. Validate preference model predictions against human judgments on held-out samples
2. Test temperature sensitivity by running optimization across different temperature ranges
3. Compare different rejection sampling thresholds to find optimal quality vs. diversity tradeoff

## Open Questions the Paper Calls Out
None

## Limitations
- Offline preference modeling introduces approximation errors that cannot fully capture human judgment complexity
- Temperature tuning requires careful hyperparameter selection that may not transfer across different model architectures
- Evaluation metrics rely on specific benchmark conditions that may not reflect real-world conversational performance

## Confidence
- High confidence: Iterative RLHF methodology implementation and benchmark results
- Medium confidence: Temperature-based exploration effectiveness and preference model quality
- Medium confidence: Claim of being "state-of-the-art" (depends on specific benchmarks and comparison conditions)

## Next Checks
1. Conduct human preference evaluation studies to validate synthetic preference model accuracy and identify systematic biases
2. Perform cross-domain generalization tests to assess whether temperature-tuned exploration strategies transfer to non-chat applications
3. Run ablation studies comparing different offline preference modeling approaches to quantify approximation error impact on final performance