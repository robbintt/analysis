---
ver: rpa2
title: 'Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping'
arxiv_id: '2402.14083'
source_url: https://arxiv.org/abs/2402.14083
tags:
- plan
- search
- sequences
- sequence
- optimal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to train Transformer models to solve
  planning tasks by learning to imitate the search dynamics of A search algorithm.
  The key idea is to express the search dynamics as a token sequence, train a Transformer
  model to predict these sequences, and then fine-tune the model to find optimal plans
  with fewer search steps than A.
---

# Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping

## Quick Facts
- **arXiv ID:** 2402.14083
- **Source URL:** https://arxiv.org/abs/2402.14083
- **Reference count:** 32
- **Primary result:** Searchformer solves 93.7% of Sokoban puzzles with 26.8% fewer search steps than A*

## Executive Summary
This paper introduces Searchformer, a method that trains Transformer models to solve planning tasks by learning to imitate A* search dynamics rather than directly predicting optimal plans. The key innovation is expressing A* execution traces as token sequences, training an encoder-decoder Transformer to predict these sequences, and then fine-tuning the model to find optimal plans with fewer search steps. Searchformer significantly outperforms baseline models that predict optimal plans directly, especially in low-data regimes and on complex tasks like Sokoban puzzles.

## Method Summary
The method involves training an encoder-decoder Transformer to predict A* search dynamics expressed as token sequences, then fine-tuning this model to generate shorter execution traces while maintaining optimality. The process begins by generating synthetic A* execution traces for planning tasks, converting these traces into token sequences with a prompt-trace-plan format, and training the Transformer on these sequences. The model is then iteratively improved through search dynamics bootstrapping, where it generates shorter sequences that are used for additional fine-tuning, ultimately discovering more efficient search patterns than the original A* implementation.

## Key Results
- Searchformer solves 93.7% of Sokoban puzzles while using 26.8% fewer search steps than A*
- Outperforms solution-only models by 5-10× in model size and 10× in training dataset size
- Search-augmented models significantly outperform solution-only models in low-data regimes
- Iterative bootstrapping reduces search dynamics length while maintaining optimality

## Why This Works (Mechanism)

### Mechanism 1
**Claim:** Searchformer learns to imitate A* search dynamics by predicting execution traces as token sequences. The model is trained to generate sequences containing A*'s frontier/closed set operations, heuristic values, and cost calculations, enabling it to reproduce the symbolic planner's decision process.

### Mechanism 2
**Claim:** Search-augmented training data enables better performance than solution-only approaches, especially with limited training data. Including intermediate computation steps in training sequences allows the model to learn "on-demand computation" rather than requiring direct input-output correlations.

### Mechanism 3
**Claim:** Search dynamics bootstrapping allows iterative improvement beyond the original A* algorithm. The trained model generates shorter execution traces through expert iteration, discovering more efficient search patterns than A*.

## Foundational Learning

- **Encoder-decoder Transformer architecture with attention mechanisms**: Needed to process task prompts and generate variable-length execution traces with complex dependencies. *Quick check:* How does the decoder's causal attention differ from the encoder's attention in this architecture?

- **Teacher forcing for sequence generation**: Enables the model to learn to predict the next token in an execution trace given the ground truth prefix. *Quick check:* What happens to training stability if we switch from teacher forcing to scheduled sampling?

- **Cross-entropy loss for sequence modeling**: Maximizes the probability of generating the correct execution trace token sequence. *Quick check:* How does sequence length affect the gradient magnitude during training?

## Architecture Onboarding

- **Component map**: Task prompt -> Encoder -> Decoder -> Tokenization layer -> RoPE embeddings -> Execution trace generation
- **Critical path**: 1) Generate synthetic A* execution traces, 2) Convert traces to token sequences, 3) Train encoder-decoder Transformer, 4) Apply search dynamics bootstrapping, 5) Evaluate on test tasks
- **Design tradeoffs**: Longer traces provide more training signal but increase computational cost; non-deterministic A* increases dataset diversity but may introduce noise
- **Failure signatures**: Invalid plan generation, trace length not decreasing during bootstrapping, significant performance degradation on larger tasks, early training loss plateau
- **First 3 experiments**: 1) Train on deterministic A* traces for small mazes, 2) Compare search-augmented vs solution-only models on non-deterministic A* data, 3) Apply bootstrapping to Sokoban dataset and measure ILR improvement

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** How do different tokenization schemes affect the performance of search-augmented models versus solution-only models? The paper mentions different tokenization patterns for maze navigation versus Sokoban tasks but doesn't provide detailed comparison of how tokenization choices impact model performance.

### Open Question 2
**Question:** Can the search dynamics bootstrapping method be extended to improve performance on tasks where optimal solutions are not known or are computationally intractable? The method relies on having optimal solutions to bootstrap from, which may not be available for more complex real-world planning problems.

### Open Question 3
**Question:** What is the impact of the randomization strategy used in non-deterministic A* on the final performance of Searchformer? The paper mentions using a non-deterministic A* implementation but does not explore alternative randomization strategies.

## Limitations
- Evidence for fundamental mechanisms remains weak, particularly regarding the theoretical justification for tokenizing A* execution traces
- Claims about iterative improvement beyond A* lack transparency in methodology and measurement
- Generalization beyond specific planning domains has not been thoroughly validated

## Confidence
- Mechanism 1: Medium - performance supports the claim but lacks theoretical justification
- Mechanism 2: High - strong empirical evidence, but domain specificity concerns remain
- Mechanism 3: Low - methodology and measurement transparency issues

## Next Checks
1. **Generalization Test:** Evaluate Searchformer on planning tasks from different domains (e.g., robotic path planning or logistics problems) that were not seen during training to assess whether the learned search dynamics transfer beyond the training distribution.

2. **Trace Analysis:** Conduct a detailed ablation study comparing the generated execution traces from the fine-tuned model against the original A* traces to identify which specific search patterns or heuristics the model has learned to modify.

3. **Scalability Assessment:** Test Searchformer on significantly larger Sokoban puzzles (beyond 10×10) and measure both success rates and computational efficiency to determine whether the 26.8% improvement in search steps scales with problem complexity.