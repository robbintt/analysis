---
ver: rpa2
title: Zero-Shot Cross-Lingual Document-Level Event Causality Identification with
  Heterogeneous Graph Contrastive Transfer Learning
arxiv_id: '2403.02893'
source_url: https://arxiv.org/abs/2403.02893
tags:
- event
- causal
- relations
- language
- events
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of zero-shot cross-lingual document-level
  event causality identification, aiming to transfer causality knowledge from source
  to low-resource languages. The proposed Heterogeneous Graph Interaction Model with
  Multi-granularity Contrastive Transfer Learning (GIMC) introduces a heterogeneous
  graph interaction network to model long-distance dependencies between events scattered
  across documents, and a multi-granularity contrastive learning module to align causal
  representations across languages.
---

# Zero-Shot Cross-Lingual Document-Level Event Causality Identification with Heterogeneous Graph Contrastive Transfer Learning

## Quick Facts
- arXiv ID: 2403.02893
- Source URL: https://arxiv.org/abs/2403.02893
- Authors: Zhitao He; Pengfei Cao; Zhuoran Jin; Yubo Chen; Kang Liu; Zhiqiang Zhang; Mengshu sun; Jun Zhao
- Reference count: 0
- Primary result: GIMC outperforms baselines by 9.4% and 8.2% in monolingual and multilingual settings; exceeds GPT-3.5 few-shot by 24.3%

## Executive Summary
This paper introduces GIMC, a heterogeneous graph interaction model with multi-granularity contrastive transfer learning for zero-shot cross-lingual document-level event causality identification. The method aims to transfer causal knowledge from source to low-resource languages by capturing long-distance dependencies between events across documents and aligning causal representations across languages. Experiments demonstrate significant improvements over previous models and strong few-shot performance relative to GPT-3.5.

## Method Summary
GIMC employs a heterogeneous graph interaction network to model long-distance dependencies between events scattered across documents, combined with a multi-granularity contrastive learning module to align causal representations across languages. The approach enables effective zero-shot transfer from source to low-resource languages by leveraging both structural dependencies and contrastive alignment mechanisms.

## Key Results
- GIMC outperforms previous models by 9.4% in monolingual and 8.2% in multilingual settings
- GIMC exceeds GPT-3.5 with few-shot learning by 24.3% in overall performance
- Strong zero-shot cross-lingual performance demonstrated on multilingual dataset

## Why This Works (Mechanism)
GIMC works by constructing heterogeneous graphs that capture long-distance event dependencies across documents, allowing the model to reason about causality that spans multiple sentences. The multi-granularity contrastive learning module aligns causal representations across languages at different abstraction levels, enabling effective knowledge transfer without requiring parallel corpora.

## Foundational Learning
- **Heterogeneous graph networks**: Why needed - to model complex relationships between different types of nodes (events, contexts); Quick check - verify graph construction captures relevant dependencies
- **Contrastive learning**: Why needed - to align representations across languages without parallel data; Quick check - measure alignment quality across language pairs
- **Zero-shot transfer**: Why needed - to enable causality identification in low-resource languages; Quick check - test performance on truly unseen languages
- **Document-level event causality**: Why needed - to capture causal relationships that span multiple sentences; Quick check - verify model handles long-distance dependencies

## Architecture Onboarding
**Component map**: Document events -> Heterogeneous Graph Construction -> Graph Interaction Network -> Event Representation -> Multi-granularity Contrastive Learning -> Cross-lingual Alignment
**Critical path**: Event extraction → Graph construction → Interaction modeling → Contrastive alignment → Causality prediction
**Design tradeoffs**: Complex heterogeneous graphs vs simpler structures; comprehensive contrastive objectives vs efficiency
**Failure signatures**: Poor performance on long documents suggests graph construction issues; degraded cross-lingual performance indicates contrastive learning misalignment
**First experiments**: 1) Ablation study removing graph component, 2) Test contrastive learning with single granularity, 3) Evaluate on additional language pairs

## Open Questions the Paper Calls Out
None provided

## Limitations
- Heterogeneous graph structure effectiveness not validated against simpler alternatives
- GPT-3.5 comparison lacks detailed methodology and hyperparameter transparency
- Dataset composition and annotation biases not fully disclosed, limiting generalizability

## Confidence
- GIMC architecture effectiveness: Medium confidence
- Graph-based modeling superiority: Low confidence
- Cross-lingual zero-shot capability: Medium confidence

## Next Checks
1. Conduct ablation studies removing the heterogeneous graph component versus using a simple sequential model to quantify the specific contribution of the graph structure
2. Replicate the few-shot comparison with GPT-3.5 under identical training data conditions and hyperparameter tuning to verify the claimed performance gap
3. Evaluate GIMC on additional language pairs and datasets (e.g., from different domains) to assess robustness and generalization beyond the current multilingual corpus