---
ver: rpa2
title: Towards Adversarial Robustness of Model-Level Mixture-of-Experts Architectures
  for Semantic Segmentation
arxiv_id: '2412.11608'
source_url: https://arxiv.org/abs/2412.11608
tags:
- adversarial
- attacks
- expert
- experts
- gate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the adversarial robustness of model-level
  mixture-of-experts (MoE) architectures for semantic segmentation in urban and highway
  traffic scenes. The authors evaluate four MoE variants against per-instance and
  universal white-box attacks (FGSM, PGD-10) and transfer attacks.
---

# Towards Adversarial Robustness of Model-Level Mixture-of-Experts Architectures for Semantic Segmentation

## Quick Facts
- arXiv ID: 2412.11608
- Source URL: https://arxiv.org/abs/2412.11608
- Reference count: 40
- Primary result: MoEs with classwise gates and conv layers achieve up to 70.61% mIoU under PGD attacks vs 8.27% for baselines

## Executive Summary
This paper investigates the adversarial robustness of model-level mixture-of-experts (MoE) architectures for semantic segmentation in urban and highway traffic scenes. The authors evaluate four MoE variants against per-instance and universal white-box attacks (FGSM, PGD-10) and transfer attacks. Their experiments on DeepLabv3+- and FRRN-based models show that MoEs, especially those with classwise gates and additional convolutional layers, exhibit greater robustness than individual experts or ensembles.

## Method Summary
The study evaluates model-level MoE architectures using two expert base models (FRRN-A and DeepLabv3+ with ResNet-101) trained on A2D2 dataset subsets. Four MoE variants are tested: simple gate, simple gate with conv layer, classwise gate, and classwise gate with conv layer. Models are trained for 100 epochs with SGD and polynomial learning rate decay. Adversarial attacks (FGSM and PGD-10 with 10 iterations) are applied at epsilon=0.05, with transfer attack experiments conducted by applying adversarial noise from one model to others.

## Key Results
- MoEs with classwise gates and conv layers achieve 70.61% mIoU under PGD attacks vs 8.27% for baselines
- Transfer attacks show MoEs are more robust than individual experts, with adversarial noise from MoEs causing smaller accuracy drops on target models
- Classwise gate MoEs outperform simple gate MoEs in both architectures, particularly under universal attacks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Classwise gates improve robustness by distributing decision-making across experts per class
- Mechanism: Instead of one weight per expert, the gate predicts a weight for each class-expert pair, allowing the model to route different classes to different experts, thus diluting the effect of adversarial perturbations on any single expert's output
- Core assumption: Adversarial noise affects all experts similarly; classwise routing can exploit heterogeneous expert strengths
- Evidence anchors: [abstract] "MoEs, especially those with classwise gates and additional convolutional layers, exhibit greater robustness than individual experts or ensembles"
- Break condition: If adversarial perturbations affect all experts uniformly or if classwise routing cannot exploit real differences in expert performance

### Mechanism 2
- Claim: Additional convolutional layers after expert combination provide non-linear mixing that mitigates adversarial transfer
- Mechanism: The extra conv layer applies learned spatial filtering to the weighted sum of expert outputs before final classification, potentially smoothing or transforming adversarial perturbations before they can fully influence predictions
- Core assumption: Learned convolutional filters can partially undo or reduce adversarial noise introduced in earlier layers
- Evidence anchors: [abstract] "MoEs with classwise gates and extra conv layers achieve up to 70.61% mIoU under PGD attacks... compared to 8.27% for baseline models"
- Break condition: If the adversarial perturbation is too strong or if the conv layer cannot learn effective denoising under attack

### Mechanism 3
- Claim: Model-level MoEs outperform ensembles because the learnable gate can dynamically adjust expert contributions per input, unlike fixed averaging/voting
- Mechanism: The gating network learns to assign higher weights to experts that are more robust or accurate for a given input, effectively creating a context-aware ensemble that adapts to input difficulty and domain
- Core assumption: Different inputs benefit from different expert combinations; the gate can learn this mapping
- Evidence anchors: [abstract] "MoEs have been shown to outperform ensembles on specific tasks, yet their susceptibility to adversarial attacks has not been studied yet"
- Break condition: If the gate cannot learn robust weighting under attack or if expert outputs are too correlated to allow effective routing

## Foundational Learning

- Concept: Adversarial attacks (FGSM, PGD)
  - Why needed here: The paper evaluates robustness under these specific attacks; understanding their mechanics is essential to interpret results
  - Quick check question: What is the difference between FGSM and PGD attacks in terms of perturbation generation?

- Concept: Mixture of Experts (MoE) architecture
  - Why needed here: The paper's core contribution is analyzing MoE robustness; knowing how gating and expert combination work is fundamental
  - Quick check question: How does a classwise gate differ from a simple gate in an MoE?

- Concept: Semantic segmentation metrics (mIoU)
  - Why needed here: The main evaluation metric is mIoU; understanding its meaning and sensitivity to attacks is critical for interpreting results
  - Quick check question: What does a drop from 44.26% to 13.01% mIoU under PGD attack indicate about model robustness?

## Architecture Onboarding

- Component map: Input image -> Expert 1 feature extraction -> Expert 2 feature extraction -> Concatenate features -> Gate (simple/classwise) -> Expert weights -> Weighted sum of expert outputs -> (Optional) Conv layer -> Final softmax

- Critical path:
  1. Input image → feature extraction by each expert
  2. Concatenate expert features → gate predicts weights
  3. Apply weights to expert outputs → sum
  4. (Optional) Conv layer → final softmax

- Design tradeoffs:
  - Simple gate vs classwise gate: Simpler is faster but less robust; classwise is more robust but adds parameters and computation
  - With vs without conv layer: Without is faster; with improves robustness at cost of extra parameters and potential overfitting

- Failure signatures:
  - High mIoU drop under attack → gating or conv layer not effective
  - Low mIoU even without attack → experts or gate poorly trained
  - Overfitting → conv layer memorizes training data instead of learning robust features

- First 3 experiments:
  1. Baseline: Train individual experts and simple MoE without conv layer; evaluate under FGSM
  2. Classwise gate: Replace simple gate with classwise gate; compare FGSM robustness
  3. Conv layer: Add conv layer to best-performing MoE; evaluate under PGD-10

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do MoE architectures with sparse expert selection at the layer level compare in adversarial robustness to model-level MoEs?
- Basis in paper: [explicit] The paper notes that adversarial robustness of sparse MoEs at the layer level has been addressed recently, but model-level MoEs have received no attention
- Why unresolved: No direct experimental comparison between layer-level and model-level MoE robustness is provided
- What evidence would resolve it: Systematic experiments comparing both MoE types under identical attack conditions and metrics

### Open Question 2
- Question: Does increasing the number of experts in a model-level MoE further improve adversarial robustness beyond the two-expert setup studied?
- Basis in paper: [inferred] The paper only evaluates two-expert MoEs, leaving scalability to more experts unexplored
- Why unresolved: Experiments are limited to two experts; no analysis of trade-offs or robustness gains with more experts
- What evidence would resolve it: Experiments varying the number of experts and measuring robustness against per-instance and transfer attacks

### Open Question 3
- Question: How does the diversity of expert sub-models affect the adversarial robustness of MoEs in semantic segmentation?
- Basis in paper: [explicit] The paper references prior work showing diversity correlates with robustness in ensembles, but does not quantify this for MoEs
- Why unresolved: No diversity metrics or analysis of expert specialization impact on robustness are provided
- What evidence would resolve it: Correlation studies between diversity measures (e.g., cosine similarity, double fault) and robustness metrics across MoE variants

## Limitations
- Robustness improvements depend heavily on specific adversarial attack implementation (FGSM/PGD with epsilon=0.05)
- Classwise gate benefits are demonstrated only on A2D2 dataset; performance on other segmentation datasets is unverified
- The conv layer's denoising effect is inferred from mIoU differences but lacks ablation studies

## Confidence

- **High Confidence**: MoEs with classwise gates and conv layers show consistent mIoU improvements under PGD attacks across both DeepLabv3+ and FRRN architectures
- **Medium Confidence**: Transfer attack resistance claims are supported by experiments but limited to single-source transfer scenarios
- **Low Confidence**: Mechanism explanations for why classwise gates improve robustness are theoretical rather than empirically validated

## Next Checks

1. **Cross-dataset validation**: Test the same MoE architectures on Cityscapes or BDD100K to verify robustness claims generalize beyond A2D2
2. **Adaptive attack testing**: Evaluate against more sophisticated attacks (e.g., PGD with momentum, C&W) to stress-test the claimed robustness
3. **Gate weight analysis**: Instrument the classwise gate to analyze expert weight distributions under attack vs clean inputs to validate the routing mechanism hypothesis