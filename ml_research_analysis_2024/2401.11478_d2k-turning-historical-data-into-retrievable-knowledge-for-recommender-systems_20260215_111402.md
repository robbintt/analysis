---
ver: rpa2
title: 'D2K: Turning Historical Data into Retrievable Knowledge for Recommender Systems'
arxiv_id: '2401.11478'
source_url: https://arxiv.org/abs/2401.11478
tags: []
core_contribution: This paper proposes a framework called D2K (Data to Knowledge)
  that turns massive user behavior data into retrievable knowledge for recommender
  systems. The core idea is to transform historical data into a knowledge base indexed
  by user-item-context cross features.
---

# D2K: Turning Historical Data into Retrievable Knowledge for Recommender Systems

## Quick Facts
- **arXiv ID**: 2401.11478
- **Source URL**: https://arxiv.org/abs/2401.11478
- **Reference count**: 38
- **Primary result**: Achieves 1.35%-2.90% AUC improvements over baseline models by transforming historical data into retrievable ternary knowledge

## Executive Summary
This paper proposes D2K (Data to Knowledge), a framework that transforms massive user behavior data into retrievable knowledge for recommender systems. Unlike traditional parameter-based knowledge storage that suffers from catastrophic forgetting, D2K explicitly stores ternary knowledge (user-item-context triplets) in a key-value knowledge base. The framework uses a Transformer-based encoder to convert these cross features into knowledge vectors, which can be retrieved and adapted to target samples through a personalized knowledge adaptation unit. Extensive experiments on two public datasets demonstrate significant performance improvements over existing baselines.

## Method Summary
D2K addresses the challenge of preserving historical user behavior knowledge while handling continuous data arrival without catastrophic forgetting. The method consists of a knowledge generation phase where a Transformer encoder converts historical user-item-context feature triplets into knowledge vectors stored in a key-value knowledge base, and a knowledge utilization phase where target samples retrieve relevant knowledge and adapt it through a personalized knowledge adaptation unit. The framework is tested on two public datasets (AD from Taobao and Eleme from Eleme food delivery platform) with backbone models DeepFM, DIN, and DCNv2, evaluating AUC and log loss for click-through rate prediction.

## Key Results
- D2K achieves 1.35%-2.90% AUC improvements over baseline models (DeepFM, DIN, DCNv2) on two public datasets
- Ternary knowledge storage outperforms unary knowledge storage (user-only or item-only) in capturing richer interaction signals
- The personalized knowledge adaptation unit significantly improves performance by adapting global knowledge to target samples
- D2K demonstrates effectiveness in both offline evaluation and online A/B testing scenarios

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Ternary knowledge captures richer interaction signals than unary knowledge.
- **Mechanism**: The model encodes user-item-context triplets as keys, preserving complete interaction context rather than partial (user-only or item-only) signals.
- **Core assumption**: User feedback is determined by the joint influence of user attributes, item attributes, and contextual features.
- **Evidence anchors**:
  - [abstract] "Different from only storing unary knowledge such as the user-side or item-side information, D2K proposes to store ternary knowledge for recommendation, which is determined by the complete recommendation factors‚Äîuser, item, and context."
  - [section] "We believe that the user feedback, positive or negative, is affected by three types of information together, i.e., user information, item information, and context information."
- **Break condition**: If the interaction is truly independent of one factor (e.g., context doesn't matter), the ternary encoding adds unnecessary noise and parameter cost.

### Mechanism 2
- **Claim**: Transformer encoder maps cross features into a linearly separable space for direct classification.
- **Mechanism**: The knowledge encoder transforms ternary cross features through a Transformer and MLP to produce knowledge vectors that can be linearly classified.
- **Core assumption**: Complex non-linear relationships can be projected into a space where linear classification suffices.
- **Evidence anchors**:
  - [section] "The Transformer and MLP layers map the knowledge into a linearly separable space."
  - [section] "The Transformer is chosen as the major component because of its superior modeling capacity and flexibility w.r.t. the input length."
- **Break condition**: If the relationship is inherently non-linear beyond what the Transformer+MLP mapping can capture, linear separability fails.

### Mechanism 3
- **Claim**: Personalized knowledge adaptation unit generates sample-specific MLP parameters to adapt global knowledge.
- **Mechanism**: For each target sample, its feature embeddings are projected to produce MLP weights and biases that transform the retrieved global knowledge into a personalized representation.
- **Core assumption**: The same knowledge has different relevance weights for different users/items/contexts.
- **Evidence anchors**:
  - [section] "The main idea of the personalized knowledge adaptation unit is to use the target sample ùë• as MLP parameters."
  - [section] "The knowledge vector is fed forward through the ùë•-personalized MLP and gets the personalized knowledge representation w.r.t ùë•."
- **Break condition**: If the target sample's embedding projection doesn't capture relevant variation, the adaptation becomes ineffective.

## Foundational Learning

- **Concept**: Transformer architecture
  - **Why needed here**: Provides flexible modeling of variable-length feature sets and captures complex feature interactions for knowledge encoding.
  - **Quick check question**: Why is a Transformer preferred over a simple feed-forward network for encoding ternary cross features?

- **Concept**: Catastrophic forgetting in continual learning
  - **Why needed here**: Explains why parameter-centric approaches fail to preserve old knowledge when new data arrives.
  - **Quick check question**: What is the fundamental limitation of preserving knowledge only in model parameters?

- **Concept**: Knowledge base vs. model parameters
  - **Why needed here**: Distinguishes between implicit (parameter-based) and explicit (key-value) knowledge storage.
  - **Quick check question**: What is the key advantage of using a key-value knowledge base over storing everything in model parameters?

## Architecture Onboarding

- **Component map**: Knowledge Generation: Transformer encoder ‚Üí Knowledge vectors ‚Üí Knowledge base; Knowledge Utilization: Query generation ‚Üí Knowledge lookup ‚Üí Personalized adaptation ‚Üí Knowledge injection; Core RS Model: Standard recommendation model (DeepFM/DIN/DCNv2) with optional separate tower

- **Critical path**: Query generation ‚Üí Knowledge lookup ‚Üí Personalized adaptation ‚Üí Knowledge injection ‚Üí RS model prediction

- **Design tradeoffs**:
  - Ternary vs. unary knowledge: More information vs. larger knowledge base
  - Separate vs. shared embedding for adaptation: Performance vs. memory efficiency
  - Concat vs. separate tower injection: Simplicity vs. modularity

- **Failure signatures**:
  - Empty knowledge vectors: Query has no matching key in knowledge base
  - Poor performance: Knowledge vectors not linearly separable despite mapping
  - Memory overflow: Knowledge base too large due to many unique ternary features

- **First 3 experiments**:
  1. Test knowledge lookup with synthetic data to verify retrieval correctness
  2. Compare AUC with/without personalized adaptation unit
  3. Measure knowledge base size vs. performance trade-off by varying feature selection

## Open Questions the Paper Calls Out
- How does the performance of D2K change when using fuzzy search instead of exact matching for ternary features?
- What is the optimal update strategy for the knowledge base when dealing with very long time periods between updates?
- How does the performance of D2K scale with extremely large knowledge bases containing millions of unique ternary features?

## Limitations
- The ternary feature space grows combinatorially with the number of feature fields, potentially causing memory explosion in large-scale systems
- The paper assumes linear separability after Transformer+MLP mapping without providing empirical verification of this property
- Knowledge update mechanisms are mentioned but not thoroughly evaluated - how frequently should knowledge be updated and what's the optimal strategy?

## Confidence
- Ternary knowledge capturing richer signals than unary: High confidence
- Transformer-based knowledge encoding effectiveness: Medium confidence
- Personalized adaptation unit improving performance: High confidence
- D2K outperforming existing methods: High confidence

## Next Checks
1. Systematically measure knowledge base size growth with different numbers of feature fields and evaluate the trade-off between memory usage and performance gains.
2. Compare different knowledge update strategies (recent priority vs average pooling) over time to determine optimal update frequency and method.
3. Conduct empirical tests to verify whether the Transformer+MLP mapping actually produces linearly separable representations, such as visualizing decision boundaries or testing with linear classifiers.