---
ver: rpa2
title: Sampling-based Distributed Training with Message Passing Neural Network
arxiv_id: '2402.15106'
source_url: https://arxiv.org/abs/2402.15106
tags:
- training
- graph
- nodes
- dataset
- s-mpnn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of scaling edge-based graph neural
  networks for large computational domains with up to O(10^5) nodes. The authors introduce
  DS-MPNN, a domain-decomposition-based distributed training approach that partitions
  the computational graph across multiple GPUs while maintaining inter-GPU communication
  of node and edge attributes through overlapping regions.
---

# Sampling-based Distributed Training with Message Passing Neural Network

## Quick Facts
- arXiv ID: 2402.15106
- Source URL: https://arxiv.org/abs/2402.15106
- Reference count: 22
- Primary result: Distributed MPNN training with domain decomposition and Nyström sampling scales to O(10^5) nodes while maintaining accuracy comparable to single-GPU implementations

## Executive Summary
This work addresses the challenge of scaling edge-based graph neural networks for large computational domains with up to O(10^5) nodes. The authors introduce DS-MPNN, a domain-decomposition-based distributed training approach that partitions the computational graph across multiple GPUs while maintaining inter-GPU communication of node and edge attributes through overlapping regions. The method combines traditional domain decomposition with Nyström approximation sampling techniques to enable efficient training and inference. Experiments on Darcy flow, AirfRANS (both low and high fidelity), and 3D step flow datasets show that DS-MPNN achieves comparable accuracy to single-GPU implementations (RMSE values around 10^-5 to 10^-2 depending on the dataset) while significantly reducing training and inference times. The method also outperforms node-based GCN approaches and demonstrates scalability with increasing node counts, enabling training on problems that would otherwise exceed single GPU memory limits.

## Method Summary
DS-MPNN partitions the computational domain into overlapping subdomains, each assigned to a separate GPU. The approach uses edge-conditioned convolution with Nyström approximation sampling to reduce memory requirements while maintaining accuracy. During message-passing, latent node attributes are communicated between GPUs through overlap regions to ensure complete kernel construction at subdomain boundaries. The method employs synchronous gradient aggregation across all GPUs, with each GPU computing gradients for its local subdomain before the aggregated gradient is used to update parameters across all GPUs simultaneously.

## Key Results
- Achieves RMSE values of 1.19×10⁻⁵ (Darcy flow), 1.02×10⁻² (low fidelity AirfRANS), 9.63×10⁻³ (high fidelity AirfRANS), and 2.42×10⁻³ (3D step flow) comparable to single-GPU implementation
- Scales to O(10^5) nodes while maintaining accuracy, with training and inference times significantly reduced compared to single-GPU approaches
- Outperforms node-based GCN approaches by substantial margins (e.g., 41% better RMSE on AirfRANS low fidelity)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distributed domain decomposition with overlapping regions enables MPNN training on large graphs without accuracy loss.
- Mechanism: The computational domain is partitioned into subdomains, each assigned to a GPU. Overlapping regions of length l ensure complete kernel construction at subdomain boundaries, preventing discontinuities. Latent space node attributes are communicated between GPUs through these overlaps during message-passing steps.
- Core assumption: The overlap length l ≥ kernel radius r ensures complete message-passing between boundary nodes.
- Evidence anchors:
  - [section] "This extended overlap is pivotal for ensuring comprehensive kernel construction at the interior edges of the domain, thereby circumventing the issues of incomplete kernel formation that can result in discontinuities in the predicted solutions across the subdomain boundaries."
  - [abstract] "Our distributed training approach, coupled with Nyström-approximation sampling techniques, we present a scalable graph neural network, referred to as DS-MPNN (D and S standing for distributed and sampled, respectively), capable of scaling up to O(10^5) nodes."

### Mechanism 2
- Claim: Edge-conditioned convolution with Nyström approximation sampling enables memory-efficient training on large graphs.
- Mechanism: Edge-conditioned convolution computes node attributes by summing weighted contributions from neighboring nodes based on edge attributes. Nyström approximation randomly samples nodes and edges, creating sparse subgraphs that approximate the full graph while reducing memory requirements.
- Core assumption: Randomly sampled subgraphs preserve the essential structural information needed for accurate PDE modeling.
- Evidence anchors:
  - [section] "The current model, adapted from Bonnet et al. (2022b), is composed of an encoder (Ne), decoder (Nd) and message-passing network (Kϕ). This encoder transforms the initial node attributes, vl=0_i, where l = 0 indicates the initial or lower-level state of these attributes, into a latent representation."
  - [abstract] "Through our distributed training approach, coupled with Nyström-approximation sampling techniques, we present a scalable graph neural network, referred to as DS-MPNN (D and S standing for distributed and sampled, respectively), capable of scaling up to O(10^5) nodes."

### Mechanism 3
- Claim: Synchronous gradient aggregation across GPUs maintains training convergence equivalent to single-GPU implementation.
- Mechanism: After message-passing steps, each GPU computes gradients for its subdomain. Gradients are aggregated across all GPUs, then parameters are updated synchronously using the aggregated gradient.
- Core assumption: Synchronous gradient updates with proper overlap communication maintain the same optimization dynamics as single-GPU training.
- Evidence anchors:
  - [section] "The computational graph accumulates gradients during the h radius hops that are computed in relation to the total loss function L across the entire computational domain. This computation encompasses a summation of the domains' interior points across all GPUs. Following this, an aggregation of the gradients from each GPU is performed, leading to a synchronous update of the neural network parameters across all the GPUs - , utilizing the aggregated gradient."
  - [abstract] "The DS-MPNN model demonstrates comparable accuracy to single-GPU implementation, can accommodate a significantly larger number of nodes compared to the single-GPU variant (S-MPNN), and significantly outperforms the node-based GCN."

## Foundational Learning

- Concept: Message-passing neural networks (MPNNs)
  - Why needed here: MPNNs are the core architecture being distributed. Understanding their message-passing mechanism is crucial for implementing the distributed version.
  - Quick check question: How does edge-conditioned convolution differ from standard graph convolution?

- Concept: Domain decomposition and parallel computing
  - Why needed here: The distributed training approach relies on partitioning the computational domain across multiple GPUs, requiring knowledge of domain decomposition techniques.
  - Quick check question: Why is overlap between subdomains necessary for maintaining accuracy?

- Concept: Graph neural networks and message-passing
  - Why needed here: Understanding how information flows through graph structures via message-passing is essential for implementing and debugging the distributed algorithm.
  - Quick check question: What information needs to be communicated between GPUs during distributed training?

## Architecture Onboarding

- Component map:
  - Encoder (Ne) -> Message-passing network (Kϕ) -> Decoder (Nd) -> Graph kernel construction -> Overlap communication module

- Critical path:
  1. Domain partitioning and overlap region identification
  2. Graph kernel construction on each GPU
  3. Message-passing with inter-GPU communication
  4. Gradient computation and aggregation
  5. Synchronous parameter update

- Design tradeoffs:
  - Overlap length vs. accuracy: Larger overlaps improve accuracy but increase communication overhead
  - Sampling rate vs. memory: Higher sampling rates improve accuracy but require more memory
  - Number of GPUs vs. communication overhead: More GPUs reduce memory per GPU but increase communication complexity

- Failure signatures:
  - Accuracy degradation at subdomain boundaries: Indicates insufficient overlap length
  - Memory overflow on individual GPUs: Indicates need for more GPUs or lower sampling rate
  - Training divergence: Indicates communication issues or improper gradient aggregation

- First 3 experiments:
  1. Run single-GPU implementation (S-MPNN) on small dataset to establish baseline accuracy
  2. Implement 2-GPU distributed version (DS-MPNN2) on same dataset to verify accuracy preservation
  3. Scale to larger dataset that exceeds single-GPU memory to demonstrate scalability advantage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical upper bound on the number of nodes that can be processed by DS-MPNN before communication overhead becomes prohibitive?
- Basis in paper: [inferred] The paper demonstrates scalability up to O(10^5) nodes but doesn't provide theoretical limits or scalability analysis beyond the tested range.
- Why unresolved: The paper focuses on empirical validation but doesn't derive theoretical bounds for communication complexity or GPU memory scaling.
- What evidence would resolve it: Theoretical analysis of communication complexity versus GPU memory constraints, or empirical scaling tests beyond O(10^5) nodes.

### Open Question 2
- Question: How does the choice of domain decomposition strategy affect the accuracy and efficiency of DS-MPNN compared to simple coordinate-based partitioning?
- Basis in paper: [explicit] The paper mentions that future work should involve more sophisticated partitioning methods like METIS but doesn't explore these alternatives.
- Why unresolved: The paper uses simple coordinate-based partitioning without comparing to more advanced graph partitioning algorithms.
- What evidence would resolve it: Comparative studies using different partitioning strategies (METIS, spectral partitioning, etc.) measuring both accuracy and communication overhead.

### Open Question 3
- Question: What is the impact of overlap region length on convergence rate and final accuracy across different PDE types?
- Basis in paper: [explicit] The paper shows that overlap length affects accuracy for AirfRANS but doesn't systematically study this relationship across different PDEs or examine convergence rate effects.
- Why unresolved: The paper only tests a few overlap lengths for specific datasets without exploring the general relationship between overlap size, convergence, and problem characteristics.
- What evidence would resolve it: Systematic ablation studies varying overlap length across multiple PDE types while measuring both convergence speed and final accuracy.

## Limitations

- The distributed training approach assumes uniform communication overhead across GPUs, which may not hold in heterogeneous computing environments
- The Nyström sampling technique introduces approximation error that is not quantified relative to theoretical error bounds
- Performance gains are demonstrated primarily on structured mesh datasets, with limited validation on highly irregular unstructured graphs common in real-world applications

## Confidence

- **Accuracy Preservation (High)**: The method demonstrates RMSE values comparable to single-GPU implementations across all tested datasets, with specific values (e.g., 1.19×10⁻⁵ for Darcy flow) providing quantitative validation.
- **Scalability Claims (Medium)**: While the method scales to O(10⁵) nodes, the exact relationship between node count, GPU count, and communication overhead is not fully characterized across diverse hardware configurations.
- **Nyström Sampling Effectiveness (Medium)**: The sampling approach enables memory efficiency, but the sensitivity of accuracy to sampling parameters is not thoroughly explored.

## Next Checks

1. **Communication Overhead Analysis**: Measure the ratio of communication time to computation time as a function of GPU count and overlap length to characterize scalability limits.
2. **Sampling Parameter Sensitivity**: Systematically vary node and edge sampling rates to establish the accuracy-memory tradeoff curve and identify optimal sampling strategies.
3. **Generalization to Irregular Graphs**: Test the method on highly irregular, real-world graph datasets (e.g., social networks, biological networks) to validate robustness beyond structured mesh applications.