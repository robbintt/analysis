---
ver: rpa2
title: Improving Multi-Center Generalizability of GAN-Based Fat Suppression using
  Federated Learning
arxiv_id: '2404.07374'
source_url: https://arxiv.org/abs/2404.07374
tags:
- data
- learning
- federated
- synthesis
- mris
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of poor generalizability of GANs
  trained on single-site data for synthesizing fat-suppressed (FS) MRI sequences from
  non-FS proton density (PD) sequences. The core method idea is to use federated learning
  (FL) to train a GAN model across multiple institutions without sharing patient data.
---

# Improving Multi-Center Generalizability of GAN-Based Fat Suppression using Federated Learning

## Quick Facts
- arXiv ID: 2404.07374
- Source URL: https://arxiv.org/abs/2404.07374
- Reference count: 4
- Primary result: Federated learning models achieved significantly higher SSIM scores (0.63 for UMB dataset, 0.58 for FastMRI dataset) compared to single-site models when tested on external data

## Executive Summary
This paper addresses the challenge of poor generalizability of GANs trained on single-site data for synthesizing fat-suppressed MRI sequences from non-FS proton density sequences. The authors propose using federated learning to train a GAN model across multiple institutions without sharing patient data. Their results demonstrate that FL models significantly outperform single-site models on external datasets, achieving SSIM scores of 0.63 for UMB dataset and 0.58 for FastMRI dataset. This work represents an important step toward making synthetic MRIs clinically viable while preserving patient privacy.

## Method Summary
The study uses pix2pix GAN architecture with U-Net generator and 3-layer 70x70 PatchGAN discriminator, trained at 256x256 resolution for 200 epochs. Four models were trained: single-site UMB, single-site FastMRI, centrally aggregated, and 2-client FL using FedGAN aggregation strategy. Two datasets were used: UMB (n=151 studies) and FastMRI (n=7,171 studies), with sequence pairs registered using ANTsPy and normalized to 0-1 range. The federated learning approach enables training across institutions without sharing patient data by exchanging only model weights.

## Key Results
- FL models achieved significantly higher SSIM scores (0.63 for UMB dataset, 0.58 for FastMRI dataset) compared to single-site models on external data
- Single-site models showed poor generalizability to external data despite higher performance on local data
- All models exhibited relatively low SSIM scores (0.46-0.64) indicating fundamental limitations in the task
- FedGAN aggregation strategy effectively combined knowledge from heterogeneous datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Federated learning enables GAN training across institutions without sharing patient data, preserving privacy while improving model generalizability.
- Mechanism: Each institution trains a local GAN model on its data, then shares only model weights with a central server for aggregation. The central server averages weights using FedGAN strategy, then sends back updated weights to all institutions.
- Core assumption: Local models learn complementary features that, when aggregated, create a more robust global model that performs better on external data.
- Evidence anchors:
  - [abstract] "Federated Learning (FL) is a promising paradigm to facilitate multi-center collaborations to collectively train a global model without sharing patient data"
  - [section] "At the end of each epoch, client weights are communicated to the central server, aggregated using FedGAN (Rasouli et al., 2020), and communicated back to the clients"
  - [corpus] Weak evidence - corpus contains related GAN papers but none specifically on federated learning for medical imaging

### Mechanism 2
- Claim: Single-site GANs trained on homogeneous data fail to generalize to external data due to domain shift between different imaging protocols and equipment.
- Mechanism: When GANs are trained on data from a single institution, they learn institution-specific patterns (scanner type, imaging parameters) that don't transfer well to data from other institutions with different protocols.
- Core assumption: The domain shift between UMB and FastMRI datasets is significant enough that single-site models cannot bridge the gap.
- Evidence anchors:
  - [abstract] "GANs trained on single-site data have poor generalizability to external data due to domain shift"
  - [section] "Single-site models had poor generalizability to external data despite exhibiting higher performance on local data"
  - [corpus] No direct corpus evidence, but related to general domain adaptation literature

### Mechanism 3
- Claim: FedGAN aggregation strategy effectively combines knowledge from heterogeneous datasets to create a model that performs well across multiple domains.
- Mechanism: The FedGAN algorithm aggregates client model weights in a way that balances contributions from each institution, creating a global model that captures diverse features from all datasets.
- Core assumption: FedGAN's weight averaging approach is robust to heterogeneous data distributions and doesn't simply average out important institution-specific features.
- Evidence anchors:
  - [section] "client weights are communicated to the central server, aggregated using FedGAN (Rasouli et al., 2020)"
  - [section] "FL models exhibited significantly higher performance on external data compared to the single-site models"
  - [corpus] Weak evidence - corpus contains no papers on FedGAN specifically

## Foundational Learning

- Concept: Generative Adversarial Networks (GANs) for image-to-image translation
  - Why needed here: The paper uses pix2pix GAN architecture to translate non-FS PD sequences to FS sequences
  - Quick check question: What are the two main components of a GAN and their roles in the training process?

- Concept: Federated learning fundamentals
  - Why needed here: Understanding how federated learning works is crucial to grasp how the multi-center training is achieved without data sharing
  - Quick check question: What is the key difference between federated learning and traditional centralized training?

- Concept: Domain adaptation and domain shift
  - Why needed here: The paper's core problem is that single-site models don't generalize due to domain differences between institutions
  - Quick check question: How does domain shift between training and test data affect model performance?

## Architecture Onboarding

- Component map: Client GAN training -> Weight communication -> Central server aggregation -> Updated weights distribution -> Client fine-tuning -> Evaluation
- Critical path: Local training → Weight communication → Aggregation → Updated weights → Local fine-tuning → Evaluation
- Design tradeoffs:
  - Communication frequency vs. training efficiency (daily aggregation vs. end-of-training)
  - FedGAN vs. other aggregation strategies (FedAvg, personalized FL)
  - Model complexity vs. computational requirements for each institution
- Failure signatures:
  - Poor SSIM scores across all models (0.46-0.64) indicate fundamental limitations
  - Large performance gap between in-domain and out-of-domain testing
  - Convergence issues during federated training due to heterogeneous data
- First 3 experiments:
  1. Train single-site GAN on UMB data and test on both UMB and FastMRI test sets
  2. Train single-site GAN on FastMRI data and test on both datasets
  3. Implement basic federated training with two clients and compare to centralized training on combined data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal federated learning aggregation strategy for GAN-based MRI synthesis across heterogeneous institutions?
- Basis in paper: [explicit] "We only use the FedGAN strategy for aggregating weights in FL. Recent literature has explored new strategies for FL with GANs (Wang et al., 2023; Dalmaz et al., 2024)."
- Why unresolved: The paper uses only one FL aggregation strategy (FedGAN) and acknowledges that other strategies exist but does not compare their performance.
- What evidence would resolve it: Direct comparison of different FL aggregation strategies (FedGAN, FedNova, FedAvg, etc.) on the same multi-institutional MRI synthesis task with quantitative metrics.

### Open Question 2
- Question: How does model performance scale with increasing number of participating institutions and dataset size?
- Basis in paper: [inferred] The paper uses only 2 clients (UMB and FastMRI) and notes that "GANs were trained on a small subset of both datasets" leading to "sub-optimal performance."
- Why unresolved: The study uses a limited number of institutions and relatively small datasets, making it unclear how performance would improve with more participants and larger datasets.
- What evidence would resolve it: Systematic experiments varying the number of participating institutions and total dataset size, measuring SSIM and other metrics to identify scaling patterns.

### Open Question 3
- Question: What are the clinical implications and diagnostic equivalence of synthetic FS MRIs compared to acquired FS MRIs?
- Basis in paper: [explicit] "In conclusion, our preliminary results suggest that FL can improve the generalizability of GANs for synthesizing FS knee MRIs in the real-world while preserving patient privacy. This represents an exciting step towards synthetic MRIs becoming a clinical reality."
- Why unresolved: The paper focuses on technical performance metrics (SSIM) but does not address whether synthetic images would be diagnostically equivalent to acquired images for clinical use.
- What evidence would resolve it: Reader study with radiologists comparing diagnostic accuracy and confidence when using synthetic versus acquired FS MRIs for detecting specific pathologies.

## Limitations
- Limited dataset size and only 2 participating institutions restrict generalizability of findings
- FedGAN aggregation strategy implementation details are not fully specified
- No clinical validation or radiologist assessment of diagnostic utility
- Domain shift between datasets not quantitatively characterized

## Confidence
Medium confidence due to several limitations including modest SSIM improvements, consistently low absolute scores, and lack of detailed implementation specifics for FedGAN.

## Next Checks
1. Implement quantitative domain adaptation analysis between UMB and FastMRI datasets to measure the extent of distribution shift
2. Conduct ablation studies testing alternative federated aggregation strategies (FedAvg, FedProx) against the proposed FedGAN approach
3. Perform clinical validation with radiologist assessment of synthetic FS image quality and diagnostic utility