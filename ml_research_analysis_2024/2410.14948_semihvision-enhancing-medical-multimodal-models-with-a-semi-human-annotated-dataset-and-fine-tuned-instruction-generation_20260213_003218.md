---
ver: rpa2
title: 'SemiHVision: Enhancing Medical Multimodal Models with a Semi-Human Annotated
  Dataset and Fine-Tuned Instruction Generation'
arxiv_id: '2410.14948'
source_url: https://arxiv.org/abs/2410.14948
tags:
- medical
- image
- dataset
- data
- diagnostic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SemiHVision, a semi-human annotated medical
  dataset that combines human annotations with automated augmentation techniques to
  enhance medical knowledge representation and diagnostic reasoning in multimodal
  large language models (MLLMs). The authors train PMC-Cambrian-AN on this dataset,
  achieving 79.0% accuracy on traditional medical VQA benchmarks, outperforming HuatuoGPT-Vision-34B
  (66.7%) and Claude3-Opus (55.7%).
---

# SemiHVision: Enhancing Medical Multimodal Models with a Semi-Human Annotated Dataset and Fine-Tuned Instruction Generation

## Quick Facts
- arXiv ID: 2410.14948
- Source URL: https://arxiv.org/abs/2410.14948
- Authors: Junda Wang; Yujan Ting; Eric Z. Chen; Hieu Tran; Hong Yu; Weijing Huang; Terrence Chen
- Reference count: 23
- Primary result: PMC-Cambrian-AN achieves 79.0% accuracy on medical VQA benchmarks, outperforming larger models like HuatuoGPT-Vision-34B (66.7%) and Claude3-Opus (55.7%).

## Executive Summary
This paper introduces SemiHVision, a semi-human annotated medical dataset that combines human annotations with automated augmentation techniques to enhance medical knowledge representation and diagnostic reasoning in multimodal large language models (MLLMs). The authors train PMC-Cambrian-AN on this dataset, achieving 79.0% accuracy on traditional medical VQA benchmarks, outperforming HuatuoGPT-Vision-34B (66.7%) and Claude3-Opus (55.7%). They also introduce the JAMA Clinical Challenge benchmark to evaluate diagnostic reasoning, where PMC-Cambrian-AN achieves a GPT-4 score of 1.29, significantly outperforming other models. The study demonstrates that traditional benchmarks inadequately reflect real-world clinical task capabilities, and highlights the importance of human-annotated diagnostic datasets for training medical MLLMs with robust diagnostic reasoning abilities.

## Method Summary
The authors developed a two-stage training approach: pretraining on a filtered PMC dataset (14M samples) using DeepSpeed Stage 2 for 420 hours on 4 H100 GPUs, followed by fine-tuning on SemiHVision (human-annotated and synthetic data) for 90 hours on 8 H100 GPUs. They then applied annealing on human-annotated data with reduced learning rate. SemiHVision combines human annotations from sources like Eurorad and Radiopaedia with synthetic data generated via a multimodal retriever (UniIR) and GPT-4o augmentation. The model uses adapter-based training to reduce computational costs while maintaining performance.

## Key Results
- PMC-Cambrian-AN achieves 79.0% average accuracy on medical VQA benchmarks, surpassing both PMC-Cambrian-8B-Mix (72.2%) and HuatuoGPT-Vision-34B (66.7%)
- On JAMA Clinical Challenge diagnostic reasoning benchmark, PMC-Cambrian-AN achieves GPT-4 score of 1.29, outperforming all compared models
- Traditional benchmarks show strong knowledge recall (78.1% in SLAKE, 76.4% in VQA-RAD) but diagnostic reasoning remains challenging (44.9% on JAMA Clinical Challenge)

## Why This Works (Mechanism)

### Mechanism 1
SemiHVision dataset improves diagnostic reasoning by combining human annotations with GPT-4o augmentation. Human-annotated ROIs provide precise guidance on critical image areas, while GPT-4o generates detailed instruction-based QA pairs and captions, creating a richer learning signal than synthetic-only data. Core assumption: Human annotations highlight diagnostically important features that general models miss, and GPT-4o can reliably augment this without introducing hallucinations.

### Mechanism 2
Multimodal retrieval system enhances image understanding by providing clinical context. UniIR-based retriever fetches relevant medical guidelines and similar cases, which GPT-4o uses to generate more specific, clinically relevant descriptions aligned with medical context. Core assumption: Context from medical guidelines and similar cases improves the specificity and accuracy of generated descriptions beyond what image analysis alone provides.

### Mechanism 3
Annealing on human-annotated data after synthetic pretraining improves diagnostic reasoning performance. Initial training on large-scale synthetic data provides broad coverage, while subsequent fine-tuning on smaller high-quality human-annotated data refines the model's ability to perform fine-grained diagnostic reasoning. Core assumption: High-quality human-annotated data provides more valuable fine-grained diagnostic reasoning signals than additional synthetic data, despite the smaller quantity.

## Foundational Learning

- Concept: Multimodal retrieval and context integration
  - Why needed here: Medical image interpretation requires understanding both local features and broader clinical context, which cannot be captured from images alone
  - Quick check question: How does the UniIR framework determine which guidelines are most relevant to a given medical image?

- Concept: Human-in-the-loop data annotation
  - Why needed here: Expert annotations identify diagnostically critical regions and provide reasoning pathways that synthetic data generation alone cannot replicate
  - Quick check question: What specific ROI information is most valuable for improving diagnostic reasoning in medical MLLMs?

- Concept: Instruction tuning with diagnostic reasoning
  - Why needed here: Standard image-text pretraining is insufficient for complex medical diagnostic tasks that require step-by-step reasoning and evidence-based conclusions
  - Quick check question: How do the three evaluation dimensions (Key Points, Inference, Evidence) capture different aspects of diagnostic reasoning quality?

## Architecture Onboarding

- Component map: Data pipeline → Retriever → GPT-4o generator → Model training (pretrain → finetune → instruction tuning → annealing) → Evaluation pipeline
- Critical path: Human annotations → Multimodal retriever → GPT-4o generation → PMC-Cambrian training → JAMA Clinical Challenge evaluation
- Design tradeoffs: Larger model size vs. computational efficiency, synthetic data quantity vs. human annotation quality, retrieval augmentation vs. hallucination risk
- Failure signatures: Poor performance on diagnostic reasoning despite good knowledge recall, high hallucination rates in generated captions, imbalance in modality representation
- First 3 experiments:
  1. Compare GPT-4o-generated captions with and without retriever augmentation on a small validation set
  2. Test annealing effectiveness by training two models: one with mixed data, one with sequential synthetic-then-human data
  3. Evaluate model performance on knowledge-based vs. inference-based questions to identify reasoning gaps

## Open Questions the Paper Calls Out

### Open Question 1
How can medical MLLMs be designed to better balance knowledge recall and diagnostic reasoning capabilities? The paper demonstrates that current medical MLLMs excel at knowledge recall but struggle with diagnostic reasoning, while general models like Claude3-Opus perform well in inference but lack medical domain knowledge. The optimal architecture and training approach for achieving both capabilities remains unclear.

### Open Question 2
What is the optimal balance between human-annotated and synthetic data for training medical MLLMs? The paper shows that PMC-Cambrian-AN, which incorporates human-annotated diagnostic datasets through annealing, outperforms models trained solely on synthetic data. However, the optimal ratio of human-annotated to synthetic data for different medical specialties and tasks is not established.

### Open Question 3
How can evaluation methodologies be improved to better reflect real-world clinical task performance? The paper identifies that traditional benchmarks focus heavily on knowledge recall (78.1% in SLAKE, 76.4% in VQA-RAD) while real-world diagnostic tasks require more inference (44.9% in JAMA Clinical Challenge). The optimal evaluation framework that balances these aspects is not established.

### Open Question 4
What is the impact of model size on medical MLLMs' diagnostic reasoning capabilities? The paper notes that PMC-Cambrian-AN is constrained to 8 billion parameters, which may limit complex reasoning capabilities. However, the relationship between model size and diagnostic performance across different medical tasks is not fully explored.

### Open Question 5
How can medical MLLMs be adapted to handle the uneven distribution of medical knowledge across different anatomical regions and modalities? The paper identifies limitations in coverage of anatomical regions due to data scarcity and uneven representation across modalities. The optimal approach for addressing this imbalance is not established.

## Limitations

- Hallucination risk in GPT-4o augmentation not quantitatively evaluated
- Benchmark representativeness questioned as traditional VQA benchmarks inadequately reflect real-world clinical tasks
- Human annotation quality variance not assessed with inter-annotator agreement statistics

## Confidence

- **SemiHVision dataset effectiveness**: High confidence - Multiple quantitative improvements demonstrated across benchmarks
- **Diagnostic reasoning enhancement mechanism**: Medium confidence - Plausible mechanism supported by results but lacking ablation studies isolating individual components
- **Generalizability to clinical practice**: Low confidence - Limited evidence beyond controlled benchmark evaluations

## Next Checks

1. **Ablation study on human vs. synthetic data**: Train three models - one on purely synthetic data, one on purely human-annotated data, and one on SemiHVision - to quantify the relative contributions of each data type to diagnostic reasoning performance.

2. **Hallucination rate quantification**: Implement automated hallucination detection using reference checking against medical literature, and measure hallucination frequency in GPT-4o-generated captions with and without retrieval augmentation.

3. **Clinical expert evaluation**: Conduct a blind study with practicing radiologists and clinicians to evaluate model outputs on JAMA Clinical Challenge cases, comparing their assessments with the automated GPT-4o and UMLS-F1 metrics.