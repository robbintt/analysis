---
ver: rpa2
title: Comparing Fairness of Generative Mobility Models
arxiv_id: '2411.04453'
source_url: https://arxiv.org/abs/2411.04453
tags:
- fairness
- gravity
- mobility
- group
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces fairness metrics to evaluate generative mobility
  models, addressing the overlooked issue of equity in model performance across geographic
  regions. The study uses Common Part of Commuters (CPC) to measure utility and reformulates
  demographic parity using KL-divergence to assess fairness across socioeconomically
  disadvantaged and advantaged groups.
---

# Comparing Fairness of Generative Mobility Models

## Quick Facts
- arXiv ID: 2411.04453
- Source URL: https://arxiv.org/abs/2411.04453
- Authors: Daniel Wang; Jack McFarland; Afra Mashhadi; Ekin Ugurel
- Reference count: 13
- Key outcome: Traditional gravity and radiation models produce fairer outcomes, although Deep Gravity achieves higher CPC (0.41)

## Executive Summary
This work introduces fairness metrics to evaluate generative mobility models, addressing the overlooked issue of equity in model performance across geographic regions. The study uses Common Part of Commuters (CPC) to measure utility and reformulates demographic parity using KL-divergence to assess fairness across socioeconomically disadvantaged and advantaged groups. Four models—Gravity, Radiation, Deep Gravity, and Non-linear Gravity—were tested on New York Taxi Dataset using Social Vulnerability Index (SVI) themes as sensitive attributes. Results show Deep Gravity achieves the highest CPC (0.41) but exhibits the worst fairness scores (e.g., 4.87 for socioeconomic theme), while traditional Gravity model achieves the fairest outcomes across all SVI themes. This demonstrates a trade-off between model accuracy and equity, with feature-rich Deep Gravity amplifying existing biases in community representations.

## Method Summary
The study evaluates four generative mobility models (Gravity, Radiation, Deep Gravity, Non-linear Gravity) on New York Taxi Dataset. Fairness is assessed using demographic parity reformulated as KL-divergence between CPC distributions for advantaged and disadvantaged groups based on SVI themes. CPC scores are calculated for all origin-destination pairs, which are then split into groups using SVI quartiles. KL-divergence measures distributional differences in model performance across socioeconomic groups.

## Key Results
- Deep Gravity achieves highest CPC (0.41) but worst fairness scores (up to 8.22 for ethnicity theme)
- Traditional Gravity model performs fairest across all SVI themes, followed by Radiation model
- Feature-rich Deep Gravity amplifies pre-existing biases in community representations through POI embeddings
- Clear trade-off exists between model accuracy and equity in mobility modeling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reformulating demographic parity using KL-divergence on CPC distributions reveals disparities in how generative models encode biases.
- Mechanism: By measuring the difference in probability distributions of CPC scores between advantaged (high SVI) and disadvantaged (low SVI) groups, KL-divergence quantifies the extent to which model errors are unevenly distributed across socioeconomic areas. Lower KL-divergence values indicate more equitable model performance.
- Core assumption: CPC scores follow probability distributions that meaningfully represent model utility for each group, and KL-divergence effectively captures distributional differences relevant to fairness.
- Evidence anchors:
  - [abstract] "fairness is evaluated using demographic parity. By reformulating demographic parity to reflect the difference in CPC distribution between two groups, our analysis reveals disparities in how various models encode biases"
  - [section] "we reformulate the above equation to compare the probability distribution of CPCs for a subset of S (i.e., P (CPC, S)) and S′(i.e., P (CPC, S′))"
  - [corpus] Weak evidence - corpus contains related fairness work but no specific mention of KL-divergence on CPC distributions
- Break condition: If CPC distributions are not representative of actual model performance differences between groups, or if KL-divergence fails to capture meaningful fairness gaps.

### Mechanism 2
- Claim: Feature-rich Deep Gravity model amplifies pre-existing biases in community representations through POI embeddings.
- Mechanism: Deep Gravity incorporates POI data embeddings that may contain historical biases in how different communities are represented in OpenStreetMap. These embeddings then influence the model's predictions, leading to worse fairness outcomes despite higher overall accuracy.
- Core assumption: POI representations in OSM contain systematic biases that reflect and reinforce existing social inequities, and these biases are transferred to the model through the embedding process.
- Evidence anchors:
  - [abstract] "feature-rich Deep Gravity model amplifying pre-existing biases in community representations"
  - [section] "Comparing non-linear gravity and deep gravity we can see that the feature embedding part of the deep gravity model that results in creating better synthetic traces also leads to unwanted biases. We hypothesize that this may be caused by existing biases in the POI representation of the poorer communities in OSM."
  - [corpus] Weak evidence - corpus contains fairness research but no specific mention of POI embeddings amplifying biases
- Break condition: If POI data is equally representative across all communities, or if the embedding mechanism fails to propagate these biases to the final predictions.

### Mechanism 3
- Claim: Traditional gravity and radiation models produce fairer outcomes by being simpler and less sensitive to feature-based biases.
- Mechanism: Simpler models with fewer parameters and less reliance on potentially biased features are less likely to amplify existing inequities in the data. Their predictions are based primarily on distance and population, which may be more uniformly distributed across communities.
- Core assumption: Simpler models are inherently less likely to encode and amplify complex biases present in feature-rich data representations.
- Evidence anchors:
  - [abstract] "traditional gravity and radiation models produce fairer outcomes, although Deep Gravity achieves higher CPC"
  - [section] "the traditional gravity model performs the fairest for all the SVI themes followed by the Radiation model"
  - [corpus] Weak evidence - corpus contains fairness research but no specific mention of simpler models being fairer
- Break condition: If the simpler models' reliance on distance and population actually reflects existing spatial inequities, or if their simplicity prevents them from correcting for known biases in the data.

## Foundational Learning

- Concept: Common Part of Commuters (CPC) metric
  - Why needed here: CPC is the primary utility metric used to compare generated mobility flows against real flows, serving as the basis for both overall model performance and fairness evaluation.
  - Quick check question: How is CPC calculated and what does a value of 0.41 (Deep Gravity's performance) indicate about the match between generated and real flows?

- Concept: Social Vulnerability Index (SVI) and its themes
  - Why needed here: SVI provides the socioeconomic stratification needed to define advantaged and disadvantaged groups for fairness evaluation across multiple dimensions including socioeconomic status, household characteristics, race/ethnicity, and housing/transportation.
  - Quick check question: What are the four SVI themes used in this study and how are they used to categorize origin-destination pairs into S and S' groups?

- Concept: KL-divergence for distributional comparison
  - Why needed here: KL-divergence measures the difference between probability distributions of CPC scores for advantaged vs disadvantaged groups, providing a quantitative fairness metric.
  - Quick check question: Why is KL-divergence zero when two probability distributions are identical, and what does this imply about group fairness?

## Architecture Onboarding

- Component map: NY Taxi Dataset -> Flow extraction -> SVI integration -> Four mobility models -> CPC calculation -> KL-divergence fairness evaluation -> Comparative analysis

- Critical path:
  1. Load and preprocess NY Taxi Dataset
  2. Integrate SVI data at census tract level
  3. Generate synthetic flows using each model
  4. Calculate CPC scores for all origin-destination pairs
  5. Split pairs into advantaged/disadvantaged groups based on SVI quartiles
  6. Compute CPC distributions for each group
  7. Calculate KL-divergence between group distributions
  8. Compare results across models and SVI themes

- Design tradeoffs:
  - Accuracy vs fairness: Deep Gravity achieves highest CPC (0.41) but worst fairness scores (up to 8.22 for ethnicity theme)
  - Model complexity: Simpler models (Gravity, Radiation) are fairer but less accurate
  - Feature selection: POI embeddings improve accuracy but may introduce biases
  - Data granularity: Census tract level provides socioeconomic detail but may introduce noise

- Failure signatures:
  - Unusually high KL-divergence values across all models suggest systemic bias in the underlying data
  - Deep Gravity showing significantly worse fairness despite high CPC indicates feature-based bias amplification
  - Large variance in fairness scores across SVI themes suggests model sensitivity to specific socioeconomic factors
  - CPC values near 0 or 1 across all models may indicate data preprocessing issues

- First 3 experiments:
  1. Baseline comparison: Run all four models without SVI integration to establish baseline CPC scores and verify implementation correctness
  2. Individual SVI theme analysis: Evaluate each SVI theme separately to identify which socioeconomic factors most strongly influence fairness outcomes
  3. Cross-dataset validation: Apply the same framework to a different mobility dataset (e.g., Chicago taxi data) to test generalizability of findings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do other generative mobility models (e.g., neural network-based approaches beyond Deep Gravity) compare in terms of the fairness-accuracy trade-off?
- Basis in paper: [explicit] The study tested four models (Gravity, Radiation, Deep Gravity, Non-linear Gravity) and found that traditional models are fairer but less accurate than Deep Gravity. The paper suggests feature-rich Deep Gravity amplifies existing biases.
- Why unresolved: The analysis was limited to four specific models. Other modern approaches like attention-based models or graph neural networks were not evaluated.
- What evidence would resolve it: Testing a broader range of generative mobility models on the same datasets and fairness metrics would reveal whether the observed trade-off is universal or model-specific.

### Open Question 2
- Question: How does the choice of sensitive attributes (beyond SVI themes) affect the fairness assessment of mobility models?
- Basis in paper: [explicit] The study used SVI themes (socioeconomic status, household characteristics, race/ethnicity, housing/transportation) as sensitive attributes to measure group fairness.
- Why unresolved: SVI themes may not capture all relevant dimensions of disadvantage or bias. Other attributes like income levels, employment status, or access to public services were not considered.
- What evidence would resolve it: Evaluating model fairness using alternative or additional sensitive attributes would show whether results are robust across different definitions of disadvantaged groups.

### Open Question 3
- Question: Can model modifications or training procedures be designed to improve fairness without sacrificing accuracy?
- Basis in paper: [inferred] The study found that Deep Gravity achieves higher CPC but worse fairness scores, suggesting a trade-off between accuracy and equity. The paper highlights the need to integrate fairness metrics but does not explore mitigation strategies.
- Why unresolved: The paper identifies the problem but does not investigate solutions like fairness-aware training, adversarial debiasing, or post-processing techniques.
- What evidence would resolve it: Implementing and testing fairness-enhancing modifications to existing models would demonstrate whether the trade-off can be mitigated or eliminated.

## Limitations

- Analysis limited to New York City taxi data and specific SVI themes, which may not represent all geographic contexts
- Hypothesis about POI embeddings introducing bias relies on qualitative reasoning rather than direct evidence linking OSM data quality to model outcomes
- Assumes CPC distributions adequately capture model utility differences between socioeconomic groups

## Confidence

- High: The empirical observation that Deep Gravity achieves higher CPC but worse fairness scores is well-supported by the data presented.
- Medium: The claim that simpler models produce fairer outcomes is supported but could benefit from additional testing across different datasets and model architectures.
- Low: The hypothesis that POI embedding biases specifically cause fairness degradation in Deep Gravity requires direct evidence and testing.

## Next Checks

1. Cross-dataset validation: Apply the same framework to mobility data from cities with different socioeconomic patterns (e.g., Chicago, San Francisco) to test if the accuracy-fairness trade-off persists across contexts.

2. Ablation study on POI features: Systematically remove or modify POI features in Deep Gravity to isolate their contribution to fairness degradation and test whether cleaning or augmenting POI data improves equity outcomes.

3. Alternative fairness metrics: Validate findings using additional fairness metrics beyond KL-divergence (e.g., Demographic Parity Difference, Equal Opportunity Difference) to ensure the observed patterns are robust to different fairness definitions.