---
ver: rpa2
title: Dual feature-based and example-based explanation methods
arxiv_id: '2401.16294'
source_url: https://arxiv.org/abs/2401.16294
tags:
- explanation
- convex
- points
- dual
- black-box
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel explanation method for black-box machine
  learning models based on dual representation using convex hulls. The key idea is
  to represent instances as convex combinations of extreme points of a convex hull,
  then uniformly generate coefficient vectors from the unit simplex to form a new
  dual dataset.
---

# Dual feature-based and example-based explanation methods

## Quick Facts
- arXiv ID: 2401.16294
- Source URL: https://arxiv.org/abs/2401.16294
- Reference count: 40
- The paper proposes a dual representation method for explaining black-box ML models using convex hulls, showing improved MSE compared to LIME

## Executive Summary
This paper introduces a novel explanation method for black-box machine learning models that leverages dual representation using convex hulls. The key innovation is to represent instances as convex combinations of extreme points from a convex hull, then generate coefficient vectors from the unit simplex to create a new dual dataset. A linear surrogate model is trained on this dual dataset, and feature importance values are computed through simple matrix calculations. This approach addresses key limitations of perturbation-based methods like LIME, including out-of-domain data issues and computational complexity, while also enabling example-based explanations by linking dual coefficients to influential instances from training data.

## Method Summary
The method constructs explanations by first identifying K nearest neighbors of the instance to be explained, then building a convex hull from these neighbors plus the instance itself. Extreme points of this convex hull are extracted, and n uniformly sampled points from the unit simplex are used to generate corresponding primal points through convex combinations. A linear regression model is trained on this dual dataset, and the resulting coefficients are transformed back to the original feature space to provide feature importance values. The same dual representation inherently enables example-based explanations by linking the importance of dual coefficients to specific extreme points (actual training instances).

## Key Results
- The dual representation method demonstrates improved explanation accuracy compared to LIME, with lower mean squared error on synthetic and real datasets
- The approach naturally enables example-based explanations by connecting dual coefficients to influential training instances
- The method successfully avoids out-of-domain data issues that plague perturbation-based methods like LIME

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual representation replaces perturbation-based sampling with uniform sampling in the unit simplex, avoiding out-of-domain data and simplifying hyperparameter tuning.
- Mechanism: By expressing instances as convex combinations of extreme points, the method generates coefficient vectors λ from the unit simplex instead of perturbing feature vectors in Euclidean space. This ensures all generated points lie within the convex hull of the training data, eliminating out-of-domain issues.
- Core assumption: The convex hull of K nearest neighbors adequately captures the local decision boundary around the explainable instance.
- Evidence anchors:
  - [abstract]: "Instead of perturbing new instances in the Euclidean feature space, vectors of convex combination coefficients are uniformly generated from the unit simplex, and they form a new dual dataset."
  - [section]: "The advantage of the smaller dimensionality in the dual representation is questionable... However, this limitation does not impact on the example-based explanation because we actually extend the mixup method and try to find influential instances among nearest neighbors."
  - [corpus]: Weak - corpus papers focus on convex hull applications in geometry and path planning, not explanation methods.

### Mechanism 2
- Claim: The dual representation inherently enables example-based explanations by linking dual coefficients to extreme points (influential instances).
- Mechanism: After training the dual linear surrogate model, normalized coefficients correspond to the importance of each extreme point. These extreme points are actual instances from the training set, so their importance directly translates to example-based explanations.
- Core assumption: The linear surrogate model trained on the dual dataset preserves the relative importance structure of the extreme points.
- Evidence anchors:
  - [abstract]: "The dual representation inherently allows us to get the example-based explanation."
  - [section]: "It turns out that the proposed method for the dual explanation inherently leads to the example-based explanation... we came to the similar example-based explanation by using the dual representation and constructing linear regression surrogate model for new variables (λ1, ..., λd)."
  - [corpus]: Weak - corpus lacks direct evidence for example-based explanation methods.

### Mechanism 3
- Claim: The dual approach reduces dimensionality and computational complexity compared to perturbation-based methods.
- Mechanism: Instead of generating perturbed instances in the full feature space (dimension m), the method works in the dual space with dimension d (number of extreme points, typically ≤ K+1). This reduces the number of parameters to optimize and the computational cost of generating samples.
- Core assumption: The number of extreme points d is significantly smaller than the feature dimension m.
- Evidence anchors:
  - [abstract]: "The dual representation inherently allows us to get the example-based explanation... The neural additive model is also considered as a tool for implementing the example-based explanation approach."
  - [section]: "Another important idea behind the proposed dual representation is to consider the example-based explanation... The dual representation of data can have a smaller dimension than the initial instances."
  - [corpus]: Weak - corpus papers focus on convex hull applications in geometry and path planning, not computational complexity comparisons.

## Foundational Learning

- Concept: Convex hull and extreme points
  - Why needed here: The method relies on representing instances as convex combinations of extreme points of the convex hull. Understanding how to compute and work with convex hulls is fundamental.
  - Quick check question: Given a set of 2D points, can you manually identify the extreme points that form the convex hull?

- Concept: Unit simplex and uniform sampling
  - Why needed here: The method generates coefficient vectors uniformly from the unit simplex. Understanding what the unit simplex is and how to sample from it is crucial.
  - Quick check question: What is the dimensionality of the unit simplex ∆³⁻¹, and how many constraints define it?

- Concept: Linear surrogate models and feature importance
  - Why needed here: The method trains a linear surrogate model on the dual dataset and computes feature importance values. Understanding how linear models work and how to interpret their coefficients is essential.
  - Quick check question: Given a linear model y = ax + b, how do you interpret the coefficient a in terms of feature importance?

## Architecture Onboarding

- Component map:
  Input -> Nearest neighbor finder -> Convex hull constructor -> Extreme point extractor -> Dual dataset generator -> Surrogate model trainer -> Importance calculator -> Output

- Critical path:
  1. Find K nearest neighbors of x₀
  2. Construct convex hull and extract extreme points
  3. Generate dual dataset by sampling from unit simplex
  4. Train linear surrogate model on dual dataset
  5. Compute feature importance values

- Design tradeoffs:
  - K vs. d: Larger K may capture more complex local boundaries but increases d and computational cost
  - n vs. accuracy: More dual samples improve surrogate model accuracy but increase computation time
  - Linear vs. non-linear surrogate: Linear models are interpretable but may not capture complex relationships

- Failure signatures:
  - Poor feature importance values: May indicate that the convex hull doesn't capture the true local boundary or that a linear surrogate is insufficient
  - High computational cost: May indicate that K or n is too large for the available resources
  - Out-of-domain predictions: Should not occur due to the convex hull constraint, but could indicate implementation errors

- First 3 experiments:
  1. Synthetic data with known ground truth: Generate synthetic data where the true feature importance is known, and verify that the method recovers it accurately.
  2. Compare with LIME on simple datasets: Apply both methods to simple 2D datasets and visually compare the explanations.
  3. Vary K and n: Systematically vary the number of neighbors K and dual samples n to understand their impact on accuracy and computational cost.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the dimensionality of the convex hull affect the accuracy of the dual representation method for feature-based explanations?
- Basis in paper: [explicit] The paper mentions that the dual representation can have a smaller dimension than the initial instances, depending on the number of extreme points of the convex hull. However, it also notes that if the number of points is smaller than the data dimensionality, it can restrict the set of generated primal points to some subspace of the initial feature space, which can be a reason for incorrect results.
- Why unresolved: The paper does not provide a clear threshold or guideline for choosing the number of extreme points in the convex hull to balance dimensionality reduction and explanation accuracy.
- What evidence would resolve it: Experimental results showing the relationship between the number of extreme points in the convex hull and the accuracy of the feature importance measures for different datasets and black-box models.

### Open Question 2
- Question: How can the convex hull-based method be adapted to SHAP to generate the removed features in a specific way?
- Basis in paper: [explicit] The paper suggests that the proposed method can be adapted to SHAP to generate the removed features in a specific way, but it does not provide details on how this adaptation can be done.
- Why unresolved: The paper does not provide a clear methodology or algorithm for adapting the convex hull-based method to SHAP.
- What evidence would resolve it: A detailed explanation of the adaptation process, along with experimental results comparing the performance of the adapted method with the original SHAP method.

### Open Question 3
- Question: How does the proposed method compare to other explanation methods, such as LIME and SHAP, in terms of computational efficiency and explanation accuracy for high-dimensional data?
- Basis in paper: [inferred] The paper mentions that the proposed method can have a smaller dimensionality than the initial instances, which can potentially improve computational efficiency. However, it also notes that the calculation of vertices of the largest convex hull is a computationally hard problem. The paper does not provide a comprehensive comparison with other explanation methods.
- Why unresolved: The paper does not provide a detailed comparison of the proposed method with other explanation methods in terms of computational efficiency and explanation accuracy for high-dimensional data.
- What evidence would resolve it: Experimental results comparing the computational efficiency and explanation accuracy of the proposed method with other explanation methods, such as LIME and SHAP, for high-dimensional datasets.

## Limitations
- The method's reliance on convex hulls may fail for highly non-convex or multi-modal local decision boundaries
- Evaluation focuses primarily on synthetic datasets with limited real-world examples and insufficient comparison to established methods
- Computational complexity claims lack rigorous benchmarking against perturbation-based methods

## Confidence
- Mathematical framework is sound: High
- Empirical validation is limited: Low
- Theoretical justification for example-based explanations: Medium
- Connection between dual coefficients and feature importance: Medium

## Next Checks
1. Conduct extensive benchmarking against SHAP, anchors, and other established methods across diverse real-world datasets (healthcare, finance, image classification) with rigorous statistical testing.
2. Perform ablation studies varying K, n, and neighborhood selection strategies to quantify their impact on explanation quality and computational efficiency.
3. Test the method's robustness to adversarial examples and out-of-distribution data to verify the claimed advantage of avoiding out-of-domain issues.