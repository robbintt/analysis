---
ver: rpa2
title: 'InstUPR : Instruction-based Unsupervised Passage Reranking with Large Language
  Models'
arxiv_id: '2403.16435'
source_url: https://arxiv.org/abs/2403.16435
tags:
- reranking
- passage
- retrieval
- llms
- unsupervised
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes InstUPR, an unsupervised passage reranking
  method based on large language models (LLMs). InstUPR leverages the instruction-following
  capabilities of instruction-tuned LLMs for passage reranking without any additional
  fine-tuning.
---

# InstUPR : Instruction-based Unsupervised Passage Reranking with Large Language Models

## Quick Facts
- arXiv ID: 2403.16435
- Source URL: https://arxiv.org/abs/2403.16435
- Reference count: 5
- Primary result: Achieves 46.63% NDCG@10 on BEIR benchmark, outperforming UPR by 7.62% and instruction-tuned reranker by 3.96%

## Executive Summary
This paper proposes InstUPR, an unsupervised passage reranking method that leverages the instruction-following capabilities of large language models (LLMs) without requiring any fine-tuning. The method uses an instruction-tuned LLM to generate relevance scores for query-passage pairs through soft score aggregation and pairwise reranking. Experiments on the BEIR benchmark demonstrate that InstUPR outperforms both unsupervised baselines and instruction-tuned rerankers, highlighting its effectiveness for zero-shot passage reranking.

## Method Summary
InstUPR employs an instruction-tuned LLM (flan-t5-xl) to score query-passage pairs using two mechanisms: pointwise scoring with soft aggregation and pairwise reranking. For pointwise scoring, the LLM follows instructions to output a relevance score (1-5) and computes a weighted sum using the probability distribution over possible scores. For pairwise reranking, the LLM compares two passages at a time to determine relative relevance, with final scores computed as the sum of win probabilities against all other passages. The method operates without any additional fine-tuning, relying solely on the LLM's instruction-following capabilities.

## Key Results
- InstUPR achieves 46.63% NDCG@10 on the BEIR benchmark
- Outperforms unsupervised baseline UPR by 7.62%
- Outperforms instruction-tuned reranker by 3.96%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction-tuned LLMs can directly generate relevance scores without additional fine-tuning.
- Mechanism: The LLM follows task-specific instructions to output a single token (1-5) representing relevance, then aggregates these discrete predictions into a continuous score via weighted sum.
- Core assumption: The LLM's instruction-following capability generalizes to relevance scoring without retrieval-specific training data.
- Evidence anchors:
  - [abstract]: "Our method leverages the instruction-following capabilities of instruction-tuned LLMs for passage reranking without any additional fine-tuning."
  - [section]: "We employ an instruction-tuned LLM to generate a relevance score for each query-passage pair."
  - [corpus]: Weak evidence - no direct corpus mentions of instruction-following generalization to scoring tasks.
- Break condition: If the LLM fails to interpret scoring instructions correctly or outputs inconsistent distributions across similar passages.

### Mechanism 2
- Claim: Soft score aggregation improves robustness by combining probability distributions over possible scores.
- Mechanism: Instead of using the most likely discrete score, compute the expected value: s1(q, di) = Σ n·p(n|q, di) where p(n|q, di) is the probability of score n.
- Core assumption: The probability distribution from the LLM contains useful signal beyond the mode, and expected value is a meaningful aggregation.
- Evidence anchors:
  - [section]: "Instead of using the generated token directly, we compute a weighted sum of the options using their probabilities as weights."
  - [section]: "The soft relevance score of a query-passage pair s1(q, di) can be calculated as..."
  - [corpus]: No corpus evidence on expected value aggregation in LLM scoring contexts.
- Break condition: If the LLM's probability estimates are poorly calibrated or if the distribution is too peaked to provide meaningful averaging.

### Mechanism 3
- Claim: Pairwise reranking improves performance by comparing passages directly rather than scoring independently.
- Mechanism: For each pair of passages (di, dj), the LLM selects which is more relevant to the query. Final score for di is the sum of probabilities it wins against all other passages: s2(q, di) = Σ p(i|q, di, dj).
- Core assumption: Relative comparisons provide more discriminative signal than absolute scores, and summing pairwise wins produces a meaningful ranking.
- Evidence anchors:
  - [section]: "Pairwise reranking has been demonstrated to outperform pointwise reranking while being more computationally expensive."
  - [section]: "The final score of a passage di... is then re-estimated as the sum of its scores against all other passage candidates."
  - [corpus]: Weak evidence - mentions of pairwise reranking in LLM contexts but not specific to this aggregation method.
- Break condition: If the LLM produces intransitive preferences (A > B, B > C, but C > A) or if computational constraints prevent evaluating all pairs.

## Foundational Learning

- Concept: Instruction-following in LLMs
  - Why needed here: The entire method relies on the LLM's ability to interpret and execute scoring instructions without retrieval-specific fine-tuning.
  - Quick check question: What happens if you change the instruction format from Likert scale to binary relevance - does the LLM still produce meaningful scores?

- Concept: Probability calibration in generative models
  - Why needed here: Soft aggregation depends on the LLM producing well-calibrated probability distributions over the 1-5 score tokens.
  - Quick check question: If the LLM always assigns ~0.99 probability to one score option, what does this imply about the effectiveness of soft aggregation?

- Concept: Pairwise comparison aggregation
  - Why needed here: The pairwise reranking method assumes that summing win probabilities produces a meaningful global ranking, which requires understanding of tournament-style ranking systems.
  - Quick check question: If passage A beats B and C individually, but C beats B, does A necessarily beat C in this system?

## Architecture Onboarding

- Component map: Input (query + retrieved passages) → Instruction Processor → LLM Scoring (Pointwise + Pairwise) → Soft Aggregation → Output Ranking
- Critical path: Query → Instruction Template → LLM Generation → Probability Extraction → Score Calculation → Reranking
- Design tradeoffs: Pairwise reranking provides better accuracy but scales quadratically with the number of passages; soft aggregation adds computation but reduces discrete score ties.
- Failure signatures: Consistent generation of same score across diverse passages suggests LLM not interpreting instructions correctly; poor pairwise consistency suggests issues with relative comparison capability.
- First 3 experiments:
  1. Run with pointwise reranking only, using hard scores (most likely token) - compare against soft aggregation baseline
  2. Run with pairwise reranking but limit to top-10 passages to test computational feasibility
  3. Compare Likert scale instructions against binary relevance instructions to test instruction sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the scale of LLMs affect reranking performance, and what is the optimal model size for InstUPR?
- Basis in paper: [explicit] The authors mention that future work could explore how the scale of LLMs affects reranking performance.
- Why unresolved: The paper only uses one LLM (flan-t5-xl) and does not compare performance across different model sizes or scales.
- What evidence would resolve it: Experiments comparing InstUPR performance using various LLM scales (e.g., small, medium, large, XL) on the BEIR benchmark would show how model size impacts effectiveness and efficiency.

### Open Question 2
- Question: How can pairwise reranking be made more computationally efficient for large-scale retrieval tasks?
- Basis in paper: [explicit] The authors acknowledge that pairwise reranking incurs high computational costs and is challenging to scale up to hundreds of passage candidates.
- Why unresolved: The paper does not propose or evaluate any specific techniques to address this computational bottleneck.
- What evidence would resolve it: Developing and evaluating novel algorithms or approximations that reduce the computational complexity of pairwise reranking (e.g., using sorting algorithms, sampling, or pruning techniques) while maintaining or improving performance would demonstrate practical solutions.

### Open Question 3
- Question: How does InstUPR generalize across diverse LLMs, and what factors contribute to performance differences?
- Basis in paper: [explicit] The authors note that different LLMs may exhibit varying behaviors and performances, and further investigation is needed to assess generalization capabilities.
- Why unresolved: The paper only uses one LLM (flan-t5-xl) and does not compare performance across multiple LLMs or analyze factors contributing to potential differences.
- What evidence would resolve it: Experiments comparing InstUPR performance using a diverse set of LLMs (e.g., different architectures, pre-training objectives, sizes) on the BEIR benchmark would reveal generalization capabilities and identify factors influencing performance variations.

## Limitations
- The method relies on instruction-tuned LLMs without retrieval-specific fine-tuning, which may not generalize well to all retrieval scenarios
- Pairwise reranking scales quadratically with passage count, limiting practical applicability for larger candidate pools
- Evaluation covers only 10 datasets from BEIR, potentially missing diverse retrieval scenarios

## Confidence

- **High confidence**: The pairwise reranking mechanism produces improved results over pointwise approaches, as this is a well-established finding in reranking literature and the experimental results align with this prior knowledge.
- **Medium confidence**: The soft score aggregation provides measurable improvement over hard score selection, though the magnitude of improvement may be influenced by the specific LLM's probability calibration.
- **Low confidence**: The claim that no fine-tuning is required is technically accurate but may be misleading - the method still requires careful prompt engineering and relies on the LLM having been instruction-tuned on vast amounts of data, which itself represents an implicit form of prior training.

## Next Checks

1. **Ablation study on instruction format**: Compare Likert scale (1-5) instructions against binary relevance instructions to determine if the multi-point scale is essential for performance or merely an artifact of the instruction format.

2. **Probability distribution analysis**: Systematically analyze the LLM's probability distributions across passages to determine if they are well-calibrated, and test whether soft aggregation provides consistent improvements when distributions are poorly calibrated.

3. **Computational scalability test**: Evaluate InstUPR's performance and runtime when applied to larger candidate pools (e.g., top-1000 passages) to quantify the practical limitations of pairwise reranking beyond the top-100 used in the paper.