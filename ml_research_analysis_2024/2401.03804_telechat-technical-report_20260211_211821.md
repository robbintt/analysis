---
ver: rpa2
title: TeleChat Technical Report
arxiv_id: '2401.03804'
source_url: https://arxiv.org/abs/2401.03804
tags:
- data
- arxiv
- training
- language
- telechat
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This technical report introduces TeleChat, a collection of large
  language models with 3B, 7B, and 12B parameters, trained on extensive Chinese and
  English text data. TeleChat undergoes supervised fine-tuning and reinforcement learning
  to align with human preferences.
---

# TeleChat Technical Report

## Quick Facts
- arXiv ID: 2401.03804
- Source URL: https://arxiv.org/abs/2401.03804
- Reference count: 40
- Primary result: TeleChat models (3B/7B/12B) outperform similar-sized open-source models on language, math, reasoning, and code benchmarks

## Executive Summary
TeleChat is a family of large language models with 3B, 7B, and 12B parameters trained on extensive Chinese and English text data. The models undergo supervised fine-tuning and reinforcement learning to align with human preferences. The 7B and 12B fine-tuned versions, along with code and partial pretraining data, are publicly released to support future research.

## Method Summary
The technical report details TeleChat's development through supervised fine-tuning and reinforcement learning from human feedback. The models are trained on multilingual data covering both Chinese and English text corpora. To address hallucinations, the authors propose augmenting prompts with knowledge graph information to provide relevant factual context during inference.

## Key Results
- TeleChat achieves competitive performance on benchmarks for language understanding, mathematics, reasoning, code generation, and knowledge-based question answering
- The 7B and 12B fine-tuned models outperform similar-sized open-source models
- Knowledge graph augmentation is proposed to mitigate hallucinations during inference

## Why This Works (Mechanism)
TeleChat's competitive performance stems from its comprehensive training approach combining supervised fine-tuning with reinforcement learning from human preferences. The multilingual training corpus enables strong cross-lingual capabilities, while the knowledge graph augmentation technique provides factual grounding that reduces hallucinatory outputs. The model sizes (3B/7B/12B) represent an optimal balance between computational efficiency and performance for many practical applications.

## Foundational Learning

1. **Knowledge Graph Augmentation** - Why needed: To provide factual grounding and reduce hallucinations during inference. Quick check: Compare model outputs with and without KG-augmented prompts on fact-based questions.

2. **Reinforcement Learning from Human Feedback** - Why needed: To align model outputs with human preferences and improve helpfulness/safety. Quick check: Evaluate model responses using human preference scoring across different tasks.

3. **Multilingual Training** - Why needed: To enable strong performance across both Chinese and English language tasks. Quick check: Test model performance on parallel corpora and cross-lingual benchmarks.

4. **Parameter Scaling** - Why needed: To find the optimal balance between model capability and computational efficiency. Quick check: Compare performance across different model sizes on standard benchmarks.

## Architecture Onboarding

Component Map: Data Pipeline -> Pretraining -> Supervised Fine-tuning -> RLHF Alignment -> Inference with KG Augmentation

Critical Path: The most critical components are the pretraining data quality, the RLHF alignment phase, and the knowledge graph integration during inference.

Design Tradeoffs: The choice of 3B/7B/12B parameter sizes balances performance with deployment feasibility. The multilingual approach trades some monolingual specialization for broader language coverage.

Failure Signatures: Hallucinations occur most frequently on niche factual questions without KG augmentation. Performance degradation is observed when training data distribution shifts significantly from the pretraining corpus.

3 First Experiments:
1. Compare TeleChat performance with and without KG-augmented prompts on fact-based questions
2. Evaluate cross-lingual transfer by testing Chinese-trained models on English tasks and vice versa
3. Test model robustness by evaluating on out-of-distribution data from different domains

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation methodology relies heavily on public benchmarks that may not reflect real-world deployment scenarios
- Knowledge graph augmentation approach lacks detailed ablation studies quantifying its effectiveness
- Limited comparison against commercial alternatives like GPT-4 or Claude for context

## Confidence
- Benchmark performance claims: Medium confidence
- Knowledge graph augmentation effectiveness: Low confidence
- Generalization across domains: Low confidence

## Next Checks
1. Conduct controlled experiments comparing knowledge graph-augmented prompts against traditional few-shot prompting and chain-of-thought techniques on hallucination-prone tasks

2. Perform extensive multilingual evaluation across low-resource language pairs to assess the actual performance gap versus monolingual benchmarks

3. Implement and publish detailed bias and fairness audits using established frameworks to evaluate model behavior across demographic dimensions and sensitive topics