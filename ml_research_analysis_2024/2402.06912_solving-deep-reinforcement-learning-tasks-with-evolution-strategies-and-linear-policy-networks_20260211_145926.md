---
ver: rpa2
title: Solving Deep Reinforcement Learning Tasks with Evolution Strategies and Linear
  Policy Networks
arxiv_id: '2402.06912'
source_url: https://arxiv.org/abs/2402.06912
tags:
- linear
- methods
- policy
- learning
- gradient-based
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares evolution strategies (ES) to gradient-based
  deep reinforcement learning (DRL) methods on classic benchmarks like CartPole, LunarLander,
  MuJoCo robotics, and Atari games. The authors benchmark three ES methods (CSA-ES,
  sep-CMA-ES, CMA-ES), three gradient-based DRL methods (DQN, PPO, SAC), and Augmented
  Random Search (ARS).
---

# Solving Deep Reinforcement Learning Tasks with Evolution Strategies and Linear Policy Networks

## Quick Facts
- arXiv ID: 2402.06912
- Source URL: https://arxiv.org/abs/2402.06912
- Reference count: 40
- Key outcome: Evolution strategies can solve many benchmark tasks with simple linear policies, challenging the need for deep networks

## Executive Summary
This paper investigates whether evolution strategies (ES) can effectively solve reinforcement learning tasks typically addressed with deep reinforcement learning (DRL) methods. The authors conduct systematic benchmarking across classic control tasks, robotics environments, and Atari games, comparing ES methods against gradient-based DRL approaches. A key innovation is the use of linear policy networks for both ES and DRL methods, revealing that many tasks can be solved effectively without deep architectures. The results suggest that current benchmark tasks may be simpler than previously assumed and that ES offers a simpler, more interpretable alternative for certain RL problems.

## Method Summary
The authors benchmark three ES methods (CSA-ES, sep-CMA-ES, CMA-ES) against three gradient-based DRL methods (DQN, PPO, SAC) and Augmented Random Search (ARS). All methods are tested using both deep neural networks and linear policy networks. For ES and ARS, only linear networks are trained due to their population-based nature. The experiments cover a range of benchmark tasks including CartPole, LunarLander, MuJoCo robotics environments, and Atari games. The evaluation focuses on learning speed, final performance, and policy simplicity, with particular attention to how linear policies perform compared to deep networks.

## Key Results
- ES methods achieve competitive performance to DRL on many benchmark tasks using only linear policies
- For complex tasks, ES often outperforms ARS and matches DRL performance
- On Atari games using RAM state input, ES achieves performance comparable to DQN trained on pixel images
- Linear policies are sufficient for solving many classic control and robotics tasks

## Why This Works (Mechanism)
The effectiveness of ES with linear policies stems from the relatively low dimensionality of the action space in many benchmark tasks and the structured nature of the state representations. Evolution strategies can efficiently explore the policy parameter space even with simple linear mappings from states to actions. The population-based search allows ES to discover effective linear transformations that map observations to appropriate actions, particularly in environments where the relationship between states and optimal actions is relatively straightforward. This suggests that the complexity of deep networks may be unnecessary for many standard RL benchmarks.

## Foundational Learning
- Evolution Strategies (ES): Population-based optimization methods that evolve policy parameters directly without gradient computation. Needed to understand the alternative to gradient-based RL methods.
- Policy Gradients vs Direct Policy Search: ES performs direct policy search by optimizing parameters, while DRL methods use policy gradients. Quick check: Can ES methods handle high-dimensional action spaces as effectively as policy gradient methods?
- Linear Policy Networks: Single-layer networks mapping states directly to actions. Needed to understand the simplicity of the learned policies. Quick check: Are linear policies sufficient for tasks with non-linear dynamics?
- CMA-ES Algorithm: Covariance Matrix Adaptation Evolution Strategy, a state-of-the-art ES method. Needed to understand the specific algorithms being compared. Quick check: How does CMA-ES adapt its search distribution compared to simpler ES variants?

## Architecture Onboarding
- Component Map: State observations -> Linear Policy Network -> Action selection -> Environment -> Reward signal -> ES/DRL optimization
- Critical Path: The direct mapping from states to actions through the linear policy represents the critical path for both ES and DRL methods.
- Design Tradeoffs: Using linear policies significantly reduces computational complexity but may limit performance on highly non-linear tasks.
- Failure Signatures: ES with linear policies may fail on tasks requiring complex temporal reasoning or perception from raw sensory data.
- First Experiments:
  1. Test ES with linear policies on CartPole to verify basic functionality
  2. Compare ES and DRL performance on LunarLander with identical linear architectures
  3. Evaluate training stability of ES versus DRL on a simple robotics task

## Open Questions the Paper Calls Out
The paper suggests that current benchmark tasks may be too simple and that the superiority of deep networks might not hold for more complex, real-world problems. It raises questions about the scalability of ES with linear policies to tasks with high-dimensional state spaces, complex visual processing requirements, or long-term credit assignment challenges.

## Limitations
- Asymmetric comparison using linear policies only for ES while DRL uses both linear and deep networks
- Benchmark tasks may not represent the complexity of real-world problems where ES typically struggles
- Atari results using RAM states don't directly compare ES on pixel inputs to DRL on pixels
- Limited exploration of ES scalability to higher-dimensional action spaces

## Confidence
- ES can solve many benchmark tasks with linear policies: High
- Current benchmarks may be too simple: Medium
- ES offers a simpler alternative to DRL for many problems: Medium
- ES achieves competitive performance across all tested benchmarks: Medium

## Next Checks
1. Replicate experiments using identical policy network architectures (linear vs deep) for both ES and DRL methods to ensure fair comparison
2. Test ES on more complex, procedurally generated tasks or real-world robotics problems to verify claims about benchmark simplicity
3. Compare ES performance on pixel inputs versus RAM states for Atari games to establish the visual processing capabilities of ES relative to DRL