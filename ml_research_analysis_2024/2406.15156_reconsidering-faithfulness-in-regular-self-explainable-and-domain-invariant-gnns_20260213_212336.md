---
ver: rpa2
title: Reconsidering Faithfulness in Regular, Self-Explainable and Domain Invariant
  GNNs
arxiv_id: '2406.15156'
source_url: https://arxiv.org/abs/2406.15156
tags:
- graph
- explanation
- faithfulness
- explanations
- edges
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work critically examines faithfulness in Graph Neural Networks
  (GNNs) by analyzing how different faithfulness metrics are not interchangeable and
  can systematically ignore important properties of explanations. The authors prove
  that for injective regular GNNs, perfectly faithful explanations are uninformative,
  but this does not apply to modular architectures like self-explainable and domain-invariant
  GNNs.
---

# Reconsidering Faithfulness in Regular, Self-Explainable and Domain Invariant GNNs

## Quick Facts
- **arXiv ID**: 2406.15156
- **Source URL**: https://arxiv.org/abs/2406.15156
- **Authors**: Steve Azzolin; Antonio Longa; Stefano Teso; Andrea Passerini
- **Reference count**: 40
- **Primary result**: Faithfulness metrics for GNN explanations are not interchangeable and can systematically ignore important properties of explanations.

## Executive Summary
This work critically examines faithfulness in Graph Neural Networks (GNNs) by analyzing how different faithfulness metrics are not interchangeable and can systematically ignore important properties of explanations. The authors prove that for injective regular GNNs, perfectly faithful explanations are uninformative, but this does not apply to modular architectures like self-explainable and domain-invariant GNNs. They show that optimizing for faithfulness is not always a sensible design goal and identify a trade-off between model expressiveness and usefulness of faithful explanations. The study reveals that faithfulness is tightly linked to out-of-distribution generalization, demonstrating that domain-invariant subgraphs must also be faithful to ensure true domain invariance. The findings challenge current GNN design principles and highlight the need for more nuanced approaches to evaluating and implementing faithful explanations.

## Method Summary
The paper parameterizes existing faithfulness metrics along two dimensions: how stability is measured (divergence d) and what perturbations are allowed (interventional distributions pR and pC). The authors prove theoretical results about the relationship between faithfulness and model architecture, showing that for injective regular GNNs, perfectly faithful explanations must cover the entire computational graph and are therefore uninformative. They then analyze modular GNN architectures (self-explainable and domain-invariant GNNs) and their faithfulness properties, connecting faithfulness to out-of-distribution generalization. The method involves conducting experiments across various synthetic and real datasets to validate these theoretical findings.

## Key Results
- Faithfulness metrics are not interchangeable because they use different interventional distributions and divergences, measuring fundamentally different properties
- For injective regular GNNs, perfectly faithful explanations are completely uninformative as they must cover the entire computational graph
- Faithfulness is tightly linked to out-of-distribution generalization, as domain-invariant subgraphs must also be faithful to ensure true domain invariance
- Modular architectures like self-explainable and domain-invariant GNNs can provide informative faithful explanations, unlike regular injective GNNs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Faithfulness metrics for GNN explanations are not interchangeable because they use different interventional distributions and divergences.
- Mechanism: Different faithfulness metrics measure sufficiency and necessity using distinct ways of perturbing the input graph and measuring output changes. Metrics that perturb in disjoint ways (like deleting all irrelevant features vs. deleting random subsets) cannot be meaningfully compared because they assess fundamentally different properties of explanations.
- Core assumption: Explanations that are faithful according to one metric may be unfaithful according to others due to these systematic differences in measurement approach.
- Evidence anchors:
  - [abstract] "We begin by showing that existing metrics are not interchangeable – i.e., explanations attaining high faithfulness according to one metric may be unfaithful according to others"
  - [section] "While both sufficiency and necessity matter, there exists a natural tension between them"
- Break condition: If all faithfulness metrics converge to measure the same property (sufficient and necessary explanations), this mechanism breaks down.

### Mechanism 2
- Claim: For injective regular GNNs, perfectly faithful explanations are completely uninformative because they must cover the entire computational graph.
- Mechanism: Injective GNNs preserve graph structure information through their layers, meaning that any change to the computational graph (the nodes whose messages affect the prediction) must affect the output. This forces strictly faithful explanations to cover all nodes in the computational graph, making them trivially large and uninformative.
- Core assumption: Injectivity is necessary for GNNs to implement the Weisfeiler-Lehman test, which is crucial for their expressive power.
- Evidence anchors:
  - [abstract] "we prove that for injective regular GNN architectures, perfectly faithful explanations are completely uninformative"
  - [section] "Consider a binary classification task, an L-layer injective GNN, any pC and pR not allowing the addition of new elements"
- Break condition: If the GNN is not injective or uses non-local aggregations that allow information to flow from outside the computational graph.

### Mechanism 3
- Claim: Faithfulness is tightly linked to out-of-distribution generalization because domain-invariant subgraphs must also be sufficient to ensure true domain invariance.
- Mechanism: Domain-invariant GNNs extract subgraphs they believe are invariant across domains, but unless these subgraphs are also sufficient (no changes to the complement affect the output), the model can still use domain-dependent information for predictions, preventing true domain invariance.
- Core assumption: The literature on domain invariance has neglected faithfulness, focusing only on plausibility (how close the extracted subgraph is to the truly invariant subgraph).
- Evidence anchors:
  - [abstract] "simply ensuring that a GNN can correctly recognize the domain-invariant subgraph, as prescribed by the literature, does not guarantee that it is invariant unless this subgraph is also faithful"
  - [section] "We show that extracting a domain invariant subgraph is not enough for a GNN to be truly domain invariant: unless the subgraph is also faithful, the information from the domain-dependent components of the input can still influence the prediction"
- Break condition: If the model architecture prevents any information leakage from the complement to the prediction regardless of faithfulness.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and message passing
  - Why needed here: The paper builds on understanding how GNNs aggregate information from neighborhoods to make predictions, which is fundamental to understanding what makes explanations faithful or not.
  - Quick check question: What is the difference between local and non-local aggregation in GNNs, and why does it matter for faithfulness?

- Concept: Faithfulness in explanations
  - Why needed here: The core contribution is about understanding what faithfulness means, how it's measured, and why it matters for both interpretability and domain generalization.
  - Quick check question: What's the difference between sufficiency and necessity in the context of faithful explanations?

- Concept: Domain invariance and out-of-distribution generalization
  - Why needed here: The paper connects faithfulness to domain invariance, showing that faithful explanations are necessary for truly invariant models.
  - Quick check question: Why might a model that extracts domain-invariant subgraphs still fail to generalize across domains?

## Architecture Onboarding

- Component map: Faithfulness metrics (sufficiency and necessity measures) -> Modular GNN architectures (SE-GNNs and DI-GNNs) -> Evaluation framework (synthetic and real datasets)
- Critical path: Understanding faithfulness metrics → Analyzing modular GNN architectures → Connecting faithfulness to domain invariance → Experimental validation across datasets
- Design tradeoffs: Faithfulness vs. model expressiveness (injective GNNs can't have informative faithful explanations), local vs. global information (modular architectures may miss global patterns), and metric strictness (stricter metrics are more reliable but harder to achieve)
- Failure signatures: Explanations that are faithful according to one metric but not others, modular GNNs that don't fully adhere to their own explanations, and domain-invariant models that fail OOD generalization despite extracting plausible subgraphs
- First 3 experiments:
  1. Implement and compare multiple faithfulness metrics on a simple GNN to demonstrate non-interchangeability
  2. Test whether injective GNNs produce uninformative faithful explanations by checking if explanations cover entire computational graphs
  3. Evaluate whether domain-invariant GNNs that extract plausible subgraphs also achieve OOD generalization

## Open Questions the Paper Calls Out

- What are the practical implications of using faithfulness metrics with different parameter choices (d, pR, pC) in high-stakes applications like loan approval?
- Can self-explainable GNNs be designed to achieve both high faithfulness and high accuracy without sacrificing model expressiveness?
- How does the concept of faithfulness extend to other types of neural networks beyond GNNs, such as transformers or CNNs?

## Limitations

- Limited experimental coverage across different GNN architectures
- Potential sensitivity to hyperparameter choices in faithfulness metrics
- Unclear generalization to large-scale real-world datasets

## Confidence

- **High confidence**: The theoretical proof that perfectly faithful explanations are uninformative for injective regular GNNs is mathematically sound and well-established
- **Medium confidence**: The practical implications of metric non-interchangeability and the faithfulness-domain invariance connection, as these require more extensive empirical validation across diverse scenarios
- **Low confidence**: The generalizability of findings to very large-scale real-world applications, as the paper focuses primarily on controlled experimental settings

## Next Checks

1. Test the non-interchangeability of faithfulness metrics on a broader range of GNN architectures beyond the ones analyzed
2. Verify the uninformative nature of perfectly faithful explanations on real-world datasets with varying graph properties
3. Empirically validate the faithfulness-domain invariance connection using diverse domain adaptation scenarios