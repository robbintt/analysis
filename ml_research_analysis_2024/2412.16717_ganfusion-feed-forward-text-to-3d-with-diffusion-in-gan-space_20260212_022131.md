---
ver: rpa2
title: 'GANFusion: Feed-Forward Text-to-3D with Diffusion in GAN Space'
arxiv_id: '2412.16717'
source_url: https://arxiv.org/abs/2412.16717
tags:
- diffusion
- photo
- wearing
- text
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces GANFusion, a feed-forward text-to-3D generation
  method that is trained using only single-view 2D image supervision. The key idea
  is to combine the strengths of GANs and denoising diffusion models: an unconditional
  GAN is first trained to generate triplane features for 3D objects, and then a text-conditioned
  diffusion model is trained to generate triplanes in the GAN''s latent space.'
---

# GANFusion: Feed-Forward Text-to-3D with Diffusion in GAN Space

## Quick Facts
- arXiv ID: 2412.16717
- Source URL: https://arxiv.org/abs/2412.16717
- Authors: Souhaib Attaiki; Paul Guerrero; Duygu Ceylan; Niloy J. Mitra; Maks Ovsjanikov
- Reference count: 40
- Primary result: Achieves CLIP similarity of 0.296 and FID of 88.23 on synthetic human dataset, outperforming RenderDiffusion

## Executive Summary
GANFusion introduces a feed-forward text-to-3D generation method that is trained using only single-view 2D image supervision. The key innovation is a two-stage approach that combines the strengths of GANs and denoising diffusion models: an unconditional GAN first generates triplane features for 3D objects, then a text-conditioned diffusion model generates triplanes in the GAN's latent space. This approach avoids costly test-time optimization while enabling high-quality, text-guided 3D generation with strong text alignment and generation quality.

## Method Summary
GANFusion operates in two stages. First, an unconditional 3D GAN (AG3D or EG3D) is trained on 2D images to learn a latent space of triplane features. Second, a text-conditioned diffusion model is trained to generate triplanes in this learned space using a large dataset of captioned triplanes. During inference, text prompts are converted to triplanes by the diffusion model, which are then rendered using the pre-trained GAN's renderer and upsampler to produce final images.

## Key Results
- Achieves CLIP similarity of 0.296 and FID of 88.23 on synthetic human dataset
- Outperforms RenderDiffusion in both text alignment and generation quality
- Demonstrates effectiveness of two-stage approach combining GAN and diffusion models

## Why This Works (Mechanism)

### Mechanism 1
GANs can be trained efficiently with 2D supervision to produce high-quality 3D objects using adversarial loss between real images and rendered fake images. The core assumption is that the GAN can learn a latent space encoding high-quality 3D objects without explicit 3D supervision. Break condition: If rendered GAN outputs diverge significantly from real image distribution, adversarial training fails.

### Mechanism 2
Diffusion models can be conditioned on text efficiently through classifier-free guidance by adding text conditioning as input to the denoiser network. The core assumption is that the denoiser network can learn to denoise triplane features while incorporating text guidance. Break condition: If guidance scale is too high or low, text alignment will be poor or generation quality will suffer.

### Mechanism 3
The two-stage approach of first training a GAN then distilling into a diffusion model avoids challenges of training a single network for both triplane encoding and denoising. The core assumption is that the GAN's learned triplane space is sufficiently rich and well-structured for the diffusion model to operate effectively. Break condition: If GAN's triplane space has significant gaps or is poorly structured, diffusion model will struggle to generate high-quality outputs.

## Foundational Learning

- **Triplane representation for 3D objects**: Provides compact representation that can be rendered efficiently while maintaining high quality, and can be learned from 2D supervision. Quick check: How does triplane representation differ from volumetric representation in terms of memory efficiency and rendering speed?

- **Adversarial training for GANs**: Uses adversarial loss which allows learning latent space from 2D supervision without requiring explicit 3D or multi-view ground truth. Quick check: What is the key difference between adversarial loss in GANs and reconstruction loss in autoencoders?

- **Denoising diffusion probabilistic models**: Can be conditioned on text through classifier-free guidance and can learn to denoise in a learned latent space. Quick check: How does noise schedule in diffusion models affect quality of generated samples?

## Architecture Onboarding

- **Component map**: 2D images → Stage 1: GAN-based triplane generator → Stage 2: Text-conditioned diffusion model → Renderer + Upsampler → Final images

- **Critical path**: 1) Train unconditional GAN with 2D images, 2) Generate triplanes, caption with BLIP, 3) Train text-conditioned diffusion model on (caption, triplane) pairs, 4) At inference: generate triplanes from text → render with GAN's renderer and upsampler

- **Design tradeoffs**: Using triplanes vs. other 3D representations (voxels, point clouds): triplanes offer better quality and efficiency; Two-stage approach vs. end-to-end training: two-stage avoids complexity of training single network for both encoding and denoising; Using pre-trained renderer/upsampler vs. training jointly: pre-trained components ensure consistency but may limit flexibility

- **Failure signatures**: Mode collapse in GAN training (limited output variety), poor text alignment in diffusion model (outputs don't match prompts despite guidance), low-quality outputs overall (issues in either stage 1 or 2 training)

- **First 3 experiments**: 1) Train unconditional GAN on small 2D subset and visualize generated triplanes/rendered outputs, 2) Generate small triplane dataset, caption with BLIP, train diffusion model to check learning in triplane space, 3) Combine trained GAN and diffusion model for end-to-end inference on test prompts and evaluate text alignment and quality

## Open Questions the Paper Calls Out

### Open Question 1
How does the two-stage training strategy of GANFusion affect the quality and diversity of generated 3D models compared to end-to-end training methods? The paper provides comparisons with RenderDiffusion but lacks comprehensive analysis of two-stage approach's impact on different aspects of 3D model generation. Detailed ablation studies comparing quality metrics with variations of training process would provide insights into benefits and limitations.

### Open Question 2
Can the GANFusion framework be extended to generate 3D models of arbitrary categories beyond humans and faces? While effective on human and face datasets, the framework's generalization to other categories remains unclear. Training and evaluating on diverse datasets representing various object categories would demonstrate generalization capabilities.

### Open Question 3
What is the impact of 2D image dataset quality on GANFusion performance? The paper mentions creating synthetic dataset but doesn't investigate how variations in dataset quality (resolution, diversity, noise) affect generated 3D model quality. Experiments with datasets of varying quality would reveal framework's sensitivity to dataset characteristics.

## Limitations
- Evaluation limited to synthetic human dataset, may not generalize to diverse real-world data
- Does not thoroughly explore limitations such as potential mode collapse or guidance scale impact
- Lacks extensive qualitative analysis of generated outputs from multiple views

## Confidence

- **High confidence**: Core methodology of using GAN to generate triplane features and diffusion model to denoise in this space is sound and well-explained
- **Medium confidence**: Quantitative results showing improvements over RenderDiffusion are convincing but limited to synthetic dataset
- **Low confidence**: Paper doesn't thoroughly explore approach limitations or provide extensive qualitative analysis

## Next Checks
1. Evaluate on real-world datasets (ShapeNet, CO3D) to assess generalization beyond synthetic human dataset
2. Conduct ablation studies on guidance scale to understand impact on text alignment and generation quality
3. Provide extensive qualitative analysis of generated 3D outputs with visualizations from multiple views and comparisons with ground truth