---
ver: rpa2
title: Pre-Trained Vision-Language Models as Partial Annotators
arxiv_id: '2406.18550'
source_url: https://arxiv.org/abs/2406.18550
tags:
- learning
- label
- sign
- traffic
- german
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel "pre-trained annotating - weakly-supervised
  learning" paradigm for applying pre-trained vision-language models to downstream
  image classification tasks without requiring additional human annotations. The authors
  use CLIP to automatically annotate unlabeled samples with multiple prompt templates,
  generating noisy partial label datasets.
---

# Pre-Trained Vision-Language Models as Partial Annotators

## Quick Facts
- arXiv ID: 2406.18550
- Source URL: https://arxiv.org/abs/2406.18550
- Reference count: 9
- Pre-trained vision-language models can achieve 94.06% accuracy on CIFAR-10 without human annotations

## Executive Summary
This paper introduces a novel "pre-trained annotating - weakly-supervised learning" paradigm that leverages pre-trained vision-language models (specifically CLIP) to automatically annotate unlabeled images with multiple prompt templates. The method generates noisy partial label datasets and employs a Collaborative consistency Regularization (Co-Reg) approach that trains two neural networks to mutually refine each other's predictions. By combining pseudo-label self-training with prototypical similarity alignment and noisy supervised contrastive learning, the method achieves state-of-the-art performance on multiple image classification benchmarks without requiring any human annotations.

## Method Summary
The method uses CLIP to generate partial label sets for unlabeled images using multiple prompt templates, then trains two neural networks simultaneously using collaborative consistency regularization. The approach includes a warm-up phase with partial cross-entropy loss, GMM-based noise detection, pseudo-label generation with temperature sharpening, self-training with both confident and unlabeled samples, prototypical similarity alignment to match representation and label distributions, and MoCo-based noisy supervised contrastive learning. The two networks mutually refine each other's predictions while the additional components enhance representation learning beyond what pseudo-label refinement alone provides.

## Key Results
- Achieves 94.06% accuracy on CIFAR-10 (vs 89.07% zero-shot)
- Outperforms other weakly supervised learning and few-shot fine-tuning methods across all benchmarks
- Shows significant improvements on challenging datasets like FER2013 (50.26% vs 27.37% zero-shot) and GTSRB (41.18% vs 30.40% zero-shot)
- Demonstrates the effectiveness of the partial annotator approach across diverse image classification tasks

## Why This Works (Mechanism)

### Mechanism 1
The collaborative consistency regularization enables the two networks to mutually refine each other's predictions, improving label quality for self-training. The two networks train on weak and strong augmentations of the same data, using pseudo-labels generated from the partner network. This mutual refinement process gradually improves the quality of the candidate label sets based on the minimal-loss criterion assumption that clean samples are easier to learn than noisy ones.

### Mechanism 2
The combination of prototypical similarity alignment and noisy supervised contrastive learning enhances the model's representation ability beyond what pseudo-label refinement alone provides. Prototypical similarity alignment ensures the representation distribution matches the label distribution, while contrastive learning pulls together representations of samples from the same class and pushes apart those from different classes, exploiting the data distribution property of downstream unlabeled images.

### Mechanism 3
Using multiple prompt templates to generate candidate label sets captures more of the true label's information than single-label predictions, making the learning problem easier. Each prompt template acts as a different "annotator" with its own biases and strengths. By combining all predictions into a partial label set, the method increases the probability that at least one prompt correctly identifies the true label, providing richer information than zero-shot inference with single predictions.

## Foundational Learning

- **Partial Label Learning (PLL)**: Why needed - The method must disambiguate which candidate label in each partial label set is the true label. Quick check - If a sample has candidate labels {cat, dog, car} and the true label is "dog", what is the partial label learning problem trying to solve?

- **Noisy Label Learning**: Why needed - The candidate labels generated by CLIP are not guaranteed to contain the true label, requiring techniques to handle noisy supervision. Quick check - How does the method determine which samples have valid candidate labels versus which should be treated as unlabeled?

- **Self-Training with Consistency Regularization**: Why needed - The method uses pseudo-labels generated from weakly-augmented data to train on strongly-augmented versions, requiring consistency between predictions. Quick check - What is the difference between the weak and strong augmentations in this context, and why are they both needed?

## Architecture Onboarding

- **Component map**: Image → Weak augmentation → Net1 prediction → GMM classification → Pseudo-label generation → Strong augmentation → Net2 training (self-training + prototypical alignment + contrastive loss)
- **Critical path**: The forward pass through weak augmentation, Net1 prediction, noise detection, pseudo-label generation, strong augmentation, and Net2 training with the three loss components
- **Design tradeoffs**: Two networks add memory and computation overhead but enable mutual refinement; multiple prompt templates increase annotation coverage but add inference time; temperature sharpening of pseudo-labels helps training but may reduce information
- **Failure signatures**: Both networks converge to the same incorrect predictions (mutual error reinforcement); pseudo-label accuracy plateaus below usable threshold; contrastive loss dominates and causes representation collapse; prototypes become unstable due to insufficient samples per class
- **First 3 experiments**: 1) Run on CIFAR-10 with default settings, verify zero-shot baseline (~89%) and final accuracy (~94%); 2) Disable collaborative pseudo-labeling (w/o co-PL) to measure impact of mutual refinement; 3) Vary the number of prompt templates (e.g., 1, 5, 10, all) to find the optimal balance between coverage and noise

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the proposed P-WSL paradigm scale with increasing dataset size or complexity in downstream tasks? The authors mention that their method outperforms zero-shot inference and other methods on several benchmarks, but they do not explore performance scaling with dataset size or complexity. What evidence would resolve it: Conducting experiments on datasets of varying sizes and complexities, and analyzing the performance trends as these factors change.

### Open Question 2
Can the P-WSL paradigm be effectively applied to other types of pre-trained models beyond vision-language models, such as large language models? The authors suggest that the P-WSL paradigm could be extended to other types of pre-trained models, including large language models, but they do not provide experimental evidence for this claim. What evidence would resolve it: Implementing and evaluating the P-WSL paradigm using large language models on relevant downstream tasks, and comparing the results to those obtained with vision-language models.

### Open Question 3
How does the proposed Co-Reg method perform under different noise levels in the partial label dataset? The authors mention that their method is designed to handle noisy partial label datasets, but they do not systematically investigate the performance under varying noise levels. What evidence would resolve it: Creating synthetic datasets with controlled noise levels and evaluating the Co-Reg method's performance across a range of noise conditions.

## Limitations

- Limited evaluation on larger-scale datasets (ImageNet, COCO) raises questions about scalability
- No analysis of computational overhead or memory requirements for the two-network architecture
- The claim of "no additional label information" is technically accurate but glosses over the complexity of partial label disambiguation

## Confidence

- **High Confidence**: The zero-shot inference baseline results and the general methodology of using CLIP as an annotator
- **Medium Confidence**: The comparative performance against other weakly supervised learning methods, given the comprehensive benchmark coverage
- **Low Confidence**: The scalability claims to larger datasets and the computational efficiency assertions, due to limited experimental validation

## Next Checks

1. **Parameter Sensitivity Analysis**: Systematically vary the number of prompt templates (1-20) and loss weights (λ1, λ2) to identify optimal configurations and robustness to hyperparameter changes

2. **Dataset Scalability Test**: Apply the method to a larger-scale dataset like ImageNet to evaluate whether the performance gains scale proportionally and identify any architectural bottlenecks

3. **Computational Overhead Measurement**: Measure memory usage, training time, and inference latency of the two-network architecture compared to single-network baselines to quantify the practical deployment costs