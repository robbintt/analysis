---
ver: rpa2
title: Effects of Using Synthetic Data on Deep Recommender Models' Performance
arxiv_id: '2406.18286'
source_url: https://arxiv.org/abs/2406.18286
tags:
- data
- synthetic
- samples
- dataset
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores the use of synthetic data generation to address
  data imbalance in recommender systems, particularly the predominance of negative
  interactions. Six methods were used to generate synthetic data, including SMOTE,
  CTGAN, CopulaGAN, TV AE, Gaussian Copula, and TabDDPM.
---

# Effects of Using Synthetic Data on Deep Recommender Models' Performance

## Quick Facts
- arXiv ID: 2406.18286
- Source URL: https://arxiv.org/abs/2406.18286
- Reference count: 28
- Primary result: Adding synthetic negative samples consistently improves CTR prediction AUC scores, with Gaussian Copula achieving up to 0.118% AUC increase

## Executive Summary
This study investigates how synthetic data generation can address data imbalance in recommender systems, where negative interactions significantly outnumber positive ones. The researchers tested six synthetic data generation methods (SMOTE, CTGAN, CopulaGAN, TV AE, Gaussian Copula, and TabDDPM) across five different scenarios using three deep recommender models (DNN, DeepFM, and MaskNet). The results demonstrate that adding synthetic negative samples consistently improves model performance, with Gaussian Copula emerging as the most effective method. The findings suggest that data augmentation strategies can effectively mitigate data sparsity and imbalance issues in recommender systems.

## Method Summary
The study evaluated the impact of synthetic data generation on CTR prediction using the Frappe dataset and three deep recommender models (DNN, DeepFM, MaskNet). Six synthetic data generation methods were applied across five scenarios: adding 25% or 50% synthetic positive samples, adding 25% or 50% synthetic negative samples, or using completely synthetic datasets. The generated data was integrated with the original dataset, and models were trained and evaluated using AUC scores. The Gaussian Copula method consistently outperformed others, particularly when generating synthetic negative samples.

## Key Results
- Adding synthetic negative samples consistently improved AUC scores across all models and scenarios
- Gaussian Copula achieved the best performance, with up to 0.118% AUC increase in the best scenario
- Synthetic positive samples did not improve performance despite the original dataset being highly imbalanced (CTR rate of 0.33)
- Models trained on synthetic-only datasets performed poorly, indicating the importance of maintaining real data in the training set

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding synthetic negative samples to imbalanced datasets improves recommender system performance more than adding synthetic positive samples.
- Mechanism: Negative samples dominate real-world recommendation datasets, causing the model to overfit to popular items. Synthetic negative samples introduce variety and balance, allowing the model to learn more generalized item-user interaction patterns.
- Core assumption: The dataset imbalance is primarily due to an excess of negative samples, and models can benefit from exposure to more diverse negative examples during training.
- Evidence anchors:
  - [abstract] "The significant impact of synthetic negative samples highlights the potential of data augmentation strategies to address issues of data sparsity and imbalance, ultimately leading to improved performance of recommender systems."
  - [section] "Interestingly, even though there were more negative samples, adding synthetic negative samples did increase the AUC score."

### Mechanism 2
- Claim: Gaussian Copula is the most effective method for generating high-quality synthetic data for recommendation tasks.
- Mechanism: Gaussian Copula models the joint distribution of variables using multivariate normal distributions and captures linear dependencies, which leads to more realistic synthetic samples that preserve the statistical properties of the original data.
- Core assumption: Linear dependencies between features are sufficient to model the relationships in the dataset, and preserving these correlations improves model performance.
- Evidence anchors:
  - [section] "Overall, the Gaussian Copula method works better in generating synthetic datasets... Regarding the GaussianCopula model, which is the best-performing one, the original AUC values for DeepFM, DNN, and MaskNet are 98.409%, 98.396%, and 98.329%, respectively. In the best-performing scenario (S3), the AUC values increase to 98.480% for DeepFM, 98.510% for DNN, and 98.447% for MaskNet."
  - [section] "Table 1 shows the number of identical samples within the real dataset. Clearly, it can be realized that SMOTEN copies most of the samples identically by comparing the other methods."

### Mechanism 3
- Claim: Using synthetic data generation to augment the training set can effectively mitigate the impact of data sparsity and improve CTR prediction accuracy.
- Mechanism: Data augmentation increases the effective size and diversity of the training dataset, helping models learn more robust patterns and reducing overfitting to limited training samples.
- Core assumption: The original dataset is sparse and contains insufficient samples to train a robust model, and synthetic samples can fill this gap without introducing harmful noise.
- Evidence anchors:
  - [abstract] "Our results show that the inclusion of generated negative samples consistently improves the Area Under the Curve (AUC) scores."
  - [section] "The findings suggest that generating synthetic negative samples is more effective than positive samples in improving recommender system performance."

## Foundational Learning

- Concept: Imbalanced classification datasets
  - Why needed here: Understanding class imbalance is critical because the study focuses on datasets with predominantly negative interactions, which bias the model toward popular items.
  - Quick check question: In an imbalanced dataset with 99% negative and 1% positive samples, which class will the model tend to predict if not properly balanced?

- Concept: Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs)
  - Why needed here: The study uses CTGAN, CopulaGAN, and TV AE, which are GAN and VAE-based methods for synthetic data generation. Understanding these architectures helps in interpreting results.
  - Quick check question: What is the main difference between how GANs and VAEs generate synthetic data?

- Concept: Click-Through Rate (CTR) prediction and evaluation metrics like AUC
  - Why needed here: The study evaluates recommender models using AUC to measure how well they predict user clicks, so understanding CTR prediction and AUC is essential.
  - Quick check question: What does an AUC score of 0.5 indicate about a model's ability to distinguish between clicked and non-clicked items?

## Architecture Onboarding

- Component map: Data Generation Layer (SMOTEN, CTGAN, CopulaGAN, TV AE, Gaussian Copula, TabDDPM) -> Data Augmentation Pipeline -> Model Training Layer (DNN, DeepFM, MaskNet) -> Evaluation Layer (AUC score computation)

- Critical path: 1. Generate synthetic data using chosen method 2. Apply constraints to maintain data distribution integrity 3. Integrate synthetic samples into original dataset 4. Train CTR prediction model 5. Evaluate model performance using AUC

- Design tradeoffs:
  - Memory vs. diversity: Chunking the dataset for SMOTEN reduces memory usage but may limit diversity
  - Realism vs. privacy: Methods like SMOTEN produce nearly identical samples, failing privacy goals but achieving high AUC
  - Complexity vs. performance: More complex models like Gaussian Copula may yield better results but require more computation

- Failure signatures:
  - Low AUC scores on synthetic-only datasets indicate poor synthetic data quality
  - Identical samples in synthetic datasets suggest overfitting or lack of diversity
  - High computational cost without performance gain indicates inefficiency

- First 3 experiments:
  1. Compare AUC scores when adding synthetic negative vs. positive samples in small increments (e.g., 10%, 20%)
  2. Evaluate synthetic data quality by computing similarity metrics between synthetic and original datasets
  3. Test model performance when trained on synthetic-only data to assess synthetic data realism

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do synthetic negative samples consistently improve AUC scores while synthetic positive samples do not, despite the original dataset being highly imbalanced (CTR rate of 0.33)?
- Basis in paper: [explicit] The paper reports that "generating more positive samples did not improve the AUC for any method and model" while "adding synthetic negative samples does increase the AUC score."
- Why unresolved: The paper does not explain the underlying reasons for this asymmetry in performance. It could be related to how negative samples affect the model's ability to distinguish between classes, or how the models handle class imbalance during training.
- What evidence would resolve it: A detailed analysis of how the model's decision boundaries change with different proportions of synthetic positive versus negative samples, including examination of the loss function behavior and feature importance distributions across classes.

### Open Question 2
- Question: How generalizable are these findings across different recommendation domains beyond CTR prediction, such as movie or product recommendations?
- Basis in paper: [inferred] The paper acknowledges that "the generalizability of our findings across different domains and datasets should be explored" and that "further experimentation on a broader range of datasets and recommendation scenarios will provide a more comprehensive understanding."
- Why unresolved: The experiments were conducted on a single dataset (Frappe), and the paper does not test whether the observed effects hold for other recommendation tasks or domains with different data characteristics.
- What evidence would resolve it: Replicating the experiments across multiple domains (e.g., e-commerce, streaming platforms, news recommendations) with varying levels of class imbalance and different feature types.

### Open Question 3
- Question: What is the optimal ratio of synthetic negative samples to original data that maximizes recommendation quality without introducing excessive noise or overfitting?
- Basis in paper: [explicit] The paper tests 25% and 50% ratios of synthetic negative samples but does not explore the full spectrum of possible ratios or their diminishing returns.
- Why unresolved: The paper only evaluates two specific ratios (25% and 50%) and does not investigate whether higher ratios provide additional benefits or if there is an optimal point beyond which performance degrades.
- What evidence would resolve it: A systematic study varying the synthetic negative sample ratio from 0% to 200% in smaller increments, measuring both AUC and other recommendation quality metrics like diversity and novelty at each level.

## Limitations

- The study is based solely on the Frappe dataset, limiting generalizability to other recommendation scenarios
- The paper doesn't explore the trade-off between synthetic data quality and computational cost
- Long-term effects of synthetic data on model performance over multiple training epochs are not examined

## Confidence

- Mechanism 1 (Negative sample benefits): High - Results are consistent across all scenarios and models
- Mechanism 2 (Gaussian Copula effectiveness): Medium - Best performance observed but may be dataset-specific
- Mechanism 3 (Data sparsity mitigation): Medium - Improvements shown but long-term generalization effects need validation

## Next Checks

1. Replicate experiments across multiple datasets with varying levels of imbalance to test generalizability
2. Conduct ablation studies to determine the optimal ratio of synthetic to real data
3. Evaluate model performance over extended training periods to assess synthetic data stability