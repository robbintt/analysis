---
ver: rpa2
title: 'More Benefits of Being Distributional: Second-Order Bounds for Reinforcement
  Learning'
arxiv_id: '2402.07198'
source_url: https://arxiv.org/abs/2402.07198
tags:
- learning
- bounds
- second-order
- bound
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proves that Distributional Reinforcement Learning (DistRL),
  which learns the return distribution, achieves second-order bounds in both online
  and offline RL with function approximation. The key idea is to use Maximum Likelihood
  Estimation (MLE) to learn the return distribution, which enables tighter variance-dependent
  bounds compared to previously known small-loss bounds.
---

# More Benefits of Being Distributional: Second-Order Bounds for Reinforcement Learning

## Quick Facts
- arXiv ID: 2402.07198
- Source URL: https://arxiv.org/abs/2402.07198
- Reference count: 40
- Primary result: DistRL achieves second-order bounds in online/offline RL and contextual bandits via MLE on return distributions

## Executive Summary
This paper establishes that Distributional Reinforcement Learning (DistRL) based on Maximum Likelihood Estimation (MLE) achieves second-order regret and PAC bounds in both online and offline RL settings with function approximation. The key insight is that learning the full return distribution, rather than just expected returns, enables tighter variance-dependent performance guarantees compared to traditional squared loss methods. The authors prove that DistRL attains second-order regret bounds scaling with the variance of returns for low-rank MDPs and general MDPs with low distributional eluder dimension, while also achieving the first second-order PAC bounds in offline RL under single-policy coverage.

## Method Summary
The core approach uses MLE to construct confidence sets over return distributions, which are then used to implement optimism (for online RL) or pessimism (for offline RL) strategies. In contextual bandits, the algorithm modifies width computation via disagreement maximization to efficiently induce optimism. The method relies on distributional Bellman completeness assumptions and bounded distributional eluder dimension to enable generalization bounds. The confidence sets are built using log-likelihood on historical data, and performance is analyzed through triangular discrimination bounds that connect distributional closeness to variance-based regret guarantees.

## Key Results
- DistRL achieves second-order regret bounds scaling with variance in online RL for low-rank MDPs and MDPs with low distributional eluder dimension
- DistRL obtains first second-order PAC bounds in offline RL under single-policy coverage
- Distributional optimistic contextual bandit algorithm outperforms squared loss baselines on real-world datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MLE on return distributions yields tighter variance-dependent bounds than squared loss methods
- Mechanism: MLE optimizes log-likelihood on conditional return distributions, and triangular discrimination bounds allow converting distributional closeness into variance-based performance guarantees
- Core assumption: The true return distribution lies within the function class F, and the distributional Bellman completeness assumption holds
- Evidence anchors: [abstract] states "DistRL based on MLE results in small-loss bounds... we prove that DistRL actually attains tighter second-order bounds"; [section 2] shows "by simply learning the return distribution with MLE, one can obtain general variance-dependent bounds"
- Break condition: If distributional Bellman completeness fails, MLE may not converge properly and the variance-dependent bounds would not hold

### Mechanism 2
- Claim: Distributional confidence sets enable optimism and pessimism strategies that achieve second-order bounds in online and offline RL
- Mechanism: The confidence set ConfSetRL contains all functions β-near-optimal in log-likelihood, and selecting optimistic/pessimistic functions within this set ensures the algorithm's performance tracks the variance of the comparator policy
- Core assumption: The function class F has bounded distributional eluder dimension, allowing generalization bounds via pigeon-hole arguments
- Evidence anchors: [section 5] describes "The O-DISCO algorithm... selects the optimistic f(k) in the confidence set Fk at each round"; [section 6] explains "The algorithm we study is P-DISCO... which adapts the pessimism-over-confidence-set approach from BCP"
- Break condition: If the distributional eluder dimension grows too quickly with the horizon H, the confidence set becomes too loose and the second-order bounds deteriorate

### Mechanism 3
- Claim: In contextual bandits (H=1), distributional optimism algorithms can be efficiently implemented via width computation, achieving better empirical performance than squared loss baselines
- Mechanism: Instead of optimizing over the entire confidence set, optimism is induced by subtracting the width wk(x,a) computed via disagreement maximization, making the algorithm tractable with neural networks
- Core assumption: The width can be accurately estimated using gradient ascent on the disagreement objective, and the neural network architecture can represent the return distributions adequately
- Evidence anchors: [section 7] states "We modify the width computation strategy of Feng et al. [2021] to deal with the log-likelihood loss"; [section 7] shows "Table 7 shows that cost distribution learning in DistUCB consistently improves the costs and regret compared to the baseline squared loss method RegCB"
- Break condition: If the width computation fails to accurately capture the confidence set boundaries, optimism may not be properly induced and performance could degrade

## Foundational Learning

- Concept: Distributional Bellman Operator
  - Why needed here: The distributional Bellman operator T^D is the key operator that DistRL algorithms must approximate, and its properties determine whether MLE can succeed
  - Quick check question: How does the distributional Bellman operator differ from the standard Bellman operator in terms of input and output types?

- Concept: Triangular Discrimination
  - Why needed here: Triangular discrimination provides the critical bridge between distributional closeness and variance-based performance bounds, enabling the second-order regret analysis
  - Quick check question: What is the relationship between triangular discrimination and Hellinger distance, and why is this relationship important for the analysis?

- Concept: Distributional Eluder Dimension
  - Why needed here: The distributional eluder dimension measures the complexity of the function class F in terms of how quickly the algorithm can generalize from past data, which directly affects the confidence set size and bound constants
  - Quick check question: How does the distributional eluder dimension relate to the standard eluder dimension, and why is the ℓ1 version specifically used here?

## Architecture Onboarding

- Component map: Data Collection -> MLE Confidence Set Construction -> Optimism/Pessimism Selection -> Policy Execution -> New Data Collection
- Critical path:
  1. Collect state-action-cost data
  2. Construct MLE confidence set
  3. Select optimistic/pessimistic function
  4. Execute policy and collect new data
  5. Repeat until convergence
- Design tradeoffs:
  - Stronger distributional completeness assumption vs. weaker standard Bellman completeness
  - Higher function class complexity (distributions vs. functions) vs. tighter second-order bounds
  - Computational cost of confidence set optimization vs. myopic exploration strategies
- Failure signatures:
  - Divergence during MLE optimization
  - Confidence set becomes empty or contains poor approximations
  - Performance degrades when distributional eluder dimension grows too quickly
  - Width computation fails to capture true confidence set boundaries
- First 3 experiments:
  1. Implement DistUCB with width computation on a simple 2-armed bandit with known distributions
  2. Test O-DISCO on a low-rank MDP with discrete cost distributions to verify second-order bounds
  3. Compare DistUCB vs RegCB on a real-world CB dataset to verify empirical improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can distributional RL algorithms be extended to obtain higher-order bounds (beyond second-order) for reinforcement learning?
- Basis in paper: [explicit] The authors state "An interesting direction is to show whether DistRL can obtain even higher-order bounds than second-order."
- Why unresolved: The paper focuses on proving second-order bounds and does not explore the possibility of achieving higher-order bounds.
- What evidence would resolve it: A theoretical proof or empirical demonstration that DistRL can achieve bounds of order higher than two, such as third-order or fourth-order bounds, would resolve this question.

### Open Question 2
- Question: Can the distributional confidence sets used in O-DISCO and P-DISCO be efficiently computed for large function classes?
- Basis in paper: [explicit] The authors mention that "Both O-DISCO and P-DISCO optimize over the confidence set to ensure optimism and pessimism, respectively, but this step is known to be computationally hard even in tabular MDPs."
- Why unresolved: The paper acknowledges the computational challenge but does not provide a solution or analysis of the computational complexity for large function classes.
- What evidence would resolve it: An analysis of the computational complexity of optimizing over distributional confidence sets for various function classes, along with empirical results on the scalability of these algorithms, would help resolve this question.

### Open Question 3
- Question: How does the performance of distributional RL algorithms compare to variance-weighted regression methods in practice?
- Basis in paper: [inferred] The authors mention that "Compared to variance weighted regression, one drawback of our DistRL approach is the requirement of a stronger, distributional completeness assumption."
- Why unresolved: The paper focuses on the theoretical benefits of DistRL and does not provide a direct comparison with variance-weighted regression methods in terms of practical performance.
- What evidence would resolve it: Empirical results comparing the performance of distributional RL algorithms with variance-weighted regression methods on various benchmark tasks would help resolve this question.

## Limitations
- Distributional Bellman completeness assumption is stronger than standard Bellman completeness
- Confidence set construction relies on bounded distributional eluder dimension that could grow rapidly with horizon
- Empirical validation limited to contextual bandits; no experiments provided for online/offline RL settings
- MLE-based approaches require careful optimization that may not always converge

## Confidence
- Online RL second-order bounds (Medium): Theoretical claims are sound but lack empirical verification
- Offline RL PAC bounds (Medium): First-of-their-kind bounds but rely on strong distributional assumptions
- Contextual bandit empirical results (High): Direct experimental comparisons show clear improvements over baselines
- Theoretical framework (High): Rigorous mathematical proofs for the main claims are provided

## Next Checks
1. Implement O-DISCO on a synthetic low-rank MDP with known cost distributions to verify the second-order regret bounds experimentally
2. Test P-DISCO on a finite offline RL benchmark to validate the first second-order PAC bounds under single-policy coverage
3. Analyze the impact of distributional eluder dimension growth on confidence set tightness across different function classes and horizons