---
ver: rpa2
title: 'Mini Honor of Kings: A Lightweight Environment for Multi-Agent Reinforcement
  Learning'
arxiv_id: '2406.03978'
source_url: https://arxiv.org/abs/2406.03978
tags:
- self
- environment
- hero
- skill
- game
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Mini Honor of Kings (Mini HoK), a lightweight
  multi-agent reinforcement learning (MARL) environment based on the popular mobile
  game Honor of Kings. Mini HoK addresses limitations of existing MARL environments
  by being highly efficient, customizable, and sufficiently challenging.
---

# Mini Honor of Kings: A Lightweight Environment for Multi-Agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2406.03978
- Source URL: https://arxiv.org/abs/2406.03978
- Reference count: 28
- Key outcome: A lightweight, efficient MARL environment based on Honor of Kings that challenges existing algorithms while running on personal computers

## Executive Summary
This paper introduces Mini Honor of Kings (Mini HoK), a lightweight multi-agent reinforcement learning environment derived from the popular mobile game Honor of Kings. Mini HoK addresses limitations of existing MARL environments by providing a highly efficient, customizable platform that can run on personal computers while still presenting significant challenges for current MARL algorithms. The environment features a map editor for scenario customization, efficient execution without rendering requirements, and various modes with different hero configurations and difficulty levels.

The authors evaluate several common MARL algorithms (VDN, QMIX, QATTEN, QPLEX, MAPPO, HAPPO) in Mini HoK and find that these algorithms have not yet found optimal solutions, even when compared to simple heuristic methods. This demonstrates the environment's potential for driving future MARL research. The environment achieves approximately 0.5 million samples per hour using a single CPU core and a single NVIDIA GeForce RTX 4090 GPU, making it accessible for research on standard hardware.

## Method Summary
Mini HoK provides a lightweight MARL environment that replicates core gameplay elements from Honor of Kings while removing rendering requirements and optimizing for computational efficiency. The environment includes a map editor for customizing scenarios and agent properties, multiple difficulty levels, and various hero configurations. The platform supports standard MARL algorithm evaluation through its Gym interface, allowing researchers to test algorithms in a controlled, reproducible setting. The environment's efficiency enables rapid experimentation on personal computers, with approximately 0.5 million samples per hour achievable on standard hardware configurations.

## Key Results
- Existing MARL algorithms (VDN, QMIX, QATTEN, QPLEX, MAPPO, HAPPO) fail to find optimal solutions in Mini HoK, even when compared to simple heuristic methods
- The environment achieves approximately 0.5 million samples per hour using a single CPU core and a single NVIDIA GeForce RTX 4090 GPU
- Mini HoK provides a challenging yet efficient testbed for MARL research, enabling experiments on personal computers that would typically require more substantial computational resources

## Why This Works (Mechanism)
Mini HoK works by providing a simplified yet strategically rich environment that captures essential elements of multi-agent competition while removing computationally expensive rendering requirements. The environment maintains sufficient complexity through its hero diversity, map configurations, and team-based objectives to challenge current MARL algorithms. The lightweight design enables rapid iteration and experimentation, while the customizable map editor allows researchers to systematically vary difficulty and scenario complexity. The combination of strategic depth and computational efficiency creates an ideal testbed for advancing MARL research.

## Foundational Learning
- Multi-agent reinforcement learning fundamentals: Understanding how multiple agents learn to cooperate or compete in shared environments, essential for designing and evaluating algorithms in Mini HoK
- MARL algorithm architectures (VDN, QMIX, MAPPO, etc.): Knowledge of different algorithmic approaches for handling multi-agent coordination, needed to implement and test algorithms in the environment
- Environment customization and scenario design: Ability to modify maps and agent properties using the editor, required for creating diverse experimental conditions and curriculum learning scenarios
- Performance evaluation metrics for MARL: Understanding how to measure and compare algorithm performance, necessary for conducting meaningful experiments and drawing valid conclusions

## Architecture Onboarding
- Component map: Mini HoK -> Map Editor -> Algorithm Interface -> Performance Metrics -> Analysis Tools
- Critical path: Algorithm training -> Environment interaction -> Performance evaluation -> Result analysis
- Design tradeoffs: Efficiency vs. complexity - simplified graphics and physics for faster execution vs. maintaining strategic depth; customization flexibility vs. implementation complexity
- Failure signatures: Algorithm performance plateaus below heuristic baselines, indicating insufficient coordination or exploration strategies; training instability or convergence issues suggesting algorithmic limitations
- 3 first experiments:
  1. Baseline algorithm comparison: Run VDN, QMIX, and MAPPO with default hyperparameters to establish performance baselines
  2. Heuristic method benchmark: Implement and evaluate simple rule-based strategies to establish performance upper bounds
  3. Difficulty scaling: Test algorithm performance across different difficulty levels to identify robustness to environmental complexity

## Open Questions the Paper Calls Out
None

## Limitations
- Limited hyperparameter tuning and random seed usage in baseline algorithm comparisons, potentially affecting the robustness of performance claims
- Claims about algorithm difficulty relative to heuristic methods require additional statistical validation to rule out implementation differences
- The environment's long-term research utility and broader applicability to other MARL domains remain to be fully established

## Confidence
- High confidence in efficiency claims (1.5M samples/hour) based on specific hardware metrics
- Medium confidence in baseline algorithm comparisons due to limited hyperparameter exploration
- Medium confidence in difficulty assessment pending broader algorithm testing

## Next Checks
1. Conduct statistical significance tests across multiple random seeds for all baseline algorithm comparisons
2. Test additional MARL algorithms beyond the initial set to establish broader baseline performance
3. Document hyperparameter ranges and optimization procedures for fair algorithm comparison