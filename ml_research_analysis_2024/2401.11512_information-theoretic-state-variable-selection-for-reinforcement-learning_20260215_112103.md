---
ver: rpa2
title: Information-Theoretic State Variable Selection for Reinforcement Learning
arxiv_id: '2401.11512'
source_url: https://arxiv.org/abs/2401.11512
tags: []
core_contribution: This paper introduces TERC, an information-theoretic criterion
  for selecting state variables in reinforcement learning. TERC identifies variables
  that transfer entropy to actions during training, provably excluding uninformative
  variables to improve sample efficiency.
---

# Information-Theoretic State Variable Selection for Reinforcement Learning

## Quick Facts
- arXiv ID: 2401.11512
- Source URL: https://arxiv.org/abs/2401.11512
- Reference count: 40
- This paper introduces TERC, an information-theoretic criterion for selecting state variables in reinforcement learning.

## Executive Summary
This paper presents TERC (Transfer Entropy Redundancy Criterion), a novel information-theoretic approach for selecting state variables in reinforcement learning. TERC identifies variables that transfer entropy to actions during training, provably excluding uninformative variables to improve sample efficiency. The method handles complex redundancies through algorithmic solutions and enhances interpretability by revealing how variable importance evolves during training. Experiments across synthetic data, RL environments (Cart Pole, Lunar Lander, Pendulum), and the Iterated Prisoner's Dilemma demonstrate TERC consistently identifies optimal variable subsets and outperforms existing feature selection methods.

## Method Summary
TERC measures the reduction in uncertainty of action realizations when a state variable is removed, using transfer entropy to quantify information transfer from state variables to actions. The method iteratively selects variables that provide unique information about actions while excluding redundant variables. Algorithm 1 handles perfect conditional multivariate redundancy (CPMCR) by checking subsets of variables, while Algorithm 2 provides a computationally efficient approximation under certain assumptions. The approach uses neural network-based estimation of transfer entropy and includes a null model comparison to validate variable selection decisions.

## Key Results
- TERC consistently identifies optimal variable subsets across synthetic data, OpenAI Gym environments, and Iterated Prisoner's Dilemma
- TERC outperforms existing feature selection methods like UMFI and PI in both sample efficiency and final performance
- The approach enhances interpretability by revealing how variable importance evolves during training

## Why This Works (Mechanism)

### Mechanism 1
TERC identifies state variables that transfer entropy to actions during training, provably excluding uninformative variables to improve sample efficiency. The method measures the reduction in entropy of actions when a variable is removed, retaining variables where this reduction is positive. This works under the assumption that actions depend conditionally on state variables, making entropy reduction a reliable indicator of importance.

### Mechanism 2
TERC handles complex redundancies through algorithmic solutions, ensuring selection of a minimal informative state representation. The method addresses CPMCR by iteratively checking subsets of variables to identify perfectly redundant variables, including only the smallest necessary subset that preserves information about actions. This assumes CPMCR exists and that identifying these redundancies enables minimal representation selection.

### Mechanism 3
TERC enhances interpretability by revealing how variable importance evolves during training. By examining transfer entropy changes at different training stages, the method provides insights into how the agent's strategy changes over time. This assumes that transfer of entropy from state variables to actions changes as the agent learns, allowing inference of evolving strategy.

## Foundational Learning

- **Concept**: Transfer Entropy (TE)
  - Why needed here: TE is the core measure used by TERC to quantify information transfer from state variables to actions
  - Quick check question: Can you explain how TE differs from mutual information and why it's suitable for measuring information transfer in sequential decision-making tasks?

- **Concept**: Perfect Conditional Multivariate Redundancy (CPMCR)
  - Why needed here: CPMCR is a key concept that TERC addresses to ensure selection of a minimal informative state representation
  - Quick check question: Can you describe a scenario where CPMCR might occur in a reinforcement learning problem and how it could impact state variable selection?

- **Concept**: Bayesian Networks
  - Why needed here: Bayesian networks are used to graphically represent directed dependencies between variables, including transfer of entropy from state variables to actions
  - Quick check question: How can Bayesian networks be used to visualize relationships between state variables and actions in a reinforcement learning problem?

## Architecture Onboarding

- **Component map**: State variables (X) -> Transfer Entropy Redundancy Criterion (TERC) -> Actions (A)
- **Critical path**: Train agent using all state variables → Calculate transfer entropy for each variable → Iteratively select minimal informative subset of variables
- **Design tradeoffs**: Main tradeoff is between computational complexity and accuracy of state variable selection. Algorithm 1 is theoretically robust but computationally expensive, while Algorithm 2 is more efficient but requires certain assumptions to be valid
- **Failure signatures**: If TERC fails to accurately identify informative variables, agent performance may degrade. If assumptions for Algorithm 2 are not met, selected state representation may not be optimal
- **First 3 experiments**:
  1. Implement TERC on simple synthetic dataset with known redundancies and synergies to verify ability to identify correct variables
  2. Apply TERC to standard RL environment (e.g., Cart Pole) with added random variables to test ability to exclude uninformative variables
  3. Use TERC to investigate how variable importance evolves during training in more complex environment (e.g., Lunar Lander) to assess interpretability benefits

## Open Questions the Paper Calls Out

### Open Question 1
How does TERC's performance scale with very high-dimensional state spaces (e.g., thousands of variables)? The paper demonstrates TERC on environments with dozens of variables but doesn't test extreme high-dimensional cases. Algorithm 2 claims linear complexity, but this needs empirical validation at scale.

### Open Question 2
Can TERC reliably detect synergistic variable combinations in complex real-world environments beyond XOR-like cases? The paper mentions synergy is "inherently conditional" and discusses XOR as a canonical example, but most experiments focus on redundant variables rather than synergistic interactions.

### Open Question 3
How sensitive is TERC to hyperparameter choices (network architecture, learning rate, batch size) in the neural TE estimator? The paper provides specific hyperparameters for each experiment but doesn't systematically explore sensitivity or provide guidelines for hyperparameter selection.

### Open Question 4
Does TERC maintain its advantages when applied to continuous action spaces beyond the Pendulum environment? The Pendulum environment uses PPO with continuous actions, but this is the only continuous action space tested.

## Limitations
- Computational complexity scales poorly with state dimensionality, particularly for Algorithm 1
- Theoretical guarantees for handling conditional redundancies require further validation in complex, high-dimensional scenarios
- Interpretability claims about revealing variable importance evolution need more rigorous systematic validation

## Confidence

**High confidence**: The core mechanism of using transfer entropy to identify informative state variables is well-grounded in information theory. The experimental results showing consistent performance improvements across multiple RL benchmarks are convincing.

**Medium confidence**: The theoretical guarantees for handling conditional redundancies through algorithmic solutions, particularly the claim of finding minimal informative subsets, requires further validation in complex, high-dimensional scenarios.

**Low confidence**: The interpretability claims about revealing how variable importance evolves during training need more rigorous validation. The paper provides qualitative observations but lacks systematic analysis of how these insights translate to actionable understanding of agent behavior.

## Next Checks

1. **Scalability analysis**: Test TERC on high-dimensional state spaces (e.g., Atari games) to evaluate computational tractability and identify breaking points where the method becomes impractical.

2. **Robustness to noise**: Evaluate TERC's performance when state variables contain varying levels of noise or when the relationship between states and actions is non-monotonic, to test the robustness of transfer entropy as a selection criterion.

3. **Interpretability validation**: Design experiments where ground truth variable importance is known (through controlled environments) and systematically compare TERC's variable importance trajectories against these ground truths to quantify interpretability benefits.