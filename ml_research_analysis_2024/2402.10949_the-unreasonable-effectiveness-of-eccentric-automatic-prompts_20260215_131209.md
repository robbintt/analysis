---
ver: rpa2
title: The Unreasonable Effectiveness of Eccentric Automatic Prompts
arxiv_id: '2402.10949'
source_url: https://arxiv.org/abs/2402.10949
tags:
- none
- math
- following
- answer
- solve
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that seemingly trivial prompt variations
  can dramatically impact large language model (LLM) performance. The authors tested
  60 hand-crafted "positive thinking" system message combinations on the GSM8K math
  dataset, using three models (7B, 13B, and 70B parameters) with and without Chain
  of Thought prompting.
---

# The Unreasonable Effectiveness of Eccentric Automatic Prompts

## Quick Facts
- **arXiv ID**: 2402.10949
- **Source URL**: https://arxiv.org/abs/2402.10949
- **Reference count**: 40
- **Primary result**: Automatic prompt optimization consistently outperforms hand-crafted positive thinking prompts across LLM sizes, with optimized prompts often appearing counterintuitive to humans.

## Executive Summary
This paper investigates the impact of seemingly trivial prompt variations on large language model (LLM) performance, specifically testing 60 hand-crafted "positive thinking" system message combinations on the GSM8K math dataset. The authors tested three models (7B, 13B, and 70B parameters) with and without Chain of Thought prompting, finding that no universal prompt worked across all models. Most models showed performance improvements from positive prompts, except Llama2-70B without Chain of Thought which performed best with no system message at all. To address the computational complexity of manual prompt tuning, they compared the best positive prompts against automatically optimized prompts generated by DSPy, finding that the automatically optimized prompts consistently outperformed hand-crafted ones and showed better generalization, despite appearing unusual and counterintuitive to human practitioners.

## Method Summary
The study tested 60 combinations of system message snippets (openers, task descriptions, closers) on GSM8K dataset subsets (10, 25, 50, 100 questions) across three models (Mistral-7B, Llama2-13B, Llama2-70B) with and without Chain of Thought prompting. System messages were constructed from combinations of 5 openers, 3 task descriptions, and 4 closers. Exact Match (EM) accuracy was used as the primary metric. The authors also employed DSPy for automatic prompt optimization, using separate optimization and evaluation sets. In-context learning was employed with 4 examples from the test set for each evaluation.

## Key Results
- Positive thinking prompts generally improved LLM performance on math tasks, except for Llama2-70B without Chain of Thought which performed best with no system message
- Automatic prompt optimization using DSPy consistently outperformed hand-crafted positive thinking prompts across all tested conditions
- The highest-scoring automatically optimized prompt for Llama2-70B-50B included Star Trek-themed content, demonstrating that effective prompts can be highly unconventional
- No universal prompt worked across all models, with performance highly sensitive to model size, Chain of Thought usage, and prompt structure

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Positive thinking prompts generally improve LLM performance on math tasks, except in specific model-chain of thought combinations.
- Mechanism: The model's internal reasoning processes are influenced by contextual framing in the system message, which affects confidence and attention allocation during problem-solving.
- Core assumption: LLMs have emergent reasoning capabilities that can be modulated by affective language patterns.
- Evidence anchors:
  - [abstract] "In most instances, the inclusion of 'positive thinking' prompts positively affected model performance."
  - [section 4.1] "In contrast, the results for 100 questions in Appendix B.4 demonstrate a reasonable spread between 0.08 and 0.11. In relative terms, Mistral-7B, when prompted without Chain of Thought, exhibits substantial prompt invariance, with the 'positive thinking' prompts only matching or marginally surpassing the baseline."
  - [corpus] Weak - no direct citations found in neighbors about positive thinking effects specifically.
- Break condition: When Chain of Thought is disabled and the model is Llama2-70B, positive prompts underperformed baseline, suggesting context sensitivity.

### Mechanism 2
- Claim: Automatic prompt optimization outperforms manual prompt engineering across different model sizes.
- Mechanism: DSPy's optimization algorithms explore prompt space more systematically than human intuition, discovering non-obvious patterns that improve both performance and generalization.
- Core assumption: LLMs can optimize their own prompts more effectively than humans can manually craft them.
- Evidence anchors:
  - [abstract] "We show that employing an automated prompt optimizer emerges as the most effective method for enhancing performance, even when working with smaller open-source models."
  - [section 4.4] "As anticipated, the prompts that underwent automatic optimization consistently equaled or surpassed the effectiveness of our manually generated 'positive thinking' prompts in nearly all instances."
  - [corpus] Strong - multiple neighbors focus on automatic prompt optimization frameworks.
- Break condition: Mistral-7B showed mixed results, with automatic optimization only clearly superior at 100 questions, suggesting model size and optimization data requirements matter.

### Mechanism 3
- Claim: Optimized prompts can appear counterintuitive to humans but still perform well.
- Mechanism: The optimization process discovers patterns that work for the model's internal representations but don't align with human expectations of what constitutes a "good" prompt.
- Core assumption: Model optimization operates in a different feature space than human intuition.
- Evidence anchors:
  - [abstract] "Additionally, our findings reveal that the highest-scoring, automatically-optimized prompt exhibits a degree of peculiarity far beyond expectations."
  - [section 4.5] "A prime example is illustrated by the highest-scoring optimized prompt and prefix generated by Llama2-70B for the 50-question subset: System Message: 'Command, we need you to plot a course through this turbulence and locate the source of the anomaly. Use all available data and your expertise to guide us through this challenging situation.'"
  - [corpus] Weak - no direct corpus evidence about counterintuitive prompts, but automatic optimization literature supports this.
- Break condition: When human practitioners can intuit effective patterns, manual optimization may still be viable.

## Foundational Learning

- Concept: Chain of Thought prompting
  - Why needed here: The paper compares performance with and without CoT, showing dramatically different effects of positive thinking prompts
  - Quick check question: What is the primary mechanism by which Chain of Thought improves LLM reasoning performance?

- Concept: Exact Match (EM) scoring
  - Why needed here: The paper uses strict EM scoring for math problems, which requires precise numerical answers without partial credit
  - Quick check question: Why would string formatting differences (e.g., "30000" vs "30,000") matter for EM scoring in this context?

- Concept: Prompt engineering combinatorial complexity
  - Why needed here: The paper demonstrates why manual prompt optimization is computationally prohibitive and motivates automatic approaches
  - Quick check question: How many total prompt combinations were tested per model in this study?

## Architecture Onboarding

- Component map: Model (7B/13B/70B parameters) -> Prompt (system message + examples) -> DSPy optimizer -> GSM8K evaluation
- Critical path: Prompt generation -> Model inference -> Answer parsing -> EM scoring -> Performance analysis
- Design tradeoffs: Manual prompt engineering (interpretable but slow) vs automatic optimization (fast but potentially unintuitive)
- Failure signatures: Performance variance across models, prompt sensitivity differences, reproducibility issues
- First 3 experiments:
  1. Test baseline performance with no system message vs simple positive thinking prompts on Mistral-7B with CoT
  2. Compare manual vs automatic optimization on Llama2-13B with 10-question subset
  3. Validate reproducibility by running the same prompt on different model instances

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms make "positive thinking" prompts effective for some models but not others?
- Basis in paper: [explicit] The paper demonstrates varying effectiveness of positive thinking prompts across different models (Mistral-7B, Llama2-13B, Llama2-70B) and conditions.
- Why unresolved: The paper shows that positive thinking prompts work for most models but fails to explain the underlying reasons for this variation in effectiveness across different models and configurations.
- What evidence would resolve it: Controlled experiments isolating specific linguistic features, sentiment analysis of model responses, and ablation studies on prompt components could reveal why certain positive prompts work better for specific models.

### Open Question 2
- Question: Why does Llama2-70B without Chain of Thought perform best with no system message at all?
- Basis in paper: [explicit] The paper notes that Llama2-70B without Chain of Thought showed optimal performance with no system message, which contradicts findings for other models and configurations.
- Why unresolved: The paper identifies this anomaly but doesn't explore the reasons behind this counterintuitive finding or what it reveals about Llama2-70B's architecture or training.
- What evidence would resolve it: Comparative analysis of model architecture, attention patterns, and internal representations when processing empty vs. filled system messages could explain this phenomenon.

### Open Question 3
- Question: How do the unusual, Star Trek-themed optimized prompts achieve superior performance?
- Basis in paper: [explicit] The paper highlights that the highest-scoring automatically optimized prompt for Llama2-70B-50B contained Star Trek-themed content and outperformed hand-crafted prompts.
- Why unresolved: While the paper demonstrates the effectiveness of these unconventional prompts, it doesn't investigate what aspects of the Star Trek theme or narrative structure contribute to improved mathematical reasoning.
- What evidence would resolve it: Comparative testing of various themed prompts (different genres, narrative structures) and analysis of model activation patterns when processing themed vs. conventional prompts could reveal the mechanism behind their effectiveness.

### Open Question 4
- Question: What is the relationship between model size and the effectiveness of automatic prompt optimization?
- Basis in paper: [explicit] The paper shows that automatic optimization works better for larger models (Llama2-13B and 70B) compared to the 7B model, but doesn't explain why.
- Why unresolved: The paper demonstrates a correlation between model size and optimization effectiveness but doesn't investigate the underlying reasons or establish causal mechanisms.
- What evidence would resolve it: Systematic testing across a wider range of model sizes, analysis of optimization algorithm convergence rates, and investigation of how model capacity affects prompt representation could clarify this relationship.

### Open Question 5
- Question: What causes the reproducibility problem in LLM performance across different implementations?
- Basis in paper: [explicit] The paper documents significant discrepancies between published scores and their own results for Mistral-7B and Llama2-13B, highlighting a broader reproducibility crisis.
- Why unresolved: While the paper identifies this issue, it doesn't investigate the root causes such as training data differences, evaluation protocols, or implementation variations that lead to these inconsistencies.
- What evidence would resolve it: Detailed comparison of training datasets, hyper-parameters, evaluation methodologies, and model architectures across different implementations could identify specific factors contributing to reproducibility issues.

## Limitations

- The GSM8K dataset represents a narrow domain where positive thinking prompts may have outsized effects compared to general-purpose language tasks
- Only 60 prompt variations were tested, representing a small fraction of possible prompt space that may have missed effective patterns
- The automatic optimization results were only tested on GSM8K and may not generalize to other domains or tasks
- Hand-crafted positive thinking prompts were not systematically generated, potentially missing effective prompt patterns

## Confidence

**High Confidence**: The core finding that prompt variations significantly impact LLM performance is well-supported by the experimental results across multiple models and conditions. The comparison between manual and automatic optimization shows consistent patterns that are unlikely to be due to chance.

**Medium Confidence**: The mechanism explanations for why positive thinking prompts work (affective framing influencing reasoning) and why automatic optimization succeeds (systematic exploration of prompt space) are plausible but not directly proven. These remain theoretical interpretations of observed correlations.

**Low Confidence**: The generalizability of these findings to other domains, model families, or larger model sizes beyond what was tested. The Star Trek-themed prompt that worked well for Llama2-70B may be an outlier specific to that model and task combination.

## Next Checks

1. **Cross-Domain Validation**: Test the same prompt optimization approach on non-math datasets (e.g., commonsense reasoning, reading comprehension) to determine if positive thinking prompts and automatic optimization generalize beyond GSM8K.

2. **Ablation Study on Prompt Components**: Systematically remove or modify individual components of the highest-performing automatic prompts to identify which specific elements drive performance improvements versus those that are incidental.

3. **Larger Scale Automatic Optimization**: Run DSPy optimization with larger optimization sets (e.g., 500-1000 examples rather than the paper's approach) to determine if performance scales with optimization data and whether the unintuitive prompts persist at larger scales.