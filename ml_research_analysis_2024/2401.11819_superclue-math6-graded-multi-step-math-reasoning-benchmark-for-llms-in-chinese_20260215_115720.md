---
ver: rpa2
title: 'SuperCLUE-Math6: Graded Multi-Step Math Reasoning Benchmark for LLMs in Chinese'
arxiv_id: '2401.11819'
source_url: https://arxiv.org/abs/2401.11819
tags:
- reasoning
- score
- steps
- accuracy
- chinese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SuperCLUE-Math6 (SC-Math6), a new Chinese
  benchmark dataset for evaluating the mathematical reasoning abilities of large language
  models (LLMs). SC-Math6 is designed as an upgraded Chinese version of the GSM8K
  dataset with enhanced difficulty, diversity, and application scope.
---

# SuperCLUE-Math6: Graded Multi-Step Math Reasoning Benchmark for LLMs in Chinese

## Quick Facts
- arXiv ID: 2401.11819
- Source URL: https://arxiv.org/abs/2401.11819
- Authors: Liang Xu; Hang Xue; Lei Zhu; Kangkang Zhao
- Reference count: 10
- Primary result: Introduces SC-Math6, a Chinese benchmark with 2144 multi-step math problems that stratifies LLM performance by reasoning complexity

## Executive Summary
This paper introduces SuperCLUE-Math6 (SC-Math6), a new Chinese benchmark dataset for evaluating the mathematical reasoning abilities of large language models (LLMs). SC-Math6 is designed as an upgraded Chinese version of the GSM8K dataset with enhanced difficulty, diversity, and application scope. It consists of over 2000 mathematical word problems requiring multi-step reasoning and providing natural language solutions. The authors propose an innovative scheme to quantify the reasoning capability of LLMs based on performance over problems with different reasoning steps. Experiments on 12 representative Chinese models demonstrate a clear stratification of reasoning levels, with top models like GPT-4 showing superior performance.

## Method Summary
SC-Math6 is constructed as a Chinese mathematical reasoning benchmark with 2144 problems (1072 pairs) requiring multi-step solutions. The benchmark employs a dual-scoring system combining Reasoning Steps Score (weighted by problem complexity) and Overall Accuracy Score to provide comprehensive evaluation. Models are assessed on both initial problems and follow-up questions to evaluate sustained reasoning ability. The dataset is designed to address limitations in existing Chinese math benchmarks by incorporating greater difficulty variation and multi-turn interaction capabilities.

## Key Results
- SC-Math6 demonstrates clear stratification of reasoning levels across 12 Chinese models, with GPT-4 showing superior performance
- Multi-turn interactions reveal performance degradation from turn 1 to turn 2 across all models, indicating limitations in sustained reasoning
- The weighted scoring system effectively differentiates between models' abilities to handle simple versus complex problems
- SC-Math6 fills the gap in Chinese mathematical reasoning benchmarks by providing enhanced difficulty and diversity compared to existing datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The multi-step reasoning benchmark effectively stratifies model performance by problem complexity.
- Mechanism: By assigning weighted scores based on the number of reasoning steps, the benchmark differentiates between models' abilities to handle simple versus complex problems. This mechanism allows for a more nuanced evaluation compared to simple accuracy metrics.
- Core assumption: The number of reasoning steps correlates with problem difficulty, and models that perform well on multi-step problems demonstrate superior reasoning capabilities.
- Evidence anchors:
  - [abstract] "We propose an innovative scheme to quantify the reasoning capability of large models based on performance over problems with different reasoning steps."
  - [section] "The advantage of the Reasoning Step Score lies in its ability to account for the varying difficulty levels of different questions."
- Break condition: If the number of reasoning steps does not correlate with problem difficulty, or if models can solve multi-step problems without true reasoning (e.g., through pattern matching).

### Mechanism 2
- Claim: The multi-turn interaction format reveals model performance degradation over extended reasoning tasks.
- Mechanism: By providing follow-up questions for each initial problem, the benchmark assesses models' ability to maintain reasoning consistency throughout an interaction. The observed decline in accuracy from turn 1 to turn 2 indicates limitations in sustained reasoning.
- Core assumption: Models' performance on initial questions predicts their ability to handle subsequent related questions in a conversation.
- Evidence anchors:
  - [section] "In all models observed, the accuracy rate of the second iteration generally falls below that of the first, indicating a decline in model performance with increasing task complexity."
  - [section] "This trend is ubiquitous across all models, suggesting that special attention should be given to the stability and adaptability of models in sustained tasks during their design and optimization processes."
- Break condition: If models can maintain or improve performance in multi-turn interactions, or if the follow-up questions do not effectively test sustained reasoning.

### Mechanism 3
- Claim: The comprehensive scoring system balances precision and fairness in evaluating mathematical reasoning.
- Mechanism: By combining the Reasoning Steps Score (which accounts for problem difficulty) and the Overall Accuracy Score (which ensures fairness), the benchmark produces a more comprehensive evaluation of model performance than either metric alone.
- Core assumption: Both the precision offered by the Reasoning Steps Score and the fairness ensured by the Overall Accuracy Score are necessary for a complete assessment of mathematical reasoning.
- Evidence anchors:
  - [section] "Therefore, we have not completely discarded the Overall Accuracy Score. Instead, we employ a weighted summation of Reasoning Steps Score and Overall Accuracy Score to calculate the unified score."
  - [section] "This method balances the precision given by the Reasoning Steps Score with the fairness ensured by the Overall Accuracy Score, aiming to provide a more comprehensive evaluation of mathematical reasoning performance."
- Break condition: If one of the scores (Reasoning Steps or Overall Accuracy) becomes irrelevant or misleading in evaluating mathematical reasoning.

## Foundational Learning

- Concept: Multi-step mathematical reasoning
  - Why needed here: The benchmark is designed to evaluate models' abilities to solve problems requiring multiple reasoning steps, which is a key aspect of mathematical intelligence.
  - Quick check question: Can you explain how a multi-step math problem differs from a simple arithmetic problem?

- Concept: Benchmark design and evaluation metrics
  - Why needed here: Understanding how benchmarks are constructed and how different evaluation metrics work is crucial for interpreting the results and limitations of the SC-Math6 benchmark.
  - Quick check question: What are the advantages and disadvantages of using accuracy as a sole evaluation metric for mathematical reasoning?

- Concept: Chinese language processing in LLMs
  - Why needed here: As SC-Math6 is a Chinese benchmark, knowledge of how LLMs handle Chinese language is important for understanding the context and potential challenges in the evaluation.
  - Quick check question: How might the structure of the Chinese language affect the performance of LLMs on mathematical reasoning tasks?

## Architecture Onboarding

- Component map: Dataset (2144 problems) -> Scoring System (Reasoning Steps + Overall Accuracy) -> Model Evaluation Framework -> Reasoning Level Stratification
- Critical path: Problem annotation → Reasoning step classification → Model evaluation → Score calculation → Performance stratification
- Design tradeoffs: Balances comprehensive evaluation (multi-step problems, multi-turn interactions) with practical constraints (dataset size, evaluation time) and precision (Reasoning Steps Score) with fairness (Overall Accuracy Score)
- Failure signatures: Models struggle with generalization from simple to complex problems, show performance degradation in sustained reasoning tasks, or exhibit inconsistent performance across problem types
- First 3 experiments:
  1. Evaluate a simple baseline model (e.g., rule-based solver) on the benchmark to establish lower bounds for performance
  2. Compare GPT-4 performance with Chinese-specific models (e.g., Ernie Bot) to assess language-specific training impact
  3. Analyze performance on problems with different reasoning steps to validate weighted scoring system effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed reasoning steps scoring method compare to other difficulty-based scoring methods like question-specific weights or adaptive testing approaches?
- Basis in paper: explicit - The paper mentions that the reasoning steps scoring method accounts for varying difficulty levels, but does not compare it to other methods.
- Why unresolved: The paper does not provide any comparison or evaluation of the proposed scoring method against alternative approaches for handling question difficulty.
- What evidence would resolve it: Conducting experiments to compare the proposed scoring method with other difficulty-based scoring methods on the same dataset and evaluating their effectiveness in terms of fairness, accuracy, and interpretability.

### Open Question 2
- Question: How do the proposed multi-turn follow-up questions impact the assessment of a model's sustained reasoning ability compared to single-turn evaluations?
- Basis in paper: explicit - The paper mentions that multi-turn follow-up questions are designed to assess the model's continuous reasoning ability during interaction with users.
- Why unresolved: The paper does not provide a detailed analysis of how the inclusion of follow-up questions affects the overall evaluation of a model's reasoning skills.
- What evidence would resolve it: Conducting experiments to compare the performance of models on single-turn and multi-turn evaluations, and analyzing the impact of follow-up questions on the assessment of sustained reasoning ability.

### Open Question 3
- Question: What are the specific factors contributing to the observed decline in model performance during multi-turn interactions, and how can they be addressed?
- Basis in paper: explicit - The paper mentions a decline in model performance during multi-turn interactions but does not delve into the underlying reasons or potential solutions.
- Why unresolved: The paper does not provide a detailed analysis of the factors causing the performance decline or discuss potential strategies to mitigate this issue.
- What evidence would resolve it: Conducting a thorough analysis of the factors contributing to the performance decline, such as model architecture, training data, or interaction design, and proposing and evaluating potential solutions to address these issues.

## Limitations

- The evaluation methodology lacks transparency regarding the reasoning step classification process and validation, with no inter-rater reliability metrics provided
- The paper does not provide statistical significance testing for performance differences between models, making it difficult to determine if observed stratification is meaningful
- Practical considerations like computational efficiency and response time are not addressed, limiting real-world deployment relevance

## Confidence

High confidence: The basic construction of SC-Math6 as a Chinese mathematical reasoning benchmark with multi-step problems is well-documented and replicable. The dataset structure and problem format are clearly described.

Medium confidence: The stratification of model performance and the correlation between reasoning steps and difficulty are plausible based on the presented evidence, but lack rigorous statistical validation.

Low confidence: Claims about SC-Math6 being superior to existing benchmarks or filling specific gaps in the evaluation landscape are not well-supported by comparative analysis or systematic benchmarking studies.

## Next Checks

1. **Annotation Reliability Assessment**: Conduct a comprehensive inter-rater reliability study to validate the reasoning step classification system. This should include multiple annotators independently classifying problems and calculating Cohen's kappa or similar agreement metrics to establish the validity of the difficulty stratification.

2. **Statistical Significance Testing**: Perform appropriate statistical tests (e.g., ANOVA, post-hoc pairwise comparisons with corrections) on the model performance data to determine whether the observed differences in reasoning levels are statistically significant and not due to random variation.

3. **Cross-Benchmark Validation**: Systematically evaluate the same models on SC-Math6 and at least two other Chinese mathematical reasoning benchmarks (e.g., Math23K, CMATH) to establish relative performance patterns and validate SC-Math6's claims about its unique contributions to the evaluation landscape.