---
ver: rpa2
title: What Makes Math Word Problems Challenging for LLMs?
arxiv_id: '2403.11369'
source_url: https://arxiv.org/abs/2403.11369
tags:
- llms
- question
- questions
- features
- count
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the linguistic and mathematical features
  that make math word problems (MWPs) challenging for large language models (LLMs).
  The authors extract a set of features from MWPs in the GSM8K dataset, including
  linguistic features (e.g., question length, syntactic complexity), mathematical
  features (e.g., number of operations, numerical token ranks), and world knowledge
  & NLU features (e.g., use of real-world knowledge).
---

# What Makes Math Word Problems Challenging for LLMs?

## Quick Facts
- arXiv ID: 2403.11369
- Source URL: https://arxiv.org/abs/2403.11369
- Reference count: 11
- Key outcome: This paper investigates linguistic and mathematical features that make math word problems challenging for LLMs, finding that question length, syntactic depth, mathematical operation diversity, infrequent numerical tokens, and real-world knowledge requirements are the most indicative of difficulty.

## Executive Summary
This study investigates the linguistic and mathematical features that make math word problems (MWPs) challenging for large language models (LLMs). The authors extract features from MWPs in the GSM8K dataset, including linguistic complexity, mathematical operation diversity, and real-world knowledge requirements. They train classifiers to predict whether MWPs will be solved correctly by different LLMs, finding that features related to mathematical operations, infrequent numerical tokens, question length, syntactic depth, readability, and real-world knowledge are most indicative of MWP difficulty. The best performing classifiers use linguistic features for some LLMs (e.g., Llama2-13B) and mathematical features for others (e.g., Llama2-70B).

## Method Summary
The authors collected solution attempts from four LLMs (Llama2-13B, Llama2-70B, Mistral-7B, MetaMath-13B) on the GSM8K dataset. They extracted 23 features from questions and solutions, including linguistic features (question length, syntactic depth, readability), mathematical features (number and diversity of operations, numerical token ranks), and world knowledge & NLU features (real-world knowledge usage). The data was preprocessed through pruning, scaling, and balancing, then used to train Logistic Regression, Decision Tree, and Random Forest classifiers. Models were evaluated using accuracy and macro-F1 scores, with hyperparameter search and feature importance analysis.

## Key Results
- Question length, syntactic depth, and readability are strong predictors of MWP difficulty for LLMs
- Mathematical features (operation diversity, infrequent numerical tokens) significantly impact LLM performance
- Real-world knowledge requirements add complexity to MWP solving
- Classifier performance varies by LLM: Llama2-13B performs better with linguistic features, while Llama2-70B responds better to mathematical features
- Top-performing classifiers use feature combinations tailored to specific LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs struggle more with MWPs that have higher linguistic complexity, specifically longer questions with more noun phrases, deeper syntactic structure, and lower readability scores.
- Mechanism: The increased linguistic complexity adds cognitive load on the LLM's natural language understanding capabilities, making it harder to parse the relevant information from the problem statement and construct the correct mathematical reasoning steps.
- Core assumption: The linguistic features of the question (length, syntactic depth, readability) are independent predictors of MWP difficulty, separate from the mathematical features.
- Evidence anchors:
  - [abstract] "features related to the number and diversity of math operations, infrequent numerical tokens, question length, syntactic depth, readability, and real-world knowledge are the most indicative of MWP difficulty for LLMs."
  - [section 4.2.1] "The list also contains linguistic features based on the phrasing of the questions: longer questions with a high number of noun phrases (Qx_np_count), mean syntactic depth (Qx_constituency_tree_depth), and readability grade (Qx_flesch_kinkaid_grade) are also difficult for LLMs to solve."
- Break condition: If the LLM's language understanding capabilities improve significantly, or if the MWP's mathematical complexity is the primary driver of difficulty, then linguistic complexity may no longer be a strong predictor of difficulty.

### Mechanism 2
- Claim: MWPs that require more diverse and numerous mathematical operations, especially those involving infrequent numerical tokens, are more challenging for LLMs to solve correctly.
- Mechanism: The increased mathematical complexity requires the LLM to perform more complex reasoning steps and maintain a larger intermediate state, increasing the likelihood of errors in the solution process.
- Core assumption: The mathematical features extracted from the problem and solution (number and diversity of operations, numerical token ranks) are independent predictors of MWP difficulty, separate from the linguistic features.
- Evidence anchors:
  - [abstract] "features related to the number and diversity of math operations, infrequent numerical tokens, question length, syntactic depth, readability, and real-world knowledge are the most indicative of MWP difficulty for LLMs."
  - [section 4.2.1] "The list also contains linguistic features based on the phrasing of the questions: longer questions with a high number of noun phrases (Qx_np_count), mean syntactic depth (Qx_constituency_tree_depth), and readability grade (Qx_flesch_kinkaid_grade) are also difficult for LLMs to solve."
- Break condition: If the LLM's mathematical reasoning capabilities improve significantly, or if the linguistic complexity is the primary driver of difficulty, then mathematical complexity may no longer be a strong predictor of difficulty.

### Mechanism 3
- Claim: MWPs that require real-world knowledge or natural language understanding (NLU) beyond the explicit information provided in the problem are more challenging for LLMs to solve correctly.
- Mechanism: The need for external knowledge or interpretation adds an additional layer of complexity, as the LLM must not only parse the problem and perform mathematical reasoning, but also draw upon its broader knowledge base or make inferences about the problem context.
- Core assumption: The real-world knowledge and NLU features extracted from the problem and solution (world knowledge usage) are independent predictors of MWP difficulty, separate from the linguistic and mathematical features.
- Evidence anchors:
  - [abstract] "features related to the number and diversity of math operations, infrequent numerical tokens, question length, syntactic depth, readability, and real-world knowledge are the most indicative of MWP difficulty for LLMs."
  - [section 4.2.1] "Additionally, the need for extraneous information (Gx_world_knowledge) such as conversion units for time, distance, or weight, can make a question challenging."
- Break condition: If the LLM's knowledge base or NLU capabilities improve significantly, or if the linguistic or mathematical complexity is the primary driver of difficulty, then real-world knowledge and NLU may no longer be strong predictors of difficulty.

## Foundational Learning

- Concept: Feature engineering and selection for machine learning models.
  - Why needed here: The paper relies on extracting relevant features from MWPs to train classifiers that predict LLM performance. Understanding feature engineering is crucial for interpreting the results and applying the methodology to new datasets or LLMs.
  - Quick check question: What are some common techniques for feature selection and how do they differ in their approach?

- Concept: Statistical analysis and hypothesis testing.
  - Why needed here: The paper uses statistical tests (e.g., Student's t-test) to determine the significance of feature thresholds and correlations. Understanding these concepts is essential for evaluating the validity and reliability of the results.
  - Quick check question: What is the purpose of hypothesis testing and how do p-values relate to the significance of results?

- Concept: Machine learning model training and evaluation.
  - Why needed here: The paper trains and evaluates multiple classifier models (e.g., Logistic Regression, Decision Trees, Random Forests) to predict LLM performance on MWPs. Understanding the training process, hyperparameters, and evaluation metrics is crucial for interpreting the results and comparing the models' performance.
  - Quick check question: What are some common evaluation metrics for classification models and how do they differ in their focus?

## Architecture Onboarding

- Component map: Data collection -> Feature extraction -> Classifier training -> Evaluation
- Critical path: GSM8K dataset → Feature extraction (linguistic, mathematical, world knowledge) → Classifier training (Logistic Regression, Decision Tree, Random Forest) → Evaluation (accuracy, macro-F1 scores, feature importance)
- Design tradeoffs:
  - Feature selection: Balancing the number and diversity of features with model complexity and interpretability
  - Classifier choice: Weighing the simplicity and interpretability of traditional classifiers against the potential performance gains of more complex models
  - Data balancing: Addressing class imbalance through oversampling techniques, which may introduce bias or overfitting
- Failure signatures:
  - Poor classifier performance: May indicate issues with feature selection, model choice, or data quality
  - Inconsistent feature importance rankings: Could suggest interactions between features or the need for more robust feature selection methods
  - Overfitting: May occur if the model is too complex relative to the amount of training data or if the data is not properly balanced
- First 3 experiments:
  1. Train and evaluate classifiers on a subset of the GSM8K dataset using only linguistic features, to assess their predictive power for LLM performance.
  2. Repeat the experiment using only mathematical features, to compare their predictive power with linguistic features.
  3. Combine linguistic and mathematical features and evaluate the classifiers, to determine if the combination improves predictive performance over individual feature sets.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the identified features impact the performance of newer or larger language models that were not included in this study?
- Basis in paper: [explicit] The authors mention that their study necessarily has limitations due to the limited set of LLMs tested and plan to include more LLMs in future work.
- Why unresolved: The study only tested a specific set of open-source LLMs, and the impact of the identified features on the performance of newer or larger models is not explored.
- What evidence would resolve it: Testing the identified features on a broader range of LLMs, including newer and larger models, to see if the same patterns hold true or if there are new insights to be gained.

### Open Question 2
- Question: Can the identified features be used to improve the performance of language models on math word problems?
- Basis in paper: [inferred] The authors mention that they plan to make informed modifications to questions based on their findings to study the impact on LLMs' reasoning and MWP solving abilities.
- Why unresolved: While the study identifies features that make MWPs challenging for LLMs, it does not explore how these features can be used to improve model performance.
- What evidence would resolve it: Experiments that modify MWPs based on the identified features and measure the impact on LLM performance, comparing the results to the original questions.

### Open Question 3
- Question: How do the identified features interact with each other and contribute to the overall difficulty of MWPs for LLMs?
- Basis in paper: [inferred] The authors mention that they cluster questions by mathematical features and observe correlations between linguistic features and success rates within each cluster.
- Why unresolved: The study identifies individual features that impact LLM performance, but does not explore how these features interact with each other or how they collectively contribute to the difficulty of MWPs.
- What evidence would resolve it: Analyzing the interactions between features and their combined impact on LLM performance, possibly through more advanced statistical methods or machine learning techniques that can handle feature interactions.

## Limitations
- Feature extraction methodology may not capture all relevant aspects of MWP difficulty, with some implementation details unspecified
- Results are based on the GSM8K dataset and may not generalize to all types of math word problems
- Different LLMs show varying sensitivities to linguistic versus mathematical features, suggesting results may not transfer uniformly across model architectures

## Confidence
- High Confidence: The correlation between linguistic complexity and MWP difficulty for LLMs is well-supported by the data and analysis.
- High Confidence: The relationship between mathematical complexity and MWP difficulty is clearly demonstrated.
- Medium Confidence: The assertion that real-world knowledge and NLU requirements significantly impact MWP difficulty is supported but could benefit from more direct measurement approaches.
- Medium Confidence: The effectiveness of classifier models in predicting LLM performance is demonstrated but may vary with different datasets or model architectures.

## Next Checks
1. Cross-Dataset Validation: Test the feature importance rankings and classifier performance on alternative math word problem datasets to assess generalizability beyond GSM8K.
2. Architecture Transferability: Evaluate whether the observed differences between Llama2-13B (linguistic features) and Llama2-70B (mathematical features) hold across different model families and scales.
3. Feature Ablation Study: Systematically remove individual features to quantify their marginal contribution to classifier performance, particularly for the real-world knowledge and NLU features which show medium confidence in their impact.