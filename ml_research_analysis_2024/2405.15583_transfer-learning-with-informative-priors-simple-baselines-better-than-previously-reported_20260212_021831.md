---
ver: rpa2
title: 'Transfer Learning with Informative Priors: Simple Baselines Better than Previously
  Reported'
arxiv_id: '2405.15583'
source_url: https://arxiv.org/abs/2405.15583
tags:
- learning
- training
- transfer
- shwartz-ziv
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the reported benefits of transfer learning
  methods using informative priors. It demonstrates that standard transfer learning
  (using only pre-trained model initialization) performs significantly better than
  previously reported, particularly on the CIFAR-10 dataset.
---

# Transfer Learning with Informative Priors: Simple Baselines Better than Previously Reported

## Quick Facts
- arXiv ID: 2405.15583
- Source URL: https://arxiv.org/abs/2405.15583
- Reference count: 40
- Primary result: Standard transfer learning performs significantly better than previously reported, challenging the reported benefits of informative prior methods

## Executive Summary
This paper challenges the reported benefits of transfer learning methods using informative priors by demonstrating that standard transfer learning (using only pre-trained model initialization) performs significantly better than previously reported, particularly on the CIFAR-10 dataset. The authors conduct extensive experiments across 5 datasets and find that the relative gains of informative prior methods over standard transfer learning vary in magnitude: negligible on 2 datasets, modest (1.5-3% accuracy) on 2 others, and substantial (>8%) on one dataset. Notably, they find that simpler isotropic covariance priors perform comparably to more complex low-rank covariance approaches.

## Method Summary
The authors conduct extensive experiments across 5 datasets to compare standard transfer learning with informative prior methods. They implement and evaluate various transfer learning approaches, including standard transfer learning, informative prior methods, and isotropic covariance priors. The study uses convolutional neural networks for image classification tasks and includes experiments on loss landscape alignment as a mechanistic justification for informative priors. All experiments are conducted with reproducible code, allowing for verification and extension of the results.

## Key Results
- Standard transfer learning performs significantly better than previously reported, especially on CIFAR-10
- Relative gains of informative prior methods over standard transfer learning: negligible on 2 datasets, modest (1.5-3% accuracy) on 2 datasets, substantial (>8%) on 1 dataset
- Isotropic covariance priors perform comparably to more complex low-rank covariance approaches
- High variability observed in empirical loss landscapes challenges proposed mechanistic justification

## Why This Works (Mechanism)
Unknown: The paper does not provide a clear mechanistic explanation for why standard transfer learning performs better than previously reported or why isotropic covariance priors work as well as low-rank approaches. The authors suggest that previous implementations may have been suboptimal or that the benefits of informative priors are more context-dependent than previously thought, but this remains speculative without deeper theoretical analysis.

## Foundational Learning
None: The paper does not establish foundational learning principles or theoretical frameworks that explain the observed phenomena. The findings are primarily empirical, highlighting the need for further theoretical work to understand when and why different transfer learning approaches succeed or fail.

## Architecture Onboarding
Component map: Pre-trained model -> Standard transfer learning vs. Informative priors -> Performance evaluation
Critical path: Pre-training → Transfer learning method selection → Fine-tuning → Performance measurement
Design tradeoffs: Complexity of informative priors vs. simplicity of standard transfer learning
Failure signatures: Overestimation of informative prior benefits, high variability in loss landscapes
First experiments:
1. Reproduce standard transfer learning baseline on CIFAR-10
2. Compare isotropic vs. low-rank covariance priors
3. Evaluate loss landscape alignment across different datasets

## Open Questions the Paper Calls Out
None: The paper does not explicitly call out specific open questions for future research, though it implicitly raises questions about the theoretical foundations of transfer learning with informative priors and the conditions under which different approaches are most effective.

## Limitations
- Experiments span 5 datasets, which may not capture all transfer learning scenarios
- Focus on image classification with convolutional neural networks limits generalizability
- Findings may not extend to non-vision domains or different architectures
- Claim about isotropic vs. low-rank covariance comparability based on fewer experimental conditions
- Lack of theoretical analysis to explain empirical findings

## Confidence
- Relative performance of standard vs. informative prior methods: High confidence
- Isotropic vs. low-rank covariance comparability: Medium confidence
- Loss landscape variability findings: Medium confidence

## Next Checks
1. Test findings on additional datasets and architectures, particularly in non-vision domains
2. Conduct ablation studies to isolate contributions of different components
3. Perform statistical significance testing across multiple random seeds to confirm robustness of performance differences
4. Develop theoretical analysis to explain why standard transfer learning performs better than previously reported