---
ver: rpa2
title: Design Principle Transfer in Neural Architecture Search via Large Language
  Models
arxiv_id: '2408.11330'
source_url: https://arxiv.org/abs/2408.11330
tags:
- search
- architecture
- architectures
- design
- principles
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces design principle transfer in neural architecture
  search (NAS) to enhance efficiency by learning and reusing architectural knowledge.
  It employs a large language model (LLM) to automatically extract design principles
  from established architectures and adapt them to new tasks, refining the search
  space.
---

# Design Principle Transfer in Neural Architecture Search via Large Language Models

## Quick Facts
- arXiv ID: 2408.11330
- Source URL: https://arxiv.org/abs/2408.11330
- Reference count: 40
- One-line primary result: Design principle transfer via LLMs significantly improves NAS efficiency and performance across multiple search spaces

## Executive Summary
This paper introduces a novel approach to neural architecture search (NAS) that leverages large language models (LLMs) to automatically learn and transfer design principles across tasks. The framework, called LAPT, extracts linguistic descriptions of architectural patterns from established networks and uses them to constrain the search space for new tasks. By refining the candidate operators and connections based on these principles, the method achieves significant improvements in both search efficiency and final architecture quality. The approach demonstrates state-of-the-art results on three major search spaces (NAS201, Trans101, and DARTs) while requiring fewer architecture evaluations than traditional methods.

## Method Summary
The LAPT framework learns design principles from established architectures using an LLM, which analyzes architectural parameters represented as Python code to identify common patterns among high-performing networks. These principles are expressed as linguistic descriptions (e.g., "average-pooling layers probably downgrade performance") and used to prune unpromising operators and connections, thereby refining the search space. For new tasks, the learned principles initialize a constrained search space, and an iterative adaptation process refines these principles based on architectures found during the search. This creates task-specific search spaces that balance generality with domain-specific requirements, significantly improving search efficiency while maintaining or improving final architecture quality.

## Key Results
- LAPT achieves state-of-the-art results on NAS201, Trans101, and DARTs search spaces
- Search efficiency improves by 37-49% compared to traditional NAS methods
- GPT-4-based principle learning outperforms GPT-3.5 and GPT-4o in both accuracy and model ranking
- Iterative principle adaptation consistently improves search performance over static transfer

## Why This Works (Mechanism)

### Mechanism 1
Design principles learned from existing architectures can prune unpromising operators and connections, thereby refining the search space and improving search efficiency. The LLM analyzes architectural parameters to identify common patterns among high-performing architectures, translating these patterns into linguistic design principles that constrain candidate operators and information sources for each layer.

### Mechanism 2
Principle adaptation based on architectures found during the search process can mitigate domain shift and build task-specific search spaces. After initial search using transferred principles, the LLM updates the design principles based on the architectures found for the new task, iteratively refining the search space to match specific task requirements.

### Mechanism 3
LLMs can effectively learn design principles from architectural parameters represented as Python code, even with limited samples. The use of Python code as input allows the LLM to gain awareness of neural architecture from source codes, leveraging its pre-training on vast knowledge to identify patterns and infer generalized design principles.

## Foundational Learning

- Concept: Neural Architecture Search (NAS)
  - Why needed here: Understanding NAS fundamentals is crucial for grasping how design principle transfer improves efficiency and performance
  - Quick check question: What are the three primary NAS methods mentioned in the related work section, and how do they differ in their approach to finding optimal architectures?

- Concept: Transfer Learning in NAS
  - Why needed here: Design principle transfer is a specific type of transfer learning applied to NAS
  - Quick check question: What are the two main categories of transferable NAS (TNAS) methods mentioned in the related work section, and how do they differ in the type of knowledge they transfer?

- Concept: Large Language Models (LLMs)
  - Why needed here: LLMs are the core component for learning and adapting design principles
  - Quick check question: According to the paper, what are the three advantages of using LLMs in the LAPT framework, and how do they address the challenges of learning design principles?

## Architecture Onboarding

- Component map: Design Principle Learning -> Principle Transfer -> Architecture Search -> Principle Adaptation (iterative)
- Critical path: Design Principle Learning → Principle Transfer → Architecture Search → Principle Adaptation → Final Architecture
- Design tradeoffs: Using more complex LLMs (e.g., GPT-4) may improve principle quality but increase computational cost; more extensive adaptation may improve search spaces but increase search time
- Failure signatures: Poor search performance indicates principles may not transfer well; slow convergence suggests initial search space too large or adaptation ineffective; incorrect principles indicate LLM may not accurately learn from architectures
- First 3 experiments: 1) Test design principle learning capability on simple search space with top-performing architectures, 2) Evaluate principle transfer effectiveness by comparing search performance with refined vs original search space, 3) Assess impact of principle adaptation by comparing performance with and without adaptation step

## Open Questions the Paper Calls Out

### Open Question 1
How does the effectiveness of design principle transfer scale with increasing architectural diversity across different search spaces? The paper suggests potential limitations when scaling to more diverse architectures, but current experiments focus on similar tasks and search spaces.

### Open Question 2
What is the impact of LLM model size and reasoning capabilities on the quality of extracted design principles and subsequent search performance? While the paper shows GPT-4 outperforms other models, it doesn't explore the threshold of capability needed or how different reasoning abilities affect principle extraction.

### Open Question 3
How does the proposed principle adaptation strategy compare to alternative transfer learning approaches in NAS when domain shift occurs? The paper introduces principle adaptation but only compares against other TNAS methods without exploring alternative adaptation strategies.

## Limitations

- Effectiveness depends on similarity between source and target tasks; significantly different tasks may not benefit from transferred principles
- Computational cost of using LLMs for principle learning and adaptation is not explicitly addressed
- Quality of learned principles heavily depends on prompt engineering and LLM reasoning capabilities, which are not extensively validated

## Confidence

- **High Confidence**: Framework's ability to improve search efficiency by refining search space using design principles
- **Medium Confidence**: Effectiveness of principle adaptation in building task-specific search spaces
- **Low Confidence**: Generalizability to tasks with significantly different architectural requirements

## Next Checks

1. Evaluate framework robustness on tasks with significantly different data distributions or architectural requirements (e.g., medical imaging or speech recognition)
2. Conduct ablation studies to assess impact of different prompt templates and LLM configurations on principle quality
3. Measure computational cost of LLM-based principle learning and adaptation steps, including GPU usage and inference time