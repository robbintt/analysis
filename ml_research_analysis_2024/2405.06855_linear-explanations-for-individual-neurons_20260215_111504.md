---
ver: rpa2
title: Linear Explanations for Individual Neurons
arxiv_id: '2405.06855'
source_url: https://arxiv.org/abs/2405.06855
tags:
- neurons
- neuron
- explanations
- linear
- activations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the problem of interpreting individual neurons\
  \ in neural networks by moving beyond the common practice of focusing only on the\
  \ highest activating inputs. The authors propose \"Linear Explanations,\" where\
  \ each neuron is described as a linear combination of interpretable concepts (e.g.,\
  \ \"w1\xD7dog + w2\xD7pet\")."
---

# Linear Explanations for Individual Neurons

## Quick Facts
- arXiv ID: 2405.06855
- Source URL: https://arxiv.org/abs/2405.06855
- Reference count: 29
- Key outcome: Linear Explanations achieve ~0.4 correlation with neuron activations, twice as high as baselines

## Executive Summary
This paper addresses the fundamental problem of interpreting individual neurons in neural networks by moving beyond the common practice of focusing only on the highest activating inputs. The authors propose "Linear Explanations," where each neuron is described as a linear combination of interpretable concepts (e.g., "w1×dog + w2×pet"). They develop an efficient method to learn these explanations by constructing a concept activation matrix using either labeled data or pretrained multimodal models like SigLIP, followed by a two-step optimization process with greedy search for sparsity.

The method is evaluated using a novel simulation-based approach, where SigLIP models predict neuron activations on unseen inputs based on the explanations. Results show that Linear Explanations significantly outperform existing methods (Network Dissection, MILAN, CLIP-Dissect) in correlation with actual neuron activations, achieving around 0.4 correlation on average. Under ablation scoring, which measures how well simulated activations preserve model performance, Linear Explanations improve performance by over 3× compared to previous methods, though overall scores remain low, suggesting room for improvement.

## Method Summary
The authors propose Linear Explanations as a method to interpret individual neurons in neural networks by describing each neuron as a linear combination of interpretable concepts. The approach involves constructing a concept activation matrix using either labeled data or pretrained multimodal models like SigLIP, followed by a two-step optimization process that employs greedy search for sparsity. The method is computationally efficient and can reveal polysemanticity across activation ranges. To evaluate explanations, the paper introduces a simulation-based approach using SigLIP models as simulators to predict neuron activations on unseen inputs, which is particularly applied to vision models.

## Key Results
- Linear Explanations achieve ~0.4 correlation with actual neuron activations, twice as high as baseline methods
- Under ablation scoring, Linear Explanations improve performance by over 3× compared to previous methods
- The method is computationally efficient and reveals polysemanticity across activation ranges

## Why This Works (Mechanism)
Linear Explanations work by decomposing complex neuron activations into interpretable linear combinations of human-understandable concepts. This approach captures the multi-faceted nature of neurons that can respond to multiple related concepts simultaneously. The method's effectiveness stems from using multimodal models like SigLIP to identify relevant concepts and then optimizing the linear combination weights to maximize correlation with actual neuron behavior. The greedy search optimization ensures sparsity while maintaining interpretability.

## Foundational Learning
- Concept Activation Matrix: A matrix that maps interpretable concepts to neuron activations; needed to quantify how strongly each concept relates to each neuron
- Greedy Search Optimization: An algorithm that iteratively selects the best concept to add to the explanation; needed for computational efficiency in finding sparse explanations
- Ablation Scoring: A metric that measures how well simulated activations preserve model performance; needed to evaluate practical utility beyond correlation
- Polysemanticity: The property of neurons responding to multiple distinct concepts; needed to understand the multi-faceted nature of neural representations
- Multimodal Models (SigLIP): Pretrained models that can process multiple input types; needed to extract meaningful concepts from images for vision neurons

## Architecture Onboarding

Component Map:
Concept Dataset -> Concept Activation Matrix -> Two-Step Optimization -> Linear Explanation

Critical Path:
Concept selection → Activation matrix construction → Weight optimization → Sparsity enforcement → Evaluation

Design Tradeoffs:
- Computational efficiency vs. explanation accuracy (greedy search vs. exhaustive search)
- Sparsity vs. completeness (fewer concepts vs. better correlation)
- Labeled data vs. pretrained models (cost vs. flexibility)

Failure Signatures:
- Low correlation with actual neuron activations indicates poor concept selection
- High computational cost suggests inefficient optimization parameters
- Non-sparse explanations indicate failed greedy search implementation

First Experiments:
1. Test concept selection on a small neuron subset with known ground truth
2. Compare greedy search vs. random concept selection on correlation metrics
3. Validate activation matrix construction with synthetic concept data

## Open Questions the Paper Calls Out
None

## Limitations
- Simulation-based evaluation may introduce circularity through evaluation model biases
- Correlation of ~0.4 still leaves substantial unexplained variance in neuron behavior
- Greedy search optimization may miss optimal sparse solutions
- Method relies on availability of labeled data or pretrained multimodal models

## Confidence

High Confidence: Computational efficiency claims and superiority over baseline methods (Network Dissection, MILAN, CLIP-Dissect)

Medium Confidence: Effectiveness of simulation-based evaluation and practical utility of Linear Explanations

Low Confidence: Generalizability to all neural network types and domains beyond vision models

## Next Checks

1. Cross-model validation: Test Linear Explanations on neurons from different model architectures (e.g., CLIP, DINOv2) and different modalities (language, multimodal) to assess generalizability beyond the current vision model focus.

2. Independent evaluation: Implement an alternative evaluation framework that doesn't rely on the same or similar models used for explanation generation to verify that the improved performance isn't an artifact of the evaluation method.

3. Human evaluation study: Conduct a user study where practitioners attempt to use Linear Explanations for practical interpretability tasks (e.g., debugging, bias detection) to assess real-world utility beyond correlation metrics.