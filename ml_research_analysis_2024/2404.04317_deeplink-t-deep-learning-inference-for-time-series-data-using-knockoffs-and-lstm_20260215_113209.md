---
ver: rpa2
title: 'DeepLINK-T: deep learning inference for time series data using knockoffs and
  LSTM'
arxiv_id: '2404.04317'
source_url: https://arxiv.org/abs/2404.04317
tags: []
core_contribution: DeepLINK-T introduces a novel method for interpretable and reproducible
  feature selection in deep learning models for high-dimensional time series data.
  It combines deep learning with knockoff inference to control the false discovery
  rate (FDR) in feature selection while accommodating complex feature distributions.
---

# DeepLINK-T: deep learning inference for time series data using knockoffs and LSTM

## Quick Facts
- arXiv ID: 2404.04317
- Source URL: https://arxiv.org/abs/2404.04317
- Reference count: 40
- Key outcome: DeepLINK-T introduces a novel method for interpretable and reproducible feature selection in deep learning models for high-dimensional time series data.

## Executive Summary
DeepLINK-T is a novel deep learning framework for feature selection in high-dimensional time series data that combines knockoff inference with LSTM networks. The method generates knockoff variables using a time-varying latent factor structure learned by an LSTM autoencoder, then applies knockoff statistics derived from an LSTM prediction network to control the false discovery rate (FDR) in feature selection. Extensive simulations demonstrate DeepLINK-T's ability to effectively control FDR while achieving superior feature selection power compared to non-time-series counterparts. The method is validated on three real-world metagenomic datasets, identifying key bacterial genera associated with infant gut microbiome, marine chlorophyll-a concentration, and dietary glycans treatment effects.

## Method Summary
DeepLINK-T generates knockoff variables by first training an LSTM autoencoder to reconstruct time series data, capturing temporal dependencies through latent factors. Knockoff variables are synthesized by adding regenerated noise to the latent factor estimates. These knockoff variables, along with the original features, are fed into an LSTM prediction network with paired filter layers. The network learns feature importance through contributions across forget, input, cell, and output gates. Knockoff statistics are computed as the difference of squared importance measures between original and knockoff features. The knockoff framework is then applied to select features while controlling FDR at a pre-specified level.

## Key Results
- DeepLINK-T effectively controls FDR at the target level while demonstrating superior feature selection power for high-dimensional longitudinal time series data compared to its non-time-series counterpart.
- Simulation studies show DeepLINK-T's robustness across different link functions, noise levels, and time-varying latent factor structures.
- Application to three metagenomic datasets validates DeepLINK-T's practical utility in identifying key bacterial genera associated with infant gut microbiome, marine chlorophyll-a concentration, and dietary glycans treatment effects.

## Why This Works (Mechanism)

### Mechanism 1
DeepLINK-T generates knockoff variables that satisfy the two key properties of model-X knockoffs by leveraging a learned latent factor structure in LSTM autoencoders. The LSTM autoencoder reconstructs the time series data matrix X, capturing its temporal and cross-feature dependencies via latent factors. The residuals from this reconstruction are treated as idiosyncratic noise, from which knockoff variables are synthesized by adding regenerated noise samples to the latent factor estimate. This approach relies on the core assumption that the true data generating process follows a time-varying latent factor model with factor vector ft and idiosyncratic noise Ïµt independent of ft.

### Mechanism 2
The LSTM prediction network produces knockoff statistics that are symmetric for unimportant features and asymmetric for important features, satisfying the sign-flip property required for FDR control. Original features and their knockoff counterparts are paired through a filter layer. The LSTM prediction network propagates these pairs through dense and LSTM layers, and the importance of each feature is measured by aggregating its contributions across forget, input, cell, and output gates. The knockoff statistic is the difference of squared importance measures. This construction assumes the network can learn to assign higher importance weights to features that are truly predictive of the response.

### Mechanism 3
DeepLINK-T controls FDR in high-dimensional time series regression by combining knockoff exchangeability with temporal modeling in LSTM networks. The method first generates knockoff variables preserving the joint distribution of features over time, then trains an LSTM prediction network to assess feature importance in predicting the response. The knockoff statistics derived from this network are used with the knockoff threshold to select variables while controlling FDR. This approach relies on the assumption that the response y_it depends on features x_it only through a small subset (Markov blanket) and that the knockoff framework's theoretical guarantees extend to the LSTM-based construction.

## Foundational Learning

- **Model-X knockoffs framework**: Provides a general method for FDR control in high-dimensional settings without relying on p-values, essential for deep learning models where p-values are intractable. Quick check: What are the two key properties that knockoff variables must satisfy?

- **LSTM networks for temporal modeling**: Capture serial dependencies in time series data, allowing knockoff generation and feature importance assessment to respect the temporal structure. Quick check: How does an LSTM cell update its hidden state and cell state at each time step?

- **False discovery rate (FDR) and power**: Core metrics for evaluating feature selection performance; DeepLINK-T aims to control FDR while maximizing power. Quick check: How is the knockoff threshold defined for FDR control?

## Architecture Onboarding

- **Component map**: LSTM autoencoder -> LSTM prediction network -> Knockoff statistics -> Feature selection

- **Critical path**:
  1. Train LSTM autoencoder on time series data to obtain latent factor estimate.
  2. Generate knockoff variables by adding regenerated noise to latent factor estimate.
  3. Form augmented data matrix with original and knockoff features.
  4. Train LSTM prediction network on augmented data.
  5. Extract knockoff statistics from network weights.
  6. Apply knockoff threshold to select features.

- **Design tradeoffs**:
  - Bottleneck dimensionality in autoencoder: Balances model complexity and risk of under/overfitting latent factors.
  - Training epochs: Sufficient epochs needed for stable knockoff generation and feature importance learning; too few may lead to underfitting.
  - Network architecture: More complex architectures may better capture feature-response relationships but increase risk of overfitting.

- **Failure signatures**:
  - FDR exceeds target level: Indicates knockoff statistics or exchangeability is not properly maintained.
  - Low power: Suggests the network is not effectively learning feature importance or the knockoff construction is too conservative.
  - Unstable selection across runs: Points to insufficient training or overly complex model.

- **First 3 experiments**:
  1. Verify knockoff exchangeability: Swap original and knockoff features and confirm the augmented data distribution remains unchanged.
  2. Check FDR control on simulated data with known ground truth: Vary signal amplitude and feature dimensions to test robustness.
  3. Validate temporal modeling: Compare feature selection performance with and without LSTM to confirm benefit of temporal modeling.

## Open Questions the Paper Calls Out

- **Open Question 1**: How would DeepLINK-T perform if Transformer models were used instead of LSTM networks in both the autoencoder and prediction components? The paper proposes this as future work without testing it. Empirical comparison studies of DeepLINK-T performance using LSTM vs Transformer architectures would resolve this question.

- **Open Question 2**: What are the theoretical guarantees for FDR control in DeepLINK-T when the latent factor model is misspecified? The paper mentions that DeepLINK-T is robust to model misspecification in simulations but does not provide theoretical analysis. Theoretical proofs or rigorous analysis showing FDR control under various forms of model misspecification would resolve this question.

- **Open Question 3**: How does the choice of knockoff statistics construction method affect the power and FDR control of DeepLINK-T? The paper uses a specific method for knockoff statistics construction but does not compare it to alternative methods. Empirical studies comparing DeepLINK-T performance using different knockoff statistics construction methods would resolve this question.

## Limitations

- Exact LSTM architecture parameters (layer depth, unit counts, learning rates) for both autoencoder and prediction network are not provided, making exact reproduction challenging.
- The method assumes a time-varying latent factor structure, but no sensitivity analysis is presented to evaluate performance when this assumption is violated.
- Real-world application results are preliminary, with limited reporting on dataset 2 and insufficient detail on hyperparameter tuning across datasets.

## Confidence

- **High Confidence**: The theoretical framework combining knockoffs with LSTM networks is well-established, and the basic methodological approach is sound.
- **Medium Confidence**: The FDR control mechanism appears theoretically valid, but practical implementation details significantly impact performance.
- **Low Confidence**: Real-world application results are preliminary, with limited reporting on dataset 2 and insufficient detail on hyperparameter tuning across datasets.

## Next Checks

1. **Architecture Sensitivity Analysis**: Systematically vary LSTM layer depth, units per layer, and bottleneck dimensionality to identify optimal configurations and robustness to architectural choices.

2. **Temporal Structure Validation**: Compare DeepLINK-T performance against DeepLINK (non-time-series version) across synthetic datasets with varying temporal correlation structures to quantify the benefit of temporal modeling.

3. **Assumption Violation Testing**: Evaluate DeepLINK-T performance on simulated data where the latent factor structure assumption is violated (e.g., non-stationary processes, external confounders) to assess robustness and identify failure modes.