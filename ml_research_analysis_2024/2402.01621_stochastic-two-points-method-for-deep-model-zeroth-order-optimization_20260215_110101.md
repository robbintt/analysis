---
ver: rpa2
title: Stochastic Two Points Method for Deep Model Zeroth-order Optimization
arxiv_id: '2402.01621'
source_url: https://arxiv.org/abs/2402.01621
tags:
- as2p
- loss
- values
- step
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Stochastic Two-Point (S2P), a zeroth-order
  optimization method for large deep models that requires only forward passes. S2P
  eliminates the non-updating component from existing methods, reducing computational
  cost.
---

# Stochastic Two Points Method for Deep Model Zeroth-order Optimization

## Quick Facts
- arXiv ID: 2402.01621
- Source URL: https://arxiv.org/abs/2402.01621
- Reference count: 40
- One-line primary result: S2P achieves O(d/ε²) query complexity while reducing computational cost by eliminating non-updating components from existing methods

## Executive Summary
This paper introduces Stochastic Two-Point (S2P), a zeroth-order optimization method designed for large deep models that requires only forward passes without backpropagation. S2P improves upon existing methods by eliminating the non-updating component f(xk) that requires an additional forward pass, reducing computational cost while maintaining convergence guarantees. Building on this foundation, the authors propose Accelerated S2P (AS2P), which incorporates dynamic step sizes and statistical information to capture first- and second-order dynamics. Extensive experiments demonstrate that AS2P outperforms baseline methods across various model types and scales, achieving up to 2× speed-up in training while requiring fewer function queries to reach specific loss values.

## Method Summary
The paper proposes S2P as a zeroth-order optimization method that requires only two forward passes per iteration instead of three, by eliminating the non-updating component f(xk) from existing methods. The algorithm uses symmetric random perturbations and compares function values at perturbed points to determine descent directions. AS2P builds upon S2P by incorporating dynamic step sizes based on statistical information in γk, which captures first- and second-order dynamics. The method includes progressive γ-clipping and automatic learning rate adjustment based on the standard deviation of recent γ values. Theoretical analysis establishes O(d/ε²) query complexity under both general and relaxed smoothness assumptions, with convergence guarantees for reaching first-order stationary points.

## Key Results
- S2P eliminates the non-updating component f(xk), reducing computational cost while maintaining convergence
- AS2P incorporates dynamic step sizes and statistical information to capture first- and second-order dynamics
- Extensive experiments show AS2P achieves up to 2× speed-up in training across various model types and scales, including language models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Removing the non-updating component f(xk) from STP reduces computational cost while maintaining convergence
- Mechanism: In STP, the f(xk) term requires an additional forward pass that cannot be reused in batch settings. S2P eliminates this by only comparing f(xk + αksk) and f(xk - αksk), requiring only 2 forward passes per iteration instead of 3
- Core assumption: The convergence properties of STP rely on comparing f(xk) with the two perturbed points, but the exact value of f(xk) is not critical for the descent direction
- Evidence anchors:
  - [abstract]: "S2P eliminates the non-updating component from existing methods, reducing computational cost"
  - [section]: "The S2P algorithm is summarized in Alg. 1. Specifically, the choice of the distribution of random perturbations... does not alter our analysis results"
- Break condition: If the non-updating component f(xk) is actually necessary for convergence in certain loss landscapes, removing it would degrade performance

### Mechanism 2
- Claim: Dynamic step size using |γk| captures first-order and second-order dynamics better than fixed step sizes
- Mechanism: The algorithm uses γk = |f(xk+ρsk) - f(xk-ρsk)| / (2ρ) as a proxy for gradient information. The step size αk = |γk| / (AL0 + √2BL1|γk|)d creates a non-linear relationship that adapts to both smooth and non-smooth regions
- Core assumption: The statistical information in γk is highly correlated with gradient norm information, which under relaxed smoothness assumptions is correlated with second-order information
- Evidence anchors:
  - [abstract]: "incorporates dynamic step sizes and statistical information to capture first- and second-order dynamics"
  - [section]: "Theorem 3.11 shows that the gradient norm ||∇f(x)|| is bounded by |γ| in expectation"
- Break condition: If the relationship between |γk| and ||∇f(x)|| breaks down in certain regions of the loss landscape, the adaptive step size could become unstable

### Mechanism 3
- Claim: Progressive γ-clipping and automatic learning rate create better training dynamics
- Mechanism: The algorithm progressively adjusts the threshold τa,k = ηa·Std Dev(γrecent) over iterations, creating a clipping effect that prevents overly aggressive steps in non-smooth regions. The learning rate is automatically scaled by 1/τb,k where τb,k = ηb·Std Dev(γrecent)
- Core assumption: The standard deviation of recent γ values provides a reliable estimate of the local smoothness of the objective function
- Evidence anchors:
  - [abstract]: "incorporates dynamic step sizes and statistical information to capture first- and second-order dynamics"
  - [section]: "AS2P augments stochastic two-point search with dynamic step sizes and incorporates statistical information related to γk"
- Break condition: If the standard deviation of γ values becomes a poor estimator of local smoothness (e.g., in highly irregular loss landscapes), the clipping could become too aggressive or too lenient

## Foundational Learning

- Concept: Zeroth-order optimization (derivative-free optimization)
  - Why needed here: The paper focuses on methods that only use function values (zeroth-order information) to optimize large deep models, avoiding backpropagation
  - Quick check question: What is the fundamental difference between zeroth-order and first-order optimization methods?

- Concept: Relaxed smoothness assumptions
  - Why needed here: The paper extends analysis beyond traditional L-smoothness to (L0, L1)-smoothness, which allows for more realistic modeling of deep learning loss landscapes
  - Quick check question: How does (L0, L1)-smoothness differ from standard L-smoothness, and why is this distinction important for deep models?

- Concept: Convergence rate analysis
  - Why needed here: The paper provides theoretical analysis of query complexity (O(d/ε²)) under both general and relaxed smoothness assumptions
  - Quick check question: What does the query complexity O(d/ε²) tell us about the scalability of the method as model dimension increases?

## Architecture Onboarding

- Component map:
  - Random perturbation generator (Rademacher/normal/uniform distributions) -> Forward pass evaluator (computes f(x) for perturbed points) -> γk calculator (estimates directional derivative) -> Step size controller (dynamic adjustment based on γk statistics) -> Parameter updater (applies the S2P update rule) -> Statistics tracker (maintains recent γ values for Std Dev calculation)

- Critical path:
  1. Sample random perturbation sk
  2. Compute f(xk + ρksk) and f(xk - ρksk)
  3. Calculate γk = |f(xk+ρksk) - f(xk-ρksk)| / (2ρ)
  4. Update statistics for Std Dev(γrecent)
  5. Compute step size αk using the dynamic formula
  6. Update parameters: xk+1 = xk + αksk

- Design tradeoffs:
  - Two symmetric perturbations per iteration vs. one: Better gradient estimation but higher computational cost
  - Choice of random distribution: Rademacher often performs better than Gaussian in practice
  - Smoothing parameter ρ: Must be small enough for accurate gradient estimation but large enough to avoid numerical issues

- Failure signatures:
  - Training divergence: Step size αk becomes too large due to poor γk estimation
  - Slow convergence: Step size remains too conservative in smooth regions
  - Numerical instability: ρk too small causing division by near-zero values

- First 3 experiments:
  1. Implement S2P with fixed step size on a simple convex function to verify basic correctness
  2. Compare convergence rates of S2P vs STP on a synthetic non-convex function with known smoothness properties
  3. Test the dynamic step size variant on a small neural network to observe the effect of the γk-based adaptation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact impact of varying the hyperparameter ηb on the convergence rate of AS2P, and how does it interact with other hyperparameters like ηa and ρend?
- Basis in paper: [explicit] The paper mentions that ηb has a relatively significant influence on the convergence rate and emphasizes the importance of the interaction between the absolute value and standard deviation of γk.
- Why unresolved: The paper only provides a brief empirical investigation of ηb's impact, showing that different values of ηb affect convergence but not quantifying the exact relationship or optimal settings.
- What evidence would resolve it: A comprehensive study varying ηb across a wide range of values while keeping other hyperparameters fixed, measuring the convergence rate and final loss for each setting.

### Open Question 2
- Question: How does the proposed AS2P method perform on different types of neural network architectures beyond ResNets, such as Transformers or LSTMs, especially for tasks requiring long-range dependencies?
- Basis in paper: [inferred] The paper focuses on evaluating AS2P on ResNets and a large language model (OPT-13B), but does not explore its performance on other common architectures like Transformers or LSTMs.
- Why unresolved: The paper's experimental scope is limited to ResNets and one LLM, leaving the generalizability of AS2P to other architectures untested.
- What evidence would resolve it: Experiments applying AS2P to train Transformers for language modeling or LSTMs for sequence prediction tasks, comparing convergence rates and final performance to standard methods.

### Open Question 3
- Question: Can the progressive γ-clipping strategy be further improved by dynamically adjusting the threshold τa based on the local curvature of the loss landscape, rather than using a fixed percentage of recent γ values?
- Basis in paper: [explicit] The paper introduces progressive γ-clipping as a strategy to mimic the non-linear dependence between the step size and |γk|, but uses a fixed threshold τa = ηa * Std Dev(γrecent).
- Why unresolved: The current approach uses a simple statistic of recent γ values to estimate the threshold, which may not capture the full complexity of the loss landscape's curvature.
- What evidence would resolve it: Developing a method to estimate the local curvature of the loss landscape (e.g., using Hessian-vector products) and using this information to dynamically adjust τa, then comparing the performance to the current fixed percentage approach.

## Limitations
- Theoretical analysis is solid but practical implementation details for large-scale deep models remain underspecified
- Does not address how to handle batch normalization layers or other components that depend on statistics computed across the batch
- 2× speed-up claim is based on theoretical comparison rather than comprehensive empirical validation across diverse model architectures and scales

## Confidence
- S2P convergence theory (High): The O(d/ε²) query complexity under general smoothness is well-established
- AS2P acceleration mechanism (Medium): While theoretically motivated, practical benefits depend heavily on hyperparameter tuning
- 2× speed-up claim (Low): Based on computational complexity analysis, not extensive empirical validation

## Next Checks
1. Implement S2P on a small ResNet model with CIFAR-10 to verify the elimination of f(xk) doesn't degrade convergence
2. Compare AS2P's dynamic step size adaptation against fixed step size baselines on a synthetic non-convex function with varying smoothness
3. Test the robustness of AS2P's γ-clipping mechanism on a model with known non-smooth regions (e.g., models with ReLU activations)