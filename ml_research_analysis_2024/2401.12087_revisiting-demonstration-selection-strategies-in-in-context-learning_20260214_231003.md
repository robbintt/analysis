---
ver: rpa2
title: Revisiting Demonstration Selection Strategies in In-Context Learning
arxiv_id: '2401.12087'
source_url: https://arxiv.org/abs/2401.12087
tags:
- topk
- performance
- in-context
- tasks
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines why the performance of in-context learning
  (ICL) varies with different demonstrations and proposes a data- and model-dependent
  demonstration selection method. The authors first show that the choice of demonstrations
  is influenced by both the test data and the model (inference model and retrieval
  model).
---

# Revisiting Demonstration Selection Strategies in In-Context Learning

## Quick Facts
- arXiv ID: 2401.12087
- Source URL: https://arxiv.org/abs/2401.12087
- Reference count: 9
- Primary result: TopK+ConE method improves ICL performance by 1.2% average accuracy on NLU tasks

## Executive Summary
This paper investigates why in-context learning (ICL) performance varies significantly with different demonstration choices. The authors demonstrate that demonstration selection is both data- and model-dependent, and propose a new method called TopK+ConE that selects demonstrations by minimizing the conditional entropy of test inputs under the inference model. Experiments show consistent improvements across different model scales (1.5B to 13B parameters) and tasks, including natural language understanding and generation.

## Method Summary
The TopK+ConE method combines a two-stage approach for demonstration selection in ICL. First, it uses a TopK selection module to find candidate demonstrations that are semantically similar to the test input, reducing the candidate space for efficiency. Then, it re-ranks these candidates based on conditional entropy estimated by the inference model itself, selecting the set that minimizes conditional entropy. This approach leverages the model's understanding of the test input to choose demonstrations that are most helpful for the specific task at hand.

## Key Results
- Achieves 1.2% average accuracy improvement over state-of-the-art TopK+MDL on 7 NLU tasks
- Shows consistent improvements across model scales from 1.5B to 13B parameters
- Works for both natural language understanding and generation tasks
- Demonstrates effectiveness for aligned models and mixed-domain demonstration pools

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ICL performance is influenced by both the test data and the model (retrieval and inference models)
- Mechanism: The effectiveness of in-context learning demonstrations depends on their ability to help the inference model better understand the test input. Demonstrations that reduce the conditional entropy of the test input under the inference model are more effective.
- Core assumption: The performance of a demonstration positively correlates with its contribution to the model's understanding of the test samples
- Evidence anchors:
  - [abstract]: "the choice of demonstration is both data- and model-dependent"
  - [section 2.2]: "Experimental results show that the ICL performance can largely vary with different models even with the same demonstrations"
  - [corpus]: Weak evidence - corpus shows related work on demonstration selection but doesn't directly address data- and model-dependency
- Break condition: If the inference model cannot properly estimate conditional entropy or if the relationship between entropy reduction and performance is not monotonic

### Mechanism 2
- Claim: The TopK+ConE method consistently improves ICL performance across different model scales and tasks
- Mechanism: The method first uses TopK to select candidate demonstrations, then re-ranks them based on conditional entropy estimated by the inference model itself. This two-stage process balances efficiency with effectiveness.
- Core assumption: The inference model can accurately estimate the conditional entropy of test inputs given different demonstration sets
- Evidence anchors:
  - [abstract]: "our method yields consistent improvements in both language understanding and generation tasks with different model scales"
  - [section 5]: Shows consistent improvements across NLU tasks (average 1.2% accuracy gain) and NLG tasks (COMET score improvements)
  - [corpus]: Weak evidence - corpus mentions related work on demonstration selection but doesn't directly validate the TopK+ConE approach
- Break condition: If the inference model's entropy estimation is unreliable or if the candidate set from TopK is too small to contain good demonstrations

### Mechanism 3
- Claim: The proposed method provides a unified explanation for the effectiveness of previous ICL demonstration selection methods
- Mechanism: Previous methods that reduce conditional entropy of test inputs (even if not explicitly framed this way) are effective because they improve the model's understanding of the test samples.
- Core assumption: The effectiveness of previous demonstration selection methods can be explained by their impact on reducing conditional entropy of test inputs
- Evidence anchors:
  - [section 8]: "we calculate the conditional entropy of the test input with respect to previous baselines across three classification tasks" and find they reduce conditional entropy
  - [abstract]: "our method provides a unified explanation for the effectiveness of previous methods"
  - [corpus]: Weak evidence - corpus shows related work on ICL but doesn't directly connect to unified explanation via entropy reduction
- Break condition: If some effective demonstration selection methods cannot be explained by their impact on conditional entropy reduction

## Foundational Learning

- Concept: Conditional entropy and its role in information theory
  - Why needed here: The method relies on selecting demonstrations that minimize conditional entropy of test inputs
  - Quick check question: What does it mean for a demonstration to reduce the conditional entropy of a test input under an inference model?

- Concept: In-context learning (ICL) and its sensitivity to demonstration selection
  - Why needed here: The paper addresses the problem of improving ICL performance through better demonstration selection
  - Quick check question: Why does the performance of ICL vary significantly with different choices of demonstrations?

- Concept: Retrieval-based in-context learning framework
  - Why needed here: The method uses a selection-rerank framework with TopK selection and conditional entropy re-ranking
  - Quick check question: How does the selection-rerank framework differ from traditional demonstration selection approaches?

## Architecture Onboarding

- Component map:
  - Data preprocessing: Dataset splitting, template application
  - Retrieval module: Sentence transformers for finding candidate demonstrations
  - Inference model: Various LLMs (GPT2-XL, Llama2 variants) for conditional entropy estimation
  - TopK selection: Finds nearest neighbors to reduce candidate space
  - ConE reranking: Ranks candidates by conditional entropy
  - Evaluation: Accuracy for NLU, COMET/BLEU/ChatGPT for NLG

- Critical path: Dataset → Retrieval module → TopK selection → ConE reranking → Inference model → Performance evaluation

- Design tradeoffs:
  - Candidate set size vs. computational efficiency (30 candidates chosen based on ablation)
  - Number of in-context examples vs. performance (4 for GPT2-XL, 8 for larger models)
  - Model scale vs. effectiveness (method works across 1.5B to 13B parameters)

- Failure signatures:
  - Performance degradation when maximum sequence length is exceeded
  - Instability when using too few candidates or in-context examples
  - Reduced effectiveness when inference model cannot accurately estimate conditional entropy

- First 3 experiments:
  1. Verify that TopK+ConE improves performance on SST-2 binary classification with GPT2-XL
  2. Test the method across different model scales (GPT2-XL, Llama2-7B, Llama2-13B) on NLU tasks
  3. Evaluate the method on machine translation tasks with Llama2-7B using COMET scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed TopK + ConE method perform when applied to models of different architectures, such as convolutional neural networks or transformers with different attention mechanisms?
- Basis in paper: [inferred] The paper evaluates the method on various decoder-only dense language models but does not explore different architectures.
- Why unresolved: The paper focuses on decoder-only dense language models and does not investigate the effectiveness of the method on other architectures.
- What evidence would resolve it: Empirical results comparing the performance of TopK + ConE on different model architectures would provide insights into its generalizability.

### Open Question 2
- Question: Can the proposed method be extended to handle more complex tasks, such as multi-modal tasks or tasks involving structured data?
- Basis in paper: [inferred] The paper evaluates the method on natural language understanding and generation tasks but does not explore its applicability to more complex tasks.
- Why unresolved: The paper's scope is limited to natural language tasks, and there is no discussion on the method's potential for handling more complex tasks.
- What evidence would resolve it: Experiments demonstrating the effectiveness of TopK + ConE on multi-modal tasks or tasks involving structured data would indicate its broader applicability.

### Open Question 3
- Question: How does the choice of the number of candidates selected by the TopK method affect the performance of the overall approach?
- Basis in paper: [explicit] The paper mentions that the selection module reduces the space of in-context examples to speed up the process but does not provide a detailed analysis of the impact of the number of candidates.
- Why unresolved: While the paper sets a default candidate number, it does not explore the sensitivity of the method's performance to this hyperparameter.
- What evidence would resolve it: A comprehensive ablation study varying the number of candidates and measuring the impact on performance would provide insights into the optimal choice for different tasks and model sizes.

### Open Question 4
- Question: Can the proposed method be adapted to handle tasks with limited or no labeled data, such as few-shot or zero-shot learning scenarios?
- Basis in paper: [inferred] The paper focuses on in-context learning with a few examples but does not explore the method's potential in few-shot or zero-shot learning scenarios.
- Why unresolved: The paper's experiments are conducted in a few-shot learning setting, and there is no discussion on the method's applicability to scenarios with limited or no labeled data.
- What evidence would resolve it: Experiments demonstrating the effectiveness of TopK + ConE in few-shot or zero-shot learning scenarios would indicate its potential for handling tasks with limited labeled data.

## Limitations

- The method's effectiveness is primarily demonstrated on decoder-only dense language models, with limited results for encoder-decoder architectures
- Performance depends on task-specific templates that are adopted from previous works but not explicitly provided in the paper
- Computational overhead increases with larger candidate sets due to the conditional entropy estimation step

## Confidence

- **High Confidence**: The claim that ICL performance varies with both test data and model selection is well-supported by controlled experiments showing different models perform differently even with identical demonstrations
- **Medium Confidence**: The TopK+ConE method's consistent improvements across different model scales and tasks are demonstrated empirically, but the paper doesn't provide extensive ablation studies on hyperparameters
- **Medium Confidence**: The unified explanation for previous methods' effectiveness through entropy reduction is theoretically sound and supported by conditional entropy calculations

## Next Checks

1. **Architecture Generalization Test**: Evaluate TopK+ConE on a diverse set of model architectures including encoder-only, encoder-decoder, and sparse models to verify the claim that it works across different model types

2. **Template Sensitivity Analysis**: Systematically vary the templates used for demonstration selection while keeping the method constant to quantify how much performance depends on template design versus the selection algorithm itself

3. **Computational Efficiency Benchmark**: Measure the wall-clock time and memory requirements of the TopK+ConE method across different candidate set sizes and model scales to validate the claimed efficiency benefits compared to exhaustive search approaches