---
ver: rpa2
title: 'Sourcerer: Sample-based Maximum Entropy Source Distribution Estimation'
arxiv_id: '2402.07808'
source_url: https://arxiv.org/abs/2402.07808
tags: []
core_contribution: The paper addresses the problem of source distribution estimation
  in scientific modeling, where the goal is to find a distribution of simulator parameters
  that produces outputs matching observed data. The authors propose Sourcerer, a method
  that targets the maximum entropy source distribution to ensure uniqueness and retain
  uncertainty.
---

# Sourcerer: Sample-based Maximum Entropy Source Distribution Estimation

## Quick Facts
- arXiv ID: 2402.07808
- Source URL: https://arxiv.org/abs/2402.07808
- Reference count: 40
- Authors: Julius Vetter; Guy Moss; Cornelius Schröder; Richard Gao; Jakob H. Macke
- One-line primary result: Method estimates maximum entropy source distributions for simulators with intractable likelihoods, recovering distributions with higher entropy without sacrificing simulation quality.

## Executive Summary
This paper addresses the challenge of source distribution estimation in scientific modeling, where the goal is to find a distribution of simulator parameters that produces outputs matching observed data. The authors propose Sourcerer, a method that targets the maximum entropy source distribution to ensure uniqueness and retain uncertainty. By using the Sliced-Wasserstein distance as a sample-based metric, the approach avoids the need for tractable likelihoods and can handle complex simulators. The method employs a penalty method to balance entropy maximization and data fidelity, demonstrating superior performance on benchmark tasks and high-dimensional simulators.

## Method Summary
Sourcerer targets the maximum entropy source distribution to resolve the ill-posedness of source distribution estimation, where multiple parameter distributions can produce the same observed data. The method uses the Sliced-Wasserstein distance to measure the discrepancy between simulated and observed data distributions, enabling sample-based optimization without requiring tractable likelihoods. A penalty method formulation transforms the constrained optimization problem into an unconstrained one, with a dynamic penalty scheme balancing entropy maximization and data fidelity. The approach is evaluated on benchmark tasks and high-dimensional simulators, showing its ability to recover source distributions with substantially higher entropy while maintaining simulation quality.

## Key Results
- Sourcerer recovers source distributions with substantially higher entropy than baselines without sacrificing simulation quality.
- The method is effective on high-dimensional simulators (SIR, Lotka-Volterra) and real-world Hodgkin-Huxley neuron model data.
- Classifier Two-Sample Test (C2ST) accuracy and Sliced-Wasserstein Distance (SWD) metrics demonstrate the method's ability to balance entropy maximization and data fidelity.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Targeting the maximum entropy source distribution resolves ill-posedness by selecting a unique solution among multiple equally valid sources.
- Mechanism: Among all source distributions that produce the same data distribution when pushed through the simulator, the one with maximum entropy is chosen. This is justified because entropy is strictly concave, ensuring a unique maximum.
- Core assumption: There exists at least one source distribution that reproduces the observed data distribution.
- Evidence anchors:
  - [abstract] "To make a principled choice among many equally valid sources, we propose an approach which targets the maximum entropy distribution"
  - [section] "Proposition 2.1. Let Q ⊂ P (Θ) be the set of source distributions q satisfying q# = po, where P (Θ) is the space of probability distributions on Θ. Suppose that Q is non-empty. Then the solution to Eq. (2) exists and is unique."
  - [corpus] No direct corpus evidence for this specific uniqueness claim; relies on mathematical proof in appendix.
- Break condition: If no source distribution exists that can reproduce the observed data distribution, the maximum entropy principle cannot be applied.

### Mechanism 2
- Claim: Using the Sliced-Wasserstein distance enables sample-based optimization without requiring tractable likelihoods.
- Mechanism: The Sliced-Wasserstein distance measures the discrepancy between the simulated and observed data distributions using only samples. This allows optimization even when the simulator's likelihood is intractable.
- Core assumption: The simulator is differentiable, allowing gradients to flow through the simulation process for optimization.
- Evidence anchors:
  - [abstract] "Our method is purely sample-based—leveraging the Sliced-Wasserstein distance to measure the discrepancy between the dataset and simulations—and thus suitable for simulators with intractable likelihoods."
  - [section] "In practice, the constraint in Eq. (2) can be formulated using any distance metric D(·, ·) on the space of probability distributions P (X ), as this constraint is satisfied if and only if D(q#, po) = 0."
  - [corpus] No direct corpus evidence for SWD-specific claims; relies on cited literature (Bonneel et al., 2015; Kolouri et al., 2019; Nadjahi et al., 2020).
- Break condition: If the simulator is non-differentiable and no good surrogate can be trained, gradients cannot be computed for optimization.

### Mechanism 3
- Claim: The penalty method formulation enables unconstrained optimization of the maximum entropy source distribution.
- Mechanism: The constrained optimization problem (maximize entropy subject to data-consistency) is transformed into an unconstrained problem by adding a penalty term that discourages violations of the data-consistency constraint.
- Core assumption: The penalty method provides a good approximation to the constrained problem, and the dynamic penalty scheme (decreasing λ) helps achieve better matching between distributions.
- Evidence anchors:
  - [section] "We adopt a dynamic penalty scheme, where λ is initialized to one at the start of the search, and decreased as the search progresses to achieve better matching between q# and po."
  - [section] "The distance D is squared following the formulation of the penalty method, while the logarithm is introduced to improve numerical stability."
  - [corpus] No direct corpus evidence for the specific dynamic penalty scheme used; relies on general penalty method literature (Platt & Barr, 1987; Bertsekas & Rheinboldt, 2014).
- Break condition: If the penalty method approximation is poor, the solution may not satisfy the data-consistency constraint exactly.

## Foundational Learning

- Concept: Maximum Entropy Principle
  - Why needed here: Provides a principled way to select a unique source distribution when multiple valid sources exist, by choosing the one that retains maximum uncertainty.
  - Quick check question: Why does maximizing entropy help when multiple source distributions can produce the same observed data distribution?

- Concept: Sliced-Wasserstein Distance
  - Why needed here: Enables sample-based optimization without requiring tractable likelihoods, making the method applicable to complex simulators.
  - Quick check question: How does the Sliced-Wasserstein distance differ from standard Wasserstein distance, and why is it computationally advantageous?

- Concept: Penalty Method for Constrained Optimization
  - Why needed here: Transforms the constrained optimization problem into an unconstrained one, making it easier to solve with gradient-based methods.
  - Quick check question: What is the role of the hyperparameter λ in the penalty method formulation, and how does its dynamic adjustment help?

## Architecture Onboarding

- Component map:
  - Source Model (MLP) -> Simulator (differentiable function) -> Sliced-Wasserstein Distance (sample-based metric) -> Entropy Estimator (Kozachenko-Leonenko) -> Penalty Scheme (dynamic λ adjustment) -> Optimizer (Adam)

- Critical path:
  1. Sample parameters from current source model
  2. Simulate observations using the simulator
  3. Compute Sliced-Wasserstein distance between simulated and observed distributions
  4. Estimate entropy of current source model
  5. Compute loss combining entropy and penalty term
  6. Backpropagate gradients and update source model

- Design tradeoffs:
  - Sliced-Wasserstein vs. other metrics: SWD is sample-based and computationally efficient but may be less sensitive to certain distribution features
  - Penalty method vs. exact constrained optimization: Penalty method is simpler but doesn't guarantee exact constraint satisfaction
  - Neural network architecture for source model: More complex architectures may capture distributions better but are harder to train

- Failure signatures:
  - High Sliced-Wasserstein distance with low entropy: Constraint not being satisfied well
  - Low Sliced-Wasserstein distance but also low entropy: Optimization stuck in local minimum or λ too large
  - Unstable training: Learning rate too high or architecture mismatch

- First 3 experiments:
  1. Run source estimation on the Two Moons benchmark with λ = 0.35 to verify basic functionality
  2. Compare results with and without the entropy objective on the IK simulator
  3. Test the method on the deterministic SIR model to verify it handles deterministic simulators correctly

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust is the maximum entropy source distribution estimation to different distance metrics beyond the Sliced-Wasserstein distance?
- Basis in paper: [explicit] The paper mentions that "different distance metrics can lead to different sources, depending on its sensitivity to different features" and that the method is compatible with any sample-based differentiable distance metric.
- Why unresolved: The experiments in the paper primarily use the Sliced-Wasserstein distance. There is no exploration of how the choice of distance metric affects the estimated source distribution.
- What evidence would resolve it: Empirical comparisons of source distributions estimated using different distance metrics (e.g., Wasserstein distance, Maximum Mean Discrepancy) on the same tasks would provide insight into the robustness of the method to the choice of distance metric.

### Open Question 2
- Question: What is the impact of the reference distribution on the estimated source distribution, especially when the reference distribution is non-uniform?
- Basis in paper: [explicit] The paper discusses the case where a reference distribution is available and adapts the objective function to minimize the Kullback-Leibler divergence between the source and the reference distribution.
- Why unresolved: The experiments primarily focus on the case where the reference distribution is uniform. There is no exploration of how the estimated source distribution changes when the reference distribution is non-uniform.
- What evidence would resolve it: Empirical comparisons of source distributions estimated with different reference distributions (e.g., Gaussian, informative prior) on the same tasks would provide insight into the impact of the reference distribution.

### Open Question 3
- Question: How does the performance of the method scale with the dimensionality of the parameter space and the observation space?
- Basis in paper: [inferred] The paper mentions that the method is sample-based and can scale to higher dimensional problems, as demonstrated on the Lotka-Volterra and SIR models with 100 and 50-dimensional observation spaces, respectively.
- Why unresolved: While the paper demonstrates the method on some high-dimensional tasks, there is no systematic exploration of how the performance scales with increasing dimensionality of the parameter and observation spaces.
- What evidence would resolve it: Empirical studies of the method's performance on tasks with varying parameter and observation space dimensionalities would provide insight into its scalability.

## Limitations

- The method's performance is sensitive to the choice of the hyperparameter λ in the penalty method, which requires careful tuning.
- The theoretical guarantees of the method rely on the existence of a source distribution that can reproduce the observed data distribution, which may not always be the case in practice.
- The method's scalability to very high-dimensional parameter spaces and its robustness to noisy observations are not thoroughly explored.

## Confidence

- **High Confidence**: The theoretical framework of using maximum entropy to select a unique source distribution and the use of Sliced-Wasserstein distance as a sample-based metric are well-supported by established literature.
- **Medium Confidence**: The specific implementation details, such as the dynamic penalty scheme and the neural network architectures used for the source model, are not fully specified and may impact the method's performance.
- **Low Confidence**: The method's generalizability to a wide range of scientific simulators and its ability to handle complex, high-dimensional parameter spaces are not thoroughly explored.

## Next Checks

1. **Ablation Study on Penalty Scheme**: Conduct a comprehensive ablation study on the dynamic penalty scheme, varying the initial value and decay rate of λ to assess its impact on the trade-off between entropy maximization and data fidelity.
2. **Robustness to Noise**: Test the method's robustness to noisy observations by adding varying levels of noise to the observed data and evaluating its ability to recover the true source distribution.
3. **Comparison with Alternative Metrics**: Compare the performance of Sourcerer using Sliced-Wasserstein distance with alternative sample-based metrics, such as Maximum Mean Discrepancy (MMD) or Energy Distance, to assess the sensitivity of the method to the choice of distance metric.