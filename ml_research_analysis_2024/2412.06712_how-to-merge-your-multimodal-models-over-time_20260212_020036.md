---
ver: rpa2
title: How to Merge Your Multimodal Models Over Time?
arxiv_id: '2412.06712'
source_url: https://arxiv.org/abs/2412.06712
tags:
- merging
- arxiv
- task
- temporal
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies temporal model merging, where expert models
  are continuously merged over time as new tasks emerge, contrasting with standard
  offline merging. The authors introduce TIME, a unified framework with three axes:
  initialization (how to start training each expert), deployment (which model to deploy
  at each step), and merging techniques (how to combine models).'
---

# How to Merge Your Multimodal Models Over Time?

## Quick Facts
- arXiv ID: 2412.06712
- Source URL: https://arxiv.org/abs/2412.06712
- Reference count: 40
- Primary result: Temporal model merging effectiveness depends primarily on initialization and deployment strategies rather than merging techniques, with EMA-based approaches showing consistent superiority

## Executive Summary
This paper studies temporal model merging, where expert models are continuously merged over time as new tasks emerge, contrasting with standard offline merging. The authors introduce TIME, a unified framework with three axes: initialization (how to start training each expert), deployment (which model to deploy at each step), and merging techniques (how to combine models). Through extensive experiments on the FoMo-in-Flux benchmark, they find that initialization and deployment strategies are far more important than the specific merging technique used. The EMA-based initialization and deployment approach (Best-in-TIME) consistently outperforms other methods, particularly as model size and compute budget increase. Larger models and higher compute budgets significantly improve temporal merging effectiveness, with Best-in-TIME approaching the performance of multitask training at higher compute levels. The work establishes best practices for temporal model merging in multimodal settings.

## Method Summary
The paper introduces TIME, a unified framework for temporal model merging that examines three key axes: initialization, deployment, and merging techniques. The framework is tested on the FoMo-in-Flux benchmark using ViT-B/16 CLIP models pretrained on LAION-2B with a DataComp-Small computation budget of 1.8 Ã— 10^9 GFLOPS. The authors evaluate performance using Knowledge Accumulation (AKA) and Zero-Shot Retention (AZS) metrics, with the geometric mean of both as the primary objective. The method involves training expert models per task using various initialization protocols (initZS, initFT, initEMA), deploying final models using different strategies (deployFT, deployEMA, deployALL), and combining them using nine merging techniques including weight averaging, SLERP, task arithmetic, TIES, DARE, Breadcrumbs, MagMax, and LiNeS.

## Key Results
- The EMA-based initialization and deployment approach (Best-in-TIME) consistently outperforms other methods across all tested conditions
- Initialization and deployment strategies are far more important than the specific merging technique used, with all techniques performing similarly when combined with optimal initialization/deployment
- Larger models and higher compute budgets significantly improve temporal merging effectiveness, enabling performance approaching multitask training
- The Best-in-TIME approach continues to improve through 100 tasks, unlike offline merging techniques which degrade significantly

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Temporal model merging succeeds when initialization and deployment strategies prioritize recent knowledge while retaining past performance.
- Mechanism: The Best-in-TIME approach uses EMA-based initialization and deployment to smoothly blend new task knowledge with existing expertise, avoiding catastrophic forgetting and maintaining adaptability.
- Core assumption: Model weights exhibit sufficient mode connectivity and stability for EMA-style interpolation to be effective across sequential tasks.
- Evidence anchors:
  - [abstract]: "The EMA-based initialization and deployment approach (Best-in-TIME) consistently outperforms other methods"
  - [section 4.4]: "One combination that stands out consistently is initEMA alongside deployEMA"
  - [corpus]: Weak - no direct evidence in cited papers about EMA for temporal merging
- Break condition: If task distributions shift too rapidly or model weights lose mode connectivity, EMA interpolation fails to preserve both new and old knowledge.

### Mechanism 2
- Claim: The choice of merging technique matters far less than initialization and deployment strategies for temporal model merging.
- Mechanism: All merging techniques converge to similar performance when combined with optimal initialization/deployment because the temporal integration problem is dominated by how models are prepared and selected rather than how their weights are combined.
- Core assumption: Over sufficiently long sequences, differences between merging algorithms become negligible compared to initialization/deployment effects.
- Evidence anchors:
  - [section 4.5]: "all merging techniques perform very similarly... the specific merging technique used is much less important"
  - [section 4.2]: "there are only marginal differences between merging techniques when deployed in an offline manner"
  - [corpus]: Weak - corpus papers focus on technique-specific improvements rather than initialization/deployment dominance
- Break condition: If merging techniques have fundamentally different biases toward preserving certain types of knowledge, the convergence assumption breaks down.

### Mechanism 3
- Claim: Scaling model size and compute budget significantly improves temporal merging effectiveness, enabling performance approaching multitask training.
- Mechanism: Larger models and higher compute budgets provide more stable optimization landscapes and better capacity to integrate diverse knowledge, making temporal merging increasingly effective.
- Core assumption: Model scale and training compute improve the quality of individual experts and the stability of their combination.
- Evidence anchors:
  - [section 4.6]: "Larger models or compute budgets allows greater benefits from temporal merging" and "Scaling enables temporal model merging to even outperform the multitask model"
  - [abstract]: "Larger models and higher compute budgets significantly improve temporal merging effectiveness"
  - [corpus]: Moderate - several cited papers discuss scaling benefits for model merging
- Break condition: If scaling introduces optimization instabilities or if the computational budget is insufficient to properly train individual experts, the scaling benefits disappear.

## Foundational Learning

- Concept: Mode connectivity and weight interpolation stability
  - Why needed here: Temporal merging relies on smooth interpolation between model weights, which requires understanding when and why weight averaging works
  - Quick check question: What conditions must hold for two fine-tuned models to be effectively merged via simple weight averaging?

- Concept: Continual learning tradeoffs (forward transfer vs. backward transfer)
  - Why needed here: Temporal merging must balance learning new tasks while retaining old knowledge, a core continual learning challenge
  - Quick check question: How does the choice of initialization strategy affect the balance between acquiring new knowledge and preserving existing knowledge?

- Concept: Exponential moving average (EMA) in optimization
  - Why needed here: Best-in-TIME uses EMA for both initialization and deployment, requiring understanding of how EMA smooths optimization trajectories
  - Quick check question: What hyperparameter controls the decay rate in EMA, and how does it affect the trade-off between stability and adaptability?

## Architecture Onboarding

- Component map: Base model (ViT-B/16 CLIP) -> Expert training pipeline -> Storage buffer -> Initialization module -> Deployment module -> Merging technique library -> Evaluation pipeline
- Critical path: 1. Initialize weights for new task using chosen strategy 2. Train expert model on task data 3. Store expert checkpoint 4. Deploy final model using chosen strategy 5. Evaluate performance
- Design tradeoffs:
  - Storage vs. computation: Storing all expert checkpoints enables arbitrary merging strategies but increases memory requirements
  - Adaptation speed vs. retention: EMA initialization favors retention while recent checkpoint initialization favors rapid adaptation
  - Complexity vs. performance: Simple merging techniques often perform as well as complex ones when combined with good initialization/deployment
- Failure signatures:
  - Rapid performance degradation on old tasks indicates poor retention
  - Stagnant performance on new tasks indicates poor adaptation
  - Inconsistent results across runs suggest hyperparameter sensitivity
  - All methods converging to similar poor performance suggests dataset or task sequence issues
- First 3 experiments:
  1. Implement baseline replay method and verify it outperforms all offline merging approaches
  2. Test initEMA + deployEMA combination to confirm Best-in-TIME performance
  3. Sweep different merging techniques with Best-in-TIME initialization/deployment to verify technique independence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different merging techniques perform when extended to non-linear architectures beyond ViT-B/16 CLIP?
- Basis in paper: [inferred] The paper only tests merging techniques on ViT-B/16 CLIP models, leaving the performance on other architectures unexplored.
- Why unresolved: The study focuses on a specific architecture, and extending results to diverse architectures requires additional experimentation.
- What evidence would resolve it: Empirical results showing performance comparisons of various merging techniques across multiple architectures like ResNets, Swin Transformers, or hybrid models.

### Open Question 2
- Question: What is the impact of task diversity on the effectiveness of temporal model merging?
- Basis in paper: [inferred] The paper uses a fixed set of 63 datasets from FoMo-in-Flux, but does not explore how varying task diversity affects merging performance.
- Why unresolved: The study does not systematically vary task diversity, which could significantly influence the effectiveness of merging strategies.
- What evidence would resolve it: Experiments varying task diversity and measuring the impact on merging performance across different initialization and deployment strategies.

### Open Question 3
- Question: How does the performance of temporal model merging scale with increasing task sequence lengths beyond 100 tasks?
- Basis in paper: [explicit] The paper tests up to 100 tasks and notes that Best-in-TIME continues to improve, but does not explore longer sequences.
- Why unresolved: The study stops at 100 tasks, leaving the behavior of merging strategies at even longer horizons unexplored.
- What evidence would resolve it: Results from experiments with task sequences longer than 100 tasks, showing whether performance plateaus, degrades, or continues to improve.

## Limitations
- The study focuses exclusively on image-text models (ViT-B/16 CLIP), limiting applicability to other modalities like text-only or audio-visual models
- Results show diminishing returns for temporal merging at lower compute budgets, suggesting limited practical utility for resource-constrained settings
- The FoMo-in-Flux benchmark, while extensive, may not capture all real-world temporal merging scenarios with heterogeneous task distributions

## Confidence
- High Confidence: The core finding that initialization and deployment strategies dominate merging technique choice is well-supported by extensive ablation studies across 63 datasets and multiple evaluation metrics.
- Medium Confidence: The scaling benefits claim (larger models and higher compute budgets improving temporal merging) is demonstrated within the tested range but may not extrapolate indefinitely. The approach of approaching multitask performance is promising but may have fundamental limits.
- Low Confidence: The assertion that "all merging techniques perform similarly" when combined with optimal initialization/deployment needs more scrutiny, as some techniques may have hidden biases not captured in the current evaluation setup.

## Next Checks
1. **Cross-modal validation:** Test the Best-in-TIME approach on text-only and audio-visual models to verify the generality of initialization/deployment dominance across modalities.

2. **Dynamic task distribution analysis:** Evaluate the framework on datasets with rapidly shifting distributions to test the break condition where EMA interpolation fails under rapid task changes.

3. **Computational efficiency benchmarking:** Compare the storage and computation costs of Best-in-TIME against alternative approaches like progressive neural networks or rehearsal-based methods across different compute budgets.