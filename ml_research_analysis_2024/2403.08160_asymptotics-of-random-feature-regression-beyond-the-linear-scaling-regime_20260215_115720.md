---
ver: rpa2
title: Asymptotics of Random Feature Regression Beyond the Linear Scaling Regime
arxiv_id: '2403.08160'
source_url: https://arxiv.org/abs/2403.08160
tags:
- where
- error
- proof
- lemma
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the asymptotic behavior of random feature\
  \ ridge regression (RFRR) in high-dimensional settings. The authors consider covariates\
  \ uniformly distributed on a d-dimensional sphere and compute sharp asymptotics\
  \ for the RFRR test error in a polynomial scaling regime where p, n, d \u2192 \u221E\
  \ while p/d\u03BA1 and n/d\u03BA2 remain constant."
---

# Asymptotics of Random Feature Regression Beyond the Linear Scaling Regime

## Quick Facts
- arXiv ID: 2403.08160
- Source URL: https://arxiv.org/abs/2403.08160
- Reference count: 40
- This paper investigates the asymptotic behavior of random feature ridge regression (RFRR) in high-dimensional settings with polynomial scaling.

## Executive Summary
This paper investigates the asymptotic behavior of random feature ridge regression (RFRR) in high-dimensional settings with polynomial scaling. The authors analyze RFRR performance when the number of random features p and sample size n scale polynomially with dimension d (p/d^κ₁ → θ₁ and n/d^κ₂ → θ₂). They provide sharp asymptotic formulas for test error across different regimes and establish an equivalence with a simpler Gaussian covariate model. The work extends previous characterizations beyond the linear scaling case (κ₁ = κ₂ = 1) and reveals an intuitive trade-off between approximation and generalization power, along with a double descent phenomenon at n = p.

## Method Summary
The paper studies random feature ridge regression (RFRR) in a high-dimensional setting where covariates are uniformly distributed on a d-dimensional sphere. The authors analyze the asymptotic behavior of RFRR when p, n, and d all approach infinity while maintaining polynomial scaling relationships. They use spherical harmonics and Gegenbauer polynomials to decompose target functions and activation functions, enabling analysis of RFRR performance across different scaling regimes. The methodology involves computing sharp asymptotics for test error by analyzing empirical distributions of singular values and establishing fixed-point equations for the critical regime.

## Key Results
- RFRR exhibits an intuitive trade-off between approximation and generalization: when n = o(p), sample size limits performance and RFRR matches kernel ridge regression; when p = o(n), the number of random features limits performance and test error matches the approximation error of the random feature model class.
- A double descent phenomenon appears at n = p in the critical regime κ₁ = κ₂, previously characterized only in linear scaling.
- The asymptotic risk of RFRR in the polynomial scaling regime is equivalent to a simpler Gaussian covariate model, extending the Gaussian equivalence principle beyond linear scaling.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Random Feature Ridge Regression (RFRR) exhibits an intuitive trade-off between approximation and generalization power in high-dimensional polynomial scaling.
- Mechanism: When n = o(p), sample size is the bottleneck and RFRR matches Kernel Ridge Regression (KRR) performance; when p = o(n), the number of random features is limiting and test error matches the approximation error of the random feature model class.
- Core assumption: The number of random features p and sample size n scale polynomially with dimension d, specifically p/d^κ₁ → θ₁ and n/d^κ₂ → θ₂.
- Evidence anchors:
  - [abstract]: "For n = o(p), the sample size n is the bottleneck and RFRR achieves the same performance as KRR (which is equivalent to taking p = ∞). On the other hand, if p = o(n), the number of random features p is the limiting factor and RFRR test error matches the approximation error of the random feature model class (akin to taking n = ∞)."
  - [paper text]: "The number of features is now the bottleneck for learning f* and RFRR achieves the best approximation error over the random feature model class FRF(W) akin to having n = ∞."
- Break condition: The scaling assumption breaks if p or n doesn't scale polynomially with d, or if the activation function doesn't satisfy Assumption 1.

### Mechanism 2
- Claim: RFRR displays a double descent phenomenon at n = p in the critical regime κ₁ = κ₂.
- Mechanism: The peak in test error at the interpolation threshold n = p is due to the divergence of the conditioning number of the feature matrix Z as p approaches n and Z becomes a square matrix.
- Core assumption: The activation function σ satisfies Assumption 1 at level ℓ, meaning μₖ ≠ 0 for k = 0, ..., ℓ and μ₂>ℓ > 0.
- Evidence anchors:
  - [abstract]: "A double descent appears at n = p, a phenomenon that was previously only characterized in the linear scaling κ₁ = κ₂ = 1."
  - [paper text]: "In this regime, the test error exhibits a non-monotonic behavior with a peak at the interpolation threshold n/p = θ₂/θ₁ = 1. This corresponds to the double-descent phenomenon, which was previously characterized by [MM22] in the linear scaling κ₁ = κ₂ = 1."
- Break condition: The double descent disappears if the activation function is a polynomial of degree ℓ, violating Assumption 1(c).

### Mechanism 3
- Claim: The asymptotic risk of RFRR in the polynomial scaling is the same as a simpler Gaussian covariate model.
- Mechanism: The empirical distribution of singular values of the feature matrices in the original random feature model and the random Gaussian feature model are asymptotically the same.
- Core assumption: The target function f* satisfies Assumption 2 with random high-degree coefficients at level ℓ ∈ N.
- Evidence anchors:
  - [abstract]: "This equivalence was already noticed in [MM22] in the linear scaling κ₁ = κ₂ = 1 (with a simplified Gaussian model). Here we show that this equivalence holds more generally in the entire polynomial scaling."
  - [paper text]: "Intuitively, this equivalence holds in the polynomial scaling regime because the empirical distribution of singular values of the feature matrices in the original random feature model and the random Gaussian feature model are asymptotically the same."
- Break condition: The equivalence fails for losses and regularizations beyond the linear regime, as noted in the paper.

## Foundational Learning

- Concept: Polynomial scaling regime (p/d^κ₁ → θ₁ and n/d^κ₂ → θ₂)
  - Why needed here: This scaling allows RFRR to fit polynomial approximations of increasing degree to the target function, revealing the staircase decay and multi-phase learning behavior.
  - Quick check question: If κ₁ = 2 and κ₂ = 1, what is the degree of the polynomial approximation that RFRR can fit?

- Concept: Spherical harmonics and Gegenbauer polynomials
  - Why needed here: These provide the orthogonal basis for L²(Sd-1(√d)) and are crucial for decomposing the target and activation functions, enabling the analysis of RFRR in high dimensions.
  - Quick check question: How does the addition theorem relate Gegenbauer polynomials to spherical harmonics?

- Concept: Fixed-point equations for asymptotic analysis
  - Why needed here: The fixed-point equations characterize the asymptotic traces of matrix functionals, which are essential for computing the test error in the critical regime κ₁ = κ₂.
  - Quick check question: What is the role of the functions τ₁ and τ₂ in the fixed-point equations?

## Architecture Onboarding

- Component map:
  Random feature model class FRF(W) -> Ridge regression with regularization parameter λ -> Target function f* ∈ L²(Sd-1(√d)) -> Covariates uniformly distributed on Sd-1(√d) -> Random feature weights uniformly distributed on Sd-1(1) -> Asymptotic analysis in polynomial scaling regime

- Critical path:
  1. Define the random feature model class and ridge regression problem
  2. Establish the polynomial scaling assumptions
  3. Compute the sharp asymptotics for the test error in different regimes (overparametrized, underparametrized, critical)
  4. Prove the Gaussian equivalence principle
  5. Validate the results with numerical experiments

- Design tradeoffs:
  - Exact vs. approximate analysis: The paper provides sharp asymptotics but assumes certain conditions on the target function and activation function.
  - Generality vs. specificity: The results are specific to the random feature model but provide insights into more general neural network models.

- Failure signatures:
  - If the target function doesn't satisfy Assumption 2, the convergence of the test error to the asymptotic formula may fail.
  - If the activation function is a polynomial of degree ℓ, the double descent phenomenon disappears.

- First 3 experiments:
  1. Verify the staircase decay of the risk curves by plotting the test error vs. log(n)/log(d) for fixed p.
  2. Confirm the double descent at n = p by plotting the test error vs. n for fixed p in the critical regime κ₁ = κ₂.
  3. Test the Gaussian equivalence principle by comparing the empirical spectral densities of the random feature matrix and the equivalent Gaussian feature matrix.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the asymptotic equivalence between random feature models and simpler Gaussian covariate models extend beyond ridge regression to other loss functions and regularization schemes?
- Basis in paper: [explicit] The paper shows Gaussian equivalence for ridge regression in polynomial scaling, building on previous work in linear scaling [MM22, HL22b, MS22]. The authors conjecture this may hold for other losses and regularizations but note it "will not be true in general for other losses and regularization functions beyond the linear regime."
- Why unresolved: The proof technique relies heavily on properties of ridge regression with squared loss. Extending to other loss functions would require different analytical approaches and the paper only provides empirical evidence for the equivalence in this case.
- What evidence would resolve it: A rigorous mathematical proof showing either (1) Gaussian equivalence holds for a broader class of convex losses with appropriate regularization, or (2) a concrete counterexample where the equivalence breaks down for non-quadratic losses.

### Open Question 2
- Question: Can the pointwise convergence results for RFRR test error be established for arbitrary target functions beyond the class with randomized high-frequency coefficients?
- Basis in paper: [explicit] The authors state their results hold with high probability over target functions with random high-degree coefficients (Assumption 2) and conjecture convergence holds for "any fixed target function f* ∈ L²(Sd-1(√d))". Previous work [MMM22] showed pointwise convergence for κ₁ ≠ κ₂, ∉ N, but this remains open for the polynomial scaling regime.
- Why unresolved: The current proof relies on randomization of high-frequency coefficients to simplify the analysis by reducing computations to trace calculations. Handling arbitrary deterministic functions would require more sophisticated deterministic equivalents for the relevant random matrix functionals.
- What evidence would resolve it: A rigorous proof establishing either (1) pointwise convergence for all f* ∈ L²(Sd-1(√d)) using deterministic matrix analysis techniques, or (2) a specific counterexample function where the convergence fails.

### Open Question 3
- Question: What is the optimal choice of activation function σ that maximizes test performance in RFRR across different polynomial scaling regimes?
- Basis in paper: [explicit] The authors show results depend on Hermite coefficients of σ through parameters like ζ = μℓ/μ>ℓ and the implicit regularization from high-frequency components. They assume σ satisfies Assumption 1 but don't explore how different choices affect performance.
- Why unresolved: The paper characterizes the impact of σ on RFRR performance but doesn't systematically study which activation functions perform best. The analysis focuses on universality conditions rather than optimization over activation functions.
- What evidence would resolve it: A comprehensive study comparing test error across different activation functions (ReLU, tanh, sigmoid, polynomial activations) in various polynomial scaling regimes (κ₁ > κ₂, κ₁ < κ₂, κ₁ = κ₂), identifying which functions provide optimal approximation-statistical trade-offs.

### Open Question 4
- Question: Does the staircase decay phenomenon persist when covariates are drawn from distributions other than uniform on the sphere?
- Basis in paper: [inferred] The analysis relies heavily on spherical harmonics and Gegenbauer polynomials which are specific to spherical geometry. The authors only consider covariates uniformly distributed on Sd-1(√d).
- Why unresolved: The spherical geometry enables the elegant decomposition in terms of polynomial degrees and the staircase behavior emerges from this structure. Alternative distributions would require different orthogonal polynomial bases and may not exhibit the same incremental learning pattern.
- What evidence would resolve it: Empirical or theoretical analysis of RFRR performance with covariates from alternative distributions (Gaussian, uniform cube, etc.) to determine whether polynomial-degree-based staircase decay generalizes beyond spherical data.

### Open Question 5
- Question: How does the double descent peak height scale with the regularization parameter λ as λ → 0+ in the critical regime κ₁ = κ₂?
- Basis in paper: [explicit] The authors note that "the peak diverges as λ → 0+" at the interpolation threshold n = p in the critical regime, but don't characterize the precise scaling relationship or provide bounds on peak height.
- Why unresolved: While the paper establishes existence of the divergence, the mathematical analysis becomes more complex near the interpolation threshold where the feature matrix becomes ill-conditioned. The exact scaling relationship requires careful analysis of the conditioning effects.
- What evidence would resolve it: A rigorous proof establishing the precise scaling of the double descent peak height with λ as λ → 0+, potentially showing it diverges as λ^(-α) for some α > 0, along with bounds on the exponent.

## Limitations
- The analysis assumes specific polynomial scaling relationships between p, n, and d, which may not capture all practical scenarios.
- The results rely heavily on spherical harmonics and Gegenbauer polynomials, making them specific to the d-dimensional sphere setting.
- The Gaussian equivalence principle, while elegant, requires careful validation in practical applications beyond the ridge regression setting.

## Confidence
- High confidence in the asymptotic formulas for test error in different scaling regimes (κ₁ > κ₂, κ₁ < κ₂, κ₁ = κ₂)
- Medium confidence in the double descent characterization at n = p, as this requires specific activation function properties
- Medium confidence in the Gaussian equivalence principle, which has been verified in previous work but may not generalize to all settings

## Next Checks
1. Verify the staircase decay of risk curves by plotting the test error vs. log(n)/log(d) for various p values and confirming the predicted phase transitions.

2. Confirm the double descent phenomenon at n = p by conducting numerical experiments with different activation functions, ensuring Assumption 1 is satisfied, and comparing empirical test errors with theoretical predictions.

3. Test the Gaussian equivalence principle by comparing empirical spectral densities of the random feature matrix and the equivalent Gaussian feature matrix for various target functions and activation functions, verifying that the distributions match asymptotically.