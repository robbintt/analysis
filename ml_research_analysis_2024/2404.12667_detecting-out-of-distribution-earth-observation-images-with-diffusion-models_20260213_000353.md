---
ver: rpa2
title: Detecting Out-Of-Distribution Earth Observation Images with Diffusion Models
arxiv_id: '2404.12667'
source_url: https://arxiv.org/abs/2404.12667
tags:
- diffusion
- images
- detection
- image
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work demonstrates that diffusion models can effectively detect
  out-of-distribution (OOD) remote sensing images by using reconstruction error as
  a plausibility score. The authors introduce ODEED, a novel reconstruction-based
  scorer that leverages the probability-flow ordinary differential equation (ODE)
  of diffusion models for improved OOD detection.
---

# Detecting Out-Of-Distribution Earth Observation Images with Diffusion Models

## Quick Facts
- **arXiv ID**: 2404.12667
- **Source URL**: https://arxiv.org/abs/2404.12667
- **Reference count**: 40
- **Primary result**: Demonstrates that diffusion model reconstruction errors can effectively detect out-of-distribution remote sensing images, with a novel ODEED scorer achieving up to 94.5 AUC and 24.6 FPR95% on near-OOD flood detection scenarios.

## Executive Summary
This work introduces ODEED, a novel reconstruction-based scorer for detecting out-of-distribution (OOD) remote sensing images using diffusion models. The method leverages the probability-flow ordinary differential equation (PF-ODE) of diffusion models to produce deterministic reconstructions that better represent whether an image belongs to the modeled distribution. Tested on SpaceNet 8 with various scenarios including geographical shifts and near-OOD cases like pre/post-flood image recognition, ODEED significantly outperforms other diffusion-based and discriminative baselines, particularly in challenging near-OOD scenarios. The approach demonstrates strong potential for unsupervised anomaly detection in remote sensing applications where labeled data for rare events is scarce.

## Method Summary
The authors train diffusion models on pre-event remote sensing images and use reconstruction error as an OOD detection score. ODEED, the proposed method, uses the PF-ODE to deterministically encode clean images into noisy latents and then decode them back, measuring similarity between original and reconstructed images. The approach is compared against diffusion loss scoring, autoencoders, and discriminative OOD detectors on SpaceNet 8 datasets with pre/post-flood, flooded/non-flooded, and domain shift scenarios.

## Key Results
- ODEED achieves 94.5 AUC and 24.6 FPR95% on near-OOD flood detection setups
- Outperforms best baseline by 13.4 points on Germany dataset and 3.6 points on Louisiana dataset in terms of AUC
- Demonstrates superior performance compared to diffusion loss, autoencoder reconstruction, and discriminative OOD detection methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diffusion models trained only on pre-event images can reconstruct pre-event images well but produce larger reconstruction errors on post-event and flooded images.
- Mechanism: Diffusion models learn the data distribution of pre-event images. When encountering post-event or flooded images that differ visually, the denoising process struggles to reverse the corruption, leading to higher reconstruction errors.
- Core assumption: The pre-event image distribution is distinct enough from post-event/flooded images that diffusion models cannot reconstruct them well.
- Evidence anchors:
  - [abstract] "We show that the reconstruction error of diffusion models can effectively serve as unsupervised out-of-distribution detectors for remote sensing images"
  - [section] "We expect it to fail to reconstruct images with clouds"
  - [corpus] Weak - no direct corpus evidence for this specific mechanism
- Break condition: If the pre-event and post-event distributions overlap significantly, reconstruction errors may not differ enough to detect OOD samples.

### Mechanism 2
- Claim: ODEED leverages the deterministic nature of the probability flow ODE to produce consistent reconstructions, improving OOD detection compared to stochastic samplers.
- Mechanism: The PF-ODE provides a deterministic path from corrupted to clean images. By using this instead of stochastic sampling, ODEED eliminates reconstruction variability, making the OOD score more reliable.
- Core assumption: The deterministic PF-ODE produces better reconstructions for in-distribution samples than stochastic sampling.
- Evidence anchors:
  - [section] "Integrating the PF-ODE Eq. (3) from 0 to t0 encodes the initial image x0 into a unique noisy latent xt0... Conversely, integrating it from t0 to 0 allows us to get back to the exact initial image"
  - [section] "ODEED a novel reconstruction-based OOD scorer based on the PF-ODE trajectories... It is entirely deterministic and therefore should better represent whether x0 belongs to the modeled distribution"
  - [corpus] Weak - no direct corpus evidence for this specific mechanism
- Break condition: If the diffusion model approximation of the score function is poor, the ODEED reconstruction may not be accurate even for in-distribution samples.

### Mechanism 3
- Claim: The LPIPS metric is more effective than MSE for detecting near-OOD samples like floods because it captures perceptual differences.
- Mechanism: LPIPS measures perceptual similarity between images, which can better capture the visual differences between flooded and non-flooded areas compared to pixel-wise MSE.
- Core assumption: Perceptual differences are more relevant for detecting near-OOD samples than pixel-wise differences.
- Evidence anchors:
  - [section] "we compute the weighted average loss... For images, common metrics are the Mean-Squared Error (MSE) and the Learned Perceptual Image Patch Similarity (LPIPS), the latter being more aligned with human perception"
  - [section] "ODEED + LPIPS outperforms the best baseline by a margin of 13.4 points on the Germany dataset and 3.6 pts on the Louisiana dataset in terms of AUC"
  - [corpus] Weak - no direct corpus evidence for this specific mechanism
- Break condition: If the visual differences between ID and OOD samples are primarily pixel-level rather than perceptual, MSE might be more effective.

## Foundational Learning

- Concept: Diffusion models and their training process
  - Why needed here: Understanding how diffusion models work is crucial for implementing ODEED and interpreting the results
  - Quick check question: What is the difference between the forward SDE and the reverse SDE in diffusion models?

- Concept: Out-of-distribution detection and its importance in remote sensing
  - Why needed here: The paper's main contribution is applying OOD detection to remote sensing images using diffusion models
  - Quick check question: Why is detecting OOD samples important in remote sensing applications?

- Concept: Probability Flow ODE and its properties
  - Why needed here: ODEED specifically leverages the PF-ODE for deterministic reconstructions
  - Quick check question: What is the key property of the PF-ODE that makes it useful for OOD detection?

## Architecture Onboarding

- Component map:
  - Diffusion model (DÎ¸) -> ODE solver -> Similarity metrics (MSE, LPIPS) -> OOD score

- Critical path:
  1. Train diffusion model on pre-event images
  2. Implement ODEED scorer using the trained diffusion model
  3. Evaluate OOD detection performance on test datasets

- Design tradeoffs:
  - Deterministic (ODEED) vs stochastic sampling - trade-off between consistency and potentially better exploration
  - MSE vs LPIPS - trade-off between pixel-level and perceptual similarity
  - Time step t0 - trade-off between reconstruction quality and computational cost

- Failure signatures:
  - Poor AUC/FPR95% scores - indicates the method isn't effectively distinguishing ID from OOD samples
  - High reconstruction errors on ID samples - suggests the diffusion model isn't learning the distribution well
  - Similar score distributions for ID and OOD - indicates the method isn't capturing the differences between the distributions

- First 3 experiments:
  1. Train a basic diffusion model on a simple dataset (e.g., MNIST) and evaluate reconstruction errors on in-distribution vs out-of-distribution samples
  2. Implement the ODEED scorer and compare its performance to the diffusion loss scorer on the same dataset
  3. Test different similarity metrics (MSE vs LPIPS) for OOD detection on a remote sensing dataset (e.g., SpaceNet 8)

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions, but based on the limitations and discussion, several areas remain unexplored.

## Limitations
- The method relies on reconstruction error differences that may not hold when ID and OOD distributions have significant overlap
- Experiments focus primarily on flood detection scenarios, limiting broader applicability claims
- ODEED implementation details, particularly the exact PF-ODE solver configuration and t0 selection, are underspecified

## Confidence
- **High Confidence**: The core claim that diffusion model reconstruction errors can detect OOD remote sensing images is well-supported by the experimental results
- **Medium Confidence**: The superiority of ODEED over stochastic samplers is demonstrated but could benefit from more ablation studies on different diffusion model architectures
- **Medium Confidence**: The LPIPS vs MSE comparison is empirically valid but lacks theoretical justification for why perceptual metrics work better for near-OOD detection

## Next Checks
1. Test ODEED on datasets with gradual distribution shifts (e.g., seasonal changes) to evaluate performance beyond binary flood detection
2. Compare ODEED with other generative OOD detection methods (e.g., likelihood-based approaches) to establish relative advantages
3. Conduct ablation studies varying diffusion model architecture parameters to identify which components most influence OOD detection performance