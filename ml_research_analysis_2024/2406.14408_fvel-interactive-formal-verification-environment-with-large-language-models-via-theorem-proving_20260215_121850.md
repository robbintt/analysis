---
ver: rpa2
title: 'FVEL: Interactive Formal Verification Environment with Large Language Models
  via Theorem Proving'
arxiv_id: '2406.14408'
source_url: https://arxiv.org/abs/2406.14408
tags:
- fvel
- proof
- verification
- isabelle
- lemmas
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FVEL introduces an interactive formal verification environment
  that integrates large language models with automated theorem proving. The approach
  transforms C code into Isabelle formal language and conducts verification via neural
  automated theorem proving with LLMs.
---

# FVEL: Interactive Formal Verification Environment with Large Language Models via Theorem Proving

## Quick Facts
- arXiv ID: 2406.14408
- Source URL: https://arxiv.org/abs/2406.14408
- Reference count: 40
- Primary result: FVEL with FVELER fine-tuning improves SV-COMP performance by 17.39% (69→81) and Mistral-7B by 12% (75→84) while reducing proof errors

## Executive Summary
FVEL introduces an interactive formal verification environment that integrates large language models with automated theorem proving. The approach transforms C code into Isabelle formal language and conducts verification via neural automated theorem proving with LLMs. A large-scale FVELER dataset is constructed containing 758 theories, 29,125 lemmas, and 200,646 proof steps with deep dependencies. Benchmarking shows that FVEL with FVELER fine-tuned Llama3-8B improves SV-COMP performance by 17.39% (69→81) and Mistral-7B by 12% (75→84), while reducing proof errors. The method leverages Isabelle's comprehensive theorem libraries and LLMs' reasoning capabilities to advance formal verification.

## Method Summary
FVEL transforms C code into Isabelle formal language, then uses neural automated theorem proving with large language models to generate proofs. The system incorporates an Isabelle prover to provide feedback such as error messages to the LLM, enabling iterative refinement of proofs. A large-scale FVELER dataset is constructed containing code dependencies and verification processes formulated in Isabelle, including 758 theories, 29,125 lemmas, and 200,646 proof steps with deep dependencies. The approach uses C-Parser and AutoCorres for C-to-Isabelle transformation, and LORA fine-tuning with global batch size 8 on Llama3-8B and Mistral-7B models.

## Key Results
- FVEL with FVELER fine-tuned Llama3-8B improves SV-COMP performance by 17.39% (69→81)
- FVEL with FVELER fine-tuned Mistral-7B improves SV-COMP performance by 12% (75→84)
- Ablation study shows fine-tuning reduces proof errors during FVEL verification
- FVELER dataset contains 758 theories, 29,125 lemmas, and 200,646 proof steps with average dependency depth of 73.687

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The FVEL framework improves formal verification performance by leveraging Isabelle's comprehensive theorem libraries combined with LLMs' reasoning capabilities
- Mechanism: FVEL transforms C code into Isabelle formal language, then uses neural automated theorem proving with LLMs to generate proofs. The Isabelle prover provides feedback on proof states, enabling iterative refinement of proofs
- Core assumption: The transformed Isabelle formulation maintains semantic equivalence to the original C code while being amenable to formal theorem proving
- Evidence anchors: [abstract], [section 3]
- Break condition: If the C-to-Isabelle transformation introduces semantic mismatches or loses critical program properties during conversion

### Mechanism 2
- Claim: The FVELER dataset provides deep dependency information that enhances LLM fine-tuning for formal verification
- Mechanism: FVELER contains 758 theories, 29,125 lemmas, and 200,646 proof steps with deep dependencies (average depth 73.687). This comprehensive dataset allows LLMs to learn complex dependency relationships and proof strategies
- Core assumption: Deep dependency relationships in the dataset correlate with the complexity of real-world formal verification problems
- Evidence anchors: [section 4.5], [abstract]
- Break condition: If the deep dependencies in the dataset don't generalize to the verification problems in Code2Inv and SV-COMP benchmarks

### Mechanism 3
- Claim: Interactive feedback from Isabelle prover reduces proof errors and improves verification success rates
- Mechanism: During inference, the LLM generates proofs which are then validated by Isabelle prover. The prover provides error messages and proof states that guide the LLM to correct mistakes iteratively
- Core assumption: The Isabelle prover's feedback is sufficiently informative to guide LLM corrections in subsequent proof attempts
- Evidence anchors: [section 5.4], [section 3]
- Break condition: If Isabelle prover feedback is too sparse or too complex for LLMs to effectively incorporate into corrections

## Foundational Learning

- Concept: Automated theorem proving (ATP) fundamentals
  - Why needed here: FVEL relies on ATP principles to verify that generated proofs are logically sound
  - Quick check question: What is the difference between proof generation and proof validation in ATP systems?

- Concept: Formal verification vs symbolic verification
  - Why needed here: FVEL represents a hybrid approach combining symbolic verification (via Isabelle) with neural approaches (via LLMs)
  - Quick check question: How does FVEL's approach differ from traditional symbolic verifiers like UAutomizer or ESBMC?

- Concept: Dependency graphs in formal systems
  - Why needed here: FVELER's effectiveness depends on understanding and leveraging deep dependency relationships among theorems
  - Quick check question: Why are deep dependencies (e.g., depth > 78) important for training effective verification models?

## Architecture Onboarding

- Component map: C code -> C-Parser -> Isabelle transformation -> LLM lemma generation -> iterative proof generation with Isabelle feedback -> verification result
- Critical path: C code → C-Parser → Isabelle transformation → LLM lemma generation → iterative proof generation with Isabelle feedback → verification result
- Design tradeoffs:
  - Pros: Leverages comprehensive Isabelle libraries, enables LLM integration, provides interactive refinement
  - Cons: C-to-Isabelle transformation complexity, dependency on Isabelle prover availability, limited to C language initially
- Failure signatures:
  - Transformation failures: C code features not supported by C-Parser
  - Lemma generation failures: LLM produces semantically incorrect specifications
  - Proof generation failures: LLM cannot find valid proof paths within timeout
  - Isabelle prover errors: Syntax or semantic errors in generated proofs
- First 3 experiments:
  1. Test C-to-Isabelle transformation on a simple C function with basic control flow
  2. Verify a known lemma from FVELER dataset to establish baseline LLM performance
  3. Run end-to-end verification on a Code2Inv sample with manual inspection of intermediate proof states

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of FVEL change when extended to program languages beyond C?
- Basis in paper: [explicit] The paper states that the current version of FVEL supports code verification in C language and leaves the generalization of FVEL to other program languages as a near future work.
- Why unresolved: The authors have not implemented or tested FVEL on other programming languages, so the impact on performance is unknown.
- What evidence would resolve it: Implementing FVEL for additional programming languages and benchmarking its performance on those languages would provide the necessary evidence.

### Open Question 2
- Question: How does the semantic alignment between lemma statements and program specifications impact the effectiveness of FVEL?
- Basis in paper: [inferred] The paper mentions that semantic alignment between lemma statements and program specifications is an unexplored area of research.
- Why unresolved: The paper does not provide any analysis or experimental results on how well the generated lemmas align with the original program specifications.
- What evidence would resolve it: Conducting a study that evaluates the semantic alignment between generated lemmas and program specifications, possibly through manual review or automated semantic comparison, would resolve this question.

### Open Question 3
- Question: What is the impact of the amount and quality of training data on the performance of FVEL-LLMs?
- Basis in paper: [explicit] The paper mentions that the amount of data available on formal verification is relatively small compared to the data required to train a general LLM, and that FVEL ER remedies the issue by incorporating data from formal verification.
- Why unresolved: The paper does not provide a detailed analysis of how the size and quality of the training data affect the performance of FVEL-LLMs.
- What evidence would resolve it: Conducting experiments that vary the amount and quality of training data and measuring the impact on the performance of FVEL-LLMs would provide the necessary evidence.

## Limitations
- The C-to-Isabelle transformation fidelity is unverified across diverse C code patterns
- The FVELER dataset's deep dependencies may not generalize to real-world verification problems
- Error reduction attribution between interactive feedback and dataset depth is unclear

## Confidence

**High Confidence**: The FVEL framework architecture is sound - combining Isabelle's formal verification capabilities with LLM reasoning is a valid approach. The methodology of using prover feedback for iterative refinement is well-established in interactive theorem proving.

**Medium Confidence**: The performance improvements on Code2Inv (17.39%) and SV-COMP (12%) are statistically significant but may be partially attributable to dataset-specific patterns rather than general verification capability. The choice of specific hyperparameters and their impact on results is not fully transparent.

**Low Confidence**: The claim that deep dependencies (>78 depth) are crucial for verification performance lacks comparative analysis. There's no evidence showing that shallower datasets would perform significantly worse.

## Next Checks
1. **Transformation Verification**: Implement a test suite comparing original C program semantics with their Isabelle-transformed counterparts using equivalence checking tools to quantify transformation fidelity across diverse C constructs.

2. **Dataset Generalization Analysis**: Conduct ablation studies training LLMs on subsets of FVELER with varying dependency depths (shallow, medium, deep) to determine if the reported performance gains are specifically attributable to deep dependencies or if shallower datasets would suffice.

3. **Error Attribution Study**: Design controlled experiments isolating the effects of interactive feedback versus dataset depth by testing: (a) static proof generation without feedback, (b) feedback with randomly generated proof steps, and (c) the full interactive approach to quantify each component's contribution to error reduction.