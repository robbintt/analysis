---
ver: rpa2
title: 'DragonVerseQA: Open-Domain Long-Form Context-Aware Question-Answering'
arxiv_id: '2412.16694'
source_url: https://arxiv.org/abs/2412.16694
tags: []
core_contribution: This paper introduces DragonVerseQA, a novel open-domain long-form
  question-answering dataset focused on the fantasy universes of "House of the Dragon"
  and "Game of Thrones" TV series. Unlike existing datasets that focus on short, fact-based
  answers from Wikipedia, DragonVerseQA integrates multi-dimensional context from
  episode summaries, user reviews, and structured data to support sophisticated narrative
  understanding.
---

# DragonVerseQA: Open-Domain Long-Form Context-Aware Question-Answering

## Quick Facts
- arXiv ID: 2412.16694
- Source URL: https://arxiv.org/abs/2412.16694
- Authors: Aritra Kumar Lahiri; Qinmin Vivian Hu
- Reference count: 40
- Primary result: Novel dataset with 3200 QA pairs (avg. 31 words) for narrative understanding in Game of Thrones/House of the Dragon domains

## Executive Summary
This paper introduces DragonVerseQA, a novel open-domain long-form question-answering dataset focused on the fantasy universes of "House of the Dragon" and "Game of Thrones" TV series. Unlike existing datasets that focus on short, fact-based answers from Wikipedia, DragonVerseQA integrates multi-dimensional context from episode summaries, user reviews, and structured data to support sophisticated narrative understanding. The dataset was created using a comprehensive methodology involving data collection from HBO, Fandom wikis, IMDb, Rotten Tomatoes, and WikiData, followed by preprocessing, spam detection, bias filtering, and context-aware QA pair generation using advanced NLP techniques.

The resulting dataset contains 3200 high-quality QA pairs with an average answer length of 31 words, significantly longer than comparable datasets like Natural Questions (22 words) and SQuAD 2.0 (19 words). Evaluation using metrics including F1-score (85.6), BLEU (0.45), and ROUGE-L (0.58) demonstrates superior performance compared to benchmark datasets. The dataset supports applications in conversational AI, narrative understanding, sentiment analysis, and relation extraction, providing a new benchmark for domain-specific QA with rich contextual complexity.

## Method Summary
DragonVerseQA was constructed through a comprehensive methodology that integrated data from multiple sources including HBO, Fandom wikis, IMDb, Rotten Tomatoes, and WikiData. The data collection process involved scraping episode summaries, user reviews, and structured information about characters, events, and relationships. Following collection, the data underwent preprocessing to remove spam and detect biases, ensuring high-quality input for QA pair generation. Advanced NLP techniques were then applied to create context-aware question-answer pairs that leverage the rich narrative complexity of the source material. The methodology specifically addressed the challenge of combining different data types - factual summaries, subjective reviews, and structured knowledge - to create a multi-dimensional context that supports sophisticated narrative reasoning.

## Key Results
- 3200 high-quality QA pairs with average answer length of 31 words (vs 19-22 in benchmark datasets)
- F1-score of 85.6, BLEU score of 0.45, and ROUGE-L of 0.58
- Superior performance compared to benchmark datasets like Natural Questions and SQuAD 2.0
- Successfully integrates multi-dimensional context from episode summaries, user reviews, and structured data

## Why This Works (Mechanism)
DragonVerseQA's effectiveness stems from its innovative integration of multiple data sources that capture different dimensions of narrative complexity. By combining factual episode summaries with subjective user reviews and structured knowledge from WikiData, the dataset provides richer context than single-source approaches. This multi-dimensional context enables models to understand not just what happened in the narrative, but also how audiences perceived and interpreted events, creating a more complete picture of the story universe. The longer answer lengths (31 words vs 19-22 in benchmarks) indicate that the dataset supports more sophisticated reasoning about relationships, motivations, and narrative developments that require extended explanations rather than simple fact retrieval.

## Foundational Learning

**Narrative Context Understanding** - Why needed: The dataset requires models to understand complex story arcs, character relationships, and plot developments across multiple episodes. Quick check: Can a model track character motivations across different narrative contexts?

**Multi-Source Data Integration** - Why needed: Combining episode summaries, reviews, and structured data creates a richer but more complex information landscape. Quick check: Does the model effectively distinguish between factual summaries and subjective reviews?

**Long-Form Answer Generation** - Why needed: The 31-word average answers require models to generate coherent, contextually appropriate explanations rather than short factoids. Quick check: Can models produce answers that maintain narrative coherence across longer text spans?

**Sentiment-Aware Reasoning** - Why needed: User reviews introduce emotional and subjective dimensions that must be reconciled with factual narrative content. Quick check: Does the model appropriately balance factual accuracy with sentiment interpretation?

**Domain-Specific Knowledge Representation** - Why needed: The Game of Thrones/House of the Dragon domain requires understanding of fantasy world-building, family dynamics, and complex political relationships. Quick check: Can models reason about inter-house relationships and their narrative significance?

## Architecture Onboarding

**Component Map**: Data Collection -> Preprocessing/Filtering -> Context Integration -> QA Pair Generation -> Evaluation Metrics

**Critical Path**: The most critical path involves the integration of multi-dimensional context sources with QA pair generation, as this determines the dataset's ability to support narrative understanding. The preprocessing and filtering stages are essential for maintaining data quality across heterogeneous sources.

**Design Tradeoffs**: The choice to include user reviews alongside factual summaries creates richer context but introduces potential noise and bias. This tradeoff enables more sophisticated narrative understanding at the cost of increased complexity in data curation and quality control.

**Failure Signatures**: Common failure modes include over-reliance on review-based sentiment when factual accuracy is required, conflation of different narrative timelines, and difficulty handling questions that span multiple context types (e.g., combining plot events with audience reactions).

**3 First Experiments**:
1. Ablation study removing review-based context to measure its specific contribution to QA performance
2. Cross-domain evaluation testing model performance on questions from different narrative universes
3. Bias analysis examining answer source distribution and potential systematic differences in answer quality

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Narrow domain focus on "House of the Dragon" and "Game of Thrones" limits generalizability to other narrative domains
- Potential noise from subjective user-generated content in reviews may affect answer quality
- Claims about effectiveness for conversational AI applications lack empirical validation
- Impact of multi-source approach on bias and answer quality distribution remains uncharacterized

## Confidence

**High Confidence**: The dataset construction methodology is well-documented, and the technical approach to combining multiple data sources is sound. The claim about longer average answer lengths (31 words vs 19-22 in benchmarks) is directly verifiable from the dataset statistics.

**Medium Confidence**: The evaluation metrics and their interpretation show reasonable internal consistency, but the comparative claims against other datasets require more rigorous ablation studies to isolate the impact of context richness versus other factors. The dataset's utility for sentiment analysis and relation extraction remains largely theoretical without empirical validation.

**Low Confidence**: Claims about the dataset's effectiveness for conversational AI applications lack empirical support in the current work. The impact of review-based content on answer quality and potential biases introduced by combining multiple source types have not been thoroughly characterized.

## Next Checks
1. Conduct ablation studies removing different context sources (reviews vs summaries vs structured data) to quantify their individual contributions to QA performance and narrative understanding.
2. Evaluate model performance when applying DragonVerseQA-trained models to questions from different narrative domains (e.g., other TV series or book series) to assess generalization capabilities.
3. Systematically analyze the distribution of answer sources across the dataset to identify potential biases introduced by the multi-source approach, particularly examining whether review-based answers show systematic differences in length, sentiment, or factual accuracy compared to summary-based answers.