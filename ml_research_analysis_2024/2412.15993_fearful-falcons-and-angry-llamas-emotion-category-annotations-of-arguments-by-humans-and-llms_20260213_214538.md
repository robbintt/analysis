---
ver: rpa2
title: 'Fearful Falcons and Angry Llamas: Emotion Category Annotations of Arguments
  by Humans and LLMs'
arxiv_id: '2412.15993'
source_url: https://arxiv.org/abs/2412.15993
tags:
- emotion
- arguments
- argument
- label
- emotions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces Emo-DeFaBel, the first corpus annotated with
  discrete emotion categories in German argumentative texts. Using crowdsourcing,
  human annotators labeled 300 arguments with emotions like joy, anger, fear, and
  interest, alongside their stance and perceived convincingness.
---

# Fearful Falcons and Angry Llamas: Emotion Category Annotations of Arguments by Humans and LLMs

## Quick Facts
- arXiv ID: 2412.15993
- Source URL: https://arxiv.org/abs/2412.15993
- Reference count: 21
- Human annotators labeled 300 German argumentative texts with emotions; models showed high recall but low precision, especially for negative emotions

## Executive Summary
This study introduces Emo-DeFaBel, the first corpus annotated with discrete emotion categories in German argumentative texts. Using crowdsourcing, human annotators labeled 300 arguments with emotions like joy, anger, fear, and interest, alongside their stance and perceived convincingness. Automatic labeling via LLMs (Falcon-7b, Llama-3.1-8B, GPT-4o-mini) was tested using zero-shot, one-shot, and chain-of-thought prompting across binary, closed-domain, and open-domain emotion settings. Results show high recall but low precision, with models strongly biased toward negative emotions like anger and fear. Positive emotions (joy, pride) correlate with higher perceived convincingness, while negative emotions correlate with lower convincingness. The study highlights the challenges of emotion classification in arguments and the need for improved models to mitigate negative emotion bias.

## Method Summary
The study collected human annotations for 300 German arguments from the DeFaBel corpus, including stance, convincingness, and 10 discrete emotion categories. Three prompting strategies (zero-shot, one-shot, chain-of-thought) were applied to three LLMs (Falcon-7b, Llama-3.1-8B, GPT-4o-mini) across three emotion settings (binary emotionality, closed-domain with predefined emotions, open-domain with free emotion labels). Model predictions were evaluated using macro-averaged precision, recall, and F1 scores, with analysis of bias patterns and correlation with human judgments.

## Key Results
- Models achieved high recall but low precision, particularly for negative emotions like anger and fear
- Binary emotionality detection performed better than discrete emotion classification
- Positive emotions (joy, pride) correlated with higher perceived convincingness, while negative emotions correlated with lower convincingness
- Inter-annotator agreement remained consistently low, highlighting the subjective nature of emotion annotation in arguments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High recall but low precision in automatic emotion predictions reflects a bias toward detecting emotionality rather than identifying specific emotions.
- Mechanism: Language models over-predict emotional content (high recall) because emotional language is prevalent in arguments, but they misclassify emotions (low precision) due to lack of fine-grained emotion understanding.
- Core assumption: The models are tuned for general language understanding, not specialized emotion recognition.
- Evidence anchors:
  - [abstract] "automatic predictions show a high recall but low precision for predicting anger and fear, indicating a strong bias toward negative emotions"
  - [section] "While inferring the binary emotionality label in arguments from closed and open-domain prompts improves the overall performance, the high recall is striking and raises the question about the reliability of the binary emotionality prediction"
- Break condition: If models were fine-tuned on emotion-specific data or if prompting strategies were optimized for precision, the precision-recall balance would shift.

### Mechanism 2
- Claim: The subjective nature of emotion annotation in arguments leads to low inter-annotator agreement and model performance.
- Mechanism: Human annotators' stances and prior knowledge influence their emotion perception, making the task inherently ambiguous. Models trained on this data inherit this ambiguity.
- Core assumption: Emotion perception in arguments is not purely textual but also contextual and subjective.
- Evidence anchors:
  - [section] "Overall, the agreement for all emotion labels remains consistently low, underscoring the inherent subjectivity of emotion annotation tasks"
  - [section] "We speculate that the annotators did not experience fear when reading the arguments because the arguments are focused on indirect or hypothetical events... rather than presenting a personal, immediate threat"
- Break condition: If a more objective emotion annotation framework were used, or if models were given contextual information about the annotator's perspective, agreement and performance might improve.

### Mechanism 3
- Claim: The bias toward negative emotions in model predictions is influenced by the framing of the prompt and the domain setting.
- Mechanism: Negative emotions like anger and fear are more salient and easier to detect in text, leading models to default to these emotions when uncertain. Prompting strategies that emphasize negative emotions reinforce this bias.
- Core assumption: Models learn and replicate patterns of emotional salience present in their training data.
- Evidence anchors:
  - [abstract] "automatic predictions show a high recall but low precision for predicting anger and fear, indicating a strong bias toward negative emotions"
  - [section] "Our results indicate a bias of all models in predicting the negative emotions of anger (Llama) and fear (GPT and Falcon)"
- Break condition: If prompts were explicitly designed to balance emotion detection or if the training data was balanced for positive and negative emotions, the bias might be reduced.

## Foundational Learning

- Concept: Difference between binary emotionality and discrete emotion categories
  - Why needed here: The paper contrasts binary emotion detection (is there emotion?) with categorical emotion labeling (which specific emotion?), which have different implications for model design and evaluation.
  - Quick check question: What is the key distinction between predicting "emotionality" versus "joy" or "anger" in a text?

- Concept: Prompt engineering techniques (zero-shot, one-shot, chain-of-thought)
  - Why needed here: The paper evaluates how different prompting strategies affect model performance in emotion detection tasks.
  - Quick check question: How might a chain-of-thought prompt differ in structure and expected output from a zero-shot prompt for emotion classification?

- Concept: Macro-averaging vs. micro-averaging in model evaluation
  - Why needed here: The paper uses macro-averaging across emotion classes, which can mask performance differences for rare emotions.
  - Quick check question: Why might macro-averaging lead to lower overall F1 scores compared to micro-averaging in a multi-class classification task?

## Architecture Onboarding

- Component map: Emotion annotation pipeline -> Data collection (crowdsourcing) -> Model prediction (LLMs with various prompts) -> Evaluation (strict vs. relaxed metrics)
- Critical path: Annotation data -> Model training/inference -> Performance evaluation -> Bias analysis
- Design tradeoffs: Binary vs. discrete emotion prediction (simplicity vs. granularity), strict vs. relaxed evaluation (precision vs. recall), model size vs. cost
- Failure signatures: High recall but low precision suggests over-prediction; consistently low agreement suggests subjective task; bias toward negative emotions suggests prompt/model bias
- First 3 experiments:
  1. Test binary emotion prediction with different prompt types to isolate the effect of prompt structure on binary vs. discrete emotion detection
  2. Evaluate model performance on individual emotion classes to identify which emotions are most/least predictable
  3. Conduct bias analysis by comparing model predictions to human annotations for specific emotion categories to identify systematic errors

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do individual stances toward the statement affect the perceived emotions in arguments?
- Basis in paper: [explicit] The paper notes that annotators with different stances (e.g., neutral vs. disagreeing) labeled the same argument with different emotions, such as surprise or shame, and suggests a potential correlation between stance and emotion.
- Why unresolved: The paper only provides anecdotal examples and does not systematically analyze the relationship between stance and emotion across the dataset.
- What evidence would resolve it: A statistical analysis correlating stance scores with specific emotion labels across all arguments would clarify whether stance reliably predicts emotion.

### Open Question 2
- Question: Can fine-tuning or prompt optimization improve the precision of discrete emotion classification in arguments?
- Basis in paper: [inferred] The authors note that models show high recall but low precision for negative emotions and suggest that fine-tuning or prompt optimization could mitigate these issues.
- Why unresolved: The paper only tests zero-shot, one-shot, and chain-of-thought prompting without exploring fine-tuning or advanced prompt engineering.
- What evidence would resolve it: Experiments comparing fine-tuned models or optimized prompts against the current approaches would show whether precision improves.

### Open Question 3
- Question: Do linguistic cues alone (e.g., words like "risk" or "cancer") reliably indicate the emotion of fear in argumentative texts?
- Basis in paper: [explicit] The paper observes that GPT often predicts fear based on linguistic cues like "Gef√§hrdung der eigenen Sicherheit" (risk to your own safety) or "Krebs" (cancer), while human annotators may not feel fear if the threat is indirect or hypothetical.
- Why unresolved: The paper speculates on this discrepancy but does not test whether models can distinguish between direct and indirect threats.
- What evidence would resolve it: A controlled study comparing model predictions on arguments with direct vs. indirect threats would test if models can make this distinction.

### Open Question 4
- Question: How do models handle the open-domain emotion prediction task compared to the closed-domain setting?
- Basis in paper: [explicit] The paper compares closed-domain (predefined emotion set) and open-domain (unrestricted emotion set) settings, finding that open-domain prompts allow broader emotion capture but performance varies.
- Why unresolved: The paper does not provide a detailed comparison of model performance or biases between these settings.
- What evidence would resolve it: A detailed analysis of model outputs in open-domain settings, including frequency of novel emotion labels and agreement with human annotations, would clarify the strengths and weaknesses of this approach.

## Limitations

- Low inter-annotator agreement suggests emotion annotation in arguments is inherently subjective and context-dependent
- Models exhibit systematic bias toward negative emotions, limiting real-world applicability
- Findings are based on German texts, potentially limiting generalizability to other languages or cultural contexts

## Confidence

- **High Confidence**: The observation that models exhibit bias toward negative emotions is well-supported by the data and consistent across multiple experiments
- **Medium Confidence**: The interpretation that low precision reflects over-prediction of emotional content is reasonable but could benefit from additional analysis of model calibration
- **Low Confidence**: The speculation about why annotators did not experience fear (indirect/hypothetical events) is based on limited evidence and requires further validation

## Next Checks

1. Conduct a follow-up annotation study with more detailed contextual information about the annotators' perspectives to better understand the subjectivity in emotion perception
2. Implement and test prompt engineering strategies specifically designed to balance emotion detection across positive and negative categories to address the observed bias
3. Perform cross-lingual validation by replicating the study with arguments in other languages to assess the generalizability of the findings