---
ver: rpa2
title: 'The Pitfalls of Publishing in the Age of LLMs: Strange and Surprising Adventures
  with a High-Impact NLP Journal'
arxiv_id: '2407.12026'
source_url: https://arxiv.org/abs/2407.12026
tags: []
core_contribution: A reviewer of a computational linguistics journal submission was
  found to have used an LLM to generate their review. The authors discovered this
  by recognizing the review's AI-generated style and content, such as offering their
  own sentence as an "improved" version.
---

# The Pitfalls of Publishing in the Age of LLMs: Strange and Surprising Adventures with a High-Impact NLP Journal

## Quick Facts
- **arXiv ID**: 2407.12026
- **Source URL**: https://arxiv.org/abs/2407.12026
- **Reference count**: 2
- **Primary result**: Authors discovered reviewer used LLM for review, raising concerns about peer review integrity

## Executive Summary
The authors encountered a reviewer who used an LLM to generate their review, as evidenced by the AI-generated style and content, including offering the authors' own sentence as an "improved" version. Upon raising this issue with the editor-in-chief, the reviewer was banned from future reviews. The authors remain concerned about the lack of preventive mechanisms and the potential breach of confidentiality, as their manuscript may have been used as training data for the LLM. This incident highlights the emerging challenges in academic publishing due to the widespread use of LLMs, raising questions about the integrity of the peer review process and the need for new policies to address such misconduct.

## Method Summary
This is a case report documenting an incident where authors discovered a reviewer likely used an LLM to generate their review of a computational linguistics journal submission. The authors identified AI-generated characteristics in the review content and style, then reported this to the editor-in-chief who confirmed the reviewer would be banned from future reviews. The report serves as an anecdotal case study highlighting the emerging challenges in academic publishing due to LLM usage.

## Key Results
- A reviewer for a computational linguistics journal was found to have used an LLM to generate their review
- The editor-in-chief confirmed the reviewer would be banned from future reviews
- The incident raised concerns about confidentiality breaches and lack of preventive mechanisms against AI-assisted reviewing

## Why This Works (Mechanism)
This case demonstrates how LLM-generated reviews can infiltrate the peer review process when reviewers misuse AI tools to shortcut their responsibilities. The mechanism involves reviewers using LLMs to generate content that appears substantive but lacks genuine scholarly engagement with the manuscript. This works because current journal systems lack technical safeguards to detect AI-generated content, and reviewers may not disclose their use of such tools.

## Foundational Learning
- **AI-generated content detection**: Why needed - to maintain review quality and integrity; Quick check - analyze linguistic patterns for repetitive phrasing, generic suggestions, or overconfidence in incorrect statements
- **Confidentiality protocols**: Why needed - to prevent manuscript leakage into training data; Quick check - track submission access logs and review content for unusual similarity to published works
- **Peer review ethics**: Why needed - to establish clear boundaries for AI tool usage; Quick check - develop explicit guidelines on acceptable vs. unacceptable AI assistance
- **Editorial oversight**: Why needed - to respond to misconduct allegations; Quick check - implement systematic review of suspicious reviews flagged by authors or AI detection tools
- **Author rights protection**: Why needed - to prevent intellectual property misuse; Quick check - require reviewers to certify they haven't shared manuscript content with external tools

## Architecture Onboarding
**Component map**: Submission system -> Review generation -> Editor review -> Author notification -> Publication decision
**Critical path**: Manuscript submission → Reviewer assignment → Review generation → Editorial evaluation → Author communication → Publication decision
**Design tradeoffs**: Open access vs. confidentiality protection; reviewer anonymity vs. accountability; efficiency vs. thoroughness
**Failure signatures**: Generic or contradictory review comments; overconfidence in incorrect technical details; offering original author text as "improvements"
**3 first experiments**: 1) Implement AI detection tools on all incoming reviews; 2) Add mandatory certification about AI tool usage for reviewers; 3) Create anonymous reporting system for authors to flag suspicious reviews

## Open Questions the Paper Calls Out
None

## Limitations
- No objective verification that an LLM generated the review
- Single data point with no comparative cases
- No technical analysis of the review text for AI signatures
- Editor-in-chief's response is reported but not independently confirmed

## Confidence
- **High confidence**: The incident was reported to the editor-in-chief and resulted in a stated ban
- **Medium confidence**: The review contained characteristics suggestive of AI generation
- **Low confidence**: That the manuscript was actually used as training data for the LLM

## Next Checks
1. Request the actual review text from the editor-in-chief (with PII redacted) to conduct linguistic analysis comparing it to known LLM outputs
2. Survey multiple editors-in-chief at computational linguistics journals to determine prevalence of suspected AI-generated reviews
3. Analyze submission and review timestamps from the journal's editorial system to identify unusual patterns that might indicate AI-assisted reviewing