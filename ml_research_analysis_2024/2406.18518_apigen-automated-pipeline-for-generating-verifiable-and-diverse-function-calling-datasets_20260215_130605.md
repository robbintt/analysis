---
ver: rpa2
title: 'APIGen: Automated Pipeline for Generating Verifiable and Diverse Function-Calling
  Datasets'
arxiv_id: '2406.18518'
source_url: https://arxiv.org/abs/2406.18518
tags: []
core_contribution: This paper introduces APIGen, an automated pipeline for generating
  high-quality datasets for function-calling agents. The key innovation is a three-stage
  verification process (format, execution, semantic) that ensures dataset reliability.
---

# APIGen: Automated Pipeline for Generating Verifiable and Diverse Function-Calling Datasets

## Quick Facts
- arXiv ID: 2406.18518
- Source URL: https://arxiv.org/abs/2406.18518
- Reference count: 40
- Key outcome: 7B model ranks 6th on Berkeley Function-Calling Leaderboard, outperforming GPT-4 models

## Executive Summary
APIGen introduces an automated pipeline for generating high-quality datasets for training function-calling agents. The key innovation is a three-stage verification process (format, execution, semantic) that ensures dataset reliability. The pipeline generates diverse datasets covering simple, multiple, parallel, and parallel-multiple function calling scenarios. Models trained on APIGen datasets show strong performance, with the 7B model ranking 6th on the Berkeley Function-Calling Leaderboard, outperforming GPT-4 models. The 1B model surpasses GPT-3.5-Turbo and Claude-3 Haiku. The work releases 60,000 verified entries spanning 3,673 APIs across 21 categories, advancing research in function-calling agent domains.

## Method Summary
APIGen uses a three-stage verification pipeline to generate high-quality function-calling datasets. APIs are first standardized into JSON format, then synthetic queries and answers are generated using LLM prompts. Each generated entry undergoes format checking, execution validation, and semantic verification before being added to the dataset. The pipeline produces diverse function-calling scenarios and trains models (xLAM-1B and xLAM-7B) that achieve state-of-the-art performance on the Berkeley Function-Calling Leaderboard.

## Key Results
- 7B model trained on APIGen data ranks 6th on Berkeley Function-Calling Leaderboard, outperforming GPT-4 models
- 1B model surpasses GPT-3.5-Turbo and Claude-3 Haiku on function-calling benchmarks
- Pipeline generates 60,000 verified entries across 3,673 APIs in 21 categories
- Three-stage verification (format, execution, semantic) ensures data reliability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-stage verification (format, execution, semantic) filters low-quality function-calling data effectively.
- Mechanism: Each generated function call is validated through three hierarchical checks: format compliance, successful execution, and semantic alignment with the query intent. Data failing any stage is discarded.
- Core assumption: API execution is deterministic and verifiable, enabling objective quality assessment beyond LLM-based checks.
- Evidence anchors:
  - [abstract]: "Each data in our dataset is verified through three hierarchical stages: format checking, actual function executions, and semantic verification, ensuring its reliability and correctness."
  - [section 3.2]: Describes the three-stage verification process and provides filtering statistics showing that format and execution checkers remove the majority of low-quality data.
  - [corpus]: Limited direct evidence; neighboring papers discuss synthetic data quality but do not provide comparable verification pipeline details.
- Break condition: If API execution becomes non-deterministic, slow, or expensive, the verification pipeline may fail or become impractical to scale.

### Mechanism 2
- Claim: Training on high-quality synthetic data enables small models (1B-7B) to match or exceed much larger models on function-calling tasks.
- Mechanism: Curated datasets with diverse query styles and rigorous verification allow effective fine-tuning, compensating for parameter count disadvantages.
- Core assumption: Data quality and diversity are more important than raw model size for function-calling performance.
- Evidence anchors:
  - [abstract]: "models trained with our curated datasets, even with only 7B parameters, can achieve state-of-the-art performance on the Berkeley Function-Calling Benchmark, outperforming multiple GPT-4 models."
  - [section 5.2]: Experimental results show the 7B model ranks 6th on BFCL, surpassing GPT-4 models; the 1B model outperforms GPT-3.5-Turbo and Claude-3 Haiku.
  - [corpus]: Weak; neighboring papers mention synthetic datasets but do not provide direct comparisons to large models.
- Break condition: If the synthetic data lacks coverage of real-world edge cases, the trained models may fail on unseen scenarios despite high training quality.

### Mechanism 3
- Claim: Standardized JSON format enables scalable, modular pipeline integration across diverse API sources.
- Mechanism: APIs, function calls, and generated data are represented in a unified JSON structure, allowing easy incorporation of new API types (Python, REST) without modifying core pipeline components.
- Core assumption: A consistent data representation simplifies verification, generation, and model training processes.
- Evidence anchors:
  - [section 3.1]: "The adoption of a standardized JSON format... provides several advantages... enables efficient checking... and scalability."
  - [appendix A.2]: Provides detailed JSON schema examples for queries, tools, and answers.
  - [corpus]: No direct evidence; assumption based on engineering best practices rather than empirical validation in the paper.
- Break condition: If API sources require fundamentally different interaction patterns (e.g., streaming APIs, non-JSON protocols), the unified format may not generalize.

## Foundational Learning

- Concept: Function-calling in LLMs
  - Why needed here: The paper's core contribution is improving function-calling model performance; understanding what function-calling is and how it's evaluated is foundational.
  - Quick check question: What are the two main evaluation categories used by the Berkeley Function-Calling Leaderboard (BFCL)?
    - Answer: Abstract Syntax Tree (AST) Evaluation and Executable Function Evaluation.

- Concept: Synthetic data generation and verification
  - Why needed here: The pipeline generates synthetic datasets; knowing how to design verification stages and prompt LLMs for data generation is critical.
  - Quick check question: What are the three stages of the APIGen verification pipeline?
    - Answer: Format checking, execution checking, and semantic verification.

- Concept: JSON data serialization
  - Why needed here: The entire pipeline relies on JSON for representing APIs, queries, and answers; engineers must understand the schema to modify or extend the pipeline.
  - Quick check question: What are the three top-level keys in the JSON data format used by APIGen?
    - Answer: query, tools, and answers.

## Architecture Onboarding

- Component map:
  API Sampler → Example Sampler → Prompt Sampler → LLM Generator → Format Checker → Execution Checker → Semantic Checker → Seed Data Updater → Training Pipeline

- Critical path:
  API Sampler → Example Sampler → Prompt Sampler → LLM Generator → Format Checker → Execution Checker → Semantic Checker → Seed Data Updater

- Design tradeoffs:
  - Verification rigor vs. generation speed: Stricter checks reduce noise but slow down pipeline throughput.
  - Prompt diversity vs. consistency: More templates increase data variety but may introduce generation noise.
  - Model size vs. performance: Smaller models trained on high-quality data can match larger models, but may still lag on very complex tasks.

- Failure signatures:
  - High rejection rate in format checker: Likely due to ambiguous or poorly structured prompts.
  - High rejection rate in execution checker: API documentation or argument parsing errors.
  - High rejection rate in semantic checker: LLM generator producing irrelevant or misaligned responses.
  - Model underperforms despite high-quality data: Possible overfitting to synthetic patterns or insufficient coverage of real-world cases.

- First 3 experiments:
  1. Run the full pipeline with a small set of APIs and manually inspect a sample of generated and verified data to check correctness and diversity.
  2. Train a small model (e.g., 1B) on a subset of the generated data and evaluate on a simple BFCL category to confirm training pipeline works.
  3. Perform an ablation study by training with and without each verification stage to quantify their impact on model performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does APIGen's performance scale with larger models (e.g., 70B+ parameters) compared to the 7B and 1B models tested?
- Basis in paper: [explicit] The paper demonstrates strong performance with 7B and 1B models but does not explore scaling to larger model sizes.
- Why unresolved: The paper focuses on smaller models to highlight the effectiveness of high-quality data, but does not investigate whether the same approach benefits larger models.
- What evidence would resolve it: Training and evaluating larger models (e.g., 70B+) on APIGen-generated datasets and comparing their performance on the Berkeley Function-Calling Leaderboard.

### Open Question 2
- Question: Can APIGen be extended to support multi-turn function-calling scenarios effectively?
- Basis in paper: [inferred] The paper mentions plans to extend APIGen to handle multi-turn interactions but does not provide experimental results or implementation details.
- Why unresolved: The current version of APIGen only implements single-turn function calling, and the paper does not explore how the framework would handle more complex, multi-turn interactions.
- What evidence would resolve it: Developing and testing APIGen's ability to generate and verify multi-turn function-calling datasets, and evaluating the performance of models trained on such data.

### Open Question 3
- Question: How does the quality of APIGen-generated datasets compare to manually curated datasets in terms of model performance and generalization?
- Basis in paper: [inferred] The paper emphasizes the high quality of APIGen-generated datasets but does not compare them directly to manually curated datasets.
- Why unresolved: While APIGen uses a rigorous verification process, the paper does not benchmark its datasets against manually curated ones to assess relative quality and effectiveness.
- What evidence would resolve it: Conducting a comparative study where models are trained on both APIGen-generated and manually curated datasets, and evaluating their performance on the same benchmarks.

### Open Question 4
- Question: What is the impact of varying the diversity of API categories on the robustness of function-calling models trained with APIGen?
- Basis in paper: [explicit] The paper uses 3,673 APIs across 21 categories but does not explore how varying this diversity affects model performance.
- Why unresolved: The paper provides a comprehensive dataset but does not investigate whether increasing or decreasing the diversity of API categories influences the model's ability to generalize across different domains.
- What evidence would resolve it: Training models on datasets with varying levels of API category diversity and testing their performance across different benchmarks and real-world scenarios.

## Limitations
- Verification pipeline scalability untested at larger API volumes
- JSON standardization may not generalize to APIs requiring streaming responses or non-standard protocols
- Semantic checker performance depends on underlying LLM capability without ablation studies

## Confidence
- High confidence: Three-stage verification process effectively filters low-quality data with clear filtering statistics
- Medium confidence: JSON standardization enables scalable integration but lacks empirical validation beyond current API types
- Low confidence: API execution determinism assumption untested across diverse API types

## Next Checks
1. Scale the pipeline to 10× the current API volume (600K+ entries) and measure execution checker throughput, failure rates, and cost per verified entry to identify performance bottlenecks.
2. Conduct an ablation study comparing model performance when trained with: (a) all three verification stages, (b) only format and execution checks, and (c) only format checking, to quantify semantic verification's marginal contribution.
3. Test the trained models on real-world API usage scenarios beyond the BFCL benchmark, including APIs with rate limiting, authentication challenges, and non-standard response formats to assess practical deployment readiness.