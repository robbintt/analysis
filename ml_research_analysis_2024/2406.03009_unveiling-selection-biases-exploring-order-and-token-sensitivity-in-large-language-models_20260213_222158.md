---
ver: rpa2
title: 'Unveiling Selection Biases: Exploring Order and Token Sensitivity in Large
  Language Models'
arxiv_id: '2406.03009'
source_url: https://arxiv.org/abs/2406.03009
tags:
- high
- school
- token
- sensitivity
- order
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates "selection biases" in Large Language Models
  (LLMs) when choosing optimal options from ordered sequences, focusing on option
  order and token usage effects. Through extensive empirical analysis across six tasks
  and six models, the research quantifies how these biases impact decision-making
  processes.
---

# Unveiling Selection Biases: Exploring Order and Token Sensitivity in Large Language Models

## Quick Facts
- arXiv ID: 2406.03009
- Source URL: https://arxiv.org/abs/2406.03009
- Reference count: 27
- Primary result: LLMs exhibit systematic selection biases based on option order and token usage, with most models showing greater sensitivity to order than token variations

## Executive Summary
This study investigates "selection biases" in Large Language Models (LLMs) when choosing optimal options from ordered sequences, focusing on option order and token usage effects. Through extensive empirical analysis across six tasks and six models, the research quantifies how these biases impact decision-making processes. The findings reveal that most LLMs are more sensitive to option order than token variations, with stronger models showing significant improvement when mitigating these biases. Three cost-effective strategies are proposed: gray-box probability weighting and calibration methods, and a black-box two-hop strategy.

## Method Summary
The study employs a systematic experimental approach to investigate selection biases in LLMs across six multi-choice tasks (ARC-Challenge, HellaSwag, MMLU, Winogrande, MathQA, OpenBookQA) using six instruction-tuned LLMs in zero-shot settings. The methodology involves conducting sensitivity experiments to measure model sensitivity to option order and token usage using the Fluctuation Rate (FR) metric, then applying proposed mitigation strategies including gray-box probability weighting and calibration, and black-box two-hop strategy to evaluate their effectiveness in improving model performance.

## Key Results
- Most LLMs demonstrate higher sensitivity to option order than to token variations across tasks
- Task difficulty correlates with model sensitivity, with harder tasks showing higher fluctuation rates
- Gray-box probability weighting and calibration methods improve performance with minimal additional cost
- The black-box two-hop strategy effectively addresses sensitivity in many tasks while being more broadly applicable

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Option order and token usage systematically influence LLMs' decision-making in selection problems.
- Mechanism: LLMs exhibit selection biases where the position of options (order bias) and the symbols used to represent options (token bias) impact the probability distribution over choices, even when the underlying semantic content is identical.
- Core assumption: LLMs rely on positional and lexical cues during in-context learning, treating option symbols as meaningful features rather than arbitrary labels.
- Evidence anchors:
  - [abstract] "We delve into biases related to option order and token usage, which significantly impact LLMs' decision-making processes."
  - [section 4.3] "we observe a notable trend: they are more sensitive to option order than to symbols/tokens in 17 out of 18 cases."
  - [corpus] Weak evidence - no direct citation, but the FMR-based neighbor clustering suggests this is an underexplored bias type.

### Mechanism 2
- Claim: Model sensitivity to selection biases correlates inversely with task difficulty.
- Mechanism: When tasks are easier (higher accuracy), the model's confidence is higher, making its output less sensitive to fluctuations in option presentation; conversely, harder tasks lead to greater variability in responses to reordered or relabeled options.
- Core assumption: Model confidence and sensitivity are inversely related; task difficulty modulates both.
- Evidence anchors:
  - [abstract] "The research also establishes a correlation between task difficulty and model sensitivity, with more challenging tasks exhibiting higher sensitivity to option order and token variations."
  - [section 4.4] "tasks with higher accuracy levels, such as ARC Challenge, HellaSwag, and OpenBookQA, tend to exhibit lower fluctuation rates."
  - [corpus] Weak evidence - no direct citation, but the neighbor papers on order bias and robustness suggest this correlation is recognized.

### Mechanism 3
- Claim: Calibration and probability weighting strategies effectively mitigate selection biases.
- Mechanism: By adjusting the model's output probabilities based on observed symbol/position frequencies or by reweighting based on token probabilities, the methods counteract the systematic preference for certain symbols or positions.
- Core assumption: The bias is consistent and predictable enough that simple reweighting or calibration can correct it without requiring complex inference.
- Evidence anchors:
  - [abstract] "Three cost-effective strategies are proposed: gray-box probability weighting and calibration methods, and a black-box two-hop strategy."
  - [section 5.1] "The weighted probability of a specific option content cf_i is derived by integrating the probabilities of its corresponding symbol sf_i in rf orward with the symbol sb_j in the second query set rbackward."
  - [corpus] Weak evidence - no direct citation, but the neighbor papers on bias mitigation and robustness suggest similar reweighting approaches.

## Foundational Learning

- Concept: In-context learning and its sensitivity to prompt structure
  - Why needed here: Understanding how LLMs use in-context examples and prompt formatting is crucial for interpreting why order and token biases arise.
  - Quick check question: How does changing the order of demonstrations in a few-shot prompt affect model predictions?

- Concept: Probability calibration and weighting in ML
  - Why needed here: The mitigation strategies rely on adjusting output probabilities based on observed frequencies or token likelihoods.
  - Quick check question: What is the effect of dividing predicted probabilities by observed output frequencies?

- Concept: Evaluation metrics for model robustness (e.g., fluctuation rate)
  - Why needed here: Measuring sensitivity to order/token changes requires a robust metric that captures variability in model outputs.
  - Quick check question: How does fluctuation rate differ from accuracy, and why is it important for detecting biases?

## Architecture Onboarding

- Component map: Input -> Question + ordered option list -> LLM inference (gray-box or black-box) -> Selected option (based on mitigation method) -> Accuracy and fluctuation rate calculation -> Evaluation

- Critical path:
  1. Prepare question and options in two orderings (forward, backward)
  2. Query LLM(s) and collect outputs
  3. Apply mitigation (weighting, calibration, or two-hop)
  4. Compute accuracy and fluctuation rate
  5. Analyze correlation with task difficulty

- Design tradeoffs:
  - Gray-box vs. black-box: Access to token probabilities allows more precise mitigation but limits applicability to models with API access.
  - Cost vs. accuracy: Mitigation methods add requests but are cheaper than full majority voting or CoT ensembles.
  - Calibration data: Need representative validation set; poor data can lead to miscalibration.

- Failure signatures:
  - Mitigation strategies degrade performance on tasks with only two options (e.g., Winogrande).
  - Fluctuation rates do not correlate with task difficulty, suggesting other factors at play.
  - Model shows no sensitivity to order/token changes, making mitigation unnecessary.

- First 3 experiments:
  1. Run sensitivity analysis (token, order, both) on a single task/model pair to confirm bias exists.
  2. Apply gray-box probability weighting and measure improvement in fluctuation rate and accuracy.
  3. Apply black-box two-hop strategy and compare results to gray-box methods.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do selection biases in LLMs change when models are fine-tuned on specific domains versus used in zero-shot settings?
- Basis in paper: [inferred] The paper focuses exclusively on zero-shot settings to isolate selection biases from in-context demonstration effects, noting this distinguishes their work from prior studies that focused on few-shot settings.
- Why unresolved: The study explicitly limits itself to zero-shot settings to better understand inherent biases, leaving the impact of fine-tuning unexamined.
- What evidence would resolve it: Comparative experiments measuring selection biases in both fine-tuned and zero-shot scenarios across the same tasks and models.

### Open Question 2
- Question: What are the underlying mechanisms that cause certain LLMs to show preference distributions that deviate significantly from ground truth distributions?
- Basis in paper: [explicit] The paper observes that most models except LLaMA2-7B show bias towards options B or C compared to ground truth proportions, and that models are more sensitive to option order than token variations.
- Why unresolved: While the paper quantifies the biases and proposes mitigation strategies, it does not investigate the root causes of these preference distributions or sensitivity patterns.
- What evidence would resolve it: Detailed analysis of model internals (attention patterns, token embeddings) to identify what drives these preference distributions and sensitivity differences.

### Open Question 3
- Question: How does the effectiveness of bias mitigation strategies scale with model size and capability?
- Basis in paper: [explicit] The paper finds that stronger models like PaLM 2 and Gemini Pro show significant improvements from mitigation strategies, while LLaMA 2 models show more varied results, with the 7B model showing less improvement.
- Why unresolved: The paper presents results across different model sizes but does not systematically analyze how mitigation strategy effectiveness correlates with model scale or capability.
- What evidence would resolve it: Comprehensive testing of mitigation strategies across a wider range of model sizes with statistical analysis of effectiveness scaling patterns.

## Limitations

- The analysis is primarily empirical and does not provide mechanistic explanations for why selection biases exist at the model architecture level
- Calibration and mitigation strategies may not generalize to all model families or prompt styles
- The study focuses exclusively on zero-shot settings, leaving unexplored how biases manifest in few-shot or fine-tuned scenarios

## Confidence

- High Confidence: The existence of selection biases related to option order and token usage, as evidenced by systematic measurement of fluctuation rates across multiple tasks and models
- Medium Confidence: The effectiveness of proposed mitigation strategies, particularly gray-box probability weighting and calibration methods
- Medium Confidence: The claim that most LLMs are more sensitive to option order than token variations

## Next Checks

1. **Cross-model generalization test**: Apply the gray-box and black-box mitigation strategies to additional model families (e.g., Claude, LLaMA-3, GPT-4) and prompt engineering paradigms (CoT, ToT) to assess robustness across different architectures and prompting styles.

2. **Mechanistic analysis**: Conduct ablation studies examining internal model activations to identify which layers or attention heads are most responsible for order and token sensitivity, potentially revealing architectural reasons for these biases.

3. **Real-world impact assessment**: Test whether selection biases manifest in practical applications beyond multiple-choice tasks, such as ranking outputs, choosing between tool calls, or selecting responses in dialogue systems, to determine the broader implications of these findings.