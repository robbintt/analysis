---
ver: rpa2
title: Distribution-Specific Auditing For Subgroup Fairness
arxiv_id: '2401.16439'
source_url: https://arxiv.org/abs/2401.16439
tags:
- learning
- auditing
- problem
- halfspaces
- fairness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'We develop the first polynomial-time approximation scheme (PTAS)
  for auditing statistical subgroup fairness over homogeneous halfspace subgroups
  under Gaussian data. Our approach leverages recent advances in agnostic learning
  for Gaussian distributions via a novel reduction: we show that auditing is equivalent
  to agnostic learning of halfspaces with a fixed classification rate.'
---

# Distribution-Specific Auditing For Subgroup Fairness

## Quick Facts
- arXiv ID: 2401.16439
- Source URL: https://arxiv.org/abs/2401.16439
- Authors: Daniel Hsu; Jizhou Huang; Brendan Juba
- Reference count: 27
- We develop the first polynomial-time approximation scheme (PTAS) for auditing statistical subgroup fairness over homogeneous halfspace subgroups under Gaussian data.

## Executive Summary
This paper establishes the first polynomial-time algorithm for auditing statistical subgroup fairness over homogeneous halfspace subgroups under Gaussian distributions. The key insight is a reduction from auditing to agnostic learning of homogeneous halfspaces, enabling the application of existing PTAS results. The authors also prove a complementary hardness result showing that no polynomial-time algorithm can achieve nontrivial approximation for general halfspace subgroups under cryptographic assumptions.

## Method Summary
The approach reduces the auditing problem to agnostic learning of homogeneous halfspaces by fixing the classification rate threshold to 1/2. This reduction allows application of the Diakonikolas et al. PTAS for learning homogeneous halfspaces under Gaussian marginals, achieving additive error O(1/log^C d). The reduction preserves the unfairness measure as an affine transformation of the classification error, enabling the use of existing learning algorithms for auditing.

## Key Results
- First PTAS for auditing homogeneous halfspace subgroups under Gaussian data with additive error O(1/log^(1/C) d)
- Reduction from auditing to agnostic learning that enables use of existing PTAS results
- Hardness result showing no polynomial-time algorithm can achieve nontrivial approximation for general halfspace subgroups under cryptographic assumptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The auditing problem reduces to agnostic learning of homogeneous halfspaces under Gaussian data.
- Mechanism: By fixing the classification rate (threshold) of halfspaces to 1/2, the directional component becomes equivalent to learning homogeneous halfspaces. The unfairness measure is affinely related to the classification error, allowing a direct reduction.
- Core assumption: The data distribution is Gaussian and the 1D marginals have continuous CDFs, ensuring the threshold can be set precisely.

### Mechanism 2
- Claim: A PTAS exists for auditing homogeneous halfspaces under Gaussian data by leveraging existing agnostic learning PTAS.
- Mechanism: The reduction from auditing to agnostic learning allows application of the Diakonikolas et al. PTAS for homogeneous halfspaces under Gaussian marginals, achieving additive error O(1/log^C d).
- Core assumption: The Gaussian distribution allows the use of the specific PTAS algorithm that requires rotational invariance and known marginals.

### Mechanism 3
- Claim: Hardness results establish lower bounds on approximation quality, showing no polynomial-time algorithm can achieve better than O(1/√k log d) multiplicative approximation for general halfspaces.
- Mechanism: Reduction from continuous Learning With Errors (cLWE) problem shows that solving the auditing problem would break the hardness assumption of LWE.
- Core assumption: LWE remains hard under the stated parameters, and the reduction preserves the approximation gap.

## Foundational Learning

- Concept: Agnostic learning
  - Why needed here: The auditing problem is shown to be equivalent to agnostic learning of halfspaces, so understanding agnostic learning is crucial for implementing the reduction.
  - Quick check question: Can you explain the difference between realizable and agnostic learning, and why agnostic learning is necessary for fairness auditing?

- Concept: Halfspaces and homogeneous halfspaces
  - Why needed here: The paper focuses on auditing subgroups defined by halfspaces, and the positive result specifically addresses homogeneous halfspaces (normal vectors with unit norm).
  - Quick check question: What distinguishes homogeneous halfspaces from general halfspaces, and how does this restriction enable the PTAS?

- Concept: Statistical parity and subgroup fairness
  - Why needed here: The fairness notion being audited is statistical parity subgroup fairness, which requires measuring deviation in positive classification rates between subgroups and the overall population.
  - Quick check question: How does the statistical parity measure differ from other fairness metrics like equalized odds or individual fairness?

## Architecture Onboarding

- Component map: Data → Oracle call → Unfairness evaluation → Max selection → Certificate output
- Critical path: Data → Oracle call → Unfairness evaluation → Max selection → Certificate output. The oracle call is the computational bottleneck.
- Design tradeoffs: Using homogeneous halfspaces enables the PTAS but limits expressiveness compared to general halfspaces. The Gaussian assumption enables efficient algorithms but may not hold in practice.
- Failure signatures: If the oracle consistently returns poor approximations, the auditing result will be inaccurate. If the data violates Gaussian assumptions, the algorithm may fail silently or produce incorrect results.
- First 3 experiments:
  1. Verify the reduction works on a simple synthetic dataset with known unfair subgroups by comparing oracle results to brute-force enumeration.
  2. Test the PTAS guarantee by measuring the additive error on increasingly high-dimensional Gaussian data.
  3. Validate the hardness result by attempting to extend the algorithm to non-Gaussian distributions and measuring performance degradation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can polynomial-time algorithms achieve sub-constant additive error for auditing halfspace subgroups under log-concave distributions?
- Basis in paper: [inferred] The paper shows that under cryptographic assumptions, no polynomial-time algorithm can guarantee nontrivial approximation for general halfspace subgroups even under Gaussian distributions. However, the paper also mentions that the hardness results rely on the additive error being too small, suggesting that achieving better additive error bounds might be possible for broader classes of distributions.
- Why unresolved: The paper establishes hardness results for achieving sub-constant additive error under Gaussian distributions but does not explore whether similar results hold for log-concave distributions.
- What evidence would resolve it: A polynomial-time algorithm that can achieve sub-constant additive error for auditing halfspace subgroups under log-concave distributions would resolve this question positively.

### Open Question 2
- Question: Is it possible to develop an agnostic learning algorithm for general halfspaces that achieves additive error close to O(1/√log d) under Gaussian distributions?
- Basis in paper: [explicit] The authors state that even under Gaussian distributions, there are no known methods for learning general halfspaces that can achieve additive error better than a constant.
- Why unresolved: Despite the availability of efficient agnostic learning algorithms for homogeneous halfspaces under Gaussian distributions, the authors note that no such algorithms exist for general halfspaces with additive error guarantees close to O(1/√log d).
- What evidence would resolve it: A polynomial-time algorithm that can achieve additive error close to O(1/√log d) for learning general halfspaces under Gaussian distributions would resolve this question positively.

### Open Question 3
- Question: Can stronger guarantees be achieved for auditing conjunctions on Gaussian distributions?
- Basis in paper: [inferred] The authors mention that conjunctions are more natural in the context of auditing and their relative lack of expressive power might enable better guarantees.
- Why unresolved: The paper focuses on auditing subgroups defined by halfspaces and does not explore the potential of conjunctions for achieving stronger guarantees.
- What evidence would resolve it: A polynomial-time algorithm that can achieve better guarantees for auditing conjunctions on Gaussian distributions compared to halfspaces would resolve this question positively.

## Limitations
- Restriction to Gaussian data distributions limits applicability to real-world scenarios where data may not follow Gaussian distributions
- Focus on homogeneous halfspaces reduces expressiveness compared to general halfspaces
- Hardness result relies on cryptographic assumptions that cannot be proven unconditionally
- PTAS achieves additive rather than multiplicative error, which may be insufficient for detecting small unfairness levels in high-dimensional settings

## Confidence

- **High confidence**: The reduction from auditing to agnostic learning is mathematically sound and the PTAS for homogeneous halfspaces under Gaussian data is well-established from [18].
- **Medium confidence**: The hardness result for general halfspaces is contingent on cryptographic assumptions that, while standard, represent a theoretical rather than practical limitation.
- **Medium confidence**: The affine relationship between classification error and unfairness measures holds under the stated conditions but may not generalize to non-Gaussian settings.

## Next Checks

1. **Distribution sensitivity test**: Evaluate the auditing algorithm's performance on non-Gaussian synthetic datasets (e.g., uniform, exponential) to quantify the degradation in accuracy and identify break points.

2. **Oracle approximation analysis**: Systematically vary the approximation quality of the agnostic learning oracle and measure the corresponding impact on the final auditing result to establish the sensitivity of the reduction.

3. **Hardness boundary exploration**: Attempt to construct polynomial-time algorithms for special cases of general halfspaces (e.g., axis-aligned, low-dimensional) to identify subclasses where the hardness result may not apply.