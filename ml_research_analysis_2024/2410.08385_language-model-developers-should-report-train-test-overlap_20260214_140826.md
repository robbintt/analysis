---
ver: rpa2
title: Language model developers should report train-test overlap
arxiv_id: '2410.08385'
source_url: https://arxiv.org/abs/2410.08385
tags:
- overlap
- train-test
- test
- data
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper highlights the critical need for transparency in train-test
  overlap when evaluating language models. While models are routinely tested on public
  datasets, the extent to which those test datasets were already seen during training
  is rarely disclosed.
---

# Language model developers should report train-test overlap

## Quick Facts
- arXiv ID: 2410.08385
- Source URL: https://arxiv.org/abs/2410.08385
- Authors: Andy K Zhang; Kevin Klyman; Yifan Mai; Yoav Levine; Yian Zhang; Rishi Bommasani; Percy Liang
- Reference count: 40
- Only 30% of surveyed major language model developers provide adequate transparency about train-test overlap in their evaluations

## Executive Summary
This paper highlights the critical need for transparency in train-test overlap when evaluating language models. While models are routinely tested on public datasets, the extent to which those test datasets were already seen during training is rarely disclosed. Without this information, reported performance metrics can be misleading, as high scores may reflect memorization rather than true generalization. The authors surveyed 30 major model developers and found that only 9 had published sufficient data to contextualize train-test overlap: 4 by releasing their training data under open licenses, and 5 by publishing detailed overlap statistics and methodologies. To address this gap, they advocate that developers should report train-test overlap statistics alongside any evaluation results on public benchmarks, just as statisticians report confidence intervals.

## Method Summary
The study surveyed 30 language model developers and their flagship models, analyzing their train-test overlap practices. The authors used a standardized procedure to document current practices, including reviewing published papers, technical reports, and company websites, querying for relevant terms, and reaching out to developers for clarification. They assigned a binary score (0 or 1) to each developer based on whether they released sufficient information about train-test overlap, either by making training data publicly available or publishing detailed overlap statistics and methodology. The authors also provide a practical protocol for computing and reporting such overlap, using n-gram analysis as a standard approach.

## Key Results
- Only 9 out of 30 surveyed developers provided sufficient information about train-test overlap
- 4 developers released their training data under open licenses
- 5 developers published detailed overlap statistics and methodologies
- The lack of transparency undermines trust in reported model performance metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: N-gram overlap metrics are computationally feasible for detecting train-test overlap.
- Mechanism: By hashing test set n-grams and iterating through training data, the algorithm identifies overlapping n-grams efficiently without loading the full training corpus into memory.
- Core assumption: The number of unique n-grams is manageable in memory for reasonable n-gram sizes.
- Evidence anchors:
  - [section] "This enables rapid iteration with different aggregation methods on the instance-level overlap stats."
  - [abstract] "We also provide a practical protocol for computing and reporting such overlap, using n-gram analysis as a standard approach"
- Break condition: If n-gram size becomes too large (e.g., long sequences), the number of unique n-grams may exceed memory capacity.

### Mechanism 2
- Claim: Binary overlap provides a simple threshold for flagging potential contamination.
- Mechanism: An instance is marked as contaminated if any n-gram from the test set appears in the training data, providing a binary signal for further investigation.
- Core assumption: Even a single n-gram match indicates potential memorization or contamination risk.
- Evidence anchors:
  - [abstract] "We advocate that developers should report train-test overlap statistics alongside any evaluation results on public benchmarks"
  - [section] "Binary overlap marks an instance as overlapping if there is at least a single overlapping n-gram."
- Break condition: High false positive rate if common phrases or shared vocabulary across domains cause spurious matches.

### Mechanism 3
- Claim: Jaccard and token overlap metrics provide granular quantification of contamination severity.
- Mechanism: These metrics measure the fraction of overlapping n-grams (Jaccard) or tokens (token overlap) relative to the total, capturing the extent of overlap rather than just presence/absence.
- Core assumption: The proportion of overlap correlates with the likelihood of memorization or unfair advantage.
- Evidence anchors:
  - [section] "Jaccard overlap measures how many n-grams are overlapping for an instance and divides by the total number of n-grams in that instance."
  - [section] "Token overlap measures how many tokens are overlapping for an instance and divides by the total number of tokens in that instance."
- Break condition: These metrics may underestimate contamination if overlapping segments are strategically placed but don't constitute a large fraction of the total.

## Foundational Learning

- Concept: Statistical significance and confidence intervals
  - Why needed here: Understanding that overlap statistics, like any statistical measure, have uncertainty and should be interpreted with appropriate confidence intervals to assess validity.
  - Quick check question: If a test set has 100 examples and 5 show overlap, what additional information would you need to determine if this is statistically significant?

- Concept: Tokenization and n-gram extraction
  - Why needed here: The protocol relies on breaking text into n-grams, so understanding how tokenization affects overlap detection is crucial for proper implementation.
  - Quick check question: How would the overlap score change if you used character-level n-grams versus word-level n-grams for the same text?

- Concept: Data provenance and leakage
  - Why needed here: Understanding how data can unintentionally leak between training and test sets (e.g., through web scraping, common sources) is essential for interpreting overlap results.
  - Quick check question: What are three ways that evaluation data could end up in training data even when developers try to avoid it?

## Architecture Onboarding

- Component map: Hashing component for test n-grams -> Iteration component for training data -> Overlap detection component -> Aggregation component for metrics
- Critical path: The most computationally intensive step is iterating through the training data to check for n-gram matches, as training sets are typically terabytes in size.
- Design tradeoffs: Using smaller n-grams increases sensitivity but also false positives; larger n-grams are more specific but may miss partial contamination.
- Failure signatures: High overlap scores across all test sets might indicate a systemic issue with training data collection rather than isolated contamination.
- First 3 experiments:
  1. Run the overlap detection on a small synthetic dataset where you know the contamination status to verify correctness.
  2. Test the system with different n-gram sizes (3, 5, 13) on the same dataset to observe sensitivity vs specificity tradeoffs.
  3. Apply the system to a public benchmark where contamination is already documented to validate the approach.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of n-gram size affect the accuracy and consistency of train-test overlap detection across different evaluation datasets?
- Basis in paper: [explicit] The paper discusses the use of n-gram analysis for detecting train-test overlap and mentions that developers make design decisions such as what to set as N.
- Why unresolved: The paper acknowledges that different n-gram sizes might capture different types of overlap but does not provide empirical evidence on how n-gram size affects detection accuracy or consistency across datasets with varying characteristics (e.g., question-based vs. text-based datasets).
- What evidence would resolve it: Comparative analysis showing overlap detection results using different n-gram sizes (e.g., 5-grams vs. 13-grams) across multiple evaluation datasets, demonstrating how detection accuracy and false positive/negative rates vary with n-gram size.

### Open Question 2
- Question: What is the practical impact of train-test overlap on model generalization performance, and how can this relationship be quantified?
- Basis in paper: [explicit] The paper mentions that high train-test overlap contributes to significant degradation in performance between seen and unseen test examples, citing examples like GPT-4's performance drop on recent problems.
- Why unresolved: While the paper provides anecdotal evidence of performance degradation due to overlap, it does not offer a systematic framework for quantifying the relationship between overlap extent and generalization performance across different model types and tasks.
- What evidence would resolve it: Empirical studies measuring model performance on contaminated vs. decontaminated test sets across multiple model architectures and tasks, establishing a quantitative relationship between overlap percentage and performance degradation.

### Open Question 3
- Question: How effective are black-box methods for estimating train-test overlap compared to white-box approaches, and what are their limitations in adversarial settings?
- Basis in paper: [explicit] The paper discusses black-box methods (prompting, word probabilities, test instance ordering) and acknowledges their limitations, particularly in adversarial settings where developers might fine-tune models to avoid revealing training data.
- Why unresolved: The paper mentions that black-box methods have limitations but does not provide comparative evaluation of their effectiveness against white-box approaches or systematic analysis of their robustness to adversarial countermeasures.
- What evidence would resolve it: Head-to-head comparison of black-box and white-box overlap detection methods across various datasets and model types, including tests with intentionally adversarially modified models to assess detection robustness.

## Limitations
- Survey methodology relies on publicly available documentation and developer responses, which may miss internal practices or undocumented evaluation procedures.
- Binary scoring system (0 or 1) may oversimplify the nuanced spectrum of transparency practices among developers.
- Effectiveness of n-gram overlap detection depends heavily on tokenization choices and n-gram size, which can affect sensitivity and specificity in ways that aren't fully characterized.

## Confidence
**High Confidence**: The core claim that train-test overlap transparency is essential for valid model evaluation is well-supported by the survey data showing that only 30% of surveyed developers provide adequate overlap information. The practical protocol for computing overlap using n-gram analysis is technically sound and feasible.

**Medium Confidence**: The assertion that current overlap reporting practices are "inconsistent, opaque, and often not comprehensive" is supported by the survey but could benefit from more systematic analysis of the actual overlap statistics reported by the 5 developers who do provide them. The claim that this lack of transparency "undermines trust in reported model performance" is plausible but not directly measured.

**Low Confidence**: The recommendation that overlap reporting should be "mandatory alongside any evaluation results" on public benchmarks assumes consensus on what constitutes sufficient overlap information and appropriate thresholds for concern, which the paper doesn't fully address.

## Next Checks
1. **Replication on New Data**: Survey an additional 20-30 recent language model releases (2024-2025) to assess whether transparency practices have improved since the original study, particularly as awareness of data contamination issues has grown in the community.

2. **Overlap Metric Comparison**: Apply the proposed n-gram overlap protocol to a set of models where ground truth contamination is known (e.g., from documented data leaks or controlled experiments) to validate that the metric accurately detects problematic overlap without excessive false positives.

3. **Threshold Analysis**: Conduct a user study with ML practitioners to determine what overlap thresholds (e.g., Jaccard overlap > 0.1, token overlap > 0.2) they consider problematic for different evaluation contexts, helping to establish more concrete reporting standards.