---
ver: rpa2
title: Closed-Form Interpretation of Neural Network Classifiers with Symbolic Gradients
arxiv_id: '2401.04978'
source_url: https://arxiv.org/abs/2401.04978
tags:
- neural
- symbolic
- network
- data
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for interpreting individual neurons
  in neural network classifiers by expressing them in closed-form equations. The method
  embeds a trained network into an equivalence class of functions containing the same
  information, then uses symbolic regression to find the intersection with human-readable
  equations.
---

# Closed-Form Interpretation of Neural Network Classifiers with Symbolic Gradients

## Quick Facts
- arXiv ID: 2401.04978
- Source URL: https://arxiv.org/abs/2401.04978
- Reference count: 40
- One-line primary result: This paper introduces a framework for interpreting individual neurons in neural network classifiers by expressing them in closed-form equations using symbolic gradients.

## Executive Summary
This paper presents a novel framework for interpreting neural network classifiers by finding closed-form expressions for individual neurons. The method embeds a trained network into an equivalence class of functions sharing the same information, then uses symbolic regression to find human-readable equations matching the network's normalized gradients. Experiments on synthetic datasets show the approach successfully recovers exact decision boundaries in 4 out of 6 cases and provides good approximations in the remaining cases, particularly excelling when multiple high-level features could solve the classification task.

## Method Summary
The method works by computing normalized gradients of a neural network's output neuron with respect to inputs, then using genetic algorithms to find symbolic expressions whose gradients match. It requires differentiable activation functions (ELU recommended) and works best with disentangled neurons in bottleneck layers. The framework exploits the property that functions encoding the same concept have parallel gradients everywhere, allowing the complex interpretation problem to be reduced to a one-dimensional symbolic regression problem. While it loses threshold/bias information, this trade-off is acceptable given the complexity of the interpretation problem solved.

## Key Results
- Recovers exact decision boundaries in 4 out of 6 synthetic experiments (2-6 dimensions)
- Successfully interprets neural networks when multiple high-level features could solve the task, where direct symbolic classification fails
- Reduces the interpretation of complex neural networks to a one-dimensional symbolic regression problem
- Works with differentiable activation functions (ELU recommended) and disentangled bottleneck neurons

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method works by finding functions in the equivalence class that share the same normalized gradient direction as the neural network neuron.
- Mechanism: The framework computes normalized gradients ∇f/∥∇f∥ for both the neural network neuron f and candidate symbolic functions T, then uses MSE between these normalized gradients as the objective function.
- Core assumption: The data manifold D is compact and simply connected, and f ∈ C¹(D ⊂ ℝⁿ, ℝ) is continuously differentiable.
- Evidence anchors:
  - [section] "Using this property, I define the equivalence set Hg... The equivalence classes ˜Hg and Hg satisfy the definition of equivalence classes."
  - [section] "The extremely customizable library SymbolicRegression.jl can be programmed to... calculate gradients of a symbolic regression tree... normalize these gradients and optimize a custom loss function that compares normalized gradients."
  - [corpus] Weak evidence - the corpus papers focus on symbolic regression and interpretation but don't directly validate this specific gradient-matching mechanism.
- Break condition: If the neural network uses non-differentiable activation functions like ReLU or contains discontinuities in the data manifold, the normalized gradient approach breaks down due to undefined or noisy gradients.

### Mechanism 2
- Claim: The method successfully interprets neural networks when multiple high-level features could solve the classification task, whereas direct symbolic classification fails.
- Mechanism: Direct symbolic classification optimizes for classification accuracy directly, which can lead to selecting any valid feature among multiple options. The gradient-based interpretation method instead finds the specific feature that the neural network actually learned by matching the exact gradient patterns.
- Core assumption: The neural network learns a specific, consistent feature rather than randomly selecting among equivalent features during training.
- Evidence anchors:
  - [section] "In one experiment the neural network has the freedom of choosing between two different high-level features to make its prediction. The interpretation method reveals that the neural network learns a combination of both features."
  - [section] "Comparing these results with other works is impossible since most related works only replicate neural network regressors. As demonstrated in experiment 7, if there are multiple high-level concepts that can be employed to solve a specific task, replication is not interpretation."
  - [corpus] Weak evidence - the corpus papers discuss symbolic regression but don't specifically address the multiple-feature ambiguity problem that this mechanism solves.
- Break condition: If the neural network weights are initialized in a way that leads to symmetry breaking between equivalent features, or if the training process randomly selects different features across runs, the method may not consistently recover the intended feature.

### Mechanism 3
- Claim: The method reduces the interpretation of complex neural networks to a one-dimensional symbolic regression problem by leveraging the equivalence class property.
- Mechanism: Instead of trying to directly find a closed-form expression of the neural network's complex multi-dimensional function, the method exploits that the neural network neuron f can be written as ϕ(g) where g is a simpler function. By matching gradients, the method effectively finds g without needing to discover ϕ.
- Core assumption: The function g that the neural network encodes can be expressed as a one-dimensional function of the input variables.
- Evidence anchors:
  - [section] "This also means one loses the numerical value of the threshold/bias of the output neuron. This is however not a problem, since solving a one-dimensional symbolic regression problem or fitting a threshold/bias is a minor challenge compared to the complexity of problems solved by the presented interpretation framework."
  - [section] "To find a symbolic expression of a neural network learning the function G(x1, x2) in the context of regression, use the framework in this paper to find z, then prepare the symbolic regression problem for g(z) and use your favorite symbolic regression algorithm to solve a simple one-dimensional symbolic regression problem."
  - [corpus] Weak evidence - the corpus papers don't discuss this dimensionality reduction approach, though they do mention symbolic regression techniques that could be applied after this step.

## Foundational Learning

- Concept: Gradient-based equivalence classes for neural network interpretation
  - Why needed here: The paper's core insight is that functions encoding the same concept have parallel gradients everywhere. This allows reducing the interpretation problem from finding a full function to matching gradient directions.
  - Quick check question: If two functions f(x) = x² and g(x) = x⁴ both classify based on whether the input is positive or negative, do they belong to the same equivalence class under this framework?

- Concept: Symbolic regression with gradient information as targets
  - Why needed here: Standard symbolic regression tries to match function values, but neural networks can learn complex transformations. Using gradients as targets allows finding the underlying concept rather than just replicating the output.
  - Quick check question: If a neural network neuron computes f(x) = sin(x²), what would be the target for symbolic regression - the function values or the gradients?

- Concept: Pareto front optimization in symbolic search
  - Why needed here: The method doesn't just find one solution but a set of solutions trading off complexity and accuracy, which is important since the exact form of the learned concept may not be unique.
  - Quick check question: If the Pareto front shows a steep drop in MSE at complexity 15 but then plateaus, where should you look for the most interpretable solution?

## Architecture Onboarding

- Component map: Data preprocessing -> Neural network training -> Gradient extraction -> Symbolic regression -> Solution selection
- Critical path: Neural network training → Gradient extraction → Symbolic regression → Solution selection
- Design tradeoffs:
  - Using normalized gradients vs raw gradients: Normalized gradients are invariant to scaling but lose magnitude information
  - ELU vs ReLU activations: ELU provides smoother gradients but may be slower to train
  - Synthetic data generation: More data improves symbolic search but increases computation
- Failure signatures:
  - No meaningful solutions on Pareto front: Likely issues with gradient extraction or inappropriate building blocks
  - Solutions don't match true function: May need different activation functions or more complex building blocks
  - Extremely complex solutions: May need to adjust symbolic search hyperparameters or provide more training data
- First 3 experiments:
  1. Simple 2D dataset with circular decision boundary (x₁² + x₂² > 1)
  2. 3D dataset with sinusoidal boundary (x₁² + sin(x₂ + x₃) > 1)
  3. Dataset where neural network could learn either of two features, to test multi-feature interpretation

## Open Questions the Paper Calls Out
- The authors explicitly state that "it is possible to apply the framework to interpret multi-class classifiers" but don't demonstrate this capability.
- The paper mentions that the method "can be extended to hidden layers" but doesn't provide experimental validation of this claim.
- The authors note that "any suitable differentiable function space can do" for the symbolic regression component, suggesting potential for alternative implementations beyond PySR.

## Limitations
- Requires differentiable activation functions, excluding commonly used ReLU activations
- Only works with disentangled neurons in bottleneck layers, limiting applicability to modern deep architectures
- Loses threshold/bias information, providing only the functional form without normalization parameters
- Computational cost can be prohibitive for high-dimensional problems (only demonstrated on 2-6 dimensions)

## Confidence

**High Confidence:** The core mechanism of using normalized gradients to find equivalence classes of functions that encode the same concept is theoretically sound and well-established in differential geometry. The framework's mathematical foundations are rigorous and the approach to reducing interpretation to a one-dimensional symbolic regression problem is valid.

**Medium Confidence:** The experimental results on synthetic datasets are promising but limited in scope. While the method successfully recovers exact decision boundaries in 4 out of 6 cases, the remaining 2 cases only show "good approximations," and the paper doesn't provide detailed error analysis or confidence intervals for these approximations.

**Low Confidence:** The claim that this method solves the ambiguity problem when multiple high-level features could solve the classification task is based on limited experimental evidence. The paper demonstrates this on one synthetic experiment but doesn't provide systematic validation across multiple scenarios or compare against alternative interpretation methods.

## Next Checks

1. **Gradient Sensitivity Analysis:** Systematically vary the δ parameter for selecting data points near the decision boundary and measure how this affects the quality of recovered symbolic expressions. This would validate whether the method is robust to choices in data selection.

2. **Activation Function Comparison:** Replicate the experiments using different activation functions (ELU, Swish, Softplus) to quantify the impact of smoothness on interpretation quality. This would test whether ELU is truly optimal or if other differentiable activations work equally well.

3. **Scalability Test:** Apply the method to a real-world dataset with higher dimensionality (e.g., 10-20 dimensions) to identify computational bottlenecks and determine practical limits on problem size. This would validate whether the method can scale beyond synthetic toy problems.