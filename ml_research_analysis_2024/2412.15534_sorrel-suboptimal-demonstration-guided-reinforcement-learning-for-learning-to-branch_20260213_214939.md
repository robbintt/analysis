---
ver: rpa2
title: 'SORREL: Suboptimal-Demonstration-Guided Reinforcement Learning for Learning
  to Branch'
arxiv_id: '2412.15534'
source_url: https://arxiv.org/abs/2412.15534
tags:
- learning
- sorrel
- branching
- tree
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SORREL, a reinforcement learning approach for
  learning to branch in mixed integer linear programming (MILP) solvers. SORREL addresses
  the challenge of relying on high-quality demonstrations by efficiently learning
  from suboptimal demonstrations through offline reinforcement learning and self-imitation
  learning.
---

# SORREL: Suboptimal-Demonstration-Guided Reinforcement Learning for Learning to Branch

## Quick Facts
- arXiv ID: 2412.15534
- Source URL: https://arxiv.org/abs/2412.15534
- Authors: Shengyu Feng; Yiming Yang
- Reference count: 12
- Primary result: SORREL consistently outperforms previous neural branching methods in both branching quality and training efficiency across various MILP benchmarks.

## Executive Summary
This paper proposes SORREL, a reinforcement learning approach for learning to branch in mixed integer linear programming (MILP) solvers. SORREL addresses the challenge of relying on high-quality demonstrations by efficiently learning from suboptimal demonstrations through offline reinforcement learning and self-imitation learning. The key idea is to use a tree Markov Decision Process to model the branching process and prioritize good experiences from past trajectories. Experiments show that SORREL consistently outperforms previous neural branching methods in both branching quality and training efficiency across various MILP benchmarks, while also demonstrating better generalization performance to larger instances.

## Method Summary
SORREL uses a two-stage approach combining offline reinforcement learning pretraining on suboptimal demonstrations with online reinforcement learning finetuning using self-imitation learning. The method employs a tree Markov Decision Process formulation to model the branching process, with a random walk-based return definition that captures expected local dual-bound improvement. A graph neural network serves as the branching agent, and good experiences are tracked in priority queues to enable efficient learning from past trajectories.

## Key Results
- SORREL outperforms previous neural branching methods in both branching quality and training efficiency
- Demonstrates better generalization performance to larger instances
- Achieves consistent improvements across various MILP benchmarks

## Why This Works (Mechanism)

### Mechanism 1
Tree MDP formulation enables more general modeling of branching than prior MDP formulations by defining a tree MDP with left and right child transitions instead of temporal transitions, allowing modeling of any B&B process regardless of node selection strategy. The core assumption is that B&B can be represented as a binary tree where each branching decision creates exactly two child nodes.

### Mechanism 2
Random walk return definition captures expected local dual-bound improvement across all leaf nodes by defining return as expected cumulative discounted reward over random walks from root to leaves, weighted by node selection policy. The stationary distribution of random walks can be controlled to match desired node selection preferences.

### Mechanism 3
Self-imitation learning on priority queue of best trajectories improves sample efficiency by tracking top trajectories by tree size on each instance, then imitating branching decisions from these high-quality experiences. Good experiences can be identified by comparing actual return to expected return (advantage > 0).

## Foundational Learning

- Concept: Markov Decision Process fundamentals (states, actions, rewards, value functions)
  - Why needed here: SORREL builds on MDP theory but extends it to tree MDPs for branching
  - Quick check question: What's the difference between a standard MDP and the tree MDP used in SORREL?

- Concept: Reinforcement learning algorithms (Q-learning, policy gradients, actor-critic)
  - Why needed here: SORREL uses actor-critic architecture with offline pretraining and online finetuning
  - Quick check question: How does the Bellman operator in Equation 4 differ from standard MDP Bellman operators?

- Concept: Graph Neural Networks for representing MILP instances
  - Why needed here: The branching agent uses GNN architecture to process MILP structure as bipartite graphs
  - Quick check question: What are the variable and constraint nodes in the bipartite graph representation of MILPs?

## Architecture Onboarding

- Component map: GNN branching agent (policy network πϕ and value network Qθ) -> Offline RL pretraining component with demonstration buffer D1 -> Online RL finetuning component with priority queues D2 -> Tree MDP environment wrapper -> SCIP backend solver integration

- Critical path: Offline RL pretraining → Online RL finetuning with SIL → Evaluation on test instances

- Design tradeoffs:
  - Memory vs accuracy: Priority queues store limited number of best trajectories per instance
  - Exploration vs exploitation: SIL encourages exploitation of known good experiences
  - Computational cost vs sample efficiency: Random walk return calculation is expensive but provides better credit assignment

- Failure signatures:
  - Poor performance on transfer instances suggests overfitting to training distribution
  - High variance across runs indicates unstable learning dynamics
  - Slow improvement during online finetuning suggests inadequate exploration

- First 3 experiments:
  1. Test offline RL pretraining alone on validation instances to verify learning from suboptimal demonstrations
  2. Enable online finetuning with SIL disabled to measure impact on sample efficiency
  3. Vary priority queue size to find optimal tradeoff between memory usage and performance

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of return definition in tree MDPs affect the performance of SORREL on different MILP benchmarks? The paper proposes a return definition based on random walk and discusses its connection to local dual-bound improvement but does not provide a systematic comparison of different return definitions or their impact on performance across various MILP types.

### Open Question 2
What is the optimal balance between offline RL pretraining and online RL finetuning for SORREL? The paper presents SORREL as a two-stage RL method but does not provide a detailed analysis of the trade-off between the two stages or quantify the optimal allocation of training time or resources between them.

### Open Question 3
How does the quality of suboptimal demonstrations impact the performance of SORREL compared to traditional IL methods? While the paper shows SORREL outperforms GCNN-VHB, it does not provide a detailed analysis of how the quality of demonstrations affects the relative performance of these methods.

### Open Question 4
Can SORREL be extended to learn other heuristics in combinatorial optimization problems beyond variable selection in MILP? The paper mentions that future works can extend SORREL to other heuristics in combinatorial optimization but does not explore the applicability of SORREL to other heuristics or problem domains.

## Limitations

- The tree MDP formulation relies on assumptions about the branching process that may not hold for all MILP instances
- The effectiveness of the random walk return definition depends on convergence properties of the Markov chain
- The priority queue mechanism introduces hyperparameters that significantly impact performance but aren't thoroughly explored

## Confidence

- **High Confidence**: Empirical results showing SORREL outperforms previous neural branching methods across multiple MILP benchmarks
- **Medium Confidence**: Theoretical foundations of the tree MDP formulation and random walk return definition
- **Medium Confidence**: The claim that offline RL pretraining from suboptimal demonstrations improves sample efficiency

## Next Checks

1. Test the random walk return calculation on a wider range of tree structures to verify convergence properties and stationarity assumptions across different MILP instance families.

2. Systematically disable components (offline pretraining, SIL, priority queues) to quantify their individual contributions to the observed performance gains.

3. Evaluate SORREL on MILP instances with varying structural properties (e.g., different constraint densities, variable types) to assess generalization beyond the training distribution.