---
ver: rpa2
title: 'Rethinking the Evaluation of Dialogue Systems: Effects of User Feedback on
  Crowdworkers and LLMs'
arxiv_id: '2404.12994'
source_url: https://arxiv.org/abs/2404.12994
tags:
- user
- setup
- system
- feedback
- usefulness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper explores how user feedback in the form of follow-up\
  \ utterances affects the evaluation of task-oriented dialogue systems by both human\
  \ crowdworkers and large language models (LLMs). Two annotation setups are compared:\
  \ one without the user\u2019s follow-up utterance and one with it."
---

# Rethinking the Evaluation of Dialogue Systems: Effects of User Feedback on Crowdworkers and LLMs

## Quick Facts
- arXiv ID: 2404.12994
- Source URL: https://arxiv.org/abs/2404.12994
- Reference count: 40
- User feedback significantly influences both human and LLM evaluations of task-oriented dialogue systems

## Executive Summary
This study investigates how user feedback in the form of follow-up utterances affects the evaluation of task-oriented dialogue systems by both human crowdworkers and large language models. Using the ReDial dataset, the research compares two annotation setups: one without user follow-up utterances and one with them. Four dialogue qualities—relevance, usefulness, interestingness, and explanation quality—are assessed on a 100-point scale. The findings reveal that user feedback significantly impacts both human and LLM evaluations, with crowdworkers showing greater sensitivity to this feedback than LLMs. This highlights the importance of incorporating user feedback to refine dialogue system evaluations and suggests potential avenues for automated feedback integration.

## Method Summary
The study employs the ReDial dataset, sampling 100 dialogue turns with user utterances requesting movie recommendations. Two experimental conditions are tested: Setup 1 (without user follow-up) and Setup 2 (with user follow-up). Human annotators from Amazon Mechanical Turk and ChatGPT (gpt-3.5-turbo API) evaluate four dialogue qualities on a 100-point scale. Pairwise Cohen's Kappa and ICC assess agreement, while Spearman's correlation with expert ratings validates the results. Crowdworkers also indicate the sources of information they relied on for judgments.

## Key Results
- User feedback significantly influences both human and LLM evaluations, especially for usefulness and interestingness
- Crowdworkers are more affected by user feedback than LLMs, leading to more personalized usefulness assessments
- LLMs generally provide lower usefulness scores and less variability across setups compared to human annotators

## Why This Works (Mechanism)
The study demonstrates that user feedback provides critical context that shapes how evaluators assess dialogue system performance. This contextual information helps annotators better understand user needs and system responses, leading to more nuanced evaluations. The mechanism appears to work by making the evaluation task more grounded in the actual conversation flow rather than abstract assessment of isolated responses.

## Foundational Learning
- **Task-oriented dialogue evaluation**: Understanding how to assess conversational AI systems in goal-directed scenarios
  - Why needed: Provides the foundation for measuring dialogue system effectiveness
  - Quick check: Can identify key metrics (relevance, usefulness, interestingness, explanation quality)
- **Human-AI annotation comparison**: Recognizing differences between human and LLM evaluation patterns
  - Why needed: Helps identify strengths and limitations of automated evaluation methods
  - Quick check: Can explain why LLMs show less variability than humans in scoring
- **User feedback integration**: Understanding how follow-up utterances affect perception of dialogue quality
  - Why needed: Critical for developing more realistic evaluation frameworks
  - Quick check: Can describe how user feedback changes evaluation outcomes

## Architecture Onboarding

**Component map**: ReDial dataset -> Annotation setups (with/without feedback) -> Human annotators (AMT) -> LLM annotator (ChatGPT) -> Quality metrics -> Agreement analysis

**Critical path**: Data preparation → Annotation (human + LLM) → Agreement calculation → Correlation with expert ratings → Analysis of feedback effects

**Design tradeoffs**: The study balances ecological validity (using real user feedback) against control (simplified 100-point scale), but faces challenges in standardizing prompts across human and LLM annotators.

**Failure signatures**: Low agreement between annotators suggests unclear instructions or highly subjective aspects; significant LLM-human divergence indicates potential model limitations in understanding conversational context.

**Three first experiments**:
1. Replicate with a larger sample size (300+ dialogues) to test statistical robustness
2. Test multiple LLM models (GPT-4, Claude) to assess generalizability across architectures
3. Conduct ablation studies removing specific user feedback types to identify most influential signals

## Open Questions the Paper Calls Out
- How do different large language models compare in their susceptibility to user feedback from follow-up utterances when evaluating task-oriented dialogue systems?
- What is the optimal balance between providing context and avoiding bias when evaluating dialogue system aspects like interestingness?
- How can we systematically integrate user feedback from follow-up utterances into automated evaluation metrics for conversational systems?
- How does the presence of user feedback affect the calibration of LLM-based annotators compared to human annotators?

## Limitations
- Small sample size (100 dialogues) may not capture full variance in user feedback patterns
- Unknown exact prompt wording for LLM annotation limits reproducibility
- Limited transparency about expert validation methodology and qualifications

## Confidence
- **Medium**: The core observation that user feedback changes both human and LLM evaluation is plausible and aligns with known sensitivity to context in dialogue assessment, but effect sizes and generalizability are uncertain due to sample size and methodology gaps

## Next Checks
1. Re-run the annotation with multiple, carefully controlled prompt variants for the LLM to test sensitivity to prompt framing
2. Expand the sample size to at least 300 dialogues and ensure demographic diversity in crowdworker pools to strengthen statistical reliability
3. Collect and report disaggregated inter-rater agreement metrics for human-human, LLM-human, and LLM-LLM pairs to clarify where evaluation divergence occurs