---
ver: rpa2
title: 'Spiking CenterNet: A Distillation-boosted Spiking Neural Network for Object
  Detection'
arxiv_id: '2402.01287'
source_url: https://arxiv.org/abs/2402.01287
tags:
- spiking
- time
- object
- detection
- snns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We propose a novel fully SNN-based object detection framework trained
  on automotive data recorded by event-based cameras. Our model significantly outperforms
  comparable previous work on the challenging GEN1 Automotive Detection Dataset while
  using less than half the energy.
---

# Spiking CenterNet: A Distillation-boosted Spiking Neural Network for Object Detection

## Quick Facts
- **arXiv ID**: 2402.01287
- **Source URL**: https://arxiv.org/abs/2402.01287
- **Reference count**: 29
- **Primary result**: Proposes a novel fully SNN-based object detection framework trained on automotive data recorded by event-based cameras, significantly outperforming comparable previous work on the GEN1 Automotive Detection Dataset while using less than half the energy.

## Executive Summary
This paper presents Spiking CenterNet, a novel fully Spiking Neural Network (SNN)-based object detection framework designed for automotive applications using event-based cameras. The authors modify the CenterNet architecture to work with SNNs, achieving state-of-the-art performance on the challenging GEN1 Automotive Detection Dataset while consuming significantly less energy than comparable models. The framework incorporates a M2U-Net-based decoder with binary skip connections and employs knowledge distillation to further enhance performance. Notably, this is the first trained SNN detector that does not require costly Non-Maximum Suppression (NMS), making it more computationally efficient.

## Method Summary
The method involves training a Spiking CenterNet architecture on event-based automotive data from the GEN1 dataset. The model uses a ResNet-18 encoder with PLIF neurons, a M2U-Net-based decoder with binary skip connections, and CenterNet heads for heatmap, offset, and width/height prediction. Training employs surrogate gradient learning with the AdamW optimizer, cosine annealing, and knowledge distillation from a non-spiking teacher network. The input event data is preprocessed by sampling 100ms of events preceding annotations, split into 5 time steps with 2 micro time bins each, resulting in 4 input channels. The model is evaluated using COCO mAP across 10 IoU thresholds, with results reported as mean and standard deviation across 5 runs.

## Key Results
- The Spiking CenterNet model achieves state-of-the-art performance on the GEN1 Automotive Detection Dataset.
- Knowledge distillation further increases the model's performance.
- The model uses less than half the energy of comparable previous work while maintaining high accuracy.

## Why This Works (Mechanism)
### Mechanism 1
- Claim: The combination of spiking CenterNet with M2U-Net-based decoder yields state-of-the-art mAP while reducing energy consumption.
- Mechanism: The M2U-Net skip connections allow the SNN to retain high-level information efficiently through binary spike transmission, improving gradient flow. This enables a deep structure that performs well on object detection tasks without requiring costly Non-Maximum Suppression (NMS).
- Core assumption: Binary skip connections are sufficient for maintaining gradient flow and improving model performance.
- Evidence anchors:
  - [abstract] "Our model significantly outperforms comparable previous work on Prophesee's challenging GEN1 Automotive Detection Dataset while using less than half the energy."
  - [section III-B] "M2U-Net's decoding [1] is particularly suited for SNNs. Its skip connections between encoder and decoder allow the SNN to retain important high-level information more easily."
  - [corpus] Weak evidence; no direct citation for M2U-Net skip connections improving gradient flow in SNNs.
- Break condition: If binary skip connections fail to maintain sufficient gradient flow, the model's performance will degrade.

### Mechanism 2
- Claim: Knowledge Distillation (KD) improves SNN performance by providing soft target signals.
- Mechanism: KD transfers the knowledge of a larger, more capable non-spiking teacher network to the smaller, more efficient SNN student network. This is achieved by using the teacher's output as a soft target signal during training, which helps the SNN learn more effectively.
- Core assumption: The teacher network's output can be effectively mirrored by the SNN to improve its performance.
- Evidence anchors:
  - [abstract] "Knowledge distillation further increases performance."
  - [section III-C] "Our approach is straightforward: First, the non-spiking teacher is trained separately and then the weights are frozen during the training of the SNN."
  - [corpus] Weak evidence; no direct citation for KD improving SNN performance in object detection tasks.
- Break condition: If the teacher network's output cannot be effectively mirrored by the SNN, the KD approach will not improve performance.

### Mechanism 3
- Claim: The event-driven information processing and sparse activations of SNNs contribute to their energy efficiency.
- Mechanism: SNNs use all-or-nothing events (spikes) for communication between neurons, which facilitates fast, cost-effective neuron interactions. This is in contrast to conventional Artificial Neural Networks (ANNs), which primarily rely on non-binary floating-point values (floats).
- Core assumption: The event-driven nature of SNNs is inherently more energy-efficient than the continuous processing of ANNs.
- Evidence anchors:
  - [abstract] "SNNs are a promising approach to address this challenge, with their event-driven information flow and sparse activations."
  - [section I] "SNNs exhibit inherent power-efficiency as their distinctive feature is event-driven information processing, achieved through all-or-nothing events (spikes) for communication between neurons."
  - [corpus] Weak evidence; no direct citation for SNNs being more energy-efficient than ANNs in object detection tasks.
- Break condition: If the energy savings from event-driven processing are outweighed by the overhead of spike-based communication, the energy efficiency advantage will be negated.

## Foundational Learning
- Concept: Spiking Neural Networks (SNNs)
  - Why needed here: Understanding the event-driven nature and energy efficiency of SNNs is crucial for grasping the paper's main contributions.
  - Quick check question: What is the primary advantage of SNNs over conventional ANNs in terms of energy consumption?
- Concept: Knowledge Distillation (KD)
  - Why needed here: KD is a key technique used in the paper to improve the performance of the SNN-based object detector.
  - Quick check question: How does KD transfer knowledge from a teacher network to a student network?
- Concept: Object Detection with SNNs
  - Why needed here: The paper focuses on developing an SNN-based object detector, so understanding the challenges and approaches in this area is essential.
  - Quick check question: What are the main challenges in training SNNs for object detection tasks?

## Architecture Onboarding
- Component map: Input event data → ResNet-18 encoder → M2U-Net decoder → CenterNet heads → Output object detections
- Critical path: Input event data → ResNet-18 encoder → M2U-Net decoder → CenterNet heads → Output object detections
- Design tradeoffs:
  - Simplicity vs. complexity: The authors opt for a simple architecture to avoid the performance degradation that complex structures can cause with spiking activations.
  - Energy efficiency vs. performance: The authors balance the energy efficiency of SNNs with the need for high performance in object detection tasks.
- Failure signatures:
  - Poor gradient flow through the network
  - Inadequate spike transmission in skip connections
  - Failure to effectively mirror teacher network output in KD
- First 3 experiments:
  1. Evaluate the performance of the SNN model with and without KD on the GEN1 dataset.
  2. Compare the energy consumption of the SNN model with that of the non-spiking teacher network.
  3. Assess the impact of varying the number of time steps on the model's performance and energy consumption.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the performance of the SNN model with fewer time steps compare to its non-spiking counterpart when trained on RGB data instead of event data?
- Basis in paper: [inferred] The paper focuses on event data and does not explore RGB data input for the SNN model.
- Why unresolved: The paper does not provide any experimental results or analysis on using RGB data as input for the SNN model.
- What evidence would resolve it: Conducting experiments with the SNN model using RGB data as input and comparing its performance to the non-spiking counterpart.

### Open Question 2
- Question: Can the proposed SNN architecture be extended to handle 3D bounding box detection and human pose estimation tasks, as mentioned in the conclusion?
- Basis in paper: [explicit] The paper mentions the potential for extending the SNN architecture to handle 3D bounding box detection and human pose estimation tasks.
- Why unresolved: The paper does not provide any experimental results or analysis on applying the SNN architecture to these tasks.
- What evidence would resolve it: Conducting experiments with the SNN architecture on 3D bounding box detection and human pose estimation tasks and evaluating its performance.

### Open Question 3
- Question: How does the proposed SNN model perform on other event-based datasets, such as DVS128 Gesture or N-CARS, compared to the GEN1 dataset?
- Basis in paper: [inferred] The paper focuses on the GEN1 dataset and does not explore the performance of the SNN model on other event-based datasets.
- Why unresolved: The paper does not provide any experimental results or analysis on using the SNN model with other event-based datasets.
- What evidence would resolve it: Conducting experiments with the SNN model on other event-based datasets and comparing its performance to the results on the GEN1 dataset.

## Limitations
- The paper lacks detailed ablation studies to isolate the contributions of individual architectural choices.
- Several critical implementation details remain underspecified, including exact PLIF neuron parameters and surrogate gradient learning hyperparameters.
- The energy efficiency claims are not supported by actual hardware-level measurements.

## Confidence
- **High confidence**: The overall SNN-based object detection framework design and the use of knowledge distillation for performance improvement are well-established concepts.
- **Medium confidence**: The claim of state-of-the-art performance on GEN1 dataset is supported by comparative results, though limited by lack of extensive baseline comparisons.
- **Low confidence**: The specific mechanisms by which M2U-Net skip connections improve gradient flow in SNNs and the energy efficiency claims require further empirical validation.

## Next Checks
1. **Ablation study**: Systematically evaluate the impact of knowledge distillation, M2U-Net skip connections, and CenterNet architecture on both performance and energy consumption.
2. **Energy measurement**: Conduct actual hardware-level energy consumption measurements comparing the SNN model with non-spiking baselines under identical conditions.
3. **Generalization test**: Evaluate model performance on out-of-distribution scenarios (different weather conditions, time of day, camera types) to assess robustness beyond the GEN1 dataset.