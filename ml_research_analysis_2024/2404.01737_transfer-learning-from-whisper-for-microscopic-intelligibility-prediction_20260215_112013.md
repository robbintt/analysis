---
ver: rpa2
title: Transfer Learning from Whisper for Microscopic Intelligibility Prediction
arxiv_id: '2404.01737'
source_url: https://arxiv.org/abs/2404.01737
tags:
- speech
- learning
- whisper
- intelligibility
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses microscopic intelligibility prediction, which
  aims to predict listeners' lexical responses to noisy speech stimuli, going beyond
  macroscopic models that predict average word error rates. The authors use Whisper,
  a large-scale pre-trained ASR model, to predict distributions over possible lexical
  responses conditioned on noisy speech waveforms.
---

# Transfer Learning from Whisper for Microscopic Intelligibility Prediction

## Quick Facts
- arXiv ID: 2404.01737
- Source URL: https://arxiv.org/abs/2404.01737
- Reference count: 0
- Pre-trained Whisper models predict listener responses to noisy speech with 5% top-1 accuracy in zero-shot and 13% after fine-tuning

## Executive Summary
This paper explores microscopic intelligibility prediction, which aims to predict individual listeners' lexical responses to noisy speech stimuli rather than just average word error rates. The authors leverage Whisper, a large-scale pre-trained ASR model, to predict distributions over possible lexical responses conditioned on noisy speech waveforms. They evaluate both zero-shot performance and fine-tuning approaches, finding that while Whisper can outperform naive baselines in zero-shot setup, fine-tuning significantly improves performance, particularly when all model modules are updated. The study highlights the importance of acoustic-level features and demonstrates how pre-trained ASR models can be adapted for intelligibility prediction tasks.

## Method Summary
The authors frame microscopic intelligibility prediction as a classification task where the model predicts listeners' lexical responses from noisy speech waveforms. They use Whisper's tokenizer to construct a fixed vocabulary from listeners' responses and evaluate different fine-tuning strategies: freezing all modules except the output layer, fine-tuning the decoder only, and fine-tuning all modules including the convolutional encoder. The models are trained on listener responses to 8,000+ utterances corrupted by six types of noise at four SNRs. Performance is evaluated using top-k accuracy metrics and average top-n coverage, comparing zero-shot predictions against naive baselines.

## Key Results
- Zero-shot Whisper achieves 5% top-1 accuracy and 25% average top-n coverage
- Fine-tuned models achieve 13% top-1 accuracy (66% relative improvement)
- Best performance comes from fine-tuning all Whisper modules, including the encoder
- Performance scales with model size and varies by noise type, with worst results on four-speaker babble noise

## Why This Works (Mechanism)
Whisper's pre-training on diverse speech data provides robust acoustic and linguistic representations that capture the mapping between speech waveforms and textual content. The model's ability to generate probability distributions over possible responses aligns with the probabilistic nature of microscopic intelligibility, where listeners may produce multiple plausible interpretations of the same stimulus. Fine-tuning allows the model to adapt these representations to the specific characteristics of the test set, including the talker's voice and the particular noise conditions encountered during evaluation.

## Foundational Learning
- **Microscopic vs macroscopic intelligibility**: Microscopic prediction targets individual listener responses while macroscopic models predict average performance metrics. This distinction is crucial because it requires modeling the variability in human perception rather than just overall intelligibility trends.
- **Whisper's multi-stage architecture**: Understanding the encoder-decoder structure with its convolutional front-end and transformer blocks is essential for knowing which components to fine-tune for optimal performance.
- **Classification with large label spaces**: The approach treats listener responses as class labels, requiring techniques to handle thousands of unique responses efficiently.
- **Fine-tuning strategies**: Different approaches to updating model parameters (frozen vs partial vs full fine-tuning) significantly impact performance and computational requirements.
- **Evaluation metrics for top-k accuracy**: Using top-k accuracy and coverage metrics provides a more nuanced assessment than simple accuracy, capturing the model's ability to rank correct responses highly.
- **Noise types and SNR effects**: Understanding how different noise conditions affect model performance helps identify robustness limitations and areas for improvement.

## Architecture Onboarding
- **Component map**: Input waveform → Conv encoder → Positional encoding → Transformer blocks → Decoder → Output layer → Predicted response distribution
- **Critical path**: The encoder-decoder architecture processes speech through multiple stages, with the convolutional encoder extracting acoustic features that are crucial for fine-tuning performance
- **Design tradeoffs**: Full fine-tuning yields best results but requires more compute; freezing layers reduces training cost but sacrifices performance; the choice affects generalization to new conditions
- **Failure signatures**: Poor performance on babble noise suggests limitations in handling overlapping speech; low zero-shot accuracy indicates domain mismatch between pre-training and test data
- **First experiments**: 1) Compare zero-shot vs fine-tuned performance across noise types, 2) Test different fine-tuning strategies (frozen, decoder-only, full), 3) Evaluate scaling effects with different Whisper model sizes

## Open Questions the Paper Calls Out
None

## Limitations
- Zero-shot performance remains low at 5% top-1 accuracy despite outperforming baselines
- Single-talker dataset limits generalizability to diverse speakers and populations
- Study only evaluates normal-hearing native speakers, excluding those with hearing impairments
- Four-speaker babble noise proves particularly challenging, suggesting robustness limitations

## Confidence
- High: Relative performance comparisons between models and fine-tuning strategies
- Medium: Absolute performance metrics and implications for real-world applications
- Low: Extent to which Whisper predictions capture true human listening behavior

## Next Checks
1. Test fine-tuned models on listeners with hearing impairments and varying linguistic backgrounds to assess generalizability beyond normal-hearing native speakers
2. Evaluate model predictions against listener responses at the individual word level rather than full utterance responses to better understand granular performance patterns
3. Compare Whisper-based predictions with established macroscopic intelligibility metrics across different noise conditions to establish ecological validity