---
ver: rpa2
title: Grounding Language about Belief in a Bayesian Theory-of-Mind
arxiv_id: '2402.10416'
source_url: https://arxiv.org/abs/2402.10416
tags:
- belief
- agent
- beliefs
- language
- about
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a Bayesian theory-of-mind framework for grounding
  natural language statements about beliefs. The approach models observers as performing
  joint inference over an agent's goals, beliefs, and plans based on observed actions,
  then evaluates belief statements against these inferences using epistemic logic.
---

# Grounding Language about Belief in a Bayesian Theory-of-Mind

## Quick Facts
- arXiv ID: 2402.10416
- Source URL: https://arxiv.org/abs/2402.10416
- Authors: Lance Ying; Tan Zhi-Xuan; Lionel Wong; Vikash Mansinghka; Joshua Tenenbaum
- Reference count: 6
- This paper presents a Bayesian theory-of-mind framework for grounding natural language statements about beliefs, showing significantly better correlation with human belief attributions than baselines.

## Executive Summary
This paper introduces a Bayesian theory-of-mind framework that grounds natural language statements about beliefs by modeling observers as performing joint inference over an agent's goals, beliefs, and plans. The approach combines epistemic logic for representing belief statements, a probabilistic generative model for the functional role of belief in guiding actions, and Bayesian inference to estimate the agent's mental states. The framework is evaluated on gridworld experiments where participants watch an agent solve puzzles and rate the agent's beliefs, demonstrating that the Bayesian ToM model correlates significantly better with human belief attributions than baselines that don't perform mentalizing.

## Method Summary
The framework uses Bayesian inverse planning to infer an agent's goals and beliefs from observed actions, then evaluates belief statements against these inferences using epistemic logic. An LLM translates natural language belief statements into first-order epistemic logic formulas, which are evaluated against the inferred distribution of agent beliefs. The model computes the expected truth value of belief formulas under the posterior distribution, producing graded responses that correlate with human ratings. The approach is implemented using a probabilistic programming architecture and tested on gridworld scenarios where agents need to reason about hidden objects to achieve goals.

## Key Results
- The Bayesian ToM model achieves significantly higher correlation with human belief attributions compared to baselines that don't perform mentalizing or ignore instrumental plans
- The model successfully grounds graded human belief judgments by computing probabilities over possible belief states rather than binary true/false judgments
- Results demonstrate the importance of theory-of-mind for understanding language about beliefs, particularly in scenarios requiring instrumental reasoning about hidden objects

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model grounds belief statements by computing the probability that a logical formula about beliefs is true in the inferred belief state distribution, conditioned on observed actions.
- Mechanism: Observers perform joint inference over goals and beliefs using a Bayesian inverse planning model. The model samples initial states and goals, updates weights based on the likelihood of observed actions given those beliefs, then evaluates the belief statement by computing the expected truth value over the resulting posterior distribution.
- Core assumption: Rational action principle - agents plan efficiently to achieve goals given their beliefs, and observers can recover these beliefs by inverting the planning process.
- Evidence anchors:
  - [abstract]: "By modeling how humans jointly infer coherent sets of goals, beliefs, and plans that explain an agent's actions, then evaluating statements about the agent's beliefs against these inferences via epistemic logic"
  - [section]: "To implement this framework, we combine the strengths of machine learning methods with the coherence and precision of Bayesian and logical reasoning... then evaluating these statements with respect to the inferences produced by a probabilistic programming architecture for Bayesian inverse planning"
  - [corpus]: Weak evidence - corpus papers focus on similar ToM approaches but don't specifically address this joint inference mechanism for belief grounding
- Break condition: If agents are not rational or if the observer cannot model the planning process accurately, the inverse inference will fail and belief grounding will break down.

### Mechanism 2
- Claim: The model achieves compositionality in belief understanding by translating natural language belief statements into epistemic logic formulas, which can then be evaluated against inferred mental states.
- Mechanism: Large language models are used as semantic parsers to translate English belief statements into first-order epistemic logic. These logical formulas are then evaluated against the inferred distribution of agent beliefs using epistemic operators like "believes".
- Core assumption: LLMs can reliably translate natural language into logical form when given appropriate examples and domain constraints.
- Evidence anchors:
  - [abstract]: "By modeling how humans jointly infer coherent sets of goals, beliefs, and plans... then evaluating statements about the agent's beliefs against these inferences via epistemic logic"
  - [section]: "we translate the natural language statement σ into a formula ψ in epistemic logic... then evaluate the expected truth value of the translated sentence ψ with respect to inferred distribution over belief states"
  - [corpus]: Moderate evidence - corpus contains papers on epistemic language understanding but none specifically using LLMs for translation to epistemic logic
- Break condition: If the LLM fails to correctly parse complex belief statements or introduces translation errors, the logical evaluation will be based on incorrect formulas.

### Mechanism 3
- Claim: The model explains graded human belief judgments by computing a probability distribution over possible belief states rather than binary true/false judgments.
- Mechanism: Instead of using pure logical deduction (which would yield binary judgments), the model computes P(ψ|a1:T) as the expected truth value of the belief formula under the posterior distribution over beliefs. This naturally produces graded responses that correlate with human ratings.
- Core assumption: Human belief attributions are probabilistic rather than deterministic, and can be modeled as expectations over possible belief states.
- Evidence anchors:
  - [abstract]: "our model provides a much better fit to human goal and belief attributions, demonstrating the importance of theory-of-mind for understanding language about beliefs"
  - [section]: "Human judgments about what agents believe are not all-or-nothing phenomena. Our account provides a semantics for such graded judgments, unlike logical models which judge statements to be either false or true"
  - [corpus]: Moderate evidence - corpus papers discuss graded ToM judgments but don't specifically address the probabilistic grounding mechanism
- Break condition: If human judgments are actually more deterministic than modeled, or if participants use different reasoning strategies, the probabilistic model may overfit to noise.

## Foundational Learning

- Concept: Epistemic logic
  - Why needed here: Provides the formal representation language for belief statements, allowing compositional evaluation of nested beliefs and complex epistemic relationships
  - Quick check question: How would you represent "The agent believes that there might be a key in box 1" in epistemic logic?

- Concept: Bayesian inverse planning
  - Why needed here: Enables observers to infer agents' mental states by inverting the planning process - reasoning backward from observed actions to likely goals and beliefs
  - Quick check question: What is the key difference between forward planning and inverse planning in this context?

- Concept: Probabilistic programming
  - Why needed here: Provides the computational framework for efficiently implementing the joint inference over goals and beliefs, handling the combinatorial complexity of mental state spaces
  - Quick check question: Why might exact enumeration of all possible goal-belief combinations be computationally intractable?

## Architecture Onboarding

- Component map: Natural language parser (LLM) → Epistemic logic translator → Bayesian inverse planning engine → Joint goal/belief inference → Epistemic logic evaluator → Truth value computation → Human response comparator → Model validation
- Critical path: Natural language statement → Logic translation → Belief inference → Formula evaluation → Probability output
- Design tradeoffs: Using LLMs for translation trades engineering complexity for potential parsing errors; probabilistic inference trades computational efficiency for approximation error
- Failure signatures: Low correlation with human judgments indicates issues with either the inference model, translation quality, or experimental design
- First 3 experiments:
  1. Verify LLM translation accuracy by comparing generated logical formulas against ground truth on a held-out validation set
  2. Test Bayesian inference engine on synthetic data with known ground truth to validate the joint inference algorithm
  3. Run ablation study removing epistemic logic component to quantify contribution of compositional belief representation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do humans account for the base rates of belief statements when evaluating their likelihood, and under what circumstances do they deviate from Bayesian reasoning?
- Basis in paper: [explicit] The paper notes that participants "did not seem to take into account these base rates when providing responses" and instead rated statements based on evidence, suggesting non-Bayesian reasoning patterns.
- Why unresolved: The study only compared two prior distributions (uniform over states vs uniform over statements) but didn't explore other potential prior distributions or investigate when humans use base rates versus evidence-based reasoning.
- What evidence would resolve it: Experiments testing human belief evaluations under different prior distributions, measuring when participants switch between base-rate reasoning and evidence-based reasoning, and examining individual differences in these strategies.

### Open Question 2
- Question: How can the framework be extended to model uncertain beliefs, false beliefs, and abstract belief representations beyond deterministic knowledge states?
- Basis in paper: [explicit] The discussion section identifies this as a key limitation, noting the framework "only explains the deterministic beliefs of other agents" and that "one of the hallmarks about belief — and how we reason and talk about it — is that it can come apart from reality in all sorts of ways."
- Why unresolved: The current framework assumes perfect knowledge and veridical perception, making it unable to model the rich variety of belief types humans naturally reason about.
- What evidence would resolve it: Development and validation of an extended framework that can model uncertainty about uncertainty, false beliefs, and abstract belief representations, along with experiments showing it better captures human belief attributions in scenarios involving these phenomena.

### Open Question 3
- Question: How do humans compose and evaluate complex belief statements involving nested beliefs, temporal aspects, and modality operators?
- Basis in paper: [explicit] The paper mentions creating "negated, conjunctive, and disjunctive belief statements" but only evaluates simple belief statements about box contents, suggesting a need to test more complex compositions.
- Why unresolved: The experiments only tested simple existential and negated belief statements, leaving unexplored how humans handle belief sentences with multiple levels of embedding, temporal qualifiers, or modal operators like "might" or "likely."
- What evidence would resolve it: Experiments testing human interpretation of increasingly complex belief statements involving multiple levels of nesting, temporal aspects, and modal operators, comparing human responses to model predictions under different compositional mechanisms.

## Limitations

- The framework assumes perfect knowledge and veridical perception, limiting its ability to model false beliefs and uncertain beliefs
- The evaluation is confined to a specific gridworld domain with relatively simple belief structures, raising questions about generalizability to more complex real-world scenarios
- Reliance on LLM-based translation introduces uncertainty about robustness across different belief statement types and linguistic variations

## Confidence

**High confidence**: The core Bayesian inverse planning mechanism and its integration with epistemic logic for belief evaluation. The mathematical framework is well-established and the implementation follows standard approaches.

**Medium confidence**: The effectiveness of LLM-based translation for epistemic logic, as this depends heavily on the quality of prompt engineering and the specific LLM used. The corpus shows limited prior work in this exact approach.

**Medium confidence**: The model's ability to generalize beyond the gridworld domain, as the evaluation is confined to a specific experimental setup with controlled variables.

## Next Checks

1. **Translation Robustness Test**: Evaluate the LLM translation pipeline across diverse belief statement types and linguistic variations to assess consistency and identify potential failure modes in the natural language to logic conversion.

2. **Domain Transfer Validation**: Test the framework on a different domain (e.g., social scenarios or physical reasoning tasks) to assess generalizability beyond the gridworld environment used in the current evaluation.

3. **Human Performance Benchmark**: Compare model predictions against detailed human behavioral data to identify systematic differences in belief attribution strategies, particularly focusing on cases where the model outperforms or underperforms human judgments.