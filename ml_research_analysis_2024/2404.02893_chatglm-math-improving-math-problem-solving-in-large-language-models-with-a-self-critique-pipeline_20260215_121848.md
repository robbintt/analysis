---
ver: rpa2
title: 'ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with
  a Self-Critique Pipeline'
arxiv_id: '2404.02893'
source_url: https://arxiv.org/abs/2404.02893
tags:
- language
- math-critique
- mathematical
- arxiv
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a self-critique pipeline to improve large
  language models' mathematical reasoning while preserving their language abilities.
  The approach trains a Math-Critique model from the base model to evaluate outputs,
  then applies rejective fine-tuning and direct preference optimization using the
  model's own feedback.
---

# ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline

## Quick Facts
- arXiv ID: 2404.02893
- Source URL: https://arxiv.org/abs/2404.02893
- Authors: Yifan Xu; Xiao Liu; Xinghan Liu; Zhenyu Hou; Yueyan Li; Xiaohan Zhang; Zihan Wang; Aohan Zeng; Zhengxiao Du; Wenyi Zhao; Jie Tang; Yuxiao Dong
- Reference count: 40
- Key outcome: Self-critique pipeline improves mathematical reasoning in LLMs while maintaining language abilities, achieving state-of-the-art among open-source models.

## Executive Summary
This work introduces a self-critique pipeline to improve large language models' mathematical reasoning while preserving their language abilities. The approach trains a Math-Critique model from the base model to evaluate outputs, then applies rejective fine-tuning and direct preference optimization using the model's own feedback. Experiments on ChatGLM3-32B show significant gains in mathematical performance (e.g., +17.5% on GSM8k, +40.6% on MATH) while improving or maintaining language capabilities. The method outperforms models twice its size and achieves state-of-the-art results among open-source models. A new benchmark, MathUserEval, is introduced to assess real-world mathematical reasoning. Limitations include difficulty with visual reasoning and precision calculations. The technique has been deployed in production.

## Method Summary
The self-critique pipeline improves mathematical reasoning by training a Math-Critique model from the base LLM to evaluate mathematical responses. This self-generated feedback is used in two sequential phases: Rejective Fine-tuning (RFT) filters and fine-tunes on responses meeting quality thresholds, while Direct Preference Optimization (DPO) trains on contrastive pairs of correct and incorrect answers. The pipeline is trained on diverse datasets including academic benchmarks and real-world problems, with the model's own critiques guiding the refinement process.

## Key Results
- +17.5% accuracy on GSM8k benchmark
- +40.6% accuracy on MATH benchmark
- Achieved 4.23 score on MathUserEval, outperforming all models with published parameters
- Outperformed models twice its size (ChatGLM3-65B)
- Maintained or improved language capabilities on AlignBench and MT-Bench

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The self-critique pipeline improves mathematical performance by training the model to evaluate its own outputs, enabling targeted refinement without external human annotations.
- Mechanism: A Math-Critique model is first trained from the base LLM to score and analyze mathematical responses. This self-generated feedback is then used in two phases: Rejective Fine-tuning (RFT) filters and fine-tunes on responses that meet quality thresholds, while Direct Preference Optimization (DPO) trains on contrastive pairs of correct and incorrect answers. This iterative, self-supervised loop progressively enhances mathematical reasoning.
- Core assumption: The model's self-generated critiques are sufficiently accurate and reliable to guide fine-tuning without external supervision.
- Evidence anchors:
  - [abstract]: "We first train a general Math-Critique model from the LLM itself to provide feedback signals. Then, we sequentially employ rejective fine-tuning and direct preference optimization over the LLM's own generations for data collection."
  - [section 3]: "We propose Math-Critique, inspired by works that use large models for evaluation purposes... This method scores mathematical responses generated by models based on questions and reference answers."
  - [corpus]: Weak evidence; neighboring papers discuss self-criticism and preference optimization but do not detail this exact pipeline or report similar improvements.
- Break condition: If Math-Critique's accuracy drops below a threshold, the feedback loop may reinforce errors rather than correct them.

### Mechanism 2
- Claim: The two-stage training (RFT then DPO) allows the model to first improve consistency and then refine its ability to distinguish correct from incorrect reasoning.
- Mechanism: RFT uses rejection sampling based on Math-Critique scores to retain diverse, high-quality responses for fine-tuning. DPO then directly learns from contrastive pairs (chosen vs rejected answers), sharpening the model's preference for correct solutions.
- Core assumption: Filtering via Math-Critique during RFT preserves diversity while improving quality, and DPO's contrastive learning effectively discriminates between correct and incorrect reasoning.
- Evidence anchors:
  - [abstract]: "We sequentially employ rejective fine-tuning and direct preference optimization over the LLM's own generations for data collection."
  - [section 4.1]: "We utilized a rejection sampling method based on Math-Critique... We conducted a selection process after 5-10 sampling iterations based on the results from Math-Critique."
  - [section 4.2]: "We employed the DPO method to enhance model capabilities further... The DPO method directly compares the correct and incorrect answers to the same question."
  - [corpus]: Weak evidence; no direct comparison of RFT+DPO two-stage approaches in the corpus.
- Break condition: If the rejection threshold is too strict, diversity may drop and model performance plateau; if too lenient, poor responses may be retained.

### Mechanism 3
- Claim: Integrating a diverse real-world dataset (MathUserEval) during training better prepares the model for practical mathematical reasoning than purely academic datasets.
- Mechanism: Training data includes both academic sources (GSM8k, MATH) and real-world sources (simulated dialogues, university exams), with the model trained on problems reflecting varied user needs.
- Core assumption: Exposure to diverse, real-world problem types during training translates to improved performance on similarly varied evaluation sets.
- Evidence anchors:
  - [abstract]: "To accurately assess LLMs' capabilities in solving real-world mathematical problems, we develop the MATHUSER EVAL dataset... It features a diverse range of questions, extending beyond academic exercises to include practical application scenarios."
  - [section 6.3]: "Our model achieved a score of 4.23 on MATHUSER EVAL... outperforming all models with published parameters."
  - [corpus]: Weak evidence; neighboring papers discuss real-world math tasks but do not explicitly integrate diverse real-world datasets into self-critique pipelines.
- Break condition: If the model overfits to training data distribution, it may not generalize to unseen problem types or difficulty levels.

## Foundational Learning

- Concept: Supervised fine-tuning (SFT) basics
  - Why needed here: SFT is the starting point for aligning the base LLM to mathematical tasks; understanding its role clarifies why the self-critique pipeline is necessary to overcome SFT's limitations.
  - Quick check question: What is the main limitation of SFT for mathematical reasoning, and how does the self-critique pipeline address it?

- Concept: Reinforcement learning from human feedback (RLHF) and preference optimization
  - Why needed here: The paper builds on RLHF concepts but replaces human feedback with AI-generated feedback; understanding this shift is key to grasping the novelty.
  - Quick check question: How does DPO differ from standard RLHF, and why is it suitable for mathematical reasoning?

- Concept: Rejection sampling and contrastive learning
  - Why needed here: These are the core mechanisms for filtering and refining responses in the self-critique pipeline; knowing how they work helps debug and tune the pipeline.
  - Quick check question: In what way does rejection sampling during RFT contribute to model diversity, and how does DPO use contrastive pairs?

## Architecture Onboarding

- Component map:
  Base LLM (e.g., ChatGLM3-32B) -> Math-Critique model (trained from base LLM) -> RFT training loop (rejection sampling + fine-tuning) -> DPO training loop (contrastive pairs + preference optimization) -> MathUserEval dataset (real-world and academic math problems) -> Evaluation pipeline (Math-Critique and GPT-4-Turbo scoring)

- Critical path:
  1. Train Math-Critique from base LLM
  2. Generate candidate responses on MathUserEval and academic datasets
  3. Apply Math-Critique to score and filter responses
  4. Fine-tune base LLM with RFT on accepted responses
  5. Generate new candidate responses from RFT model
  6. Select contrastive pairs (chosen vs rejected) via Math-Critique
  7. Fine-tune with DPO on contrastive pairs
  8. Evaluate on MathUserEval and academic benchmarks

- Design tradeoffs:
  - Using self-generated feedback avoids manual annotation but depends on Math-Critique's accuracy.
  - Rejection sampling balances quality and diversity; overly strict thresholds may harm generalization.
  - DPO's regularization term stabilizes training but may slow convergence.

- Failure signatures:
  - Math-Critique accuracy < 0.7 on held-out set → unreliable feedback, risk of error propagation.
  - RFT step shows minimal improvement → rejection threshold too strict or dataset too narrow.
  - DPO step causes instability or overfitting → learning rate or regularization coefficient mis-tuned.
  - Final model performs well on academic datasets but poorly on MathUserEval → overfitting to academic style.

- First 3 experiments:
  1. Train Math-Critique on a small subset of academic math problems; evaluate accuracy on a held-out set.
  2. Run RFT on a small sample of problems with varying rejection thresholds; measure diversity and quality of retained responses.
  3. Conduct a small-scale DPO experiment with contrastive pairs; check stability and improvement in distinguishing correct/incorrect reasoning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ChatGLM-Math on visual reasoning tasks compare to other large language models that have been specifically fine-tuned on visual data?
- Basis in paper: [explicit] The paper states that ChatGLM-Math has deficiencies in handling questions requiring drawing and understanding images, as it is a purely linguistic model.
- Why unresolved: The paper does not provide a direct comparison of ChatGLM-Math's visual reasoning performance against models with visual capabilities. This comparison would require testing on datasets with visual components.
- What evidence would resolve it: A benchmark comparison of ChatGLM-Math's performance on visual math problems against multimodal models like GPT-4V or Claude-3 with similar visual tasks.

### Open Question 2
- Question: What is the optimal coefficient setting (λ and β) for the direct preference optimization (DPO) stage to maximize mathematical reasoning performance while maintaining language capabilities?
- Basis in paper: [explicit] The paper mentions that the optimal results under different coefficient settings for λ (0.5, 1, 1.5) and β (0.5, 1, 2) are reported in the experimental section, but the specific optimal values are not provided in the abstract or introduction.
- Why unresolved: The paper does not explicitly state which coefficient values yielded the best performance, leaving uncertainty about the optimal configuration.
- What evidence would resolve it: Detailed ablation studies showing performance metrics (e.g., accuracy on MATHUSER EVAL, GSM8k, MATH) for each combination of λ and β values, identifying the optimal settings.

### Open Question 3
- Question: How does the Math-Critique model's performance scale with model size, and is there a point of diminishing returns?
- Basis in paper: [inferred] The paper introduces Math-Critique as a general critic for math, and while it demonstrates effectiveness on a 32B parameter model, it does not explore how performance changes with different model sizes or if there's a threshold beyond which additional parameters provide minimal improvement.
- Why unresolved: The paper focuses on a single model size (32B) and does not investigate the relationship between model size and Math-Critique performance.
- What evidence would resolve it: A study comparing Math-Critique performance across multiple model sizes (e.g., 7B, 13B, 70B) on the same mathematical tasks, measuring both accuracy and computational efficiency to identify any diminishing returns.

## Limitations
- The self-critique pipeline depends critically on the accuracy of the Math-Critique model; if accuracy falls below ~0.7, the feedback loop may reinforce errors.
- The method shows degraded performance on visual reasoning and high-precision calculations, limiting applicability to geometry and numerical computation tasks.
- While performance improves on academic benchmarks and MathUserEval, there's no evidence of generalization to entirely novel problem types or domains not represented in training data.

## Confidence
- **High Confidence**: The mechanism by which self-critique improves mathematical reasoning is well-supported by experimental results and ablation studies. The claim that ChatGLM-Math outperforms models twice its size is directly verifiable from benchmark comparisons.
- **Medium Confidence**: The claim that language capabilities are maintained or improved is supported by AlignBench and MT-Bench scores, but these are proxy metrics and may not capture all aspects of language proficiency.
- **Low Confidence**: The assertion that the method achieves "state-of-the-art results among open-source models" lacks context about commercial models and may not generalize to all mathematical domains.

## Next Checks
1. Conduct an ablation study removing Math-Critique and comparing RFT+DPO with standard supervised fine-tuning to isolate the contribution of the self-critique component.
2. Test the model on held-out mathematical problem types not represented in any training data (e.g., advanced calculus, abstract algebra) to assess true generalization.
3. Implement cross-validation with human evaluators to verify Math-Critique's accuracy and identify failure modes in the self-critique pipeline.