---
ver: rpa2
title: Fairness, Accuracy, and Unreliable Data
arxiv_id: '2408.16040'
source_url: https://arxiv.org/abs/2408.16040
tags:
- positive
- fairness
- group
- equal
- opportunity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This thesis explores three areas to improve machine learning reliability:
  fairness, strategic classification, and adversarial robustness. Each domain has
  unique properties that complicate learning.'
---

# Fairness, Accuracy, and Unreliable Data

## Quick Facts
- arXiv ID: 2408.16040
- Source URL: https://arxiv.org/abs/2408.16040
- Authors: Kevin Stangl
- Reference count: 0
- One-line primary result: Theoretical analysis of fairness, strategic classification, and adversarial robustness showing how fairness constraints can recover Bayes optimal classifiers from biased data and improve robustness to malicious noise.

## Executive Summary
This thesis explores three areas to improve machine learning reliability: fairness, strategic classification, and adversarial robustness. Each domain has unique properties that complicate learning. The thesis develops mathematical models to expose failure modes and make trade-offs explicit, providing algorithmic advice to practitioners. Key findings include showing how fairness constraints can recover optimal classifiers from biased data, demonstrating the benefits of randomized classifiers for robustness, and developing algorithms for sequential screening processes that enforce fairness while handling strategic behavior.

## Method Summary
The thesis develops theoretical frameworks and algorithms across three main areas: fairness in learning from biased data, fair learning under malicious noise, and strategic behavior in screening processes. Methods include empirical risk minimization with fairness constraints, convex optimization for sequential screening, and boosting-based approaches for multi-group robustness. The work combines mathematical analysis with experimental validation on synthetic and semi-synthetic datasets, particularly using the ACS-Folktables dataset for real-world relevance.

## Key Results
- Equal Opportunity fairness constraints can recover the true Bayes optimal classifier from biased training data when base rates are equal across groups
- Randomized classifiers in fair-ERM with malicious noise reduce accuracy loss compared to proper learners, matching best possible without fairness constraints for Demographic Parity
- Sequential screening processes exhibit new behavior where agents can exploit sequential ordering to manipulate their way through multiple classifiers with lower total cost than simultaneous testing
- ERM oracles can be used to learn robust classifiers against patch attacks by reducing effective perturbations from exponential to polynomial

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Equal Opportunity fairness constraints recover the Bayes optimal classifier from biased training data when base rates are equal across groups.
- **Mechanism**: When training data is biased (e.g., under-representation of positive examples from Group B), Equal Opportunity forces the classifier to equalize true positive rates across groups. This pushes the classifier to classify more Group B examples as positive, counteracting the bias that makes ERM overly pessimistic on Group B.
- **Core assumption**: Base rates (fraction of positive examples) are equal across groups (p = PDA(h*_A(x) = 1) = PDB(h*_B(x) = 1)).
- **Evidence anchors**:
  - [abstract]: "Fairness constraints like Equal Opportunity can recover the true Bayes optimal classifier from biased training data, even when plain ERM fails."
  - [section]: "We prove that Equal Opportunity strongly recovers from Under-Representation Bias so long as (1 - r)(1 - 2η) + r((1 - η)β - η) > 0"
  - [corpus]: "The average neighbor FMR=0.471" - weak relevance to this specific mechanism
- **Break condition**: If base rates are not equal across groups, Equal Opportunity may not recover the Bayes optimal classifier.

### Mechanism 2
- **Claim**: Allowing randomized classifiers in fair-ERM with malicious noise reduces accuracy loss compared to proper learners, matching best possible without fairness constraints for Demographic Parity and improving upon prior pessimistic results.
- **Mechanism**: By expanding the hypothesis class to include randomized classifiers (hp,q), the learner can introduce controlled noise to "smooth out" the hypothesis class, making it more resilient against adversarial manipulation. This bypasses the way an adversary uses fairness constraints to amplify their power.
- **Core assumption**: The original hypothesis class H contains at least one classifier that satisfies the fairness constraint.
- **Evidence anchors**:
  - [abstract]: "Allowing randomized classifiers in fair-ERM with malicious noise reduces accuracy loss compared to proper learners, matching best possible without fairness constraints for Demographic Parity and improving upon prior pessimistic results."
  - [section]: "We bypass the impossibility results in Konstantinov and Lampert [2021] by allowing the learner to produce a randomized improper classifier."
  - [corpus]: "The average neighbor FMR=0.471" - weak relevance to this specific mechanism
- **Break condition**: If the adversary can corrupt data beyond the malicious noise model considered, the effectiveness of randomization may diminish.

### Mechanism 3
- **Claim**: Sequential screening processes allow agents to exploit ordering to manipulate their way through multiple classifiers with lower total cost than simultaneous testing.
- **Mechanism**: In sequential screening, once an agent passes a test, they can "forget" about it and manipulate for the next test. This allows zig-zag strategies where agents can pass individual tests without being in the intersection of all positive regions, reducing total manipulation cost compared to simultaneous testing.
- **Core assumption**: The cost function is convex in the feature space.
- **Evidence anchors**:
  - [abstract]: "Sequential screening pipelines exhibit new behavior where agents can exploit sequential ordering to manipulate their way through multiple classifiers with lower total cost than simultaneous testing."
  - [section]: "We show that sequential screening pipelines exhibit new and surprising behavior where individuals can exploit the sequential ordering of the tests to 'zig-zag' between classifiers without having to simultaneously satisfy all of them."
  - [corpus]: "The average neighbor FMR=0.471" - weak relevance to this specific mechanism
- **Break condition**: If classifiers are monotonic (i.e., passing one test implies passing subsequent tests with feature-wise increases), sequential advantage disappears.

## Foundational Learning

- **Concept**: Empirical Risk Minimization (ERM)
  - Why needed here: ERM is the baseline learning algorithm that is shown to fail in biased data scenarios and under malicious noise, motivating the need for fairness constraints and robust learning methods.
  - Quick check question: What is the goal of ERM and what assumption does it make about the training data?

- **Concept**: VC Dimension
  - Why needed here: VC dimension is used to bound the sample complexity and generalization error in the robust learning results, particularly when extending finite hypothesis class guarantees to infinite classes.
  - Quick check question: How does VC dimension relate to the sample complexity of learning a hypothesis class?

- **Concept**: Uniform Convergence
  - Why needed here: Uniform convergence is implicitly assumed in the analysis of ERM and its variants, ensuring that empirical risk approximates true risk across the hypothesis class.
  - Quick check question: What does uniform convergence guarantee about the relationship between training and test error?

## Architecture Onboarding

- **Component map**: Introduction -> Fairness and Biased Data -> Fair Learning and Malicious Noise -> Fairness and Multi-Stage Screening -> Sequential Strategic Screening -> Agnostic Multi-Robust Learning Using ERM

- **Critical path**: The critical path is the progression from understanding how fairness constraints interact with biased data (Sections 2-3), to applying these insights in screening problems (Sections 4-5), and finally extending to adversarial robustness (Section 6). Each section depends on the theoretical foundations established in earlier sections.

- **Design tradeoffs**: The main tradeoffs involve balancing fairness, accuracy, and robustness. For example, in Section 2, Equal Opportunity recovers the Bayes optimal classifier but may not be the most expressive fairness notion. In Section 3, allowing randomized classifiers improves robustness to malicious noise but adds complexity. In Section 4, maximizing precision subject to Equal Opportunity may reduce recall.

- **Failure signatures**: Key failure modes include: (1) ERM failing to recover the Bayes optimal classifier from biased data, (2) fairness-constrained learning being vulnerable to malicious noise, (3) screening processes being exploitable by strategic agents, and (4) robust learning failing in non-realizable cases. Each section provides theoretical conditions under which these failures occur.

- **First 3 experiments**:
  1. Verify Equal Opportunity recovery from under-representation bias by generating synthetic data with known Bayes optimal classifier and corrupting training data according to the bias model.
  2. Test the effectiveness of randomized classifiers in fair-ERM with malicious noise by comparing accuracy loss of proper vs. improper learners on synthetic data with injected noise.
  3. Demonstrate zig-zag manipulation in sequential screening by implementing the two-classifier, two-dimensional case from Section 5.2.4 and comparing manipulation costs of sequential vs. simultaneous testing.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Equal Opportunity constrained ERM recover the Bayes optimal classifier in more complex bias models beyond under-representation, labeling, and their combination?
- Basis in paper: [explicit] The thesis proves recovery for these three bias models, but notes that all bias models are agnostic to distance from the decision boundary.
- Why unresolved: The theoretical analysis relies on simplified bias models that do not account for spatial distribution of errors relative to the decision boundary.
- What evidence would resolve it: Empirical and theoretical analysis of Equal Opportunity recovery in bias models incorporating distance from the decision boundary, such as bias increasing with distance from the optimal decision surface.

### Open Question 2
- Question: How does the effectiveness of Equal Opportunity compare to other fairness constraints in the presence of malicious noise when allowing improper randomized classifiers?
- Basis in paper: [explicit] Chapter 3 shows Equal Opportunity incurs O(√α) accuracy loss with malicious noise, while Demographic Parity achieves O(α) loss. The paper notes this separation recommends aligning fairness interventions with specific bias concerns.
- Why unresolved: The paper provides bounds for several fairness constraints but does not offer a comprehensive comparison across all major fairness notions in the malicious noise setting.
- What evidence would resolve it: Systematic experimental and theoretical comparison of accuracy loss across different fairness constraints (Equalized Odds, Calibration, etc.) under malicious noise with improper randomized classifiers.

### Open Question 3
- Question: Can screening processes be made robust to strategic behavior while maintaining fairness guarantees, and what is the optimal trade-off between robustness and fairness?
- Basis in paper: [inferred] Chapter 5 introduces strategic screening and shows agents can exploit sequentiality to reduce manipulation costs. Chapter 4 discusses enforcing fairness in screening. The paper briefly mentions fairness implications but leaves substantial open directions.
- Why unresolved: The thesis introduces the problem of strategic screening but does not fully explore the interaction between strategic robustness and fairness constraints, nor characterize optimal trade-offs.
- What evidence would resolve it: Development and analysis of screening algorithms that simultaneously enforce fairness constraints and defend against strategic manipulation, with characterization of achievable fairness-robustness trade-offs.

## Limitations

- Real-world applicability may be limited by assumptions about data generation (e.g., equal base rates across groups) that may not hold in practice
- Computational feasibility concerns exist for algorithms developed for sequential screening and multi-group robustness when applied to high-dimensional data or large numbers of groups
- The work relies on specific fairness definitions that may not capture all aspects of fairness relevant to practitioners, and adversarial robustness results assume specific attack models

## Confidence

- **High Confidence**: The theoretical foundations for fairness constraints recovering Bayes optimal classifiers from biased data are well-established with rigorous proofs. The results on sequential screening behavior and zig-zag manipulation strategies are mathematically sound.
- **Medium Confidence**: The extensions to semi-synthetic data and the practical implications of the theoretical results have medium confidence due to the gap between controlled experiments and real-world deployment scenarios.
- **Low Confidence**: The scalability and computational efficiency of the proposed algorithms in large-scale, high-dimensional settings remain uncertain and would require extensive empirical validation.

## Next Checks

1. **Scalability Analysis**: Implement the sequential screening algorithm on a large-scale dataset (e.g., >1 million examples) to measure computational complexity and runtime. Compare with baseline methods to quantify the practical overhead.

2. **Robustness to Model Misspecification**: Test the fairness recovery results when base rates are not equal across groups, as assumed in the theory. Systematically vary the degree of base rate mismatch and measure the degradation in performance.

3. **Cross-Domain Generalization**: Apply the multi-group robustness algorithm to datasets from different domains (e.g., healthcare, finance, education) to assess whether the theoretical guarantees hold across diverse real-world scenarios with varying bias patterns and noise characteristics.