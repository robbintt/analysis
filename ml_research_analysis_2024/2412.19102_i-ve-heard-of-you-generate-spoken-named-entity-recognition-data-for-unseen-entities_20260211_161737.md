---
ver: rpa2
title: '"I''ve Heard of You!": Generate Spoken Named Entity Recognition Data for Unseen
  Entities'
arxiv_id: '2412.19102'
source_url: https://arxiv.org/abs/2412.19102
tags:
- data
- spoken
- entities
- entity
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of Spoken Named Entity Recognition
  (NER) for previously unseen entities. The authors propose HeardU, a method that
  generates Spoken NER data using a named entity dictionary (NED), a large language
  model (LLM), and a text-to-speech (TTS) system.
---

# "I've Heard of You!": Generate Spoken Named Entity Recognition Data for Unseen Entities

## Quick Facts
- arXiv ID: 2412.19102
- Source URL: https://arxiv.org/abs/2412.19102
- Reference count: 28
- Primary result: Proposed HeardU method achieves state-of-the-art Spoken NER performance for unseen entities with F1 score improvements of 2.76%-32.98% across multiple settings

## Executive Summary
This paper addresses the challenge of Spoken Named Entity Recognition (NER) for previously unseen entities in speech data. The authors propose HeardU, a method that generates Spoken NER data using a named entity dictionary (NED), a large language model (LLM), and a text-to-speech (TTS) system. To evaluate their approach, they release a new Spoken NER benchmark in Chinese (ST-CMDS-NER) with a corresponding human-refined NED containing 8,853 entities. Experiments show that HeardU achieves state-of-the-art performance in in-domain, zero-shot domain adaptation, and fully zero-shot settings, with F1 score increases ranging from 2.76% to 32.98% depending on the setting and language.

## Method Summary
The HeardU method addresses Spoken NER for unseen entities by synthesizing training data from a named entity dictionary. The approach uses an LLM to generate contextual sentences containing NED entities, then synthesizes speech using TTS. The synthetic data is filtered based on word error rate (WER) to remove noisy examples. Pre-trained ASR and NER models (wav2vec2.0 and DeBERTa) are then fine-tuned on this synthetic data. The method is evaluated across multiple settings: in-domain, zero-shot domain adaptation, and fully zero-shot, demonstrating consistent performance improvements across both English and Chinese benchmarks.

## Key Results
- English experiments using LLM-refined NED show F1 score improvements of 2.76%, 19.76%, and 18.33% in in-domain, zero-shot domain adaptation, and fully zero-shot settings respectively
- Chinese experiments using human-refined NED achieve F1 score improvements of 32.98% and 9.66% in zero-shot domain adaptation and fully zero-shot settings
- The proposed ST-CMDS-NER benchmark with 8,853 entities enables reproducible evaluation of Spoken NER systems

## Why This Works (Mechanism)
The method works by generating synthetic speech data that contains diverse contexts for previously unseen entities. By using an LLM to create natural-sounding sentences around entities from the NED, and then synthesizing this text into speech, the approach effectively expands the training data distribution to include entities that may not appear in the original dataset. The noise filtering step ensures that only high-quality synthetic examples are used for training, preventing the introduction of artifacts that could harm model performance.

## Foundational Learning
- Spoken NER task definition: Why needed - establishes the problem context; Quick check - verify understanding of entity recognition in speech vs. text
- Named Entity Dictionary (NED) construction: Why needed - foundation for data synthesis; Quick check - confirm NED contains sufficient entity diversity
- LLM-based text generation: Why needed - creates contextual sentences around entities; Quick check - verify generated sentences are grammatically correct and contextually appropriate
- TTS synthesis for speech generation: Why needed - converts text to realistic speech; Quick check - ensure synthesized speech maintains entity pronunciation clarity
- WER-based noise filtering: Why needed - removes low-quality synthetic examples; Quick check - confirm threshold effectively balances data quantity and quality
- ASR model fine-tuning: Why needed - adapts speech recognition to domain-specific entities; Quick check - measure WER improvement after fine-tuning
- NER model fine-tuning: Why needed - improves entity recognition for unseen entities; Quick check - verify F1 score improvements on target entities

## Architecture Onboarding

Component Map:
NED -> LLM Text Generation -> TTS Synthesis -> WER Filtering -> ASR Fine-tuning -> NER Fine-tuning

Critical Path:
NED preparation → LLM entity generation → TTS speech synthesis → WER filtering → Model fine-tuning → Evaluation

Design Tradeoffs:
- Quality vs. quantity of synthetic data: Higher WER thresholds produce fewer but more reliable examples
- Human vs. LLM NED refinement: Human annotation ensures quality but is expensive; LLM is scalable but may introduce errors
- Domain specificity vs. generalization: More domain-specific NEDs improve in-domain performance but may hurt zero-shot generalization

Failure Signatures:
- High WER after TTS synthesis indicates pronunciation or speech quality issues
- Low F1 improvements despite large amounts of synthetic data suggest LLM-generated contexts are not diverse enough
- Degradation in original entity recognition performance indicates overfitting to synthetic data distribution

First Experiments:
1. Test TTS quality by measuring WER on synthetic data before and after fine-tuning
2. Compare entity diversity in LLM-generated sentences vs. original training data
3. Evaluate the impact of WER threshold on both data quantity and downstream performance

## Open Questions the Paper Calls Out
None

## Limitations
- Specific LLM prompts and generation parameters are not provided, limiting exact reproducibility
- Different WER threshold values used across experiments without clear justification
- Limited exploration of the relationship between LLM refinement quality and downstream performance

## Confidence

Claims about method effectiveness: High - multiple experiments show consistent improvements
Claims about human vs. LLM NED quality: Medium - differences observed but not thoroughly analyzed
Claims about zero-shot generalization: Medium - limited number of domains tested

## Next Checks

1. Replicate the English experiment using the provided SLUE-VoxPopuli dataset to verify the 2.76%, 19.76%, and 18.33% F1 improvements
2. Test the noise filtering mechanism by varying WER thresholds and measuring impact on downstream performance
3. Compare model performance using human-refined vs. LLM-refined NEDs to quantify quality differences