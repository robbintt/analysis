---
ver: rpa2
title: Coordination Failure in Cooperative Offline MARL
arxiv_id: '2407.01343'
source_url: https://arxiv.org/abs/2407.01343
tags:
- learning
- policy
- offline
- dataset
- maddpg
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses coordination failure in offline multi-agent
  reinforcement learning (MARL), where agents learn from static datasets without the
  ability to test and correct miscoordination. The authors focus on the "Best Response
  Under Data" (BRUD) approach, which uses other agents' actions from the dataset during
  policy updates, but can lead to catastrophic coordination failure.
---

# Coordination Failure in Cooperative Offline MARL

## Quick Facts
- arXiv ID: 2407.01343
- Source URL: https://arxiv.org/abs/2407.01343
- Authors: Callum Rhys Tilbury; Claude Formanek; Louise Beyers; Jonathan P. Shock; Arnu Pretorius
- Reference count: 9
- This paper proposes PJAP to mitigate coordination failure in offline MARL by prioritizing dataset samples based on joint-action similarity

## Executive Summary
This paper addresses coordination failure in offline multi-agent reinforcement learning (MARL), where agents learn from static datasets without the ability to test and correct miscoordination. The authors focus on the "Best Response Under Data" (BRUD) approach, which uses other agents' actions from the dataset during policy updates, but can lead to catastrophic coordination failure. Using two-player polynomial games as an analytical tool, they demonstrate how BRUD can cause agents to move in opposite directions, worsening joint performance. They propose Proximal Joint Action Prioritisation (PJAP), a method that prioritizes samples from the dataset based on joint-action similarity to the current policy during learning. Experiments show PJAP successfully mitigates coordination failure in both polynomial games and the MAMuJoCo 2-Agent HalfCheetah environment, outperforming standard MADDPG+CQL.

## Method Summary
The authors propose Proximal Joint Action Prioritisation (PJAP) to address coordination failure in offline MARL. PJAP prioritizes samples from the dataset based on joint-action similarity to the current policy during learning. The method computes a distance metric between the current joint policy and the dataset-generating policy, then prioritizes samples with smaller distances. The approach is evaluated using two-player polynomial games (sign-agreement, coupled rewards, twin peaks) and the MAMuJoCo 2-Agent HalfCheetah environment, comparing MADDPG+CQL with and without PJAP.

## Key Results
- PJAP successfully mitigates coordination failure in polynomial games where standard BRUD fails
- PJAP improves performance on MAMuJoCo 2-Agent HalfCheetah, outperforming MADDPG+CQL baseline
- The effectiveness of PJAP increases with the degree of agent interaction in polynomial games

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BRUD (Best Response Under Data) causes coordination failure because agents optimize their own actions based on static data from other agents, ignoring the evolving joint policy during learning.
- Mechanism: In offline MARL, each agent's policy update depends on the average actions of other agents in the dataset, which may not reflect the current joint policy. This mismatch leads to updates that move the joint policy in suboptimal directions.
- Core assumption: The dataset contains actions from multiple policies, not just the optimal joint policy.
- Evidence anchors:
  - [abstract]: "coordination failure and investigate the role of joint actions in multi-agent policy gradients with offline data"
  - [section]: "the possibility of miscoordination as demonstrated here is present under any BRUD-style update—regardless of whether we are learning online from a dynamic buffer, or offline from a static dataset"
  - [corpus]: Weak evidence. No direct mention of BRUD or coordination failure in neighbor papers.

### Mechanism 2
- Claim: Increasing agent interaction in polynomial games leads to stricter dataset requirements for successful offline learning.
- Mechanism: As the degree of interaction between agents increases (e.g., from decoupled rewards to twin peaks), the dataset must contain actions that are increasingly similar to the optimal joint policy for learning to succeed.
- Core assumption: The dataset is static and cannot be modified during learning.
- Evidence anchors:
  - [section]: "as the degree of agent interaction increases, the requirements of the dataset become more stringent for learning to converge to the true optimum with BRUD"
  - [section]: "For learning to converge to the true optimum, we have a^*_x = a^†_x ⇐ ⇒ σ_y(ā_y) = ...", showing the strict requirement on dataset variance
  - [corpus]: Weak evidence. No direct mention of polynomial games or dataset requirements in neighbor papers.

### Mechanism 3
- Claim: PJAP (Proximal Joint Action Prioritisation) mitigates coordination failure by prioritizing samples from the dataset that are similar to the current joint policy.
- Mechanism: PJAP uses a distance metric to measure the similarity between the current joint policy and the actions in the dataset. Samples with smaller distances are prioritized, increasing the likelihood that the policy update is based on actions similar to the current joint policy.
- Core assumption: A suitable distance metric can be defined for the problem domain.
- Evidence anchors:
  - [abstract]: "by prioritising samples from the dataset based on joint-action similarity during policy learning"
  - [section]: "in PJAP, we set the priority, ρk+1, for each of the trajectories τ ∼ B, to be inversely proportional to some function of the distance between the current joint policy and the dataset-generating policy, d(μ(k), βτ)"
  - [corpus]: Weak evidence. No direct mention of PJAP or prioritized sampling in neighbor papers.

## Foundational Learning

- Concept: Multi-Agent Reinforcement Learning (MARL)
  - Why needed here: The paper focuses on coordination failure in offline MARL, so understanding the basics of MARL is essential.
  - Quick check question: What is the difference between single-agent RL and MARL?

- Concept: Policy Gradients
  - Why needed here: The paper uses policy gradients to update the agents' policies in the polynomial games and MAMuJoCo environment.
  - Quick check question: How does the policy gradient update differ between online and offline settings?

- Concept: Polynomial Games
  - Why needed here: The paper uses two-player polynomial games as an analytical tool to study coordination failure in offline MARL.
  - Quick check question: What is the difference between a polynomial game and a matrix game?

## Architecture Onboarding

- Component map:
  - Replay buffer or dataset (B) -> Joint policy (μ) -> Critic (Q) -> PJAP prioritization module -> Policy update module

- Critical path:
  1. Sample trajectories from the replay buffer or dataset.
  2. Compute the distance between the current joint policy and the actions in the sampled trajectories.
  3. Prioritize the sampled trajectories based on the computed distances.
  4. Update the joint policy using the prioritized sampled trajectories.

- Design tradeoffs:
  - PJAP introduces additional computational overhead due to the distance computations and prioritization.
  - The choice of distance metric can significantly impact the performance of PJAP.

- Failure signatures:
  - If the distance metric is poorly chosen, PJAP may not effectively mitigate coordination failure.
  - If the dataset is too small or contains actions from multiple, suboptimal joint policies, PJAP may struggle to find a good joint policy.

- First 3 experiments:
  1. Run MADDPG with and without PJAP on the sign-agreement game (R(ax, ay) = axay) to verify that PJAP mitigates coordination failure.
  2. Run MADDPG with and without PJAP on the twin peaks game (R = -A(ax^2 + ay^2) - B(axay)^2 + Caxay) to verify that PJAP works for games with higher degrees of agent interaction.
  3. Run MADDPG+CQL with and without PJAP on the 2-Agent HalfCheetah environment from MAMuJoCo to verify that PJAP improves performance in a more complex, real-world setting.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the most effective distance metrics for implementing PJAP in high-dimensional action spaces?
- Basis in paper: [inferred] The paper discusses using L1 norm for polynomial games and mentions this as a challenge for higher-dimensional settings, noting that "coming up with a good distance measure for a particular problem can be tricky, especially in higher-dimensional action spaces."
- Why unresolved: The paper only explores the L1 distance metric in the MAMuJoCo experiments, leaving open the question of whether other metrics (e.g., cosine similarity, learned distance functions) might perform better in complex environments.
- What evidence would resolve it: Comparative experiments across multiple distance metrics on various MARL benchmarks, showing which metric consistently yields the best performance across different environment types and dimensionalities.

### Open Question 2
- Question: How does the performance of PJAP scale with the number of agents in the system?
- Basis in paper: [inferred] The paper focuses on two-agent scenarios (polynomial games and 2-agent HalfCheetah) but notes that "coming up with a good distance measure for a particular problem can be tricky, especially in higher-dimensional action spaces," which becomes more complex with more agents.
- Why unresolved: The experiments are limited to two-agent systems, and the computational complexity of computing distances and priorities grows with the number of agents. The paper doesn't address whether PJAP maintains its effectiveness in larger multi-agent systems.
- What evidence would resolve it: Systematic scaling experiments showing PJAP performance on MARL benchmarks with varying numbers of agents (3, 5, 10+ agents), measuring both absolute performance and relative improvement over baseline methods.

### Open Question 3
- Question: Can PJAP be effectively combined with other offline MARL techniques like policy regularization and value decomposition methods?
- Basis in paper: [explicit] The authors state that "prioritised dataset sampling is a promising area for innovation in offline MARL that can be combined with other effective approaches such as critic and policy regularisation" and see their work as "more exploratory in nature."
- Why unresolved: While the paper demonstrates PJAP's effectiveness on its own, it doesn't explore combinations with other state-of-the-art offline MARL techniques. The authors acknowledge this as an open area for investigation.
- What evidence would resolve it: Experiments combining PJAP with various offline MARL techniques (e.g., value decomposition methods like VDA, policy regularization approaches, or conservative Q-learning variants) on standard benchmarks, showing whether these combinations yield synergistic improvements.

## Limitations

- Theoretical analysis relies heavily on polynomial games, which may not fully capture real-world complexity
- Effectiveness of PJAP depends on having a suitable distance metric for joint-action similarity
- Computational overhead introduced by prioritization is not quantified

## Confidence

**High Confidence:** The coordination failure mechanism under BRUD is well-established through both theoretical analysis (polynomial games) and empirical validation (MAMuCo 2-Agent HalfCheetah). The proposed solution (PJAP) shows consistent improvements across different experimental setups.

**Medium Confidence:** The theoretical claims about dataset requirements becoming stricter with increased agent interaction are supported by polynomial game analysis but need further validation in more complex environments. The effectiveness of different distance metrics for PJAP prioritization across diverse domains requires additional investigation.

**Low Confidence:** The scalability of PJAP to environments with many agents (>2) and its performance with heterogeneous agent types remain unexplored. The computational overhead introduced by prioritization in large-scale applications is not quantified.

## Next Checks

1. **Scalability Test:** Evaluate PJAP performance on environments with 4+ agents to assess scalability beyond the 2-agent setting. Measure both sample efficiency and computational overhead as agent count increases.

2. **Metric Ablation Study:** Systematically compare different distance metrics (L1, L2, cosine similarity, learned metrics) for PJAP prioritization across multiple domains to identify which metrics generalize best.

3. **Dataset Quality Analysis:** Vary the quality distribution in offline datasets (e.g., different ratios of optimal vs suboptimal trajectories) to quantify how dataset composition affects PJAP's effectiveness compared to baseline methods.