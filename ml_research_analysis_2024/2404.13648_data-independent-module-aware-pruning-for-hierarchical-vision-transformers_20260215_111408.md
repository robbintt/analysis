---
ver: rpa2
title: Data-independent Module-aware Pruning for Hierarchical Vision Transformers
arxiv_id: '2404.13648'
source_url: https://arxiv.org/abs/2404.13648
tags:
- pruning
- vision
- transformer
- layers
- weight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a Data-independent Module-Aware Pruning (DIMAP)
  method to compress hierarchical vision transformers (ViTs). DIMAP addresses two
  key challenges in pruning hierarchical ViTs: 1) Comparing attention weights across
  windows and layers in a fair manner, and 2) Considering the unique weight distributions
  at different hierarchical levels for extracting coarse-to-fine features.'
---

# Data-independent Module-aware Pruning for Hierarchical Vision Transformers

## Quick Facts
- arXiv ID: 2404.13648
- Source URL: https://arxiv.org/abs/2404.13648
- Authors: Yang He; Joey Tianyi Zhou
- Reference count: 29
- Primary result: DIMAP achieves only 0.07% top-5 accuracy drop when removing 52.5% FLOPs and 52.7% parameters of Swin-B.

## Executive Summary
This paper proposes Data-independent Module-Aware Pruning (DIMAP), a method to compress hierarchical vision transformers (ViTs) by pruning weights in a data-independent, module-aware manner. DIMAP addresses key challenges in pruning hierarchical ViTs: comparing attention weights across windows and layers fairly, and considering unique weight distributions at different hierarchical levels. The method evaluates weight importance by analyzing information distortion using Frobenius norm, treating different layers within a module as a whole.

## Method Summary
DIMAP is a data-independent module-aware pruning method that uses Frobenius norm to evaluate weight importance for hierarchical vision transformers. It treats different layers within a module as a whole and computes a novel weight importance metric that is solely based on weights, eliminating dependence on input images and the patch merging process. The method generalizes layer-wise magnitude pruning by providing a global ranking at the module level without altering the local ranking within a layer.

## Key Results
- DIMAP achieves only 0.07% top-5 accuracy drop when removing 52.5% FLOPs and 52.7% parameters of Swin-B.
- When reducing 33.2% FLOPs and 33.2% parameters of Swin-S, DIMAP achieves 0.8% higher relative top-5 accuracy than the original model.
- DIMAP demonstrates effective compression of hierarchical ViTs while maintaining high accuracy, outperforming traditional magnitude pruning approaches.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DIMAP compares attention weights across windows and layers fairly by treating them as a module and evaluating weight importance through information distortion using Frobenius norm.
- Mechanism: Instead of comparing attention weights directly by magnitude (which can be misleading due to different local ranges), DIMAP aggregates all layers within a module and computes their joint importance based on how much information distortion would occur if certain weights were pruned. This allows weights from different windows/layers to be compared on the same scale.
- Core assumption: The Frobenius norm of the difference between original and pruned weights is a valid proxy for the contribution of those weights to the overall output.
- Evidence anchors:
  - [abstract]: "treats different layers within a module as a whole and evaluates weight importance by analyzing their information distortion using Frobenius norm"
  - [section 3.2]: Derives the importance metric Imp(Wpj) := (Wpj)^2 / (Σ_{i≤pj}(Wi)^2) based on Frobenius distortion minimization.
  - [corpus]: No direct evidence; corpus papers focus on different pruning strategies (e.g., regularization, quantization, activation-aware compression) but none use this module-aware Frobenius distortion approach.

### Mechanism 2
- Claim: DIMAP introduces a novel data-independent weight importance metric that eliminates dependence on input images and the patch merging process.
- Mechanism: The importance metric is computed purely from the weight values themselves, without requiring input data or gradient information. This avoids the need to adapt the metric to varying image patch sizes across hierarchical levels.
- Core assumption: A purely weight-based metric can capture the true importance of weights for model performance without input-dependent signals.
- Evidence anchors:
  - [abstract]: "introduces a novel weight metric that is solely based on weights and does not require input images, thereby eliminating the dependence on the patch merging process"
  - [section 3.2]: Derives the importance formula Imp(Wpj) that uses only weight values, making it data-independent.
  - [corpus]: Weak evidence; corpus papers like TAP-ViTs and PQV-Mobile focus on task-adaptive or quantization-based pruning, not data-independent metrics.

### Mechanism 3
- Claim: DIMAP generalizes layer-wise magnitude pruning by providing a "global" ranking at the module level without altering the "local" ranking within a layer.
- Mechanism: The proposed importance metric Imp(Wpj) is monotonic with the original magnitude ranking within a layer, but adjusts the relative importance across layers/modules to account for their different scales and roles.
- Core assumption: The local magnitude ranking within a layer is a reasonable proxy for importance, and the module-level adjustment can correct for scale differences without breaking this local ordering.
- Evidence anchors:
  - [abstract]: "provides a 'global' ranking of the importance of weights at the module level without altering the 'local' ranking within a layer"
  - [section 3.2]: The formula Imp(Wpj) is derived as a monotonic transformation of squared weights, preserving local order while enabling global comparison.
  - [corpus]: No direct evidence; corpus papers do not discuss this property of preserving local order while enabling global ranking.

## Foundational Learning

- Concept: Frobenius norm as a measure of matrix distortion
  - Why needed here: DIMAP uses Frobenius norm to quantify how much pruning a set of weights distorts the output. Understanding this norm is essential to grasp the importance metric.
  - Quick check question: What is the Frobenius norm of a matrix, and how does it relate to the ℓ2 norm of the difference between two matrices?

- Concept: Vision Transformer architecture (Swin Transformer)
  - Why needed here: DIMAP is designed specifically for hierarchical ViTs like Swin Transformer. Knowing the structure (patch embedding, attention layers, MLP, patch merging) is crucial to understand the module definitions and pruning targets.
  - Quick check question: How does the Swin Transformer's shifted window attention work, and why is it more efficient than global self-attention?

- Concept: Weight pruning in neural networks
  - Why needed here: DIMAP is a pruning method, so understanding standard pruning techniques (magnitude pruning, structured pruning) and their limitations is important to appreciate the novelty of DIMAP.
  - Quick check question: What are the main drawbacks of magnitude-based pruning, and how does structured pruning address some of them?

## Architecture Onboarding

- Component map:
  - Input: Images resized to 224x224
  - Backbone: Swin Transformer with four stages (Patch Embedding → Swin Blocks → Patch Merging)
  - Swin Blocks: Contain LN → ATT-QKV → ATT-PRJ → LN → MLP-FC1 → MLP-FC2, with residual connections
  - Modules: QKV-M (ATT-QKV across blocks), PRJ-M (ATT-PRJ across blocks), MLP-M (MLP-FC1+FC2 across blocks), AUX-M (auxiliary layers, not pruned)
  - Output: Classification head (fully connected layer)

- Critical path:
  - Forward pass: Input → Patch Embedding → Stage 1 → Patch Merging → Stage 2 → ... → Classification Head
  - Pruning: Compute module-level importance → Sort weights by importance → Prune lowest-importance weights → Fine-tune

- Design tradeoffs:
  - Module granularity: Finer modules (e.g., per-layer) might capture more nuanced importance but increase computational overhead; coarser modules (e.g., per-block) are faster but might miss important distinctions.
  - Data independence: Weight-only metric is simple and fast but might miss data-dependent importance; data-dependent metrics could be more accurate but require more computation.
  - Module definition: The choice of which layers to group into modules (e.g., QKV-M, PRJ-M, MLP-M) affects the pruning results and must be justified.

- Failure signatures:
  - Accuracy drop larger than expected for a given pruning ratio: Could indicate incorrect importance ranking or over-aggressive pruning.
  - Inconsistent results across different model sizes (Swin-T/S/B): Might suggest the module definitions or importance metric are not robust.
  - Slow convergence during fine-tuning: Could mean the pruned model has lost important features and needs more training.

- First 3 experiments:
  1. Prune a small percentage (e.g., 10%) of weights from Swin-T using DIMAP and compare accuracy to magnitude pruning.
  2. Prune a large percentage (e.g., 50%) of weights from Swin-B using DIMAP and measure accuracy drop and FLOPs reduction.
  3. Compare DIMAP's data-independent metric to a data-dependent metric (e.g., gradient-based) on a small dataset to validate the data-independent approach.

## Open Questions the Paper Calls Out
- Question: How does the proposed DIMAP method perform on downstream tasks such as object detection and segmentation?
  - Basis in paper: [inferred] The authors mention in the conclusion that they are interested in exploring the performance of DIMAP on downstream tasks.
  - Why unresolved: The paper only validates DIMAP on ImageNet-1k classification and does not provide results on other tasks.
  - What evidence would resolve it: Experiments evaluating DIMAP on tasks like object detection and segmentation.

- Question: How does DIMAP perform on other vision transformer variants beyond Swin Transformers?
  - Basis in paper: [explicit] The authors state in the conclusion that they are interested in exploring the performance of DIMAP on other ViT variants.
  - Why unresolved: The paper only validates DIMAP on Swin Transformers of different sizes and does not provide results on other architectures.
  - What evidence would resolve it: Experiments evaluating DIMAP on other vision transformer variants like DeiT, PVT, etc.

- Question: Can DIMAP be combined with other compression techniques to further improve efficiency?
  - Basis in paper: [inferred] The authors mention that DIMAP focuses on weight pruning and could potentially be combined with approaches that improve the attention process.
  - Why unresolved: The paper does not explore combining DIMAP with other compression methods.
  - What evidence would resolve it: Experiments applying DIMAP in conjunction with other techniques like attention improvement or quantization.

## Limitations
- The exact mechanism for how Frobenius norm captures true weight importance remains somewhat abstract.
- The module definitions (QKV-M, PRJ-M, MLP-M, AUX-M) are not fully specified across different model sizes, which could affect reproducibility.
- The comparison to other pruning methods is limited to magnitude pruning, with no direct comparison to other structured pruning approaches.

## Confidence
- **High confidence**: The core mechanism of using Frobenius norm for module-aware importance ranking is well-supported by the derivation and experimental results.
- **Medium confidence**: The data-independent nature of the metric is demonstrated, but the comparison to data-dependent metrics is limited.
- **Medium confidence**: The module definitions and their impact on pruning performance are explained, but the exact implementation details across model sizes are not fully specified.

## Next Checks
1. Implement a data-dependent importance metric (e.g., gradient-based) and compare its performance to DIMAP's data-independent metric on a small dataset.
2. Test DIMAP with different module definitions (e.g., per-block vs. per-module) to understand the impact on pruning performance.
3. Validate the pruning results on a dataset other than ImageNet-1k (e.g., CIFAR-100) to assess the generalizability of DIMAP.