---
ver: rpa2
title: 'MediFact at MEDIQA-M3G 2024: Medical Question Answering in Dermatology with
  Multimodal Learning'
arxiv_id: '2405.01583'
source_url: https://arxiv.org/abs/2405.01583
tags:
- response
- image
- medical
- generation
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The MediFact-M3G framework addresses the challenge of multilingual
  and multimodal medical question-answering in dermatology by combining weakly supervised
  learning with multimodal fusion. It uses a VGG16-CNN-SVM model to extract informative
  skin condition representations from images, then bridges visual and textual information
  through multimodal fusion using pre-trained QA models.
---

# MediFact at MEDIQA-M3G 2024: Medical Question Answering in Dermatology with Multimodal Learning

## Quick Facts
- arXiv ID: 2405.01583
- Source URL: https://arxiv.org/abs/2405.01583
- Authors: Nadia Saeed
- Reference count: 12
- Key outcome: 7th rank in English response generation, 3rd rank in Chinese and Spanish response generation out of 75 participants

## Executive Summary
This paper presents MediFact-M3G, a framework for multilingual and multimodal medical question-answering in dermatology. The system combines weakly supervised learning with multimodal fusion to address the challenge of generating accurate medical responses from both textual queries and dermatological images. Using a VGG16-CNN-SVM model for image feature extraction and pre-trained QA models for text analysis, the framework achieves competitive performance across English, Chinese, and Spanish language settings in the MEDIQA-M3G 2024 challenge.

## Method Summary
The MediFact-M3G framework addresses medical question-answering by first extracting visual features from dermatological images using a VGG16-CNN-SVM pipeline in a weakly supervised manner. It then bridges visual and textual information through multimodal fusion using pre-trained QA models. The system generates comprehensive answers by feeding the ViT-CLIP model with multiple responses alongside images, quantifying semantic similarity through cosine distance to select the most relevant response. The approach handles multiple languages by translating non-English responses using Google Translate before CLIP-based selection.

## Key Results
- Achieved 7th rank in English language response generation
- Achieved 3rd rank in Chinese and Spanish language response generation out of 75 participants
- DeltaBLEU scores ranging from 0.565 to 0.744 across different language settings
- BERTScore ranging from 0.702 to 0.845 across different language settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weakly supervised learning enables the system to extract meaningful visual features from dermatological images even when labeled data is limited.
- Mechanism: The VGG16 CNN extracts high-level image features, which are then mapped to textual responses via an SVM classifier. This mapping learns the relationship between images and relevant descriptions, allowing unlabeled images to benefit from the textual information during training.
- Core assumption: Visual features extracted by pre-trained CNN can be effectively aligned with textual descriptions through a classifier even without explicit image labels.
- Evidence anchors:
  - [abstract] "Our system leverages readily available MEDIQA-M3G images via a VGG16-CNN-SVM model, enabling multilingual (English, Chinese, Spanish) learning of informative skin condition representations."
  - [section 2.2] "We employed a weakly supervised learning approach that leverages the available data effectively... The SVM essentially learns to map images to their most relevant textual descriptions."
  - [corpus] No direct corpus evidence; the claim is supported primarily by internal methodology.
- Break condition: If the pre-trained CNN features are not sufficiently discriminative for dermatological conditions, or if the textual descriptions are too sparse or inconsistent to provide meaningful supervision.

### Mechanism 2
- Claim: Multimodal fusion bridges the gap between visual and textual information to generate more accurate and comprehensive medical responses.
- Mechanism: The system combines user queries, relevant textual content, and image representations learned through weakly supervised learning into a comprehensive feature vector. This vector is used by pre-trained QA models (both extractive and abstractive) to generate responses that integrate both visual and textual context.
- Core assumption: Combining visual features with textual context provides richer information for QA models than either modality alone.
- Evidence anchors:
  - [abstract] "Using pre-trained QA models, we further bridge the gap between visual and textual information through multimodal fusion."
  - [section 2.3] "A comprehensive feature vector for each query-response pair is created by combining... The user's query itself... Relevant textual content... The image representation learned from the weakly supervised approach."
  - [corpus] Weak evidence; no direct corpus support for multimodal fusion effectiveness in dermatology QA.
- Break condition: If the image representations are not semantically aligned with the textual content, or if the QA models cannot effectively integrate multimodal inputs.

### Mechanism 3
- Claim: Contrastive learning with CLIP quantifies the semantic similarity between images and potential responses, enabling effective response selection across multiple languages.
- Mechanism: CLIP uses a Vision Transformer to extract image embeddings and a text encoder for responses. Cosine similarity between image and response embeddings determines the most relevant response. For non-English languages, the English response is translated using Google Translate.
- Core assumption: CLIP's contrastive learning effectively captures the semantic relationship between dermatological images and their descriptions across languages.
- Evidence anchors:
  - [abstract] "We empower the generation of comprehensive answers by feeding the ViT-CLIP model with multiple responses alongside images."
  - [section 2.4] "CLIP receives the ViT-extracted image embedding and multiple response lists... It calculates the cosine similarity between each response embedding and the image embedding."
  - [corpus] No direct corpus evidence; the mechanism is described in the methodology but not validated externally.
- Break condition: If the CLIP model's embeddings do not capture domain-specific nuances in medical terminology, or if translation inaccuracies significantly degrade response quality.

## Foundational Learning

- Concept: Weakly supervised learning
  - Why needed here: Labeled dermatological image data is scarce due to ethical considerations and data access limitations. Weakly supervised learning allows the system to leverage available text-image pairs without requiring explicit labels for every image.
  - Quick check question: Can a classifier trained on image features and text descriptions effectively generalize to unlabeled images in the same domain?

- Concept: Multimodal fusion
  - Why needed here: Dermatological diagnosis relies on both visual examination and textual patient information. Combining these modalities provides a more holistic understanding of the patient's condition than either modality alone.
  - Quick check question: Does the combined feature vector from text and image modalities improve QA model performance compared to using either modality individually?

- Concept: Contrastive learning
  - Why needed here: Selecting the most relevant response from multiple candidates requires quantifying the semantic similarity between images and text. Contrastive learning learns this relationship by comparing positive and negative pairs.
  - Quick check question: Does CLIP's contrastive learning framework effectively capture the semantic relationship between dermatological images and their textual descriptions?

## Architecture Onboarding

- Component map: Data preprocessing -> Weakly supervised image representation (VGG16-CNN-SVM) -> Multimodal feature fusion -> Response generation (extractive + abstractive QA models) -> Response selection (CLIP with ViT) -> Translation (Google Translate for non-English)
- Critical path: Image representation extraction -> Multimodal feature fusion -> Response generation -> Response selection
- Design tradeoffs:
  - Using weakly supervised learning vs. fully supervised: Sacrifices some precision for broader applicability when labeled data is limited.
  - Combining extractive and abstractive QA models: Balances factual accuracy with comprehensive response generation but increases computational complexity.
  - Using CLIP for response selection: Leverages strong visual-text alignment but may struggle with domain-specific medical terminology.
- Failure signatures:
  - Poor image feature extraction: Responses lack visual grounding and may be irrelevant to the dermatological condition.
  - Misalignment in multimodal fusion: Generated responses may contain inconsistencies between visual and textual information.
  - CLIP embedding mismatch: Selected responses may have low semantic similarity to the actual image content.
- First 3 experiments:
  1. Validate weakly supervised image representation: Train VGG16-CNN-SVM on a subset of labeled data, then test on unlabeled images to ensure the SVM generalizes the image-text mapping.
  2. Test multimodal fusion effectiveness: Compare QA model performance using only text, only images, and the combined multimodal feature vector on a validation set.
  3. Evaluate CLIP response selection: Measure the semantic similarity (e.g., using human evaluation or automated metrics) between CLIP-selected responses and ground truth across different languages.

## Open Questions the Paper Calls Out

- How effective is contrastive learning in quantifying uncertainty for ambiguous medical queries with limited content information? The paper explicitly states this as a key question to address but doesn't thoroughly evaluate its effectiveness in quantifying uncertainty for ambiguous queries with limited information. Experiments comparing uncertainty quantification performance across different levels of query ambiguity and content richness would resolve this.

- Can the Medifact-M3G model trained solely on the MEDIQA-M3G training dataset adequately capture similarities and relatedness for unseen samples? The paper only reports performance on the test set within the MEDIQA-M3G dataset and doesn't evaluate the model's ability to generalize to completely new, unseen dermatological datasets. Testing the model on external dermatological datasets would resolve this.

- Which CNN architecture (VGG16, Inception, or ResNet) is most effective for extracting image features in this medical question-answering task? While the paper mentions comparing different CNN architectures, it doesn't provide a detailed analysis of which architecture performs best or under what conditions. A comprehensive comparison across different evaluation metrics and dataset sizes would resolve this.

## Limitations
- The effectiveness of weakly supervised learning for dermatological image representation remains largely untested with no ablation studies isolating this component's contribution
- Cross-lingual performance relies heavily on Google Translate, but translation quality for specialized medical terminology is not evaluated
- The paper does not report computational requirements or inference latency, which are critical for clinical deployment

## Confidence
- **Medium**: Multimodal learning effectiveness claims, as the methodology is described but not rigorously validated with controlled experiments
- **Medium**: Cross-lingual performance rankings, as translation quality and its impact on downstream metrics are not addressed
- **Low**: Weakly supervised learning benefits, as no comparison is made against fully supervised baselines or alternative feature extraction methods

## Next Checks
1. Conduct ablation studies comparing performance with and without weakly supervised image representation to quantify its contribution
2. Evaluate translation quality of medical terminology using specialized translation evaluation metrics or human expert review
3. Perform cross-modal consistency checks by measuring semantic alignment between generated responses and corresponding dermatological images using human annotators