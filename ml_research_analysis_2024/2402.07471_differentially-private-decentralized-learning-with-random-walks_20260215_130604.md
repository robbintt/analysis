---
ver: rpa2
title: Differentially Private Decentralized Learning with Random Walks
arxiv_id: '2402.07471'
source_url: https://arxiv.org/abs/2402.07471
tags: []
core_contribution: This work analyzes the privacy guarantees of decentralized learning
  with random walk algorithms, where a model is updated by traveling from one node
  to another along the edges of a communication graph. The authors introduce a private
  version of decentralized stochastic gradient descent based on random walks on arbitrary
  graphs and establish its convergence rate for strongly convex loss functions.
---

# Differentially Private Decentralized Learning with Random Walks

## Quick Facts
- **arXiv ID**: 2402.07471
- **Source URL**: https://arxiv.org/abs/2402.07471
- **Reference count**: 40
- **Primary result**: Analysis of privacy guarantees for decentralized learning with random walks, showing better privacy for nearby nodes compared to gossip algorithms

## Executive Summary
This paper analyzes the privacy guarantees of decentralized learning algorithms based on random walks, where a model token travels from node to node along the edges of a communication graph. The authors introduce a private variant of decentralized stochastic gradient descent that adds Gaussian noise to model updates and establish convergence rates for strongly convex loss functions. They derive closed-form expressions for the privacy loss between each pair of nodes, showing that random walk algorithms provide better privacy guarantees for nodes that are close to each other compared to gossip algorithms. The analysis uses Pairwise Network Differential Privacy to capture the effect of graph topology on privacy guarantees through graph-theoretic quantities like communicability.

## Method Summary
The paper proposes a private random walk gradient descent algorithm where each node updates the model with a local SGD step, adds Gaussian noise, and forwards the model to a random neighbor according to a transition matrix W. The algorithm runs for T iterations, with each node contributing to the model update only when it holds the token. The privacy analysis uses Pairwise Network Differential Privacy to derive closed-form expressions for the privacy loss between each pair of nodes, capturing the impact of graph topology through communicability metrics. The method is evaluated on synthetic and real datasets for logistic regression, with experiments showing the privacy-utility tradeoff for different graph structures and noise levels.

## Key Results
- Random walk algorithms provide better privacy guarantees for nearby nodes compared to gossip algorithms
- Privacy loss between nodes is proportional to their communicability in the graph
- Achieves O(1/n²) privacy amplification compared to local DP, matching central DP with n users
- Closed-form expressions derived for privacy loss that capture graph topology effects

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Privacy amplification arises from the fact that each node only observes model updates from its direct neighbors, not from the entire network.
- **Mechanism:** In random walk algorithms, the model token visits nodes sequentially. At each step, only the current node and its direct neighbors see the model update. This limited view reduces the information available to any single node about any other node's private data.
- **Core assumption:** The graph topology is known and the random walk follows the transition matrix W.
- **Evidence anchors:**
  - [abstract] "using a recent variant of differential privacy tailored to the study of decentralized algorithms, namely Pairwise Network Differential Privacy, we derive closed-form expressions for the privacy loss between each pair of nodes where the impact of the communication topology is captured by graph theoretic quantities."
  - [section 3.2] "We consider the graph G and the transition matrix W to be known by all nodes."
  - [corpus] Weak evidence - no direct mention of neighbor-limited view, but aligns with the decentralized learning theme.
- **Break condition:** If the graph becomes fully connected or if nodes can collude to reconstruct the entire token trajectory.

### Mechanism 2
- **Claim:** The privacy loss between two nodes decreases with their graph distance, following communicability metrics.
- **Mechanism:** The privacy loss formula includes a term proportional to the communicability between nodes, which measures how well connected they are. Nodes that are closer in the graph have higher communicability and thus higher privacy loss.
- **Core assumption:** The transition matrix is symmetric and bi-stochastic (Assumption 2).
- **Evidence anchors:**
  - [section 5.1] "This quantity can be seen as a variant of known graph centrality metrics, and more precisely communicability."
  - [section 5.1] "Having the second term of the privacy loss proportional to the communicability of the nodes shows that our formula matches the intuition that nodes leak more information to closer nodes than to more distant ones."
  - [corpus] Weak evidence - no direct mention of communicability, but aligns with graph-based privacy analysis.
- **Break condition:** If the graph has poor connectivity or if the random walk mixing time is very long.

### Mechanism 3
- **Claim:** The privacy loss is amplified by the factor O(1/n²) compared to local DP, where n is the number of nodes.
- **Mechanism:** The random walk involves all nodes in the network, and the privacy loss is averaged over all possible contributions. This averaging provides a baseline privacy amplification factor similar to what would be obtained in central DP with n users.
- **Core assumption:** The transition matrix is bi-stochastic, ensuring uniform contribution probability.
- **Evidence anchors:**
  - [section 5.1] "we have an O(1/n²) privacy amplification factor compared to local DP, matching what would be obtained in central DP with n users."
  - [section 5] "The first term is the same as in Cyffers & Bellet (2022) for the complete graph: we have an O(1/n²) privacy amplification factor compared to local DP."
  - [corpus] Weak evidence - no direct mention of O(1/n²) amplification, but aligns with differential privacy theory.
- **Break condition:** If the transition matrix is not bi-stochastic or if the number of nodes is very small.

## Foundational Learning

- **Concept: Differential Privacy (DP)**
  - Why needed here: DP provides the theoretical foundation for quantifying privacy loss in the algorithm.
  - Quick check question: What is the difference between local DP and central DP in terms of trust model and privacy guarantees?

- **Concept: Pairwise Network Differential Privacy (PNDP)**
  - Why needed here: PNDP is tailored to decentralized algorithms and allows reasoning about privacy loss between specific pairs of nodes based on their local views.
  - Quick check question: How does PNDP differ from standard DP in terms of the threat model and the adjacency relation?

- **Concept: Random Walks on Graphs**
  - Why needed here: The algorithm uses random walks to propagate the model updates, and the privacy guarantees depend on the graph structure and the random walk properties.
  - Quick check question: What are the key properties of a random walk that ensure convergence and good privacy guarantees?

## Architecture Onboarding

- **Component map:**
  - Transition matrix W -> Token passing -> Local SGD update -> Gaussian noise addition -> Forward to neighbor

- **Critical path:**
  1. Initialize the model and the token at a starting node.
  2. At each step, the current node updates the model with a local SGD step and adds Gaussian noise.
  3. The node passes the token to a random neighbor according to the transition matrix W.
  4. Repeat steps 2-3 until convergence or a maximum number of steps is reached.

- **Design tradeoffs:**
  - Privacy vs. utility: Increasing the noise level improves privacy but reduces the accuracy of the learned model.
  - Communication cost vs. convergence speed: More frequent communication can speed up convergence but increases the communication overhead.
  - Graph structure vs. privacy guarantees: Some graph structures (e.g., expanders) provide better privacy guarantees than others.

- **Failure signatures:**
  - Poor convergence: If the random walk mixing time is too long or if the noise level is too high, the algorithm may not converge to a good solution.
  - Privacy leakage: If the graph structure is known to an attacker or if nodes can collude, the privacy guarantees may be compromised.

- **First 3 experiments:**
  1. Implement the random walk algorithm on a simple graph (e.g., a ring or a star) and verify the convergence and privacy guarantees.
  2. Compare the privacy-utility tradeoff of the random walk algorithm to that of a gossip algorithm on the same graph.
  3. Study the impact of graph structure on the privacy guarantees by testing the algorithm on different types of graphs (e.g., expanders, grids, random geometric graphs).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the presence of collusion affect the privacy guarantees in random walk algorithms compared to gossip algorithms?
- **Basis in paper:** [explicit] The paper mentions that collusion can be seen as a modification of the graph and provides a formula for the privacy loss under collusion (Equation 18).
- **Why unresolved:** The paper does not provide a detailed comparison of privacy guarantees under collusion for random walk and gossip algorithms, nor does it discuss the fundamental limitations of amplification by decentralization in the presence of collusion.
- **What evidence would resolve it:** A detailed theoretical and experimental comparison of privacy guarantees for both algorithms under various collusion scenarios would provide insights into the relative robustness of each approach.

### Open Question 2
- **Question:** What is the impact of data heterogeneity on the convergence and privacy guarantees of random walk algorithms?
- **Basis in paper:** [inferred] The paper mentions that random walks should mix heterogeneous data easily, but does not provide mathematical guarantees on heterogeneity or its impact on convergence and privacy.
- **Why unresolved:** The paper only provides a numerical experiment illustrating the impact of data heterogeneity on convergence, but does not offer a theoretical analysis of how heterogeneity affects privacy guarantees.
- **What evidence would resolve it:** A theoretical analysis of how data heterogeneity affects the convergence rate and privacy guarantees of random walk algorithms, supported by extensive experiments on synthetic and real-world datasets with varying levels of heterogeneity.

### Open Question 3
- **Question:** How do different graph topologies and their spectral properties influence the privacy-utility trade-off in random walk algorithms?
- **Basis in paper:** [explicit] The paper discusses the impact of graph topology on privacy guarantees through the lens of communicability and provides closed-form expressions for specific graphs like star graphs and ring graphs.
- **Why unresolved:** While the paper provides insights into the influence of graph topology on privacy guarantees, it does not offer a comprehensive analysis of how different spectral properties of graphs (e.g., spectral gap, diameter) affect the privacy-utility trade-off.
- **What evidence would resolve it:** A thorough theoretical and experimental study of how various graph spectral properties impact the privacy-utility trade-off for random walk algorithms, potentially leading to guidelines for choosing graph topologies that optimize this trade-off for specific applications.

## Limitations

- The privacy analysis relies on strong assumptions about the graph topology and transition matrix being known and bi-stochastic.
- The analysis focuses on strongly convex loss functions, and the privacy-utility tradeoff for non-convex settings remains unexplored.
- The assumption of honest-but-curious nodes is critical, as any collusion among nodes could significantly compromise privacy guarantees.

## Confidence

**High Confidence** (3 claims):
- The privacy amplification mechanism through neighbor-limited visibility is well-supported by the theoretical framework and aligns with established principles in decentralized learning.
- The O(1/n²) privacy amplification factor compared to local DP is rigorously derived and matches the theoretical expectations for random walk algorithms.
- The closed-form expressions for privacy loss between node pairs are mathematically sound and properly capture the impact of graph topology.

**Medium Confidence** (2 claims):
- The claim that random walk algorithms generally yield better privacy guarantees than gossip algorithms for nearby nodes is supported by the theoretical analysis but may depend on specific graph structures and parameter choices.
- The relationship between communicability metrics and privacy loss is well-motivated but requires empirical validation across diverse graph topologies.

**Low Confidence** (1 claim):
- The practical utility of the derived privacy loss expressions in real-world scenarios with unknown or dynamic graph topologies is uncertain and requires further investigation.

## Next Checks

1. **Graph Structure Sensitivity**: Implement the algorithm on multiple graph types (ring, star, grid, expander) and systematically vary the graph diameter and connectivity to validate the claimed relationship between graph distance and privacy loss.

2. **Parameter Sensitivity Analysis**: Conduct experiments varying the noise variance σ², step size γ, and number of iterations T to identify the regimes where the O(1/n²) privacy amplification holds and where it breaks down.

3. **Empirical vs. Theoretical Gap**: Compare the theoretical privacy loss bounds with empirical estimates obtained through gradient reconstruction attacks on synthetic and real datasets to assess the tightness of the bounds and identify potential vulnerabilities.