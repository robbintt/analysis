---
ver: rpa2
title: 'Dissecting Paraphrases: The Impact of Prompt Syntax and supplementary Information
  on Knowledge Retrieval from Pretrained Language Models'
arxiv_id: '2404.01992'
source_url: https://arxiv.org/abs/2404.01992
tags:
- knowledge
- syntax
- information
- prompt
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how syntax and semantics of prompts affect
  knowledge retrieval from pretrained language models. The authors introduce CONPARE-LAMA,
  a novel probe with 34 million controlled paraphrases, enabling systematic comparison
  of prompt variations.
---

# Dissecting Paraphrases: The Impact of Prompt Syntax and supplementary Information on Knowledge Retrieval from Pretrained Language Models

## Quick Facts
- **arXiv ID:** 2404.01992
- **Source URL:** https://arxiv.org/abs/2404.01992
- **Reference count:** 8
- **Primary result:** Clausal syntax significantly outperforms appositive syntax in knowledge retrieval from PLMs, with range information providing greater performance boosts than domain information.

## Executive Summary
This paper investigates how syntax and semantics of prompts affect knowledge retrieval from pretrained language models. The authors introduce CONPARE-LAMA, a novel probe with 34 million controlled paraphrases, enabling systematic comparison of prompt variations. They find that clausal syntax (e.g., adding domain/range info via subordinate clauses) significantly outperforms appositive syntax in retrieving relational knowledge. Knowledge retrieved via clausal prompts is more consistent and involves lower response uncertainty. Adding range information boosts performance more than domain information, though domain info is more reliably helpful across syntaxes. These findings highlight the fragility of information processing in PLMs and suggest syntax-aware pretraining could improve robustness.

## Method Summary
The study uses CONPARE-LAMA, a probe with 34 million controlled paraphrases, to systematically vary prompt syntax (clausal vs appositive) and semantics (domain/range information). Knowledge base constraints are extracted from Wikidata to construct prompts with controlled semantic content. Three base PLM architectures (BERT, RoBERTa, Luke) are evaluated using cloze-style prompts on TREx, GoogleRE, and ConceptNet corpora. Knowledge retrieval performance is measured by P@1, with additional metrics including consistency and binary entropy to assess uncertainty in predictions.

## Key Results
- Clausal syntax consistently outperforms appositive syntax across all tested models and corpora
- Range information provides greater performance boosts than domain information
- Clausal syntax reduces response uncertainty for known facts, as measured by lower binary entropy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Clausal syntax consistently outperforms appositive syntax for knowledge retrieval from PLMs.
- Mechanism: Clausal syntax encodes supplementary information as explicit assertions in coordinated or subordinated clauses, reducing ambiguity and improving model attention to key relational content.
- Core assumption: PLMs better process explicit, assertive clauses than embedded or descriptive appositives.
- Evidence anchors:
  - [abstract] "... prompts following clausal syntax have several desirable properties in comparison to appositive syntax: i) they are more useful when querying PLMs with a combination of supplementary information..."
  - [section] "... knowledge retrieved through prompts that rely on clausal syntax when adding sInf is more consistent given assumptions about the a priori knowledge available in the prompt."
- Break condition: If the model's pre-training corpus contained significant examples of appositive constructions with similar information structure, the advantage of clausal syntax might diminish.

### Mechanism 2
- Claim: Adding range information boosts retrieval performance more than domain information.
- Mechanism: Range information provides broader contextual grounding, enabling PLMs to disambiguate entities more effectively than domain info.
- Core assumption: PLMs leverage broader entity classification to resolve masked tokens more accurately.
- Evidence anchors:
  - [abstract] "... range information can boost knowledge retrieval performance more than domain information..."
  - [section] "We observe that sInf through prompts using clausal (compound=... complex=...) syntax increases the performance for all three models on all corpora."
- Break condition: If domain information uniquely identifies the masked object more precisely than range information in certain relations, the performance advantage could reverse.

### Mechanism 3
- Claim: Clausal syntax reduces response uncertainty for known facts.
- Mechanism: The assertive nature of clausal syntax narrows the model's probability distribution over possible completions, leading to lower binary entropy in predictions.
- Core assumption: Lower entropy correlates with more confident and correct predictions.
- Evidence anchors:
  - [abstract] "... they decrease response uncertainty when retrieving known facts."
  - [section] "We observe that for clausal syntax the Quality Completion offers uncertainty decrease with the addition of information..."
- Break condition: If model architecture or pre-training objectives prioritize token diversity over certainty, the entropy reduction might not translate to better accuracy.

## Foundational Learning

- **Syntax vs semantics distinction in natural language**: The paper systematically manipulates syntax (clausal vs appositive) while holding semantics constant to isolate effects on PLM performance.
  - Quick check question: Can you give an example where two sentences have identical semantics but different syntax?

- **Type constraints (domain/range) in knowledge bases**: Supplementary information is derived from Wikidata's domain and range restrictions to construct prompts with controlled semantics.
  - Quick check question: What is the difference between domain and range in a relation like "capitalOf"?

- **Entropy as a measure of uncertainty**: Binary entropy of the model's output distribution is used to quantify how syntax affects prediction confidence.
  - Quick check question: If a model outputs uniform probability over 8 tokens, what is its binary entropy?

## Architecture Onboarding

- **Component map:** Prompt generation module -> Knowledge base interface -> PLM inference layer -> Evaluation engine
- **Critical path:** Prompt generation → PLM inference → Metric computation → Analysis
- **Design tradeoffs:** Controlled paraphrasing ensures comparability but limits syntactic diversity; using base models reduces compute but may underrepresent larger models' capabilities
- **Failure signatures:** Inconsistent P@1 across syntactically equivalent prompts suggests model fragility; high entropy despite correct top-1 prediction indicates uncertainty issues
- **First 3 experiments:**
  1. Run baseline P@1 on simple prompts for all models and relations
  2. Generate and test clausal vs appositive prompts with domain info only
  3. Measure binary entropy for correctly predicted samples across syntax types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do appositive phrases affect the dependency distance between relation text and masked object tokens, and how does this impact knowledge retrieval performance?
- Basis in paper: [inferred] The authors suggest that appositive syntax may increase dependency distance between relation text and masked object token, potentially distracting the model.
- Why unresolved: The paper does not explicitly measure or analyze the dependency distance or its impact on knowledge retrieval.
- What evidence would resolve it: Quantitative analysis of dependency distances for different prompt syntaxes and their correlation with retrieval performance.

### Open Question 2
- Question: How would knowledge retrieval performance change if PLMs were pre-trained on non-fictional, factually correct corpora like Wikipedia or scholarly publications?
- Basis in paper: [explicit] The authors suggest that experiments on models trained on non-fictional, factually correct pre-training corpora are crucial to distinguish between false pre-training knowledge and wrong retrieval.
- Why unresolved: The paper does not conduct experiments on models pre-trained on such corpora.
- What evidence would resolve it: Comparative experiments on PLMs pre-trained on different types of corpora, including non-fictional ones.

### Open Question 3
- Question: How would incorporating more complex syntactic structures and additional relations from knowledge bases like Wikidata affect knowledge retrieval performance?
- Basis in paper: [explicit] The authors mention plans to expand the template to include more diversity in syntax and knowledge, applying it to additional relations derived from knowledge bases.
- Why unresolved: The paper does not explore these extensions in its current study.
- What evidence would resolve it: Experimental results comparing knowledge retrieval performance using more complex syntax and additional relations.

## Limitations
- Low confidence in cross-model generalizability: Findings may not extend to larger or more recent PLM architectures
- Uncontrolled confounding variables in supplementary information: Domain/range constraints may correlate with relation types, affecting performance
- Limited real-world applicability: Evaluation uses standard LAMA corpora that may not reflect realistic query distributions

## Confidence
- **High confidence** in the core finding that clausal syntax outperforms appositive syntax across all tested models and corpora
- **Medium confidence** in the claim that range information provides greater performance boosts than domain information
- **Medium confidence** in the entropy reduction findings

## Next Checks
1. **Cross-model scalability test:** Replicate main experiments using larger model variants (BERT-large, RoBERTa-large) and more recent architectures (DeBERTa, T5) to determine if syntax sensitivity persists across model scales and architectures

2. **Corpus naturalness validation:** Construct a test set of prompts with naturally occurring combinations of domain and range information from real query logs or question-answering datasets, then measure if clausal syntax advantages persist in more realistic settings

3. **Ablation on information content:** Create matched pairs of clausal and appositive prompts where the supplementary information content is held constant but expressed differently, to isolate whether syntax effects persist when semantic content is maximally controlled