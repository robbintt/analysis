---
ver: rpa2
title: New Textual Corpora for Serbian Language Modeling
arxiv_id: '2405.09250'
source_url: https://arxiv.org/abs/2405.09250
tags: []
core_contribution: "The paper presents new textual corpora for Serbian language modeling,\
  \ addressing the scarcity of high-quality training data. Three new corpora are introduced:\
  \ Umbrella corp, an aggregated web corpus combining existing resources; S.T.A.R.S.,\
  \ a textbook-quality corpus based on doctoral dissertations from NaRDuS; and PaSa\u02C7\
  \ z, a parallel corpus of English-Serbian abstracts."
---

# New Textual Corpora for Serbian Language Modeling

## Quick Facts
- arXiv ID: 2405.09250
- Source URL: https://arxiv.org/abs/2405.09250
- Reference count: 14
- New Serbian textual corpora created to address scarcity of high-quality training data

## Executive Summary
This paper addresses the critical shortage of high-quality textual corpora for Serbian language modeling by introducing three new resources. The authors created Umbrella corp through aggregation and deduplication of existing web corpora, S.T.A.R.S. from Serbian doctoral dissertations providing textbook-quality content, and PaSaˇ z as a parallel corpus of English-Serbian abstracts. These corpora significantly expand the available training data for Serbian language models, with S.T.A.R.S. alone containing over 560 million words. The work represents a substantial contribution to the Serbian NLP community by providing diverse, high-quality resources for model development.

## Method Summary
The authors developed three new Serbian corpora through systematic data collection and processing. Umbrella corp was created by aggregating existing web corpora (srWaC, cc100_sr, mC4_sr) and applying n-gram based deduplication to remove redundant content. S.T.A.R.S. was extracted from Serbian doctoral dissertations available through the NaRDuS platform, providing high-quality academic text. PaSaˇ z was constructed by extracting parallel English-Serbian abstracts from these dissertations. All corpora underwent deduplication and cleaning processes, with evaluation performed using word-frequency-based similarity analysis comparing against existing Serbian corpora.

## Key Results
- S.T.A.R.S. corpus contains over 560 million words from Serbian doctoral dissertations
- PaSaˇ z parallel corpus includes 7,678 English-Serbian abstract pairs
- Word frequency analysis shows S.T.A.R.S. is highly unique compared to existing corpora, particularly the literary corpus SrpELTeC
- Umbrella corp aggregation reduced from 28 billion to 18.6 billion words after deduplication while maintaining linguistic diversity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Corpus aggregation and deduplication increases training data size while reducing redundancy
- Mechanism: Multiple web corpora are aggregated and 6-gram based deduplication with 75% threshold removes overlapping content
- Core assumption: 75% n-gram threshold effectively removes redundant content without losing unique material
- Evidence: MaCoCu-sr reduced from 2,491M to 2,152M words after deduplication
- Break condition: Aggressive thresholds (>75%) may remove unique content

### Mechanism 2
- Claim: Textbook-quality corpora provide unique linguistic features distinct from web corpora
- Mechanism: S.T.A.R.S. uses peer-reviewed doctoral dissertations with standardized sections and academic language
- Core assumption: Academic texts have different word frequency distributions than web text
- Evidence: S.T.A.R.S. shows highest uniqueness in word frequency comparisons
- Break condition: Formulaic dissertation content may lose linguistic diversity

### Mechanism 3
- Claim: Parallel corpora of abstracts provide valuable translation data
- Mechanism: Extracts parallel Serbian-English abstracts from doctoral dissertations
- Core assumption: Professionally translated abstracts maintain consistency and quality
- Evidence: 7,687 parallel abstracts successfully extracted
- Break condition: Variable translation quality across institutions reduces corpus utility

## Foundational Learning

- Concept: N-gram based deduplication
  - Why needed: To remove redundant content while preserving unique material
  - Quick check: What is the minimum n-gram length for effective deduplication?

- Concept: Cosine similarity for corpus comparison
  - Why needed: To evaluate corpus uniqueness through word frequency distributions
  - Quick check: How does cosine similarity differ from Euclidean distance for high-dimensional vectors?

- Concept: Parallel corpus alignment
  - Why needed: To create high-quality translation data from dissertation abstracts
  - Quick check: What challenges arise when aligning parallel texts with different formatting?

## Architecture Onboarding

- Component map: NaRDuS data extraction -> text processing -> deduplication -> corpus evaluation -> release
- Critical path: Data extraction → text normalization → deduplication → evaluation → repository upload
- Design tradeoffs: Larger corpora provide more data but require more storage; higher deduplication thresholds remove more redundancy but risk losing unique content
- Failure signatures: Excessive deduplication shrinks corpus size; high similarity between corpora indicates lack of complementary features
- First 3 experiments:
  1. Test different n-gram lengths (4-gram vs 6-gram) for deduplication optimization
  2. Compare word frequency distributions using different sample sizes
  3. Sample and manually review parallel corpus translation quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does S.T.A.R.S. compare to existing textbook-quality corpora in terms of model performance?
- Basis: S.T.A.R.S. introduced as new textbook-quality corpus with 560M words
- Why unresolved: No experimental results comparing model performance with different corpora
- What would resolve: Empirical results showing model performance (perplexity, task accuracy) when training on S.T.A.R.S. versus other resources

### Open Question 2
- Question: What is the impact of deduplication on Umbrella corp quality and diversity?
- Basis: Umbrella corp created through aggregation and deduplication, reducing from 28B to 18.6B words
- Why unresolved: No analysis of deduplication effects on linguistic diversity or representativeness
- What would resolve: Analysis of linguistic features before/after deduplication or model performance comparisons

### Open Question 3
- Question: How does SrpELTeC uniqueness translate to practical benefits in language modeling?
- Basis: SrpELTeC identified as most unique corpus with 0.40 similarity score
- Why unresolved: Uniqueness established but practical implications not explored
- What would resolve: Experimental results showing model performance and task-specific results using SrpELTeC

## Limitations

- Deduplication methodology not validated against alternative approaches
- Evaluation relies solely on word frequency similarity, potentially missing morphological and syntactic differences
- Parallel corpus quality assumed based on institutional requirements without empirical verification

## Confidence

- High confidence: New Serbian corpora created and made available (specific statistics provided, CLARIN.SI repository)
- Medium confidence: Uniqueness claims based on word frequency analysis (cosine similarity presented but methodology lacks validation)
- Low confidence: Parallel corpus quality assessment (assumes high quality without direct evaluation)

## Next Checks

1. Validate S.T.A.R.S. uniqueness using multiple similarity metrics (Jensen-Shannon, Kullback-Leibler divergence)
2. Analyze deduplication threshold effects by testing different n-gram lengths and thresholds on sample data
3. Audit parallel corpus quality by manually evaluating 100 sample translation pairs for accuracy and consistency