---
ver: rpa2
title: 'See Through Their Minds: Learning Transferable Neural Representation from
  Cross-Subject fMRI'
arxiv_id: '2403.06361'
source_url: https://arxiv.org/abs/2403.06361
tags:
- fmri
- brain
- data
- visual
- decoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for cross-subject fMRI decoding using
  shallow subject-specific adapters to map fMRI data into unified representations,
  followed by a shared deeper decoding model. The approach incorporates both visual
  and textual supervision and includes high-level perception and pixel-wise reconstruction
  pipelines.
---

# See Through Their Minds: Learning Transferable Neural Representation from Cross-Subject fMRI

## Quick Facts
- **arXiv ID**: 2403.06361
- **Source URL**: https://arxiv.org/abs/2403.06361
- **Reference count**: 40
- **Primary result**: Cross-subject fMRI decoding using shallow adapters achieves improved reconstruction metrics and outperforms state-of-the-art methods like Mind-Vis and fMRI-PTE

## Executive Summary
This paper proposes a method for cross-subject fMRI decoding that uses shallow subject-specific adapters to map fMRI data into unified representations, followed by a shared deeper decoding model. The approach incorporates both visual and textual supervision and includes high-level perception and pixel-wise reconstruction pipelines. The method demonstrates robust neural representation learning across subjects and improved reconstruction metrics, with successful transfer learning to new subjects using limited data.

## Method Summary
The method uses subject-specific adapters (linear projection + residual block) to align cross-subject fMRI data into a unified feature space, enabling a shared deeper MLP decoder to capture general patterns. A high-level perception decoding pipeline with contrastive learning (visual-linguistic and fine-grained visual contrastive learning) generates semantic embeddings, while a pixel-wise reconstruction pipeline uses these high-level features to guide low-level image generation. Transfer learning is achieved by training new adapters for held-out subjects while freezing the shared decoder, enabling effective zero-shot decoding with limited data.

## Key Results
- Achieves superior reconstruction metrics (PixCorr, SSIM, AlexNet, Inception, CLIP, EfficientNet, SwAV) compared to state-of-the-art methods
- Demonstrates improved retrieval accuracy (image, text, brain retrieval) over baselines like Mind-Vis and fMRI-PTE
- Successfully transfers learned general knowledge to new subjects with limited training data
- Shows robust neural representation learning across subjects from both NSD and GOD datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Shallow subject-specific adapters map cross-subject fMRI into a unified feature space, enabling a shared deeper decoder to capture general patterns.
- **Mechanism**: The adapters compensate for inter-subject anatomical and functional variability, aligning fMRI voxels across subjects into a common latent representation. This alignment allows the shared MLP backbone to learn representations that generalize across subjects.
- **Core assumption**: Coarse-grained response topographies are highly similar across subjects, while fine-grained individual differences can be handled by shallow adapters.
- **Break condition**: If the similarity assumption fails (e.g., subjects have highly divergent neural encoding), the unified decoder will not generalize and performance will degrade.

### Mechanism 2
- **Claim**: High-level semantic guidance improves low-level reconstruction by mimicking top-down perceptual processes.
- **Mechanism**: Features from the high-level pipeline (semantic CLIP embeddings) are merged with low-level adapter outputs. This guides the low-level decoder to produce reconstructions that better match semantic content, not just pixel statistics.
- **Core assumption**: The human brain uses top-down feedback to refine low-level perception; this process can be simulated in the model.
- **Break condition**: If high-level features do not correlate with low-level visual details, the guidance may introduce noise rather than improvement.

### Mechanism 3
- **Claim**: Cross-subject pre-training followed by adapter-based transfer enables effective zero-shot decoding for new subjects with limited data.
- **Mechanism**: The model first learns general fMRI-to-visual mappings from multiple subjects. For a new subject, only a shallow adapter is trained (freezing the shared decoder), allowing adaptation without overfitting to scarce data.
- **Core assumption**: General visual decoding patterns are shared across subjects and can be transferred via adapters.
- **Break condition**: If the new subject's brain activity patterns are too idiosyncratic or the data is too scarce, adapter training may not bridge the gap.

## Foundational Learning

- **Concept**: fMRI preprocessing and ROI alignment
  - **Why needed here**: fMRI data must be spatially normalized and masked to comparable voxel sets before adapters can meaningfully align across subjects.
  - **Quick check question**: What preprocessing steps ensure that subject A's voxel 1234 corresponds to the same anatomical region as subject B's voxel 1234?

- **Concept**: Multi-modal contrastive learning
  - **Why needed here**: Aligning brain signals with visual and textual embeddings requires understanding contrastive loss formulations and modality gap issues.
  - **Quick check question**: How does BiMixCo differ from SoftCLIP in handling modality gaps?

- **Concept**: Diffusion prior and latent space mapping
  - **Why needed here**: The diffusion prior maps brain-derived embeddings into CLIP image space, which is then used to guide image generation.
  - **Quick check question**: Why does the diffusion prior use a transformer decoder rather than an encoder?

## Architecture Onboarding

- **Component map**: Subject adapters (linear + residual) → Shared MLP backbone → Tokenization → Projector & Diffusion Prior (high-level) OR CNN upsample + VAE (low-level) → Img2img generation using Versatile Diffusion
- **Critical path**: Adapters → MLP → Tokenization → Projector/Guidance → Final generation
- **Design tradeoffs**: Shallow adapters vs. full subject-specific models: fewer parameters, better generalization, but requires alignment assumption to hold. Separate high-level/low-level pipelines vs. unified model: modular, easier to guide low-level with high-level, but more complex to train jointly.
- **Failure signatures**: Poor retrieval accuracy → token alignment or contrastive learning issues. Blurry or semantically incorrect reconstructions → guidance signal too weak or high-level adapter misaligned. Overfitting on single subject → insufficient cross-subject regularization.
- **First 3 experiments**:
  1. Train adapters + MLP on 2 subjects, test retrieval accuracy; verify alignment.
  2. Add high-level guidance to low-level pipeline, compare SSIM/PixCorr vs. baseline.
  3. Freeze shared decoder, train new adapter for a held-out subject, evaluate zero-shot transfer.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do the shallow subject-specific adapters affect the ability to generalize to subjects with very different brain anatomy or functional organization compared to those in the pre-training dataset?
- **Basis in paper**: [inferred] The paper mentions using subject-specific adapters to transform cross-subject fMRI data into unified representations, but doesn't explicitly test on subjects with highly divergent brain structures.
- **Why unresolved**: The experiments focus on subjects from the same datasets (NSD and GOD) with presumably similar brain characteristics, limiting the generalizability assessment.
- **What evidence would resolve it**: Testing the model on subjects from diverse populations (e.g., different age groups, neurological conditions, or species) would reveal the adapters' robustness to anatomical and functional variations.

### Open Question 2
- **Question**: What is the impact of the interaction between high-level and low-level perception pipelines on reconstruction performance when using different types of visual stimuli (e.g., abstract art, low-contrast images, or rapidly changing scenes)?
- **Basis in paper**: [explicit] The paper discusses the importance of the interaction between high-level and low-level perceptions and proposes utilizing high-level perceptions to guide pixel-wise reconstruction.
- **Why unresolved**: The experiments primarily use natural scenes, which may not fully challenge the model's ability to handle diverse visual content and processing demands.
- **What evidence would resolve it**: Evaluating the model's performance on a wider range of visual stimuli, including abstract art, low-contrast images, and rapidly changing scenes, would reveal the robustness of the high-level and low-level perception interaction.

### Open Question 3
- **Question**: How does the model's performance change when using different types of textual supervision (e.g., longer descriptions, different levels of abstraction, or multiple captions per image) during training?
- **Basis in paper**: [explicit] The paper mentions leveraging both visual and textual supervision for multi-modal brain decoding, but doesn't explore the impact of different types of textual input.
- **Why unresolved**: The experiments use fixed-length captions from the COCO and GOD-Cap datasets, limiting the exploration of how textual supervision quality and diversity affect the model.
- **What evidence would resolve it**: Training and evaluating the model with different types of textual supervision, such as longer descriptions, varying levels of abstraction, or multiple captions per image, would reveal the impact on reconstruction performance and the model's ability to capture semantic information.

## Limitations

- The alignment assumption may fail for subjects with highly divergent neural encoding patterns or brain anatomy
- The effectiveness of high-level semantic guidance depends on the correlation between semantic features and low-level visual details, which may not hold for all stimulus types
- Transfer learning success depends on having sufficient shared neural patterns across subjects and adequate adapter training data

## Confidence

- **High confidence**: The architectural design of subject adapters + shared decoder is technically sound and well-motivated by neuroscience literature
- **Medium confidence**: The retrieval accuracy improvements over baselines (Mind-Vis, fMRI-PTE) given the quantitative comparisons provided
- **Medium confidence**: The transfer learning effectiveness, though this depends on the critical assumption about shared neural patterns holding true
- **Lower confidence**: The claim about simulating top-down perceptual processes - while conceptually plausible, the evidence is more mechanistic than empirical

## Next Checks

1. **Alignment verification**: Test adapter performance with artificially introduced misalignment between subjects to quantify how much degradation occurs when the alignment assumption fails
2. **Guidance ablation**: Systematically vary the strength of high-level semantic guidance in the low-level pipeline to determine optimal guidance ratios and test whether guidance ever degrades performance
3. **Transfer learning stress test**: Evaluate zero-shot transfer performance across varying amounts of adapter training data (5, 10, 20, 50 samples) to identify the minimum viable data threshold for successful adaptation