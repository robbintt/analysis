---
ver: rpa2
title: Fostering the Ecosystem of Open Neural Encoders for Portuguese with Albertina
  PT* Family
arxiv_id: '2403.01897'
source_url: https://arxiv.org/abs/2403.01897
tags:
- language
- albertina
- portuguese
- ptbr
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents new open encoder models for Portuguese, expanding
  the existing ecosystem. The authors introduce Albertina 1.5B PT, a large 1.5 billion
  parameter model for top performance, and Albertina 100M PT, a smaller 100 million
  parameter model for efficiency.
---

# Fostering the Ecosystem of Open Neural Encoders for Portuguese with Albertina PT* Family

## Quick Facts
- arXiv ID: 2403.01897
- Source URL: https://arxiv.org/abs/2403.01897
- Reference count: 0
- Primary result: Introduces two new open encoder models for Portuguese (Albertina 1.5B PT and Albertina 100M PT) achieving state-of-the-art performance on GLUE and SuperGLUE benchmarks

## Executive Summary
This paper presents two new open neural encoder models for Portuguese: Albertina 1.5B PT, a large 1.5 billion parameter model optimized for performance, and Albertina 100M PT, a smaller 100 million parameter model optimized for efficiency. Both models are trained on Portuguese data from CulturaX and OSCAR datasets, with additional corpora for the European Portuguese variant. The authors demonstrate that larger models perform better on Portuguese downstream tasks, and that continuing pre-training on monolingual data improves performance even when starting from multilingual models. The models are publicly available under a permissive license from Hugging Face.

## Method Summary
The Albertina models are developed through continued pre-training of DeBERTa-Base (for Albertina 100M PT) and DeBERTa-XXLarge (for Albertina 1.5B PT) on Portuguese data from CulturaX and OSCAR datasets, plus additional corpora for European Portuguese. The training uses a learning rate of 1e-5 with linear decay and 10k warm-up steps. Models are then fine-tuned on downstream tasks from GLUE and SuperGLUE benchmarks, translated into Portuguese using DeepL. Performance is evaluated using accuracy, F1 score, and Pearson correlation metrics depending on the task.

## Key Results
- Albertina 1.5B PT achieves state-of-the-art performance on Portuguese downstream tasks
- Larger models consistently outperform smaller ones on Portuguese benchmarks
- Continuing pre-training on monolingual Portuguese data improves performance compared to multilingual models
- Albertina 100M PT performs comparably to BERTimbau despite being smaller, suggesting improved parameter utilization in the DeBERTa architecture

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger models perform better on Portuguese downstream tasks.
- Mechanism: Increased parameter count allows the model to capture more complex linguistic patterns and nuances specific to Portuguese.
- Core assumption: The training data is sufficient and diverse enough to effectively train the larger model.
- Evidence anchors:
  - [abstract] "The larger the model, the better the performance"
  - [section 5.4] "Taking a broad view of the results in Tables 4 and 5, overall and as expected, the larger the Albertina model the better is its performance in downstream tasks."
- Break condition: If the training data is not sufficiently large or diverse to train the larger model effectively, the performance gain may not materialize.

### Mechanism 2
- Claim: Continuing pre-training on monolingual data improves performance for the target language.
- Mechanism: Fine-tuning a multilingual model on monolingual data allows it to specialize and adapt to the specific linguistic features of the target language.
- Core assumption: The multilingual base model has learned transferable representations that can be adapted to the target language.
- Evidence anchors:
  - [abstract] "The results demonstrate the importance of continuing the pre-training of models with monolingual data for the language of interest, even if they started multilingual or were initially developed for another language."
  - [section 5.4] "This adds to the empirical evidence in the literature, commented in Section 2, for the importance of continuing the pre-training of models with monolingual data for the language of interest, even if they started multilingual or were initially developed for another language."
- Break condition: If the base model is not sufficiently good or the monolingual data is not representative of the target language, the performance gain may not be significant.

### Mechanism 3
- Claim: Using a more advanced base model leads to better performance in the target language.
- Mechanism: Advanced base models have learned more sophisticated representations that can be leveraged for the target language.
- Core assumption: The advancements in the base model are transferable to the target language.
- Evidence anchors:
  - [section 5.4] "However, it is important also to note that the difference between the performance scores of Albertina 100M PTBR and of the 335M BERTimbau is rather small, which seems to suggest that the improvements in DeBERTa, on which our Albertina 100M PT is based, over BERT, which used as a base model by BERTimbau, have allowed for more efficient parameter utilization and improved performance in general."
- Break condition: If the advancements in the base model are not relevant to the target language, the performance gain may not be significant.

## Foundational Learning

- Concept: Transformer architecture
  - Why needed here: The models are based on the Transformer architecture, which is crucial for understanding their design and functioning.
  - Quick check question: What are the key components of the Transformer architecture and how do they contribute to language modeling?

- Concept: Pre-training and fine-tuning
  - Why needed here: The models are pre-trained on large datasets and then fine-tuned on downstream tasks, which is a common approach in NLP.
  - Quick check question: What is the difference between pre-training and fine-tuning, and why is this approach effective for NLP tasks?

- Concept: Language modeling for low-resource languages
  - Why needed here: Portuguese is considered a low-resource language, and the paper addresses the challenges and approaches for developing effective language models for such languages.
  - Quick check question: What are the specific challenges in developing language models for low-resource languages, and how does this paper address them?

## Architecture Onboarding

- Component map: DeBERTa-Base/XXLarge -> Continued pre-training on Portuguese data -> Fine-tuning on GLUE/SuperGLUE tasks
- Critical path: Data preparation (quality filtering) -> Continued pre-training -> Fine-tuning with hyperparameter search -> Evaluation
- Design tradeoffs: Model size vs. performance (1.5B vs 100M parameters) and multilingual base vs. monolingual specialization
- Failure signatures: Poor downstream performance, overfitting on training data, ineffective fine-tuning
- First 3 experiments:
  1. Train a small Albertina model (e.g., 100M parameters) on Portuguese data and evaluate its performance on a downstream task.
  2. Compare the performance of the small Albertina model with a multilingual base model fine-tuned on Portuguese data.
  3. Train a larger Albertina model (e.g., 1.5B parameters) and evaluate its performance on the same downstream task.

## Open Questions the Paper Calls Out
None explicitly called out in the paper.

## Limitations
- Evaluation relies entirely on machine-translated GLUE and SuperGLUE benchmarks rather than native Portuguese test data
- Limited comparison to existing Portuguese models, focusing mainly on BERT-based approaches
- Does not report results on native Portuguese benchmarks or human-evaluated quality metrics

## Confidence

**High confidence**: The claim that larger models achieve better performance on Portuguese downstream tasks is supported by clear empirical evidence showing consistent performance improvements with increased model size.

**Medium confidence**: The assertion that continuing pre-training on monolingual data improves performance is supported by the results, but the comparison is limited to a single multilingual base model (XLM-R) and doesn't explore alternative approaches.

**Medium confidence**: The observation that more advanced base models lead to better performance is based on comparing DeBERTa-based models with BERT-based ones, but this comparison doesn't control for other architectural differences.

## Next Checks
1. Evaluate Albertina models on native Portuguese benchmarks (if available) rather than machine-translated GLUE/SuperGLUE tasks to verify that performance gains generalize to authentic Portuguese language data.

2. Compare Albertina's performance against more recent Portuguese language models beyond BERT-based approaches, including other DeBERTa variants and newer architectures, to establish a more comprehensive performance baseline.

3. Conduct ablation studies to isolate the contribution of model size versus architecture improvements by training multiple models with identical sizes but different base architectures, and vice versa.