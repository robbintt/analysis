---
ver: rpa2
title: 'D2SP: Dynamic Dual-Stage Purification Framework for Dual Noise Mitigation
  in Vision-based Affective Recognition'
arxiv_id: '2406.16473'
source_url: https://arxiv.org/abs/2406.16473
tags:
- uncertainty
- samples
- recognition
- learning
- dfer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses uncertainty in Dynamic Facial Expression
  Recognition (DFER) datasets, specifically two types: low-quality samples and mislabelled
  samples due to annotation bias. The authors propose a two-stage framework called
  SCIU (Seeking Certainty In Uncertainty) to mitigate these issues.'
---

# D2SP: Dynamic Dual-Stage Purification Framework for Dual Noise Mitigation in Vision-based Affective Recognition

## Quick Facts
- **arXiv ID**: 2406.16473
- **Source URL**: https://arxiv.org/abs/2406.16473
- **Reference count**: 40
- **Primary result**: Two-stage framework (CGP and FGC) improves recognition performance on tested DFER datasets

## Executive Summary
This paper addresses uncertainty in Dynamic Facial Expression Recognition (DFER) datasets, specifically two types: low-quality samples and mislabelled samples due to annotation bias. The authors propose a two-stage framework called SCIU (Seeking Certainty In Uncertainty) to mitigate these issues. The first stage, Coarse-Grained Pruning (CGP), removes low-quality samples by assigning weights and pruning those below a threshold. The second stage, Fine-Grained Correction (FGC), corrects mislabeled samples by identifying those with stable predictions across epochs and relabeling them based on prediction consistency.

The framework is evaluated on three DFER datasets (FERV39k, DFEW, and MAFW) using six different backbone architectures. Results show significant performance improvements across all datasets and methods. On FERV39k, SCIU achieved an average improvement of 3.97% in Weighted Average Recall (WAR) and 2.45% in Unweighted Average Recall (UAR). Similar improvements were observed on DFEW and MAFW, demonstrating the framework's effectiveness in handling uncertainty in DFER datasets and its generalizability across different models and datasets.

## Method Summary
The SCIU framework consists of two stages: Coarse-Grained Pruning (CGP) and Fine-Grained Correction (FGC). In CGP, each sample is assigned a weight based on its quality, and samples with weights below a threshold are removed. This stage aims to eliminate low-quality samples that may introduce noise into the training process. In FGC, samples with stable predictions across multiple epochs are identified, and their labels are corrected based on the most frequent prediction. This stage addresses mislabeling issues by leveraging the model's confidence in its predictions over time. The framework is designed to be flexible and can be integrated with various backbone architectures, making it suitable for different DFER datasets and models.

## Key Results
- SCIU achieved an average improvement of 3.97% in Weighted Average Recall (WAR) on FERV39k dataset
- SCIU achieved an average improvement of 2.45% in Unweighted Average Recall (UAR) on FERV39k dataset
- Similar performance improvements were observed on DFEW and MAFW datasets across multiple backbone architectures

## Why This Works (Mechanism)
The two-stage framework works by first removing low-quality samples that introduce noise, then correcting mislabeled samples based on prediction consistency. CGP eliminates samples that are likely to be outliers or contain excessive noise, reducing the overall uncertainty in the dataset. FGC leverages the model's ability to make consistent predictions over time to identify and correct mislabeled samples. By addressing both low-quality and mislabeled samples, the framework improves the overall quality of the training data, leading to better model performance.

## Foundational Learning
- **Dynamic Facial Expression Recognition (DFER)**: Understanding how facial expressions change over time in video sequences. Needed to contextualize the specific challenges of DFER datasets and the importance of handling uncertainty in temporal data.
- **Coarse-Grained Pruning (CGP)**: A technique for removing low-quality samples from a dataset based on a confidence threshold. Needed to understand how the first stage of SCIU improves data quality by eliminating noisy samples.
- **Fine-Grained Correction (FGC)**: A method for correcting mislabeled samples by leveraging prediction consistency across epochs. Needed to grasp how the second stage of SCIU addresses annotation bias and improves label accuracy.
- **Weighted Average Recall (WAR) and Unweighted Average Recall (UAR)**: Evaluation metrics used to assess the performance of multi-class classification models, particularly in imbalanced datasets. Needed to interpret the reported performance improvements and understand the significance of the results.
- **Backbone architectures**: The underlying neural network structures used as the base for DFER models. Needed to appreciate the generalizability of SCIU across different model architectures.
- **Annotation bias**: Systematic errors in labeling data that can lead to mislabeled samples and reduced model performance. Needed to understand the importance of addressing mislabeled samples in DFER datasets.

## Architecture Onboarding
### Component Map
Dataset -> CGP -> FGC -> Model Training

### Critical Path
The critical path involves first applying CGP to remove low-quality samples, then using FGC to correct mislabeled samples, and finally training the model on the purified dataset. This sequential approach ensures that the model is trained on high-quality data with accurate labels.

### Design Tradeoffs
- Fixed confidence thresholds vs. adaptive thresholds: Using fixed thresholds simplifies implementation but may not generalize well across different datasets or noise distributions.
- Prediction consistency vs. model confidence: Relying on prediction consistency across epochs may be more robust than using model confidence alone, but it requires multiple training iterations.

### Failure Signatures
- Over-pruning: If the confidence threshold in CGP is set too high, it may remove too many samples, leading to a smaller and potentially less representative dataset.
- Over-correction: If the prediction consistency threshold in FGC is set too low, it may introduce new biases by incorrectly relabeling samples.

### First Experiments
1. Evaluate the impact of different confidence thresholds on CGP performance across multiple DFER datasets.
2. Compare the effectiveness of FGC with other label correction methods, such as active learning or semi-supervised learning.
3. Test the framework's performance on additional vision-based affective recognition tasks beyond DFER to assess cross-task applicability.

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on fixed confidence thresholds for sample pruning and relabeling may not generalize well across different datasets or noise distributions.
- Performance improvements are measured on specific DFER datasets (FERV39k, DFEW, MAFW), but effectiveness on other vision-based affective recognition tasks remains untested.
- Potential overfitting risks when using prediction consistency across epochs for relabeling, which could introduce new biases.

## Confidence
- **High**: The two-stage framework (CGP and FGC) improves recognition performance on tested DFER datasets, given the reported statistical improvements in WAR and UAR across multiple backbone architectures.
- **Medium**: The generalizability of the framework across different models and datasets, as the results are based on a limited set of experiments.
- **Low**: The long-term robustness of the relabeling strategy, particularly in scenarios with high levels of annotation noise or varying data quality.

## Next Checks
1. Test the framework on additional vision-based affective recognition datasets beyond DFER to assess cross-task applicability.
2. Conduct ablation studies to determine the optimal confidence thresholds for pruning and relabeling under different noise conditions.
3. Evaluate the framework's performance in real-time or streaming scenarios to ensure its practical utility in dynamic environments.