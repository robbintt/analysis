---
ver: rpa2
title: Investigating Wit, Creativity, and Detectability of Large Language Models in
  Domain-Specific Writing Style Adaptation of Reddit's Showerthoughts
arxiv_id: '2405.01660'
source_url: https://arxiv.org/abs/2405.01660
tags:
- showerthoughts
- texts
- generated
- text
- reddit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the ability of different Large Language
  Models (LLMs) to generate creative, witty texts in the style of Reddit's Showerthoughts
  community. The authors fine-tune GPT-2 and GPT-Neo on Reddit data and use GPT-3.5
  in a zero-shot manner to generate Showerthoughts.
---

# Investigating Wit, Creativity, and Detectability of Large Language Models in Domain-Specific Writing Style Adaptation of Reddit's Showerthoughts

## Quick Facts
- **arXiv ID:** 2405.01660
- **Source URL:** https://arxiv.org/abs/2405.01660
- **Reference count:** 40
- **Primary result:** Fine-tuned GPT-2 and GPT-Neo models generate Showerthoughts nearly indistinguishable from human-written ones, but RoBERTa classifiers can detect AI-generated content with high accuracy.

## Executive Summary
This paper investigates the ability of Large Language Models (LLMs) to generate creative, witty texts in the style of Reddit's Showerthoughts community. The authors fine-tune GPT-2 and GPT-Neo on Reddit data and use GPT-3.5 in a zero-shot manner to generate Showerthoughts. They compare these AI-generated texts with human-authored ones through human evaluation surveys and automated authorship identification using RoBERTa classifiers. The results show that while human evaluators rate the generated texts slightly lower in terms of creative quality, they are unable to reliably distinguish between human-written and AI-generated texts. The automated classifiers, however, achieve high accuracy in authorship identification. The study provides insights into the capabilities of LLMs in creative text generation and the challenges of detecting AI-generated content.

## Method Summary
The authors fine-tune GPT-2 Medium and GPT-Neo on a dataset of 411,189 Reddit Showerthoughts posts, generating 5,000 samples per model. They compare these outputs with human-authored texts using human evaluation surveys that rate creative quality across dimensions like logical validity, creativity, humor, and cleverness. Additionally, they train RoBERTa classifiers to automatically detect AI-generated content by learning linguistic patterns that distinguish it from human writing. The study employs both fine-tuned models (GPT-2, GPT-Neo) and zero-shot generation (GPT-3.5) to assess different approaches to creative text generation.

## Key Results
- Human evaluators rated AI-generated Showerthoughts slightly lower on average than human-written ones, particularly in humor and cleverness dimensions.
- Human evaluators could not reliably distinguish between AI-generated and human-written texts, with accuracy rates below chance for some models.
- RoBERTa classifiers achieved high accuracy in detecting AI-generated content by learning subtle linguistic patterns.
- Fine-tuned GPT-2 and GPT-Neo models successfully captured the stylistic patterns of Showerthoughts, producing creative and witty text comparable to human-authored content.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-tuning small GPT-2 and GPT-Neo models on Reddit Showerthoughts data enables them to generate creative, witty text that closely matches human writing style.
- **Mechanism:** The fine-tuning process adapts the models' parameters to the stylistic patterns and semantic structures characteristic of Showerthoughts, capturing the balance of wit, creativity, and logical validity.
- **Core assumption:** The Showerthoughts dataset contains sufficient diversity and quality examples for the models to learn generalizable patterns of creative short-form text generation.
- **Evidence anchors:**
  - [abstract] states that GPT-2 and GPT-Neo were fine-tuned on Reddit data and generated texts were evaluated for creative quality.
  - [section 4.1] describes the fine-tuning process and evaluation criteria.
  - [corpus] shows related work on creativity evaluation in LLMs, supporting the feasibility of this approach.
- **Break condition:** If the training data lacks sufficient variety or contains too many low-quality examples, the models may fail to capture the full range of creative expression.

### Mechanism 2
- **Claim:** Human evaluators cannot reliably distinguish between human-written and AI-generated Showerthoughts, indicating that the generated texts achieve near-human quality.
- **Mechanism:** The combination of fine-tuning on high-quality Showerthoughts and careful prompt engineering for zero-shot generation results in texts that are semantically and stylistically similar to human-authored content.
- **Core assumption:** Human evaluators' judgments are consistent and reliable measures of creative quality and detectability.
- **Evidence anchors:**
  - [abstract] reports that human evaluators rated generated texts slightly lower on average but could not reliably distinguish them from human-written texts.
  - [section 5.2] provides detailed survey results showing accuracy rates below chance for some models.
  - [corpus] includes studies on AI text detection, supporting the relevance of this finding.
- **Break condition:** If evaluators are provided with additional context or examples of AI-generated text, their ability to detect it may improve.

### Mechanism 3
- **Claim:** RoBERTa classifiers can accurately detect AI-generated Showerthoughts by learning subtle linguistic patterns that distinguish them from human-written texts.
- **Mechanism:** The classifiers learn to identify specific tokens, punctuation patterns, and sentence structures that are more prevalent in AI-generated text, enabling high-precision authorship identification.
- **Core assumption:** There are consistent, learnable differences in the way AI models and humans generate short creative texts.
- **Evidence anchors:**
  - [abstract] states that RoBERTa classifiers achieved high accuracy in authorship identification.
  - [section 5.3] describes the token attribution analysis and classification performance.
  - [corpus] shows related work on authorship identification, supporting the feasibility of this approach.
- **Break condition:** If AI generation techniques evolve to mimic human patterns more closely, the classifiers' performance may degrade.

## Foundational Learning

- **Concept: Fine-tuning language models**
  - Why needed here: Fine-tuning adapts pre-trained models to the specific style and content of Showerthoughts, enabling generation of creative, witty text.
  - Quick check question: What is the difference between fine-tuning and prompt engineering for adapting language models?

- **Concept: Human evaluation of creative content**
  - Why needed here: Human evaluators provide subjective ratings on dimensions like creativity, humor, and logical validity, which are crucial for assessing the quality of generated Showerthoughts.
  - Quick check question: How might the demographics of human evaluators affect their ratings of creative text?

- **Concept: Authorship identification with classifiers**
  - Why needed here: RoBERTa classifiers learn to distinguish AI-generated from human-written text, providing an objective measure of detectability and insights into generation patterns.
  - Quick check question: What features might a classifier learn to identify AI-generated creative text?

## Architecture Onboarding

- **Component map:** Data collection -> Fine-tuning GPT-2/GPT-Neo -> Generation -> Human evaluation -> RoBERTa classification -> Analysis
- **Critical path:** 1. Collect and preprocess Showerthoughts dataset 2. Fine-tune GPT-2 Medium and GPT-Neo models 3. Generate texts using fine-tuned models and ChatGPT 4. Conduct human evaluation survey 5. Train and evaluate RoBERTa classifiers 6. Analyze results and identify patterns
- **Design tradeoffs:**
  - Model size vs. generation quality: Smaller models may be less capable but more efficient
  - Fine-tuning vs. zero-shot: Fine-tuning allows for more targeted adaptation but requires labeled data
  - Human evaluation vs. automated metrics: Human judgment captures subjective qualities but is more resource-intensive
- **Failure signatures:**
  - Poor generation quality: Models produce nonsensical or repetitive text
  - Low human evaluation scores: Generated texts fail to capture the essence of Showerthoughts
  - Low classifier accuracy: Models and human-written texts are too similar to distinguish
- **First 3 experiments:**
  1. Fine-tune GPT-2 on a subset of Showerthoughts and generate a small set of texts for qualitative evaluation
  2. Conduct a pilot human evaluation survey with a limited number of generated and human-written Showerthoughts
  3. Train a simple classifier (e.g., logistic regression) on lexical features to distinguish generated from human-written texts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the quality of AI-generated Showerthoughts be improved to match or exceed human-authored ones in terms of creativity, humor, and logical validity?
- Basis in paper: [inferred] The study found that while AI-generated texts are rated slightly lower on average regarding their creative quality compared to human-written texts, they are still able to fool human evaluators in distinguishing between the two. This suggests room for improvement in the AI's ability to generate more creative and witty texts.
- Why unresolved: The paper does not explore potential techniques or approaches to enhance the creative quality of AI-generated texts. It focuses on comparing the existing capabilities of different LLMs rather than investigating ways to improve them.
- What evidence would resolve it: Conducting experiments with different fine-tuning strategies, larger model sizes, or alternative architectures to assess their impact on the creative quality of generated texts. Comparing the results with human-authored texts to determine if the gap in quality can be bridged.

### Open Question 2
- Question: Can the patterns learned by the RoBERTa classifiers for detecting AI-generated texts be generalized to other types of creative writing beyond Showerthoughts?
- Basis in paper: [explicit] The paper mentions that the RoBERTa classifiers were fine-tuned on Showerthoughts data and achieved high accuracy in distinguishing between human-written and AI-generated texts within this specific domain. However, it also notes that the classifiers' performance may be limited when dealing with texts from other domains or generated by different LLMs.
- Why unresolved: The study focuses on the specific case of Showerthoughts and does not explore the generalizability of the learned patterns to other creative writing styles or domains. It is unclear whether the identified features are unique to Showerthoughts or can be applied more broadly.
- What evidence would resolve it: Evaluating the RoBERTa classifiers on datasets from different creative writing domains, such as poetry, short stories, or marketing copy. Assessing the classifiers' performance and comparing the identified patterns across domains to determine their generalizability.

### Open Question 3
- Question: What are the key factors that influence human evaluators' ability to distinguish between AI-generated and human-written texts in creative writing domains?
- Basis in paper: [inferred] The study found that human evaluators were unable to reliably distinguish between AI-generated and human-written Showerthoughts, even when they had prior experience with Reddit and Showerthoughts. This suggests that there are specific factors that contribute to the evaluators' perception of text authenticity.
- Why unresolved: The paper does not delve into the specific factors that influence human evaluators' ability to detect AI-generated texts. It mentions some common indicators mentioned by participants, such as illogical statements and good grammar, but does not provide a comprehensive analysis of the underlying reasons for the evaluators' performance.
- What evidence would resolve it: Conducting a detailed analysis of the survey responses to identify the key factors that participants relied on when evaluating the authenticity of the texts. This could involve qualitative analysis of the participants' reasoning and quantitative analysis of the relationship between specific text features and the evaluators' judgments.

## Limitations
- The study focuses on a specific subreddit style, which may limit generalizability to other forms of creative writing.
- The human evaluation was conducted primarily with a single demographic group (students), which may bias the results.
- The study doesn't explore the long-term evolution of detectability as AI generation techniques improve.

## Confidence
- **High confidence:** The experimental methodology and data collection process are well-documented and reproducible. The human evaluation survey design and automated classification approach are standard and appropriate for this research question.
- **Medium confidence:** The comparison between human and AI-generated texts relies on subjective human evaluation, which may be influenced by evaluator demographics and expectations. The study acknowledges this limitation but doesn't fully explore how different evaluator backgrounds might affect the results.
- **Medium confidence:** While the RoBERTa classifiers show high accuracy, the study doesn't thoroughly investigate what specific linguistic patterns enable this detection, leaving some uncertainty about whether these patterns would persist as AI generation techniques evolve.

## Next Checks
1. Conduct a cross-cultural human evaluation with diverse demographic groups to assess whether the observed trends in creative quality ratings hold across different populations.
2. Perform an ablation study on the RoBERTa classifiers to identify which specific linguistic features (token patterns, punctuation, sentence structures) are most predictive of AI authorship.
3. Test the same models on a different creative writing dataset (e.g., short stories or poetry) to evaluate the generalizability of the findings beyond the Showerthoughts style.