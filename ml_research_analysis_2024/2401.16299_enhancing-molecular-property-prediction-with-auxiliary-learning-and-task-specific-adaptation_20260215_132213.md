---
ver: rpa2
title: Enhancing Molecular Property Prediction with Auxiliary Learning and Task-Specific
  Adaptation
arxiv_id: '2401.16299'
source_url: https://arxiv.org/abs/2401.16299
tags:
- tasks
- task
- auxiliary
- rcgrad
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes novel adaptation strategies for pretrained
  molecular GNNs using auxiliary tasks to improve target task performance. The key
  methods include RCGrad (rotation of conflicting gradients) and BLO+RCGrad (bi-level
  optimization with gradient rotation) that learn to align and scale task gradients
  to mitigate negative transfer.
---

# Enhancing Molecular Property Prediction with Auxiliary Learning and Task-Specific Adaptation

## Quick Facts
- **arXiv ID**: 2401.16299
- **Source URL**: https://arxiv.org/abs/2401.16299
- **Reference count**: 26
- **Key outcome**: RCGrad and BLO+RCGrad adaptation strategies improve ROC-AUC by up to 7.7% over fine-tuning on 8 molecular property prediction datasets.

## Executive Summary
This paper addresses the challenge of adapting pretrained molecular Graph Neural Networks (GNNs) to specific target tasks using auxiliary learning. The authors propose novel adaptation strategies that go beyond simple fine-tuning by learning to selectively incorporate auxiliary task gradients based on their alignment with target task gradients. The key innovations are RCGrad, which rotates conflicting gradients to align them constructively, and BLO+RCGrad, which combines bi-level optimization for task weighting with gradient rotation. Experimental results on eight molecular property prediction datasets show consistent improvements over baseline adaptation methods, particularly on smaller datasets where negative transfer is most problematic.

## Method Summary
The method adapts pretrained molecular GNNs to target tasks by jointly training with auxiliary tasks while selectively incorporating their gradients. RCGrad rotates conflicting auxiliary task gradients to align non-conflicting components with the target task direction, while gradient scaling dynamically adjusts gradient magnitudes to prevent dominance by any single task. BLO+RCGrad combines bi-level optimization (learning task weights via validation loss) with RCGrad's gradient alignment. The framework uses cosine similarity to measure gradient alignment between tasks, then applies learned rotations and scalings during parameter updates to maximize beneficial information transfer while minimizing interference.

## Key Results
- RCGrad and BLO+RCGrad achieve up to 7.7% improvement in ROC-AUC over fine-tuning baselines
- Performance gains are most pronounced on smaller datasets where negative transfer is most problematic
- The methods outperform competing approaches including GCS, GNS, and PCGrad across multiple molecular property prediction tasks
- Results are consistent across two different pretrained GNN architectures (Sup-CP and Sup)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RCGrad improves performance by learning to align conflicting task gradients through rotation rather than simply discarding them
- Mechanism: Instead of projecting conflicting gradients to zero (like GCS), RCGrad learns a rotation matrix that reorients the auxiliary task gradient before projection, allowing retention of non-conflicting components while aligning them with the target task direction
- Core assumption: The non-conflicting component of auxiliary task gradients contains valuable information that can benefit the target task if properly aligned
- Evidence anchors:
  - [abstract]: "Rotation of Conflicting Gradients (RCGrad), that learns to align conflicting auxiliary task gradients through rotation"
  - [section]: "RCGrad only negates the component of the conflicting gradient that is completely opposite to the target task gradient. Additionally, RCGrad explicitly learns how much of the non-conflicting component should be incorporated"
  - [corpus]: Found related work on synthetic task augmentation and few-shot learning, suggesting gradient alignment strategies are an active research area
- Break condition: If the non-conflicting components of auxiliary task gradients are themselves harmful or misleading, the rotation mechanism could amplify negative transfer

### Mechanism 2
- Claim: BLO+RCGrad combines the strengths of task weight learning and gradient alignment to achieve superior performance
- Mechanism: BLO learns task weights by minimizing validation loss through bi-level optimization, while RCGrad handles gradient conflicts through learned rotation and scaling, creating a dual optimization framework
- Core assumption: Task weights learned via validation loss optimization can effectively guide the gradient alignment process to improve generalization
- Evidence anchors:
  - [abstract]: "BLO+RCGrad (bi-level optimization with gradient rotation) that learn to align and scale task gradients to mitigate negative transfer"
  - [section]: "BLO+RCGrad leverages the learned scaling factors s via BLO to guide the gradient surgery process introduced by RCGrad"
  - [corpus]: Found work on bi-level contrastive learning for molecular representations, supporting the use of bi-level optimization in this domain
- Break condition: If the approximation of hyper-gradients in the bi-level optimization is too noisy (especially with limited data), the learned weights may not effectively guide gradient alignment

### Mechanism 3
- Claim: Gradient scaling addresses magnitude imbalances between task gradients to prevent dominance by any single task
- Mechanism: Dynamically adjusts auxiliary task gradient magnitudes relative to the target task gradient norm during updates, ensuring balanced contribution from all tasks
- Core assumption: Large differences in gradient magnitudes between tasks can cause certain tasks to dominate parameter updates, leading to suboptimal learning
- Evidence anchors:
  - [section]: "if the gradient of an auxiliary task is much larger than that of the target task, Θ updates will be most dominated by such auxiliary tasks, thereby potentially resulting in worse target performance"
  - [section]: "We use a simple gradient scaling to dynamically adjust the influence of auxiliary tasks during updates of Θ"
  - [corpus]: Found work on meta-balance for multi-task recommendations via adapting gradient magnitudes, suggesting this is a recognized challenge
- Break condition: If gradient magnitudes naturally vary significantly due to task difficulty differences, uniform scaling might over- or under-correct for legitimate task-specific requirements

## Foundational Learning

- Concept: Gradient similarity as relatedness measure
  - Why needed here: Determines which auxiliary tasks are beneficial versus harmful to the target task by measuring alignment between task gradients
  - Quick check question: What does high gradient cosine similarity between an auxiliary task and target task indicate about their relationship?

- Concept: Bi-level optimization and implicit differentiation
  - Why needed here: Enables learning task weights by optimizing for validation performance rather than just training loss, improving generalization
  - Quick check question: In the bi-level optimization framework, what is being optimized in the upper level versus the lower level?

- Concept: Gradient surgery and projection operations
  - Why needed here: Provides mathematical tools for manipulating task gradients to resolve conflicts and align them constructively
  - Quick check question: How does orthogonal projection of a gradient onto the normal plane of another gradient mathematically resolve their conflict?

## Architecture Onboarding

- Component map: Pretrained GNN -> Task-specific projection layers -> Multiple task heads (target + auxiliary) -> Combined loss with adaptive weighting -> Parameter updates with gradient manipulation (scaling/rotation/projection)
- Critical path: Input molecules -> GNN forward pass -> Task head predictions -> Task losses -> Gradient computation -> Gradient manipulation (scaling/rotation/projection) -> Parameter updates
- Design tradeoffs: Balancing computational complexity of bi-level optimization against potential performance gains; choosing between discarding conflicting gradients versus aligning them through rotation
- Failure signatures: Degradation in target task performance despite auxiliary task presence; unstable training dynamics; poor generalization on held-out data
- First 3 experiments:
  1. Implement RCGrad with one auxiliary task and verify gradient rotation occurs as expected
  2. Compare RCGrad against GCS and GNS on a single dataset with known gradient conflicts
  3. Test BLO+RCGrad with simplified bi-level optimization (fewer unrolling steps) to establish baseline performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed adaptation strategies (RCGrad, BLO+RCGrad) perform when fine-tuning pretrained molecular GNNs on regression tasks instead of classification tasks?
- Basis in paper: [inferred] The paper only evaluates the methods on classification datasets from MoleculeNet and mentions "We will further investigate the benefit of adapting GNNs to diverse downstream molecular regression tasks" as future work.
- Why unresolved: The paper's experiments are limited to classification tasks, and the authors explicitly state this as a direction for future research without providing any results.
- What evidence would resolve it: Experimental results showing the performance of RCGrad and BLO+RCGrad on regression datasets (e.g., ESOL, FreeSolv, Lipophilicity from MoleculeNet) would provide concrete evidence of their effectiveness on regression tasks.

### Open Question 2
- Question: What is the impact of using different numbers of auxiliary tasks on the performance of RCGrad and BLO+RCGrad?
- Basis in paper: [explicit] The paper conducts experiments using all five auxiliary tasks and a subset of three auxiliary tasks (AM, IG, MP), showing varying performance. However, it does not systematically explore the impact of using different numbers of auxiliary tasks.
- Why unresolved: While the paper shows some comparison between using all tasks versus a subset, it does not provide a systematic study of how the number of auxiliary tasks affects performance. The authors do not explore intermediate numbers of tasks or provide a clear rationale for task selection.
- What evidence would resolve it: Experiments varying the number of auxiliary tasks (e.g., using 1, 2, 3, 4, and 5 tasks) and analyzing the resulting performance would provide insights into the optimal number of auxiliary tasks for these adaptation strategies.

### Open Question 3
- Question: How do RCGrad and BLO+RCGrad perform when adapted to pretrained GNNs with different pretraining objectives (e.g., pretraining on different self-supervised tasks or supervised tasks)?
- Basis in paper: [explicit] The paper uses two pretrained GNNs (Sup-CP and Sup) with different pretraining objectives and shows that performance varies depending on the pretrained GNN. However, it does not explore a wider range of pretraining objectives or systematically compare their impact.
- Why unresolved: The paper only uses two pretrained GNNs, which may not be representative of the full range of possible pretraining objectives. The authors do not explore how the choice of pretraining objective affects the effectiveness of the adaptation strategies.
- What evidence would resolve it: Experiments adapting RCGrad and BLO+RCGrad to pretrained GNNs with various pretraining objectives (e.g., different combinations of self-supervised tasks, different supervised tasks) and comparing their performance would provide insights into the generalizability of these adaptation strategies across different pretraining scenarios.

## Limitations

- The theoretical guarantees of gradient rotation versus projection remain unclear, with limited analysis of when the assumption about non-conflicting component utility breaks down
- The BLO+RCGrad approach introduces significant computational overhead through bi-level optimization without ablation studies quantifying individual component contributions
- Evaluation focuses on relatively small molecular property prediction datasets, leaving uncertainty about scalability to larger, more complex molecular tasks

## Confidence

- **High Confidence**: The core observation that auxiliary task gradients can be selectively aligned or scaled to improve target task performance is well-supported by experimental results across multiple datasets
- **Medium Confidence**: The specific mechanisms of RCGrad and BLO+RCGrad are empirically validated, but the theoretical understanding of why gradient rotation is superior to projection remains incomplete
- **Medium Confidence**: The choice of auxiliary tasks (AM, CP, EP, IG, MP) is justified through preliminary analysis, but the paper doesn't explore whether other auxiliary tasks might yield better performance

## Next Checks

1. **Ablation Study**: Perform systematic ablation of BLO+RCGrad components to quantify the individual contributions of task weight learning versus gradient rotation, particularly on the smallest datasets where improvements are most dramatic

2. **Gradient Analysis**: Conduct detailed analysis of gradient similarity distributions across datasets to identify patterns in which task pairs benefit from RCGrad versus those that don't, testing the assumption about non-conflicting component utility

3. **Scalability Test**: Evaluate the methods on larger molecular datasets (e.g., >100K molecules) to assess computational overhead of BLO+RCGrad and determine whether performance gains scale with dataset size