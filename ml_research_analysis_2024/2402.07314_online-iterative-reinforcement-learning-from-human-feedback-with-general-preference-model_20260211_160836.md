---
ver: rpa2
title: Online Iterative Reinforcement Learning from Human Feedback with General Preference
  Model
arxiv_id: '2402.07314'
source_url: https://arxiv.org/abs/2402.07314
tags:
- preference
- arxiv
- learning
- policy
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates reinforcement learning from human feedback
  (RLHF) under a general preference oracle without assuming a reward function or the
  Bradley-Terry model. The authors formulate the problem as a reverse-KL regularized
  minimax game between two large language models and propose sample-efficient algorithms
  for both offline and online settings.
---

# Online Iterative Reinforcement Learning from Human Feedback with General Preference Model

## Quick Facts
- arXiv ID: 2402.07314
- Source URL: https://arxiv.org/abs/2402.07314
- Reference count: 40
- Authors: Chenlu Ye, Wei Xiong, Yuheng Zhang, Hanze Dong, Nan Jiang, Tong Zhang
- Key outcome: Proposed method strictly more general than reward-based RLHF and captures non-transitive preferences

## Executive Summary
This paper addresses reinforcement learning from human feedback (RLHF) without relying on traditional reward functions or the Bradley-Terry model for preference learning. The authors formulate the problem as a reverse-KL regularized minimax game between two large language models, establishing theoretical guarantees for both offline and online settings. Their approach demonstrates improved performance over existing methods like DPO and IPO, particularly in capturing complex preference structures including non-transitive preferences.

The work bridges the gap between reward-based RLHF and preference-based learning by introducing a more general framework that can handle arbitrary preference oracles. The online iterative algorithm shows state-of-the-art results in aligning language models with human preferences while maintaining theoretical sample efficiency guarantees.

## Method Summary
The paper proposes a novel formulation of RLHF as a reverse-KL regularized minimax game between two LLM models - one serving as the policy and another as the preference model. The method operates without assuming either a reward function or the Bradley-Terry model, making it strictly more general than existing approaches. For the offline setting, the authors provide a sample-efficient algorithm with finite-sample guarantees, while the online setting features an iterative algorithm that alternates between policy updates and preference model refinement. The theoretical analysis establishes convergence and sample efficiency bounds, and practical implementations demonstrate improved performance over baselines across multiple benchmarks.

## Key Results
- Demonstrated strict generalization over reward-based RLHF methods
- Achieved state-of-the-art results in language model alignment tasks
- Theoretical guarantees established for both offline and online settings
- Successfully captured non-transitive preference structures

## Why This Works (Mechanism)
The method works by treating preference learning as a minimax game between policy and preference models, allowing it to learn directly from pairwise comparisons without intermediate reward modeling. This approach avoids the pitfalls of reward misspecification while maintaining computational tractability. The reverse-KL regularization ensures stable learning dynamics and prevents mode collapse. The iterative nature of the online algorithm allows for continuous refinement of both the policy and preference model, leading to improved alignment with human preferences over time.

## Foundational Learning
- **Minimax optimization**: Required for understanding the game-theoretic formulation between policy and preference models. Quick check: Verify convergence properties of saddle-point optimization algorithms.
- **Reverse-KL regularization**: Provides stability in learning and prevents mode collapse. Quick check: Analyze the effect of different regularization strengths on learning dynamics.
- **Preference learning without reward functions**: Enables direct learning from human feedback without reward misspecification. Quick check: Test performance with non-transitive preference structures.
- **Large language model fine-tuning**: Essential for practical implementation using existing LLM architectures. Quick check: Validate computational efficiency of fine-tuning procedures.

## Architecture Onboarding

**Component Map**: Preference Oracle -> Preference Model -> Policy Model -> (back to) Preference Oracle

**Critical Path**: The core iteration involves collecting pairwise preferences from the oracle, updating the preference model based on these comparisons, then using the refined preference model to guide policy updates through the minimax objective.

**Design Tradeoffs**: The approach trades computational complexity (maintaining two LLM models) for improved expressiveness and robustness to reward misspecification. The reverse-KL regularization adds stability but may slow convergence compared to direct optimization methods.

**Failure Signatures**: Performance degradation occurs when the preference model becomes inconsistent with the oracle, when the policy model overfits to early preference data, or when the minimax optimization fails to converge due to poor initialization.

**3 First Experiments**:
1. Test the offline algorithm on a synthetic preference dataset with known ground truth to validate theoretical guarantees
2. Compare convergence rates of the online iterative algorithm versus one-shot offline approaches
3. Evaluate sensitivity to preference noise by introducing controlled corruption in the preference oracle

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the main text, but several implications arise from the limitations section regarding the practicality of the general preference oracle assumption and the scalability of the approach to non-language domains.

## Limitations
- Theoretical analysis assumes idealized preference oracles without accounting for real-world noise
- Practical implementation requires access to two separate LLM models, limiting accessibility
- Experiments focus primarily on language model alignment, leaving applicability to other domains uncertain
- Comparison to baselines limited to specific benchmarks without broader generalization testing

## Confidence

**High**: Theoretical framework and finite-sample guarantees
**Medium**: Offline algorithm performance, Online iterative algorithm performance, Comparison to existing methods
**Low**: Generalizability to non-language domains

## Next Checks

1. Test the online iterative algorithm on domains beyond language modeling (e.g., robotics control, recommendation systems) to assess generalizability
2. Conduct ablation studies on the number of preference queries needed versus performance gains to validate claimed sample efficiency
3. Implement the algorithm with different types of preference noise models to assess robustness to imperfect human feedback