---
ver: rpa2
title: 'BAMO at SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common Sense'
arxiv_id: '2406.04947'
source_url: https://arxiv.org/abs/2406.04947
tags:
- round
- task
- puzzles
- each
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an approach to SemEval 2024 Task 9, BRAINTEASER,
  which evaluates language models' ability to perform lateral thinking through creative
  problem-solving. The authors fine-tune BERT and RoBERTa models and apply zero-shot
  prompting with Chain of Thought reasoning across six large language models.
---

# BAMO at SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common Sense

## Quick Facts
- arXiv ID: 2406.04947
- Source URL: https://arxiv.org/abs/2406.04947
- Reference count: 11
- Achieves 85% overall accuracy on sentence puzzles using ReConcile multi-agent consensus technique

## Executive Summary
This paper presents an approach to SemEval 2024 Task 9 BRAINTEASER, which evaluates language models' ability to perform lateral thinking through creative problem-solving. The authors fine-tune BERT and RoBERTa models and apply zero-shot prompting with Chain of Thought reasoning across six large language models. Their main innovation is the ReConcile technique, which employs a multi-agent "round table" consensus approach where multiple models iteratively discuss and refine their answers. Using three selected models (Mixtral8x7b, GPT3.5, and Claude), they achieve an overall accuracy of 85% on sentence puzzles through collaborative reasoning. The ReConcile method shows progressive improvement across discussion rounds, demonstrating that multiple agents can effectively complement each other to solve lateral thinking puzzles that challenge common sense reasoning.

## Method Summary
The authors employ a multi-faceted approach combining fine-tuning and zero-shot prompting. They fine-tune BERT-Base and RoBERTa-Large models on the BRAINTEASER dataset using cross-entropy loss with standard hyperparameters. For zero-shot prompting, they apply Chain of Thought (CoT) reasoning to six large language models including Mixtral8x7b, GPT3.5, Claude, and others. The key innovation is ReConcile, a multi-agent consensus technique where selected models iteratively discuss and refine their answers over multiple rounds. Models generate initial responses with confidence scores, then engage in collaborative discussion to reach consensus, with the final answer selected based on cumulative confidence across rounds.

## Key Results
- ReConcile achieves 85% overall accuracy on sentence puzzles using three selected models
- Consensus accuracy improves by 0.3 to 0.5 points after two discussion rounds compared to individual model performance
- RoBERTa-Large fine-tuned model achieves 0.766 overall accuracy on sentence puzzles
- Multi-agent consensus outperforms individual model baselines across all test scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The ReConcile technique achieves higher accuracy through iterative consensus building among diverse models.
- Mechanism: Multiple models generate initial responses with confidence scores, then iteratively discuss and refine their answers through a round table approach, ultimately selecting the answer with highest cumulative confidence.
- Core assumption: Models with similar baseline performance can complement each other's weaknesses when reasoning collaboratively.
- Evidence anchors:
  - [abstract] "Using three selected models (Mixtral8x7b, GPT3.5, and Claude), they achieve an overall accuracy of 85% on sentence puzzles through collaborative reasoning."
  - [section] "We repeated this process for two discussion rounds... At the conclusion of round 2, the consensus overall accuracy stands at 0.758, which is 0.3 to 0.5 points higher than the initial round results of all three models."
  - [corpus] Weak - no direct corpus evidence for this specific consensus mechanism.

### Mechanism 2
- Claim: Chain of Thought prompting improves model performance on lateral thinking tasks by forcing detailed reasoning.
- Mechanism: Models are prompted to provide step-by-step reasoning for their answers rather than just selecting an option, which helps them focus on details and reduce errors.
- Core assumption: Breaking down reasoning into explicit steps helps models avoid surface-level pattern matching and engage more deeply with the problem structure.
- Evidence anchors:
  - [abstract] "Next, we employ a Chain of Thought (CoT) zero-shot prompting approach with 6 large language models"
  - [section] "we compel the model to analyze and provide step-by-step reasoning for its answer instead of simply providing a correct option alone. This approach helps the model focus more on details and answer questions with fewer errors."
  - [corpus] Weak - no direct corpus evidence for this specific lateral thinking application.

### Mechanism 3
- Claim: Fine-tuning transformer models on BRAINTEASER dataset improves performance compared to zero-shot approaches.
- Mechanism: BERT and RoBERTa models are fine-tuned using cross-entropy loss with standard hyperparameters, adapting them to the specific task of lateral thinking puzzles.
- Core assumption: Domain-specific fine-tuning can improve model performance even when the task requires creative thinking beyond standard reasoning patterns.
- Evidence anchors:
  - [section] "We utilize BERT-Base and RoBERTa-Large models... These models are finetuned using the Hugging Face trainer... The best performance is achieved by RoBERTa for both sentence and word puzzles."
  - [table 2] Shows RoBERTa Large Finetune achieves 0.766 overall accuracy on sentence puzzles.
  - [corpus] Weak - no direct corpus evidence for this specific dataset.

## Foundational Learning

- Concept: Chain of Thought reasoning
  - Why needed here: BRAINTEASER questions require models to explain their reasoning process, not just provide answers. CoT prompting helps models engage with the creative problem-solving required.
  - Quick check question: What's the difference between standard prompting and Chain of Thought prompting in the context of lateral thinking puzzles?

- Concept: Multi-agent consensus systems
  - Why needed here: No single model performs perfectly on all question types. ReConcile leverages diverse model perspectives to improve overall accuracy.
  - Quick check question: Why does ReConcile require models with similar baseline performance rather than mixing high and low performers?

- Concept: Fine-tuning transformer models for multiple-choice tasks
  - Why needed here: Standard pre-trained models need adaptation to the specific format and reasoning patterns of BRAINTEASER questions.
  - Quick check question: How does the input format [CLS] Q [SEP] Choicei [SEP] help BERT/RoBERTa understand multiple-choice questions?

## Architecture Onboarding

- Component map: Input processing → Fine-tuning (BERT/RoBERTa) → Zero-shot prompting (6 LLMs) → ReConcile consensus system → Output aggregation
- Critical path: ReConcile rounds → Consensus calculation → Final answer selection
- Design tradeoffs: Using multiple discussion rounds improves accuracy but increases latency; selecting similar-performing models ensures positive collaboration but may miss complementary strengths.
- Failure signatures: Consensus accuracy stalls or decreases across rounds; one model consistently dominates discussions; confidence scores don't correlate with accuracy.
- First 3 experiments:
  1. Run ReConcile with models of widely varying performance to observe negative effects
  2. Test different numbers of discussion rounds to find optimal convergence point
  3. Compare consensus accuracy against individual model best performers to measure collaborative benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact impact of model diversity on the ReConcile technique's performance?
- Basis in paper: Explicit
- Why unresolved: The paper mentions that models should have "roughly equal performance" for effective collaboration, but doesn't quantify how different levels of model diversity affect consensus quality or accuracy gains.
- What evidence would resolve it: Systematic experiments varying model architectures, capabilities, and initial accuracies while measuring consensus performance across multiple rounds.

### Open Question 2
- Question: What is the optimal number of discussion rounds for the ReConcile method before diminishing returns set in?
- Basis in paper: Inferred
- Why unresolved: The paper reports results for two discussion rounds showing improvement, but doesn't establish whether more rounds would continue improving accuracy or when the method reaches its maximum effectiveness.
- What evidence would resolve it: Testing the ReConcile method with 3-5+ discussion rounds and measuring accuracy plateaus or degradation over time.

### Open Question 3
- Question: How does the ReConcile technique generalize to different types of reasoning tasks beyond BRAINTEASER puzzles?
- Basis in paper: Inferred
- Why unresolved: The method is only tested on sentence puzzles from one specific dataset, leaving unclear whether the collaborative approach works equally well for standard NLP tasks, mathematical reasoning, or other creative problem-solving domains.
- What evidence would resolve it: Applying ReConcile to diverse reasoning benchmarks (e.g., GSM8K, ARC, LogiQA) and comparing performance against single-model baselines and other ensemble methods.

## Limitations

- The ReConcile technique is specifically validated only on BRAINTEASER puzzles and may not generalize to other types of reasoning tasks
- Model selection criteria for ReConcile are based on overall accuracy without systematic analysis of which model combinations work best for different question types
- Confidence scoring mechanisms vary across models, potentially introducing bias in the consensus calculation without standardized confidence evaluation

## Confidence

- **High Confidence**: The observation that collaborative reasoning improves accuracy over individual model performance (85% achieved vs. individual model scores)
- **Medium Confidence**: The effectiveness of Chain of Thought prompting for lateral thinking tasks, supported by comparative results but lacking ablation studies
- **Low Confidence**: The specific claim that 2-3 discussion rounds are optimal for ReConcile, as this appears empirically determined without systematic exploration of the parameter space

## Next Checks

1. **Model Diversity Experiment**: Test ReConcile with models having significantly different baseline performances to quantify the impact of model similarity on consensus effectiveness and identify failure modes when mixing high and low performers.

2. **Round Number Optimization**: Systematically vary the number of discussion rounds (1, 2, 3, 4, 5) to determine the optimal convergence point and identify when additional rounds cease to provide accuracy improvements or potentially degrade performance.

3. **Task Transferability Assessment**: Apply the ReConcile technique to other creative reasoning benchmarks beyond BRAINTEASER to evaluate whether the multi-agent consensus approach generalizes to different types of lateral thinking or unconventional reasoning tasks.