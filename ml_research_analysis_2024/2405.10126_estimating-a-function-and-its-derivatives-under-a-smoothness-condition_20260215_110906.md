---
ver: rpa2
title: Estimating a Function and Its Derivatives Under a Smoothness Condition
arxiv_id: '2405.10126'
source_url: https://arxiv.org/abs/2405.10126
tags: []
core_contribution: 'This paper studies the problem of estimating an unknown function
  and its partial derivatives from noisy data, assuming only that the function is
  smooth in the sense of having square integrable derivatives of order m. The author
  proposes two estimation methods: one that minimizes the sum of squared errors subject
  to a smoothness constraint (Problem A), and another that minimizes smoothness subject
  to a bound on the average squared error (Problem B).'
---

# Estimating a Function and Its Derivatives Under a Smoothness Condition
## Quick Facts
- arXiv ID: 2405.10126
- Source URL: https://arxiv.org/abs/2405.10126
- Authors: Eunji Lim
- Reference count: 40
- Key outcome: Proposes two convex quadratic program estimators for smooth functions and their derivatives, with consistency and convergence rate guarantees

## Executive Summary
This paper addresses the problem of estimating an unknown function and its partial derivatives from noisy observations, assuming the function has square integrable derivatives up to order m. The author develops two complementary estimation approaches: Problem A minimizes squared error subject to smoothness constraints, while Problem B minimizes smoothness subject to error bounds. Both methods are formulated as convex quadratic programs, making them computationally tractable. The theoretical analysis establishes consistency and convergence rates for these estimators, and numerical experiments demonstrate their effectiveness on a stock option pricing problem where accurate derivative estimation is crucial for risk management.

## Method Summary
The proposed methodology centers on two optimization formulations. Problem A finds function estimates that minimize the sum of squared residuals while constraining the function's smoothness (measured by integrated squared derivatives) below a threshold. Problem B takes the dual approach, minimizing the smoothness measure subject to keeping the average squared error below a specified bound. Both problems are shown to be equivalent to solving convex quadratic programs, which can be efficiently computed using standard optimization software. The smoothness assumption is formalized through the Sobolev space $W^{m,2}$, requiring square integrable derivatives up to order m. This framework allows simultaneous estimation of the function value and its derivatives without requiring separate derivative approximation steps.

## Key Results
- Both Problems A and B are computationally equivalent to convex quadratic programs, ensuring efficient solution methods
- The estimators achieve consistency and derive explicit convergence rates under standard regularity conditions
- Numerical experiments on option pricing demonstrate superior performance compared to penalized least squares with cross-validation, particularly for derivative estimation

## Why This Works (Mechanism)
The effectiveness of these estimators stems from the natural connection between smoothness regularization and derivative estimation. By incorporating Sobolev space constraints directly into the optimization problem, the method ensures that the estimated function not only fits the observed data but also possesses the required smoothness properties. The convexity of the resulting quadratic programs guarantees global optimality and enables efficient computation. The dual formulations (Problems A and B) provide flexibility in choosing between data fidelity and smoothness based on the specific application requirements. The quadratic programming framework also naturally handles the coupling between function values and derivatives through the design matrix structure.

## Foundational Learning
- Sobolev space $W^{m,2}$: Why needed - provides the mathematical framework for defining smoothness through square integrable derivatives; Quick check - verify that the target function has derivatives up to order m in $L^2$
- Convex quadratic programming: Why needed - ensures global optimality and efficient computation; Quick check - confirm positive semi-definiteness of the Hessian matrix
- Reproducing kernel Hilbert spaces: Why needed - provides theoretical foundation for the estimator's properties; Quick check - identify the kernel associated with the Sobolev space
- Consistency and convergence rates: Why needed - establishes theoretical guarantees for the estimator's performance; Quick check - verify that the error bounds decrease appropriately with sample size
- Derivative estimation from function values: Why needed - distinguishes this work from standard function estimation; Quick check - confirm that the design matrix structure enables simultaneous function and derivative estimation

## Architecture Onboarding
The estimation framework consists of the following components: Data preprocessing -> Design matrix construction -> Quadratic program formulation -> Solution computation -> Derivative extraction. The critical path involves building the design matrix that encodes both function values and derivative relationships, formulating the quadratic program with appropriate constraints, and solving it using standard optimization algorithms. Design tradeoffs include choosing between Problems A and B based on whether data fidelity or smoothness is prioritized, and selecting the appropriate smoothness order m. Failure signatures include numerical instability when the design matrix is ill-conditioned or when the smoothness assumptions are severely violated. Three first experiments: 1) Test on a simple polynomial function with known derivatives, 2) Evaluate performance under varying noise levels, 3) Compare computational time with increasing sample size.

## Open Questions the Paper Calls Out
None

## Limitations
- Relies heavily on exact smoothness assumptions that may be difficult to verify in practice
- Assumes independent and identically distributed noise structure
- Computational requirements may be challenging for very large datasets or high-dimensional problems

## Confidence
- High confidence in the convexity and computability of Problems A and B as quadratic programs
- Medium confidence in the theoretical consistency and convergence rate proofs
- Low confidence in the generalizability of numerical results from the stock option pricing example

## Next Checks
1. Validate the numerical performance on additional test functions with known derivatives across different smoothness classes
2. Test the estimator's robustness to non-i.i.d. noise and violations of the smoothness assumptions
3. Compare computational efficiency and scalability with alternative derivative estimation methods on large datasets