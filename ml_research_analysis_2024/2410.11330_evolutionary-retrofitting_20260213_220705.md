---
ver: rpa2
title: Evolutionary Retrofitting
arxiv_id: '2410.11330'
source_url: https://arxiv.org/abs/2410.11330
tags:
- afterlearner
- optimization
- section
- loss
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'AfterLearnER optimizes small sets of parameters or hyperparameters
  of fully trained ML models using non-differentiable, exact error signals without
  requiring gradient computation. The approach is demonstrated across eight diverse
  domains: depth sensing, speech resynthesis, Doom gameplay, code translation, and
  interactive image generation.'
---

# Evolutionary Retrofitting

## Quick Facts
- arXiv ID: 2410.11330
- Source URL: https://arxiv.org/abs/2410.11330
- Authors: Mathurin Videau; Mariia Zameshina; Alessandro Leite; Laurent Najman; Marc Schoenauer; Olivier Teytaud
- Reference count: 31
- Primary result: AfterLearnER optimizes small sets of parameters or hyperparameters of fully trained ML models using non-differentiable, exact error signals without requiring gradient computation.

## Executive Summary
AfterLearnER is a method for improving fully trained machine learning models by optimizing small sets of parameters (â„µ-parameters) using evolutionary algorithms and black-box optimization. Unlike traditional fine-tuning or hyperparameter optimization, AfterLearnER works with non-differentiable loss functions and requires no gradient computation. The approach has been demonstrated across eight diverse domains including depth sensing, speech resynthesis, Doom gameplay, code translation, and interactive image generation. Results show consistent improvements over baselines with only dozens to hundreds of scalar feedbacks.

## Method Summary
AfterLearnER applies evolutionary optimization to refine fully trained models by optimizing carefully chosen parameters or hyperparameters with respect to non-differentiable error signals. The method operates in a black-box optimization setting, allowing direct optimization of actual performance metrics (like word error rate or kill/death ratio) rather than differentiable proxies. It uses black-box optimization algorithms (e.g., NGOpt, Discrete(1+1), DiagonalCMA) without gradient computation, and selects optimal â„µ-parameters based on validation performance. The approach supports both offline and online modes, with online applications including real-time user preference incorporation in GANs and latent diffusion models.

## Key Results
- AfterLearnER improves model alignment with real-world objectives by directly optimizing actual performance metrics rather than differentiable proxies.
- The method demonstrates success across eight diverse domains: depth sensing, speech resynthesis, Doom gameplay, code translation, and interactive image generation.
- Theoretical analysis shows limited overfitting risk, and experiments demonstrate success with only dozens to hundreds of scalar feedbacksâ€”orders of magnitude less than typical approaches.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Small sets of parameters or hyperparameters can be effectively optimized using evolutionary methods after full model training.
- Mechanism: AfterLearnER uses black-box optimization to refine a small subset of model parameters (â„µ-parameters) by minimizing a user-defined, potentially non-differentiable loss function on a validation set.
- Core assumption: The chosen â„µ-parameters are impactful enough to improve model performance without requiring retraining of the entire model.
- Evidence anchors:
  - [abstract] "optimizes small sets of parameters or hyperparameters of fully trained ML models"
  - [section] "AfterLearnER tunes the 6 normalization parameters... for optimizing the â„µ-loss"
  - [corpus] Weak - related work focuses on embedding retrofitting and evolutionary exploration but not this specific post-training optimization framework.
- Break condition: If â„µ-parameters are too small to affect model behavior or too large to be efficiently optimized by black-box methods.

### Mechanism 2
- Claim: Non-differentiable loss functions can be optimized effectively without gradient computation.
- Mechanism: AfterLearnER operates in a black-box optimization setting, allowing direct optimization of actual performance metrics (like word error rate or kill/death ratio) rather than differentiable proxies.
- Core assumption: Black-box optimization algorithms can find good solutions in the â„µ-parameter space without gradient information.
- Evidence anchors:
  - [abstract] "without requiring gradient computation"
  - [section] "optimizes any specific loss function, here termed the â„µ-loss, that can be different from the one used during the model training"
  - [corpus] Weak - related work on EA4LLM mentions gradient-free optimization but not specifically for non-differentiable losses in this context.
- Break condition: If the â„µ-loss landscape is too rugged or noisy for black-box methods to find improvements.

### Mechanism 3
- Claim: Limited overfitting risk when using black-box optimization with validation-based selection.
- Mechanism: Theoretical analysis shows that selecting â„µ-parameters based on validation performance has bounded generalization error, especially with multiple independent optimization runs.
- Core assumption: The validation set is representative enough of the test distribution to prevent overfitting.
- Evidence anchors:
  - [section] "Theoretical Analysis of Overfitting Risk" with Bonferroni correction and branching factor calculations
  - [section] "Increasing the number of runs (parameter ð‘˜), or the parallelism ðœ†, makes the code closer to random search, and less prone to overfitting"
  - [corpus] Missing - no direct evidence in related work about overfitting bounds for this specific approach.
- Break condition: If the validation set is too small or unrepresentative, leading to poor generalization.

## Foundational Learning

- Concept: Black-box optimization
  - Why needed here: AfterLearnER relies on evolutionary algorithms that only need function evaluations, not gradients
  - Quick check question: What is the key difference between black-box optimization and gradient-based optimization?

- Concept: Validation-based model selection
  - Why needed here: AfterLearnER selects optimal â„µ-parameters based on performance on a validation set
  - Quick check question: How does Bonferroni correction apply to the risk of overfitting in this context?

- Concept: Hyperparameter optimization
  - Why needed here: Understanding the distinction between traditional HPO and AfterLearnER's approach
  - Quick check question: What makes AfterLearnER different from standard hyperparameter tuning?

## Architecture Onboarding

- Component map:
  Trained ML model (frozen) -> â„µ-parameter subset -> Black-box optimizer -> Validation set -> â„µ-loss function -> Optimized model

- Critical path:
  1. Load fully trained model
  2. Identify â„µ-parameters to optimize
  3. Define â„µ-loss function
  4. Run black-box optimizer on validation set
  5. Apply optimized â„µ-parameters to model
  6. (Optional) Integrate online feedback loop

- Design tradeoffs:
  - Small vs. large â„µ-parameter space: smaller is faster but may have less impact
  - Differentiable vs. non-differentiable â„µ-loss: non-differentiable allows direct optimization of real metrics
  - Single vs. multiple optimization runs: more runs reduce overfitting risk but increase computation

- Failure signatures:
  - No improvement after optimization: â„µ-parameters may be too small or loss function poorly defined
  - Worsening performance: overfitting to validation set or poor optimizer choice
  - Extremely slow convergence: â„µ-parameter space too large for black-box methods

- First 3 experiments:
  1. Depth sensing with MiDaS model: Optimize 6 normalization parameters using threshold-based loss
  2. Speech resynthesis: Tune 2 hard-coded parameters using word error rate
  3. Doom gameplay: Adjust 35 output layer weights using kill/death ratio

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical limit on the number of scalar feedbacks needed for effective AfterLearnER optimization across different domains?
- Basis in paper: [explicit] The paper demonstrates that AfterLearnER works with "a few dozen to a few hundred scalars" but seeks to understand the minimum viable feedback count.
- Why unresolved: The experiments show effectiveness across different applications but don't establish a theoretical lower bound or explore the relationship between problem complexity and minimum feedback requirements.
- What evidence would resolve it: Systematic experiments varying feedback quantity across diverse domains while measuring optimization quality, or theoretical analysis establishing relationships between problem dimensionality, latent space complexity, and minimum feedback requirements.

### Open Question 2
- Question: How does AfterLearnER performance scale when optimizing larger sets of parameters beyond the small, carefully chosen subsets used in the experiments?
- Basis in paper: [explicit] The paper states that "black-box optimization algorithms do not scale up very well in general" and restricts â„µ-parameters to small subsets.
- Why unresolved: The experiments deliberately use small parameter sets for tractability, leaving open questions about performance degradation with larger parameter sets and potential strategies for scaling.
- What evidence would resolve it: Empirical studies testing AfterLearnER on progressively larger parameter sets, comparison with alternative optimization approaches for larger parameter spaces, and development of hybrid approaches combining black-box optimization with gradient-based methods.

### Open Question 3
- Question: What is the optimal strategy for selecting which parameters to include in the â„µ-parameter set for maximum impact?
- Basis in paper: [explicit] The paper recommends selecting "small families of parameters that intersect all paths from input to output" but doesn't provide systematic selection criteria.
- Why unresolved: The parameter selection appears to be heuristic in the experiments, and there's no framework for determining optimal â„µ-parameter composition for different model architectures or tasks.
- What evidence would resolve it: Analysis comparing different parameter selection strategies across multiple model types, development of automated methods for identifying high-impact parameter subsets, and theoretical analysis of parameter sensitivity and influence on model outputs.

## Limitations

- The theoretical analysis of overfitting risk, while providing bounds through Bonferroni correction, lacks empirical validation across the tested applications.
- The choice of â„µ-parameters appears somewhat arbitrary in some cases without justification for why specific subsets were chosen.
- The computational efficiency claims are not fully quantified - actual wall-clock time and computational resources required for black-box optimization are not reported.

## Confidence

- **High Confidence**: The core mechanism of using black-box optimization for post-training parameter tuning is technically sound and well-supported by the experimental results across all eight domains.
- **Medium Confidence**: The claim about limited overfitting risk is theoretically justified but lacks comprehensive empirical validation.
- **Low Confidence**: The assertion that AfterLearnER achieves better performance than more complex models while using significantly less data is not fully substantiated.

## Next Checks

1. **Overfitting Analysis**: Conduct systematic experiments varying the size of the validation set and number of optimization runs to empirically validate the theoretical overfitting bounds presented in the paper.

2. **Parameter Sensitivity Study**: Systematically test different â„µ-parameter selections for the same model (e.g., try optimizing different subsets of MiDaS parameters) to determine how critical the initial parameter choice is to success.

3. **Computational Efficiency Benchmarking**: Measure and compare the actual computational resources (GPU hours, wall-clock time) required for AfterLearnER versus full model retraining or traditional fine-tuning approaches across multiple domains.