---
ver: rpa2
title: Leveraging Intra-modal and Inter-modal Interaction for Multi-Modal Entity Alignment
arxiv_id: '2404.17590'
source_url: https://arxiv.org/abs/2404.17590
tags:
- entity
- knowledge
- alignment
- multi-modal
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of multi-modal entity alignment
  (MMEA), which aims to identify equivalent entity pairs across different multi-modal
  knowledge graphs (MMKGs). The authors propose a Multi-Grained Interaction framework
  for Multi-Modal Entity Alignment (MIMEA) that effectively realizes multi-granular
  interaction within the same modality or between different modalities.
---

# Leveraging Intra-modal and Inter-modal Interaction for Multi-Modal Entity Alignment

## Quick Facts
- arXiv ID: 2404.17590
- Source URL: https://arxiv.org/abs/2404.17590
- Reference count: 40
- MIMEA achieves 8.9%, 2.4%, and 0.5% MRR improvements over SoTA at 20%, 50%, and 80% pre-aligned seeds on FB15K-YAGO15K.

## Executive Summary
The paper tackles the problem of multi-modal entity alignment (MMEA) across multi-modal knowledge graphs (MMKGs), proposing the Multi-Grained Interaction framework for Multi-Modal Entity Alignment (MIMEA). MIMEA introduces a structured approach that explicitly weights modality importance and captures both intra-modal and inter-modal interactions using four modules: multi-modal knowledge embedding, probability-guided modal fusion, optimal transport modal alignment, and modal-adaptive contrastive learning. Evaluated on FB15K-DB15K and FB15K-YAGO15K, MIMEA consistently outperforms state-of-the-art baselines, especially in low-resource settings with fewer pre-aligned seeds.

## Method Summary
MIMEA leverages a multi-granular interaction framework to align entities across multi-modal knowledge graphs. It embeds multi-modal knowledge, fuses modalities guided by learned probabilities, aligns modalities via optimal transport, and refines embeddings through contrastive learning adapted to each modality. This approach explicitly distinguishes modality importance and captures both intra-modal (within the same modality) and inter-modal (across modalities) interactions, enabling effective alignment even with limited seed pairs.

## Key Results
- On FB15K-YAGO15K: MIMEA achieves 8.9%, 2.4%, and 0.5% MRR gains over best SoTA at 20%, 50%, and 80% seed ratios, respectively.
- Strong performance in low-resource scenarios with minimal pre-aligned seeds.
- Consistently outperforms baselines on both FB15K-DB15K and FB15K-YAGO15K datasets.

## Why This Works (Mechanism)
MIMEA works by explicitly modeling the importance of each modality and enabling rich interactions between uni-modal and joint-modal embeddings. By using probability-guided fusion and optimal transport, it can dynamically balance and align information across modalities, which is crucial for accurate entity matching. The modal-adaptive contrastive learning further refines embeddings, ensuring that entities with the same semantic meaning are brought closer in the embedding space.

## Foundational Learning
- **Multi-modal Knowledge Embedding**: Embeds entities using structural, textual, and visual features from MMKGs. Needed to represent heterogeneous information in a unified space. Quick check: Can each modality's embedding be retrieved independently?
- **Optimal Transport Modal Alignment**: Uses optimal transport to align embeddings across modalities. Needed to find the best correspondence between entity distributions in different modalities. Quick check: Is the transport plan sparse enough to indicate meaningful matches?
- **Contrastive Learning**: Pulls together embeddings of aligned entities and pushes apart non-aligned ones. Needed to refine alignment quality post-modal fusion. Quick check: Does contrastive loss decrease as training progresses?

## Architecture Onboarding
- **Component Map**: Input MMKG → Multi-modal Knowledge Embedding → Probability-guided Modal Fusion → Optimal Transport Modal Alignment → Modal-adaptive Contrastive Learning → Aligned Embeddings
- **Critical Path**: Embedding generation → modality fusion → alignment → contrastive refinement
- **Design Tradeoffs**: Balances explicit modality weighting (increases interpretability) vs. learned fusion (increases flexibility). Uses optimal transport for robust alignment at higher computational cost.
- **Failure Signatures**: Poor alignment if one modality is too noisy; over-reliance on structural modality if others are weak.
- **First Experiments**: (1) Ablate optimal transport vs. naive alignment; (2) Test with and without contrastive learning; (3) Vary modality importance weights and observe impact on MRR.

## Open Questions the Paper Calls Out
None.

## Limitations
- Performance gains may be inflated due to dense structural information in FB15K-DB15K and FB15K-YAGO15K; results may not generalize to sparser MMKGs.
- Missing ablation studies to isolate the contributions of modal-adaptive contrastive learning vs. optimal transport alignment.
- Claims of low-resource suitability rest mainly on 20% seed results; needs validation on even fewer seeds or noisier datasets.

## Confidence
- Core claim of outperforming SoTA on two datasets: **High**
- Generalizability to other MMKGs: **Medium**
- Relative contribution of each module: **Medium**

## Next Checks
1. Evaluate MIMEA on at least two additional MMKGs with sparser or noisier modalities (e.g., Wikidata with images, or biomedical MMKGs) to test robustness.
2. Perform a detailed ablation study isolating the effects of modal-adaptive contrastive learning and optimal transport modal alignment.
3. Run experiments with fewer than 20% pre-aligned seeds to assess the true low-resource capability of MIMEA.