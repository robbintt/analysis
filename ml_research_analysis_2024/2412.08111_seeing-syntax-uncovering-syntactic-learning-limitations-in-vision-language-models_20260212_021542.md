---
ver: rpa2
title: 'Seeing Syntax: Uncovering Syntactic Learning Limitations in Vision-Language
  Models'
arxiv_id: '2412.08111'
source_url: https://arxiv.org/abs/2412.08111
tags:
- syntactic
- clip
- text
- language
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the syntactic learning capabilities of vision-language
  model (VLM) text encoders and compares them with those of unimodal language models
  (ULMs). The authors use DepProbe to evaluate how well these models encode Universal
  Dependencies (UD) by probing their representations at different layers.
---

# Seeing Syntax: Uncovering Syntactic Learning Limitations in Vision-Language Models

## Quick Facts
- **arXiv ID**: 2412.08111
- **Source URL**: https://arxiv.org/abs/2412.08111
- **Reference count**: 40
- **Primary result**: VLMs show significantly weaker syntactic encoding than ULMs, especially for predicate-argument relations, with contrastive loss alone leading to poor syntactic performance.

## Executive Summary
This paper investigates how well vision-language models (VLMs) encode syntactic information compared to unimodal language models (ULMs). Using the DepProbe tool to evaluate Universal Dependencies (UD) encoding, the authors compare several VLMs (CLIP, FLA V A) with ULMs (RoBERTa, MiniLM) and sentence language models (SLMs). The study reveals that VLMs perform substantially worse than ULMs in capturing syntactic relationships, particularly predicate-argument dependencies. The research identifies the pre-training objective as the primary factor influencing syntactic learning, with contrastive loss alone proving insufficient. Notably, increasing model size or training data does not improve syntactic learning, and most models show peak syntactic knowledge at middle layers, except for CLIP which exhibits declining performance across layers.

## Method Summary
The authors employ the DepProbe tool to evaluate how well vision-language model (VLM) text encoders and unimodal language models (ULMs) encode Universal Dependencies (UD) syntax. They analyze representations at different layers to assess syntactic knowledge, comparing VLMs (CLIP, FLA V A) with ULMs (RoBERTa, MiniLM) and sentence language models (SLMs). The evaluation focuses on predicate-argument relations and word order encoding, examining how these capabilities vary with model architecture, pre-training objectives, and layer depth.

## Key Results
- VLMs perform significantly worse than ULMs in encoding syntactic information, particularly predicate-argument relations
- Pre-training objective has the strongest influence on syntactic learning, with contrastive loss alone leading to poor performance
- Most models show peak syntactic knowledge at middle layers, except CLIP which shows decreasing performance across layers

## Why This Works (Mechanism)
The paper demonstrates that VLMs struggle to capture syntactic dependencies because their pre-training objectives prioritize cross-modal alignment over linguistic structure. Contrastive learning, which drives most VLMs, focuses on matching images with relevant text without requiring deep syntactic understanding. This mechanism works well for visual grounding but fails to encode grammatical relationships. The findings suggest that syntactic learning requires explicit linguistic objectives rather than just cross-modal prediction tasks.

## Foundational Learning
- **Universal Dependencies (UD)**: A framework for consistent syntactic annotation across languages - needed to standardize syntactic evaluation across diverse language models and enable fair comparisons
- **Contrastive Learning**: Training method that learns by comparing positive and negative pairs - needed to understand the VLM training paradigm that prioritizes visual-text alignment over linguistic structure
- **Probing Classifiers**: Tools that test what linguistic information is encoded in model representations - needed to assess syntactic knowledge without requiring models to generate syntactically correct output
- **Layer-wise Analysis**: Examining model representations at different depths - needed to identify where syntactic information is most strongly encoded during the model's processing hierarchy
- **Predicate-Argument Relations**: Grammatical relationships between verbs and their arguments - needed as a core syntactic capability that VLMs struggle with, affecting downstream task performance

## Architecture Onboarding
**Component Map**: Input Text -> Text Encoder -> Layer Representations -> Probing Classifier -> UD Score
**Critical Path**: The pre-training objective directly shapes what linguistic information models encode, with contrastive loss prioritizing cross-modal alignment over syntactic structure
**Design Tradeoffs**: VLMs trade syntactic precision for multimodal integration, accepting weaker grammatical encoding to maintain strong visual-text alignment capabilities
**Failure Signatures**: Poor performance on predicate-argument relations, declining syntactic knowledge in deeper layers (CLIP), and inability to improve with scale suggest fundamental architectural limitations
**First Experiments**: 1) Test VLMs on additional syntactic evaluation frameworks beyond UD, 2) Implement controlled experiments ablating different pre-training objectives, 3) Conduct systematic scaling experiments varying model size while holding other factors constant

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses exclusively on Universal Dependencies, potentially missing other aspects of syntactic competence relevant for multimodal tasks
- Probing classifier performance may be influenced by factors beyond underlying representations, including classifier architecture and training procedures
- The analysis does not fully control for potential confounding variables like dataset composition or tokenization differences between models

## Confidence
- VLMs significantly underperform ULMs in syntactic encoding: High
- Pre-training objective is the dominant factor in syntactic learning: Medium
- Increasing model size does not improve syntactic learning: Medium

## Next Checks
1. Conduct systematic scaling experiments varying model size while holding other factors constant to verify the relationship between model scale and syntactic learning
2. Test models on additional syntactic evaluation frameworks beyond Universal Dependencies to assess the generalizability of findings
3. Implement controlled experiments ablating different aspects of the pre-training objective (contrastive loss, language modeling, etc.) to isolate their specific contributions to syntactic learning