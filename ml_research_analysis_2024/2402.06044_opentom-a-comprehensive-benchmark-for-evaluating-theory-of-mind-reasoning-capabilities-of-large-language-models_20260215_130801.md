---
ver: rpa2
title: 'OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning
  Capabilities of Large Language Models'
arxiv_id: '2402.06044'
source_url: https://arxiv.org/abs/2402.06044
tags: []
core_contribution: This paper introduces OpenToM, a new benchmark for evaluating large
  language models' (LLMs) Theory-of-Mind (ToM) capabilities. OpenToM addresses shortcomings
  in existing benchmarks by featuring longer, clearer narratives, characters with
  explicit personality traits and preferences, actions triggered by character intentions,
  and diverse questions targeting both physical and psychological mental states.
---

# OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models

## Quick Facts
- arXiv ID: 2402.06044
- Source URL: https://arxiv.org/abs/2402.06044
- Reference count: 40
- Key outcome: Introduces OpenToM benchmark revealing LLMs struggle significantly with psychological mental states compared to physical ones

## Executive Summary
This paper introduces OpenToM, a new benchmark designed to comprehensively evaluate large language models' Theory-of-Mind (ToM) reasoning capabilities. Unlike existing benchmarks, OpenToM features longer, clearer narratives with characters possessing explicit personality traits and preferences, and actions triggered by character intentions. The benchmark includes diverse questions targeting both physical and psychological mental states, making it more challenging and comprehensive than previous evaluations.

The authors evaluate several state-of-the-art LLMs including Llama2-Chat, Mixtral-8x7B-Instruct, GPT-3.5-Turbo, and GPT-4-Turbo using zero-shot evaluation with various prompting techniques. Results reveal that while these models excel at modeling physical mental states, they struggle significantly when tracking characters' psychological mental states. The study uncovers LLMs' lack of faithfulness in reasoning, performance discrepancies based on character roles, and inability to incorporate social commonsense, particularly in understanding and predicting characters' attitudes and emotions.

## Method Summary
The authors construct OpenToM by first filtering and adapting existing narratives from multiple ToM benchmarks, then employing crowd workers to refine these narratives and create questions covering physical and psychological mental states. The benchmark contains 696 narratives with 23 questions per story. They evaluate four state-of-the-art LLMs using zero-shot prompting with vanilla, Chain-of-Thought, and Simulated-ToM techniques. Performance is measured using macro-averaged F1 scores for binary and ternary classification tasks, with corrupted outputs (answers that don't follow the expected format) discarded during evaluation.

## Key Results
- LLMs achieve significantly higher accuracy on physical mental state questions compared to psychological mental state questions
- Models show poor faithfulness, often providing contradictory answers to coarse and fine-grained location questions about the same object
- GPT-4-Turbo demonstrates the best overall performance but still struggles with attitude questions requiring social commonsense reasoning
- Performance varies by character role, with models performing better on questions about non-character entities versus main characters

## Why This Works (Mechanism)
None provided

## Foundational Learning
- Theory-of-Mind: The ability to attribute mental states to oneself and others, understanding that others have beliefs, desires, and intentions different from one's own. [Why needed: To establish the cognitive capability being evaluated]
- Zero-shot prompting: Evaluating models without any examples in the prompt, testing their ability to generalize to new tasks. [Why needed: The evaluation methodology used]
- Macro-averaged F1 score: The average F1 score calculated per class and then averaged across all classes, treating all classes equally regardless of frequency. [Why needed: The primary metric for evaluating model performance]
- Chain-of-Thought prompting: A prompting technique that encourages models to generate intermediate reasoning steps before providing a final answer. [Why needed: One of the prompting strategies evaluated]
- Simulated-ToM prompting: A prompting strategy that explicitly asks models to simulate the mental states of characters in the narrative. [Why needed: Another prompting strategy evaluated]

## Architecture Onboarding
**Component Map:** OpenToM narratives -> Question generation -> LLM evaluation -> F1 score computation

**Critical Path:** Narrative construction → Question categorization → Model prompting → Response parsing → F1 calculation

**Design Tradeoffs:** The benchmark prioritizes narrative quality and question diversity over scale, resulting in fewer narratives but more comprehensive coverage of ToM capabilities. This trade-off favors depth of evaluation over breadth.

**Failure Signatures:** Models frequently provide corrupted outputs that don't follow the expected format, answer multiple questions as a single response, or show inconsistent reasoning between related questions.

**First Experiments:**
1. Evaluate a baseline model on a single OpenToM narrative to verify the evaluation pipeline
2. Test prompt corruption handling by intentionally creating malformed responses
3. Compare vanilla vs. Chain-of-Thought prompting on a small subset of questions

## Open Questions the Paper Calls Out
**Open Question 1:** Can LLMs be trained to improve their faithfulness in answering N-ToM questions, particularly in maintaining consistency between coarse and fine-grained location information?

**Open Question 2:** How do LLMs' performance and reasoning differ when dealing with non-linear narratives compared to the strictly chronological narratives used in OpenToM?

**Open Question 3:** What specific aspects of social commonsense are LLMs lacking in when trying to understand characters' psychological mental states, particularly in the context of the Attitude questions in OpenToM?

## Limitations
- Zero-shot evaluation only, without exploring few-shot or fine-tuning approaches
- Potential biases in constructed narratives related to cultural specificity and character diversity
- Uncontrolled handling of corrupted model outputs during F1 calculation
- Benchmark scalability to complex scenarios with multiple characters having conflicting mental states remains untested

## Confidence
**High Confidence:** LLMs perform significantly better on physical mental state tasks compared to psychological mental state tasks - supported by substantial statistical differences across multiple models and prompting strategies.

**Medium Confidence:** Models exhibit role-based performance differences (better on non-character questions vs. main character questions) - supported by data but potentially influenced by uncontrolled narrative construction choices.

**Low Confidence:** Models lack faithfulness in reasoning - based on qualitative analysis rather than systematic measurement of reasoning chains.

## Next Checks
1. Reproduce with Controlled Corruption Handling: Implement and test exact procedure for handling corrupted model outputs to verify whether reported F1 scores are affected by inconsistent processing of malformed responses.

2. Cross-Cultural Validation: Evaluate the benchmark with narratives featuring diverse cultural contexts to test whether observed performance gaps persist across different social and cultural settings.

3. Extended Prompting Experiments: Test additional prompting strategies including few-shot examples and dynamic prompting that adapts based on narrative complexity to determine whether performance gaps can be reduced through better prompting techniques.