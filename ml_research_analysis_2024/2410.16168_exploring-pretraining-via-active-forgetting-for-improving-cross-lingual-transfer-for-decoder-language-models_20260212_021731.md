---
ver: rpa2
title: Exploring Pretraining via Active Forgetting for Improving Cross Lingual Transfer
  for Decoder Language Models
arxiv_id: '2410.16168'
source_url: https://arxiv.org/abs/2410.16168
tags:
- languages
- language
- baseline
- pretraining
- adapting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Active forgetting improves cross-lingual transfer for decoder-only
  LLMs. The authors find that adapting a multilingual base LLM to a narrow set of
  new languages degrades performance on other languages, hurting overall cross-lingual
  transfer.
---

# Exploring Pretraining via Active Forgetting for Improving Cross Lingual Transfer for Decoder Language Models

## Quick Facts
- arXiv ID: 2410.16168
- Source URL: https://arxiv.org/abs/2410.16168
- Reference count: 24
- Key outcome: Active forgetting improves cross-lingual transfer for decoder-only LLMs, outperforming baselines on 6 of 7 multilingual tasks.

## Executive Summary
This paper addresses the challenge of improving cross-lingual transfer for decoder-only language models. The authors propose a novel pretraining approach called "active forgetting" where token embeddings are periodically reset to random values during training. Through extensive experimentation across multiple model scales (400M to 2.8B parameters), they demonstrate that this method produces better multilingual representations as measured by lower perplexity and higher isotropy, which translates to improved cross-lingual transfer performance without requiring additional labeled data.

## Method Summary
The method involves pretraining a base multilingual decoder-only LLM on 12 pretraining languages with active forgetting (resetting token embeddings every k steps), then adapting to 14 new languages via vocabulary expansion where only new tokens are trained while original embeddings remain frozen. Finally, the model is instruction-tuned on English-only data (OpenOrca) and evaluated on multilingual downstream tasks. The active forgetting mechanism prevents the model from becoming overly specialized to specific languages by periodically resetting token embeddings, forcing the model to continuously relearn and maintain more balanced multilingual representations.

## Key Results
- Models pretrained with active forgetting show improved cross-lingual transfer, outperforming baselines on 6 of 7 multilingual tasks.
- Active forgetting produces better multilingual representations as measured by lower perplexity and higher isotropy.
- The benefits hold across multiple model scales (400M to 2.8B parameters) without requiring additional labeled data.

## Why This Works (Mechanism)

### Mechanism 1: Active Forgetting Prevents Catastrophic Forgetting of Original Languages
- Claim: Resetting token embeddings every k steps prevents the model from becoming overly specialized to the adapting languages, preserving multilingual representation quality.
- Mechanism: By periodically resetting token embeddings to random values during pretraining, the model cannot form permanent, language-specific representations. This forces the model to continuously relearn embeddings, promoting more general and balanced multilingual representations.
- Core assumption: The model's ability to maintain performance across all languages depends on avoiding permanent entrenchment of language-specific embeddings.
- Evidence anchors:
  - [abstract] "Pretraining with active forgetting (resetting token embeddings every k steps) produces better multilingual representations, as measured by lower perplexity and higher isotropy."
  - [section] "In this work, we study benefits of active forgetting to train and adapt decoder-only models to new and unseen languages... Through extensive experimentation, we find that LLMs pretrained with active forgetting are able to learn better multilingual representations which translates to better performance in many downstream tasks."
  - [corpus] Weak - the corpus neighbors focus on cross-lingual transfer methods but don't directly address active forgetting mechanisms.
- Break condition: If the reset frequency k is too low (not enough forgetting) or too high (destroys learning), the benefits would diminish.

### Mechanism 2: Improved Representation Quality Enables Better Cross-Lingual Transfer
- Claim: Better multilingual representations (lower perplexity, higher isotropy) directly translate to improved cross-lingual transfer performance.
- Mechanism: High-quality, isotropic embeddings capture language-agnostic features that transfer across languages. When the model is instruction-tuned on English, these features enable better performance on non-English tasks without additional fine-tuning.
- Core assumption: Cross-lingual transfer effectiveness is primarily determined by the quality and isotropy of the underlying multilingual representations.
- Evidence anchors:
  - [abstract] "Models pretrained this way show improved cross-lingual transfer, outperforming baselines on 6 of 7 multilingual tasks."
  - [section] "We illustrate that base LLMs pretrained with active forgetting lead to higher quality multilingual representations... These improved representations also lead to better cross lingual transfer."
  - [corpus] Weak - neighbors discuss cross-lingual transfer but don't specifically address representation quality metrics like isotropy.
- Break condition: If the downstream tasks require language-specific features rather than general representations, this mechanism would be less effective.

### Mechanism 3: Active Forgetting Mitigates the Curse of Multilinguality
- Claim: By preventing over-specialization to specific languages, active forgetting helps manage the curse of multilinguality where performance degrades as more languages are added.
- Mechanism: Standard multilingual training can lead to token embeddings that become optimized for the most frequent languages, degrading performance on less-represented languages. Active forgetting prevents this entrenchment by continuously resetting embeddings.
- Core assumption: The curse of multilinguality is partly caused by permanent, unbalanced embedding learning across languages.
- Evidence anchors:
  - [abstract] "Building multilingual LLMs by simply having a large number of languages in the pretraining also does not work well due to the so-called 'curse of multilinguality' (Conneau et al., 2020)."
  - [section] "Our results show that such adaptation significantly worsens the overall multilingual capabilities of the resultant model thereby limiting the cross lingual transfer capabilities of LLMs."
  - [corpus] Weak - corpus neighbors discuss multilinguality challenges but don't specifically address active forgetting as a solution.
- Break condition: If the pretraining corpus is perfectly balanced across languages, the curse of multilinguality would be less severe, reducing the relative benefit of active forgetting.

## Foundational Learning

- Concept: Token embedding dynamics in multilingual models
  - Why needed here: Understanding how token embeddings evolve during training is crucial for grasping why periodic resetting helps.
  - Quick check question: What happens to token embeddings for rare languages during standard multilingual pretraining?

- Concept: Isotropy in embedding spaces
  - Why needed here: Isotropy measures how evenly distributed embeddings are in the vector space, which correlates with representation quality.
  - Quick check question: Why would higher self-similarity (isotropy) scores indicate better multilingual representations?

- Concept: Cross-lingual transfer mechanisms
  - Why needed here: The paper relies on the model's ability to transfer knowledge from English to other languages without explicit training.
  - Quick check question: How does a model trained primarily on English data achieve reasonable performance on non-English tasks?

## Architecture Onboarding

- Component map: Token embedding → attention layers → prediction head. Active forgetting affects only the token embedding stage during pretraining.

- Critical path: Token embedding → attention layers → prediction head. Active forgetting affects only the token embedding stage during pretraining.

- Design tradeoffs: More frequent embedding resets (lower k) provides better multilingual balance but may slow convergence. Less frequent resets (higher k) allows faster learning but risks language specialization.

- Failure signatures: If k is too low, the model may fail to learn stable representations. If k is too high, the model may not converge. If the reset mechanism is implemented incorrectly, embeddings may not actually reset.

- First 3 experiments:
  1. Implement active forgetting with k=10,000 steps and compare perplexity on mC4 vs baseline
  2. Measure isotropy scores for both models to confirm improved representation quality
  3. Evaluate cross-lingual transfer performance on one downstream task (e.g., MLQA) to verify the transfer benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does active forgetting's benefit scale to larger multilingual LLMs (e.g., >10B parameters) and more diverse language sets?
- Basis in paper: Explicit - The authors note that their experiments were limited to models up to 2.8B parameters and 26 "other" languages, with the data and compute budget constraining larger-scale studies.
- Why unresolved: The paper acknowledges the need for experiments on larger models and more languages to confirm if active forgetting's improvements in cross-lingual transfer hold at scale.
- What evidence would resolve it: Experiments with models >10B parameters trained on datasets with >100 languages, comparing perplexity, isotropy, and downstream task performance against baseline and baseline-adapted models.

### Open Question 2
- Question: Can active forgetting be applied post-hoc to existing pretrained multilingual LLMs to improve their cross-lingual transfer without full retraining?
- Basis in paper: Inferred - The authors suggest future work could explore applying active forgetting to intermediate checkpoints of open-source models like TinyLlama or OLMo by resetting embeddings and continuing training.
- Why unresolved: The paper only explores active forgetting during initial pretraining, not as a method to enhance already pretrained models.
- What evidence would resolve it: Experiments where embeddings of existing pretrained models are reset at intervals during continued training, measuring changes in multilingual representation quality and cross-lingual transfer performance.

### Open Question 3
- Question: Does active forgetting introduce or amplify biases in multilingual representations, particularly for low-resource languages?
- Basis in paper: Inferred - The ethics statement mentions that active forgetting directly affects token embeddings and that thorough bias studies are needed before deployment, but no such analysis is provided.
- Why unresolved: The paper focuses on performance improvements but does not analyze the bias characteristics of active forgetting models across different language groups.
- What evidence would resolve it: Comprehensive bias audits of active forgetting models, including fairness metrics across languages, sensitivity analysis for low-resource languages, and comparison with baseline models on bias benchmarks.

## Limitations
- Limited ablation on reset frequency - no systematic exploration of how different reset frequencies affect the balance between learning stability and multilingual representation quality.
- Narrow downstream task evaluation - the paper demonstrates improvements on 6 of 7 multilingual tasks but these may not span the full range of cross-lingual transfer scenarios.
- Simplified vocabulary adaptation - the adaptation mechanism only trains new vocabulary tokens while keeping original embeddings frozen, not capturing more complex adaptation scenarios.

## Confidence

**High confidence**: The core finding that active forgetting improves cross-lingual transfer is well-supported by empirical results across multiple model scales and task types. The mechanism of preventing language specialization through periodic embedding resets is theoretically sound.

**Medium confidence**: The explanation that improved isotropy and perplexity directly cause better cross-lingual transfer is plausible but not definitively proven. The relationship between representation quality metrics and task performance could be more nuanced.

**Low confidence**: The claim that active forgetting specifically mitigates the curse of multilinguality is inferred from the results but not directly tested. Alternative explanations (such as better regularization or implicit curriculum learning) could also account for the improvements.

## Next Checks

1. **Reset frequency ablation study**: Systematically vary the reset frequency k (e.g., 5k, 10k, 20k, 50k steps) and measure the tradeoff between learning stability and cross-lingual transfer performance. Identify the optimal k value and test whether it generalizes across different model sizes and pretraining datasets.

2. **Alternative adaptation mechanisms**: Compare the current vocabulary-only adaptation approach against more comprehensive adaptation strategies that include partial fine-tuning of existing embeddings. Measure whether allowing some adaptation of original language embeddings improves or degrades cross-lingual transfer.

3. **Representation quality causality analysis**: Design experiments that directly test whether isotropy improvements cause better cross-lingual transfer or merely correlate with it. This could involve artificially manipulating embedding isotropy (e.g., through orthogonal regularization) and measuring the impact on downstream task performance.