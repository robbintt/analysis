---
ver: rpa2
title: Identifying Policy Gradient Subspaces
arxiv_id: '2401.06604'
source_url: https://arxiv.org/abs/2401.06604
tags:
- subspace
- gradient
- actor
- critic
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates gradient subspaces in deep reinforcement
  learning, focusing on whether the phenomenon of low-dimensional gradient subspaces
  found in supervised learning also exists in policy gradient methods. The authors
  analyze the actor and critic networks of PPO and SAC across 12 benchmark tasks.
---

# Identifying Policy Gradient Subspaces

## Quick Facts
- arXiv ID: 2401.06604
- Source URL: https://arxiv.org/abs/2401.06604
- Reference count: 40
- Primary result: Policy gradient methods (PPO and SAC) exhibit gradients concentrated in low-dimensional, stable subspaces across 12 benchmark tasks

## Executive Summary
This paper investigates whether the low-dimensional gradient subspace phenomenon observed in supervised learning also exists in deep reinforcement learning. Through analysis of 12 benchmark tasks using PPO and SAC algorithms, the authors demonstrate that both actor and critic networks operate within subspaces spanned by a small number of high-curvature directions in parameter space. These subspaces remain stable throughout training and contain the majority of gradient information. The findings suggest that RL optimization could potentially be accelerated by focusing updates on these informative directions, opening possibilities for more efficient training methods.

## Method Summary
The authors analyze gradient subspaces in policy gradient methods by estimating Hessian eigenvectors using the Lanczos method on large datasets (10^6 samples for PPO, full replay buffer for SAC). They compute two key metrics: gradient subspace fraction (Sfrac) measuring how much of the gradient lies in the high-curvature subspace, and subspace overlap (Soverlap) quantifying stability across training timesteps. Experiments were conducted on 12 benchmark tasks from OpenAI Gym, Gym Robotics, and DeepMind Control Suite, evaluating both on-policy (PPO) and off-policy (SAC) algorithms. The analysis focuses on identifying parameter-space directions with significantly larger curvature than others and tracking how gradients align with these directions throughout training.

## Key Results
- A small number of parameter-space directions exhibit significantly larger curvature than others across all tested tasks
- Gradients of both actor and critic losses primarily lie within the subspace spanned by high-curvature directions
- The high-curvature subspace remains relatively stable throughout training, with considerable overlap even after 400,000 timesteps
- Gradient subspace fraction is significantly higher than random chance for both PPO and SAC algorithms

## Why This Works (Mechanism)
The phenomenon works because neural network optimization in reinforcement learning, like in supervised learning, exhibits concentration of curvature and gradients in specific parameter directions. The Hessian matrix's spectrum shows that while most directions have small curvature, a few directions dominate. Policy gradients, being estimates of the true gradient, naturally align with these high-curvature directions due to their larger impact on the loss landscape. This creates a feedback loop where optimization naturally concentrates in these subspaces, making them both informative and stable throughout training.

## Foundational Learning
- **Hessian matrix and curvature**: Understanding the second-order structure of the loss landscape is crucial for identifying high-curvature directions that dominate optimization. Quick check: Compute eigenvalues of a simple quadratic function to see how curvature concentrates.
- **Lanczos method for eigenvalue estimation**: This iterative algorithm efficiently approximates the dominant eigenvectors of large matrices without full decomposition, essential for practical Hessian analysis in deep networks. Quick check: Verify Lanczos converges to top eigenvectors on a small test matrix.
- **Gradient subspace fraction**: This metric quantifies the proportion of gradient information contained in a specific subspace, directly measuring the concentration phenomenon. Quick check: Project random gradients onto random subspaces to establish baseline Sfrac values.
- **Reinforcement learning algorithms (PPO/SAC)**: Understanding the specific update rules and loss functions of these algorithms is necessary to interpret how their gradients behave in parameter space. Quick check: Trace through one update step of PPO to see how policy and value gradients are computed.

## Architecture Onboarding

**Component Map:**
- Task environment -> RL agent (PPO/SAC) -> Parameter updates -> Hessian estimation -> Subspace analysis -> Performance metrics

**Critical Path:**
The critical path flows from training the RL agent to checkpointing parameters, then estimating the Hessian via Lanczos, computing gradient subspace metrics, and analyzing stability over time. Each component depends on the previous: accurate training is needed for meaningful checkpoints, good Hessian estimates require sufficient data, and reliable metrics need accurate subspace calculations.

**Design Tradeoffs:**
The main tradeoff is between computational cost and estimation accuracy. Full Hessian computation is infeasible for deep networks, so Lanczos provides an efficient approximation but requires careful tuning of iteration count and data sampling. The choice of how many top eigenvectors to analyze (e.g., top 1%, 5%, 10%) involves balancing computational tractability against capturing sufficient gradient information.

**Failure Signatures:**
- Poor gradient subspace fraction may indicate insufficient data for Lanczos estimation or excessive gradient noise
- Low subspace overlap suggests unstable Hessian estimates or rapidly changing loss landscapes
- Unexpectedly high Sfrac in random subspaces indicates errors in gradient projection or metric computation

**First Experiments:**
1. Verify Lanczos method recovers known eigenvectors on a small, analytically tractable network
2. Compute Sfrac for random projections to establish baseline values for comparison
3. Test subspace overlap computation on synthetic time series data with known stability properties

## Open Questions the Paper Calls Out
### Open Question 1
How do gradient subspaces identified in early training stages perform when applied to later stages of reinforcement learning training? While the paper demonstrates stability of subspaces over time, it doesn't test whether using subspaces identified early in training provides computational benefits or improved performance when used for subsequent optimization.

### Open Question 2
What is the impact of different exploration strategies when parameter noise is restricted to the high-curvature subspace? The paper mentions this as a potential direction without implementing or testing it, leaving unclear whether subspace-restricted exploration actually improves sample efficiency or final performance compared to traditional exploration methods.

### Open Question 3
How robust are gradient subspaces to different neural network architectures and hyperparameters in reinforcement learning? The analysis is limited to specific tasks and doesn't explore how different network architectures or a broader range of hyperparameters affect subspace properties and utility.

## Limitations
- Lack of reported hyperparameter values for PPO and SAC implementations may affect reproducibility
- Specific implementation details of the Lanczos method for Hessian eigenvector estimation are not fully specified
- Limited exploration of how findings generalize to different network architectures and broader hyperparameter ranges

## Confidence
- **High confidence**: Existence of high-curvature directions and gradient containment in subspaces (well-supported by multiple tasks and algorithms)
- **Medium confidence**: Subspace stability claims (dependent on precise Hessian estimation quality)
- **Low confidence**: Generality of findings to all RL tasks and architectures without further validation

## Next Checks
1. Replicate the gradient subspace fraction measurements using publicly available PPO and SAC implementations with tuned hyperparameters to verify reported Sfrac values across all 12 tasks
2. Conduct ablation studies varying the number of Lanczos iterations and data sample sizes to establish robustness of Hessian eigenvector estimates
3. Test the subspace analysis on additional RL algorithms (e.g., TD3, A2C) and task types (sparse rewards, continuous control) to evaluate generalizability of the low-dimensional subspace phenomenon