---
ver: rpa2
title: Curating Grounded Synthetic Data with Global Perspectives for Equitable AI
arxiv_id: '2406.10258'
source_url: https://arxiv.org/abs/2406.10258
tags:
- dataset
- data
- entity
- articles
- supplementary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of data scarcity and underrepresentation
  in AI training data by introducing a novel approach to generating synthetic datasets
  grounded in real-world diversity. The method synthesizes data using a comprehensive
  collection of news articles spanning 12 languages and 125 countries, ensuring broad
  linguistic and cultural representation.
---

# Curating Grounded Synthetic Data with Global Perspectives for Equitable AI

## Quick Facts
- arXiv ID: 2406.10258
- Source URL: https://arxiv.org/abs/2406.10258
- Authors: Elin Törnquist; Robert Alexander Caulk
- Reference count: 19
- Primary result: Synthetic dataset grounded in global news articles improves NER performance by up to 7.3% on benchmarks

## Executive Summary
This paper addresses the challenge of data scarcity and underrepresentation in AI training data by introducing a novel approach to generating synthetic datasets grounded in real-world diversity. The method synthesizes data using a comprehensive collection of news articles spanning 12 languages and 125 countries, ensuring broad linguistic and cultural representation. Through topic diversification, translation, and summarization, the resulting dataset mirrors real-world complexities and addresses underrepresentation in traditional datasets. Applied to Named Entity Recognition (NER), the approach shows substantial performance improvements on standard NER benchmarks.

## Method Summary
The methodology involves curating a synthetic NER dataset from news articles in 125 countries and 12 languages. The process includes topic diversification using semantic embeddings and HDBSCAN clustering, followed by LLM-based translation, summarization, and entity labeling. The resulting AskNews-NER-v0 dataset contains 5,049 samples covering 73 unique topics and 54 entity types. GLiNER models (Small, Medium, Large) are fine-tuned on this dataset using AdamW optimizer with a learning rate of 1e-6, ReduceLROnPlateau scheduler, 25 epochs, and batch size 5, achieving improved performance on standard NER benchmarks.

## Key Results
- Fine-tuned GLiNER models show 4.6% average improvement on 18 common NER datasets
- Small model achieved 5.5% increase (2.5 points) on NER benchmarks
- Up to 7.3% improvement in micro-F1 scores on standard NER benchmarks
- Demonstrated effectiveness for Named Entity Recognition tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diversity-grounded synthetic data improves NER model performance by exposing models to broader linguistic and cultural contexts during training.
- Mechanism: Synthetic data generation leverages news articles from 125 countries and 12 languages, then applies topic diversification, translation, and summarization to create varied training samples that mirror real-world complexity.
- Core assumption: Exposure to diverse, real-world grounded data during training leads to better generalization on out-of-domain NER tasks.
- Evidence anchors:
  - [abstract] "This methodology, applied initially to Named Entity Recognition (NER), serves as a model for numerous AI disciplines where data diversification is critical for generalizability."
  - [section] "Preliminary results demonstrate substantial improvements in performance on traditional NER benchmarks, by up to 7.3%..."
  - [corpus] Found 25 related papers; average neighbor FMR=0.461, indicating moderate relevance to synthetic data and NER topics.
- Break condition: If the synthetic data fails to maintain semantic fidelity during translation/summarization, or if the topic diversification introduces excessive noise.

### Mechanism 2
- Claim: Fine-tuning GLiNER models on diversified synthetic datasets improves zero-shot performance on common NER benchmarks compared to base models.
- Mechanism: Fine-tuning on the AskNews-NER-v0 dataset (5,049 samples covering 73 unique topics and 54 entity types) adjusts the model weights to better recognize entities across varied contexts.
- Core assumption: Model architecture (GLiNER) can effectively learn from synthetic data and transfer that learning to unseen NER datasets.
- Evidence anchors:
  - [section] "Tables 1 and 2 show that our fine-tuned models improve the performance of the base models by 4.6%, on average..."
  - [section] "The fine-tuned small model showed increased scores of 2.5 points (5.5%) on the 18 NER datasets..."
  - [corpus] "Does Synthetic Data Help Named Entity Recognition for Low-Resource Languages?" indicates related work on synthetic data for NER.
- Break condition: If the synthetic dataset lacks sufficient diversity or contains systematic biases that reinforce incorrect entity type associations.

### Mechanism 3
- Claim: Vector database indexing of embedded news articles enables efficient retrieval of semantically similar articles for topic diversification.
- Mechanism: Articles are embedded using gte-base model, stored in Qdrant vector database, and clustered using HDBSCAN to ensure topic and country distribution balance.
- Core assumption: Semantic similarity in embedding space corresponds to topical and contextual similarity relevant for NER training.
- Evidence anchors:
  - [section] "6. Embed the titles and summaries using thethenlper/gte-base embedding model [Li et al., 2023]."
  - [section] "7. Store the embeddings in a vector database (we are using Qdrant [Qdrant, 2024])."
  - [section] "Cluster each bucket of embeddings after L2-normalizarion using scikit-learn's [Pedregosa et al., 2011] HDBSCAN algorithm..."
  - [corpus] No direct corpus evidence for this specific vector database + HDBSCAN approach in NER context.
- Break condition: If the embedding model fails to capture relevant semantic distinctions, or if HDBSCAN clustering parameters are suboptimal.

## Foundational Learning

- Concept: Named Entity Recognition (NER)
  - Why needed here: The entire methodology is built around improving NER model performance through better training data.
  - Quick check question: What are the typical entity types that NER models aim to identify in text?

- Concept: Large Language Models (LLMs) for synthetic data generation
  - Why needed here: LLMs are used to translate, summarize, and label entities in news articles to create the synthetic dataset.
  - Quick check question: How do LLMs differ from traditional rule-based approaches in generating synthetic text data?

- Concept: Vector embeddings and semantic similarity
  - Why needed here: Embeddings are used to cluster semantically similar articles for topic diversification and to enable efficient retrieval from the vector database.
  - Quick check question: What is the relationship between cosine similarity of embeddings and semantic similarity of text?

## Architecture Onboarding

- Component map:
  - News article parser → Language filter → Topic diversification pipeline (embedding + clustering) → LLM translation/summarization → Entity labeling → Dataset split → GLiNER fine-tuning → Evaluation
  - Key components: Qdrant vector database, WizardLM 13B v1.2, Llama-3-70B-Instruct, GLiNER models

- Critical path:
  1. Article ingestion and preprocessing
  2. Embedding and clustering for topic diversification
  3. LLM-based translation, summarization, and entity labeling
  4. Dataset creation and splitting
  5. GLiNER model fine-tuning
  6. Zero-shot evaluation on benchmark datasets

- Design tradeoffs:
  - Language coverage vs. LLM capabilities: Limited to 12 languages due to LLM translation abilities
  - Dataset size vs. diversity: 5,049 samples chosen to balance coverage and computational efficiency
  - Fine-tuning time vs. performance: Trained for 25 epochs, balancing improvement with overfitting risk

- Failure signatures:
  - Poor clustering results indicate embedding model issues or inappropriate HDBSCAN parameters
  - Entity labeling errors suggest LLM prompt engineering problems or entity type confusion
  - Stagnant or declining validation scores during fine-tuning indicate overfitting or data quality issues

- First 3 experiments:
  1. Verify embedding quality: Cluster a small subset of articles manually and compare to HDBSCAN results
  2. Test LLM pipeline: Run translation/summarization on a few articles and check semantic fidelity
  3. Validate entity labeling: Compare LLM-labeled entities against human annotations on a small sample

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the GLiNER models trained on the AskNews-NER-v0 dataset compare to models trained on other synthetic datasets generated using different methodologies (e.g., without grounding in real-world diversity or without topic diversification)?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of the proposed methodology for grounding and diversifying synthetic data, but does not directly compare the performance of the resulting dataset to other synthetic datasets generated using different approaches.
- Why unresolved: The paper focuses on showcasing the benefits of the proposed methodology rather than conducting a comprehensive comparison with alternative synthetic data generation techniques.
- What evidence would resolve it: Training GLiNER models on various synthetic datasets generated using different methodologies and comparing their performance on the same benchmark datasets.

### Open Question 2
- Question: How does the performance of the GLiNER models trained on the AskNews-NER-v0 dataset generalize to NER tasks in languages not covered by the dataset (e.g., Chinese, Japanese, Hindi)?
- Basis in paper: [explicit] The dataset is generated using news articles in 12 languages, and the paper acknowledges the current limitation to these languages.
- Why unresolved: The paper does not evaluate the performance of the models on NER tasks in languages outside the 12 languages covered by the dataset.
- What evidence would resolve it: Evaluating the performance of the fine-tuned GLiNER models on NER benchmark datasets in languages not present in the AskNews-NER-v0 dataset.

### Open Question 3
- Question: How does the performance of the GLiNER models trained on the AskNews-NER-v0 dataset scale with the size of the dataset (e.g., what is the impact of doubling or halving the number of articles)?
- Basis in paper: [inferred] The paper presents results using a dataset of 5,049 articles, but does not explore the impact of dataset size on model performance.
- Why unresolved: The paper does not investigate the relationship between dataset size and model performance, leaving open the question of how much data is optimal for training NER models using this methodology.
- What evidence would resolve it: Training GLiNER models on subsets of the AskNews-NER-v0 dataset of varying sizes and comparing their performance on benchmark datasets to determine the impact of dataset size on model performance.

## Limitations
- Dataset distribution across countries and languages within the 5,049 samples is not specified, raising questions about actual diversity representation
- Results may be specific to GLiNER architecture and not transfer to other NER model types
- Multiple LLM processing steps introduce potential semantic drift that isn't quantitatively measured

## Confidence

**High Confidence**: The methodology for creating the AskNews-NER-v0 dataset using news articles with topic diversification and LLM processing is clearly specified, and the fine-tuning procedure for GLiNER models is well-documented with specific hyperparameters.

**Medium Confidence**: The claim of 4.6% average improvement on NER benchmarks is supported by presented results, but the evaluation setup details are limited, and the improvement magnitude may depend heavily on the specific GLiNER architecture and base model initialization.

**Low Confidence**: The paper's broader claim that this methodology "serves as a model for numerous AI disciplines where data diversification is critical for generalizability" lacks empirical support beyond the NER application, making it speculative without testing on other AI tasks.

## Next Checks

1. **Dataset Distribution Audit**: Analyze the actual country and language distribution in the AskNews-NER-v0 dataset to verify whether the claimed global representation translates to balanced sample distribution, particularly checking for overrepresentation of major languages or regions.

2. **Semantic Fidelity Assessment**: Conduct a small-scale human evaluation comparing source articles to their translated and summarized versions to measure semantic drift, focusing on whether entity information and contextual relationships are preserved through the processing pipeline.

3. **Architecture Transferability Test**: Evaluate whether the synthetic dataset provides similar performance improvements when used to fine-tune alternative NER architectures (e.g., BERT-based models) rather than just GLiNER, to assess the dataset's general utility beyond the specific model family used in the paper.