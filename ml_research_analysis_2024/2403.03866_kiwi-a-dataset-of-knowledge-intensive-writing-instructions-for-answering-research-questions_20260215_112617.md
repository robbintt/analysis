---
ver: rpa2
title: 'KIWI: A Dataset of Knowledge-Intensive Writing Instructions for Answering
  Research Questions'
arxiv_id: '2403.03866'
source_url: https://arxiv.org/abs/2403.03866
tags:
- answer
- instruction
- question
- instructions
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KIWI, a dataset of 1,260 expert-written writing
  instructions for improving model-generated answers to NLP research questions. Researchers
  iteratively issue instructions to revise answers based on relevant papers, with
  three LLMs (GPT-4, GPT-3.5-turbo, LLaMA-2-70b-chat) generating responses across
  234 sessions.
---

# KIWI: A Dataset of Knowledge-Intensive Writing Instructions for Answering Research Questions

## Quick Facts
- arXiv ID: 2403.03866
- Source URL: https://arxiv.org/abs/2403.03866
- Authors: Fangyuan Xu; Kyle Lo; Luca Soldaini; Bailey Kuehl; Eunsol Choi; David Wadden
- Reference count: 40
- Key outcome: KIWI dataset of 1,260 expert-written writing instructions for improving model-generated answers to NLP research questions, revealing that GPT-4 successfully followed instructions only 59% of the time

## Executive Summary
This paper introduces KIWI, a dataset of 1,260 expert-written writing instructions for improving model-generated answers to NLP research questions. Researchers iteratively issue instructions to revise answers based on relevant papers, with three LLMs (GPT-4, GPT-3.5-turbo, LLaMA-2-70b-chat) generating responses across 234 sessions. The dataset reveals that even the best model (GPT-4) struggles with instruction-following, successfully following instructions only 59% of the time. Models particularly struggle with incorporating new information from multiple documents and performing precise edits, while also showing limitations in evaluating their own responses.

## Method Summary
The KIWI dataset was created through a multi-stage process involving expert researchers and three large language models. Researchers formulated 78 NLP research questions and used relevant papers to iteratively issue writing instructions to revise model-generated answers. Three LLMs (GPT-4, GPT-3.5-turbo, LLaMA-2-70b-chat) generated responses across 234 sessions. For each instruction, three different models attempted to follow it, with human evaluators determining success. The evaluation included both following instructions and self-evaluation accuracy, with human agreement serving as the gold standard. The dataset captures the full interaction history including questions, papers, instructions, and model responses.

## Key Results
- GPT-4 successfully followed instructions 59% of the time, while GPT-3.5-turbo and LLaMA-2-70b-chat achieved 50% and 40% respectively
- Models struggled most with "expand coverage" instructions requiring information from multiple documents (average 37% success)
- GPT-4 achieved 68% accuracy in evaluating whether responses followed instructions, compared to human agreement at 80%

## Why This Works (Mechanism)
The paper does not explicitly discuss the mechanism behind why current instruction-following works or fails. However, based on the observed results, it appears that current LLMs have difficulty maintaining context and coherence when integrating new information from multiple sources while preserving the structure of existing responses. The models also struggle with fine-grained editing operations that require precise manipulation of text without introducing inconsistencies.

## Foundational Learning
The paper does not explicitly discuss foundational learning concepts. The dataset creation process suggests that models may benefit from training approaches that emphasize multi-turn instruction-following and document integration, but these are not explicitly addressed in the paper.

## Architecture Onboarding
The paper does not provide detailed architectural information about the models used. Based on the results, it can be inferred that current transformer-based architectures, while powerful for generation, have limitations in precise text manipulation and multi-document information integration that may require architectural innovations or specialized fine-tuning approaches.

## Open Questions the Paper Calls Out
Based on the paper's findings, some key open research questions include:

### Question 1
- Question: How can large language models be improved to better follow precise user instructions, particularly for tasks requiring multi-document information integration and specific editing operations?
- Basis in paper: The paper found that even the best model (GPT-4) struggled with incorporating new information from multiple documents and performing precise edits.
- Why unresolved: Current models struggle with maintaining answer consistency, satisfying hard constraints, and coherently integrating new information into existing text.
- What evidence would resolve it: Development of models that achieve higher success rates (>80%) on diverse instruction types and can maintain consistency across multiple turns.

### Question 2
- Question: Can large language models be trained to serve as reliable evaluators for assessing whether responses follow user instructions, particularly for specific and precise instructions?
- Basis in paper: GPT-4 struggled to reliably evaluate responses for specific instructions, with accuracy lagging behind human agreement by 12%.
- Why unresolved: LLMs are currently biased and struggle with judging responses for precise instructions, even when they can perform the task themselves.
- What evidence would resolve it: Development of models that can evaluate responses with accuracy comparable to human agreement (>80%) across diverse instruction types.

### Question 3
- Question: How can the instruction-following capabilities of large language models be improved for knowledge-intensive writing tasks, particularly those requiring integration of information from multiple documents?
- Basis in paper: Models struggled with "expand coverage" instructions requiring retrieval and integration of information from multiple papers.
- Why unresolved: Current models struggle with multi-document summarization and coherent integration of new information.
- What evidence would resolve it: Development of models that can successfully follow "expand coverage" instructions at rates comparable to simpler instruction types (>70% success).

## Limitations
- The study relies on expert-written instructions and evaluations, which may introduce subjective biases in what constitutes "successful" instruction following
- The dataset focuses specifically on NLP research questions and may not generalize to other domains or question types
- The evaluation framework measures instruction-following success at a binary level rather than capturing nuanced degrees of compliance or partial success
- The study does not explore the impact of different prompting strategies or model configurations on instruction-following performance

## Confidence
- High confidence: The observation that GPT-4 successfully followed instructions only 59% of the time is well-supported by the experimental design and evaluation methodology
- Medium confidence: The specific struggles with incorporating information from multiple documents and performing precise edits, as these depend on the particular instructions and answer formats used
- Medium confidence: The self-evaluation accuracy rates, as these depend on the specific evaluation criteria and may vary with different question types
- Medium confidence: The generalizability of findings to other domains beyond NLP research questions

## Next Checks
1. Test the instruction-following capabilities on a broader range of research domains beyond NLP to assess generalizability
2. Implement a more granular evaluation framework that captures degrees of instruction compliance rather than binary success/failure
3. Conduct ablation studies to determine which types of instructions (e.g., addition vs. modification) pose the greatest challenges for current LLMs
4. Explore the impact of different prompting strategies and model configurations on instruction-following performance
5. Investigate whether specialized fine-tuning approaches could improve multi-document information integration capabilities