---
ver: rpa2
title: Can LLMs Beat Humans in Debating? A Dynamic Multi-agent Framework for Competitive
  Debate
arxiv_id: '2408.04472'
source_url: https://arxiv.org/abs/2408.04472
tags:
- justice
- debate
- interests
- agent4debate
- social
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Agent4Debate, a dynamic multi-agent framework
  that enhances LLMs' performance in competitive debate through collaborative agents
  (Searcher, Analyzer, Writer, Reviewer) simulating human debate team interactions.
  The framework addresses LLM challenges like hallucination and competitiveness in
  sustained debate scenarios.
---

# Can LLMs Beat Humans in Debating? A Dynamic Multi-agent Framework for Competitive Debate

## Quick Facts
- arXiv ID: 2408.04472
- Source URL: https://arxiv.org/abs/2408.04472
- Reference count: 13
- Top Agent4Debate models achieved 1034.15 Debatrix-Elo and 1040.64 Human-Elo, exceeding human averages of 978.35 and 1006.46 respectively

## Executive Summary
This paper introduces Agent4Debate, a dynamic multi-agent framework that enables large language models to compete at human-level performance in structured debate competitions. The system employs four specialized agents (Searcher, Analyzer, Writer, Reviewer) that collaborate throughout the debate process, addressing key LLM challenges like hallucination and sustained competitiveness. Experiments conducted in a constructed Competitive Debate Arena with 66 Chinese debate motions and 200 matches demonstrate that top Agent4Debate models outperform human debaters in Elo rankings, achieving human-level performance across multiple evaluation metrics.

## Method Summary
Agent4Debate implements a collaborative multi-agent system where four specialized agents work together throughout competitive debate proceedings. The Searcher retrieves external knowledge to ground arguments and reduce hallucinations, the Analyzer provides strategic guidance and content planning, the Writer formulates and drafts arguments, and the Reviewer provides quality control through iterative feedback. The framework operates across three debate stages (Argument, Rebuttal, Summary) with each agent following customized prompts. Performance is evaluated using a Competitive Debate Arena with 66 Chinese debate motions, employing both automatic (Debatrix) and human judges, with results ranked using an improved Bradley-Terry model with score-difference weighting.

## Key Results
- Top Agent4Debate models (Gemini-1.5-Pro) achieved 1034.15 Debatrix-Elo vs. human average 978.35
- In Human-Elo rankings, top Agent4Debate scored 1040.64 vs. human 1006.46
- Ablation studies confirmed each component's effectiveness in improving debate performance

## Why This Works (Mechanism)

### Mechanism 1
Dynamic multi-agent collaboration improves LLM debate performance by addressing hallucination and competitiveness. The framework uses four specialized agents that interact and adapt roles based on debate stage, mimicking human debate team behavior. This specialization allows better management of debate complexity than monolithic models.

### Mechanism 2
The Searcher agent mitigates hallucination by grounding debate content in external knowledge bases. It decomposes queries, retrieves relevant information from external sources, and organizes it into a knowledge base accessible to all agents. This external grounding significantly reduces the likelihood of LLM hallucinations in debate contexts.

### Mechanism 3
An improved BT model with weight function enhances ranking accuracy in the Competitive Debate Arena. The weight function adjusts importance of each match based on score differences between participants, improving ability parameter estimation. This provides more reliable performance comparisons between participants.

## Foundational Learning

- **Multi-agent systems**: Understanding how multiple specialized agents collaborate to solve complex tasks is crucial for grasping Agent4Debate's architecture. *Quick check: How do multi-agent systems differ from single-agent approaches in handling complex, dynamic tasks?*

- **Computational argumentation**: Competitive debate is a form of computational argumentation, requiring understanding of argument mining, generation, and evaluation techniques. *Quick check: What are the key components of computational argumentation, and how do they apply to competitive debate?*

- **Elo ranking systems**: Elo rankings are used to evaluate and compare debate participant performance. *Quick check: How do Elo rankings work, and why are they suitable for evaluating competitive debate performance?*

## Architecture Onboarding

- **Component map**: Searcher → Analyzer → Writer → Reviewer (iterative refinement) → Debate output
- **Critical path**: The four agents work iteratively through debate stages, with each agent building upon the previous agent's work to produce final debate outputs
- **Design tradeoffs**: Specialization vs. flexibility (specialized agents vs. general agents), external knowledge reliance vs. internal model capabilities, evaluation method diversity vs. consistency
- **Failure signatures**: Poor agent communication leading to inconsistent arguments, over-reliance on external sources causing delays, inadequate reviewer feedback resulting in low-quality outputs
- **First 3 experiments**: 1) Implement basic multi-agent framework with simplified roles and test on small debate motion set, 2) Add external knowledge retrieval and evaluate hallucination reduction, 3) Integrate iterative review process and measure argument quality improvements

## Open Questions the Paper Calls Out

### Open Question 1
How does Agent4Debate performance scale with foundation model size and capability? The paper shows results with several models but doesn't systematically analyze the relationship between model capabilities and debate performance. What evidence would resolve it: A systematic study testing Agent4Debate across models of varying sizes/capabilities with statistical analysis of performance correlation.

### Open Question 2
Can Agent4Debate maintain performance across different languages and cultural contexts? The paper focuses exclusively on Chinese debate motions. What evidence would resolve it: Testing Agent4Debate across multiple languages and cultural contexts with native speakers, comparing performance metrics.

### Open Question 3
What is the optimal balance between agent autonomy and human oversight in competitive debate settings? The paper presents Agent4Debate as an autonomous system but doesn't explore hybrid human-AI collaboration scenarios. What evidence would resolve it: Experiments comparing pure Agent4Debate performance against human-AI collaborative teams.

## Limitations

- Evaluation reliability concerns due to lack of reported inter-rater reliability between Debatrix and human evaluations
- Limited generalization evidence as experiments were conducted exclusively on Chinese debate motions
- Single-turn limitations with no evidence for scalability to multi-agent settings beyond four participants

## Confidence

**High Confidence**: The core claim that specialized multi-agent collaboration improves debate performance is well-supported by ablation studies and Elo ranking comparisons.

**Medium Confidence**: The assertion that Agent4Debate achieves "human-level performance" is supported by Elo scores but lacks statistical validation and confidence intervals.

**Low Confidence**: The scalability claims regarding the framework's ability to handle diverse debate formats and languages are not empirically tested beyond the Chinese debate motions used in experiments.

## Next Checks

1. **Statistical Validation**: Conduct statistical significance testing between top Agent4Debate models and human debaters using confidence intervals for Elo scores to determine if performance differences are meaningful.

2. **Cross-Linguistic Evaluation**: Implement the framework with English debate motions and conduct parallel experiments to assess performance consistency across languages.

3. **Dynamic Role-Switching Test**: Design experiments where agents must dynamically switch roles mid-debate or handle more than four participants simultaneously to test framework adaptability and scalability.