---
ver: rpa2
title: 'RobustBlack: Challenging Black-Box Adversarial Attacks on State-of-the-Art
  Defenses'
arxiv_id: '2412.20987'
source_url: https://arxiv.org/abs/2412.20987
tags:
- attacks
- black-box
- robust
- adversarial
- against
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates the effectiveness of recent black-box adversarial\
  \ attacks against state-of-the-art robust models from the RobustBench leaderboard.\
  \ Contrary to common benchmarking practices that use weak defenses, we demonstrate\
  \ that even simple adversarial training drastically reduces the success of black-box\
  \ attacks, with the strongest attack\u2019s success rate dropping from 89.56% to\
  \ 3.56%."
---

# RobustBlack: Challenging Black-Box Adversarial Attacks on State-of-the-Art Defenses

## Quick Facts
- arXiv ID: 2412.20987
- Source URL: https://arxiv.org/abs/2412.20987
- Authors: Mohamed Djilani; Salah Ghamizi; Maxime Cordy
- Reference count: 32
- Key outcome: Black-box attacks on robust models show dramatically reduced success rates (from 89.56% to 3.56%)

## Executive Summary
This study systematically evaluates black-box adversarial attacks against state-of-the-art robust models from the RobustBench leaderboard. Contrary to common benchmarking practices that use weak defenses, we demonstrate that even simple adversarial training drastically reduces the success of black-box attacks. Our comprehensive evaluation covers 13 different attack methods against 9 robust models, revealing that defenses optimized against AutoAttack also exhibit enhanced black-box robustness, while ensemble defenses mixing robust and vanilla models are particularly vulnerable to black-box attacks.

The research further reveals a paradoxical finding: robust models can improve black-box attacks when used as surrogates, increasing success rates by up to 15× against robust targets. These findings underscore the need for more challenging black-box attack benchmarks and highlight the importance of considering robust models in both attack and defense research, suggesting that current evaluation practices may underestimate the effectiveness of robust defenses.

## Method Summary
The study evaluates 13 black-box adversarial attack methods against 9 state-of-the-art robust models from the RobustBench leaderboard. Transfer-based attacks include MI-FGSM, DI-FGSM, TI-FGSM, VMI-FGSM, VNI-FGSM, ADMIX, UAP, GHOST, and LGV, while query-based attacks include BASES and TREMBA. All attacks use a WideResNet50-2 surrogate model, with ensemble surrogates employed for BASES and TREMBA. The evaluation uses the ImageNet validation set (5000 images) with an L∞ perturbation budget of 4/255 and step size of 2/255 over 10 iterations. AutoAttack success rates are measured for baseline comparison, and all experiments are repeated with 3 random seeds.

## Key Results
- Black-box attack success rates drop dramatically on robust models, from 89.56% to 3.56% for the strongest attack
- Defenses optimized against AutoAttack show enhanced black-box robustness beyond white-box protection
- Ensemble defenses mixing robust and vanilla models are particularly vulnerable to black-box attacks
- Robust models paradoxically improve black-box attacks when used as surrogates, increasing success rates by up to 15×

## Why This Works (Mechanism)
The mechanism underlying the reduced black-box attack success on robust models relates to the fundamental difference in decision boundaries between naturally trained and adversarially trained models. Robust models develop smoother, more linear decision boundaries that are less susceptible to gradient-based perturbations and transferability attacks. This creates a significant barrier for black-box attacks that rely on transferability from surrogate models, as the gradient information from standard models becomes less effective when transferred to robust targets.

## Foundational Learning
- Adversarial training and its impact on decision boundaries (why needed: understanding robust model characteristics; quick check: compare decision boundary smoothness between robust and standard models)
- Transferability of adversarial examples in black-box settings (why needed: core mechanism of black-box attacks; quick check: measure transferability rates between different model architectures)
- RobustBench leaderboard and model selection criteria (why needed: understanding the evaluated models' properties; quick check: verify model robustness scores against known benchmarks)
- Gradient-based attack methods and their hyperparameters (why needed: proper attack configuration; quick check: validate attack implementations against known baselines)

## Architecture Onboarding
- Component map: ImageNet validation set -> Attack method (MI-FGSM, DI-FGSM, etc.) -> Surrogate model (WideResNet50-2) -> Target robust model -> ASR computation
- Critical path: Image preprocessing → Attack generation → Model inference → Success/failure determination → ASR aggregation
- Design tradeoffs: Single surrogate vs ensemble surrogates (BASES, TREMBA) balances attack strength with computational cost
- Failure signatures: Low ASR across all attacks indicates effective robustness; high ASR on ensemble models suggests vulnerability to mixed defense strategies
- First experiments: 1) Verify AutoAttack performance on target models matches known benchmarks, 2) Run MI-FGSM with default parameters to establish baseline transferability, 3) Test single surrogate attack against one robust model to validate implementation

## Open Questions the Paper Calls Out
None

## Limitations
- Limited to 9 robust models from RobustBench, which may not represent the full diversity of robust defenses
- Experiments conducted only on ImageNet validation set, restricting domain generalization insights
- Specific hyperparameter settings may influence attack success rates and transferability properties
- Focus on L∞ norm attacks may not capture vulnerabilities under different perturbation constraints

## Confidence
- Confidence in reduced black-box attack success on robust models: High
- Confidence in ensemble defenses vulnerability: Medium
- Confidence in paradoxical surrogate effect: Medium

## Next Checks
1. Verify the exact versions of RobustBench models and confirm the specific pretrained weights used in the experiments
2. Conduct additional experiments with a wider variety of robust models and ensemble configurations to further validate the vulnerability of ensemble defenses
3. Test the paradoxical improvement of black-box attacks using robust surrogates across different target model families and attack methods to generalize the findings