---
ver: rpa2
title: 'Simpler becomes Harder: Do LLMs Exhibit a Coherent Behavior on Simplified
  Corpora?'
arxiv_id: '2404.06838'
source_url: https://arxiv.org/abs/2404.06838
tags:
- simplification
- language
- prediction
- simplified
- change
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether pre-trained classifiers maintain
  prediction coherence on original and simplified texts across multiple languages.
  Using 11 pre-trained models on six datasets, the authors found that models change
  predictions for up to 50% of samples when exposed to simplified text.
---

# Simpler becomes Harder: Do LLMs Exhibit a Coherent Behavior on Simplified Corpora?

## Quick Facts
- **arXiv ID**: 2404.06838
- **Source URL**: https://arxiv.org/abs/2404.06838
- **Reference count**: 0
- **Primary result**: Pre-trained classifiers change predictions for up to 50% of samples when exposed to simplified text, with changes increasing with simplification strength.

## Executive Summary
This study investigates whether pre-trained classifiers maintain prediction coherence when processing original versus simplified texts across multiple languages. Using 11 pre-trained models on six datasets spanning English, German, and Italian, the authors found that models exhibit alarming inconsistencies, changing predictions for up to 50% of samples when presented with simplified versions. The prediction change rates increase with stronger simplification levels and are weakly correlated with edit distance. The findings reveal a significant robustness gap in language models when dealing with simplified inputs, suggesting potential for zero-iteration adversarial attacks using text simplification.

## Method Summary
The researchers evaluated 11 pre-trained models (BERT variants and GPT-3.5) on six simplification corpora spanning three languages. For each sample, they obtained predictions for both original and simplified text versions, then calculated the Prediction Change Rate (PCR) as the percentage of samples where predictions differed. The study analyzed factors influencing prediction changes including simplification strength, named entities, and specific simplification operations. Models were tested across various tasks including topic classification, sentiment analysis, and formality detection.

## Key Results
- Models change predictions for up to 50% of samples when processing simplified versus original texts
- Prediction change rates increase systematically with stronger simplification levels
- Named entities significantly impact prediction consistency, with effects varying by task
- Even state-of-the-art models like GPT-3.5 show sensitivity to text simplification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Models exhibit inconsistent behavior on simplified inputs due to distributional mismatch between training data and simplified language.
- Mechanism: Pre-trained classifiers learn patterns from standard language corpora during training. When presented with simplified versions that alter vocabulary, sentence structure, and complexity, the learned associations break down, leading to prediction changes.
- Core assumption: Simplified texts contain fundamentally different linguistic features than standard language, causing distribution shift.
- Evidence anchors: [abstract] "Our findings reveal alarming inconsistencies across all languages and models"; [section] "Our results show that models change their predictions for up to 50% of the samples, depending on the language and task used"

### Mechanism 2
- Claim: Stronger simplification levels lead to higher prediction change rates due to increased alteration of original text features.
- Mechanism: As simplification operations become more extensive (word deletions, sentence splitting, lexical substitution), the resulting text diverges further from the original distribution, causing classifiers to lose alignment with their learned patterns.
- Core assumption: Each simplification operation progressively removes or alters features critical for accurate classification.
- Evidence anchors: [abstract] "Prediction change rates increase with stronger simplification levels"; [section] "The prediction change rates in-crease with a higher level of simplification"

### Mechanism 3
- Claim: Named entities and their potential changes during simplification significantly impact model consistency.
- Mechanism: Named entities often carry critical classification information (topic, sentiment, etc.). When simplifications alter or generalize named entities, models lose access to this information, leading to prediction changes.
- Core assumption: Named entities are essential features for classification tasks and their modification disrupts model predictions.
- Evidence anchors: [section] "We further investigated which simplification operations especially tempt the classifiers to change their predictions" and "Named entities (NE) can strongly impact the sentiment or topic of a phrase"

## Foundational Learning

- Concept: Text simplification and its linguistic features
  - Why needed here: Understanding what text simplification entails and how it alters language is crucial for interpreting why models fail on simplified inputs.
  - Quick check question: What are the main types of simplification operations mentioned in the paper, and how might each affect a text classifier's ability to maintain consistent predictions?

- Concept: Adversarial attacks and model robustness
  - Why needed here: The paper frames simplification as a zero-iteration adversarial attack, requiring understanding of how such attacks work and why they're effective.
  - Quick check question: How does a zero-iteration adversarial attack differ from other types, and why does this make text simplification particularly effective as an attack vector?

- Concept: Distributional shift and domain adaptation
  - Why needed here: The core issue appears to be distributional mismatch between training data and simplified language, which relates to broader concepts of domain adaptation.
  - Quick check question: What is distributional shift, and how might it explain why models trained on standard language perform poorly on simplified inputs?

## Architecture Onboarding

- Component map: Pre-trained classifiers (BERT variants, GPT-3.5) → Original and simplified text pairs → Dual prediction generation → Prediction comparison → PCR calculation → Analysis of change factors
- Critical path: Model selection → Text simplification application → Dual prediction generation → Prediction comparison → Analysis of change factors. The most critical step is ensuring that simplified texts are truly meaning-preserving and aligned with originals.
- Design tradeoffs: Using pre-trained models allows for quick experimentation across multiple tasks but introduces domain mismatch issues. The choice of simplification corpora affects generalizability - human-created datasets ensure quality but may not represent all simplification scenarios.
- Failure signatures: High prediction change rates (>20%) indicate model sensitivity to simplification. Task-specific failures (e.g., named entity changes affecting topic classification more than sentiment) reveal which features are most critical for different tasks.
- First 3 experiments:
  1. Replicate the basic PCR calculation on a small subset of data to verify the methodology works as expected.
  2. Test the effect of named entity masking on a single task/language combination to validate the NE hypothesis.
  3. Compare prediction changes between strong and weak simplifications on the same model to confirm the strength relationship.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the lack of robustness to text simplification impact the deployment of language models in real-world applications for users with reading difficulties?
- Basis in paper: [explicit] The paper highlights that prediction change rates of up to 50% can be exploited as zero-iteration model-agnostic adversarial attacks.
- Why unresolved: The paper discusses the technical findings but does not explore the practical implications or mitigation strategies for deployment.

### Open Question 2
- Question: What specific pre-training data or techniques could improve model robustness to text simplification?
- Basis in paper: [inferred] The paper suggests that pre-training data lacks samples in simplified language, leading to incoherence.
- Why unresolved: The paper identifies the issue but does not propose specific solutions or data augmentation techniques to address it.

### Open Question 3
- Question: How do different types of simplification operations (e.g., word substitution vs. sentence splitting) differentially affect model consistency across various tasks?
- Basis in paper: [explicit] The paper discusses the impact of different simplification operations on prediction change rates but does not provide a detailed comparative analysis.
- Why unresolved: The analysis is limited to a subset of operations and tasks, leaving broader patterns unexplored.

### Open Question 4
- Question: To what extent does the quality of human-aligned simplification datasets affect model robustness to simplified inputs?
- Basis in paper: [inferred] The paper uses human-created or human-aligned datasets but does not evaluate the impact of dataset quality on model performance.
- Why unresolved: The paper assumes high quality but does not empirically assess how variations in dataset quality influence model robustness.

## Limitations

- Generalizability across simplification methods: Findings based on human-created corpora may not extend to automated simplification tools or different simplification strategies.
- Task complexity confounding: The relationship between task difficulty and simplification sensitivity remains unclear, with complex tasks potentially being inherently more sensitive to input perturbations.
- Model-specific behaviors: The aggregation across 11 models may mask important architectural differences in robustness to simplification.

## Confidence

- **High confidence**: The core finding that prediction change rates increase with stronger simplification levels is well-supported by quantitative results across multiple datasets and languages.
- **Medium confidence**: The claim that named entities significantly impact prediction consistency is supported by experiments but shows task-dependent effects.
- **Low confidence**: The framing of simplification as a zero-iteration adversarial attack lacks direct evidence of systematic exploitation in real-world scenarios.

## Next Checks

1. Cross-simplification method validation: Test the same models on outputs from multiple automated simplification tools to determine if findings generalize beyond human-created corpora.
2. Fine-tuning robustness test: Fine-tune models on simplified versions of their training data and re-run the experiments to assess whether adaptation mitigates prediction changes.
3. Feature importance analysis: Conduct ablation studies to identify which specific linguistic features (vocabulary, syntax, named entities) contribute most to prediction changes when simplified.