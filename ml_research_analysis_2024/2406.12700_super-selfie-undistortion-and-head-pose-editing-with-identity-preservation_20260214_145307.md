---
ver: rpa2
title: 'SUPER: Selfie Undistortion and Head Pose Editing with Identity Preservation'
arxiv_id: '2406.12700'
source_url: https://arxiv.org/abs/2406.12700
tags:
- face
- image
- head
- camera
- super
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of perspective distortion and
  head pose misalignment in close-up selfies, which can result in unnatural or unattractive
  facial features. The authors propose SUPER, a novel method that combines 3D GAN
  inversion with visibility-based blending to eliminate distortions and adjust head
  pose while preserving identity.
---

# SUPER: Selfie Undistortion and Head Pose Editing with Identity Preservation

## Quick Facts
- **arXiv ID:** 2406.12700
- **Source URL:** https://arxiv.org/abs/2406.12700
- **Reference count:** 0
- **Primary result:** Combines 3D GAN inversion with visibility-based blending to eliminate perspective distortion and adjust head pose while preserving identity

## Executive Summary
This paper addresses the problem of perspective distortion and head pose misalignment in close-up selfies, which can result in unnatural or unattractive facial features. The authors propose SUPER, a novel method that combines 3D GAN inversion with visibility-based blending to eliminate distortions and adjust head pose while preserving identity. The method optimizes camera parameters and face latent code using a 3D GAN, estimates depth from the latent code, creates a depth-induced 3D mesh, and renders it with updated camera parameters. The final novel view is synthesized by blending the warped image (from the mesh) and the generated image (from the 3D GAN) based on visibility.

## Method Summary
SUPER addresses selfie distortion by optimizing camera parameters and face latent codes within a 3D GAN framework. The method first inverts an input selfie into a 3D GAN latent space to obtain both a face representation and depth estimation. A 3D mesh is then constructed from this depth information, which can be rendered from any desired camera viewpoint. The system optimizes camera parameters to achieve the target head pose while maintaining the subject's identity. The final output combines a warped version of the original image (based on the mesh) with a GAN-generated image through visibility-based blending, ensuring both geometric consistency and high-quality appearance.

## Key Results
- Achieves higher PSNR, SSIM, and ID scores than previous approaches on face undistortion benchmarks
- Outperforms prior methods both qualitatively and quantitatively on the newly collected Head Rotation dataset
- Successfully preserves identity while eliminating perspective distortions and adjusting head pose

## Why This Works (Mechanism)
SUPER works by leveraging the 3D structure learned by GANs to reason about facial geometry. By inverting selfies into 3D GAN latent space, the method gains access to depth information that enables realistic 3D mesh reconstruction. The visibility-based blending approach ensures that the final output maintains consistency between the geometric warping of the original image and the photorealistic rendering from the GAN, addressing the limitations of pure GAN generation or simple image warping approaches.

## Foundational Learning
- **3D GAN Inversion:** The process of mapping an image back into the latent space of a 3D-aware GAN. Needed to access both facial appearance and underlying 3D structure from the input selfie. Quick check: Verify that the inverted latent code produces a reconstruction close to the input.
- **Depth Estimation from Latent Code:** Extracting 3D geometry information from the GAN's latent representation. Required for constructing the 3D mesh used in warping. Quick check: Compare estimated depth maps against ground truth when available.
- **Visibility-Based Blending:** A compositing technique that weights contributions from multiple sources based on which parts are visible from the target viewpoint. Essential for combining warped and generated imagery without artifacts. Quick check: Ensure seamless transitions at blending boundaries.
- **Camera Parameter Optimization:** Adjusting intrinsic and extrinsic camera parameters to achieve desired viewpoint changes. Needed to correct perspective distortion and enable head pose editing. Quick check: Validate that optimized parameters produce geometrically correct results.
- **3D Mesh Rendering:** Projecting a 3D mesh onto a 2D image plane from a specific viewpoint. Required to generate the warped image component. Quick check: Confirm correct perspective projection and texture mapping.

## Architecture Onboarding

**Component Map:** Input Image -> 3D GAN Inversion -> Depth Estimation -> 3D Mesh Construction -> Camera Parameter Optimization -> Mesh Rendering + GAN Generation -> Visibility-Based Blending -> Output Image

**Critical Path:** The most time-consuming operations are 3D GAN inversion and mesh rendering. The blending step is relatively fast but depends on the quality of both inputs.

**Design Tradeoffs:** The method trades computational complexity for higher quality results compared to 2D-only approaches. The use of 3D GAN inversion provides better identity preservation than pure image-based methods but requires more processing time.

**Failure Signatures:** Common failure modes include incorrect depth estimation leading to mesh artifacts, suboptimal camera parameter optimization resulting in residual distortion, and visibility estimation errors causing blending artifacts.

**First Experiments:**
1. Test reconstruction quality by comparing inverted latent code outputs to original inputs
2. Validate depth estimation by comparing against ground truth depth maps on available datasets
3. Evaluate blending quality by examining transitions between warped and generated regions

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Performance on extreme head poses beyond tested ranges remains unverified
- Potential robustness issues with diverse skin tones and facial features not well-represented in training data
- Computational cost may limit real-time applications

## Confidence
- **High confidence** in identity preservation claims based on reported ID scores
- **High confidence** in distortion removal based on PSNR and SSIM metrics
- **Medium confidence** in superiority over previous approaches given limited Head Rotation dataset scale
- **Medium confidence** in generalizability to in-the-wild conditions and unseen head poses

## Next Checks
1. Test SUPER on extreme head poses (±45° yaw/pitch) not covered in current benchmarks to assess robustness limits
2. Evaluate identity preservation using cross-database testing with subjects absent from training set
3. Measure inference time and memory requirements across different hardware configurations to establish practical deployment constraints