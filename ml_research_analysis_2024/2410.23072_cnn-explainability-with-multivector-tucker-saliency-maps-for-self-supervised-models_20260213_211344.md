---
ver: rpa2
title: CNN Explainability with Multivector Tucker Saliency Maps for Self-Supervised
  Models
arxiv_id: '2410.23072'
source_url: https://arxiv.org/abs/2410.23072
tags:
- methods
- saliency
- eigencam
- maps
- mtsm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of explaining decisions made
  by Convolutional Neural Networks (CNNs), particularly self-supervised models. Existing
  methods for generating saliency maps are often label-dependent, limiting their applicability.
---

# CNN Explainability with Multivector Tucker Saliency Maps for Self-Supervised Models

## Quick Facts
- arXiv ID: 2410.23072
- Source URL: https://arxiv.org/abs/2410.23072
- Reference count: 35
- This paper proposes Tucker Saliency Maps (TSM) using Tucker tensor decomposition to improve CNN explainability, achieving approximately 50% better performance than EigenCAM on both supervised and self-supervised models.

## Executive Summary
This paper addresses the challenge of explaining decisions made by Convolutional Neural Networks (CNNs), particularly self-supervised models. Existing methods for generating saliency maps are often label-dependent, limiting their applicability. The authors propose Tucker Saliency Maps (TSM), which leverages Tucker tensor decomposition to better capture the inherent structure of feature maps compared to existing methods like EigenCAM that use Singular Value Decomposition (SVD). TSM produces more accurate singular vectors and values, resulting in high-fidelity saliency maps that effectively highlight objects of interest. The authors extend both EigenCAM and TSM into multivector variants, Multivec-EigenCAM and Multivector Tucker Saliency Maps (MTSM), which utilize all singular vectors and values for richer saliency maps. Evaluations demonstrate that TSM enhances explainability by approximately 50% over EigenCAM for both supervised and self-supervised models. MTSM achieves the best results, advancing state-of-the-art explainability performance on self-supervised models.

## Method Summary
The proposed method involves applying Tucker tensor decomposition to feature maps extracted from the last convolutional layer of CNN models. The decomposition generates singular vectors and values that are used to create weighted sums of feature maps, producing saliency maps. The multivector variants extend this approach by utilizing all singular vectors and values rather than just the first one, creating richer and more informative saliency maps. The method is evaluated using metrics such as Average Drop, Average Increase, Mean Squared Error, and mean Intersection over Union on standard datasets including ImageNet and Pascal VOC.

## Key Results
- TSM achieves approximately 50% better explainability performance compared to EigenCAM for both supervised and self-supervised models
- MTSM produces the best results among all evaluated methods, setting a new state-of-the-art for self-supervised model explainability
- The multivector approach consistently outperforms single-vector methods across all evaluation metrics

## Why This Works (Mechanism)
Tucker tensor decomposition provides a more structured and accurate way to capture the inherent relationships within feature maps compared to traditional SVD. By preserving the multi-way nature of feature maps, TSM generates more faithful representations of what the CNN considers important for its decisions. The multivector extension further improves this by incorporating information from all singular vectors, creating saliency maps that better capture the complexity of CNN decision-making processes.

## Foundational Learning
- **Tucker Tensor Decomposition**: A higher-order generalization of matrix decomposition that preserves multi-way structures; needed to capture the inherent structure of feature maps; quick check: verify decomposition preserves original tensor information
- **Saliency Map Generation**: Process of creating visual explanations for CNN decisions; needed to understand how different decomposition methods affect explanation quality; quick check: ensure saliency maps highlight relevant image regions
- **Self-Supervised Learning**: Training models without explicit labels; needed as the primary application domain for this work; quick check: confirm models can learn meaningful representations without supervision

## Architecture Onboarding

**Component Map**: Feature maps -> Tucker decomposition -> Singular vectors/values -> Weighted sum -> Normalized saliency map

**Critical Path**: The core workflow involves extracting feature maps from CNN layers, applying Tucker decomposition to obtain singular vectors and values, creating weighted sums using these components, and normalizing the results to produce final saliency maps.

**Design Tradeoffs**: The method trades computational complexity for improved explainability quality. Tucker decomposition is more computationally intensive than SVD but provides better structured representations. The multivector approach increases information content at the cost of additional computation.

**Failure Signatures**: Poor saliency maps may result from incorrect feature map extraction, numerical instability in tensor decomposition, or improper normalization. Common failure modes include saliency maps that don't align with actual objects of interest or produce overly noisy visualizations.

**First Experiments**:
1. Apply TSM to a simple CNN trained on CIFAR-10 to verify basic functionality
2. Compare TSM and EigenCAM on a small subset of ImageNet images to observe qualitative differences
3. Test multivector extension with varying numbers of singular vectors to determine optimal configuration

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation is limited to standard benchmarks (ImageNet, Pascal VOC) without exploring more challenging or diverse datasets
- Comparison with other explainability methods is limited to EigenCAM and its multivector variant, missing opportunities to benchmark against more recent approaches
- Computational efficiency is not thoroughly analyzed, particularly for real-time applications

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Tucker decomposition methodology | High |
| Quantitative evaluation results | Medium |
| Practical applicability | Medium |

## Next Checks
1. Test TSM/MTSM on out-of-distribution images and adversarial examples to assess robustness
2. Conduct ablation studies varying the number of singular vectors used in MTSM to determine optimal configuration
3. Compare computational efficiency against other state-of-the-art explainability methods, particularly for real-time applications