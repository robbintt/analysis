---
ver: rpa2
title: Modality-Inconsistent Continual Learning of Multimodal Large Language Models
arxiv_id: '2412.13050'
source_url: https://arxiv.org/abs/2412.13050
tags:
- task
- step
- image
- learning
- captioning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Modality-Inconsistent Continual Learning (MICL),
  a new continual learning scenario for Multimodal Large Language Models (MLLMs) that
  combines modality shifts (image, audio, video) and task type shifts (captioning,
  QA). This setting leads to catastrophic forgetting as models struggle to retain
  knowledge across inconsistent modalities and tasks.
---

# Modality-Inconsistent Continual Learning of Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2412.13050
- Source URL: https://arxiv.org/abs/2412.13050
- Reference count: 22
- The paper introduces Modality-Inconsistent Continual Learning (MICL) for Multimodal Large Language Models (MLLMs), combining modality shifts (image, audio, video) and task type shifts (captioning, QA), and proposes MoInCL to address catastrophic forgetting with up to 74% reduction in forgetting.

## Executive Summary
This paper introduces Modality-Inconsistent Continual Learning (MICL), a novel continual learning scenario for Multimodal Large Language Models (MLLMs) that addresses the challenge of learning across inconsistent modalities and task types. The setting leads to catastrophic forgetting as models struggle to retain knowledge when exposed to new modalities and tasks sequentially. To tackle this, the authors propose MoInCL, a method that leverages Pseudo Target Generation for handling task type shifts within seen modalities and Instruction-based Knowledge Distillation to preserve modality- and task-aware knowledge when learning new modalities.

## Method Summary
MoInCL addresses modality-inconsistent continual learning by integrating two core mechanisms. The Pseudo Target Generation Module synthesizes intermediate targets to bridge task type shifts within already-seen modalities, enabling the model to adapt to new tasks without forgetting prior knowledge. The Instruction-based Knowledge Distillation constraint preserves modality- and task-specific information when the model encounters new modalities, ensuring that previously learned capabilities are retained. This dual approach allows the model to effectively manage both modality and task type shifts, reducing catastrophic forgetting in the MICL setting.

## Key Results
- MoInCL significantly outperforms state-of-the-art baselines, reducing average forgetting by up to 74%.
- Achieves the highest average CIDEr score (55.31) and accuracy (42.29) across task orders.
- Demonstrates robust performance across six tasks spanning three modalities (image, audio, video).

## Why This Works (Mechanism)
MoInCL works by addressing the core challenge of catastrophic forgetting in modality-inconsistent continual learning through two complementary strategies. The Pseudo Target Generation Module enables the model to adapt to new task types within seen modalities by synthesizing intermediate targets, preventing disruption of existing knowledge. The Instruction-based Knowledge Distillation constraint ensures that when new modalities are introduced, the model retains previously learned modality- and task-specific knowledge by distilling it into the new model parameters. Together, these mechanisms allow the model to flexibly adapt to both modality and task shifts without losing prior capabilities.

## Foundational Learning
- **Catastrophic Forgetting**: Why needed: Models lose previously learned knowledge when trained on new tasks sequentially. Quick check: Monitor performance drop on old tasks after training on new ones.
- **Knowledge Distillation**: Why needed: Preserves learned knowledge when adapting to new tasks or modalities. Quick check: Compare student model performance with and without distillation from teacher.
- **Modality Shift**: Why needed: Models must generalize across different input types (image, audio, video). Quick check: Evaluate model performance on each modality independently.
- **Task Type Shift**: Why needed: Models must handle different types of outputs (captioning vs QA) within same modality. Quick check: Measure accuracy across different task types within same modality.
- **Continual Learning**: Why needed: Enables lifelong learning without catastrophic forgetting. Quick check: Track performance stability across sequential task learning.

## Architecture Onboarding
- **Component Map**: Input Modality -> Pseudo Target Generation -> Instruction-based Knowledge Distillation -> Output
- **Critical Path**: Modality input → Task type identification → Pseudo target synthesis → Knowledge distillation → Final prediction
- **Design Tradeoffs**: Pseudo targets vs. real data collection (synthetic data saves annotation costs but may introduce bias); distillation temperature balancing (higher preserves more knowledge but may slow adaptation).
- **Failure Signatures**: Performance degradation on previously learned tasks; inability to generalize to new task types within seen modalities; loss of modality-specific knowledge when new modalities are introduced.
- **First 3 Experiments**: 1) Evaluate forgetting on seen tasks after learning new modalities; 2) Test task type generalization within each modality; 3) Compare performance with and without knowledge distillation across task orders.

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Evaluation scope limited to six tasks across three modalities, leaving uncertainty about generalization to other multimodal domains.
- Assumes prior knowledge of task types during training and inference, which may not hold in open-world scenarios.
- Relies on limited synthetic data for Pseudo Target Generation, raising concerns about scalability and fidelity for more complex task types.

## Confidence
- High: Experimental results demonstrate consistent improvements over baselines in the studied modality-inconsistent continual learning setting, with clear reductions in catastrophic forgetting.
- Medium: Effectiveness of Pseudo Target Generation Module and Instruction-based Knowledge Distillation is supported by results, but mechanisms are not fully validated beyond tested tasks.
- Low: Generalizability of MoInCL to other multimodal domains, open-world task scenarios, and real-world deployment settings remains uncertain due to limited scope and lack of robustness analysis.

## Next Checks
1. Evaluate MoInCL on a broader set of multimodal tasks, including cross-modal retrieval and multimodal reasoning, to test generalizability beyond captioning and QA.
2. Conduct experiments in open-world settings where task types are unknown or emerge dynamically, to assess adaptability without manual template engineering.
3. Perform robustness tests using adversarial examples, domain-shifted inputs, and noisy multimodal data to verify stability under real-world conditions.