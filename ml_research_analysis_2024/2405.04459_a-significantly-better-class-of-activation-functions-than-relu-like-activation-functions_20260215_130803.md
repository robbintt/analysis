---
ver: rpa2
title: A Significantly Better Class of Activation Functions Than ReLU Like Activation
  Functions
arxiv_id: '2405.04459'
source_url: https://arxiv.org/abs/2405.04459
tags: []
core_contribution: This paper proposes a new class of activation functions (Cone and
  Parabolic-Cone) that outperform ReLU-like and sigmoidal activation functions on
  CIFAR-10 and Imagenette benchmarks. Unlike ReLU-like functions that separate inputs
  using half-spaces, Cone-like functions use hyper-strips (regions between two parallel
  hyper-planes) to separate positive and negative classes.
---

# A Significantly Better Class of Activation Functions Than ReLU Like Activation Functions

## Quick Facts
- arXiv ID: 2405.04459
- Source URL: https://arxiv.org/abs/2405.04459
- Reference count: 4
- This paper proposes a new class of activation functions (Cone and Parabolic-Cone) that outperform ReLU-like and sigmoidal activation functions on CIFAR-10 and Imagenette benchmarks.

## Executive Summary
This paper introduces a novel class of activation functions called Cone and Parabolic-Cone that outperform traditional ReLU-like functions on image classification tasks. Unlike ReLU which creates half-spaces (unbounded regions on one side of a hyperplane), Cone-like functions create hyper-strips (bounded regions between two parallel hyperplanes), allowing for more precise partitioning of the input feature space. The authors demonstrate that these activation functions achieve higher accuracies with significantly fewer neurons and speed up training due to larger derivative values.

## Method Summary
The paper proposes two new activation functions: Cone activation function g(z) = 1 - |z - 1| and Parabolic-Cone activation function g(z) = z(2 - z). These functions were evaluated on CIFAR-10 and Imagenette datasets using CNN architectures with a single dense layer followed by a softmax output layer. The models were trained using the Adam optimizer with a learning rate of 1e-4 for 30 epochs, with categorical cross-entropy loss. The key innovation is the geometric property that Cone-like functions create hyper-strips rather than half-spaces, enabling more precise classification boundaries.

## Key Results
- Cone and Parabolic-Cone activation functions outperform ReLU and Leaky ReLU on CIFAR-10 and Imagenette benchmarks
- These functions achieve higher accuracy with significantly fewer neurons (e.g., 10 neurons vs 32-64 for ReLU)
- Cone-like activation functions significantly speed up training due to larger derivative values compared to ReLU
- The XOR problem can be solved with a single neuron using Cone-like activation functions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cone and Parabolic-Cone activation functions enable more precise partitioning of input space compared to ReLU-like functions.
- Mechanism: Unlike ReLU which creates half-spaces (infinite regions on one side of a hyperplane), Cone-like functions create hyper-strips (bounded regions between two parallel hyperplanes). This allows neurons to make finer, bounded decisions about input classification.
- Core assumption: Real-world datasets can be separated more efficiently using bounded regions rather than unbounded half-spaces.
- Evidence anchors:
  - [abstract]: "Cone-like functions use hyper-strips (regions between two parallel hyper-planes) to separate positive and negative classes. This allows neurons to more finely divide the input feature space."
  - [section 1.1]: "Since a hyper strip is the region between two parallel hyper-planes, it allows neurons to more finely divide the input feature space into positive and negative classes than with infinitely wide half-spaces."
  - [corpus]: Weak evidence - no directly comparable mechanisms found in neighbor papers.
- Break condition: If the dataset structure is inherently aligned with half-space separation, the hyper-strip advantage disappears.

### Mechanism 2
- Claim: Cone-like activation functions solve the XOR problem with a single neuron.
- Mechanism: Because C+ (inputs producing positive output) is a hyper-strip rather than a half-space, a single neuron can separate the XOR pattern (two diagonal regions) that normally requires at least two neurons with ReLU-like functions.
- Core assumption: The XOR pattern represents a fundamental limitation of half-space separation that hyper-strips overcome.
- Evidence anchors:
  - [abstract]: "In particular the XOR function can be learn by a single neuron with cone-like activation functions."
  - [section 1.1]: "Fig. 5 illustrates how the classic XOR problem can be solved with a single neuron with Cone-like activation function."
  - [corpus]: Weak evidence - no XOR-specific analysis in neighbor papers.
- Break condition: If the XOR pattern is extended to higher dimensions where hyper-strip representation becomes insufficient.

### Mechanism 3
- Claim: Larger derivative values in Cone-like functions accelerate training compared to ReLU.
- Mechanism: Cone and Parabolic-Cone activation functions maintain larger gradients across their input range, avoiding the vanishing gradient problem that affects sigmoidal functions and the dead neuron problem that affects ReLU in the negative region.
- Core assumption: Larger gradients directly translate to faster convergence in optimization.
- Evidence anchors:
  - [abstract]: "The Cone and Parabolic-Cone activation functions have larger derivatives than ReLU and are shown to significantly speedup training."
  - [section 1.1]: "Cone-like activation functions never have small or zero derivative for any input. Cone-like activation functions also have larger derivative values than ReLU for most inputs facilitating faster learning."
  - [section 2]: "The Cone and the Parabolic-Cone activation functions proposed in this paper also significantly speedup training (Fig. 6)."
- Break condition: If the larger gradients lead to instability or overshooting during optimization, potentially causing exploding gradient problems.

## Foundational Learning

- Concept: Activation function properties and their impact on neural network behavior
  - Why needed here: Understanding how different activation functions shape decision boundaries is crucial for grasping why Cone-like functions offer advantages over traditional ReLU
  - Quick check question: What is the fundamental difference between how ReLU and Cone-like functions partition the input space?

- Concept: Hyperplane geometry and convex sets in high-dimensional spaces
  - Why needed here: The paper relies heavily on geometric concepts like hyperplanes, half-spaces, and hyper-strips to explain the advantages of the new activation functions
  - Quick check question: How many connected regions does a single hyperplane divide an n-dimensional space into?

- Concept: XOR problem and linear separability
  - Why needed here: The XOR problem serves as a key example demonstrating why Cone-like functions can solve problems that ReLU-based networks cannot with a single neuron
  - Quick check question: Why can't a single linear classifier (perceptron) solve the XOR problem?

## Architecture Onboarding

- Component map: Input → Convolutional layers → Dense layer (Cone/Parabolic-Cone) → Softmax → Loss computation → Backpropagation
- Critical path: Data → Convolutional layers → Dense layer (Cone/Parabolic-Cone) → Softmax → Loss computation → Backpropagation with larger gradients
- Design tradeoffs:
  - Advantage: Fewer neurons needed for equivalent accuracy, faster training
  - Disadvantage: Cone-like functions only output values in [-∞, 1], which may require normalization adjustments
  - Risk: Larger gradients could potentially lead to training instability if not properly managed
- Failure signatures:
  - Training instability or exploding gradients due to large derivative values
  - Underperformance if dataset structure doesn't benefit from hyper-strip separation
  - Convergence to suboptimal solutions if learning rate isn't properly tuned for larger gradients
- First 3 experiments:
  1. Replicate CIFAR-10 results: Replace ReLU in a single dense layer with Cone activation and compare accuracy with 10, 32, and 64 neurons
  2. Test XOR problem: Create a synthetic XOR dataset and verify that a single neuron with Cone activation can learn it perfectly
  3. Gradient analysis: Plot and compare derivative values across input ranges for ReLU, Cone, and Parabolic-Cone to verify the larger gradient claim

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do activation functions with even larger derivatives than Cone-like activation functions lead to faster training or exploding gradients?
- Basis in paper: [explicit] The paper states "The fundamental question of whether activation functions with even larger derivatives will train faster or lead to the exploding gradient problem remains unanswered."
- Why unresolved: The paper does not provide experimental results comparing Cone-like activation functions with other activation functions that have even larger derivatives.
- What evidence would resolve it: Experimental results comparing the training speed and final accuracy of models using Cone-like activation functions versus models using activation functions with even larger derivatives.

### Open Question 2
- Question: Are there other types of decision boundaries (besides hyper-strips) that can lead to smaller neural networks?
- Basis in paper: [inferred] The paper focuses on hyper-strips as a way to more finely divide the input feature space. It is possible that other types of decision boundaries could also achieve this goal.
- Why unresolved: The paper does not explore other types of decision boundaries beyond hyper-strips.
- What evidence would resolve it: Research investigating the use of different types of decision boundaries in neural networks and comparing their performance to hyper-strips.

### Open Question 3
- Question: Can Cone-like activation functions be extended to recurrent neural networks (RNNs) and transformers to improve their performance?
- Basis in paper: [inferred] The paper only evaluates Cone-like activation functions on convolutional neural networks (CNNs). It is unclear whether these activation functions can be effectively used in other types of neural networks.
- Why unresolved: The paper does not provide any experimental results using Cone-like activation functions in RNNs or transformers.
- What evidence would resolve it: Experimental results comparing the performance of RNNs and transformers using Cone-like activation functions versus other commonly used activation functions.

## Limitations
- The paper demonstrates significant improvements on two image datasets but lacks extensive validation across diverse problem domains.
- The comparison is limited to single dense layer architectures, leaving questions about performance in deeper networks.
- The unbounded negative output range of Cone-like functions (-∞ to 1) may introduce numerical stability challenges not addressed in the paper.

## Confidence
- **High confidence**: The geometric argument for hyper-strip advantages over half-spaces is mathematically sound and well-established in convex analysis literature.
- **Medium confidence**: Empirical results showing improved accuracy and training speed on CIFAR-10 and Imagenette are promising but require broader validation across different network architectures and problem types.
- **Low confidence**: Claims about solving the XOR problem with a single neuron, while theoretically plausible, lack detailed experimental verification and may not scale to higher-dimensional analogs.

## Next Checks
1. Test Cone-like activation functions across diverse benchmark datasets (MNIST, Fashion-MNIST, SVHN) with varying network depths to assess generalizability beyond the reported experiments.
2. Implement ablation studies comparing Cone activation in deeper networks (2-3 hidden layers) versus single-layer architectures to understand how the benefits scale with network complexity.
3. Conduct rigorous numerical stability analysis by monitoring gradient norms, activation distributions, and weight updates throughout training to identify potential optimization challenges with the unbounded negative output range.