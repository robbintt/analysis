---
ver: rpa2
title: Contrastive Balancing Representation Learning for Heterogeneous Dose-Response
  Curves Estimation
arxiv_id: '2403.14232'
source_url: https://arxiv.org/abs/2403.14232
tags: []
core_contribution: This paper tackles heterogeneous dose-response curve estimation
  in continuous treatment settings by introducing a novel Contrastive balancing Representation
  learning Network (CRNet). The key innovation is the "double balancing representation"
  condition, which combines both balancing representation (ensuring treatment-covariate
  independence) and prognostic representation (ensuring covariate-outcome independence)
  for unbiased HDRC estimation.
---

# Contrastive Balancing Representation Learning for Heterogeneous Dose-Response Curves Estimation

## Quick Facts
- arXiv ID: 2403.14232
- Source URL: https://arxiv.org/abs/2403.14232
- Authors: Minqin Zhu; Anpeng Wu; Haoxuan Li; Ruoxuan Xiong; Bo Li; Xiaoqing Yang; Xuan Qin; Peng Zhen; Jiecheng Guo; Fei Wu; Kun Kuang
- Reference count: 20
- Key outcome: CRNet achieves MISE scores of 1.69 (Data-1), 2.07 (Data-2), and 3.05 (Data-5), significantly outperforming baseline methods across varying treatment dimensions

## Executive Summary
This paper introduces CRNet, a novel method for estimating heterogeneous dose-response curves (HDRC) in continuous treatment settings. The key innovation is the "double balancing representation" condition, which combines balancing representation (ensuring treatment-covariate independence) and prognostic representation (ensuring covariate-outcome independence) to achieve unbiased HDRC estimation. CRNet employs a contrastive regularizer with partial distance measures to learn treatment-balanced representations while maintaining treatment continuity, paired with a mean squared error loss for prognostic representation. Experiments demonstrate state-of-the-art performance across synthetic and semi-synthetic datasets.

## Method Summary
CRNet is a two-head neural network that learns representations satisfying both balancing and prognostic conditions. The method uses a contrastive regularizer with partial distance measures to enforce treatment-covariate independence while preserving treatment continuity. A mean squared error loss ensures outcome prediction accuracy. The double balancing representation condition (T⊥⊥X|Φ(X) and Y⊥⊥X|Φ(X)) is enforced through a combination of contrastive learning and direct outcome prediction, enabling unbiased HDRC estimation without discretizing treatments.

## Key Results
- CRNet achieves MISE scores of 1.69 on Data-1, 2.07 on Data-2, and 3.05 on Data-5
- Outperforms baseline methods including DRNet, VCNet, and SCIGAN across all tested datasets
- Demonstrates effectiveness in high-dimensional treatment settings (Data-10)
- Shows robustness across varying treatment dimensions and dataset complexities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The double balancing representation ensures unbiased HDRC estimation by satisfying both treatment-covariate and outcome-covariate conditional independence.
- Mechanism: CRNet uses a contrastive regularizer with partial distance measures to enforce the balancing representation (T⊥⊥X|Φ(X)) while the MSE loss ensures the prognostic representation (Y⊥⊥X|Φ(X)).
- Core assumption: The observed outcomes Y are sufficient to approximate the potential outcomes Y(t) for the prognostic representation condition.
- Evidence anchors:
  - [abstract] "the learned representations are constrained to satisfy the conditional independence between the covariates and both of the treatment variables and the potential responses"
  - [section] "We systematically define a double balancing representation condition which satisfies the conditional independence constraint between the covariates and both of the continuous treatments and the observed outcomes"
  - [corpus] Weak evidence - no directly comparable methods found in corpus
- Break condition: If the unconfoundedness assumption fails (Y(t) not independent of T given X), the double balancing representation cannot ensure unbiasedness.

### Mechanism 2
- Claim: The contrastive regularizer with partial distance measures preserves treatment continuity while enforcing balancing representation.
- Mechanism: Positive samples (original X) and negative samples (shuffled X') are compared using partial distance measures, with the loss function designed to minimize the distance for positive samples while maximizing it for negative samples.
- Core assumption: The partial distance measure effectively captures conditional dependence between treatments and covariates.
- Evidence anchors:
  - [section] "we adopt partial distance measure (Sz´ekely and Rizzo 2014) to evaluate the unbiasedness condition and design a contrastive regularizer loss to minimize the partial distance measure while discriminating among positive and negative samples"
  - [abstract] "a novel Contrastive balancing Representation learning Network using a partial distance measure, called CRNet, for estimating the heterogeneous dose-response curves without losing the continuity of treatments"
  - [corpus] No direct evidence found in corpus for this specific mechanism
- Break condition: If the partial distance measure fails to capture the true conditional dependence structure, the balancing representation may be incorrectly learned.

### Mechanism 3
- Claim: The MSE loss on double balancing representations directly enforces the prognostic representation condition without mode collapse.
- Mechanism: Unlike previous methods that use propensity scores or treat-balanced representations, CRNet applies MSE loss directly to representations that satisfy both balancing conditions, ensuring confounder information is preserved.
- Core assumption: The double balancing representation contains sufficient confounder information for accurate outcome prediction.
- Evidence anchors:
  - [section] "we design a two-head neural network... we adopt a mean square error loss (MSE) to directly constrain the condition in the double balancing representation that Y⊥⊥X|Φ(X)"
  - [abstract] "we design a mean squared error loss specifically tailored to address prognostic representation"
  - [corpus] Weak evidence - no directly comparable methods found in corpus
- Break condition: If the double balancing representation loses critical confounder information, the MSE loss cannot ensure accurate outcome prediction.

## Foundational Learning

- Concept: Conditional independence and its role in causal inference
  - Why needed here: The entire method relies on enforcing conditional independence constraints (T⊥⊥X|Φ(X) and Y⊥⊥X|Φ(X)) to ensure unbiased HDRC estimation
  - Quick check question: Can you explain why X⊥⊥T|Φ(X) is equivalent to Y(t)⊥⊥T|Φ(X) under the unconfoundedness assumption?

- Concept: Contrastive learning and its application to causal representation learning
  - Why needed here: The contrastive regularizer uses positive and negative samples to enforce the balancing representation without discretizing treatments or causing mode collapse
  - Quick check question: How does the contrastive loss in CRNet differ from standard contrastive learning used in computer vision?

- Concept: Partial distance measures and their properties
  - Why needed here: Partial distance measures are used to quantify conditional dependence between treatments and covariates in the contrastive regularizer
  - Quick check question: What is the relationship between partial distance measures and conditional independence in Gaussian vs non-Gaussian settings?

## Architecture Onboarding

- Component map: X, T → Φ(X), Ψ(T) → g(Φ(X)) → ℓCR + h(Φ(X), Ψ(T)) → ℓMSE → final loss

- Critical path: Covariates and treatments are encoded through two-head network, passed through projection head for contrastive learning, and used for outcome prediction with MSE loss

- Design tradeoffs:
  - Tradeoff between α (contrastive loss weight) and model capacity for outcome prediction
  - Tradeoff between representation dimension KΦ(X) and computational efficiency
  - Tradeoff between number of negative sample augmentations m and training stability

- Failure signatures:
  - High MISE scores with high variance suggest mode collapse or insufficient balancing
  - Poor performance on high-dimensional treatment settings suggests inadequate contrastive regularization
  - Degraded performance when α=0 indicates balancing representation is critical

- First 3 experiments:
  1. Test CRNet on Data-1 with varying α values to find optimal balance between balancing and prognostic representation
  2. Compare CRNet with and without contrastive regularization (m=0) on Data-10 to verify the importance of continuity preservation
  3. Evaluate CRNet's sensitivity to representation dimension KΦ(X) on IHDP-1 to determine minimum sufficient dimensionality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of partial distance measure affect the performance and stability of CRNet across different data distributions?
- Basis in paper: [explicit] The paper discusses using partial distance measure to evaluate unbiasedness but acknowledges that "different conditional dependence measures may lead to different conclusions about the performance of our method"
- Why unresolved: The paper uses partial distance measure but doesn't systematically compare it with other dependence measures or evaluate its robustness across varying data distributions
- What evidence would resolve it: Comparative experiments using alternative dependence measures (e.g., kernel-based methods, mutual information estimators) across diverse synthetic data distributions

### Open Question 2
- Question: What is the optimal number of negative samples (m) for balancing computational efficiency and estimation accuracy in practice?
- Basis in paper: [explicit] The paper shows results for m ranging from 0 to 10 and notes "our best results from the main text can be further improved by increasing the number of m" but defaults to m=1 "to enhance training efficiency"
- Why unresolved: The paper identifies a trade-off between performance improvement and computational cost but doesn't provide guidance on how to select m for specific applications
- What evidence would resolve it: Empirical studies showing performance gains versus computational overhead for different values of m across varying dataset sizes and treatment dimensions

### Open Question 3
- Question: How does CRNet perform when the unconfoundedness assumption is violated by unmeasured confounders?
- Basis in paper: [explicit] The paper acknowledges "there remains a possibility that unmeasured or unknown confounders may influence the results"
- Why unresolved: All experiments assume unconfoundedness holds, and the paper doesn't explore sensitivity to violations of this key assumption
- What evidence would resolve it: Simulation studies introducing varying degrees of unmeasured confounding and evaluating CRNet's bias under these conditions

## Limitations
- The double balancing representation relies heavily on the unconfoundedness assumption, which may not hold in many real-world scenarios
- The effectiveness of partial distance measures in high-dimensional settings needs further validation
- The relationship between treatment continuity preservation and HDRC estimation accuracy requires more theoretical analysis

## Confidence
- High confidence: The overall framework design and MISE evaluation methodology
- Medium confidence: The contrastive regularizer's effectiveness in preserving treatment continuity
- Medium confidence: The generalizability of results to real-world datasets beyond synthetic experiments

## Next Checks
1. Conduct sensitivity analysis on the unconfoundedness assumption by introducing hidden confounders and measuring impact on MISE scores
2. Test CRNet on high-dimensional real-world datasets with known dose-response relationships to validate scalability
3. Perform ablation studies to quantify the individual contributions of balancing representation vs. prognostic representation to overall performance