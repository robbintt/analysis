---
ver: rpa2
title: Conditional sampling within generative diffusion models
arxiv_id: '2409.09650'
source_url: https://arxiv.org/abs/2409.09650
tags:
- conditional
- generative
- equation
- sampling
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper reviews methods for conditional sampling in generative
  diffusion models, focusing on scenarios where either joint data or marginal data
  with explicit likelihood is available. The authors discuss three main approaches:
  joint bridging, filtering-based methods, and Feynman-Kac models.'
---

# Conditional sampling within generative diffusion models

## Quick Facts
- **arXiv ID:** 2409.09650
- **Source URL:** https://arxiv.org/abs/2409.09650
- **Reference count:** 8
- **Key outcome:** Comprehensive review of conditional sampling methods for generative diffusion models using joint data, marginal data with likelihood, or Feynman-Kac formulations

## Executive Summary
This paper provides a comprehensive survey of methods for conditional sampling in generative diffusion models, focusing on scenarios where either joint data or marginal data with explicit likelihood is available. The authors systematically categorize three main approaches: joint bridging, filtering-based methods, and Feynman-Kac models. Each method is analyzed in terms of its theoretical foundations, implementation strategies, and practical applications. The paper serves as a valuable reference for researchers and practitioners seeking to understand and implement conditional sampling techniques within the diffusion model framework.

## Method Summary
The paper reviews three primary approaches to conditional sampling in generative diffusion models. Joint bridging constructs a reverse SDE that directly targets the conditional distribution by incorporating conditioning information into the score function. Filtering-based methods treat conditional sampling as a stochastic filtering problem, using recursive Bayesian estimation to update beliefs about the latent state given observations. Feynman-Kac models leverage explicit likelihood functions and pre-trained generative diffusion models to approximate the conditional distribution through sequential Monte Carlo sampling, combining importance sampling with the Feynman-Kac formalism. The authors provide detailed mathematical formulations for each approach and discuss their respective strengths and limitations.

## Key Results
- Three main approaches identified: joint bridging, filtering-based methods, and Feynman-Kac models
- Each method addresses different scenarios of conditional sampling (joint data vs. marginal data with likelihood)
- Joint bridging directly constructs conditional reverse SDEs, while filtering views it as a stochastic filtering problem
- Feynman-Kac approach combines importance sampling with likelihood information for approximate conditional sampling
- Pedagogical example included to illustrate joint bridging and Feynman-Kac methodologies

## Why This Works (Mechanism)
Conditional sampling in diffusion models requires incorporating additional information (conditioning variables) into the generative process. Joint bridging works by modifying the reverse-time dynamics to directly target the conditional distribution, effectively weaving the conditioning information into the score function. Filtering-based approaches leverage the recursive nature of Bayesian filtering to sequentially update the latent state distribution given observations, treating the problem as inference over time-evolving states. The Feynman-Kac method exploits the mathematical framework that connects stochastic processes with partial differential equations, using importance sampling to approximate the conditional distribution when explicit likelihood information is available.

## Foundational Learning
- **Reverse-time SDEs**: The mathematical foundation for diffusion model sampling, describing how to denoise data starting from pure noise
  - *Why needed*: Essential for understanding how joint bridging modifies the generative process
  - *Quick check*: Verify understanding of the connection between forward and reverse SDEs

- **Stochastic filtering theory**: The framework for recursive Bayesian estimation in continuous-time systems
  - *Why needed*: Core concept for filtering-based conditional sampling methods
  - *Quick check*: Confirm understanding of the Kushner-Stratonovich equation

- **Feynman-Kac formula**: A mathematical tool linking stochastic processes to solutions of certain PDEs
  - *Why needed*: Fundamental to the Feynman-Kac approach for conditional sampling
  - *Quick check*: Ensure comprehension of how importance sampling relates to the formula

- **Sequential Monte Carlo methods**: Particle-based approaches for approximating complex distributions
  - *Why needed*: Key implementation technique for the Feynman-Kac method
  - *Quick check*: Verify understanding of particle filtering and resampling

## Architecture Onboarding

**Component map:** Data/Input → Conditioning Variables → [Joint Bridging/Filters/Feynman-Kac] → Conditional Samples → Evaluation Metrics

**Critical path:** The most important workflow is the transformation from unconditional generative model to conditional sampler, where conditioning information is integrated either through modified reverse dynamics (joint bridging), recursive filtering updates (filtering methods), or importance weighting (Feynman-Kac).

**Design tradeoffs:** Joint bridging requires joint data but provides direct sampling; filtering methods need recursive updates but can handle streaming data; Feynman-Kac approaches leverage likelihood but depend on accurate importance distributions.

**Failure signatures:** Poor conditioning may manifest as mode collapse in joint bridging, filter divergence in filtering methods, or weight degeneracy in Feynman-Kac approaches.

**First experiments:**
1. Implement joint bridging on a simple conditional dataset (e.g., class-conditional MNIST) to verify direct sampling capability
2. Test filtering-based approach on a linear Gaussian system to validate recursive estimation
3. Apply Feynman-Kac method to a scenario with known likelihood (e.g., Gaussian conditional) to confirm importance sampling effectiveness

## Open Questions the Paper Calls Out
None

## Limitations
- Limited empirical comparisons between the three approaches across diverse use cases
- Computational complexity analysis not thoroughly explored for high-dimensional problems
- Practical guidance on method selection based on problem characteristics is somewhat theoretical
- Scalability challenges for each method in high-dimensional conditional sampling scenarios not fully addressed

## Confidence
- **Methodological accuracy:** High - the theoretical foundations are well-established
- **Practical guidance:** Medium - effectiveness depends heavily on specific implementation and problem domain
- **Computational efficiency claims:** Low - varies significantly with problem size and implementation details

## Next Checks
1. Conduct systematic empirical benchmarks comparing joint bridging, filtering-based, and Feynman-Kac approaches across multiple conditional sampling scenarios (different types of conditioning variables, data distributions, and dimensionality)
2. Evaluate the robustness of each method when the likelihood function is approximate or misspecified, particularly for the Feynman-Kac approach which relies on explicit likelihood
3. Test the scalability of these conditional sampling methods to high-dimensional problems and assess how the computational complexity scales with problem size compared to unconditional diffusion sampling