---
ver: rpa2
title: Few-Shot Adversarial Prompt Learning on Vision-Language Models
arxiv_id: '2403.14774'
source_url: https://arxiv.org/abs/2403.14774
tags:
- adversarial
- prompt
- natural
- learning
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of adversarial robustness in
  vision-language models by proposing a few-shot adversarial prompt learning framework.
  The core idea is to learn adversarially correlated text supervision end-to-end from
  adversarial examples, rather than relying on static hand-crafted prompts.
---

# Few-Shot Adversarial Prompt Learning on Vision-Language Models

## Quick Facts
- arXiv ID: 2403.14774
- Source URL: https://arxiv.org/abs/2403.14774
- Reference count: 40
- Primary result: Achieves 38.05% adversarial accuracy on base classes and 21.86% on new classes in base-to-new generalization setting

## Executive Summary
This paper addresses the critical challenge of adversarial robustness in vision-language models (VLMs) through a novel few-shot adversarial prompt learning framework. The key innovation is learning adversarially correlated text supervision end-to-end from adversarial examples, rather than relying on static hand-crafted prompts. By introducing learnable text prompt tokens that are projected from visual prompt tokens, the method generates text supervision specifically correlated with adversarial visual features. The proposed dual-term training objective balances natural and adversarial generalization, significantly improving adversarial robustness while maintaining natural accuracy. The framework demonstrates state-of-the-art performance, matching full zero-shot results with only 1% of the training data.

## Method Summary
The method builds upon CLIP's dual-encoder architecture by introducing learnable text prompt tokens that are derived from visual prompt tokens through a projection function. During training, natural and adversarial examples are generated using a 2-step PGD attack, and the model learns to align features in the joint text-image space while maintaining differentiation in the uni-modal visual space. The dual-term loss function combines natural cross-entropy with a weighted KL divergence term that encourages consistency between natural and adversarial similarity distributions. An adversarial-aware mechanism further promotes cosine similarity minimization between natural and adversarial visual features. The entire framework operates in a few-shot setting (16-shot) with pre-trained CLIP models frozen, updating only the learnable prompt tokens.

## Key Results
- Achieves 38.05% adversarial accuracy on base classes and 21.86% on new classes in base-to-new generalization setting
- Matches state-of-the-art zero-shot performance with only 1% of the training data
- Outperforms existing methods by significant margins across 11 benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1
Learnable adversarial text supervision aligns adversarial visual features better than static hand-crafted prompts. By inserting learnable prompt tokens into text embeddings and projecting them from visual prompt tokens, the model generates text supervision specifically correlated with adversarial visual features, improving cross-modal alignment. Core assumption: Adversarial examples have distinct feature distributions from natural examples, requiring specialized text supervision. Break condition: If adversarial perturbations are too small to create meaningful distributional differences, learned text supervision may not provide additional benefit over static prompts.

### Mechanism 2
Dual-term training objective balances natural and adversarial generalization in few-shot settings. The objective combines natural term (minimizing natural text-image similarity error) with adversarial KL divergence term (ensuring cross-modal consistency), weighted by λ. Core assumption: Few-shot adaptation needs to preserve both natural generalization capability and adversarial robustness. Break condition: If λ is set too high, the model may over-prioritize adversarial robustness at the expense of natural generalization.

### Mechanism 3
Adversarial-aware mechanism in uni-modal space prevents catastrophic forgetting of natural features. By encouraging cosine similarity minimization between natural and adversarial visual features, the model learns to distinguish between these distributions while maintaining consistency in joint text-image space. Core assumption: Forcing consistent outputs for natural and adversarial examples in visual models compromises generalization. Break condition: If adversarial perturbations are too strong, uni-modal distinction may become too extreme, harming overall performance.

## Foundational Learning

- Concept: Vision-Language Models (VLMs) and their contrastive learning framework
  - Why needed here: The entire method builds upon CLIP's dual-encoder architecture and contrastive learning principles
  - Quick check question: How does CLIP compute similarity between image and text embeddings for classification?

- Concept: Adversarial training and robustness
  - Why needed here: The method specifically addresses adversarial robustness through few-shot adaptation
  - Quick check question: What is the difference between natural and adversarial training objectives in standard adversarial training?

- Concept: Prompt learning and parameter-efficient adaptation
  - Why needed here: The method uses learnable prompt tokens rather than full model fine-tuning
  - Quick check question: How does prompt learning differ from full fine-tuning in terms of parameter efficiency and generalization?

## Architecture Onboarding

- Component map:
  Pre-trained CLIP model (frozen) -> Learnable visual prompt tokens (Pv) -> Learnable text prompt tokens (Pt=h(Pv)) -> Dual-term loss function (natural CE + weighted adversarial KL) -> Adversarial-aware mechanism (cosine similarity minimization) -> Updated prompts

- Critical path:
  1. Initialize learnable prompts Pv and Pt=h(Pv)
  2. Generate natural embeddings through image and text encoders
  3. Generate adversarial examples via inner maximization
  4. Compute dual-term loss with adversarial-aware mechanism
  5. Update prompts via gradient descent
  6. Test natural and adversarial accuracy

- Design tradeoffs:
  - Prompt size vs. computational efficiency
  - λ value vs. balance between natural and adversarial generalization
  - Number of prompt layers vs. adaptation capacity
  - Strength of adversarial perturbations vs. learning stability

- Failure signatures:
  - High variance across runs with small shot numbers
  - Degradation in natural accuracy while improving adversarial accuracy
  - Instability when λ is in moderate range
  - Loss plateauing early during training

- First 3 experiments:
  1. Baseline comparison: Run AdvVP with same few-shot setup to establish performance floor
  2. Ablation study: Test with only natural term in loss (λ=0) to measure contribution of adversarial consistency
  3. Parameter sensitivity: Vary λ across [0.5, 1.0, 1.5, 2.0, 2.5, 3.0] to find optimal balance

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions but raises several implicit ones. The framework's dependence on learnable prompts for adversarial examples assumes that adversarial perturbations create meaningfully different feature distributions. The paper acknowledges the challenge of balancing the natural and adversarial terms in the loss function. Additionally, the method's reliance on the dual-encoder architecture limits its applicability to other VLM architectures.

## Limitations

- The framework's effectiveness depends on finding the right balance between natural and adversarial objectives, with instability reported when λ is in moderate ranges
- The method assumes that adversarial perturbations create meaningfully different feature distributions, which may not hold for all attack strengths
- The dual-encoder architecture constraint limits applicability to VLMs that don't use this structure

## Confidence

- **High Confidence**: The core mechanism of using learnable text supervision derived from adversarial examples is well-grounded in the contrastive learning framework
- **Medium Confidence**: The empirical results showing state-of-the-art performance are promising but rely on specific datasets and attack settings
- **Low Confidence**: The discussion of catastrophic forgetting and the adversarial-aware mechanism in uni-modal space lacks extensive empirical validation

## Next Checks

1. **Ablation of Adversarial-Aware Mechanism**: Remove the cosine similarity minimization component that encourages differentiated uni-modal features and evaluate whether the method still maintains robustness without this mechanism.

2. **Cross-Dataset Robustness Testing**: Evaluate the method on additional datasets not included in the original study, particularly those with different visual characteristics (e.g., medical imaging, satellite imagery).

3. **Perturbation Strength Sensitivity Analysis**: Systematically vary the ℓ∞ perturbation budget (ϵ) from very small (1/255) to larger values and measure how the performance degrades to identify breaking points.