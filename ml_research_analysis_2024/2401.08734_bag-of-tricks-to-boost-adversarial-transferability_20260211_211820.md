---
ver: rpa2
title: Bag of Tricks to Boost Adversarial Transferability
arxiv_id: '2401.08734'
source_url: https://arxiv.org/abs/2401.08734
tags:
- adversarial
- attack
- transferability
- attacks
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies adversarial transferability in deep neural networks.
  The authors identify that standard hyperparameters like iteration count, step size,
  and momentum decay can significantly impact attack performance, motivating a systematic
  exploration.
---

# Bag of Tricks to Boost Adversarial Transferability

## Quick Facts
- **arXiv ID:** 2401.08734
- **Source URL:** https://arxiv.org/abs/2401.08734
- **Reference count:** 40
- **Primary result:** Proposes several tricks to enhance adversarial transferability, achieving up to +19.9% success rate against strong defenses when combined.

## Executive Summary
This work systematically explores how standard hyperparameters (iteration count, step size, momentum decay) impact adversarial transferability and proposes several practical tricks to improve it. The authors identify that random global momentum initialization, scheduled step sizes, dual examples with ensemble strategies, spectral-based input transformations, and ensemble strategies like gradient alignment and model shuffling can significantly boost transferability. Evaluated on ImageNet with CNNs and transformers, these tricks consistently improve attack success rates, with notable gains when combined, and are also validated on audio-visual models.

## Method Summary
The authors propose several tricks to enhance adversarial transferability: random global momentum initialization to better explore the perturbation space, scheduled step sizes to balance convergence and transferability, dual examples with ensemble strategies for richer gradient estimation, spectral-based input transformations targeting high-frequency components, and ensemble strategies like gradient alignment and model shuffling. These methods are evaluated on ImageNet with multiple CNN and transformer models, showing consistent improvements in attack success rates, particularly when combined. The approach is also validated on audio-visual models, demonstrating broad applicability.

## Key Results
- Proposed tricks consistently improve attack success rates against multiple CNN and transformer models on ImageNet.
- Notable gains of up to +19.9% success rate when tricks are combined, even against strong defenses.
- Validated on audio-visual models, showing broad applicability beyond visual tasks.

## Why This Works (Mechanism)
The proposed tricks enhance adversarial transferability by improving the exploration of the perturbation space, balancing convergence and transferability through scheduled step sizes, and leveraging richer gradient estimates via dual examples and ensemble strategies. Spectral-based input transformations target high-frequency components that are more transferable, while ensemble strategies like gradient alignment and model shuffling help find perturbations that fool multiple models simultaneously. These mechanisms collectively improve the ability of adversarial examples to transfer across different model architectures and tasks.

## Foundational Learning
- **Adversarial examples:** Small, intentional perturbations added to inputs to fool machine learning models. *Why needed:* Core concept being studied. *Quick check:* Understand how perturbations affect model predictions.
- **Transferability:** The ability of adversarial examples generated for one model to fool another model. *Why needed:* Central phenomenon being improved. *Quick check:* Verify examples fool multiple models.
- **Momentum-based attacks:** Use momentum to stabilize gradient updates during attack generation. *Why needed:* One of the proposed tricks modifies momentum initialization. *Quick check:* Compare with and without momentum in attacks.
- **Spectral transformations:** Modify input frequencies to target more transferable components. *Why needed:* Proposed trick to enhance transferability. *Quick check:* Analyze frequency components of adversarial examples.
- **Ensemble strategies:** Combine gradients or predictions from multiple models. *Why needed:* Used in several proposed tricks. *Quick check:* Evaluate transferability gains from ensemble methods.

## Architecture Onboarding

**Component Map:** Input -> Preprocessing (spectral transform) -> Attack Generation (momentum, step size, dual examples) -> Ensemble (gradient alignment, model shuffling) -> Adversarial Example

**Critical Path:** Input → Spectral Transform → Attack Generation (with momentum, step size, dual examples) → Ensemble Strategies → Adversarial Example

**Design Tradeoffs:** The tricks balance between improving transferability and maintaining perceptibility of adversarial examples. Some methods (e.g., high-frequency targeting) may increase perceptibility, while others (e.g., model shuffling) aim to find more transferable perturbations without significantly altering visual quality.

**Failure Signatures:** Reduced effectiveness when defenses are aware of and specifically designed to counter the proposed tricks, such as input preprocessing targeting high-frequency components or detection mechanisms for momentum-based attacks.

**First Experiments:**
1. Evaluate transferability gains from individual tricks on a held-out model.
2. Test the combined effect of multiple tricks against a strong defense.
3. Compare perceptibility of adversarial examples generated with and without the proposed tricks.

## Open Questions the Paper Calls Out
None

## Limitations
- The paper does not thoroughly address robustness against adaptive defenses designed to detect or mitigate the specific techniques used.
- The impact of the tricks on the perceptibility of adversarial perturbations is not explicitly analyzed.
- Generalization of improvements across diverse datasets beyond ImageNet and AudioSet is not extensively validated.

## Confidence
- **High:** The core claim that hyperparameter tuning and proposed tricks enhance adversarial transferability is well-supported by extensive empirical results across multiple model architectures and tasks.
- **Medium:** The effectiveness of the tricks in improving transferability against defended models is demonstrated, but the robustness against adaptive, defense-aware scenarios is less certain.
- **Medium:** The broad applicability to both visual and audio domains is suggested by results on ImageNet and AudioSet, but deeper validation across more diverse datasets and tasks is needed.

## Next Checks
1. Evaluate the proposed tricks against adaptive defenses specifically designed to detect or mitigate the identified techniques, such as input preprocessing or detection mechanisms targeting high-frequency perturbations or momentum-based methods.
2. Conduct perceptual studies or quantitative metrics (e.g., LPIPS, SSIM) to assess the impact of the tricks on the visual/audio quality of adversarial examples and their detectability by humans.
3. Test the transferability improvements on a wider range of datasets (e.g., CIFAR-10, medical imaging, speech commands) and tasks (e.g., object detection, semantic segmentation) to validate generalization beyond the evaluated domains.