---
ver: rpa2
title: 'From Representational Harms to Quality-of-Service Harms: A Case Study on Llama
  2 Safety Safeguards'
arxiv_id: '2403.13213'
source_url: https://arxiv.org/abs/2403.13213
tags: []
core_contribution: "The paper examines how safety measures in large language models\
  \ like Llama 2, designed to mitigate representational harms, can inadvertently create\
  \ quality-of-service harms by producing biased refusals for certain demographic\
  \ groups. The authors created 1,792 non-toxic prompts targeting stereotypes previously\
  \ addressed in Llama 2\u2019s safety training, then evaluated three Llama 2-Chat\
  \ models (7B, 13B, 70B) and Llama 1 on these prompts."
---

# From Representational Harms to Quality-of-Service Harms: A Case Study on Llama 2 Safety Safeguards

## Quick Facts
- arXiv ID: 2403.13213
- Source URL: https://arxiv.org/abs/2403.13213
- Reference count: 8
- Primary result: Llama 2 safety measures reduce toxic content but create quality-of-service harms through biased refusal rates across demographic groups

## Executive Summary
This study examines how safety measures in large language models like Llama 2, designed to mitigate representational harms, can inadvertently create quality-of-service harms by producing biased refusals for certain demographic groups. The authors created 1,792 non-toxic prompts targeting stereotypes previously addressed in Llama 2's safety training, then evaluated three Llama 2-Chat models (7B, 13B, 70B) and Llama 1 on these prompts. Outputs were manually labeled into six categories: answer, partial answer, failure to answer, refusal to answer, harmful refusal, and harmful answer.

Results showed that while Llama 2 models reduced explicit toxic content compared to Llama 1, they still encoded harmful stereotypes through disproportionately high refusal rates for certain groups, particularly Muslims (22-40% refusal rates across models), Asians, Chinese, and Black names. The study reveals that safety measures merely mask biases rather than eliminate them, leading to disparate service quality across demographic groups. The authors recommend rethinking toxicity evaluation to include contextual factors, moving beyond competitive benchmarking, and prioritizing safer training data collection practices.

## Method Summary
The study evaluated Llama 2 safety measures by creating 1,792 non-toxic prompts targeting stereotypes that were likely included in the model's safety training. These prompts were designed to test whether the model would refuse to answer requests related to demographic groups while potentially encoding harmful stereotypes. The evaluation included three Llama 2-Chat models (7B, 13B, 70B parameters) and Llama 1, with outputs manually labeled into six categories: answer, partial answer, failure to answer, refusal to answer, harmful refusal, and harmful answer. This approach allowed the researchers to assess both the presence of toxic content and the patterns of refusals across different demographic groups.

## Key Results
- Llama 2 models significantly reduced explicit toxic content compared to Llama 1, with refusal rates increasing substantially for stereotype-related prompts
- Refusal rates showed clear disparities across demographic groups, with Muslims experiencing 22-40% refusal rates across models, while other groups like Asians, Chinese, and Black names also faced disproportionately high refusal rates
- The study found that safety measures merely mask biases rather than eliminate them, as harmful stereotypes persisted through refusal patterns that created unequal service quality across demographic groups

## Why This Works (Mechanism)
The safety measures in Llama 2 work by identifying and refusing to engage with prompts that could lead to harmful outputs, particularly those related to stereotypes and sensitive demographic information. However, this mechanism creates a trade-off where the model's attempts to avoid representational harms through refusals inadvertently create quality-of-service harms by differentially affecting certain demographic groups. The study demonstrates that while the safety measures successfully reduce explicit toxicity, they fail to address underlying biases in the training data, instead creating a new form of harm through biased refusal patterns that reflect and amplify existing disparities.

## Foundational Learning

**LLM Safety Training** - why needed: Models need mechanisms to avoid generating harmful content and stereotypes that could cause real-world damage or perpetuate discrimination. quick check: Review safety benchmarks and refusal rate distributions across demographic groups.

**Representational Harms** - why needed: Language models can perpetuate harmful stereotypes about demographic groups, leading to systemic discrimination in their outputs. quick check: Analyze prompt-response patterns for stereotype-related queries across different demographic groups.

**Quality-of-Service Harms** - why needed: Safety measures that create differential service quality for different demographic groups represent a new form of algorithmic harm that needs to be identified and addressed. quick check: Measure and compare service metrics (refusal rates, response quality) across demographic groups.

## Architecture Onboarding

The Llama 2 safety architecture consists of a base language model trained on general web data, followed by safety fine-tuning that adds refusal mechanisms and toxicity filters. The critical path flows through prompt preprocessing → safety classifier → base model response generation → post-processing filters. The key design tradeoff involves balancing between preventing harmful outputs and maintaining equitable service quality across all user demographics.

**Critical path**: User prompt → Safety classifier → Base model → Output filters → Final response
**Design tradeoffs**: Strict safety measures reduce toxic content but increase refusal rates, particularly affecting certain demographic groups
**Failure signatures**: Disproportionate refusal rates for specific demographic groups, masking of underlying biases rather than elimination
**First experiments**: 1) Test refusal rates for prompts mentioning different demographic groups 2) Compare toxic content reduction vs refusal rate increases 3) Analyze response patterns for stereotype-related queries

## Open Questions the Paper Calls Out
The study highlights several open questions regarding the effectiveness and implications of current safety measures in large language models. These include whether refusal patterns actually reduce harm or merely shift it to different forms of service inequality, how to balance safety considerations with equitable service quality, and what alternative approaches might better address both representational and quality-of-service harms simultaneously. The authors also question how to properly evaluate safety measures beyond simple toxicity metrics to include contextual and systemic factors.

## Limitations

The evaluation methodology relies on manually labeled outputs categorized into six types, but the criteria for these categories and inter-rater reliability measures are not clearly documented, introducing potential subjectivity in the results. The sample of 1,792 prompts, while substantial, represents a specific set of stereotype-related queries and may not generalize to broader usage patterns or other types of safety concerns. The study focuses exclusively on Llama models, limiting generalizability to other large language models or different safety training approaches.

## Confidence

**High confidence**: The finding that Llama 2 models show reduced explicit toxic content compared to Llama 1 is well-supported by the data
**Medium confidence**: The observation of disparate refusal rates across demographic groups, given the manual labeling process and limited documentation of reliability
**Medium confidence**: The conclusion that safety measures mask rather than eliminate biases, though the mechanism and full implications require further investigation

## Next Checks

1. Conduct inter-rater reliability analysis on the manual labeling process to establish consistency in category assignments
2. Test the same prompt set across additional LLM architectures (GPT-4, Claude, etc.) to assess generalizability of refusal rate disparities
3. Perform controlled experiments varying prompt phrasing and context to determine whether refusal patterns reflect genuine safety concerns or systematic bias in safety training