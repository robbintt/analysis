---
ver: rpa2
title: 'Large Language Models on Small Resource-Constrained Systems: Performance Characterization,
  Analysis and Trade-offs'
arxiv_id: '2412.15352'
source_url: https://arxiv.org/abs/2412.15352
tags:
- orin
- power
- devices
- device
- jetson
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study characterizes the performance of large language models\
  \ (LLMs) on resource-constrained NVIDIA Jetson Orin embedded devices, focusing on\
  \ trade-offs between latency, power, memory, and accuracy. Using five Pythia LLM\
  \ models (70M\u20131.4B parameters) and varying quantization, power models, and\
  \ device configurations, the research identifies optimal configurations for constrained\
  \ applications."
---

# Large Language Models on Small Resource-Constrained Systems: Performance Characterization, Analysis and Trade-offs

## Quick Facts
- arXiv ID: 2412.15352
- Source URL: https://arxiv.org/abs/2412.15352
- Reference count: 36
- Primary result: Characterizes LLM performance on NVIDIA Jetson Orin devices, identifying optimal configurations for constrained applications through analysis of quantization, power models, and device families.

## Executive Summary
This study systematically characterizes the performance of large language models (LLMs) on NVIDIA Jetson Orin embedded devices, focusing on trade-offs between latency, power, memory, and accuracy. Using five Pythia LLM models ranging from 70M to 1.4B parameters, the research explores how quantization, power management, and device configuration impact deployment feasibility on resource-constrained systems. The study identifies non-intuitive findings, such as quantization increasing latency for smaller models while decreasing it for larger ones, and provides a modular testing framework for further research into embedded LLM optimization.

## Method Summary
The study uses NVIDIA Jetson Orin developer kits with six different configurations (AGX Orin Devkit, AGX Orin 32GB, Orin NX 16GB, Orin NX 8GB, Orin Nano 8GB, Orin Nano 4GB) to test five Pythia LLM models (70Mâ€“1.4B parameters). Testing involves running batch tests with PyTorch and HuggingFace Transformers libraries, measuring latency (model loading and token generation), power usage, memory allocation, and accuracy via LM Evaluation Harness. The methodology varies quantization (4-bit or no quantization), NV power models (MAXN, 50W, 30W, 15W, etc.), and device configurations to analyze trade-offs across different resource constraints.

## Key Results
- Quantization reduces latency for larger models (1B+ parameters) but increases latency for smaller models due to dequantization overhead
- Unified memory architecture creates performance bottlenecks for large models, leading to out-of-memory errors and system freezes
- Device family (AGX Orin, Orin NX, Orin Nano) impacts performance consistency, with members of the same family showing similar time-per-token results
- AGX Orin Devkit shows minimal memory variation across tests while smaller configurations exhibit more volatility

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quantization reduces latency for larger LLMs but increases latency for smaller LLMs.
- Mechanism: For larger models (1B+ parameters), quantization reduces the size of weights, decreasing memory access time and improving throughput. For smaller models, quantization overhead (computational cost of dequantization during inference) outweighs the memory savings, increasing latency.
- Core assumption: The computational cost of quantization and dequantization is significant enough to impact latency, especially for smaller models.
- Evidence anchors:
  - [abstract]: "Results show quantization reduces latency for larger models but increases it for smaller ones."
  - [section]: "Figure 3 demonstrates this connection... quantization reduces the latency from the baseline for larger models (1b, 1.4b)."
  - [corpus]: Weak corpus evidence. No directly relevant papers found.
- Break condition: If the quantization overhead is reduced or if hardware optimizations mitigate the computational cost of dequantization.

### Mechanism 2
- Claim: Unified memory in Jetson devices creates performance bottlenecks for large LLMs.
- Mechanism: PyTorch allocates memory as-needed from the same pool as the operating system. When a large model is loaded, it can consume a significant portion of the unified memory, leading to out-of-memory (OOM) errors or system freezes.
- Core assumption: The operating system and other processes do not release memory, creating a race condition that prevents memory allocations or deallocations.
- Evidence anchors:
  - [section]: "One particular issue during our testing involved running tests on the device configuration with the smallest amount of memory... the system would frequently reach an OOM error before the loading process completes."
  - [section]: "As such, PyTorch allocates directly from the same memory pool as the operating system."
  - [corpus]: Weak corpus evidence. No directly relevant papers found.
- Break condition: If memory management is improved to allow for more efficient memory allocation and deallocation, or if the operating system is optimized to reduce its memory footprint.

### Mechanism 3
- Claim: Device family (AGX Orin, Orin NX, Orin Nano) impacts performance consistency across metrics.
- Mechanism: Devices within the same family share similar SoC architecture and GPU resources, leading to similar performance characteristics. Moving across families introduces variations in GPU cores, unified memory, and power constraints, affecting performance metrics like latency, power usage, and memory allocation.
- Core assumption: The shared SoC architecture and GPU resources within a family are the primary determinants of performance consistency.
- Evidence anchors:
  - [section]: "One potential correlation we found in our test results is the effect of device configuration 'family' on some metrics... the time-per-token metric results are very close between members of the same family."
  - [section]: "Of the Orin configurations in this study, there are three device families: the AGX Orin, the Orin NX, and the Orin Nano."
  - [corpus]: Weak corpus evidence. No directly relevant papers found.
- Break condition: If future device families introduce significant architectural changes that break the performance consistency observed within the current families.

## Foundational Learning

- Concept: Unified Memory Architecture
  - Why needed here: Understanding how memory is shared between CPU and GPU in Jetson devices is crucial for optimizing LLM deployment and avoiding OOM errors.
  - Quick check question: How does unified memory affect memory allocation for large models compared to systems with dedicated GPU memory?

- Concept: Quantization Techniques
  - Why needed here: Quantization is a key optimization technique for deploying LLMs on resource-constrained devices, but its impact on latency varies depending on model size.
  - Quick check question: Why does quantization improve latency for larger models but increase it for smaller models?

- Concept: Power Models and Thermal Constraints
  - Why needed here: Understanding how power models affect device performance is essential for optimizing LLM deployment under specific power constraints.
  - Quick check question: How do different NV power models impact the performance of LLMs on Jetson devices?

## Architecture Onboarding

- Component map:
  Jetson Orin device (AGX Orin, Orin NX, Orin Nano) -> PyTorch backend -> HuggingFace Transformers library -> Quantization (4-bit or no quantization) -> NV power model (MAXN, 50W, 30W, 15W, etc.) -> Pythia LLM models (70M, 160M, 410M, 1B, 1.4B parameters)

- Critical path:
  1. Flash Jetson Orin device with desired configuration.
  2. Install PyTorch and HuggingFace libraries.
  3. Load LLM model with or without quantization.
  4. Generate tokens and measure latency, power, memory, and accuracy.

- Design tradeoffs:
  - Model size vs. latency: Larger models offer better accuracy but increase latency.
  - Quantization vs. accuracy: Quantization reduces memory usage and latency but can decrease accuracy.
  - Power model vs. performance: Higher power models enable better performance but consume more energy.

- Failure signatures:
  - System freeze during model loading (OOM error).
  - Inconsistent performance across device configurations.
  - Unexpected latency increase with quantization for smaller models.

- First 3 experiments:
  1. Measure latency, power, and memory usage for a small LLM (70M parameters) on the Orin Nano 4GB with and without quantization.
  2. Compare the performance of the AGX Orin and Orin NX 16GB for a medium-sized LLM (410M parameters) at the MAXN power model.
  3. Analyze the impact of different NV power models on the latency of a large LLM (1B parameters) on the AGX Orin Devkit.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does model quantization affect the performance of large language models (LLMs) on Jetson Orin devices, and why does it increase latency for smaller models but decrease it for larger ones?
- Basis in paper: [explicit] The paper states that quantization reduces latency for larger models but increases it for smaller ones, which is contrary to the general consensus.
- Why unresolved: The paper does not provide a detailed explanation for this phenomenon, leaving it as an observed pattern without a theoretical foundation.
- What evidence would resolve it: A theoretical analysis or empirical study that explains the underlying mechanisms of quantization's impact on different model sizes, possibly involving memory access patterns or computational overhead.

### Open Question 2
- Question: What are the optimal hardware and software configurations for running LLMs on Jetson Orin devices under specific resource constraints, such as power, latency, energy, and memory?
- Basis in paper: [explicit] The paper discusses trade-offs between these metrics and provides use cases for constrained applications, but does not identify definitive optimal configurations.
- Why unresolved: The optimal configurations depend on specific application requirements and constraints, which are not universally defined.
- What evidence would resolve it: A comprehensive study that tests a wide range of configurations under various constraints and identifies the best-performing setups for different use cases.

### Open Question 3
- Question: How does the "memory wall" problem affect the performance of LLMs on resource-constrained devices, and what strategies can mitigate its impact?
- Basis in paper: [explicit] The paper mentions the "memory wall" problem, where models too large to be loaded entirely suffer from weight loading/unloading mid-generation.
- Why unresolved: The paper does not explore solutions or strategies to mitigate this issue, focusing instead on characterizing its effects.
- What evidence would resolve it: Research into memory management techniques, such as model pruning, quantization, or offloading, that can reduce the impact of the memory wall on LLM performance.

### Open Question 4
- Question: What are the effects of different power management models on the performance and energy efficiency of LLMs on Jetson Orin devices?
- Basis in paper: [explicit] The paper tests various NVIDIA power models but does not provide a detailed analysis of their impact on performance and energy efficiency.
- Why unresolved: The study focuses on characterizing performance under different power models without delving into their specific effects on energy consumption and efficiency.
- What evidence would resolve it: A comparative study that measures the energy efficiency and performance of LLMs under different power management settings, identifying the most effective models for various applications.

## Limitations
- Limited generalizability to hardware platforms beyond Jetson Orin and model architectures beyond Pythia
- Weak corpus evidence with no citations and average neighbor FMR of 0.247
- Unspecified testing parameters including exact token generation inputs and iteration counts
- Does not explore optimizations beyond quantization such as model pruning or specialized kernels

## Confidence
**High Confidence**: The observation that quantization reduces latency for larger models (1B+ parameters) while increasing it for smaller models is well-supported by the empirical data presented in Figure 3 and the abstract. The mechanism linking quantization overhead to smaller model performance is logically sound.

**Medium Confidence**: The characterization of unified memory bottlenecks is based on observed system behavior but lacks deep architectural analysis or comparison with alternative memory management approaches. The failure signatures described are real but may be specific to the testing environment.

**Low Confidence**: The performance consistency claims across device families rely on observed correlations without rigorous statistical validation. The underlying assumption that shared SoC architecture is the primary determinant of performance consistency requires further testing with more granular metrics and statistical analysis.

## Next Checks
1. **Quantization Overhead Analysis**: Measure the exact computational overhead of dequantization operations during inference for different model sizes to quantify the break-even point where quantization transitions from beneficial to detrimental.

2. **Memory Management Profiling**: Conduct detailed profiling of unified memory allocation patterns during model loading, including memory pressure from the operating system and background processes, to identify specific bottlenecks and potential optimizations.

3. **Cross-Family Performance Variance**: Implement statistical tests (ANOVA or similar) to determine whether performance differences across device families are significant or within expected variance, and identify which architectural features (GPU cores, memory bandwidth, thermal design) drive these differences.