---
ver: rpa2
title: 'CSEPrompts: A Benchmark of Introductory Computer Science Prompts'
arxiv_id: '2404.02540'
source_url: https://arxiv.org/abs/2404.02540
tags: []
core_contribution: CSEPrompts is a benchmark dataset for evaluating large language
  models (LLMs) on introductory computer science tasks, including Python coding exercises
  and multiple-choice questions (MCQs) sourced from coding websites and MOOCs. The
  dataset contains 269 prompts, with 118 from coding websites and 101 from academic
  sources, plus 50 MCQs.
---

# CSEPrompts: A Benchmark of Introductory Computer Science Prompts

## Quick Facts
- arXiv ID: 2404.02540
- Source URL: https://arxiv.org/abs/2404.02540
- Reference count: 40
- Dataset contains 269 prompts for evaluating LLMs on introductory CS tasks

## Executive Summary
CSEPrompts is a benchmark dataset designed to evaluate large language models on introductory computer science tasks, including Python coding exercises and multiple-choice questions sourced from coding websites and MOOCs. The dataset addresses a gap in existing benchmarks by focusing specifically on educational programming tasks. The authors evaluated eight different LLMs including GPT-3.5, Llama-2, and fine-tuned Code LLMs, finding that GPT-3.5 outperformed others on both coding and MCQ tasks.

## Method Summary
The authors created a benchmark dataset of 269 prompts by collecting Python coding exercises and MCQs from popular coding websites (HackerRank, HackerEarth) and academic MOOCs (Coursera, Udemy). They evaluated eight LLMs including GPT-3.5, Llama-2, and several fine-tuned Code LLMs. The evaluation measured model performance on both coding tasks (using execution correctness) and MCQs, comparing results across different model types and prompt sources.

## Key Results
- GPT-3.5 outperformed all other evaluated LLMs on both coding and MCQ tasks
- Code LLMs excelled at coding tasks while raw LLMs performed better on MCQs
- LLMs performed better on coding websites than academic MOOCs

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its focus on real-world educational content from established coding platforms and MOOCs. By using authentic programming exercises and MCQs, the evaluation captures practical coding scenarios rather than artificial test cases. The comparison across different model types (raw vs fine-tuned) reveals distinct strengths in code generation versus conceptual understanding.

## Foundational Learning
- **Prompt engineering basics**: Why needed - to effectively communicate tasks to LLMs; Quick check - can you write clear, unambiguous instructions
- **Code execution evaluation**: Why needed - to verify correctness of generated code; Quick check - can you run and test Python code
- **Multiple-choice question design**: Why needed - to assess conceptual understanding; Quick check - can you create unambiguous answer choices
- **Benchmark dataset construction**: Why needed - to ensure representative and unbiased evaluation; Quick check - can you document selection criteria and sources

## Architecture Onboarding
**Component Map**: Dataset Collection -> Prompt Curation -> LLM Evaluation -> Performance Analysis -> Results Reporting
**Critical Path**: Prompt collection → Model evaluation → Correctness verification → Performance comparison
**Design Tradeoffs**: Focused on introductory CS vs comprehensive coverage; Coding execution vs conceptual assessment; Single correctness metric vs detailed error analysis
**Failure Signatures**: Poor prompt clarity → incorrect responses; Model limitations → execution errors; Dataset bias → skewed performance metrics
**3 First Experiments**:
1. Test prompt clarity by having multiple evaluators assess ambiguity
2. Run baseline evaluation with simplified correctness criteria
3. Compare performance across different difficulty levels within the dataset

## Open Questions the Paper Calls Out
The paper raises questions about the scalability of the benchmark to more advanced computer science topics and whether the observed performance differences between model types would persist at higher difficulty levels. It also questions how well the benchmark captures the full range of programming competencies needed for real-world applications.

## Limitations
- Benchmark dataset size (269 prompts) may be insufficient for robust generalization
- Evaluation only covers introductory-level content, limiting applicability to advanced scenarios
- Single correctness metric without detailed error analysis or qualitative assessment

## Confidence
- High confidence: GPT-3.5 outperforms other evaluated LLMs on both coding and MCQ tasks
- Medium confidence: Code LLMs excel at coding tasks while raw LLMs perform better on MCQs
- Medium confidence: LLMs perform better on coding websites than academic MOOCs

## Next Checks
1. Expand the benchmark dataset to include at least 500 prompts with balanced representation from multiple CS education domains and diverse difficulty levels
2. Conduct error analysis studies to categorize failure modes by error type and evaluate whether LLMs can provide meaningful feedback beyond binary correctness
3. Test additional prompting strategies including chain-of-thought prompting, few-shot examples, and instruction-tuned variants to determine if performance improvements are possible beyond the base model comparisons presented