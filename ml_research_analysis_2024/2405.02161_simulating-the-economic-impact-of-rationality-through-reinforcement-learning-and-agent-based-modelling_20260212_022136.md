---
ver: rpa2
title: Simulating the Economic Impact of Rationality through Reinforcement Learning
  and Agent-Based Modelling
arxiv_id: '2405.02161'
source_url: https://arxiv.org/abs/2405.02161
tags:
- agents
- rational
- economic
- learning
- firms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a framework to extend traditional macroeconomic
  agent-based models (ABMs) by substituting bounded rational firms with fully rational
  reinforcement learning (RL) agents. The framework is demonstrated on a credit and
  capital macroeconomic ABM, where RL agents are trained to maximize profits.
---

# Simulating the Economic Impact of Rationality through Reinforcement Learning and Agent-Based Modelling

## Quick Facts
- arXiv ID: 2405.02161
- Source URL: https://arxiv.org/abs/2405.02161
- Reference count: 40
- Primary result: RL agents in macroeconomic models learn distinct price-quantity strategies (market power, dumping, perfect competition) based on market competition levels, with independent policies increasing total profits through strategic segregation.

## Executive Summary
This work introduces a framework that extends macroeconomic agent-based models by replacing bounded rational firms with fully rational reinforcement learning agents. The framework is demonstrated on a credit and capital macroeconomic model where RL agents are trained to maximize profits through price-quantity decisions. The results show that RL agents spontaneously learn three distinct strategies depending on market competition: market power under low competition, dumping under moderate competition, and perfect competition under high competition. When agents have independent policies, they spontaneously segregate into strategic groups, increasing overall market profits. On the macroeconomic level, higher rationality always improves total output, but macroeconomic stability is improved only when agents adopt perfect competition strategies.

## Method Summary
The method extends a credit and capital macroeconomic agent-based model (CC-MABM) by replacing bounded rational consumption good firms (C-firms) with reinforcement learning agents. The RL agents use Q-learning with ε-greedy exploration to learn price-quantity strategies that maximize profits. The state space consists of discretized logarithmic transformations of price delta and firm stock, while the action space consists of discrete price and quantity adjustments. Training occurs over 100 episodes with decaying exploration rates. The macroeconomic impact is evaluated through total output (GDP) and its volatility across different numbers of RL agents and competition levels.

## Key Results
- RL agents spontaneously learn three distinct price-quantity strategies (market power, dumping, perfect competition) depending on market competition and rationality levels
- Independent RL agents with separate Q-matrices spontaneously segregate into strategic groups, increasing total profits compared to shared policies
- Higher rationality (more RL agents) always improves total macroeconomic output, but macroeconomic stability is improved only under perfect competition strategy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RL agents learn three distinct price-quantity strategies depending on competition level and number of rational agents
- Mechanism: The environment's competition parameter and number of RL agents jointly determine profitability of different strategies. Low competition allows market power exploitation, while higher competition pushes toward dumping or perfect competition
- Core assumption: Agents learn optimal policies through interaction with partial market visibility (price delta and firm stock only)
- Evidence anchors: Abstract finding of three distinct strategies; Section 4.1 showing market power at low zc and strategy emergence

### Mechanism 2
- Claim: Independent RL agents spontaneously segregate into strategic groups, increasing overall profits
- Mechanism: Independent policies allow parallel exploration of different strategies. When some agents exploit market power or dumping, others find niches avoiding direct competition, collectively achieving higher total profits
- Core assumption: Economic environment is not zero-sum; agents can increase total profits by exploiting different market segments
- Evidence anchors: Abstract showing spontaneous segregation; Section 4.1 demonstrating higher profits with independent policies

### Mechanism 3
- Claim: Higher rationality improves macroeconomic output but only perfect competition improves stability
- Mechanism: Rational agents optimize for profit, increasing aggregate output. However, exploitative strategies increase volatility by pushing bounded-rational firms toward bankruptcy. Perfect competition reduces strategic volatility
- Core assumption: RL agents maximize profit without explicit stability constraints; stability emerges as byproduct of adopted strategy
- Evidence anchors: Abstract showing output improvement but stability tradeoff; Section 4.2 demonstrating perfect competition uniquely improves stability

## Foundational Learning

- Concept: Q-learning and Bellman equation for policy optimization
  - Why needed here: Agents learn to maximize profit by iteratively updating Q-values based on observed rewards and future state estimates
  - Quick check question: How does the discount factor γ influence long-term vs short-term profit optimization?

- Concept: Partial observability and state discretization
  - Why needed here: Agents only observe price delta and firm stock, requiring discretization to work with tabular Q-learning
  - Quick check question: What happens to learning performance if state space discretization is too coarse or too fine?

- Concept: Multi-agent reinforcement learning dynamics
  - Why needed here: Multiple agents interact in same environment, so each agent's policy affects others' rewards; independent vs shared policies lead to different emergent behaviors
- Quick check question: Why do independent policies lead to higher total profits than shared policies in this setting?

## Architecture Onboarding

- Component map: CC-MABM environment -> RL agent layer -> Q-learning training loop -> Evaluation loop -> Configuration system
- Critical path: 1) Initialize MABM with specified RL agents 2) Run training episodes collecting state-action-reward transitions 3) Update Q-matrices 4) Decay exploration rate 5) Test trained agents 6) Aggregate microeconomic and macroeconomic outcomes
- Design tradeoffs: Discrete vs continuous action spaces (tabular Q-learning vs policy granularity); Shared vs independent policies (simplicity vs strategic diversity); State discretization granularity (detail vs computational cost)
- Failure signatures: Q-values not converging (coarse discretization or insufficient training); All agents adopting same strategy (strong reward signal or insufficient exploration); Macroeconomic instability spiking (exploitative strategies dominating)
- First 3 experiments: 1) Baseline MABM (no RL agents) to establish benchmarks 2) Single RL agent with shared policy at zc=2 to observe market power strategy 3) N=5 RL agents with shared policy at zc=5 to observe perfect competition transition

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different levels of agent rationality affect the emergence of strategic behavior and market outcomes in economies with varying degrees of market competition?
- Basis in paper: The paper finds RL agents learn three distinct strategies depending on competition and rationality, and agents with independent policies segregate into strategic groups
- Why unresolved: The paper provides framework but doesn't explore full range of interactions between agent rationality and market competition
- What evidence would resolve it: Comprehensive simulation study varying market competition, agent rationality, and other parameters to map strategic behaviors and market outcomes

### Open Question 2
- Question: How do strategies learned by RL agents compare to those observed in real-world economies?
- Basis in paper: The paper finds three distinct strategies with analogues in economic theory but doesn't compare to empirical data
- Why unresolved: The paper provides theoretical framework without validating against real-world data, limiting assessment of realism
- What evidence would resolve it: Comparative analysis of RL-learned strategies with empirical studies of firm behavior in different market environments

### Open Question 3
- Question: How do macroeconomic impacts of RL agents vary with different agent learning algorithms and reward functions?
- Basis in paper: The paper uses Q-learning with profit rewards and compares shared vs independent policies, but doesn't explore other learning algorithms or reward functions
- Why unresolved: Choice of learning algorithm and reward function can significantly impact outcomes, but paper only considers limited options
- What evidence would resolve it: Simulation study comparing macroeconomic impacts of different learning algorithms (policy gradients, actor-critic) and reward functions (market share, social welfare)

## Limitations
- State discretization may oversimplify complex market dynamics in high-dimensional economic environments
- Assumption of profit maximization ignores other business objectives like market share growth or sustainability
- Fixed episode structure and deterministic bankruptcy penalties may not capture full spectrum of economic shocks and recovery patterns

## Confidence

- Mechanism 1 (Strategy Emergence): High - Well-supported by controlled experiments across multiple competition levels with clear quantitative metrics
- Mechanism 2 (Strategic Segregation): Medium - Supported by results, but causal mechanism linking independent policies to segregation could benefit from additional analysis
- Mechanism 3 (Rationality-Stability Tradeoff): Medium - Results show relationship but don't fully explore underlying economic mechanisms or potential interventions

## Next Checks

1. **Robustness to State Discretization**: Systematically vary the number of state space poles and evaluate whether three strategy types still emerge reliably

2. **Policy Transferability**: Train RL agents on one competition level and test performance on unseen competition environments to assess strategy adaptability and generalization

3. **Alternative Reward Structures**: Replace profit-only rewards with multi-objective functions including market share or stability metrics to test whether strategic diversity persists under different incentive structures