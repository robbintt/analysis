---
ver: rpa2
title: Visual Car Brand Classification by Implementing a Synthetic Image Dataset Creation
  Pipeline
arxiv_id: '2406.01071'
source_url: https://arxiv.org/abs/2406.01071
tags: []
core_contribution: This work demonstrates that synthetic image datasets generated
  by Stable Diffusion, combined with YOLOv8 for quality assessment and bounding box
  detection, can effectively train classifiers for real-world car brand identification.
  The authors automate a pipeline that creates labeled synthetic images, evaluates
  different Stable Diffusion modes, and uses transfer learning with Resnet-18 to classify
  eight car brands from traffic camera footage.
---

# Visual Car Brand Classification by Implementing a Synthetic Image Dataset Creation Pipeline

## Quick Facts
- arXiv ID: 2406.01071
- Source URL: https://arxiv.org/abs/2406.01071
- Authors: Jan Lippemeier; Stefanie Hittmeyer; Oliver Niehörster; Markus Lange-Hegermann
- Reference count: 40
- Primary result: Achieved 75% accuracy classifying 8 car brands using only synthetic training data

## Executive Summary
This work presents a novel pipeline for creating synthetic image datasets to train car brand classifiers without requiring manual labeling. The approach combines Stable Diffusion for synthetic image generation with YOLOv8 for quality assessment and bounding box detection. By using German car registration data to generate prompts and training ResNet-18 via transfer learning, the authors demonstrate that purely synthetic training data can achieve 75% accuracy on real-world traffic camera footage. The method particularly excels for frequently occurring car brands and offers a scalable solution for reducing human effort in dataset creation for visual classification tasks.

## Method Summary
The pipeline generates synthetic car images using Stable Diffusion XL Turbo in both Text-to-Image and Image-to-Image modes, with prompts derived from German car registration data covering brands, models, years, and colors. YOLOv8 detects and crops cars from these synthetic images while filtering low-quality outputs based on confidence scores. The resulting images are resized to 64x64 pixels and used to train ResNet-18 via transfer learning, with the model pre-trained on ImageNet. The trained classifier is evaluated on 1,317 manually labeled real-world images from traffic cameras in Lemgo, Germany, covering eight car brands.

## Key Results
- Achieved 75% accuracy on real-world test data using only synthetic training images
- Models trained on 50,000 combined-mode synthetic images outperformed those trained on 400,000 images from a single mode
- Classification accuracy varied significantly by brand, ranging from 50% to 93% depending on frequency in the dataset

## Why This Works (Mechanism)

### Mechanism 1
Synthetic images generated by Stable Diffusion can effectively replace real labeled data for training classifiers when real data is scarce. The pipeline generates synthetic images with controlled distribution across car brands and models, then uses YOLOv8 to detect and crop cars, providing labeled training data without manual annotation. This works because synthetic images maintain enough visual similarity to real traffic photos to enable effective transfer learning. Break condition: If domain gap between synthetic and real images becomes too large (e.g., unrealistic lighting, perspective, or proportions).

### Mechanism 2
Combining Text-to-Image and Image-to-Image modes of Stable Diffusion improves classification accuracy by increasing dataset diversity. Text-to-Image generates varied perspectives while Image-to-Image preserves realistic proportions, and combining both modes creates a more diverse training set. This works because diversity in training data improves model generalization to real-world variations. Break condition: If one mode consistently produces low-quality or unrealistic images that degrade overall performance.

### Mechanism 3
YOLOv8 quality assessment effectively filters out poor synthetic images while providing bounding boxes and labels. YOLOv8 detects cars in synthetic images and provides confidence scores; images with low confidence or multiple detections are discarded, ensuring high-quality training data. This works because YOLOv8 can reliably detect cars in synthetic images generated by Stable Diffusion. Break condition: If YOLOv8 fails to detect cars in synthetic images or produces too many false positives/negatives.

## Foundational Learning

- Concept: Transfer learning with pre-trained models
  - Why needed here: The paper uses ResNet-18 pre-trained on ImageNet as the base classifier, adapting it for car brand classification
  - Quick check question: Why is transfer learning particularly effective when training data is limited?

- Concept: Domain adaptation between synthetic and real data
  - Why needed here: The model must generalize from synthetic training data to real traffic camera images
  - Quick check question: What characteristics must synthetic data share with real data for effective domain adaptation?

- Concept: Data augmentation techniques
  - Why needed here: The paper applies random rotation and resizes images to 64x64 pixels to improve model robustness
  - Quick check question: How does data augmentation help prevent overfitting when training on synthetic data?

## Architecture Onboarding

- Component map: Car registration data -> Label generation -> Image synthesis (Stable Diffusion) -> Quality assessment (YOLOv8) -> Preprocessing (resize/rotate) -> Model training (ResNet-18) -> Evaluation (real test data)

- Critical path: Data input → Label generation → Image synthesis → Quality assessment → Preprocessing → Model training → Evaluation

- Design tradeoffs:
  - Image resolution (64x64) vs. computational efficiency
  - Dataset size vs. training time
  - Text-to-Image diversity vs. Image-to-Image realism
  - YOLOv8 confidence threshold vs. dataset completeness

- Failure signatures:
  - Low accuracy on frequently occurring brands indicates domain gap issues
  - High variance in accuracy across different dataset sizes suggests insufficient diversity
  - Poor performance on specific brands may indicate Stable Diffusion bias

- First 3 experiments:
  1. Train ResNet-18 on 50K images from Text-to-Image mode only and evaluate on validation set
  2. Train ResNet-18 on 50K images from Image-to-Image mode only and compare performance
  3. Train ResNet-18 on 50K images combining both modes and measure accuracy improvement

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal balance between Text-to-Image and Image-to-Image modes for maximizing classification accuracy across all car brands? The paper shows that combining both modes leads to better performance than using either mode alone, but doesn't specify the optimal ratio. This remains unresolved because the authors only tested combined datasets without exploring different proportions of each mode. Systematic experiments varying the ratio of Text-to-Image to Image-to-Image images in the training dataset would resolve this question.

### Open Question 2
How does dataset size affect classification performance for underrepresented car brands like Renault and Skoda? The paper notes these brands have lower accuracy but doesn't explore whether increasing their representation in the dataset would improve performance. This remains unresolved because the study focused on balanced datasets without examining class-specific scaling effects. Experiments training models with different class-specific dataset sizes while maintaining overall balance would resolve this question.

### Open Question 3
Can the pipeline be adapted to generate synthetic images for car brands not present in YOLO's training data? The authors note the pipeline is limited by YOLO's capabilities and that adapting it for completely new classes would require engineering a different quality assessment method. This remains unresolved because the paper doesn't propose or test alternative quality assessment approaches for novel classes. Successful implementation of the pipeline for a car brand not included in YOLO's detection classes, demonstrating a new quality assessment method, would resolve this question.

## Limitations
- 75% accuracy leaves substantial room for improvement, particularly for less frequent car brands
- Domain gap between synthetic and real images appears significant for certain classes
- No detailed comparisons to baseline methods using real training data

## Confidence
- Synthetic data effectiveness: Medium - Pipeline works well but lacks comparison to real data approaches
- Mode combination benefits: High - Clear improvement demonstrated with systematic testing
- Brand-specific performance: Medium - Results show significant variation but underlying causes not fully explored

## Next Checks
1. Conduct ablation studies comparing synthetic-only training against models trained on real labeled data to quantify the performance gap and cost-benefit tradeoff.
2. Implement additional data augmentation techniques on the synthetic images and measure their impact on real-world accuracy, particularly for underrepresented brands.
3. Test the pipeline's robustness by generating synthetic data for different geographic regions with distinct car distributions and evaluating cross-regional performance.