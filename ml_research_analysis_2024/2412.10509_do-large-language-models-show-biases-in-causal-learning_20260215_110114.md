---
ver: rpa2
title: Do Large Language Models Show Biases in Causal Learning?
arxiv_id: '2412.10509'
source_url: https://arxiv.org/abs/2412.10509
tags: []
core_contribution: "This paper investigates whether large language models (LLMs) exhibit\
  \ causal illusions\u2014a cognitive bias where people perceive causal relationships\
  \ without sufficient evidence. The authors designed three novel tasks to assess\
  \ this bias in LLMs, including a contingency judgment task adapted from human experiments,\
  \ and curated a dataset of over 2,000 samples across spurious correlations, null-contingency\
  \ scenarios, and temporal cues that negate causality."
---

# Do Large Language Models Show Biases in Causal Learning?

## Quick Facts
- **arXiv ID**: 2412.10509
- **Source URL**: https://arxiv.org/abs/2412.10509
- **Reference count**: 1
- **Primary result**: LLMs exhibit significant causal illusion bias, particularly in tasks requiring responses on a 0-100 scale

## Executive Summary
This paper investigates whether large language models exhibit causal illusions—a cognitive bias where relationships are perceived without sufficient evidence. The authors designed three novel tasks to assess this bias in LLMs, including a contingency judgment task adapted from human experiments, and curated a dataset of over 2,000 samples across spurious correlations, null-contingency scenarios, and temporal cues that negate causality. Three LLMs (GPT-4o-Mini, Claude-3.5-Sonnet, and Gemini-1.5-Pro) were evaluated on these tasks. Results show that LLMs exhibit significant causal illusion bias, particularly in tasks requiring responses on a 0-100 scale, where they overestimated causal effectiveness. In headline generation tasks involving spurious correlations, the models showed bias levels comparable to or lower than humans. The study highlights that LLMs have not consistently internalized normative principles essential for accurate causal learning, raising concerns about their reliability in critical decision-making contexts.

## Method Summary
The authors developed three experimental tasks to assess causal illusion bias in LLMs. First, they adapted a contingency judgment task from human psychology experiments, where models must estimate the effectiveness of a potential cause based on presented evidence. Second, they created headline generation tasks using spurious correlations to evaluate bias in real-world applications. Third, they designed null-contingency and temporal-cue scenarios to test whether models can correctly identify when causality does not exist. The study evaluated three commercial LLMs (GPT-4o-Mini, Claude-3.5-Sonnet, and Gemini-1.5-Pro) using over 2,000 curated samples across these tasks. Performance was measured using both binary response formats and continuous 0-100 scale responses, with comparisons made to human performance baselines from previous studies.

## Key Results
- LLMs showed significant overestimation of causal effectiveness in contingency judgment tasks, particularly when responses required a 0-100 scale
- Headline generation tasks revealed that LLM bias levels were comparable to or lower than human bias in similar scenarios
- Models demonstrated difficulty correctly identifying null-contingency scenarios and temporal cues that negate causality

## Why This Works (Mechanism)
The paper does not explicitly detail the underlying mechanisms of how LLMs process causal information or why they exhibit these biases. The authors focus on empirical observation rather than theoretical explanation of the bias mechanisms.

## Foundational Learning
- **Causal illusion bias**: A cognitive bias where people perceive causal relationships without sufficient evidence. *Why needed*: Provides the theoretical framework for understanding the phenomenon being studied. *Quick check*: Can be assessed by comparing perceived causality against actual statistical relationships.
- **Contingency judgment**: The ability to estimate the effectiveness of a potential cause based on observed evidence. *Why needed*: Forms the basis of the primary experimental task used to measure causal illusion in LLMs. *Quick check*: Validated through controlled experimental designs with known ground truth.
- **Spurious correlation**: Apparent relationships between variables that are actually coincidental or due to confounding factors. *Why needed*: Represents the type of flawed causal reasoning being tested in headline generation tasks. *Quick check*: Can be identified through statistical analysis showing lack of true causal relationship.

## Architecture Onboarding
**Component Map**: Data Generation -> Task Design -> LLM Evaluation -> Bias Measurement -> Human Comparison
**Critical Path**: Experimental Design → Dataset Curation → Model Testing → Statistical Analysis → Results Interpretation
**Design Tradeoffs**: The study prioritized controlled experimental conditions over real-world complexity, using curated datasets rather than naturalistic inputs to isolate causal reasoning capabilities.
**Failure Signatures**: Models consistently overestimate causal effectiveness in 0-100 scale responses, fail to identify null-contingency scenarios, and struggle with temporal cues that negate causality.
**First Experiments**: 1) Test additional LLM architectures to determine if bias is model-specific. 2) Vary prompt engineering approaches to assess impact on causal judgments. 3) Conduct longitudinal studies tracking bias evolution across model updates.

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow selection of only three LLM models limits generalizability to broader model populations
- Does not address potential confounding factors like prompt engineering effects or temperature settings
- Human comparison baseline lacks detailed demographic information for proper context

## Confidence
- **High Confidence**: LLMs exhibit causal illusion bias in contingency judgment tasks; bias is more pronounced in 0-100 scale responses
- **Medium Confidence**: LLMs' causal learning capabilities are unreliable for critical decision-making contexts; LLM bias comparable to human bias in headline tasks
- **Low Confidence**: LLMs have not consistently internalized normative principles essential for accurate causal learning

## Next Checks
1. Test additional LLM architectures and sizes (including open-source models) to determine if the causal illusion bias is model-specific or a general LLM phenomenon
2. Conduct experiments with varied prompt engineering approaches and temperature settings to quantify their impact on causal judgment outputs
3. Design longitudinal studies tracking how causal judgment capabilities evolve as models are updated or retrained to identify potential improvements or degradations in causal reasoning