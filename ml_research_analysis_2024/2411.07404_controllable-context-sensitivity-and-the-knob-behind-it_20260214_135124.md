---
ver: rpa2
title: Controllable Context Sensitivity and the Knob Behind It
arxiv_id: '2411.07404'
source_url: https://arxiv.org/abs/2411.07404
tags:
- context
- answer
- subspace
- prior
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how language models trade off context sensitivity
  versus reliance on prior knowledge, a key capability for applications like retrieval-augmented
  generation. To study this, the authors design a controlled task where models are
  given a context and a question, then explicitly instructed to either use context
  or prior knowledge to answer.
---

# Controllable Context Sensitivity and the Knob Behind It

## Quick Facts
- arXiv ID: 2411.07404
- Source URL: https://arxiv.org/abs/2411.07404
- Reference count: 40
- Key outcome: A single 1-dimensional subspace in a single transformer layer can effectively control whether language models use context or prior knowledge, generalizing across model families and training states.

## Executive Summary
This paper investigates how language models trade off context sensitivity versus reliance on prior knowledge, a key capability for applications like retrieval-augmented generation. To study this, the authors design a controlled task where models are given a context and a question, then explicitly instructed to either use context or prior knowledge to answer. Models are adapted to this task via fine-tuning or in-context learning and evaluated for their ability to switch between context and prior knowledge correctly.

The authors develop a linear-time algorithm to identify transformer layers that integrate context, prior knowledge, and the decision to use one versus the other. They then identify a single 1-dimensional subspace in a single layer that encodes whether the model follows context or prior knowledge. Remarkably, this subspace, discovered in a fine-tuned model, can also steer non-fine-tuned models of the same family effectively. The findings suggest a simple, fundamental mechanism underlying context sensitivity in language models.

## Method Summary
The authors create controlled datasets (CCS-BF, CCS-MH, CCS-AR) where models must answer questions using either context or prior knowledge based on explicit instructions. They fine-tune models on these tasks and develop an iterative patching algorithm to identify transformer layers responsible for encoding the context-versus-prior decision. Using pyvene, they learn a rank-1 orthogonal projection matrix that captures this decision in a 1-dimensional subspace. The subspace is then used to steer model behavior by setting scalar values that push the residual stream toward context-following or prior-knowledge-following responses. The approach is tested across different model families, sizes, and training states to verify generalization.

## Key Results
- A single 1-dimensional subspace in layer 16 of Llama-3.1-8B models can effectively control whether the model uses context or prior knowledge, achieving high pair accuracy across datasets
- The same subspace discovered in fine-tuned models effectively steers non-fine-tuned models of the same family, including base and instruct variants
- Strong correlation (0.908) between model performance on CCS tasks and how distinctly the subspace separates context-agreeing from context-ignoring answers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A single 1-dimensional subspace in a single transformer layer encodes whether the model follows context or prior knowledge.
- Mechanism: The subspace acts as a "knob" that linearly separates context-agreeing from context-ignoring responses. When the projection of the residual stream onto this subspace crosses a threshold, the model switches behavior from context-based to prior-based answering.
- Core assumption: The context-versus-prior decision is low-dimensional and can be represented by a rank-1 orthogonal projection in activation space.
- Break condition: If the context/prompt requires reasoning chains beyond direct copying, the subspace intervention loses effectiveness as the model must use more complex intermediate processing.

### Mechanism 2
- Claim: The model executes three high-level steps to solve the controllable context sensitivity task: extracting answers from prior knowledge, extracting answers from context, and deciding which answer to use.
- Mechanism: Attention heads in specific layers integrate the intent (w) into the residual stream. Later layers read this intent and conditionally load either the context answer or prior answer. The subspace captures the decision variable that gates which answer is loaded.
- Core assumption: The intent is encoded as a binary variable in the residual stream, which later layers can read to trigger the appropriate answer-loading pathway.
- Break condition: If the model architecture changes (e.g., different positional encoding or attention mechanism), the layer identification and subspace location may shift significantly.

### Mechanism 3
- Claim: The identified subspace generalizes across model families, sizes, and training states (fine-tuned, in-context learning, zero-shot).
- Mechanism: The subspace is a fundamental representation of the context-versus-prior decision that is learned during pretraining and reused regardless of downstream task adaptation. Fine-tuning or ICL merely adjusts the subspace value to align with the desired behavior.
- Core assumption: The context-versus-prior decision is a fundamental cognitive operation that all transformer-based LLMs must implement, leading to similar geometric representations across models.
- Break condition: If the model uses fundamentally different architectural components (e.g., RNNs, different attention mechanisms), the subspace representation may not exist or may require different identification methods.

## Foundational Learning

- Concept: Linear algebra and vector space geometry
  - Why needed here: The entire intervention mechanism relies on understanding how high-dimensional activation vectors can be projected onto lower-dimensional subspaces to capture specific concepts. The rank-1 projection matrix and subspace steering require comfort with vector projections, orthogonal complements, and basis vectors.
  - Quick check question: If u is a unit vector and h is a hidden state vector, what does the dot product u⊤h represent geometrically?

- Concept: Transformer architecture and residual streams
  - Why needed here: The intervention patches the residual stream after a specific transformer layer, requiring understanding of how information flows through transformer blocks, what the residual stream contains at each layer, and how attention heads integrate context.
  - Quick check question: In a standard transformer block, what is the mathematical relationship between the input residual stream, attention output, MLP output, and final output residual stream?

- Concept: Mechanistic interpretability and activation patching
  - Why needed here: The methodology uses activation patching to identify which components are responsible for specific behaviors. Understanding causal intervention in neural networks, how to measure intervention effects, and how to interpret patched outputs is crucial for reproducing and extending the work.
  - Quick check question: If you patch attention head outputs from a source example into a target example, what does it mean if the target model's output probability for the source answer increases?

## Architecture Onboarding

- Component map:
  Tokenizer → embeddings → positional encoding → Transformer blocks (MHA → residual → LN → MLP → residual → LN) → Final residual stream → unembedding → logits → softmax
  Intervention point: Residual stream after layer 16
  Steering mechanism: Rank-1 orthogonal projection P = uu⊤ applied to residual stream

- Critical path: Context → residual stream (layer 16) → projection P → subspace value → decision gate → answer selection pathway → output

- Design tradeoffs:
  - Layer selection: Earlier layers may capture raw intent but lack context integration; later layers have full context but may be harder to steer; layer 16 was chosen as a balance point
  - Subspace dimensionality: Rank-1 subspace is simple and interpretable but may miss multi-dimensional aspects of the decision; higher-rank subspaces could capture more nuance but are harder to identify and control
  - Intervention strength: Too small c(w) values may not overcome model biases; too large may cause instability or unexpected behaviors

- Failure signatures:
  - No effect: Subspace does not capture the decision variable; incorrect layer selection; projection direction u is wrong
  - Opposite effect: c(w) values have wrong sign; subspace captures inverse of intended concept
  - Partial effect: Subspace captures related but not exact concept; requires additional conditioning or multi-dimensional steering
  - Dataset-specific failure: Subspace works on in-domain data but not out-of-domain; suggests task-specific rather than fundamental representation

- First 3 experiments:
  1. Verify layer identification: Run the search algorithm on a simple controllable context sensitivity dataset and confirm that patching identified layers switches model behavior as expected
  2. Test subspace learning: Train the projection matrix P on a subset of examples and verify that patched outputs match the intended intent by checking answer probabilities
  3. Transfer test: Apply the learned subspace from a fine-tuned model to a non-fine-tuned model of the same family and measure whether steering still works, confirming generalization across training states

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent can models use contextual information as part of an intermediate reasoning chain rather than just direct copying?
- Basis in paper: [inferred] The authors note that models perform better when context answers are explicitly stated, and that more investigation is needed to understand if models can use context information in intermediate reasoning chains.
- Why unresolved: The current CCS task design primarily tests whether models can directly copy answers from context versus using prior knowledge, without examining their ability to reason through contextual information.
- What evidence would resolve it: Experiments with CCS-style tasks that require reasoning through context information (e.g., multi-step inference, arithmetic with context) rather than simple direct copying would show whether models can effectively use context as part of a reasoning chain.

### Open Question 2
- Question: Does the identified 1-dimensional subspace generalize to control other binary model behaviors beyond context sensitivity?
- Basis in paper: [explicit] The authors propose investigating whether this subspace influences additional behaviors like instruction-following, and suggest this could help understand mechanisms behind other functionalities.
- Why unresolved: The paper only demonstrates the subspace's effectiveness for controlling context versus prior knowledge, without testing it on other binary decision-making tasks.
- What evidence would resolve it: Applying the same subspace identification and intervention methodology to other binary model behaviors (e.g., sentiment, truthfulness, instruction-following) would reveal whether this is a general mechanism for binary decisions.

### Open Question 3
- Question: What is the precise relationship between the subspace's effectiveness and a model's overall performance on the CCS task?
- Basis in paper: [explicit] The authors show a strong correlation (0.908) between model performance and how distinctly the subspace separates context-agreeing from context-ignoring answers, but don't fully characterize this relationship.
- Why unresolved: While correlation is established, the paper doesn't determine if the subspace is a necessary component, sufficient component, or just one of many factors affecting CCS performance.
- What evidence would resolve it: Ablation studies varying the subspace's distinctiveness while measuring CCS performance, and testing whether artificially enhancing the subspace's separation improves model performance, would clarify its role in overall task capability.

## Limitations
- The subspace-based intervention may not capture complex reasoning chains or multi-hop inference tasks that require integrating information across multiple context sentences
- The layer identification process is sensitive to hyperparameters (threshold, margin, epsilon), and incorrect settings could lead to misidentification of the steering layer
- The approach requires access to model activations for subspace learning, which may not be feasible for all deployment scenarios

## Confidence
- High confidence: The subspace exists and can effectively steer model behavior between context and prior knowledge in the tested model families (Llama-3.1, Mistral-v0.3, Gemma-2). The generalization across fine-tuned, in-context learning, and zero-shot settings is well-supported.
- Medium confidence: The subspace represents a fundamental decision variable across all transformer-based LLMs. While evidence is strong within tested families, broader architectural diversity needs validation.
- Medium confidence: The mechanism captures the complete decision process. The current evidence shows effective steering for direct context-matching tasks but may not extend to complex reasoning scenarios.

## Next Checks
1. Test the subspace intervention on multi-hop reasoning tasks where the model must integrate information across multiple context sentences, verifying whether the rank-1 subspace can still effectively steer behavior.
2. Apply the same methodology to a non-transformer architecture (e.g., RNN or Mamba) to determine if similar subspace-based decision variables exist and can be leveraged for control.
3. Investigate the temporal stability of the subspace by fine-tuning the same model on different datasets over time and measuring how the subspace direction and effectiveness change with continued training.