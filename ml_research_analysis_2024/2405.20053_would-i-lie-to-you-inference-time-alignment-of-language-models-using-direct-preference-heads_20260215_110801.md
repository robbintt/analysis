---
ver: rpa2
title: Would I Lie To You? Inference Time Alignment of Language Models using Direct
  Preference Heads
arxiv_id: '2405.20053'
source_url: https://arxiv.org/abs/2405.20053
tags:
- which
- language
- glue
- question
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Direct Preference Heads (DPH), a fine-tuning
  framework that enables language models to learn human preference signals through
  an auxiliary reward head without directly affecting the output distribution of the
  language modeling head. DPH addresses the issue of RLHF potentially harming a language
  model's reasoning capabilities and introducing artifacts such as hallucinations.
---

# Would I Lie To You? Inference Time Alignment of Language Models using Direct Preference Heads

## Quick Facts
- arXiv ID: 2405.20053
- Source URL: https://arxiv.org/abs/2405.20053
- Reference count: 40
- Introduces Direct Preference Heads (DPH) to enable language models to learn human preferences without affecting core language modeling capabilities

## Executive Summary
This paper introduces Direct Preference Heads (DPH), a fine-tuning framework that enables language models to learn human preference signals through an auxiliary reward head without directly affecting the output distribution of the language modeling head. DPH addresses the issue of RLHF potentially harming a language model's reasoning capabilities and introducing artifacts such as hallucinations. The method uses an auxiliary reward head to self-evaluate outputs sampled at inference time and select the highest scoring candidate. The authors perform a theoretical analysis of their objective function and find strong ties to Conservative Direct Preference Optimization (cDPO).

## Method Summary
DPH works by adding an auxiliary reward head to the language model that learns to predict human preferences through preference data. During inference, the model generates multiple candidate outputs and uses the reward head to self-evaluate and select the highest-scoring response. The reward head is trained using a contrastive loss that encourages it to assign higher scores to preferred responses. The authors theoretically analyze their objective and establish connections to Conservative DPO, showing that DPH can be viewed as a variant of DPO with additional regularization.

## Key Results
- DPH produces models that achieve higher scores than those fine-tuned with SFT or DPO alone on GLUE, RACE, and GPT4All evaluation suite
- The method successfully decouples preference learning from language modeling, avoiding degradation in reasoning capabilities
- Theoretical analysis reveals strong ties between DPH and Conservative DPO, providing a foundation for understanding the method's behavior

## Why This Works (Mechanism)
DPH works by separating the preference learning task from the core language modeling task through an auxiliary reward head. During training, the reward head learns to predict human preferences from pairwise comparisons. At inference time, the model generates multiple candidates and uses the reward head to select the most preferred output. This architecture allows the language model to maintain its original capabilities while gaining the ability to produce preference-aligned outputs. The self-evaluation mechanism at inference time enables the model to adaptively choose responses based on learned preferences.

## Foundational Learning
- **Preference learning via contrastive objectives**: Why needed - to train models to distinguish preferred from non-preferred outputs; Quick check - verify the reward head learns to assign higher scores to preferred responses
- **Inference-time candidate generation and selection**: Why needed - to enable adaptive response selection based on learned preferences; Quick check - test that multiple candidates are generated and the highest-scoring one is selected
- **Decoupling preference alignment from language modeling**: Why needed - to prevent degradation of core language capabilities during alignment; Quick check - compare GLUE/RACE scores before and after DPH fine-tuning
- **Conservative DPO connection**: Why needed - to understand theoretical foundations and potential limitations; Quick check - verify the mathematical relationship between DPH and cDPO holds
- **Reward head temperature scaling**: Why needed - to control the sharpness of preference predictions; Quick check - test performance across different temperature values
- **Contrastive loss for reward head training**: Why needed - to learn effective preference discrimination; Quick check - ensure the loss properly separates preferred from non-preferred responses

## Architecture Onboarding

Component Map:
Language Model Head -> Output Distribution -> (candidate generation) -> Reward Head -> Score Comparison -> Selected Output

Critical Path:
Input text → Language model generation → Multiple candidate outputs → Reward head evaluation → Score comparison → Best candidate selection

Design Tradeoffs:
- Computational overhead: Generating multiple candidates increases inference time but enables better preference alignment
- Model complexity: Adding a reward head increases parameters but allows separation of preference learning from language modeling
- Hyperparameter sensitivity: Temperature and weight scaling affect performance but require careful tuning

Failure Signatures:
- Degraded reasoning capabilities if the reward head overfits to preferences
- Inconsistent outputs across generations if temperature is poorly tuned
- Failure to improve over base model if preference data is insufficient or noisy

First 3 Experiments:
1. Test DPH on a simple binary preference task to verify the reward head learns to discriminate
2. Compare GLUE/RACE scores between base model, SFT, DPO, and DPH to verify reasoning preservation
3. Generate multiple outputs for the same prompt and verify the reward head selects different candidates based on learned preferences

## Open Questions the Paper Calls Out
None

## Limitations
- Lack of direct evaluation of how DPH affects reasoning capabilities, relying on indirect evidence through benchmark scores
- Self-evaluation mechanism may introduce new failure modes not captured by evaluation suites
- Strong theoretical ties to cDPO need empirical validation - practical implications remain unclear
- Training procedure's sensitivity to hyperparameter choices not thoroughly explored
- Hallucination reduction claims need more rigorous testing across different contexts and datasets

## Confidence
- High confidence: Technical implementation of DPH and its basic functionality (output selection via reward head) appears sound and well-documented
- Medium confidence: Theoretical analysis connecting DPH to cDPO, though practical implications need more empirical validation
- Medium confidence: Performance improvements over SFT and DPO, though ablation studies could be more comprehensive
- Low confidence: Claims about avoiding reasoning degradation and hallucinations, as these require more targeted evaluation

## Next Checks
1. Conduct targeted reasoning capability tests using chain-of-thought prompting and mathematical reasoning benchmarks to directly assess whether DPH preserves or improves reasoning compared to RLHF
2. Implement systematic hallucination detection tests using known fact-based queries and consistency checks across multiple generations to validate the hallucination reduction claims
3. Perform ablation studies varying the temperature parameter and reward head weight to understand the robustness of DPH to hyperparameter choices and identify optimal configurations