---
ver: rpa2
title: 'Language Complexity and Speech Recognition Accuracy: Orthographic Complexity
  Hurts, Phonological Complexity Doesn''t'
arxiv_id: '2406.09202'
source_url: https://arxiv.org/abs/2406.09202
tags:
- language
- languages
- writing
- speech
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors investigated the impact of linguistic complexity on
  Automatic Speech Recognition (ASR) accuracy. They hypothesized that orthographic
  and phonological complexities would both negatively affect ASR performance.
---

# Language Complexity and Speech Recognition Accuracy: Orthographic Complexity Hurts, Phonological Complexity Doesn't

## Quick Facts
- arXiv ID: 2406.09202
- Source URL: https://arxiv.org/abs/2406.09202
- Authors: Chihiro Taguchi; David Chiang
- Reference count: 8
- Primary result: Orthographic complexity significantly hurts ASR accuracy while phonological complexity does not

## Executive Summary
This study investigates how linguistic complexity affects Automatic Speech Recognition (ASR) accuracy by fine-tuning Wav2Vec2-XLSR-53 on 25 languages with 15 different writing systems. The authors hypothesized that both orthographic and phonological complexities would negatively impact ASR performance. Through systematic experimentation and analysis, they discovered that orthographic complexity (measured by grapheme inventory, entropy, and logographicity) significantly correlates with lower ASR accuracy, while phonological complexity (measured by phoneme inventory) shows no significant correlation. Languages with logographic writing systems and large grapheme inventories perform worse than phonographic systems.

## Method Summary
The researchers fine-tuned the Wav2Vec2-XLSR-53 model on 25 languages using 10,000 seconds of audio data per language with standardized hyperparameters (20 epochs, learning rate 0.0003). They evaluated performance using Character Error Rate (CER) and Calibrated Errors Per Second (CEPS) metrics, then analyzed correlations between ASR accuracy and linguistic complexity measures including grapheme inventory size, unigram entropy, logographicity score, and phoneme inventory size. The study controlled for data quantity across languages to isolate the effects of writing system complexity.

## Key Results
- Orthographic complexity measures (grapheme inventory size, unigram entropy, logographicity) significantly correlate with lower ASR accuracy (p < 0.05)
- Phonological complexity (phoneme inventory size) shows no significant correlation with ASR accuracy
- Languages with logographic writing systems (Chinese Hanzi) and large grapheme inventories (Korean Hangul syllables) perform worse than phonographic systems (Japanese Kana, Hangul Jamo)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Orthographic complexity measured by number of grapheme types correlates with higher character error rates in ASR models.
- Mechanism: When fine-tuning Wav2Vec2-XLSR-53, the model must learn to map audio input to orthographic output. Languages with more grapheme types present a larger and more complex output space, increasing the probability of misclassifications during the fine-tuning process.
- Core assumption: The number of grapheme types is a proxy for orthographic complexity that impacts the learning difficulty of the ASR model.
- Evidence anchors:
  - [abstract] "The results demonstrate that orthographic complexities significantly correlate with low ASR accuracy, while phonological complexity shows no significant correlation."
  - [section] "The results demonstrate a significant correlation (ð‘ < 0.05) between CER and orthography-related variables (the number of graphemes, unigram entropy, and logographicity score), as shown in the correlation matrix in Table 3."

### Mechanism 2
- Claim: Logographic writing systems increase the difficulty of ASR learning due to the irregular mapping between pronunciation and orthography.
- Mechanism: Logographicity is measured by how much attention the model must spread beyond the target word's phonemes to predict its orthography. Higher logographicity means the model must rely more on context to disambiguate, increasing the complexity of the fine-tuning task.
- Core assumption: The attention spread metric accurately captures the degree to which orthography depends on context rather than just pronunciation.
- Evidence anchors:
  - [abstract] "Hypothesis 2. The more a language's writing system encodes word- or morpheme-level information, the more ASR accuracy decreases."
  - [section] "The results demonstrate a significant correlation (ð‘ < 0.05) between CER and orthography-related variables (the number of graphemes, unigram entropy, and logographicity score)."

### Mechanism 3
- Claim: Phonological complexity (number of phoneme types) does not significantly impact ASR accuracy.
- Mechanism: The Wav2Vec2-XLSR-53 model is pretrained on diverse speech data and fine-tuned on a limited dataset, making it robust to learning different phonological systems regardless of their complexity.
- Core assumption: The self-supervised pretraining phase equips the model with sufficient phonological representations that fine-tuning on 10,000 seconds of data is adequate for any phonological system.
- Evidence anchors:
  - [abstract] "while phonological complexity shows no significant correlation."
  - [section] "there is no significant correlation ( ð‘ > 0.05) in Table 3 between ASR accuracy and the number of phoneme types."

## Foundational Learning

- Concept: Character Error Rate (CER) calculation and interpretation
  - Why needed here: CER is the primary metric for evaluating ASR accuracy across different languages and writing systems, especially when word boundaries are unclear.
  - Quick check question: How is CER calculated and why is it preferred over WER in this study?

- Concept: Logographicity and its measurement using attention spread
  - Why needed here: Understanding how logographicity is quantified helps explain why languages like Chinese Hanzi perform worse in ASR tasks.
  - Quick check question: What does the attention spread metric measure and how does it relate to logographicity?

- Concept: Calibrated Errors Per Second (CEPS) as a cross-linguistic error metric
  - Why needed here: CEPS normalizes error rates across languages with different writing systems, providing a more comparable measure than raw CER.
  - Quick check question: How does CEPS adjust for differences in writing system granularity compared to CER?

## Architecture Onboarding

- Component map:
  Wav2Vec2-XLSR-53 -> Feature encoder (CNN) -> Quantization layer -> Transformer -> CTC layer -> Grapheme set

- Critical path:
  1. Load pretrained Wav2Vec2-XLSR-53 model
  2. Prepare language-specific dataset (10k seconds audio + transcription)
  3. Tokenize transcriptions according to writing system
  4. Fine-tune model with CTC loss for specified epochs
  5. Evaluate using CER and CEPS metrics
  6. Analyze correlation with orthographic and phonological complexity measures

- Design tradeoffs:
  - Using a single pretrained model for all languages limits language-specific optimization but enables cross-linguistic comparison
  - Fixed training data duration (10k seconds) controls for data quantity but may not be optimal for all languages
  - Choice of CTC loss simplifies training but may not capture all linguistic nuances

- Failure signatures:
  - High CER with low CEPS difference between writing systems suggests orthographic complexity issues
  - Unstable validation curves during training indicate learning difficulties with complex orthographies
  - Poor performance on logographic languages despite adequate phoneme representation

- First 3 experiments:
  1. Fine-tune on English (Latin script) as a baseline to verify model functionality
  2. Fine-tune on Korean with Hangul syllables vs. Hangul Jamo to isolate orthographic complexity effects
  3. Fine-tune on Chinese with Hanzi, Pinyin, and Zhuyin to examine logographicity impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ASR accuracy vary across different training dataset sizes and epochs for complex orthographies compared to phonographic scripts?
- Basis in paper: [explicit] The paper mentions that different learning curves are observed for different writing systems, with complex orthographies showing slower learning and less stable curves. However, the study limited the amount of training data to 10k seconds and 20 epochs for all languages.
- Why unresolved: The study did not explore how varying the training dataset size and number of epochs would affect ASR accuracy for complex orthographies versus phonographic scripts.
- What evidence would resolve it: Experiments comparing ASR accuracy across a range of training dataset sizes and epochs for complex orthographies and phonographic scripts would provide insights into the learning dynamics and potential performance ceilings for each type of writing system.

### Open Question 2
- Question: How do other multilingual pre-trained Wav2Vec2 models (e.g., Wav2Vec2-XLS-R, Wav2Vec2-BERT) perform on languages with complex orthographies compared to the Wav2Vec2-XLSR-53 model used in this study?
- Basis in paper: [explicit] The paper mentions that other multilingual pre-trained Wav2Vec2 models have been developed with more pretraining data and various model parameter sizes, but the study did not show results from these models.
- Why unresolved: The study only used the Wav2Vec2-XLSR-53 model and did not explore how other pre-trained models might perform on languages with complex orthographies.
- What evidence would resolve it: Experiments using different multilingual pre-trained Wav2Vec2 models on languages with complex orthographies would reveal if the findings are specific to the Wav2Vec2-XLSR-53 model or if they generalize to other pre-trained models.

### Open Question 3
- Question: How does the attention-based logographicity measure (S_token) account for the many-to-one mapping relationship between phonemes and graphemes in orthographic complexity?
- Basis in paper: [explicit] The paper discusses the concept of logographicity and its measurement using an attention-based metric (S_token). However, it also mentions that this metric only considers how much the model looks outside the target word and fails to take into account the many-to-one mapping relationship between phonemes and graphemes.
- Why unresolved: The current attention-based logographicity measure does not fully capture the orthographic complexity arising from the many-to-one mapping between phonemes and graphemes.
- What evidence would resolve it: Developing and validating a modified attention-based metric that accounts for both the one-to-many and many-to-one mapping relationships between phonemes and graphemes would provide a more comprehensive measure of orthographic complexity.

## Limitations
- The study uses a single ASR architecture (Wav2Vec2-XLSR-53) and standardized training protocol, limiting generalizability to other models
- Fixed training data duration (10,000 seconds) may be insufficient for some complex orthographies
- The analysis does not account for other orthographic features like consistency of grapheme-phoneme mapping or morphological complexity

## Confidence
- High confidence: The correlation between orthographic complexity (grapheme inventory size, unigram entropy, logographicity) and ASR accuracy is supported by statistical analysis with p < 0.05
- Medium confidence: The claim that phonological complexity does not significantly impact ASR accuracy is supported by data but the underlying mechanism is inferred
- Medium confidence: The distinction between logographic and phonographic systems' impact on ASR performance is observed but relies on a specific attention-based metric

## Next Checks
1. Cross-model validation: Test whether the observed relationship between orthographic complexity and ASR accuracy holds across different ASR architectures (e.g., HuBERT, Whisper) and training protocols
2. Data quantity sensitivity: Conduct experiments with varying amounts of training data (e.g., 5k, 10k, 20k seconds) for languages with different orthographic complexities
3. Orthographic feature decomposition: Isolate effects of different orthographic features (grapheme inventory size, consistency, morphological complexity) on ASR performance