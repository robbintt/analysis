---
ver: rpa2
title: 'Linguistic Collapse: Neural Collapse in (Large) Language Models'
arxiv_id: '2405.17767'
source_url: https://arxiv.org/abs/2405.17767
tags:
- neural
- https
- collapse
- width
- depth
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the phenomenon of Neural Collapse (NC) in large
  language models (LLMs) by empirically investigating the impact of scaling architectures
  and training of causal language models (CLMs) on their progression towards NC. The
  authors train a suite of Transformer-based CLMs across a grid of model widths, depths,
  and training epochs on the TinyStories dataset to assess the degrees to which NC
  properties develop and how they relate to generalization performance.
---

# Linguistic Collapse: Neural Collapse in (Large) Language Models

## Quick Facts
- arXiv ID: 2405.17767
- Source URL: https://arxiv.org/abs/2405.17767
- Reference count: 40
- Key finding: Neural Collapse properties correlate with improved generalization in language models

## Executive Summary
This paper explores Neural Collapse (NC) in large language models by empirically investigating how scaling architectures and training of causal language models affects their progression toward NC. The authors train Transformer-based causal language models across various model widths, depths, and training epochs on the TinyStories dataset. They assess how NC properties develop and relate to generalization performance, finding that NC properties emerge with scale and regularization and correlate with improved validation performance.

The study reveals a relationship between NC and generalization independent of model scale, suggesting NC may be a factor of generalization in its own right. These results demonstrate the generality of NC as it extends to the challenging setting of language modeling and inspire further research to deepen understanding of LLMs and improve architectures based on NC-related properties.

## Method Summary
The authors conduct a systematic empirical investigation of Neural Collapse properties in Transformer-based causal language models (CLMs). They train a suite of models across a grid of model widths, depths, and training epochs on the TinyStories dataset. The study evaluates multiple NC properties including within-class variability collapse, hyperspherical uniformity, and classifier agreement. They analyze how these properties develop across different architectural configurations and training durations, and correlate them with validation performance to understand the relationship between NC and generalization in language models.

## Key Results
- NC properties (within-class variability collapse, hyperspherical uniformity, classifier agreement) emerge with scale and regularization
- These NC properties correlate with improved validation performance in language models
- A relationship exists between NC and generalization independent of model scale

## Why This Works (Mechanism)
The paper demonstrates that Neural Collapse properties naturally emerge in language models as they scale and are trained longer, similar to observations in computer vision. The mechanism appears to involve the model's representations becoming increasingly structured and aligned during training, with class-conditional means moving toward a common direction while within-class variability collapses. This structural organization appears to facilitate better generalization, though the exact causal mechanisms remain to be fully elucidated.

## Foundational Learning
- Neural Collapse: The phenomenon where features from different classes collapse to their class-means while these means align toward a common direction
  - Why needed: Provides the theoretical framework for understanding the structural patterns observed in trained models
  - Quick check: Verify that class-conditional means converge to a common direction while within-class variance decreases

- Causal Language Models: Language models that predict the next token given previous context
  - Why needed: The specific architecture type studied, which differs from masked or bidirectional models
  - Quick check: Confirm the model only attends to previous tokens when making predictions

- Hyperspherical Uniformity: The property where feature representations are uniformly distributed on a high-dimensional sphere
  - Why needed: One of the key NC properties measured and correlated with performance
  - Quick check: Measure angular distribution of features to verify uniform spread

## Architecture Onboarding

Component Map: Data -> Token Embedding -> Transformer Blocks -> MLP Head -> Predictions

Critical Path: Input sequence → Token embeddings → Self-attention layers → Feed-forward layers → Output layer → Next token prediction

Design Tradeoffs: Model width vs. depth (affects NC emergence), training duration vs. overfitting, capacity vs. generalization

Failure Signatures: NC properties not emerging despite sufficient training, performance degradation with NC emergence, inconsistent NC patterns across architectures

First Experiments:
1. Train a minimal Transformer CLM on TinyStories and measure initial NC property development
2. Compare NC emergence across different width/depth combinations at fixed parameter count
3. Analyze NC property progression during training at different learning rates

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Analysis based on synthetic TinyStories dataset limits generalizability to real-world tasks
- Grid search covers limited range of model scales compared to state-of-the-art LLMs
- Focus primarily on encoder-only causal language models leaves other architectures unexplored

## Confidence

High confidence in core finding that NC properties correlate with improved validation performance, supported by systematic experiments across multiple architectural dimensions.

Medium confidence in claim that NC is "a factor of generalization in its own right," as study establishes correlation but cannot definitively establish causation independent of other factors.

Low confidence in broader implications for understanding LLMs, given synthetic dataset and simplified experimental setup.

## Next Checks

1. Replicate NC analysis on real-world language datasets (e.g., Wikipedia, BooksCorpus) to verify relationships hold beyond synthetic data.

2. Extend study to decoder-only architectures (GPT-style) and encoder-decoder models (T5-style) to assess whether NC properties generalize across different LLM families.

3. Conduct ablation studies isolating specific NC properties (e.g., within-class collapse vs. hyperspherical uniformity) to determine their individual contributions to generalization performance.