---
ver: rpa2
title: 'Creative Beam Search: LLM-as-a-Judge For Improving Response Generation'
arxiv_id: '2405.00099'
source_url: https://arxiv.org/abs/2405.00099
tags:
- beam
- search
- creative
- response
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Creative Beam Search (CBS), a novel generate-and-test
  sampling scheme that uses Diverse Beam Search (DBS) and LLM-as-a-Judge to simulate
  aspects of the human creative process. CBS first generates a diverse set of candidate
  solutions using DBS, then performs a self-evaluation step where the LLM selects
  the best candidate from the top-K generated outputs.
---

# Creative Beam Search: LLM-as-a-Judge For Improving Response Generation

## Quick Facts
- arXiv ID: 2405.00099
- Source URL: https://arxiv.org/abs/2405.00099
- Reference count: 5
- One-line primary result: CBS preferred over standard sampling in 45% of cases with significant margin

## Executive Summary
Creative Beam Search (CBS) is a novel generate-and-test sampling scheme that uses Diverse Beam Search (DBS) and LLM-as-a-Judge to improve response generation creativity. The method first generates diverse candidate solutions using DBS, then performs a self-evaluation step where the LLM selects the best candidate from the top-K outputs. A qualitative experiment with 31 participants found CBS outputs were preferred over standard sampling in 45% of cases, with the self-evaluation step shown to be effective by selecting different candidates than DBS in 71% of cases.

## Method Summary
CBS generates diverse candidate solutions through Diverse Beam Search, which divides the beam budget into groups and applies a diversity penalty to encourage exploration of distinct solution paths. The LLM-as-a-Judge module then evaluates the top-K candidates by creating K prompts with rotated candidate positions to mitigate positional bias. The LLM votes for the most creative candidate in each prompt, and votes are aggregated to select the final output. This two-stage process simulates aspects of human creative problem-solving by first exploring multiple possibilities then selecting the best solution.

## Key Results
- CBS was preferred over standard sampling in 45% of cases by human evaluators
- The self-evaluation step selected different candidates than DBS in 71% of cases
- Results show significant margin over baseline for creative output quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diverse Beam Search generates more diverse candidate solutions by dividing the beam budget into groups with diversity penalty
- Mechanism: DBS splits beam budget B into G groups, selecting best B/G solutions per group from B/G · |V| candidates, optimizing sequence likelihood plus dissimilarity term (e.g., Hamming diversity)
- Core assumption: Diversity penalty encourages exploration of distinct solution paths while maintaining quality via likelihood optimization
- Evidence anchors: Abstract mentions DBS generates diverse solutions; section describes objective with sequence likelihood and dissimilarity terms; no corpus neighbor discusses DBS mechanics

### Mechanism 2
- Claim: LLM-as-a-Judge selects best candidate from top-K DBS outputs by aggregating votes across rotated candidate positions
- Mechanism: Creates K evaluative prompts by rotating top-K candidates so each appears in every position; LLM votes for most creative candidate in each prompt; votes aggregated and most-voted candidate selected
- Core assumption: LLM can reliably judge creativity and positional bias can be mitigated by rotating candidates
- Evidence anchors: Abstract states LLM selects best candidate from top-K; section describes balanced position calibration with rotation scheme; no corpus neighbor discusses LLM-as-a-Judge rotation

### Mechanism 3
- Claim: Self-evaluation step improves over DBS alone by selecting different candidates in 71% of cases
- Mechanism: CBS uses LLM-as-a-Judge to override DBS's highest-scoring candidate; study found LLM-chosen candidate differed from DBS output in 71% of cases
- Core assumption: LLM's creativity judgment is better than pure likelihood/diversity scoring
- Evidence anchors: Abstract states self-evaluation effective with 71% difference rate; section reports 29% overlap between LLM and DBS choices (less than random 35.3%); no corpus neighbor discusses effectiveness comparison

## Foundational Learning

- Concept: Beam Search and its variants
  - Why needed here: Understanding how standard beam search differs from diverse beam search is crucial to grasp why CBS introduces diversity groups and penalties
  - Quick check question: How does Diverse Beam Search modify the objective function compared to standard beam search?

- Concept: LLM-as-a-Judge methodology
  - Why needed here: CBS relies on the LLM evaluating its own outputs; knowing how this works (including positional bias and calibration) is essential
  - Quick check question: What is positional bias in LLM-as-a-Judge, and how does balanced position calibration mitigate it?

- Concept: Creativity evaluation metrics
  - Why needed here: CBS aims to improve creativity; understanding how creativity is measured (human preference, diversity, novelty) helps interpret results
  - Quick check question: Why did the authors use human participants to evaluate CBS outputs instead of automated metrics?

## Architecture Onboarding

- Component map: Input prompt -> DBS Module (generates K candidates) -> LLM-as-a-Judge Module (evaluates via rotated prompts) -> Output (selected candidate)

- Critical path:
  1. Prompt → DBS generation (K candidates)
  2. DBS candidates → LLM-as-a-Judge prompts (K prompts with rotations)
  3. LLM responses → Vote aggregation
  4. Most-voted candidate → Output

- Design tradeoffs:
  - DBS diversity vs quality: Stronger diversity penalty increases variety but may reduce coherence
  - LLM evaluation cost: Self-evaluation adds inference time; using fewer candidates (K=4) keeps latency manageable
  - Positional bias mitigation: Rotating candidates increases prompt count but improves fairness

- Failure signatures:
  - All candidates too similar → CBS offers no advantage over standard sampling
  - LLM consistently picks same candidate as DBS → Self-evaluation adds no value
  - LLM votes evenly split → Final choice defaults to DBS score, may feel arbitrary

- First 3 experiments:
  1. Replace Hamming diversity with another metric (e.g., cosine similarity) and measure impact on candidate diversity
  2. Vary K (number of candidates evaluated) to see how many are needed for LLM selection to improve over DBS
  3. Test different LLM-as-a-Judge prompt structures to see if phrasing affects creativity judgment reliability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of Creative Beam Search outputs scale with the size of the LLM used?
- Basis in paper: [explicit] The paper used Llama 2 7B and mentions the method can be extended to other, potentially more powerful LLMs
- Why unresolved: The paper only tested with Llama 2 7B, so the impact of using larger models (e.g., Llama 2 70B) on output quality is unknown
- What evidence would resolve it: Comparative experiments testing CBS with multiple LLM sizes on the same creative tasks

### Open Question 2
- Question: What is the optimal beam budget B and number of groups G for balancing diversity and quality in Diverse Beam Search?
- Basis in paper: [explicit] The paper used B=8 divided into G=8 groups but notes this is a resource-constrained choice
- Why unresolved: The paper selected these values based on computational constraints rather than systematic optimization
- What evidence would resolve it: Grid search experiments testing various B and G combinations on creativity metrics

### Open Question 3
- Question: How does Creative Beam Search compare to fine-tuned LLMs specifically trained for creative generation?
- Basis in paper: [explicit] The paper mentions fine-tuning as an alternative approach but doesn't compare against it
- Why unresolved: The paper only compared CBS to standard sampling, not to specialized creative models
- What evidence would resolve it: Direct comparison of CBS outputs with outputs from LLMs fine-tuned on creative writing datasets

### Open Question 4
- Question: Does the self-evaluation step in CBS show positional bias like other LLM-as-a-Judge approaches?
- Basis in paper: [inferred] The paper mentions positional bias in LLM-as-a-Judge literature but only used balanced calibration to mitigate it
- Why unresolved: The paper didn't test whether CBS's self-evaluation step itself exhibits positional bias
- What evidence would resolve it: Testing CBS with different candidate orderings to measure consistency of self-evaluation

## Limitations
- Qualitative experiment with only 31 participants provides limited statistical power for generalizability
- Comparison lacks clear baseline specifications (temperature sampling, nucleus sampling, or other method unclear)
- No address of computational costs or latency implications of LLM-as-a-Judge step

## Confidence
- High confidence: The mechanism of Diverse Beam Search splitting beam budget into groups with diversity penalties is well-established in the literature and the paper correctly describes its operation
- Medium confidence: The effectiveness of LLM-as-a-Judge for creativity evaluation is supported by the 71% difference rate, but the limited participant pool and lack of statistical significance testing reduce confidence in generalizability
- Low confidence: The claim that CBS provides "more creative outputs than standard sampling techniques" is based on a small qualitative study without clear baseline specifications or quantitative creativity metrics

## Next Checks
1. Conduct a larger-scale user study (n>100) with proper statistical analysis to verify whether CBS consistently produces more creative outputs across different domains and prompt types

2. Benchmark computational latency and cost of CBS versus standard sampling, including both DBS generation time and LLM-as-a-Judge evaluation time, to assess practical deployment feasibility

3. Test CBS with different diversity penalties (Hamming, cosine similarity, coverage diversity) and LLM judge configurations to determine optimal parameter settings and robustness to hyperparameter choices