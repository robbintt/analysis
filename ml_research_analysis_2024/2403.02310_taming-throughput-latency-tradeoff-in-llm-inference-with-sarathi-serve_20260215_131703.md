---
ver: rpa2
title: Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve
arxiv_id: '2403.02310'
source_url: https://arxiv.org/abs/2403.02310
tags: []
core_contribution: This paper addresses the throughput-latency tradeoff in large language
  model (LLM) inference serving. The authors identify that batching improves throughput
  but can introduce generation stalls that harm latency.
---

# Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve

## Quick Facts
- **arXiv ID**: 2403.02310
- **Source URL**: https://arxiv.org/abs/2403.02310
- **Reference count**: 40
- **Primary result**: Sarathi-Serve achieves up to 2.6x higher serving capacity for Mistral-7B on single A100 GPU and up to 6.9x higher capacity for Falcon-180B on 8 A100 GPUs compared to vLLM

## Executive Summary
This paper addresses a fundamental challenge in LLM inference serving: the throughput-latency tradeoff inherent in batching mechanisms. The authors identify that while batching improves throughput, it can introduce generation stalls that significantly harm latency, particularly during the transition between prefill and decode phases. To solve this, they propose Sarathi-Serve, an innovative scheduler that uses chunked-prefills and stall-free batching to achieve both high throughput and low latency. The system demonstrates substantial improvements in serving capacity while maintaining competitive latency profiles.

## Method Summary
The core innovation lies in splitting prefill requests into smaller chunks and interleaving them with ongoing decode iterations, rather than pausing decodes to wait for prefill batches. This "stall-free batching" approach allows continuous processing of requests while still benefiting from batching efficiencies. The system introduces a lightweight request management layer that tracks partial prefill progress and coordinates the interleaving without disrupting ongoing decodes. The implementation is built on top of existing LLM serving frameworks but modifies the scheduling logic to enable this new batching pattern.

## Key Results
- Achieves up to 2.6x higher serving capacity for Mistral-7B on single A100 GPU compared to vLLM
- Demonstrates up to 6.9x higher capacity for Falcon-180B on 8 A100 GPUs
- Maintains low tail latency despite aggressive batching optimizations

## Why This Works (Mechanism)
The mechanism works by fundamentally rethinking the batching strategy during the prefill phase. Traditional approaches batch all prefill requests together and pause decoding until the batch completes, creating idle time. Sarathi-Serve instead breaks prefill into chunks, allowing partial prefills to be interleaved with decode iterations. This creates a pipeline where some requests can start decoding while others are still being prefilled, effectively hiding the prefill latency behind ongoing decode work. The scheduler maintains careful state tracking to ensure requests are assembled correctly despite being processed in fragments.

## Foundational Learning
- **Prefill-Decode Pipeline**: Understanding the two-phase LLM inference process is crucial - why needed to grasp the core problem; quick check: identify which phase dominates latency for your workload
- **Batching Efficiency vs Latency Tradeoff**: The fundamental tension between throughput and responsiveness; why needed to understand the problem space; quick check: measure how batch size affects both metrics
- **Chunked Processing**: Breaking large operations into smaller pieces for pipelining; why needed to understand the core innovation; quick check: verify chunk size affects neither correctness nor performance
- **Stall-Free Scheduling**: Maintaining continuous operation by avoiding idle periods; why needed to grasp the solution approach; quick check: monitor for any pipeline bubbles during execution
- **Request State Management**: Tracking partial progress across interleaved operations; why needed to ensure correctness; quick check: verify all requests complete with correct outputs

## Architecture Onboarding

**Component Map**: Request Queue -> Chunk Scheduler -> Prefill Workers -> Decode Workers -> Output Assembler

**Critical Path**: The most critical operations occur during the interleaving phase where the scheduler must coordinate between partial prefill results and ongoing decodes without causing stalls. The chunk size selection and scheduling policy directly impact this path's efficiency.

**Design Tradeoffs**: The approach trades increased memory usage (to track partial prefills) and scheduling complexity for improved throughput and latency. Smaller chunks reduce latency impact but increase scheduling overhead, while larger chunks improve batching efficiency but may reintroduce stalls.

**Failure Signatures**: Potential failures include request state corruption from improper interleaving, memory fragmentation from tracking many partial prefills, and scheduler overload from managing complex interleaving patterns. Monitoring for increased error rates or memory leaks would indicate these issues.

**3 First Experiments**:
1. Compare throughput and latency curves with varying chunk sizes to identify optimal configuration
2. Stress test with mixed request patterns to verify tail latency claims under realistic loads
3. Measure memory usage patterns over extended sessions to identify fragmentation risks

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluation limited to specific model sizes (Mistral-7B, Falcon-180B) and hardware (A100 GPUs)
- Potential memory fragmentation from chunked prefill approach over extended use
- Generalizability to other transformer architectures or fine-tuned variants remains uncertain

## Confidence
- **Throughput Improvements**: High confidence - specific quantitative results provided with clear methodology
- **Latency Preservation**: Medium confidence - claims are well-founded but real-world edge cases could affect tail latency
- **Stall-Free Characterization**: Medium confidence - mechanism appears sound but implementation details may introduce unexpected stalls

## Next Checks
1. Test Sarathi-Serve across broader range of model sizes (from small <1B to very large >100B parameters) to verify consistent performance scaling
2. Evaluate memory usage patterns and fragmentation over extended inference sessions to identify potential long-term resource management issues
3. Conduct stress testing with mixed workloads containing highly variable sequence lengths and generation patterns to assess tail latency behavior under non-ideal conditions