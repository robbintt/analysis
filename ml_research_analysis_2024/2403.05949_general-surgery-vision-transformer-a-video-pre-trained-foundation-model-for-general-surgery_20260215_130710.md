---
ver: rpa2
title: 'General surgery vision transformer: A video pre-trained foundation model for
  general surgery'
arxiv_id: '2403.05949'
source_url: https://arxiv.org/abs/2403.05949
tags:
- surgery
- surgical
- arxiv
- vision
- gsvit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We address the challenge of limited surgical video datasets and
  specialized models by introducing GSViT, a general surgery vision transformer pre-trained
  on 680 hours of surgical videos across 28 procedures. The model uses forward video
  prediction for pre-training, enabling real-time inference (12.1ms on RTX A5500)
  while achieving 86.3% accuracy on the Cholec80 phase annotation task.
---

# General surgery vision transformer: A video pre-trained foundation model for general surgery

## Quick Facts
- arXiv ID: 2403.05949
- Source URL: https://arxiv.org/abs/2403.05949
- Reference count: 5
- General surgery vision transformer (GSViT) achieves 86.3% accuracy on Cholec80 phase detection with real-time inference

## Executive Summary
GSViT addresses the challenge of limited surgical video datasets and specialized models by introducing a general surgery vision transformer pre-trained on 680 hours of surgical videos across 28 procedures. The model uses forward video prediction for pre-training, enabling real-time inference (12.1ms on RTX A5500) while achieving 86.3% accuracy on the Cholec80 phase annotation task. The authors release the GenSurgery dataset (70M frames) and fine-tuned models for 10 procedures, demonstrating that their approach outperforms single-frame predictors while maintaining parameter efficiency (13.7M tunable parameters).

## Method Summary
The method involves pre-training a vision transformer on surgical videos using forward video prediction, where the model predicts the next frame from the current input. This approach captures both spatial and temporal features of surgical procedures. The GSViT architecture uses a sandwich layout with cascaded group attention to reduce memory consumption while maintaining performance. After pre-training on the GenSurgery dataset, the model is fine-tuned on specific surgical procedures for tasks like phase detection. The asymmetric decoder used during pre-training is removed during fine-tuning, and an MLP classifier is added for task-specific outputs.

## Key Results
- Achieves 86.3% accuracy on Cholec80 phase detection task
- Real-time inference capability (12.1ms on RTX A5500)
- Maintains parameter efficiency with 13.7M tunable parameters
- Outperforms single-frame predictors while being faster than multi-frame approaches

## Why This Works (Mechanism)

### Mechanism 1
Forward video prediction pre-training develops spatial-temporal priors useful for surgical understanding. By predicting the next frame from current input, the model learns to encode both spatial features (anatomy) and temporal dynamics (tissue deformation, tool motion) in its latent representation. This works because surgical procedures exhibit predictable temporal patterns that can be learned through frame prediction.

### Mechanism 2
Cascaded Group Attention reduces memory consumption while maintaining performance by splitting features and processing attention heads in a cascaded manner. This reduces redundant computations while preserving discriminative power. The approach assumes attention heads exhibit redundancy that can be eliminated through feature splitting.

### Mechanism 3
Sandwich architecture with more MLPs than attention layers enables real-time inference by maintaining channel communication while reducing memory-intensive attention computations. Multiple MLP layers between single attention layers process channel-wise information effectively without full self-attention, prioritizing real-time performance over potential accuracy gains from more attention layers.

## Foundational Learning

- Vision Transformer fundamentals: Understanding how ViTs process images as sequences of patches and use self-attention is critical for grasping GSViT's architecture. Quick check: What is the purpose of the "class token" in ViT architecture?

- Transformer self-attention mechanism: CGA and sandwich architectures modify standard self-attention; understanding the base mechanism is essential. Quick check: How does multi-head self-attention differ from single-head attention in terms of representational capacity?

- Video prediction techniques: GSViT uses forward video prediction for pre-training; understanding this approach is key to grasping the training methodology. Quick check: What is the difference between next-frame prediction and masked video prediction approaches?

## Architecture Onboarding

- Component map: Input: 1 frame → Patch embedding → Class token + positional embeddings → Core: Sandwich architecture (N MLPs + 1 Attention layer repeated) with Cascaded Group Attention → Output: Asymmetric decoder for frame reconstruction during pre-training → Fine-tuning: Remove decoder, add MLP classifier for task-specific output

- Critical path: Pre-training: Video frames → Patch embedding → Transformer blocks → Asymmetric decoder → Loss (frame reconstruction). Fine-tuning: Video frames → Patch embedding → Transformer blocks → MLP classifier → Loss (task-specific)

- Design tradeoffs: Speed vs. accuracy: Sandwich architecture prioritizes real-time inference over potential accuracy gains from more attention layers. Memory vs. performance: CGA reduces memory usage but may limit some attention capabilities. Single-frame vs. temporal modeling: GSViT uses single-frame input but learns temporal patterns through video prediction

- Failure signatures: Slow inference: Indicates sandwich architecture or CGA implementation issues. Poor surgical phase detection: May indicate insufficient pre-training data or inappropriate fine-tuning. Memory errors: Likely issues with CGA feature splitting or transformer block configuration

- First 3 experiments: 1) Verify pre-training: Run forward video prediction on a small subset of surgical videos and visualize predicted vs. actual frames. 2) Test fine-tuning: Fine-tune on Cholec80 with frozen backbone vs. trainable backbone to identify optimal transfer learning strategy. 3) Benchmark inference: Measure throughput on different hardware (CPU vs. GPU) to verify real-time capability claims

## Open Questions the Paper Calls Out

- Can temporal and spatial mask reconstruction techniques improve GSViT's performance beyond next-frame prediction during pre-training? The paper states "Future work could expand the pre-training by incorporating temporal and spatial mask reconstruction" as a potential direction.

- How would incorporating longer video sequences affect GSViT's surgical phase detection performance compared to the single-frame approach? The authors note their model "recognizes phase by only looking at the current frame, whereas other approaches such as TeCNO [Cze+20] take in the entire video history for a phase classification" and achieved 86.3% accuracy.

- Can GSViT's foundation model architecture be effectively extended to incorporate multiple medical imaging modalities (MRI, CT, endoscopy) while maintaining surgical video performance? The authors mention "Our dataset could be used for building a more general-purpose medical foundation model that includes surgery as well as additional modalities such as MRI and CT images."

## Limitations

- Data Domain Specificity: While GSViT is trained on 680 hours across 28 procedures, generalizability to rare surgical procedures or completely different surgical domains remains untested.

- Temporal Modeling Constraints: The model uses single-frame input during both pre-training and fine-tuning, potentially missing longer-range temporal dependencies beyond the immediate next frame.

- Hardware Dependency: The real-time inference claim (12.1ms on RTX A5500) is tightly coupled to specific hardware and may not maintain across different configurations.

## Confidence

**High Confidence Claims:**
- The GSViT architecture with sandwich layout and cascaded group attention is technically sound
- Pre-training on 680 hours of surgical video data is feasible and implemented
- The model achieves 86.3% accuracy on Cholec80 phase detection task

**Medium Confidence Claims:**
- The model's performance generalizes to procedures beyond those in the training set
- Forward video prediction effectively captures necessary spatial-temporal priors for surgical understanding
- Real-time inference capability maintains across different hardware configurations

**Low Confidence Claims:**
- GSViT functions as a true "foundation model" for all general surgery applications
- The parameter efficiency (13.7M tunable parameters) represents optimal balance for all surgical tasks
- The model's performance scales linearly with additional training data

## Next Checks

1. Cross-Procedure Robustness Test: Evaluate GSViT on surgical procedures not included in the GenSurgery training set to quantify generalization capabilities and identify failure modes when encountering novel surgical contexts.

2. Temporal Dependency Analysis: Systematically test the model's performance on tasks requiring different temporal horizons (e.g., 1-second vs. 5-second predictions) to determine the effective temporal range of the learned representations.

3. Hardware Portability Benchmark: Measure inference latency and throughput across multiple hardware configurations (different GPU generations, CPU inference) to validate real-time capability claims under diverse clinical deployment scenarios.