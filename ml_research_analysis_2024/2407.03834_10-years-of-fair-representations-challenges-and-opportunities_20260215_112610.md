---
ver: rpa2
title: '10 Years of Fair Representations: Challenges and Opportunities'
arxiv_id: '2407.03834'
source_url: https://arxiv.org/abs/2407.03834
tags: []
core_contribution: This paper revisits the first 10 years of Fair Representation Learning
  (FRL) by combining theoretical analysis with extensive empirical evaluation. The
  authors leverage recent theoretical results in deep learning to show that deterministic
  neural networks with injective activations cannot remove sensitive information from
  representations.
---

# 10 Years of Fair Representations: Challenges and Opportunities

## Quick Facts
- arXiv ID: 2407.03834
- Source URL: https://arxiv.org/abs/2407.03834
- Reference count: 40
- Deterministic neural networks with injective activations cannot remove sensitive information from representations

## Executive Summary
This paper provides a comprehensive 10-year retrospective on Fair Representation Learning (FRL), combining theoretical analysis with massive empirical evaluation. The authors demonstrate that deterministic neural networks with injective activations cannot remove sensitive information from learned representations, regardless of fairness training objectives. Through 335,000 model fits across 6 datasets and 8 FRL methods, they show that while stochastic models can achieve sensitive attribute invariance at high fairness tradeoffs, deterministic models consistently fail to remove sensitive information despite appearing fair under traditional allocation metrics.

## Method Summary
The authors conduct a massive empirical study using their EvalFRL framework, which employs AutoML to adversarially mine sensitive information from learned representations. They evaluate 8 FRL methods (BinaryMI, DetBinaryMI, DebiasClassifier, NVP, VFAE, ICVAE, LFR, DeepDomainConfusion) across 6 datasets (Adult, Banks, German, Folktables AK/HI, Compas) with 225,000 model fits and 110,000 AutoML fits. The framework uses a γ tradeoff parameter to control the balance between fairness and utility, measuring performance through fairness allocation metrics (AUC-Discrimination, Statistical Parity Difference, Delta, rND) and AutoML accuracy/AUC for invariant representations.

## Key Results
- Deterministic models with injective activations (tanh, sigmoid) cannot remove sensitive information from representations
- Stochastic models (BinaryMI, DetBinaryMI, VFAE, ICVAE) can achieve sensitive attribute invariance at high fairness tradeoffs
- Traditional fairness allocation metrics can appear favorable even when representations retain sensitive information
- The EvalFRL framework reveals significant limitations in current FRL evaluation practices

## Why This Works (Mechanism)
The theoretical foundation rests on recent results in deep learning showing that deterministic neural networks with injective activations create bijective mappings between inputs and hidden representations. Since these mappings are invertible, all information from the input—including sensitive attributes—must be preserved in the representations. This mathematical constraint means that fairness objectives applied after the representation layer cannot remove sensitive information that was already encoded.

## Foundational Learning
- **Fair Representation Learning (FRL)**: The task of learning representations that are both predictive and fair with respect to sensitive attributes. Needed to understand the problem space and why traditional fairness approaches may fail.
- **Injective Activations**: Activation functions like tanh and sigmoid that are bijective mappings. Quick check: Verify that tanh and sigmoid are indeed bijective on their domains.
- **AutoML for Fairness Evaluation**: Using automated machine learning to mine sensitive information from representations. Quick check: Confirm that AutoML can effectively recover sensitive attributes from representations.
- **Fairness Frames**: The two main perspectives on fairness—fair allocation (equal outcomes) and invariant representations (removing sensitive information). Quick check: Ensure understanding of how these frames differ in evaluation criteria.
- **γ Tradeoff Parameter**: A hyperparameter controlling the balance between fairness and utility in FRL methods. Quick check: Verify that γ=0 means no fairness constraint and γ=1 means maximum fairness.
- **Statistical Parity Difference**: A metric measuring the difference in positive outcome rates between groups. Quick check: Confirm that SPD=0 indicates perfect fairness under this metric.

## Architecture Onboarding

**Component Map**: Data -> FRL Model -> Representation -> AutoML Predictor -> Fairness Metrics -> Utility Metrics

**Critical Path**: The most critical path is Data -> FRL Model -> Representation, as the quality and fairness properties of representations directly determine downstream performance.

**Design Tradeoffs**: The paper trades massive computational resources (335,000 model fits) for comprehensive empirical coverage, sacrificing interpretability for breadth. The use of AutoML provides strong adversarial testing but introduces black-box elements.

**Failure Signatures**: 
- Deterministic models failing to achieve low AutoML accuracy despite favorable allocation metrics
- Stochastic models showing high variance in fairness performance across different γ values
- AutoML baselines achieving near-perfect sensitive attribute prediction from representations

**First Experiments**:
1. Run a single FRL method (e.g., VFAE) on one dataset (e.g., Adult) to verify the experimental pipeline works
2. Compare AutoML prediction accuracy on representations from a fair vs. unfair model to confirm the framework's sensitivity
3. Test the theoretical claim by training a deterministic model with tanh activation and measuring AutoML performance on its representations

## Open Questions the Paper Calls Out

### Open Question 1
Can stochastic models achieve perfect invariance to sensitive attributes while maintaining reasonable predictive performance? The paper shows stochastic models perform better than deterministic ones, but it's unclear if perfect invariance is achievable without sacrificing predictive performance entirely. Experiments showing performance across a wide range of γ values, including cases where γ approaches 1, would clarify this trade-off.

### Open Question 2
How does the choice of activation function affect the ability of deterministic models to remove sensitive information? While the paper mentions that ReLU activations might help deterministic models, it doesn't provide experimental evidence. Experiments comparing deterministic models with different activation functions on the same datasets would clarify the impact of activation choice.

### Open Question 3
Are there alternative fairness metrics or evaluation strategies that could better capture the limitations of deterministic FRL methods? The paper's evaluation strategy is comprehensive but limited to specific metrics and frames. Exploring alternative approaches based on causal inference or multi-dimensional fairness concepts would provide a more complete picture of FRL limitations.

## Limitations
- Focus on binary sensitive attributes limits generalizability to continuous or multi-category attributes
- Heavy dependence on AutoML for sensitive attribute prediction introduces black-box elements
- Limited to tabular datasets, leaving questions about performance on images or text data
- Massive computational requirements make reproduction challenging

## Confidence
- Theoretical claims about injective activations: High confidence based on established mathematical proofs
- Empirical claims about specific FRL methods' performance: Medium confidence due to AutoML framework complexity
- Massive scale of experiments (335,000 fits) suggests robustness, but computational environment dependencies introduce uncertainty

## Next Checks
1. Replicate core findings using a simpler, manually-tuned logistic regression baseline for sensitive attribute prediction instead of AutoML to isolate framework effects
2. Test theoretical claims with additional activation functions beyond those studied, particularly modern sparse activation functions
3. Evaluate a subset of FRL methods on at least one non-tabular dataset (e.g., facial recognition) to assess generalizability of findings