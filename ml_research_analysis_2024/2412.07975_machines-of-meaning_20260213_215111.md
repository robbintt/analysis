---
ver: rpa2
title: Machines of Meaning
arxiv_id: '2412.07975'
source_url: https://arxiv.org/abs/2412.07975
tags:
- language
- meaning
- symbols
- words
- what
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper discusses the challenges in building machines of meaning,
  focusing on how computational models can acquire meaningful semantics from natural
  language. It clarifies the notion of symbols and meaning, highlighting the need
  for detachment from anthropocentrism in studying machine behavior.
---

# Machines of Meaning

## Quick Facts
- arXiv ID: 2412.07975
- Source URL: https://arxiv.org/abs/2412.07975
- Authors: Davide Nunes; Luis Antunes
- Reference count: 40
- Primary result: Current language models are limited by fixed lexicons and inability to adapt to open linguistic domains; proposes solutions like compressive encoding and likelihood-free estimation.

## Executive Summary
This paper addresses the fundamental challenge of building machines that can acquire meaningful semantics from natural language. It argues that current large language models are constrained by their fixed vocabularies and inability to adapt to dynamic linguistic environments. The authors propose a conceptual framework for "machines of meaning" that can process any symbols and adapt to changing contexts through techniques like compressive encoding and likelihood-free estimation.

## Method Summary
The paper does not describe a specific computational method or training procedure. Instead, it presents a conceptual and philosophical framework for understanding meaning in computational systems. The authors discuss the limitations of current language models and propose potential solutions including compressive encoding using dimensionality reduction techniques and likelihood-free estimation methods to overcome the prediction frame problem.

## Key Results
- Current language models are fundamentally limited by fixed lexicons and inability to adapt to open linguistic domains
- Meaning is characterized as a learned connection between symbols and referents in context, not inherent content
- Proposed solutions include compressive encoding and likelihood-free estimation to enable true machines of meaning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Machines of Meaning require detachment from anthropocentrism to properly measure AI capabilities.
- Mechanism: By shifting focus away from human-centric benchmarks and understanding, researchers can better evaluate AI systems based on their own operational semantics and goals rather than human expectations.
- Core assumption: Current AI evaluation methods are biased by human expectations and fail to capture the unique ways AI systems process meaning.
- Evidence anchors:
  - [abstract]: "The pressing need to analyse AI risks and ethics requires a proper measurement of its capabilities which cannot be productively studied and explained while using ambiguous language."
  - [section]: "Our view is that for a MoM, symbols themselves do not carry any intrinsic meaning, the meaning of symbols is always learned and thus part of each symbol user experience."
  - [corpus]: Weak - no direct corpus evidence supporting this specific mechanism
- Break condition: If AI systems are evaluated solely through human-centric benchmarks, the unique capabilities and limitations of these systems will remain obscured.

### Mechanism 2
- Claim: Current large language models are limited by fixed lexicons and inability to adapt to open linguistic domains.
- Mechanism: Language models trained on static vocabularies cannot process novel symbols or adapt to emerging linguistic patterns, creating a fundamental barrier to true machines of meaning.
- Core assumption: The paper identifies this as a "prediction frame problem" where models must output probability distributions over pre-defined vocabularies.
- Evidence anchors:
  - [abstract]: "They propose solutions like compressive encoding and likelihood-free estimation to enable true machines of meaning that can process any symbols and adapt to changing contexts."
  - [section]: "The restriction on the number of symbols that can be processed by language models...could be tackled with compressive encoding using for instance dimensionality reduction techniques like random projections."
  - [corpus]: Weak - no direct corpus evidence supporting this specific mechanism
- Break condition: If language models continue to rely on fixed vocabularies and probability distributions over known symbols, they cannot achieve true meaning processing.

### Mechanism 3
- Claim: Meaning is a learned connection between symbols and referents in context, not inherent content.
- Mechanism: AI systems can develop meaningful representations through interaction with their environment and feedback about symbol use, rather than through pre-programmed semantics.
- Core assumption: The paper draws on Wittgenstein's philosophy that "meaning is use" and extends this to computational systems.
- Evidence anchors:
  - [abstract]: "We characterize 'meaning' in a computational setting, while highlighting the need for detachment from anthropocentrism in the study of the behaviour of machines of meaning."
  - [section]: "Meaning is the learned connection between a symbol and its referent in a context. A context is a set of features, or consistent behaviour."
  - [corpus]: Weak - no direct corpus evidence supporting this specific mechanism
- Break condition: If AI systems cannot learn symbol-referent relationships through interaction and feedback, they cannot develop true meaning.

## Foundational Learning

- Concept: Symbol grounding and the symbol grounding problem
  - Why needed here: Understanding how symbols connect to real-world referents is fundamental to building machines that can process meaning
  - Quick check question: Can you explain why a Chinese Room could produce correct responses without understanding Chinese?

- Concept: Distributional hypothesis and statistical semantics
  - Why needed here: Modern language models rely on statistical patterns in language use to build representations
  - Quick check question: How does the distributional hypothesis relate to the idea that "a word is characterized by the company it keeps"?

- Concept: Neural network architectures for language modeling
  - Why needed here: Understanding current limitations requires knowledge of how language models process symbols and output predictions
  - Quick check question: What is the fundamental limitation of neural language models regarding vocabulary size and symbol processing?

## Architecture Onboarding

- Component map: Symbol input → Encoding transformation → Context analysis → Goal alignment → Output generation → Feedback integration → Vocabulary update
- Critical path: Symbol input → Encoding transformation → Context analysis → Goal alignment → Output generation → Feedback integration → Vocabulary update
- Design tradeoffs: Fixed vs. dynamic vocabularies, computational efficiency vs. expressiveness, centralized vs. distributed representation learning, deterministic vs. probabilistic output generation
- Failure signatures: Inability to process novel symbols, catastrophic forgetting when learning new concepts, output distributions that don't reflect context, failure to adapt to changing linguistic patterns
- First 3 experiments:
  1. Test dynamic vocabulary expansion with incremental symbol learning using compressive encoding
  2. Evaluate context-sensitive symbol grounding through goal-oriented task completion
  3. Measure adaptation to emerging linguistic patterns in open domains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can computational models truly ground symbols without prior designer input, and what mechanisms would enable such autonomous grounding?
- Basis in paper: [explicit] The paper discusses the grounding problem and how current models rely on human-designed semantics, contrasting this with the ideal of autonomous grounding in experience.
- Why unresolved: Current models like LLMs use predefined lexicons and lack mechanisms for adapting to new symbols without retraining, as highlighted in the "prediction frame problem" section.
- What evidence would resolve it: Demonstration of a model that can incrementally learn and ground new symbols from experience without catastrophic forgetting, using techniques like compressive encoding and likelihood-free estimation.

### Open Question 2
- Question: How can we develop language models that can process open and dynamic linguistic domains without being constrained by fixed vocabularies?
- Basis in paper: [explicit] The paper identifies the limitation of current models tied to predefined vocabularies and proposes solutions like compressive encoding and likelihood-free estimation.
- Why unresolved: Existing models like LLMs use techniques like byte-pair encoding but still cannot adapt to unseen words or dynamic domains without retraining or massive computational costs.
- What evidence would resolve it: A model that can process and learn from any symbol, including those from emerging or artificial languages, without requiring a fixed lexicon or extensive retraining.

### Open Question 3
- Question: What is the role of embodiment in the grounding of symbols for artificial agents, and can disembodied agents achieve meaningful grounding?
- Basis in paper: [explicit] The paper discusses how human grounding is tied to embodiment (e.g., hunger) but suggests artificial agents could ground symbols behaviorally, even if disembodied.
- Why unresolved: The paper raises the question of whether first-hand experience is necessary for grounding, but does not provide a definitive answer on the sufficiency of behavioral grounding for artificial agents.
- What evidence would resolve it: Empirical studies showing that a disembodied agent can ground symbols meaningfully through behavioral interactions, achieving comparable results to embodied agents in communication tasks.

## Limitations
- The paper presents a conceptual framework rather than a concrete computational system, making independent assessment difficult
- Proposed solutions (compressive encoding, likelihood-free estimation) are mentioned but not rigorously specified or implemented
- The philosophical arguments rely heavily on theoretical foundations without empirical validation or experimental results

## Confidence
- High confidence in identifying current language model limitations (fixed lexicons, prediction frame problem)
- Medium confidence in proposed conceptual solutions, as they are not fully elaborated
- Low confidence in practical implementation feasibility without further specification

## Next Checks
1. Implement a prototype compressive encoding system to test dynamic vocabulary expansion capabilities on open-domain text streams
2. Design experiments comparing symbol grounding performance between fixed-vocabulary and dynamic-vocabulary language models on novel symbol introduction tasks
3. Evaluate the computational efficiency and representational capacity tradeoffs of likelihood-free estimation approaches versus traditional probability distribution methods in language modeling