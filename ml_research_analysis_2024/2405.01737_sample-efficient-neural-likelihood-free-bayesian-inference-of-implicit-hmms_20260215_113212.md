---
ver: rpa2
title: Sample-efficient neural likelihood-free Bayesian inference of implicit HMMs
arxiv_id: '2405.01737'
source_url: https://arxiv.org/abs/2405.01737
tags: []
core_contribution: This paper introduces a novel neural density estimation approach
  for sample-efficient Bayesian inference of implicit Hidden Markov Models (HMMs).
  The key idea is to learn the intractable posterior distribution of the hidden states
  using an autoregressive flow, exploiting the Markov property to decompose the posterior
  into homogeneous factors.
---

# Sample-efficient neural likelihood-free Bayesian inference of implicit HMMs

## Quick Facts
- arXiv ID: 2405.01737
- Source URL: https://arxiv.org/abs/2405.01737
- Reference count: 27
- Key outcome: Introduces neural density estimation approach for sample-efficient Bayesian inference of implicit HMMs using autoregressive flows

## Executive Summary
This paper presents a novel approach to perform Bayesian inference in implicit Hidden Markov Models (HMMs) where the likelihood function is intractable. The method combines neural density estimation with autoregressive flows to learn the posterior distribution of hidden states, leveraging the Markov property to decompose the posterior into manageable factors. When integrated with existing neural likelihood-free inference methods for parameter estimation, this approach achieves comparable or superior performance to Sequential Monte Carlo (SMC) methods while requiring significantly fewer simulations.

The key innovation lies in exploiting the Markov structure of HMMs to recursively sample the full path of hidden states, enabling efficient Bayesian inference without explicit likelihood calculations. This sample-efficient approach is particularly valuable for complex models where traditional methods become computationally prohibitive due to the need for extensive simulation.

## Method Summary
The proposed method addresses Bayesian inference in implicit HMMs by learning the intractable posterior distribution of hidden states using an autoregressive flow architecture. The approach decomposes the posterior into homogeneous factors based on the Markov property, allowing for recursive sampling of the complete hidden state trajectory. This neural density estimator is then combined with off-the-shelf neural likelihood-free inference methods for parameter estimation, creating a unified framework for full Bayesian inference. The method is evaluated on nonlinear state-space models and implicit biological HMMs, demonstrating significant improvements in sample efficiency compared to classical approaches like ABC-SMC while maintaining or improving estimation accuracy.

## Key Results
- Achieves hidden state estimates comparable to or better than SMC with significantly fewer simulations
- Provides accurate posterior predictive distributions for goodness-of-fit assessment
- Demonstrates superior sample efficiency compared to naive neural likelihood-free inference that ignores hidden state estimation
- Successfully handles nonlinear state-space models and implicit biological HMMs

## Why This Works (Mechanism)
The method works by exploiting the Markov property of HMMs to decompose the posterior distribution into a product of conditional distributions. The autoregressive flow architecture learns to approximate these conditional distributions, enabling recursive sampling of the full hidden state path. This decomposition transforms an otherwise intractable joint posterior into manageable sequential inference problems. By integrating this hidden state estimation with neural likelihood-free inference for parameters, the approach maintains the sample efficiency benefits of neural methods while ensuring accurate reconstruction of the latent dynamics that are crucial for many applications.

## Foundational Learning

1. **Hidden Markov Models (HMMs)** - Why needed: Core problem domain where likelihood functions are often intractable. Quick check: Can identify states, observations, and transition/emission structures.

2. **Autoregressive Flows** - Why needed: Enable tractable density estimation through sequential factorization. Quick check: Understand how flows transform simple distributions into complex posteriors.

3. **Neural Likelihood-Free Inference** - Why needed: Framework for Bayesian inference without explicit likelihood calculations. Quick check: Can distinguish between different neural density estimation approaches (eBNN, SNL, etc.).

4. **Sequential Monte Carlo (SMC)** - Why needed: Benchmark method for comparison and understanding of particle-based inference. Quick check: Can explain how particles are weighted and resampled in SMC.

5. **Bayesian Inference in Implicit Models** - Why needed: Context for why traditional MCMC methods are computationally prohibitive. Quick check: Understand the relationship between simulator-based models and intractable likelihoods.

## Architecture Onboarding

**Component Map:**
Observation data → Neural likelihood-free inference (parameters) → Autoregressive flow (hidden states) → Posterior samples

**Critical Path:**
The essential inference pipeline flows from observed data through parameter estimation to hidden state reconstruction. The autoregressive flow is the critical component that enables sample-efficient hidden state inference by exploiting the Markov structure.

**Design Tradeoffs:**
- Accuracy vs. computational efficiency: More complex flow architectures could improve accuracy but increase computation
- Sample size vs. convergence: Fewer simulations improve efficiency but may require more sophisticated network architectures
- Model complexity vs. tractability: More complex HMMs may require deeper networks but risk overfitting with limited samples

**Failure Signatures:**
- Poor hidden state reconstruction when Markov assumption is violated
- Degraded performance on models with long-range dependencies
- Instability in parameter estimation when observation noise is high

**First 3 Experiments to Run:**
1. Verify hidden state reconstruction accuracy on a simple linear Gaussian HMM with known ground truth
2. Compare sample efficiency against ABC-SMC on a nonlinear benchmark model
3. Test robustness to observation noise levels across different flow architectures

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes strict Markov property which may not hold for all HMM variants
- Performance on extremely high-dimensional hidden state spaces remains unexplored
- Integration with off-the-shelf neural likelihood-free inference methods may vary in effectiveness

## Confidence

| Claim | Confidence |
|-------|------------|
| Computational efficiency gains | High |
| Sample efficiency improvements | High |
| Generalizability across diverse HMM structures | Medium |
| Seamless integration with any off-the-shelf neural likelihood-free inference method | Medium |

## Next Checks

1. Evaluate the method's performance on HMMs with explicit non-Markovian dependencies to test the robustness of the autoregressive flow decomposition.

2. Conduct scalability tests on high-dimensional hidden state spaces (e.g., >100 dimensions) to identify potential computational bottlenecks or accuracy degradation.

3. Compare the method's performance against specialized particle filtering techniques in scenarios with severe observation noise or infrequent observations to assess relative strengths and weaknesses.