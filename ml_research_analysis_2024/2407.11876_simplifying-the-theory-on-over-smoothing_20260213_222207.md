---
ver: rpa2
title: Simplifying the Theory on Over-Smoothing
arxiv_id: '2407.11876'
source_url: https://arxiv.org/abs/2407.11876
tags:
- graph
- over-smoothing
- neural
- rank
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of over-smoothing in graph neural
  networks (GNNs), which causes node representations to become increasingly similar
  as the number of convolutional layers increases. The author proposes a novel perspective
  by connecting over-smoothing to the classical power iteration method, greatly simplifying
  the existing theory on over-smoothing.
---

# Simplifying the Theory on Over-Smoothing

## Quick Facts
- arXiv ID: 2407.11876
- Source URL: https://arxiv.org/abs/2407.11876
- Reference count: 26
- Primary result: Over-smoothing in GNNs is a special case of rank collapse via Kronecker product eigenvectors in power iteration

## Executive Summary
This paper addresses the fundamental problem of over-smoothing in graph neural networks (GNNs), where node representations become increasingly similar as network depth increases. The author proposes a novel theoretical framework that connects over-smoothing to classical power iteration methods, significantly simplifying the existing theory. By viewing graph convolutions as a special case of power iteration, the paper introduces a comprehensive definition of rank collapse as a generalized form of over-smoothing and proposes a rank-one distance metric to quantify it. The empirical evaluation demonstrates that more GNN models suffer from rank collapse than previously known, and identifies three general directions to prevent it.

## Method Summary
The paper connects graph convolutions to power iteration through Kronecker product spectral properties. The core method involves applying graph convolutions X(k+1) = AX(k)W(k) and analyzing the resulting representations using the rank-one distance metric. The theoretical framework shows that when the dominant eigenvector of the Kronecker product W ⊗ A is a rank-one matrix, all node representations converge to multiples of a single vector. The experimental setup implements 14 commonly used GNN methods on the KarateClub dataset, tracking rank-one distance and Dirichlet energy over 96 iterations with random feature initialization, repeated across 50 random seeds.

## Key Results
- Rank collapse affects more GNN models than previously identified through traditional over-smoothing metrics
- Normalization layers, depth limiting mechanisms, and complex non-linear transformations effectively prevent rank collapse
- The rank-one distance metric provides a stable and computationally efficient way to quantify rank collapse across different GNN architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph convolutions amplify the same signal across all feature columns when the dominant eigenvector is a Kronecker product.
- Mechanism: The Kronecker product of the dominant eigenvectors from the aggregation matrix and transformation matrix causes rank collapse by converging all columns to multiples of a single vector.
- Core assumption: |λS₁| > |λS₂| for the Kronecker product matrix S = W ⊗ A, ensuring power iteration convergence.
- Evidence anchors:
  - [abstract] "the author introduces a comprehensive definition of rank collapse as a generalized form of over-smoothing"
  - [section 4] "The Kronecker product has a key spectral property affecting power iteration: All eigenvectors vSᵢⱼ = v(Wᵀ)ᵢ ⊗ vAⱼ of W ⊗ A are Kronecker products"
  - [corpus] Weak - neighbors don't directly discuss Kronecker products
- Break condition: When the aggregation or transformation matrices don't share dominant eigenvectors, or when multiple distinct signals are amplified.

### Mechanism 2
- Claim: Normalization is required for over-smoothing to occur as a special case of rank collapse.
- Mechanism: Without normalization, the norm of representations can go to zero while not converging to a constant state, leading to misinterpretation of over-smoothing.
- Core assumption: The theory requires normalized representations to properly identify over-smoothing as rank collapse to constant states.
- Evidence anchors:
  - [abstract] "Based on the theory, we provide a novel comprehensive definition of rank collapse as a generalized form of over-smoothing"
  - [section 2] "When not considering the normalized state, the norm of X(k) going to zero can be wrongly interpreted as a convergence to a constant state"
  - [corpus] Weak - neighbors don't discuss normalization requirements
- Break condition: When working with unnormalized representations or when the focus is on rank collapse rather than specific constant-state convergence.

### Mechanism 3
- Claim: Rank-one distance metric captures rank collapse by measuring convergence to a rank-one approximation.
- Mechanism: ROD computes the distance between normalized representations and their closest rank-one approximation using dominant singular vectors.
- Core assumption: The closest rank-one approximation provides a stable measure of how close a matrix is to rank collapse.
- Evidence anchors:
  - [abstract] "we introduce the rank-one distance as a corresponding metric"
  - [section 4] "The singular vectors corresponding to the largest singular value give the closest rank-one approximation of a given matrix"
  - [corpus] Weak - neighbors don't discuss rank-one distance metric
- Break condition: When matrices are already rank-one or when numerical instability makes singular value computation unreliable.

## Foundational Learning

- Concept: Kronecker product spectral properties
  - Why needed here: The entire theory connects over-smoothing to power iteration through Kronecker product eigenvectors
  - Quick check question: What is the relationship between the eigenvectors of A ⊗ B and the eigenvectors of A and B individually?

- Concept: Power iteration convergence
  - Why needed here: The paper shows graph convolutions are a special case of power iteration
  - Quick check question: Under what conditions does power iteration converge to the dominant eigenvector?

- Concept: Rank collapse vs over-smoothing distinction
  - Why needed here: The paper provides a generalized definition that encompasses both phenomena
  - Quick check question: How does the definition of rank collapse extend beyond traditional over-smoothing?

## Architecture Onboarding

- Component map:
  Graph convolution layer -> Normalization layer -> Rank-one distance calculator -> Monitoring system

- Critical path:
  1. Initialize node features X(0)
  2. Apply graph convolution X(k+1) = AX(k)W(k)
  3. Normalize representations
  4. Compute rank-one distance
  5. Monitor for convergence to zero

- Design tradeoffs:
  - Depth vs over-smoothing: Deeper networks suffer more from rank collapse
  - Normalization choice: Different normalizations affect which eigenvectors dominate
  - Feature transformation complexity: More complex transformations can prevent rank collapse

- Failure signatures:
  - Rank-one distance converging to zero indicates rank collapse
  - Dirichlet energy converging to zero (for symmetric normalization) indicates over-smoothing
  - Feature magnitudes exploding or diminishing indicates numerical instability

- First 3 experiments:
  1. Implement GCN with symmetric normalization and measure rank-one distance vs depth
  2. Compare GCNII with and without parameter scaling on KarateClub dataset
  3. Test GAT with different attention mechanisms for rank collapse resistance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the power iteration perspective on over-smoothing apply to directed graphs with asymmetric adjacency matrices?
- Basis in paper: [explicit] The paper focuses on undirected graphs and symmetric adjacency matrices, particularly in the context of over-smoothing definitions and experimental validation on the KarateClub dataset.
- Why unresolved: The theoretical framework presented relies on properties of symmetric matrices and their eigenvectors, which may not hold for asymmetric matrices common in directed graphs.
- What evidence would resolve it: A theoretical extension of the power iteration framework to asymmetric matrices, along with empirical validation on directed graph datasets.

### Open Question 2
- Question: How does the rank-one distance metric behave for extremely large graphs where computing the full rank-one approximation becomes computationally prohibitive?
- Basis in paper: [explicit] The paper introduces the rank-one distance metric (Definition 4.5) as a computationally efficient alternative to singular value decomposition for quantifying rank collapse.
- Why unresolved: While the metric is designed to be efficient, the paper does not explore its scalability to large-scale graphs where even computing row and column norms may become challenging.
- What evidence would resolve it: An analysis of the computational complexity of the rank-one distance metric on large graphs, along with potential approximations or sampling-based approaches.

### Open Question 3
- Question: Can the three proposed directions for preventing rank collapse (normalization, depth limiting, non-linear transformations) be combined in a principled way, or are there trade-offs between them?
- Basis in paper: [inferred] The paper identifies three general directions to prevent rank collapse but does not explore their interactions or potential conflicts.
- Why unresolved: The paper treats these directions as separate solutions without investigating whether they can be synergistically combined or if there are inherent limitations to their joint application.
- What evidence would resolve it: A systematic study of GNN architectures that combine multiple prevention strategies, measuring both effectiveness against rank collapse and any negative impacts on other performance metrics.

## Limitations
- Theoretical framework primarily applies to undirected graphs with symmetric adjacency matrices
- Empirical evaluation limited to a single small dataset (KarateClub) with only 34 nodes
- Rank-one distance metric may be sensitive to numerical instabilities in computing dominant singular vectors

## Confidence

- High: The theoretical connection between graph convolutions and power iteration via Kronecker products
- Medium: The rank collapse definition as a generalized form of over-smoothing
- Low: The practical effectiveness of the three proposed prevention strategies across diverse datasets

## Next Checks

1. Test the rank-one distance metric on synthetic graphs with controlled spectral properties to validate its sensitivity to different types of rank collapse
2. Evaluate the proposed prevention strategies (normalization, restart mechanisms, complex transformations) on multiple real-world datasets with varying graph sizes and densities
3. Extend the theory to include attention-based GNNs and measure whether the Kronecker product framework still applies when aggregation weights are learned rather than fixed by graph structure