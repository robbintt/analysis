---
ver: rpa2
title: Joint Optimization of Age of Information and Energy Consumption in NR-V2X System
  based on Deep Reinforcement Learning
arxiv_id: '2407.08458'
source_url: https://arxiv.org/abs/2407.08458
tags:
- vehicles
- ieee
- nr-v2x
- communication
- energy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a joint optimization approach for Age of Information
  (AoI) and energy consumption in NR-V2X vehicular networks using Non-Orthogonal Multiple
  Access (NOMA) and Deep Reinforcement Learning (DRL). The key idea is to employ NOMA's
  serial interference cancellation to mitigate resource collisions in NR-V2X Mode
  2, and use a Multi-Pass Deep Q-Network (MPDQN) to dynamically adjust resource reservation
  intervals (RRI) and transmission power for vehicles.
---

# Joint Optimization of Age of Information and Energy Consumption in NR-V2X System based on Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2407.08458
- Source URL: https://arxiv.org/abs/2407.08458
- Reference count: 40
- This paper proposes a joint optimization approach for Age of Information (AoI) and energy consumption in NR-V2X vehicular networks using Non-Orthogonal Multiple Access (NOMA) and Deep Reinforcement Learning (DRL).

## Executive Summary
This paper addresses the challenge of balancing timely information delivery (AoI) and energy efficiency in NR-V2X vehicular networks. The authors propose using NOMA's serial interference cancellation to reduce resource collisions in Mode 2, combined with a Multi-Pass Deep Q-Network (MPDQN) to dynamically optimize resource reservation intervals and transmission power. The approach aims to minimize a weighted sum of average AoI and energy consumption across all vehicles. Simulation results demonstrate significant improvements over baseline methods like genetic algorithms and random strategies, particularly in scenarios with varying vehicle densities and message sizes.

## Method Summary
The proposed method employs NOMA to mitigate resource collisions in NR-V2X Mode 2 by using serial interference cancellation. A Multi-Pass Deep Q-Network (MPDQN) with discrete-continuous action space (RRI and power) is trained to optimize the joint objective. The algorithm uses an experience replay buffer, Adam optimizer, and Ornstein-Uhlenbeck exploration noise. The state representation captures channel conditions, queue states, and interference information. The reward function is the negative weighted sum of average AoI and energy consumption. Training occurs over 1500 episodes with specific network parameters including 10 MHz bandwidth, 4 priority queues, and highway scenarios.

## Key Results
- MPDQN outperforms genetic algorithms and random strategies in minimizing average AoI and energy consumption
- Achieves better performance across various vehicular network scenarios with different vehicle counts (5-50) and message sizes
- Successfully balances the trade-off between timely information delivery and energy efficiency in NR-V2X Mode 2

## Why This Works (Mechanism)
The approach leverages NOMA's interference cancellation capability to reduce resource collisions, while the MPDQN algorithm dynamically adapts resource allocation and power control based on real-time network conditions. The discrete-continuous action space allows fine-grained control over both RRI and transmission power, enabling optimal trade-offs between AoI and energy consumption.

## Foundational Learning
- **NOMA (Non-Orthogonal Multiple Access)**: Enables multiple users to share the same frequency-time resources using different power levels and successive interference cancellation. Why needed: To reduce resource collisions in dense vehicular networks.
- **Deep Reinforcement Learning (DRL)**: Uses neural networks to approximate optimal policies in complex environments with high-dimensional state spaces. Why needed: To handle the dynamic and stochastic nature of vehicular networks.
- **Multi-Pass Deep Q-Network (MPDQN)**: An extension of DQN that handles both discrete and continuous action spaces. Why needed: To simultaneously optimize both discrete (RRI) and continuous (power) control parameters.
- **Age of Information (AoI)**: Measures the freshness of information by calculating the time elapsed since the last update was generated. Why needed: Critical metric for real-time applications in vehicular networks.
- **Resource Reservation Interval (RRI)**: The time interval between consecutive resource reservations by vehicles. Why needed: Controls the frequency of transmissions and directly impacts AoI and energy consumption.
- **Ornstein-Uhlenbeck Noise**: A stochastic process used for exploration in continuous action spaces that exhibits mean-reverting behavior. Why needed: To balance exploration and exploitation during DRL training.

## Architecture Onboarding

**Component Map**: Vehicle Sensors -> State Representation -> MPDQN -> Action Selection -> NOMA Transmission -> Channel Feedback -> Reward Calculation -> Experience Replay

**Critical Path**: The most critical path is the end-to-end loop from vehicle state sensing through DRL decision-making to transmission execution and reward feedback. The MPDQN must quickly process state information to make timely decisions that affect AoI.

**Design Tradeoffs**: The paper trades off implementation simplicity for potential performance by using a single hidden layer neural network and fixed λ weights. While this simplifies training, it may limit the ability to capture complex state-action relationships and adapt to varying network conditions.

**Failure Signatures**: 
- High variance in AoI across vehicles suggests poor state representation or insufficient exploration
- Training instability or divergence indicates problems with learning rate or noise parameters
- Poor performance in dense scenarios (50+ vehicles) suggests scalability issues with the state representation approach

**First Experiments**:
1. Verify basic NOMA functionality by simulating interference cancellation with two vehicles at different power levels
2. Test MPDQN with a simplified state space (only RSRP) to validate the core learning algorithm
3. Compare performance with different λ values (0.05, 0.1, 0.2) to understand sensitivity to the AoI-energy trade-off

## Open Questions the Paper Calls Out
None

## Limitations
- Neural network architecture details are not specified, which could significantly impact performance
- Ornstein-Uhlenbeck noise implementation details are vague despite being crucial for exploration
- Assumes perfect channel state information without accounting for real-world channel fading
- Simulation assumes single RSU with fixed coverage, not reflecting realistic urban V2X deployments

## Confidence

**High Confidence**: The basic problem formulation (joint AoI-energy optimization), use of NOMA for interference mitigation, and overall DRL framework structure

**Medium Confidence**: The specific MPDQN algorithm implementation details and reward function formulation

**Low Confidence**: Performance claims in scenarios with 50+ vehicles due to potential scalability issues with the state representation approach

## Next Checks
1. **Architecture Verification**: Implement the neural network with various hidden layer sizes (e.g., [64, 32], [128, 64], [256, 128]) to determine optimal configuration matching the paper's reported performance
2. **Scalability Testing**: Evaluate the algorithm's performance in dense scenarios (50-100 vehicles) to verify if the state representation can effectively capture interference patterns in large-scale networks
3. **Reward Sensitivity Analysis**: Test different λ values (0.05, 0.1, 0.2, 0.5) to understand how sensitive the trade-off between AoI and energy consumption is to the weighting parameter