---
ver: rpa2
title: Rethinking Sparse Lexical Representations for Image Retrieval in the Age of
  Rising Multi-Modal Large Language Models
arxiv_id: '2408.16296'
source_url: https://arxiv.org/abs/2408.16296
tags:
- image
- retrieval
- images
- recall
- sparse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using multi-modal large language models (M-LLMs)
  to convert images into textual data for keyword-based image retrieval. By employing
  cropping techniques to expand key sets and encoding textual data into sparse vectors,
  the method improves image feature extraction and retrieval performance.
---

# Rethinking Sparse Lexical Representations for Image Retrieval in the Age of Rising Multi-Modal Large Language Models

## Quick Facts
- arXiv ID: 2408.16296
- Source URL: https://arxiv.org/abs/2408.16296
- Authors: Kengo Nakata; Daisuke Miyashita; Youyang Ng; Yasuto Hoshi; Jun Deguchi
- Reference count: 40
- Key outcome: This paper explores using multi-modal large language models (M-LLMs) to convert images into textual data for keyword-based image retrieval. By employing cropping techniques to expand key sets and encoding textual data into sparse vectors, the method improves image feature extraction and retrieval performance. Experiments on benchmark datasets (MS-COCO, PASCAL VOC, NUS-WIDE) demonstrate that this approach outperforms conventional vision-language model-based methods in terms of precision and recall, especially when keywords are iteratively incorporated into search queries. The system also benefits from enhanced interpretability and analyzability through textual representations.

## Executive Summary
This paper proposes a novel approach to image retrieval that leverages multi-modal large language models (M-LLMs) to convert images into textual data for efficient keyword-based retrieval. By employing cropping techniques to expand key sets and encoding textual data into sparse vectors, the method improves image feature extraction and retrieval performance compared to conventional methods. The system's interpretability and analyzability are enhanced through the use of textual representations, and the retrieval performance can be further improved by iteratively incorporating keywords into search queries.

## Method Summary
The method uses pre-trained M-LLMs (specifically LLaVA) to generate descriptive captions from images and their cropped segments. Images are divided into 17 fixed-pattern segments with overlaps to improve feature extraction. These captions are concatenated and encoded into sparse vectors using BM25, enabling efficient retrieval through inverted indexes. The system supports iterative keyword refinement, allowing users to progressively improve search results by adding relevant terms.

## Key Results
- Outperforms conventional vision-language model-based methods on MS-COCO, PASCAL VOC, and NUS-WIDE datasets
- Improved precision and recall through iterative keyword incorporation into search queries
- Enhanced interpretability and analyzability through textual representations of image content

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-modal LLMs can extract image features into textual data that can be encoded into sparse vectors for efficient retrieval.
- Mechanism: M-LLMs with visual prompting capabilities process images and generate descriptive textual data (tags and captions). This textual data is then encoded into sparse lexical vectors using algorithms like BM25, enabling efficient retrieval using standard inverted indexes.
- Core assumption: The textual descriptions generated by the M-LLM are semantically aligned with the image content and can be effectively represented in sparse vectors.
- Evidence anchors:
  - [abstract] "By utilizing multi-modal large language models (M-LLMs) that support visual prompting, we can extract image features and convert them into textual data, enabling us to utilize efficient sparse retrieval algorithms employed in natural language processing for image retrieval tasks."
  - [section] "By utilizing the advanced capabilities of M-LLMs, we can extract features from images and linguistically represent them in textual data like tags and captions. Then, we can leverage the advantages of natural language processing (NLP) techniques for image retrieval tasks."
- Break condition: If the M-LLM fails to generate accurate or comprehensive textual descriptions of the image content, the sparse vector encoding will be ineffective for retrieval.

### Mechanism 2
- Claim: Cropping images into segments improves the M-LLM's ability to extract detailed features from images.
- Mechanism: By dividing images into multiple cropped segments, the M-LLM can focus on describing smaller regions of the image, leading to more detailed and comprehensive textual descriptions. These descriptions are then concatenated to form a complete representation of the image.
- Core assumption: Smaller image regions reduce the cognitive load on the M-LLM, allowing it to generate more focused and detailed descriptions.
- Evidence anchors:
  - [section] "We empirically show the effectiveness of cropping images using CLIPScore. Specifically, when the number of cropped images increases to 17, averaged CLIPScore based on Eq. (3) for all the images in the dataset also increases."
  - [section] "By employing the cropping technique, images can be divided into multiple segments, which reduces the number of features that the M-LLM needs to focus on and describe for a single image."
- Break condition: If the cropping process results in significant information loss from the original image, the concatenated descriptions may not accurately represent the complete image content.

### Mechanism 3
- Claim: Incorporating additional keywords into search queries iteratively improves retrieval performance.
- Mechanism: After an initial retrieval based on a search query, users can iteratively add keywords related to the desired image content. This refinement process narrows down the search results and improves the precision of the retrieval.
- Core assumption: Users can provide relevant feedback in the form of additional keywords that help specify the desired image content.
- Evidence anchors:
  - [abstract] "We also demonstrate that the retrieval performance can be improved by iteratively incorporating keywords into search queries."
  - [section] "After an initial retrieval based on a search query, a user iteratively incorporates keywords into the search query as user feedback to gradually clarify the vision for the desired image like a multi-turn refined search."
- Break condition: If the additional keywords do not accurately reflect the user's desired image content, the retrieval performance may not improve or could even degrade.

## Foundational Learning

- Concept: Sparse vector encoding and retrieval algorithms (e.g., BM25)
  - Why needed here: The system converts textual descriptions of images into sparse vectors for efficient retrieval using inverted indexes.
  - Quick check question: How does BM25 calculate the relevance score between a query and a document?

- Concept: Multi-modal large language models (M-LLMs) with visual prompting
  - Why needed here: M-LLMs are used to extract features from images and generate descriptive textual data.
  - Quick check question: What is visual prompting, and how does it enable M-LLMs to process images?

- Concept: Image cropping and data augmentation techniques
  - Why needed here: Cropping images into segments improves the M-LLM's ability to extract detailed features from images.
  - Quick check question: How does dividing an image into smaller segments affect the M-LLM's feature extraction process?

## Architecture Onboarding

- Component map: Image input -> M-LLM with visual prompting (LLaVA) -> Image cropping module -> Textual data generation -> Sparse vector encoding (BM25) -> Inverted index for retrieval -> Search query input -> Iterative keyword incorporation module

- Critical path:
  1. Image input → M-LLM → Textual data generation
  2. Image cropping → M-LLM (per cropped image) → Textual data generation
  3. Textual data concatenation → Sparse vector encoding → Inverted index storage
  4. Search query input → Sparse vector encoding → Inverted index retrieval → Results

- Design tradeoffs:
  - Using pre-trained M-LLMs vs. fine-tuning: Pre-trained models offer general performance without the need for training data but may not be optimized for specific image domains.
  - Fixed pattern cropping vs. object detection-based cropping: Fixed patterns ensure coverage of the entire image but may include irrelevant regions, while object detection focuses on relevant areas but may miss some features.

- Failure signatures:
  - Poor retrieval performance: Could indicate issues with M-LLM's feature extraction, sparse vector encoding, or the relevance of the search query.
  - High computational cost: Could indicate inefficient use of M-LLM or unnecessary image cropping.

- First 3 experiments:
  1. Evaluate the impact of different cropping patterns (e.g., number of segments, overlap) on the M-LLM's feature extraction performance.
  2. Compare the retrieval performance of the system using different sparse vector encoding algorithms (e.g., BM25, TF-IDF).
  3. Assess the effectiveness of iterative keyword incorporation by measuring the improvement in retrieval performance with each additional keyword.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the performance and limitations of using M-LLMs for feature extraction compare when applied to medical images or images in anomaly detection tasks, which are difficult to describe in language?
- Basis in paper: [explicit] The paper discusses that their approach might not be well-suited for images that are difficult to describe in language, such as medical images or defects in anomaly detection tasks.
- Why unresolved: The paper mentions this limitation but does not explore how the performance and limitations of using M-LLMs for feature extraction compare in these specific cases.
- What evidence would resolve it: Experimental results comparing the performance of M-LLMs on medical images or anomaly detection images against traditional methods, as well as an analysis of the limitations and challenges encountered in these scenarios.

### Open Question 2
- Question: How does the cropping technique's effectiveness vary across different image types and categories, and what are the underlying reasons for these variations?
- Basis in paper: [explicit] The paper mentions that the cropping technique enhances the ability of the M-LLM to extract features from images more precisely, but it also notes that if the precision and recall are sufficiently high, the cropping technique may not improve the performance and could potentially degrade the precision by retrieving irrelevant cropped images.
- Why unresolved: The paper provides some examples and analysis, but it does not comprehensively explore how the cropping technique's effectiveness varies across different image types and categories, nor does it delve into the underlying reasons for these variations.
- What evidence would resolve it: A detailed analysis of the cropping technique's performance across a wide range of image types and categories, including quantitative metrics and qualitative insights into the factors influencing its effectiveness.

### Open Question 3
- Question: What are the potential trade-offs between the interpretability and analyzability of textual representations generated by M-LLMs and the accuracy of image retrieval compared to dense vector representations?
- Basis in paper: [explicit] The paper highlights the enhanced interpretability and analyzability of textual representations generated by M-LLMs compared to dense vector representations, but it also acknowledges that its method still lags behind the conventional methods in caption-to-image retrieval settings.
- Why unresolved: The paper provides insights into the interpretability and analyzability benefits of textual representations, but it does not comprehensively explore the potential trade-offs between these benefits and the accuracy of image retrieval compared to dense vector representations.
- What evidence would resolve it: A comparative study evaluating the trade-offs between interpretability, analyzability, and accuracy in image retrieval tasks using both textual representations generated by M-LLMs and dense vector representations, along with user studies or expert evaluations to assess the practical implications of these trade-offs.

## Limitations
- The evaluation relies heavily on the M-LLM's captioning quality and assumes iterative keyword refinement is practical in real-world usage.
- The cropping mechanism improves CLIPScore but doesn't fully explore alternative segmentation strategies like object detection.
- BM25 parameters are fixed without sensitivity analysis, and scalability with larger datasets remains untested.

## Confidence
- High confidence: The core mechanism of converting images to textual data via M-LLMs and using sparse retrieval is sound, supported by strong experimental results across multiple benchmarks.
- Medium confidence: The cropping improvement is empirically demonstrated but could benefit from deeper analysis of optimal segmentation strategies.
- Medium confidence: Iterative keyword incorporation shows promise but lacks analysis of user experience and practical implementation details.

## Next Checks
1. **Ablation study on cropping patterns**: Compare fixed-pattern cropping against object detection-based segmentation to determine if adaptive cropping yields better feature extraction for complex images.

2. **User study on iterative refinement**: Conduct a controlled experiment measuring how many keyword iterations users typically need and whether the performance gains justify the additional interaction cost.

3. **Scaling analysis**: Test the system's performance and computational efficiency on larger datasets (e.g., ImageNet-1K) to validate real-world applicability and identify bottlenecks in the retrieval pipeline.