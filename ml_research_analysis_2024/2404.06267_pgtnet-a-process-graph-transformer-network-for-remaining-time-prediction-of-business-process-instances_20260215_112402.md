---
ver: rpa2
title: 'PGTNet: A Process Graph Transformer Network for Remaining Time Prediction
  of Business Process Instances'
arxiv_id: '2404.06267'
source_url: https://arxiv.org/abs/2404.06267
tags:
- event
- process
- time
- graph
- pgtnet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents PGTNet, a novel approach for predicting remaining
  time of business process instances using Process Graph Transformer Networks. PGTNet
  converts event logs into graph datasets and employs a Graph Transformer architecture
  to make predictions.
---

# PGTNet: A Process Graph Transformer Network for Remaining Time Prediction of Business Process Instances

## Quick Facts
- arXiv ID: 2404.06267
- Source URL: https://arxiv.org/abs/2404.06267
- Authors: Keyvan Amiri Elyasi; Han van der Aa; Heiner Stuckenschmidt
- Reference count: 26
- Primary result: Achieves 12.92 average MAE vs 24.63 MAE for next best approach

## Executive Summary
This paper introduces PGTNet, a novel approach for predicting remaining time of business process instances using Process Graph Transformer Networks. The key innovation is converting event logs into graph datasets where nodes represent event classes and edges capture control-flow relationships with weighted frequencies. PGTNet employs a Graph Transformer architecture combining MPNN blocks for local message passing with Transformer blocks for global attention, enabling simultaneous learning of local control-flow patterns and long-range dependencies. Experiments on 20 real-world event logs demonstrate that PGTNet consistently outperforms state-of-the-art deep learning approaches, particularly for highly complex processes where existing methods struggle.

## Method Summary
PGTNet converts event logs into graph representations where nodes correspond to event classes (activity + lifecycle) and edges represent directly-follows relations with weighted frequencies. Edge features incorporate temporal information (duration, timestamps), case attributes, event attributes, and concurrent workload. The Graph Transformer architecture processes these graphs through embedding modules, hybrid GPS layers combining MPNN and Transformer blocks, a readout layer, and an MLP predictor. The model is trained using 5-fold cross-validation with AdamW optimizer and cosine learning rate scheduling, evaluating performance using Mean Absolute Error (MAE) across different prefix lengths to measure earliness of predictions.

## Key Results
- PGTNet achieves an average MAE of 12.92 days across 20 event logs, compared to 24.63 days for the next best approach
- The method shows superior performance particularly on highly complex processes where existing methods struggle
- PGTNet demonstrates consistent improvement across different prefix lengths, enabling early prediction capability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph Transformer architecture captures both local control-flow relationships and long-range dependencies simultaneously
- Mechanism: Combines MPNN blocks for local message passing with Transformer blocks for global attention, allowing information to propagate across the graph while learning control-flow patterns
- Core assumption: Local MPNN operations can learn control-flow relationships between event classes while Transformer attention can capture dependencies across the entire event prefix
- Evidence anchors: [abstract], [section 4.2]
- Break condition: If graph structure becomes too dense or loses meaningful control-flow information, MPNN component may fail to learn useful local patterns

### Mechanism 2
- Claim: Graph representation of event prefixes preserves control-flow information better than vector embeddings
- Mechanism: Creates nodes for event classes and edges for directly-follows relationships with weighted frequencies, capturing actual control-flow structure rather than flattening into sequential vectors
- Core assumption: Directly-follows relationships in graph structure contain sufficient information to learn meaningful control-flow patterns
- Evidence anchors: [section 4.1], [abstract]
- Break condition: If event log contains very repetitive patterns with minimal variation, graph may become too sparse to provide meaningful learning signals

### Mechanism 3
- Claim: Multi-perspective feature encoding (temporal, case attributes, workload) enhances predictive performance
- Mechanism: Edge features incorporate temporal information, case attributes, and concurrent workload, providing richer context than control-flow alone
- Core assumption: These additional features contain predictive information about remaining time that complements control-flow structure
- Evidence anchors: [section 4.1], [section 5.2]
- Break condition: If feature engineering is incorrect or normalization is improperly applied, additional features may introduce noise rather than useful information

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and their limitations
  - Why needed here: Understanding why GNNs alone are insufficient (over-smoothing, over-squashing) helps explain why hybrid Graph Transformer approach is necessary
  - Quick check question: What are the two main problems that GNNs face which Graph Transformers aim to solve?

- Concept: Transformer architecture and self-attention mechanism
  - Why needed here: Global attention component is crucial for capturing long-range dependencies across event prefixes
  - Quick check question: How does self-attention mechanism in Transformers differ from message passing in GNNs?

- Concept: Graph isomorphism and graph representation learning
  - Why needed here: Understanding how different graph structures represent different process variants helps explain why approach works well for complex processes
  - Quick check question: What does it mean for two graphs to be isomorphic, and why is this relevant for process mining?

## Architecture Onboarding

- Component map: Embedding modules → GPS layers (MPNN + Transformer blocks) → Readout layer → MLP predictor
- Critical path: Graph representation creation → Embedding initialization → 5 GPS layers processing → Readout aggregation → Remaining time prediction
- Design tradeoffs: Using 5 GPS layers with 8 heads provides good balance between expressivity and computational cost; fewer layers might miss complex patterns, more layers increase training time
- Failure signatures: If training loss plateaus early, it may indicate insufficient capacity or poor feature engineering; if validation loss increases while training loss decreases, it suggests overfitting
- First 3 experiments:
  1. Train with only control-flow edges (no additional features) to establish baseline performance
  2. Add temporal features incrementally to measure their individual impact on MAE
  3. Test different numbers of GPS layers (3, 5, 7) to find optimal architecture depth for each dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of different positional and structural encoding (PE/SE) initialization strategies on PGTNet's performance?
- Basis in paper: [explicit] The paper mentions that PGTNet uses various PE/SE initialization strategies, including Laplacian eigenvector encodings, random-walk structural encoding, and centrality encoding, but does not extensively explore their impact
- Why unresolved: The paper only briefly mentions these strategies without providing a detailed comparison of their effectiveness
- What evidence would resolve it: A comprehensive ablation study comparing the performance of PGTNet using different PE/SE initialization strategies on various event logs

### Open Question 2
- Question: How does PGTNet perform on object-centric event logs?
- Basis in paper: [explicit] The paper mentions that future work aims to extend PGTNet to handle object-centric event logs
- Why unresolved: The paper only discusses PGTNet's performance on traditional event logs and does not provide any insights into its applicability to object-centric logs
- What evidence would resolve it: An evaluation of PGTNet's performance on a dataset of object-centric event logs, comparing it to existing approaches for this type of data

### Open Question 3
- Question: What is the impact of incorporating multi-task learning in PGTNet?
- Basis in paper: [explicit] The paper suggests exploring multi-task learning as a potential avenue for improving PGTNet's predictive accuracy
- Why unresolved: The paper does not investigate the benefits of multi-task learning for PGTNet
- What evidence would resolve it: An experiment comparing the performance of PGTNet with and without multi-task learning on various event logs and prediction tasks

## Limitations

- The approach relies heavily on quality of graph representation construction, assuming directly-follows relationships adequately capture process semantics
- Performance advantage may diminish if event logs contain significant noise or if graph structure becomes too dense to maintain meaningful control-flow patterns
- "Complexity" is not clearly defined or measured across datasets, making claims about superior performance on complex processes need more nuanced validation

## Confidence

- **High confidence**: Core architectural design (MPNN + Transformer hybrid) is well-justified and empirical results showing superior MAE performance are reproducible
- **Medium confidence**: Multi-perspective feature engineering approach is sound, but its contribution could vary significantly based on specific characteristics of different event logs
- **Low confidence**: Claim that PGTNet works particularly well for highly complex processes needs more nuanced validation

## Next Checks

1. Test PGTNet on event logs with varying levels of control-flow noise to determine robustness boundaries and identify when graph representation becomes ineffective
2. Implement ablation studies removing specific feature types (temporal, case attributes, workload) to quantify their individual contributions to the 12.92 MAE performance
3. Compare PGTNet's performance against simpler baseline models (LSTM, GRU) on simpler processes to validate the claim about superior performance on complex processes specifically