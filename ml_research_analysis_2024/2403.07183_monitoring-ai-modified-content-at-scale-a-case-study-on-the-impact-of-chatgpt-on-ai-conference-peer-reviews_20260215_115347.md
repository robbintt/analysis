---
ver: rpa2
title: 'Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT
  on AI Conference Peer Reviews'
arxiv_id: '2403.07183'
source_url: https://arxiv.org/abs/2403.07183
tags:
- neurips
- iclr
- corl
- reviews
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces a maximum likelihood estimation framework
  to quantify the fraction of AI-generated or AI-modified text in large corpora. By
  modeling documents as a mixture of human-written and AI-generated distributions,
  the method estimates the proportion of AI content without classifying individual
  documents.
---

# Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews

## Quick Facts
- arXiv ID: 2403.07183
- Source URL: https://arxiv.org/abs/2403.07183
- Reference count: 40
- The study estimates 6.5%-16.9% of post-ChatGPT peer review sentences are AI-modified, with higher usage near deadlines and in low-confidence reviews

## Executive Summary
This paper introduces a maximum likelihood estimation framework to quantify AI-generated or AI-modified text in large corpora without classifying individual documents. The method models documents as mixtures of human-written and AI-generated distributions, estimating the proportion of AI content through token frequency analysis. Applied to peer reviews from top AI conferences, the approach reveals significant AI usage patterns, particularly near submission deadlines and in reviews with lower confidence scores, while also detecting correlations with reduced scholarly citations.

## Method Summary
The approach estimates the fraction α of AI-generated content by modeling a corpus as a mixture of human-written (P) and AI-generated (Q) distributions. Using maximum likelihood estimation, it computes the optimal mixing coefficient based on token occurrence probabilities. The method requires reference corpora of known human-written and AI-generated texts to estimate token usage distributions, then applies this to target corpora with uncertain authorship. Validation on synthetic data shows estimation error under 2.4%, and the approach is 10 million times more computationally efficient than instance-level detection methods.

## Key Results
- 6.5%-16.9% of post-ChatGPT peer review sentences estimated as AI-modified
- AI usage increases near submission deadlines and in low-confidence reviews
- Generated text correlates with corpus-level homogenization and reduced scholarly citations
- Nature journal reviews show no significant AI increase, suggesting field-specific patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The MLE framework accurately estimates the fraction of AI-generated content in a corpus without classifying individual documents.
- Mechanism: By modeling the corpus as a mixture of human-written (P) and AI-generated (Q) distributions, and using maximum likelihood estimation on token occurrence probabilities, the method computes the optimal mixing coefficient α.
- Core assumption: The token occurrence patterns in human-written and AI-generated texts are sufficiently distinct to be captured by frequency distributions.
- Evidence anchors:
  - [abstract]: "Our maximum likelihood model leverages expert-written and AI-generated reference texts to accurately and efficiently examine real-world LLM-use at the corpus level."
  - [section]: "Using the notation t ∈ x to denote that token t occurs in document x, we can then estimate P via P (xi) = Y t∈x ˆp(t) × Y t̸∈x (1 − ˆp(t)) and similarly for Q."
  - [corpus]: Corpus-level trends are detected that are too subtle for individual-level detection, indicating the distributional approach captures aggregate patterns.
- Break condition: If AI-generated and human-written texts converge in their token usage patterns, making P and Q indistinguishable.

### Mechanism 2
- Claim: The method is computationally efficient compared to instance-level detection methods.
- Mechanism: By avoiding the need to classify each individual document or sentence, the approach reduces computational complexity from document-level inference to aggregate frequency counting and MLE.
- Core assumption: Token frequency distributions can be computed efficiently and capture sufficient signal for mixture estimation.
- Evidence anchors:
  - [abstract]: "the approach is 10 million times more computationally efficient than state-of-the-art detection methods."
  - [section]: "the training cost is also negligible compared to any backpropagation-based algorithms as we are only counting word frequencies in the training corpora."
  - [corpus]: No explicit corpus-level efficiency comparison provided; this is primarily a theoretical advantage.
- Break condition: If the corpus size becomes so large that even frequency counting becomes a bottleneck, or if the dimensionality of token space explodes.

### Mechanism 3
- Claim: The method can detect AI-generated content even when it's used to substantially modify human-written text.
- Mechanism: By focusing on token-level occurrence probabilities rather than exact text matching, the method captures the cumulative effect of AI modifications on writing style and vocabulary usage.
- Core assumption: Substantial AI modifications alter the token distribution in ways that persist even when human-written content is present.
- Evidence anchors:
  - [abstract]: "Our results suggest that between 6.5% and 16.9% of text submitted as peer reviews to these conferences could have been substantially modified by LLMs, i.e. beyond spell-checking or minor writing updates."
  - [section]: "Rather than directly using LLMs to generate feedback, we expand a bullet-pointed skeleton of incomplete sentences into a full review using LLMs... The estimated α from our approach is consistent with review reviewers using LLM to substantially expand their bullet points into full reviews."
  - [corpus]: The detection of increased AI usage near deadlines and in low-confidence reviews suggests the method captures meaningful patterns of AI assistance.
- Break condition: If AI modifications are too subtle to affect token frequency distributions, or if human reviewers mimic AI writing styles perfectly.

## Foundational Learning

- Concept: Maximum Likelihood Estimation (MLE)
  - Why needed here: MLE provides a principled way to estimate the mixing coefficient α that best explains the observed corpus data under the mixture model assumption.
  - Quick check question: If we observe a corpus where 70% of documents follow distribution P and 30% follow distribution Q, what should the MLE estimate of α be?

- Concept: Mixture Models
  - Why needed here: The assumption that documents are generated from a mixture of human-written and AI-generated distributions allows us to model the uncertainty about the origin of each document.
  - Quick check question: In a mixture model X = (1-α)P + αQ, what does the parameter α represent?

- Concept: Token Frequency Distributions
  - Why needed here: The method relies on comparing the frequency of specific tokens (adjectives in the main experiments) between human-written and AI-generated reference corpora to distinguish between the two distributions.
  - Quick check question: If the adjective "commendable" appears in 10% of human-written documents but 40% of AI-generated documents, how would this affect the likelihood calculations?

## Architecture Onboarding

- Component map: Data collection → Reference corpus generation (human and AI) → Token frequency estimation → MLE computation → Validation → Application to target corpus
- Critical path: The accuracy of the reference corpora generation and token frequency estimation directly determines the quality of the P and Q estimates, which in turn determines the accuracy of the α estimation.
- Design tradeoffs: Using adjectives as the vocabulary provides stability but may miss other linguistic signals; using more comprehensive vocabularies could capture more information but may introduce noise.
- Failure signatures: Overestimation of α when human-written texts coincidentally match AI-generated token patterns; underestimation when AI modifications are too subtle to affect token frequencies.
- First 3 experiments:
  1. Validate the method on synthetic data with known α values to confirm accuracy bounds.
  2. Test robustness to different LLM prompts and models by training on one prompt and testing on another.
  3. Compare performance using different vocabulary choices (adjectives vs. adverbs vs. verbs) to understand sensitivity to this design decision.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different parts of speech (e.g., adverbs, verbs, nouns) compare to adjectives in detecting AI-modified text?
- Basis in paper: [explicit] The paper tested adjectives and found they were more stable than other parts of speech, but conducted additional experiments with adverbs, verbs, and nouns.
- Why unresolved: While the paper provides results comparing adjectives to other parts of speech, it does not fully explore the implications of these differences or determine the optimal part of speech for detection.
- What evidence would resolve it: A comprehensive analysis comparing the performance of different parts of speech across various corpora and AI models would clarify their relative effectiveness.

### Open Question 2
- Question: What is the impact of using different LLMs (e.g., GPT-4 vs. GPT-3.5) on the detection of AI-modified text?
- Basis in paper: [explicit] The paper mentions that results are robust to using different LLMs, but does not extensively explore the impact of using different models.
- Why unresolved: The paper does not provide a detailed comparison of the detection performance when using different LLMs, leaving the question of model-specific effects open.
- What evidence would resolve it: A systematic comparison of detection accuracy and stability across multiple LLMs and their various versions would shed light on model-specific effects.

### Open Question 3
- Question: How does the temporal distribution shift in token frequencies affect the detection of AI-modified text?
- Basis in paper: [inferred] The paper acknowledges that approximations made to the review generating process introduce error and that temporal distribution shift in token frequencies can occur due to changes in topics, reviewers, etc.
- Why unresolved: The paper does not delve into the specific impact of temporal distribution shifts on detection accuracy or propose methods to mitigate this issue.
- What evidence would resolve it: An analysis of detection performance over time and the development of techniques to account for temporal shifts would address this question.

### Open Question 4
- Question: What are the causal relationships between factors like deadline proximity, citation usage, and reviewer reply rates with the use of AI in reviews?
- Basis in paper: [explicit] The paper identifies correlations between these factors and estimated AI usage but does not establish causality.
- Why unresolved: The paper presents correlations but does not explore the underlying mechanisms or conduct experiments to determine causal relationships.
- What evidence would resolve it: Controlled experiments manipulating these factors and observing their effects on AI usage would establish causality.

### Open Question 5
- Question: How does the use of AI in peer reviews impact the overall quality and diversity of scientific discourse?
- Basis in paper: [explicit] The paper discusses the potential implications of AI use, such as homogenization of content and reduced scholarly citations, but does not directly measure the impact on scientific discourse.
- Why unresolved: The paper raises concerns about the potential negative impacts of AI use but does not provide empirical evidence or a framework for assessing the overall impact on scientific discourse.
- What evidence would resolve it: A longitudinal study comparing the quality and diversity of scientific discourse before and after the widespread adoption of AI tools would provide insights into the long-term impact.

## Limitations

- The method's accuracy depends on the assumption that token frequency distributions from historical human-written and AI-generated texts remain stable over time.
- The 2.4% error bound from synthetic validation may not fully capture real-world complexity with nuanced linguistic shifts.
- The approach may miss other linguistic signals beyond token frequencies, potentially limiting detection of subtle AI modifications.

## Confidence

**High Confidence**: The computational efficiency claim (10 million times faster than detection methods) and the basic MLE framework implementation appear well-supported. The observed correlations between AI usage and factors like deadlines, low confidence scores, and reduced citations are empirically demonstrated.

**Medium Confidence**: The accuracy bounds from synthetic validation (under 2.4% error) are promising but may not fully translate to real-world scenarios. The interpretation of "substantial modification" relies on implicit assumptions about what constitutes meaningful AI assistance.

**Low Confidence**: The generalizability of findings across different research communities and time periods remains uncertain. The Nature journal comparison showing no significant AI increase could reflect differences in editorial practices or timing relative to LLM adoption patterns.

## Next Checks

1. **Temporal Stability Analysis**: Replicate the analysis on chronologically segmented subsets of the peer review data to assess whether the estimated α values and correlations remain consistent across different time periods within the post-ChatGPT era.

2. **Cross-Discipline Validation**: Apply the method to peer review corpora from disciplines outside AI (e.g., medicine, physics, social sciences) to test whether the observed patterns of AI usage and their correlations with citation patterns generalize beyond the AI community.

3. **Vocabulary Sensitivity Testing**: Conduct controlled experiments varying the vocabulary choice (adjectives vs. other parts of speech, or using n-grams) to quantify how sensitive the α estimates are to this methodological choice and whether alternative vocabularies provide complementary insights into AI modification patterns.