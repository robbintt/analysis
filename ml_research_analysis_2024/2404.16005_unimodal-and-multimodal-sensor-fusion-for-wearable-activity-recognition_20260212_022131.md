---
ver: rpa2
title: Unimodal and Multimodal Sensor Fusion for Wearable Activity Recognition
arxiv_id: '2404.16005'
source_url: https://arxiv.org/abs/2404.16005
tags:
- fusion
- recognition
- facial
- wearable
- body
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This Ph.D. research demonstrates that combining unimodal, multimodal,
  and multi-positional sensing modalities significantly enhances the robustness of
  human activity recognition (HAR) models.
---

# Unimodal and Multimodal Sensor Fusion for Wearable Activity Recognition

## Quick Facts
- arXiv ID: 2404.16005
- Source URL: https://arxiv.org/abs/2404.16005
- Authors: Hymalai Bello
- Reference count: 11
- Primary result: Multimodal sensor fusion achieves up to 94.3% accuracy for hand position estimation on edge devices with ≤2 MB RAM

## Executive Summary
This Ph.D. research demonstrates that combining multiple sensing modalities significantly enhances the robustness of human activity recognition (HAR) models using wearable devices. The work explores fusion of inertial, pressure (audio and atmospheric), and textile capacitive sensors across facial muscle activity, hand position tracking, and body posture recognition applications. Multimodal approaches consistently outperformed unimodal methods, with fusion models achieving high accuracy while maintaining real-time inference capabilities on resource-constrained edge devices. The research also introduced novel sensor modalities including sound mechanomyography and differential atmospheric pressure for HAR applications.

## Method Summary
The research employed a systematic approach to multimodal sensor fusion for HAR, combining unimodal, multimodal, and multi-positional sensing modalities. Experimental validation was conducted across three distinct application domains using different wearable form factors and sensor configurations. Machine learning models were trained and evaluated using controlled experimental protocols with 7-10 participants. Edge device implementations were validated on Arduino Nano 33 BLE Sense and ESP32 platforms, demonstrating real-time inference capabilities with memory footprints of 4-16 MB Flash and ≤2 MB RAM for fusion models.

## Key Results
- Hand position recognition achieved 94.3% accuracy using multimodal fusion compared to unimodal baselines
- Real-time inference demonstrated on edge devices with ≤2 MB RAM and 4-16 MB Flash memory
- Novel sensor modalities (sound mechanomyography, differential atmospheric pressure) validated for HAR applications
- Multimodal fusion consistently outperformed unimodal approaches across all tested application domains

## Why This Works (Mechanism)
Multimodal sensor fusion improves HAR by capturing complementary information from different sensing modalities, reducing ambiguity in complex human movements. Each modality provides unique information about body dynamics - inertial sensors capture acceleration and rotation, pressure sensors detect force distribution, and novel modalities like sound mechanomyography capture muscle vibrations. By fusing these diverse data streams, the system can better handle variations in movement patterns, environmental conditions, and individual differences. The redundancy and complementarity between modalities enable more robust feature extraction and classification, particularly in challenging scenarios where single modalities may fail or produce ambiguous results.

## Foundational Learning
- **Sensor fusion principles**: Why needed - different modalities capture different aspects of human movement; Quick check - does fusion improve accuracy over individual sensors?
- **Edge ML deployment**: Why needed - wearables require real-time inference on resource-constrained hardware; Quick check - can models fit within memory constraints while maintaining accuracy?
- **Feature engineering for multimodal data**: Why needed - different sensors produce different data types requiring appropriate preprocessing; Quick check - are features normalized and synchronized across modalities?
- **Cross-validation methodology**: Why needed - ensures model generalization across participants and movement variations; Quick check - is leave-one-subject-out validation performed?
- **Real-time processing constraints**: Why needed - wearables must process data continuously without significant latency; Quick check - does inference meet timing requirements?
- **Power consumption optimization**: Why needed - battery life is critical for wearable deployments; Quick check - what is the power profile of continuous sensing?

## Architecture Onboarding

**Component Map:**
Edge device (Arduino Nano 33 BLE Sense/ESP32) -> Sensor array (inertial, pressure, capacitive) -> Preprocessing pipeline -> Feature extraction -> Multimodal fusion layer -> Classification model -> Output interface

**Critical Path:**
Sensor data acquisition → Synchronization → Feature extraction → Multimodal fusion → Classification → Output

**Design Tradeoffs:**
- Memory vs. accuracy: Larger models provide better accuracy but require more memory
- Sampling rate vs. power consumption: Higher rates improve accuracy but drain battery faster
- Sensor count vs. form factor: More sensors improve recognition but increase device size
- Processing latency vs. classification confidence: More complex fusion requires more computation

**Failure Signatures:**
- Sensor drift causing classification errors
- Feature misalignment between modalities leading to poor fusion
- Memory overflow during high-frequency data acquisition
- Classification instability when sensor readings fall outside training distribution

**First 3 Experiments:**
1. Baseline comparison: Unimodal vs. multimodal accuracy on controlled activity dataset
2. Memory footprint analysis: Model size optimization for edge deployment
3. Cross-participant validation: Generalization performance across different users

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Performance metrics based on limited subject diversity (n=7-10 participants) may not generalize to broader populations
- Hardware implementations validated only on specific edge devices, scalability to diverse consumer wearables uncertain
- Benefits of fusion are context-dependent rather than universally applicable across all application domains
- Real-world robustness to environmental interference, sensor degradation, and cross-user variability not thoroughly investigated
- Privacy implications of continuous multimodal sensing not addressed

## Confidence
- **High Confidence**: Fusion of inertial and pressure sensors improves hand position recognition accuracy over unimodal approaches
- **Medium Confidence**: Multimodal fusion provides consistent benefits across all tested application domains
- **Medium Confidence**: Novel sensor modalities (sound mechanomyography, differential pressure) are viable for HAR applications

## Next Checks
1. Conduct longitudinal studies with diverse participant pools (age, gender, body composition) to assess generalization of fusion models across demographic variations
2. Implement real-world field testing of the multimodal systems in uncontrolled environments with varying noise levels, temperature fluctuations, and physical obstructions
3. Perform comprehensive power consumption analysis and wearability assessments over extended periods to evaluate practical deployment feasibility on consumer wearables