---
ver: rpa2
title: 'What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A Gradient
  Perspective'
arxiv_id: '2410.23743'
source_url: https://arxiv.org/abs/2410.23743
tags:
- detailed
- none
- simplified0
- responses
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how fast versus slow thinking during LLM
  training affects layer-wise gradients. Using spectral analysis of gradients across
  attention-related layers, the research compares training dynamics when models learn
  with varying levels of Chain-of-Thought reasoning paths, correct versus irrelevant
  responses, and different initial model types (base vs.
---

# What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A Gradient Perspective

## Quick Facts
- arXiv ID: 2410.23743
- Source URL: https://arxiv.org/abs/2410.23743
- Authors: Ming Li; Yanhong Li; Tianyi Zhou
- Reference count: 40
- Key outcome: Slow thinking (detailed CoT) produces stable gradient norms across layers; fast thinking causes larger gradients and variation, indicating instability.

## Executive Summary
This study investigates how fast versus slow thinking during LLM training affects layer-wise gradients using spectral analysis of gradients across attention-related layers. The research compares training dynamics when models learn with varying levels of Chain-of-Thought reasoning paths, correct versus irrelevant responses, and different initial model types (base vs. instruction-tuned). Key findings show that slow thinking produces stable, uniform gradient norms across layers, while fast thinking causes larger gradients and greater variation across layers, indicating instability. Slow thinking also enables models to distinguish correct from irrelevant responses—a capability absent without CoT. However, these patterns do not extend to knowledge-learning tasks, where increasing response length alone does not replicate slow-thinking gradient behaviors. Instruction-tuned models show no advantage over base models in detecting irrelevant responses.

## Method Summary
The study uses SVD-based gradient analysis focusing on Q, K, V, O projection layers during instruction tuning with backpropagation. It compares 10 models (5 base pretrained, 5 instruction-finetuned) across 3 task types: Math, Commonsense Reasoning, and Wiki Knowledge. Datasets include AQuA, GSM8K, MATH, StrategyQA, ECQA, CREAK, Sensemaking, and Wikipedia articles. Training configurations vary correct vs irrelevant responses and None/Simplified/Detailed Chain-of-Thought reasoning paths. The method calculates nuclear norms and σ1 ratios for each layer after each training step, using 500 samples per dataset randomly selected.

## Key Results
- Slow thinking (Detailed CoT) produces stable, uniform gradient norms across layers
- Fast thinking causes larger gradients and greater variation across layers, indicating training instability
- Slow thinking enables models to distinguish correct from irrelevant responses—a capability absent without CoT
- These gradient patterns do not extend to knowledge-learning tasks where response length alone doesn't replicate slow-thinking behaviors
- Instruction-tuned models show no advantage over base models in detecting irrelevant responses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Slow thinking (detailed CoT) produces stable gradient norms across layers, while fast thinking causes large gradients and greater variation, indicating training instability.
- Mechanism: Detailed CoT provides a more gradual, step-by-step alignment process, allowing gradients to propagate smoothly through all layers. Fast thinking skips intermediate reasoning steps, forcing the model to jump directly from question to answer, which causes sharp, high-magnitude gradients especially in early layers.
- Core assumption: The model's internal knowledge representation aligns better with detailed reasoning steps than with direct answer mapping.
- Evidence anchors:
  - [abstract]: "slow thinking (Detailed CoT) produces stable, uniform gradient norms across layers, while fast thinking (simplified or no CoT) causes larger gradients and greater variation across layers, indicating instability."
  - [section 4.1.1]: "fast thinking, the mean absolute differences (MADs) are the largest on all the curves of different projection layers, representing a severe fluctuation of the gradient scales across all the layers of the LLM, which might cause instability for training."
- Break condition: If the CoT steps themselves are logically flawed or irrelevant, the gradient stability advantage disappears.

### Mechanism 2
- Claim: Slow thinking enables the model to distinguish correct from irrelevant responses via gradient patterns; without CoT, the model cannot differentiate them.
- Mechanism: Detailed CoT creates an explicit reasoning path that the model can evaluate. When the reasoning path is broken (irrelevant responses), the model detects the mismatch between expected and actual reasoning flow, producing larger gradients. Without CoT, the model lacks this intermediate evaluation layer and treats correct and irrelevant responses similarly.
- Core assumption: The model can monitor and evaluate its own reasoning process when explicitly provided with it.
- Evidence anchors:
  - [abstract]: "Slow thinking also enables models to distinguish correct from irrelevant responses—a capability absent without CoT."
  - [section 4.1.2]: "when no CoT paths are given, the gradient behaviors between learning the correct and nonsense responses are almost identical... On the contrary, when the detailed CoT reasoning paths are provided... the gradient behaviors will be different, mainly reflected by the larger scale of the gradient."
- Break condition: If the CoT is too simplistic or the reasoning steps are not internally consistent, the model may still fail to distinguish correctness.

### Mechanism 3
- Claim: The gradient stability and correctness detection benefits of slow thinking do not extend to knowledge learning tasks; longer responses alone do not replicate slow-thinking gradient behaviors.
- Mechanism: Knowledge learning tasks lack the structured reasoning process that CoT provides. Simply increasing response length adds more tokens but not more reasoning steps, so the gradient patterns remain similar to fast thinking. The model's gradients reflect the absence of reasoning structure rather than response length.
- Core assumption: The observed gradient patterns are due to reasoning structure, not response length.
- Evidence anchors:
  - [abstract]: "these patterns do not extend to knowledge-learning tasks, where increasing response length alone does not replicate slow-thinking gradient behaviors."
  - [section 4.2.1]: "it is observed that for the knowledge-intensive task, the lengths of responses do not affect the gradient scales and fluctuations. This phenomenon is largely different from the findings observed in the reasoning tasks, where detailed CoTs can largely reduce the gradient scale and fluctuation."
- Break condition: If the knowledge task inherently requires multi-step reasoning (e.g., multi-hop reasoning), the slow-thinking gradient benefits might reappear.

## Foundational Learning

- Concept: Gradient norm and its spectral properties (nuclear norm, singular values)
  - Why needed here: The paper uses spectral analysis of gradients to compare training dynamics. Understanding how gradient magnitude and distribution across layers reflect learning stability is essential.
  - Quick check question: If two models have the same average gradient norm but different layer-wise distributions, what might that indicate about their training dynamics?

- Concept: Chain-of-Thought (CoT) reasoning and its role in model training
  - Why needed here: The study contrasts fast vs. slow thinking, where slow thinking corresponds to detailed CoT. Knowing how CoT affects model behavior is crucial to interpreting gradient differences.
  - Quick check question: How might a model's ability to distinguish correct from incorrect responses change if it is trained with vs. without CoT?

- Concept: Instruction tuning and base model differences
  - Why needed here: The paper compares pre-trained base LLMs with instruction-tuned versions to see if alignment affects gradient patterns. Understanding the distinction is necessary for interpreting the results.
  - Quick check question: If an instruction-tuned model and a base model show different gradient patterns on the same task, what might that suggest about their internal representations?

## Architecture Onboarding

- Component map: Data pipeline -> Model loader -> Training loop -> Gradient analyzer -> Visualization module
- Critical path:
  1. Load dataset (correct vs. irrelevant, with/without CoT)
  2. Load model (base vs. instructed)
  3. For each batch: forward pass, compute loss, backward pass
  4. Extract and store gradients for Q, K, V, O layers
  5. Compute nuclear norm and singular value statistics
  6. Aggregate and compare across conditions
- Design tradeoffs:
  - Full gradient storage vs. on-the-fly aggregation: Full storage enables more detailed post-hoc analysis but requires significant memory; aggregation saves memory but limits flexibility
  - Layer-wise vs. global analysis: Layer-wise provides more granular insights but increases complexity; global analysis is simpler but may miss important patterns
- Failure signatures:
  - Gradients not being extracted correctly: Check that the gradient extraction hooks are properly attached to Q, K, V, O layers
  - Nuclear norm values unexpectedly small or large: Verify gradient scaling and that the loss function is appropriate for the task
  - No difference between conditions: Ensure that the irrelevant responses are truly irrelevant (e.g., shuffled reasoning steps) and that CoT is being generated correctly
- First 3 experiments:
  1. Run a small subset of the AQuA dataset with Qwen2-1.5B base model, comparing None CoT vs. Detailed CoT gradients to verify the instability pattern
  2. Repeat experiment 1 with an instruction-tuned version to check if alignment affects the gradient patterns
  3. Test a knowledge task (e.g., Wiki Popular, length 100) to confirm that response length alone does not replicate the slow-thinking gradient stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do gradient patterns across layers differ between reasoning tasks (math, commonsense) and knowledge learning tasks (wiki) when training with different response lengths or levels of detail?
- Basis in paper: [explicit] The authors observe that slow thinking (detailed CoT) produces stable, uniform gradient norms across layers for reasoning tasks, while fast thinking causes larger gradients and greater variation. However, these patterns do not extend to knowledge learning tasks, where increasing response length alone does not replicate slow-thinking gradient behaviors.
- Why unresolved: The study shows the difference in gradient patterns but doesn't investigate the underlying mechanisms that cause these differences or whether there are intermediate levels of response complexity that might bridge the gap.
- What evidence would resolve it: Experiments systematically varying response length and detail across both task types, combined with analysis of how different layer types (attention vs. feed-forward) respond to these variations, would clarify the fundamental differences in learning dynamics.

### Open Question 2
- Question: Do instruction-tuned LLMs show any advantages over base models in detecting incorrect reasoning paths when trained with detailed CoT?
- Basis in paper: [explicit] The authors find that instruction-tuned models do not show superior capability over base models in identifying incorrect reasoning paths, despite their alignment training.
- Why unresolved: The study only compares gradient patterns and doesn't examine whether instruction-tuned models might use different internal mechanisms to achieve similar gradient behaviors, or whether other alignment techniques might show advantages.
- What evidence would resolve it: Direct comparison of model performance on detecting incorrect reasoning paths, combined with interpretability analysis of attention patterns and internal representations, would reveal whether instruction-tuned models have hidden advantages.

### Open Question 3
- Question: What specific layer types or positions are most sensitive to training with irrelevant responses, and how does this vary across different initial model types?
- Basis in paper: [explicit] The authors find that earlier layers show greater sensitivity to irrelevant responses when training with detailed CoT, but this pattern doesn't extend to knowledge learning tasks.
- Why unresolved: The study identifies general trends but doesn't pinpoint specific layer types (attention heads, feed-forward networks) or provide a mechanistic explanation for why earlier layers are more sensitive to irrelevant responses.
- What evidence would resolve it: Layer-wise ablation studies and targeted analysis of attention head behavior would reveal which components are most critical for detecting irrelevant responses and how this varies across model architectures.

## Limitations
- The findings rely on controlled gradient analysis using specific model architectures and datasets, which may not generalize across larger models (beyond 8B parameters) and different training paradigms
- The study uses nuclear norm as a primary metric, but its relationship to actual training stability and convergence remains somewhat theoretical
- The generation of irrelevant responses through shuffling may not capture all forms of response irrelevance that models encounter in real-world applications

## Confidence
- **High Confidence**: The observation that slow thinking produces more stable, uniform gradient norms across layers is well-supported by the spectral analysis and consistent across multiple experiments
- **Medium Confidence**: The claim that instruction-tuned models show no advantage over base models in detecting irrelevant responses is supported, but the sample size and model diversity may limit generalizability
- **Low Confidence**: The specific mechanism by which CoT enables gradient-based correctness detection is proposed but not fully validated through ablation studies or alternative explanation testing

## Next Checks
1. Test whether the gradient stability advantage of slow thinking persists when using alternative gradient metrics (e.g., Frobenius norm, spectral radius) to rule out metric-specific artifacts
2. Conduct ablation studies where CoT steps are progressively removed to determine the minimum reasoning depth needed for gradient stability benefits
3. Validate findings on larger models (e.g., 70B+ parameters) and with different training objectives to assess scalability of the observed patterns