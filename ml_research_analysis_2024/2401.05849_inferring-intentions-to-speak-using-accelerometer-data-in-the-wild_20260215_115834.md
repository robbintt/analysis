---
ver: rpa2
title: Inferring Intentions to Speak Using Accelerometer Data In-the-Wild
arxiv_id: '2401.05849'
source_url: https://arxiv.org/abs/2401.05849
tags:
- intentions
- speak
- data
- speaker
- unsuccessful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores inferring intentions to speak from accelerometer
  data in the wild. The authors propose training a residual neural network on successful
  intentions to speak and evaluating it on both successful and unsuccessful cases.
---

# Inferring Intentions to Speak Using Accelerometer Data In-the-Wild

## Quick Facts
- arXiv ID: 2401.05849
- Source URL: https://arxiv.org/abs/2401.05849
- Authors: Litian Li; Jord Molhoek; Jing Zhou
- Reference count: 40
- Primary result: Accelerometer data alone shows limited ability to predict intentions to speak, with AUC scores close to random chance for most time windows.

## Executive Summary
This work explores whether accelerometer data can predict intentions to speak in real-world social settings. The authors trained a residual neural network on accelerometer signals preceding speech, using voice activity detection from microphone data to identify successful cases and manual annotation for unsuccessful cases. They evaluated their model on both types of cases using different time windows. The results indicate that while accelerometer data captures some information about speaking intentions, it is insufficient for reliable prediction, suggesting that multi-modal approaches incorporating audio and video data may be necessary.

## Method Summary
The study uses accelerometer data from the REWIND dataset collected at a social networking event. Successful intentions to speak are identified using voice activity detection from synchronized microphone data, creating positive samples from time windows before speaking onset. Unsuccessful intentions are manually annotated from a 10-minute segment. A residual neural network is trained on successful cases and evaluated on both successful and unsuccessful cases using 3-fold cross-validation. The model's performance is measured using AUC scores across different time windows (1-4 seconds) before speaking.

## Key Results
- The model performs better than random guessing for 1-2 second time windows but performance decreases for longer windows
- AUC scores for unsuccessful "INTS start" cases are slightly better than for "INTS continue" cases
- Overall, accelerometer data shows limited ability to distinguish between successful and unsuccessful intentions to speak

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Accelerometer data captures body movement patterns correlated with intentions to speak.
- Mechanism: Body movements like posture shifts and breath patterns manifest before speech and are detectable as changes in accelerometer signals.
- Core assumption: These movements are sufficiently distinct from other movements to be detectable.
- Evidence anchors:
  - [abstract] "posture shifts are correlated with intentions to speak"
  - [section] "the accelerometer data that can be collected may be affected by the placement of the sensor and the degree of interaction between other body movements"
  - [corpus] Weak - corpus papers focus on different tasks like bed detection, not speaking intention
- Break condition: If body movements are too subtle or masked by unrelated movements, the signal becomes indistinguishable.

### Mechanism 2
- Claim: A residual neural network can learn temporal patterns in accelerometer data that precede speech.
- Mechanism: The model learns to recognize short time windows (1-2 seconds) of accelerometer patterns that correlate with successful and unsuccessful intentions to speak.
- Core assumption: The temporal patterns are consistent enough across different individuals to be learned.
- Evidence anchors:
  - [section] "we refactored the model by only using the accelerometer data as input of the residual neural network model"
  - [section] "The model is trained on the successful intentions to speak and evaluated on both the successful and unsuccessful cases"
  - [corpus] Weak - no direct corpus evidence for this specific architecture on speaking intention task
- Break condition: If the patterns vary too much between individuals or contexts, the model cannot generalize.

### Mechanism 3
- Claim: Manually annotated unsuccessful intentions to speak provide ground truth for model evaluation.
- Mechanism: Annotators identify perceived intentions to speak from video and audio, creating labeled data for model validation.
- Core assumption: Annotators can reliably perceive and identify intentions to speak from observable cues.
- Evidence anchors:
  - [section] "one (native Dutch) of the authors of this work performed the annotation"
  - [section] "For this, the elan software was used [28]"
  - [corpus] Weak - no corpus evidence on annotation reliability for speaking intentions
- Break condition: If annotators disagree significantly or miss cases, the ground truth becomes unreliable.

## Foundational Learning

- Residual neural networks
  - Why needed here: They can learn temporal patterns in sequential accelerometer data while preserving gradient flow through residual connections.
  - Quick check question: What advantage do residual connections provide in deep networks compared to plain convolutional networks?

- Voice activity detection (VAD)
  - Why needed here: VAD is used to extract ground truth labels for successful intentions to speak from microphone data.
  - Quick check question: How does VAD differentiate between speech and non-speech audio segments?

- Area Under the Curve (AUC) metric
  - Why needed here: AUC provides a threshold-independent measure of model performance that's invariant to class prior probabilities.
  - Quick check question: Why is AUC preferred over accuracy when dealing with imbalanced classes?

## Architecture Onboarding

- Component map: Accelerometer data → Preprocessing (window extraction) → Residual neural network → Prediction → AUC evaluation
- Critical path: Data preprocessing → Model training → Evaluation on test set
- Design tradeoffs: Using only accelerometer data preserves privacy but limits information compared to multi-modal approaches; shorter time windows capture more specific patterns but may miss longer-term intentions.
- Failure signatures: AUC scores close to 0.5 indicate the model performs no better than random guessing; decreasing AUC with longer time windows suggests temporal patterns are concentrated just before speech.
- First 3 experiments:
  1. Train and evaluate on 1-second time windows to find optimal window size
  2. Compare model performance on successful vs unsuccessful intention cases
  3. Test model on annotated unsuccessful "INTS start" vs "INTS continue" cases to identify differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does the cultural background of participants influence the effectiveness of using accelerometer data to predict intentions to speak?
- Basis in paper: [inferred] The authors mention that the dataset used is fully set in Dutch, implying a single cultural background, and suggest that people with different cultural backgrounds tend to behave with different social cues.
- Why unresolved: The study only uses a dataset from a single cultural background, and does not explore how cultural differences might affect the patterns of intentions to speak detectable by accelerometers.
- What evidence would resolve it: Conducting similar research with data and annotators from countries other than the Netherlands and comparing the results to identify cultural differences in accelerometer patterns related to speaking intentions.

### Open Question 2
- Question: How does the granularity of the time window affect the accuracy of predicting intentions to speak using accelerometer data?
- Basis in paper: [explicit] The authors mention that the current granularity of the time window is based on the synchronized level of the REWIND dataset and suggest that detectable intentions could appear within a very small time interval.
- Why unresolved: The study only tests time windows of 1, 2, 3, and 4 seconds, and does not explore the impact of smaller or larger time windows on the model's performance.
- What evidence would resolve it: Testing the model with smaller scales of granularity and comparing the results to determine the optimal time window size for predicting intentions to speak.

### Open Question 3
- Question: How does the placement of the accelerometer sensor affect the ability to detect intentions to speak?
- Basis in paper: [inferred] The authors mention that the accelerometer data that can be collected may be affected by the placement of the sensor and the degree of interaction between other body movements.
- Why unresolved: The study does not explore the impact of sensor placement on the quality and reliability of the accelerometer data for predicting intentions to speak.
- What evidence would resolve it: Conducting experiments with accelerometers placed in different locations on the body and comparing the results to determine the optimal sensor placement for detecting intentions to speak.

## Limitations

- Single accelerometer sensor placement limits the range of detectable body movements
- Manually annotated ground truth for unsuccessful intentions represents only 10 minutes of data from one participant
- Voice activity detection assumes perfect performance which may not hold in noisy environments

## Confidence

**High Confidence**: The finding that accelerometer data alone cannot reliably distinguish between successful and unsuccessful intentions to speak is well-supported by the experimental results showing AUC scores close to random chance for most time windows.

**Medium Confidence**: The claim that body movements like posture shifts correlate with intentions to speak is supported by the literature but the specific patterns learned by the model are not fully characterized or validated against alternative explanations.

**Low Confidence**: The assertion that multi-modal approaches combining audio, video, and accelerometer data would significantly improve performance remains speculative without experimental validation of such hybrid systems.

## Next Checks

1. **Ground Truth Quality Assessment**: Evaluate the inter-rater reliability of the manual annotations for unsuccessful intentions to speak by having multiple annotators label the same 10-minute segment and calculating agreement metrics (Cohen's kappa, Fleiss' kappa).

2. **Sensor Placement Impact Study**: Repeat the experiments with accelerometer sensors placed on different body locations (wrist, thigh, ankle) to determine if certain placements capture more discriminative movement patterns for speaking intention detection.

3. **Multi-Modal Fusion Validation**: Implement a preliminary multi-modal system combining accelerometer data with audio features from the microphone, then conduct an ablation study to quantify the contribution of each modality to intention prediction accuracy.