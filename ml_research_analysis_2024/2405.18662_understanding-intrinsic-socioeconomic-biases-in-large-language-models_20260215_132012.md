---
ver: rpa2
title: Understanding Intrinsic Socioeconomic Biases in Large Language Models
arxiv_id: '2405.18662'
source_url: https://arxiv.org/abs/2405.18662
tags: []
core_contribution: This paper introduces a novel dataset to evaluate socioeconomic
  biases in large language models (LLMs) across demographic groups. The authors assess
  four LLMs (Falcon, Llama 2, GPT-2, and BERT) using a fill-in-the-blank task with
  masked tokens representing financial status terms.
---

# Understanding Intrinsic Socioeconomic Biases in Large Language Models

## Quick Facts
- arXiv ID: 2405.18662
- Source URL: https://arxiv.org/abs/2405.18662
- Reference count: 13
- Four LLMs evaluated: Falcon, Llama 2, GPT-2, and BERT show pervasive socioeconomic biases

## Executive Summary
This paper introduces a novel dataset to evaluate socioeconomic biases in large language models across demographic groups. The authors assess four LLMs using a fill-in-the-blank task with masked tokens representing financial status terms. Results show pervasive socioeconomic biases in all models, with biases significantly amplified when considering intersectionality between gender, race, and marital status. The study also demonstrates that LLMs can extract demographic attributes from names and exhibit biases towards them.

## Method Summary
The authors created a dataset of one million English sentences with masked tokens representing financial status terms (poor/rich). Four LLMs (Falcon, Llama 2, GPT-2, and BERT) were evaluated using this dataset with demographic terms for gender, marital status, race, and religion. The evaluation used three metrics: Language Model Coherence Score (LMCS), Poverty Association Ratio (PAR), and EquiLexi Score (ELS). The study examined both individual demographic attributes and intersectional combinations to measure bias amplification.

## Key Results
- All four LLMs exhibit significant socioeconomic biases when predicting financial status terms
- Intersectionality amplifies socioeconomic biases, with models extracting multiple demographic attributes from names and correlating them with specific socioeconomic biases
- Falcon and Llama 2 show the highest socioeconomic biases, while BERT shows lower bias but also lower language model coherence

## Why This Works (Mechanism)

### Mechanism 1
Names serve as strong semantic cues that LLMs associate with demographic groups (gender, race), which then trigger pre-existing socioeconomic bias patterns encoded during training. The training corpus contains sufficient correlations between names and demographic attributes, allowing LLMs to implicitly learn these associations. This mechanism would fail if names lack consistent demographic associations in the training data.

### Mechanism 2
Combining multiple demographic attributes creates compounded bias effects where the model's prediction shifts further toward extreme socioeconomic associations than any single attribute would cause. The model's learned representations treat intersectional combinations as multiplicative rather than additive bias factors. This amplification effect could be reduced if the model's architecture normalizes intersectional effects.

### Mechanism 3
Autoregressive models like Falcon and Llama 2, trained on diverse web data, reflect more complex socioeconomic biases than BERT, which is trained on cleaner, structured sources like Wikipedia and BookCorpus. The diversity and cleanliness of training data directly correlate with the richness and accuracy of bias patterns the model learns and reproduces. This correlation could be weakened if bias mitigation techniques are applied post-training.

## Foundational Learning

- Concept: Understanding socioeconomic bias measurement metrics
  - Why needed here: The paper introduces specific metrics (LMCS, PAR, ELS) to quantify bias that are essential for interpreting results
  - Quick check question: How does PAR differ from traditional bias metrics that compare stereotypical vs. anti-stereotypical associations?

- Concept: Intersectionality in bias research
  - Why needed here: The study examines how multiple demographic attributes compound bias effects, a key methodological contribution
  - Quick check question: Why might combining "indigenous mother" produce different bias effects than analyzing "indigenous" and "mother" separately?

- Concept: Zero-shot learning for attribute extraction
  - Why needed here: The paper uses zero-shot approaches to evaluate LLMs' ability to infer demographic attributes from names without fine-tuning
  - Quick check question: What are the advantages and limitations of using zero-shot methods for bias evaluation compared to supervised approaches?

## Architecture Onboarding

- Component map: Dataset generation -> Template creation with perturbations -> LLM inference (mask filling) -> Bias measurement (LMCS, PAR, ELS) -> Results analysis
- Critical path: Generate templates → Apply perturbations → Run LLM inference → Calculate metrics → Analyze results. The most critical path is ensuring template perturbations maintain semantic integrity while creating robustness against prompt sensitivity.
- Design tradeoffs: Fill-in-the-blank tasks provide controlled bias measurement but may not capture all forms of bias present in natural language generation. Autoregressive vs. bidirectional models affects both bias measurement capability and interpretability.
- Failure signatures: Low LMCS scores suggest template quality issues or model incoherence. Uniformly high PAR scores indicate systematic bias rather than demographic-specific effects. Inconsistent results across perturbations suggest prompt sensitivity issues.
- First 3 experiments:
  1. Run a baseline test with neutral terms only to establish the "Neutral Level" for each model
  2. Test individual demographic attributes (gender only, race only) to understand baseline bias levels
  3. Test intersectional combinations to verify the amplification effect and identify the most problematic combinations

## Open Questions the Paper Calls Out

### Open Question 1
How do the socioeconomic biases in LLMs impact downstream tasks in critical domains such as loan approvals and visa applications? The paper identifies potential harm but does not empirically test how these biases manifest in real-world decision-making systems. Empirical studies measuring the impact of biased LLM outputs on downstream task performance would resolve this.

### Open Question 2
What are the underlying causes of the differences in socioeconomic bias between autoregressive models (Falcon, Llama 2, GPT-2) and BERT? The paper presents hypotheses but does not conduct experiments to isolate specific contributing factors. Controlled experiments varying training data characteristics and model architectures would resolve this.

### Open Question 3
How can socioeconomic biases in LLMs be effectively mitigated, particularly considering the amplification of biases through intersectionality? The paper highlights the need for bias mitigation techniques but does not explore specific strategies or their effectiveness. Development and evaluation of bias mitigation techniques tailored to socioeconomic biases would resolve this.

## Limitations

- Template-based approach may not capture all real-world manifestations of socioeconomic bias
- Static dataset may not reflect evolving societal dynamics and language patterns
- Assumes multiplicative rather than complex interaction patterns for intersectional attributes

## Confidence

**High Confidence:**
- Existence of measurable socioeconomic biases across all evaluated LLMs
- Amplification of bias through intersectional demographic combinations
- Differential bias levels between model architectures

**Medium Confidence:**
- Specific magnitude of bias differences between models
- Relative effectiveness of different models for bias mitigation
- Generalizability of findings to other LLM architectures

**Low Confidence:**
- Predictions about real-world deployment risks without application-specific contexts
- Permanence of observed bias patterns over time
- Universal applicability of evaluation framework to all scenarios

## Next Checks

1. Cross-validate findings using open-ended text generation tasks to verify template-based measurements generalize to natural language scenarios
2. Evaluate the same LLMs and metrics after 3-6 months to assess temporal stability of socioeconomic biases
3. Implement LLMs in specific socioeconomic decision-making contexts (e.g., loan processing, job screening) to measure actual bias impact versus controlled experimental measurements