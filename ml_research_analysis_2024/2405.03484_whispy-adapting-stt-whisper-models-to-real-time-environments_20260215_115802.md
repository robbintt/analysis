---
ver: rpa2
title: 'Whispy: Adapting STT Whisper Models to Real-Time Environments'
arxiv_id: '2405.03484'
source_url: https://arxiv.org/abs/2405.03484
tags:
- whispy
- audio
- transcription
- whisper
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Whispy introduces a real-time adaptation of Whisper models by processing
  audio in short overlapping chunks within a shifting buffer. It uses Levenshtein
  distance-based suggestion generation to merge transcriptions and includes hallucination
  filtering.
---

# Whispy: Adapting STT Whisper Models to Real-Time Environments

## Quick Facts
- arXiv ID: 2405.03484
- Source URL: https://arxiv.org/abs/2405.03484
- Authors: Antonio Bevilacqua; Paolo Saviano; Alessandro Amirante; Simon Pietro Romano
- Reference count: 31
- WER within 1-2% of offline Whisper across multiple datasets

## Executive Summary
Whispy introduces a real-time adaptation of Whisper models by processing audio in short overlapping chunks within a shifting buffer. It uses Levenshtein distance-based suggestion generation to merge transcriptions and includes hallucination filtering. Tested across multiple ASR datasets (ESIC, LibriSpeech, TEDlium, Rev16), Whispy achieves Word Error Rates within 1-2% of offline Whisper, with delays between 0.44 and 1.66 seconds depending on model size. Latency can be tuned via chunk length and buffer size. Results show strong robustness, low latency, and competitive accuracy for live transcription.

## Method Summary
Whispy processes audio in overlapping chunks using a shifting buffer, where each chunk undergoes Whisper inference with overlapping regions to maintain context. The system employs Levenshtein distance-based suggestion generation to merge partial transcriptions, filtering hallucinations by comparing consecutive outputs. Latency is tunable through chunk length and buffer size parameters, allowing trade-offs between responsiveness and accuracy.

## Key Results
- WER within 1-2% of offline Whisper across ESIC, LibriSpeech, TEDlium, and Rev16 datasets
- Latency ranges from 0.44 to 1.66 seconds depending on model size
- Cross-dataset robustness demonstrates adaptability to different speech patterns and domains

## Why This Works (Mechanism)
The overlapping chunk processing preserves contextual information that would otherwise be lost in strict segmentation. Levenshtein distance filtering effectively identifies and removes hallucinated content by comparing consecutive transcriptions. The shifting buffer maintains a continuous stream of audio while allowing sufficient context for each chunk's inference, balancing latency and accuracy.

## Foundational Learning
- **Chunk-based streaming ASR**: Why needed - enables real-time processing without waiting for full audio. Quick check - verify chunk boundaries don't split words.
- **Overlapping regions**: Why needed - preserves context between chunks for coherent transcription. Quick check - measure WER improvement with vs without overlap.
- **Levenshtein distance filtering**: Why needed - detects and removes hallucinated text between consecutive chunks. Quick check - count hallucinations filtered vs total.
- **Shifting buffer architecture**: Why needed - maintains continuous audio stream while processing chunks. Quick check - verify buffer size affects latency predictably.

## Architecture Onboarding

**Component Map**: Audio Stream -> Chunk Splitter -> Overlap Buffer -> Whisper Inference -> Levenshtein Filter -> Transcription Merger

**Critical Path**: The core processing chain involves audio chunking with overlap, Whisper inference on each chunk, Levenshtein-based hallucination filtering, and merging of partial transcriptions into final output.

**Design Tradeoffs**: Fixed overlap (15-30%) balances context preservation with computational overhead. Buffer size tuning allows latency-accuracy trade-offs. Levenshtein filtering adds computational cost but improves output quality.

**Failure Signatures**: Excessive chunk size causes high latency; insufficient overlap leads to context loss and transcription errors; aggressive filtering may remove valid content; buffer overflow during high speech rates.

**First Experiments**: 1) Measure WER with varying overlap percentages (10%, 20%, 30%). 2) Test latency at different buffer sizes. 3) Evaluate hallucination filtering effectiveness on synthetic hallucinated data.

## Open Questions the Paper Calls Out
None

## Limitations
- Levenshtein filtering adds computational overhead during real-time processing
- Fixed overlap strategy may be suboptimal for varying speech rates and accents
- Evaluation focuses on WER metrics without examining semantic preservation

## Confidence

**Confidence Labels:**
- Real-time accuracy claims (WER within 1-2% of offline): High
- Latency measurements (0.44-1.66s): Medium (depends on hardware specifics)
- Hallucination filtering effectiveness: Medium (limited qualitative analysis)
- Cross-dataset robustness: Medium (four datasets provide reasonable but not exhaustive coverage)

## Next Checks

1. Test Whispy's performance on additional diverse datasets including spontaneous conversational speech and accented English to verify claimed robustness
2. Measure computational overhead and memory usage during real-time processing to assess deployment feasibility on embedded/mobile devices
3. Evaluate semantic preservation by comparing transcriptions against reference transcripts for meaning retention using automated metrics like BERTScore