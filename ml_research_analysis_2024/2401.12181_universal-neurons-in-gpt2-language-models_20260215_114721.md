---
ver: rpa2
title: Universal Neurons in GPT2 Language Models
arxiv_id: '2401.12181'
source_url: https://arxiv.org/abs/2401.12181
tags:
- neurons
- neuron
- arxiv
- figure
- universal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies the universality of individual neurons in GPT2
  language models, finding that only 1-5% of neurons are universal across models trained
  from different random seeds. The authors compute pairwise correlations of neuron
  activations over 100 million tokens for every neuron pair across five different
  seeds.
---

# Universal Neurons in GPT2 Language Models

## Quick Facts
- arXiv ID: 2401.12181
- Source URL: https://arxiv.org/abs/2401.12181
- Authors: Wes Gurnee; Theo Horsley; Zifan Carl Guo; Tara Rezaei Kheirkhah; Qinyi Sun; Will Hathaway; Neel Nanda; Dimitris Bertsimas
- Reference count: 25
- Primary result: Only 1-5% of neurons are universal across GPT2 models trained from different random seeds

## Executive Summary
This paper studies the universality of individual neurons in GPT2 language models by computing pairwise correlations of neuron activations across five models trained from different random seeds. The authors find that only 1-5% of neurons are universal, consistently activating on the same inputs across models. These universal neurons are more likely to be interpretable and can be categorized into functional families that perform specific roles such as deactivating attention heads, changing entropy of next token distributions, and predicting whether the next token belongs to particular sets.

## Method Summary
The authors compute pairwise Pearson correlations of neuron activations over 100 million tokens for every neuron pair across five GPT2 models (small, medium, and Pythia-160m) trained from different random seeds on the Pile dataset. Universal neurons are identified as those with excess correlation (maximum correlation minus baseline correlation) greater than 0.5. The study analyzes statistical properties of universal neurons including weight norms, activation distributions, and functional roles through weight analysis and causal interventions.

## Key Results
- Only 1-5% of neurons (1253 out of 98,304 in GPT2-medium) are universal across models trained from different random seeds
- Universal neurons are more interpretable and form a small number of functional neuron families
- Identified functional roles include attention head deactivation, entropy modulation, and prediction of token set membership
- Universal neurons typically have large weight norms and negative input biases, resulting in large negative pre-activation means

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Universality emerges from redundancy in specialized feature detection across independent random seeds
- Mechanism: When training from different random seeds, the model converges to similar internal representations for core linguistic features, leading to overlapping activation patterns in certain neurons
- Core assumption: The training data and loss function constrain the model to discover the same fundamental features despite different initialization
- Evidence anchors:
  - [abstract] "we compute pairwise correlations of neuron activations over 100 million tokens for every neuron pair across five different seeds and find that 1-5% of neurons are universal"
  - [section 4.1] "only 1253 out of the 98304 neurons in GPT2-medium-a have an excess correlation greater than 0.5"
  - [corpus] Weak evidence - corpus neighbors focus on brain-inspired models rather than neural network universality

### Mechanism 2
- Claim: Universal neurons are more interpretable because they represent monosemantic features
- Mechanism: Neurons with consistent activation patterns across models are less likely to be polysemantic, making them easier to interpret through their activation and weight patterns
- Core assumption: Polysemantic neurons would have varying activation patterns across models, while monosemantic neurons would have consistent patterns
- Evidence anchors:
  - [abstract] "universal neurons were more likely to be interpretable"
  - [section 4.2] "universal neurons typically have large weight norm... and have a large negative input bias, resulting in a large negative pre-activation mean"
  - [corpus] No direct corpus evidence for this specific claim about interpretability

### Mechanism 3
- Claim: Universal neurons develop specialized functional roles in simple circuits through weight patterns
- Mechanism: The compositional structure of weights in universal neurons creates consistent effects on downstream components, forming identifiable functional patterns like prediction, suppression, and entropy modulation
- Core assumption: The weight patterns in universal neurons are preserved across models due to similar optimization pressures
- Evidence anchors:
  - [abstract] "we study patterns in neuron weights to establish several universal functional roles of neurons in simple circuits: deactivating attention heads, changing the entropy of the next token distribution, and predicting the next token to (not) be within a particular set"
  - [section 5.1] "we find many examples of prediction neurons that positively increase the predicted probability of a coherent set of tokens"
  - [corpus] No corpus evidence for this specific mechanism of circuit formation

## Foundational Learning

- Concept: Neuron activation correlation computation
  - Why needed here: Understanding how universality is measured through pairwise neuron activation correlations across models
  - Quick check question: What mathematical operation is used to measure the similarity between neuron activation patterns across different models?

- Concept: Transformer architecture and neuron computation
  - Why needed here: Understanding how individual neurons function within the MLP layers and how their weights affect downstream predictions
  - Quick check question: How does the GeLU activation function affect the computation of individual neurons in transformer models?

- Concept: Statistical analysis of weight distributions
  - Why needed here: Understanding how weight properties like norm, bias, and cosine similarity relate to neuron interpretability and function
  - Quick check question: What does a high kurtosis in neuron weight distributions typically indicate about the neuron's behavior?

## Architecture Onboarding

- Component map:
  GPT2 model architecture with focus on MLP layers containing neurons -> Token embedding and unembedding matrices -> Layer normalization layers -> Attention mechanisms -> Weight preprocessing steps

- Critical path:
  1. Load pre-trained GPT2 models from different seeds
  2. Compute neuron activations over large text corpus
  3. Calculate pairwise correlation matrices between all neurons across models
  4. Identify universal neurons based on correlation thresholds
  5. Analyze weight and activation properties of universal neurons
  6. Test functional roles through causal interventions

- Design tradeoffs:
  - Correlation threshold selection: Higher thresholds give fewer but more reliable universal neurons
  - Dataset size: Larger datasets provide more stable correlation estimates but require more computation
  - Model size: Smaller models are faster to analyze but may have different universality properties than larger models

- Failure signatures:
  - No neurons passing correlation threshold: Could indicate dataset issues or fundamental differences in learned representations
  - High correlation in rotated basis: Suggests no privileged neuron basis, making interpretation meaningless
  - Inconsistent weight patterns: Could indicate different optimization paths leading to same functional outcomes

- First 3 experiments:
  1. Replicate correlation analysis on a smaller dataset to verify methodology
  2. Test universality on models with different random seeds but same architecture
  3. Compare universality between different model sizes (small vs medium) to check scale effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the true mechanism behind the observed entropy-modulating neurons, and how do they interact with the model's overall uncertainty quantification?
- Basis in paper: [explicit] The authors hypothesize that these neurons modulate the model's uncertainty over the next token by using the layer norm to squeeze the logit distribution, similar to manually increasing the temperature during inference
- Why unresolved: The authors note that their experiments with fixed activation values do not necessarily imply that the model uses these neurons to increase the entropy as a general uncertainty mechanism. They observed cases where increasing the activation of the entropy neuron decreased entropy, suggesting a more complex mechanism
- What evidence would resolve it: Further experiments that manipulate the activation of these neurons during the model's training process, rather than just causal interventions, could reveal their true role in uncertainty quantification. Additionally, studying the behavior of these neurons across different model architectures and datasets could provide insights into their generalizability

### Open Question 2
- Question: How does the number of universal neurons change as model size increases, and what factors contribute to this relationship?
- Basis in paper: [explicit] The authors observe that GPT2-small-a has a higher percentage of universal neurons (4.16%) compared to GPT2-medium and Pythia-160M (1.23% and 1.26%, respectively). They speculate that the number of universal neurons may decrease in larger models but lack sufficient evidence
- Why unresolved: The authors did not have access to larger models trained from random seeds to test this hypothesis. Additionally, other factors such as model architecture, training data, and optimization techniques could also influence the number of universal neurons
- What evidence would resolve it: Conducting similar experiments on larger models with different architectures, trained from random seeds, and trained on diverse datasets would provide insights into the relationship between model size and the number of universal neurons. Comparing the results across these different settings could help identify the key factors influencing this relationship

### Open Question 3
- Question: What is the role of attention deactivation neurons in the overall attention mechanism, and how do they contribute to the model's performance?
- Basis in paper: [explicit] The authors hypothesize that these neurons control the extent to which attention heads attend to the beginning-of-sequence (BOS) token, effectively turning the head on or off. They observe that increasing the activation of these neurons increases the attention to BOS, reducing the output norm of the head
- Why unresolved: While the authors provide evidence for the existence of these neurons and their effect on attention patterns, the broader implications of this mechanism on the model's performance and the specific tasks it enables remain unclear
- What evidence would resolve it: Ablating these neurons or modifying their behavior during the model's training process could reveal their impact on the model's performance across various tasks. Additionally, studying the interaction between these neurons and other attention mechanisms, such as induction heads, could provide insights into their role in the overall attention mechanism

## Limitations
- Only 1-5% of neurons are universal across models, limiting the scope of interpretability approaches based on individual neuron analysis
- Correlation-based methodology assumes consistent activation patterns indicate monosemantic features without rigorous theoretical justification
- Functional role analysis through weight patterns and causal interventions provides suggestive but not definitive causal evidence

## Confidence
- Neuron universality correlation findings: **High** - The methodology is straightforward and the results are reproducible
- Universality implies interpretability: **Medium** - Supported by empirical observations but lacks theoretical justification
- Weight patterns determine functional roles: **Medium** - Correlation patterns are clear but causal relationships are not fully established

## Next Checks
1. Replicate the universality analysis on GPT3 or larger models to test scalability of findings
2. Conduct ablation studies removing universal neurons to measure impact on model performance
3. Compare universality findings across different training datasets to test sensitivity to data distribution