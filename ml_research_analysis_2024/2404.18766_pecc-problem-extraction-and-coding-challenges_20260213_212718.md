---
ver: rpa2
title: 'PECC: Problem Extraction and Coding Challenges'
arxiv_id: '2404.18766'
source_url: https://arxiv.org/abs/2404.18766
tags:
- code
- problems
- problem
- euler
- challenges
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PECC, a benchmark for evaluating large language
  models on narrative-embedded coding problems that require problem understanding,
  requirement extraction, and code generation. PECC consists of 2,396 problems derived
  from Advent of Code and Project Euler challenges, transformed into both narrative
  and neutral styles.
---

# PECC: Problem Extraction and Coding Challenges

## Quick Facts
- arXiv ID: 2404.18766
- Source URL: https://arxiv.org/abs/2404.18766
- Reference count: 2
- PECC benchmark evaluates LLMs on narrative-embedded coding problems, with GPT-3.5-Turbo achieving 50% Pass@3 on Advent of Code but only 8% on Project Euler

## Executive Summary
PECC is a novel benchmark for evaluating large language models on narrative-embedded coding problems that require problem understanding, requirement extraction, and code generation. The benchmark contains 2,396 problems from Advent of Code and Project Euler, transformed into both narrative and neutral styles. The authors evaluate multiple state-of-the-art models using zero-shot prompting and Pass@k metrics. Results show that models perform better on narrative-style problems than neutral ones, with significant challenges on complex mathematical problems. The study demonstrates that while LLMs can handle story-driven coding tasks reasonably well, they struggle with precise mathematical problems even when presented in narrative form.

## Method Summary
The PECC benchmark evaluates LLMs on coding problems requiring understanding of narrative-embedded requirements and generation of executable Python code. The benchmark contains 2,396 problems from Advent of Code and Project Euler, presented in both narrative and neutral styles. Models are evaluated using zero-shot prompting with Pass@k metrics, where k code samples are generated and execution occurs in an isolated Python environment with 60-second timeout. The evaluation framework classifies errors into five types: Syntax Error, Runtime Error, Timeout Error, Wrong Output, and Part1 Failed. Different prompt formats are used for chat-based AoC problems versus instruction-based Euler problems.

## Key Results
- GPT-3.5-Turbo achieves 50% Pass@3 on narrative-style Advent of Code problems but only 8% on neutral-style Project Euler problems
- Multi-sampling with k=3 significantly improves Pass@k scores compared to k=1 across all models
- Chain-of-thought prompting enhances performance across difficulty levels, enabling GPT-3.5-Turbo to solve >80% of simple problems and some harder ones
- Models show consistent difficulty with precise mathematical problems regardless of narrative formatting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs perform better on narrative-style problems than neutral ones because stories provide contextual scaffolding that aids comprehension and requirement extraction.
- Mechanism: Narrative context reduces ambiguity in problem specification by embedding requirements within a coherent story, allowing models to leverage world knowledge and implicit understanding.
- Core assumption: Models trained on web data have implicit knowledge of common narrative structures and can map story elements to coding requirements.
- Evidence anchors: Results show varying model performance between narrative and neutral problems, with specific challenges in the Euler math-based subset; the story-driven subset AoC proved better suited for models than its neutrally formulated counterpart.

### Mechanism 2
- Claim: Multi-sampling (Pass@k) increases success rates because different generations explore different solution paths and error patterns.
- Mechanism: Independent code generations provide multiple attempts to overcome specific failure modes like syntax errors, runtime errors, or logical mistakes in a single attempt.
- Core assumption: The model's stochastic generation process produces diverse outputs that may succeed where others fail, particularly for complex problems with multiple valid approaches.
- Evidence anchors: Employing a multi-sampling approach with k = 3 generally improves the Pass@k scores compared to when k = 1; the increment in scores from k = 1 to k = 3 suggests that providing models with multiple attempts to solve a problem enhances the likelihood of generating a correct solution.

### Mechanism 3
- Claim: Chain-of-thought prompting improves performance by forcing explicit reasoning steps that reveal logical gaps before code generation.
- Mechanism: Decomposing problems into intermediate reasoning steps allows models to catch errors early and structure solutions more systematically.
- Core assumption: LLMs can benefit from explicit reasoning chains that mirror human problem-solving approaches, particularly for mathematical and algorithmic problems.
- Evidence anchors: Prompting GPT to provide reasons for its solutions enhances performance across all difficulty levels; in the most straightforward problem setting, gpt-3.5-turbo solves > 80% problems, and this technique also enables it to solve some of the more challenging problems.

## Foundational Learning

- Concept: Problem extraction and requirement understanding
  - Why needed here: PECC requires models to interpret narrative-embedded problems and extract precise coding requirements before generating solutions
  - Quick check question: Given a story about elves organizing gifts, what are the three key computational requirements the model must identify?

- Concept: Natural language to code translation
  - Why needed here: The benchmark evaluates models' ability to convert prose descriptions into executable Python code that solves the underlying computational problem
  - Quick check question: How would you convert "find the sum of all multiples of 3 or 5 below 1000" into a Python function?

- Concept: Error analysis and debugging
  - Why needed here: The evaluation framework categorizes multiple error types (syntax, runtime, timeout, wrong output) requiring understanding of debugging strategies
  - Quick check question: If code runs but produces wrong output, what systematic approach would you use to identify the logical error?

## Architecture Onboarding

- Component map: Problem loader -> LLM inference layer -> Isolated Python execution environment -> Result validation -> Error classification -> Pass@k evaluation
- Critical path: Load problem → Generate code via LLM → Execute in sandbox → Capture output → Compare with expected → Classify errors → Aggregate Pass@k metrics
- Design tradeoffs: Zero-shot prompting maximizes generality but sacrifices performance compared to fine-tuning; isolated execution ensures security but adds overhead; narrative augmentation increases realism but may introduce ambiguity
- Failure signatures: Syntax errors indicate generation problems, runtime errors suggest logical flaws, timeouts indicate inefficiency, wrong output shows incorrect algorithms, Part1 failed reveals dependency understanding issues
- First 3 experiments:
  1. Run single Pass@1 evaluation on AoC problems with GPT-3.5-Turbo to establish baseline performance
  2. Test narrative vs neutral formulation impact by running both versions of identical problems
  3. Evaluate chain-of-thought prompting impact on Euler subset to measure reasoning benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs on PECC compare to human performance on similar coding challenges?
- Basis in paper: The paper introduces PECC as a benchmark for evaluating LLMs on narrative-embedded coding problems but does not provide any comparison to human performance.
- Why unresolved: The paper focuses solely on evaluating LLMs and does not discuss or compare their performance to human performance on similar coding challenges.
- What evidence would resolve it: Human performance data on the same coding challenges used in PECC, or a study comparing LLM and human performance on similar tasks.

### Open Question 2
- Question: How does the performance of LLMs on PECC vary across different programming languages?
- Basis in paper: The paper mentions that the dataset allows for universal applicability across various programming languages, but does not provide any specific analysis of LLM performance across different languages.
- Why unresolved: The paper evaluates LLMs on Python code generation but does not explore how their performance might differ when generating code in other programming languages.
- What evidence would resolve it: Evaluation of LLMs on PECC problems using different programming languages, and comparison of their performance across these languages.

### Open Question 3
- Question: How does the performance of LLMs on PECC change with the inclusion of few-shot examples?
- Basis in paper: The paper mentions that they employ a zero-shot format for PECC, but does not explore how the inclusion of few-shot examples might impact model performance.
- Why unresolved: The paper focuses on zero-shot evaluation but does not investigate the potential benefits of few-shot learning for LLMs on PECC problems.
- What evidence would resolve it: Evaluation of LLMs on PECC with few-shot examples and comparison of their performance to the zero-shot results presented in the paper.

## Limitations
- Limited problem diversity: The benchmark draws exclusively from Advent of Code and Project Euler, representing a narrow slice of real-world coding challenges
- Prompt engineering variability: Different prompt formats used for AoC versus Euler problems without systematic exploration of prompt variations
- Zero-shot constraint: Exclusive focus on zero-shot evaluation doesn't reflect practical deployment scenarios where few-shot examples often improve performance

## Confidence

**High Confidence**: The observation that models perform better on narrative-style problems than neutral ones is well-supported by the data, with 50% vs 8% Pass@3 difference between AoC and Euler problems.

**Medium Confidence**: The assertion that multi-sampling improves success rates is reasonably supported but could benefit from additional analysis of optimal k values for different problem types.

**Medium Confidence**: The claim about chain-of-thought prompting improving performance is supported by experimental results, but the mechanism isn't fully explored regarding why reasoning chains help for some problems but not others.

## Next Checks
- Check 1: Prompt Sensitivity Analysis - Systematically vary prompt formulations across all models and problem types to quantify the impact of prompt engineering on Pass@k scores
- Check 2: Cross-Domain Generalization - Test PECC-trained models on coding problems from other domains to assess whether performance on AoC and Euler problems generalizes to broader coding tasks
- Check 3: Few-Shot vs Zero-Shot Comparison - Conduct parallel evaluations using identical problems with minimal few-shot examples to determine whether reported performance gaps can be reduced through prompt augmentation