---
ver: rpa2
title: 'Keep It Private: Unsupervised Privatization of Online Text'
arxiv_id: '2405.10260'
source_url: https://arxiv.org/abs/2405.10260
tags:
- author
- text
- obfuscation
- authorship
- comments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an unsupervised authorship obfuscation method
  using reinforcement learning to fine-tune large language models. The approach rewrites
  text to hide the original author's identity while maintaining meaning and naturalness.
---

# Keep It Private: Unsupervised Privatization of Online Text

## Quick Facts
- arXiv ID: 2405.10260
- Source URL: https://arxiv.org/abs/2405.10260
- Authors: Calvin Bao; Marine Carpuat
- Reference count: 40
- Key outcome: RL-based authorship obfuscation achieves 2% R@8 and 2% MRR in attribution, versus 93% and 83% for unmodified text

## Executive Summary
This paper presents Keep It Private (KiP), an unsupervised method for authorship obfuscation that uses reinforcement learning to fine-tune large language models. The approach rewrites text to hide author identity while maintaining meaning and naturalness. Evaluated on 68k Reddit authors, KiP successfully evades automated authorship attribution and verification attacks more effectively than baseline approaches like round-trip translation and stylometric rewriting. The method achieves strong privacy preservation while maintaining grammatical correctness and semantic similarity.

## Method Summary
KiP uses a reinforcement learning framework with Self-Critical Sequence Training (k-SCST) to optimize language models for authorship obfuscation. The model samples k=8 candidate rewrites per input, computes rewards based on LUAR distance (privacy), SBERT similarity (meaning), and CoLA acceptability (soundness), then applies a self-critical baseline to reinforce high-reward samples. The approach fine-tunes various base generators including GPT2-medium, BART-large, BART-para (paraphrasing fine-tuned), and DIPPER, achieving improved privacy-utility trade-offs compared to vanilla language models.

## Key Results
- KiP models achieve 2% R@8 and 2% MRR in attribution versus 93% and 83% for unmodified text
- Using paraphrasing fine-tuned base models (KiP-BART-Para, KiP-DIPPER) provides the best balance of privacy and meaning preservation
- Human evaluation confirms rewritten outputs are grammatical paraphrases with maintained meaning
- Verification performance degrades slightly with longer author profiles, suggesting robustness limitations

## Why This Works (Mechanism)

### Mechanism 1
The RL framework directly optimizes for authorship obfuscation by balancing privacy, meaning, and soundness in a unified reward. The model samples multiple candidates per input, computes a reward based on LUAR distance (privacy), SBERT similarity (meaning), and CoLA acceptability (soundness), then applies a self-critical baseline to reinforce high-reward samples. Core assumption: the reward components are orthogonal and can be combined via weighted log-sum without destructive interference.

### Mechanism 2
Using paraphrasing fine-tuned base models (BART-Para, DIPPER) improves the trade-off between privacy and meaning preservation compared to vanilla language models. Fine-tuning on paraphrase datasets encourages the base model to produce meaning-preserving edits, which the RL stage then enhances for privacy without destroying semantics. Core assumption: paraphrasing fine-tuning induces latent representations that are more amenable to controlled edits.

### Mechanism 3
Sampling k=8 candidate rewrites per input (k-SCST) stabilizes training by providing a reliable baseline for reward comparison. Multiple samples reduce variance in the reward estimate, making the RL signal more stable and reducing the chance of overfitting to noisy rewards. Core assumption: the mean reward over k samples is a better baseline than a single self-sample.

## Foundational Learning

- Concept: Authorship obfuscation as a multi-objective optimization problem
  - Why needed here: The task requires balancing three competing goals—privacy, meaning preservation, and soundness—so naive single-objective training fails
  - Quick check question: What happens if we train only for privacy (e.g., maximize LUAR distance) without meaning or soundness rewards?

- Concept: Self-Critical Sequence Training (SCST) with sampling
  - Why needed here: Provides a way to optimize non-differentiable rewards in text generation by treating sampled outputs as pseudo-ground truth weighted by reward
  - Quick check question: How does the self-critical baseline reduce variance compared to vanilla REINFORCE?

- Concept: Authorship attribution and verification as evaluation adversaries
  - Why needed here: These tasks simulate real-world privacy threats; obfuscation must fool both attribution (who wrote this) and verification (is this the same author) to be robust
  - Quick check question: Why might a model that fools attribution still fail at verification, and vice versa?

## Architecture Onboarding

- Component map: Base generator (GPT2-medium, BART-large, BART-para, DIPPER) → outputs raw candidate rewrites → Reward functions (LUAR distance, SBERT similarity, CoLA acceptability, guardrails) → compute scalar scores → RL trainer (k-SCST with Lambda optimizer) → updates generator parameters based on reward deltas → Evaluation pipeline (LUAR, VERIF_CNG, human judges) → validates privacy, soundness, and meaning

- Critical path:
  1. Sample k candidates per input
  2. Compute rewards for each candidate
  3. Compute mean reward baseline
  4. Calculate reward delta for each candidate
  5. Backpropagate weighted log-likelihood loss
  6. Update generator parameters

- Design tradeoffs:
  - Sampling k candidates increases privacy robustness but multiplies computation cost
  - Using paraphrasing fine-tuned models improves meaning preservation but may reduce edit diversity
  - Guardrails prevent degenerate outputs but may overly constrain creative rewrites

- Failure signatures:
  - Output length ratio >> 1.4 or << 0.8 → guardrails triggered or model instability
  - LUAR distance close to 1 but SBERT similarity drops sharply → privacy achieved at meaning cost
  - CoLA acceptability drops → model prioritizes privacy over grammaticality

- First 3 experiments:
  1. Run KiP with k=1 (no sampling) vs k=8 to measure variance reduction in privacy metrics
  2. Compare base generators (GPT2 vs BART vs BART-Para) on meaning preservation while keeping privacy constant
  3. Ablation: remove LUAR reward component to see impact on attribution vs verification performance

## Open Questions the Paper Calls Out

### Open Question 1
How robust is the KiP approach to different types of authorship obfuscation attacks beyond LUAR-based and character n-gram similarity methods? The paper notes that privacy improvements come at the cost of a small degradation in meaning preservation and soundness, and that verification performance worsens with longer author profiles. It also states that the KiP models improve performance against both privacy adversaries but sees a larger reduction in meaning preservation with the DIPPER model.

### Open Question 2
How does the KiP approach perform on longer text samples and in different domains beyond short to medium-length Reddit posts? The paper states that the primary evaluation is limited to two English datasets on short to medium-length texts, and acknowledges that the approach should be validated in a broader range of settings.

### Open Question 3
How does the KiP approach impact the readability and coherence of the obfuscated text from a human perspective? While the paper includes a human evaluation to validate the paraphrasing adherence of the obfuscation methods, it does not explicitly assess the readability and coherence of the obfuscated text from a human perspective.

## Limitations

- Privacy mechanism relies heavily on LUAR embeddings with unspecified implementation details
- Method shows degradation in verification performance with longer author profiles
- Human evaluation was conducted on only 100 samples, potentially missing edge cases

## Confidence

- **High confidence**: Meaning preservation metrics (SBERT similarity) and soundness (CoLA scores) show consistent improvement over baselines
- **Medium confidence**: Privacy improvements against attribution attacks (LUAR R@8, MRR) are substantial but depend on LUAR implementation details
- **Low confidence**: Verification robustness across varying profile lengths and general applicability to non-Reddit domains

## Next Checks

1. Implement LUAR embedding generation independently and verify privacy reward computation matches paper results
2. Test KiP models on author profiles of varying lengths (50, 100, 250, 500 words) to quantify the verification performance degradation
3. Evaluate the method on a non-social media corpus (e.g., news articles or academic papers) to assess domain generalization