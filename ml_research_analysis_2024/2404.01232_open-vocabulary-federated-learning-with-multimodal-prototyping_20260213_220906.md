---
ver: rpa2
title: Open-Vocabulary Federated Learning with Multimodal Prototyping
arxiv_id: '2404.01232'
source_url: https://arxiv.org/abs/2404.01232
tags:
- fed-mp
- client
- classes
- test
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Fed-MP, the first vision-language model-based
  federated learning framework explicitly designed for open-vocabulary settings. Fed-MP
  addresses the challenge where new users may query with unseen classes, which traditional
  federated learning approaches cannot handle.
---

# Open-Vocabulary Federated Learning with Multimodal Prototyping

## Quick Facts
- arXiv ID: 2404.01232
- Source URL: https://arxiv.org/abs/2404.01232
- Authors: Huimin Zeng; Zhenrui Yue; Dong Wang
- Reference count: 36
- Primary result: First federated learning framework for open-vocabulary vision tasks with 3% accuracy improvement over baselines

## Executive Summary
This paper introduces Fed-MP, the first federated learning framework designed to handle open-vocabulary vision tasks where test queries may involve classes unseen during training. Traditional federated learning approaches fail in such scenarios because they rely on local models trained on disjoint class sets. Fed-MP addresses this by adaptively aggregating client models based on semantic similarity between client residuals and test queries, while employing a multimodal prototyping mechanism that combines text and visual prototypes for robust prediction on unseen classes.

The method achieves this through a novel architecture that uses CLIP-based vision-language models with light-weight adapter fine-tuning and client-specific residuals that protect privacy while encoding semantic information. Experiments on six diverse image classification datasets demonstrate significant performance improvements over state-of-the-art baselines, with average accuracy gains of 3% while maintaining privacy through minimal parameter updates (less than 0.3% of model parameters).

## Method Summary
Fed-MP extends federated learning to open-vocabulary settings by combining adaptive model aggregation with multimodal prototyping. The framework uses CLIP's pre-trained encoders for feature extraction, with light-weight adapter networks for visual modality fine-tuning and client residuals (perturbations) added to text prompts for privacy protection. During training, clients learn residuals that capture their class-specific semantic information without revealing class names. The server aggregates models based on semantic similarity between client residuals and test prompts, ensuring the global model aligns with the semantic distribution of test users. Inference uses a multimodal prototyping mechanism that combines text prototypes with dynamically updated visual prototypes from high-confidence predictions.

## Key Results
- Achieves average accuracy improvements of 3% over best baseline across six datasets
- Outperforms FedAvg and FedCLIP baselines in open-vocabulary federated learning settings
- Maintains strong performance across varying numbers of training samples and clients
- Requires minimal parameter updates (less than 0.3% of model parameters) for privacy protection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptive model aggregation based on semantic similarity between client residuals and test queries enables open-vocabulary learning in federated settings.
- Mechanism: Fed-MP computes expected cosine similarity between perturbed client prompt representations and test prompt representations. Clients with higher similarity receive higher aggregation weights, ensuring the global model aligns with the semantic distribution of the test user.
- Core assumption: The semantic closeness between training classes (clients) and unseen test classes can be captured through cosine similarity in the embedding space of CLIP.
- Evidence anchors: Abstract states Fed-MP "adaptively aggregates the local model weights based on light-weight client residuals." Section explains semantically closer clients have learned more useful visual concepts related to open-vocabulary queries.

### Mechanism 2
- Claim: Multimodal prototyping with dynamic visual prototype sets improves generalization to unseen classes by leveraging high-confidence predictions as templates.
- Mechanism: Fed-MP maintains visual prototypes per test class, adding high-confidence predictions (low entropy) to corresponding prototype sets. Predictions are based on weighted distance to both text prototypes and visual prototype centroids.
- Core assumption: High-confidence predictions on unseen classes are likely correct and can serve as reliable prototypes for similar samples.
- Evidence anchors: Abstract mentions Fed-MP "robustifies the adapted VLM to unseen categories." Section rationale states high-quality predictions can serve as templates for other test samples.

### Mechanism 3
- Claim: Client residuals protect privacy while enabling semantic-aware model adaptation by perturbing encoded text prompts without revealing class names.
- Mechanism: Each client learns perturbations (residuals) added to CLIP-encoded text prompts for their local classes. These residuals interact with local image features during training, embedding semantic information without exposing raw class labels.
- Core assumption: Perturbing encoded prompts preserves enough semantic structure for meaningful aggregation while preventing direct inference of class identities.
- Evidence anchors: Section states client residuals protect class information by perturbing text representations and that class names and training data are not shared with the server.

## Foundational Learning

- Concept: Federated Learning (FL) basics - decentralized training with local updates and weighted aggregation.
  - Why needed here: Fed-MP builds directly on FL framework, extending it to open-vocabulary settings.
  - Quick check question: What ensures convergence in FL when clients have non-i.i.d. data distributions?

- Concept: Vision-Language Models (VLMs) - models like CLIP that learn joint embeddings of images and text.
  - Why needed here: Fed-MP uses CLIP's pre-trained encoders for both feature extraction and prototype-based inference.
  - Quick check question: How does CLIP's contrastive loss encourage alignment between image and text embeddings?

- Concept: Zero-shot / Open-vocabulary learning - classifying objects from classes not seen during training.
  - Why needed here: Fed-MP's core contribution is enabling FL systems to handle queries from unseen classes.
  - Quick check question: Why does standard supervised learning fail on unseen classes, and how do VLMs address this?

## Architecture Onboarding

- Component map: CLIP image encoder (fixed) -> CLIP text encoder (fixed) -> Adapter network (trainable) -> Client residuals (trainable) -> Aggregation module -> Prototype manager
- Critical path: Local training → residual and adapter update → upload to server → similarity-based aggregation → global model → multimodal inference
- Design tradeoffs:
  - Privacy vs. semantic fidelity: larger residuals improve privacy but may degrade aggregation quality
  - Prototype quality vs. computation: more prototypes improve accuracy but increase inference cost
  - Adapter size vs. communication: smaller adapters reduce bandwidth but may limit adaptation capacity
- Failure signatures:
  - Low similarity scores across all clients → aggregation defaults to uniform weighting, losing semantic alignment
  - High entropy in predictions → prototype sets fail to form, reverting to text-only inference
  - Divergence in adapter weights → unstable global model, possibly due to noisy gradients or poor client sampling
- First 3 experiments:
  1. Run Fed-MP on Caltech101 with 10 clients, 10 samples/class, compare accuracy to FedAvg and FedCLIP.
  2. Remove adaptive aggregation (use FedAvg weighting) and measure drop in performance.
  3. Remove multimodal prototyping (use text-only inference) and measure drop in performance.

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions beyond those addressed in the limitations section.

## Limitations

- Reliance on CLIP's embedding space may not capture fine-grained distinctions in specialized domains like FGVC and StanfordCars
- Prototype-based inference assumes high-confidence predictions are correct, risking error propagation if early predictions are systematically biased
- Insufficient detail on entropy threshold selection across different datasets, critical for multimodal prototyping effectiveness

## Confidence

- High confidence: The adaptive aggregation mechanism based on semantic similarity (Mechanism 1) - supported by clear mathematical formulation and experimental validation
- Medium confidence: The multimodal prototyping mechanism (Mechanism 2) - conceptually sound but lacks detailed implementation specifics
- Medium confidence: Privacy protection through client residuals (Mechanism 3) - plausible but lacks quantitative privacy analysis

## Next Checks

1. **Similarity distribution analysis**: Analyze cosine similarity distributions between client residuals and test prompts across dataset splits to verify semantic alignment and meaningful weight differentiation.
2. **Prototype error propagation test**: Implement controlled experiments with intentionally corrupted predictions to measure how quickly prototype-based inference degrades.
3. **Privacy-utility tradeoff evaluation**: Systematically vary client residual magnitudes while measuring classification accuracy and class inference vulnerability.