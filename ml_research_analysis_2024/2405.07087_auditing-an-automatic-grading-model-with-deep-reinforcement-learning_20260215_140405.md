---
ver: rpa2
title: Auditing an Automatic Grading Model with deep Reinforcement Learning
arxiv_id: '2405.07087'
source_url: https://arxiv.org/abs/2405.07087
tags: []
core_contribution: This paper explores using deep reinforcement learning to audit
  automatic short answer grading (ASAG) models by training an RL agent to iteratively
  revise student responses to achieve high scores from an ASAG model in the fewest
  revisions. The authors train a BERT-base classification model as their ASAG model
  using 781 human ratings, achieving a Quadratic Weighted Kappa of 0.7919 and ROC
  AUC of 0.9185 on held-out test data.
---

# Auditing an Automatic Grading Model with deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2405.07087
- Source URL: https://arxiv.org/abs/2405.07087
- Reference count: 0
- Primary result: RL agent exploits ASAG model by adding repeated phrases and irrelevant terminology

## Executive Summary
This paper presents a novel approach to auditing automatic short answer grading (ASAG) models using deep reinforcement learning. The authors train an RL agent to iteratively revise student responses with the goal of achieving high scores from an ASAG model while minimizing the number of revisions. Through three experiments testing different action spaces (adding helpful phrases, unhelpful phrases, or both), the RL agent successfully exploits weaknesses in the ASAG model by identifying patterns that lead to artificially inflated scores, revealing that the model relies too heavily on specific terminology rather than true semantic understanding.

## Method Summary
The authors develop a reinforcement learning framework where an agent learns to revise student responses to maximize scores from a pre-trained BERT-base ASAG model. The RL agent operates through three action types: adding predefined helpful phrases, adding unhelpful phrases, or deleting sections of responses. The agent is trained using the PPO algorithm and receives rewards based on the ASAG model's score improvements relative to the number of revisions made. The ASAG model itself is trained on 781 human-rated responses using a classification approach, achieving strong performance metrics on held-out test data. The RL agent iteratively generates revised responses, which are then scored by the ASAG model, creating a feedback loop that allows the agent to learn exploitation strategies.

## Key Results
- RL agent successfully exploits ASAG model by adding the same phrase repeatedly to achieve high scores
- Adding unhelpful phrases containing relevant terminology can also artificially inflate scores
- ASAG model achieves Quadratic Weighted Kappa of 0.7919 and ROC AUC of 0.9185 on test data

## Why This Works (Mechanism)
The RL agent exploits the ASAG model's reliance on specific terminology and phrase patterns rather than genuine semantic understanding. By iteratively testing different modifications and observing score changes, the agent learns to identify and exploit shallow patterns in the ASAG model's scoring mechanism. The ASAG model appears to give disproportionate weight to the presence of certain keywords and phrases, allowing the RL agent to achieve high scores through superficial modifications rather than meaningful content improvements.

## Foundational Learning
The RL agent learns through trial and error using proximal policy optimization (PPO), discovering that the ASAG model's scoring mechanism can be manipulated through repetitive phrase insertion and strategic use of relevant terminology. The agent's learning process reveals that the ASAG model has learned to associate specific linguistic patterns with high scores, rather than truly understanding the semantic content of responses. This finding demonstrates a fundamental limitation in how current transformer-based models process and evaluate natural language responses.

## Architecture Onboarding
The reinforcement learning architecture consists of an agent that takes student responses as input and outputs modified versions through three possible actions: adding predefined helpful phrases, adding unhelpful phrases, or deleting sections. The agent uses a transformer-based architecture (implementation details not specified) and is trained using the PPO algorithm. The ASAG model uses a BERT-base architecture with a classification head for scoring responses. Both models operate on tokenized text, with the RL agent learning to navigate the discrete action space of phrase insertion and deletion to maximize rewards based on ASAG scores.

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions for future research.

## Limitations
- Small dataset size (781 human ratings) may limit generalizability
- RL agent action space limited to predefined phrases and deletions
- Analysis relies on manual inspection of top-scoring revisions
- Study focuses on single BERT-base architecture
- No validation of whether exploited patterns would fool human graders
- Limited exploration of different reward function designs
- Potential bias in predefined phrase sets

## Confidence
- High confidence: RL agent successfully exploits ASAG model through phrase repetition and terminology manipulation
- Medium confidence: ASAG model weaknesses stem from terminology reliance rather than semantic understanding
- Low confidence: Generalizability to larger datasets and different ASAG architectures

## Next Checks
1. Test RL auditing approach on larger datasets with more diverse human ratings
2. Apply methodology to multiple ASAG architectures (RoBERTa, GPT-based models)
3. Conduct human evaluation studies with educators reviewing RL-generated revisions
4. Investigate alternative reward functions that penalize superficial modifications
5. Explore continuous action spaces for more nuanced response modifications