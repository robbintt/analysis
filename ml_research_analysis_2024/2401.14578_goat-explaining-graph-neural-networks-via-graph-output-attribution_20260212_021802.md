---
ver: rpa2
title: 'GOAt: Explaining Graph Neural Networks via Graph Output Attribution'
arxiv_id: '2401.14578'
source_url: https://arxiv.org/abs/2401.14578
tags:
- graph
- explanations
- goat
- variables
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GOAt, a novel method for explaining Graph
  Neural Networks (GNNs) via Graph Output Attribution. The method addresses the challenge
  of interpreting GNN decision-making processes by attributing graph outputs to input
  graph features.
---

# GOAt: Explaining Graph Neural Networks via Graph Output Attribution

## Quick Facts
- arXiv ID: 2401.14578
- Source URL: https://arxiv.org/abs/2401.14578
- Authors: Shengyao Lu, Keith G. Mills, Jiao He, Bang Liu, Di Niu
- Reference count: 40
- Primary result: GOAt outperforms state-of-the-art GNN explainers in fidelity, discriminability, and stability metrics

## Executive Summary
This paper introduces GOAt, a novel method for explaining Graph Neural Networks (GNNs) via Graph Output Attribution. The method addresses the challenge of interpreting GNN decision-making processes by attributing graph outputs to input graph features. GOAt expands the GNN as a sum of scalar products involving node features, edge features, and activation patterns, then efficiently computes the contribution of each node or edge feature to each scalar product. The contributions from all scalar products are aggregated to derive the importance of each node and edge. The primary results show that GOAt outperforms various state-of-the-art GNN explainers in terms of fidelity, discriminability, and stability metrics.

## Method Summary
GOAt is an analytical method that explains GNN predictions by attributing graph outputs to input graph features. It expands the GNN output into a sum of scalar products involving node features, edge features, and activation patterns. For each scalar product, the contribution of each feature is calculated based on the principle that all variables in a scalar product contribute equally. These contributions are then aggregated across all scalar products to derive the importance of each node and edge feature. The method does not require training auxiliary models, making it computationally efficient and avoiding errors across different runs.

## Key Results
- GOAt demonstrates superior performance in generating faithful, discriminative, and stable explanations for GNNs compared to state-of-the-art methods
- The method achieves high fidelity by effectively identifying important edges in the graph that contribute to the GNN's prediction
- GOAt shows strong discriminability, with explanation embeddings that can effectively distinguish between different classes
- The method exhibits stability across different sparsity levels and datasets, maintaining consistent performance even when the graph structure is perturbed

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** GOAt expands GNN outputs into a sum of scalar products involving node features, edge features, and activation patterns, then attributes contributions based on the principle that all variables in a scalar product contribute equally.
- **Mechanism:** The GNN output is expressed as a linear transformation of input matrices and parameters, which can be expanded into scalar product terms. Each scalar product term is attributed to its factors (input features, adjacency entries, activation patterns), and the contribution of each feature is calculated by aggregating its contributions across all scalar products it participates in.
- **Core assumption:** The variables in a scalar product term are uncorrelated and the number of nodes is large enough that all variables contribute equally to the scalar product.
- **Evidence anchors:**
  - [abstract]: "By expanding the GNN as a sum of scalar products involving node features, edge features and activation patterns, we propose an efficient analytical method to compute contribution of each node or edge feature to each scalar product and aggregate the contributions from all scalar products in the expansion form to derive the importance of each node and edge."
  - [section 3.2]: "Given a certain data sample and a pretrained GNN, for an element-wise activation function we can define the activation pattern as the ratio between the output and input of the activation function... Thus, we can expand the expression of each output entry in a GNN f (A, X ) into a sum of scalar products..."
  - [corpus]: Missing. No corpus evidence directly supports the equal contribution principle or the expansion methodology.
- **Break condition:** If the variables in a scalar product term are correlated, or if the number of nodes is small, the equal contribution assumption breaks down, leading to inaccurate attribution.

### Mechanism 2
- **Claim:** GOAt calibrates contributions by subtracting the contribution of activation patterns when all input features and adjacency entries are set to zero.
- **Mechanism:** When calculating the contribution of an input feature to the GNN output, GOAt considers the contribution of activation patterns that are non-zero even when all input features and adjacency entries are zero. This calibration ensures that the attribution is not inflated by these pre-existing activation patterns.
- **Core assumption:** The activation patterns can be non-zero even when all input features and adjacency entries are zero, and this needs to be accounted for in the attribution.
- **Evidence anchors:**
  - [section 3.2]: "With Equation (13), we find that when both V and X are set to zeros, f (Â·) remains non-zero... we will need to subtract the contribution of each features on these P from the contribution values calculated by Equation (11)."
  - [corpus]: Missing. No corpus evidence directly supports the need for this calibration step.
- **Break condition:** If the activation patterns are always zero when all input features and adjacency entries are zero, the calibration step is unnecessary and may introduce errors.

### Mechanism 3
- **Claim:** GOAt introduces discriminability and stability metrics in addition to fidelity to evaluate the quality of GNN explanations.
- **Mechanism:** Discriminability measures the ability of explanations to distinguish between classes, while stability measures the consistency of explanations across similar data instances. These metrics provide a more comprehensive evaluation of the explanation quality compared to fidelity alone.
- **Core assumption:** Fidelity alone is not sufficient to evaluate the quality of GNN explanations, and discriminability and stability are important aspects that need to be considered.
- **Evidence anchors:**
  - [abstract]: "Besides the fidelity metric, which is commonly used to assess the faithfulness of GNN explanations, we introduce two new metrics to evaluate the discriminability and stability of the explanation, which are under-investigated by prior literature."
  - [section 4.2]: "Discriminability, also known as discrimination ability (Bau et al., 2017; Iwana et al., 2019), refers to the ability of the explanations to distinguish between the classes... We show the discriminability across various sparsity levels on GCN..."
  - [corpus]: Weak. No corpus evidence directly supports the importance of discriminability and stability metrics for GNN explanations.

## Foundational Learning

- **Concept:** Graph Neural Networks (GNNs)
  - Why needed here: GOAt is a method for explaining GNNs, so a solid understanding of GNNs is essential.
  - Quick check question: What are the key components of a GNN, and how do they work together to learn representations from graph-structured data?

- **Concept:** Graph representation and adjacency matrix
  - Why needed here: GOAt operates on graph-structured data, so understanding how graphs are represented and how adjacency matrices are used is crucial.
  - Quick check question: How is a graph typically represented in a GNN, and what information does the adjacency matrix encode?

- **Concept:** Activation functions and element-wise operations
  - Why needed here: GOAt relies on the element-wise nature of activation functions in GNNs to define activation patterns and expand the output into scalar products.
  - Quick check question: What are activation functions, and why are they typically applied element-wise in GNNs?

## Architecture Onboarding

- **Component map:**
  - Input: Graph data (node features, adjacency matrix)
  - Pre-trained GNN model
  - GOAt explainer:
    - Expansion module: Expands GNN output into scalar products
    - Attribution module: Calculates contributions of input features to scalar products
    - Aggregation module: Aggregates contributions across all scalar products
  - Output: Attribution scores for input features (edges or nodes)

- **Critical path:**
  1. Load pre-trained GNN model and graph data
  2. Expand GNN output into scalar products using the expansion module
  3. Calculate contributions of input features to each scalar product using the attribution module
  4. Aggregate contributions across all scalar products using the aggregation module
  5. Output attribution scores for input features

- **Design tradeoffs:**
  - Accuracy vs. efficiency: The expansion and attribution calculations can be computationally expensive, especially for large graphs. Tradeoffs may be needed between the accuracy of the attribution and the efficiency of the computation.
  - Fidelity vs. discriminability and stability: GOAt aims to optimize not only fidelity but also discriminability and stability. There may be tradeoffs between these different aspects of explanation quality.

- **Failure signatures:**
  - Inaccurate attributions: If the equal contribution assumption is violated or the calibration step is not performed correctly, the attribution scores may be inaccurate.
  - High computational cost: If the graph is large or the GNN is deep, the expansion and attribution calculations may be computationally expensive, leading to slow performance.

- **First 3 experiments:**
  1. Verify the expansion of a simple GNN output into scalar products and check that the contributions of input features sum up to the output.
  2. Test the calibration step by setting all input features and adjacency entries to zero and verifying that the contribution of activation patterns is correctly subtracted.
  3. Evaluate the discriminability and stability metrics on a simple dataset and compare the results with other explanation methods.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GOAt compare when applied to larger graphs with significantly more nodes and edges?
- Basis in paper: [inferred] The paper mentions that the theoretical analysis assumes a large number of nodes (N) in the graphs, but it's not clear how the method performs on graphs that are much larger than those used in the experiments.
- Why unresolved: The experiments conducted in the paper use relatively small graphs. There is no evidence provided on how the method scales to much larger graphs.
- What evidence would resolve it: Running GOAt on graphs with orders of magnitude more nodes and edges than those in the paper's experiments would provide insight into its scalability.

### Open Question 2
- Question: How does GOAt handle graphs with features that are not binary or categorical?
- Basis in paper: [inferred] The paper discusses attributing graph outputs to input graph features, but it's not clear how the method handles continuous or real-valued features.
- Why unresolved: The paper's experiments and case studies focus on binary features, and there is no mention of how the method generalizes to other types of features.
- What evidence would resolve it: Testing GOAt on graphs with continuous or real-valued features and comparing its performance to other methods would provide insight into its versatility.

### Open Question 3
- Question: How does GOAt perform on graphs with dynamic or time-varying structures?
- Basis in paper: [inferred] The paper focuses on static graph structures, but many real-world graphs are dynamic and change over time.
- Why unresolved: There is no discussion or experiments on how GOAt handles dynamic graph structures.
- What evidence would resolve it: Applying GOAt to dynamic graphs and comparing its performance to other methods designed for dynamic graphs would provide insight into its applicability to real-world scenarios.

## Limitations

- The equal contribution assumption may not hold for graphs with correlated features or small numbers of nodes, leading to inaccurate attributions
- The method's performance on graphs with continuous or real-valued features is not evaluated, limiting its applicability to real-world scenarios
- GOAt's scalability to large graphs with significantly more nodes and edges than those used in the experiments is unknown

## Confidence

- Mechanism 1 (Equal contribution principle): Medium - The theoretical foundation is presented, but empirical validation is limited
- Mechanism 2 (Calibration step): Low - The need for calibration is stated but not empirically demonstrated
- Mechanism 3 (New evaluation metrics): Low - Metrics are introduced but lack theoretical justification and corpus support

## Next Checks

1. Test the equal contribution assumption on synthetic datasets where correlation structures between variables are controlled and known
2. Implement batch normalization handling explicitly and evaluate its impact on attribution accuracy across different sparsity levels
3. Compare GOAt's discriminability and stability metrics against established explanation evaluation frameworks to validate their effectiveness in capturing meaningful properties of GNN explanations