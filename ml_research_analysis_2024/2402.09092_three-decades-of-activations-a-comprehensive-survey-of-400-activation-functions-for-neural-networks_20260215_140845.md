---
ver: rpa2
title: 'Three Decades of Activations: A Comprehensive Survey of 400 Activation Functions
  for Neural Networks'
arxiv_id: '2402.09092'
source_url: https://arxiv.org/abs/2402.09092
tags:
- activation
- function
- functions
- neural
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey addresses the absence of a consolidated reference for
  activation functions, which leads to redundancy and unintentional rediscovery in
  neural network research. The core method is an exhaustive compilation and systematization
  of 400 activation functions, categorized as fixed or adaptive (trainable).
---

# Three Decades of Activations: A Comprehensive Survey of 400 Activation Functions for Neural Networks

## Quick Facts
- **arXiv ID:** 2402.09092
- **Source URL:** https://arxiv.org/abs/2402.09092
- **Reference count:** 40
- **Primary result:** Exhaustive compilation and categorization of 400 activation functions, providing the most comprehensive overview to date with links to original sources, enabling informed selection and reducing redundancy.

## Executive Summary
This survey addresses the absence of a consolidated reference for activation functions, which leads to redundancy and unintentional rediscovery in neural network research. The core method is an exhaustive compilation and systematization of 400 activation functions, categorized as fixed or adaptive (trainable). The primary result is providing the most comprehensive overview to date, with links to original sources, enabling researchers to make informed choices and avoid wasteful repetition of proposals.

## Method Summary
The authors conducted a systematic literature review, collecting activation functions from research papers, surveys, and benchmarks spanning three decades of neural network research. Functions were categorized as fixed (non-trainable) or adaptive (trainable) and listed with citations to original sources. The method focuses on compilation rather than experimental evaluation, creating a reference catalog rather than benchmarking performance.

## Key Results
- Cataloged 400 activation functions spanning three decades of neural network research
- Categorized functions as fixed or adaptive (trainable) for systematic organization
- Provided comprehensive references to original sources, enabling verification and reducing redundancy

## Why This Works (Mechanism)

### Mechanism 1
Activation functions introduce essential non-linearity, enabling neural networks to model complex, non-linear relationships beyond linear regression. By applying a non-linear transformation to the affine output of each neuron, activation functions allow the network to approximate arbitrarily complex functions through composition. Without non-linearity, even deep networks with multiple layers can only represent linear mappings, severely limiting their expressive power.

### Mechanism 2
Adaptive activation functions with trainable parameters improve network performance by customizing activation shapes to data distributions, enhancing learning efficiency and accuracy. Parameters within adaptive AFs (e.g., slope in PReLU, scale in swish) are optimized during training via gradient descent, allowing the network to adjust non-linearity per neuron or channel based on input statistics.

### Mechanism 3
Activation functions directly govern gradient flow during backpropagation, impacting convergence speed, stability, and the mitigation of vanishing/exploding gradients. The derivative of the activation function scales gradients; saturated functions (e.g., sigmoid, tanh) produce small gradients for large inputs, causing vanishing gradients, while ReLU variants avoid saturation but risk dead neurons from zero gradients for negative inputs.

## Foundational Learning

- **Concept:** Non-linearity in neural networks
  - **Why needed here:** Activation functions are the sole source of non-linearity in standard feedforward networks, enabling hierarchical feature learning and universal approximation.
  - **Quick check question:** Explain why a network with only linear activation functions (including identity) is equivalent to a single linear layer, regardless of depth.

- **Concept:** Gradient descent and backpropagation
  - **Why needed here:** Training relies on gradient-based optimization; activation functions must have well-defined, non-vanishing derivatives in active regions to propagate error signals.
  - **Quick check question:** What happens to the gradient during backpropagation if an activation function is non-differentiable at a point, like ReLU at zero?

- **Concept:** Vanishing and exploding gradients
  - **Why needed here:** Deep networks suffer from gradient instability; activation function design (e.g., avoiding saturation, controlling output range) mitigates these issues.
  - **Quick check question:** How do the derivatives of sigmoid and tanh contribute to vanishing gradients in very deep networks?

## Architecture Onboarding

- **Component map:** Activation functions are embedded within each neuron immediately after the affine transformation (weights and bias). They take the pre-activation value z and output f(z), which feeds into subsequent layers. In convolutional networks, AFs are applied per channel after convolution.
- **Critical path:** 
  1. For initial designs, use standard AFs: ReLU family (ReLU, LReLU) for hidden layers in CNNs and MLPs due to simplicity and performance; softmax for multi-class output; sigmoid for binary output.
  2. If training is unstable or plateaus, experiment with adaptive variants (e.g., swish, PReLU) based on the paper's categorizations.
  3. For specific tasks (e.g., regression with bounded outputs), consider bounded AFs (e.g., tanh, bounded ReLU variants) or normalize inputs/outputs.
- **Design tradeoffs:**
  - Fixed vs. adaptive: Fixed AFs have zero additional parameters and are computationally cheap; adaptive AFs add parameters per neuron/channel, increasing capacity but also risk of overfitting and slower inference.
  - Computational cost: Some AFs involve expensive operations (e.g., exponential in sigmoid, tanh); ReLU and variants are typically faster.
  - Gradient behavior: Non-monotonic or bounded AFs may suffer from vanishing gradients; ReLU avoids this for positive inputs but can deaden neurons.
  - Output distribution: Unbounded AFs (ReLU, swish) produce unbounded activations, requiring careful initialization or normalization; bounded AFs constrain outputs but may saturate.
- **Failure signatures:**
  - High percentage of zero activations in ReLU-based layers indicates dead neurons, often from large negative gradients or poor initialization.
  - Loss plateau or slow convergence suggests vanishing gradients, common with saturated AFs like sigmoid/tanh in deep networks.
  - Numerical instability (NaNs, exploding loss) may arise from unbounded AFs without proper initialization or normalization.
  - Overfitting observed when using adaptive AFs without sufficient regularization or data.
- **First 3 experiments:**
  1. **Baseline comparison:** Train a simple CNN on CIFAR-10 with ReLU, sigmoid, and tanh as hidden layer activations; measure validation accuracy and training loss curve to observe convergence speed and final performance.
  2. **Adaptive test:** Replace ReLU with swish or PReLU in a ResNet on CIFAR-100; compare validation accuracy and training epochs to assess benefits of adaptivity.
  3. **Gradient analysis:** During training, log gradient norms for each layer with different AFs; identify layers with near-zero gradients (vanishing) or large spikes (exploding) to diagnose issues.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do the 400 surveyed activation functions compare in performance, convergence speed, and computational efficiency across different neural network architectures and tasks?
- **Basis in paper:** [explicit] The conclusion explicitly states that the paper does not conduct extensive benchmarks, and such analysis would require a dedicated effort beyond the scope of this comprehensive listing.
- **Why unresolved:** No comparative evaluation is provided; the survey compiles functions but does not measure them.
- **What evidence would resolve it:** Systematic benchmarks on standard datasets (e.g., ImageNet, CIFAR) using common architectures (CNNs, RNNs, transformers) with metrics for accuracy, training time, and resource consumption.

### Open Question 2
- **Question:** What systematic criteria can be used to differentiate and categorize activation functions to prevent unintentional rediscovery and redundancy in future research?
- **Basis in paper:** [explicit] The authors state that the absence of a consolidated overview leads to redundancy and reinvention, citing examples like ReLU-Swish=FTS and RePU=RePU2.
- **Why unresolved:** The paper provides a list but only a two-tier classification (fixed vs. adaptive); no deeper taxonomy or distinguishing principles are developed.
- **What evidence would resolve it:** A formal taxonomy based on mathematical properties (e.g., monotonicity, fixed points, derivative regimes) or functional embeddings that can automatically flag redundant functions.

### Open Question 3
- **Question:** For complex-valued, quaternion-valued, and other non-real neural networks (excluded from this survey), how do their activation functions compare in capability and efficiency to the real-valued ones surveyed?
- **Basis in paper:** [explicit] The abstract and introduction explicitly limit the scope to real-valued activation functions, noting that complex-valued, quaternion-valued, and other exotic NNs are out of scope despite their existence in literature.
- **Why unresolved:** The survey does not cover activation functions for these domains, leaving a gap in understanding their relative advantages and appropriate use cases.
- **What evidence would resolve it:** Dedicated surveys or comparative studies that define, analyze, and benchmark activation functions for complex-valued, quaternion-valued, photonic, fuzzy, and quantum neural networks.

### Open Question 4
- **Question:** Under what conditions (data type, network depth, optimization algorithm) do adaptive activation functions improve training stability and final performance compared to fixed ones?
- **Basis in paper:** [inferred] The paper categorizes many adaptive activation functions (AAFs) and notes they are receiving more attention, but provides no comparative analysis or guidelines for when adaptivity is beneficial.
- **Why unresolved:** While AAFs are listed, there is no empirical or theoretical investigation into their advantages over fixed functions, leaving practitioners without evidence-based selection criteria.
- **What evidence would resolve it:** Controlled experiments comparing fixed and adaptive activation functions across diverse tasks and architectures, measuring convergence, generalization error, and sensitivity to hyperparameter settings.

## Limitations
- **Limited experimental validation:** The survey compiles functions but provides no performance benchmarking or comparative analysis.
- **Unclear novelty verification:** Without explicit criteria for distinguishing unique functions from variants, the 400-count may include redundancies.
- **Scope limitations:** Excludes complex-valued, quaternion-valued, and other specialized neural network domains.

## Confidence
- **Completeness of compilation:** Medium - extensive but lacks systematic verification for redundancy
- **Novelty claims:** Low - no rigorous analysis to distinguish truly distinct functions from minor variants
- **Categorization framework:** High - fixed vs. adaptive follows standard neural network taxonomy
- **Practical utility:** Medium - provides comprehensive reference but requires external validation for selection

## Next Checks
1. Compare the 400-function list against independent databases of activation functions to quantify potential redundancy and identify missing entries.
2. Perform systematic de-duplication analysis: identify functions with identical mathematical formulations but different names to establish true novelty count.
3. Survey active neural network researchers to assess whether this compilation would actually reduce redundant proposals or if the survey format itself creates barriers to adoption.