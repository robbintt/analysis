---
ver: rpa2
title: 'Psychological Assessments with Large Language Models: A Privacy-Focused and
  Cost-Effective Approach'
arxiv_id: '2402.03435'
source_url: https://arxiv.org/abs/2402.03435
tags: []
core_contribution: This study demonstrates that using carefully crafted prompts with
  open-source Large Language Models (LLMs) can effectively extract highlights and
  generate summaries to support psychological assessments of suicidal risk in online
  text data. The approach, which does not require fine-tuning or complex operations,
  achieves remarkable results with cost-effective LLMs running on consumer-grade hardware.
---

# Psychological Assessments with Large Language Models: A Privacy-Focused and Cost-Effective Approach

## Quick Facts
- **arXiv ID**: 2402.03435
- **Source URL**: https://arxiv.org/abs/2402.03435
- **Reference count**: 7
- **Primary result**: Open-source LLMs can extract highlights and generate summaries for psychological assessments of suicidal risk with high accuracy, achieving recall scores up to 0.910 and consistency scores up to 0.976 without requiring fine-tuning or complex operations.

## Executive Summary
This study demonstrates that carefully crafted prompts with open-source Large Language Models can effectively extract highlights and generate summaries to support psychological assessments of suicidal risk in online text data. The approach achieves remarkable results using cost-effective LLMs running on consumer-grade hardware, without requiring fine-tuning or complex operations. The method successfully identifies relevant excerpts and produces comprehensive, consistent summaries that justify preassigned risk levels. This privacy-focused approach offers an accessible solution for supporting psychological assessments in online forums while maintaining data confidentiality.

## Method Summary
The study employs prompt engineering techniques with open-source LLMs to extract highlights and generate summaries from text data in online forums. The approach focuses on identifying relevant excerpts and producing comprehensive summaries that justify preassigned risk levels. The methodology relies on consumer-grade hardware and cost-effective models, avoiding the need for model fine-tuning or complex operations. The evaluation is based on metrics such as recall scores for highlight extraction and consistency scores for summarization quality.

## Key Results
- For highlights extraction, top models achieved recall scores up to 0.910 and weighted recall scores up to 0.803
- For summarization, best models reached consistency scores of 0.976 and contradiction scores as low as 0.045
- The approach successfully identifies relevant excerpts and produces comprehensive, consistent summaries justifying preassigned risk levels

## Why This Works (Mechanism)
The effectiveness stems from the LLMs' ability to process natural language patterns and extract semantically relevant information when guided by carefully designed prompts. The models leverage their pre-trained understanding of language structure, context, and risk indicators to identify key information and synthesize coherent summaries. The prompt engineering approach allows the models to focus on relevant risk factors while maintaining consistency in their outputs.

## Foundational Learning
- **Prompt Engineering**: The practice of designing specific prompts to guide LLM behavior. Why needed: To direct models toward relevant risk indicators without fine-tuning. Quick check: Verify prompts consistently produce desired output formats.
- **Information Extraction Metrics**: Recall and weighted recall measure how well models identify relevant information. Why needed: To quantify the completeness and accuracy of extracted highlights. Quick check: Compare extracted highlights against ground truth annotations.
- **Consistency Scoring**: Measures the logical coherence between summaries and source text. Why needed: To ensure generated summaries accurately represent the underlying risk assessment. Quick check: Calculate contradiction scores between summaries and source material.
- **Privacy-Preserving Analysis**: Processing text data without accessing or storing personal information. Why needed: To maintain confidentiality in sensitive psychological assessments. Quick check: Verify no personal identifiers are retained in processing pipeline.
- **Consumer-Grade Hardware Deployment**: Running LLMs on standard computing equipment. Why needed: To make the approach accessible and cost-effective. Quick check: Confirm model runs within hardware constraints while maintaining performance.
- **Text Classification Without Fine-Tuning**: Using prompt-based approaches instead of model adaptation. Why needed: To maintain privacy and reduce computational requirements. Quick check: Test classification accuracy across different model architectures.

## Architecture Onboarding

**Component Map**: Raw Text Data -> Prompt Engineering Layer -> LLM Processing -> Information Extraction -> Consistency Validation -> Summary Generation

**Critical Path**: Text input → Prompt generation → LLM processing → Highlight extraction → Summary synthesis → Quality validation

**Design Tradeoffs**: The choice to use prompt engineering over fine-tuning prioritizes privacy and accessibility but may limit domain-specific adaptation depth. Consumer-grade hardware deployment reduces costs but may constrain model size and processing speed.

**Failure Signatures**: 
- Low recall scores indicate missed relevant risk indicators
- High contradiction scores suggest inconsistent or inaccurate summary generation
- Performance degradation on consumer hardware may indicate model complexity issues
- Biased outputs may reflect training data imbalances

**3 First Experiments**:
1. Test prompt variations with a small validation set to optimize highlight extraction performance
2. Compare consistency scores across different open-source LLM architectures
3. Benchmark processing time and memory usage on various consumer hardware configurations

## Open Questions the Paper Calls Out
None

## Limitations
- The approach relies entirely on prompt engineering rather than fine-tuning, limiting domain-specific adaptation depth
- Evaluation metrics focus on information extraction quality rather than clinical validity or real-world impact
- Privacy-focused approach prevents validation against actual clinical outcomes or patient data
- Potential biases in LLMs' training data may affect risk assessment for different demographic groups

## Confidence
- **High confidence**: Technical methodology and evaluation metrics are sound and reproducible
- **Medium confidence**: Results demonstrate effective information extraction and summary generation capabilities
- **Low confidence**: Clinical applicability and real-world impact on suicide risk assessment accuracy

## Next Checks
1. Conduct blinded clinical validation studies comparing LLM-assisted assessments against expert clinician evaluations on the same text data
2. Test model performance across diverse demographic groups and cultural contexts to assess potential bias in risk assessment
3. Evaluate the approach's effectiveness in real-time crisis intervention scenarios versus retrospective analysis of forum posts