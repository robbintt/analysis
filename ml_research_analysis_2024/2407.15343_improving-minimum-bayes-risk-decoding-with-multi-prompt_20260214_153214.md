---
ver: rpa2
title: Improving Minimum Bayes Risk Decoding with Multi-Prompt
arxiv_id: '2407.15343'
source_url: https://arxiv.org/abs/2407.15343
tags:
- prompt
- multi-prompt
- prompts
- text
- simplification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the instability and suboptimality in performance
  of instruction fine-tuned large language models (LLMs) due to their sensitivity
  to prompt construction. To improve generation quality, the authors propose multi-prompt
  decoding, which samples candidates from a bank of human- or model-written prompts
  at inference time.
---

# Improving Minimum Bayes Risk Decoding with Multi-Prompt

## Quick Facts
- arXiv ID: 2407.15343
- Source URL: https://arxiv.org/abs/2407.15343
- Authors: David Heineman; Yao Dou; Wei Xu
- Reference count: 40
- Primary result: Multi-prompt decoding consistently improves MBR performance across three distinct tasks (text simplification, machine translation, code generation)

## Executive Summary
This paper addresses the instability and suboptimality in performance of instruction fine-tuned large language models (LLMs) due to their sensitivity to prompt construction. To improve generation quality, the authors propose multi-prompt decoding, which samples candidates from a bank of human- or model-written prompts at inference time. These candidates are then selected using Minimum Bayes Risk (MBR) decoding with a trained value metric. Experiments across three distinct tasks—text simplification, machine translation, and code generation—demonstrate that multi-prompt consistently improves MBR performance over single-prompt MBR. This improvement is attributed to estimating a more diverse and higher quality candidate space. The method shows robust gains across different tasks, models, and metrics, with up to 7% improvement on HumanEval and 5 points on LENS for SIMP EVAL. Further, the study explores the dynamics between different utility and evaluation metrics, showing that multi-prompt MBR with one metric improves performance universally across metrics.

## Method Summary
The authors propose multi-prompt decoding to address the sensitivity of LLMs to prompt construction. This approach samples candidates from a bank of human- or model-written prompts at inference time, then selects among them using Minimum Bayes Risk (MBR) decoding with a trained value metric. The method generates multiple candidate outputs from different prompts, evaluates them using a utility function, and selects the candidate with the highest expected utility when paired with other candidates. Experiments demonstrate consistent improvements over single-prompt MBR across text simplification, machine translation, and code generation tasks.

## Key Results
- Multi-prompt MBR improves performance over single-prompt MBR across all three tasks tested
- Up to 7% improvement on HumanEval for code generation
- 5-point improvement on LENS for SIMP EVAL in text simplification
- Improvements are consistent across different models, tasks, and evaluation metrics

## Why This Works (Mechanism)
Multi-prompt MBR works by expanding the candidate space through diverse prompt sampling, which captures a broader range of possible outputs. By using MBR with a trained value metric, the system can better identify high-quality candidates from this expanded pool. The diversity in prompts helps mitigate the sensitivity of LLMs to specific prompt formulations, leading to more robust and higher-quality generations.

## Foundational Learning
- Minimum Bayes Risk (MBR) decoding: A decoding strategy that selects the output maximizing expected utility when paired with other candidates. Needed to evaluate and select the best candidate from the multi-prompt pool. Quick check: Verify MBR implementation correctly computes expected utility across candidate pairs.
- Prompt engineering sensitivity: LLMs show significant performance variation based on prompt formulation. Understanding this helps justify the need for multi-prompt approaches. Quick check: Test performance variance across different single prompts for the same task.
- Utility metrics: Trained metrics that evaluate generation quality and guide candidate selection in MBR. Critical for effective candidate ranking. Quick check: Validate utility metric performance correlates with human judgments.

## Architecture Onboarding

Component map: Prompt bank -> Candidate generation -> Utility metric evaluation -> MBR selection -> Final output

Critical path: Prompt bank → Candidate generation → Utility metric evaluation → MBR selection

Design tradeoffs:
- Multiple prompts increase diversity but add computational overhead
- Trained utility metrics may generalize differently across tasks
- Candidate quality vs. quantity in the prompt bank
- Single vs. multiple utility metrics for MBR

Failure signatures:
- Performance degradation if prompt bank lacks diversity
- Overfitting to specific utility metrics
- Computational inefficiency with large prompt banks
- Inconsistent gains across different tasks

First experiments:
1. Compare single-prompt MBR vs multi-prompt MBR on a held-out validation set
2. Test performance sensitivity to prompt bank size and diversity
3. Evaluate different utility metrics for MBR candidate selection

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy reliance on automated metrics without sufficient human validation
- Limited exploration of how prompt diversity impacts results
- Computational overhead of generating and evaluating multiple candidates
- Focus on English-centric settings without cross-lingual validation

## Confidence
- High: Core claim supported by consistent improvements across established metrics (MT, code generation)
- Medium: Text simplification improvements, as LENS metric correlation with human judgment is not fully established
- Claim of universal metric improvement should be interpreted cautiously due to imperfect metric correlations

## Next Checks
1. Conduct human evaluation studies to validate whether multi-prompt MBR improvements in automated metrics correspond to better human-perceived quality, particularly for text simplification.
2. Investigate the relationship between prompt diversity in the multi-prompt bank and performance gains, including experiments with different prompt selection strategies and their impact on computational efficiency.
3. Extend experiments to additional tasks and languages to assess the robustness and generalizability of multi-prompt MBR beyond the three tasks and English-centric settings examined.