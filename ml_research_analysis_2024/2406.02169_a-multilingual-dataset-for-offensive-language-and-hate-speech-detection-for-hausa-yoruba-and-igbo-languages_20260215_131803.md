---
ver: rpa2
title: A multilingual dataset for offensive language and hate speech detection for
  hausa, yoruba and igbo languages
arxiv_id: '2406.02169'
source_url: https://arxiv.org/abs/2406.02169
tags:
- language
- hate
- offensive
- speech
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of detecting offensive language
  and hate speech in Nigerian social media content by developing and introducing novel
  datasets for Hausa, Yoruba, and Igbo languages. Data was collected from Twitter
  and manually annotated by native speakers.
---

# A multilingual dataset for offensive language and hate speech detection for hausa, yoruba and igbo languages

## Quick Facts
- arXiv ID: 2406.02169
- Source URL: https://arxiv.org/abs/2406.02169
- Reference count: 40
- Primary result: 90% accuracy on Hausa, Yoruba, and Igbo datasets for offensive language detection

## Executive Summary
This study addresses the challenge of detecting offensive language and hate speech in Nigerian social media content by developing and introducing novel datasets for Hausa, Yoruba, and Igbo languages. Data was collected from Twitter and manually annotated by native speakers. Four pre-trained language models were evaluated, with the best-performing model achieving an accuracy of 90% on the Hausa, Yoruba, and Igbo datasets. The study highlights the importance of considering linguistic diversity in multilingual offensive language detection and provides publicly available datasets and models to support further research in this area.

## Method Summary
The study collected 5,000 tweets per language from Twitter using keyword queries developed from crowd-sourced offensive and hate keywords validated by language experts. The tweets were manually annotated by three native speakers per language following drafted guidelines, with annotations accepted at 60% inter-annotator agreement. The datasets were then split into 80% training and 20% test sets, and four pre-trained multilingual language models (XLM-RoBERTa-Base, BERT-Base-Multilingual-Cased, Morit/XLM-T-Full-XNLI, and Davlan/Naija-Twitter-Sentiment-Afriberta-Large) were fine-tuned on the training data using Hugging Face's AutoModel and AutoTokenizer classes. Model performance was evaluated using accuracy on the test sets.

## Key Results
- Best-performing model achieved 90% accuracy on Hausa, Yoruba, and Igbo datasets
- Datasets contain 4,467 Hausa, 4,042 Yoruba, and 3,856 Igbo annotated tweets
- Four pre-trained language models were fine-tuned and evaluated

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Native speaker annotations provide linguistically grounded labels that capture cultural nuance in offensive language detection.
- Mechanism: Human annotators fluent in Hausa, Yoruba, and Igbo apply culturally informed interpretation to classify tweets as offensive, hate, or neither based on local context and social norms.
- Core assumption: Native speakers can reliably distinguish between offensive language and hate speech in low-resource languages where formal definitions may not align with local usage.
- Evidence anchors:
  - [abstract] The study collected data from Twitter and manually annotated it using native speakers.
  - [section] Three native speakers per language were employed and trained for the annotation task with drafted guidelines.
  - [corpus] The corpus contains annotated tweets in three Nigerian languages, supporting the feasibility of native annotation.
- Break condition: If annotators lack cultural context or training quality is inconsistent, inter-annotator agreement may fall below the 60% threshold, undermining label reliability.

### Mechanism 2
- Claim: Fine-tuning multilingual pre-trained models on language-specific tweet data improves offensive language detection accuracy in low-resource African languages.
- Mechanism: Pre-trained models like XLM-RoBERTa and BERT are adapted to the linguistic patterns of Hausa, Yoruba, and Igbo tweets, leveraging transfer learning from high-resource languages.
- Core assumption: Multilingual embeddings capture enough cross-lingual transfer to be fine-tuned effectively on small datasets of low-resource languages.
- Evidence anchors:
  - [abstract] Four pre-trained language models were evaluated, with the best model achieving 90% accuracy.
  - [section] Models such as morit/XLM-T-full-xnli and Davlan/Naija-Twitter-Sentiment-Afriberta-Large were fine-tuned on the datasets.
  - [corpus] Corpus includes multilingual data enabling cross-lingual model training.
- Break condition: If the tweet data is too limited or linguistically divergent, fine-tuning may overfit or fail to generalize, reducing accuracy.

### Mechanism 3
- Claim: Keyword-based data collection increases the proportion of offensive and hateful tweets, improving dataset balance for model training.
- Mechanism: Offensive and hate keywords are crowd-sourced and validated by language experts, then used in Twitter API queries to filter tweets likely containing harmful content.
- Core assumption: Keyword queries effectively surface relevant tweets without introducing severe sampling bias that would distort model learning.
- Evidence anchors:
  - [abstract] Keywords were collected through crowd-sourcing and validated by language experts.
  - [section] Queries were developed using keywords and the Twitter academic API was used to crawl 5,000 tweets per language.
  - [corpus] Corpus signals mention related datasets using similar multilingual and keyword strategies.
- Break condition: If keywords are too narrow or culturally misaligned, the dataset may miss relevant content or overrepresent certain topics, hurting model robustness.

## Foundational Learning

- Concept: Inter-annotator agreement measurement (Fleiss' kappa)
  - Why needed here: Ensures annotation quality and consistency across multiple annotators for reliable dataset labels.
  - Quick check question: What Fleiss' kappa threshold was set for accepting annotations in this study?

- Concept: Fine-tuning pre-trained language models
  - Why needed here: Adapts multilingual embeddings to the specific linguistic and cultural context of Hausa, Yoruba, and Igbo tweets.
  - Quick check question: Which pre-trained models were fine-tuned on the tweet datasets to achieve high accuracy?

- Concept: Tokenization and encoding for low-resource languages
  - Why needed here: Handles unique linguistic constructs and scripts of Hausa, Yoruba, and Igbo for effective feature extraction.
  - Quick check question: How does the AutoTokenizer class support multilingual text processing in this study?

## Architecture Onboarding

- Component map: Data collection (Twitter API + keyword queries) → Pre-processing (cleaning, normalization) → Annotation (native speakers, guidelines) → Model fine-tuning (pre-trained encoders) → Evaluation (accuracy on test splits)
- Critical path: Annotation quality → Dataset balance → Model fine-tuning → Evaluation accuracy
- Design tradeoffs: Native annotation ensures cultural nuance but is slower and costlier than automated labeling; keyword queries bias dataset toward certain topics but improve label relevance.
- Failure signatures: Low inter-annotator agreement, imbalanced class distribution, overfitting on small datasets, poor cross-lingual generalization.
- First 3 experiments:
  1. Evaluate baseline accuracy of XLM-RoBERTa on the Hausa dataset without fine-tuning.
  2. Measure inter-annotator agreement on a pilot set of 100 tweets per language.
  3. Compare accuracy of fine-tuned vs. non-fine-tuned multilingual models on the Yoruba test set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are current multilingual models at detecting offensive language in Hausa, Yoruba, and Igbo when considering cultural nuances and context-specific meanings?
- Basis in paper: [inferred] The paper mentions that cultural nuances may play a significant role in model performance, yet it does not explore how these nuances affect detection accuracy.
- Why unresolved: The study does not provide an in-depth analysis of cultural context and its impact on model accuracy.
- What evidence would resolve it: Detailed experiments evaluating model performance with culturally nuanced data, and analysis of how cultural context affects detection accuracy.

### Open Question 2
- Question: How does the imbalance in class labels (offensive vs. non-offensive) impact the effectiveness of hate speech detection models for these languages?
- Basis in paper: [explicit] The paper acknowledges that the dataset contains a very small number of hateful tweets, which may affect model performance.
- Why unresolved: The paper does not address strategies to mitigate class imbalance or its impact on detection efficacy.
- What evidence would resolve it: Experiments showing model performance with balanced datasets and strategies to handle class imbalance.

### Open Question 3
- Question: What is the impact of using different data sources (e.g., YouTube, Instagram) on the performance of offensive language detection models compared to Twitter?
- Basis in paper: [inferred] The paper suggests collecting more comments from YouTube and Instagram as future work, indicating potential differences in performance across platforms.
- Why unresolved: The study only uses Twitter data, leaving the performance on other platforms unexplored.
- What evidence would resolve it: Comparative studies evaluating model performance across different social media platforms using the same detection models.

## Limitations
- Inter-annotator agreement threshold of 60% may impact label reliability
- Dataset sizes (4,467 Hausa, 4,042 Yoruba, 3,856 Igbo tweets) are limited for robust model training
- Evaluation relies solely on accuracy metrics, lacking precision-recall analysis for minority classes

## Confidence
- Dataset creation methodology: High
- Model performance claims (90% accuracy): Medium
- Generalizability to broader contexts: Low

## Next Checks
1. Replicate the annotation process on a 100-tweet validation set to verify the 60% inter-annotator agreement threshold is appropriate for this domain
2. Conduct error analysis on model predictions to identify systematic failures, particularly for the hate speech class
3. Test model robustness by evaluating on out-of-domain Nigerian social media data (e.g., Facebook or forum posts) to assess generalization beyond Twitter