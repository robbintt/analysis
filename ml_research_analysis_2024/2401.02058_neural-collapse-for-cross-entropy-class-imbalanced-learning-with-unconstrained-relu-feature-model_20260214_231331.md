---
ver: rpa2
title: Neural Collapse for Cross-entropy Class-Imbalanced Learning with Unconstrained
  ReLU Feature Model
arxiv_id: '2401.02058'
source_url: https://arxiv.org/abs/2401.02058
tags:
- collapse
- training
- features
- loss
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the Neural Collapse phenomenon in imbalanced
  datasets for deep neural networks trained with cross-entropy loss. It extends the
  unconstrained feature model (UFM) to include non-negative features, motivated by
  ReLU activations.
---

# Neural Collapse for Cross-entropy Class-Imbalanced Learning with Unconstrained ReLU Feature Model

## Quick Facts
- arXiv ID: 2401.02058
- Source URL: https://arxiv.org/abs/2401.02058
- Authors: Hien Dang; Tho Tran; Tan Nguyen; Nhat Ho
- Reference count: 35
- One-line primary result: Proves Neural Collapse phenomenon in imbalanced datasets with non-negative ReLU features, showing orthogonal class-means with varying norms and aligned classifier weights.

## Executive Summary
This paper extends the Neural Collapse phenomenon to class-imbalanced settings by analyzing the unconstrained feature model (UFM) with non-negative features motivated by ReLU activations. The authors prove that under cross-entropy loss, class-means converge to an orthogonal structure with different norms based on class sizes, while classifier weights align to scaled and centered class-means. The study derives closed-form expressions for the relationships between learned features and classifier weights, quantifying how these relationships vary across different class distributions. Experimental results on imbalanced CIFAR10 subsets validate the theoretical findings, demonstrating convergence to orthogonal feature structures and aligned classifiers.

## Method Summary
The study analyzes Neural Collapse in imbalanced datasets using the unconstrained feature model with non-negative features. The method involves training deep networks (MLP and ResNet18) on imbalanced CIFAR10 subsets with varying class sample sizes [100, 100, 200, 200, 300, 300, 400, 400, 500, 500]. Models are trained using cross-entropy loss with SGD optimization, weight decay parameters λW = 1e-4 and λH = 1e-5 for 4000 epochs. The theoretical analysis derives the optimal structure of class-means and classifier weights, proving orthogonal class-mean formation with varying norms and alignment of classifier weights to scaled class-means. Neural Collapse metrics (NC1, NC2, NC3) are calculated at each epoch to verify theoretical predictions.

## Key Results
- Proves orthogonal structure of class-means with varying lengths under non-negative feature model and cross-entropy loss
- Derives closed-form expressions for classifier weight alignment to scaled and centered class-means
- Provides exact threshold formula for Minority Collapse where minority classes become indistinguishable

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Neural Collapse emerges in imbalanced datasets when training deep networks with cross-entropy loss under a non-negative feature model.
- **Mechanism:** The cross-entropy loss with non-negative ReLU features drives the optimization to a global minimum where within-class features collapse to a single vector (class-mean), and class-means form an orthogonal structure with different norms based on class sizes.
- **Core assumption:** The feature dimension is at least as large as the number of classes, ensuring enough degrees of freedom for orthogonal structure.
- **Evidence anchors:**
  - [abstract] "The authors prove that under this setting, the optimal class-means form an orthogonal structure with varying lengths..."
  - [section] "We prove that, while the within-class features collapse property still holds in this setting, the class-means will converge to a structure consisting of orthogonal vectors with different lengths."
  - [corpus] Weak - no direct corpus match for "non-negative ReLU features + cross-entropy + imbalanced" combination.
- **Break condition:** If the feature dimension is less than the number of classes, or if the imbalance ratio is extreme enough to trigger complete collapse where all class-means become zero.

### Mechanism 2
- **Claim:** Classifier weights are aligned to scaled and centered class-means, generalizing Neural Collapse's self-duality property to imbalanced settings.
- **Mechanism:** The optimal classifier weight for class k is proportional to the scaled and centered class-mean: wk ∝ K√nkhk - Σm√nmhm, where nk is the number of samples in class k.
- **Core assumption:** The scaling factors depend on class sizes and regularization parameters, maintaining the alignment structure.
- **Evidence anchors:**
  - [abstract] "Furthermore, we find that the classifier weights are aligned to the scaled and centered class-means with scaling factors depend on the number of training samples of each class..."
  - [section] "Our results indicate that class k's classifier, wk, is aligned to the scaled and centered class-mean hk, with scaling factor √nk..."
  - [corpus] Weak - no direct corpus match for the specific alignment formula.
- **Break condition:** When minority classes collapse completely (hk = 0), making the alignment undefined or trivial.

### Mechanism 3
- **Claim:** There exists a threshold for Minority Collapse where minority classes become indistinguishable when the imbalance ratio exceeds a critical value.
- **Mechanism:** When the number of samples in a class falls below C(N,K,λW,λH) = N²K/(K-1)λWλH, the class-mean becomes zero, causing the classifier to collapse to the same vector as other collapsed classes.
- **Core assumption:** The regularization parameters and total sample size determine the collapse threshold.
- **Evidence anchors:**
  - [abstract] "The study provides closed-form expressions for the norms and angles between learned features and classifier weights, quantifying their relationships across different class distributions."
  - [section] "We derive the exact threshold for a class to collapse and become indistinguishable from other classes."
  - [corpus] Weak - no direct corpus match for the specific threshold formula.
- **Break condition:** If the regularization parameters are too low or the dataset is too large, complete collapse of all classes may occur.

## Foundational Learning

- **Concept:** Unconstrained Feature Model (UFM)
  - Why needed here: UFM simplifies the analysis by treating last-layer features as free optimization variables, capturing the key characteristics of Neural Collapse.
  - Quick check question: How does UFM differ from analyzing the full neural network, and why is this simplification valid for studying Neural Collapse?

- **Concept:** Neural Collapse phenomenon
  - Why needed here: Understanding the original Neural Collapse properties (variability collapse, simplex ETF, self-duality, nearest class-center) is crucial for generalizing to imbalanced settings.
  - Quick check question: What are the four key properties of Neural Collapse in balanced datasets, and how do they change under class imbalance?

- **Concept:** Cross-entropy loss geometry
  - Why needed here: The specific structure of cross-entropy loss drives the formation of orthogonal class-means and the alignment of classifier weights.
  - Quick check question: How does the geometry of cross-entropy loss differ from other losses like MSE or SVM in terms of class-mean structure?

## Architecture Onboarding

- **Component map:** Unconstrained feature matrix H (non-negative, d×N) -> Classifier weight matrix W (K×d) -> Cross-entropy loss with regularization -> Class-means hk and global mean hG

- **Critical path:**
  1. Initialize H and W
  2. Compute cross-entropy loss with regularization
  3. Optimize to find global minimum
  4. Analyze resulting structure (orthogonal class-means, aligned classifiers)
  5. Compute thresholds for minority/complete collapse

- **Design tradeoffs:**
  - Feature dimension d vs. number of classes K: Must have d ≥ K for orthogonal structure
  - Regularization parameters λW and λH: Balance between fitting and generalization
  - Non-negativity constraint: Motivated by ReLU activations but restricts solution space

- **Failure signatures:**
  - Minority Collapse: Minority class-means become zero, classifiers indistinguishable
  - Complete Collapse: All class-means become zero, trivial solution (W,H) = (0,0)
  - Non-orthogonal structure: Suggests violation of assumptions or saddle points

- **First 3 experiments:**
  1. Balanced CIFAR10 subset: Verify recovery of original Neural Collapse properties
  2. Imbalanced CIFAR10 with 10:1 ratio: Test Minority Collapse threshold and orthogonal structure
  3. Vary regularization parameters: Study impact on class-mean norms and collapse thresholds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the geometric structure of learned features and classifier weights change when the feature dimension d is less than the number of classes K?
- Basis in paper: [explicit] The paper explicitly states that it only studies the convergence geometries under the condition that the feature dimension d is at least the number of classes K. It mentions that the geometric structure of the features and classifier in the bottle-neck situation d < K is still unknown and left for future work.
- Why unresolved: The analysis in the paper relies on the assumption that d ≥ K to derive the orthogonal structure of the class-means. When d < K, this structure might not hold, and the optimal solution could have a different geometry.
- What evidence would resolve it: Theoretical analysis or empirical experiments demonstrating the convergence geometry of the features and classifier weights when d < K would resolve this question. This could involve extending the current proofs or conducting experiments with various imbalanced datasets and network architectures.

### Open Question 2
- Question: How does the Minority Collapse threshold vary with different imbalance ratios and regularization parameters?
- Basis in paper: [explicit] The paper derives the exact threshold for Minority Collapse occurrence for every class in terms of the number of training samples and regularization parameters. It provides a formula for the imbalance ratio R at which Minority Collapse happens, but it does not explore how this threshold varies with different imbalance ratios and regularization parameters.
- Why unresolved: The paper provides a theoretical threshold for Minority Collapse, but it does not investigate how this threshold behaves in practice with different imbalance levels and regularization settings. Understanding the sensitivity of the threshold to these parameters could provide insights into avoiding Minority Collapse in highly imbalanced scenarios.
- What evidence would resolve it: Conducting experiments with various imbalance ratios and regularization parameters to empirically determine the Minority Collapse threshold and compare it with the theoretical prediction would provide evidence to resolve this question.

### Open Question 3
- Question: How does the Neural Collapse phenomenon generalize to other loss functions beyond cross-entropy and mean squared error?
- Basis in paper: [explicit] The paper focuses on the Neural Collapse phenomenon under the unconstrained ReLU feature model and cross-entropy loss. It mentions that Neural Collapse has been studied for other loss functions such as mean squared error, supervised contrastive loss, and focal loss, but it does not explore the generalization of Neural Collapse to these other loss functions.
- Why unresolved: While the paper provides a thorough analysis of Neural Collapse under cross-entropy loss, it does not investigate how the phenomenon behaves with other commonly used loss functions. Understanding the generalization of Neural Collapse to different loss functions could provide a more comprehensive understanding of the phenomenon.
- What evidence would resolve it: Conducting theoretical analysis or empirical experiments to study the convergence geometry of the features and classifier weights under other loss functions, such as supervised contrastive loss or focal loss, would provide evidence to resolve this question.

## Limitations
- The theoretical analysis relies on the Unconstrained Feature Model (UFM) approximation, which may not fully capture practical deep network behavior
- The study focuses on the case where training samples are fewer than feature dimension (N ≤ d), leaving open questions about the N > d regime
- Derived collapse thresholds depend on specific regularization parameters that may not generalize across different architectures

## Confidence

- **High confidence**: The proof of orthogonal class-mean structure with varying norms under the non-negative feature model (Mechanism 1)
- **Medium confidence**: The classifier alignment formula and its generalization from balanced to imbalanced settings (Mechanism 2)
- **Medium confidence**: The Minority Collapse threshold derivation and its practical implications (Mechanism 3)

## Next Checks

1. **Extend to larger datasets**: Validate the theoretical predictions on larger-scale imbalanced datasets (e.g., ImageNet-1K subsets) to test the robustness of the orthogonal structure formation beyond CIFAR10
2. **Study the N > d regime**: Investigate scenarios where the number of training samples exceeds the feature dimension to understand how the Neural Collapse properties evolve in this regime
3. **Cross-architecture verification**: Test the theoretical findings across different network architectures (e.g., Vision Transformers, DenseNet) to assess the generality of the non-negative feature model's predictions