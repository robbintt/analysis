---
ver: rpa2
title: 'HiTZ at VarDial 2025 NorSID: Overcoming Data Scarcity with Language Transfer
  and Automatic Data Annotation'
arxiv_id: '2412.10095'
source_url: https://arxiv.org/abs/2412.10095
tags:
- data
- norwegian
- dialect
- intent
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We participated in all three subtasks of the NorSID Shared Task
  at VarDial 2025, focusing on intent detection, slot filling, and dialect identification
  in Norwegian dialects. For intent detection and slot filling, we fine-tuned a multilingual
  multitask model in a cross-lingual setting, leveraging the xSID dataset available
  in 17 languages.
---

# HiTZ at VarDial 2025 NorSID: Overcoming Data Scarcity with Language Transfer and Automatic Data Annotation

## Quick Facts
- arXiv ID: 2412.10095
- Source URL: https://arxiv.org/abs/2412.10095
- Reference count: 7
- Primary result: 1st in Dialect Identification, 2nd in Intent Detection, 3rd in Slot Filling at VarDial 2025 NorSID

## Executive Summary
This paper presents HiTZ's participation in the NorSID Shared Task at VarDial 2025, addressing intent detection, slot filling, and dialect identification for Norwegian dialects. The team employed cross-lingual multitask learning using the xSID dataset across 17 languages to overcome data scarcity for Norwegian. For dialect identification, they fine-tuned a Norwegian-specific pre-trained model (NorBERT3-L) on the development set. The results show consistent performance between development and test sets, achieving strong rankings across all three tasks.

## Method Summary
The approach leverages cross-lingual transfer learning by fine-tuning XLM-RoBERTa-large for intent detection and slot filling in a multitask setting, combining English xSID data with Norwegian dialects. A weighted multitask loss (λ = 0.7) enables joint parameter updates for both tasks. For dialect identification, NorBERT3-L is fine-tuned on the NoMusic development set. The method addresses data scarcity through language transfer from high-resource languages and domain-specific training data.

## Key Results
- Placed 1st in Dialect Identification with 84.17 F1 on test set
- Placed 2nd in Intent Detection with 97.69% accuracy on test set
- Placed 3rd in Slot Filling with 85.37% F1 on test set
- Models showed consistent performance between development and test sets

## Why This Works (Mechanism)

### Mechanism 1
Cross-lingual multitask training with xSID languages improves intent detection and slot filling for Norwegian dialects by leveraging shared linguistic structures and domain features. A single XLM-RoBERTa-large model is fine-tuned for both tasks simultaneously using a weighted sum of cross-entropy losses (λ = 0.7 for slot filling). The multitask objective enables shared parameter updates, allowing knowledge from high-resource languages (English) to transfer to low-resource Norwegian dialects while slot and intent predictions benefit from joint representation learning.

### Mechanism 2
Domain specificity and similar data distributions between development and test sets prevent performance drop. The NoMusic dataset is constructed from manual translations of a limited set of English commands into Norwegian dialects. This results in a narrow semantic space where models learn high-coverage word-level cues for intent classification, and the test set inherits the same command distribution, yielding stable performance.

### Mechanism 3
Norwegian-pretrained encoder (NorBERT3) outperforms multilingual encoders for dialect identification due to richer subword and morphological modeling. NorBERT3-L is fine-tuned on the development split; its pre-training on Norwegian text gives it a better tokenization and representation space for dialectal variation than XLM-RoBERTa-large, which is trained on multilingual web text with less Norwegian coverage.

## Foundational Learning

- **Concept**: Cross-lingual transfer learning
  - Why needed here: The NorSID tasks have limited Norwegian training data, so models must leverage high-resource languages in xSID to learn generalizable representations.
  - Quick check question: If we train only on English data, how does performance on Norwegian intents compare to training on English + Norwegian?

- **Concept**: Multitask learning
  - Why needed here: Intent detection and slot filling are tightly coupled in virtual assistant domains; joint training allows parameter sharing and mutual reinforcement.
  - Quick check question: Does the multitask model maintain or improve slot F1 compared to training two separate single-task models?

- **Concept**: Domain adaptation
  - Why needed here: The data is drawn from a narrow assistant-command domain; models must avoid overfitting to spurious lexical cues while generalizing to similar dialects.
  - Quick check question: If we test on an out-of-domain Norwegian corpus, does intent accuracy drop significantly?

## Architecture Onboarding

- **Component map**: Input tokenizer → XLM-RoBERTa-large (multilingual) or NorBERT3-L (Norwegian) → [CLS] for intent, token-level for slot → classifier heads → multitask loss
- **Critical path**:
  1. Load encoder and classifier heads
  2. Tokenize input text
  3. Forward pass to get [CLS] and token embeddings
  4. Apply classifier heads
  5. Compute weighted multitask loss (intent + slot)
  6. Backpropagate and update parameters
- **Design tradeoffs**:
  - Multilingual vs. Norwegian-pretrained encoder: multilingual covers more languages but with weaker Norwegian representation; Norwegian-pretrained encoder is narrower but deeper in dialectal cues
  - Multitask λ weighting: higher λ prioritizes slot filling but may underfit intent detection given intent accuracy is near ceiling
  - Training data choice: machine-translated vs. manually translated data introduces noise vs. quality
- **Failure signatures**:
  - Near-100% intent accuracy with low slot F1 → model relying on lexical cues instead of semantic understanding
  - Large dev-test gap → distribution shift or overfitting
  - Poor dialect classification → encoder lacks subword/morphological coverage for target dialects
- **First 3 experiments**:
  1. Train multitask XLM-RoBERTa-large on English only vs. English + Norwegian; compare intent accuracy and slot F1
  2. Fine-tune NorBERT3-L on dev-train only; evaluate dialect F1 vs. XLM-RoBERTa-large
  3. Train multitask model with λ = 0.5, 0.7, 0.9; measure impact on slot F1 vs. intent accuracy trade-off

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of dialect identification models vary when trained on manually annotated data versus automatically labeled data from tweets and transcriptions?
- Basis in paper: [explicit] The paper compares results from models trained on automatically labeled tweets and transcriptions against models trained on manually annotated development data.
- Why unresolved: The paper notes that automatic annotation methods perform worse than manual annotation but does not explore hybrid approaches or investigate which types of automatic annotation errors are most detrimental.
- What evidence would resolve it: Experiments comparing manual annotation, automatic annotation, and hybrid approaches, with detailed error analysis of automatic annotation failures.

### Open Question 2
- Question: What is the impact of using more advanced encoder and decoder models (e.g., DeBERTa, Llama 3.1 70B) on the performance of intent detection and slot filling tasks?
- Basis in paper: [inferred] The authors suggest that better encoder and decoder models could improve results but do not test these alternatives.
- Why unresolved: The paper only uses XLM-RoBERTa and NorBERT3 models, leaving the potential benefits of more advanced models unexplored.
- What evidence would resolve it: Performance comparisons between current models and more advanced alternatives like DeBERTa and Llama 3.1 70B on the same tasks and datasets.

### Open Question 3
- Question: How do the dialectal features in NB Samtale transcriptions differ from those in the NoMusic dataset, and why does training on NB Samtale lead to poor results?
- Basis in paper: [explicit] The paper observes that training on NB Samtale leads to poor results on the development set and suggests that dialectal features differ between the datasets.
- Why unresolved: The paper does not provide a detailed analysis of the specific dialectal features that differ between the datasets or why these differences impact model performance.
- What evidence would resolve it: A comparative linguistic analysis of dialectal features in both datasets, identifying specific differences and their impact on model performance.

## Limitations

- The paper does not provide statistical analysis of whether the development and test sets come from the same distribution, despite attributing the lack of performance drop to this assumption.
- No direct comparison is provided between the Norwegian-pretrained NorBERT3-L and multilingual XLM-RoBERTa encoders for dialect identification, making the superiority claim difficult to verify.
- The multitask learning setup's effectiveness is primarily evaluated through final test scores rather than ablation studies or controlled experiments isolating the benefits of joint training versus separate training.

## Confidence

- **High Confidence**: The reported results for the three subtasks (97.69% accuracy for intent detection, 85.37% F1 for slot filling, and 84.17 F1 for dialect identification) are supported by the experimental setup described, though verification would require access to the NorSID test set.
- **Medium Confidence**: The claim that cross-lingual multitask training improves performance is supported by the methodology but lacks ablation studies comparing single-task versus multitask approaches on the same data.
- **Medium Confidence**: The assertion that domain specificity explains the stable dev-test performance is plausible given the dataset description but unverified without statistical testing of distribution similarity.

## Next Checks

1. Conduct statistical tests (e.g., KL divergence, Wasserstein distance) to verify that the development and test sets come from the same distribution, confirming or refuting the domain-specificity hypothesis.
2. Perform an ablation study comparing single-task models versus the multitask model on the same training data to isolate the benefits of joint learning for intent detection and slot filling.
3. Test the models on an out-of-domain Norwegian corpus (e.g., general Norwegian text or a different virtual assistant domain) to assess true generalization beyond the narrow NoMusic dataset commands.