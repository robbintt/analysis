---
ver: rpa2
title: 'GenX: Mastering Code and Test Generation with Execution Feedback'
arxiv_id: '2412.13464'
source_url: https://arxiv.org/abs/2412.13464
tags:
- code
- test
- generation
- cases
- solutions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of improving code and test generation
  in scenarios where test cases are limited or unavailable. The core idea is to concurrently
  train code and test generation models, using execution feedback to iteratively refine
  both.
---

# GenX: Mastering Code and Test Generation with Execution Feedback

## Quick Facts
- arXiv ID: 2412.13464
- Source URL: https://arxiv.org/abs/2412.13464
- Authors: Nan Wang; Yafei Liu; Chen Chen; Haonan Lu
- Reference count: 5
- One-line primary result: Dual execution feedback approach improves code and test generation quality on APPS dataset

## Executive Summary
This work introduces a novel approach to improve both code and test generation quality through concurrent training and iterative refinement using execution feedback. The method addresses the challenge of limited or unavailable test cases by leveraging mutual execution results between generated code and tests to guide the generation process. By introducing dual execution feedback loops and a new scoring function, the approach achieves superior performance compared to models trained on the original dataset alone.

## Method Summary
The proposed approach trains code and test generation models concurrently, using execution feedback to iteratively refine both components. Two data augmentation strategies are employed: generating test cases from ground truth code and filtering correct code solutions using generated test cases. A novel scoring function ranks the quality of generated code and tests based on their mutual execution results. The system uses this feedback loop to continuously improve both code and test generation quality, demonstrating effectiveness on the APPS dataset.

## Key Results
- Outperforms baseline models trained on original dataset with improved pass@1 and pass@10 metrics for code generation
- Achieves higher pass rate and pass num metrics for test generation
- Scoring function effectively ranks high-quality code and tests based on execution results

## Why This Works (Mechanism)
The dual execution feedback approach works by creating a virtuous cycle where code and test generation mutually reinforce each other. Generated tests provide quality feedback on code solutions, while generated code helps validate and refine test cases. This iterative refinement process, guided by execution results, allows the system to progressively improve both code correctness and test coverage.

## Foundational Learning
- **Execution Feedback**: Using program execution results as quality signals for both code and tests; needed to guide iterative refinement, quick check: verify feedback correlates with actual quality
- **Concurrent Training**: Simultaneously training code and test generators; needed for mutual reinforcement, quick check: ensure balanced learning rates
- **Data Augmentation**: Creating synthetic training pairs from ground truth; needed when test data is limited, quick check: validate augmentation preserves distribution
- **Scoring Functions**: Ranking generated artifacts based on mutual execution; needed to select best candidates, quick check: test scoring correlation with human evaluation
- **Iterative Refinement**: Repeated cycles of generation and evaluation; needed for progressive improvement, quick check: monitor convergence behavior

## Architecture Onboarding

Component Map: Ground Truth Code -> Test Generator -> Execution -> Code Generator -> Execution -> Scoring Function -> Selection

Critical Path: The execution feedback loop forms the critical path, where generated tests execute against code solutions (and vice versa) to provide quality signals for the scoring function and subsequent selection.

Design Tradeoffs: The approach trades increased computational overhead for iterative refinement against improved generation quality. Ground truth availability is assumed for data augmentation, limiting practical applicability in some scenarios.

Failure Signatures: Performance degradation may occur if execution feedback becomes noisy or if the scoring function fails to properly distinguish between high and low-quality generations. The iterative refinement may also get stuck in local optima.

First Experiments:
1. Test the scoring function's ability to rank generated code/test pairs on a small validation set
2. Measure the impact of different iteration counts on final generation quality
3. Evaluate the sensitivity of results to variations in execution timeout settings

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily conducted on APPS dataset, limiting generalizability to other benchmarks
- Assumes availability of ground truth solutions for data augmentation, which may not be feasible in all applications
- Does not address computational overhead or scalability concerns for large codebases

## Confidence
- Performance improvements demonstrated on single dataset: Medium
- Methodology assumes idealized execution conditions: Medium
- Scoring function robustness across domains: Medium

## Next Checks
1. Test the approach on additional coding benchmarks like HumanEval or CodeContests to assess generalizability
2. Conduct ablation studies to quantify the contribution of each component (dual training, execution feedback, scoring function)
3. Evaluate performance in scenarios where ground truth solutions are partially or completely unavailable