---
ver: rpa2
title: Distilled ChatGPT Topic & Sentiment Modeling with Applications in Finance
arxiv_id: '2403.02185'
source_url: https://arxiv.org/abs/2403.02185
tags:
- topic
- sentiment
- topics
- sentences
- earnings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of efficiently analyzing large
  volumes of earnings call transcripts for financial insights by leveraging large
  language models (LLMs) and knowledge distillation techniques. The proposed method
  uses ChatGPT to identify financial topics and sentiment from earnings calls, then
  distills this knowledge into smaller, lightweight models using transfer learning
  and supervised training.
---

# Distilled ChatGPT Topic & Sentiment Modeling with Applications in Finance

## Quick Facts
- arXiv ID: 2403.02185
- Source URL: https://arxiv.org/abs/2403.02185
- Reference count: 2
- Primary result: Knowledge distillation from ChatGPT achieves F1 scores of 72.8% (topic) and 77.8% (sentiment) on financial text analysis

## Executive Summary
This study addresses the challenge of efficiently analyzing large volumes of earnings call transcripts for financial insights by leveraging large language models (LLMs) and knowledge distillation techniques. The proposed method uses ChatGPT to identify financial topics and sentiment from earnings calls, then distills this knowledge into smaller, lightweight models using transfer learning and supervised training. The resulting topic and sentiment classification models are evaluated on expert-annotated datasets, achieving F1 scores of 72.8% and 77.8% respectively, compared to 83.1% for ChatGPT. The models are applied to identify correlations between topic propensities, sentiment scores, and future stock returns, demonstrating their utility in quantitative investing scenarios. The approach reduces computational demands while maintaining accuracy, enabling more efficient analysis of earnings calls at scale.

## Method Summary
The method employs knowledge distillation from ChatGPT to train smaller, efficient models for financial topic and sentiment classification. First, ChatGPT annotates a large sample of earnings call sentences with financial topics and sentiment scores. This labeled dataset is then used to train lightweight MPNet-based models with MLP classifiers through supervised learning. The student models are evaluated against expert-annotated benchmarks to measure performance. The approach combines traditional machine learning with LLM capabilities to achieve a balance between computational efficiency and classification accuracy. The distilled models are subsequently applied to analyze topic-sentiment relationships and their correlation with future stock returns in quantitative investing scenarios.

## Key Results
- Distilled models achieve F1 scores of 72.8% (topic) and 77.8% (sentiment) compared to ChatGPT's 83.1%
- Knowledge distillation reduces computational demands while maintaining reasonable accuracy
- Topic propensities and sentiment scores show correlations with future stock returns in quantitative investing applications
- MPNet architecture with MLP layers provides optimal balance between model size and classification performance

## Why This Works (Mechanism)

### Mechanism 1
Knowledge distillation from ChatGPT to smaller models preserves high task performance while drastically reducing computational cost. The teacher model (ChatGPT) generates labeled data that captures its learned representations. A smaller student model is then trained on this synthetic labeled dataset, effectively transferring the teacher's knowledge without requiring the student to learn from scratch. Core assumption: The synthetic labels from ChatGPT contain sufficient information to train a competent student model. Evidence anchors: [abstract] states the distilled models achieve F1 scores of 72.8% (topic) and 77.8% (sentiment) compared to ChatGPT's 83.1%; [section] describes the knowledge distillation pipeline where ChatGPT labels sentences, creating a dataset for supervised training of smaller models. Break condition: If the synthetic labels contain significant errors or hallucinations that the student model cannot correct through training.

### Mechanism 2
Combining topic modeling with sentiment analysis provides more nuanced financial insights than either approach alone. Topics identify what subjects are being discussed in earnings calls, while sentiment scores capture the emotional tone toward those topics. This dual analysis reveals relationships between topic propensities, sentiment, and future stock performance. Core assumption: Financial outcomes correlate with both the subjects discussed and the emotional tone toward those subjects. Evidence anchors: [abstract] mentions applying models to identify correlations between topic propensities, sentiment scores, and future stock returns; [section] shows that sentiment correlation with performance varies by topic and doesn't always align with positivity of statements. Break condition: If financial markets don't respond to management tone or if topic-sentiment relationships change over time.

### Mechanism 3
Using MPNet architecture with MLP layers provides an optimal balance between model size and classification performance. MPNet, being lighter than ChatGPT but still powerful, captures contextual embeddings effectively. Adding an MLP layer allows for task-specific classification while maintaining computational efficiency. Core assumption: MPNet's pre-trained representations contain sufficient information for financial topic and sentiment classification. Evidence anchors: [section] describes choosing MPNet architecture and comparing different pre-trained models, reporting F1 scores; Table 2 shows MPNet achieves 63.1% F1 vs teacher and 72.8% vs human for topic classification. Break condition: If the financial domain requires specialized language understanding that MPNet's general pre-training doesn't capture.

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: Enables transferring capabilities from resource-intensive LLMs to efficient models suitable for production deployment.
  - Quick check question: What is the primary benefit of using knowledge distillation over directly deploying the teacher model?

- Concept: Topic Modeling Fundamentals
  - Why needed here: Understanding traditional approaches (like LDA) helps appreciate why the LLM-based approach improves upon them.
  - Quick check question: What are the main limitations of traditional topic modeling approaches that this paper addresses?

- Concept: Sentiment Analysis Classification
  - Why needed here: Essential for understanding how the models distinguish between positive, negative, and neutral sentiments in financial contexts.
  - Quick check question: How does the sentiment scoring mechanism work in this study?

## Architecture Onboarding

- Component map: Data Ingestion → ChatGPT Annotation Pipeline → Dataset Creation → Student Model Training → Evaluation → Application
- Critical path: ChatGPT annotation → dataset filtering → student model training → performance evaluation against benchmarks
- Design tradeoffs: Model size vs. performance (MPNet vs. full LLMs); Annotation volume vs. quality (filtering noisy ChatGPT outputs); Computational cost vs. inference speed
- Failure signatures: Low F1 scores indicating poor knowledge transfer; Hallucination artifacts in ChatGPT annotations propagating to student models; Overfitting on synthetic data not generalizing to real data
- First 3 experiments:
  1. Run ChatGPT annotation on a small sample (100 sentences) to verify output format compliance
  2. Train student model on the annotated sample and measure performance gap vs. ChatGPT
  3. Test the complete pipeline on a held-out expert-labeled dataset to validate real-world performance

## Open Questions the Paper Calls Out

None

## Limitations

- Performance gap remains substantial between distilled models (72.8% F1) and ChatGPT (83.1% F1), raising questions about complete knowledge transfer
- Reliance on synthetic labeled data from ChatGPT introduces potential propagation of model hallucinations or systematic biases
- Evaluation focuses on specific financial domains (earnings calls) and may not generalize to other financial text types or broader domains
- Real-world impact on investment decisions and robustness to evolving financial language patterns are not fully established

## Confidence

**High Confidence**: The knowledge distillation methodology and MPNet architecture selection are well-established approaches with clear implementation paths. The general finding that smaller models can achieve reasonable performance on financial topic and sentiment classification tasks is well-supported by the experimental results.

**Medium Confidence**: The specific performance metrics (F1 scores) and their comparison between models are reliable within the tested domain, but the exact generalizability to other financial contexts or broader applications remains uncertain. The correlation findings between topic propensities, sentiment scores, and stock returns show promise but require further validation across different market conditions and time periods.

**Low Confidence**: The claim that this approach significantly outperforms traditional methods in practical financial applications lacks comprehensive comparative analysis. The real-world impact on investment decisions and the robustness of the models to evolving financial language patterns are not fully established.

## Next Checks

1. **Generalization Testing**: Evaluate the distilled models on earnings calls from different time periods and across various industries to assess temporal and sector robustness. This would verify whether the models maintain performance when applied to unseen financial contexts.

2. **Error Analysis and Hallucination Detection**: Conduct detailed error analysis on the ChatGPT-generated labels to identify systematic biases or hallucination patterns. Implement automated validation checks during the annotation pipeline to filter out potentially erroneous labels before training the student models.

3. **Cross-Validation with Alternative Teacher Models**: Compare performance when using different large language models as teachers (e.g., Claude, Gemini) to determine whether the results are specific to ChatGPT's capabilities or represent a more general knowledge distillation phenomenon in financial text analysis.