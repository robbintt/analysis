---
ver: rpa2
title: Sampling Audit Evidence Using a Naive Bayes Classifier
arxiv_id: '2403.14069'
source_url: https://arxiv.org/abs/2403.14069
tags:
- sampling
- data
- audit
- evidence
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces a novel sampling method for auditing by integrating
  a Naive Bayes classifier with traditional sampling techniques to handle large volumes
  of audit data efficiently. This integration helps avoid sampling bias, maintain
  randomness and variability, and target riskier samples.
---

# Sampling Audit Evidence Using a Naive Bayes Classifier

## Quick Facts
- arXiv ID: 2403.14069
- Source URL: https://arxiv.org/abs/2403.14069
- Authors: Guang-Yih Sheu; Nai-Ru Liu
- Reference count: 18
- Primary result: A novel method combining Naive Bayes classifier with sampling techniques for efficient, unbiased audit evidence selection from large datasets

## Executive Summary
This paper introduces an innovative approach to audit sampling that integrates Naive Bayes classification with traditional sampling methods to handle large audit datasets efficiently. The method classifies data into classes and then applies user-based, item-based, or hybrid sampling approaches to draw audit evidence. The representativeness index is used to measure how well samples reflect the original data distribution. Experimental results demonstrate that this approach effectively draws unbiased samples while handling complex patterns, correlations, and unstructured data.

## Method Summary
The method involves first classifying audit data using a Naive Bayes classifier, which computes posterior probabilities for each data item belonging to different classes. After classification, three sampling approaches can be employed: user-based sampling draws symmetric samples around class medians for representativeness, item-based sampling targets riskier samples based on asymmetric posterior probability thresholds, and hybrid approaches balance both objectives. The representativeness index quantifies how well the sampled evidence reflects the original data distribution, and statistical validation ensures samples come from the same distribution as the source data.

## Key Results
- Successfully demonstrates unbiased sampling across three diverse datasets (customer ad click data, spam messages, and Panama Papers financial accounts)
- Effectively handles unstructured data while preserving key keywords during sampling
- Achieves improved efficiency in sampling big data compared to traditional methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Naive Bayes classifier's conditional independence assumption allows efficient probabilistic modeling of class membership, which supports targeted sampling of audit evidence.
- Mechanism: By computing posterior probabilities Pr(Ci|Xj) for each data item, the method can identify members of each class and their relative likelihoods. This enables symmetric sampling around class medians for representativeness (user-based) or asymmetric sampling based on posterior probabilities for risk detection (item-based).
- Core assumption: Features Xij are conditionally independent given class Ci, allowing decomposition of joint probabilities into products of marginal probabilities.
- Evidence anchors:
  - [abstract]: "We first classify data using a Naive Bayes classifier into some classes. Next, a user-based, item-based, or hybrid approach is employed to draw audit evidence."
  - [section 3]: "Applying the assumption of features Xi1, Xi2 . . ., Xin are independent of each other yields Pr(Ci|Xj) = Pr(Ci) ∏ Pr(Xjk|Ci) / Pr(Xj)"
  - [corpus]: Weak - no direct corpus neighbors discussing conditional independence in audit sampling contexts
- Break condition: When features exhibit strong dependencies within classes, violating the independence assumption and reducing classification accuracy

### Mechanism 2
- Claim: The representativeness index (RI) provides a quantitative measure of how well sampled audit evidence reflects the original data distribution.
- Mechanism: RI compares the empirical distribution of sampled data to the theoretical distribution using order statistics. Values closer to 1 indicate better representation. The index accounts for sample size and class membership to penalize under-sampling.
- Core assumption: The cumulative distribution function F can be approximated from the posterior probability distribution, and discrete samples can be evaluated against this continuous approximation.
- Evidence anchors:
  - [section 3.1]: "Representativeness index (RI) = 1 - 12N(Ci) / [4[N(Ci)]² - 1] ∑(F(XL) - 2r-1/2N(Ci))"
  - [section 4.1]: "Substituting the resulting audit evidence into Equation (7) obtains the representativeness indices RI listed in the legend of Figure 5."
  - [corpus]: Weak - no corpus neighbors discussing representativeness indices in audit contexts
- Break condition: When sample sizes are too small relative to class populations, causing RI to be unstable or misleading

### Mechanism 3
- Claim: Hybrid sampling combining user-based and item-based approaches balances representativeness and risk detection in audit evidence selection.
- Mechanism: First applies symmetric sampling around class medians to ensure representative coverage, then applies asymmetric sampling based on posterior probabilities to identify high-risk items within the representative sample. This two-stage process maintains coverage while prioritizing suspicious items.
- Core assumption: Riskier items tend to have extreme posterior probability values that can be detected through asymmetric sampling, and these items can be identified within a representative sample.
- Evidence anchors:
  - [abstract]: "Auditors may hybridize these user- and item-based approaches to balance the representativeness and riskiness in selecting audit evidence."
  - [section 3.3]: "Auditors may hybridize the resulting works in Sections 3.1-3.2 to balance representativeness and riskiness."
  - [section 4.3]: "Suppose a 75% confidence interval to sample members of each class Ci... However, we agree that the Ci = 5 class has the risker members."
- Break condition: When the posterior probability distribution doesn't effectively distinguish risky from non-risky items, or when the hybrid approach creates selection bias

## Foundational Learning

- Concept: Bayes' Theorem and conditional probability
  - Why needed here: The entire sampling method relies on computing posterior probabilities Pr(Ci|Xj) to classify data and guide sampling decisions
  - Quick check question: Given Pr(Ci) = 0.3, Pr(Xj|Ci) = 0.8, and Pr(Xj) = 0.5, what is Pr(Ci|Xj)?

- Concept: Probability distributions and percentiles
  - Why needed here: User-based sampling requires identifying symmetric percentiles around medians, and RI calculations depend on cumulative distribution functions
  - Quick check question: If you have a sorted list of posterior probabilities and need the 25th and 75th percentiles, how would you find these values?

- Concept: Statistical hypothesis testing (Kolmogorov-Smirnov test)
  - Why needed here: Validating that sampled audit evidence comes from the same distribution as original data requires comparing empirical distributions
  - Quick check question: What does it mean if the Kolmogorov-Smirnov statistic between two samples is less than the critical value?

## Architecture Onboarding

- Component map: Data preprocessing → Naive Bayes classifier training → Classification → Sampling (user-based/item-based/hybrid) → Representativeness validation → Output audit evidence
- Key components: Classifier model, sampling algorithms, RI calculator, validation tests

- Critical path:
  1. Preprocess data and split into train/test sets
  2. Train Naive Bayes classifier on training data
  3. Apply classifier to full dataset to obtain posterior probabilities
  4. Execute chosen sampling approach (user-based, item-based, or hybrid)
  5. Calculate representativeness index
  6. Validate samples using statistical tests

- Design tradeoffs:
  - Accuracy vs. efficiency: More complex classifiers might improve accuracy but increase computation time
  - Representativeness vs. risk detection: User-based sampling ensures coverage but may miss high-risk items; item-based focuses on risk but may miss representative samples
  - Threshold selection: Setting appropriate thresholds (σ1, σ2, σ3) for item-based sampling requires balancing sensitivity and specificity

- Failure signatures:
  - Low classification accuracy leading to biased sampling
  - RI values consistently below 0.8 indicating poor representation
  - Failed Kolmogorov-Smirnov tests suggesting distribution mismatch
  - Excessive computation time on large datasets

- First 3 experiments:
  1. Customer ad click prediction data with 103 records to test basic functionality and validate unbiased sampling
  2. Spam message dataset with 5,572 messages to test unstructured data handling and keyword preservation
  3. Panama Papers graph data with 535,891 vertices to test scalability and risk detection in complex network structures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the accuracy of the Naive Bayes classifier be further improved to reduce the risk of biased samples?
- Basis in paper: [explicit] The paper states that inaccurate classification results can lead to biased samples and sampling frame errors.
- Why unresolved: While the paper mentions the need to test classification accuracy, it does not provide specific methods or strategies for improving the classifier's performance beyond using baseline models for comparison.
- What evidence would resolve it: Experimental results showing improved classification accuracy and reduced sampling bias when applying advanced techniques or parameter tuning to the Naive Bayes classifier.

### Open Question 2
- Question: What are the optimal threshold values (σ1, σ2, σ3) for the item-based approach, and how can they be determined effectively?
- Basis in paper: [explicit] The paper discusses the need for threshold values in the item-based approach but notes the necessity to inspect variations of prior probabilities to determine proper values.
- Why unresolved: The paper does not provide a concrete method for determining these threshold values, leaving it as a limitation of the approach.
- What evidence would resolve it: A detailed methodology or algorithm for selecting threshold values based on prior probability variations, validated through experiments.

### Open Question 3
- Question: How can the hybrid approach be optimized to better balance representativeness and riskiness in sampling audit evidence?
- Basis in paper: [explicit] The paper introduces a hybrid approach but does not explore its optimization or effectiveness compared to using user-based or item-based approaches alone.
- Why unresolved: The paper does not provide experimental results or theoretical analysis on the performance of the hybrid approach in balancing representativeness and riskiness.
- What evidence would resolve it: Comparative studies or simulations demonstrating the effectiveness of the hybrid approach in achieving a better balance between representativeness and riskiness than the individual approaches.

## Limitations

- The method's accuracy depends heavily on the Naive Bayes classifier's performance, which can be compromised when feature independence assumptions are violated
- The paper doesn't specify exact threshold values (σ1, σ2, σ3) for the item-based approach or confidence interval percentiles for user-based sampling, making precise reproduction difficult
- The representativeness index formula appears to have potential typographical errors in its presentation, and there's limited validation of its statistical properties

## Confidence

- **High confidence**: The overall framework of combining Naive Bayes classification with sampling approaches is well-established and theoretically sound
- **Medium confidence**: The representativeness index as a measure of sampling quality, though the formula implementation needs verification
- **Medium confidence**: The effectiveness of handling unstructured data and preserving keywords in sampling
- **Low confidence**: Specific threshold values and parameter settings for optimal performance across different datasets

## Next Checks

1. **Validate classifier accuracy thresholds**: Test the Naive Bayes classifier's performance metrics (accuracy, precision, recall, F1 score) on each dataset before applying the sampling methods, ensuring classification accuracy exceeds 80% as a prerequisite for reliable sampling

2. **Verify representativeness index calculations**: Implement the RI formula and test it on controlled datasets with known distributions to confirm it accurately measures sample representativeness and identify any typographical errors in the formula

3. **Test threshold sensitivity**: Systematically vary the σ1, σ2, σ3 thresholds and confidence interval percentiles to determine their optimal values for balancing representativeness and risk detection across different datasets and document the sensitivity of results to these parameter choices