---
ver: rpa2
title: Scaling and evaluating sparse autoencoders
arxiv_id: '2406.04093'
source_url: https://arxiv.org/abs/2406.04093
tags:
- latents
- autoencoders
- loss
- figure
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper develops a scalable methodology for training extremely
  sparse autoencoders on language model activations, achieving 16 million latents
  on GPT-4 activations. Key contributions include: (1) using k-sparse autoencoders
  to directly control sparsity and eliminate L1 activation shrinkage; (2) preventing
  dead latents through tied initialization and auxiliary loss; (3) establishing clean
  scaling laws relating autoencoder size, sparsity, and reconstruction error; (4)
  introducing new evaluation metrics including downstream loss, feature recovery via
  probes, explainability via N2G patterns, and ablation sparsity.'
---

# Scaling and evaluating sparse autoencoders

## Quick Facts
- arXiv ID: 2406.04093
- Source URL: https://arxiv.org/abs/2406.04093
- Authors: Leo Gao; Tom Dupré la Tour; Henk Tillman; Gabriel Goh; Rajan Troll; Alec Radford; Ilya Sutskever; Jan Leike; Jeffrey Wu
- Reference count: 40
- Key outcome: Scalable methodology for training extremely sparse autoencoders on language model activations, achieving 16 million latents on GPT-4

## Executive Summary
This paper develops a scalable methodology for training extremely sparse autoencoders (SAEs) on language model activations, achieving 16 million latents on GPT-4 activations. The authors introduce k-sparse autoencoders using TopK activation to directly control sparsity, tied initialization with auxiliary loss to prevent dead latents, and establish clean scaling laws relating autoencoder size, sparsity, and reconstruction error. The work demonstrates that larger autoencoders generally improve on all metrics, with TopK activation outperforming ReLU baselines.

## Method Summary
The methodology centers on k-sparse autoencoders using TopK activation to directly control sparsity without L1 penalties. Key innovations include tied initialization (encoder weights as transpose of decoder) and auxiliary loss to prevent dead latents. The approach uses input normalization, MSE reconstruction loss, and Adam optimization with specific hyperparameters. The largest autoencoder trained has 16 million latents and was trained for 40 billion tokens on GPT-4 activations, achieving reconstruction corresponding to 10% of GPT-4's pretraining compute.

## Key Results
- k-sparse autoencoders with TopK activation outperform ReLU baselines across all evaluation metrics
- Scaling laws show reconstruction error decreases with autoencoder size following power law relationships
- Tied initialization and auxiliary loss effectively prevent dead latents even at 16 million latents
- Largest 16 million latent autoencoder achieves reconstruction quality equivalent to 10% of GPT-4's pretraining compute

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using k-sparse autoencoders directly controls sparsity and eliminates L1 activation shrinkage
- Mechanism: k-sparse autoencoders use a TopK activation function that only keeps the k largest latents, zeroing the rest, removing the need for L1 penalties that shrink activations toward zero
- Core assumption: The TopK function can effectively replace L1 penalties while maintaining reconstruction quality
- Evidence anchors:
  - [abstract] "We propose using k-sparse autoencoders [Makhzani and Frey, 2013] to directly control sparsity, simplifying tuning and improving the reconstruction-sparsity frontier"
  - [section 2.3] "Using k-sparse autoencoders has a number of benefits: • It removes the need for the L1 penalty. L1 is an imperfect approximation of L0, and it introduces a bias of shrinking all positive activations toward zero"
  - [corpus] Weak evidence - no direct supporting papers found in the 8 neighbor papers
- Break condition: If the TopK function fails to adequately approximate the desired L0 sparsity level or reconstruction quality degrades significantly

### Mechanism 2
- Claim: Tied initialization and auxiliary loss prevent dead latents in large autoencoders
- Mechanism: Initializing encoder weights as transpose of decoder weights and using auxiliary loss on top-kaux dead latents prevents latents from becoming permanently inactive during training
- Core assumption: The initialization scheme and auxiliary loss provide sufficient gradient signal to keep latents active
- Evidence anchors:
  - [abstract] "Additionally, we find modifications that result in few dead latents, even at the largest scales we tried"
  - [section 2.4] "We find two important ingredients for preventing dead latents: we initialize the encoder to the transpose of the decoder, and we use an auxiliary loss that models reconstruction error using the top-kaux dead latents"
  - [corpus] Weak evidence - no direct supporting papers found in the 8 neighbor papers
- Break condition: If dead latents persist despite these modifications or if the auxiliary loss causes instability

### Mechanism 3
- Claim: Clean scaling laws relate autoencoder size, sparsity, and reconstruction error
- Mechanism: As the number of latents increases, reconstruction error decreases following a power law relationship, while maintaining sparsity levels through the TopK activation
- Core assumption: The relationship between size, sparsity, and reconstruction error follows predictable patterns that can be modeled
- Evidence anchors:
  - [abstract] "Using these techniques, we find clean scaling laws with respect to autoencoder size and sparsity"
  - [section 3.1.2] "We find that the number of tokens to convergence increases as approximately Θ(n^0.6) for GPT-2 small and Θ(n^0.65) for GPT-4"
  - [corpus] Weak evidence - no direct supporting papers found in the 8 neighbor papers
- Break condition: If the scaling laws break down at extreme sizes or if the relationship becomes non-monotonic

## Foundational Learning

- Concept: Sparse coding and dictionary learning
  - Why needed here: Understanding how sparse autoencoders decompose activations into interpretable features
  - Quick check question: How does sparse coding differ from traditional autoencoders in terms of feature representation?

- Concept: Scaling laws in machine learning
  - Why needed here: Understanding how performance metrics scale with model size and compute budget
  - Quick check question: What is the relationship between compute budget and model performance in language models?

- Concept: Activation functions and their impact on training
  - Why needed here: Understanding how different activation functions affect sparsity and reconstruction quality
  - Quick check question: How does the TopK activation function differ from ReLU in terms of sparsity control?

## Architecture Onboarding

- Component map:
  Input normalization -> Encoder (TopK activation) -> Decoder -> Loss computation -> Auxiliary loss for dead latents -> Parameter updates

- Critical path:
  1. Data preprocessing (normalization)
  2. Encoder forward pass with TopK
  3. Decoder forward pass
  4. Loss computation
  5. Backward pass with auxiliary loss
  6. Parameter updates

- Design tradeoffs:
  - TopK vs L1 penalty: Direct sparsity control vs potential activation shrinkage
  - Tied initialization vs random initialization: Better dead latent prevention vs potential loss of representational power
  - Batch size: Larger batches for parallelism vs potential loss of generalization

- Failure signatures:
  - High dead latent ratio (>50%)
  - Unstable training with NaN losses
  - Poor scaling laws (non-monotonic relationships)
  - Suboptimal reconstruction-sparsity frontier

- First 3 experiments:
  1. Compare TopK vs ReLU on a small autoencoder with varying sparsity levels
  2. Test dead latent prevention with and without tied initialization and auxiliary loss
  3. Verify scaling laws by training autoencoders of different sizes on the same data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the number of active latents (k) affect the irreducibility of the reconstruction loss, and if so, how?
- Basis in paper: [explicit] The paper mentions that including an irreducible loss term substantially improves the quality of fits for both L(C) and L(N), and that the irreducible loss decreases with k (η = -0.085 in Equation 3).
- Why unresolved: The paper does not provide a clear explanation for why the irreducible loss term exists or why it varies with k. The authors hypothesize that activations may contain unstructured noise, but this is not definitively proven.
- What evidence would resolve it: Experiments varying k while keeping other factors constant, and measuring the irreducible loss component. Analyzing the structure of activations at different k values to identify unstructured noise.

### Open Question 2
- Question: Can we develop a metric that accurately measures the interpretability of autoencoder latents without relying on strong assumptions about what features are natural?
- Basis in paper: [explicit] The paper introduces several metrics for evaluating feature quality, including downstream loss, probe loss, explainability via N2G patterns, and ablation sparsity. However, the authors acknowledge that the probe-based metric is noisy and relies on strong assumptions.
- Why unresolved: Current metrics either focus on reconstruction accuracy (which doesn't guarantee interpretability) or rely on specific tasks that may not capture all relevant features.
- What evidence would resolve it: Developing and validating a new metric that directly measures interpretability, perhaps by combining multiple existing metrics or using human evaluation.

### Open Question 3
- Question: How do the scaling laws for sparse autoencoders change when using different activation functions or optimization techniques?
- Basis in paper: [explicit] The paper compares TopK with ReLU, ProLU, and Gated activation functions, finding that TopK generally performs better. However, the scaling laws are only studied for TopK.
- Why unresolved: The paper does not explore how other activation functions or optimization techniques affect the scaling laws.
- What evidence would resolve it: Training autoencoders with different activation functions and optimization techniques, and measuring how the scaling laws (e.g., L(C), L(N), L(N,K)) change.

## Limitations
- Reliance on GPT-4 for evaluation introduces potential distribution shifts between training (GPT-2 small) and evaluation data
- Scaling laws may not generalize to other model architectures or modalities beyond language models
- Interpretability claims lack direct human evaluation studies to validate semantic meaningfulness

## Confidence
- **High Confidence**: Claims about technical implementation (k-sparse autoencoder architecture, tied initialization, auxiliary loss for dead latents) and basic scaling relationships between autoencoder size and reconstruction error
- **Medium Confidence**: Claims about interpretability metrics (feature recovery, N2G patterns, ablation sparsity) and downstream task performance improvements
- **Low Confidence**: Claims about the semantic meaningfulness of discovered features and their alignment with human concepts, as these rely on indirect metrics rather than direct human evaluation

## Next Checks
1. **Human Evaluation Study**: Conduct a rigorous human evaluation where annotators assess the interpretability and semantic meaningfulness of top features discovered by the autoencoders across multiple layers and models

2. **Cross-Architecture Validation**: Train and evaluate similar sparse autoencoders on different model architectures (e.g., Llama, Claude) and modalities (vision, code) to test the generalizability of the scaling laws and feature discovery capabilities

3. **Ablation of Dead Latent Prevention**: Systematically disable the tied initialization and auxiliary loss components to quantify their individual contributions to preventing dead latents and improving reconstruction quality across different scale regimes