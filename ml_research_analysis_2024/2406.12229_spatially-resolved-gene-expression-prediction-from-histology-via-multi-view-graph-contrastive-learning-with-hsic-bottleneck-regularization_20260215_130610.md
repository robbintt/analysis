---
ver: rpa2
title: Spatially Resolved Gene Expression Prediction from Histology via Multi-view
  Graph Contrastive Learning with HSIC-bottleneck Regularization
arxiv_id: '2406.12229'
source_url: https://arxiv.org/abs/2406.12229
tags: []
core_contribution: This paper proposes a Multi-view Graph Contrastive Learning framework
  with HSIC-bottleneck Regularization (ST-GCHB) for predicting spatially resolved
  gene expression from histology images. The method integrates intra-modal graph contrastive
  learning, cross-modal contrastive learning, and HSIC-bottleneck regularization to
  capture spatial dependencies among spots and align multi-modal data.
---

# Spatially Resolved Gene Expression Prediction from Histology via Multi-view Graph Contrastive Learning with HSIC-bottleneck Regularization

## Quick Facts
- arXiv ID: 2406.12229
- Source URL: https://arxiv.org/abs/2406.12229
- Reference count: 25
- Achieved average correlation of 0.1509 for marker genes, 0.4310 for highly variable genes, and 0.3545 for highly expressed genes on DLPFC dataset

## Executive Summary
This paper presents ST-GCHB, a novel framework for predicting spatially resolved gene expression from histology images. The method integrates intra-modal graph contrastive learning, cross-modal contrastive learning, and HSIC-bottleneck regularization to capture spatial dependencies among spots and align multi-modal data. The approach was evaluated on the DLPFC dataset, demonstrating significant improvements over existing methods like BLEEP and Hist2ST.

## Method Summary
ST-GCHB integrates intra-modal graph contrastive learning (DGI) for spatial gene expression, HSIC-bottleneck regularization for feature purification, and cross-modal contrastive learning for multi-modal alignment. The method uses ResNet50 for image feature extraction, PCA for gene expression dimensionality reduction, and contrastive loss for alignment. Training employs AdamW optimizer with batch size 16 and 5-fold cross-validation.

## Key Results
- Average correlation of 0.1509 for marker genes (ATP2B4, RASGRF2, LAMP5, B3GALT2)
- Average correlation of 0.4310 for top 50 highly variable genes
- Average correlation of 0.3545 for top 50 highly expressed genes
- Outperforms baseline methods BLEEP and Hist2ST on DLPFC dataset

## Why This Works (Mechanism)

### Mechanism 1: HSIC-bottleneck Regularization
- Reduces redundancy between modalities and enhances feature efficiency
- Measures statistical dependence between gene expression features (Tα) and image features (Tρ), minimizing mutual information between raw input (Tι) and compressed representation (Tρ) while maximizing alignment with target output
- Break condition: If regularization removes essential features, prediction accuracy drops despite improved efficiency

### Mechanism 2: Graph Contrastive Learning
- Captures spatial dependencies among spots that traditional methods ignore
- Creates positive samples from spatially adjacent spots (Aij = 1 when distance < R) and negative samples by shuffling node representations
- Break condition: Poor spatial adjacency threshold R fails to reflect true biological relationships

### Mechanism 3: Cross-modal Contrastive Learning
- Aligns gene expression and histopathology features in shared latent space
- Computes similarity matrices between modalities and within modalities, then aligns matching pairs using cross-entropy loss
- Break condition: Mis-tuned temperature hyperparameter T makes contrastive signal too weak or all pairs equally similar

## Foundational Learning

### Spatial transcriptomics technology
- Why needed: The entire prediction task depends on understanding how gene expression data is spatially organized across tissue spots
- Quick check: What spatial resolution does 10X Visium typically provide, and how does this affect adjacency calculations?

### Graph neural networks and contrastive learning
- Why needed: The DGI module and cross-modal contrastive learning require understanding of graph representations and contrastive optimization
- Quick check: How does the InfoMax principle in DGI differ from standard supervised node classification?

### Hilbert space embeddings and kernel methods
- Why needed: HSIC-bottleneck relies on kernel-based measures of statistical dependence in reproducing kernel Hilbert spaces
- Quick check: Why is the Gaussian kernel commonly used for HSIC computation, and what happens if you change the kernel bandwidth?

## Architecture Onboarding

### Component map
Input preprocessing -> DGI graph encoder (gene modality) -> ResNet50 encoder (image modality) -> HSIC-bottleneck regularization -> Cross-modal contrastive alignment -> K-nearest neighbor prediction

### Critical path
Adjacency matrix construction -> DGI encoding -> ResNet50 feature extraction -> HSIC regularization -> Cross-modal alignment -> Nearest neighbor prediction

### Design tradeoffs
Using graph structure adds spatial information but requires careful adjacency threshold selection; HSIC regularization improves efficiency but may remove useful features; cross-modal contrastive learning enables alignment but needs balanced temperature tuning

### Failure signatures
Poor spatial predictions indicate DGI adjacency issues; reduced correlation with ground truth suggests HSIC regularization is too aggressive; alignment failure manifests as low cross-modal similarity scores

### First 3 experiments
1. Test adjacency threshold R values (1-5 spots distance) and measure correlation impact
2. Compare with and without HSIC regularization on a subset of genes to assess feature efficiency
3. Evaluate cross-modal similarity scores before and after contrastive alignment to verify learning progress

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal trade-off between the dimensionality reduction and the HSIC-bottleneck regularization for different datasets and tasks?
- Basis: The paper mentions these techniques enhance performance but doesn't discuss optimal balance
- Why unresolved: No systematic study on tuning the balance across different datasets and tasks
- What evidence would resolve it: Systematic study varying dimensionality reduction ratio and HSIC regularization strength across datasets

### Open Question 2
- Question: How does ST-GCHB performance compare to other methods on datasets with different tissue types and disease states?
- Basis: Only evaluated on DLPFC dataset, no comparison on diverse datasets
- Why unresolved: Only one dataset evaluated, lacking comprehensive comparison
- What evidence would resolve it: Comprehensive evaluation on diverse datasets with different tissue types and disease states

### Open Question 3
- Question: What is the impact of spatial resolution and spot size on ST-GCHB performance?
- Basis: Spatial information is considered but impact of resolution and spot size is not discussed
- Why unresolved: No study on how spatial resolution and spot size affect performance
- What evidence would resolve it: Study varying spatial resolution and spot size in input data, reporting performance impact

## Limitations
- Lack of ablation studies isolating each mechanism's contribution to prediction accuracy
- No statistical significance testing comparing performance improvements
- Specific implementation details of HSIC-bottleneck regularization not fully explained or validated in isolation

## Confidence
- High confidence: Overall architecture design integrating graph contrastive learning with cross-modal alignment is technically sound
- Medium confidence: Quantitative improvements over baselines reported but without statistical significance testing
- Low confidence: Specific implementation details of HSIC-bottleneck regularization and claimed efficiency benefits not fully explained

## Next Checks
1. Perform ablation studies removing each component (DGI, HSIC-bottleneck, cross-modal contrastive learning) to quantify individual contributions
2. Test model generalization on different spatial transcriptomics dataset with varying spatial resolution and tissue types
3. Conduct statistical significance testing (paired t-tests) comparing ST-GCHB performance against baselines across all genes and gene subsets