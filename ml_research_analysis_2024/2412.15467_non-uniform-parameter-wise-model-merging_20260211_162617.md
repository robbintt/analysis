---
ver: rpa2
title: Non-Uniform Parameter-Wise Model Merging
arxiv_id: '2412.15467'
source_url: https://arxiv.org/abs/2412.15467
tags:
- merging
- merge
- parameters
- learning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Non-Uniform Parameter-wise Model Merging
  (NP Merge), a novel method for combining machine learning models that learns the
  contribution of each parameter through gradient-based optimization. Unlike traditional
  uniform averaging, NP Merge assigns individual weights to each parameter, allowing
  for more flexible and precise model aggregation.
---

# Non-Uniform Parameter-Wise Model Merging

## Quick Facts
- arXiv ID: 2412.15467
- Source URL: https://arxiv.org/abs/2412.15467
- Reference count: 40
- Primary result: NP Merge outperforms traditional model merging by learning per-parameter interpolation weights

## Executive Summary
NP Merge introduces a novel approach to model merging that learns individual interpolation weights for each parameter through gradient-based optimization. Unlike traditional uniform averaging methods, this technique assigns unique contribution values to each parameter, enabling more flexible and precise model aggregation. The method demonstrates particular effectiveness when merging models trained on different data distributions, including disjoint class subsets, and consistently outperforms existing approaches while maintaining accuracy levels close to ensemble methods without their computational overhead.

## Method Summary
NP Merge works by optimizing per-parameter interpolation weights (α values) between aligned models using gradient descent. The method first aligns models through permutation-based alignment to ensure linear mode connectivity, then initializes all α parameters to 0.5 and optimizes them over 10 epochs using ADAM with learning rate 0.01. The approach is tested on CIFAR-10 and CIFAR-100 datasets with VGG11 and ResNet20 architectures, using both 80-20 splits and Dirichlet class distributions. The method shows particular strength when merging models trained on non-uniform data distributions and demonstrates robustness to limited training data.

## Key Results
- Consistently outperforms existing model merging approaches across multiple architectures and datasets
- Maintains accuracy levels close to ensemble methods while avoiding their computational overhead
- Shows robustness to limited training data and scales effectively to multiple model merging through pairwise combinations

## Why This Works (Mechanism)
NP Merge leverages the observation that different parameters contribute unequally to model performance when merging models trained on different data distributions. By learning individual weights for each parameter rather than using uniform averaging, the method can capture complex relationships between model parameters that arise from different training conditions. The gradient-based optimization allows the method to adapt these weights based on the specific characteristics of the optimization data, leading to better generalization than static merging approaches.

## Foundational Learning
- Linear mode connectivity: Why needed - ensures models can be interpolated without loss barriers; Quick check - verify loss remains stable during interpolation
- Permutation-based model alignment: Why needed - matches corresponding parameters across models; Quick check - confirm correlation matrix correctly identifies parameter correspondences
- Gradient-based weight optimization: Why needed - learns optimal per-parameter contributions; Quick check - monitor α parameter convergence during training
- ADAM optimizer: Why needed - handles sparse gradients and adaptive learning rates; Quick check - verify parameter updates follow expected patterns
- Dirichlet distribution sampling: Why needed - creates non-uniform class distributions for testing; Quick check - confirm class proportions match intended distribution
- Linear interpolation: Why needed - provides smooth transition between model parameters; Quick check - validate interpolation formula implementation

## Architecture Onboarding

**Component Map:**
Data preparation -> Model training -> Permutation alignment -> NP Merge optimization -> Evaluation

**Critical Path:**
Model alignment -> α parameter initialization -> Gradient optimization -> Performance evaluation

**Design Tradeoffs:**
The method trades increased memory overhead (loading all models simultaneously) for improved accuracy through learned per-parameter weights. Computational cost increases during the optimization phase but is offset by avoiding expensive fine-tuning of merged models.

**Failure Signatures:**
Loss barrier after merging indicates alignment issues; overfitting during α optimization suggests insufficient or poorly chosen optimization data; poor performance on aligned models may indicate implementation errors in the interpolation logic.

**First Experiments:**
1. Merge two VGG11 models trained on disjoint CIFAR-10 class subsets and verify accuracy improvement over uniform averaging
2. Test NP Merge with varying optimization dataset sizes to establish data requirements
3. Compare memory usage during merging against baseline methods to quantify overhead

## Open Questions the Paper Calls Out
None

## Limitations
- Requires careful model alignment before merging, adding computational overhead and complexity
- Demands optimization data for learning per-parameter weights, creating additional data requirements
- Significant memory overhead as all models must be loaded simultaneously during merging
- Performance on extremely large-scale models and real-world federated learning scenarios remains unexplored

## Confidence
- **High Confidence:** The core algorithmic contribution and its theoretical foundation are well-established and reproducible
- **Medium Confidence:** The empirical results showing superior performance over baselines are convincing but limited to specific datasets and architectures
- **Medium Confidence:** The claims about scalability to multiple models through pairwise merging are supported but need further validation on larger model sets

## Next Checks
1. **Scale Testing:** Validate NP Merge on larger architectures (ResNet50, Vision Transformers) and datasets beyond CIFAR to assess scalability claims
2. **Memory Profiling:** Conduct detailed memory usage analysis during the merging process to quantify the overhead and identify optimization opportunities
3. **Alignment Robustness:** Test the method's sensitivity to alignment quality by varying alignment procedures and measuring the impact on final merged model performance