---
ver: rpa2
title: Disentangle Sample Size and Initialization Effect on Perfect Generalization
  for Single-Neuron Target
arxiv_id: '2405.13787'
source_url: https://arxiv.org/abs/2405.13787
tags:
- sample
- initialization
- size
- seed
- generalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper systematically investigates how initialization scale
  and sample size affect the ability of overparameterized two-layer neural networks
  to recover single-neuron target functions. The key findings are: (1) Smaller initialization
  scales lead to better generalization performance.'
---

# Disentangle Sample Size and Initialization Effect on Perfect Generalization for Single-Neuron Target

## Quick Facts
- arXiv ID: 2405.13787
- Source URL: https://arxiv.org/abs/2405.13787
- Reference count: 40
- One-line primary result: Smaller initialization scales lead to better generalization in overparameterized networks, with two critical sample size thresholds determining recovery capability.

## Executive Summary
This paper systematically investigates how initialization scale and sample size affect the ability of overparameterized two-layer neural networks to recover single-neuron target functions. Through controlled experiments, the authors identify that smaller initialization scales improve generalization performance by constraining parameter trajectories to low-dimensional manifolds determined by an "initial imbalance ratio." They empirically delineate two critical sample size thresholds—the "optimistic sample size" (minimum needed for recovery with zero-measure initialization set) and the "separation sample size" (minimum needed for recovery with positive-measure initialization set)—that align with theoretical frameworks for perfect generalization in overparameterized settings.

## Method Summary
The authors conduct controlled experiments on synthetic datasets generated from single-neuron target functions using two-layer neural networks with tanh activation. They systematically vary initialization scales (ranging from 10^-20 to 10^-4) and sample sizes (2-6 samples in 1D experiments, 500 in 500D experiment) while training with gradient descent. The training procedure uses fixed learning rates (0.01-0.5 depending on experiment) and continues until loss reaches 10^-15 or maximum iterations (10^6-10^7). Recovery is measured by the L2 distance between the learned function and the target function, with perfect recovery defined as zero generalization error.

## Key Results
- Smaller initialization scales are consistently associated with improved generalization performance across different sample sizes
- Two critical sample size thresholds are identified: the "optimistic sample size" and the "separation sample size"
- The "initial imbalance ratio" C/||C||2 governs training dynamics and generalization under small initialization, constraining parameter trajectories to specific manifolds
- At sample size equal to parameter count, all small-scale initialization can recover the target function

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Small initialization scales lead to better generalization in overparameterized networks.
- Mechanism: As initialization scale decreases, parameter trajectories become constrained to low-dimensional manifolds determined by the initial imbalance ratio C/||C||2, which guides the system toward better generalization.
- Core assumption: The activation function is analytic and the data vector γ is non-zero.
- Evidence anchors:
  - [abstract]: "Our experiments reveal that a smaller initialization scale is associated with improved generalization"
  - [section]: "Our experimental findings suggest a relationship between a small initialization scale of network's parameters and a lower generalization error"
  - [corpus]: Weak evidence - related papers discuss initialization but not this specific small-scale effect
- Break condition: If the activation function is not analytic or γ=0, the theoretical guarantees no longer hold.

### Mechanism 2
- Claim: The "initial imbalance ratio" C/||C||2 governs training dynamics and generalization under small initialization.
- Mechanism: Under small initialization, the trajectory of parameters is determined by the normalized vector C/||C||2, where C is a vector combining the initial parameters and data information. This ratio determines both the trajectory and the final convergence point.
- Core assumption: The gradient flow solution has a well-defined limit as initialization scale approaches zero.
- Evidence anchors:
  - [abstract]: "we identify a critical quantity called the 'initial imbalance ratio' that governs training dynamics and generalization under small initialization"
  - [section]: "The normalized vector C(θ0) / ||C(θ0)||2, representing the initial relative magnitudes of all neurons, determines trajectory of the parameters"
  - [corpus]: Weak evidence - no related papers directly discuss this specific ratio mechanism
- Break condition: If the system doesn't converge to a limit point or if the conditions of Theorem 2 aren't met.

### Mechanism 3
- Claim: Two critical sample size thresholds determine recovery capability: the "optimistic sample size" and the "separation sample size".
- Mechanism: Below the optimistic sample size, recovery is impossible. At the optimistic sample size, recovery is possible but only for specific initialization ratios (zero-measure set). At the separation sample size, recovery becomes achievable for a non-zero fraction of initialization. When sample size equals parameter count, all small-scale initialization can recover the target.
- Core assumption: The target set Q* has the structural properties described in the theoretical framework.
- Evidence anchors:
  - [abstract]: "Additionally, we empirically delineate two critical thresholds in sample size—termed the 'optimistic sample size' and the 'separation sample size'—that align with the theoretical frameworks"
  - [section]: "Our empirical results highlight two critical thresholds in sample size—the optimistic sample size and the separation sample size"
  - [corpus]: Weak evidence - related papers discuss sample size effects but not these specific thresholds
- Break condition: If the target function or network architecture deviates significantly from the single-neuron case studied here.

## Foundational Learning

- Concept: Gradient flow and its relationship to gradient descent
  - Why needed here: The paper analyzes the continuous-time limit of gradient descent (gradient flow) to understand the dynamics under small initialization
  - Quick check question: What is the mathematical relationship between gradient flow and gradient descent with small learning rates?

- Concept: Eigenvalue decomposition and its role in determining dominant dynamics
  - Why needed here: The trajectory under small initialization is governed by the largest eigenvalue of the Hessian, which determines the dominant direction of parameter evolution
  - Quick check question: How does the largest eigenvalue of the Hessian affect the long-term behavior of gradient flow?

- Concept: Implicit regularization and its connection to initialization
  - Why needed here: Small initialization implicitly regularizes the network by constraining trajectories to specific manifolds, affecting which global minima are reached
  - Quick check question: How does initialization scale affect which global minima gradient descent converges to in overparameterized networks?

## Architecture Onboarding

- Component map:
  - Single-neuron target function f*(x) = a0σ(w0⊤x)
  - Two-layer neural network fθ(x) = Σaiσ(wi⊤x)
  - Loss function ℓ(θ) = ½Σ(fθ(xi) - yi)²
  - Initialization scale σ controlling Gaussian initialization
  - Sample size n determining the dataset size

- Critical path:
  1. Generate dataset from target function
  2. Initialize network parameters with Gaussian distribution (scale σ)
  3. Train using gradient descent until convergence
  4. Measure generalization error (L2 distance between f* and learned function)
  5. Analyze results across different initialization scales and sample sizes

- Design tradeoffs:
  - Smaller initialization scales improve generalization but may slow convergence
  - Larger sample sizes improve recovery probability but increase computational cost
  - The choice of activation function affects the theoretical guarantees (must be analytic)

- Failure signatures:
  - Generalization error remains high despite small initialization → possible data issues or non-convergence
  - Convergence to saddle points instead of minima → learning rate too high or initialization problematic
  - No recovery at optimistic sample size → target set Q* may not be accessible from given initialization

- First 3 experiments:
  1. Vary initialization scale σ while keeping sample size fixed at 2, 3, 4, 5, 6 to observe generalization trends
  2. Fix small initialization scale and vary sample size to identify the optimistic and separation sample sizes
  3. For fixed small initialization and sample size at separation threshold, test multiple random seeds to measure probability of recovery

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis is restricted to single-neuron target functions, limiting applicability to real-world problems
- Only one activation function (tanh) is considered, though theoretical framework requires analyticity
- Sample sizes studied are very small (2-6 samples in 1D experiments), limiting practical relevance

## Confidence

**High confidence**: The empirical observation that smaller initialization scales generally improve generalization performance. This is well-supported by the experimental results across multiple sample sizes and is consistent with established literature on initialization effects.

**Medium confidence**: The identification of optimistic and separation sample size thresholds. While the experimental evidence is compelling for the specific single-neuron case studied, the theoretical framework supporting these thresholds is not fully developed, and generalization to more complex targets remains unclear.

**Low confidence**: The claim that the initial imbalance ratio C/||C||2 governs training dynamics in a universal way. This mechanism is theoretically plausible but lacks direct empirical validation beyond the specific experimental setup, and the relationship to other forms of implicit regularization is not explored.

## Next Checks

1. **Cross-target validation**: Repeat the experiments with multi-neuron target functions (e.g., sums of multiple neurons) to test whether the optimistic and separation sample size thresholds generalize beyond the single-neuron case.

2. **Activation function robustness**: Test whether the small initialization benefits persist across different activation functions (ReLU, sigmoid, softplus) to validate that the mechanism relies on analyticity rather than specific properties of tanh.

3. **Scaling to realistic sample sizes**: Extend the experiments to larger sample sizes (100-1000) to verify whether the theoretical thresholds maintain predictive power in more practical settings and to examine how the probability of recovery scales with sample size beyond the separation threshold.