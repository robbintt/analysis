---
ver: rpa2
title: Are large language models superhuman chemists?
arxiv_id: '2404.01475'
source_url: https://arxiv.org/abs/2404.01475
tags:
- questions
- arxiv
- performance
- github
- chemistry
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ChemBench, an automated framework for evaluating
  the chemical knowledge and reasoning abilities of large language models (LLMs) against
  expert chemists. The authors curated over 2,700 question-answer pairs across diverse
  chemistry topics and skills, and evaluated leading open- and closed-source LLMs.
---

# Are large language models superhuman chemists?

## Quick Facts
- arXiv ID: 2404.01475
- Source URL: https://arxiv.org/abs/2404.01475
- Authors: Adrian Mirza; Nawaf Alampara; Sreekanth Kunchapu; Martiño Ríos-García; Benedict Emoekabu; Aswanth Krishnan; Tanya Gupta; Mara Schilling-Wilhelmi; Macjonathan Okereke; Anagha Aneesh; Amir Mohammad Elahi; Mehrdad Asgari; Juliane Eberhardt; Hani M. Elbeheiry; María Victoria Gil; Maximilian Greiner; Caroline T. Holick; Christina Glaubitz; Tim Hoffmann; Abdelrahman Ibrahim; Lea C. Klepsch; Yannik Köster; Fabian Alexander Kreth; Jakob Meyer; Santiago Miret; Jan Matthias Peschel; Michael Ringleb; Nicole Roesner; Johanna Schreiber; Ulrich S. Schubert; Leanne M. Stafast; Dinga Wonanke; Michael Pieler; Philippe Schwaller; Kevin Maik Jablonka
- Reference count: 40
- Key outcome: Best models outperformed best human chemists on average across 2,700+ chemistry questions

## Executive Summary
This paper introduces ChemBench, an automated framework for evaluating large language models' chemical knowledge against expert chemists. The authors compiled over 2,700 question-answer pairs across diverse chemistry topics and skills, then evaluated leading LLMs alongside human experts. Results demonstrate that the best models significantly outperform average human chemists, though they struggle with basic tasks and provide overconfident predictions. The study highlights both the impressive capabilities of LLMs in chemistry and the need for further research to improve their reliability and safety.

## Method Summary
The authors developed ChemBench, a comprehensive evaluation framework consisting of a curated question corpus (2,700+ pairs), automated model evaluation pipelines, human expert baseline surveys, and standardized leaderboards. Questions were sourced from diverse materials including university exams, research papers, and curated chemical databases, then categorized by chemistry subfield and skill type. Models were evaluated using various prompting strategies and parsing methods, with results compared against responses from 24 human experts across multiple institutions. The framework supports both multiple-choice and open-ended questions to assess different aspects of chemical knowledge and reasoning.

## Key Results
- Best LLMs outperformed the best human chemists on average across the full benchmark
- Models showed significant variation in performance across chemistry subfields, with particular struggles in analytical chemistry and chemical preference tasks
- LLMs largely failed to align with expert chemists' preferences, performing at or near random guessing levels on chemical preference questions
- Models provided overconfident predictions, with little correlation between predicted difficulty and actual accuracy for several models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs can be systematically evaluated for domain-specific capabilities using curated question-answer pairs.
- **Mechanism:** The ChemBench framework provides a structured approach to assess LLM performance in chemistry by compiling a large corpus of questions across diverse topics and skills, then comparing model performance against expert chemists.
- **Core assumption:** Questions in the corpus are valid representations of the required chemical knowledge and reasoning skills, and human expert responses serve as reliable ground truth.
- **Evidence anchors:**
  - [abstract] "We curated more than 2,700 question-answer pairs, evaluated leading open- and closed-source LLMs, and found that the best models outperformed the best human chemists in our study on average."
  - [section] "To compile our benchmark corpus, we utilized a broad list of sources... ranging from completely novel, manually crafted questions over university exams to semi-automatically generated questions based on curated subsets of data in chemical databases."
- **Break condition:** If the questions do not accurately reflect the full scope of chemistry knowledge or if the human experts are not truly representative of the field, the benchmark's validity is compromised.

### Mechanism 2
- **Claim:** Specialized LLM performance in chemistry varies significantly across topics and question types.
- **Mechanism:** By categorizing questions into topics (e.g., analytical chemistry, organic chemistry) and skill types (knowledge, reasoning, calculation, intuition), the ChemBench framework reveals that model performance is not uniform and can highlight specific areas of strength and weakness.
- **Core assumption:** The categorization of questions is accurate and meaningful, and the variation in performance is due to the inherent difficulty of the topics and skills rather than artifacts of the evaluation process.
- **Evidence anchors:**
  - [abstract] "While general and technical receive relatively high scores for many models, this is not the case for topics such as toxicity and safety or analytical chemistry."
  - [section] "In the subfield of analytical chemistry, the prediction of the number of signals observable in a nuclear magnetic resonance (NMR) spectrum proved difficult even for the best models (e.g., 22 percent correct answers for o1)."
- **Break condition:** If the categorization is flawed or the evaluation process introduces bias, the observed variation in performance may not accurately reflect the true capabilities of the models.

### Mechanism 3
- **Claim:** Current LLMs struggle with tasks requiring chemical intuition and have unreliable confidence estimates.
- **Mechanism:** The ChemBench framework includes questions designed to test chemical preference and intuition, and it also prompts models to estimate their confidence in their answers, revealing limitations in these areas.
- **Core assumption:** The chemical preference questions are valid measures of chemical intuition and that the confidence estimation method is appropriate for capturing the models' uncertainty.
- **Evidence anchors:**
  - [abstract] "Our models largely fail to align with expert chemists' preferences... Their performance is often indistinguishable from random guessing."
  - [section] "For some models, there is no significant correlation between the estimated difficulty and whether the models answered the question correctly or not."
- **Break condition:** If the chemical preference questions are not valid or the confidence estimation method is flawed, the conclusions about LLM limitations in intuition and confidence may be incorrect.

## Foundational Learning

- **Concept:** Understanding the difference between multiple-choice questions (MCQs) and open-ended questions.
  - **Why needed here:** ChemBench uses both MCQ and open-ended questions to assess a broader range of chemical capabilities, reflecting real-world chemistry education and research.
  - **Quick check question:** What is the main difference between MCQ and open-ended questions, and why is it important to include both in a chemistry benchmark?

- **Concept:** Familiarity with the different subfields of chemistry (e.g., analytical, organic, inorganic, physical, technical, materials science).
  - **Why needed here:** The ChemBench framework categorizes questions into various chemistry topics to provide a comprehensive assessment of LLM performance across the field.
  - **Quick check question:** Name at least three subfields of chemistry and provide an example of a topic or concept that would be relevant to each.

- **Concept:** Knowledge of basic chemical concepts such as molecular structure, chemical reactions, and chemical properties.
  - **Why needed here:** A foundational understanding of chemistry is essential for interpreting the results of the ChemBench framework and understanding the significance of the findings.
  - **Quick check question:** Explain the difference between a covalent bond and an ionic bond, and provide an example of each.

## Architecture Onboarding

- **Component map:** Question corpus -> Model evaluation pipeline -> Human baseline survey -> Leaderboard
- **Critical path:** The critical path is the process of curating the question corpus, evaluating the models, collecting human responses, and generating the leaderboard. Each step depends on the previous one, and delays in any step will impact the overall timeline.
- **Design tradeoffs:** The ChemBench framework prioritizes automation and standardization to enable large-scale evaluation of LLMs. This comes at the cost of flexibility and customization, as the framework is designed to work with a specific set of questions and models.
- **Failure signatures:** Common failure modes include incorrect question categorization, biased human responses, and inaccurate model evaluation. These failures can lead to misleading results and incorrect conclusions about LLM capabilities.
- **First 3 experiments:**
  1. Run the model evaluation pipeline on a small subset of the question corpus to verify the accuracy of the prompting and parsing methods.
  2. Conduct a pilot human baseline survey with a small group of experts to identify any issues with the survey design or data collection process.
  3. Generate a preliminary leaderboard with a small set of models to test the functionality and identify any bugs or inconsistencies.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLMs be improved to better handle knowledge-intensive chemistry questions that require lookup in specialized databases?
- Basis in paper: [explicit] The authors note that models struggle with knowledge-intensive questions and that simple retrieval-augmented generation (RAG) systems like PaperQA2 do not suffice, as the required knowledge often resides in specialized databases rather than papers.
- Why unresolved: Current models and RAG systems are not well-integrated with specialized chemistry databases, and there is no clear method to effectively incorporate such databases into LLM workflows.
- What evidence would resolve it: Development and evaluation of LLM systems that integrate specialized chemistry databases (e.g., PubChem, Gestis) and demonstrate improved performance on knowledge-intensive chemistry questions.

### Open Question 2
- Question: How can LLMs be trained to better align with human chemists' preferences in drug discovery and other chemical applications?
- Basis in paper: [explicit] The authors find that models largely fail to align with expert chemists' preferences in chemical preference tasks, performing at or near random guessing levels despite excelling in other tasks.
- Why unresolved: Preference tuning for chemical settings is a promising but unexplored area, and there is no established method to train LLMs to mimic human chemists' decision-making processes.
- What evidence would resolve it: Development and evaluation of LLM systems trained on preference data from human chemists, demonstrating improved alignment with expert preferences in chemical applications.

### Open Question 3
- Question: How can LLMs be improved to better reason about chemical structures, such as predicting the number of NMR signals or isomers?
- Basis in paper: [inferred] The authors find that model performance does not correlate with molecular complexity indicators, suggesting that models may not be able to reason about structures in the expected way but instead rely on proximity to training data.
- Why unresolved: The exact mechanisms by which LLMs process and reason about chemical structures are not well understood, and current models may not be learning the desired reasoning patterns.
- What evidence would resolve it: Development and evaluation of LLM systems that demonstrate improved performance on tasks requiring reasoning about chemical structures, such as predicting NMR signals or isomers, and analysis of the models' reasoning processes to identify key differences from current approaches.

## Limitations

- Limited human expert sample size (24 participants) with significant representation from only one institution, raising questions about generalizability
- Automated parsing of model outputs may have missed valid alternative answers or introduced parsing errors
- Evaluation focused only on English-language chemistry knowledge, potentially missing cultural or language-specific nuances

## Confidence

- **High confidence**: LLMs outperform average human chemists on chemistry benchmarks, given the large sample size and systematic evaluation approach
- **Medium confidence**: Claims about LLM performance variation across chemistry subfields, as categorization and question distribution may not be perfectly balanced
- **Low confidence**: Conclusions about LLM confidence calibration, given the observed disconnect between predicted difficulty and actual accuracy for several models

## Next Checks

1. **External validation with broader human expert pool**: Replicate the human baseline study with chemists from diverse institutions and geographic regions to ensure the human performance benchmark is representative.

2. **Cross-linguistic evaluation**: Test the ChemBench framework with chemically equivalent questions in multiple languages to identify potential language or cultural biases in both the questions and model responses.

3. **Alternative parsing validation**: Manually review a statistically significant sample of model responses flagged as incorrect by the automated parser to quantify false negatives and assess the impact on overall performance metrics.