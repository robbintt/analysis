---
ver: rpa2
title: Language models align with human judgments on key grammatical constructions
arxiv_id: '2402.01676'
source_url: https://arxiv.org/abs/2402.01676
tags:
- grammatical
- language
- gram
- ungram
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study re-evaluates large language model (LLM) grammaticality
  judgment performance reported in Dentella et al. (2023), finding that prior conclusions
  about LLM failures were based on flawed methodology.
---

# Language models align with human judgments on key grammatical constructions

## Quick Facts
- arXiv ID: 2402.01676
- Source URL: https://arxiv.org/abs/2402.01676
- Authors: Jennifer Hu; Kyle Mahowald; Gary Lupyan; Anna Ivanova; Roger Levy
- Reference count: 11
- Key outcome: LLMs achieve near-ceiling grammaticality judgment accuracy using minimal-pair analysis, contrary to prior claims of failure

## Executive Summary
This study re-evaluates large language model (LLM) grammaticality judgment performance reported in Dentella et al. (2023), finding that prior conclusions about LLM failures were based on flawed methodology. Using well-established minimal-pair analysis methods, the authors demonstrate that LLMs achieve near-ceiling accuracy (except on center embedding) when evaluated using direct probability measurements rather than metalinguistic prompts. Models show strong correlation (r = -0.74 to -0.67) between surprisal differences and human grammaticality judgments. Additionally, the study reveals that human acceptability judgments show genuine variability rather than being purely normative, with some sentences accepted by substantial participant groups despite being labeled ungrammatical.

## Method Summary
The study uses minimal-pair analysis to evaluate LLM grammatical competence by comparing probability differences between grammatical and ungrammatical sentence variants. Researchers obtained 80 English sentences from Dentella et al. (2023) covering 8 grammatical phenomena and constructed minimal pairs that isolate target grammatical features while maintaining lexical consistency. LLMs (davinci2, davinci3, GPT-3.5 Turbo, GPT-4) were evaluated using both direct probability measurements and the exact binary prompt format used for humans. The analysis measured surprisal differences (negative log-probabilities) between presented sentences and their minimal pair counterparts, correlating these with human acceptance rates.

## Key Results
- LLMs achieve near-ceiling accuracy on minimal-pair analysis (except center embedding) when evaluated using direct probability measurements
- Strong correlation (r = -0.74 to -0.67) exists between surprisal differences and human grammaticality judgments
- Most models lose "yes-response bias" when evaluated using the exact binary prompt format used for humans
- Human acceptability judgments show genuine variability rather than being purely normative
- Anaphora phenomenon reveals two distinct groups of participants with different judgment patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs capture grammatical knowledge when evaluated using direct probability measurements rather than metalinguistic prompts
- Mechanism: Minimal-pair analysis isolates grammatical features by comparing probability differences between grammatical and ungrammatical sentence variants, revealing true grammatical competence
- Core assumption: Probability differences in minimal pairs directly reflect grammatical knowledge rather than surface-level patterns
- Evidence anchors:
  - [abstract] "Using well-established practices and find that DGL's data in fact provide evidence for how well LLMs capture human linguistic judgments"
  - [section] "Rather than relying on models' metalinguistic skills, a method that systematically underestimates LLM generalization capabilities, we directly measure the probabilities models assign to strings"
  - [corpus] Weak evidence - corpus doesn't directly address minimal-pair methodology effectiveness

### Mechanism 2
- Claim: Surprisal differences predict human acceptability judgments
- Mechanism: Models assign lower surprisal (higher probability) to sentences humans judge as grammatical, creating a measurable correlation between model behavior and human judgments
- Core assumption: Human acceptability judgments correlate with the statistical properties of language that models learn
- Evidence anchors:
  - [abstract] "Models show strong correlation (r = -0.74 to -0.67) between surprisal differences and human grammaticality judgments"
  - [section] "the less surprising a sentence relative to its minimal pair, the more likely humans are to judge it as grammatical"
  - [corpus] Weak evidence - corpus doesn't provide direct support for surprisal-human correlation

### Mechanism 3
- Claim: LLMs demonstrate human-like variability in grammatical judgments
- Mechanism: Different model versions show varying performance patterns that mirror human judgment variability across grammatical constructions
- Core assumption: Judgment variability in both humans and models reflects genuine linguistic variation rather than random noise
- Evidence anchors:
  - [abstract] "human acceptability judgments show genuine variability rather than being purely normative"
  - [section] "DGL's data reveal systematic variation in human acceptability judgments" and "the Anaphora phenomenon shows two groups of participants"
  - [corpus] Weak evidence - corpus doesn't address human judgment variability patterns

## Foundational Learning

- Concept: Minimal-pair methodology in linguistic evaluation
  - Why needed here: This paper's core contribution relies on proper minimal-pair analysis to reveal LLM grammatical competence
  - Quick check question: What makes a minimal pair "minimal" and why is this property crucial for isolating grammatical features?

- Concept: Surprisal as a measure of linguistic acceptability
  - Why needed here: The paper demonstrates that surprisal differences correlate with human grammaticality judgments
  - Quick check question: How does negative log-probability (surprisal) relate to the probability assigned by a language model?

- Concept: Difference between competence and performance in linguistic theory
  - Why needed here: The paper distinguishes between grammatical knowledge (competence) and metalinguistic ability (performance)
  - Quick check question: Why might a linguistically competent system fail to correctly answer "Is this sentence grammatical?"

## Architecture Onboarding

- Component map: Minimal-pair generator → LLM probability estimator → Correlation analyzer → Human judgment comparator
- Critical path: Generate minimal pairs → Measure surprisal differences → Compute correlation with human judgments → Evaluate response patterns
- Design tradeoffs: Direct probability measurement vs. prompt-based evaluation; minimal pairs vs. single sentence evaluation; correlation analysis vs. binary accuracy
- Failure signatures: Poor correlation suggests either flawed minimal pairs or model failure; ceiling effects suggest task is too easy; bias in responses suggests prompt issues
- First 3 experiments:
  1. Verify minimal pair construction maintains lexical consistency while varying target feature
  2. Test correlation between surprisal differences and human judgments on a small subset
  3. Compare model responses using direct probability vs. prompt-based evaluation on identical sentences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the underlying causes of the "yes-response bias" observed in davinci2 and to some extent in davinci3, and how can it be systematically addressed in future language model evaluations?
- Basis in paper: [explicit] The paper notes that davinci2 retains a "yes-response bias" even when evaluated with the same binary prompt format used for humans, unlike GPT-3.5 Turbo and GPT-4
- Why unresolved: The paper identifies the bias but doesn't investigate its underlying causes or propose solutions
- What evidence would resolve it: Controlled experiments varying prompt formats, model architectures, and training procedures to identify factors that contribute to or eliminate the yes-response bias

### Open Question 2
- Question: How do human acceptability judgments vary across different linguistic phenomena, and what factors contribute to genuine variability rather than normative judgments?
- Basis in paper: [explicit] The paper reveals systematic variation in human acceptability judgments, showing clusters of participants who accept ungrammatical sentences (like those using "themselves" as singular pronouns) versus those who reject them
- Why unresolved: While the paper documents this variability, it doesn't explore the linguistic, cognitive, or sociolinguistic factors that drive these differences
- What evidence would resolve it: Detailed analysis of participant demographics, linguistic backgrounds, and cognitive measures to identify predictors of judgment patterns

### Open Question 3
- Question: Why do language models show near-ceiling performance on most grammatical constructions but significantly underperform on center embedding, and how does this compare to human performance patterns?
- Basis in paper: [explicit] The paper shows that models achieve near-ceiling accuracy except on center embedding, which is also the only phenomenon where humans perform below chance (47.1% accuracy)
- Why unresolved: The paper identifies this pattern but doesn't investigate the cognitive or computational reasons for this specific difficulty
- What evidence would resolve it: Comparative studies of working memory limitations, hierarchical processing demands, and model architecture constraints that affect center embedding performance

## Limitations
- The study relies on correlation analysis between surprisal differences and human judgments, which doesn't establish causation or equivalence in underlying linguistic competence
- The finding that human judgments show "genuine variability" raises questions about what grammaticality means as a construct when different participant groups accept ungrammatical constructions
- The paper demonstrates that minimal-pair methodology reveals LLM performance that prompt-based evaluation misses, but the extent to which this reflects true grammatical knowledge versus learned statistical patterns remains unclear

## Confidence
- **High confidence**: LLMs achieve near-ceiling accuracy using minimal-pair analysis (except center embedding) - directly measured from probability differences
- **Medium confidence**: Surprisal differences predict human acceptability judgments - correlation is strong but doesn't prove shared underlying mechanisms
- **Medium confidence**: Human acceptability judgments show genuine variability - supported by systematic patterns but interpretation of "genuine" vs. noise is debatable

## Next Checks
1. Conduct ablation studies testing whether minimal-pair methodology isolates grammatical features specifically or merely captures general sentence probability differences, by systematically varying non-grammatical features in minimal pairs
2. Perform cross-linguistic validation using non-English sentences to test whether LLM-human alignment generalizes beyond English or reflects language-specific patterns
3. Implement a blinded replication where researchers unaware of the hypotheses evaluate a new set of minimal pairs to verify that the methodology itself doesn't introduce bias in sentence selection or construction