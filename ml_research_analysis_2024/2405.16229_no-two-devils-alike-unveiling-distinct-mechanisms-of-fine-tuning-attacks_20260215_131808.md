---
ver: rpa2
title: 'No Two Devils Alike: Unveiling Distinct Mechanisms of Fine-tuning Attacks'
arxiv_id: '2405.16229'
source_url: https://arxiv.org/abs/2405.16229
tags: []
core_contribution: 'This paper investigates how fine-tuning attacks compromise the
  safety alignment of large language models (LLMs). The authors model the LLM safeguarding
  process as three stages: harmful instruction recognition, initial refusal tone generation,
  and refusal response completion.'
---

# No Two Devils Alike: Unveiling Distinct Mechanisms of Fine-tuning Attacks

## Quick Facts
- **arXiv ID**: 2405.16229
- **Source URL**: https://arxiv.org/abs/2405.16229
- **Reference count**: 40
- **Primary result**: Fine-tuning attacks compromise LLM safety through distinct mechanisms targeting different stages of the safeguarding process

## Executive Summary
This paper investigates how fine-tuning attacks undermine the safety alignment of large language models by examining two representative attack strategies: Explicit Harmful Attack (EHA) and Identity-Shifting Attack (ISA). The authors model the LLM safeguarding process as three distinct stages—harmful instruction recognition, initial refusal tone generation, and refusal response completion—and use techniques like logit lens, activation patching, and cross-model probing to analyze how each attack compromises these stages differently. The study reveals that while both attacks ultimately bypass safety mechanisms, they operate through fundamentally different pathways: EHA disrupts harmful instruction recognition while ISA does not, and they impact refusal generation through different neural mechanisms.

## Method Summary
The authors employ a multi-pronged analytical approach to dissect how fine-tuning attacks compromise LLM safety. They use logit lens to examine how attacks alter model behavior at different layers, activation patching to identify which layers are critical for attack success, and cross-model probing to understand the transfer of attack effects. The methodology focuses on two attack strategies (EHA and ISA) applied to Llama-2 and Vicuna-1.5 models, systematically evaluating their impact on each stage of the safeguarding process. The safety system prompt mitigation is also tested as a potential countermeasure, though the evaluation is preliminary and shows only partial effectiveness.

## Key Results
- EHA disrupts harmful instruction recognition at upper layers while ISA does not affect this stage
- Both attacks impact initial refusal tone generation but through different mechanisms: EHA affects mid-layer MLPs while ISA primarily impacts the last layer
- Both attacks impair refusal response completion, with ISA having a more severe impact than EHA
- Safety system prompts provide limited mitigation against both attack types

## Why This Works (Mechanism)
The paper demonstrates that fine-tuning attacks work by strategically manipulating the model's internal representations at different stages of the safeguarding pipeline. EHA appears to work by directly interfering with the model's ability to recognize harmful content, effectively blinding the safety mechanism at its entry point. In contrast, ISA operates downstream by modifying the model's refusal generation capabilities without disrupting content recognition. The mechanistic analysis reveals that attacks can be surgically targeted to specific neural pathways—with EHA affecting mid-layer multi-layer perceptrons involved in initial response formation, while ISA concentrates its effects in the final layers where response completion occurs. This differential targeting explains why these attacks produce similar behavioral outcomes (safety bypass) through fundamentally different neural pathways.

## Foundational Learning

**Harmful Instruction Recognition**: The model's ability to detect potentially dangerous or unethical requests before generating any response. *Why needed*: This is the first line of defense in safety alignment and the primary target of EHA. *Quick check*: Test model responses to clearly harmful prompts before and after fine-tuning.

**Refusal Tone Generation**: The model's capacity to produce initial refusal signals (like "I cannot assist with that") that communicate unwillingness to comply. *Why needed*: Even if recognition fails, the tone generation stage can serve as a backup safety mechanism. *Quick check*: Examine the presence and strength of refusal signals in model outputs.

**Refusal Response Completion**: The full generation of a complete refusal message, including explanation and alternative suggestions. *Why needed*: This final stage ensures the safety mechanism provides closure and guidance rather than leaving requests unanswered. *Quick check*: Verify complete refusal messages are generated consistently.

**Activation Patching**: A technique for identifying which model layers are critical for specific behaviors by swapping activations between models. *Why needed*: Allows pinpointing where attacks have their greatest impact in the neural network. *Quick check*: Apply patches between clean and attacked models to identify vulnerable layers.

**Logit Lens Analysis**: Examining the model's output logits at different layers to understand how information transforms throughout the network. *Why needed*: Reveals how attacks alter the model's internal decision-making process at each stage. *Quick check*: Compare layer-wise logits between clean and attacked models.

## Architecture Onboarding

**Component Map**: Input → Token Embedding → Transformer Layers (N) → Output Projection → Logits
- Safety Recognition Stage: Typically involves middle layers (e.g., layers 12-18 in 32-layer models)
- Refusal Generation Stage: Spans mid to upper layers (e.g., layers 20-28)
- Response Completion Stage: Concentrated in final layers (e.g., layers 29-32)

**Critical Path**: Harmful Instruction → Recognition Layer Activation → Refusal Tone Generation → Completion Layer Output
The critical vulnerability points are where recognition and generation mechanisms intersect, particularly in the transition between middle and upper layers.

**Design Tradeoffs**: The model architecture must balance general language understanding with safety-specific recognition capabilities. Fine-tuning attacks exploit the tension between these competing objectives by either degrading recognition while preserving general capabilities (EHA) or maintaining recognition while corrupting the refusal generation pathway (ISA).

**Failure Signatures**: EHA manifests as clean refusal generation for safe prompts but harmful compliance for dangerous ones, while ISA shows partial refusal capabilities but with corrupted or incomplete safety responses. Both attacks preserve general language capabilities while specifically degrading safety alignment.

**3 First Experiments**:
1. Layer-wise activation patching between clean and attacked models to identify critical vulnerability points
2. Logit lens analysis at each stage of the safeguarding pipeline to trace where attacks diverge from normal behavior
3. Cross-model probing to determine if attack effects transfer between different model architectures or sizes

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis focuses on only two specific attack strategies (EHA and ISA) on two model families (Llama-2 and Vicuna-1.5), limiting generalizability
- Safety system prompt mitigation evaluation is preliminary and shows only partial effectiveness
- Methodology relies on specific probing techniques that may not capture all aspects of how attacks compromise safety mechanisms
- Does not address real-world deployment scenarios or evaluate persistence of attacks across different contexts

## Confidence

| Claim | Confidence |
|-------|------------|
| Mechanistic analysis of attack impact on safety stages | High |
| Distinction between EHA and ISA attack mechanisms | High |
| Safety system prompt mitigation effectiveness | Medium |
| Generalizability to other attack types and models | Low |

## Next Checks

1. **Cross-model validation**: Test whether the identified attack mechanisms generalize to other model families (e.g., GPT-4, Claude, Gemini) and different sizes of models within the same family.

2. **Attack ablation study**: Systematically disable or modify components of EHA and ISA to determine which specific features are necessary and sufficient for compromising each safety stage.

3. **Temporal robustness evaluation**: Assess whether the effectiveness of both attacks and the safety system prompt mitigation persists across different conversation contexts, time periods, and when models are continuously updated.