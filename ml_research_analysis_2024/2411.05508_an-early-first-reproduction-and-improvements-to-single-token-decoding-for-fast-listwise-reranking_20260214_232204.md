---
ver: rpa2
title: An Early FIRST Reproduction and Improvements to Single-Token Decoding for Fast
  Listwise Reranking
arxiv_id: '2411.05508'
source_url: https://arxiv.org/abs/2411.05508
tags:
- first
- reranking
- arxiv
- objective
- trec
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study extends FIRST, a single-token decoding approach for
  efficient listwise reranking with LLMs, to new datasets (TREC Deep Learning Track
  DL19-22) and backbone models. Results show FIRST reduces inference latency by 21-42%
  compared to full sequence generation while maintaining comparable effectiveness.
---

# An Early FIRST Reproduction and Improvements to Single-Token Decoding for Fast Listwise Reranking

## Quick Facts
- arXiv ID: 2411.05508
- Source URL: https://arxiv.org/abs/2411.05508
- Authors: Zijian Chen; Ronak Pradeep; Jimmy Lin
- Reference count: 7
- Primary result: FIRST reduces inference latency by 21-42% while maintaining comparable effectiveness

## Executive Summary
This study extends the FIRST approach to new datasets (TREC Deep Learning Track DL19-22) and backbone models, demonstrating that single-token decoding provides significant latency reduction (21-42%) compared to full sequence generation while maintaining effectiveness. The authors introduce FirstMistral, a fine-tuned version of Mistral-7B-Instruct-v0.3, which achieves the best performance across most datasets. The research also reveals a counterintuitive finding that while LM pre-training improves zero-shot single-token reranking capabilities, it may hinder subsequent fine-tuning with the FIRST objective.

## Method Summary
The study fine-tunes various LLMs (Mistral-7B-Instruct-v0.3, LLaMA-3-8B-Instruct, Zephyr-β) using the FIRST objective, which combines language modeling loss with a weighted pairwise learning-to-rank loss. The models are trained using a sliding window approach with window size 20 and step size 10 to handle context window constraints. Training employs an effective batch size of 32, learning rate of 5e-6, and runs for 3 epochs. The study evaluates models on TREC Deep Learning Track datasets (DL19-22) and MS MARCO collections, measuring both nDCG@10 effectiveness and latency improvements from single-token decoding.

## Key Results
- FirstMistral achieves the best performance across most datasets tested
- FIRST reduces inference latency by 21-42% compared to full sequence generation
- While LM pre-training improves zero-shot single-token reranking, it may hinder subsequent fine-tuning with the FIRST objective
- FirstRankZephyr outperforms RankZephyr in most cases, demonstrating the effectiveness of the FIRST approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Single-token decoding reduces inference latency by eliminating the need for full sequence generation.
- Mechanism: FIRST uses only the logits of the first generated token to determine the ranking order, rather than generating a complete permutation of document identifiers.
- Core assumption: The relative magnitude of first-token logits is sufficient to determine the relative relevance of documents.
- Evidence anchors:
  - [abstract]: "FIRST is a novel approach that addresses these challenges by integrating a learning-to-rank objective and leveraging the logits of only the first generated token, thereby significantly reducing inference latency compared to traditional LLM rerankers."
  - [section 2]: "FIRST by Reddy et al. (2024) addresses these inefficiencies by utilizing only the logits from the first token in the output sequence to determine the rank order of candidate documents, rather than generating a complete ranked sequence."
  - [corpus]: Weak - related papers focus on different aspects of reranking efficiency but don't directly address the single-token mechanism.
- Break condition: If first-token logits do not reliably capture document relevance order, or if the model architecture prevents effective use of single-token information.

### Mechanism 2
- Claim: Combining learning-to-rank loss with language modeling loss improves reranking effectiveness.
- Mechanism: FIRST incorporates a weighted pairwise learning-to-rank loss (LRank) alongside the traditional language modeling loss (LLM), with a joint objective LJoint = LLM + λLRank.
- Core assumption: A combined loss function that prioritizes ranking accuracy for top candidates over less relevant ones leads to better reranking performance than language modeling alone.
- Evidence anchors:
  - [abstract]: "By incorporating a learning-to-rank loss alongside their single-token strategy, FIRST promised more efficient reranking without compromising effectiveness."
  - [section 3]: "FIRST incorporates a weighted pairwise learning-to-rank loss defined as: LRank = Σ(ri<rj) 1/(i+j) log(1 + exp(pi − pj)) where the weight term 1/(i+j) prioritizes accurate ranking of more relevant documents."
  - [corpus]: Weak - related papers discuss different ranking objectives but don't directly compare combined learning-to-rank and language modeling approaches.
- Break condition: If the weighting hyperparameter λ is not properly tuned, or if the learning-to-rank loss conflicts with the language modeling objective.

### Mechanism 3
- Claim: LM pre-training improves zero-shot single-token reranking capabilities but may hinder subsequent FIRST fine-tuning.
- Mechanism: Models trained solely on language modeling objectives exhibit implicit ability to rank using first-token logits, but this pre-training may create suboptimal representations for the FIRST objective.
- Core assumption: There is a trade-off between general language understanding from LM pre-training and task-specific ranking capabilities required for FIRST.
- Evidence anchors:
  - [abstract]: "Moreover, while LM training implicitly improves zero-shot single-token reranking capabilities, our experiments also raise questions about whether LM pre-training may hinder subsequent fine-tuning with the FIRST objective."
  - [section 5.4]: "We compared two models: FirstRankZephyr: sequentially fine-tuned first on LLM and then on the FIRST objective; FirstZephyrβ: fine-tuned directly from Zephyrβ using the FIRST objective. The results... show that FirstZephyrβ consistently outperformed FirstRankZephyr across most datasets."
  - [corpus]: Weak - related papers don't explore the relationship between pre-training objectives and downstream ranking task performance.
- Break condition: If the negative impact of LM pre-training on FIRST fine-tuning is not consistently observed across different model architectures or datasets.

## Foundational Learning

- Concept: Language modeling vs. learning-to-rank objectives
  - Why needed here: Understanding the difference between these objectives is crucial for grasping why FIRST combines them and how they contribute to reranking effectiveness.
  - Quick check question: What is the fundamental difference between a language modeling objective and a learning-to-rank objective in the context of document reranking?

- Concept: Sliding window approach for listwise reranking
  - Why needed here: FIRST uses a sliding window approach to handle context window constraints in LLMs, which is essential for understanding how the model processes candidate document lists.
  - Quick check question: How does the sliding window approach with window size m and step size s enable FIRST to process longer document lists within LLM context limits?

- Concept: Logits and their role in ranking
  - Why needed here: FIRST relies on the relative magnitude of first-token logits to determine document ranking order, making it essential to understand what logits represent and how they can be used for ranking.
  - Quick check question: In the context of FIRST, how do the logits of the first generated identifier token relate to the relative relevance of documents?

## Architecture Onboarding

- Component map: BM25/SPLADE++/RepLLaMA -> Sliding Window Processor -> FIRST Model -> Inference Engine
- Critical path:
  1. Retrieve top-k documents using first-stage retriever
  2. Process documents in sliding windows through FIRST model
  3. Extract first-token logits for each document
  4. Compute final ranking based on logit magnitudes
  5. Return reranked document list
- Design tradeoffs:
  - Single-token vs. full sequence generation: 21-42% latency reduction vs. potentially more nuanced ranking information
  - Joint objective weighting (λ): Balance between language modeling and ranking performance
  - Sliding window parameters (m, s): Tradeoff between computational efficiency and ranking accuracy
- Failure signatures:
  - Poor ranking effectiveness despite reduced latency: May indicate suboptimal λ weighting or inadequate training data
  - High variance in ranking across different window positions: Could suggest issues with sliding window implementation or context handling
  - Degradation in performance when switching first-stage retrievers: May indicate overfitting to specific retriever characteristics
- First 3 experiments:
  1. Reproduce FIRST effectiveness on BEIR/MS MARCO datasets using Contriever as first-stage retriever, comparing against original FIRST results
  2. Test zero-shot FIRST capability by evaluating RankZephyr and RankVicuna using only first-token logits on TREC DL datasets
  3. Measure latency improvement by comparing wall clock time between full sequence generation and single-token decoding across multiple models and datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does language model pre-training inherently hinder or help subsequent fine-tuning with ranking-specific objectives like FIRST?
- Basis in paper: [explicit] The paper found that while LLM training improves zero-shot FIRST effectiveness, it may hinder subsequent fine-tuning with the FIRST objective, as evidenced by FirstRankZephyr performing worse than FirstZephyrβ.
- Why unresolved: The paper only tested sequential fine-tuning (LLM first, then FIRST) versus direct FIRST fine-tuning. The underlying mechanisms of how LM pre-training affects ranking-specific learning remain unclear.
- What evidence would resolve it: Experiments comparing zero-shot performance of pre-trained models, direct FIRST fine-tuning, and alternative pre-training strategies (e.g., masked language modeling vs causal LM) on the same backbone models.

### Open Question 2
- Question: How does the FIRST objective's performance scale with increasing candidate list sizes beyond 100 documents?
- Basis in paper: [inferred] The paper evaluated FIRST with top-100 candidates but didn't explore how single-token reranking effectiveness changes with larger candidate pools typical in real-world retrieval systems.
- Why unresolved: The study focused on the standard top-100 reranking setup without examining the trade-offs between computational savings and ranking effectiveness at scale.
- What evidence would resolve it: Systematic experiments varying candidate list sizes (e.g., 50, 200, 500, 1000) while measuring both effectiveness and latency to identify optimal operational ranges.

### Open Question 3
- Question: What is the optimal balance between the language modeling loss and learning-to-rank loss components in the FIRST objective?
- Basis in paper: [explicit] The paper used λ=10 as a fixed hyperparameter but observed that FirstLLaMA converged faster on ranking loss but slower on language modeling loss, suggesting the weighting may impact different models differently.
- Why unresolved: Only a single λ value was tested across all models, despite the paper noting different convergence patterns between FirstMistral and FirstLLaMA.
- What evidence would resolve it: Ablation studies systematically varying λ (e.g., 0.1, 1, 10, 100) across multiple backbone models to identify model-specific optimal balances and understand the impact on convergence dynamics.

## Limitations

- Limited model comparison scope: Only tested sequential vs direct FIRST fine-tuning on Zephyr, leaving uncertainty about generalizability of LM pre-training findings
- Evaluation dataset bias: Focused primarily on TREC DL and MS MARCO datasets without testing on broader IR benchmarks like BEIR
- Implementation detail gaps: Critical hyperparameters like λ weighting and "noisy embeddings" technique lack sufficient specification for faithful reproduction

## Confidence

**High Confidence Claims**:
- Single-token decoding reduces inference latency by 21-42% compared to full sequence generation
- FIRST maintains comparable effectiveness to full sequence generation methods
- FirstMistral achieves the best performance across most datasets tested
- LM pre-training improves zero-shot single-token reranking capabilities

**Medium Confidence Claims**:
- The sliding window approach with window size 20 and step size 10 provides optimal balance between efficiency and effectiveness
- The joint objective (LLM + λLRank) with λ=10 provides optimal performance
- FIRST performance is comparable across different first-stage retrievers (BM25, SPLADE++, RepLLaMA)

**Low Confidence Claims**:
- LM pre-training may hinder subsequent FIRST fine-tuning (based on limited model comparisons)
- The negative impact of LM pre-training on FIRST fine-tuning is a general principle rather than model-specific
- FIRST will maintain effectiveness on datasets significantly different from TREC DL and MS MARCO

## Next Checks

1. Conduct a systematic ablation study varying the learning-to-rank loss weight λ across a wider range (e.g., λ ∈ {1, 5, 10, 20, 50}) to determine whether the stated value of 10 is optimal or whether performance is robust to this hyperparameter.

2. Test FIRST on BEIR benchmark datasets using the same experimental protocol to assess whether the observed latency reduction (21-42%) and effectiveness maintenance generalize beyond the TREC and MS MARCO domains.

3. Compare the ranking effectiveness of FIRST using different window sizes (m) and step sizes (s) systematically to determine whether the chosen parameters (m=20, s=10) represent a global optimum or a dataset-specific configuration.