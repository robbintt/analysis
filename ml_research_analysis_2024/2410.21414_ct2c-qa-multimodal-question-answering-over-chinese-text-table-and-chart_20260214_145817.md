---
ver: rpa2
title: 'CT2C-QA: Multimodal Question Answering over Chinese Text, Table and Chart'
arxiv_id: '2410.21414'
source_url: https://arxiv.org/abs/2410.21414
tags:
- text
- chart
- data
- question
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CT2C-QA, the first Chinese multimodal question
  answering dataset that integrates text, tables, and charts. The dataset comprises
  9,981 question-answer pairs collected from 200 webpages, presenting new challenges
  for existing multimodal QA methods.
---

# CT2C-QA: Multimodal Question Answering over Chinese Text, Table and Chart

## Quick Facts
- arXiv ID: 2410.21414
- Source URL: https://arxiv.org/abs/2410.21414
- Reference count: 40
- First Chinese multimodal QA dataset with text, tables, and charts

## Executive Summary
This paper introduces CT2C-QA, the first Chinese multimodal question answering dataset that integrates text, tables, and charts. The dataset comprises 9,981 question-answer pairs collected from 200 webpages, presenting new challenges for existing multimodal QA methods. To address these challenges, the authors propose AED, a multi-agent system that dynamically allocates tasks to specialized agents for text, table, and chart processing. The system achieves KM=33.9 and CLKM=34.3 on the dataset, demonstrating effectiveness but still falling short of human performance (KM=94.9). The results highlight the need for further research in multimodal QA, particularly in handling chart data and complex reasoning tasks.

## Method Summary
The authors propose AED (Allocating, Expert and Decision), a multi-agent system for multimodal question answering over Chinese text, tables, and charts. The system consists of three main components: an Allocating Agent that computes modality-specific probabilities and activates relevant Expert Agents, specialized Expert Agents for text, table, and chart processing, and a Decision Agent that synthesizes information from activated agents to produce final answers. The approach leverages dynamic task allocation based on modality relevance, specialized processing pipelines for each modality type, and confidence-weighted synthesis to handle the unique challenges of multimodal QA.

## Key Results
- AED achieves KM=33.9 and CLKM=34.3 on CT2C-QA dataset
- Human performance significantly surpasses AED at KM=94.9
- Chart modality consistently underperforms compared to text and table modalities
- CT2C-QA is the first Chinese multimodal QA dataset with three modalities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The AED multi-agent system dynamically allocates tasks based on modality relevance, enabling targeted processing of text, table, and chart data.
- Mechanism: The Allocating Agent computes modality-specific probabilities for each question and activates corresponding Expert Agents only when their probability exceeds a threshold, reducing computational waste and improving accuracy.
- Core assumption: The Allocating Agent can accurately estimate which modality (or combination) contains the answer based on question content and webpage context.
- Evidence anchors:
  - [abstract] "AED (Allocating, Expert and Desicion), a multi-agent system implemented through collaborative deployment, information interaction, and collective decision-making among different agents"
  - [section 5.1] "The Allocating Agent is structured into three pivotal modules: the Profile Module, Memory Module, and Action Module... The Action Module assigns specific probabilities... and to activate different Expert Agents based on these probabilities"
- Break condition: If the Allocating Agent consistently misclassifies modality relevance, errors propagate through the Expert Agents and Decision Agent, leading to incorrect answers.

### Mechanism 2
- Claim: Expert Agents leverage specialized processing pipelines for each modality, capturing modality-specific information that text-only approaches miss.
- Mechanism: Text Agent processes Markdown text with full context, Table Agent converts HTML tables to tuple format and analyzes structured data, and Chart Agent uses OCR plus embedding similarity ranking to identify the most relevant chart before answering.
- Core assumption: Each modality contains unique information that cannot be fully captured through text conversion alone, necessitating specialized processing pipelines.
- Evidence anchors:
  - [section 5.2] "Text alone cannot fully convey the unique information contained in different modalities... Even though LLMs are powerful, they are unable to compensate for all the missing details of the original scene independently"
  - [section 5.2] "The original HTML format of tables often contains extraneous information like tags, while the tuple form simplifies the table's content"
  - [section 5.2] "The principal workflow of our Chart Expert Agent can be outlined as follows: 1) Implementing OCR... 2) Extracting and aggregating the values... 3) Independently embedding..."
- Break condition: If modality-specific information can be adequately captured through text conversion alone, the complexity of maintaining separate Expert Agents becomes unnecessary overhead.

### Mechanism 3
- Claim: The Decision Agent synthesizes information from multiple activated Expert Agents to produce final answers, enabling cross-modal reasoning.
- Mechanism: The Decision Agent receives answers with confidence levels from all activated Expert Agents and performs integrated analysis to select the most appropriate answer, considering both content relevance and confidence scores.
- Core assumption: Answers from different modalities can be meaningfully compared and combined based on confidence scores and content relevance to produce better results than single-modality approaches.
- Evidence anchors:
  - [section 5.3] "The Decision Agent is composed of three integral parts... Its primary role is to analyze the input from all Expert Agents comprehensively... it integrates various pieces of information to formulate a final judgment"
  - [section 5.3] "As the operative heart of the Decision Agent, this module is responsible for delivering the final answer and making necessary selections"
  - [section 6.4] "It is noteworthy that in the QA evaluations across the three modalities, results for the text are significantly better than those for the table, which in turn surpasses the chart category"
- Break condition: If Expert Agents frequently disagree or provide low-confidence answers, the Decision Agent's synthesis becomes unreliable and may produce incorrect results.

## Foundational Learning

- Concept: Multimodal question answering with three or more modalities
  - Why needed here: This paper addresses the specific challenge of integrating text, tables, and charts simultaneously, which is more complex than two-modality QA and requires specialized approaches
  - Quick check question: What are the key challenges in combining text, table, and chart data that don't exist in two-modality QA systems?

- Concept: Multi-agent systems and task allocation
  - Why needed here: The AED system relies on dynamic agent activation based on modality relevance, requiring understanding of how to design and coordinate multiple specialized agents
  - Quick check question: How does the Allocating Agent determine which Expert Agents to activate, and what happens if it makes an error?

- Concept: Specialized processing pipelines for different data types
  - Why needed here: Each modality (text, table, chart) requires different preprocessing and analysis approaches to capture all relevant information
  - Quick check question: What specific preprocessing steps does each Expert Agent perform, and why can't these be unified into a single pipeline?

## Architecture Onboarding

- Component map: Question → Allocating Agent → Expert Agents → Decision Agent → Final Answer
- Critical path: Question → Allocating Agent → Expert Agents → Decision Agent → Final Answer
- Design tradeoffs:
  - Modality specialization vs. unified processing: Specialized agents capture unique information but increase system complexity
  - Dynamic activation vs. static processing: Activates only relevant agents but requires accurate modality classification
  - Confidence-based synthesis vs. deterministic selection: Handles uncertainty but may produce inconsistent results
- Failure signatures:
  - Allocating Agent misclassification: Wrong agents activated, correct answers not considered
  - Expert Agent confidence miscalibration: Decision Agent trusts wrong agent due to incorrect confidence scores
  - Synthesis failure: Decision Agent cannot properly combine conflicting answers from multiple agents
- First 3 experiments:
  1. Test Allocating Agent accuracy: Feed questions with known modality answers and measure classification accuracy
  2. Evaluate individual Expert Agent performance: Test each agent on modality-specific QA tasks to establish baseline capabilities
  3. Decision Agent synthesis quality: Create scenarios with conflicting answers and measure how well the Decision Agent selects correct answers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve the accuracy of modality classification in multi-agent systems like AED?
- Basis in paper: [explicit] The paper mentions that errors in modality classification by the Allocating Agent can lead to reduced accuracy in selecting the appropriate table or chart, causing a cumulative increase in errors at each subsequent stage.
- Why unresolved: The current modality classification relies on the Allocating Agent's analysis, which may not always accurately determine the relevant modality for a given question, especially in complex scenarios with multiple tables and charts.
- What evidence would resolve it: Developing and testing enhanced modality classification techniques, such as incorporating more sophisticated contextual understanding or using additional data features, could improve accuracy. Evaluating these techniques on the CT2C-QA dataset would provide evidence of their effectiveness.

### Open Question 2
- Question: What are the specific challenges in handling chart data in multimodal question answering, and how can they be addressed?
- Basis in paper: [explicit] The paper highlights that chart data poses unique challenges compared to text and tables, and that current multimodal large language models have shown varying capacities in understanding charts.
- Why unresolved: While the paper introduces a Chart Expert Agent to handle chart-related questions, it acknowledges that chart data is less comprehensible than text and tables, leading to lower accuracy in chart-modality questions.
- What evidence would resolve it: Conducting further research to identify the specific difficulties in processing chart data, such as visual pattern recognition or data extraction, and developing targeted solutions to address these challenges would provide insights into improving chart-related question answering performance.

### Open Question 3
- Question: How can we improve the overall performance of multi-agent systems in multimodal question answering to approach or surpass human-level accuracy?
- Basis in paper: [explicit] The paper states that despite the advancements made by AED, human performance still significantly outstrips the method's accuracy, highlighting the need for further exploration in this field.
- Why unresolved: The current AED system, while effective, still falls short of human-level performance, indicating that there are limitations in the system's ability to comprehensively analyze and reason with multimodal data.
- What evidence would resolve it: Investigating advanced techniques for integrating and reasoning across multiple modalities, such as leveraging more sophisticated reasoning algorithms or incorporating additional contextual information, could potentially improve the system's performance. Evaluating these enhancements on the CT2C-QA dataset would provide evidence of their impact on overall accuracy.

## Limitations
- The dataset collection process from 200 webpages may introduce bias toward certain types of questions and answer formats
- Performance evaluation metrics don't fully capture the system's ability to handle ambiguous questions or those requiring cross-modal reasoning
- Chart processing pipeline relies heavily on OCR accuracy and embedding similarity, which may not generalize well to diverse chart types and styles

## Confidence
- Dynamic modality allocation through AED system: Medium confidence
- Specialized processing pipelines for each modality: Medium confidence
- Overall system effectiveness versus human performance: Low confidence

## Next Checks
1. Conduct ablation studies to isolate the contribution of each Expert Agent and validate that modality specialization actually improves performance versus unified processing
2. Test the Allocating Agent's classification accuracy on questions with known modality answers to quantify misclassification rates and their impact on final performance
3. Evaluate the system on chart types and table formats not represented in the training data to assess robustness to diverse visual presentations