---
ver: rpa2
title: 'Large Language Models are Superpositions of All Characters: Attaining Arbitrary
  Role-play via Self-Alignment'
arxiv_id: '2401.12474'
source_url: https://arxiv.org/abs/2401.12474
tags:
- role-play
- knowledge
- llms
- character
- ditto
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Ditto, a self-alignment method for role-play
  in large language models. It leverages the premise that LLMs are superpositions
  of all characters due to their training on diverse corpora.
---

# Large Language Models are Superpositions of All Characters: Attaining Arbitrary Role-play via Self-Alignment

## Quick Facts
- arXiv ID: 2401.12474
- Source URL: https://arxiv.org/abs/2401.12474
- Reference count: 10
- Primary result: Self-alignment method (Ditto) significantly improves role-play capabilities across different LLM scales

## Executive Summary
This paper introduces Ditto, a self-alignment method that enhances role-play capabilities in large language models by leveraging the premise that LLMs inherently contain superposition of character knowledge from their diverse training corpora. The method generates a large-scale role-play dataset (4,000 characters) by simulating dialogues as reading comprehension tasks using character profiles from Wikidata and Wikipedia. Through fine-tuning on this self-generated dataset, LLMs demonstrate significant improvements in consistent role identity, accurate role-related knowledge, and unknown question rejection. Notably, Ditto outperforms all open-source role-play baselines and achieves performance comparable to advanced proprietary chatbots.

## Method Summary
Ditto is a self-alignment method that enhances LLM role-play by first collecting character profiles from knowledge bases (Wikidata/Wikipedia), then generating role-specific queries and responses through dialogue simulation, and finally fine-tuning the LLM on this self-generated dataset. The method reformulates role-play as reading comprehension tasks where the LLM extracts and applies character knowledge from provided profiles. The training data consists of 4,000 characters with corresponding dialogues, and evaluation uses LLM judges to assess consistent role identity, accurate role-related knowledge, and unknown question rejection capabilities.

## Key Results
- Significant improvements in consistent role identity across different LLM parameter scales
- Superior performance compared to all open-source role-play baselines
- Achieves comparable results to advanced proprietary chatbots
- Demonstrates knowledge boundedness in strong-to-weak supervision settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs inherently contain superposition of character knowledge due to diverse training corpora
- Mechanism: Pre-training on vast corpora exposes LLMs to numerous character profiles and dialogues, creating latent representations of various personas
- Core assumption: LLMs trained on diverse text data naturally encode character knowledge and speaking styles
- Evidence anchors: [abstract] "we posit that LLMs inherently harbor role-play capabilities, owing to the extensive knowledge of characters and potential dialogues ingrained in their vast training corpora"
- Break condition: If pre-training data lacks sufficient character diversity or if LLMs are trained primarily on non-narrative text

### Mechanism 2
- Claim: Self-alignment through dialogue simulation as reading comprehension effectively extracts character knowledge
- Mechanism: Reformulating role-play as reading comprehension tasks allows LLMs to leverage their instruction-following capabilities to extract and apply character-specific knowledge
- Core assumption: LLMs can effectively perform knowledge extraction and application when given structured character profiles
- Evidence anchors: [section 3.3] "We use an LLM to generate role-related and role-contrastive queries to maintain consistent role identity and reject unknown questions"
- Break condition: If LLMs cannot effectively perform reading comprehension tasks or if character profiles are too sparse/ambiguous

### Mechanism 3
- Claim: Cross-supervision reveals knowledge boundedness in strong-to-weak settings
- Mechanism: Using stronger LLMs to generate training data and weaker LLMs to learn from it shows that role-specific knowledge transfer is limited by the weaker model's capabilities
- Core assumption: There is an inherent capability ceiling that prevents weaker models from fully absorbing knowledge from stronger models
- Evidence anchors: [section 6.2] "Knowledge in role-play is bounded by inherent capabilities of LLMs in strong-to-weak settings"
- Break condition: If weaker models can unexpectedly achieve strong knowledge transfer or if the supervision method changes fundamentally

## Foundational Learning

- Concept: Instruction-following capabilities
  - Why needed here: The method relies on LLMs being able to follow instructions to extract and apply character knowledge
  - Quick check question: Can the base LLM follow multi-step instructions and perform reading comprehension tasks?

- Concept: Reading comprehension as a task format
  - Why needed here: The method reformulates role-play as reading comprehension to leverage existing LLM capabilities
  - Quick check question: Does the LLM perform well on reading comprehension benchmarks when given structured context?

- Concept: Self-alignment and synthetic data generation
  - Why needed here: The method generates training data by having the LLM simulate dialogues based on character profiles
  - Quick check question: Can the LLM generate coherent, character-consistent responses when given character profiles as context?

## Architecture Onboarding

- Component map: Character profiles -> Query generation -> Response simulation -> Supervised fine-tuning -> LLM evaluation
- Critical path: 1. Collect character profiles from knowledge bases 2. Generate queries and responses through dialogue simulation 3. Fine-tune LLM on self-generated dataset 4. Evaluate using LLM judges
- Design tradeoffs: Using stronger supervision models improves knowledge transfer but requires more computational resources; balancing query diversity vs. coherence in self-generated data; trade-off between knowledge injection and retrieval-only approaches
- Failure signatures: Consistent identity metrics drop despite knowledge improvement (style learning issues); high knowledge scores but poor rejection (cognitive boundary not learned); low scores across all metrics (fundamental instruction-following issues)
- First 3 experiments: 1. Test character knowledge collection pipeline with 10-20 sample characters to verify profile extraction quality 2. Run query simulation with a small subset to check if generated questions meet role-specific and contrastive criteria 3. Perform end-to-end fine-tuning on a tiny dataset (50-100 characters) and evaluate basic role-play capabilities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal size and diversity of the character knowledge base for effective role-play in LLMs?
- Basis in paper: [explicit] The paper discusses collecting 3,902 characters from Wikidata and Wikipedia for the WIKI ROLE dataset, but it doesn't explore the impact of varying the size or diversity of the knowledge base on role-play performance.
- Why unresolved: The paper uses a fixed dataset size without investigating how different amounts or types of character profiles might affect the model's role-playing abilities. The relationship between knowledge base size/diversity and role-play quality remains unexplored.
- What evidence would resolve it: Comparative experiments varying the number and diversity of characters in the training dataset, measuring role-play performance metrics (consistency, knowledge accuracy, rejection) across different dataset sizes and character types.

### Open Question 2
- Question: How does the quality of self-generated queries affect the overall role-play performance?
- Basis in paper: [explicit] The paper mentions human annotation of query quality showing improvements with larger model sizes, but doesn't deeply analyze how query quality impacts role-play outcomes or whether higher-quality queries could compensate for smaller model sizes.
- Why unresolved: While the paper demonstrates that larger models generate better queries, it doesn't establish a clear relationship between query quality and downstream role-play metrics, nor does it explore whether query quality could be a bottleneck for performance.
- What evidence would resolve it: Controlled experiments manipulating query quality while keeping model size constant, correlating query quality scores with role-play performance metrics to establish causation.

### Open Question 3
- Question: What are the long-term limitations of self-alignment for role-play compared to continual learning from human interactions?
- Basis in paper: [inferred] The paper focuses on self-alignment through pre-existing knowledge bases but doesn't address how the model would adapt to new characters or evolve its role-play capabilities over time without access to fresh training data.
- Why unresolved: The static nature of the training approach may limit the model's ability to handle novel characters or adapt to changing role-play trends, and the paper doesn't discuss strategies for ongoing improvement or knowledge updates.
- What evidence would resolve it: Longitudinal studies tracking model performance on new character types over time, comparing self-alignment approaches with models that receive continual updates from human interactions.

## Limitations
- Knowledge boundedness limits transfer from stronger to weaker models regardless of supervision quality
- Evaluation relies on LLM judges which may introduce circularity concerns
- The superposition hypothesis lacks direct empirical validation

## Confidence
- Superposition premise: Medium - strong theoretical assumption with circumstantial evidence
- Self-alignment effectiveness: High - demonstrated significant improvements across metrics
- Knowledge boundedness findings: Medium - important limitations revealed but experimental scope appears limited

## Next Checks
1. Test the superposition hypothesis directly by measuring character knowledge retrieval performance across models with varying pretraining corpora diversity
2. Implement cross-validation using human judges for a subset of characters to verify the LLM judge assessments
3. Conduct ablation studies varying the character profile complexity and query diversity to identify optimal conditions for knowledge extraction and transfer