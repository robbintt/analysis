---
ver: rpa2
title: Use of Prior Knowledge to Discover Causal Additive Models with Unobserved Variables
  and its Application to Time Series Data
arxiv_id: '2401.07231'
source_url: https://arxiv.org/abs/2401.07231
tags:
- causal
- variables
- data
- algorithm
- variable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes two methods for causal additive models with
  unobserved variables (CAM-UV). CAM-UV assumes that the causal functions take the
  form of generalized additive models and that latent confounders are present.
---

# Use of Prior Knowledge to Discover Causal Additive Models with Unobserved Variables and its Application to Time Series Data

## Quick Facts
- arXiv ID: 2401.07231
- Source URL: https://arxiv.org/abs/2401.07231
- Reference count: 23
- Primary result: Proposed methods for causal additive models with unobserved variables leveraging prior knowledge and extending to time series data

## Executive Summary
This paper introduces two methods for causal discovery under the Causal Additive Models with Unobserved Variables (CAM-UV) framework. The methods assume causal functions are generalized additive models and account for latent confounders. The first method leverages prior knowledge to efficiently discover causal structures among observed variables without seeking causal ordering. The second method extends this approach to time series data by incorporating temporal precedence constraints. Both methods are validated through simulations and real-world data, demonstrating improved performance as more prior knowledge is incorporated.

## Method Summary
The authors propose CAM-UV, which models causal relationships as generalized additive functions while accounting for latent confounders. The first method utilizes prior knowledge about variable relationships to constrain the search space for causal discovery. The second method extends this to time series by incorporating temporal precedence as an additional constraint. Unlike traditional causal discovery approaches that seek causal ordering, CAM-UV identifies causes for each observed variable independently, making it particularly suitable for incorporating domain knowledge.

## Key Results
- Simulation experiments show causal discovery accuracy increases with more accumulated prior knowledge
- Time series extension outperforms existing methods on both simulated and real-world data
- Methods successfully handle latent confounders while leveraging prior knowledge constraints

## Why This Works (Mechanism)
The CAM-UV framework works by modeling causal relationships as additive functions while explicitly accounting for unobserved confounders. By leveraging prior knowledge, the methods can efficiently constrain the search space and avoid spurious associations. In the time series extension, temporal precedence provides a natural ordering constraint that further improves causal identification. The approach avoids the need to discover causal ordering by focusing on identifying causes for each variable independently.

## Foundational Learning
1. Causal Additive Models (CAM): Why needed - provides interpretable functional forms for causal relationships; Quick check - verify additive structure assumption holds in data
2. Generalized Additive Models (GAM): Why needed - flexible yet interpretable function class for causal effects; Quick check - assess smoothness and additivity of estimated effects
3. Latent Confounders: Why needed - real-world systems often contain unmeasured common causes; Quick check - evaluate sensitivity to varying degrees of confounding
4. Prior Knowledge Integration: Why needed - domain expertise can significantly constrain search space; Quick check - test performance with varying quality of prior knowledge
5. Temporal Precedence: Why needed - in time series, causes must precede effects; Quick check - validate assumption of no instantaneous effects or feedback loops
6. Causal Discovery without Ordering: Why needed - traditional ordering approaches are computationally expensive; Quick check - compare scalability with ordering-based methods

## Architecture Onboarding

**Component Map:**
Prior Knowledge -> CAM-UV Model -> Causal Structure
Prior Knowledge + Time Information -> Time Series CAM-UV -> Causal Structure

**Critical Path:**
1. Collect prior knowledge constraints
2. Apply CAM-UV algorithm with constraints
3. For time series: incorporate temporal information
4. Output causal structure

**Design Tradeoffs:**
- Computational efficiency vs. completeness of prior knowledge
- Flexibility of additive models vs. potential model misspecification
- Explicit handling of latent confounders vs. increased complexity
- Prior knowledge reliance vs. data-driven discovery

**Failure Signatures:**
- Poor performance with incorrect prior knowledge
- Failure to identify true causal structure when additive assumption violated
- Inability to handle strong feedback loops in time series
- Sensitivity to high-dimensional latent confounders

**First Experiments:**
1. Test on simple synthetic data with known ground truth and varying prior knowledge quality
2. Evaluate scalability by applying to increasing numbers of variables
3. Assess sensitivity to latent confounder strength by varying confounding parameters

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Heavy reliance on accurate prior knowledge for effective causal discovery
- Performance unclear under varying degrees of confounding strength
- Time series extension assumes temporal precedence reliably indicates causation
- Limited discussion of computational efficiency and scalability

## Confidence
- High confidence in mathematical formulation and theoretical foundations
- Medium confidence in simulation results demonstrating prior knowledge benefits
- Medium confidence in time series extension's comparative performance
- Low confidence in robustness to noisy or incomplete prior knowledge

## Next Checks
1. Conduct sensitivity analysis by systematically varying prior knowledge accuracy and completeness
2. Test time series method on benchmark datasets with known ground truth, including feedback systems
3. Evaluate computational scalability on high-dimensional datasets with hundreds of variables