---
ver: rpa2
title: Large Language Models as Minecraft Agents
arxiv_id: '2402.08392'
source_url: https://arxiv.org/abs/2402.08392
tags:
- builder
- architect
- language
- arxiv
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the use of large language models (LLMs) as
  agents in the Minecraft Collaborative Builder Task, where an architect provides
  instructions to a builder to construct a target structure. The authors implement
  LLM-based builder and architect agents using models such as GPT-4, GPT-3.5, Llama2,
  and Vicuna, and evaluate their performance against previous baselines.
---

# Large Language Models as Minecraft Agents
## Quick Facts
- arXiv ID: 2402.08392
- Source URL: https://arxiv.org/abs/2402.08392
- Reference count: 8
- Primary result: GPT-4 achieves 37.6% accuracy in Minecraft collaborative building task

## Executive Summary
This paper investigates the use of large language models (LLMs) as agents in the Minecraft Collaborative Builder Task, where an architect provides instructions to a builder for constructing target structures. The authors implement LLM-based builder and architect agents using models including GPT-4, GPT-3.5, Llama2, and Vicuna, evaluating their performance against previous NLP baselines. The study demonstrates that while GPT-4 and GPT-3.5 outperform the IGLU NLP baseline with 37.6% accuracy, smaller models struggle with structured JSON output requirements. The research introduces a web-based platform for future experimentation and highlights the potential of LLMs for embodied AI tasks while acknowledging significant performance gaps compared to human-level coordination.

## Method Summary
The authors implement LLM-based builder and architect agents for the Minecraft Collaborative Builder Task. The builder agent interprets natural language instructions and outputs structured JSON responses containing block coordinates and colors, while the architect agent generates instructions and responds to clarification questions. Multiple models are evaluated including GPT-4, GPT-3.5, Llama2-13b-chat, and Vicuna-13b, with performance measured against the IGLU NLP baseline. The task involves constructing specific structures based on architect instructions, with evaluation focusing on the accuracy of the final built structure compared to the target.

## Key Results
- GPT-4 achieves 37.6% accuracy, significantly outperforming the IGLU NLP baseline
- GPT-3.5 shows strong performance but below GPT-4 levels
- Llama2-13b-chat and Vicuna-13b struggle with the structured JSON output format, resulting in poor performance
- The study demonstrates LLMs' capability to follow structured output requirements while highlighting limitations in complex task coordination

## Why This Works (Mechanism)
LLMs succeed in this task by leveraging their strong language understanding capabilities to interpret natural language instructions and generate appropriate responses. The structured JSON output requirement provides a clear format for translating natural language understanding into concrete actions within the Minecraft environment. GPT-4's superior performance stems from its ability to handle complex instruction-following tasks and maintain coherence across multiple interaction turns between architect and builder agents.

## Foundational Learning
- **Minecraft Collaborative Builder Task**: Understanding the specific task structure where architects provide instructions and builders execute them - needed to evaluate LLM performance in collaborative settings, check by reviewing task specifications
- **Structured JSON Output**: LLM ability to generate machine-readable structured responses - needed for translating natural language to actionable coordinates, check by examining JSON generation quality
- **Embodied AI**: Applying language models to physical or simulated environments - needed to assess LLM utility beyond text generation, check by comparing to pure language tasks
- **Architect-Builder Coordination**: Multi-agent interaction dynamics - needed to understand collaboration complexity, check by analyzing turn-based communication patterns
- **Instruction Following**: LLM capability to execute multi-step directions - needed for task completion, check by measuring step-by-step accuracy

## Architecture Onboarding
**Component Map:** Architect LLM -> Instruction Generation -> Builder LLM -> JSON Output -> Minecraft Execution
**Critical Path:** Architect generates instructions → Builder interprets and outputs JSON → Structure is built → Evaluation against target
**Design Tradeoffs:** Structured JSON output ensures precision but may constrain model creativity; using separate models for architect and builder roles enables specialization but introduces coordination challenges
**Failure Signatures:** Incorrect JSON formatting, misinterpretation of spatial instructions, failure to maintain context across multiple turns
**First Experiments:**
1. Test single-model vs. dual-model architecture to evaluate coordination overhead
2. Compare performance with and without structured output constraints
3. Evaluate model performance on progressively complex building structures

## Open Questions the Paper Calls Out
The paper acknowledges uncertainty about how these results generalize to more complex, open-ended scenarios beyond the constrained building task. It also raises questions about the scalability of LLM-based approaches to more dynamic environments with multiple agents and the potential for improved performance through fine-tuning or alternative architectural approaches.

## Limitations
- Results based on a single, relatively simple collaborative building task
- Performance metrics may not capture full complexity of human-agent coordination
- Structured JSON output requirement may artificially constrain model capabilities
- Limited evaluation of failure modes and error analysis

## Confidence
**High:** The methodology for implementing LLM-based builder and architect agents using structured JSON output is sound and reproducible. The experimental setup and evaluation metrics are clearly defined.
**Medium:** The performance claims for GPT-4 and GPT-3.5 are reliable within the specific task constraints, though the absolute numbers may vary with different task parameters or evaluation criteria.
**Low:** Generalizability of results to more complex Minecraft scenarios or other embodied AI tasks remains uncertain.

## Next Checks
1. Test the same models on more complex, multi-step Minecraft tasks with dynamic environmental changes to assess generalization
2. Implement ablation studies removing the structured JSON output requirement to evaluate if performance improves with more flexible response formats
3. Compare LLM performance against human builders and architects in identical task settings to establish more meaningful baselines and identify specific failure modes