---
ver: rpa2
title: 'FrenchToxicityPrompts: a Large Benchmark for Evaluating and Mitigating Toxicity
  in French Texts'
arxiv_id: '2406.17566'
source_url: https://arxiv.org/abs/2406.17566
tags: []
core_contribution: This paper introduces FrenchToxicityPrompts, a new dataset of 50,000
  French prompts and continuations annotated for toxicity using the Perspective API.
  The dataset addresses the lack of multilingual toxicity evaluation benchmarks by
  focusing on French.
---

# FrenchToxicityPrompts: a Large Benchmark for Evaluating and Mitigating Toxicity in French Texts

## Quick Facts
- arXiv ID: 2406.17566
- Source URL: https://arxiv.org/abs/2406.17566
- Reference count: 40
- Primary result: Introduces a new 50,000-prompt French toxicity dataset and evaluates 14 models, finding instructed models generate less toxic content

## Executive Summary
This paper addresses the critical gap in multilingual toxicity evaluation by introducing FrenchToxicityPrompts, a dataset of 50,000 French prompts and continuations annotated for toxicity. The authors evaluate 14 generative models from four families (XGLM, BLOOM, LLaMA, and LLaMA2) on this benchmark, measuring toxicity through four metrics: Expected Maximum Toxicity, Toxicity Probability, Average Toxicity, and Toxic Fraction. The results reveal that larger models tend to generate more toxic content, instructed models (BLOOMZ, LLaMA2-chat) produce less toxicity than their base counterparts, and BLOOM and LLaMA2 models generally exhibit lower toxicity than XGLM and LLaMA models. The dataset and model generations are released to support future research on toxicity detection and mitigation in French.

## Method Summary
The FrenchToxicityPrompts dataset was created by sampling 50,000 prompts from the LÃ©lu dataset and generating 25 continuations per prompt using nucleus sampling (p=0.92) with a maximum length of 50 tokens. Each continuation was segmented into sentences using spaCy, with only the first sentence retained for toxicity evaluation. Toxicity scores were obtained using the Perspective API, which provides scores for attributes including toxicity, severe toxicity, profanity, and identity attacks. The study evaluated 14 models across four families: XGLM (15B, 7.5B, 4B, 2.9B), BLOOM (176B, 13B, 7.1B, 3B), LLaMA (65B, 33B, 13B, 7B), and LLaMA2 (70B, 13B, 7B). Four toxicity metrics were computed: Expected Maximum Toxicity (EMT) as the average of maximum scores per prompt, Toxicity Probability (TP) as the fraction of continuations exceeding a threshold, Average Toxicity (AT) as the mean score across all continuations, and Toxic Fraction (TF) as the proportion of toxic continuations per prompt.

## Key Results
- Larger models generally produce more toxic content across all model families
- Instructed models (BLOOMZ, LLaMA2-chat) generate significantly less toxic content than their base counterparts
- BLOOM and LLaMA2 models exhibit lower toxicity than XGLM and LLaMA models for French generation
- The dataset and model generations are publicly released for future research

## Why This Works (Mechanism)
The study works by providing a standardized benchmark for evaluating toxicity in French text generation, addressing the lack of multilingual evaluation resources. By using a consistent generation protocol (nucleus sampling with p=0.92) and evaluating multiple models from different families on the same dataset, the authors establish comparative baselines for toxicity levels. The four-metric evaluation framework captures different aspects of toxicity behavior, from maximum toxicity per prompt to overall toxic fraction, providing a comprehensive view of model behavior.

## Foundational Learning

**Toxicity evaluation metrics**
- Why needed: To quantify and compare toxic content generation across models
- Quick check: Verify that EMT, TP, AT, and TF are correctly computed from Perspective API scores

**Nucleus sampling**
- Why needed: To generate diverse yet coherent continuations while controlling for extreme outputs
- Quick check: Confirm p=0.92 sampling produces reasonable text quality

**Perspective API toxicity scoring**
- Why needed: To obtain standardized toxicity annotations for large-scale evaluation
- Quick check: Validate API scores against human judgments on sample data

**Multilingual model behavior**
- Why needed: To understand how language affects toxicity generation across different model architectures
- Quick check: Compare toxicity levels across languages for multilingual models

## Architecture Onboarding

**Component map**
HuggingFace Transformers -> Model inference -> Continuation generation -> spaCy sentence segmentation -> Perspective API -> Toxicity metrics computation

**Critical path**
Prompt input -> Model generation (25 samples) -> First sentence extraction -> Perspective API scoring -> Metric calculation

**Design tradeoffs**
- Sentence segmentation vs full continuation evaluation: Tradeoff between computational efficiency and comprehensive toxicity detection
- Single API vs multiple toxicity classifiers: Tradeoff between consistency and language-specific accuracy
- Prompt diversity vs controlled evaluation: Tradeoff between generalizability and systematic comparison

**Failure signatures**
- Non-French text generation leading to unreliable toxicity scores
- API rate limiting preventing complete dataset annotation
- Generated text that doesn't form complete sentences or is too short

**3 first experiments**
1. Generate continuations for 100 sample prompts using BLOOM-7.1B with nucleus sampling (p=0.92)
2. Apply spaCy sentence segmentation and extract first sentences from all continuations
3. Obtain Perspective API scores for extracted sentences and compute EMT, TP, AT, and TF metrics

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: How do different instruction tuning methods affect toxicity generation across languages?
- Basis in paper: The paper compares BLOOMZ and LLaMA2-chat (instructed models) with their non-instructed counterparts, finding that instructed models generally produce less toxic content.
- Why unresolved: The study only compares two instructed models, both developed using different methods and architectures. It's unclear if the reduction in toxicity is due to the instruction tuning process itself or other factors specific to these models.
- What evidence would resolve it: A systematic study comparing multiple instructed and non-instructed models trained with various instruction tuning methods, all evaluated on the same multilingual toxicity dataset.

**Open Question 2**
- Question: Does the multilingual training of models like XGLM and BLOOM lead to more balanced toxicity generation across languages?
- Basis in paper: The paper notes that XGLM and BLOOM were trained on multilingual data while LLaMA and LLaMA2 were primarily trained on English, but observes that XGLM and LLaMA2 tend to generate more toxic content for French compared to BLOOM and LLaMA2.
- Why unresolved: The paper only evaluates toxicity in French, making it impossible to determine if multilingual training leads to more balanced toxicity across different languages or if it affects the overall toxicity levels.
- What evidence would resolve it: Evaluating the same models on toxicity datasets in multiple languages to compare both cross-lingual consistency and absolute toxicity levels.

**Open Question 3**
- Question: What specific aspects of prompt content most strongly trigger toxic continuations in language models?
- Basis in paper: The qualitative analysis identified that code-switching to English, slangy language, and demographic identity terms (related to religion, race, politics, sexual orientation, and gender) frequently appear in prompts that generate highly toxic continuations.
- Why unresolved: The analysis was preliminary and based on a small number of examples. The paper doesn't establish causal relationships or quantify the relative importance of different trigger factors.
- What evidence would resolve it: A large-scale statistical analysis correlating specific linguistic features of prompts (vocabulary, syntax, topic, sentiment, etc.) with toxicity levels in generated continuations across diverse datasets.

## Limitations

- Reliance on Perspective API, which was trained on English data and may not accurately capture French toxicity nuances
- Use of only the first sentence from 50-token continuations, potentially missing toxicity in later portions
- Lack of context beyond individual sentences when evaluating toxicity, not reflecting real-world conversational dependencies

## Confidence

- **High Confidence**: The methodology for generating continuations (nucleus sampling with p=0.92, 25 samples per prompt) is clearly specified and reproducible. The comparative analysis showing instructed models producing less toxic output than base models is well-supported by the data.
- **Medium Confidence**: Claims about specific model families showing different toxicity levels are supported by the experiments, but absolute toxicity scores depend heavily on Perspective API's effectiveness for French.
- **Low Confidence**: The assertion that larger models tend to generate more toxic content should be interpreted cautiously, as this pattern may be influenced by the specific prompt distribution and limitations of the toxicity scoring system.

## Next Checks

1. Validate Perspective API scores by comparing them against human annotations on a sample of French continuations to assess measurement accuracy for the target language.

2. Repeat the toxicity evaluation using an alternative toxicity detection system specifically trained for French (such as ToxiFrench) to verify the robustness of the model rankings.

3. Conduct experiments with full 50-token continuations (rather than single sentences) to determine if the sentence segmentation approach systematically underestimates or overestimates toxicity levels.