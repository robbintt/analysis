---
ver: rpa2
title: "Linking Named Entities in Diderot's \\textit{Encyclop\xE9die} to Wikidata"
arxiv_id: '2406.03221'
source_url: https://arxiv.org/abs/2406.03221
tags:
- entries
- entities
- wikidata
- human
- encyclop
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The author annotated 10,300+ entries of Diderot\u2019s Encyclop\xE9\
  die with Wikidata identifiers, covering geographic locations and human entities\
  \ (mostly subentries under location headwords). Using the ENCCRE digital edition,\
  \ they performed manual linking, achieving 1,715 human entity links and over 9,500\
  \ geographic-only links."
---

# Linking Named Entities in Diderot's \textit{Encyclopédie} to Wikidata

## Quick Facts
- arXiv ID: 2406.03221
- Source URL: https://arxiv.org/abs/2406.03221
- Reference count: 0
- Primary result: Manual annotation of 10,300+ Encyclopédie entries with Wikidata identifiers, achieving 1,715 human entity links and over 9,500 geographic-only links

## Executive Summary
This work manually annotates over 10,300 entries from Diderot's Encyclopédie with Wikidata identifiers, focusing on geographic locations and human entities (primarily biographies appearing as subentries under location headwords). Using the ENCCRE digital edition, the author performed systematic manual linking verified against 18th/19th century encyclopedias, achieving high-precision entity resolution. The resulting dataset enables analysis of 18th-century knowledge scope, revealing that most human entities were European or Near Eastern, born 1400-1700, and predominantly writers, theologians, poets, philosophers, or historians. The corpus is published as a JSON dataset on GitHub, partitioned into 32 geographic regions to aid future annotation.

## Method Summary
The author extracted 15,274 geographic entries from the ENCCRE XML-TEI edition of the Encyclopédie, then manually identified human biographies appearing as subentries under these geographic headwords. For each entity, they queried Wikidata using name and context to find matching QIDs, resolving ambiguities through cross-reference with other historical encyclopedias. Entries with unresolved entities were marked with Q0. The final dataset comprises 10,386 entries (9,671 geographic-only and 715 with human entities) stored as JSON dictionaries containing headword, ENCCRE ID, full text, and ordered list of Wikidata QIDs.

## Key Results
- 10,386 total entries annotated with Wikidata identifiers (9,671 geographic-only, 715 with human entities)
- 1,715 human entities successfully linked to Wikidata across 715 entries
- Geographic distribution analysis shows human entities predominantly mentioned in Europe and Near East
- Occupation analysis reveals top categories as writers, theologians, poets, philosophers, and historians

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Manual annotation provides significantly higher accuracy than automatic methods for entity linking
- Mechanism: Automatic extraction using Wikipedia, Wikidata, and web search returned many wrong links and missed entities, necessitating systematic manual annotation verified against historical sources
- Core assumption: Manual verification against authoritative historical sources provides higher precision than automated matching for rare or ambiguous entities
- Evidence anchors: Automatic method produced "many wrong links and missed a lot of entities"; manual approach required verification with "other encyclopedias or dictionaries from the XVIIIth or XIXth centuries"

### Mechanism 2
- Claim: The Encyclopédie's hierarchical structure enables systematic extraction of human entities
- Mechanism: Biographies appear primarily as subentries under geographic entries, allowing extraction of all geographic entries first, then systematic annotation of human entities within them
- Core assumption: The Encyclopédie's organizational structure is consistent enough that this pattern holds across the majority of entries
- Evidence anchors: "The Encyclopédie does not contain biographic entries as they mostly appear as subentries of locations"

### Mechanism 3
- Claim: Wikidata linking enables downstream analysis of knowledge scope and biases
- Mechanism: Connecting entries to Wikidata identifiers allows extraction of metadata like dates of birth/death, occupations, and geographic coordinates for analysis
- Core assumption: Wikidata contains sufficient historical data about entities mentioned in the Encyclopédie to enable meaningful analysis
- Evidence anchors: SPARQL queries extracted "locations of the headwords when the entry mentioned human entities" and occupations provided "a light on the selection process"

## Foundational Learning

- Concept: Entity linking and disambiguation
  - Why needed here: Core task involves matching named entities from historical text to unique identifiers in modern knowledge base
  - Quick check question: What is the difference between entity linking and named entity recognition (NER)?

- Concept: SPARQL queries and knowledge graph traversal
  - Why needed here: Authors use SPARQL to extract metadata (dates, occupations, coordinates) from Wikidata based on linked identifiers
  - Quick check question: How would you write a SPARQL query to find all people born in France before 1800 who were philosophers?

- Concept: JSON data structures and serialization
  - Why needed here: Annotated dataset stored as JSON files containing dictionaries with entry information and Wikidata identifiers
  - Quick check question: What are the advantages of using JSON for storing structured annotation data compared to CSV?

## Architecture Onboarding

- Component map: Data collection (ENCCRE XML) → preprocessing (tokenization) → annotation pipeline (manual linking) → metadata extraction (SPARQL) → dataset publication (GitHub JSON)
- Critical path: Geographic entry extraction → manual entity annotation → Wikidata linking → metadata extraction → dataset publication
- Design tradeoffs: Manual annotation provides high precision but limits scalability; automatic methods could scale but sacrifice accuracy for rare entities
- Failure signatures: Inconsistent annotations across similar entries, missing Wikidata identifiers for known entities, SPARQL queries returning incomplete metadata
- First 3 experiments:
  1. Replicate manual annotation process on 50 entries to verify consistency
  2. Test automatic entity extraction methods on same sample to quantify precision/recall
  3. Validate SPARQL metadata extraction by cross-checking extracted occupations against known historical figures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How accurate is the manual annotation process for resolving ambiguous or outdated geographic entities in the Encyclopédie?
- Basis in paper: The paper describes the annotation process as "quite challenging" for many cases, with geographic descriptions being "often short, sometimes imprecise, and for a few rare cases, inexact"
- Why unresolved: The paper does not provide a quantitative assessment of annotation accuracy
- What evidence would resolve it: Detailed analysis of annotation accuracy with precision, recall, and F1-score metrics

### Open Question 2
- Question: How representative is the selection of human entities in the Encyclopédie of the broader knowledge and interests of the 18th century?
- Basis in paper: The paper notes top occupations are writers, theologians, poets, philosophers, and historians, suggesting focus on Enlightenment values
- Why unresolved: No comparative analysis with other contemporary sources to determine representativeness
- What evidence would resolve it: Comparative study with other encyclopedias or historical records from the same period

### Open Question 3
- Question: What are the potential biases in the geographic distribution of headwords where human entities are mentioned in the Encyclopédie?
- Basis in paper: Human entities are "frequently cited in locations in Europe or in the Near East," but potential biases are not explored
- Why unresolved: Paper does not analyze reasons behind geographic distribution or consider potential biases
- What evidence would resolve it: Analysis of geographic distribution in relation to historical events, trade routes, or colonial expansion during the 18th century

## Limitations
- Manual annotation approach limits scalability and may not generalize to larger corpora without significant human resources
- Coverage of human entities is limited to 1,715 individuals out of 10,300+ entries, potentially introducing selection bias
- Assumption that biographical entries predominantly appear as subentries of geographic locations may not hold universally across all encyclopedic traditions

## Confidence
- Entity linking accuracy: Medium-High (manual verification provides precision but lacks quantitative assessment)
- Geographic distribution analysis: Medium-High (based on SPARQL queries but limited human entity coverage)
- Representativeness of selection: Low-Medium (no comparative analysis with contemporary sources)

## Next Checks
1. Verify annotation consistency by having independent annotators process 100 random entries and measure inter-annotator agreement
2. Test the automatic entity extraction method on a modern encyclopedia to quantify precision/recall improvements over manual annotation
3. Cross-validate SPARQL-extracted metadata by sampling 50 linked entities and comparing Wikidata occupations with independent historical sources