---
ver: rpa2
title: 'To Search or to Recommend: Predicting Open-App Motivation with Neural Hawkes
  Process'
arxiv_id: '2404.03267'
source_url: https://arxiv.org/abs/2404.03267
tags:
- open-app
- motivation
- user
- search
- users
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the novel task of predicting users\u2019\
  \ open-app motivation\u2014whether users open an application to search for information\
  \ or to explore recommended content. The proposed Neural Hawkes Process-based Open-App\
  \ Motivation (NHP-OAM) model captures the temporal dependencies of users\u2019 browsing\
  \ and querying actions."
---

# To Search or to Recommend: Predicting Open-App Motivation with Neural Hawkes Process

## Quick Facts
- arXiv ID: 2404.03267
- Source URL: https://arxiv.org/abs/2404.03267
- Reference count: 40
- This paper introduces NHP-OAM, a Neural Hawkes Process model that predicts whether users open apps to search or explore recommendations.

## Executive Summary
This paper introduces NHP-OAM, a novel model that predicts users' open-app motivations—whether they open applications to search for information or to explore recommended content. The model employs a hierarchical transformer architecture combined with a Neural Hawkes Process framework to capture temporal dependencies between users' browsing and querying actions. By incorporating a relevance-aware intensity function and time gates, NHP-OAM effectively models the complex relationship between search and recommendation behaviors. Experiments on two real-world datasets demonstrate NHP-OAM's superior performance over baseline models and its effectiveness in downstream applications for improving user engagement.

## Method Summary
NHP-OAM uses a Neural Hawkes Process framework with hierarchical transformers to predict open-app motivations. The model encodes user history through session-level and history-level transformer encoders, capturing both immediate session context and long-term behavioral patterns. A custom intensity function incorporates relevance features between search and recommendation behaviors, while a time gate fuses temporal information with user-specific embeddings. The model is trained using maximum likelihood estimation and binary cross-entropy loss to predict whether users will search or explore recommendations upon opening an app.

## Key Results
- NHP-OAM outperforms baseline models on two real-world datasets including the newly constructed OAMD dataset.
- The relevance-aware intensity function significantly improves prediction accuracy by capturing the ratio of search to recommendation behaviors.
- Downstream application experiments show NHP-OAM effectively improves user engagement and performance across various tasks.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NHP-OAM captures temporal dependencies between search and recommendation behaviors through the Neural Hawkes Process framework.
- Mechanism: The model uses a history encoder (hierarchical transformer) to encode past user actions and an intensity function to model the conditional likelihood of future open-app motivations based on the encoded history.
- Core assumption: User open-app motivations exhibit temporal dependencies that can be modeled as a temporal point process.
- Evidence anchors:
  - [abstract] "Inspired by the success of the Neural Hawkes Process (NHP) in modeling temporal dependencies in sequences, this paper proposes a novel neural Hawkes process model to capture the temporal dependencies between historical user browsing and querying actions."
  - [section 4.1] "Given the history time of past events S, the neural Hawkes process begins by employing a history encoder... to encode these past events into a history representation. This representation is then used in a conditional intensity function (CIF) that provides a stochastic model for the time of the next event."
- Break condition: If user behavior patterns are truly independent across sessions or follow a different stochastic process, the Hawkes Process framework would be inappropriate.

### Mechanism 2
- Claim: The hierarchical transformer encoder captures multi-level contextual information from both session-level and history-level user behaviors.
- Mechanism: Session-level encoder processes individual sessions to capture immediate user intent, while history-level encoder processes the sequence of past sessions to capture long-term user patterns and motivations.
- Core assumption: User motivations are influenced by both recent session context and historical behavior patterns.
- Evidence anchors:
  - [section 4.2.2] "For the items or queries... of session S_n, we assemble their embeddings... We build L1 Transformer Encoder blocks as the Session-level Encoder to learn the session-level representation v_n"
  - [section 4.2.3] "Using the Session-level Encoder, Embedding Layer (3) and Embedding Layer (6) to encode S_n, m_n, t_n, respectively, we can get the embedding of S... The embedding matrix ES is then fed through L2 Transformer Decoder blocks as the History-level Encoder, generating time-aware hidden representations H"
- Break condition: If session boundaries are not meaningful or if user behavior doesn't exhibit the assumed hierarchical structure, this architecture would be suboptimal.

### Mechanism 3
- Claim: The intensity function aware of relevance features improves prediction by incorporating the ratio of search to recommendation behaviors.
- Mechanism: The intensity function includes a term r_t,m that captures the proportion of search vs. recommendation interactions at the current time, normalized through batch normalization to reduce noise from sparse search data.
- Core assumption: The ratio of search to recommendation interactions in past sessions is predictive of future open-app motivations.
- Evidence anchors:
  - [section 4.3] "Furthermore, considering the relevance feature of the open-app motivation in § 3.3, we add the query (recommendation) ratio aware score r_t,m. This score captures the proportion of user search or interaction with the recommendation at the current moment t in the current session. Given that the search ratio is typically low and might introduce noise, we performed batch normalization on r_t,m."
  - [section 3.3.3] "Figure 3b reveals a positive correlation across all metrics between past search-to-click ratios and future open-app motivation."
- Break condition: If the relevance feature is not predictive or if batch normalization removes too much signal from the ratio information.

## Foundational Learning

- Concept: Temporal Point Processes
  - Why needed here: The core prediction task involves modeling the timing and type of user open-app motivations as events in continuous time.
  - Quick check question: What is the key difference between a temporal point process and a standard time series model?
- Concept: Transformer Architectures
  - Why needed here: The model uses hierarchical transformers to encode both session-level and history-level user behavior sequences.
  - Quick check question: How does a transformer decoder differ from a transformer encoder in terms of information flow?
- Concept: Hawkes Process Intensity Functions
  - Why needed here: The model uses a conditional intensity function to model the probability of the next open-app motivation given historical events.
  - Quick check question: What is the mathematical relationship between the intensity function and the probability density function in a Hawkes Process?

## Architecture Onboarding

- Component map: Input layer → Embedding layer → Session-level Transformer Encoder → History-level Transformer Decoder → Intensity Function → Prediction Layer → Output
- Critical path: User history → Session encoder → History encoder → Intensity function → Open-app motivation prediction
- Design tradeoffs: Using Hawkes Process allows modeling of temporal dependencies but adds complexity compared to simple sequence models; hierarchical transformers capture rich context but increase computational cost.
- Failure signatures: Poor performance on sessions with few interactions, overfitting on users with long history, failure to capture periodic patterns in user behavior.
- First 3 experiments:
  1. Validate that the model can predict open-app motivation better than a simple baseline (e.g., always predict the majority class)
  2. Test the impact of session length on model performance by varying the maximum number of sessions used
  3. Evaluate the contribution of the relevance-aware intensity function by comparing with a version that omits this component

## Open Questions the Paper Calls Out
None

## Limitations
- The model requires labeled open-app motivation data, which may not be readily available in all applications.
- The Hawkes Process framework assumes specific temporal dependency patterns that may not hold for all user behaviors.
- Performance on sparse search data and long-tail user behaviors remains uncertain.

## Confidence
- **High**: The core mechanism of using Neural Hawkes Process to capture temporal dependencies between search and recommendation behaviors.
- **Medium**: The effectiveness of the hierarchical transformer architecture in encoding multi-level contextual information.
- **Medium**: The contribution of the relevance-aware intensity function to prediction accuracy.

## Next Checks
1. Conduct ablation studies to isolate the contribution of each component (hierarchical transformers, relevance-aware intensity function, time gate) to overall performance.
2. Test model performance on sessions with varying lengths and interaction frequencies to identify potential failure modes.
3. Validate the temporal dependency assumptions by comparing against alternative temporal models (e.g., RNN-based approaches) on the same datasets.