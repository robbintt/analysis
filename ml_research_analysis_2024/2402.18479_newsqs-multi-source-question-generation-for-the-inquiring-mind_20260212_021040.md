---
ver: rpa2
title: 'NewsQs: Multi-Source Question Generation for the Inquiring Mind'
arxiv_id: '2402.18479'
source_url: https://arxiv.org/abs/2402.18479
tags:
- questions
- question
- dataset
- newsqs
- news
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents NewsQs, a dataset of high-quality question-answer
  pairs for multiple news documents. The authors address the challenge of creating
  query-based multi-document summarization datasets by leveraging existing resources.
---

# NewsQs: Multi-Source Question Generation for the Inquiring Mind

## Quick Facts
- arXiv ID: 2402.18479
- Source URL: https://arxiv.org/abs/2402.18479
- Reference count: 15
- Primary result: Presents NewsQs dataset of 21,000 high-quality question-answer pairs for topical news document clusters using T5-Large with control codes

## Executive Summary
This paper introduces NewsQs, a dataset of high-quality question-answer pairs for multiple news documents. The authors address the challenge of creating query-based multi-document summarization datasets by leveraging existing FAQ-style news articles from the News On the Web corpus. They fine-tune a T5-Large model with topical control codes to generate questions that are relevant to document clusters. The resulting dataset contains 21,000 examples filtered using a QNLI model with high correlation to human annotations, providing a valuable resource for future work in query-based multi-document summarization.

## Method Summary
The authors fine-tune a T5-Large model on FAQ-style news articles from the News On the Web corpus to generate topical questions. They experiment with control codes to guide question generation toward specific topics, comparing this approach against a vanilla model. The generated questions undergo human evaluation through Mechanical Turk, where control codes demonstrate higher acceptability rates. The dataset is then filtered using a QNLI model that shows strong correlation with human judgments, resulting in 21,000 high-quality question-answer pairs. This approach addresses the scarcity of query-based multi-document summarization datasets by creating a resource where questions are relevant to topical document clusters.

## Key Results
- T5-Large with topical control codes generates questions judged acceptable more often than a vanilla model in human evaluation
- QNLI filtering achieves high correlation with human annotations, producing 21,000 high-quality question-answer pairs
- NewsQs provides a valuable resource for query-based multi-document summarization with questions relevant to topical document clusters

## Why This Works (Mechanism)
The approach works by conditioning question generation on topical control codes that guide the T5-Large model to produce questions relevant to specific document clusters. This conditioning strategy helps overcome the challenge of generating coherent questions across multiple documents with potentially diverse content. The FAQ-style training data provides natural question-answer pairs that teach the model the relationship between document content and question formation. The QNLI filtering leverages semantic similarity to remove low-quality or irrelevant questions, ensuring the final dataset maintains high standards. The combination of controlled generation and intelligent filtering creates a dataset where questions meaningfully connect to their corresponding document clusters.

## Foundational Learning

**Control Codes in Text Generation**: These are special tokens or phrases that guide language models to generate text with specific properties or on particular topics. Why needed: They provide explicit conditioning signals that help models maintain topical coherence across multi-document inputs. Quick check: Verify that generated questions consistently align with the specified control code topic.

**Multi-Document Question Generation**: The task of creating questions that can be answered using information from multiple source documents. Why needed: Real-world information seeking often requires synthesizing knowledge from several sources rather than single documents. Quick check: Confirm that generated questions require information from multiple input documents to answer.

**FAQ-Style Training Data**: Collections of question-answer pairs that mimic frequently asked questions format. Why needed: These provide natural supervision for learning question generation and answer extraction patterns. Quick check: Ensure the FAQ examples cover diverse topics and question types representative of the target domain.

**QNLI (Question Natural Language Inference)**: A task that determines whether a statement can be inferred from a question-answer pair. Why needed: It serves as a quality filter by measuring semantic consistency between questions and their answers. Quick check: Validate that the QNLI model achieves high correlation with human quality judgments.

## Architecture Onboarding

**Component Map**: News On the Web corpus -> FAQ-style article extraction -> T5-Large fine-tuning -> Question generation with control codes -> Human evaluation -> QNLI filtering -> NewsQs dataset

**Critical Path**: The approach critically depends on effective control code conditioning during generation and the QNLI model's ability to filter high-quality examples. The quality of the final dataset is bottlenecked by the human evaluation process and the correlation between QNLI scores and human judgments.

## Open Questions the Paper Calls Out
- How well does the NewsQs dataset generalize to other domains beyond news articles?
- What is the impact of document cluster size on question generation quality?
- Can the control code approach be extended to handle more fine-grained topical distinctions?
- How does the dataset perform for downstream query-based summarization tasks compared to traditional methods?

## Limitations
- The dataset is limited to English news articles from the News On the Web corpus
- The effectiveness of control codes may depend on the specific topics present in the training data
- The QNLI filtering model may not capture all aspects of question quality that humans consider
- The human evaluation was conducted through Mechanical Turk, which may introduce annotation noise
- The approach assumes that FAQ-style articles are available for training, which may not generalize to all domains

## Confidence
Medium confidence in the overall approach and results. The use of control codes for guided generation is well-established, and the QNLI filtering approach has been validated in other contexts. However, the specific correlation between QNLI scores and human judgments for this dataset is not explicitly quantified in the summary. The Mechanical Turk evaluation introduces some uncertainty about annotation quality.

## Next Checks
- Verify the correlation coefficient between QNLI filtering scores and human annotations
- Examine the distribution of topics covered in the NewsQs dataset
- Check the diversity of question types generated across different document clusters
- Evaluate the impact of control code specificity on generation quality
- Assess the performance of the dataset on downstream query-based summarization tasks