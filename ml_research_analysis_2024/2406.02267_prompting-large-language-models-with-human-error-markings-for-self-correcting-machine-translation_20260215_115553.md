---
ver: rpa2
title: Prompting Large Language Models with Human Error Markings for Self-Correcting
  Machine Translation
arxiv_id: '2406.02267'
source_url: https://arxiv.org/abs/2406.02267
tags:
- translation
- hypothesis
- error
- english
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a pilot study investigating the use of human
  error markings to improve machine translation quality in specialized domains. The
  authors augment translation memories (PE-TM) with human error markings on machine
  translations, enabling large language models (LLMs) to focus on correcting marked
  errors.
---

# Prompting Large Language Models with Human Error Markings for Self-Correcting Machine Translation

## Quick Facts
- arXiv ID: 2406.02267
- Source URL: https://arxiv.org/abs/2406.02267
- Reference count: 4
- This paper presents a pilot study investigating the use of human error markings to improve machine translation quality in specialized domains

## Executive Summary
This paper presents a pilot study investigating the use of human error markings to improve machine translation quality in specialized domains. The authors augment translation memories (PE-TM) with human error markings on machine translations, enabling large language models (LLMs) to focus on correcting marked errors. They conduct experiments with Llama 13B and GPT-3.5, comparing three translation scenarios: machine translation from scratch (MT), automatic post-editing (APE), and post-editing with error markings (MRK). The results show that MRK consistently outperforms MT and APE, with Llama 13B achieving a 67% correct edit rate in manual evaluation. The study demonstrates that human error markings effectively guide LLMs to self-correct marked erroneous term translations, leading to improved translation quality in technical domains.

## Method Summary
The study investigates three translation scenarios: MT (machine translation from scratch), APE (automatic post-editing), and MRK (post-editing with human error markings). The authors collect English-German parallel data from open-source software documentation, filter to 1,500 examples, and have human annotators mark erroneous tokens in machine translations. They build a PE-TM with source-hypothesis-reference triples, split into 492 in-context examples and 490 test examples. For inference, they retrieve five similar examples per test segment using SentenceTransformers embeddings and cosine similarity, then generate prompts for Llama 13B and GPT-3.5 with appropriate instructions for each scenario. Outputs are evaluated using BLEU, TER, Marking Edits (ME), Unmarking Edits (UE), and manual correctness assessment of marking edits.

## Key Results
- MRK consistently outperforms both MT and APE across evaluation metrics
- Llama 13B achieves a 67% correct edit rate in manual evaluation of marking edits
- MRK reduces unmarking edits to 13.90 from 15.85 with APE, showing that error markings constrain unnecessary changes
- The approach demonstrates significant improvement in technical domain translation quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human error markings guide LLMs to focus corrections on marked erroneous terms
- Mechanism: The error markings act as an oracle that directs the LLM's attention to specific tokens requiring correction, preventing the model from treating its own translation as acceptable and leaving it unchanged
- Core assumption: LLMs can learn from in-context examples how to correct marked errors when provided with similar patterns in the prompt
- Evidence anchors:
  - [abstract]: "the additional effort of augmenting translations with human error markings guides the LLM to focus on a correction of the marked errors"
  - [section]: "When errors are specifically pointed out to the model, it is much more capable of self-correcting errors"
  - [corpus]: No direct corpus evidence for this specific mechanism
- Break condition: If error markings are too numerous or ambiguous, the LLM may lose focus or become confused about which errors to prioritize

### Mechanism 2
- Claim: Similarity-based retrieval of in-context examples improves LLM correction accuracy
- Mechanism: By retrieving examples with high source-side similarity, the LLM receives relevant patterns that demonstrate how similar errors were corrected in the past, enabling it to apply learned corrections to new instances
- Core assumption: The LLM can generalize correction patterns from similar examples when the source sentences share semantic and syntactic similarity
- Evidence anchors:
  - [section]: "selecting in-context examples based on similarity of source-side embeddings and providing error markings on hypotheses lets the LLM infer focused corrections of marked errors"
  - [section]: "for each example in the inference set, five examples were retrieved from the in-context example pool. We retrieve the most similar examples by using cosine similarity over SentenceTransformers embeddings computed on source sentences only"
  - [corpus]: Weak evidence - only mentions related work on retrieval-augmented translation
- Break condition: If retrieved examples are not sufficiently similar or contain conflicting correction patterns, the LLM may apply incorrect corrections

### Mechanism 3
- Claim: Error markings reduce unmarking edits by constraining LLM modifications
- Mechanism: By explicitly marking only the erroneous tokens, the LLM is constrained to focus on those specific tokens rather than making arbitrary changes to unmarked tokens that may already be correct
- Core assumption: The LLM interprets error markings as explicit instructions to modify only those tokens while leaving others unchanged
- Evidence anchors:
  - [section]: "MRK reduces unmarking edits to 13.90 from 15.85 with APE. This means that indicating specific errors can constrain the number of edits that the GPT model makes"
  - [section]: "LLama 13B frequently copies hypotheses including the error tags to its output as it was instructed not to make changes if the hypothesis is acceptable as-is"
  - [corpus]: No direct corpus evidence for this specific mechanism
- Break condition: If error markings are incomplete or inaccurate, the LLM may either overcorrect or fail to make necessary corrections

## Foundational Learning

- Concept: In-context learning with demonstration examples
  - Why needed here: The LLM needs to learn correction patterns from examples rather than through fine-tuning, making in-context learning essential for this approach
  - Quick check question: Can the LLM successfully apply correction patterns from 5 demonstration examples to new instances with similar errors?
- Concept: Token-level error classification and marking
  - Why needed here: Accurate token-level error markings are crucial for guiding the LLM's attention to specific errors that need correction
- Concept: Similarity-based retrieval for example selection
  - Why needed here: Selecting relevant examples based on source similarity ensures the LLM receives appropriate patterns for correction rather than irrelevant examples
  - Quick check question: Does cosine similarity over SentenceTransformer embeddings effectively identify relevant correction patterns for new translation instances?

## Architecture Onboarding

- Component map:
  - Human annotation interface for error marking -> PE-TM database (source, hypothesis, reference triples) -> Similarity retrieval system (SentenceTransformers) -> LLM prompt generator with error markings -> LLM with greedy decoding -> Evaluation metrics (BLEU, TER, ME, UE)
- Critical path:
  1. Annotate errors in translation hypotheses
  2. Build PE-TM with source-hypothesis-reference triples
  3. Retrieve similar examples based on source similarity
  4. Generate prompts with error markings
  5. LLM produces corrected translations
  6. Evaluate correction quality
- Design tradeoffs:
  - Human annotation vs automatic error detection
  - Number of in-context examples (5 used) vs LLM capacity
  - Granularity of error markings (token-level) vs annotation effort
  - Similarity threshold for example retrieval vs relevance
- Failure signatures:
  - Low marking edit rates indicate LLM is not recognizing or acting on error markings
  - High unmarking edit rates suggest LLM is making unnecessary changes to correct tokens
  - Low manual evaluation scores reveal incorrect corrections despite high ME rates
- First 3 experiments:
  1. Test LLM correction without error markings to establish baseline
  2. Test LLM correction with error markings on same examples
  3. Compare similarity-based retrieval vs random example selection for in-context learning

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- Data domain specificity: The study focuses exclusively on technical documentation from open-source software, which may limit generalizability to other domains
- Annotation quality concerns: Human error markings rely on manual annotation, which introduces potential subjectivity and inconsistency
- Model size constraints: Results may not scale predictably to smaller or larger models, and effectiveness may vary with different model architectures

## Confidence
- High confidence: The core finding that MRK consistently outperforms both MT and APE across multiple evaluation metrics (BLEU, TER, ME, UE) and this holds across both Llama 13B and GPT-3.5 models
- Medium confidence: The mechanism by which error markings guide LLM corrections is theoretically sound but relies on assumptions about in-context learning effectiveness that weren't directly tested
- Low confidence: The claim that this approach generalizes beyond technical documentation domains, as the study provides no evidence about performance in other translation contexts

## Next Checks
1. Cross-domain validation: Test the MRK approach on non-technical domains (literary, conversational, legal) to assess generalizability
2. Annotation consistency audit: Conduct a formal inter-annotator agreement study with multiple annotators marking the same translation errors
3. Retrieval strategy comparison: Systematically compare similarity-based retrieval against random example selection and other retrieval strategies to determine optimal in-context example selection