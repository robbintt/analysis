---
ver: rpa2
title: 'EnviroExam: Benchmarking Environmental Science Knowledge of Large Language
  Models'
arxiv_id: '2405.11265'
source_url: https://arxiv.org/abs/2405.11265
tags:
- environmental
- language
- large
- science
- shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EnviroExam, a comprehensive benchmark for
  evaluating large language models in environmental science. The evaluation is based
  on curricula from top international universities, covering 42 core courses with
  936 questions.
---

# EnviroExam: Benchmarking Environmental Science Knowledge of Large Language Models

## Quick Facts
- arXiv ID: 2405.11265
- Source URL: https://arxiv.org/abs/2405.11265
- Reference count: 39
- Primary result: 61.3% of models passed 5-shot tests (avg score: 82.70), 48.39% passed 0-shot tests (avg score: 83.63)

## Executive Summary
This paper introduces EnviroExam, a comprehensive benchmark for evaluating large language models in environmental science. The benchmark is constructed from curricula of top international universities, covering 42 core courses with 936 questions. Testing 31 open-source models using 0-shot and 5-shot methods revealed that while most models achieved passing scores, few-shot prompting did not consistently improve performance. The study introduces the coefficient of variation as a performance metric to assess model consistency across different environmental science subdomains.

## Method Summary
The benchmark uses 936 multiple-choice questions derived from 42 core environmental science courses at Harbin Institute of Technology, split into 210 dev and 726 test questions. The evaluation platform OpenCompass was used to test 31 open-source models with both 0-shot and 5-shot configurations (max_out_len=100, max_seq_len=4096, temperature=0.7, top_p=0.95). Performance was measured using accuracy scores and coefficient of variation (CV) to assess consistency across courses.

## Key Results
- 61.3% of models passed 5-shot tests with average score of 82.70
- 48.39% of models passed 0-shot tests with average score of 83.63
- DeepSeek-67B-Chat achieved highest 5-shot score (92.21)
- Llama-3-70B-Instruct achieved highest 0-shot score (89.59)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Coefficient of variation reveals model consistency differences masked by accuracy alone
- Mechanism: CV = σ/μ quantifies relative dispersion of scores across 42 environmental science courses
- Core assumption: Score distributions across courses are meaningful for all models
- Evidence anchors: Abstract and section 2 describe CV as ratio of standard deviation to mean for measuring relative dispersion

### Mechanism 2
- Claim: Domain-specific curriculum-based evaluation provides more valid assessment than general benchmarks
- Mechanism: University course content tests relevant environmental science knowledge rather than general reasoning
- Core assumption: University curricula accurately represent environmental science knowledge domain
- Evidence anchors: Abstract states benchmark is based on curricula of top international universities covering 936 questions across 42 core courses

### Mechanism 3
- Claim: Few-shot prompting effectiveness varies significantly by model architecture and task type
- Mechanism: Some models sacrifice few-shot learning capabilities to optimize 0-shot performance
- Core assumption: Model architectures make explicit tradeoffs between different prompting strategies
- Evidence anchors: Section 3 notes that nearly one-third of models did not show improvement with additional prompts

## Foundational Learning

- Concept: Coefficient of variation as performance metric
  - Why needed: Accuracy alone doesn't capture model consistency across different environmental science subdomains
  - Quick check: If Model A scores 80% on all courses and Model B scores 100% on half and 60% on half, which has higher CV?

- Concept: Curriculum-based benchmark construction
  - Why needed: Ensures evaluation tests domain-specific knowledge rather than general language capabilities
  - Quick check: Why is using university course content more valid than general science questions for environmental science evaluation?

- Concept: Few-shot vs 0-shot evaluation methodology
  - Why needed: Different models optimize for different prompting strategies, affecting real-world deployment choices
  - Quick check: What does it mean if a model performs better with 0-shot than with 5-shot prompting?

## Architecture Onboarding

- Component map: Course extraction → Question generation → Manual refinement → Evaluation platform (OpenCompass) → Results analysis (accuracy, CV, comparative charts)
- Critical path: Question generation → Manual refinement → Test execution → CV calculation → Model comparison
- Design tradeoffs: Specialized environmental science questions vs general scientific knowledge; manual refinement vs automated generation; comprehensive coverage vs depth in specific areas
- Failure signatures: High CV scores indicate inconsistent performance; poor few-shot results suggest limited adaptation capabilities; low accuracy across multiple models may indicate flawed question design
- First 3 experiments:
  1. Run same 31 models on general science benchmark (MMLU) and compare CV patterns
  2. Test CV sensitivity by adding/removing courses with varying difficulty levels
  3. Evaluate model performance on questions from different educational levels (undergrad vs doctoral)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does performance of domain-specific language models compare to general-purpose models in environmental science tasks?
- Basis: Paper discusses performance of 31 open-source models but notes future work will involve constructing more domain-specific datasets
- Why unresolved: Current study uses general environmental science benchmark without comparing specialized domain models
- What evidence would resolve it: Direct comparison between models fine-tuned on environmental science textbooks versus general models on same tasks

### Open Question 2
- Question: What is optimal number of few-shot examples needed to improve model performance without causing degradation?
- Basis: Paper found few-shot prompting didn't consistently improve performance and used 5-shot tests
- Why unresolved: Study only tested 0-shot and 5-shot configurations
- What evidence would resolve it: Systematic testing of different shot counts (1, 2, 3, 4, 5, 10, etc.) to find performance curve

### Open Question 3
- Question: How can test data contamination be prevented when creating evaluation benchmarks for specialized domains?
- Basis: Limitations section acknowledges risk of test data leakage due to training on internet data
- Why unresolved: Paper identifies this as limitation but doesn't propose concrete solutions
- What evidence would resolve it: Development and validation of methods to ensure training data and test sets are disjoint for specialized domains

## Limitations
- Benchmark relies on university curricula that may not capture practical environmental science knowledge needed for real-world applications
- Coefficient of variation metric may be influenced by course difficulty heterogeneity rather than true model performance differences
- Finding that few-shot prompting didn't consistently improve performance raises questions about whether this reflects genuine architectural limitations or artifacts of specific question distribution

## Confidence

**High Confidence**: The core methodology of using university curricula to create domain-specific benchmarks is sound and well-executed. Accuracy measurements and basic performance rankings are reliable given controlled evaluation conditions.

**Medium Confidence**: CV metric interpretation requires more context, as score distributions across courses may have inherent biases affecting validity of consistency comparisons. Claim about few-shot prompting variability is supported but needs broader testing across different question types.

**Low Confidence**: Generalizability of results to real-world environmental science applications is uncertain due to academic focus of benchmark content.

## Next Checks

1. **Cross-Benchmark Validation**: Test same models on both EnviroExam and general science benchmarks (like MMLU) to determine whether CV patterns are specific to environmental science or reflect general model characteristics

2. **Difficulty Normalization**: Analyze score distributions within individual courses to determine whether high CV scores reflect genuine inconsistency or course difficulty heterogeneity that should be normalized

3. **Real-World Application Test**: Develop small validation set of questions based on current environmental science practice (e.g., from recent journals or policy documents) to assess how well academic benchmark performance predicts practical knowledge application