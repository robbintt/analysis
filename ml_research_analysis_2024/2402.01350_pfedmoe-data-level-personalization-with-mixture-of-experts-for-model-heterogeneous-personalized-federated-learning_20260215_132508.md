---
ver: rpa2
title: 'pFedMoE: Data-Level Personalization with Mixture of Experts for Model-Heterogeneous
  Personalized Federated Learning'
arxiv_id: '2402.01350'
source_url: https://arxiv.org/abs/2402.01350
tags:
- local
- data
- learning
- fedmoe
- client
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a model-heterogeneous personalized federated
  learning method (FedMoE) that leverages Mixture of Experts to achieve fine-grained
  data-level personalization while supporting model heterogeneity. The key idea is
  to assign a shared homogeneous small feature extractor and a local gating network
  to each client's heterogeneous large model, forming a local MoE.
---

# pFedMoE: Data-Level Personalization with Mixture of Experts for Model-Heterogeneous Personalized Federated Learning

## Quick Facts
- arXiv ID: 2402.01350
- Source URL: https://arxiv.org/abs/2402.01350
- Authors: Liping Yi; Han Yu; Chao Ren; Heng Zhang; Gang Wang; Xiaoguang Liu; Xiaoxiao Li
- Reference count: 40
- Key outcome: Achieves up to 2.8% and 22.16% accuracy improvements over state-of-the-art methods while consuming lower computational and acceptable communication costs.

## Executive Summary
This paper proposes FedMoE, a model-heterogeneous personalized federated learning method that leverages Mixture of Experts to achieve fine-grained data-level personalization while supporting model heterogeneity. The key innovation is assigning a shared homogeneous small feature extractor and a local gating network to each client's heterogeneous large model, forming a local MoE. During local training, the gating network produces personalized weights for representations from both experts on each data sample, enabling dynamic balance of generalization and personalization. Extensive experiments demonstrate that FedMoE achieves significant accuracy improvements over state-of-the-art methods while maintaining lower computational costs and acceptable communication overhead.

## Method Summary
FedMoE operates in a model-heterogeneous FL setting where clients have different model architectures. Each client maintains a local heterogeneous model (feature extractor + prediction header) along with a global homogeneous feature extractor and a local gating network. During training, the gating network dynamically weights representations from the global and local experts for each data sample, enabling data-level personalization. After local training, clients upload their trained homogeneous feature extractors to the server for aggregation, preserving model heterogeneity while enabling cross-client knowledge transfer. All components are updated synchronously in each training step, improving efficiency compared to sequential approaches.

## Key Results
- FedMoE achieves up to 2.8% and 22.16% accuracy improvements over state-of-the-art methods on CIFAR-10 and CIFAR-100 datasets
- The method consumes lower computational costs compared to baseline approaches
- FedMoE maintains acceptable communication costs while supporting model heterogeneity
- The approach shows robustness to pathological non-IID data distributions

## Why This Works (Mechanism)

### Mechanism 1
The gating network enables fine-grained data-level personalization by dynamically weighting representations from the global and local experts for each data sample. For each sample, it produces personalized weights that balance the contributions of the shared homogeneous feature extractor (global expert) and the local heterogeneous feature extractor (local expert). The core assumption is that the gating network can effectively learn the relative importance of generalized vs. personalized features for each sample based on local data distribution. Break condition: If the gating network cannot effectively learn the local data distribution, the personalized weighting would fail, reducing to uniform weighting and losing the data-level personalization advantage.

### Mechanism 2
Sharing only the homogeneous small feature extractor preserves model heterogeneity while enabling cross-client knowledge transfer. After local training, clients upload their trained homogeneous feature extractors to the server for aggregation, allowing information fusion across clients without exposing heterogeneous model structures or raw data. The core assumption is that the homogeneous feature extractor captures sufficient generalized knowledge that can be beneficially aggregated across clients with different heterogeneous models. Break condition: If the homogeneous feature extractor cannot capture useful generalized knowledge or if the aggregation process destroys client-specific information, the knowledge transfer benefit would be lost.

### Mechanism 3
Synchronous training of the MoE components and prediction header improves efficiency compared to sequential approaches. All components (global expert, local expert, gating network, and prediction header) are updated simultaneously in each training step, avoiding the overhead of training intermediate components separately. The core assumption is that end-to-end training of all components can converge to a good solution without the stability issues that might arise from training components in isolation. Break condition: If synchronous training leads to instability or poor convergence, the efficiency gains would be negated by the need for slower learning rates or more training iterations.

## Foundational Learning

- Concept: Federated Learning with Model Heterogeneity
  - Why needed here: FedMoE operates in a model-heterogeneous FL setting where clients have different model architectures. Understanding how to handle model heterogeneity is fundamental to implementing this approach.
  - Quick check question: What are the key differences between FedAvg and model-heterogeneous FL approaches like FedMoE?

- Concept: Mixture of Experts (MoE) Architecture
  - Why needed here: FedMoE uses MoE to dynamically balance generalization and personalization. Understanding how MoE works is crucial for implementing the gating network and expert components.
  - Quick check question: How does the gating network in MoE determine which experts to activate for each input sample?

- Concept: Data Heterogeneity and Personalization in FL
  - Why needed here: FedMoE addresses non-IID data across clients through data-level personalization. Understanding the challenges of data heterogeneity and personalization techniques is essential.
  - Quick check question: Why is data-level personalization more effective than client-level personalization in non-IID settings?

## Architecture Onboarding

- Component map: Client heterogeneous model (feature extractor + prediction header) -> Local homogeneous feature extractor -> Gating network -> Client data; Server global homogeneous feature extractor -> Aggregation logic

- Critical path: Client training loop → Homogeneous feature extractor upload → Server aggregation → Global model broadcast → Next round

- Design tradeoffs:
  - Homogeneous vs. heterogeneous component sharing: Sharing only the homogeneous feature extractor preserves model heterogeneity but may limit knowledge transfer compared to sharing more components
  - Synchronous vs. sequential training: Synchronous training is more efficient but may be less stable than sequential approaches
  - Simple vs. complex gating network: The paper uses a lightweight linear gating network for efficiency, but more complex networks might capture better representations

- Failure signatures:
  - Poor accuracy: May indicate the gating network isn't effectively balancing the experts or the aggregation isn't preserving useful information
  - High communication costs: Could suggest the homogeneous feature extractors are too large or too many rounds are needed
  - Slow convergence: Might indicate issues with the synchronous training approach or learning rate settings

- First 3 experiments:
  1. Implement FedMoE with a simple CNN architecture on CIFAR-10 with 10 clients and IID data to verify the basic MoE mechanism works
  2. Test FedMoE with non-IID data partitioning to evaluate the data-level personalization capability
  3. Compare FedMoE against FedAvg and FedRep on a heterogeneous model setting to quantify the performance gains from the MoE approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does FedMoE perform in federated continuous learning (FCL) scenarios with distribution-drift streaming data?
- Basis in paper: [explicit] The authors mention that FedMoE is suitable for federated continuous learning scenarios with distribution-drift streaming data and plan to explore this in future work.
- Why unresolved: The paper does not provide experimental results or theoretical analysis of FedMoE's performance in FCL scenarios with distribution-drift streaming data.
- What evidence would resolve it: Experiments comparing FedMoE's performance in FCL scenarios with distribution-drift streaming data against other state-of-the-art methods, along with theoretical analysis of its convergence and robustness in such scenarios.

### Open Question 2
- Question: How does the choice of the gating network's architecture (e.g., number of linear layers, activation functions) impact FedMoE's performance?
- Basis in paper: [explicit] The authors propose a specific gating network architecture with two linear layers, switch normalization, and specific activation functions. They mention that the input dimension is flattened and the output dimension is 2.
- Why unresolved: The paper does not explore the impact of different gating network architectures on FedMoE's performance or provide ablation studies.
- What evidence would resolve it: Experiments comparing FedMoE's performance with different gating network architectures (e.g., varying the number of linear layers, activation functions) and analysis of the impact on convergence, accuracy, and computational costs.

### Open Question 3
- Question: How does FedMoE perform when the number of seen classes assigned to each client varies significantly?
- Basis in paper: [explicit] The authors analyze FedMoE's robustness to pathological non-IIDness by varying the number of seen classes assigned to each client. They show that model accuracy drops as non-IIDness reduces (number of seen classes rises).
- Why unresolved: The paper does not explore the extreme case where the number of seen classes assigned to each client varies significantly (e.g., some clients have very few classes while others have many).
- What evidence would resolve it: Experiments comparing FedMoE's performance in scenarios with highly imbalanced class distributions across clients, along with analysis of the impact on convergence, accuracy, and personalization.

## Limitations
- The gating network architecture details are not fully specified, particularly regarding normalization layers and activation functions
- Computational cost analysis focuses on local training but doesn't account for potential communication overhead
- Evaluation is limited to image classification tasks on CIFAR datasets, leaving uncertainty about generalization to other domains

## Confidence
- **High Confidence**: The core mechanism of using MoE for data-level personalization in heterogeneous FL settings is well-supported by experimental results and aligns with established MoE principles.
- **Medium Confidence**: The claims about computational efficiency improvements over state-of-the-art methods are supported by ablation studies, though the communication cost analysis could be more comprehensive.
- **Low Confidence**: The scalability analysis is limited to 100 clients, making it unclear how the approach would perform in larger-scale deployments with thousands of participants.

## Next Checks
1. Implement the gating network with various normalization schemes (switch vs. batch normalization) to determine which configuration yields optimal performance, as this component is critical for data-level personalization.

2. Measure the actual bandwidth requirements for exchanging homogeneous feature extractors across different network conditions to validate the claimed communication efficiency.

3. Test FedMoE on non-image datasets (e.g., healthcare or text data) to evaluate whether the approach generalizes beyond the image classification tasks presented in the paper.