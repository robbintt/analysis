---
ver: rpa2
title: Class-Balanced and Reinforced Active Learning on Graphs
arxiv_id: '2402.10074'
source_url: https://arxiv.org/abs/2402.10074
tags: []
core_contribution: The paper addresses class imbalance in active learning for graph
  neural networks (GNNs). Existing methods may lead to imbalanced labeled data, causing
  bias towards majority classes.
---

# Class-Balanced and Reinforced Active Learning on Graphs

## Quick Facts
- arXiv ID: 2402.10074
- Source URL: https://arxiv.org/abs/2402.10074
- Authors: Chengcheng Yu; Jiapeng Zhu; Xiang Li
- Reference count: 40
- Primary result: GraphCBAL framework achieves class-balanced labeled sets and competitive classification performance on six graph datasets

## Executive Summary
This paper addresses class imbalance in active learning for graph neural networks (GNNs), where traditional active learning methods often lead to labeled data dominated by majority classes, causing biased predictions. The authors propose GraphCBAL, a reinforcement learning framework that learns an optimal policy to select nodes for annotation while maintaining class balance. Using Advantage Actor-Critic (A2C) training, GraphCBAL incorporates class-balance-aware states and a reward function that balances classification performance with class diversity. GraphCBAL++ extends this with a punishment mechanism for even more class-balanced labeled sets. Experiments on Cora, Citeseer, Pubmed, Reddit, Coauthor-CS, and Coauthor-Physics datasets demonstrate superior performance compared to state-of-the-art baselines, achieving better class balance while maintaining or improving classification accuracy.

## Method Summary
GraphCBAL formulates active learning as a Markov Decision Process where an agent learns to select nodes for annotation. The framework uses a two-level reinforcement learning architecture with A2C algorithm, where a classification network evaluates current model performance, a state representation captures graph structure and labeling information, and a policy network determines which nodes to label next. The state representation includes centrality features (node importance), uncertainty measures (classification confidence), class-diversity metrics (label distribution), selectivity scores (query efficiency), and criteria-similarity measures (relationship to already labeled nodes). The reward function balances performance improvement (Micro-F1 gain) with class balance (normalized entropy of labeled set). GraphCBAL++ adds a punishment mechanism that discourages selecting nodes from already well-represented classes, achieving higher class balance at the cost of slightly lower classification performance.

## Key Results
- GraphCBAL achieves significantly better class balance (imbalance ratio closer to 1) than baseline methods while maintaining competitive or superior classification performance
- On average, GraphCBAL improves class balance by 30.0% over state-of-the-art methods while achieving comparable or better Micro-F1 and Macro-F1 scores
- GraphCBAL++ achieves even higher class balance (up to 38.5% improvement) but with slightly lower classification performance than GraphCBAL
- The framework demonstrates consistent performance across six diverse graph datasets including citation networks and co-authorship graphs

## Why This Works (Mechanism)
The framework works by learning an optimal policy that simultaneously considers both informativeness and class balance through reinforcement learning. Traditional active learning methods focus solely on selecting the most uncertain or informative nodes, which tends to favor majority classes. GraphCBAL's reward function explicitly incorporates class diversity through normalized entropy of the labeled set, encouraging the policy to select nodes from underrepresented classes. The state representation captures both local (node uncertainty, centrality) and global (class distribution, selectivity) information, allowing the policy to make informed decisions about which nodes will improve overall model performance while maintaining balance. The A2C algorithm provides stable training by reducing variance in policy updates through value function estimation.

## Foundational Learning

**Graph Neural Networks (GNNs)**: Neural networks designed to operate on graph-structured data by aggregating information from neighboring nodes. Needed to capture relational information between nodes for classification tasks. Quick check: Verify message passing and aggregation operations work correctly on graph structures.

**Reinforcement Learning with A2C**: Actor-Critic method where the actor selects actions (node queries) and the critic evaluates their value. Needed for stable policy learning in the active learning setting. Quick check: Monitor policy and value network convergence during training.

**Active Learning**: Framework where the model iteratively selects the most informative samples for annotation. Needed to reduce labeling costs while maintaining model performance. Quick check: Track labeled set size versus model performance curve.

## Architecture Onboarding

**Component Map**: Graph data -> State representation (centrality, uncertainty, class-diversity, selectivity, criteria-similarity) -> Policy network (actor) -> Action (node selection) -> Reward (performance + class balance) -> Value network (critic) -> A2C update -> Updated policy

**Critical Path**: State representation → Policy network → Action selection → Reward computation → A2C update. This sequence determines which nodes get labeled and how the policy improves over time.

**Design Tradeoffs**: The framework balances exploration (selecting uncertain nodes) with exploitation (maintaining class balance). Higher weight on class diversity in the reward function improves balance but may reduce performance. The punishment mechanism in GraphCBAL++ further emphasizes balance at the cost of some accuracy.

**Failure Signatures**: Poor class balance despite training indicates insufficient weight on class-diversity component in reward function. Low classification performance suggests state representation is not capturing informative features or policy is too focused on balance. Unstable training indicates learning rate or network architecture issues.

**First Experiments**: 1) Train with only performance reward (no class balance) to verify baseline active learning behavior. 2) Train with only class balance reward to assess pure balance optimization. 3) Test different α values in reward function to find optimal balance-performance tradeoff.

## Open Questions the Paper Calls Out
None specified in the paper.

## Limitations
- Computational overhead of reinforcement learning training compared to simple heuristics like random or uncertainty sampling
- Hyperparameter sensitivity, particularly the weighting factor α in the reward function that controls the balance-performance tradeoff
- Dependence on implementation details of baseline methods (especially ALLIE) that may not be fully reproducible without public code

## Confidence
**High Confidence**: Core methodology and theoretical framework are well-established and clearly described. Experimental design with multiple datasets and comprehensive baselines appears robust.

**Medium Confidence**: Specific implementation details such as exact hyperparameter values for classification network, precise reward function weighting, and baseline method implementations may affect exact reproducibility.

## Next Checks
1. Verify the class-diversity component weighting (α parameter) in the reward function by testing different values and observing their impact on both class balance and classification performance.

2. Reimplement the baseline methods (particularly ALLIE) independently to ensure fair comparison, given that the original implementation is not publicly available.

3. Conduct ablation studies to determine the relative importance of each state representation component (centrality, uncertainty, class-diversity, selectivity, criteria-similarity) in achieving the reported performance.