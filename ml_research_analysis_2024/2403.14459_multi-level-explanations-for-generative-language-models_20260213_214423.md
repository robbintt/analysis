---
ver: rpa2
title: Multi-Level Explanations for Generative Language Models
arxiv_id: '2403.14459'
source_url: https://arxiv.org/abs/2403.14459
tags:
- mexgen
- prob
- input
- text
- scalarizer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MExGen, a framework for explaining generative
  language models by assigning attribution scores to input context units that quantify
  their influence on model outputs. MExGen extends perturbation-based attribution
  methods like LIME and SHAP to handle challenges of text outputs, high inference
  costs, and long inputs in context-grounded tasks like summarization and QA.
---

# Multi-Level Explanations for Generative Language Models

## Quick Facts
- arXiv ID: 2403.14459
- Source URL: https://arxiv.org/abs/2403.14459
- Authors: Lucas Monteiro Paes, Dennis Wei, Hyo Jin Do, Hendrik Strobelt, Ronny Luss, Amit Dhurandhar, Manish Nagireddy, Karthikeyan Natesan Ramamurthy, Prasanna Sattigeri, Werner Geyer, Soumya Ghosh
- Reference count: 40
- Key outcome: MExGen framework achieves higher fidelity explanations than baselines like PartitionSHAP and CaptumLIME, and even outperforms LLM self-explanations from models like DeepSeek-V3

## Executive Summary
This paper introduces MExGen, a framework for explaining generative language models by assigning attribution scores to input context units that quantify their influence on model outputs. MExGen extends perturbation-based attribution methods like LIME and SHAP to handle challenges of text outputs, high inference costs, and long inputs in context-grounded tasks like summarization and QA. The framework uses multi-level refinement from coarser to finer units and offers multiple scalarizers to convert text outputs to real numbers, including text-only options. Automated evaluations show MExGen achieves higher fidelity than baselines like PartitionSHAP and CaptumLIME, and even outperforms LLM self-explanations from models like DeepSeek-V3. Human studies corroborate these findings, with users perceiving MExGen as more faithful and preferring its explanations.

## Method Summary
MExGen addresses the challenge of explaining generative language models by providing a framework that assigns attribution scores to input context units. The method uses multi-level refinement, starting with coarse-grained attribution (e.g., sentences), identifying the most influential units, and only refining those into finer-grained units (e.g., phrases, words). This selective refinement strategy reduces computational cost while maintaining attribution accuracy. The framework employs linear-complexity attribution algorithms (LOO, C-LIME, L-SHAP) that scale linearly with input units, and multiple scalarizers to convert text outputs to real numbers, including text-only options like BERTScore and BARTScore. The method was evaluated on summarization datasets (XSUM, CNN/DM), QA dataset (SQuAD), and compared against baselines including PartitionSHAP, CaptumLIME, and LLM self-explanations.

## Key Results
- MExGen variants achieve higher fidelity than baselines (PartitionSHAP, CaptumLIME) in automated evaluations using perturbation curves and AUPC
- Text-only scalarizers (BERTScore, BARTScore) provide faithful explanations comparable to logit-based methods in user studies
- MExGen outperforms LLM self-explanations (DeepSeek-V3) in both automated and human evaluations
- Multi-level refinement strategy reduces computational cost while maintaining attribution accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MExGen uses multi-level refinement to reduce computational cost while maintaining attribution accuracy
- Mechanism: The framework starts with coarse-grained attribution (e.g., sentences), identifies the most influential units, and only refines those into finer-grained units (e.g., phrases, words), avoiding attribution computation for less relevant content
- Core assumption: A small fraction of input units contribute disproportionately to the output, making selective refinement sufficient
- Evidence anchors:
  - [section] "The multi-level approach is essential for obtaining fine-grained attributions to the most important units in the context without a drastic increase in the computational cost"
  - [abstract] "The framework uses multi-level refinement from coarser to finer units"
  - [corpus] Found 25 related papers; none directly test multi-level refinement strategies, though hierarchical explanation methods exist
- Break condition: If the most important content is distributed uniformly across the input, the selective refinement strategy would miss critical contributions from non-refined regions

### Mechanism 2
- Claim: Text-only scalarizers can approximate explanations based on model logits
- Mechanism: Scalarizers like BERTScore and BARTScore map text outputs to real numbers using auxiliary models, enabling attribution even when only text output is available, and these approximations correlate well with log-probability-based explanations
- Core assumption: Semantic similarity between generated and target outputs correlates with their importance for the model's decision
- Evidence anchors:
  - [abstract] "Our framework can provide more faithful explanations of generated output than available alternatives, including LLM self-explanations"
  - [section] "BERT uses only text output from the LM while Log Prob depends on output logits" but "BERT to be higher in fidelity than Log Prob" in user studies
  - [corpus] Limited corpus evidence for text-only scalarizer effectiveness in attribution contexts
- Break condition: If the scalarizer's semantic similarity measure diverges significantly from the model's actual probability distribution, the approximation fails

### Mechanism 3
- Claim: Linear-complexity attribution algorithms enable scalable explanation of long inputs
- Mechanism: MExGen uses algorithms like LOO, C-LIME, and L-SHAP that scale linearly with input units rather than exponentially, making them practical for long documents
- Core assumption: Perturbation-based explanations can be computed efficiently by limiting the number of model inferences needed
- Evidence anchors:
  - [abstract] "MExGen provides a framework to address these challenges" of "high inference cost" and "long text inputs"
  - [section] "We consider only perturbation-based attribution algorithms that scale linearly with the number of units d in terms of model queries"
  - [corpus] No direct corpus evidence comparing linear vs exponential attribution algorithm scalability in LLM contexts
- Break condition: If the linear-complexity approximations introduce systematic biases that make the attributions less faithful than full SHAP would provide

## Foundational Learning

- Concept: Perturbation-based attribution
  - Why needed here: MExGen needs to measure how input changes affect outputs to quantify feature importance
  - Quick check question: How does removing a single input unit help determine its importance to the model's output?

- Concept: Scalarization of text outputs
  - Why needed here: Generative models output text, but attribution methods require real-valued functions to quantify sensitivity
  - Quick check question: What is the difference between using log probabilities versus semantic similarity measures as scalarizers?

- Concept: Multi-level refinement strategy
  - Why needed here: Long inputs make full attribution computationally expensive, so selective refinement focuses computation where it matters most
  - Quick check question: Why would refining only the top-k sentences into phrases be more efficient than refining all sentences?

## Architecture Onboarding

- Component map: Input → Linguistic segmentation → Multi-level attribution (LOO/C-LIME/L-SHAP) → Scalarization (Log Prob/BERT/BART/SUMM/Log NLI) → Output attribution scores
- Critical path: Input segmentation → Initial coarse attribution → Unit selection for refinement → Fine-grained attribution → Scalarization → Final attribution
- Design tradeoffs: Linear-complexity algorithms trade some attribution fidelity for computational feasibility; text-only scalarizers trade logit access for broader applicability
- Failure signatures: Low Spearman correlation between scalarizers indicates poor scalarizer choice; slow refinement indicates too many units selected for refinement; high computational cost indicates inefficient unit selection
- First 3 experiments:
  1. Run MExGen with Log Prob scalarizer on a short summarization example to verify basic functionality
  2. Compare perturbation curves between MExGen and baseline methods on a single example
  3. Test multi-level refinement by comparing sentence-level vs mixed-level attributions on the same example

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different scalarizers compare in terms of computational efficiency and trade-offs between explanation quality and inference cost?
- Basis in paper: [explicit] The paper discusses computational costs of scalarizers, noting that BERTScore and BARTScore require additional LLM inference while Log Prob only requires one forward pass.
- Why unresolved: The paper provides some qualitative comparisons but doesn't systematically quantify the trade-offs between explanation quality (measured by fidelity metrics) and computational cost across scalarizers.
- What evidence would resolve it: A comprehensive benchmark comparing scalarizer fidelity scores against their respective computational costs (inference time, memory usage) across various model sizes and tasks.

### Open Question 2
- Question: Can the multi-level attribution approach be extended to incorporate hierarchical explanations that capture feature interactions at different levels?
- Basis in paper: [inferred] The paper mentions that hierarchical explanations (like HEDGE) could be incorporated into MExGen's multi-level framework but doesn't explore this possibility.
- Why unresolved: The current MExGen framework segments text into linguistic units but doesn't explicitly model interactions between units at different levels or capture compositional semantics.
- What evidence would resolve it: Experimental results comparing MExGen's current multi-level approach against a version that incorporates hierarchical interaction detection, measuring improvements in explanation fidelity.

### Open Question 3
- Question: How does the choice of unit segmentation granularity affect explanation quality and interpretability across different domains and text types?
- Basis in paper: [explicit] The paper discusses linguistic segmentation into paragraphs, sentences, phrases, and words but notes that the optimal granularity depends on the task and user preferences.
- Why unresolved: While the paper shows that different granularities work for summarization versus QA, it doesn't systematically investigate how segmentation choices affect explanations for different text domains (legal, medical, technical) or user groups.
- What evidence would resolve it: A user study comparing explanation effectiveness across different segmentation strategies for domain-specific texts, measuring both quantitative fidelity metrics and qualitative user satisfaction scores.

## Limitations
- The human study with 88 participants may not represent all user types or domains
- Comparison with LLM self-explanations is limited to a single self-explanation model (DeepSeek-V3)
- Automated evaluation metrics may not fully capture attribution fidelity in real-world scenarios where user objectives vary

## Confidence
- **High Confidence**: The core mechanism of multi-level refinement for computational efficiency is well-supported by the described implementation and comparison with baseline methods
- **Medium Confidence**: Claims about text-only scalarizers achieving comparable fidelity to logit-based methods are supported by user studies but would benefit from additional automated validation across diverse models
- **Medium Confidence**: The assertion that MExGen outperforms LLM self-explanations is based on limited comparisons and specific models, warranting broader testing

## Next Checks
1. Test MExGen's attribution fidelity across diverse domains beyond summarization (e.g., dialogue generation, code generation) to verify generalization claims
2. Conduct ablation studies removing the multi-level refinement component to quantify its contribution to computational efficiency and attribution quality
3. Compare MExGen against additional self-explanation methods beyond DeepSeek-V3, including different model families and prompt-based approaches, to strengthen the superiority claims