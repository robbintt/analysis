---
ver: rpa2
title: 'Large Stepsize Gradient Descent for Logistic Loss: Non-Monotonicity of the
  Loss Improves Optimization Efficiency'
arxiv_id: '2402.15926'
source_url: https://arxiv.org/abs/2402.15926
tags:
- lemma
- loss
- have
- stepsize
- assumption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper analyzes the convergence of gradient descent (GD) for\
  \ logistic regression with linearly separable data under large stepsizes, where\
  \ the loss may initially oscillate. The authors show that GD exits the oscillatory\
  \ (EoS) phase in O(\u03B7) steps and then achieves an O(1/(\u03B7t)) convergence\
  \ rate in the stable phase."
---

# Large Stepsize Gradient Descent for Logistic Loss: Non-Monotonicity of the Loss Improves Optimization Efficiency

## Quick Facts
- arXiv ID: 2402.15926
- Source URL: https://arxiv.org/abs/2402.15926
- Reference count: 40
- Large stepsize GD achieves O(1/T²) loss for logistic regression with linearly separable data by exiting oscillatory phase in O(η) steps

## Executive Summary
This paper analyzes gradient descent (GD) for logistic regression with linearly separable data under large stepsizes, where the loss may initially oscillate. The authors show that GD exits the oscillatory (edge of stability) phase in O(η) steps and then achieves an O(1/(ηt)) convergence rate in the stable phase. They prove that with a stepsize η = Θ(T) for a total of T steps, GD attains an accelerated O(1/T²) loss, without using momentum or variable stepsize schedulers. The technique relies on a novel split-optimization bound that handles large stepsizes. The results are extended to general classification losses, nonlinear predictors in the NTK regime, and online SGD with large stepsizes under suitable separability conditions.

## Method Summary
The paper analyzes constant-stepsize GD with w_{t+1} = w_t - η∇L(w_t) for logistic loss on bounded, linearly separable data. The key innovation is a split-optimization bound that enables tracking average loss even when local oscillations occur. The analysis proceeds in two phases: an initial oscillatory phase where the loss may increase locally but average loss decreases, followed by a stable phase with monotonic convergence. By carefully choosing stepsize η = Θ(T), the algorithm balances time spent in each phase to achieve O(1/T²) total loss.

## Key Results
- GD exits the initial oscillatory phase in O(η) steps for linearly separable logistic regression
- After phase transition, GD achieves O(1/(ηt)) convergence rate in stable phase
- With η = Θ(T), GD attains accelerated O(1/T²) loss for total T steps
- Results extend to general classification losses, NTK regime, and online SGD under suitable conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large stepsize GD exits the initial oscillatory phase in O(η) steps, then achieves O(1/(ηt)) convergence in the stable phase.
- Mechanism: The split-optimization bound shows that for a carefully chosen comparator u = u1 + u2, the non-positive third term in the descent inequality cancels large gradient norm effects, enabling control of average loss even when the loss oscillates locally.
- Core assumption: The data is bounded (||x_i|| ≤ 1) and linearly separable with margin γ > 0, ensuring ⟨y_i x_i, w^*⟩ ≥ γ for all i.
- Evidence anchors:
  - [abstract]: "We show that GD exits this initial oscillatory phase rapidly — in O(η) steps — and subsequently achieves an O(1/(ηt)) convergence rate after t additional steps."
  - [section]: Lemma 7's split-optimization bound with u2 = η w*/(2γ) cancels the large gradient norm term.
  - [corpus]: Missing direct support; this is a novel theoretical construction not referenced in neighbors.
- Break condition: If data is not linearly separable or margin γ is too small, the comparator construction fails and the oscillation may persist indefinitely.

### Mechanism 2
- Claim: Given a budget of T steps, GD can achieve O(1/T²) loss by choosing η = Θ(T), balancing time spent in oscillatory and stable phases.
- Mechanism: The phase transition time bound τ scales linearly with η, while the stable-phase convergence rate improves with larger η. Setting η proportional to T balances these effects, yielding acceleration.
- Core assumption: The total step budget T is large enough that τ ≤ T/2 for the chosen η, ensuring sufficient time in the stable phase.
- Evidence anchors:
  - [abstract]: "Our results imply that, given a budget of T steps, GD can achieve an accelerated loss of O(1/T²) with an aggressive stepsize η := Θ(T)."
  - [section]: Corollary 2 verifies τ ≤ T/2 and derives the O(1/T²) bound.
  - [corpus]: Weak support; neighbors discuss stepsize effects but not this specific acceleration mechanism.
- Break condition: If T is not sufficiently large relative to problem parameters (n, γ), the phase transition may not complete, preventing acceleration.

### Mechanism 3
- Claim: The results extend to general classification losses, nonlinear predictors in NTK regime, and online SGD with large stepsizes under suitable separability conditions.
- Mechanism: The split-optimization technique generalizes by incorporating loss-specific properties (Lipschitzness, self-boundedness, exponential tail) into the comparator construction and error bounds.
- Core assumption: The loss function satisfies Assumptions 3A-3D (regularity, Lipschitzness, self-boundedness, exponential tail), and the NTK features maintain sufficient margin under the network width condition.
- Evidence anchors:
  - [abstract]: "Our proof technique is versatile and also handles general classification loss functions... nonlinear predictors in the neural tangent kernel regime, and online stochastic gradient descent (SGD) with a large stepsize, under suitable separability conditions."
  - [section]: Theorem 6 generalizes the analysis to general losses and NTK regime with specific width conditions.
  - [corpus]: Limited direct evidence; neighbors discuss stepsize effects but not this broad generalization.
- Break condition: If the loss lacks the required properties (e.g., no exponential tail for acceleration), the phase transition bound degrades and O(1/T²) acceleration fails.

## Foundational Learning

- Concept: Convexity and smoothness of logistic loss
  - Why needed here: Ensures the descent inequality holds and enables the split-optimization bound to control average loss even with large stepsizes.
  - Quick check question: Why does the logistic loss satisfy |ℓ''(z)| ≤ ℓ(z) for all z, and how does this help prove 1-smoothness of ln L(w)?

- Concept: Neural Tangent Kernel (NTK) regime and linearization
  - Why needed here: Allows treating the two-layer network as approximately linear in the infinite-width limit, enabling reuse of linear analysis techniques.
  - Quick check question: How does the NTK feature margin condition in Assumption 4 ensure that the dataset remains separable in the feature space induced by the network gradient?

- Concept: Martingale concentration and Freedman's inequality
  - Why needed here: Converts per-iteration risk bounds into population risk bounds for SGD by controlling the deviation between empirical and population losses.
  - Quick check question: Why does the uniform bound on gradient norms in Lemma 17 imply that |L(w_k) - L_k(w_k)| ≤ M almost surely, and how does this feed into Freedman's inequality?

## Architecture Onboarding

- Component map: Linear logistic regression analysis -> Split-optimization bound construction -> Phase transition analysis -> General loss extension -> NTK regime application -> SGD generalization
- Critical path: Start with linear logistic regression → establish split-optimization bound → prove phase transition → extend to general losses → apply to NTK regime → handle SGD case. Each step requires the previous as foundation.
- Design tradeoffs: Larger stepsizes improve asymptotic convergence but require longer phase transition. Generalizing to NTK increases width requirements polynomially in T. Exponential tail conditions enable acceleration but exclude some common losses.
- Failure signatures: Loss fails to decrease monotonically even after O(η) steps (phase transition incomplete), gradient potential grows too large violating bounds, or width condition m ≥ Ω(η²) becomes computationally prohibitive.
- First 3 experiments:
  1. Verify O(1/(ηt)) convergence rate empirically for linear logistic regression with varying η and t, comparing to theory.
  2. Test phase transition timing by plotting loss vs steps for increasing η, checking if transition completes by O(η) steps.
  3. Validate NTK generalization by training a two-layer network with large η, measuring if width condition m ≥ Ω(η²) is necessary for convergence.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the acceleration benefit of large stepsizes for SGD (Corollary 2 for GD) extend beyond logarithmic factors, or can SGD with large stepsizes achieve better than O(1/sqrt(t)) rates?
- Basis in paper: The paper states "Unlike the result in Section 2 for batch GD, Theorem 4 does not show the benefits of large stepsize for SGD... We leave it as an open problem to investigate whether or not a large stepsize benefits SGD."
- Why unresolved: The paper provides bounds for SGD with large stepsizes but shows these do not improve upon small stepsize bounds except for logarithmic factors. The theoretical framework for analyzing large stepsizes in SGD may need different techniques than those used for GD.
- What evidence would resolve it: A rigorous proof showing either (a) a matching lower bound proving SGD with large stepsizes cannot beat O(1/sqrt(t)) rates, or (b) a new analysis technique that extracts acceleration benefits from large stepsizes in SGD.

### Open Question 2
- Question: Can the phase transition time bound for GD (Theorem 1) be improved to remove the linear dependence on n, allowing acceleration with T = o(n)?
- Basis in paper: The paper states "We conjecture our phase transition time bound is improvable and the T ≥ Ω(n) condition in Corollary 2 can be relaxed."
- Why unresolved: The current analysis requires T ≥ Ω(n) because the phase transition time bound scales linearly with n. The techniques used may be too conservative in tracking when GD exits the EoS phase.
- What evidence would resolve it: A refined analysis showing phase transition time bound of O(max{η, e, η ln η/η}) independent of n, or a counterexample proving the linear dependence on n is necessary.

### Open Question 3
- Question: How does the implicit bias and generalization of GD with very large stepsizes (η ≥ ω(1) compared to total steps T) differ from the classical max-margin direction analysis?
- Basis in paper: The paper states "We leave for future work the study of the implicit bias and generalization of GD with a very large stepsize, that is, η ≥ ω(1) compared to the total number of steps T."
- Why unresolved: Standard implicit bias results assume η = O(1), but the paper's analysis allows η = Θ(T), leading to iterates with norm scaling as O(η). The classical margin analysis may not apply in this regime.
- What evidence would resolve it: A theoretical analysis characterizing the direction of the final iterate and its margin, or empirical studies showing the generalization behavior of GD with very large stepsizes.

### Open Question 4
- Question: What is the minimum network width required for GD to escape the NTK regime when using polynomially large stepsizes (η = poly(T))?
- Basis in paper: The paper states "the width condition in Theorem 6 is at least Ω(η2), which will become polynomial in the number of steps when GD is equipped with a polynomially large stepsize η = poly(T)".
- Why unresolved: While the paper provides width bounds for various loss functions and stepsizes, the exact minimum width needed for GD to exhibit feature learning (escape NTK regime) with large stepsizes remains unclear.
- What evidence would resolve it: A lower bound on the minimum width required for GD to achieve non-trivial test error with large stepsizes, or a constructive proof showing a specific width suffices.

### Open Question 5
- Question: Can the analysis techniques for large stepsizes be extended to other optimization methods like momentum-based algorithms or adaptive stepsize schedulers?
- Basis in paper: The paper states "We remark that our results for constant stepsize can be applied recursively for analyzing variable stepsize schedulers, but this is beyond the scope of the current paper."
- Why unresolved: The paper focuses on constant-stepsize GD, but the techniques (split optimization bound, handling non-monotonic losses) may be applicable to more sophisticated optimization methods.
- What evidence would resolve it: A rigorous analysis showing acceleration or convergence benefits of large stepsizes for momentum methods or adaptive stepsize schedulers, or a proof that such benefits cannot be obtained.

## Limitations

- The polynomial width condition m ≥ Ω(η²) becomes computationally prohibitive for large T in NTK regime
- Acceleration mechanism relies on exponential tail assumption, excluding common losses like hinge loss
- Strong separability condition for SGD may not hold in practice, limiting online learning applications

## Confidence

- Phase Transition Analysis: Medium - bound's tightness depends on margin and data geometry
- NTK Regime Extensions: Medium - linearization assumption may break down before reaching theoretical rate
- General Loss Generalization: Low-Medium - exponential tail assumption excludes common losses
- Online SGD Analysis: Low - separability condition stronger than standard margin assumptions

## Next Checks

1. Empirically measure phase transition timing across different η values and compare against O(η) theoretical bound, checking sensitivity to margin γ and data dimension d.

2. Train two-layer networks with varying widths m relative to η² to verify when NTK analysis breaks down and measure actual convergence rates vs predicted O(1/T²).

3. Test acceleration mechanism with losses lacking exponential tails (e.g., hinge, squared loss) to quantify degradation in convergence rates and identify critical assumptions.