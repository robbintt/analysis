---
ver: rpa2
title: Leveraging Large Language Models with Chain-of-Thought and Prompt Engineering
  for Traffic Crash Severity Analysis and Inference
arxiv_id: '2408.04652'
source_url: https://arxiv.org/abs/2408.04652
tags:
- crash
- severity
- accident
- llms
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores the use of large language models (LLMs) for
  traffic crash severity analysis, framing it as a classification task. The authors
  convert tabular crash data into textual narratives and incorporate Chain-of-Thought
  (CoT) reasoning to guide LLMs in analyzing crash causes and inferring severity.
---

# Leveraging Large Language Models with Chain-of-Thought and Prompt Engineering for Traffic Crash Severity Analysis and Inference

## Quick Facts
- arXiv ID: 2408.04652
- Source URL: https://arxiv.org/abs/2408.04652
- Authors: Hao Zhen; Yucheng Shi; Yongcan Huang; Jidong J. Yang; Ninghao Liu
- Reference count: 6
- Key outcome: LLaMA3-70B consistently outperforms other models in crash severity classification using textual narratives, Chain-of-Thought reasoning, and prompt engineering techniques

## Executive Summary
This study explores the application of large language models (LLMs) for traffic crash severity analysis, framing it as a multi-class classification task. The authors convert structured crash data into textual narratives and employ Chain-of-Thought (CoT) reasoning and prompt engineering techniques to guide LLMs in analyzing crash causes and inferring severity levels. The approach leverages LLMs' natural language understanding capabilities rather than traditional statistical feature engineering.

The research evaluates three state-of-the-art LLMs (GPT-3.5-turbo, LLaMA3-8B, and LLaMA3-70B) across multiple settings including zero-shot, few-shot, CoT, and prompt engineering. Results demonstrate that LLaMA3-70B consistently outperforms the other models, particularly in zero-shot settings, while both CoT and prompt engineering significantly enhance performance by improving logical reasoning and addressing alignment constraints that bias severity classifications.

## Method Summary
The study converts tabular crash data from Victoria, Australia (2006-2020) into textual narratives using a template-based approach that incorporates domain knowledge. These narratives serve as input to three LLMs (GPT-3.5-turbo, LLaMA3-8B, and LLaMA3-70B) for multi-class severity classification across three categories: fatal accidents, serious injury accidents, and minor/non-injury accidents. The research tests six experimental settings: zero-shot (ZS), zero-shot with Chain-of-Thought (ZS_CoT), zero-shot with prompt engineering (ZS_PE), zero-shot with both CoT and PE (ZS_PE_CoT), few-shot (FS), and few-shot with prompt engineering (FS_PE). Performance is evaluated using macro F1-score, macro-accuracy, and class-specific accuracies.

## Key Results
- LLaMA3-70B consistently outperformed GPT-3.5-turbo and LLaMA3-8B across all experimental settings, particularly in zero-shot scenarios
- Chain-of-Thought prompting significantly improved logical reasoning and classification accuracy across all three models
- Prompt engineering successfully addressed alignment constraints, improving fatal accident classification from 0% to 62% accuracy for GPT-3.5-turbo in zero-shot settings
- The combination of CoT and prompt engineering produced the best overall performance, though with some trade-offs in fatal accident detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can process unstructured textual narratives derived from tabular crash data to perform severity classification.
- Mechanism: The conversion of structured crash attributes into coherent textual narratives leverages LLMs' pre-trained language understanding capabilities, allowing them to analyze complex, multi-factor crash scenarios through natural language processing rather than traditional statistical feature engineering.
- Core assumption: The textual narrative template effectively captures the essential domain knowledge and causal relationships present in the original tabular data.
- Evidence anchors: [abstract] states "We generate textual narratives from original traffic crash tabular data using a pre-built template infused with domain knowledge." [section 3.2] describes the template-based conversion process showing how crash attributes are transformed into human-readable narratives. [corpus] shows related work (CrashSage, SDIGLM) that similarly convert structured data to text for LLM analysis.
- Break Condition: If the narrative template fails to preserve critical crash information or introduces bias that misleads the LLM's reasoning process.

### Mechanism 2
- Claim: Chain-of-Thought (CoT) prompting significantly enhances LLM reasoning for crash severity analysis.
- Mechanism: CoT guides LLMs through structured, step-by-step reasoning about crash causation and severity factors, making their decision-making process more transparent and improving accuracy by explicitly considering multiple factors like environmental conditions, driver behavior, and vehicle characteristics.
- Core assumption: The LLM's internal knowledge includes sufficient domain-specific understanding of traffic safety factors to benefit from guided reasoning.
- Evidence anchors: [abstract] states "CoT offers valuable insights into LLMs' reasoning processes, unleashing their capacity to consider diverse factors such as environmental conditions, driver behavior, and vehicle characteristics." [section 4.2.1] describes CoT prompts requiring models to "methodically reason through the details of the accident to determine both the cause and the severity outcome." [section 5.4] shows CoT improved macro F1-score and macro-accuracy across all three models.
- Break Condition: If CoT prompts become too complex or introduce noise that overwhelms the model's reasoning capacity, potentially degrading performance.

### Mechanism 3
- Claim: Prompt engineering effectively addresses alignment issues and improves fatal accident classification.
- Mechanism: By rephrasing "Fatal accident" labels to softer alternatives like "Serious accident with potentially fatal outcomes," prompt engineering circumvents LLMs' alignment constraints that avoid discussing unpleasant topics, while maintaining classification accuracy.
- Core assumption: The alignment constraints in LLMs are strong enough to impact fatal accident classification but can be mitigated through careful prompt reformulation.
- Evidence anchors: [section 2.3] explains "LLMs often exhibit a tendency to avoid assigning this label to relevant cases" due to alignment training. [section 4.2.1] describes the specific prompt engineering approach of softening fatal accident labels. [section 5.2] shows GPT-3.5 with PE achieved 62% accuracy for fatal accidents vs 0% in plain zero-shot setting.
- Break Condition: If the softened labels become too vague and reduce the model's ability to distinguish between severity levels.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: CoT provides structured reasoning steps that help LLMs analyze complex crash scenarios by considering multiple contributing factors systematically.
  - Quick check question: How does CoT prompting differ from standard prompting in terms of the model's output format?

- Concept: Prompt engineering techniques
  - Why needed here: Prompt engineering tailors input prompts to guide LLMs toward more relevant and reliable analysis while addressing alignment constraints that might bias severity classifications.
  - Quick check question: What specific prompt engineering technique was used to improve fatal accident classification accuracy?

- Concept: Zero-shot vs few-shot learning
  - Why needed here: Understanding these learning paradigms is crucial for evaluating how LLMs perform crash severity inference with varying amounts of training examples.
  - Quick check question: According to the results, which learning approach showed better performance for larger models like LLaMA3-70B?

## Architecture Onboarding

- Component map: Crash tabular data -> Textual narrative generator (template-based) -> LLM input -> Prompt modules (Plain, CoT, PE, CoT+PE) -> LLM models (GPT-3.5-turbo, LLaMA3-8B, LLaMA3-70B) -> Severity Classification -> Evaluation metrics
- Critical path: Data -> Narrative Generation -> Prompt Application -> LLM Inference -> Severity Classification -> Evaluation
- Design tradeoffs: Model size vs. performance (LLaMA3-70B outperforms but requires more resources); Zero-shot vs. few-shot (few-shot improves performance but needs labeled examples); CoT vs. PE (both improve performance, PE more effective for fatal accidents)
- Failure signatures: Systematic bias toward intermediate severity categories; Reduced performance on specific severity categories when combining CoT and PE; Inconsistent performance improvements across model sizes
- First 3 experiments: 1) Implement and test the narrative generation template with a small sample of crash data to verify information preservation; 2) Evaluate plain zero-shot performance of each LLM model to establish baseline classification accuracy; 3) Test CoT prompting with one model to verify improvement in reasoning transparency and classification accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different prompt engineering strategies affect the classification of minor vs. serious injury accidents in traffic crash severity analysis?
- Basis in paper: [explicit] The paper discusses prompt engineering techniques, including modifying class labels to "Serious accident with potentially fatal outcomes" instead of "Fatal accidents," but does not specifically compare strategies for minor vs. serious injury classifications.
- Why unresolved: The study primarily focuses on the impact of prompt engineering on fatal accident classification and overall model performance, with limited analysis on its differential effects across minor and serious injury categories.
- What evidence would resolve it: Conducting experiments that systematically vary prompt engineering strategies specifically for minor and serious injury accident classifications, comparing model performance metrics across these categories.

### Open Question 2
- Question: What is the optimal balance between Chain-of-Thought reasoning steps and response conciseness for different LLM sizes in traffic crash severity analysis?
- Basis in paper: [inferred] The paper notes that GPT-3.5 responses are more concise than LLaMA models, and that CoT settings produce longer responses, but does not investigate optimal balance or size-specific considerations.
- Why unresolved: The study applies uniform CoT prompting across different model sizes without exploring how the length and depth of reasoning steps might affect performance differently for smaller vs. larger models.
- What evidence would resolve it: Systematic experiments varying the complexity and length of CoT prompts for different model sizes, measuring the trade-off between reasoning depth and inference accuracy.

### Open Question 3
- Question: How does the inclusion of specific domain knowledge (e.g., traffic safety engineering principles) in prompts affect LLM performance compared to general CoT and prompt engineering approaches?
- Basis in paper: [explicit] The paper mentions that domain-specific knowledge could be added to prompts but does not investigate this possibility, focusing instead on general CoT and PE techniques.
- Why unresolved: The study uses generic prompts and CoT approaches without incorporating specialized traffic safety knowledge that could potentially enhance model reasoning and classification accuracy.
- What evidence would resolve it: Experiments comparing model performance using prompts with embedded traffic safety engineering principles versus standard CoT and PE approaches, measuring improvements in classification accuracy and reasoning quality.

## Limitations

- Template design and data fidelity concerns: The narrative conversion process may lose critical crash information or introduce bias, as the specific template design is not fully detailed
- Model size dependency without clear architectural understanding: Performance improvements may be attributed to parameter count rather than architectural differences, leaving uncertainty about smaller model potential
- Limited generalizability to different datasets and contexts: Results are based on a single Australian dataset, and the narrative template may not translate well to different crash reporting standards or geographic regions

## Confidence

**High Confidence**: The core finding that LLMs can effectively analyze crash severity when provided with textual narratives and appropriate prompting techniques. The experimental methodology is sound and results are statistically significant across multiple evaluation metrics.

**Medium Confidence**: The claim that Chain-of-Thought reasoning significantly improves performance. While results show improvement, the paper doesn't fully explore the trade-offs between reasoning depth and computational efficiency, nor does it provide systematic ablation studies on CoT components.

**Low Confidence**: The generalizability of results to different crash datasets or geographic regions. The study uses a single Australian dataset from a specific time period, and the narrative template may not translate well to different crash reporting standards or cultural contexts.

## Next Checks

1. **Template Fidelity Audit**: Conduct a systematic comparison between the original tabular data and generated narratives to quantify information loss. Create a validation dataset where crash experts manually annotate which critical factors are preserved in the textual conversion, then correlate these findings with LLM performance variations.

2. **Cross-Dataset Generalization Test**: Apply the exact methodology to crash datasets from different countries (e.g., US FARS data or European CARE database) using the same narrative template and prompting strategies. Compare performance degradation to assess whether the approach generalizes beyond the Victoria, Australia context.

3. **Alignment Mechanism Investigation**: Design controlled experiments that test alternative approaches to handling fatal accident classification, such as fine-tuning on balanced datasets or using reinforcement learning from human feedback, to determine whether prompt engineering is the most effective solution for alignment constraints.