---
ver: rpa2
title: 'GPT or BERT: why not both?'
arxiv_id: '2410.24159'
source_url: https://arxiv.org/abs/2410.24159
tags:
- language
- causal
- training
- masked
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We present GPT-BERT, a hybrid language model that unifies causal
  and masked language modeling objectives. By reformulating masked next-token prediction
  and shifting predictions one position to the right, we create a single model that
  can seamlessly switch between masked and causal modes.
---

# GPT or BERT: why not both?
## Quick Facts
- arXiv ID: 2410.24159
- Source URL: https://arxiv.org/abs/2410.24159
- Reference count: 23
- Key result: GPT-BERT hybrid model outperforms both pure MLM and pure CLM approaches across multiple benchmarks including BLiMP (81.2%), MNLI (80.1%), and GLUE (76.5%)

## Executive Summary
This paper introduces GPT-BERT, a hybrid language model architecture that unifies causal and masked language modeling objectives. The key innovation is reformulating masked next-token prediction by shifting predictions one position to the right, enabling a single model to seamlessly switch between masked and causal modes. Tested within the BabyLM Challenge 2024 with 119M and 30M parameter models, GPT-BERT demonstrates superior performance across multiple benchmarks without requiring additional parameters or training time compared to single-objective models. The approach also exhibits in-context learning capabilities despite being trained on limited data.

## Method Summary
GPT-BERT achieves its hybrid capability through a novel reformulation of masked language modeling that shifts predictions one position to the right. This architectural modification allows the model to alternate between masked and causal training objectives during pretraining while maintaining a unified architecture. The alternating training schedule leverages both CLM and MLM objectives without requiring separate model components. The authors demonstrate that this approach achieves better performance across multiple benchmarks while maintaining the ability to generate text when using repetition penalties.

## Key Results
- GPT-BERT achieves 81.2% on BLiMP, 80.1% on MNLI, and 76.5% on GLUE benchmarks
- Outperforms both pure MLM and pure CLM approaches in the BabyLM Challenge setting
- Demonstrates in-context learning capabilities despite limited training data
- Achieves better performance without requiring additional parameters or training time

## Why This Works (Mechanism)
The hybrid architecture works by reformulating masked language modeling to shift predictions one position to the right, effectively unifying the causal and masked objectives into a single framework. This allows the model to leverage both types of supervision during training while maintaining the ability to perform both tasks during inference. The alternating training schedule ensures the model learns from both objectives without requiring separate architectural components.

## Foundational Learning
1. **Masked Language Modeling (MLM)**: Why needed - Enables bidirectional context understanding by predicting masked tokens using surrounding context. Quick check - Model should correctly predict masked words given bidirectional context.
2. **Causal Language Modeling (CLM)**: Why needed - Enables autoregressive generation by predicting next tokens sequentially. Quick check - Model should generate coherent text in forward direction.
3. **Transformer Architecture**: Why needed - Provides the foundation for both MLM and CLM through self-attention mechanisms. Quick check - Verify attention patterns for both bidirectional and causal modes.
4. **Alternating Training Schedules**: Why needed - Balances learning from both MLM and CLM objectives. Quick check - Monitor loss curves for both objectives during training.
5. **Position Shifting Mechanism**: Why needed - Enables unification of MLM and CLM objectives in a single framework. Quick check - Verify correct alignment of predictions with target tokens.

## Architecture Onboarding
Component Map: Input Embeddings -> Transformer Layers -> Position Shifted MLM Head <-> Alternating Schedule Controller
Critical Path: Token input flows through standard transformer layers, with the key modification being the position-shifted MLM head that enables hybrid training
Design Tradeoffs: Single unified architecture vs. separate MLM and CLM models; alternating training vs. joint training
Failure Signatures: Degraded performance on causal generation tasks; reduced bidirectional context understanding; training instability during schedule transitions
First Experiments: 1) Verify alternating schedule correctly switches between objectives, 2) Test position shifting mechanism independently, 3) Evaluate performance on single-task benchmarks before hybrid evaluation

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental validation limited to BabyLM Challenge setting (119M and 30M parameters), unclear if gains scale to larger models
- No detailed analysis of computational overhead during alternating training phases or training stability ablations
- In-context learning capabilities demonstrated qualitatively without systematic evaluation of prompt sensitivity or robustness
- Evaluation focuses primarily on English benchmarks, multilingual generalization unexamined

## Confidence
- Core hybrid architecture claims: High - Well-supported by experimental results
- Performance superiority over single-objective models: Medium - Valid within BabyLM constraints but limited generalization
- In-context learning capabilities: Medium - Demonstrated but not systematically evaluated

## Next Checks
1. Test GPT-BERT at larger scales (1B+ parameters) to verify if performance gains persist
2. Conduct systematic ablation studies on the alternating training schedule and right-shift mechanism
3. Evaluate prompt robustness and in-context learning across diverse task types and prompt variations