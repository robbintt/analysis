---
ver: rpa2
title: 'FactCheck Editor: Multilingual Text Editor with End-to-End fact-checking'
arxiv_id: '2404.19482'
source_url: https://arxiv.org/abs/2404.19482
tags:
- claim
- claims
- evidence
- detection
- editor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FactCheck Editor is a multilingual text editor that automates fact-checking
  by detecting check-worthy claims, retrieving relevant web evidence, and using NLI
  models to predict claim veracity. It supports over 90 languages and provides evidence
  summaries and textual correction suggestions.
---

# FactCheck Editor: Multilingual Text Editor with End-to-End fact-checking

## Quick Facts
- **arXiv ID**: 2404.19482
- **Source URL**: https://arxiv.org/abs/2404.19482
- **Reference count**: 20
- **Primary result**: FactCheck Editor automates multilingual fact-checking with fine-tuned transformers outperforming LLMs across 118 languages

## Executive Summary
FactCheck Editor is a multilingual text editor that automates the entire fact-checking pipeline, from claim detection to evidence retrieval and veracity prediction. The system supports over 90 languages and uses a combination of fine-tuned XLM-Roberta-Large models and LLMs like Mistral-7b to provide end-to-end fact-checking capabilities. The editor not only identifies check-worthy claims but also retrieves relevant web evidence, predicts claim veracity using natural language inference, and suggests textual corrections for factual errors.

The system was evaluated on a translated news dataset across 118 languages, demonstrating that fine-tuned XLM-Roberta-Large outperforms both GPT-3.5-Turbo and Mistral-7b for claim detection (macro F1 0.743 vs 0.562 and 0.477) and veracity prediction (macro F1 0.575 vs 0.427 and 0.509). This performance advantage is particularly notable given that the model was fine-tuned on limited English datasets and leveraged multilingual transfer learning to generalize across languages without requiring task-specific training data in each language.

## Method Summary
The FactCheck Editor employs a comprehensive pipeline for multilingual fact-checking. It begins with sentence segmentation using Spacy, followed by co-reference resolution using Mistral-7b to ensure claims are fully contextualized. The system then detects check-worthy claims using a fine-tuned XLM-Roberta-Large model trained on ClaimBuster and CLEF CheckThat! Lab datasets. For evidence retrieval, Mistral-7b generates optimized search queries from the original claims, which are then used to search across multiple sources including Google, Bing, Wikipedia, Google Fact-check Explorer, and Semantic Scholar. The retrieved evidence snippets are passed to another fine-tuned XLM-Roberta-Large model for natural language inference, which performs binary classification (supports/refutes) on each claim-evidence pair. Finally, majority voting aggregates individual predictions, and Mistral-7b generates justification summaries and suggests textual corrections for factual errors.

## Key Results
- Fine-tuned XLM-Roberta-Large outperforms GPT-3.5-Turbo and Mistral-7b for claim detection (macro F1 0.743 vs 0.562 and 0.477)
- Fine-tuned XLM-Roberta-Large outperforms both LLMs for veracity prediction (macro F1 0.575 vs 0.427 and 0.509)
- System supports fact-checking across 118 languages using translated datasets
- Query generation using LLMs improves evidence retrieval quality compared to using original claims

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuned transformer models outperform LLMs in multilingual claim detection and veracity prediction by leveraging multilingual transfer learning from pre-trained models on diverse languages, enabling generalization across 118 languages without requiring task-specific training data in each language.

### Mechanism 2
Query generation using LLMs improves evidence retrieval quality for fact-checking by reformulating original claims into more effective search queries that better retrieve relevant evidence, especially when claims contain incorrect information.

### Mechanism 3
NLI-based binary classification (supports/refutes) is effective for veracity prediction when evidence relevance is pre-filtered, as pre-filtering reduces noise and makes binary classification more accurate than three-way classification.

## Foundational Learning

- **Multilingual Transformers**: Why needed here: The system needs to handle 90+ languages without task-specific training data for each language. Quick check question: How does multilingual pre-training enable zero-shot or few-shot learning across languages?
- **Natural Language Inference (NLI)**: Why needed here: NLI provides the core mechanism for determining whether evidence supports or refutes a claim. Quick check question: What's the difference between NLI as three-way classification versus binary classification in this context?
- **Chain-of-Thought Reasoning**: Why needed here: Used to prompt LLMs for tasks like claim detection and query generation, where step-by-step reasoning can improve performance. Quick check question: How does CoT prompting help LLMs perform better on fact-checking tasks compared to direct prompting?

## Architecture Onboarding

- **Component map**: React frontend with TinyMCE text editor -> FastAPI backend with REST endpoints -> ML models (XLM-Roberta-Large for claim detection/NLI, Mistral-7b for query generation/summaries, multilingual-MiniLM for embeddings) -> External services (Google Translate, search APIs, co-reference resolution)
- **Critical path**: Text input → sentence segmentation → co-reference resolution → claim detection → query generation → evidence retrieval → NLI → majority voting → summary generation → correction suggestions
- **Design tradeoffs**: Fine-tuned transformers vs LLMs (better performance but requires task-specific training vs general capability but weaker performance); Binary vs three-way NLI (simpler classification with pre-filtered evidence vs more nuanced classification with potential noise); Translation-based evaluation vs native multilingual datasets (enables evaluation across 118 languages but may introduce translation artifacts)
- **Failure signatures**: Low claim detection F1 (poor model generalization to target languages or inadequate training data); Missing relevant evidence (ineffective query generation or limited search API coverage); Incorrect veracity predictions (insufficient evidence snippets or model bias toward certain labels)
- **First 3 experiments**: 1) Evaluate claim detection performance across different language families to identify systematic weaknesses; 2) Compare query generation strategies (original claims vs LLM-generated vs hybrid) on evidence retrieval quality; 3) Test NLI performance with varying numbers of evidence snippets to find optimal evidence aggregation strategy

## Open Questions the Paper Calls Out

### Open Question 1
How effective is the fine-tuned XLM-Roberta-Large model when evaluated on languages it was not explicitly trained on, and what are the limits of this transfer learning capability? The paper notes that the model was trained on a limited number of languages (Norwegian, German, Danish) and surprisingly transferred knowledge to other languages without training data, but does not provide detailed results for each language not in the training set.

### Open Question 2
What are the specific challenges in end-to-end multilingual fact-checking that remain unresolved, and how can they be addressed? The paper mentions that end-to-end multilingual fact-checking presents unresolved challenges for both academia and industry but does not delve into the specific challenges or propose solutions.

### Open Question 3
How does the performance of the system vary across different language families, and what factors contribute to these variations? The evaluation results show varying performance across languages, with some languages performing better than others, but the paper does not analyze the reasons behind these performance variations across different language families.

### Open Question 4
What are the limitations of using LLMs for generative tasks such as summarization and claim correction in a multilingual context? The paper notes that LLMs excel at generative tasks such as summarization and suggesting claim corrections but does not explore the limitations or potential drawbacks of using LLMs for these tasks, particularly in a multilingual setting.

## Limitations
- Reliance on translated English datasets rather than native multilingual data may introduce artifacts affecting model performance
- Critical hyperparameters for fine-tuning and prompts for LLMs are not specified, making exact replication challenging
- System effectiveness depends on external search APIs that may have rate limits or algorithmic changes

## Confidence
- **High confidence**: Core finding that fine-tuned transformer models outperform LLMs for multilingual fact-checking tasks (specific F1 score comparisons provided)
- **Medium confidence**: Generalizability of results to real-world multilingual content (uses translated datasets which may not reflect natural multilingual text)
- **Low confidence**: Long-term stability of evidence retrieval component (depends on external search APIs and LLM-based query generation that could change)

## Next Checks
1. Replicate claim detection evaluation using native multilingual datasets rather than translated English data to verify performance gap persistence
2. Systematically compare evidence retrieval quality using three strategies: original claims, LLM-generated queries, and hybrid approach
3. Evaluate model performance across different language families (Romance vs. Slavic vs. East Asian) to identify systematic weaknesses in multilingual transfer learning