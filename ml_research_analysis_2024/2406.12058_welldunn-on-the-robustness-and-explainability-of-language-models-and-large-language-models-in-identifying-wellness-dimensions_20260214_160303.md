---
ver: rpa2
title: 'WellDunn: On the Robustness and Explainability of Language Models and Large
  Language Models in Identifying Wellness Dimensions'
arxiv_id: '2406.12058'
source_url: https://arxiv.org/abs/2406.12058
tags:
- mental
- health
- attention
- wellness
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WellDunn, an evaluation framework designed
  to assess the robustness and explainability of Language Models (LMs) and Large Language
  Models (LLMs) in identifying Wellness Dimensions (WDs) within mental health contexts.
  The framework utilizes two domain-grounded datasets, MULTI WD and WELL XPLAIN, to
  evaluate models based on their predictive performance, attention mechanism fidelity,
  and alignment with expert-provided explanations.
---

# WellDunn: On the Robustness and Explainability of Language Models and Large Language Models in Identifying Wellness Dimensions

## Quick Facts
- arXiv ID: 2406.12058
- Source URL: https://arxiv.org/abs/2406.12058
- Reference count: 40
- Primary result: WellDunn framework reveals that general-purpose LMs outperform domain-specific ones in mental health wellness dimension identification, with attention mechanisms failing to align with ground truth explanations

## Executive Summary
WellDunn introduces a novel evaluation framework for assessing how well Language Models and Large Language Models identify Wellness Dimensions in mental health contexts. The framework evaluates models across three dimensions: predictive performance, attention mechanism fidelity, and alignment with expert-provided explanations. Using two domain-grounded datasets (MULTI WD and WELL XPLAIN), the study systematically compares general-purpose LMs against domain-specific LMs and various LLMs. The results reveal unexpected performance patterns and significant limitations in using attention mechanisms for explainability in wellness applications.

## Method Summary
The WellDunn framework evaluates models using two datasets: MULTI WD for multi-label wellness dimension classification and WELL XPLAIN for explanation alignment. Models are assessed across three dimensions: predictive performance (accuracy, F1 scores), attention mechanism fidelity (alignment between attention weights and ground truth explanations), and explanation alignment (consistency with expert annotations). The evaluation includes general-purpose LMs (RoBERTa), domain-specific LMs, and various LLMs (GPT-3.5/4, fine-tuned LLAMA). The framework provides a structured approach to assess both robustness and explainability in mental health applications.

## Key Results
- General-purpose LMs (RoBERTa) significantly outperformed domain-specific LMs and fine-tuned LLMs in identifying wellness dimensions
- Attention mechanisms in evaluated models showed poor alignment with ground truth explanations provided by domain experts
- GPT-3.5/4 and fine-tuned LLAMA models underperformed compared to RoBERTa despite their larger parameter counts

## Why This Works (Mechanism)
The study's approach works by providing a structured evaluation framework that simultaneously assesses both performance and explainability aspects of language models in mental health contexts. By using ground truth explanations from domain experts and comparing them against model attention mechanisms, the framework exposes the gap between model predictions and interpretable reasoning. The dual-dataset approach allows for comprehensive evaluation across different aspects of wellness dimension identification, while the multi-faceted assessment criteria capture both quantitative performance and qualitative explanation quality.

## Foundational Learning
- Wellness Dimensions (WDs): Multi-dimensional aspects of mental health and well-being that need to be identified in text. Understanding these dimensions is crucial for mental health applications and forms the basis for evaluation metrics.
- Attention Mechanisms: Neural network components that assign weights to input tokens, theoretically indicating importance. Critical for interpretability but shown to be unreliable as explanation proxies in this study.
- Domain-grounded Datasets: Specialized datasets created with expert input for specific applications. Essential for ensuring evaluation quality in specialized domains like mental health.
- Multi-label Classification: Task of assigning multiple categories to a single input. Necessary for capturing the complex, overlapping nature of wellness dimensions.
- Explanation Alignment: Process of comparing model-generated explanations against ground truth expert explanations. Provides a measure of model interpretability beyond simple accuracy metrics.

## Architecture Onboarding

Component Map: Input Text -> Model Architecture -> Prediction Layer -> Attention Weights -> Performance Metrics + Explanation Metrics

Critical Path: The evaluation flow moves from input text through model processing to generate predictions and attention weights, which are then compared against ground truth labels and expert explanations. This creates a feedback loop for assessing both accuracy and interpretability.

Design Tradeoffs: The framework prioritizes explainability alongside performance, requiring both ground truth labels and expert explanations. This increases evaluation complexity but provides more comprehensive assessment. The choice to include attention mechanisms despite their known limitations reflects the current state of LLM interpretability research.

Failure Signatures: Models may show high accuracy but poor explanation alignment, indicating correct predictions without proper reasoning. Attention mechanisms may highlight irrelevant tokens, suggesting unreliable interpretability. Domain-specific models may underperform due to overfitting or insufficient training data.

First Experiments:
1. Baseline evaluation using MULTI WD dataset to establish performance benchmarks
2. Attention mechanism analysis comparing weight distributions against expert explanations
3. Cross-model comparison using WELL XPLAIN to assess explanation quality

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- The framework's focus on mental health contexts may limit generalizability to other domains
- Reliance on attention mechanisms as explainability proxies is problematic given their poor alignment with ground truth explanations
- The study does not fully explore why general-purpose models outperform domain-specific ones, leaving open questions about domain adaptation effectiveness

## Confidence

- Model performance comparison: High confidence (well-documented experimental setup and reproducible results)
- Attention mechanism limitations: High confidence (clearly demonstrated and consistent with prior literature)
- Domain-specific model underperformance: Medium confidence (results clear but underlying reasons require further investigation)
- Framework generalizability: Low confidence (applicability to other domains remains untested)

## Next Checks

1. Conduct ablation studies to isolate the impact of fine-tuning procedures on model performance, particularly for GPT-3.5/4 and LLAMA models

2. Develop and test alternative explainability metrics beyond attention mechanisms to validate the explainability findings

3. Evaluate the framework on additional domains beyond mental health to assess its generalizability and identify domain-specific factors affecting performance