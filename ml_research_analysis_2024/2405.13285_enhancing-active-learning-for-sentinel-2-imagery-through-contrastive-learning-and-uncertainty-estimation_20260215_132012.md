---
ver: rpa2
title: Enhancing Active Learning for Sentinel 2 Imagery through Contrastive Learning
  and Uncertainty Estimation
arxiv_id: '2405.13285'
source_url: https://arxiv.org/abs/2405.13285
tags: []
core_contribution: This paper presents a novel active learning method for Sentinel-2
  imagery that combines contrastive learning with uncertainty estimation to improve
  label efficiency. The approach uses MoCo for contrastive learning to encode satellite
  images into a lower-dimensional feature space, then applies Monte Carlo Dropout
  to estimate uncertainty for sample selection.
---

# Enhancing Active Learning for Sentinel 2 Imagery through Contrastive Learning and Uncertainty Estimation

## Quick Facts
- arXiv ID: 2405.13285
- Source URL: https://arxiv.org/abs/2405.13285
- Authors: David Pogorzelski; Peter Arlinghaus; Wenyan Zhang
- Reference count: 0
- Primary result: Combines MoCo contrastive learning with MC Dropout uncertainty estimation to improve active learning efficiency for Sentinel-2 classification, particularly in unbalanced class scenarios

## Executive Summary
This paper presents a novel active learning method for Sentinel-2 imagery that combines contrastive learning with uncertainty estimation to improve label efficiency. The approach uses MoCo for contrastive learning to encode satellite images into a lower-dimensional feature space, then applies Monte Carlo Dropout to estimate uncertainty for sample selection. Experiments on the Eurosat dataset demonstrate that the proposed method significantly outperforms random selection in unbalanced class scenarios, achieving 90% accuracy with only 1.8% of labeled data. While random sampling performs better in balanced settings, the method shows particular promise for real-world applications where class imbalance is common.

## Method Summary
The method combines self-supervised contrastive learning (MoCo) with uncertainty-based active learning. Sentinel-2 patches are first encoded into 2048-dimensional features using a pre-trained MoCo encoder. During active learning, Monte Carlo Dropout generates uncertainty estimates by performing multiple forward passes with dropout enabled. The algorithm selects samples using farthest-point-sampling for diversity combined with uncertainty estimation, iteratively querying labels and fine-tuning the model. The approach is evaluated on the Eurosat dataset across balanced and unbalanced class scenarios, comparing against random sampling baselines.

## Key Results
- Achieves 90% accuracy with only 1.8% of labeled data in unbalanced class scenarios
- Outperforms random selection in unbalanced settings while showing comparable performance in balanced scenarios
- MCFPS reaches 90% accuracy faster than random sampling when classes are imbalanced
- Method particularly effective for real-world applications where class imbalance is common

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning via MoCo effectively encodes Sentinel-2 patches into a discriminative feature space
- Mechanism: MoCo trains an encoder to map semantically similar views of the same image patch close together in feature space while pushing dissimilar patches apart. This creates a rich representation that captures spectral and spatial patterns in multispectral data.
- Core assumption: The positive pair (augmented views of same patch) contains sufficient semantic similarity and negative pairs provide effective contrast for learning meaningful features
- Evidence anchors:
  - [abstract] "Our approach utilizes contrastive learning together with uncertainty estimations via Monte Carlo Dropout (MC Dropout)"
  - [section] "A popular SSL model is MoCo [3]. [2] showed that MoCo generally outperforms other well-known methods in remote sensing classification tasks"
  - [corpus] Weak evidence - no direct citations about MoCo performance on Sentinel-2 in neighbor papers
- Break condition: If data augmentation fails to preserve class semantics or if negative sampling introduces noise that overwhelms the signal

### Mechanism 2
- Claim: Uncertainty estimation via Monte Carlo Dropout identifies samples that provide maximal information gain for model training
- Mechanism: By applying dropout at inference time multiple times, the model generates a distribution of predictions for each sample. High variance in this distribution indicates the model is uncertain, suggesting these samples would be most informative for learning.
- Core assumption: Model uncertainty correlates with sample informativeness and label error would be most beneficial for improving model performance
- Evidence anchors:
  - [abstract] "Our approach utilizes contrastive learning together with uncertainty estimations via Monte Carlo Dropout (MC Dropout)"
  - [section] "MC Dropout not only simplifies the estimation of the posterior but also provides a computationally efficient approximation by utilizing dropout at both training and inference stages"
  - [corpus] Weak evidence - no neighbor papers directly cite MC Dropout for active learning in remote sensing
- Break condition: If uncertainty estimates become unreliable due to model collapse or if high-uncertainty samples consistently belong to the same easy-to-learn patterns

### Mechanism 3
- Claim: Combining contrastive pre-training with uncertainty-based active learning outperforms random sampling in class-imbalanced scenarios
- Mechanism: Pre-trained contrastive features provide a meaningful embedding space where uncertainty sampling can effectively identify informative samples. This combination is particularly effective when some classes are underrepresented, as uncertainty sampling naturally seeks out rare or ambiguous examples.
- Core assumption: The embedding space preserves class separability even in unbalanced data, allowing uncertainty to meaningfully distinguish between easy and hard examples across all classes
- Evidence anchors:
  - [abstract] "Our results show that for unbalanced classes, our method is superior to the random approach"
  - [section] "In the second scenario, we modify the training data in a way that the training data becomes unbalanced"
  - [section] "When the data is unbalanced, MCFPS reaches the 90% mark quicker than random"
  - [corpus] No direct evidence - neighbor papers don't compare active learning methods on class imbalance
- Break condition: If pre-training introduces bias toward majority classes or if uncertainty estimation fails to capture true information density in imbalanced distributions

## Foundational Learning

- Concept: Self-supervised learning and contrastive learning principles
  - Why needed here: Understanding how MoCo creates meaningful representations without labels is essential for grasping why pre-training improves active learning efficiency
  - Quick check question: How does MoCo prevent model collapse when mapping all samples to a single point?

- Concept: Bayesian uncertainty estimation and Monte Carlo methods
  - Why needed here: MC Dropout approximates posterior distributions by sampling different model weights through dropout, providing uncertainty estimates without intractable Bayesian computations
  - Quick check question: Why does applying dropout at inference time provide a practical approximation of Bayesian uncertainty?

- Concept: Active learning query strategies and diversity sampling
  - Why needed here: The method combines uncertainty sampling with diversity preservation (farthest-point-sampling) to ensure selected samples are both informative and representative
  - Quick check question: What problem does combining uncertainty sampling with diversity sampling solve that pure uncertainty sampling cannot?

## Architecture Onboarding

- Component map: Sentinel-2 patches (64x64x13) → MoCo encoder (ResNet50 backbone) → 2048-dimensional features → MC Dropout uncertainty model → farthest-point-sampling → candidate selection → human labeling → model fine-tuning → repeat

- Critical path: MoCo pre-training → feature encoding → uncertainty model initialization → iterative active learning loop (diversity sampling → uncertainty estimation → candidate selection → labeling → model update)

- Design tradeoffs:
  - Pre-training vs. from-scratch: Pre-training with MoCo provides significant accuracy gains with fewer labeled samples but requires additional computational resources and a separate training phase
  - Uncertainty vs. random sampling: Uncertainty sampling excels in imbalanced scenarios but may underperform in balanced settings where random sampling captures sufficient diversity
  - Neighborhood size (k) vs. computational cost: Larger k provides more robust uncertainty estimates but increases computation quadratically

- Failure signatures:
  - Pre-training collapse: If MoCo training fails, all downstream performance degrades; check contrastive loss convergence and negative sampling quality
  - Uncertainty miscalibration: If MC Dropout uncertainty doesn't correlate with actual model performance; validate by checking if high-uncertainty samples actually improve model accuracy when labeled
  - Diversity loss: If farthest-point-sampling repeatedly selects samples from the same region; monitor feature space coverage across iterations

- First 3 experiments:
  1. Baseline: Train a model from scratch on randomly sampled labeled data without any pre-training or active learning
  2. Pre-training only: Apply MoCo pre-training but use random sampling instead of uncertainty-based selection
  3. Full pipeline validation: Run the complete MCFPS method on a small subset of data to verify all components work together before scaling to full experiments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the MCFPS method perform on real-world remote sensing datasets with more complex classification tasks beyond the Eurosat dataset?
- Basis in paper: [explicit] The authors state "evaluating this effect with only one dataset is insufficient to draw comprehensive conclusions" and plan to incorporate a real-world dataset focusing on semantic segmentation
- Why unresolved: The current study only tested on the Eurosat dataset which was "relatively easy to classify after encoding"
- What evidence would resolve it: Results from experiments using more complex real-world remote sensing datasets, particularly for semantic segmentation tasks

### Open Question 2
- Question: How does MCFPS compare to other state-of-the-art active learning methods beyond random sampling?
- Basis in paper: [explicit] The authors note they will "update this paper with additional findings, including comparisons with other methods" and mention a hybrid approach from Jin et al. 2022 as related work
- Why unresolved: The current study only compares MCFPS against random selection
- What evidence would resolve it: Comparative results showing MCFPS performance against other established active learning methods

### Open Question 3
- Question: What is the optimal number of Monte Carlo Dropout iterations (t) for uncertainty estimation in the MCFPS method?
- Basis in paper: [inferred] The methodology mentions using "t forward passes" for uncertainty estimation but doesn't specify or analyze the impact of different values of t
- Why unresolved: The paper doesn't explore how varying the number of MC Dropout iterations affects performance or computational efficiency
- What evidence would resolve it: Systematic experiments varying the number of MC Dropout iterations and measuring the trade-off between uncertainty estimation quality and computational cost

### Open Question 4
- Question: How does the performance of MCFPS scale with different levels of class imbalance in the training data?
- Basis in paper: [explicit] The authors tested only two scenarios (balanced and one unbalanced setting) and noted that "MCFPS reaches the 90% mark quicker than random" in the unbalanced case
- Why unresolved: The study didn't explore a spectrum of imbalance ratios or analyze the relationship between imbalance severity and method effectiveness
- What evidence would resolve it: Experiments with varying degrees of class imbalance to establish performance trends and identify thresholds where MCFPS becomes advantageous

## Limitations

- Eurosat dataset may be too simple for real-world deployment testing, limiting generalizability
- Only compared against random sampling baseline rather than other state-of-the-art active learning methods
- Limited exploration of parameter sensitivity (e.g., MC Dropout iterations, MoCo hyperparameters)

## Confidence

- High confidence: The fundamental mechanism of combining contrastive learning with uncertainty estimation is sound and theoretically justified
- Medium confidence: The specific implementation details (MoCo architecture, MC Dropout configuration) would produce the reported results, pending hyperparameter verification
- Medium confidence: The comparative advantage over random sampling in unbalanced scenarios, based on experimental results though limited to synthetic imbalance

## Next Checks

1. Validate the approach on real Sentinel-2 imagery with actual class imbalance patterns and environmental noise
2. Test the sensitivity of results to MoCo training hyperparameters and MC Dropout configuration through ablation studies
3. Evaluate long-term performance across multiple active learning cycles to assess whether early gains compound or diminish over time