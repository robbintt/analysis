---
ver: rpa2
title: 'RLStop: A Reinforcement Learning Stopping Method for TAR'
arxiv_id: '2405.02525'
source_url: https://arxiv.org/abs/2405.02525
tags:
- recall
- documents
- rlstop
- stopping
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RLStop is a novel stopping rule for technology-assisted review
  (TAR) based on reinforcement learning. It is trained on example rankings to identify
  the optimal point to stop examining documents, reducing the manual review workload.
---

# RLStop: A Reinforcement Learning Stopping Method for TAR

## Quick Facts
- arXiv ID: 2405.02525
- Source URL: https://arxiv.org/abs/2405.02525
- Authors: Reem Bin-Hezam; Mark Stevenson
- Reference count: 40
- Key outcome: RLStop achieved recall levels close to maximum possible while substantially reducing manual review workload across multiple benchmark datasets

## Executive Summary
RLStop introduces a novel reinforcement learning approach for determining optimal stopping points in technology-assisted review (TAR). The method trains an agent to decide when to stop examining documents based on estimated recall achievement, using Proximal Policy Optimization (PPO) to learn from example document rankings. The approach demonstrated superior performance compared to seven alternative stopping methods across six benchmark datasets, achieving high recall with significantly fewer documents reviewed.

## Method Summary
RLStop employs reinforcement learning with PPO to learn a policy for stopping document examination in TAR workflows. Documents are examined in fixed-size batches, with the agent deciding to stop or continue after each batch based on the proportion of relevant documents found. The reward function incentivizes stopping at or near the target recall level, with positive rewards for stopping before or at the target and negative rewards for overshooting. The method was trained separately for target recall levels of 80%, 90%, and 100% using 100,000 timesteps per level, with performance evaluated against seven baseline methods across six benchmark datasets.

## Key Results
- RLStop consistently outperformed all baseline methods across multiple benchmark datasets
- Achieved recall levels very close to the maximum possible in several cases
- Substantially reduced the number of documents requiring manual review while maintaining target recall levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The agent learns when to stop by estimating if target recall has been reached using a reward function that penalizes stopping too early or too late.
- Mechanism: The agent evaluates states representing batches of examined documents, receiving a positive reward if it stops at or before the target recall and a negative reward if it overshoots. The policy is trained to maximize cumulative reward.
- Core assumption: The state representation (proportion of relevant documents in examined batches) is sufficient to estimate whether target recall has been reached.
- Evidence anchors:
  - [abstract] "RLStop is trained on example rankings using a reward function to identify the optimal point to stop examining documents."
  - [section 2.1] "The following function achieves these goals: R(Si) = {1 - i/T if i â‰¤ T; -(i - T)/(B - T) if i > T} where ... T is the batch at which the target recall is reached."
  - [corpus] Weak evidence - no directly comparable RL-based stopping methods found in corpus.
- Break condition: If the ranking distribution changes dramatically (e.g., all relevant documents clustered at the end), the learned policy may not generalize.

### Mechanism 2
- Claim: The use of Proximal Policy Optimization (PPO) allows efficient learning from multiple rankings simultaneously.
- Mechanism: PPO uses parallel actors to collect trajectories from different environments (rankings) and updates the policy based on these experiences, making it sample-efficient.
- Core assumption: Training on multiple topics improves generalization to unseen topics.
- Evidence anchors:
  - [section 2.1] "PPO is also more sample-efficient than some alternative methods, such as DQN [24], thereby reducing the amount of data required to learn effective policies."
  - [section 3.3] "The RL environment was created using the Gymnasium library [34] which allows multiple environments to be stacked, thereby allowing simultaneous training on multiple topics to ensure the agent is as general as possible."
  - [corpus] Weak evidence - while PPO is a known algorithm, no direct comparison to other RL algorithms for this specific task found in corpus.
- Break condition: If the training topics are not representative of the test topics, performance may degrade.

### Mechanism 3
- Claim: Dividing the ranking into fixed-size batches allows efficient parallel processing of relevance judgments.
- Mechanism: Documents are examined in batches of N_B documents, with relevance judgments obtained for the entire batch simultaneously. This speeds up the process compared to examining documents one-by-one.
- Core assumption: Batch size can be chosen to balance efficiency and precision of stopping decisions.
- Evidence anchors:
  - [section 2.1] "For efficiency, documents are examined in batches with relevance judgements obtained for the entire batch simultaneously."
  - [section 2.1] "The initial state for each ranking, S1, occurs when the first batch (but none of the subsequent batches) has been explored."
  - [corpus] No direct evidence found in corpus about batch processing for TAR stopping.
- Break condition: If batch size is too large, the agent may overshoot the target recall by a significant margin.

## Foundational Learning

- Concept: Reinforcement Learning (RL) and Proximal Policy Optimization (PPO)
  - Why needed here: RL is used to learn a policy for deciding when to stop examining documents, and PPO is chosen for its sample efficiency and stability.
  - Quick check question: What are the key components of an RL system (state, action, reward, policy) and how are they implemented in RLStop?

- Concept: Technology-Assisted Review (TAR) and stopping rules
  - Why needed here: Understanding the TAR problem and the goal of stopping rules (minimize documents examined while achieving target recall) is crucial for understanding the problem RLStop addresses.
  - Quick check question: What are the two main objectives of TAR stopping rules and why are they in opposition?

- Concept: State representation and feature engineering
  - Why needed here: The state representation (proportion of relevant documents in examined batches) is a key design choice that affects the agent's ability to learn an effective policy.
  - Quick check question: Why is the state represented as a vector of proportions of relevant documents in examined batches, and what are the potential limitations of this representation?

## Architecture Onboarding

- Component map:
  RL Environment -> PPO Algorithm -> Neural Network Policy -> Evaluation Metrics
  Datasets (CLEF, TREC Total Recall, RCV1) -> AutoTAR Rankings -> Training/Testing

- Critical path:
  1. Preprocess datasets: Split into training and test sets, rank documents using AutoTAR.
  2. Train RLStop: Train the PPO policy on the training set for each target recall level.
  3. Evaluate RLStop: Apply the trained policy to the test set and measure recall, cost, and excess.
  4. Compare with baselines: Compare RLStop's performance against alternative stopping methods.

- Design tradeoffs:
  - Batch size: Larger batches are more efficient but may lead to overshooting the target recall.
  - Number of batches: More batches allow finer-grained stopping decisions but increase computational cost.
  - Training data: More training data may improve generalization but requires more computational resources.

- Failure signatures:
  - Consistently overshooting or undershooting the target recall: May indicate a problem with the reward function or state representation.
  - High variance in performance across topics: May indicate that the training data is not representative of the test data.
  - Poor performance compared to baselines: May indicate a problem with the RL algorithm or hyperparameters.

- First 3 experiments:
  1. Verify the RL environment: Check that the state, action, and reward functions are implemented correctly and that the agent can learn a simple policy (e.g., always stop after a fixed number of batches).
  2. Test on a simple dataset: Apply RLStop to a small, synthetic dataset with a known optimal stopping point to verify that it can learn the correct policy.
  3. Compare with a random baseline: Compare RLStop's performance against a simple baseline that stops randomly to establish a performance floor.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RLStop vary with different ranking algorithms beyond AutoTAR?
- Basis in paper: [inferred] The paper uses AutoTAR rankings for evaluation but does not explore other ranking methods.
- Why unresolved: The paper does not test RLStop with different ranking algorithms, leaving its generalizability to other methods unknown.
- What evidence would resolve it: Experiments comparing RLStop's performance using rankings from different TAR algorithms (e.g., continuous active learning, rank-based methods) would clarify its adaptability to various ranking approaches.

### Open Question 2
- Question: What is the impact of increasing the number of batches on RLStop's ability to avoid overshooting the target recall?
- Basis in paper: [explicit] The paper mentions that overshooting occurs due to the fixed batch size, suggesting that increasing the number of batches could be a potential solution.
- Why unresolved: The paper does not empirically test the effect of varying the number of batches on RLStop's performance.
- What evidence would resolve it: Conducting experiments with different batch sizes and analyzing the resulting recall and cost metrics would determine the optimal batch configuration for minimizing overshooting.

### Open Question 3
- Question: How does RLStop perform in scenarios where the relevant documents are uniformly distributed throughout the ranking?
- Basis in paper: [inferred] The paper focuses on non-uniform distributions of relevant documents, as is typical in TAR workflows, but does not address uniform distributions.
- Why unresolved: The paper does not provide data on RLStop's effectiveness when relevant documents are evenly spread across the ranking.
- What evidence would resolve it: Evaluating RLStop on datasets with artificially created uniform distributions of relevant documents would reveal its performance in such scenarios and highlight any limitations.

## Limitations
- The method relies on AutoTAR rankings as input, constraining effectiveness by underlying ranking algorithm quality
- Batch-based examination may overshoot target recall by several documents, though this is minor in practice
- Requires training separate models for each target recall level, increasing computational overhead

## Confidence
- High confidence in the RL mechanism and implementation details, as the architecture is well-specified and uses established algorithms
- Medium confidence in the generalizability across datasets, as performance varies notably between collections despite overall strong results
- Low confidence in the scalability to very large collections, as batch sizes and number of batches are fixed parameters that may need tuning

## Next Checks
1. **Robustness Testing**: Evaluate RLStop's performance when trained on one dataset collection and tested on completely different collections to assess generalization capability beyond the reported within-collection validation.

2. **Hyperparameter Sensitivity**: Systematically vary batch size (N_B) and number of batches to identify optimal configurations and understand the tradeoffs between efficiency and precision in stopping decisions.

3. **Comparison with Oracle Methods**: Compare RLStop against an oracle stopping rule that has perfect knowledge of document relevance to quantify the performance gap and identify potential improvements to the reward function design.