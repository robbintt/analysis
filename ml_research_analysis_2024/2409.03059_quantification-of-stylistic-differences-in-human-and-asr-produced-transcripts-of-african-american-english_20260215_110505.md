---
ver: rpa2
title: Quantification of stylistic differences in human- and ASR-produced transcripts
  of African American English
arxiv_id: '2409.03059'
source_url: https://arxiv.org/abs/2409.03059
tags:
- transcript
- differences
- speech
- verbatim
- transcription
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses the impact of stylistic differences between
  human and ASR-produced transcripts on ASR evaluation metrics, particularly for underrepresented
  varieties like African American English. The authors analyzed six transcription
  versions of 10 hours of AAE speech: four human-produced (including CORAAL, Rev,
  Rev with AA tag, and Amberscript) and two ASR-generated (Rev ASR and Whisper).'
---

# Quantification of stylistic differences in human- and ASR-produced transcripts of African American English

## Quick Facts
- **arXiv ID:** 2409.03059
- **Source URL:** https://arxiv.org/abs/2409.03059
- **Reference count:** 0
- **Primary result:** Reference transcript choice significantly affects ASR evaluation metrics for African American English

## Executive Summary
This work addresses the impact of stylistic differences between human and ASR-produced transcripts on ASR evaluation metrics, particularly for underrepresented varieties like African American English. The authors analyzed six transcription versions of 10 hours of AAE speech: four human-produced (including CORAAL, Rev, Rev with AA tag, and Amberscript) and two ASR-generated (Rev ASR and Whisper). They categorized transcription differences into three hypotheses: morpho-syntactic features, reduction/contraction conventions, and verbatim vs. non-verbatim style. Using WER and automated difference detection, they found WERs between human transcripts ranging from 10% to 20%, with verbatim and morpho-syntactic differences being the largest categories. The Rev ASR model showed more similarity to its human counterparts than Whisper, suggesting training data influence. The study highlights that reference transcript choice significantly affects ASR evaluation and proposes clearer style guidelines for underrepresented varieties to reduce bias in ASR systems.

## Method Summary
The authors analyzed six transcription versions of 10 hours of AAE speech from the CORAAL corpus. They employed a three-pronged approach: (1) Word Error Rate (WER) comparison across all transcription pairs, (2) automated difference detection using ASR models to identify systematic differences, and (3) qualitative analysis of transcription style categories. The analysis focused on three hypothesized sources of variation: morpho-syntactic features (tense, negation, pronouns), reduction/contraction conventions, and verbatim vs. non-verbatim style differences. They used manual and automated methods to categorize differences and assess their prevalence across transcription types.

## Key Results
- WER between human transcripts ranged from 10% to 20%, with CORAAL vs. verbatim transcripts showing the highest divergence
- Verbatim and morpho-syntactic differences were the largest categories of systematic variation across all transcription pairs
- Rev ASR transcripts were more similar to their human counterparts than Whisper, suggesting training data influence on transcription style
- The choice of reference transcript significantly impacts ASR evaluation outcomes for African American English

## Why This Works (Mechanism)
The analysis demonstrates that transcription style differences systematically affect ASR evaluation metrics through multiple mechanisms. When reference transcripts use different conventions for representing AAE morpho-syntactic features, reduction patterns, or verbatim style, the resulting WER calculations become unreliable indicators of true ASR performance. The automated difference detection approach works by leveraging ASR models to identify systematic variations between transcription versions, revealing patterns that human inspection might miss. The correlation between Rev ASR's similarity to human transcripts and its training data suggests that ASR systems inherit and perpetuate transcription style biases present in their training corpora.

## Foundational Learning

**Word Error Rate (WER)** - Standard metric for ASR evaluation that counts substitutions, deletions, and insertions. Why needed: Serves as the primary quantitative measure of transcription differences. Quick check: WER = (S+D+I)/N where S=substitutions, D=deletions, I=insertions, N=reference length.

**Morpho-syntactic variation** - Systematic differences in how grammatical features are realized across dialects. Why needed: Represents a major source of transcription variation that affects evaluation validity. Quick check: Look for patterns in tense marking, negation, and pronoun usage across transcripts.

**Transcripts as training data** - ASR models learn patterns from human-produced reference transcripts. Why needed: Explains why different ASR systems produce stylistically different outputs. Quick check: Compare model outputs to their likely training corpora characteristics.

## Architecture Onboarding

**Component map:** Audio recordings -> Multiple transcription pipelines (human + ASR) -> Automated difference detection -> WER calculation -> Qualitative categorization -> Analysis

**Critical path:** CORAAL recordings → human transcription variants → ASR model outputs → pairwise WER computation → systematic difference identification → style category assignment → interpretation

**Design tradeoffs:** The study prioritizes comprehensive comparison across multiple transcription variants over deep analysis of individual linguistic features. This breadth-vs-depth tradeoff enables identification of broad patterns but limits granular linguistic analysis. The automated difference detection approach trades some precision for scalability across the six transcription variants.

**Failure signatures:** High WER between human transcripts may indicate either genuine linguistic variation or inconsistent transcription conventions. Systematic differences concentrated in specific style categories suggest transcription bias rather than true ASR performance differences. Similarity between an ASR model and specific human transcripts may reflect training data bias rather than superior performance.

**First experiments:** 1) Replicate WER calculations using different matching algorithms to test sensitivity to alignment choices. 2) Manually annotate a subset of differences to validate automated categorization accuracy. 3) Test whether observed patterns hold across different AAE sub-varieties.

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size (10 hours across 6 transcription variants) limits generalizability
- Focus on single dialect region restricts findings to specific AAE varieties
- Automated difference detection may misclassify nuanced linguistic variations
- Interpretation of training data influence is suggestive but not definitively proven

## Confidence
**High:** Core finding that reference transcript choice affects ASR evaluation metrics
**Medium:** Systematic patterns of morpho-syntactic and style differences across transcripts
**Low:** Definitive claims about specific training data influences on ASR model behavior

## Next Checks
1. Replicate the transcription comparison analysis using a larger, multi-dialect corpus of African American English to assess generalizability of the observed stylistic patterns.
2. Conduct human annotation of a subset of transcription differences to validate the automated categorization system and estimate its precision/recall across the three difference types.
3. Perform correlation analysis between observed transcription style differences and specific demographic variables (region, age, education) within the AAE speaker population to better understand systematic variation patterns.