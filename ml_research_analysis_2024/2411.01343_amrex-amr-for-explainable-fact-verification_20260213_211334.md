---
ver: rpa2
title: 'AMREx: AMR for Explainable Fact Verification'
arxiv_id: '2411.01343'
source_url: https://arxiv.org/abs/2411.01343
tags:
- claim
- evidence
- fact
- verification
- name
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AMREx is an AMR-based fact verification system that combines structural
  similarity via Smatch with textual similarity via SBERT embeddings to predict claim
  veracity. The system provides explainable justifications by mapping AMR nodes between
  claims and evidence.
---

# AMREx: AMR for Explainable Fact Verification

## Quick Facts
- arXiv ID: 2411.01343
- Source URL: https://arxiv.org/abs/2411.01343
- Authors: Chathuri Jayaweera; Sangpil Youm; Bonnie Dorr
- Reference count: 14
- Primary result: AMREx achieves 0.44 and 0.50 macro F1 scores on FEVER and AVeriTeC datasets respectively, outperforming AVeriTeC baseline in accuracy

## Executive Summary
AMREx is an AMR-based fact verification system that combines structural similarity via Smatch with textual similarity via SBERT embeddings to predict claim veracity. The system provides explainable justifications by mapping AMR nodes between claims and evidence. Tested on FEVER and AVeriTeC datasets, AMREx achieves 0.44 and 0.50 macro F1 scores respectively, outperforming the AVeriTeC baseline in accuracy. The explainable output supports error analysis and can guide LLMs to generate natural-language explanations, reducing hallucination risks. While performance is lower than black-box baselines, AMREx offers interpretability and diagnostic value.

## Method Summary
AMREx uses Abstract Meaning Representation (AMR) graphs to represent both claims and evidence, then combines structural similarity (measured by Smatch precision score) with textual similarity (measured by SBERT cosine similarity) using a weighted sum. The system applies threshold functions to classify entailment between claim and evidence pairs, predicting veracity labels (Supports, Refutes, NotEnoughInfo/NEI, or ConflictingEvidence). The AMR node mappings between claims and evidence provide explicit justifications for predictions, allowing users to see which semantic elements correspond between the two texts.

## Key Results
- Achieves 0.44 macro F1 score on FEVER dataset
- Achieves 0.50 macro F1 score on AVeriTeC dataset
- Outperforms AVeriTeC baseline in accuracy metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AMREx combines Smatch precision score with SBERT cosine similarity to measure both structural and textual similarity between claim and evidence AMRs.
- Mechanism: The system uses a weighted sum of the Smatch precision score (measuring structural similarity between AMR graphs) and cosine similarity of SBERT embeddings (measuring textual similarity) to calculate an entailment score. This score is then thresholded to classify the claim as supporting, refuting, or not enough information.
- Core assumption: Semantic entailment aligns with subsumption in AMR graph space, meaning if sentence A entails sentence B, then the meaning of B's AMR is contained within A's AMR.
- Evidence anchors:
  - [abstract]: "AMREx is an AMR-based fact verification system that combines structural similarity via Smatch with textual similarity via SBERT embeddings to predict claim veracity."
  - [section 3.2.1]: "We apply an existing AMR evaluation metric (Cai and Knight, 2013), to map Claims (e.g., X was produced Y) to relevant Evidence (e.g., X is a film produced by Y)."
- Break condition: When AMR nodes represent the same concept but with different semantic roles (e.g., "romantic" mapped to "direct" instead of a genre relationship), leading to false structural similarity despite factual differences.

### Mechanism 2
- Claim: AMR node mappings provide explainable justifications by showing which concepts in the claim align with concepts in the evidence.
- Mechanism: The system returns explicit node-to-node mappings between the claim's AMR and the evidence's AMR, allowing users to see exactly which semantic elements correspond between the two texts. These mappings clarify why the model made its veracity prediction.
- Core assumption: Visual mapping between AMRs of claims and evidence helps clarify why the model returns a particular prediction for a (Claim, Evidence) pair in terms of structural similarity.

## Foundational Learning

### AMR Parsing
- Why needed: Converts natural language sentences into graph representations capturing semantic meaning
- Quick check: Verify AMR parser correctly captures core semantic relationships in simple sentences

### Smatch Metric
- Why needed: Evaluates structural similarity between AMR graphs by calculating precision, recall, and F1 scores
- Quick check: Test Smatch scores on AMR pairs with known structural similarity

### SBERT Embeddings
- Why needed: Provides semantic textual similarity scores between AMR graph representations
- Quick check: Verify SBERT embeddings capture semantic similarity for paraphrased sentences

## Architecture Onboarding

### Component Map
- AMR Parser -> Smatch Calculator -> SBERT Embedder -> Weighted Sum -> Threshold Function -> Veracity Prediction

### Critical Path
AMR Parser → Smatch Calculator → Weighted Sum → Threshold Function → Veracity Prediction

### Design Tradeoffs
- Deterministic vs learned approach: Uses similarity metrics instead of neural networks for interpretability
- Structural vs textual emphasis: Balances AMR structure with word-level semantics through λ parameter

### Failure Signatures
- Low Smatch scores indicate poor structural alignment between claim and evidence AMRs
- High SBERT similarity but low Smatch scores suggest textual similarity without semantic alignment
- Threshold misconfiguration leads to incorrect class predictions

### First Experiments
1. Test AMR parser accuracy on sample sentences from FEVER dataset
2. Evaluate Smatch scores for known entailment relationships
3. Optimize λ parameter for weighted sum of similarity scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does AMREx's performance change when using different AMR parsers with varying accuracy levels?
- Basis in paper: [explicit] The paper states "Since AMREx relies heavily on AMRs, it is crucial to use a high-performing AMR parser when converting the sentences to AMRs. Therefore, the overall performance of the system depends on the accuracy of the AMR parser."
- Why unresolved: The paper does not experiment with different AMR parsers or analyze how parser performance impacts AMREx's fact verification accuracy.
- What evidence would resolve it: Empirical testing of AMREx with multiple AMR parsers of varying quality levels, comparing performance metrics across them.

### Open Question 2
- Question: Can the threshold values in AMREx's classification function be optimized automatically rather than determined experimentally?
- Basis in paper: [inferred] The paper mentions "The exact threshold values were determined experimentally" but doesn't explore automated optimization methods.
- Why unresolved: The paper uses manually determined threshold values without exploring whether machine learning approaches could find optimal thresholds.
- What evidence would resolve it: Experiments comparing manually set thresholds against automatically optimized thresholds using techniques like grid search or reinforcement learning.

### Open Question 3
- Question: How does AMREx perform on real-world data compared to benchmark datasets like FEVER and AVeriTeC?
- Basis in paper: [explicit] The paper states "The AMR node mappings provide a partial, post hoc explanation of the system, while the interpretability of the entire system fully encompasses the prediction process. An evaluation of the explainable aspect of AMREx model in comparison to current structural explainable fact verification systems is also necessary."
- Why unresolved: The paper only tests on curated benchmark datasets and doesn't evaluate on raw, real-world claims from social media or news sources.
- What evidence would resolve it: Testing AMREx on real-world fact verification datasets or live social media claims, comparing performance to benchmark results.

## Limitations
- Moderate performance metrics (0.44 and 0.50 macro F1) compared to black-box transformer baselines
- Reliance on deterministic similarity metrics may limit ability to capture complex semantic relationships
- Smatch-based structural similarity may struggle with paraphrases or different syntactic structures

## Confidence
- Confidence in core mechanisms: Medium
  - Combination of structural and textual similarity is theoretically sound
  - Explainability approach is well-founded but lacks quantitative validation
  - Performance metrics suggest limitations in real-world applicability

## Next Checks
1. Test AMREx on a held-out validation set with varying λ parameters to identify optimal weighting between structural and textual similarity components
2. Conduct human evaluation studies to assess whether the AMR node mappings genuinely improve user understanding of the model's predictions compared to black-box baselines
3. Evaluate the system's performance on out-of-domain fact verification tasks to assess generalizability beyond the FEVER and AVeriTeC datasets