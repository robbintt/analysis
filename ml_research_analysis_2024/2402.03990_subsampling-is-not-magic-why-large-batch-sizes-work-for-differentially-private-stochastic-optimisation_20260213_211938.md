---
ver: rpa2
title: 'Subsampling is not Magic: Why Large Batch Sizes Work for Differentially Private
  Stochastic Optimisation'
arxiv_id: '2402.03990'
source_url: https://arxiv.org/abs/2402.03990
tags:
- subsampling
- privacy
- mechanism
- gaussian
- batch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a theoretical explanation for why large batch
  sizes are beneficial in differentially private stochastic gradient descent (DP-SGD).
  The authors decompose the total gradient variance in DP-SGD into subsampling-induced
  and noise-induced components, and prove that in the limit of infinite iterations,
  the effective noise-induced variance becomes invariant to batch size.
---

# Subsampling is not Magic: Why Large Batch Sizes Work for Differentially Private Stochastic Optimisation

## Quick Facts
- arXiv ID: 2402.03990
- Source URL: https://arxiv.org/abs/2402.03990
- Authors: Ossi Räisä; Joonas Jälkö; Antti Honkela
- Reference count: 40
- Key outcome: This paper provides a theoretical explanation for why large batch sizes are beneficial in differentially private stochastic gradient descent (DP-SGD).

## Executive Summary
This paper addresses the counterintuitive observation that larger batch sizes can improve performance in differentially private stochastic gradient descent (DP-SGD), contrary to standard privacy amplification theory which suggests subsampling should always help. The authors decompose the total gradient variance in DP-SGD into subsampling-induced and noise-induced components, and prove that in the limit of infinite iterations, the effective noise-induced variance becomes invariant to batch size. This means larger batch sizes reduce the effective total gradient variance by decreasing the subsampling-induced variance. They also show that for a single iteration, large batch sizes similarly reduce the effective DP noise variance under a sufficient condition that holds across practical hyperparameter values.

## Method Summary
The authors use theoretical analysis of the Poisson subsampled Gaussian mechanism and its privacy accounting properties to study the effect of batch size on gradient variance in DP-SGD. They decompose the total gradient variance into subsampling-induced and noise-induced components, analyze the behavior of the privacy accounting oracle (AOS) as the number of iterations T approaches infinity, and derive a sufficient condition for when larger batch sizes reduce effective noise variance for a single iteration. The theoretical results are validated through numerical experiments that verify the convergence behavior and sufficient condition across a range of privacy parameters.

## Key Results
- In the limit of infinite iterations, the effective noise-induced variance in DP-SGD becomes invariant to batch size
- For a single iteration, large batch sizes reduce effective DP noise variance under a sufficient condition verified across practical hyperparameters
- Outside the asymptotic regime, the total gradient variance decreases even more with large batch sizes than predicted by asymptotic theory

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: In the limit of infinite iterations, the effective noise-induced variance in DP-SGD becomes invariant to batch size.
- **Mechanism**: The authors prove that as T → ∞, the privacy accounting behavior of the Poisson subsampled Gaussian mechanism converges to that of the non-subsampled Gaussian mechanism, making the noise standard deviation σ(q,T) proportional to q, thus keeping σ²/q² constant.
- **Core assumption**: The accounting oracle for the Poisson subsampled Gaussian mechanism converges to the accounting oracle for the Gaussian mechanism as T → ∞.
- **Evidence anchors**: [abstract] "We then prove that in the limit of an infinite number of iterations, the effective noise-induced variance is invariant to the batch size." [section] "Theorem 5.3: For any σ, q1, q2, ∆ and ϵ |AOS(σ, ∆, q1, T, ϵ) − AOS(σ · q2/q1, ∆, q2, T, ϵ)| → 0 as T → ∞."

### Mechanism 2
- **Claim**: For a single iteration of DP-SGD, large batch sizes reduce the effective DP noise variance under a sufficient condition.
- **Mechanism**: The authors derive condition (a < b) ensuring the derivative of σ(q)/q with respect to q is negative, meaning larger subsampling rates reduce effective noise variance, verified numerically across practical hyperparameters.
- **Core assumption**: The sufficient condition a < b holds for a wide range of practical privacy parameters.
- **Evidence anchors**: [abstract] "We also find a sufficient condition that implies that large batch sizes similarly reduce effective DP noise variance for one iteration of DP-SGD." [section] "Theorem 6.2: If a < b for a and b defined in Equations(46) and (47), then d/dq σ(q)/q < 0."

### Mechanism 3
- **Claim**: Outside the asymptotic regime, the total gradient variance decreases even more with large batch sizes.
- **Mechanism**: Empirical observations show that when not in the asymptotic regime, the effective injected DP noise variance decreases more rapidly with larger batch sizes than predicted by asymptotic theory.
- **Core assumption**: Empirical observations outside the asymptotic regime are representative of practical settings.
- **Evidence anchors**: [abstract] "We confirm numerically that the asymptotic regime is relevant in practical settings when the batch size is not small, and find that outside the asymptotic regime, the total gradient variance decreases even more with large batch sizes." [section] Figure 1 shows σ_eff approaching σ(1,T) as T grows.

## Foundational Learning

- **Concept**: Differential Privacy (DP) and its composition properties
  - Why needed here: The entire paper builds on understanding how DP mechanisms compose and how privacy accounting works for the Poisson subsampled Gaussian mechanism.
  - Quick check question: Can you explain the difference between the Gaussian mechanism and the Poisson subsampled Gaussian mechanism in terms of their privacy guarantees?

- **Concept**: Subsampling amplification and its effect on privacy
  - Why needed here: The paper directly addresses the counterintuitive observation that subsampling doesn't always help in DP-SGD, contrary to standard privacy amplification theory.
  - Quick check question: Why does subsampling typically amplify privacy, and how does this interact with the need to scale gradients by 1/q in DP-SGD?

- **Concept**: Privacy Loss Random Variables (PLRVs) and their use in accounting
  - Why needed here: The proofs rely on analyzing the behavior of PLRVs under composition, particularly showing convergence to Gaussian behavior.
  - Quick check question: How does the PLRV of the Poisson subsampled Gaussian mechanism differ from that of the standard Gaussian mechanism, and why is this difference important for the paper's results?

## Architecture Onboarding

- **Component map**: Gradient variance decomposition -> Privacy accounting oracle (AOS) -> Poisson subsampled Gaussian mechanism -> Numerical verification framework

- **Critical path**: 
  1. Implement Poisson subsampled Gaussian mechanism
  2. Compute privacy accounting using the oracle
  3. Decompose gradient variance as shown in Equation (25)
  4. Verify the sufficient condition (a < b) numerically
  5. Analyze convergence behavior as T increases

- **Design tradeoffs**:
  - Analytical rigor vs. practical applicability - the paper balances theoretical proofs with numerical verification
  - Computational complexity of privacy accounting - exact computation vs. approximations
  - Generality of results vs. specificity to Poisson sampling

- **Failure signatures**:
  - If the convergence to Gaussian mechanism doesn't hold, the asymptotic results break
  - If the sufficient condition fails for practical parameters, single-iteration results break
  - If numerical verification shows edge cases where the theory fails, the practical applicability is limited

- **First 3 experiments**:
  1. Reproduce Figure 1 to verify the convergence of σ_eff to σ(1,T) as T increases for different q values
  2. Implement the numerical verification of the sufficient condition (a < b) across a grid of (ϵ, q, δ) values
  3. Test the gradient variance decomposition empirically on a simple DP-SGD implementation with varying batch sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise behavior of the effective noise variance σeff in the intermediate regime between T = 1 and the asymptotic limit T → ∞?
- Basis in paper: The authors discuss both the T = 1 case and the asymptotic case, but note that "it would be important to theoretically study how to interpolate our results between the T = 1 and the asymptotic case."
- Why unresolved: The paper only provides theoretical results for the two extreme cases (T = 1 and T → ∞), and while they provide numerical evidence, a complete theoretical understanding of the intermediate regime is lacking.
- What evidence would resolve it: A mathematical proof showing how σeff behaves as a function of both q and T for finite but large T, or additional empirical results showing the transition behavior.

### Open Question 2
- Question: Does the sufficient condition a < b (Conjecture 6.3) hold for all practical values of ϵ, q, and δ?
- Basis in paper: The authors state "We were unable to provide a formal proof of when this condition is satisfied, but verified numerically that it appears valid for a broad range of practically relevant parameters with δ = 10−5."
- Why unresolved: While the authors provide numerical evidence that the condition holds for many parameter values, they were unable to prove it analytically for all cases.
- What evidence would resolve it: A rigorous mathematical proof of Conjecture 6.3, or a counterexample showing cases where a ≥ b for practical parameter values.

### Open Question 3
- Question: How does the choice of subsampling scheme (Poisson vs. without replacement vs. with replacement) affect the relationship between batch size and effective noise variance?
- Basis in paper: The authors mention that "Recent work has questioned the validity of these works for WOR subsampling" and note that "WR subsampling is rarely used in practice, and the accounting for it is much more complex."
- Why unresolved: The paper focuses specifically on Poisson subsampling and notes limitations in the analysis of other subsampling schemes, leaving the question of how different schemes compare unanswered.
- What evidence would resolve it: A theoretical analysis comparing the effective noise variance behavior across different subsampling schemes, or empirical results showing how the batch size effects differ between schemes.

## Limitations

- The paper's results depend on the convergence of the Poisson subsampled Gaussian mechanism's accounting oracle to the standard Gaussian mechanism, which may not be practical for small T values.
- The sufficient condition a < b for single-iteration batch size effects is numerically verified but not proven to hold universally across all practical parameter ranges.
- The analysis focuses specifically on Poisson subsampling, with limited discussion of how other subsampling schemes might affect the relationship between batch size and effective noise variance.

## Confidence

- **High confidence**: The gradient variance decomposition (Mechanism 1) and the numerical verification of the sufficient condition (Mechanism 2) are well-supported by both theoretical proofs and empirical validation.
- **Medium confidence**: The asymptotic convergence results (Mechanism 1) and the practical relevance of the asymptotic regime (Mechanism 3) are theoretically sound but depend on the number of iterations being sufficiently large.
- **Low confidence**: The claim that the asymptotic regime is "relevant in practical settings" (Mechanism 3) requires more extensive empirical validation across diverse datasets and models.

## Next Checks

1. Extend the numerical verification of the sufficient condition (a < b) to a wider range of privacy parameters, including extreme values of ε and δ, to identify potential edge cases where the condition might fail.
2. Conduct empirical studies on multiple real-world datasets and models to verify that the asymptotic regime is indeed reached in practical DP-SGD training scenarios, particularly for different values of T and q.
3. Analyze the impact of non-asymptotic effects on the effective noise variance for finite T values, and quantify the error introduced by assuming asymptotic behavior in practical implementations.