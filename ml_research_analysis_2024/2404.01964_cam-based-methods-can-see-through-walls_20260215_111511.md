---
ver: rpa2
title: CAM-Based Methods Can See through Walls
arxiv_id: '2404.01964'
source_url: https://arxiv.org/abs/2404.01964
tags:
- image
- methods
- gradcam
- part
- cam-based
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that CAM-based interpretability methods,
  including GradCAM, can produce misleading saliency maps by highlighting image regions
  that the model cannot access. The authors prove this theoretically by showing that
  GradCAM scores are positive at initialization for unseen image regions in a masked
  CNN model, and confirm this experimentally by training a VGG-like model that ignores
  the lower 24% of images.
---

# CAM-Based Methods Can See through Walls

## Quick Facts
- arXiv ID: 2404.01964
- Source URL: https://arxiv.org/abs/2404.01964
- Authors: Magamed Taimeskhanov; Ronan Sicre; Damien Garreau
- Reference count: 0
- CAM-based methods highlight regions models cannot access

## Executive Summary
This paper demonstrates that CAM-based interpretability methods, including GradCAM, can produce misleading saliency maps by highlighting image regions that the model cannot access. The authors prove this theoretically by showing that GradCAM scores are positive at initialization for unseen image regions in a masked CNN model, and confirm this experimentally by training a VGG-like model that ignores the lower 24% of images. Testing on two new datasets (STACK-MIX and STACK-GEN) reveals that all CAM methods except HiResCAM incorrectly highlight unseen regions, with GradCAM showing 22.7% activity in these areas on average. This work reveals a fundamental limitation in widely-used interpretability methods and highlights the need for more reliable evaluation frameworks.

## Method Summary
The authors create a masked VGG-like model (rVGGs) trained on ImageNet with the lower 24% of input weights zeroed out, effectively blocking information flow from the bottom of images. They then evaluate various CAM-based interpretability methods (GradCAM, GradCAM++, XGradCAM, ScoreCAM, Opti-CAM, AblationCAM, EigenCAM, and HiResCAM) on two novel datasets: STACK-MIX (100 images with same-class objects stacked) and STACK-GEN (100 DALL·E 3-generated stacked animal images). The primary metric measures activity in the unseen lower region, calculated as μ(Λ) = ||Λ₁₇₁:₂₂₄,:||₂ / ||Λ||₂, where lower values indicate less highlighting of unseen regions.

## Key Results
- GradCAM scores are positive at initialization for unseen image regions in masked CNN models
- All CAM methods except HiResCAM incorrectly highlight unseen regions in the STACK-MIX dataset
- HiResCAM creates zero saliency in a larger area (61% of image) than the dead zone (24% of image)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GradCAM highlights unseen image regions because its weighting coefficients are global and blind to whether the underlying network can actually use those regions.
- Mechanism: In GradCAM, the α coefficient is computed as a global average of gradients across the entire activation map, regardless of whether some parts of that map are zeroed out by network masking. Thus, if any filter shows activation in the masked (unseen) region, GradCAM assigns a positive score there even though the network's weights block information flow from that region.
- Core assumption: The global averaging in α treats all spatial positions equally, ignoring architectural constraints that may render some positions irrelevant.
- Evidence anchors:
  - [abstract] "CAM-based methods, including GradCAM, can produce misleading saliency maps by highlighting image regions that the model cannot access."
  - [section] "The coefficient associated to each individual map is global, in the sense that the same coefficient is applied to the whole map."
  - [corpus] Weak - no direct neighbor papers discuss this global-coefficient failure mode.
- Break condition: If α were computed locally per spatial location or conditioned on downstream connectivity, the failure would disappear.

### Mechanism 2
- Claim: The failure persists after training because the masking constraint is enforced only at the input of the first dense layer, but gradients flow through all preceding layers, allowing spurious activations to survive.
- Mechanism: When the lower weights in the first dense layer are zeroed out, this removes the network's ability to use information from the lower image region, but GradCAM's gradient computation still propagates through the convolution and ReLU layers. As a result, activation maps still contain non-zero values in the masked region, and the global α weights them positively.
- Core assumption: The masking is a hard architectural constraint applied only to the first dense layer, not upstream to convolutional activations.
- Evidence anchors:
  - [abstract] "Experimentally, we train a VGG-like model constrained to not use the lower part of the image and nevertheless observe positive scores in the unseen part of the image."
  - [section] "We permanently set to 0 a band of height 9 corresponding to the lower weights... Effectively, we are building a wall that stops all information flowing from the last convolutional layer to the remainder of the network."
  - [corpus] Weak - no neighbor papers mention gradient flow through masked architectures.
- Break condition: If the masking were enforced earlier (e.g., at the convolutional layer), activations in the unseen region would be zeroed and could not contribute to positive scores.

### Mechanism 3
- Claim: The problem is not specific to GradCAM; other CAM-based methods fail similarly because they also use global coefficients.
- Mechanism: GradCAM++, XGradCAM, ScoreCAM, AblationCAM, and EigenCAM all compute a single scalar weight per filter and apply it uniformly across the spatial map. HiResCAM avoids the issue because it uses per-pixel gradient weights, which become zero where the network is masked.
- Core assumption: All CAM variants except HiResCAM share the same global-coefficient design pattern.
- Evidence anchors:
  - [abstract] "All CAM methods except HiResCAM incorrectly highlight unseen regions, with GradCAM showing 22.7% activity in the unseen part of the image."
  - [section] "A close inspection of these definitions reveals that they also use global weighting coefficients applied to the corresponding activation maps, with the notable exception of HiResCAM."
  - [corpus] Weak - neighbor papers focus on saliency map aggregation, not this specific masking failure.
- Break condition: Switching to a method with spatially-varying coefficients (like HiResCAM) would eliminate the spurious highlighting.

## Foundational Learning

- Concept: Gradient-based saliency maps
  - Why needed here: Understanding how GradCAM computes α coefficients from gradients is essential to see why global averaging causes the failure.
  - Quick check question: In GradCAM, what operation is applied to the gradient of the class score w.r.t. each activation map before computing α?

- Concept: Convolutional receptive fields
  - Why needed here: The masking operation in the network blocks information from certain image regions; knowing how receptive fields map image pixels to activation maps explains why some activations survive in unseen regions.
  - Quick check question: In a convolution with filter size k×k, which image patch contributes to a single activation map entry?

- Concept: Expected value under Gaussian initialization
  - Why needed here: The theoretical proof relies on computing expectations of rectified Gaussian random variables to show that GradCAM scores are positive at initialization.
  - Quick check question: For a zero-mean Gaussian variable X, what is E[σ(X)] (the expectation of its ReLU)?

## Architecture Onboarding

- Component map: Input image → Conv blocks (VGG-like) → ReLU activations → Max pooling → Dense layers (masked) → Output logits
- Critical path:
  1. Forward pass through masked VGG to get feature maps B.
  2. Backward pass to compute ∂y/∂B.
  3. Global average pooling of gradients to get α.
  4. Weighted sum αᵀB → ReLU → saliency map.
- Design tradeoffs:
  - Masking only at the dense layer preserves VGG architecture but allows spurious activations to survive in GradCAM; masking earlier would prevent this but might hurt accuracy.
  - Using global α is computationally cheap but blind to spatial connectivity; local α would be more accurate but costlier.
- Failure signatures:
  - Positive saliency values in the masked (red) lower region of images.
  - Consistent across multiple CAM variants except HiResCAM.
  - Theoretical prediction: positive scores at initialization even before training.
- First 3 experiments:
  1. Run GradCAM on a simple masked CNN at initialization (no training) and check if lower region gets positive scores.
  2. Train the masked VGG, generate saliency maps on STACK-MIX, measure μ(lower region) vs. upper region.
  3. Replace GradCAM with HiResCAM on the same model and confirm zero saliency in the masked region.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can HiResCAM be modified to avoid highlighting larger areas than the dead zone while maintaining its advantage of not highlighting unseen regions?
- Basis in paper: [explicit] The paper notes that HiResCAM avoids highlighting unseen regions but creates zero saliency in a larger area (61% of image) than the dead zone (24% of image).
- Why unresolved: The paper identifies this as an issue but does not propose solutions or modifications to address it.
- What evidence would resolve it: A modified HiResCAM implementation that maintains its advantage of not highlighting unseen regions while restricting saliency to only the dead zone area, with quantitative comparison to current performance.

### Open Question 2
- Question: Would the theoretical findings about GradCAM's behavior at initialization hold for more complex architectures like ResNet?
- Basis in paper: [explicit] The paper's theoretical analysis is limited to a simple CNN with a single linear layer (L=1), and mentions extending to ResNet-like architectures as future work.
- Why unresolved: The paper only proves results for a simplified model and acknowledges this limitation without extending the analysis to deeper architectures.
- What evidence would resolve it: A theoretical proof showing that GradCAM scores remain positive in unseen regions at initialization for ResNet-like architectures with multiple residual blocks.

### Open Question 3
- Question: How do other CAM-based methods like LayerCAM perform on the dead zone highlighting problem compared to GradCAM and its variants?
- Basis in paper: [explicit] The paper mentions LayerCAM [36] as a potential future direction for extending the analysis but does not test it.
- Why unresolved: The paper only tests GradCAM and its direct variants on the proposed datasets, leaving other CAM methods unexplored.
- What evidence would resolve it: Experimental results showing LayerCAM's performance on the STACK-MIX and STACK-GEN datasets, specifically measuring µ(·) in the unseen region and comparing to GradCAM variants.

## Limitations

- The masking approach (blocking lower 24% at dense layer) may not represent all real-world scenarios where information loss occurs
- The datasets, while novel, are synthetic constructs that may not fully capture natural image distributions
- The analysis focuses primarily on GradCAM variants without exploring alternative interpretability methods

## Confidence

- High confidence in the theoretical proof showing positive GradCAM scores at initialization for unseen regions
- High confidence in the experimental results demonstrating consistent behavior across multiple CAM variants (except HiResCAM)
- Medium confidence in the generalizability to other masked architectures beyond the specific VGG variant tested

## Next Checks

1. Test the same masking approach on a ResNet architecture to verify if the failure mode persists across different CNN backbones
2. Apply the masking at the convolutional layer level (rather than just dense layer) to confirm if this eliminates the spurious highlighting
3. Validate the findings on a naturally-occurring dataset where certain image regions are genuinely irrelevant to the classification task (e.g., datasets with prominent watermarks or consistent borders)