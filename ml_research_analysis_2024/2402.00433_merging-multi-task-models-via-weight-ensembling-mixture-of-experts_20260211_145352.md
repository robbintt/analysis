---
ver: rpa2
title: Merging Multi-Task Models via Weight-Ensembling Mixture of Experts
arxiv_id: '2402.00433'
source_url: https://arxiv.org/abs/2402.00433
tags:
- task
- tasks
- merging
- layer
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of merging multiple task-specific
  Transformer-based models into a single unified model that can handle all tasks concurrently.
  The key issue is mitigating interference between parameters of different models,
  which can substantially deteriorate performance.
---

# Merging Multi-Task Models via Weight-Ensembling Mixture of Experts

## Quick Facts
- arXiv ID: 2402.00433
- Source URL: https://arxiv.org/abs/2402.00433
- Reference count: 40
- Outperforms state-of-the-art methods on multi-task model merging experiments

## Executive Summary
This paper introduces a method to merge multiple task-specific Transformer models into a single unified model capable of handling all tasks concurrently. The approach addresses the challenge of parameter interference between different models by implementing a weight-ensembling Mixture of Experts (MoE) module in the MLP layers of Transformer architectures. This dynamic integration mechanism combines shared and task-specific knowledge based on input requirements, achieving superior performance while maintaining parameter efficiency.

## Method Summary
The core method involves transforming the standard MLP layers in Transformer models into weight-ensembling MoE modules. These modules dynamically select and combine task-specific and shared parameters based on input characteristics, allowing the unified model to adapt its behavior for different tasks without interference. The approach separates knowledge into shared components (common across tasks) and task-specific components (unique to each task), then uses the MoE architecture to gate and integrate these components at inference time. This design enables the model to maintain performance across diverse tasks while only requiring 1.25% additional parameters compared to a single pre-trained model.

## Key Results
- Achieves 89.4% average accuracy across eight image classification tasks using CLIP-ViT-B/32
- Outperforms traditional multi-task learning baseline by 0.5% absolute improvement
- Converges rapidly to high accuracy within 200 training steps
- Demonstrates robustness to out-of-distribution data and stable performance across different scaling coefficients

## Why This Works (Mechanism)
The method works by addressing the fundamental challenge of parameter interference in multi-task learning. When merging multiple task-specific models, parameters from different tasks can conflict and degrade performance. The weight-ensembling MoE approach mitigates this by creating a gating mechanism that routes inputs to appropriate task-specific and shared parameters. The MoE module acts as a dynamic selector that activates relevant expert networks based on the input, allowing the model to leverage task-specific knowledge when needed while maintaining shared representations for common features. This selective activation prevents parameter conflicts while preserving the specialized capabilities of each individual model.

## Foundational Learning
- **Mixture of Experts (MoE)**: A neural network architecture that combines multiple specialized "expert" networks with a gating network that selects which experts to use for each input. Needed to enable dynamic parameter selection and prevent interference between task-specific models. Quick check: Verify that the gating network properly balances load across experts and doesn't collapse to using only a few experts.
- **Transformer MLP Layers**: The feed-forward network components in Transformer architectures that typically consist of linear layers with activation functions. Needed as the target for MoE integration since these layers contain the majority of parameters and learning capacity. Quick check: Confirm that the MoE modification preserves the dimensional compatibility with surrounding attention layers.
- **Parameter Interference**: The degradation in model performance that occurs when parameters from different tasks conflict or compete during joint training or inference. Needed to understand the core problem being solved. Quick check: Measure performance drop when naively combining task-specific models without MoE gating.
- **Task-Specific vs Shared Knowledge**: The distinction between parameters that are specialized for individual tasks versus those that capture common features across tasks. Needed to justify the separation approach in the MoE design. Quick check: Analyze the activation patterns of task-specific versus shared components across different input types.
- **Dynamic Parameter Routing**: The mechanism by which inputs are directed to appropriate parameter subsets based on their characteristics. Needed to understand how the model adapts to different tasks at inference time. Quick check: Visualize routing decisions for different task inputs to verify appropriate expert selection.
- **Parameter Efficiency**: The ratio of model performance to the number of parameters used. Needed to evaluate the practical utility of the approach in real-world applications with resource constraints. Quick check: Compare parameter counts and performance against baseline multi-task learning approaches.

## Architecture Onboarding

**Component Map:** Input -> Transformer Encoder (with MoE-augmented MLPs) -> Task-specific Gating Network -> Output

**Critical Path:** The critical execution path flows from input through the Transformer layers where MoE modules dynamically select and combine task-specific and shared parameters, then through the task-specific gating network that determines which output head to use for the given task.

**Design Tradeoffs:** The approach trades increased model complexity (MoE layers with additional gating parameters) for improved task performance and parameter efficiency. The 1.25% parameter increase is justified by the significant performance gains and elimination of interference. Alternative designs could use simpler gating mechanisms but would likely sacrifice the fine-grained control over parameter selection that enables high performance.

**Failure Signatures:** The model may fail when task boundaries are unclear or when inputs could reasonably belong to multiple tasks, as the gating network might struggle to make definitive routing decisions. Performance degradation could also occur if the MoE routing becomes imbalanced, with certain experts being underutilized while others become bottlenecks.

**First Experiments:**
1. Test the MoE gating mechanism on a simple two-task scenario to verify that parameters are properly routed and interference is minimized
2. Measure the load balance across MoE experts using different input distributions to ensure the gating network is functioning correctly
3. Compare performance against a baseline that simply concatenates task-specific models without MoE integration to quantify the benefit of the gating approach

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental scope limited primarily to image classification tasks with CLIP-ViT-B/32 architecture
- Robustness claims based only on CelebA-HQ dataset, representing limited validation of out-of-distribution performance
- Performance may be sensitive to task distribution imbalances, with some tasks having 2.6M samples versus others with only 5K
- Lacks comprehensive ablation studies on MoE architecture design choices and comparison with alternative merging strategies

## Confidence
- Experimental validation scope: Low - limited task diversity and model architectures tested
- Performance claims: Medium - strong results on image classification but unclear generalizability
- Parameter efficiency claims: Medium - 1.25% figure needs verification across different task distributions
- Rapid convergence claims: Medium - impressive but lacks computational efficiency comparison

## Next Checks
1. Test the method across diverse task types (NLP, multimodal, structured prediction) to verify cross-domain effectiveness beyond image classification
2. Conduct comprehensive ablation studies on the MoE design parameters (number of experts, routing strategies, task vector scaling) to understand their impact on performance
3. Evaluate performance under varying task distributions and imbalances to assess robustness to real-world deployment scenarios where task frequencies are unpredictable