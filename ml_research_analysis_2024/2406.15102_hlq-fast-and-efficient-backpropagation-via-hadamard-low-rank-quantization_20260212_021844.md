---
ver: rpa2
title: 'HLQ: Fast and Efficient Backpropagation via Hadamard Low-rank Quantization'
arxiv_id: '2406.15102'
source_url: https://arxiv.org/abs/2406.15102
tags:
- training
- quantization
- hadamard
- low-rank
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficient backpropagation
  for deep neural networks by introducing Hadamard Low-rank Quantization (HLQ). HLQ
  selectively applies Hadamard quantization and low-rank approximation to activation
  and weight gradients, respectively, based on their sensitivity analysis.
---

# HLQ: Fast and Efficient Backpropagation via Hadamard Low-rank Quantization

## Quick Facts
- arXiv ID: 2406.15102
- Source URL: https://arxiv.org/abs/2406.15102
- Authors: Seonggon Kim; Eunhyeok Park
- Reference count: 32
- Key outcome: Achieves up to 2.5x speedup and 78.5% memory reduction compared to full-precision training with minimal accuracy degradation

## Executive Summary
This paper introduces Hadamard Low-rank Quantization (HLQ), a novel method to accelerate deep neural network backpropagation while maintaining training quality. HLQ selectively applies 4-bit Hadamard quantization to activation gradients and Hadamard low-rank approximation to weight gradients based on their sensitivity analysis. By leveraging Hadamard transform and low-precision arithmetic, HLQ significantly reduces computational cost and memory footprint during training. Extensive experiments demonstrate substantial performance gains across various models and datasets.

## Method Summary
HLQ is a backpropagation optimization technique that selectively applies different quantization and low-rank strategies to activation and weight gradients. The method uses 4-bit Hadamard quantization for activation gradients, which are robust to low-precision errors due to batch dimension averaging, and Hadamard low-rank approximation for weight gradients, which can tolerate rank reduction. The forward pass remains in full precision (fp32 or fp16) to preserve accurate loss evaluation, while the backward pass is optimized through custom CUDA kernels implementing Hadamard transforms, quantization, and low-rank projections. Activation compression stores intermediate activations in 4-bit format to further reduce memory usage.

## Key Results
- Achieves up to 2.5x speedup compared to full-precision training
- Reduces memory footprint by up to 78.5% through selective quantization and low-rank approximation
- Maintains minimal accuracy degradation across various models and datasets
- Demonstrates effectiveness for both training from scratch and fine-tuning scenarios

## Why This Works (Mechanism)

### Mechanism 1
HLQ selectively applies HQ and HLA to different gradient paths to optimize backpropagation while minimizing quality degradation. Activation gradients (gx) use 4-bit HQ with Hadamard transform for speed and memory reduction because they are robust to quantization. Weight gradients (gw) use Hadamard low-rank approximation for further memory and computation savings since they are sensitive to quantization but tolerate HLA. The core assumption is that gradient path sensitivities differ significantly, allowing selective optimization without harming training quality.

### Mechanism 2
Hadamard transform reduces quantization error by mapping values to frequency domain before quantization. Applying Hadamard transform smooths the gradient distribution, reducing outliers and allowing lower-bit quantization with less error. This enables 4-bit quantization to match or exceed the accuracy of higher-bit naive quantization. The core assumption is that Hadamard transform effectively decorrelates and smooths gradient distributions, making them more amenable to low-precision representation.

### Mechanism 3
Batch dimension information averaging in activation gradients allows low-precision quantization without significant quality loss. Since activation gradients are averaged across the batch dimension during weight updates, quantization errors can be amortized across samples, making 4-bit quantization viable for gx without harming convergence. The core assumption is that batch-wise averaging of activation gradients sufficiently distributes quantization error to prevent accumulation that would destabilize training.

## Foundational Learning

- Concept: Backpropagation and chain rule for gradient computation in neural networks
  - Why needed here: Understanding how gradients flow through layers is essential to grasp why HLQ targets specific gradient paths (activation vs weight) differently.
  - Quick check question: In a linear layer, if the forward pass is y = x·w^T, what are the formulas for the gradients gx and gw in terms of gy, x, and w?

- Concept: Low-rank approximation and Hadamard transform
  - Why needed here: HLQ relies on reducing tensor rank via Hadamard low-rank approximation and transforming data using Hadamard matrices for both quantization and low-rank benefits.
  - Quick check question: What is the computational complexity of a full matrix multiplication versus one using Hadamard low-rank approximation with rank r?

- Concept: Quantization and stochastic rounding
  - Why needed here: HLQ uses 4-bit quantization with Hadamard transform; understanding how quantization introduces error and how stochastic rounding mitigates bias is key to why this works.
  - Quick check question: Why is stochastic rounding preferred over deterministic rounding in low-precision training?

## Architecture Onboarding

- Component map:
  Forward pass (fp32/fp16) -> Loss computation -> Backward pass split into gx and gw paths
  gx path: Hadamard transform → 4-bit quantization → TensorCore GEMM → dequantization
  gw path: Low-rank projection → 8-bit quantization → TensorCore GEMM → dequantization
  Activation compression: Store intermediate activations in 4-bit during forward pass

- Critical path:
  During backward pass, the gx and gw computations are the main bottlenecks; optimizing these with low-precision arithmetic and reduced rank is the core efficiency gain. Memory footprint is dominated by storing activations; ACBP (activation compression) is critical for memory reduction.

- Design tradeoffs:
  Using fp32 forward pass ensures stable convergence but forgoes some potential speed/memory gains. Selective application of HQ vs HLA requires careful sensitivity analysis; wrong choices degrade accuracy. Low-rank projection reduces computation but can lose spatial/batch information critical for activation gradients. 4-bit quantization for gx is fast but risks error accumulation if batch averaging is insufficient.

- Failure signatures:
  Significant accuracy drop in trained models (especially if HLA is applied to gx or HQ to gw). Training instability or divergence (likely from gw quantization errors). Minimal speedup or memory savings (kernels not optimized or rank too high). High memory usage (ACBP not implemented or rank selection too conservative).

- First 3 experiments:
  1. Profile baseline fp32 training to establish accuracy, memory, and speed as reference
  2. Apply HLQ to a small CNN (e.g., ResNet-18) on CIFAR-10; measure accuracy, memory, and speed
  3. Ablation: Swap HQ and HLA between gx and gw paths to confirm sensitivity findings and validate selective optimization

## Open Questions the Paper Calls Out

### Open Question 1
How would HLQ perform on large language models (LLMs) compared to computer vision models? The authors state that while HLQ was effectively demonstrated on computer vision models, they did not experimentally validate its effectiveness on LLMs and intend to investigate the extension of HLQ to LLMs.

### Open Question 2
Can the Hadamard transform-based quantization be extended to other numerical formats beyond 4-bit and int8? The paper focuses on 4-bit Hadamard quantization for activation gradients and int8 for weight gradients, but does not explore other numerical formats.

### Open Question 3
How does the sensitivity of activation and weight gradients to quantization and low-rank approximation vary across different model architectures and datasets? The paper's analysis is based on specific model architectures and datasets, and it is unclear how these sensitivities generalize to other models and tasks.

## Limitations

- The paper does not specify exact thresholds for applying HQ vs HLA, making it difficult to assess generalizability
- Computational complexity savings depend on CUDA kernel efficiency, which is not fully detailed
- Method's robustness across diverse architectures and batch sizes requires further validation

## Confidence

- **High confidence**: The mechanism of selective application of HQ to activation gradients and HLA to weight gradients based on their sensitivity is well-supported by both theoretical reasoning and experimental evidence
- **Medium confidence**: The claimed speedups and memory reductions are credible based on the reported experiments, but exact performance gains may vary depending on hardware and implementation details
- **Low confidence**: The paper does not provide sufficient detail on the sensitivity analysis methodology or exact thresholds used to determine which gradients to quantize vs approximate

## Next Checks

1. **Sensitivity analysis replication**: Systematically test the sensitivity of activation and weight gradients to HQ and HLA across multiple network architectures and batch sizes to validate the selective optimization strategy

2. **Kernel implementation verification**: Profile the custom CUDA kernels for Hadamard transform, quantization, and low-rank projection to ensure the claimed computational complexity reductions are realized in practice

3. **Generalization testing**: Apply HLQ to a diverse set of tasks and model architectures beyond those presented in the paper to assess its robustness and identify potential failure modes or limitations