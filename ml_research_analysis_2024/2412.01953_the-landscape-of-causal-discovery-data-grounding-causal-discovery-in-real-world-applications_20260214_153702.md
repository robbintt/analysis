---
ver: rpa2
title: 'The Landscape of Causal Discovery Data: Grounding Causal Discovery in Real-World
  Applications'
arxiv_id: '2412.01953'
source_url: https://arxiv.org/abs/2412.01953
tags:
- causal
- datasets
- discovery
- data
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper performs a systematic review of causal discovery research,
  focusing on datasets and evaluation practices used between 2019-2024. The main finding
  is that despite progress in causal discovery methods, the field still relies heavily
  on synthetic datasets and simple evaluation metrics.
---

# The Landscape of Causal Discovery Data: Grounding Causal Discovery in Real-World Applications

## Quick Facts
- arXiv ID: 2412.01953
- Source URL: https://arxiv.org/abs/2412.01953
- Reference count: 40
- The paper finds that causal discovery research over-relies on synthetic datasets and structural evaluation metrics, with only 64% of studies using real-world data and most of these being small-scale.

## Executive Summary
This systematic review analyzes causal discovery research from 2019-2024, revealing that despite methodological advances, the field remains heavily dependent on synthetic datasets and inadequate evaluation metrics. The review finds that most real-world datasets are small (80% have 20 or fewer variables) and that only 11.4% of studies use interventional metrics for evaluation. Three scientific domains—biology, neuroscience, and Earth sciences—are identified as particularly promising for real-world causal discovery applications. The authors recommend shifting toward more realistic datasets, including pseudo-real datasets generated from real-world data, and emphasize the importance of interventional metrics for meaningful evaluation.

## Method Summary
The paper conducts a systematic review of causal discovery literature, analyzing 96 papers from major machine learning conferences (NeurIPS, ICML, ICLR, UAI, AISTATS) published between 2019-2024. The review categorizes papers based on dataset types used (synthetic, pseudo-real, real-world), evaluation metrics employed (structural, observational, interventional), and scientific domains. The authors also catalog available real-world datasets and analyze their characteristics, including variable count, presence of interventions, and structural complexity. The review identifies trends in dataset usage and evaluation practices, highlighting gaps between current practices and recommendations for more realistic evaluation.

## Key Results
- Only 64% of studies use real-world datasets, and 80% of these have 20 or fewer variables
- Just 11.4% of studies use interventional metrics for real-world dataset evaluation
- Biology, neuroscience, and Earth sciences offer the most promising real-world datasets for causal discovery
- Current evaluation practices overestimate method performance due to unrealistic synthetic dataset assumptions
- Interventional metrics better capture the practical utility of causal discovery methods than structural metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic datasets lead to overestimation of method performance because they perfectly satisfy causal discovery assumptions
- Mechanism: Researchers generate synthetic data where the data-generating process matches their algorithm's assumptions, creating artificially favorable conditions
- Core assumption: Synthetic datasets are generated to perfectly match the assumptions of causal discovery methods being evaluated
- Evidence anchors: [abstract] "Current methods often rely on unrealistic assumptions and are evaluated only on simple synthetic toy datasets"; [section] "Synthetic datasets are usually generated by following these steps: first, the causal graph is sampled, then the causal mechanisms parameters, and finally, the data is sampled using ancestral sampling"
- Break condition: If synthetic datasets are generated with intentional violations of common assumptions

### Mechanism 2
- Claim: Real-world datasets provide natural stress tests because they inherently violate common assumptions
- Mechanism: Biology, neuroscience, and Earth sciences datasets contain structural complexities like feedback loops, unobserved confounders, and measurement errors
- Core assumption: Real-world datasets from these domains contain structural and measurement complexities that violate standard causal discovery assumptions
- Evidence anchors: [abstract] "The review highlights three key scientific domains—biology, neuroscience, and Earth sciences—that offer promising real-world datasets"; [section] "In biology, biomolecular network datasets contain even more interventions than before thanks to new technological advances"
- Break condition: If these domains' datasets were preprocessed to remove assumption-violating features

### Mechanism 3
- Claim: Interventional metrics provide more meaningful evaluation than structural metrics
- Mechanism: Instead of comparing learned graphs to ground-truth graphs, interventional metrics evaluate how well learned models predict data from unseen interventions
- Core assumption: The primary goal of causal discovery is to predict the effects of interventions, not just to recover graph structure
- Evidence anchors: [abstract] "The authors recommend using more realistic datasets... and emphasize the importance of interventional metrics"; [section] "Observational and interventional metrics correspond to evaluating how well the learned model predicts held-out observational data and data from an unseen intervention"
- Break condition: If the primary goal shifts from intervention prediction to other objectives

## Foundational Learning

- Concept: Causal Bayesian Networks (CBNs) and their associated assumptions (acyclicity,