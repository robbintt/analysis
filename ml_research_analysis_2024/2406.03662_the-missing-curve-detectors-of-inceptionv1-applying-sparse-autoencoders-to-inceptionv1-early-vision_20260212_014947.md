---
ver: rpa2
title: 'The Missing Curve Detectors of InceptionV1: Applying Sparse Autoencoders to
  InceptionV1 Early Vision'
arxiv_id: '2406.03662'
source_url: https://arxiv.org/abs/2406.03662
tags:
- features
- curve
- neurons
- inceptionv1
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies sparse autoencoders (SAEs) to the early vision
  layers of InceptionV1 to address the problem of polysemantic neurons caused by superposition.
  SAEs decompose neural activations into interpretable features, potentially revealing
  hidden patterns.
---

# The Missing Curve Detectors of InceptionV1: Applying Sparse Autoencoders to InceptionV1 Early Vision

## Quick Facts
- arXiv ID: 2406.03662
- Source URL: https://arxiv.org/abs/2406.03662
- Reference count: 5
- One-line primary result: Sparse autoencoders uncover new interpretable features in InceptionV1 early vision layers, including additional curve detectors and decomposition of polysemantic neurons.

## Executive Summary
This paper applies sparse autoencoders (SAEs) to early vision layers of InceptionV1 to address the problem of polysemantic neurons caused by superposition. The SAEs decompose neural activations into interpretable features, revealing hidden patterns not apparent from examining individual neurons. Key findings include the discovery of additional curve detectors that fill gaps in previous studies and the decomposition of polysemantic neurons into more monosemantic constituent features. The work demonstrates that SAEs can be a powerful tool for understanding the internal representations of convolutional neural networks.

## Method Summary
The paper applies sparse autoencoders to sampled activations from InceptionV1's convolutional layers (conv2d0, conv2d1, conv2d2, mixed3a, mixed3b) using ImageNet training data. The SAEs were trained with modified techniques including oversampling large activations and branch-specific training for mixed3a/mixed3b to save compute. The analysis relied on dataset examples and feature visualizations to identify interpretable features and study polysemantic neuron decomposition. Training involved approximately 500 epochs on ~5 billion total activations with various hyperparameters tuned for each layer.

## Key Results
- SAEs discovered additional curve detector features that fill "gaps" where specific orientations were previously missing
- Polysemantic neurons were successfully decomposed into more monosemantic constituent features, with relative feature strengths adjustable via SAE hyperparameters
- Modified training techniques (oversampling, branch-specific SAEs) enabled effective feature discovery while managing computational constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sparse autoencoders decompose polysemantic neurons into more monosemantic constituent features.
- Mechanism: SAEs learn a sparse dictionary that maps the original high-dimensional activation space into a lower-dimensional space where each feature is more selective and interpretable. When reconstructing activations, the SAE uses a linear combination of these features, effectively splitting the original neuron's mixed response into distinct, specialized components.
- Core assumption: The original neural network activations can be accurately represented as a sparse linear combination of interpretable features.
- Evidence anchors:
  - [abstract] "We also find that SAEs can decompose some polysemantic neurons into more monosemantic constituent features."
  - [section] "For example, the SAE splits the double curve detector neuron mixed3b/n/359 into separate curve and double curve features, with their relative strengths adjusting based on SAE hyperparameters."
  - [corpus] Weak evidence - no directly comparable SAE decomposition studies found in neighbors.
- Break condition: If the underlying activation space is inherently non-sparse or if superposition is so extreme that no meaningful decomposition exists, SAEs may fail to produce interpretable features.

### Mechanism 2
- Claim: SAEs can uncover "missing" interpretable features that were not apparent when examining individual neurons.
- Mechanism: By operating at a higher level of abstraction than individual neurons, SAEs can detect feature combinations and patterns that individual neurons miss. The SAE's dictionary learning process can identify meaningful feature directions that are not aligned with any single neuron's response.
- Core assumption: Some features in the neural network are encoded through distributed patterns across multiple neurons rather than through individual neurons.
- Evidence anchors:
  - [abstract] "Our results demonstrate that SAEs can uncover new interpretable features not apparent from examining individual neurons, including additional curve detectors that fill in previous gaps."
  - [section] "We find curve detector features which closely match the curve detector neurons known to exist, and others which fill in 'gaps' where there wasn't a curve detector for a particular orientation."
  - [corpus] No direct evidence in neighbors, but related to general sparse coding theory.
- Break condition: If all meaningful features are already captured by individual neurons or if the SAE dictionary becomes too large to be useful.

### Mechanism 3
- Claim: Modified SAE training techniques (oversampling, branch-specific training) improve feature quality while managing computational constraints.
- Mechanism: Oversampling large activations ensures the SAE learns features from the most significant signals rather than being dominated by background activations. Branch-specific training leverages the architectural specialization of InceptionV1 to focus SAE learning on the most relevant convolutional branches for each layer.
- Core assumption: The distribution of activations is highly skewed, and architectural specialization exists that can be exploited for more efficient learning.
- Evidence anchors:
  - [section] "To avoid spending lots of compute modelling these small activations, we oversampled large activations by sampling activations from positions in the image proportional to their activation magnitude."
  - [section] "For layers mixed3a and mixed3b, we perform dictionary learning only on the 3x3 and 5x5 convolutional branches respectively."
  - [corpus] Weak evidence - no directly comparable techniques in neighbors.
- Break condition: If the assumption about activation distribution is wrong or if cross-branch superposition is more significant than anticipated.

## Foundational Learning

- Concept: Dictionary Learning
  - Why needed here: SAEs are based on dictionary learning principles where activations are decomposed into a sparse combination of basis vectors (dictionary elements).
  - Quick check question: If you have an activation vector x and a dictionary D with K atoms, what is the mathematical form of the sparse reconstruction problem?

- Concept: Superposition in Neural Networks
  - Why needed here: Understanding why individual neurons are often polysemantic is crucial for appreciating why SAEs are necessary.
  - Quick check question: What is the key difference between a monosemantic neuron and a polysemantic neuron in terms of their response to different input features?

- Concept: Sparse Coding
  - Why needed here: SAEs enforce sparsity in the feature activation space, which is fundamental to their ability to produce interpretable features.
  - Quick check question: How does the L1 regularization term in the SAE loss function encourage sparsity in the learned feature activations?

## Architecture Onboarding

- Component map:
  - Data pipeline: ImageNet ILSVRC activations sampled from InceptionV1
  - SAE model: Sparse autoencoder with modified training (oversampling, branch-specific)
  - Analysis tools: Dataset examples, feature visualization, synthetic curve stimuli
  - Validation: Comparison with known curve detector literature

- Critical path:
  1. Sample activations from InceptionV1 layers
  2. Train SAE on sampled activations
  3. Extract and visualize learned features
  4. Compare with existing neuron-level analysis
  5. Validate using synthetic stimuli and dataset examples

- Design tradeoffs:
  - Dictionary size vs. interpretability: Larger dictionaries may capture more features but become harder to interpret
  - Training compute vs. coverage: Branch-specific training saves compute but may miss cross-branch features
  - Sparsity level vs. reconstruction accuracy: Higher sparsity produces cleaner features but may lose information

- Failure signatures:
  - Dead neurons: Some features never activate (check sparsity regularization)
  - Uninterpretable features: SAE learns features that don't correspond to meaningful visual patterns
  - Poor reconstruction: SAE fails to accurately reconstruct original activations

- First 3 experiments:
  1. Train a basic SAE on conv2d0 layer with default hyperparameters and visualize the top 10 features
  2. Compare reconstruction error with and without oversampling to verify its effectiveness
  3. Test different L1 regularization strengths on mixed3b to observe the effect on feature decomposition quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does branch specialization in SAEs reveal previously unknown architectural patterns in InceptionV1?
- Basis in paper: [explicit] The paper notes that while training SAEs on all branches of mixed3b, they found that features are not uniformly distributed across branches, with some being highly branch-specific. They also mention that curve detectors are disproportionately concentrated on the 5x5 branch, consistent with prior work by V oss et al. (2021).
- Why unresolved: The paper presents preliminary findings on branch specialization but leaves many questions unanswered, such as whether other layers exhibit similar patterns and what the nature of branch-localized features might be.
- What evidence would resolve it: Systematic experiments across multiple layers and models to determine the prevalence and characteristics of branch specialization, along with comparisons to known architectural patterns.

### Open Question 2
- Question: How do SAE hyperparameters, particularly the L1 coefficient, affect the decomposition of polysemantic neurons into monosemantic features?
- Basis in paper: [explicit] The paper shows that as the L1 coefficient increases in SAEs, the maximum activation of double curve features decreases while the left and right curve features' activations increase, suggesting a tunable decomposition of polysemantic neurons.
- Why unresolved: The paper only demonstrates this effect for one specific example (the double curve detector neuron mixed3b/n/359) and doesn't explore the general relationship between hyperparameters and feature decomposition.
- What evidence would resolve it: Extensive ablation studies varying L1 coefficients and other hyperparameters across multiple polysemantic neurons to establish general principles of feature decomposition.

### Open Question 3
- Question: Can SAEs consistently identify interpretable features in other well-studied neural networks beyond InceptionV1, and how do these features compare to those found in language models?
- Basis in paper: [explicit] The paper concludes by suggesting that SAEs show promise for understanding InceptionV1 and convolutional neural networks more generally, drawing parallels to their success in language models.
- Why unresolved: The paper only applies SAEs to InceptionV1 without comparing results to other CNN architectures or exploring whether similar interpretable features emerge in different types of neural networks.
- What evidence would resolve it: Applying SAEs to a diverse set of neural networks (e.g., ResNet, Vision Transformers) and comparing the interpretability and nature of discovered features across architectures.

## Limitations

- The modified training techniques (oversampling, branch-specific learning) show promise but lack systematic comparative analysis against standard SAE training methods
- The paper demonstrates SAE capabilities but doesn't establish whether discovered features are truly "missing" or simply differently represented compared to neuron-level analysis
- Core mechanisms rely on assumptions about activation sparsity and architectural specialization that are not directly validated in this work

## Confidence

- **High confidence**: SAEs can decompose polysemantic neurons into constituent features (directly demonstrated with mixed3b/n/359)
- **Medium confidence**: SAEs uncover genuinely "missing" curve detectors (evidence is circumstantial, based on gap-filling claims)
- **Medium confidence**: Modified training techniques improve feature quality (supported by implementation details but not systematically evaluated)

## Next Checks

1. Conduct systematic comparison of SAE training with and without oversampling to quantify its impact on feature discovery and reconstruction quality
2. Evaluate cross-branch SAE training for mixed3a/mixed3b layers to determine if branch-specific specialization misses important features
3. Perform ablation studies varying dictionary size and L1 regularization strength to identify optimal tradeoffs between feature interpretability and reconstruction accuracy