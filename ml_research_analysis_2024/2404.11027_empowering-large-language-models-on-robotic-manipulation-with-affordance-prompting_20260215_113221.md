---
ver: rpa2
title: Empowering Large Language Models on Robotic Manipulation with Affordance Prompting
arxiv_id: '2404.11027'
source_url: https://arxiv.org/abs/2404.11027
tags:
- affordance
- tasks
- task
- llms
- robotic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LLM+A, a framework that leverages large language
  models (LLMs) to serve as both high-level sub-task planners and low-level motion
  controllers for language-conditioned robotic manipulation tasks in a training-free
  paradigm. The key idea is to use affordance prompting, which stimulates LLMs to
  predict consequences of actions and generate affordance values for relevant objects,
  thereby grounding generated plans and control sequences on the physical world.
---

# Empowering Large Language Models on Robotic Manipulation with Affordance Prompting

## Quick Facts
- **arXiv ID:** 2404.11027
- **Source URL:** https://arxiv.org/abs/2404.11027
- **Reference count:** 7
- **Primary result:** LLM+A framework uses affordance prompting to achieve 59% success rate on pushing tasks, significantly outperforming baselines (19-32%)

## Executive Summary
This paper introduces LLM+A, a framework that uses large language models (LLMs) for both high-level task planning and low-level motion control in robotic manipulation tasks. The key innovation is "affordance prompting," which stimulates LLMs to predict the consequences of actions and generate affordance values for objects, grounding generated plans in physical reality. LLM+A operates in a training-free paradigm, relying on pre-trained vision-language models (CLIP, LLMs) and a custom motion planner. The approach is evaluated on language-conditioned robotic manipulation tasks from the Language-Table and CLIPORT benchmarks.

## Method Summary
LLM+A employs affordance prompting to generate grounded plans and control sequences for robotic manipulation. The framework uses an LLM to plan sub-tasks and predict affordance values for relevant objects, which are then used to guide low-level motion planning via a custom RT-RRT* algorithm. The system leverages a state tracker to map object states to affordance values, creating a feedback loop between high-level planning and low-level execution. The approach is evaluated on synthetic benchmarks without requiring real-world robot experiments.

## Key Results
- LLM+A achieves 59% average success rate on pushing tasks, outperforming Naive LLM (19%), ReAct (24%), and Code as Policies (32%)
- Demonstrates strong generalization to heterogeneous tasks, reaching ~85% average success rate
- Significantly reduces invalid action sequences compared to non-grounded baselines

## Why This Works (Mechanism)
The approach grounds LLM-generated plans in physical reality by predicting action consequences through affordance values. This creates a feedback loop where high-level planning is informed by low-level physical constraints, enabling more realistic and executable action sequences. The affordance values act as a bridge between symbolic planning and geometric execution.

## Foundational Learning
1. **Affordance Prompting** - Why needed: To ground LLM plans in physical reality; Quick check: Compare plan validity with/without affordance values
2. **Vision-Language Models** - Why needed: To interpret object states and predict affordances; Quick check: Evaluate affordance prediction accuracy across object categories
3. **RT-RRT* Motion Planning** - Why needed: To generate collision-free paths based on affordance-guided goals; Quick check: Success rate comparison with alternative motion planners

## Architecture Onboarding

**Component Map:** Vision Model -> State Tracker -> LLM (with Affordance Prompting) -> Motion Planner -> Robot Controller

**Critical Path:** Vision Model -> State Tracker -> LLM -> Motion Planner -> Robot Controller

**Design Tradeoffs:** Training-free approach vs. dependence on pre-trained models; Strong performance on benchmarks vs. lack of real-world validation

**Failure Signatures:** Invalid action sequences, failure to handle occlusions, degradation in long-horizon tasks

**First 3 Experiments:**
1. Baseline comparison on synthetic benchmarks (Language-Table, CLIPORT)
2. Ablation study on affordance prompting effectiveness
3. Generalization test across heterogeneous task types

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies entirely on synthetic benchmarks without real-world robot experiments
- Depends on a custom RT-RRT* motion planner that is not publicly available
- Does not address challenges like object occlusions, multi-object occlusion, or long-horizon reasoning

## Confidence

**High confidence in:**
- The novel affordance prompting approach and its theoretical grounding
- The comparative performance gains over baseline methods on tested benchmarks

**Medium confidence in:**
- The generality of the approach across different task types and object categories
- The robustness of state tracking and affordance prediction under varied conditions

**Low confidence in:**
- Real-world deployment potential without physical robot experiments
- Scalability to complex, long-horizon tasks requiring extensive sequential reasoning

## Next Checks
1. Physical robot implementation: Deploy LLM+A on a real robot platform to verify synthetic benchmark performance translates to real-world manipulation tasks
2. Ablation studies on motion planning: Evaluate the contribution of RT-RRT* by comparing against alternative planners
3. Long-horizon task testing: Extend evaluation to tasks requiring 10+ sequential steps to assess effectiveness for complex multi-step reasoning