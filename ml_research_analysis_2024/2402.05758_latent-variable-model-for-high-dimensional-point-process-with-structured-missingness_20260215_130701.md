---
ver: rpa2
title: Latent variable model for high-dimensional point process with structured missingness
arxiv_id: '2402.05758'
source_url: https://arxiv.org/abs/2402.05758
tags:
- missingness
- process
- point
- latent
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a novel deep latent variable model (DLVM)
  designed to handle structured missingness in longitudinal data. The approach leverages
  Gaussian processes (GPs) to capture temporal correlations between samples and their
  associated missingness masks, as well as to model the underlying point process.
---

# Latent variable model for high-dimensional point process with structured missingness

## Quick Facts
- arXiv ID: 2402.05758
- Source URL: https://arxiv.org/abs/2402.05758
- Authors: Maksim Sinelnikov; Manuel Haussmann; Harri Lähdesmäki
- Reference count: 38
- Key outcome: Introduces a deep latent variable model with Gaussian process priors to handle structured missingness in longitudinal data, outperforming baselines on imputation and prediction tasks.

## Executive Summary
This paper presents a novel deep latent variable model (DLVM) for high-dimensional longitudinal data with structured missingness. The approach combines variational autoencoders with Gaussian processes to capture temporal correlations and missingness patterns simultaneously. The model introduces three sets of latent variables - one for observations, one for missingness masks, and one for the point process intensity - allowing it to effectively model complex dependencies in healthcare data. Experimental results on synthetic and real-world datasets demonstrate competitive performance in missing value imputation and future prediction tasks.

## Method Summary
The method is a variational autoencoder framework with Gaussian process priors on latent variables. It uses longitudinal additive kernels to model interactions between continuous and categorical covariates, and incorporates a temporal point process to capture the stochastic nature of observation times. The model employs scalable amortized variational inference with inducing points to approximate the posterior over latent variables. Two variants are proposed: LLSM without the temporal point process component, and LLPPSM with full temporal point process modeling.

## Key Results
- Competitive performance on missing value imputation and future prediction tasks compared to baseline methods
- Improved handling of structured missingness patterns through the introduction of latent variables for missingness masks
- Effective modeling of irregularly sampled data through incorporation of temporal point process intensity in GP kernels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Three-way coupling of latent variables (zy for observations, zm for missingness masks, zλ for point process intensity) captures both data structure and missingness mechanism simultaneously.
- Mechanism: By treating missingness mask as a latent variable with GP prior, the model infers structured missingness patterns that depend on both observed and unobserved data.
- Core assumption: Missingness pattern is conditionally dependent on underlying latent representations of both observations and missingness mask.
- Evidence anchors:
  - [abstract]: "The key innovation lies in incorporating three sets of latent variables with GP priors to model observations, missingness masks, and the point process."
  - [section 3.2]: "To model non-random missingness in VAE models various approaches exist... In this work, we follow Collier et al. (2020) and introduce a second latent variable zm ∈ RLm associated with a missingness mask m..."
- Break condition: If missingness is truly independent of data (MCAR), added complexity of zm may not improve performance and could overfit.

### Mechanism 2
- Claim: Incorporating temporal point process intensity into GP kernel of both zy and zm captures stochastic nature of observation times.
- Mechanism: Intensity λ(t) from Hawkes process, modeled via GP, modulates covariance between latent variables based on expected rate of future events.
- Core assumption: Similarity between observations should depend on underlying event rate, not just elapsed time.
- Evidence anchors:
  - [abstract]: "Our approach utilizes Gaussian processes to capture temporal correlations between samples and their associated missingness masks as well as to model the underlying point process."
  - [section 3.3]: "To properly account for such variations, we model t by a temporal point process and add the intensity of this point process as an additional input to the GP kernel computation."
- Break condition: If observation times are deterministic or irrelevant to data structure, added complexity of modeling point process may not yield benefits.

### Mechanism 3
- Claim: Using longitudinal additive kernels in GP priors allows flexible modeling of interactions between continuous and categorical covariates.
- Mechanism: Additive kernel structure decomposes covariance into components for each covariate type and their interactions, capturing complex dependencies without overfitting.
- Core assumption: Data-generating process can be well-approximated by additive interactions between covariates.
- Evidence anchors:
  - [section 2]: "Ramchandran et al. (2021) introduced L-VAE, a model that uses a multi-output additive GP prior and is well-suited for longitudinal data by leveraging carefully designed interaction kernels."
  - [section 3.2]: "To adapt the model for longitudinal data, we rely on longitudinal additive kernels (Ramchandran et al., 2021) for the latent representations of observations and masks."
- Break condition: If true data structure involves highly non-additive interactions, additive kernel may be insufficient.

## Foundational Learning

- Concept: Gaussian Processes (GPs)
  - Why needed here: GPs provide flexible, non-parametric way to model latent space, capturing complex temporal dependencies and structured missingness without assuming fixed functional form.
  - Quick check question: What is the key difference between standard VAE prior and GP prior used in this model?

- Concept: Variational Inference and ELBO
  - Why needed here: Model uses amortized variational inference to approximate intractable posterior over latent variables, optimizing lower bound on marginal likelihood.
  - Quick check question: Why is KL divergence term in ELBO approximated with upper bound in this model?

- Concept: Temporal Point Processes (TPPs)
  - Why needed here: TPPs model stochastic nature of observation times, allowing model to infer intensity of events and use this information to improve predictions.
  - Quick check question: How does Hawkes process intensity function differ from simple Poisson process intensity?

## Architecture Onboarding

- Component map:
  - Encoder networks: Map observations and masks to variational distributions over zy and zm
  - Decoder networks: Generate observations and masks from latent variables
  - GP priors: Define prior distributions over zy, zm, and zλ using longitudinal additive kernels
  - Temporal point process: Model intensity function λ(t) using GP and Hawkes process
  - Variational inference: Approximate posterior using reparameterization and Monte Carlo sampling

- Critical path:
  1. Encode observations and masks to get variational parameters for zy and zm
  2. Sample from variational distributions to get zy and zm
  3. Compute ELBO using generative model, GP priors, and variational distributions
  4. Optimize ELBO using gradient descent

- Design tradeoffs:
  - Using three sets of latent variables increases model complexity but allows for structured missingness modeling
  - Incorporating point process intensity into GP kernels adds flexibility but requires careful tuning of hyperparameters
  - Using additive kernels captures interactions but may miss highly non-additive patterns

- Failure signatures:
  - Poor imputation performance may indicate issues with missingness model (zm) or encoder/decoder networks
  - Instability during training may suggest problems with variational inference or GP kernel hyperparameters
  - Overfitting may occur if model is too complex for available data

- First 3 experiments:
  1. Test imputation performance on simple dataset with MCAR missingness to verify basic VAE functionality
  2. Evaluate model's ability to capture structured missingness patterns on synthetic dataset with known missingness mechanisms
  3. Assess impact of point process component by comparing performance on regularly vs. irregularly sampled data

## Open Questions the Paper Calls Out
None

## Limitations
- Model performance critically depends on correct specification of missingness mechanism and temporal point process
- Computational complexity of training GP-based models with multiple latent variables remains practical limitation for very large datasets
- Use of additive kernels, while flexible, may miss highly non-linear interactions in data

## Confidence
- **High confidence**: Core architecture combining VAEs with GP priors for structured missingness is well-founded and theoretically sound
- **Medium confidence**: Effectiveness of model on real-world healthcare data depends on specific characteristics of datasets and may vary across different domains
- **Low confidence**: Model's ability to scale to extremely high-dimensional data (e.g., genomics) without significant performance degradation is uncertain

## Next Checks
1. Test model's performance when true missingness mechanism is MCAR, MAR, and MNAR to quantify benefits of modeling structured missingness
2. Compare performance of LLPPSM with simpler variant that does not include temporal point process component on datasets with varying degrees of irregularity in observation times
3. Evaluate model's performance and training time on increasingly high-dimensional datasets (e.g., from 100 to 10,000 dimensions) to identify practical limits