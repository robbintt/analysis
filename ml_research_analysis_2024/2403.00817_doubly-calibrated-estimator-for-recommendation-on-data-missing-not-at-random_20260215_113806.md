---
ver: rpa2
title: Doubly Calibrated Estimator for Recommendation on Data Missing Not At Random
arxiv_id: '2403.00817'
source_url: https://arxiv.org/abs/2403.00817
tags:
- propensity
- calibration
- estimator
- imputation
- estimators
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the issue of miscalibration in imputation
  and propensity models used in doubly robust (DR) estimators for recommender systems.
  The authors propose a Doubly Calibrated Estimator that involves calibrating both
  the imputation and propensity models through calibration experts.
---

# Doubly Calibrated Estimator for Recommendation on Data Missing Not At Random

## Quick Facts
- arXiv ID: 2403.00817
- Source URL: https://arxiv.org/abs/2403.00817
- Reference count: 40
- Primary result: Up to 8.37% improvement in recommendation performance over existing doubly robust estimators

## Executive Summary
This paper addresses the critical issue of miscalibration in doubly robust (DR) estimators used for recommendation systems on data missing not at random (MNAR). The authors identify that both imputation and propensity models in standard DR frameworks can suffer from systematic biases that degrade recommendation quality. To address this, they propose a Doubly Calibrated Estimator that simultaneously calibrates both models through a novel tri-level joint learning framework incorporating calibration experts. These experts learn to adjust for different user-specific logit distributions, leading to more accurate propensity scoring and imputation. Experimental results on three real-world datasets demonstrate substantial improvements over existing methods, with up to 8.37% better performance across multiple evaluation metrics including MSE, AUC, and NDCG@K.

## Method Summary
The proposed Doubly Calibrated Estimator addresses miscalibration in doubly robust recommendation by introducing a tri-level joint learning framework. At its core, the method employs calibration experts for both imputation and propensity models that learn user-specific adjustment functions. The imputation model estimates missing ratings using a prediction function plus imputed errors, while the propensity model estimates the probability that each rating is observed. The calibration experts, organized into K clusters, learn to correct systematic biases in both models by adjusting logits based on user-specific distributions. These experts are simultaneously optimized with the prediction and imputation models using a complex training procedure involving Gumbel-Softmax sampling for user assignment to calibration experts. The method is trained in three levels: first optimizing calibration experts on a validation set, then training the main models with calibrated outputs, and finally fine-tuning all components together. This approach effectively addresses the variance-inflation problem caused by poorly calibrated propensity scores while improving the accuracy of imputed errors.

## Key Results
- Achieves up to 8.37% improvement in recommendation performance compared to existing DR estimators
- Shows consistent improvements across all tested metrics: MSE, AUC, NDCG@5, NDCG@10, NDCG@50, and NDCG@100
- Outperforms baseline methods including IPS, SNIPS, CVIB, AT, DR-JL, MRDR-JL, ESCM2-DR, BRD-DR, MR, DR-MSE, StableDR, and TDR-CL on all three datasets (Coat, Yahoo!R3, KuaiRec)
- Demonstrates effectiveness across different dataset sizes, from the small Coat dataset (290 users, 300 items) to larger datasets with thousands of users and items

## Why This Works (Mechanism)
The effectiveness stems from simultaneously addressing two sources of miscalibration in DR estimators. First, the imputation model's errors are systematically biased due to MNAR sampling, which the calibration experts correct by learning user-specific adjustment functions. Second, propensity scores often suffer from variance inflation when estimated poorly, which the calibrated propensity model mitigates through improved user clustering. The tri-level learning framework ensures that calibration experts learn meaningful user clusters before being integrated into the main prediction pipeline, creating a feedback loop that progressively improves both calibration and prediction accuracy.

## Foundational Learning
**MNAR data handling**: Understanding how data missing not at random differs from MAR data is crucial, as standard imputation fails when missingness depends on the rating value itself. Quick check: Verify that missingness mechanism is indeed MNAR by checking correlation between rating values and observation probability.

**Doubly robust estimation**: DR estimators combine imputation and propensity weighting to achieve consistency if either model is correct. Quick check: Confirm that both models are properly specified and that their combination reduces overall error.

**Calibration in classification**: Proper calibration ensures predicted probabilities match empirical frequencies, critical for propensity scoring. Quick check: Plot reliability diagrams for propensity scores before and after calibration.

**Gumbel-Softmax sampling**: Enables differentiable sampling from categorical distributions for user assignment to calibration experts. Quick check: Monitor entropy of assignment probabilities during training to ensure meaningful clustering.

**Tri-level optimization**: Coordinating optimization across multiple model components requires careful scheduling. Quick check: Track loss values at each level separately to ensure stable convergence.

## Architecture Onboarding

**Component Map**: Data -> Preprocess -> Propensity Model -> Calibration Experts -> Prediction Model -> Imputation Model -> Evaluation

**Critical Path**: The most important flow is Data → Propensity Model → Calibration Experts → Prediction Model → Final Output, as the calibrated propensity scores directly impact the DR estimator's variance and bias.

**Design Tradeoffs**: The method trades increased model complexity (additional calibration experts and tri-level training) for improved accuracy. The number of experts K (chosen from {5, 10, 20}) represents a key hyperparameter balancing model capacity against overfitting risk.

**Failure Signatures**: Poor calibration manifests as high variance in DR estimates or systematic bias in predicted ratings. If calibration experts fail to learn meaningful clusters, assignment probabilities will be uniform across experts. Training instability may occur if learning rates aren't properly balanced across the three optimization levels.

**First Experiments**: 
1. Train baseline DR estimator without calibration to establish performance floor
2. Implement single calibration expert (K=1) to verify basic functionality
3. Test different values of K (5, 10, 20) to identify optimal number of clusters

## Open Questions the Paper Calls Out
The paper explicitly identifies several unresolved questions. First, while it discusses how propensity model calibration affects DR estimator bias, it doesn't establish a formal relationship between calibration error and estimator variance - a formal proof or theorem demonstrating this connection would be valuable. Second, the paper briefly mentions that K is chosen from {5, 10, 20} through grid search but doesn't provide detailed analysis of how performance varies with different numbers of calibration experts. Third, while the paper notes that training time isn't significantly increased compared to TDR-CL, it lacks comprehensive comparison of computational efficiency with other debiasing methods across different dataset sizes.

## Limitations
- The method requires careful hyperparameter tuning, particularly for the number of calibration experts K and learning rate scheduling across tri-level optimization
- Computational overhead increases with more calibration experts, though the paper claims this is manageable
- Performance improvements, while significant, are demonstrated primarily on datasets with relatively small user populations (max 15,401 users)
- The method's effectiveness depends on having sufficient observed data to learn meaningful user clusters in the calibration experts

## Confidence
**Methodological Confidence: Medium-High** - The theoretical framework is sound but lacks complete implementation details for critical components like neural architectures and training procedures.

**Experimental Confidence: Medium** - Results are promising but based on limited dataset sizes and don't explore temporal stability or robustness to varying missingness patterns.

**Generalizability Confidence: Medium** - Performance gains are consistent across three datasets but with relatively small user populations, limiting conclusions about scalability.

## Next Checks
1. Replicate core experiments on additional MNAR datasets with varying sizes and missingness patterns to assess robustness beyond the three tested datasets
2. Conduct ablation studies systematically removing calibration experts to quantify their specific contribution to performance gains and identify minimum effective K
3. Implement alternative neural architectures (e.g., different collaborative filtering models) to test the method's architecture-agnostic properties and identify potential bottlenecks