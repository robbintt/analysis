---
ver: rpa2
title: 'Are You Sure? Rank Them Again: Repeated Ranking For Better Preference Datasets'
arxiv_id: '2405.18952'
source_url: https://arxiv.org/abs/2405.18952
tags:
- training
- responses
- response
- prompts
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses inconsistencies in AI evaluator rankings for
  Reinforcement Learning from AI Feedback (RLAIF). The proposed Repeat Ranking method
  evaluates the same responses multiple times and trains only on those with consistently
  high agreement, measured by Kendall's W.
---

# Are You Sure? Rank Them Again: Repeated Ranking For Better Preference Datasets

## Quick Facts
- arXiv ID: 2405.18952
- Source URL: https://arxiv.org/abs/2405.18952
- Authors: Peter Devine
- Reference count: 5
- Key outcome: Repeat Ranking method improves RLAIF model quality by filtering preference data based on evaluation consistency, achieving better MT-Bench scores with smaller, higher-quality training sets

## Executive Summary
This paper addresses inconsistencies in AI evaluator rankings for Reinforcement Learning from AI Feedback (RLAIF). The proposed Repeat Ranking method evaluates the same responses multiple times and trains only on those with consistently high agreement, measured by Kendall's W. Using 2,714 multilingual prompts and seven LLMs, GPT-4 ranked responses five times each. Training on the top 25%, 50%, and 75% most consistent rankings outperformed using all data. On MT-Bench, the 50% subset (Suzume-ORPO-50) achieved the best average score of 7.91, exceeding the full dataset model (Suzume-ORPO-100 at 7.71) and matching GPT-3.5-Turbo in multiple languages. The approach demonstrates quality-over-quantity trade-offs in preference dataset generation, offering a stackable strategy for improving RLAIF model quality.

## Method Summary
The method generates preference datasets by first collecting responses from seven different LLMs to 2,714 multilingual prompts. Each response set is ranked five times by GPT-4. Kendall's W is computed to measure agreement between these rankings. Responses are then filtered based on consistency - only those with high Kendall's W values are selected for training. The filtered datasets (25%, 50%, 75% most consistent) are used to train ORPO models, which are then evaluated on MT-Bench across multiple languages.

## Key Results
- Training on consistently ranked responses (50% subset) achieved MT-Bench score of 7.91, outperforming the full dataset model (7.71)
- Only 8.4% of top responses were ranked top across all 5 trials, demonstrating significant evaluator inconsistency
- 25% subset reduced training costs by up to 4x while maintaining competitive performance
- Quality-over-quantity approach showed clear benefits for multilingual chat evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Repeated ranking exposes inconsistent evaluator behavior, allowing filtering of low-confidence preference pairs.
- Mechanism: GPT-4 rankings are repeated multiple times for the same response sets. Kendall's W is computed to measure inter-ranking agreement. Low W indicates inconsistent rankings, suggesting ambiguous response quality. Only consistently ranked (high W) prompts are used for training.
- Core assumption: GPT-4's repeated rankings on the same response set will vary when the responses are of similar quality, and that these variations indicate low confidence in the preference labels.
- Evidence anchors:
  - [abstract] "However, the rankings from popular evaluator models such as GPT-4 can be inconsistent. We propose the Repeat Ranking method - where we evaluate the same responses multiple times and train only on those responses which are consistently ranked."
  - [section] "We found that only 8.4% of all top responses were ranked top all 5 times, and only 20.2% of bottom responses were ranked bottom all 5 times, which again motivates our work in generating multiple evaluations for each set of responses per prompt."
  - [corpus] Weak - the corpus neighbors discuss ranking and LLM robustness but do not directly address repeated ranking for consistency filtering.
- Break condition: If the evaluator model is deterministic or always outputs identical rankings, repeated ranking provides no additional information and filtering becomes irrelevant.

### Mechanism 2
- Claim: Training on consistently ranked responses leads to better downstream evaluation performance than training on all available rankings.
- Mechanism: By selecting only the top 25%, 50%, or 75% of prompts with highest Kendall's W scores, the training set contains preference pairs where the evaluator had high confidence. This results in clearer positive/negative signal for the RL training, improving the model's ability to learn meaningful preferences.
- Core assumption: Preference pairs with high Kendall's W scores represent clearer quality distinctions that the RL training algorithm can learn from more effectively than ambiguous pairs.
- Evidence anchors:
  - [abstract] "Our results show that training on the more consistently ranked responses gives greater downstream evaluation performance compared to training on all data for a majority of languages tested."
  - [section] "We found that the MT-Bench scores of the Suzume-ORPO-25, Suzume-ORPO-50, Suzume-ORPO-75 models, on average, outperformed the Suzume-ORPO-100 across the evaluated languages."
  - [corpus] Weak - corpus neighbors discuss ranking and LLM evaluation but do not specifically address the impact of training data consistency on downstream performance.
- Break condition: If the evaluation consistency threshold is set too high, the training set becomes too small to provide sufficient learning signal, potentially degrading performance.

### Mechanism 3
- Claim: Quality-over-quantity tradeoff in preference dataset generation improves model performance.
- Mechanism: Instead of using all available preference data, selecting a subset with higher quality (consistency) yields better results. This demonstrates that having cleaner, more reliable training signals is more valuable than having more noisy data points.
- Core assumption: The marginal benefit of adding more training examples decreases when those examples have low consistency, and the noise introduced outweighs the benefit of additional data.
- Evidence anchors:
  - [abstract] "Our work highlights the quality versus quantity trade-off in RLAIF dataset generation and offers a stackable strategy for enhancing dataset and thus model quality."
  - [section] "This has the double benefits of increasing performance while reducing training cost by as much as four times for training on our 25% training subset."
  - [corpus] Weak - corpus neighbors discuss ranking and evaluation but do not directly address the quality-quantity tradeoff in preference dataset generation.
- Break condition: If the evaluation consistency threshold removes too many examples, the reduced dataset size may not provide sufficient coverage of the prompt space, limiting the model's ability to generalize.

## Foundational Learning

- Concept: Kendall's W (coefficient of concordance)
  - Why needed here: Used to measure agreement between multiple rankings of the same response set, serving as the consistency metric for filtering training data.
  - Quick check question: If three evaluators rank items A, B, C as A>B>C, A>C>B, and B>A>C, what does this imply about Kendall's W and why would this matter for training data selection?

- Concept: Borda Count
  - Why needed here: Used to aggregate multiple rankings of responses to determine which responses should be selected as positive and negative examples for ORPO training.
  - Quick check question: Given five rankings of responses A, B, C, D, E where A appears first twice, second twice, and third once, what is A's Borda count and why is this useful for selecting training examples?

- Concept: Odds Ratio Policy Optimization (ORPO)
  - Why needed here: The training method used to fine-tune the base model using the filtered preference dataset, requiring understanding of how preference data drives policy updates.
  - Quick check question: In ORPO, how does the odds ratio between positive and negative responses influence the policy gradient update, and why would cleaner preference signals improve this process?

## Architecture Onboarding

- Component map: Base model (Suzume 8B) -> ORPO training loop -> filtered Mitsu dataset -> MT-Bench evaluation
- Critical path: Prompt generation -> response generation from 7 models -> 5 repeated GPT-4 rankings -> Kendall's W computation -> dataset filtering -> ORPO training -> evaluation
- Design tradeoffs: Higher consistency thresholds (25%) reduce training data size but may improve quality; lower thresholds (100%) include more data but potentially noisier signals. The choice of Kendall's W as consistency metric versus simpler metrics like pairwise agreement.
- Failure signatures: If MT-Bench scores don't improve with filtering, it could indicate that the evaluator's inconsistencies don't correlate with actual response quality differences, or that the ORPO training isn't sensitive enough to benefit from cleaner data.
- First 3 experiments:
  1. Implement repeated ranking pipeline with 3 repetitions instead of 5 to validate Kendall's W computation and observe variance in rankings.
  2. Test different consistency thresholds (e.g., 60%, 80%) to find the optimal balance between data quantity and quality.
  3. Compare ORPO training with filtered data versus training on randomly sampled subsets of the same size to isolate the effect of consistency-based filtering.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Repeat Ranking method maintain its effectiveness with significantly larger training datasets (e.g., hundreds of thousands of examples)?
- Basis in paper: Explicit - The authors acknowledge that their Mitsu dataset is much smaller than popular datasets like Nectar (hundreds of thousands of examples) and state this as a limitation
- Why unresolved: The authors note they haven't shown whether their proposed response selection technique extends to datasets of that size
- What evidence would resolve it: Training and evaluating models using the Repeat Ranking method on datasets with hundreds of thousands of examples, then comparing performance against models trained on all data regardless of consistency

### Open Question 2
- Question: What is the optimal Kendall's W threshold for different tasks or languages, and how does it vary across domains?
- Basis in paper: Explicit - The authors state "We theorize that the correct balance between consistency and data volume... may vary between tasks" but only demonstrate results for multilingual chat
- Why unresolved: The authors only tested one application (multilingual chat) and used fixed thresholds (25%, 50%, 75%) without exploring optimal thresholds for different scenarios
- What evidence would resolve it: Systematic experiments varying Kendall's W thresholds across different task types, languages, and domains to identify optimal cutoff values for each scenario

### Open Question 3
- Question: Would using multiple evaluator models (rather than just GPT-4) create more robust evaluations and further improve model performance?
- Basis in paper: Explicit - The authors suggest "Future work could include... using more than one evaluator model" and theorize that "combining the evaluations of multiple high performance LLMs could serve to create more robust evaluations"
- Why unresolved: This was identified as future work but not implemented or tested in the current study
- What evidence would resolve it: Implementing the Repeat Ranking method with multiple evaluator models (e.g., GPT-4, Claude 3, Gemini 1.5 Pro) and comparing the resulting model performance against single-evaluator results

### Open Question 4
- Question: Does training on prompts filtered by difficulty level (rather than random sampling) lead to better improvements on difficult tasks while avoiding wasting resources on tasks where models are already proficient?
- Basis in paper: Explicit - The authors theorize "Training on tasks that LLMs are already highly proficient at might be a waste of training resources" and suggest filtering prompts based on perceived difficulty
- Why unresolved: This was proposed as future work but not implemented or tested
- What evidence would resolve it: Creating difficulty-filtered datasets where prompts are categorized by difficulty level, then comparing model performance improvements on difficult tasks versus random-sampling approaches

### Open Question 5
- Question: Would augmenting the evaluation process with external tools (search engines, calculators, etc.) lead to more accurate evaluations and ultimately better-performing models?
- Basis in paper: Explicit - The authors propose that "Future work could explore using tools or agents to enhance the evaluation abilities of the evaluator LLM" and theorize this would lead to "more accurate evaluation and would ultimately lead to more accurate LLMs"
- Why unresolved: This remains untested as it was identified as future work
- What evidence would resolve it: Implementing tool-augmented evaluation (e.g., search tools for fact-checking, calculators for math verification) and comparing resulting model performance against standard LLM-only evaluation

## Limitations
- Relies on a single evaluator (GPT-4), potentially reflecting specific evaluator biases rather than universal quality distinctions
- Only tested on one base model (Suzume 8B) and one RL training method (ORPO), limiting generalizability
- MT-Bench evaluation represents a single evaluation protocol that may not capture all aspects of model quality

## Confidence
- High Confidence: The observation that repeated rankings show low consistency (8.4% of top responses ranked consistently top across all 5 trials) is empirically demonstrated and directly measurable. The downstream performance improvement when training on consistently ranked data (Suzume-ORPO-50 achieving 7.91 vs Suzume-ORPO-100 at 7.71 on MT-Bench) is also well-supported by the experimental results.
- Medium Confidence: The claim that quality-over-quantity tradeoffs improve RLAIF dataset generation is supported but requires more diverse validation. The 25% subset showing competitive performance while reducing training costs by up to 4x is promising, but the optimal consistency threshold likely depends on the specific use case and evaluation criteria.
- Low Confidence: The generalizability of these findings to other base models, RL methods beyond ORPO, and different evaluator models remains uncertain. The specific value of Kendall's W as the optimal consistency metric versus alternatives is not thoroughly explored.

## Next Checks
1. **Evaluator Diversity Test**: Repeat the entire pipeline using multiple evaluators (GPT-4, Claude, human annotators) to determine if consistency patterns generalize across evaluators or are specific to GPT-4's ranking behavior.

2. **Threshold Optimization Study**: Systematically vary consistency thresholds (10%, 20%, 30%, 40%, 50%) and evaluate the non-linear relationship between dataset size, consistency, and downstream performance to identify optimal filtering parameters for different evaluation objectives.

3. **Cross-Architecture Generalization**: Apply the Repeat Ranking method to a different base model (e.g., Llama 3 or Mistral) and RL training approach (e.g., DPO instead of ORPO) to validate whether consistency-based filtering provides similar benefits across different model architectures and training paradigms.