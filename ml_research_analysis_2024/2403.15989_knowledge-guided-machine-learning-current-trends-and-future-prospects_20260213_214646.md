---
ver: rpa2
title: 'Knowledge-guided Machine Learning: Current Trends and Future Prospects'
arxiv_id: '2403.15989'
source_url: https://arxiv.org/abs/2403.15989
tags:
- data
- scientific
- learning
- arxiv
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper reviews the field of scientific knowledge-guided machine
  learning (KGML), which integrates scientific knowledge with machine learning to
  improve generalizability, scientific consistency, and explainability. It discusses
  three primary methods for incorporating scientific knowledge: knowledge-guided learning
  (e.g., using physics-informed loss functions), knowledge-guided architecture (e.g.,
  designing neural networks to respect physical laws), and knowledge-guided pretraining
  (e.g., initializing models using simulated data).'
---

# Knowledge-guided Machine Learning: Current Trends and Future Prospects

## Quick Facts
- arXiv ID: 2403.15989
- Source URL: https://arxiv.org/abs/2403.15989
- Reference count: 40
- Primary result: KGML integrates scientific knowledge with ML to improve generalizability, consistency, and explainability in environmental sciences

## Executive Summary
This paper provides a comprehensive review of scientific knowledge-guided machine learning (KGML), a field that bridges process-based modeling and data-driven machine learning. The authors present KGML as a methodology that leverages scientific knowledge (physical laws, differential equations, process-based models) to enhance machine learning performance across environmental science applications. The paper categorizes KGML approaches along three dimensions—type of scientific knowledge, form of integration, and method of incorporation—and identifies four primary use cases including forward modeling, inverse modeling, generative modeling, and downscaling.

## Method Summary
The paper synthesizes KGML methods through a framework that integrates scientific knowledge into machine learning in three primary ways: knowledge-guided learning (incorporating physical constraints into loss functions), knowledge-guided architecture (designing neural networks to respect physical laws), and knowledge-guided pretraining (initializing models using simulated data). The authors review applications across environmental sciences including lake temperature modeling, hydrology, climate downscaling, and ocean modeling. The methodology emphasizes the importance of balancing scientific consistency with predictive performance while addressing challenges like data scarcity and model interpretability.

## Key Results
- KGML methods can be categorized along three dimensions: type of scientific knowledge (perfect vs. imperfect), form of integration (process-centric vs. ML-centric), and method of incorporation (learning, architecture, pretraining)
- Four key use cases for KGML in environmental sciences: forward modeling (surrogate modeling for PDEs), inverse modeling (parameter estimation), generative modeling (synthetic data creation), and downscaling (converting coarse to fine-scale data)
- KGML offers opportunities for building foundation models in environmental science while addressing challenges of limited data, interpretability, and scientific consistency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scientific knowledge improves ML generalizability by guiding the loss function toward physically consistent solutions
- Mechanism: Physics-informed loss functions penalize deviations from known scientific laws, reducing overfitting to spurious data patterns
- Core assumption: The scientific laws used in the loss function accurately represent the underlying physical system
- Evidence anchors:
  - [abstract] "knowledge-guided learning (e.g., using physics-informed loss functions)"
  - [section 4.1] "Physics-guided neural network (PGNN) [79] was one of the first works to add physical constraints (in particular, the density-depth relationship) as loss functions to guide the training of neural networks to physically consistent solutions"
  - [corpus] Weak evidence - corpus papers focus on applications rather than loss function mechanics
- Break condition: If the scientific laws used are incomplete or incorrect, the loss function may mislead the model

### Mechanism 2
- Claim: Scientific knowledge improves ML explainability by constraining neural network architectures to reflect known physical processes
- Mechanism: Embedding scientific principles directly into network architecture (e.g., using density-depth physics in LSTM connections) forces the model to learn physically meaningful representations
- Core assumption: The scientific principles can be effectively translated into architectural constraints
- Evidence anchors:
  - [abstract] "knowledge-guided architecture (e.g., designing neural networks to respect physical laws)"
  - [section 4.2] "In the work by Daw et al., a physics-guided architecture of LSTM models (PGA-LSTM) was developed to explicitly encode the density-depth physics in the connections among LSTM nodes"
  - [corpus] No direct evidence found in corpus
- Break condition: If the architectural constraints are too restrictive, the model may fail to capture complex relationships not captured by the scientific knowledge

### Mechanism 3
- Claim: Scientific knowledge improves ML sample efficiency by providing informative initialization for model parameters
- Mechanism: Pretraining on simulated data generated by process-based models initializes ML parameters with physically consistent patterns, reducing the need for large observational datasets
- Core assumption: Simulated data from process-based models capture essential physical patterns relevant to the target system
- Evidence anchors:
  - [abstract] "knowledge-guided pretraining (e.g., initializing models using simulated data)"
  - [section 4.3] "Use of Simulated data: Simulated data generated by process-based models (even generic uncalibrated models) can be used to pre-train ML models"
  - [corpus] Weak evidence - corpus papers focus on applications rather than pretraining mechanics
- Break condition: If the simulated data are biased or unrepresentative, pretraining may harm rather than help model performance

## Foundational Learning

- Concept: Physics-informed neural networks (PINNs)
  - Why needed here: PINNs represent a foundational method for solving PDEs using ML while incorporating physical laws as constraints
  - Quick check question: How do PINNs incorporate physical laws into the loss function?

- Concept: Transfer learning and meta-learning
  - Why needed here: These concepts are crucial for understanding how knowledge from well-observed systems can be transferred to less-observed ones
  - Quick check question: What is the difference between transfer learning and meta-learning in the context of KGML?

- Concept: Generative modeling with physical constraints
  - Why needed here: Understanding how to incorporate physical constraints into generative models is important for creating synthetic data that respects physical laws
  - Quick check question: How can physical constraints be incorporated into generative adversarial networks?

## Architecture Onboarding

- Component map: Data preprocessing -> Scientific knowledge integration (loss functions/architecture constraints/pretraining) -> ML model training -> Evaluation with scientific consistency metrics
- Critical path: 1) Define scientific knowledge to incorporate, 2) Choose integration method (learning, architecture, pretraining), 3) Implement integration, 4) Train model, 5) Evaluate scientific consistency
- Design tradeoffs: More restrictive scientific constraints improve consistency but may reduce model flexibility; pretraining on simulated data improves sample efficiency but may introduce bias
- Failure signatures: Poor performance on out-of-distribution data, physically inconsistent predictions, overfitting to training data despite scientific constraints
- First 3 experiments:
  1. Implement a simple PINN for solving a known PDE (e.g., heat equation) and compare with numerical solution
  2. Add physics-guided loss function to a standard ML model (e.g., LSTM) for a simple physical system (e.g., spring-mass system)
  3. Pretrain a model on simulated data from a simple process-based model and fine-tune on limited real observations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can scientific knowledge be effectively integrated into foundation models for environmental science to improve their generalizability and interpretability?
- Basis in paper: [explicit] The paper discusses the potential of foundation models in environmental science but also highlights several challenges, including the lack of environmental data in existing foundation models, the difficulty of fine-tuning large models with limited observational data, and the limited interpretability of these models from a scientific perspective
- Why unresolved: While the paper mentions the potential of foundation models, it does not provide specific methods or approaches for integrating scientific knowledge into these models to address the identified challenges
- What evidence would resolve it: Empirical studies demonstrating the effectiveness of incorporating scientific knowledge into foundation models for environmental science, along with quantitative measures of improved generalizability and interpretability

### Open Question 2
- Question: How can KGML methods be developed to discover new scientific knowledge from data while ensuring consistency with existing scientific theories?
- Basis in paper: [explicit] The paper emphasizes the importance of KGML methods not only improving predictive accuracy but also discovering new scientific knowledge from data. However, it acknowledges the need for novel advances in KGML to achieve this goal, particularly in terms of defining explainability and causality in scientific applications
- Why unresolved: The paper does not provide specific methods or frameworks for KGML to discover new scientific knowledge while ensuring consistency with existing theories
- What evidence would resolve it: Case studies or empirical results demonstrating the successful application of KGML methods to discover new scientific knowledge, along with validation of the discovered knowledge against existing scientific theories

### Open Question 3
- Question: How can uncertainty quantification be improved in scientific applications of ML, particularly for large-scale models like foundation models?
- Basis in paper: [explicit] The paper highlights the need for better tools for uncertainty quantification in scientific applications of ML, including understanding the limits of large-scale models and predicting when ML models will fail
- Why unresolved: The paper does not provide specific methods or approaches for improving uncertainty quantification in scientific ML applications
- What evidence would resolve it: Development and validation of new uncertainty quantification techniques specifically designed for scientific ML applications, along with empirical studies demonstrating their effectiveness in predicting model failures and quantifying uncertainty

## Limitations

- The paper lacks systematic empirical validation comparing KGML methods against traditional ML approaches across multiple scenarios
- Evidence supporting the three core mechanisms relies heavily on individual case studies rather than comprehensive benchmarking
- The assertion that KGML can effectively bridge the gap between process-based and ML models lacks rigorous evaluation of potential limitations or failure modes

## Confidence

- High confidence: The categorization framework (type of scientific knowledge, form of integration, method of incorporation) provides a useful taxonomy for understanding KGML approaches
- Medium confidence: The claimed benefits of KGML (improved generalizability, scientific consistency, and explainability) are plausible but require more rigorous empirical validation
- Low confidence: The assertion that KGML can effectively bridge the gap between process-based and ML models lacks systematic evaluation

## Next Checks

1. **Benchmark KGML against traditional ML**: Implement a comparative study using multiple KGML methods (physics-informed loss, knowledge-guided architecture, pretraining) on a standardized environmental dataset, comparing performance against traditional ML approaches across metrics including accuracy, generalization, and physical consistency

2. **Stress test scientific knowledge integration**: Design experiments that systematically vary the quality and completeness of incorporated scientific knowledge to identify failure conditions where KGML degrades performance or produces physically inconsistent predictions

3. **Evaluate cross-domain transferability**: Test whether KGML models trained on one environmental system can be effectively transferred to similar but distinct systems, measuring the trade-off between scientific constraint benefits and model flexibility