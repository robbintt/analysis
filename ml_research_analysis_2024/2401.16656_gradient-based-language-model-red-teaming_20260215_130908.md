---
ver: rpa2
title: Gradient-Based Language Model Red Teaming
arxiv_id: '2401.16656'
source_url: https://arxiv.org/abs/2401.16656
tags:
- prompts
- prompt
- response
- classifier
- unsafe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Gradient-Based Red Teaming (GBRT) is a method for automatically
  generating diverse adversarial prompts that cause large language models (LMs) to
  produce unsafe responses. It works by optimizing learnable prompts via gradient
  descent to minimize the safety score of the LM's output as judged by a safety classifier.
---

# Gradient-Based Language Model Red Teaming

## Quick Facts
- arXiv ID: 2401.16656
- Source URL: https://arxiv.org/abs/2401.16656
- Reference count: 40
- Primary result: GBRT finds significantly more unsafe prompts than RL-based red teaming baseline

## Executive Summary
Gradient-Based Red Teaming (GBRT) is a novel method for automatically generating adversarial prompts that cause large language models to produce unsafe responses. The approach optimizes learnable prompt probabilities via gradient descent to minimize the safety score of the LM's output as judged by a safety classifier. Two key variants improve prompt realism: adding a realism loss that penalizes divergence from a pretrained LM, and fine-tuning a separate LM to generate the prompts. Experiments show GBRT outperforms a strong RL-based red teaming baseline, finding significantly more unsafe prompts even on LMs fine-tuned to be safer.

## Method Summary
GBRT works by initializing learnable prompt probabilities and iteratively optimizing them to minimize the safety score of the LM's response. The method uses Gumbel-softmax sampling to enable differentiable approximation of the autoregressive decoding process, allowing gradients to flow back to the prompt probabilities. To improve prompt coherence, GBRT introduces a realism loss that penalizes divergence from a pretrained LM, and a variant that fine-tunes a separate LM to generate the prompts. The approach is evaluated on multiple LMs and compared against a strong RL-based red teaming baseline, demonstrating superior performance in finding unsafe prompts.

## Key Results
- GBRT finds significantly more unsafe prompts than a strong RL-based red teaming baseline
- GBRT-ResponseOnly variant performs competitively while being more efficient
- Human evaluation confirms improved coherence of prompts with the realism loss
- GBRT remains effective on LMs fine-tuned to be safer

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient-based updates to the prompt probabilities directly optimize for safety classifier misclassification.
- Mechanism: By backpropagating through the frozen LM and safety classifier, the prompt probabilities are updated to minimize the safety score, which corresponds to generating responses that the classifier labels as unsafe.
- Core assumption: The safety classifier is differentiable and its gradients encode useful information about how to modify the prompt to induce unsafe responses.
- Evidence anchors:
  - [abstract] "GBRT is a form of prompt learning, trained by scoring an LM response with a safety classifier and then backpropagating through the frozen safety classifier and LM to update the prompt."
  - [section] "We backpropagate the gradients through this setup to update the prompt. Note that both the LM and safety classifier are frozen."
- Break condition: If the safety classifier is non-differentiable or provides gradients that are uninformative for prompt modification, gradient-based optimization would fail.

### Mechanism 2
- Claim: The Gumbel-softmax trick enables differentiable sampling, making the autoregressive decoding process amenable to gradient-based optimization.
- Mechanism: By approximating sampling with the Gumbel-softmax distribution, the discrete token selection steps become differentiable, allowing gradients to flow back to the prompt probabilities.
- Core assumption: The Gumbel-softmax approximation is sufficiently accurate to maintain meaningful gradients for prompt optimization.
- Evidence anchors:
  - [abstract] "To circumvent this issue, we use the Gumbel softmax trick (Jang et al., 2017; Maddison et al., 2017), which provides a differentiable approximation to categorical sampling."
  - [section] "In each decoding step, we sample from the model output logits using the Gumbel softmax distribution."
- Break condition: If the Gumbel-softmax approximation is too coarse, the gradients may not effectively guide prompt optimization.

### Mechanism 3
- Claim: Realism loss and fine-tuning a prompt model improve the coherence and diversity of generated prompts.
- Mechanism: Adding a realism loss encourages the prompt probabilities to align with a pretrained LM, producing more sensible prompts. Fine-tuning a separate LM to generate prompts leverages the LM's existing language knowledge for better quality.
- Core assumption: The pretrained LM's logits provide a good prior for realistic prompts, and fine-tuning can adapt this prior to generate adversarial prompts.
- Evidence anchors:
  - [abstract] "To improve the coherence of input prompts, we introduce two variants that add a realism loss and fine-tune a pretrained model to generate the prompts instead of learning the prompts directly."
  - [section] "To encourage finding more sensible prompts, we introduce an additional realism loss regularization term that penalizes the divergence between the prompt distribution and a pretrained language model."
- Break condition: If the realism loss is too strong, it might prevent the method from finding prompts that trigger unsafe responses. If fine-tuning is not effective, the generated prompts may not be diverse or coherent enough.

## Foundational Learning

- Concept: Differentiable sampling using the Gumbel-softmax trick.
  - Why needed here: The autoregressive sampling steps in the LM are non-differentiable, preventing gradient-based optimization of the prompt.
  - Quick check question: How does the Gumbel-softmax trick approximate sampling while maintaining differentiability?

- Concept: Gradient-based optimization for adversarial prompt generation.
  - Why needed here: Directly optimizing the prompt to minimize the safety score requires differentiable access to the LM and classifier outputs.
  - Quick check question: Why is backpropagation through the frozen LM and safety classifier necessary for prompt optimization?

- Concept: Realism regularization and prompt model fine-tuning.
  - Why needed here: Unconstrained optimization can produce nonsensical prompts; realism loss and fine-tuning improve coherence and diversity.
  - Quick check question: How do realism loss and fine-tuning a prompt model address the issue of generating realistic adversarial prompts?

## Architecture Onboarding

- Component map:
  - Learnable prompt probabilities (x) -> Language model (pLM) -> Safety classifier (psafe)
  - Gumbel-softmax sampling for differentiable approximation
  - Realism loss (optional)
  - Prompt model (optional, for fine-tuning)

- Critical path:
  1. Initialize prompt probabilities (or prompt model).
  2. Sample from prompt probabilities using Gumbel-softmax.
  3. Feed sampled prompt to LM and decode response using Gumbel-softmax at each step.
  4. Score response with safety classifier.
  5. Backpropagate gradients to update prompt probabilities (or prompt model).
  6. Repeat until desired number of adversarial prompts is generated.

- Design tradeoffs:
  - Direct prompt optimization vs. fine-tuning a prompt model: Direct optimization is faster but may produce less coherent prompts. Fine-tuning is slower but leverages LM knowledge for better quality.
  - Realism loss strength: Stronger realism loss improves coherence but may reduce effectiveness in finding unsafe prompts.
  - Gumbel-softmax temperature: Lower temperature gives more deterministic sampling but may reduce gradient quality.

- Failure signatures:
  - Prompts that do not trigger unsafe responses: Could indicate ineffective optimization, poor safety classifier gradients, or insufficient training.
  - Nonsensical or repetitive prompts: May suggest the need for stronger realism loss or better fine-tuning.
  - Slow convergence or poor performance: Could be due to suboptimal hyperparameters (learning rate, Gumbel-softmax parameters) or insufficient training iterations.

- First 3 experiments:
  1. Run GBRT with default hyperparameters on a simple LM and safety classifier to verify basic functionality.
  2. Compare the effectiveness of GBRT with and without realism loss to assess the impact on prompt coherence and safety violation rate.
  3. Evaluate the performance of GBRT-ResponseOnly vs. GBRT on a model fine-tuned to be safer to test robustness.

## Open Questions the Paper Calls Out
None

## Limitations
- The effectiveness of GBRT relies heavily on the quality and coverage of the safety classifier, which may not detect all types of unsafe responses or may have blind spots that GBRT cannot exploit
- The Gumbel-softmax approximation, while enabling gradient-based optimization, may not perfectly capture the true sampling distribution, potentially limiting the method's effectiveness
- GBRT is tested primarily on English language models and may not generalize as effectively to other languages or specialized domains

## Confidence
- **High Confidence**: The core gradient-based optimization mechanism and its ability to find more unsafe prompts than RL baselines
- **Medium Confidence**: The effectiveness of the realism loss and prompt model fine-tuning in improving prompt coherence, as human evaluation is limited
- **Medium Confidence**: The claim that GBRT finds prompts that trigger unsafe responses in safer fine-tuned models, as this is based on comparison with a single RL baseline

## Next Checks
1. Test GBRT on multiple safety classifiers with different architectures and training data to assess robustness to classifier variations
2. Conduct extensive human evaluation with diverse annotators to validate both the safety of generated prompts and the realism of responses
3. Evaluate GBRT's performance on multilingual models and specialized domain-specific language models to test generalization beyond the tested English models