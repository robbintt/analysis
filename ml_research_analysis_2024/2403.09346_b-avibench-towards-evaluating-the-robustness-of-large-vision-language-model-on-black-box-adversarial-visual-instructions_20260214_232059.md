---
ver: rpa2
title: 'B-AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model
  on Black-box Adversarial Visual-Instructions'
arxiv_id: '2403.09346'
source_url: https://arxiv.org/abs/2403.09346
tags:
- avis
- attacks
- lvlms
- image
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces B-AVIBench, a benchmark designed to evaluate
  the robustness of Large Vision-Language Models (LVLMs) against adversarial visual
  instructions (AVIs). The benchmark addresses the lack of comprehensive evaluation
  methods for LVLMs facing various attacks on their text and image inputs, as well
  as content bias issues.
---

# B-AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Black-box Adversarial Visual-Instructions

## Quick Facts
- arXiv ID: 2403.09346
- Source URL: https://arxiv.org/abs/2403.09346
- Reference count: 40
- Key outcome: Introduces B-AVIBench benchmark evaluating LVLM robustness against adversarial visual instructions across 316K test cases

## Executive Summary
This paper introduces B-AVIBench, a comprehensive benchmark designed to evaluate the robustness of Large Vision-Language Models (LVLMs) against adversarial visual instructions (AVIs). The benchmark addresses a critical gap in LVLM evaluation by providing systematic testing against image-based, text-based, and content bias attacks across multiple multimodal capabilities. The authors evaluate 14 open-source LVLMs and two closed-source models (GeminiProVision and GPT-4V), demonstrating that even advanced models exhibit notable vulnerabilities to various attack types.

## Method Summary
The B-AVIBench benchmark generates 316K AVIs from the Tiny LVLM-eHub dataset using three attack types: image-based (corruptions and optimized attacks), text-based (character/word/sentence/semantic levels), and content bias attacks targeting demographic dimensions. The benchmark employs output-agnostic black-box attack methods that require only text responses, enabling evaluation of closed-source models. Key metrics include Average Score Drop Rate (ASDR) for corruptions, Attack Success Rate (ASR) and Average Euclidean Distance (AED) for optimized attacks, and accuracy for content bias detection.

## Key Results
- Decision-based optimized image attacks effectively degrade LVLM performance across all tested models
- Text-based attacks pose significant challenges, particularly semantic-level attacks
- Even advanced closed-source models (GPT-4V, GeminiProVision) exhibit notable content biases across gender, race, and cultural dimensions
- Weak correlations exist between general model performance and robustness to most attack types, except for stronger correlations with decision-based optimized image attacks and content bias attacks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The benchmark evaluates LVLM robustness using output-agnostic black-box attacks that only require text responses.
- Mechanism: By using attack methods that don't depend on output probability distributions, the benchmark can evaluate closed-source models like GPT-4V and GeminiProVision.
- Core assumption: Text responses alone contain sufficient information to determine attack success across all task types.
- Evidence anchors: [abstract]: "We adapt LVLM-agnostic and output probability distributions-agnostic black-box attacks to construct AVIs."

### Mechanism 2
- Claim: Image corruptions serve as effective proxy attacks because they degrade performance predictably across different LVLM architectures.
- Mechanism: By applying standardized corruption techniques (19 types across 3 severity levels), the benchmark can compare robustness across models regardless of their internal architecture.
- Core assumption: All LVLMs process corrupted images in ways that produce comparable performance degradation patterns.
- Evidence anchors: [section]: "We assess LVLM performance under these different corruption categories...all LVLMs are with average ASDR values consistently below 20%."

### Mechanism 3
- Claim: Content bias AVIs reveal inherent model biases by constructing controlled scenarios across multiple demographic dimensions.
- Mechanism: By creating 55K controlled visual-instruction pairs targeting gender, race, culture, and safety categories, the benchmark quantifies bias exposure systematically.
- Core assumption: Controlled question-answer pairs can reliably expose bias patterns without being influenced by model-specific safety filters.
- Evidence anchors: [section]: "Our analysis focuses on unsafe information, cultural bias, racial bias, and gender bias present within the LVLMs."

## Foundational Learning

- Concept: Adversarial attack methodology
  - Why needed here: Understanding how different attack types (white-box, black-box, decision-based) work is crucial for interpreting benchmark results
  - Quick check question: Why would output probability distribution-agnostic attacks be necessary for evaluating closed-source LVLMs?

- Concept: Multimodal capability evaluation
  - Why needed here: The benchmark covers five multimodal capabilities (visual perception, knowledge acquisition, reasoning, commonsense, hallucination) that must be understood to interpret results
  - Quick check question: How does the evaluation metric differ between visual perception tasks and visual reasoning tasks?

- Concept: Bias measurement in AI systems
  - Why needed here: Content bias AVIs require understanding how to quantify and measure bias across different demographic dimensions
  - Quick check question: What makes content bias attacks different from traditional adversarial attacks in terms of their objectives?

## Architecture Onboarding

- Component map: Image processing pipeline -> Text attack generation system -> Content bias dataset construction module -> LVLM evaluation framework -> Result aggregation and analysis tools

- Critical path: 1. Generate base dataset (Tiny LVLM-eHub) 2. Apply attack transformations to create AVIs 3. Run evaluations across all LVLMs 4. Aggregate results by attack type and capability 5. Generate comparative analysis

- Design tradeoffs:
  - Black-box vs white-box attacks: Black-box enables closed-source evaluation but may miss subtle vulnerabilities
  - Task coverage vs depth: 260K AVIs provide broad coverage but may miss task-specific nuances
  - Automation vs manual validation: Automated generation enables scale but requires validation of bias scenarios

- Failure signatures:
  - Low ASDR values across all models suggest attack generation issues
  - Inconsistent results between open and closed-source models indicate evaluation bias
  - High variance in content bias results may indicate dataset construction problems

- First 3 experiments:
  1. Run single corruption type (e.g., Gaussian noise) across all LVLMs to validate basic functionality
  2. Test one text attack method on a subset of models to verify attack implementation
  3. Evaluate content bias AVIs on a single model to validate bias detection logic

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different visual encoders (e.g., CLIP vs. EVA) affect LVLM robustness to adversarial visual-instructions?
- Basis in paper: [inferred] The paper mentions various models using different visual encoders but does not analyze their impact on robustness.
- Why unresolved: The paper controls for model architecture and training data volume but does not isolate the effect of visual encoder choice on robustness to AVIs.
- What evidence would resolve it: A controlled experiment comparing LVLMs with identical LLM and adapter configurations but different visual encoders on the same AVI benchmark.

### Open Question 2
- Question: Are there specific types of content bias (e.g., racial vs. gender) that are more challenging for LVLMs to detect and respond to appropriately?
- Basis in paper: [explicit] The paper evaluates multiple content bias types but doesn't analyze which types are most challenging.
- Why unresolved: While the paper presents accuracy scores for each content bias type, it doesn't perform a comparative analysis of difficulty across bias categories.
- What evidence would resolve it: A statistical analysis of detection accuracy across different content bias categories, identifying which types consistently pose the greatest challenge for LVLMs.

### Open Question 3
- Question: What is the relationship between a model's general performance (pre-attack scores) and its robustness to specific types of adversarial attacks?
- Basis in paper: [explicit] The paper includes a figure showing weak correlations between pre-attack scores and robustness to image corruptions, but stronger correlations for decision-based optimized image attacks and content bias attacks.
- Why unresolved: The paper presents these correlations but doesn't explore the underlying reasons or provide a comprehensive analysis across all attack types and multimodal capabilities.
- What evidence would resolve it: A detailed correlation analysis across all attack types, multimodal capabilities, and model architectures to identify patterns in the relationship between general performance and attack robustness.

## Limitations
- Output-agnostic attack efficacy uncertainty: The claim that text-only responses suffice for black-box attack evaluation lacks strong validation evidence
- Cross-architecture corruption comparability: Assumption that standardized corruption techniques produce comparable degradation across different LVLM architectures lacks direct validation
- Bias detection methodology: Content bias evaluation framework doesn't fully address potential interference from model-specific safety filters

## Confidence
- Claim that B-AVIBench comprehensively evaluates LVLM robustness: High confidence
- Claim that decision-based optimized image attacks are effective: High confidence  
- Claim that text-based attacks pose significant challenges: Medium confidence
- Claim that closed-source models exhibit notable content biases: Medium confidence

## Next Checks
1. Test whether attack success rates differ significantly when using probability distribution information versus text-only responses for a subset of models where both are available
2. Evaluate whether specific corruption types show dramatically different effectiveness across different LVLM architectural families (e.g., vision transformer vs CNN-based models)
3. Conduct controlled experiments to determine whether model-specific safety filters artificially suppress bias detection rates in the content bias evaluation