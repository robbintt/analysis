---
ver: rpa2
title: "D\xF3lares or Dollars? Unraveling the Bilingual Prowess of Financial LLMs\
  \ Between Spanish and English"
arxiv_id: '2402.07405'
source_url: https://arxiv.org/abs/2402.07405
tags:
- financial
- spanish
- english
- datasets
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses the scarcity of Spanish financial NLP models\
  \ by introducing Tois\xF3n de Oro, the first bilingual framework for Spanish-English\
  \ financial language models. The authors created FIT-ES, a 144K-sample bilingual\
  \ instruction dataset, and FinMA-ES, a fine-tuned 7B LLM for Spanish-English financial\
  \ tasks."
---

# Dólares or Dollars? Unraveling the Bilingual Prowess of Financial LLMs Between Spanish and English

## Quick Facts
- arXiv ID: 2402.07405
- Source URL: https://arxiv.org/abs/2402.07405
- Reference count: 40
- Introduces Toisón de Oro, the first bilingual framework for Spanish-English financial language models

## Executive Summary
This work addresses the scarcity of Spanish financial NLP models by introducing Toisón de Oro, the first bilingual framework for Spanish-English financial language models. The authors created FIT-ES, a 144K-sample bilingual instruction dataset, and FinMA-ES, a fine-tuned 7B LLM for Spanish-English financial tasks. They also established FLARE-ES, a 21-dataset benchmark covering 9 tasks for cross-lingual evaluation. FinMA-ES models outperform GPT-4 in Spanish financial tasks, demonstrating that instruction tuning with both languages improves performance. The ablation study reveals that bilingual fine-tuning unexpectedly boosts performance on high-resource language datasets, highlighting positive cross-linguistic transfer.

## Method Summary
The authors developed a comprehensive bilingual framework consisting of three main components: FIT-ES (144K bilingual instruction dataset), FinMA-ES (7B fine-tuned LLM), and FLARE-ES (21-dataset benchmark). The instruction dataset was created through translation and synthesis of financial data in both languages. The model was fine-tuned using parameter-efficient techniques on the bilingual dataset. The benchmark covers 9 financial tasks across 21 datasets for cross-lingual evaluation. The methodology emphasizes cross-linguistic transfer learning and parameter-efficient fine-tuning to address resource constraints.

## Key Results
- FinMA-ES models outperform GPT-4 in Spanish financial tasks
- Bilingual fine-tuning unexpectedly boosts performance on high-resource language datasets
- Demonstrated positive cross-linguistic transfer in financial NLP tasks

## Why This Works (Mechanism)
The framework leverages cross-linguistic transfer learning where knowledge gained from one language enhances performance in another. The bilingual instruction tuning creates shared representations between Spanish and English financial concepts, allowing the model to transfer patterns and structures across languages. The parameter-efficient fine-tuning approach maximizes performance gains while minimizing computational requirements, making the system practical for real-world deployment.

## Foundational Learning
- **Cross-lingual transfer learning**: Why needed - enables knowledge sharing between languages; Quick check - compare performance on monolingual vs bilingual fine-tuning
- **Instruction tuning**: Why needed - aligns model behavior with specific task requirements; Quick check - evaluate instruction-following accuracy across languages
- **Parameter-efficient fine-tuning**: Why needed - reduces computational costs while maintaining performance; Quick check - measure parameter updates vs performance gains
- **Financial domain adaptation**: Why needed - financial terminology requires specialized understanding; Quick check - domain-specific task accuracy
- **Bilingual dataset construction**: Why needed - ensures balanced representation of both languages; Quick check - language distribution analysis
- **Benchmark design for cross-lingual tasks**: Why needed - enables fair comparison across language pairs; Quick check - task coverage and difficulty calibration

## Architecture Onboarding
Component map: Financial data -> FIT-ES (144K samples) -> FinMA-ES (7B LLM) -> FLARE-ES (21 datasets, 9 tasks) -> Performance evaluation

Critical path: Bilingual instruction dataset creation → Parameter-efficient fine-tuning → Cross-lingual benchmark evaluation → Performance comparison

Design tradeoffs: 7B parameter limit vs. computational efficiency; Bilingual vs monolingual fine-tuning; Synthetic vs real financial data balance

Failure signatures: Performance degradation on out-of-domain financial tasks; Language-specific bias in translations; Overfitting to bilingual patterns at expense of single-language proficiency

First experiments: 1) Ablation study comparing bilingual vs monolingual fine-tuning; 2) Cross-lingual transfer evaluation on unseen financial tasks; 3) Comparison with GPT-4 on Spanish financial benchmarks

## Open Questions the Paper Calls Out
The primary uncertainty centers on the generalizability of the observed cross-linguistic transfer, as the study focuses exclusively on Spanish-English pairs in the financial domain. The unexpected performance gains on high-resource language datasets during bilingual fine-tuning warrant further investigation to determine whether this phenomenon extends beyond the specific language pair and domain examined.

## Limitations
- Computational constraint of 7B parameters limits complexity capture
- Evaluation scope focuses on task-specific metrics without bias analysis
- Narrow focus on Spanish-English pair without testing broader language generalizations

## Confidence
- High confidence in methodology for creating bilingual instruction datasets and benchmarks
- Medium confidence in cross-lingual transfer findings, pending replication with additional language pairs
- Medium confidence in performance comparisons with GPT-4, given controlled evaluation conditions
- Low confidence in real-world deployment effectiveness without additional testing scenarios

## Next Checks
1. Replicate the bilingual fine-tuning experiments with additional language pairs (e.g., French-English, German-English) in financial contexts to verify the cross-linguistic transfer phenomenon
2. Conduct extensive bias and fairness analysis across different financial subdomains and user demographics to ensure model robustness
3. Perform long-context evaluation tests with financial documents requiring extended reasoning capabilities to assess performance limitations