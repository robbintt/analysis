---
ver: rpa2
title: 'LLMForecaster: Improving Seasonal Event Forecasts with Unstructured Textual
  Data'
arxiv_id: '2412.02525'
source_url: https://arxiv.org/abs/2412.02525
tags:
- demand
- time
- forecast
- forecasting
- products
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLMForecaster fine-tunes large language models to improve demand
  forecasts by incorporating unstructured product information like descriptions and
  titles. It uses the LLM to generate scaling factors that adjust forecasts from an
  existing MQ-Transformer model, particularly for products with holiday-driven demand
  surges.
---

# LLMForecaster: Improving Seasonal Event Forecasts with Unstructured Textual Data

## Quick Facts
- arXiv ID: 2412.02525
- Source URL: https://arxiv.org/abs/2412.02525
- Reference count: 32
- Primary result: LLM-based post-processing improves retail demand forecasts for holiday-driven products by 10-105 basis points

## Executive Summary
LLMForecaster addresses a critical gap in time series forecasting by incorporating unstructured product information (titles, descriptions) to improve demand predictions for seasonal events. The approach fine-tunes a large language model to predict scaling factors that correct biases in existing MQ-Transformer forecasts, particularly for holiday-driven demand surges. Through industry-scale retail experiments across five major holidays, LLMForecaster demonstrates statistically significant improvements in forecast accuracy, capturing seasonal patterns that traditional models miss.

## Method Summary
LLMForecaster operates as a post-processor to existing time series forecasts, using fine-tuned LLMs to generate scaling factors based on product textual information. The method employs LoRA fine-tuning on MPT-7b-Instruct to predict log(yi,t/fi,t), where yi,t is actual demand and fi,t is the baseline forecast. An innovative holiday-encoding prompt provides temporal context about holiday proximity, crucial for moving holidays like Easter. The approach processes product titles, descriptions, and numerical features through the LLM, concatenates embeddings with numerical features, and uses an MLP head to predict scaling factors that adjust the original forecasts.

## Key Results
- Statistically significant improvements of 10-105 basis points in weighted p50 quantile loss across five holidays
- Non-fine-tuned LLM versions showed no improvement, confirming the necessity of LoRA fine-tuning
- Successfully captured localized holiday demand surges that baseline MQ-Transformer models missed
- Performance varied by holiday, with Easter and Mother's Day showing the largest improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMForecaster improves demand forecasts by fine-tuning LLMs to predict scaling factors that correct biases in existing MQ-Transformer models for holiday-driven demand surges.
- Mechanism: The LLM is fine-tuned using LoRA to predict log(yi,t/fi,t), where yi,t is actual demand and fi,t is the MQ-Transformer forecast. This scaling factor is then used to adjust the original forecast: f*i,t = e^λ(xi,t,text,xi,t,num) * fi,t.
- Core assumption: The scaling factor can capture systematic forecast errors related to holiday-driven demand patterns that the MQ-Transformer model misses.
- Evidence anchors: [abstract] "In an industry-scale retail application, we demonstrate that our technique yields statistically significantly forecast improvements across several sets of products subject to holiday-driven demand surges."

### Mechanism 2
- Claim: LLMForecaster successfully captures localized holiday demand surges by incorporating unstructured product information that the MQ-Transformer model cannot learn from time series alone.
- Mechanism: The LLM processes product titles, descriptions, and other unstructured text to identify holiday relevance, then adjusts forecasts accordingly using LoRA fine-tuning.
- Core assumption: Product descriptions contain sufficient information about holiday relevance to improve demand forecasts.
- Evidence anchors: [section] "In Figure 1, we illustrate this problem... For both groups of products, we see that the existing forecasting model appropriately anticipates surges in demand during the holiday season (between Black Friday and Christmas)... By contrast, the model fails to anticipate surges in demand during the Mother's Day and Easter holiday periods themselves."

### Mechanism 3
- Claim: The holiday-encoding prompt significantly improves LLMForecaster's ability to capture moving holidays by providing explicit temporal context about holiday proximity.
- Mechanism: The prompt includes information about when holidays occur relative to the forecast date, helping the LLM understand temporal patterns for holidays like Easter that vary by date each year.
- Core assumption: LLMs benefit from explicit temporal context about holiday timing to improve forecast accuracy.
- Evidence anchors: [section] "To improve the LLMForecaster's ability to capture holiday-driven demand patterns, we have implemented a 'Holiday-Encoding Prompt'. This prompt provides the model with contextual information about the temporal relationship between the target forecast date and surrounding holidays."

## Foundational Learning

- Concept: Time series forecasting with exogenous features
  - Why needed here: The MQ-Transformer model already incorporates numerical and categorical features, but lacks unstructured text information that LLMForecaster adds
  - Quick check question: What are the limitations of traditional time series forecasting models when dealing with unstructured information like product descriptions?

- Concept: Fine-tuning with Low-Rank Adaptation (LoRA)
  - Why needed here: LoRA allows efficient fine-tuning of large language models for the specific task of predicting demand scaling factors without full model retraining
  - Quick check question: How does LoRA differ from full fine-tuning in terms of parameter efficiency and training time?

- Concept: Quantile loss and forecast calibration
  - Why needed here: The evaluation uses weighted p50 quantile loss, and the approach aims to improve forecast calibration by adjusting existing predictions
  - Quick check question: What is the difference between point forecasts and quantile forecasts, and why is quantile loss used here?

## Architecture Onboarding

- Component map: Product text and numerical features → LLM with LoRA fine-tuning → MLP head → Scaling factor prediction → Adjusted forecast output. The LLM processes unstructured text (product titles, descriptions) and numerical features (price, forecast values) through prompts, generates embeddings, which are then concatenated with numerical features and passed through an MLP to predict the scaling factor.
- Critical path: The LLMForecaster takes the MQ-Transformer forecast as input, processes product information through the LLM to generate scaling factors, then applies these factors to create the final adjusted forecast.
- Design tradeoffs: Using LLM as a post-processor preserves the existing MQ-Transformer model while adding unstructured information processing, but requires maintaining two models and may not capture all forecast errors. The LoRA approach balances fine-tuning effectiveness with computational efficiency.
- Failure signatures: No improvement over baseline (as seen with non-fine-tuned emb model), statistical insignificance in improvements, overfitting to training data, inability to generalize to new products or holidays.
- First 3 experiments:
  1. Implement basic LLMForecaster with fixed prompts and no LoRA fine-tuning to establish baseline performance
  2. Add LoRA fine-tuning with varying ranks (r16, r64, r128, r256) to find optimal balance between performance and efficiency
  3. Implement and test holiday-encoding prompts to improve handling of moving holidays like Easter

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the LLMForecaster's performance scale with the number of historical years of data available for products?
- Basis in paper: [inferred] The paper mentions that many products have only one year of sales history, and suggests that training with data spanning multiple years could improve predictions, particularly for Valentine's Day products.
- Why unresolved: The experiments were conducted on a dataset spanning 88 weeks (approximately 2 years), which may not be sufficient to fully evaluate the model's performance with longer historical data.
- What evidence would resolve it: Experiments comparing LLMForecaster performance using datasets with varying numbers of historical years (e.g., 1, 3, 5, 10 years) for the same products.

### Open Question 2
- Question: What is the optimal balance between the base LLM size and the LoRA rank size for achieving the best forecast accuracy?
- Basis in paper: [explicit] The paper tested different LoRA ranks (r16, r64, r128, r256) but used a fixed base model (MPT7b-Instruct).
- Why unresolved: The paper only varied the LoRA rank while keeping the base model constant, leaving the interaction between base model size and LoRA rank unexplored.
- What evidence would resolve it: Comparative experiments testing different base LLM sizes (e.g., 7B, 13B, 30B parameters) with various LoRA ranks to identify the optimal combination for forecast accuracy.

### Open Question 3
- Question: How does the LLMForecaster handle products with no textual description or title information available?
- Basis in paper: [inferred] The methodology relies on incorporating product titles and descriptions, but doesn't address cases where this information might be missing or incomplete.
- Why unresolved: The paper demonstrates success with products that have complete textual information but doesn't explore scenarios where this information is absent or sparse.
- What evidence would resolve it: Experiments comparing LLMForecaster performance on products with complete vs. incomplete textual information, or a proposed method for handling missing textual data.

### Open Question 4
- Question: How does the LLMForecaster's performance change when incorporating multimodal inputs such as product images alongside textual descriptions?
- Basis in paper: [explicit] The conclusion section mentions exploring multimodal inputs like product images as future work.
- Why unresolved: The current implementation only uses textual information, and the potential benefits of adding visual information remain untested.
- What evidence would resolve it: Experiments comparing LLMForecaster performance with and without incorporating product images, measuring the impact on forecast accuracy across different product categories.

## Limitations

- Dataset Generalization: Results demonstrated on single retail dataset; effectiveness across different domains and regions untested
- Model Complexity Trade-offs: Two-stage approach adds complexity without comprehensive cost-benefit analysis compared to alternatives
- LLM Dependency: Performance heavily depends on LLM's ability to extract patterns from text; paper doesn't explore different LLM architectures

## Confidence

**High Confidence**: The LoRA fine-tuning approach works as described, and the methodology for generating scaling factors is sound. The statistical significance testing and evaluation metrics are appropriate.

**Medium Confidence**: The claim that unstructured product information significantly improves holiday demand forecasts. While results show improvements, the paper doesn't explore whether these improvements would persist with different LLM architectures or with alternative methods for incorporating text information.

**Low Confidence**: The assertion that this approach is superior to all other methods for incorporating unstructured information in time series forecasting. The paper lacks comprehensive comparisons with alternative approaches like feature engineering, attention mechanisms, or other post-processing methods.

## Next Checks

1. **Cross-Domain Validation**: Test LLMForecaster on retail datasets from different regions, product categories, and time periods to assess generalization beyond the original dataset. Focus on capturing performance consistency across domains with varying holiday patterns and product characteristics.

2. **Alternative Architecture Comparison**: Implement and compare LLMForecaster against alternative approaches: direct fine-tuning of the MQ-Transformer with text features, attention-based mechanisms for incorporating unstructured information, and simpler feature engineering approaches. Measure both forecast accuracy and computational efficiency.

3. **Ablation Study on LLM Components**: Systematically test the contribution of different LLM components by varying: LLM size (from 7B to 1B parameters), fine-tuning method (LoRA vs full fine-tuning), and prompt engineering approaches. This would clarify whether the benefits come from the LLM architecture itself or from the specific fine-tuning and prompting strategy.