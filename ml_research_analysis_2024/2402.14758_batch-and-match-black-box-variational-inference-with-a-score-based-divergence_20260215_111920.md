---
ver: rpa2
title: 'Batch and match: black-box variational inference with a score-based divergence'
arxiv_id: '2402.14758'
source_url: https://arxiv.org/abs/2402.14758
tags:
- batch
- variational
- divergence
- gradient
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Batch and Match (BaM), a new black-box variational
  inference (BBVI) algorithm that optimizes a score-based divergence instead of the
  traditional ELBO. BaM alternates between sampling batches from the current variational
  approximation and updating the parameters via a closed-form proximal update for
  Gaussian variational families.
---

# Batch and match: black-box variational inference with a score-based divergence

## Quick Facts
- arXiv ID: 2402.14758
- Source URL: https://arxiv.org/abs/2402.14758
- Reference count: 0
- Primary result: Introduces BaM, a black-box variational inference algorithm that optimizes a score-based divergence, achieving faster convergence than state-of-the-art methods on Gaussian and non-Gaussian targets.

## Executive Summary
Batch and Match (BaM) introduces a novel black-box variational inference algorithm that optimizes a score-based divergence instead of the traditional ELBO. The method alternates between sampling batches from the current variational approximation and updating parameters via a closed-form proximal update for Gaussian variational families. BaM demonstrates exponential convergence to target means and covariances for Gaussian targets in the infinite batch size limit, while empirically outperforming state-of-the-art BBVI methods on both synthetic and real-world problems.

## Method Summary
BaM is a black-box variational inference algorithm that optimizes a score-based divergence through alternating "batch" and "match" steps. The algorithm samples batches from the current Gaussian approximation, evaluates target score gradients at those points, and updates variational parameters via closed-form proximal updates. For Gaussian variational families, the covariance update involves solving a quadratic matrix equation, while the mean update has an analytical solution. The method uses different learning rate schedules depending on the target distribution type: constant rates for Gaussian targets and decaying rates for non-Gaussian targets.

## Key Results
- BaM converges exponentially fast to target mean and covariance for Gaussian targets in the infinite batch size limit
- Outperforms ADVI and GSM on synthetic Gaussian targets with dimensions D=4,16,64,256
- Achieves faster convergence on non-Gaussian sinh-arcsinh targets with varying skew parameters
- Demonstrates superior performance on Bayesian hierarchical models and deep generative models for CIFAR-10 image reconstruction

## Why This Works (Mechanism)

### Mechanism 1
BaM's alternating batch-match updates create stable convergence by matching score gradients at sampled points while controlling deviation from current estimate. The algorithm draws a batch from the current Gaussian approximation, computes target score gradients at those points, and updates parameters via a closed-form proximal step that minimizes a regularized score-based divergence. The core assumption is that the score-based divergence can be estimated unbiasedly from samples and optimized analytically for Gaussian families. This mechanism breaks when target distributions are highly non-Gaussian, as the closed-form update may no longer approximate the optimal solution well.

### Mechanism 2
Affine invariance ensures consistent behavior under coordinate transformations, avoiding ill-conditioning issues. The weighted norm in the divergence definition, mediated by Cov(q), makes the divergence invariant to affine transformations of the input space. The core assumption is that affine invariance contributes to algorithmic stability. This mechanism fails when the covariance estimate becomes singular or near-singular, as the weighted norm may amplify numerical errors.

### Mechanism 3
The proximal regularization term ensures updates don't overshoot, maintaining stable convergence even with large learning rates. The KL divergence regularization in the objective function penalizes large deviations from the current iterate, effectively controlling the step size. The core assumption is that KL regularization provides sufficient stability for proximal updates to converge. This mechanism breaks if the regularization parameter is set too small, causing instability, or too large, dramatically slowing convergence.

## Foundational Learning

- Concept: Score matching and Fisher divergences
  - Why needed here: The algorithm is built on optimizing a score-based divergence, which requires understanding how to match gradients of log densities.
  - Quick check question: What is the difference between the Fisher divergence and the score-based divergence used in BaM?

- Concept: Quadratic matrix equations and their solutions
  - Why needed here: The covariance update involves solving a quadratic matrix equation, which is central to the algorithm's efficiency.
  - Quick check question: How does the solution to the quadratic matrix equation XUX + X = V relate to the covariance update in BaM?

- Concept: Proximal point algorithms and their convergence properties
  - Why needed here: BaM uses a stochastic proximal point approach, requiring understanding of how regularization affects convergence.
  - Quick check question: How does the KL regularization term in BaM's objective function relate to the stability of proximal point methods?

## Architecture Onboarding

- Component map: Batch sampling module -> Score evaluation module -> Statistic computation module -> Update solver module -> Mean update module -> Regularization controller
- Critical path:
  1. Sample batch from current Gaussian approximation
  2. Evaluate target scores at sampled points
  3. Compute batch statistics (means, covariances)
  4. Solve quadratic matrix equation for covariance update
  5. Compute mean update using closed-form expression
  6. Check convergence criteria
- Design tradeoffs:
  - Batch size vs. computational cost: Larger batches improve estimate quality but increase computation
  - Learning rate schedule: Constant rates work well for Gaussian targets; decaying rates needed for non-Gaussian
  - Full covariance vs. diagonal approximation: Full covariance is more expressive but computationally heavier
- Failure signatures:
  - Covariance matrix becoming singular or near-singular
  - Mean estimates diverging or oscillating
  - Convergence stalling despite decreasing KL divergence
  - Numerical instability in solving quadratic matrix equation
- First 3 experiments:
  1. Verify closed-form updates: Test BaM on a simple Gaussian target with known solution, comparing convergence to ground truth
  2. Batch size sensitivity: Run BaM on a Gaussian target with varying batch sizes, measuring convergence speed and stability
  3. Non-Gaussian robustness: Test BaM on a sinh-arcsinh distribution with varying skew, comparing to ADVI and GSM performance

## Open Questions the Paper Calls Out

### Open Question 1
How does the convergence rate of BaM change for non-Gaussian target distributions with heavier tails (e.g., sinh-arcsinh with tau > 1.7) compared to Gaussian targets? The paper evaluates BaM on sinh-arcsinh distributions with varying tail weights but does not provide theoretical analysis of convergence rates for non-Gaussian targets. This remains unresolved because the paper only offers empirical observations without rigorous convergence proofs.

### Open Question 2
What is the impact of batch size on the stability and accuracy of BaM for high-dimensional non-Gaussian targets (e.g., deep generative models with latent dimensions > 256)? The paper shows BaM benefits from larger batch sizes but doesn't explore limits for very high-dimensional targets or potential instability with smaller batch sizes. This remains unresolved because experiments focus on latent dimensions up to 256 without discussing behavior in the limit of very large or small batch sizes.

### Open Question 3
How does the choice of the regularization parameter λt affect the convergence of BaM for different types of target distributions (Gaussian vs. non-Gaussian)? The paper uses different λt schedules and observes optimal schedules depend on target distribution, but doesn't provide theoretical framework for choosing optimal schedules. This remains unresolved because empirical results are based on limited experiments without comprehensive comparison across diverse target distributions.

## Limitations

- The empirical evaluation lacks systematic hyperparameter sensitivity analysis, with effective but not rigorously justified learning rate schedules
- Performance gains over state-of-the-art methods may not generalize to all posterior inference problems due to limited comparison to other recent BBVI methods
- Scalability claims for high-dimensional problems (D=256) are based on limited experiments with Gaussian targets, with untested performance on truly high-dimensional, non-Gaussian posteriors

## Confidence

- High Confidence: Theoretical foundation for closed-form updates for Gaussian variational families is well-established through derivation of score-based divergence and its proximal optimization
- Medium Confidence: Empirical superiority claims are supported by diverse testbeds but limited by absence of systematic hyperparameter tuning and comparison to other recent BBVI methods
- Low Confidence: Scalability claims for high-dimensional problems are based on limited Gaussian target experiments, with untested performance on truly high-dimensional, non-Gaussian posteriors

## Next Checks

1. Conduct systematic ablation studies varying learning rate schedules, batch sizes, and initialization schemes across multiple benchmark problems to identify failure modes and optimal configurations

2. Evaluate BaM on higher-dimensional problems (D > 256) with both Gaussian and non-Gaussian posteriors, comparing computational costs against ADVI and GSM baselines

3. Apply BaM to real-world Bayesian inference problems beyond current evaluation (e.g., epidemiology models, astrophysical inference) to test generalizability to domains with different posterior characteristics