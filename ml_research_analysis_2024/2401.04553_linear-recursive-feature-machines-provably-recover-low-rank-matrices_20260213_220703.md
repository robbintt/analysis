---
ver: rpa2
title: Linear Recursive Feature Machines provably recover low-rank matrices
arxiv_id: '2401.04553'
source_url: https://arxiv.org/abs/2401.04553
tags:
- lin-rfm
- networks
- linear
- matrix
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper establishes theoretical guarantees for Recursive Feature
  Machines (RFMs) on sparse linear regression and low-rank matrix recovery. The authors
  show that RFMs with linear predictors generalize Iteratively Reweighted Least Squares
  (IRLS) algorithms, providing explicit characterizations of fixed points as minimizers
  of sparsity-inducing objectives.
---

# Linear Recursive Feature Machines provably recover low-rank matrices

## Quick Facts
- arXiv ID: 2401.04553
- Source URL: https://arxiv.org/abs/2401.04553
- Reference count: 40
- Primary result: RFMs provably recover low-rank matrices and generalize IRLS algorithms

## Executive Summary
This paper establishes theoretical guarantees for Recursive Feature Machines (RFMs) on sparse linear regression and low-rank matrix recovery. The authors show that RFMs with linear predictors generalize Iteratively Reweighted Least Squares (IRLS) algorithms, providing explicit characterizations of fixed points as minimizers of sparsity-inducing objectives. They prove the Neural Feature Ansatz (NFA) for deep linear networks, demonstrating that weight covariances align with gradient outer products. The authors also introduce an SVD-free implementation of RFMs that scales to matrices with millions of missing entries, achieving superior performance to deep linear networks on matrix completion tasks.

## Method Summary
The paper introduces Linear Recursive Feature Machines (lin-RFM) that iteratively update feature transformations using the Average Gradient Outer Product (AGOP) of the current predictor. The algorithm alternates between solving a linear regression problem with transformed features and updating the transformation matrix using a function ϕ applied to the AGOP. For specific choices of ϕ (power functions), lin-RFM provably recovers minimum nuclear norm solutions for low-rank matrix completion. The authors also develop deep lin-RFM, which captures the implicit bias of deep linear networks without backpropagation by aligning weight covariances with AGOP as suggested by the Neural Feature Ansatz.

## Key Results
- Lin-RFM with ϕ(s) = s^α generalizes IRLS-p with p = 2 - 4α, recovering minimum nuclear norm solutions
- Deep lin-RFM accurately captures the implicit bias of deep linear networks without backpropagation
- SVD-free implementation of lin-RFM achieves superior performance on matrix completion, requiring up to thousands fewer observed entries than deep linear networks
- Lin-RFM with appropriate parameters provably recovers ground truth low-rank matrices under standard incoherence assumptions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Lin-RFM fixed points correspond to minimizers of sparsity-inducing objectives depending on the choice of ϕ applied to AGOP.
- **Mechanism**: The AGOP of a predictor f with respect to data captures gradient variations relevant for prediction. By iteratively reweighting feature vectors using ϕ(AGOP) and learning in the transformed space, Lin-RFM implicitly minimizes objectives like ℓp norms of singular values.
- **Core assumption**: ϕ is continuous and takes only positive values, ensuring nonsingular transformation matrices and well-defined fixed point equations.
- **Evidence anchors**:
  - [abstract] "Specifically, we show that RFM restricted to linear models (lin-RFM) generalizes the well-studied Iteratively Reweighted Least Squares (IRLS) algorithm."
  - [section 2.3] "Our results shed light on the connection between feature learning in neural networks and classical sparse recovery algorithms."
  - [corpus] Weak evidence; neighbor papers focus on generalization bounds and theoretical analysis but don't directly address Lin-RFM's mechanism.
- **Break condition**: If ϕ is not continuous or takes non-positive values, the transformation matrices become singular and fixed point equations break down.

### Mechanism 2
- **Claim**: Deep Lin-RFM captures the implicit bias of deep linear networks without backpropagation.
- **Mechanism**: By using layer-wise AGOP updates suggested by the Neural Feature Ansatz (NFA), deep Lin-RFM aligns weight covariances with gradient outer products, effectively mimicking the implicit regularization of deep linear networks.
- **Core assumption**: Initial weight matrices are balanced (W^(ℓ)_0 W^(ℓ)_0^T = W^(ℓ+1)_0^T W^(ℓ+1)_0), ensuring balancedness is preserved during training.
- **Evidence anchors**:
  - [abstract] "We then introduce deep lin-RFM, a variant of lin-RFM using deep linear predictors that accurately captures the implicit bias of deep linear networks when using AGOP scaling suggested by the deep NFA."
  - [section 3] "Theorem 2. Let f(A) = ⟨A, W(L)W (L-1) ... W(1)⟩ denote an L-layer linear network... NFA indeed holds with the specific powers αℓ = 1/(L-ℓ+1)."
  - [corpus] Weak evidence; neighbor papers discuss low-rank layers and rank-sparse networks but don't directly validate deep Lin-RFM's mechanism.
- **Break condition**: If initial weights are not balanced or training diverges, the alignment between weight covariances and AGOP breaks, losing the implicit bias property.

### Mechanism 3
- **Claim**: Lin-RFM with ϕ(s) = s^α is a re-parameterization of IRLS-p with p = 2 - 4α, generalizing sparse recovery algorithms.
- **Mechanism**: By transforming the update rule through substitution Xt = WtMt and Pt = M^(-2)_t, Lin-RFM recovers the IRLS update without requiring matrix inversions for specific α values.
- **Core assumption**: ϕ(s) = s^α where α is an integer multiple of 1/2, enabling efficient SVD-free implementation through matrix multiplications.
- **Evidence anchors**:
  - [abstract] "Specifically, we show that RFM restricted to linear models (lin-RFM) generalizes the well-studied Iteratively Reweighted Least Squares (IRLS) algorithm."
  - [section 2.2] "When ϕ is a power function, ϕ(s) = s^α, lin-RFM is a re-parameterized version of the prominent IRLS-p algorithm with p = 2 - 4α."
  - [section 4] "We now introduce a computationally efficient, SVD-free formulation of Algorithm 1 when selecting ϕ to be matrix powers ϕ(s) = s^α where α is any integer multiply of 1/2."
- **Break condition**: If α is not an integer multiple of 1/2, SVD computations become necessary, losing computational efficiency and potentially breaking the IRLS connection.

## Foundational Learning

- **Concept**: Iteratively Reweighted Least Squares (IRLS)
  - Why needed here: Lin-RFM generalizes IRLS, so understanding IRLS's update rules and convergence properties is essential for grasping Lin-RFM's behavior.
  - Quick check question: How does IRLS-p update weights differently from standard least squares, and what role does the p parameter play in sparsity promotion?

- **Concept**: Average Gradient Outer Product (AGOP)
  - Why needed here: AGOP measures gradient variations relevant for prediction and serves as the key ingredient for feature reweighting in Lin-RFM.
  - Quick check question: How does AGOP differ from standard gradient covariance, and why is it particularly useful for structured data like matrices or images?

- **Concept**: Neural Feature Ansatz (NFA)
  - Why needed here: NFA establishes the connection between weight covariances in trained networks and AGOP, guiding the design of deep Lin-RFM's layer-wise updates.
  - Quick check question: What does NFA claim about the relationship between trained weights and AGOP, and how does this insight inform Lin-RFM's architecture?

## Architecture Onboarding

- **Component map**: Predictor (W) -> Transformation (M) -> AGOP computation -> ϕ function update

- **Critical path**:
  1. Initialize M₀ = I
  2. Compute Wt = arg min_W ||W||_F² subject to ⟨AiMt, W⟩ = yi
  3. Compute AGOP of predictor with respect to original data
  4. Update Mt+1 = ϕ(AGOP)
  5. Repeat until convergence

- **Design tradeoffs**:
  - ϕ choice: Power functions enable IRLS connection and SVD-free implementation but limit objective flexibility
  - Regularization: Ridge regularization stabilizes linear solves but may affect convergence to true solution
  - Initialization: M₀ = I provides neutral starting point but other initializations might accelerate convergence

- **Failure signatures**:
  - Divergence: If ϕ produces non-positive values or linear solves become ill-conditioned
  - Stalling: If AGOP becomes rank-deficient or ϕ over-suppresses relevant features
  - Incorrect solution: If regularization is too strong or ϕ choice mismatches problem structure

- **First 3 experiments**:
  1. Verify Lin-RFM recovers minimum nuclear norm solution for rank-1 matrix completion with sufficient observations
  2. Compare Lin-RFM (α = 1/2) vs IRLS-0 convergence and solution quality on synthetic low-rank matrices
  3. Test deep Lin-RFM vs depth-2 linear network on matrix completion task, measuring test error vs number of observations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions does lin-RFM with ϵ = 0 provably converge to the low-rank matrix Y♯ in matrix completion problems?
- Basis in paper: [explicit] The authors conjecture that lin-RFM with ϵ = 0 can recover the ground truth low-rank matrix Y♯ with high probability under standard incoherence assumptions and with sufficiently many observed entries, but do not provide a formal proof.
- Why unresolved: While the authors provide some special case analyses (like matrices of size m × 2), a general convergence proof for arbitrary matrix sizes and observation patterns remains open. The convergence analysis for IRLS with negative p values (which lin-RFM corresponds to when α > 1/2) is also not well-established in the literature.
- What evidence would resolve it: A formal proof showing that lin-RFM with ϵ = 0 converges to Y♯ under standard incoherence assumptions, or a counterexample demonstrating failure to converge under certain conditions.

### Open Question 2
- Question: How does the implicit bias of deep linear networks compare to lin-RFM when both are applied to noisy matrix completion?
- Basis in paper: [explicit] The authors demonstrate that lin-RFM outperforms deep linear networks on noiseless matrix completion, but only provide preliminary results on noisy matrix completion in Appendix Fig. 7.
- Why unresolved: The relative performance of these methods in noisy settings remains unclear. The implicit bias of deep linear networks may lead to different solutions than lin-RFM when noise is present, potentially affecting generalization.
- What evidence would resolve it: Comprehensive experiments comparing lin-RFM and deep linear networks across various noise levels, matrix sizes, and ranks, including analysis of their implicit biases and generalization performance.

### Open Question 3
- Question: Can the SVD-free implementation of lin-RFM be extended to other choices of ϕ beyond matrix powers that are integer multiples of 1/2?
- Basis in paper: [explicit] The authors provide an SVD-free implementation only for ϕ(s) = s^α where α is an integer multiple of 1/2, noting that other choices would require full SVD computations at each iteration.
- Why unresolved: While the authors show this specific case is computationally efficient, it's unclear whether similar computational savings could be achieved for other function classes of ϕ.
- What evidence would resolve it: Development of SVD-free algorithms for other function classes of ϕ (e.g., exponential functions, trigonometric functions) or a proof that such implementations are impossible for certain function classes.

## Limitations
- The empirical evaluation relies heavily on synthetic data rather than real-world datasets
- The SVD-free implementation for general power functions α remains computationally challenging
- Scalability claims for millions of missing entries need verification on real-world datasets
- The assumption of balanced initial weights for deep Lin-RFM may not hold in practice

## Confidence
- High confidence: Theoretical guarantees for Lin-RFM recovering minimum nuclear norm solutions (Theorem 1)
- Medium confidence: Connection between Lin-RFM and IRLS algorithms (Theorem 2)
- Medium confidence: Deep Lin-RFM capturing implicit bias of deep linear networks (Theorem 3)
- Low confidence: Empirical superiority over deep linear networks on real matrix completion tasks

## Next Checks
1. **Robustness to initialization**: Test Lin-RFM with random initial M₀ (not identity) on synthetic low-rank matrix recovery to verify convergence properties are maintained.
2. **Real-world scalability**: Implement the SVD-free Lin-RFM on a real matrix completion dataset (e.g., MovieLens) with millions of missing entries and measure wall-clock time vs deep linear networks.
3. **Hyperparameter sensitivity**: Systematically vary the ridge regularization parameter λ and early stopping criteria across multiple runs to quantify variance in test performance and identify optimal tuning ranges.