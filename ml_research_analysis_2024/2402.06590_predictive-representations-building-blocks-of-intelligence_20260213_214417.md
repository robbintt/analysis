---
ver: rpa2
title: 'Predictive representations: building blocks of intelligence'
arxiv_id: '2402.06590'
source_url: https://arxiv.org/abs/2402.06590
tags:
- learning
- policy
- agent
- state
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reviews the concept of predictive representations, which
  are useful for reinforcement learning (RL) tasks. Predictive representations cache
  the answers to certain queries about the future, making them accessible with limited
  computational cost, but losing some flexibility compared to predictive models.
---

# Predictive representations: building blocks of intelligence

## Quick Facts
- arXiv ID: 2402.06590
- Source URL: https://arxiv.org/abs/2402.06590
- Reference count: 40
- This paper reviews predictive representations as versatile building blocks of intelligence, focusing on successor representations and their generalizations for efficient reinforcement learning.

## Executive Summary
This paper reviews the concept of predictive representations, which cache answers to future queries for efficient reinforcement learning. The successor representation (SR) and its variants like successor features (SFs) and successor models (SMs) enable rapid policy adaptation, transfer learning, and efficient computation of values and policies. The paper discusses practical learning algorithms and applications across AI, neuroscience, and cognitive science, suggesting predictive representations may function as fundamental building blocks of intelligence.

## Method Summary
The paper presents a theoretical framework for learning and applying predictive representations in reinforcement learning. It describes how successor representations can be learned using temporal difference updates, and how successor features generalize this to high-dimensional state spaces through feature-based representations. The method involves learning these representations from interaction with the environment, then using them to compute values and policies efficiently, particularly when reward functions change or when transferring knowledge across tasks.

## Key Results
- Successor representations enable rapid policy adaptation by caching future state occupancy patterns
- Successor features generalize SR to high-dimensional spaces by predicting feature occupancy
- Successor models extend predictive representations to continuous state spaces through probability distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Successor representations enable rapid policy adaptation by caching future state occupancy patterns.
- Mechanism: The successor representation Mπ(s, s') represents the discounted expected future occupancy of state s' when starting from state s and following policy π. This allows fast recomputation of values for new reward functions via Vπ(s) = Σ Mπ(s, s')R(s'), avoiding costly full model-based planning.
- Core assumption: The transition structure T is stable; only the reward function R changes between tasks.
- Evidence anchors:
  - [abstract]: "Predictive representations cache the answers to certain queries about the future, making them accessible with limited computational cost."
  - [section]: "the SR, unlike model-based algorithms, obviates the need to simulate roll-outs or iterate over dynamic programming updates, because it has already compiled transition information into a convenient form."
  - [corpus]: Weak evidence - corpus neighbors discuss predictive representations in general ML contexts but not SR-specific mechanisms.
- Break Condition: If transition dynamics T change significantly, cached occupancy patterns become invalid and recomputation of SR is required.

### Mechanism 2
- Claim: Successor features generalize the SR to high-dimensional state spaces by predicting feature occupancy rather than state occupancy.
- Mechanism: Instead of maintaining a matrix over all states, successor features ψπ(s) predict discounted sums of state features φ(s). For a reward function decomposed as R(s,w) = φ(s)⊤w, value computation becomes Qπ(s,a,w) = ψπ(s,a)⊤w, enabling transfer to new tasks via generalized policy improvement.
- Core assumption: Reward can be linearly decomposed into state features, and state features generalize across similar states.
- Evidence anchors:
  - [abstract]: "The SR and its variants, such as successor features (SFs)... enable efficient computation of values and policies, adapt quickly to changes in the environment, and support transfer learning across tasks."
  - [section]: "SFs are then predictions of accumulated features φ the agent can expect to encounter when following a policy π."
  - [corpus]: Weak evidence - corpus contains general ML work on predictive representations but lacks specific discussion of successor features.
- Break Condition: If reward function cannot be linearly decomposed into state features, or if state features don't generalize, successor features lose their transfer advantage.

### Mechanism 3
- Claim: Successor models extend predictive representations to continuous state spaces by defining probability distributions over future states.
- Mechanism: The successor model µπ(s̃|s) defines a probability distribution over future states s̃ when following policy π from state s. This allows density estimation techniques and sampling from future states, enabling model-based control and planning in continuous spaces.
- Core assumption: The future state distribution can be estimated and sampled from, even without explicit transition model.
- Evidence anchors:
  - [abstract]: "Predictive representations cache the answers to certain queries about the future, making them accessible with limited computational cost."
  - [section]: "Since the SM integrates to 1, a key difference to the SR is that it defines a valid probability distribution."
  - [corpus]: Weak evidence - corpus mentions predictive representations but not successor models specifically.
- Break Condition: If the state space is extremely high-dimensional or continuous, accurate density estimation becomes computationally intractable.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: The entire framework of successor representations is built on the MDP formalism, defining states, actions, transitions, and rewards.
  - Quick check question: What are the four components of an MDP tuple M = (γ,S,A,T,R)?

- Concept: Temporal Difference (TD) Learning
  - Why needed here: Both successor representations and successor features are learned using TD-style updates that bootstrap from estimates.
  - Quick check question: What is the Bellman equation for the value function, and how does it relate to TD learning?

- Concept: Linear Function Approximation
  - Why needed here: Successor features rely on decomposing rewards into linear combinations of state features, enabling transfer across tasks.
  - Quick check question: How does the assumption R(s,w) = φ(s)⊤w enable generalization across different reward functions?

## Architecture Onboarding

- Component map:
  State representation module -> Features -> Successor Representation -> Value Computation -> Action Selection

- Critical path: State → Features → Successor Representation → Value Computation → Action Selection

- Design tradeoffs:
  - Tabular vs. function approximation: Tabular SR is exact but doesn't scale; function approximation scales but introduces approximation error
  - State vs. feature space: Direct state occupancy (SR) vs. feature occupancy (SFs) - SFs scale better but require good feature engineering
  - Model-based vs. predictive representations: Full models offer flexibility but are computationally expensive; predictive representations trade flexibility for efficiency

- Failure signatures:
  - Poor transfer performance: Likely due to inadequate state features or non-linear reward functions
  - Slow learning: May indicate inappropriate learning rate or insufficient exploration
  - Catastrophic forgetting: Could suggest insufficient regularization when learning multiple tasks

- First 3 experiments:
  1. Implement tabular SR on a gridworld with changing rewards to verify rapid adaptation
  2. Add successor features with learned state representations on a simple control task
  3. Test successor models on a continuous control task requiring long-horizon planning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do predictive representations bridge the gap between model-based and model-free reinforcement learning, and what are the specific computational advantages they offer in terms of efficiency and flexibility?
- Basis in paper: [explicit] The paper discusses how predictive representations like the successor representation (SR) and its generalizations combine aspects of both model-based and model-free algorithms, offering efficiency through caching and flexibility through generalization to new tasks.
- Why unresolved: The paper provides a theoretical framework and examples of how predictive representations achieve this balance, but further research is needed to quantify the exact computational advantages in various RL tasks and environments.
- What evidence would resolve it: Empirical studies comparing the performance of predictive representation-based algorithms to pure model-based and model-free algorithms on a range of RL tasks, measuring both computational efficiency and flexibility.

### Open Question 2
- Question: What are the biological mechanisms underlying the learning and computation of predictive representations in the brain, and how do these mechanisms support cognitive functions like spatial navigation, memory, and decision-making?
- Basis in paper: [explicit] The paper reviews evidence from neuroscience suggesting that the brain uses predictive representations, particularly in the hippocampus and medial temporal lobe, for various cognitive functions. It also discusses biologically plausible learning algorithms for these representations.
- Why unresolved: While the paper presents evidence and potential mechanisms, the exact neural implementation and the specific roles of different brain regions in predictive representation learning and computation are still not fully understood.
- What evidence would resolve it: Detailed neural recordings and computational models linking specific neural activity patterns to the learning and computation of predictive representations, along with behavioral studies demonstrating the causal role of these representations in cognitive functions.

### Open Question 3
- Question: How can predictive representations be scaled and applied to complex real-world problems, such as autonomous driving, robotics, and natural language processing, and what are the key challenges and limitations in doing so?
- Basis in paper: [inferred] The paper discusses the potential of predictive representations in various AI applications, including exploration, transfer learning, and multi-agent systems. However, it also acknowledges the challenges of scaling these representations to large, high-dimensional state spaces and dealing with non-stationary environments.
- Why unresolved: While the paper provides a theoretical foundation and examples of successful applications, further research is needed to develop efficient learning algorithms, address scalability issues, and overcome limitations in complex real-world settings.
- What evidence would resolve it: Successful implementations of predictive representation-based algorithms on complex real-world problems, demonstrating improved performance and efficiency compared to existing methods. Additionally, studies identifying and addressing the key challenges and limitations in scaling and applying these representations.

## Limitations

- The review lacks detailed experimental validation of the theoretical claims about practical performance
- Success of successor features heavily depends on appropriate state feature engineering, which is not deeply addressed
- Extension to continuous state spaces via successor models is computationally challenging in practice

## Confidence

- High Confidence: The core mathematical relationships between successor representations and value functions are well-established and mathematically rigorous.
- Medium Confidence: The claims about transfer learning and rapid adaptation are supported by existing literature but would benefit from direct experimental validation.
- Low Confidence: The claims about applications to neuroscience and brain function modeling require substantial additional evidence beyond what is presented in this review.

## Next Checks

1. Implement a suite of transfer learning experiments across multiple task families to quantify the actual transfer benefits of successor features versus baseline methods.

2. Systematically evaluate how different feature representations (learned vs. hand-crafted) impact the performance of successor features on complex control tasks.

3. Benchmark successor models on high-dimensional continuous control tasks to identify computational bottlenecks and evaluate density estimation accuracy in practice.