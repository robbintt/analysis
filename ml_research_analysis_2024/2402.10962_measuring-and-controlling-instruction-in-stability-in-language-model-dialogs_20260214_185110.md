---
ver: rpa2
title: Measuring and Controlling Instruction (In)Stability in Language Model Dialogs
arxiv_id: '2402.10962'
source_url: https://arxiv.org/abs/2402.10962
tags:
- persona
- what
- your
- user
- london
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Language model chatbots rely on system prompts to maintain consistent\
  \ behavior during extended conversations, but these prompts often lose influence\
  \ over time\u2014a phenomenon called persona drift. This study introduces a benchmark\
  \ to measure persona drift quantitatively and finds that popular models like LLaMA2-chat-70B\
  \ exhibit significant drift within eight conversational rounds."
---

# Measuring and Controlling Instruction (In)Stability in Language Model Dialogs

## Quick Facts
- arXiv ID: 2402.10962
- Source URL: https://arxiv.org/abs/2402.10962
- Reference count: 40
- Language model chatbots lose adherence to system prompts during extended conversations

## Executive Summary
Language model chatbots rely on system prompts to maintain consistent behavior during extended conversations, but these prompts often lose influence over timeâ€”a phenomenon called persona drift. This study introduces a benchmark to measure persona drift quantitatively and finds that popular models like LLaMA2-chat-70B exhibit significant drift within eight conversational rounds. The research links drift to transformer attention decay, where attention to initial prompt tokens diminishes as conversation progresses. To counter this, the authors propose a lightweight method called split-softmax that boosts attention to system prompt tokens during inference.

## Method Summary
The authors developed a synthetic dialog benchmark where two instances of the same LLM converse with different personas, measuring how well each agent maintains its persona over multiple conversational rounds. They evaluated three mitigation methods: System Prompt Repetition (repeating the prompt before user utterances), Classifier-Free Guidance (combining logits from runs with and without the prompt), and Split-softmax (scaling attention weights to system prompt tokens). The evaluation used automated persona measures and task performance metrics like MMLU accuracy to compare trade-offs between persona stability and capability.

## Key Results
- LLaMA2-chat-70B exhibits significant persona drift within eight conversational rounds
- Attention weights to system prompt tokens decrease progressively as conversations lengthen
- Split-softmax achieves better trade-offs between persona stability and task performance than baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attention decay causes persona drift in language model chatbots.
- Mechanism: As conversations progress, transformer attention heads allocate progressively less weight to system prompt tokens, reducing the prompt's influence on generated responses.
- Core assumption: The model's output distribution is heavily influenced by the attention weights assigned to system prompt tokens versus conversation tokens.
- Evidence anchors:
  - [abstract] "An empirical and theoretical analysis of this phenomenon suggests the transformer attention mechanism plays a role, due to attention decay over long exchanges."
  - [section] "We measure this effect precisely and find that there is indeed a strong attention decay effect."
  - [corpus] Weak - corpus mentions attention logit control but not specifically decay in prompt attention.
- Break condition: If the model uses attention mechanisms that don't show this decay pattern, or if other factors dominate persona drift.

### Mechanism 2
- Claim: Split-softmax amplifies attention to system prompt tokens to counter drift.
- Mechanism: By scaling attention weights to prompt tokens during inference, split-softmax increases the relative importance of the system prompt in the token generation process.
- Core assumption: Increasing attention to system prompt tokens will proportionally increase the model's adherence to the specified persona.
- Evidence anchors:
  - [abstract] "To combat attention decay and persona drift, we propose a lightweight method called split-softmax, which compares favorably against two strong baselines."
  - [section] "split-softmax works by inserting a scaling operation between Equation 3 and Equation 4 for every attention operation."
  - [corpus] Weak - corpus mentions attention logit control but not the specific split-softmax method.
- Break condition: If the relationship between attention weights and persona adherence is non-linear or if the scaling disrupts other model behaviors.

### Mechanism 3
- Claim: Persona stability can be quantified without human annotation using self-chat benchmarks.
- Mechanism: Two instances of the same model with different personas converse, and the agent's adherence to its persona is measured by probing with predetermined questions and automated persona measures.
- Core assumption: Automated persona measures can reliably detect whether responses align with the specified persona.
- Evidence anchors:
  - [abstract] "We propose a quantitative benchmark to test this assumption, evaluating instruction stability via self-chats between two instructed chatbots."
  - [section] "The persona measure function can be Python code that calls a library to determine the confidence that a reply is in French."
  - [corpus] Moderate - corpus includes similar self-chat and automated evaluation approaches.
- Break condition: If the automated persona measures fail to capture the nuances of persona adherence or if self-chat doesn't reflect real user interactions.

## Foundational Learning

- Concept: Transformer attention mechanism
  - Why needed here: Understanding how attention weights decay over conversation length is central to explaining persona drift.
  - Quick check question: How does the attention mechanism in transformers work, and why might attention weights to initial tokens decrease over time?

- Concept: Self-attention and token generation process
  - Why needed here: The paper relies on understanding how tokens are generated based on attention distributions across previous tokens.
  - Quick check question: In the transformer token generation process, how are attention weights used to compute the next token distribution?

- Concept: Empirical evaluation methodology for language models
  - Why needed here: The paper introduces a new benchmark approach that doesn't rely on human annotation, requiring understanding of automated evaluation methods.
  - Quick check question: What are the advantages and limitations of using automated measures versus human evaluation for assessing language model behavior?

## Architecture Onboarding

- Component map: Base language model (LLaMA2-chat-70B) -> Split-softmax intervention module -> Persona stability benchmark (personas, probe questions, automated measures) -> Evaluation metrics
- Critical path: 1) Initialize two model instances with different personas, 2) Generate conversation, 3) Apply intervention method, 4) Probe agent model with persona-specific questions, 5) Evaluate stability using automated measures
- Design tradeoffs: Split-softmax trades some performance (MMLU accuracy) for increased persona stability; simpler than fine-tuning but may be less effective long-term
- Failure signatures: If persona stability doesn't improve with split-softmax, or if MMLU performance drops too severely; if automated measures don't capture true persona adherence
- First 3 experiments:
  1. Reproduce the baseline persona drift measurement on LLaMA2-chat-70B without any intervention to verify the phenomenon
  2. Implement and test split-softmax with different k values to find the optimal trade-off between stability and performance
  3. Compare split-softmax against the two baselines (system prompt repetition and classifier-free guidance) at matched performance levels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does persona drift occur in multi-turn conversations when the system prompt is repeated at the beginning of each turn?
- Basis in paper: [inferred] The paper evaluates system prompt repetition as a baseline method, suggesting that repeating the prompt might mitigate drift.
- Why unresolved: The paper tests SPR but does not specifically analyze whether repetition alone, without additional interventions, is sufficient to prevent drift in multi-turn conversations.
- What evidence would resolve it: Experiments comparing persona stability in conversations where the system prompt is repeated at each turn versus those where it is only given once at the start.

### Open Question 2
- Question: How does the length of the context window affect the rate of persona drift?
- Basis in paper: [inferred] The paper discusses attention decay over long exchanges and uses LLaMA2-70B with a context window of 4,096 tokens, implying that context length may influence drift.
- Why unresolved: The paper does not systematically vary context window sizes to measure their impact on drift rates.
- What evidence would resolve it: Experiments measuring persona stability across conversations with different context window lengths, controlling for other variables.

### Open Question 3
- Question: Can architectural modifications to the transformer model reduce attention decay and persona drift?
- Basis in paper: [inferred] The paper proposes split-softmax as a lightweight intervention but suggests future work could explore architectural changes.
- Why unresolved: The paper focuses on inference-time interventions rather than fundamental architectural modifications.
- What evidence would resolve it: Comparative studies of persona stability between standard transformers and modified architectures designed to maintain attention on initial tokens.

### Open Question 4
- Question: Does fine-tuning on conversation data improve persona stability compared to prompting alone?
- Basis in paper: [explicit] The paper mentions instruction tuning as a method that has been widely adopted to align models to task instructions.
- Why unresolved: The paper does not experimentally compare fine-tuned models with prompted-only models for persona stability.
- What evidence would resolve it: Experiments measuring persona drift in models that are fine-tuned on conversational data versus those that rely solely on system prompts.

### Open Question 5
- Question: How does the complexity or specificity of the system prompt affect the rate of persona drift?
- Basis in paper: [inferred] The paper uses a diverse set of 100 system prompts but does not analyze whether certain types of prompts are more prone to drift.
- Why unresolved: The paper does not perform a detailed analysis correlating prompt characteristics with drift rates.
- What evidence would resolve it: Statistical analysis of persona stability across different prompt categories (e.g., simple vs. complex, specific vs. general) to identify patterns.

## Limitations

- The paper relies on correlational evidence linking attention decay to persona drift rather than establishing definitive causal mechanisms
- The split-softmax intervention may introduce new artifacts or biases not captured by the measured metrics
- The benchmark methodology depends on automated persona measures whose reliability across diverse conversational contexts remains untested
- Evaluation focuses only on LLaMA2-chat-70B, leaving uncertainty about generalizability to other model architectures

## Confidence

*High Confidence:* The empirical finding of persona drift across conversational rounds is robustly demonstrated with quantitative measurements. The observation that attention weights to system prompt tokens decrease over time is directly measured and consistently observed.

*Medium Confidence:* The theoretical link between attention decay and persona drift is plausible but not definitively proven. While the correlation is strong, other factors could contribute to the observed drift. The effectiveness of split-softmax compared to baselines is demonstrated but evaluated on a limited set of metrics.

*Low Confidence:* The claim that automated persona measures can fully substitute for human evaluation in assessing conversational quality and persona adherence is not sufficiently validated. The long-term implications of attention scaling interventions on model behavior are not explored.

## Next Checks

1. **Causal Mechanism Validation**: Design an ablation study that isolates attention decay from other potential drift factors by comparing models with different attention mechanisms (e.g., sparse attention variants) to determine if attention decay is necessary and sufficient for persona drift.

2. **Cross-Model Generalization**: Test split-softmax on diverse model architectures (GPT-style, encoder-decoder, and other decoder-only models) and training approaches to verify that the attention decay phenomenon and intervention effectiveness generalize beyond LLaMA2-chat-70B.

3. **Automated Measure Reliability**: Conduct a validation study comparing the automated persona measure scores against human judgments on a subset of conversations to establish the correlation between automated and human assessments, particularly for edge cases where personas might be partially followed or interpreted ambiguously.