---
ver: rpa2
title: 'GUIDE: A Guideline-Guided Dataset for Instructional Video Comprehension'
arxiv_id: '2406.18227'
source_url: https://arxiv.org/abs/2406.18227
tags:
- video
- videos
- steps
- guideline
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GUIDE, a dataset for instructional video
  comprehension that addresses the lack of experiential guidelines at the task level
  in existing datasets. GUIDE includes 3.5K videos across 560 tasks in 8 domains,
  each annotated with a guideline representing a common pattern and systematic specific
  steps with timestamps and descriptions.
---

# GUIDE: A Guideline-Guided Dataset for Instructional Video Comprehension

## Quick Facts
- arXiv ID: 2406.18227
- Source URL: https://arxiv.org/abs/2406.18227
- Reference count: 14
- Primary result: Introduces GUIDE dataset with 3.5K videos across 560 tasks, showing language models benefit from guideline guidance for clearer step generation

## Executive Summary
This paper introduces GUIDE, a dataset for instructional video comprehension that addresses the lack of experiential guidelines at the task level in existing datasets. GUIDE includes 3.5K videos across 560 tasks in 8 domains, each annotated with a guideline representing a common pattern and systematic specific steps with timestamps and descriptions. The dataset supports three sub-tasks: step captioning, guideline summarization, and guideline-guided captioning. Evaluations show that while video foundation models outperform on video segment captioning, they struggle with entire video comprehension and guideline summarization. Language models perform better with guidelines, indicating their usefulness in generating clearer steps. Human evaluation demonstrates the practical applicability of GUIDE in real-world scenarios.

## Method Summary
The GUIDE dataset was constructed by first collecting instructional videos from 8 domains and transcribing them using Whisper to generate subtitles. GPT-3.5-turbo was used to generate specific steps with timestamps from the video subtitles, followed by clustering videos and generating guideline steps using GPT-4. The dataset was then manually verified to ensure coverage of the three sub-tasks. Various foundation models (VideoChat, Video-LLaMA, mPLUG-Owl, GPT-3.5-turbo, GPT-4, Vicuna, Flan-T5) were benchmarked with both zero-shot and fine-tuning approaches, using METEOR, CIDEr, and SPICE metrics to evaluate performance on the three sub-tasks.

## Key Results
- Video foundation models outperform on video segment captioning but struggle with entire video comprehension and guideline summarization
- Language models perform better with guidelines, indicating their usefulness in generating clearer steps
- Human evaluation demonstrates the practical applicability of GUIDE in real-world scenarios

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Guideline annotations improve the clarity and learnability of instructional steps.
- **Mechanism**: Guidelines provide a hierarchical structure that reduces cognitive load by organizing specific steps into a coherent task-level pattern.
- **Core assumption**: Learners benefit from structured procedural guidance more than from a flat list of steps.
- **Evidence anchors**:
  - [abstract] "language models perform better with guidelines, indicating their usefulness in generating clearer steps."
  - [section 4.4] "both video and language foundation models perform much better with the guide of guidelines."
  - [corpus] Weak; no direct citation, but related works on cognitive load theory support the assumption.
- **Break condition**: If guideline generation becomes too generic or loses task-specific relevance, the benefit disappears.

### Mechanism 2
- **Claim**: Single-video understanding is a prerequisite for multi-video guideline extraction.
- **Mechanism**: Models must first learn to segment and describe individual procedural steps before they can identify shared patterns across videos.
- **Core assumption**: Pattern mining across videos requires accurate intra-video comprehension as a foundation.
- **Evidence anchors**:
  - [section 4.5] "the single-video understanding ability is the basis of learning multiple videos."
  - [section 4.5] "models only with single-video comprehension capabilities are incapable of multiple-video comprehension."
  - [corpus] Moderate; neighboring works on multi-modal learning suggest prerequisite skill hierarchies.
- **Break condition**: If single-video models are overfit to specific domains, they may fail to generalize patterns.

### Mechanism 3
- **Claim**: Visual encoders are a bottleneck for procedural video understanding.
- **Mechanism**: Irrelevant visual information interferes with temporal reasoning, so specialized visual-language bridges are needed.
- **Core assumption**: Audio descriptions are more directly aligned with procedural semantics than raw visual frames.
- **Evidence anchors**:
  - [section 4.5] "mPLUG-Owl shows a significant improvement given only audio compared to when given only video."
  - [section 4.5] "much irrelevant information is mixed in during the visual feature extraction process."
  - [corpus] Weak; no direct citation, but literature on vision-language models supports modality-specific bottlenecks.
- **Break condition**: If visual features are fine-tuned on procedural datasets, the gap may narrow.

## Foundational Learning

- **Concept**: Instructional video segmentation
  - Why needed here: Accurate step boundaries are required for both model training and evaluation.
  - Quick check question: Can the model identify start/end timestamps for each procedural step in a video?

- **Concept**: Hierarchical task modeling
  - Why needed here: Guidelines organize steps into a higher-level pattern that aids comprehension.
  - Quick check question: Does the model generate a consistent task-level structure across multiple videos?

- **Concept**: Multi-modal alignment
  - Why needed here: Steps must be aligned across video, audio, and text to support cross-modal reasoning.
  - Quick check question: Can the model match a spoken instruction to the correct visual action segment?

## Architecture Onboarding

- **Component map**:
  - Video encoder (ViT/GPT backbone) → Step segmenter → Text generator
  - Guideline extractor (GPT-4) → Pattern matcher → Summary generator
  - Audio-to-text module (Whisper) → Subtitle aligner → Step describer

- **Critical path**: Video → Segment timestamps → Generate step captions → Align to guideline → Output tutorial

- **Design tradeoffs**:
  - Frame sampling rate vs. temporal resolution
  - Guideline generality vs. task specificity
  - Audio reliance vs. visual context

- **Failure signatures**:
  - Low METEOR/CIDEr scores on EVC but high on VSC → segmentation problem
  - Guideline mismatch across videos → clustering or abstraction issue
  - Audio-only strong performance → visual encoder underfit

- **First 3 experiments**:
  1. Compare EVC vs. VSC performance to isolate segmentation weakness.
  2. Ablate audio input to confirm visual modality bottleneck.
  3. Test guideline-guided vs. unguided captioning to measure guideline utility.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the GUIDE dataset compare to other existing datasets in terms of accuracy and efficiency for instructional video comprehension tasks?
- Basis in paper: [explicit] The paper mentions that the GUIDE dataset is larger and more diverse than other datasets, but does not provide a direct comparison of performance metrics.
- Why unresolved: The paper focuses on introducing the GUIDE dataset and its tasks, rather than conducting a comprehensive comparison with other datasets.
- What evidence would resolve it: Conducting experiments on both the GUIDE dataset and other datasets, using the same models and evaluation metrics, to compare their performance in terms of accuracy and efficiency.

### Open Question 2
- Question: What are the limitations of using GPT-3.5-turbo and GPT-4 for automatic annotation of instructional videos, and how can these limitations be addressed?
- Basis in paper: [explicit] The paper mentions that GPT-3.5-turbo and GPT-4 are used for automatic annotation, but also notes that the results are not reliable and require manual refinement.
- Why unresolved: The paper does not provide a detailed analysis of the limitations of these models or potential solutions to improve their performance.
- What evidence would resolve it: Conducting a thorough analysis of the strengths and weaknesses of GPT-3.5-turbo and GPT-4 in the context of instructional video annotation, and proposing specific improvements or alternative approaches to address their limitations.

### Open Question 3
- Question: How does the presence of background music in instructional videos affect the performance of video comprehension models, and what strategies can be employed to mitigate this issue?
- Basis in paper: [inferred] The paper mentions that the volume of background music should be lower than the volume of voice explanations, suggesting that background music can potentially interfere with the comprehension process.
- Why unresolved: The paper does not provide empirical evidence on the impact of background music on model performance or propose strategies to address this issue.
- What evidence would resolve it: Conducting experiments on instructional videos with varying levels of background music to assess the impact on model performance, and developing techniques to filter out or reduce the influence of background music on the comprehension process.

### Open Question 4
- Question: What are the key factors that contribute to the difficulty of the guideline summarization task, and how can models be improved to better extract common patterns from multiple instructional videos?
- Basis in paper: [explicit] The paper mentions that the guideline summarization task is challenging for both video and language foundation models, and that the ability to analyze correlations across multiple videos is crucial for this task.
- Why unresolved: The paper does not provide a detailed analysis of the specific challenges or propose concrete solutions to improve model performance on this task.
- What evidence would resolve it: Conducting a comprehensive analysis of the factors that make the guideline summarization task difficult, such as the diversity of video content and the complexity of common patterns, and developing techniques to enhance the models' ability to identify and extract these patterns from multiple videos.

### Open Question 5
- Question: How does the performance of the GUIDE dataset vary across different domains of instructional videos, and what are the implications for the generalizability of the models trained on this dataset?
- Basis in paper: [explicit] The paper mentions that the GUIDE dataset covers 8 different domains of instructional videos, but does not provide a detailed analysis of the performance variation across these domains.
- Why unresolved: The paper does not provide empirical evidence on the performance of the models across different domains or discuss the implications for their generalizability.
- What evidence would resolve it: Conducting experiments to evaluate the performance of the models on each domain separately and comparing their results, and analyzing the factors that contribute to the variation in performance across domains to assess the generalizability of the models trained on the GUIDE dataset.

## Limitations

- The dataset construction relies heavily on automatic annotation pipelines using GPT models, introducing uncertainty about annotation quality consistency
- The study focuses primarily on foundation models without exploring specialized architectures that might better leverage the guideline structure
- The limited number of videos per task (average of 6.25) may constrain the ability to capture comprehensive procedural patterns

## Confidence

- **High Confidence**: The observation that video foundation models outperform on video segment captioning but struggle with entire video comprehension (supported by direct quantitative comparisons in Table 3)
- **Medium Confidence**: The claim that language models benefit significantly from guideline guidance (supported by performance improvements but lacks ablation studies isolating the guideline contribution)
- **Medium Confidence**: The assertion that audio descriptions are more directly aligned with procedural semantics than raw visual frames (based on mPLUG-Owl performance but without controlled visual feature ablation)

## Next Checks

1. **Annotation Quality Validation**: Conduct a detailed manual evaluation of the automatically generated guidelines and specific steps across multiple domains to quantify annotation consistency and identify systematic errors in the GPT-based annotation pipeline.

2. **Visual Modality Ablation Study**: Systematically compare model performance using only visual features, only audio features, and both modalities to isolate whether the observed audio advantage is due to visual information quality or modality-specific processing limitations.

3. **Guideline Utility Isolation**: Design an ablation experiment that removes guideline guidance from the captioning task while controlling for all other variables to quantify the specific contribution of guidelines versus other dataset features.