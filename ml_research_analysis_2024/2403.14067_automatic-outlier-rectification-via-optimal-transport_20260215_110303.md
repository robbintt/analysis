---
ver: rpa2
title: Automatic Outlier Rectification via Optimal Transport
arxiv_id: '2403.14067'
source_url: https://arxiv.org/abs/2403.14067
tags: []
core_contribution: This paper introduces an automatic outlier rectification mechanism
  using optimal transport with a concave cost function. The key innovation is integrating
  outlier detection and estimation within a joint optimization framework, contrasting
  with traditional two-stage approaches.
---

# Automatic Outlier Rectification via Optimal Transport

## Quick Facts
- **arXiv ID**: 2403.14067
- **Source URL**: https://arxiv.org/abs/2403.14067
- **Reference count**: 40
- **Primary result**: Introduces automatic outlier rectification using optimal transport with concave cost functions

## Executive Summary
This paper presents a novel approach to outlier rectification that integrates detection and estimation within a joint optimization framework using optimal transport with a concave cost function. The method constructs a rectification set in probability space, allowing for effective identification and correction of outliers during estimation rather than through separate detection and estimation stages. The approach demonstrates significant improvements in various applications, particularly in financial contexts such as option implied volatility surface fitting, achieving 30.4% smoother surfaces and 6.3% lower MAPE compared to baseline methods.

## Method Summary
The core innovation lies in using optimal transport with a concave cost function to perform joint outlier detection and rectification. The concave cost encourages "long-haul" transportation, effectively moving outliers closer to the main data distribution. This approach contrasts with traditional two-stage methods by integrating the outlier handling directly into the estimation process. Theoretical analysis establishes equivalence to adaptive quantile regression for mean and LAD regression cases. The method operates by constructing a rectification set that identifies which data points should be moved and how, all within a single optimization framework that maintains computational tractability.

## Key Results
- Achieves 30.4% smoother volatility surfaces in option implied volatility surface fitting
- Demonstrates 6.3% lower MAPE compared to baseline approaches
- Shows theoretical equivalence to adaptive quantile regression for mean and LAD regression cases

## Why This Works (Mechanism)
The method works by leveraging optimal transport theory with a carefully designed concave cost function. The concave cost structure specifically encourages transportation between points that are far apart in the original space, which naturally identifies outliers as points that need to be moved significantly to align with the main data distribution. By integrating this transportation cost into a joint optimization framework, the method simultaneously identifies outliers and determines their optimal rectified positions. This joint approach avoids the pitfalls of two-stage methods where initial detection might be suboptimal or where estimation is performed on potentially corrupted data. The concavity of the cost function is crucial - it ensures that moving outliers requires less "cost" than moving many inliers slightly, creating a natural mechanism for outlier prioritization.

## Foundational Learning
- **Optimal Transport Theory**: Mathematical framework for finding optimal ways to move mass between probability distributions; needed to formulate the joint outlier detection and rectification problem as a transportation problem
- **Concave Cost Functions**: Cost structures that decrease marginal cost as distance increases; needed to encourage moving outliers (long-haul transport) rather than adjusting inliers slightly
- **Quantile Regression**: Regression method that estimates conditional quantiles rather than conditional means; needed to establish theoretical connections and understand the method's behavior under different loss functions
- **Robust Statistics**: Statistical methods designed to perform well even when assumptions about data distribution are violated; needed to contextualize the method within existing outlier handling approaches
- **Probability Space Transformations**: Mathematical operations that map one probability distribution to another; needed to understand how the rectification set transforms the original data distribution
- **Convex Analysis**: Mathematical framework for studying convex functions and sets; needed to analyze the optimization landscape and prove theoretical properties

## Architecture Onboarding

**Component Map**: Data points -> Optimal Transport Problem -> Concave Cost Evaluation -> Rectification Set Construction -> Joint Optimization -> Estimated Parameters

**Critical Path**: The core computation involves solving the optimal transport problem with concave costs, which dominates the computational complexity. This requires computing the cost matrix between all data points, solving the transportation problem (typically via linear programming or specialized OT solvers), constructing the rectification set from the optimal transport plan, and finally performing the joint optimization that yields both the outlier identification and parameter estimates.

**Design Tradeoffs**: The choice of concave cost function involves balancing sensitivity to outliers against robustness to legitimate extreme values. A more concave function will more aggressively identify outliers but risks over-correcting valid data points in low-density regions. The method trades computational complexity (solving OT problems) for statistical efficiency (joint estimation). Memory requirements scale quadratically with sample size due to the cost matrix, limiting scalability to very large datasets without approximation techniques.

**Failure Signatures**: The method may over-correct data points that are legitimate but lie in low-density regions of the distribution, particularly with highly concave cost functions. It may struggle with clustered outliers where the main distribution is not clearly separable. Performance degrades when the proportion of outliers is very high (>30-40%), as the assumption of a dominant inlier distribution breaks down. Computational failure modes include memory exhaustion for large datasets and numerical instability in the OT solver for ill-conditioned cost matrices.

**First Experiments**: 1) Test on synthetic data with known outlier proportions and types to validate detection accuracy and rectification quality. 2) Compare performance against traditional robust regression methods (RANSAC, Huber regression) on benchmark datasets with controlled contamination. 3) Evaluate computational scaling by testing on datasets of increasing size (n=1000, 10000, 100000) to identify practical limitations.

## Open Questions the Paper Calls Out
None specified in the provided material.

## Limitations
- The concave cost function design may be sensitive to parameter choices and could over-correct legitimate data points in low-density regions
- Theoretical equivalence to adaptive quantile regression may not generalize beyond mean and LAD regression cases
- Experimental validation focuses primarily on synthetic and financial data, limiting generalizability to other domains

## Confidence
- **High**: The joint optimization framework using optimal transport with concave costs
- **Medium**: The theoretical equivalence results and their generalizability to all convex loss functions
- **Low**: Performance claims in domains beyond the tested financial applications

## Next Checks
1. **Robustness testing**: Evaluate the method's performance across diverse outlier types (additive noise, multiplicative corruption, clustered outliers) and data distributions (heavy-tailed, multimodal) to assess generalizability.

2. **Scalability analysis**: Test the computational efficiency and memory requirements for high-dimensional datasets (d > 10) to determine practical limitations in real-world applications.

3. **Cross-domain validation**: Apply the method to non-financial domains such as image processing, sensor data analysis, or bioinformatics to verify its effectiveness beyond the financial applications presented.