---
ver: rpa2
title: One-Step Image Translation with Text-to-Image Models
arxiv_id: '2403.12036'
source_url: https://arxiv.org/abs/2403.12036
tags:
- image
- translation
- input
- diffusion
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method to adapt single-step text-to-image
  models like SD-Turbo for image-to-image translation tasks, addressing the slow inference
  of iterative diffusion models and the need for paired data. The authors introduce
  a new generator architecture that consolidates the three modules of latent diffusion
  models into a single end-to-end network with small trainable weights.
---

# One-Step Image Translation with Text-to-Image Models

## Quick Facts
- **arXiv ID:** 2403.12036
- **Source URL:** https://arxiv.org/abs/2403.12036
- **Reference count:** 40
- **Primary result:** CycleGAN-Turbo outperforms existing GAN and diffusion methods on unpaired image-to-image translation while being faster than iterative diffusion models

## Executive Summary
This paper presents a method to adapt single-step text-to-image models like SD-Turbo for image-to-image translation tasks, addressing the slow inference of iterative diffusion models and the need for paired data. The authors introduce a new generator architecture that consolidates the three modules of latent diffusion models into a single end-to-end network with small trainable weights. They also incorporate skip connections between the encoder and decoder to preserve input image details. For unpaired translation, their CycleGAN-Turbo model outperforms existing GAN and diffusion methods on various tasks like day-to-night conversion and weather effects, achieving lower FID and DINO-Struct. scores while being faster. For paired translation, their pix2pix-Turbo model is competitive with ControlNet but with single-step inference. The method can also generate diverse outputs by interpolating noise maps.

## Method Summary
The authors present a novel approach to adapt single-step text-to-image models for image-to-image translation by modifying the SD-Turbo architecture. Their key innovation is a unified generator that combines the three modules of latent diffusion models (VAE encoder, U-Net, and VAE decoder) into a single end-to-end network with small trainable weights. The method incorporates skip connections between the encoder and decoder to preserve input image details, and uses latent code conditioning to enable flexible translations. For unpaired translation, they adapt the CycleGAN framework with their one-step generator (CycleGAN-Turbo), while for paired translation they create pix2pix-Turbo. The approach eliminates the need for iterative denoising steps, achieving significant speed improvements while maintaining or improving translation quality compared to existing methods.

## Key Results
- CycleGAN-Turbo achieves lower FID and DINO-Struct. scores than existing GAN and diffusion methods on unpaired tasks like day-to-night and weather effects
- pix2pix-Turbo is competitive with ControlNet on paired translation tasks while using single-step inference
- The method generates diverse outputs through noise map interpolation
- Significant speed improvements over iterative diffusion models due to single-step generation

## Why This Works (Mechanism)
The method works by leveraging the strengths of latent diffusion models while addressing their key limitation: slow inference speed. By consolidating the three modules of latent diffusion (encoder, U-Net, decoder) into a single trainable network, the authors eliminate the need for iterative denoising steps. The skip connections preserve spatial details from the input image, while the latent code conditioning allows flexible control over the translation. For unpaired translation, the CycleGAN framework provides cycle consistency without requiring paired data, and the single-step generator makes this process fast. The small trainable weights enable efficient adaptation while maintaining the pre-trained model's strong generation capabilities.

## Foundational Learning
**Latent Diffusion Models (LDMs)**
- *Why needed:* Understanding the base architecture being modified
- *Quick check:* Know the three components (VAE encoder, U-Net, VAE decoder) and how they work together

**CycleGAN Framework**
- *Why needed:* Core framework for unpaired image-to-image translation
- *Quick check:* Understand cycle consistency loss and its role in unpaired translation

**Evaluation Metrics (FID, DINO-Struct.)**
- *Why needed:* Metrics used to evaluate translation quality
- *Quick check:* Know what FID measures and how DINO-Struct. differs from other perceptual metrics

**Skip Connections in U-Nets**
- *Why needed:* Key architectural element for preserving input details
- *Quick check:* Understand how skip connections work and why they're important for detail preservation

## Architecture Onboarding

**Component Map:**
VAE Encoder -> Unified Generator -> VAE Decoder
(Latent Code Conditioning -> Cycle Consistency Loss)

**Critical Path:**
Input Image → VAE Encoder → Unified Generator (with skip connections) → VAE Decoder → Output Image

**Design Tradeoffs:**
- Single-step inference vs. iterative quality refinement
- Small trainable weights vs. full fine-tuning for adaptation
- Skip connections for detail preservation vs. computational overhead

**Failure Signatures:**
- Loss of fine details in translated images (skip connection issues)
- Mode collapse or limited diversity in outputs
- Artifacts at boundaries between translated and non-translated regions

**First 3 Experiments:**
1. Test skip connections ablation: Compare with and without skip connections on detail preservation metrics
2. Noise interpolation diversity: Generate multiple outputs from same input using different noise maps
3. Speed benchmark: Measure inference time vs. iterative diffusion models across different resolutions

## Open Questions the Paper Calls Out
None

## Limitations
- The method's scalability to more complex translation tasks or higher resolution images remains unclear
- Potential artifacts or failure modes from single-step generation are not thoroughly addressed
- The claim about diverse output generation through noise interpolation needs more rigorous evaluation

## Confidence
- **High Confidence:** Architectural contributions are well-documented and implementation details are reproducible; speed improvements are verifiable
- **Medium Confidence:** Quantitative results are presented with appropriate rigor, but perceptual quality assessments are limited to visual examples
- **Low Confidence:** Diversity claims from noise interpolation lack thorough evaluation and significance metrics

## Next Checks
1. Conduct user preference studies comparing CycleGAN-Turbo outputs against baseline methods to validate perceptual quality gains across diverse user groups
2. Test the method's robustness on out-of-distribution translation tasks (medical imaging, satellite imagery) to assess generalization beyond artistic and weather transformations
3. Perform systematic ablation studies removing skip connections to quantify their specific contribution to detail preservation across different image regions and translation types