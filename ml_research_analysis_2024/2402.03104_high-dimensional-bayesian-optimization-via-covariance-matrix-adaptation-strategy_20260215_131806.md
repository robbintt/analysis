---
ver: rpa2
title: High-dimensional Bayesian Optimization via Covariance Matrix Adaptation Strategy
arxiv_id: '2402.03104'
source_url: https://arxiv.org/abs/2402.03104
tags:
- local
- iteration
- search
- data
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel meta-algorithm to enhance Bayesian Optimization
  (BO) for high-dimensional problems by combining local search with Covariance Matrix
  Adaptation (CMA) strategy. The key idea is to use CMA to define local regions based
  on a search distribution that estimates probabilities of data points being the global
  optimum, then apply BO optimizers within these regions.
---

# High-dimensional Bayesian Optimization via Covariance Matrix Adaptation Strategy

## Quick Facts
- arXiv ID: 2402.03104
- Source URL: https://arxiv.org/abs/2402.03104
- Reference count: 40
- Authors: Lam Ngo; Huong Ha; Jeffrey Chan; Vu Nguyen; Hongyu Zhang
- Key outcome: CMA-based meta-algorithm integrates CMA strategy with BO optimizers to define local regions for high-dimensional BO

## Executive Summary
This paper addresses the challenge of high-dimensional Bayesian Optimization (BO) by proposing a novel meta-algorithm that combines Covariance Matrix Adaptation (CMA) strategy with existing BO methods. The key innovation is using CMA to learn a search distribution that estimates probabilities of data points being the global optimum, then applying BO optimizers within the resulting local regions. The approach is integrated with BO, TuRBO, and BAxUS to create CMA-BO, CMA-TuRBO, and CMA-BAxUS variants that show significant performance improvements over their respective base optimizers.

## Method Summary
The method combines CMA strategy with BO optimizers to create a meta-algorithm for high-dimensional optimization. It initializes a CMA search distribution, defines local regions based on high-probability ellipsoids from the CMA distribution, and applies BO optimization within these regions. The approach includes a restart strategy when optimization stagnates. The CMA distribution is updated iteratively based on function evaluations, with the mean and covariance matrix adapting to concentrate search around promising regions. The BO optimizer (using Thompson Sampling) selects points from a candidate pool sampled from the CMA distribution within the local region.

## Key Results
- CMA-BO and CMA-TuRBO significantly improve upon BO and TuRBO across all 11 benchmark problems
- CMA-BAxUS shows notable gains on 6 out of 11 problems compared to BAxUS
- The method demonstrates superior ability to guide search toward promising regions and approach the global optimum
- CMA-based methods maintain data-efficiency while addressing the curse of dimensionality in BO

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CMA search distributions estimate probabilities of data points being the global optimum, guiding BO to promising regions.
- Mechanism: Covariance Matrix Adaptation (CMA) iteratively updates a multivariate normal search distribution's mean, covariance, and step size based on function evaluations. Higher-ranked points pull the mean toward them; covariance shapes the probability landscape around promising regions.
- Core assumption: The search distribution can approximate the true probability landscape of global optimum locations.
- Evidence anchors:
  - [abstract] "we use CMA to learn a search distribution that can estimate the probabilities of data points being the global optimum"
  - [section] "CMA tends to maximize the probability of generating successful data points... that highly likely contain the global optima"
  - [corpus] Weak (no corpus evidence of CMA distribution accuracy)
- Break condition: If the objective function has deceptive landscapes or many isolated local optima, the CMA distribution may mislead BO toward suboptimal regions.

### Mechanism 2
- Claim: Local regions defined by high-probability ellipsoids prevent over-exploration in high dimensions.
- Mechanism: The CMA distribution's confidence ellipsoid (α-level) is used as a local region; BO optimization is confined within this ellipsoid, focusing search where the CMA believes the optimum lies.
- Core assumption: Promising regions can be localized using the CMA distribution without missing the global optimum.
- Evidence anchors:
  - [abstract] "we then define the local regions consisting of data points with high probabilities of being the global optimum"
  - [section] "we propose defining the local region as the α-level confidence hyper-ellipsoid... containing α percent of the population"
  - [corpus] Weak (no corpus evidence that ellipsoid regions improve convergence)
- Break condition: If the true global optimum lies outside the ellipsoid in early iterations, the method may never recover.

### Mechanism 3
- Claim: Integrating CMA's restart strategy with BO's exploration-exploitation balance prevents premature convergence.
- Mechanism: When the CMA search stagnates (e.g., small evolution path), a restart resets the distribution. This couples with BO's Thompson Sampling, which injects randomness into acquisition, maintaining exploration.
- Core assumption: BO's stochastic acquisition combined with CMA restarts can globally explore without losing local search benefits.
- Evidence anchors:
  - [abstract] "A restart strategy is also included to restart the algorithm when the current optimization process is stuck at a local minimum"
  - [section] "we use the CMA's restart strategy: a new local search will be initialized when the current one is stuck at a local minimum"
  - [corpus] Weak (no corpus evidence that CMA restarts help BO avoid local traps)
- Break condition: If restarts are too frequent, BO never exploits found promising regions; if too rare, the method may converge prematurely.

## Foundational Learning

- Concept: Multivariate Normal Distribution
  - Why needed here: CMA maintains a search distribution over the optimization domain; understanding how mean and covariance shape the probability density is essential to grasp how CMA updates guide search.
  - Quick check question: If the covariance matrix is scaled by factor L in CMA-TuRBO, how does that affect the Mahalanobis distance and ellipsoid shape?

- Concept: Bayesian Optimization with Gaussian Processes
  - Why needed here: The meta-algorithm uses BO (with Thompson Sampling) inside CMA-defined local regions; knowing how GP surrogate models and acquisition functions balance exploration/exploitation is key to understanding why CMA+BO outperforms pure CMA.
  - Quick check question: In the local optimization step, why is the pool of candidates sampled from the CMA distribution before applying BO?

- Concept: Mahalanobis Distance
  - Why needed here: Local regions are defined by Mahalanobis distance thresholds; grasping this metric is necessary to understand how the ellipsoid shape adapts with covariance updates.
  - Quick check question: How does a large eigenvalue in the covariance matrix affect the shape of the confidence ellipsoid?

## Architecture Onboarding

- Component map:
  - CMA Core: Updates mean, covariance, step size (Eq. 4)
  - Local Region Builder: Constructs ellipsoid based on CMA distribution and BO variant
  - BO Optimizer: Selects λ points from candidate pool (sampled from CMA distribution) using TS acquisition
  - Restart Handler: Monitors stagnation; triggers CMA restart if needed

- Critical path:
  1. Initialize CMA distribution (mean, cov, step size)
  2. Define local region (ellipsoid)
  3. Sample candidate pool from CMA distribution inside region
  4. BO selects λ points from pool using TS
  5. Evaluate objective; update CMA distribution
  6. Check restart condition; loop or restart

- Design tradeoffs:
  - Candidate pool size nc: Larger pools reduce bias but increase cost; default nc = min(100d, 5000)
  - Local region shape: Ellipsoid vs hyper-rectangle (CMA-BO vs CMA-TuRBO) trades generality for adaptability
  - Restart frequency: Too frequent → loss of exploitation; too infrequent → stagnation

- Failure signatures:
  - CMA distribution mean drifting away from optimum → over-exploration
  - Ellipsoid too small → missed optimum; too large → loss of efficiency
  - BO failing to select good candidates from pool → ineffective guidance

- First 3 experiments:
  1. Run CMA-BO vs BO on Alpine-100D with fixed random seed; compare regret curves
  2. Vary nc (100d, 5000, 15000) for CMA-BO on Levy-100D; measure sensitivity
  3. Enable/disable CMA restarts in CMA-TuRBO on Schaffer2-100D; observe convergence differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the CMA-based meta-algorithm perform on high-dimensional problems with a budget larger than 2000 function evaluations?
- Basis in paper: [inferred] The paper only tested up to 2000 evaluations. The authors note that CMA-ES tends to over-explore in high-dimensional spaces, but do not test whether the CMA-based methods maintain their advantage with larger budgets.
- Why unresolved: The experimental results section only includes results up to 2000 evaluations. Testing with larger budgets would require significantly more computational resources.
- What evidence would resolve it: Running the CMA-based methods (CMA-BO, CMA-TuRBO, CMA-BAxUS) and baselines on problems with evaluation budgets of 5000, 10000, and 20000 evaluations, comparing performance metrics like simple regret and distance to global optimum over time.

### Open Question 2
- Question: What is the sensitivity of the CMA-based methods to the initial search distribution parameters (mean vector and covariance matrix)?
- Basis in paper: [explicit] The paper states that the initial mean vector is selected by minimizing 20 initial data points and the initial covariance matrix is set to identity matrix, but does not analyze how different initializations affect performance.
- Why unresolved: The paper uses a standard initialization but does not explore how sensitive the methods are to different starting points or covariance structures.
- What evidence would resolve it: Systematically testing the CMA-based methods with different initializations (random mean vectors, different covariance matrix structures, multiple restarts from different initial points) and comparing convergence speed and final solution quality.

### Open Question 3
- Question: How does the CMA-based meta-algorithm perform on constrained high-dimensional optimization problems?
- Basis in paper: [inferred] The paper focuses on unconstrained optimization problems. The authors mention that TuRBO has its own restart strategy but do not test constrained problems or incorporate constraint handling into their framework.
- Why unresolved: The experimental setup only includes unconstrained synthetic and real-world problems. Adding constraints would require modifications to the local region definition and acquisition function.
- What evidence would resolve it: Extending the CMA-based methods to handle constraints (e.g., through penalty methods or constraint-aware acquisition functions) and testing on benchmark constrained optimization problems like those in the COCO/BBOB constrained test suite.

## Limitations
- The method's effectiveness heavily depends on proper CMA parameter tuning, which wasn't extensively validated across all benchmark problems
- The ellipsoid-based local region definition may struggle with highly multimodal functions where the global optimum lies in a narrow basin
- Computational overhead scales poorly with dimensionality, potentially limiting practical applicability for very high-dimensional problems (d > 1000)

## Confidence
- Overall effectiveness of CMA-BO variants: Medium
- CMA's ability to accurately estimate global optimum probabilities: Low
- Integration mechanisms between CMA and BO variants: Medium

## Next Checks
1. Test CMA-BO variants on real-world optimization problems with known global optima to validate practical effectiveness beyond synthetic benchmarks
2. Conduct ablation studies varying CMA parameters (cm, c1, cµ) to quantify sensitivity and identify optimal settings for different problem classes
3. Analyze the scaling behavior on problems with d = 500-2000 to determine the practical dimensionality limits and identify potential computational bottlenecks