---
ver: rpa2
title: 'One Sample Fits All: Approximating All Probabilistic Values Simultaneously
  and Efficiently'
arxiv_id: '2410.23808'
source_url: https://arxiv.org/abs/2410.23808
tags: []
core_contribution: The paper proposes a one-sample-fits-all framework for approximating
  all probabilistic values simultaneously and efficiently. The key idea is to use
  a sampling vector to approximate intermediate terms that can be converted to any
  probabilistic value without amplifying scalars.
---

# One Sample Fits All: Approximating All Probabilistic Values Simultaneously and Efficiently

## Quick Facts
- arXiv ID: 2410.23808
- Source URL: https://arxiv.org/abs/2410.23808
- Authors: Weida Li; Yaoliang Yu
- Reference count: 40
- Primary result: Develops a one-sample-fits-all framework achieving O(n log n) convergence for all probabilistic values simultaneously by using a sampling vector to approximate intermediate terms without amplifying scalars

## Executive Summary
This paper introduces a unified framework for approximating all probabilistic values simultaneously and efficiently. The key innovation is the one-sample-fits-all (OFA) estimator that uses a sampling vector to approximate intermediate terms {φ⁺ᵢ,ₛ, φ⁻ᵢ,ₛ} without introducing multiplicative scalars that would deteriorate convergence rates. The framework achieves the currently best time complexity O(n log n) for all probabilistic values on average. Additionally, the paper establishes a theoretical connection between probabilistic values and least squares regression used in (regularized) datamodels, showing that the proposed estimator can solve a family of datamodels simultaneously without recomputing for different regularization parameters.

## Method Summary
The OFA framework uses a sampling vector q ∈ R^(n-3) to control which subset sizes to sample from the power set. Algorithm 1 implements the framework by sampling subsets according to q and updating intermediate term estimates for all players simultaneously, maximizing sample reuse. Two variants exist: OFA-A (optimized for all values on average) and OFA-S (optimized per value). The convergence rate is governed by D(m,q) = Σ n/qs-1 · (m²s/s + m²s+1/(n-s)), which can be minimized to achieve O(n log n) convergence. The framework applies to semi-values and weighted Banzhaf values, and establishes connections to least squares problems in datamodels where θ* = ϕ + c for some constant c.

## Key Results
- Achieves O(n log n) convergence rate for all probabilistic values simultaneously on average
- Introduces OFA-A and OFA-S sampling vectors that optimize D(m,q) for different use cases
- Establishes theoretical connection between probabilistic values and least squares regression in datamodels
- Outperforms existing methods (WSL-Shapley, SHAP-IQ, weightedSHAP, kernelSHAP, ARM, complement, AME, MSR) in empirical evaluations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The one-sample-fits-all estimator achieves O(n log n) convergence by avoiding amplifying scalars and maximizing sample reuse
- Mechanism: Uses sampling vector q to approximate intermediate terms without ps/qs multiplicative scalars, updating all player estimates simultaneously
- Core assumption: Utility function U is bounded (||U||_∞ ≤ u) and D(m,q) ∈ O(1)
- Evidence anchors: [abstract], [section 4.1], [corpus]
- Break condition: If D(m,q) grows faster than O(1) or utility function violates boundedness

### Mechanism 2
- Claim: Framework connects probabilistic values to least squares regression, enabling simultaneous solution of multiple datamodels
- Mechanism: Optimal solution θ* equals ϕ + c for some constant c, allowing OFA estimator to approximate θ* without recomputing
- Core assumption: Weight vector η satisfies ηs = ps-1 + ps for 2 ≤ s ≤ n
- Evidence anchors: [abstract], [section 4.3], [corpus]
- Break condition: If weight vector η doesn't satisfy required condition or θ* ≠ ϕ + c

### Mechanism 3
- Claim: D(m,q) formula effectively determines convergence rate, enabling optimal sampling vector design
- Mechanism: Convergence rate depends on D(m,q) = Σ n/qs-1 · (m²s/s + m²s+1/(n-s)), optimized to minimize convergence
- Core assumption: Formula accurately captures convergence behavior and can be minimized analytically
- Evidence anchors: [abstract], [section 4], [corpus]
- Break condition: If formula doesn't accurately predict empirical convergence or optimization is too complex

## Foundational Learning

- Concept: Semi-values and their relationship to probability measures
  - Why needed here: Understanding that probabilistic values can be represented as integrals over probability measures is crucial for recognizing the unified framework's scope
  - Quick check question: If a probabilistic value has ps = ∫₀¹ w^(s-1)(1-w)^(n-s) dμ(w), what type of value is this?

- Concept: (ϵ, δ)-approximation and convergence rate analysis
  - Why needed here: The theoretical framework relies on analyzing how many samples are needed to achieve a certain approximation quality with high probability
  - Quick check question: If an estimator achieves (ϵ, δ)-approximation with T samples, what happens to T if we want to halve ϵ while keeping δ constant?

- Concept: Variance reduction techniques in sampling estimators
  - Why needed here: Understanding why amplifying scalars and lack of sample reuse deteriorate convergence rates helps appreciate the framework's design choices
  - Quick check question: Why does multiplying by ps/qs in weighted sampling potentially worsen convergence rates?

## Architecture Onboarding

- Component map:
  - Sampling vector q ∈ R^(n-3) -> controls which subset sizes to sample
  - Intermediate term estimators {φ⁺ᵢ,ₛ, φ⁻ᵢ,ₛ} -> approximated using Algorithm 1
  -> Aggregation phase -> combines intermediate terms into final probabilistic values
  -> Utility function U -> black box function being evaluated

- Critical path:
  1. Optimize sampling vector q (either OFA-A for all values or OFA-S for specific values)
  2. Sample subsets according to q
  3. Update intermediate term estimates for all players simultaneously
  4. Aggregate intermediate terms to compute final estimates

- Design tradeoffs:
  - Fixed vs. value-specific sampling vectors (OFA-A vs OFA-S)
  - Memory vs. computation tradeoff in storing intermediate estimates
  - Bias vs. variance tradeoff in sampling strategy

- Failure signatures:
  - Slow convergence despite theoretical guarantees → check D(m,q) calculation
  - High variance in estimates → verify boundedness assumption on U
  - Memory overflow → check storage requirements for intermediate terms

- First 3 experiments:
  1. Verify OFA-A vs OFA-S performance on Beta(1,1) (Shapley value) - should show identical performance
  2. Test convergence rate on Beta(1,4) - should demonstrate O(n log n) behavior
  3. Validate datamodel connection by checking if θ* - θ*j are constant across different regularization parameters

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact relationship between the convergence rate of OFA-A and the properties of the probability measure μ for semi-values?
- Basis in paper: [explicit] The paper states that OFA-A achieves O(n log n) convergence for semi-values with bounded probability density functions and identifies this as a key condition.
- Why unresolved: The paper provides sufficient conditions but does not fully characterize the necessary and sufficient conditions on μ for OFA-A to achieve O(n log n) convergence.
- What evidence would resolve it: A rigorous proof establishing the necessary and sufficient conditions on μ for OFA-A to achieve O(n log n) convergence, along with empirical validation across a broader range of semi-values.

### Open Question 2
- Question: How does the performance of OFA-S compare to OFA-A in practice for specific probabilistic values beyond those tested in the paper?
- Basis in paper: [explicit] The paper demonstrates that OFA-S achieves better convergence rates than OFA-A for specific probabilistic values but only tests a limited set.
- Why unresolved: The paper only tests OFA-S against OFA-A on a subset of probabilistic values and utility functions, leaving the general performance comparison open.
- What evidence would resolve it: Extensive empirical studies comparing OFA-S and OFA-A across a wide range of probabilistic values and utility functions, including those not covered in the paper.

### Open Question 3
- Question: What are the theoretical limits of the one-for-all framework in terms of the types of probabilistic values it can approximate?
- Basis in paper: [inferred] The paper focuses on semi-values and weighted Banzhaf values but does not explore the framework's applicability to other types of probabilistic values.
- Why unresolved: The paper does not investigate the theoretical boundaries of the one-for-all framework, such as its applicability to non-semi-values or more complex probabilistic values.
- What evidence would resolve it: A comprehensive theoretical analysis of the framework's limitations, including proofs of impossibility or reductions for certain classes of probabilistic values, along with empirical tests on a broader range of values.

## Limitations
- The framework relies heavily on the boundedness assumption of the utility function, which may not hold for all real-world applications
- The practical implementation details for utility functions are not fully specified, making exact reproduction challenging
- The connection to datamodels requires specific weight vector conditions that may be difficult to satisfy in practice

## Confidence

- **High confidence**: The theoretical convergence rate analysis and the derivation of the D(m,q) formula that governs convergence
- **Medium confidence**: The practical applicability of the framework to real-world utility functions and the experimental results showing superiority over existing methods
- **Low confidence**: The claim that the framework "solves a family of datamodels simultaneously" due to the stringent conditions on weight vectors

## Next Checks

1. **Convergence rate verification**: Run controlled experiments on Beta(1,1) and Beta(1,4) Shapley values with varying n to empirically verify the O(n log n) convergence rate predicted by the D(m,q) formula.

2. **Sampling vector sensitivity**: Systematically test how different sampling vector choices (q) affect the convergence rate and variance, particularly for edge cases where D(m,q) might not be O(1).

3. **Datamodel connection validation**: Implement least squares problems with different regularization parameters and verify that the OFA estimator indeed produces consistent solutions (θ* - θ*j being constant) as claimed.