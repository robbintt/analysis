---
ver: rpa2
title: Model-Based Counterfactual Explanations Incorporating Feature Space Attributes
  for Tabular Data
arxiv_id: '2404.13224'
source_url: https://arxiv.org/abs/2404.13224
tags:
- input
- methods
- fastdcflow
- variables
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FastDCFlow, a model-based method for generating
  diverse and valid counterfactual explanations (CFs) for tabular data. The method
  uses normalizing flows to capture complex data distributions and learns a latent
  space for generating CFs.
---

# Model-Based Counterfactual Explanations Incorporating Feature Space Attributes for Tabular Data

## Quick Facts
- arXiv ID: 2404.13224
- Source URL: https://arxiv.org/abs/2404.13224
- Reference count: 35
- Key outcome: FastDCFlow outperforms existing methods in inner diversity, outer diversity, proximity, and validity while being computationally efficient.

## Executive Summary
This paper introduces FastDCFlow, a model-based method for generating diverse and valid counterfactual explanations for tabular data. The method leverages normalizing flows to learn a latent space for efficient counterfactual generation, combined with TargetEncoding to handle categorical variables while respecting ordinal relationships. FastDCFlow demonstrates superior performance across multiple metrics compared to existing methods while maintaining computational efficiency.

## Method Summary
FastDCFlow is a model-based method that generates counterfactual explanations by learning a normalizing flow to map data distributions into a latent space. The method employs TargetEncoding to transform categorical variables into continuous representations that maintain meaningful ordinal relationships. During training, the model optimizes a combined loss function incorporating negative log-likelihood, validity, and proximity components. For generation, inputs are mapped to the latent space, perturbed, and mapped back to the input space, enabling efficient counterfactual generation without repeated optimization.

## Key Results
- FastDCFlow achieved higher inner diversity (ID) and outer diversity (OD) scores compared to optimization-based methods (DiCE, CeFlow) and other model-based approaches.
- The method demonstrated superior validity (V) rates, with CFs successfully changing the prediction while maintaining proximity (P) to the original input.
- FastDCFlow showed significant computational efficiency gains, with lower run times (RT) compared to optimization-based methods while maintaining or improving CF quality metrics.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FastDCFlow generates diverse counterfactuals efficiently by learning a latent space rather than optimizing per input.
- Mechanism: The method uses normalizing flows to map the data distribution into a latent space, where perturbations can be sampled efficiently. By learning this mapping once during training, CFs for any test input can be generated without repeated optimization.
- Core assumption: The learned latent space captures the structure of the data distribution in a way that supports meaningful perturbations.
- Evidence anchors:
  - [abstract]: "The proposed method outperformed existing methods in multiple metrics, striking a balance between trade offs for counterfactual explanations."
  - [section]: "we leveraged normalizing flows [8, 9, 10] to capture complex data distributions through a reversible latent space."
  - [corpus]: Weak support; no directly comparable mechanism described in neighbor papers.
- Break condition: If the latent space does not preserve the necessary structure for meaningful perturbations, the generated CFs may be invalid or lack diversity.

### Mechanism 2
- Claim: TargetEncoding (TE) transforms categorical variables into continuous variables while respecting ordinal relationships and including perturbation costs.
- Mechanism: TE substitutes categorical values with the average target value for each category, creating a continuous representation that maintains meaningful order and reflects the impact of category changes on the target.
- Core assumption: The average target value for each category provides a meaningful continuous representation that captures the relationship between categories and the target.
- Evidence anchors:
  - [abstract]: "For categorical variables, we employed TargetEncoding, which respects ordinal relationships and includes perturbation costs."
  - [section]: "Thus, categorical variables were effectively transitioned into continuous variables, maintaining a meaningful order of magnitude relative to the predicted values."
  - [corpus]: No direct evidence; neighbor papers focus on different encoding methods.
- Break condition: If the average target values do not capture the true relationship between categories and the target, the continuous representation may be misleading.

### Mechanism 3
- Claim: The combination of validity, proximity, and negative log-likelihood losses in the training objective ensures that generated CFs are valid, close to the original input, and well-calibrated.
- Mechanism: The loss function combines three components: negative log-likelihood to capture data distribution, validity loss to ensure the CF changes the prediction, and proximity loss to keep CFs close to the original input.
- Core assumption: These three loss components can be effectively balanced to generate CFs that satisfy all desired properties.
- Evidence anchors:
  - [abstract]: "The proposed method outperformed existing methods in multiple metrics, striking a balance between trade offs for counterfactual explanations."
  - [section]: "The overall optimization problem can be represented as θ∗ = argmin θ λLnll(θ, D) + Ly(θ, D) + Lwprox(θ, D)."
  - [corpus]: Weak support; no directly comparable loss formulation in neighbor papers.
- Break condition: If the loss components are not properly balanced, the generated CFs may prioritize one property at the expense of others.

## Foundational Learning

- Concept: Normalizing flows
  - Why needed here: Normalizing flows provide a way to learn a reversible transformation between the data distribution and a latent space, which is essential for efficient CF generation.
  - Quick check question: What property of normalizing flows allows them to be used for both density estimation and sampling?
- Concept: Counterfactual explanations
  - Why needed here: Counterfactual explanations are the target output of the method, so understanding their properties (validity, proximity, diversity) is crucial for designing the method and evaluating its performance.
  - Quick check question: What are the three main properties that counterfactual explanations should satisfy?
- Concept: Categorical variable encoding
  - Why needed here: Tabular data often contains categorical variables, which need to be encoded in a way that allows meaningful perturbations for CF generation.
  - Quick check question: Why is OneHotEncoding not suitable for counterfactual explanations of categorical variables?

## Architecture Onboarding

- Component map: Input data -> TargetEncoding -> Normalizing Flow (RealNVP coupling layers) -> Latent space perturbation -> Inverse mapping -> Generated CFs
- Critical path: Training involves learning the normalizing flow parameters by minimizing the combined loss. Generation involves mapping the input to the latent space, perturbing it, and mapping back to the input space.
- Design tradeoffs: Using normalizing flows allows efficient generation but may not capture all data distributions perfectly. TargetEncoding provides a meaningful continuous representation but may lose some categorical information.
- Failure signatures: If the generated CFs are not diverse, it may indicate that the latent space does not capture the necessary structure. If the CFs are not valid, it may indicate that the validity loss is not properly balanced.
- First 3 experiments:
  1. Train the model on a simple dataset (e.g., Adult) and verify that it can generate valid CFs.
  2. Compare the diversity of CFs generated by the model with those generated by an optimization-based method.
  3. Evaluate the effect of the temperature parameter on the diversity and proximity of generated CFs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of temperature parameter t in FastDCFlow affect the trade-off between diversity and validity of counterfactual explanations across different types of tabular datasets?
- Basis in paper: [explicit] The paper discusses the effect of the temperature parameter on the diversity (ID and OD) and proximity (P) metrics, showing a trade-off between internal diversity and proximity.
- Why unresolved: The paper does not provide a comprehensive analysis of how the temperature parameter impacts the validity metric (V) or how it performs across different dataset types (e.g., datasets with varying levels of categorical vs. continuous variables).
- What evidence would resolve it: A systematic study varying the temperature parameter across multiple datasets with different characteristics (e.g., varying ratios of categorical to continuous variables) and analyzing its impact on all evaluation metrics, particularly validity.

### Open Question 2
- Question: Can FastDCFlow be extended to handle datasets with a large number of categorical levels or rare categorical levels without overfitting?
- Basis in paper: [inferred] The paper mentions that TargetEncoding (TE) can lead to overfitting when applied to categories with rarely seen levels or data-deprived categories.
- Why unresolved: The paper does not explore techniques to mitigate overfitting in such scenarios or provide empirical results demonstrating FastDCFlow's performance on datasets with rare categorical levels.
- What evidence would resolve it: Experiments comparing FastDCFlow's performance on datasets with rare categorical levels using different encoding techniques (e.g., frequency-based encoding, embedding-based encoding) and regularization methods to prevent overfitting.

### Open Question 3
- Question: How does the performance of FastDCFlow compare to other state-of-the-art methods when applied to high-dimensional sparse tabular data?
- Basis in paper: [explicit] The paper mentions that both input-based and model-based methods require ML training, and sparse high-dimensional data can hinder the utility of counterfactual explanations.
- Why unresolved: The paper does not include experiments on high-dimensional sparse datasets or compare FastDCFlow's performance to other methods specifically in this context.
- What evidence would resolve it: A comparative study of FastDCFlow and other methods (e.g., DiCE, CeFlow, CF V AE) on high-dimensional sparse tabular datasets, evaluating their performance in terms of diversity, validity, proximity, and computational efficiency.

## Limitations

- The paper does not provide a sensitivity analysis for the temperature parameter, which affects the trade-off between diversity and proximity of generated CFs.
- Specific implementation details for the weighted proximity loss and RealNVP coupling layer hyperparameters are underspecified, making exact reproduction difficult.
- The performance of TargetEncoding for categories with rarely seen levels is not thoroughly evaluated, and potential overfitting issues are not addressed with alternative encoding strategies.

## Confidence

- **High confidence**: The core mechanism of using normalizing flows for efficient CF generation is well-established and the theoretical framework is sound.
- **Medium confidence**: The TargetEncoding approach for categorical variables is reasonable but lacks comprehensive comparison with alternative encoding strategies.
- **Medium confidence**: The combined loss function successfully balances multiple objectives, though the optimal weighting between components is not fully justified.

## Next Checks

1. Perform sensitivity analysis on the temperature parameter t to quantify its effect on the diversity-proximity trade-off and determine optimal settings for different datasets.
2. Compare TargetEncoding against alternative categorical encoding methods (such as frequency encoding or learned embeddings) to assess whether the observed benefits are encoding-specific or method-general.
3. Evaluate model robustness by testing on out-of-distribution inputs and measuring how CF validity and diversity degrade when inputs differ from the training distribution.