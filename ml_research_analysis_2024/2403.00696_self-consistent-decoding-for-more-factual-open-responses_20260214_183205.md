---
ver: rpa2
title: Self-Consistent Decoding for More Factual Open Responses
arxiv_id: '2403.00696'
source_url: https://arxiv.org/abs/2403.00696
tags:
- decoding
- sentence
- each
- score
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces a novel decoding method for large language\
  \ models that iteratively samples and selects sentences based on token overlap consistency\
  \ scores, aiming to reduce hallucinations in open-ended text generation. By re-sampling\
  \ and choosing the most consistent sentence at each step, conditioning on previously\
  \ selected content, the method significantly improves factuality\u2014achieving\
  \ up to 30% relative improvement in NLI-based factuality metrics over strong baselines\
  \ such as greedy decoding, beam search, nucleus sampling, and recently proposed\
  \ hallucination-aware decoders."
---

# Self-Consistent Decoding for More Factual Open Responses

## Quick Facts
- arXiv ID: 2403.00696
- Source URL: https://arxiv.org/abs/2403.00696
- Authors: Christopher Malon; Xiaodan Zhu
- Reference count: 9
- Primary result: Sample & Select decoding achieves up to 30% relative improvement in factuality metrics over strong baselines for summarization

## Executive Summary
This paper introduces Sample & Select, a novel decoding method that improves factuality in open-ended text generation by iteratively sampling and selecting sentences based on token overlap consistency scores. The method generates multiple candidate sentences at each step, filters for grammaticality, and selects the sentence with highest token overlap with other samples. By conditioning subsequent sampling on previously selected content, the approach reduces hallucinations while maintaining competitive ROUGE scores. Experiments on CNN/DM and XSum datasets show significant improvements in factuality metrics (SummaC, QAFactEval) compared to greedy decoding, beam search, nucleus sampling, and recently proposed hallucination-aware decoders.

## Method Summary
The Sample & Select method generates open responses iteratively by sampling n sentences (typically n=5) using nucleus sampling (p=0.9), filtering for grammaticality, scoring each sentence by token overlap consistency with other samples, and selecting the highest-scoring sentence. The selected sentence is appended to the output and used to condition the next sampling step. This process repeats until a stopping condition is met. The token overlap score counts how many times each token in a candidate sentence appears across all other sampled sentences, with higher counts indicating more factual content. The method is zero-shot, requiring no fine-tuning, and can be applied to any large language model.

## Key Results
- Sample & Select achieves up to 30% relative improvement in NLI-based factuality metrics (SummaC) over strong baselines
- The method maintains competitive ROUGE-1 F1 scores while significantly improving factuality
- Human evaluations confirm reduced unsupported statements compared to baseline methods
- Conditioning later sentences on previously chosen samples is critical for factuality improvement
- Token overlap consistency scoring outperforms alternative hallucination detection methods like SelfCheckGPT

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative re-sampling conditioned on previously selected sentences improves factuality.
- Mechanism: By sampling multiple candidate sentences and selecting the one with highest token overlap consistency score, then using that selected sentence as context for the next sampling step, the method enforces consistency across the generated text.
- Core assumption: Sentences that share more tokens with other sampled sentences are more likely to be factually accurate.
- Evidence anchors:
  - [abstract] "Each output sentence is selected from among multiple samples, conditioning on the previous selections, based on a simple token overlap score."
  - [section 3] "Additional sentences are added to the final output iteratively...collecting new samples conditioning on the prompt including previously chosen sentences."
  - [corpus] "Average neighbor FMR=0.532" - moderate similarity in related works suggests this approach is novel but related to existing consistency techniques.
- Break condition: If no sampled sentence passes the grammaticality filter, generation aborts. This could happen if the LLM consistently generates incomplete or ungrammatical fragments under the sampling constraints.

### Mechanism 2
- Claim: Token overlap consistency scoring outperforms other hallucination detection scores.
- Mechanism: The score counts how many times each token in a candidate sentence appears across all other sampled sentences, with higher counts indicating more factual content.
- Core assumption: Factual details are more likely to be repeated across multiple samples than hallucinated details.
- Evidence anchors:
  - [abstract] "Each output sentence is selected...based on a simple token overlap score."
  - [section 3] "This score embodies the intuition that details are represented by tokens, and that each trustworthy detail should occur in a large number of other samples."
  - [section 5] "SelfCheckGPT was at least 10% worse in NLI-based evaluation" - comparison shows token overlap scoring is superior to alternative hallucination detection methods.
- Break condition: The scoring mechanism could fail if the LLM generates multiple consistent but factually incorrect responses (group hallucination), causing incorrect sentences to receive high overlap scores.

### Mechanism 3
- Claim: Conditioning later sentences on previously chosen samples is critical for factuality improvement.
- Mechanism: The "independent" ablation baseline decodes entire responses then selects sentences without reconditioning, showing worse performance than the full method.
- Core assumption: Each sentence should be generated in context of the final, fact-checked content chosen so far.
- Evidence anchors:
  - [section 5] "The comparison with independent shows that conditioning later sentences on finally chosen samples is critical."
  - [abstract] "Each output sentence is selected from among multiple samples, conditioning on the previous selections"
  - [corpus] No direct evidence found, but the moderate FMR suggests related work exists on conditional generation.
- Break condition: If the first selected sentence is factually incorrect, all subsequent sentences will be conditioned on that error, potentially compounding the hallucination throughout the generation.

## Foundational Learning

- Concept: Conditional probability in language modeling
  - Why needed here: Understanding that each token prediction depends on both the prompt and previously generated tokens is essential to grasp why conditioning on selected sentences matters
  - Quick check question: If a language model generates "The capital of France is" and then conditions on "Paris" vs "London" for the next token, how does this affect the probability distribution of the next word?

- Concept: Self-consistency voting mechanisms
  - Why needed here: The method extends self-consistency from short answer extraction to open response generation by applying voting at the sentence level
  - Quick check question: In traditional self-consistency, multiple samples are generated and the most frequent answer is selected. How does applying this principle at the sentence level differ from applying it to entire responses?

- Concept: Factuality evaluation metrics
  - Why needed here: Understanding SummaC (NLI-based) and QAFactEval (question-answering based) metrics is crucial for interpreting the experimental results
  - Quick check question: SummaC uses natural language inference to determine if generated text is supported by source articles, while QAFactEval generates questions from summaries. What are the strengths and weaknesses of each approach?

## Architecture Onboarding

- Component map:
  - LLM inference engine (Llama 2 or Mistral)
  - Nucleus sampling module with p=0.9
  - Sentence segmentation component
  - Token overlap consistency scoring module
  - Grammaticality filter (POS tagging + dependency parsing)
  - Iterative generation loop with conditioning
  - Evaluation pipeline (SummaC, QAFactEval, ROUGE)

- Critical path: Generate n samples → Apply grammaticality filter → Compute token overlap scores → Select highest-scoring sentence → Append to output and prompt → Repeat until end condition

- Design tradeoffs:
  - Sampling n=5 sentences provides good diversity but increases computation time
  - Token overlap is simple but may miss semantic consistency
  - Grammaticality filter prevents fragments but could reject valid stylistic choices
  - Zero-shot summarization avoids fine-tuning but may perform worse than specialized models

- Failure signatures:
  - Generation aborts due to no grammatical sentences passing filter
  - Consistently low factuality scores despite high ROUGE scores (indicating fluent but unsupported content)
  - High variance in token overlap scores suggesting sampling instability
  - Grammaticality filter rejecting too many valid sentences (tuning needed)

- First 3 experiments:
  1. Run baseline comparison: Greedy decoding vs Nucleus sampling vs Beam search on same articles, measure SummaC and QAFactEval scores
  2. Ablation study: Implement "independent" baseline (decode entire responses then select sentences without reconditioning) and compare to full method
  3. Hyperparameter sweep: Test different values of n (number of samples) and p (nucleus sampling threshold) to find optimal configuration for factuality vs efficiency tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the token overlap consistency score remain effective for languages with different morphological structures or writing systems compared to English?
- Basis in paper: [inferred] The authors acknowledge that their method is sensitive to tokenization and sentence splitting, and that the grammar filter was designed specifically for English text.
- Why unresolved: The paper only evaluates the method on English language news articles. The scoring mechanism relies on token overlap, which may not translate well to languages with different tokenization needs or morphological complexity.
- What evidence would resolve it: Evaluating the method on non-English datasets with different morphological structures and writing systems, comparing performance to the English results, and potentially adapting the scoring mechanism for these languages.

### Open Question 2
- Question: How does the Sample & Select method perform on generation tasks beyond summarization, such as dialogue generation or creative writing?
- Basis in paper: [explicit] The authors state that "it is beyond the scope of the current paper, Sample & Select can apply beyond summarization to other generation tasks" and hope future researchers will find it helpful for other tasks.
- Why unresolved: The paper only evaluates the method on summarization tasks. Different generation tasks may have different requirements for factuality, creativity, or coherence that could affect the method's performance.
- What evidence would resolve it: Applying the method to various generation tasks beyond summarization, measuring performance using task-specific metrics, and comparing to existing methods for those tasks.

### Open Question 3
- Question: What is the computational overhead of the Sample & Select method compared to other decoding methods, and how does it scale with input length or model size?
- Basis in paper: [inferred] The authors mention that "The time to compute token overlap scores is negligible, so the method could run as efficiently as collecting multiple samples from nucleus sampling, if the implementation would stop the generation as soon as the end of a sentence is reached."
- Why unresolved: While the authors suggest the method could be efficient, they don't provide actual runtime comparisons with other decoding methods or analyze how the method scales with different input lengths or model sizes.
- What evidence would resolve it: Conducting runtime experiments comparing Sample & Select to other decoding methods on various input lengths and model sizes, measuring both wall-clock time and computational resources used.

## Limitations

- The method's effectiveness for non-summarization tasks remains unproven despite theoretical applicability
- The grammaticality filter implementation details are not provided, potentially affecting reproducibility
- Zero-shot approach may underperform specialized fine-tuned summarization models on complex tasks

## Confidence

**High Confidence** in the following claim clusters:
- The iterative re-sampling and selection process improves factuality compared to standard decoding methods
- The token overlap consistency scoring mechanism is superior to alternative hallucination detection methods
- The conditioning mechanism is critical for performance

**Medium Confidence** in the following claim clusters:
- The method maintains competitive ROUGE scores while improving factuality
- The approach generalizes to different LLM architectures
- Human evaluation confirms the quantitative findings

## Next Checks

1. **Ablation of Grammar Filtering**: Implement the decoding method without the grammaticality filter to quantify its contribution to factuality improvements. Compare factuality metrics with and without filtering across multiple sampling rates (n=3, n=5, n=10) to determine if the filter is essential or if sampling alone provides sufficient quality control.

2. **Cross-Domain Factuality Testing**: Apply the method to non-news domains such as scientific papers, legal documents, or technical manuals where factual consistency requirements differ from news summarization. Measure whether the token overlap mechanism remains effective when the definition of "factual" extends beyond simple source-article correspondence.

3. **Group Hallucination Vulnerability Test**: Design a controlled experiment where the LLM is prompted to generate consistent but factually incorrect responses (e.g., asking about fictional events or deliberately misleading content). Test whether the token overlap mechanism correctly identifies hallucinated content even when multiple samples agree on the same incorrect information, or if it reinforces group hallucinations by selecting the most consistent (but wrong) responses.