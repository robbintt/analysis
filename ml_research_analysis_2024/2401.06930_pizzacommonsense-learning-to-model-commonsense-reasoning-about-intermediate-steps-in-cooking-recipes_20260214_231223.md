---
ver: rpa2
title: 'PizzaCommonSense: Learning to Model Commonsense Reasoning about Intermediate
  Steps in Cooking Recipes'
arxiv_id: '2401.06930'
source_url: https://arxiv.org/abs/2401.06930
tags:
- output
- input
- crackers
- cooking
- cheese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PizzaCommonSense, a new dataset for learning
  commonsense reasoning about intermediate steps in cooking recipes. The dataset contains
  1087 pizza recipes parsed into atomic cooking steps, with each step annotated with
  the input and output preparations.
---

# PizzaCommonSense: Learning to Model Commonsense Reasoning about Intermediate Steps in Cooking Recipes

## Quick Facts
- arXiv ID: 2401.06930
- Source URL: https://arxiv.org/abs/2401.06930
- Authors: Aissatou Diallo; Antonis Bikakis; Luke Dickens; Anthony Hunter; Rob Miller
- Reference count: 14
- Key outcome: New dataset for commonsense reasoning about intermediate steps in cooking recipes, achieving 16.2% exact matching accuracy for input and 54.5% for output with T5, and 32.6% exact matching accuracy for input with GPT-3.5 fine-tuning

## Executive Summary
This paper introduces PizzaCommonSense, a new dataset designed to develop models that can reason about intermediate steps in cooking recipes by predicting both input and output preparations for each instruction. The dataset contains 1087 pizza recipes parsed into atomic cooking steps, with each step annotated for instruction, input, action, and output. The authors present baseline methods using T5 and GPT-3.5 models, demonstrating that the task is challenging and requires sophisticated commonsense reasoning about ingredient transformations. The work aims to bridge the gap between natural language instructions and the actual state changes that occur during cooking.

## Method Summary
The authors created PizzaCommonSense by parsing 1087 pizza recipes from Recipe1M into atomic steps, each containing a single cooking action. Crowd workers annotated each step with input and output preparations, including both explicit and implicit ingredients. The dataset was split using distribution-based clustering to ensure diverse representation. Baseline models were developed using T5-base and Flan-T5-base fine-tuned on serialized tabular representations of the recipe data, as well as GPT-3.5 with in-context learning and fine-tuning approaches. Models were evaluated using exact matching accuracy, BLEU, ROUGE, METEOR, and BERTScore metrics.

## Key Results
- T5-base achieves 16.2% exact matching accuracy for input predictions and 54.5% for output predictions
- GPT-3.5 fine-tuning achieves 32.6% exact matching accuracy for input predictions and 90.6% BERTScore for output predictions
- All models struggle with longer recipes and capturing implicit ingredient transformations
- GPT-3.5 with in-context learning performs competitively without requiring fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The task requires understanding implicit and explicit ingredient transformations across recipe steps, forcing the model to infer unstated intermediate states.
- Mechanism: By annotating both input and output states for each step, the dataset creates supervision signals that bridge the gap between natural language instructions and the actual state changes in cooking. This explicit labeling of transformations forces models to learn procedural reasoning rather than simple pattern matching.
- Core assumption: Cooking instructions contain sufficient implicit information about intermediate states that can be extracted through commonsense reasoning.
- Evidence anchors:
  - [abstract]: "For a model to effectively reason about cooking recipes, it must accurately discern and understand the inputs and outputs of intermediate steps within the recipe."
  - [section]: "The intermediate steps of a cooking recipe are the steps that are performed after the initial preparation and before the final plating and presentation of the dish."
  - [corpus]: Weak - the corpus search found related papers on recipe datasets and procedural text understanding, but none specifically address intermediate state reasoning with explicit input/output annotations.

### Mechanism 2
- Claim: Fine-tuning T5 and Flan-T5 models on the serialized tabular format allows them to learn the mapping between instructions and their corresponding input/output states.
- Mechanism: The span mask denoising objective in T5 is adapted to predict missing input/output values in the serialized recipe table. The model learns to generate coherent descriptions of intermediate states based on the instruction and action columns.
- Core assumption: The serialized tabular format preserves enough semantic information for the model to learn meaningful relationships between instructions and state transformations.
- Evidence anchors:
  - [section]: "We take advantage of this pretrained objective and we build a baseline through fine tuning the pretrained T5-base model in a sequence to sequence fashion."
  - [section]: "For doing this, we use as input the serialized table with the input and output masked out and the output are the content of corresponding input and output."
  - [corpus]: Weak - no corpus evidence directly supports this mechanism, but related work on tabular data serialization for LLMs exists.

### Mechanism 3
- Claim: GPT-3.5's in-context learning capability allows it to leverage demonstration examples to predict input/output pairs without requiring extensive fine-tuning.
- Mechanism: By providing demonstration examples from the training set alongside test samples, GPT-3.5 can infer the task format and generate appropriate input/output descriptions through pattern recognition and few-shot learning.
- Core assumption: GPT-3.5's few-shot learning capability is sufficient to learn the complex task of predicting intermediate states from demonstrations alone.
- Evidence anchors:
  - [section]: "Due to the specificity of the structure of the task, we investigate in-context learning setting for predicting the input/output pairs."
  - [section]: "To do this, for each test sample from the annotated dataset, we sample a recipe table from the training set which is used as demonstration for GPT-3.5 to perform the task."
  - [corpus]: Weak - no corpus evidence directly supports this mechanism, but in-context learning is a well-established technique for LLMs.

## Foundational Learning

- Concept: Procedural text understanding and state tracking
  - Why needed here: The task requires understanding how ingredients transform through sequential cooking steps and tracking these state changes
  - Quick check question: Can you explain how the state of an ingredient changes when it goes from "chopped onion" to "sautéed onion"?

- Concept: Commonsense reasoning in cooking contexts
  - Why needed here: Many intermediate states are not explicitly stated in recipes and require inference based on cooking knowledge
  - Quick check question: What implicit transformation occurs when you "season the sauce" and how would you describe the input/output states?

- Concept: Natural language generation for procedural descriptions
  - Why needed here: The model must generate coherent, descriptive text for intermediate states rather than just listing ingredients
  - Quick check question: How would you describe the transformation of "flour and water" when mixed together as an output state?

## Architecture Onboarding

- Component map: Recipe → Preprocessing → Annotation → Model Training → Evaluation → Analysis
- Critical path: Recipe → Preprocessing → Annotation → Model Training → Evaluation → Analysis
- Design tradeoffs:
  - T5-based models require fine-tuning but can learn task-specific patterns, while GPT-3.5 can leverage in-context learning but may need extensive demonstrations
  - Exact matching accuracy is strict but may not capture semantic correctness, while BERTScore captures semantic similarity but may miss exact state descriptions
  - Random splits may show better performance due to distribution differences, but distribution-based splits provide more realistic generalization testing
- Failure signatures:
  - Models predicting non-comestibles in input/output pairs
  - Missing predictions for longer recipes or complex transformations
  - Non-descriptive predictions that don't convey intermediate state information
  - Incorrect handling of implicit vs explicit ingredients
- First 3 experiments:
  1. Fine-tune T5-base on the PizzaCommonSense dataset using the serialized tabular format and evaluate on the test set using exact matching accuracy
  2. Implement GPT-3.5 in-context learning with demonstration examples and compare performance to T5-based models
  3. Create a random split version of the dataset and compare model performance to distribution-based splits to understand generalization capabilities

## Open Questions the Paper Calls Out
- How can the PizzaCommonSense dataset be expanded to include multiple interpretations for each instruction, and what impact would this have on model performance and generalizability?
- What are the most effective techniques for handling non-comestibles and non-descriptive predictions in the output of language models when predicting input/output pairs for cooking recipes?
- How can the evaluation metrics used in the PizzaCommonSense task be improved to better capture the semantic meaning and context of the generated descriptions, beyond simple n-gram overlap and exact matching?

## Limitations
- The dataset's focus on pizza recipes creates domain-specific bias that may not generalize to other cooking domains
- The strict exact matching accuracy metric may underestimate models' ability to capture semantically correct intermediate states
- The annotation process relies on human judgment for implicit ingredient identification, which may introduce inconsistencies

## Confidence
- High confidence: The dataset construction methodology and the general task formulation are sound and well-justified
- Medium confidence: The effectiveness of the T5-based approaches is supported by the results, but the comparison with GPT-3.5 is limited by available resources
- Low confidence: The evaluation metrics, particularly the combination of exact matching accuracy with semantic similarity measures, may not provide a complete picture of model performance

## Next Checks
1. Test model generalization by evaluating on a diverse set of non-pizza recipes to assess whether the learned reasoning capabilities transfer across cooking domains
2. Conduct a human evaluation study to assess the semantic correctness of model predictions, comparing them against exact matching accuracy to understand the trade-off between precision and meaningfulness
3. Experiment with different serialization formats and input representations to determine if the current approach is optimal for capturing the relationships between instructions and intermediate states