---
ver: rpa2
title: Discrete Multimodal Transformers with a Pretrained Large Language Model for
  Mixed-Supervision Speech Processing
arxiv_id: '2406.06582'
source_url: https://arxiv.org/abs/2406.06582
tags:
- speech
- training
- text
- discrete
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a decoder-only Discrete Multimodal Language
  Model (DMLM) that can perform multiple tasks across text, speech, and vision modalities.
  The key method innovation is a length-normalized tri-modal loss function that accounts
  for varying token lengths across modalities.
---

# Discrete Multimodal Transformers with a Pretrained Large Language Model for Mixed-Supervision Speech Processing

## Quick Facts
- arXiv ID: 2406.06582
- Source URL: https://arxiv.org/abs/2406.06582
- Authors: Viet Anh Trinh; Rosy Southwell; Yiwen Guan; Xinlu He; Zhiyong Wang; Jacob Whitehill
- Reference count: 0
- Primary result: DMLM achieves WER 10.6% on LibriSpeech-other while performing multiple multimodal tasks

## Executive Summary
This paper introduces a decoder-only Discrete Multimodal Language Model (DMLM) that performs multiple tasks across text, speech, and vision modalities. The key innovation is a length-normalized tri-modal loss function that addresses varying token lengths across modalities during training. The model is initialized from a pretrained LLM (OPT-125M) and trained with mixed supervision combining labeled and unlabeled data. Experiments demonstrate that DMLM achieves competitive speech recognition performance (WER 10.6% on LibriSpeech-other) while also performing text-to-speech, speech-to-text translation, and image captioning tasks.

## Method Summary
DMLM is a decoder-only transformer model initialized from OPT-125M LLM, operating on discrete tokens from text, speech, and image modalities. The model uses a length-normalized tri-modal loss function that divides cross-entropy losses by token counts for each modality (nS, nT, nI) and weights them with modality-specific λ values (λS=0.25, λT=0.93, λI=0.25). Training combines supervised data (speech-text pairs) with unsupervised data (speech-only or text-only) to improve generalization. Speech is tokenized using either Seamless codec or K-means clustering on Whisper activations, while text uses OPT-125 tokenizer and images use DALL-E-based tokenization.

## Key Results
- DMLM achieves WER 10.6% on LibriSpeech-other test set for speech recognition
- Whisper-derived codebooks outperform Seamless codebooks by 0.5% WER in ASR tasks
- Model shows consistent accuracy improvements from LLM pretraining, particularly on out-of-domain AMI dataset
- Mixed supervision training improves performance across all tasks compared to supervised-only training
- DMLM achieves competitive results on image captioning (CIDEr 99.2 on COCO) while maintaining strong ASR performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Length-normalized tri-modal loss enables stable training across modalities with different token lengths
- Mechanism: Normalizing loss by token count (nS, nT, nI) prevents longer sequences from dominating gradient updates
- Core assumption: Token count is a good proxy for computational weight across modalities
- Evidence anchors: [abstract] "length-normalized tri-modal loss function", [section 3] "normalizes the lengths of tokens"
- Break condition: If token count doesn't reflect computational importance or token distributions change significantly

### Mechanism 2
- Claim: LLM pretraining provides linguistic knowledge that improves speech recognition
- Mechanism: OPT initialization transfers linguistic representations beneficial for speech-to-text relationships
- Core assumption: Linguistic knowledge from LLM is transferable to speech processing
- Evidence anchors: [abstract] "DMLM benefits significantly from LLM pretraining", [section 4.2] "large difference for out-of-domain AMI dataset"
- Break condition: If pretrained knowledge conflicts with speech-specific patterns or LLM data is too dissimilar

### Mechanism 3
- Claim: Mixed-supervision training improves performance across multiple tasks
- Mechanism: Learning from both labeled speech-text pairs and unlabeled data improves generalization
- Core assumption: Unsupervised data contains useful patterns that complement supervised training
- Evidence anchors: [abstract] "DMLM benefits significantly... from a combination of supervised and unsupervised training", [section 4.3] "unsupervised training... improves WER on LibriSpeech-other"
- Break condition: If unsupervised data introduces noise or conflicting patterns

## Foundational Learning

- Concept: Discrete speech tokenization and codebooks
  - Why needed here: DMLM operates on discrete tokens requiring understanding of speech-to-discrete representation conversion
  - Quick check question: What is the difference between Seamless and Whisper-derived codebooks, and how does this affect ASR performance?

- Concept: Transformer decoder architecture and masked language modeling
  - Why needed here: DMLM is decoder-only, requiring understanding of causal attention and autoregressive generation
  - Quick check question: How does decoder-only architecture differ from encoder-decoder models in handling multimodal inputs?

- Concept: Cross-entropy loss and its normalization
  - Why needed here: Loss function is modified to account for different token lengths across modalities
  - Quick check question: Why is simply summing cross-entropy losses across modalities problematic when token counts differ significantly?

## Architecture Onboarding

- Component map: Raw input → modality tokenizer → discrete tokens → shared embedding layer (extended from OPT) → transformer decoder → output token probabilities → loss computation (with length-normalized tri-modal loss)
- Critical path: Discrete tokens flow through shared embedding, processed by transformer decoder with causal attention, generating next-token probabilities for each modality
- Design tradeoffs: Discrete tokens enable unified processing but may lose fine-grained information; OPT initialization provides linguistic knowledge but may introduce biases; mixed supervision leverages more data but requires careful handling of unlabeled inputs
- Failure signatures: High WER indicates codebook or loss function issues; poor image captioning suggests vision tokenizer problems; training instability often stems from incorrect λ values
- First 3 experiments:
  1. Verify length-normalized loss by training on small multimodal dataset with known token length differences
  2. Test LLM initialization effect by comparing random vs. OPT initialization on speech recognition with in/out-domain test sets
  3. Validate mixed supervision by training with only supervised data, then incrementally adding unsupervised data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DMLM performance vary when trained on datasets with different modality distributions?
- Basis in paper: [inferred] Paper mentions DMLM can be trained with mixed supervision but doesn't explore dataset modality distribution impact
- Why unresolved: No experiments comparing performance across datasets with varying modality distributions
- What evidence would resolve it: Experiments training DMLM on datasets with different modality distributions and comparing downstream task performance

### Open Question 2
- Question: What is the optimal balance between supervised and unsupervised data for training DMLM across different tasks?
- Basis in paper: [explicit] Paper explores mixed-supervision training but doesn't systematically investigate optimal supervised-to-unsupervised ratio
- Why unresolved: While showing benefits of unsupervised training, paper doesn't determine ideal proportion for each task
- What evidence would resolve it: Comprehensive study varying supervised-to-unsupervised ratios and measuring resulting performance

### Open Question 3
- Question: How does choice of initial layer for clustering in Whisper-based tokenization affect DMLM performance?
- Basis in paper: [explicit] Paper compares Whisper-derived to XLS-R tokens but doesn't explore impact of using different encoder layers
- Why unresolved: Only uses final Whisper layer, leaving question of whether other layers might be more effective
- What evidence would resolve it: Experiments training DMLM with tokenization from different Whisper encoder layers and comparing performance

### Open Question 4
- Question: Can DMLM effectively handle real-time processing requirements for tasks like speech-to-speech translation?
- Basis in paper: [inferred] Paper doesn't discuss latency or real-time processing capabilities
- Why unresolved: Demonstrates capabilities but doesn't address suitability for real-time applications
- What evidence would resolve it: Benchmarks measuring inference speed and latency on streaming data for real-time tasks

## Limitations

- Limited empirical evidence for core mechanisms primarily supported by ablation studies on the same model rather than independent validation
- Token length normalization assumption may not hold when token distributions or semantic densities differ substantially across modalities
- Discrete tokenization quality significantly impacts performance but lacks detailed analysis of why Whisper-derived codebooks perform better

## Confidence

**High confidence** in overall framework design and task formulations. The decoder-only multimodal transformer architecture is well-established and task formulations are clearly specified with appropriate evaluation metrics.

**Medium confidence** in specific contributions of individual mechanisms. Ablation studies support benefits of each component, but interactions between mechanisms are not fully characterized and magnitude of improvements varies across tasks.

**Low confidence** in generalizability of length-normalized loss function and optimality of proposed λ values. Paper provides theoretical justification but limited empirical validation across different modalities, tokenizers, or task combinations.

## Next Checks

1. Systematically vary λ values across a wider range and measure performance degradation to establish sensitivity curves and validate whether current configuration is near-optimal

2. Train identical DMLM models using different speech tokenization approaches (Seamless, Whisper-derived, HuBERT, Wav2Vec) on same datasets to isolate tokenization quality impact from model architecture

3. Train separate models isolating each mechanism (LLM pretraining only, mixed supervision only, length-normalized loss only) while controlling for other variables, then compare performance against full DMLM on in/out-of-domain datasets