---
ver: rpa2
title: 'XNLIeu: a dataset for cross-lingual NLI in Basque'
arxiv_id: '2404.06996'
source_url: https://arxiv.org/abs/2404.06996
tags:
- basque
- language
- xnlieu
- dataset
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces XNLIeu, a cross-lingual natural language
  inference (NLI) dataset for Basque. The dataset was created by machine translating
  and post-editing the English XNLI corpus, and includes a smaller native Basque test
  set for comparison.
---

# XNLIeu: a dataset for cross-lingual NLI in Basque

## Quick Facts
- arXiv ID: 2404.06996
- Source URL: https://arxiv.org/abs/2404.06996
- Reference count: 18
- Primary result: Post-edition improves Basque XNLI dataset quality and translate-train is the best cross-lingual strategy, though gains are smaller on a native test set.

## Executive Summary
This paper introduces XNLIeu, a cross-lingual NLI dataset for Basque created by machine translating and post-editing the English XNLI corpus, plus a smaller native Basque test set. Experiments with various monolingual and multilingual models show that post-edition is important for dataset quality and that translate-train cross-lingual strategy works best for Basque NLI. The native dataset is less biased and harder for models, suggesting that translated datasets may contain artifacts that models exploit.

## Method Summary
The authors created XNLIeu by machine translating the English XNLI development and test sets into Basque and then performing professional post-editing to correct translation artifacts. They also constructed a small native Basque test set by scraping recent news sentences and having native speakers write entailment, neutral, and contradiction hypotheses. Various monolingual and multilingual NLI models were fine-tuned using translate-train and zero-shot strategies and evaluated on the post-edited, machine-translated, and native Basque test sets.

## Key Results
- Post-edition is important for dataset quality, as models perform worse on the machine-translated XNLIeuMT compared to the post-edited XNLIeu.
- Translate-train cross-lingual strategy obtains better results overall on all models, with slightly higher differences in the XNLIeuMT dataset.
- Discriminative models perform worse on the native dataset, with approximately 10% lower accuracy on average, suggesting the native set is less biased.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Post-edition improves dataset quality by removing translation artifacts.
- Mechanism: Professional translators correct machine translation errors that alter semantic relations (e.g., polarity flips, incorrect auxiliaries, omissions).
- Core assumption: Translationese artifacts in XNLIeuMT bias models toward superficial cues.
- Evidence anchors:
  - [abstract] "post-edition is important for dataset quality"
  - [section 5.1] "systems perform consistently worse when evaluated on the machine-translated XNLIeuMT dataset compared to the post-edited XNLIeu"
  - [corpus] weak - dataset neighbors mostly unrelated to Basque; missing direct corpus-level quality signal.
- Break condition: If post-edition is superficial or fails to address systematic translation errors.

### Mechanism 2
- Claim: Translate-train yields better results than zero-shot because training and test data match in origin.
- Mechanism: Aligning the origin of train/test data (both translated) reduces distribution shift and translationese artifacts.
- Core assumption: Mismatched origins harm cross-lingual transfer more than language proximity.
- Evidence anchors:
  - [abstract] "translate-train cross-lingual strategy obtains better results overall"
  - [section 5.1] "Translate-train obtains better results overall on all models, and the difference is slightly higher in the XNLIeuMT dataset"
  - [corpus] weak - no corpus-level evidence linking translationese artifacts to model performance gaps.
- Break condition: If translation quality is poor or source language differs too much from target.

### Mechanism 3
- Claim: Native test set is less biased, making it harder for models.
- Mechanism: Without lexical overlap and negation artifacts, models must rely on deeper semantic understanding.
- Core assumption: Dataset biases (e.g., lexical overlap, negation cues) enable shortcut learning.
- Evidence anchors:
  - [section 3.1] "the native dataset does not seem to be biased towards negation words" and "no specific biases were detected in neutral instances"
  - [section 5.2] "Discriminative models perform worse on the native dataset, with approximately 10% lower accuracy on average"
  - [corpus] weak - corpus neighbors do not address bias analysis or native dataset construction.
- Break condition: If models adapt to rely on other surface cues or if bias-free datasets become standard.

## Foundational Learning

- Concept: Cross-lingual transfer learning
  - Why needed here: Basque is low-resource; transfer from high-resource languages is necessary for NLI.
  - Quick check question: What is the difference between zero-shot and translate-train cross-lingual strategies?

- Concept: Translationese and artifacts in NLI datasets
  - Why needed here: Understanding how translation artifacts bias models is key to interpreting results.
  - Quick check question: What are common artifacts in NLI datasets that models exploit?

- Concept: Lexical overlap and semantic relation cues
  - Why needed here: These cues are central to why post-edition and native datasets affect performance.
  - Quick check question: How does lexical overlap between premise and hypothesis relate to entailment?

## Architecture Onboarding

- Component map: English XNLI -> Machine Translation -> Post-edition -> XNLIeu (train) and Native Test Set -> Fine-tuning/Prompting -> Evaluation
- Critical path: MT -> post-edition -> dataset release -> model training (translate-train or zero-shot) -> evaluation
- Design tradeoffs: Post-edition cost vs. dataset quality; translate-train vs. zero-shot generalization
- Failure signatures: Low accuracy on native set suggests reliance on dataset artifacts; high variance across seeds suggests instability
- First 3 experiments:
  1. Train XLM-RoBERTa-base on MNLI translated to Basque (translate-train) and evaluate on XNLIeu vs XNLIeuMT.
  2. Fine-tune XLM-RoBERTa-base in English, then zero-shot transfer to Basque and compare results on translated vs native test.
  3. Train XLM-RoBERTa-base in a typologically similar language (e.g., Spanish) and evaluate on Basque native set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of zero-shot prompting with generative models compare to fine-tuned approaches when evaluated on the native Basque NLI dataset?
- Basis in paper: [explicit] The paper reports that generative models yield results comparable to those obtained with machine-translated and post-edited sets when tested on the native dataset using zero-shot prompting.
- Why unresolved: The paper does not provide a detailed comparison of the performance gap between zero-shot prompting and fine-tuned approaches on the native dataset, leaving uncertainty about the relative effectiveness of these strategies.
- What evidence would resolve it: Conducting experiments that directly compare the accuracy of zero-shot prompting versus fine-tuned approaches on the native dataset would provide insights into their relative performance.

### Open Question 2
- Question: What is the impact of the choice of source language on the performance of zero-shot cross-lingual transfer for NLI in Basque, and how does this vary across different languages?
- Basis in paper: [explicit] The paper conducts experiments fine-tuning XLM-RoBERTa-base in 14 different languages and testing them on the Basque NLI datasets, showing small differences in XNLIeu and XNLIeuMT but larger differences in the native dataset.
- Why unresolved: While the paper presents results for various source languages, it does not provide a comprehensive analysis of why certain languages perform better than others and how typological factors influence the choice of source language.
- What evidence would resolve it: Further analysis of the linguistic features and typological similarities between Basque and the source languages could help explain the observed performance differences and guide the selection of optimal source languages.

### Open Question 3
- Question: How does the presence of translation artifacts and biases in the dataset affect the performance of NLI models, and what strategies can be employed to mitigate these effects?
- Basis in paper: [explicit] The paper discusses the presence of biases and artifacts in NLI datasets, particularly in translated sets, and analyzes the effects of post-edition on dataset quality and model performance.
- Why unresolved: While the paper identifies the presence of translation artifacts and biases, it does not provide a detailed analysis of their specific impact on model performance or propose comprehensive strategies to mitigate these effects.
- What evidence would resolve it: Conducting experiments that systematically introduce and measure the impact of various types of translation artifacts and biases on model performance, and evaluating the effectiveness of different mitigation strategies, would provide valuable insights into addressing these issues.

## Limitations
- Reliance on machine translation and a small native test set may limit the robustness of conclusions about dataset artifacts and cross-lingual transfer.
- The exact details of post-editing guidelines and the composition of the native set are not fully specified, making it difficult to assess the reproducibility of these steps.
- The paper does not provide corpus-level evidence linking translation artifacts directly to model performance gaps, relying instead on qualitative error analysis and performance comparisons.

## Confidence
- Post-edition improves dataset quality: Medium (consistent model performance drops on MT-only set but lacking direct artifact detection)
- Translate-train strategy is superior: Medium (gains observed but smaller on native set, suggesting benefit may be partly due to reduced distribution shift)
- Native set is less biased: Medium (qualitative evidence and error analysis provided, but no exhaustive bias detection)

## Next Checks
1. Analyze the XNLIeuMT and XNLIeu datasets for specific translation artifacts (e.g., lexical overlap, negation cues) and correlate these with model error patterns.
2. Expand the native Basque test set size and re-evaluate model performance to confirm that the native set is indeed less biased and harder for models.
3. Conduct cross-validation experiments using different random seeds and model initializations to assess the stability of observed performance differences across datasets.