---
ver: rpa2
title: 'PureEBM: Universal Poison Purification via Mid-Run Dynamics of Energy-Based
  Models'
arxiv_id: '2405.19376'
source_url: https://arxiv.org/abs/2405.19376
tags:
- poison
- training
- poisoned
- pure
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces PURE EBM, a universal defense method against
  data poisoning attacks in machine learning. It uses iterative Langevin sampling
  of Energy-Based Models (EBMs) to preprocess poisoned data, moving them back toward
  the natural image manifold with minimal impact on classifier performance.
---

# PureEBM: Universal Poison Purification via Mid-Run Dynamics of Energy-Based Models

## Quick Facts
- **arXiv ID**: 2405.19376
- **Source URL**: https://arxiv.org/abs/2405.19376
- **Reference count**: 40
- **Primary result**: Introduces PURE EBM, a universal defense against data poisoning attacks that uses iterative Langevin sampling of Energy-Based Models to preprocess poisoned data while maintaining high natural accuracy

## Executive Summary
This work presents PURE EBM, a universal defense mechanism against data poisoning attacks in machine learning. The method leverages iterative Langevin sampling of Energy-Based Models (EBMs) to preprocess poisoned data, moving it back toward the natural image manifold with minimal impact on classifier performance. Unlike existing approaches, PURE EBM is classifier-agnostic and doesn't require train-time model information, making it practical for real-world applications. The approach achieves state-of-the-art defense against leading poisoning attacks including Gradient Matching, Narcissus, and Bullseye Polytope while maintaining high natural accuracy.

## Method Summary
PURE EBM operates by using pre-trained Energy-Based Models to iteratively refine poisoned data samples through Langevin sampling. The EBM learns the distribution of clean data during training, and when poisoned samples are passed through this sampling process, they are pushed back toward the clean data manifold. This preprocessing step occurs before classifier training or inference, making the defense compatible with any downstream model. The method works even when the EBM training data includes poisoned samples or out-of-distribution images, demonstrating robustness to training-time contamination.

## Key Results
- Achieves state-of-the-art defense against Gradient Matching, Narcissus, and Bullseye Polytope poisoning attacks
- Maintains high natural accuracy on clean data while providing poison defense
- Remains effective even when EBM training data contains poisoned samples or out-of-distribution images
- Demonstrates classifier-agnostic performance without requiring train-time model information

## Why This Works (Mechanism)
PURE EBM exploits the fundamental property that poisoned data deviates from the natural data manifold. Energy-Based Models learn to assign low energy (high probability) to natural images and high energy to unnatural or corrupted samples. Through iterative Langevin sampling, samples are moved along the energy gradient toward regions of lower energy, effectively "purifying" poisoned data by pushing it back to the clean manifold. This process is classifier-agnostic because it operates purely on the data distribution learned by the EBM, not on the downstream model's parameters or architecture.

## Foundational Learning
- **Energy-Based Models (EBMs)**: Undirected probabilistic models that learn the data distribution by assigning low energy to high-probability regions - needed to understand how PURE EBM identifies and corrects poisoned samples
- **Langevin Sampling**: Stochastic optimization technique that uses gradient information and noise to sample from complex distributions - needed to understand how samples are iteratively refined
- **Data Manifold**: The lower-dimensional subspace where natural data points lie in the high-dimensional input space - needed to understand why poisoned samples deviate and how purification works
- **Poisoning Attacks**: Adversarial techniques that inject corrupted training data to manipulate model behavior - needed to understand the threat model PURE EBM defends against
- **Classifier-Agnostic Defense**: Defense mechanisms that work independently of the specific model architecture - needed to understand PURE EBM's practical advantage

## Architecture Onboarding

**Component Map**: Poisoned Data → EBM (Langevin Sampling) → Purified Data → Classifier

**Critical Path**: The essential sequence is: poisoned input → iterative Langevin updates using EBM gradients → output refined sample → use in classifier training/inference

**Design Tradeoffs**: Computational cost of iterative sampling vs. defense effectiveness; complexity of training EBM vs. universality of defense; number of Langevin steps vs. purification quality

**Failure Signatures**: If EBM poorly represents natural data manifold, purification will be ineffective; if Langevin sampling is insufficient (too few steps), poisoned samples won't be adequately refined; if EBM is trained on heavily poisoned data without proper safeguards, it may learn corrupted manifold

**First Experiments**: 1) Test purification on synthetic poisoned CIFAR-10 with varying poison ratios (5%, 10%, 20%); 2) Evaluate natural accuracy retention on clean test sets after purification; 3) Compare against baseline defenses on Gradient Matching attack with fixed poison budget

## Open Questions the Paper Calls Out
None

## Limitations
- Method assumes poisoned data deviates from natural image manifold, which may not hold for all sophisticated poisoning strategies
- Computational cost of iterative Langevin sampling may be prohibitive for large-scale or real-time applications
- Effectiveness against adaptive attackers who design poisons specifically to evade EBM purification remains untested
- Generalization to non-image data modalities and complex downstream tasks beyond standard classification is unclear

## Confidence

**High Confidence Claims:**
- PURE EBM achieves state-of-the-art defense against the specific poisoning attacks tested (Gradient Matching, Narcissus, Bullseye Polytope)
- The method maintains high natural accuracy while providing poison defense
- PURE EBM works effectively even when training data includes poisoned samples or out-of-distribution images

**Medium Confidence Claims:**
- PURE EBM is truly "universal" across different types of poisoning attacks beyond those tested
- The classifier-agnostic nature holds across diverse model architectures and training paradigms
- The method scales efficiently for real-world deployment scenarios

**Low Confidence Claims:**
- PURE EBM's effectiveness against adaptive attackers who specifically design poisons to evade EBM purification
- Performance guarantees in scenarios with extreme poisoning ratios (>30% of training data)
- Generalization to non-image data modalities and complex downstream tasks

## Next Checks
1. **Cross-Attack Generalization Test**: Evaluate PURE EBM against a broader suite of poisoning attacks including dynamic and adaptive strategies, as well as label-flipping and clean-label poisoning methods not covered in the current study.

2. **Scalability and Efficiency Analysis**: Conduct comprehensive benchmarking of computational overhead across different dataset sizes, image resolutions, and real-time application scenarios to quantify the practical deployment costs.

3. **Cross-Modal Generalization**: Test PURE EBM's effectiveness on non-image data modalities (text, tabular, graph data) and complex downstream tasks (object detection, semantic segmentation, NLP tasks) to validate claims of universal applicability beyond standard image classification.