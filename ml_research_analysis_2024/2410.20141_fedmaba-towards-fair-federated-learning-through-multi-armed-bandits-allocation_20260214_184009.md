---
ver: rpa2
title: 'FedMABA: Towards Fair Federated Learning through Multi-Armed Bandits Allocation'
arxiv_id: '2410.20141'
source_url: https://arxiv.org/abs/2410.20141
tags:
- performance
- fedmaba
- fairness
- learning
- client
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses fairness issues in federated learning caused
  by statistical heterogeneity among clients. The authors introduce a novel algorithm
  called FedMABA that combines multi-armed bandit allocation with federated learning
  to mitigate performance unfairness among diverse clients.
---

# FedMABA: Towards Fair Federated Learning through Multi-Armed Bandits Allocation

## Quick Facts
- arXiv ID: 2410.20141
- Source URL: https://arxiv.org/abs/2410.20141
- Reference count: 21
- Primary result: FedMABA achieves at least 33% better variance on Fashion-MNIST and 8% better variance on CIFAR-100 compared to baseline methods

## Executive Summary
This paper addresses fairness issues in federated learning caused by statistical heterogeneity among clients. The authors introduce FedMABA, a novel algorithm that combines multi-armed bandit allocation with federated learning to mitigate performance unfairness. By using explicit constraints on client performance distribution and optimizing them through an adversarial multi-armed bandit approach, FedMABA achieves significant improvements in fairness while maintaining competitive server model performance. The method includes a fair update strategy combining MAB weights with average weights to ensure server model performance.

## Method Summary
FedMABA is a federated learning algorithm that uses multi-armed bandit allocation to improve fairness among clients with heterogeneous data distributions. The algorithm transforms the NP-hard variance minimization problem into a tractable convex relaxation using explicit constraints on performance disparity. It employs an adversarial multi-armed bandit approach to optimize allocation weights that balance learning resources across clients. The final update combines MAB weights with average weights through a hyper-parameter α to maintain both fairness and server model performance. The method theoretically proves that constraining performance disparities improves both generalization and fairness, with a convergence rate of O(1/√(NKT) + 1/T).

## Key Results
- FedMABA achieves at least 33% better variance on Fashion-MNIST compared to other baselines
- The algorithm achieves 8% better variance on CIFAR-100 while maintaining competitive server model performance
- Theoretical analysis shows server model's generalization error is upper bounded by client performance disparities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FedMABA improves fairness by directly constraining the variance of client performance through multi-armed bandit allocation.
- Mechanism: The algorithm transforms the NP-hard optimization problem into a convex upper surrogate using explicit constraints on performance disparity, then uses adversarial multi-armed bandit to optimize allocation weights that balance learning resources across clients.
- Core assumption: Constraining the variance of client-specific performance will not compromise the generalization performance of the global server model.
- Evidence anchors:
  - [abstract] "We theoretically prove that the server model's generalization error is upper bounded by the variance of client performance, showing that constraining performance disparities improves both generalization and fairness."
  - [section IV-A] "Theorem 1 states that under the considered FL setting, the generalization performance error of the server model is upper bounded by the consistency of the empirical loss variance."
  - [corpus] Found 25 related papers, average neighbor FMR=0.511, indicating moderate relevance to fairness in federated learning literature.

### Mechanism 2
- Claim: The combination of MAB weights with average weights ensures both fairness and server model performance.
- Mechanism: FedMABA uses a weighted aggregation strategy where MAB weights prioritize weaker clients while average weights maintain overall model quality, with parameter α controlling the trade-off between these objectives.
- Core assumption: The hybrid weighting approach can simultaneously improve fairness for disadvantaged clients without degrading overall model performance.
- Evidence anchors:
  - [section IV-B] "We use the hyper-parameter α to combine the normalized MAB weights with the average aggregation to obtain the final update formula" and "This is because in practical FL training, it is hard to ensure that all clients participating in each communication round."
  - [section V-B] "Despite significantly improving client performance consistency, FedMABA's server model generalization performance remains above average."
  - [corpus] No direct evidence found in corpus, but related papers on fair FL suggest this is a common approach.

### Mechanism 3
- Claim: The convergence rate of FedMABA is O(1/√(NKT) + 1/T), ensuring efficient training.
- Mechanism: The algorithm uses mirror gradient ascent-descent method with Legendre function to optimize the dual space problem, combined with local client updates and periodic server aggregation.
- Core assumption: The smoothness and boundedness assumptions about client objectives and gradients hold in practical scenarios.
- Evidence anchors:
  - [section IV-C] "Theorem 2 (Convergence Analysis of FedMABA). Under Assumptions above, the total communication rounds T is predetermined... the convergence rate can be formulated as follows: min t∈[T] E ∥∇f(wt)∥2 ≤ O(1/√(NKT) + 1/T)."
  - [section III-A] "Each objective function of clients is Lipschitz smooth with constant L" and "The stochastic gradient calculated by each client can be an unbiased estimator of the clients' gradient."
  - [corpus] No direct convergence rate evidence found in corpus, but related bandit-based FL papers suggest similar rates are achievable.

## Foundational Learning

- Concept: Multi-armed bandit allocation
  - Why needed here: To dynamically allocate training resources and attention across clients based on their current performance and contribution potential
  - Quick check question: How does the algorithm decide which clients should receive more attention in each round?

- Concept: Convex relaxation of NP-hard optimization problems
  - Why needed here: To transform the intractable variance minimization problem into a tractable optimization that can be solved efficiently
  - Quick check question: What is the upper surrogate used to relax the original variance constraint?

- Concept: Federated learning with statistical heterogeneity
  - Why needed here: To understand why performance disparities exist and how they impact both fairness and model quality
  - Quick check question: What are the two main sources of statistical heterogeneity mentioned in the paper?

## Architecture Onboarding

- Component map: Server -> MAB module -> Aggregation module; Client -> Local training -> Loss reporting
- Critical path: Client training -> Loss reporting -> MAB weight update -> Aggregation weight computation -> Model update -> Next round
- Design tradeoffs: MAB weights prioritize fairness vs average weights prioritize overall performance; more communication rounds improve fairness but increase latency; different α values balance these competing objectives
- Failure signatures: High variance persists despite MAB allocation (weights not effective); server model accuracy drops significantly (MAB weights too aggressive); convergence slows dramatically (step sizes poorly tuned)
- First 3 experiments:
  1. Test FedMABA with α=0.5 on Fashion-MNIST with full client participation to verify basic functionality
  2. Vary ηb parameter to find optimal trade-off between fairness and performance
  3. Test partial client participation scenario to verify robustness under real-world conditions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can FedMABA be made robust against malicious clients who manipulate their reported losses to gain unfair advantage in the multi-armed bandit allocation?
- Basis in paper: [explicit] The paper mentions in the conclusion section that ensuring robustness against adversarial attacks in FL is a direction for future work.
- Why unresolved: The current FedMABA algorithm does not include any defense mechanisms against clients that might falsely report losses to receive more favorable weights.
- What evidence would resolve it: A modified FedMABA algorithm that incorporates Byzantine-robust techniques (such as Krum, median aggregation, or trimmed mean) and experimental results showing performance under adversarial conditions.

### Open Question 2
- Question: How does FedMABA's performance scale with extremely large numbers of clients (thousands to millions) in terms of computational overhead and convergence speed?
- Basis in paper: [inferred] The paper provides convergence analysis with O(1/√(NKT) + 1/T) rate but does not empirically test with very large client populations or discuss scalability bottlenecks.
- Why unresolved: The experiments use relatively small client numbers (50-100), and the MAB allocation mechanism may become computationally expensive with massive client counts.
- What evidence would resolve it: Large-scale experiments with varying client counts (N=1000, 10000, 100000) measuring communication rounds, computation time, and fairness metrics.

### Open Question 3
- Question: What is the optimal trade-off between server model performance and fairness in FedMABA across different application domains?
- Basis in paper: [explicit] The paper uses a hyper-parameter α to balance average aggregation and MAB aggregation, but doesn't systematically explore the Pareto frontier between fairness and accuracy.
- Why unresolved: The experiments show FedMABA maintains competitive accuracy while improving fairness, but don't quantify the maximum achievable fairness or minimum accuracy loss across different scenarios.
- What evidence would resolve it: Systematic experiments varying α and ρ across multiple datasets to plot fairness-accuracy trade-offs, identifying domain-specific optimal configurations.

## Limitations
- Limited theoretical convergence analysis for the multi-armed bandit component, with most proofs focusing on the convex relaxation rather than the full MAB optimization procedure
- Potential sensitivity to hyper-parameter tuning, particularly the trade-off parameter α and the MAB learning rate ηb, though specific sensitivity analysis is not provided
- No ablation study showing the individual contribution of MAB weights versus average weights to the final performance-fairness trade-off

## Confidence
- **High confidence**: The core mechanism of using multi-armed bandit allocation to improve fairness in federated learning is theoretically sound and well-supported by the variance-based generalization error bounds
- **Medium confidence**: The empirical results showing 33% better variance on Fashion-MNIST and 8% better variance on CIFAR-100 are promising but would benefit from more extensive hyper-parameter sensitivity analysis
- **Medium confidence**: The convergence rate of O(1/√(NKT) + 1/T) is theoretically derived but assumes idealized conditions that may not hold in practice

## Next Checks
1. Conduct hyper-parameter sensitivity analysis for α and ηb to determine optimal settings across different dataset characteristics and client heterogeneity levels
2. Perform ablation studies comparing pure MAB-weighted aggregation versus the hybrid approach to quantify the contribution of each component
3. Test the algorithm under extreme non-IID conditions (e.g., clients with completely disjoint label sets) to verify robustness when statistical heterogeneity is severe