---
ver: rpa2
title: Evaluating Zero-Shot GPT-4V Performance on 3D Visual Question Answering Benchmarks
arxiv_id: '2405.18831'
source_url: https://arxiv.org/abs/2405.18831
tags:
- scene
- agent
- performance
- agents
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the zero-shot performance of GPT-4V on established
  3D Visual Question Answering (VQA) benchmarks (3D-VQA and ScanQA) without fine-tuning.
  The authors use GPT-4V to generate scene captions from 3D mesh frames and then prompt
  GPT-4 Turbo to answer benchmark questions.
---

# Evaluating Zero-Shot GPT-4V Performance on 3D Visual Question Answering Benchmarks

## Quick Facts
- arXiv ID: 2405.18831
- Source URL: https://arxiv.org/abs/2405.18831
- Reference count: 30
- Zero-shot GPT agents achieve competitive performance on 3D VQA benchmarks, within 10% of DNN baselines

## Executive Summary
This paper evaluates the zero-shot performance of GPT-4V and GPT-4 Turbo on established 3D Visual Question Answering benchmarks (3D-VQA and ScanQA) without any fine-tuning. The authors employ GPT-4V to generate scene captions from 3D mesh frames, which are then used as contextual input for GPT-4 Turbo to answer benchmark questions. They explore both vocabulary-agnostic and vocabulary-grounded captioning approaches, finding that scene-specific vocabulary significantly improves captioning quality and downstream VQA performance. The study reveals that blind GPT agents (without visual input) perform surprisingly well on closed-vocabulary benchmarks, confirming the effectiveness of language priors. The authors release their workflow and findings to stimulate further research on 3D VQA benchmarks in the era of foundation models.

## Method Summary
The authors evaluate zero-shot performance of GPT-4V and GPT-4 Turbo on 3D-VQA and ScanQA benchmarks. They use GPT-4V to generate per-frame captions from 3D mesh frames, employing both vocabulary-agnostic and vocabulary-grounded approaches. The vocabulary-grounded method uses scene-specific objects extracted from benchmark ground truth to improve object grounding. Generated captions are aggregated into scene descriptions and provided as in-context information to GPT-4 Turbo, which answers benchmark questions using Chain-of-Thought prompting. The framework includes a parallel task scheduler for concurrent VQA prompt execution across multiple GPT endpoints.

## Key Results
- GPT-4V achieves competitive performance on 3D-VQA and ScanQA benchmarks, with scores within 10% of DNN baselines on some metrics
- Blind GPT-4 agents establish surprisingly strong baselines on closed-vocabulary benchmarks, corroborating recent findings about language priors
- GPT-4V significantly benefits from scene-specific vocabulary grounding in captioning, improving object detection and reducing synonym mismatches
- Zero-shot GPT agents show varying performance across different question types, with better results on attribute and relation questions compared to counting and location questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4V generates scene captions that serve as contextual input for GPT-4 Turbo to answer 3D VQA questions.
- Mechanism: GPT-4V is prompted to create Scene-Graph Captions (SGC) that encode objects, attributes, and relationships in a scene. These captions are then provided as in-context information to GPT-4 Turbo, which uses them to reason about the question.
- Core assumption: GPT-4V can effectively capture spatial and semantic details from 3D mesh frames in textual form.
- Evidence anchors:
  - "We employ GPT-V as a powerful captioning model to produce per-frame captions that capture the detailed characteristics of different items within the scene."
  - "We instruct GPT-V to craft Scene-Graph Captions-like (SGC) representations directly."
- Break condition: If GPT-4V fails to capture critical objects or relationships in the captions, GPT-4 Turbo will lack the necessary context to answer questions accurately.

### Mechanism 2
- Claim: Scene-specific vocabulary grounding improves GPT-4V captioning quality and downstream VQA performance.
- Mechanism: When GPT-4V is provided with a list of scene-specific objects (from benchmark ground truth), it focuses its captions on those items, improving object grounding and reducing synonym mismatches.
- Core assumption: GPT-4V can effectively incorporate vocabulary constraints into its captioning without losing contextual richness.
- Evidence anchors:
  - "We investigate a vocabulary grounded approach, where GPT-V is given additional context. For this, we extract the scene-specific list of objects from the benchmark ground truths and we instruct GPT-V to concentrate on these specified items during its description process."
  - "We note a significant drop in performance when GPT-V is tasked with open-ended, ungrounded scene descriptions."
- Break condition: If the provided vocabulary list is incomplete or contains incorrect objects, the grounding may mislead GPT-4V or omit important scene elements.

### Mechanism 3
- Claim: "Blind" GPT-4 agents achieve surprisingly strong performance on closed-vocabulary benchmarks due to language priors.
- Mechanism: GPT-4 Turbo can leverage its training on vast text corpora to answer questions based on common-sense knowledge and statistical priors about object attributes and relationships, even without visual input.
- Core assumption: The closed-vocabulary nature of the benchmarks aligns with patterns and relationships commonly present in pre-training data.
- Evidence anchors:
  - "Our findings corroborate recent results that 'blind' models establish a surprisingly strong baseline in closed-vocabulary settings."
  - "Adding to the conversation that OpenEQA initiated recently [16] that 'language provides an easier prior about the world' [3], we confirm that blind agents exhibit unexpectedly strong performance on ScanQA as well."
- Break condition: If benchmark questions require highly specific visual details not inferable from language priors, blind agents will underperform compared to vision-augmented agents.

## Foundational Learning

- Concept: Scene-Graph Captions (SGC)
  - Why needed here: SGC provides a structured textual representation of the 3D scene that captures objects, attributes, and relationships in a format GPT-4 Turbo can process.
  - Quick check question: How does the SGC format differ from standard image captions, and why is this difference important for 3D VQA?

- Concept: Vocabulary grounding in prompt engineering
  - Why needed here: Grounding GPT-4V with scene-specific vocabulary improves object detection and reduces synonym mismatches in captions.
  - Quick check question: What could happen if the vocabulary list contains incorrect objects or is missing key scene elements?

- Concept: Zero-shot vs. fine-tuned performance evaluation
  - Why needed here: Understanding how foundation models perform without task-specific fine-tuning establishes a baseline for their capabilities and limitations.
  - Quick check question: Why might zero-shot performance be a more meaningful comparison when evaluating foundation models on established benchmarks?

## Architecture Onboarding

- Component map: 3D mesh frames -> GPT-4V captioning -> scene description aggregation -> GPT-4 Turbo Q&A
- Critical path: 3D mesh frames → GPT-4V captioning → scene description aggregation → GPT-4 Turbo Q&A
- Design tradeoffs:
  - Frame sampling rate (F): Fewer frames reduce computational cost but may miss important scene details
  - Batch size (Q): Larger batches reduce API calls but may slightly decrease performance
  - Vocabulary grounding: Improves object grounding but requires accurate benchmark ground truth
- Failure signatures:
  - Poor captioning quality (missing objects, incorrect attributes)
  - Inconsistent outputs across multiple runs (check temperature settings)
  - Performance drop when switching between RGB and mesh frames
- First 3 experiments:
  1. Run blind GPT-4 Turbo on a small subset of questions to establish baseline performance
  2. Test GPT-4V captioning on individual frames with and without vocabulary grounding
  3. Vary frame sampling rate (F) on a small scene to find optimal balance between cost and quality

## Open Questions the Paper Calls Out

- How does the performance of zero-shot GPT agents compare to human performance on 3D-VQA and ScanQA benchmarks?
  - Basis in paper: The authors note that human performance was only reported in the original 3D-VQA paper, and suggest that human-in-the-loop evaluation could provide useful context for GPT agent performance.
  - Why unresolved: Human performance data is not currently available for these benchmarks in the context of foundation models.
  - What evidence would resolve it: A study comparing human performance to zero-shot GPT agents on the same benchmark questions would provide a direct comparison.

- How much does advanced prompting (e.g., few-shot ReAct, Tree-of-Thought) improve the performance of GPT agents on 3D VQA tasks compared to simple Chain-of-Thought prompting?
  - Basis in paper: The authors mention that current GPT agents use straightforward Chain-of-Thought prompts and suggest investigating the effect of more advanced prompting strategies.
  - Why unresolved: The paper does not explore or compare different prompting techniques beyond the basic Chain-of-Thought approach.
  - What evidence would resolve it: A systematic comparison of GPT agent performance using various prompting strategies on the same benchmark tasks would quantify the impact of advanced prompting.

- Can in-context visual grounding (e.g., using coordinates or markers) improve GPT-V's captioning performance for 3D meshes compared to vocabulary-grounded textual prompts?
  - Basis in paper: The authors suggest exploring whether visual grounding techniques, similar to those used in segmentation tasks, could be beneficial for 3D mesh captioning.
  - Why unresolved: The paper only investigates vocabulary-grounded textual prompts and does not experiment with in-context visual grounding.
  - What evidence would resolve it: A controlled experiment comparing GPT-V captioning performance using vocabulary-grounded textual prompts versus in-context visual grounding on the same 3D mesh frames would demonstrate the effectiveness of visual grounding.

## Limitations
- Performance gap between GPT-based methods and DNN baselines (up to 21.5% on EM@1 for 3D-VQA) suggests current GPT models may lack certain capabilities for complex 3D reasoning tasks
- Reliance on vocabulary-grounded captioning raises questions about generalizability to real-world scenarios where ground truth object lists may not be available
- Evaluation framework depends on automated metrics that may not fully capture semantic correctness for complex spatial reasoning questions

## Confidence
- **High Confidence**: Blind GPT-4 agents achieving strong performance on closed-vocabulary benchmarks; effectiveness of vocabulary grounding in improving captioning quality
- **Medium Confidence**: Claims about GPT-4V's competitive performance relative to DNN baselines; assertion that GPT-4V "benefits significantly" from scene-specific vocabulary
- **Low Confidence**: Implications for real-world applications; claims about general applicability of findings to broader 3D VQA tasks

## Next Checks
1. Conduct ablation studies by systematically removing different components of the GPT-4V captioning pipeline (e.g., vocabulary grounding, frame sampling rates) to quantify their individual contributions to final performance.
2. Evaluate the model on a held-out test set from ScanNet not used in the benchmark development to assess whether vocabulary grounding creates overfitting to specific object distributions.
3. Implement a human evaluation protocol to assess the semantic correctness of GPT-4 Turbo's answers beyond automated metrics, particularly for questions requiring complex spatial reasoning or multi-step inference.