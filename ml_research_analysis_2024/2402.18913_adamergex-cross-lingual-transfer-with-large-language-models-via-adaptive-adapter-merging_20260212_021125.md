---
ver: rpa2
title: 'AdaMergeX: Cross-Lingual Transfer with Large Language Models via Adaptive
  Adapter Merging'
arxiv_id: '2402.18913'
source_url: https://arxiv.org/abs/2402.18913
tags:
- language
- task
- adamergex
- merging
- ability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AdaMergeX, a novel cross-lingual transfer
  method that leverages adaptive adapter merging to improve multilingual capabilities
  of large language models (LLMs). The core idea is to recognize the mutual reliance
  between task ability and language ability, and use a reference task to obtain language
  ability which is then merged with task ability from the source language.
---

# AdaMergeX: Cross-Lingual Transfer with Large Language Models via Adaptive Adapter Merging

## Quick Facts
- arXiv ID: 2402.18913
- Source URL: https://arxiv.org/abs/2402.18913
- Reference count: 13
- Primary result: 31.1% relative improvement over Arimerge across all languages and tasks

## Executive Summary
AdaMergeX introduces a novel cross-lingual transfer method for large language models that leverages adaptive adapter merging to improve multilingual capabilities. The method recognizes the mutual reliance between task ability and language ability, using a reference task to obtain language ability which is then merged with task ability from the source language. Through structure-adaptive adapter merging that aligns with how adapters are integrated with LLMs, AdaMergeX achieves significant improvements over existing methods across all settings.

## Method Summary
AdaMergeX is built on the insight that task ability and language ability are mutually reliant rather than separable. The method uses a reference task to obtain language ability, which is then merged with task ability from the source language through a structure-adaptive adapter merging approach. This approach aligns with how adapters are integrated with LLMs, allowing for more effective cross-lingual transfer. The method is evaluated across 5 representative cross-lingual tasks using Llama2, demonstrating consistent improvements over baseline methods including direct transfer, prompting, MAD-X, and Arimerge.

## Key Results
- Achieves 1.4-3.4% absolute improvement compared to direct transfer and prompting methods
- Achieves 8.0-15.9% improvement compared to MAD-X which separates task and language ability
- Achieves 31.1% relative improvement on all languages and tasks with Llama2 compared to Arimerge

## Why This Works (Mechanism)
The effectiveness of AdaMergeX stems from its recognition that task ability and language ability are interdependent rather than independent components that can be simply combined. By using a reference task to capture language ability and then merging it with task-specific knowledge through a structure-adaptive mechanism, the method better aligns with how LLMs naturally integrate multilingual and task-specific information.

## Foundational Learning

### Adapter Integration in LLMs
Why needed: Understanding how adapters modify LLM behavior is crucial for effective merging strategies
Quick check: Verify that the adapter architecture matches the target LLM's integration method

### Cross-Lingual Transfer Learning
Why needed: Forms the theoretical basis for transferring knowledge across language boundaries
Quick check: Confirm that source and target languages share sufficient linguistic overlap for transfer

### Reference Task Methodology
Why needed: Provides a mechanism to extract generalizable language abilities
Quick check: Ensure the reference task captures diverse linguistic patterns across target languages

## Architecture Onboarding

### Component Map
Input -> Reference Task Adapter -> Language Ability Extraction -> Structure-Adaptive Merger -> Task Adapter -> Output

### Critical Path
The critical path flows from input through the reference task adapter for language ability extraction, through the structure-adaptive merger that combines this with task-specific knowledge, and finally through the task adapter to produce the output.

### Design Tradeoffs
The method trades increased model complexity (multiple adapters) for improved cross-lingual transfer performance. This approach requires careful calibration of merging weights and may introduce computational overhead during inference.

### Failure Signatures
Potential failures include: over-merging leading to task-specific knowledge loss, under-merging resulting in insufficient language adaptation, and reference task selection bias that could skew language ability extraction.

### First Experiments
1. Verify adapter integration compatibility with the target LLM architecture
2. Test reference task selection impact on language ability extraction quality
3. Validate merging weight calibration across different language pairs

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Does not explicitly discuss limitations or uncertainties in the presentation
- Evaluation focuses primarily on comparison with MAD-X and Arimerge without comprehensive analysis of performance degradation under different conditions
- Does not provide analysis of failure modes or scenarios where the method might underperform

## Confidence

High confidence in reported improvements over MAD-X and Arimerge (1.4-3.4% and 8.0-15.9% improvements respectively) across multiple tasks and languages.

Medium confidence in generalizability to other LLM architectures beyond Llama2, as evaluation is limited to one model family.

Low confidence in scalability to extremely large-scale multilingual settings or very low-resource languages, as these scenarios are not explicitly tested.

## Next Checks
1. Evaluate AdaMergeX performance on additional LLM architectures beyond Llama2 (e.g., GPT, BERT variants) to assess generalizability across different model families.

2. Test the method's robustness with varying amounts of training data in target languages, particularly for extremely low-resource language pairs not covered in the current evaluation.

3. Conduct ablation studies to quantify the individual contributions of the adaptive merging mechanism versus the reference task approach to the overall performance improvements.