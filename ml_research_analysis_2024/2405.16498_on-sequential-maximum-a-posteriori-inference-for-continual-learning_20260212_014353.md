---
ver: rpa2
title: On Sequential Maximum a Posteriori Inference for Continual Learning
arxiv_id: '2405.16498'
source_url: https://arxiv.org/abs/2405.16498
tags:
- task
- learning
- neural
- split
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new framework for continual learning based
  on sequential maximum a posteriori (MAP) inference. The key idea is to formulate
  the problem as a recursion of loss functions, where the previous task's loss is
  approximated to prevent catastrophic forgetting.
---

# On Sequential Maximum a Posteriori Inference for Continual Learning

## Quick Facts
- arXiv ID: 2405.16498
- Source URL: https://arxiv.org/abs/2405.16498
- Authors: Menghao Waiyan William Zhu; Ercan Engin Kuruoğlu
- Reference count: 33
- Key outcome: Sequential MAP inference framework with AQC and NC methods for continual learning, achieving comparable performance to joint MAP training on image tasks

## Executive Summary
This paper proposes a new framework for continual learning based on sequential maximum a posteriori (MAP) inference. The key idea is to formulate the problem as a recursion of loss functions, where the previous task's loss is approximated to prevent catastrophic forgetting. Two core methods are introduced: autodiff quadratic consolidation (AQC), which uses a full quadratic approximation of the previous loss, and neural consolidation (NC), which uses a neural network to approximate the previous loss. The authors focus on classification tasks using a fixed pre-trained feature extractor to make the methods scalable.

## Method Summary
The method formulates sequential MAP inference as a recursion of loss functions, reducing continual learning to approximating the previous loss function. AQC uses a second-order Taylor series approximation (full Hessian) around the previous MAP estimate, while NC uses a neural network to approximate the previous loss function. Both methods are combined with fixed pre-trained feature extractors for image tasks. Hyperparameters are tuned via grid search on validation data, and performance is evaluated using final average accuracy on test datasets.

## Key Results
- AQC performs consistently well in image tasks, achieving comparable performance to joint MAP training in many cases
- NC performs well in classical tasks with smaller input dimensions but is less effective in high-dimensional image feature spaces
- The sequential MAP recursion framework provides a general solution for continual learning across different task types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The sequential MAP recursion converts continual learning into a function approximation problem where each new task's loss is augmented with an approximation of the previous task's loss.
- Mechanism: By defining the loss function as the negative log joint probability density function (PDF), the authors create a recursive formulation where the loss at time t equals the loss at time t-1 plus the current task's negative log likelihood. This allows forgetting to be prevented by approximating and preserving the previous loss function rather than storing old data.
- Core assumption: Tasks are similar enough that the form of the likelihood function remains constant across tasks, enabling the recursive formulation to work.

### Mechanism 2
- Claim: AQC's full quadratic approximation provides more accurate preservation of previous task information than diagonal approximations used in EWC and SI.
- Mechanism: AQC uses a second-order Taylor series approximation around the previous MAP estimate, capturing the full curvature of the loss landscape through the complete Hessian matrix rather than just diagonal elements. This preserves more information about which parameters are important for previous tasks.
- Core assumption: The previous loss function is sufficiently smooth and convex near its minimum to justify quadratic approximation.

### Mechanism 3
- Claim: NC's neural network approximation can better capture non-quadratic loss landscape features than AQC's quadratic approximation.
- Mechanism: A neural network is trained to approximate the previous loss function by fitting to samples generated around the previous MAP estimate. This allows capturing complex, non-quadratic features of the loss landscape that a simple quadratic approximation would miss.
- Core assumption: The consolidator neural network has sufficient capacity and training data to accurately approximate the previous loss function.

## Foundational Learning

- Concept: Bayesian inference and MAP estimation
  - Why needed here: The entire framework is built on formulating continual learning as sequential Bayesian inference, where each task updates the posterior distribution over network parameters
  - Quick check question: What is the relationship between MAP estimation and minimizing the negative log posterior?

- Concept: Quadratic approximation and Taylor series
  - Why needed here: AQC relies on second-order Taylor series approximation around the MAP estimate to capture the curvature of the loss landscape
  - Quick check question: Why does the linear term disappear in the Taylor series approximation at the MAP estimate?

- Concept: Neural network function approximation
  - Why needed here: NC uses a neural network to approximate the previous loss function, requiring understanding of how neural networks can serve as universal function approximators
  - Quick check question: What properties must the consolidator neural network have to effectively approximate the previous loss function?

## Architecture Onboarding

- Component map: Feature extractor -> AQC/NC module -> Loss function recursion module -> Model training module -> Hyperparameter tuner

- Critical path:
  1. Initialize loss function (standard Gaussian prior + first task NLL)
  2. For each new task:
     - Update loss function using AQC or NC approximation
     - Train network on updated loss function
     - Store necessary information (Hessian for AQC, consolidator parameters for NC)
  3. Evaluate final performance across all tasks

- Design tradeoffs:
  - AQC vs NC: AQC provides more accurate approximation but doesn't scale with network size; NC scales better but may struggle with high-dimensional feature spaces
  - Full Hessian vs diagonal approximation: Full Hessian preserves more information but is computationally expensive
  - Fixed feature extractor vs end-to-end training: Feature extractor reduces forgetting but may limit task-specific adaptation

- Failure signatures:
  - AQC: Memory errors or extremely slow computation during Hessian calculation for large networks
  - NC: Poor performance on high-dimensional tasks, indicating the consolidator network cannot learn effective approximation
  - Both: Catastrophic forgetting when λ is set too low, indicating insufficient regularization strength

- First 3 experiments:
  1. Implement AQC on CI Split Iris with softmax regression to verify basic functionality and hyperparameter sensitivity
  2. Compare AQC vs EWC on the same task to validate the claim of improved performance through full quadratic approximation
  3. Implement NC on CI Split Wine to test neural network approximation capability on a slightly higher-dimensional classical task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of neural consolidation scale with increasing feature dimension in high-dimensional image tasks, and what architectural modifications could improve its effectiveness?
- Basis in paper: [explicit] The paper notes that neural consolidation (NC) performs well in classical tasks with small input dimensions but does not perform as well in high-dimensional image feature spaces, suggesting inefficiency in random sampling in high dimensions.
- Why unresolved: The paper identifies the issue but does not explore architectural modifications or alternative sampling strategies to improve NC's performance in high-dimensional spaces.
- What evidence would resolve it: Experiments comparing NC with different neural network architectures (e.g., convolutional consolidator networks) or sampling strategies (e.g., importance sampling) on high-dimensional tasks would clarify potential improvements.

### Open Question 2
- Question: What is the optimal balance between the quadratic approximation (AQC) and neural network approximation (NC) for continual learning across different task types and dimensionalities?
- Basis in paper: [explicit] The paper shows that AQC performs consistently well in image tasks while NC excels in classical tasks with small dimensions, but does not explore hybrid approaches that might leverage the strengths of both methods.
- Why unresolved: The paper presents these as separate methods without investigating potential synergies or hybrid approaches that could combine their advantages.
- What evidence would resolve it: Comparative experiments testing hybrid methods that combine AQC and NC, or adaptive methods that switch between them based on task characteristics, would reveal optimal strategies.

### Open Question 3
- Question: How does the choice of pre-trained feature extractor affect the continual learning performance across different task sequences and domains?
- Basis in paper: [explicit] The paper demonstrates that using pre-trained feature extractors significantly improves continual learning performance but does not systematically explore how different pre-training tasks or architectures affect results.
- Why unresolved: While the paper shows the benefit of pre-training, it uses fixed pre-trained models without exploring the impact of different pre-training strategies, architectures, or domain relevance.
- What evidence would resolve it: Experiments varying the pre-training task, architecture depth, and domain similarity across different continual learning scenarios would quantify the impact of pre-training choices.

## Limitations

- High-dimensional scalability: NC shows poor performance in image feature spaces due to random sampling inefficiency, but the paper doesn't explore alternative sampling strategies or consolidator architectures that might address this limitation.
- Fixed feature extractor constraint: While the paper argues that using fixed pre-trained feature extractors makes the methods scalable, this constraint may limit applicability to real-world scenarios where end-to-end training is necessary.
- Evaluation scope: Experiments focus primarily on classification tasks with relatively simple architectures. The framework's performance on more complex architectures, regression tasks, or reinforcement learning scenarios remains unexplored.

## Confidence

- High confidence: AQC provides more accurate preservation of previous task information than diagonal approximations (supported by direct experimental comparison showing consistent performance gains).
- Medium confidence: NC can better capture non-quadratic loss landscape features than AQC's quadratic approximation (supported by classical task experiments but contradicted by poor performance in image tasks).
- Medium confidence: The sequential MAP recursion framework provides a general solution for continual learning (theoretically sound but practical effectiveness depends heavily on task similarity and feature extractor choice).

## Next Checks

1. **Alternative sampling strategies for NC**: Test NC with importance sampling or other non-random sampling methods in high-dimensional feature spaces to determine if the poor performance is due to sampling inefficiency rather than fundamental limitations of neural network approximation.

2. **End-to-end training comparison**: Implement a variant of AQC or NC that allows end-to-end training (rather than fixed feature extractors) to assess performance trade-offs and identify scenarios where this constraint becomes limiting.

3. **Cross-task feature extractor validation**: Test the framework on task sequences where different task types require different feature extractors (e.g., combining image and text classification) to evaluate the robustness of the fixed feature extractor assumption.