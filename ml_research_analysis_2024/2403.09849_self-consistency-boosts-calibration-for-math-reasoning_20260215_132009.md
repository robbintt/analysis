---
ver: rpa2
title: Self-Consistency Boosts Calibration for Math Reasoning
arxiv_id: '2403.09849'
source_url: https://arxiv.org/abs/2403.09849
tags: []
core_contribution: This work focuses on improving calibration for math reasoning tasks
  using large language models (LLMs). The authors propose three new calibration methods
  based on self-consistency, which involves clustering multiple LLM samples before
  selecting the most consistent answer.
---

# Self-Consistency Boosts Calibration for Math Reasoning

## Quick Facts
- arXiv ID: 2403.09849
- Source URL: https://arxiv.org/abs/2403.09849
- Reference count: 13
- Key outcome: Three new self-consistency-based calibration methods improve math reasoning task calibration over existing approaches.

## Executive Summary
This paper addresses the challenge of improving calibration for math reasoning tasks using large language models (LLMs). The authors propose three new calibration methods based on self-consistency, which involves clustering multiple LLM samples before selecting the most consistent answer. Evaluated on GSM8K and MathQA benchmarks using Mistral and LLaMA2 models, the proposed methods achieve better calibration than existing approaches like p(True) and logit, as measured by Brier Score and Expected Calibration Error (ECE). The FCS method shows the most consistent performance across different settings.

## Method Summary
The authors propose three off-the-shelf calibration methods based on self-consistency for math reasoning tasks. These methods estimate confidence using cluster size, cluster number, or pairwise comparison of clusters. The approach involves generating multiple LLM samples with Chain-of-Thought prompting, clustering the generated answers, and computing confidence scores based on the selected cluster's properties. The methods are evaluated on GSM8K and MathQA benchmarks using Mistral and LLaMA2 models, comparing their calibration performance to existing methods like logit and p(True).

## Key Results
- The proposed self-consistency-based calibration methods outperform existing approaches like p(True) and logit on both GSM8K and MathQA benchmarks.
- The FCS method (using cluster size) shows the most consistent performance across different settings and model sizes.
- Calibration performance improves with larger sampling sizes (N) up to a point, after which gains plateau.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Clustering multiple LLM samples before selecting the most consistent answer improves calibration by aligning confidence with accuracy.
- Mechanism: By grouping generated answers into clusters and using cluster-based metrics, the method captures agreement among diverse reasoning paths, which correlates with model confidence.
- Core assumption: Greater agreement among multiple sampled answers reflects higher model confidence in the correctness of those answers.
- Evidence anchors:
  - [abstract] "We design three off-the-shelf calibration methods based on self-consistency (Wang et al., 2022) for math reasoning tasks."
  - [section] "Self-consistency performs clustering over multiple LLM samples before picking one from the largest cluster as the response to each input query."
- Break condition: If the model generates diverse, inconsistent reasoning paths even when confident, the clustering signal may not correlate with true confidence.

### Mechanism 2
- Claim: Using cluster size as a confidence estimate works because larger clusters indicate stronger consensus among generated reasoning paths.
- Mechanism: The FCS method normalizes the number of samples in the selected cluster by the total number of samples, producing a confidence score that reflects the proportion of agreement.
- Core assumption: When a model is confident, it tends to generate similar reasoning paths, leading to larger clusters.
- Evidence anchors:
  - [section] "FCS (x, θ) = ni / N . In contrast to the cluster number, the cluster size is more universally applicable across diverse prompts..."
  - [abstract] "We design three off-the-shelf calibration methods based on self-consistency..."
- Break condition: In tasks with restricted output spaces (e.g., multiple-choice), cluster sizes may not meaningfully vary, reducing calibration signal.

### Mechanism 3
- Claim: Pairwise comparison between clusters captures relative confidence by measuring how dominant the chosen cluster is compared to others.
- Mechanism: FP C computes the product of winning rates between the selected cluster and all others, emphasizing the margin of victory.
- Core assumption: A cluster that decisively beats all others in size is more likely to represent the correct answer and thus should have higher confidence.
- Evidence anchors:
  - [section] "FP C(x, θ) = ∏(|C|j̸=i) ni / (ni + nj), where ni / (ni + nj) represents the winning rate of selected cluster ci against another cluster cj."
  - [abstract] "...or pairwise comparison that captures relative differences between pairs of clusters."
- Break condition: When cluster sizes are similar, the product of winning rates may become very small, leading to artificially low confidence scores.

## Foundational Learning

- Concept: Self-consistency via majority voting
  - Why needed here: It provides the foundation for clustering diverse reasoning paths, which is essential for the proposed calibration methods.
  - Quick check question: What does self-consistency do before selecting an answer, and why is this useful for calibration?

- Concept: Calibration metrics (Brier Score, ECE)
  - Why needed here: These metrics quantify the alignment between predicted confidence and actual accuracy, which is the target of the proposed methods.
  - Quick check question: How do Brier Score and ECE differ in how they measure calibration, and which is more robust to unbalanced binning?

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: CoT generates intermediate reasoning steps that, when sampled multiple times, create the diversity needed for clustering and calibration.
  - Quick check question: Why is CoT prompting important for generating multiple reasoning paths in self-consistency?

## Architecture Onboarding

- Component map:
  - Input → LLM with CoT → N samples → Answer extraction → Clustering → Cluster-based confidence estimation → Calibration metrics
- Critical path:
  1. Generate N reasoning paths via LLM + CoT
  2. Extract answers from each path
  3. Cluster identical answers
  4. Compute confidence score using chosen method
  5. Compare predicted confidence vs. accuracy using ECE/Brier
- Design tradeoffs:
  - Sampling size N: Higher N increases stability but costs more compute; N=8 often sufficient per experiments
  - Clustering granularity: Exact string matching vs. semantic similarity—current work uses exact match
  - Confidence metric choice: FCS is most general; FCN can be weak for restricted output spaces; FP C captures relative dominance but sensitive to similar cluster sizes
- Failure signatures:
  - Calibration metrics flat or degraded despite increased N
  - Cluster sizes all similar (no dominant cluster)
  - ECE/Brier scores worsen compared to baselines on certain tasks
- First 3 experiments:
  1. Run FCS, FCN, FP C on GSM8K with N=8, compare Brier and ECE to logit and p(True) baselines
  2. Vary N (4, 8, 16, 32) for Mixtral-8×7B-Inst, plot calibration curves to find saturation point
  3. Test on MathQA (multiple-choice) to verify FCS robustness vs. FCN limitations

## Open Questions the Paper Calls Out
- How can self-consistency-based calibration methods be extended to non-mathematical reasoning tasks, such as question-answering or text generation?
- What are the trade-offs between the different self-consistency-based calibration methods (FCN, FCS, and FPC) in terms of calibration performance and computational efficiency?
- How can the self-consistency-based calibration methods be further improved by incorporating additional features or techniques, such as temperature scaling or ensembling?

## Limitations
- The paper does not provide explicit implementation details for the self-consistency-based calibration methods, such as the specific clustering algorithm used.
- The calibration performance may be sensitive to hyperparameters like the number of samples (N) and temperature settings.
- The proposed methods are evaluated on only two math reasoning benchmarks (GSM8K and MathQA), limiting generalization assessment.

## Confidence
- **High Confidence**: The core mechanism of using cluster size as a confidence estimate (FCS method) is well-supported by the results.
- **Medium Confidence**: The pairwise comparison method (FP C) captures relative confidence, but its sensitivity to similar cluster sizes is a concern.
- **Medium Confidence**: The claim that self-consistency improves calibration is supported, but the exact conditions under which this holds are not fully explored.

## Next Checks
1. Systematically vary the number of samples (N) and temperature settings to assess the robustness of calibration performance across different configurations.
2. Apply the proposed methods to additional math reasoning tasks or even non-math domains to test their broader applicability and identify potential failure modes.
3. Implement and compare semantic clustering (e.g., using embeddings) against exact string matching to evaluate if this improves calibration in tasks with semantically equivalent but syntactically different answers.