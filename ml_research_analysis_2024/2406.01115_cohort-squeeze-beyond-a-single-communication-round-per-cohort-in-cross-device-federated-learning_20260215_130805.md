---
ver: rpa2
title: 'Cohort Squeeze: Beyond a Single Communication Round per Cohort in Cross-Device
  Federated Learning'
arxiv_id: '2406.01115'
source_url: https://arxiv.org/abs/2406.01115
tags: []
core_contribution: This paper investigates whether cohorts in federated learning can
  participate in multiple communication rounds to reduce total communication costs,
  challenging the common single-round design. The authors propose SPPM-AS, a novel
  variant of stochastic proximal point method supporting arbitrary client sampling
  procedures, including clustering-based strategies.
---

# Cohort Squeeze: Beyond a Single Communication Round per Cohort in Cross-Device Federated Learning

## Quick Facts
- arXiv ID: 2406.01115
- Source URL: https://arxiv.org/abs/2406.01115
- Reference count: 40
- Up to 74% reduction in total communication cost by allowing multiple local rounds within cohorts

## Executive Summary
This paper challenges the conventional single-communication-round design in cross-device federated learning by proposing a method that allows cohorts to participate in multiple local rounds before global synchronization. The authors introduce SPPM-AS, a stochastic proximal point method variant that supports arbitrary client sampling procedures including clustering-based strategies. Theoretical analysis establishes convergence rates and neighborhood bounds under strong convexity, while experiments demonstrate significant communication cost reductions across both convex and non-convex settings.

## Method Summary
SPPM-AS extends the stochastic proximal point method to support arbitrary learning rates and flexible proximal solvers, enabling multiple local communication rounds within cohorts. The method incorporates stratified sampling for client selection, which outperforms traditional block and nice sampling by reducing variance in gradient estimates. The approach maintains theoretical convergence guarantees while allowing practical implementation with various solvers including first-order methods like Conjugate Gradient and second-order methods like BFGS.

## Key Results
- Up to 74% reduction in total communication cost across datasets (a6a, mushrooms, ijcnn1, FEMNIST)
- Stratified sampling outperforms block and nice sampling in convergence speed and neighborhood size
- Second-order solvers like BFGS show superior performance in convex settings compared to first-order methods
- Hierarchical FL settings amplify communication savings from multi-round cohort participation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Increasing local communication rounds within cohorts reduces total communication cost.
- Mechanism: Multiple local rounds leverage strong convexity and proximal operator flexibility to converge faster with fewer global rounds.
- Core assumption: Cohorts can maintain state across local rounds without violating cross-device constraints.
- Evidence anchors:
  - [abstract] "up to 74% reduction in the total communication cost needed to train a FL model in the cross-device setting"
  - [section] "our approach leads to up to 74% reduction in the total communication cost needed to train a FL model"
  - [corpus] Weak: no direct corpus evidence, but SPPM-AS's multi-round strategy aligns with general federated learning literature on communication efficiency.
- Break condition: If local computation cost outweighs communication savings, or if client state retention is not feasible.

### Mechanism 2
- Claim: Stratified sampling outperforms block and nice sampling due to variance reduction.
- Mechanism: Sampling one client from each cluster captures broader global information per round, reducing convergence neighborhood.
- Core assumption: Clusters are homogeneous internally but heterogeneous across clusters.
- Evidence anchors:
  - [section] "stratified sampling outperforms block sampling and nice sampling, advocating for stratified sampling as the superior method"
  - [section] "Lemma 5, we demonstrate that stratified sampling outperforms block sampling due to reduced variance"
  - [corpus] Weak: corpus lacks specific evidence on variance reduction in federated learning, but stratified sampling is a known variance reduction technique in statistics.
- Break condition: If clusters are not well-defined or if inter-cluster homogeneity is high.

### Mechanism 3
- Claim: Second-order solvers (e.g., BFGS) improve convergence in convex settings compared to first-order methods.
- Mechanism: Second-order methods use curvature information to take more informed steps, reducing iterations needed for convergence.
- Core assumption: Problem structure allows efficient second-order updates without prohibitive computational cost.
- Evidence anchors:
  - [section] "comparing first-order and second-order solvers in strongly convex settings, we observed that CG outperforms BFGS for our specific problem"
  - [section] "we compare the performance of first-order methods, such as the Conjugate Gradient (CG) method, against second-order methods, like the Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm"
  - [corpus] Weak: corpus lacks direct evidence on second-order solver performance in federated learning, but second-order methods are known to be effective in optimization.
- Break condition: If computational cost of second-order updates is prohibitive or if problem is non-convex.

## Foundational Learning

- Concept: Stochastic Proximal Point Method (SPPM)
  - Why needed here: SPPM forms the basis of SPPM-AS, allowing for arbitrary learning rates and flexible proximal solvers.
  - Quick check question: What is the key advantage of SPPM over standard SGD in terms of learning rate selection?

- Concept: Stratified Sampling
  - Why needed here: Stratified sampling reduces variance in gradient estimates, improving convergence speed and reducing neighborhood size.
  - Quick check question: How does stratified sampling differ from simple random sampling in terms of variance reduction?

- Concept: Strong Convexity
  - Why needed here: Strong convexity ensures unique minimizers and enables the convergence analysis of SPPM-AS.
  - Quick check question: What property of the objective function allows for the application of SPPM-AS's convergence theory?

## Architecture Onboarding

- Component map: Server -> Cohort Selection -> Clients -> Proximal Solver -> Server
- Critical path:
  1. Server selects cohort via sampling strategy
  2. Server broadcasts current model to cohort
  3. Clients perform K local communication rounds using proximal solver
  4. Clients send updated models to server
  5. Server aggregates updates and updates global model
- Design tradeoffs:
  - More local rounds (K) vs. global rounds (T): Higher K reduces T but increases local computation
  - Sampling strategy: Stratified sampling reduces variance but requires clustering
  - Proximal solver: Second-order solvers may converge faster but are computationally expensive
- Failure signatures:
  - Communication cost not reduced: K is too low or sampling strategy is suboptimal
  - Slow convergence: Proximal solver is not well-suited for the problem or learning rate is too low
  - Client drift: Local updates diverge due to high heterogeneity or insufficient regularization
- First 3 experiments:
  1. Vary K (number of local rounds) and observe impact on total communication cost
  2. Compare stratified sampling vs. block sampling on convergence speed and variance
  3. Test different proximal solvers (e.g., CG vs. BFGS) on convex logistic regression

## Open Questions the Paper Calls Out
None

## Limitations

- Heterogeneity Impact: Performance on highly heterogeneous data distributions or complex neural network architectures remains uncertain, with FEMNIST showing only moderate gains (53% reduction).
- Computational Overhead: The paper doesn't thoroughly address the trade-off between reduced communication and increased local computation, particularly for second-order methods like BFGS.
- Sampling Strategy Dependency: The claimed superiority of stratified sampling assumes well-defined clusters, which may be challenging to maintain in real-world scenarios with evolving data distributions.

## Confidence

**High Confidence**: Theoretical convergence analysis under strong convexity conditions is mathematically rigorous and well-established.

**Medium Confidence**: Extension to non-convex neural networks shows promise, but experiments are limited to relatively simple architectures.

**Low Confidence**: The claim that SPPM-AS "supports arbitrary learning rates" needs clarification regarding practical impact on convergence speed and stability.

## Next Checks

1. Cross-architecture testing: Evaluate SPPM-AS on deeper neural networks (ResNet, Transformer-based models) and more complex federated learning tasks to assess generalization beyond current experimental scope.

2. Cost-benefit analysis: Conduct experiments measuring total energy consumption (communication + computation) across different proximal solvers and local round configurations to validate practical efficiency claims.

3. Dynamic cluster adaptation: Implement and test an adaptive clustering mechanism that can handle evolving data distributions, measuring impact on convergence when clusters become suboptimal over time.