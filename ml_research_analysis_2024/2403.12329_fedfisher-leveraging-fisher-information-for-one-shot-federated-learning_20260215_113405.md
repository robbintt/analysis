---
ver: rpa2
title: 'FedFisher: Leveraging Fisher Information for One-Shot Federated Learning'
arxiv_id: '2403.12329'
source_url: https://arxiv.org/abs/2403.12329
tags:
- learning
- fedfisher
- local
- data
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FedFisher introduces a one-shot federated learning approach using
  Fisher information matrices computed on local client models. Motivated by a Bayesian
  perspective, the method reformulates the global posterior decomposition and approximates
  the Hessian with Fisher matrices to enable tractable inference.
---

# FedFisher: Leveraging Fisher Information for One-Shot Federated Learning

## Quick Facts
- arXiv ID: 2403.12329
- Source URL: https://arxiv.org/abs/2403.12329
- Authors: Divyansh Jhunjhunwala, Shiqiang Wang, Gauri Joshi
- Reference count: 40
- One-line primary result: FedFisher achieves 5-10% accuracy improvement over baselines like knowledge distillation, neuron matching, and model fusion in one-shot federated learning settings.

## Executive Summary
FedFisher introduces a one-shot federated learning approach that leverages Fisher information matrices computed on local client models. Motivated by a Bayesian perspective, the method reformulates the global posterior decomposition and approximates the Hessian with Fisher matrices to enable tractable inference. Theoretical analysis for two-layer over-parameterized ReLU networks shows that FedFisher's error becomes vanishingly small as network width and local training steps increase. Practical variants using diagonal Fisher and K-FAC approximations maintain communication and compute efficiency while improving accuracy by 5-10% over baselines across various datasets.

## Method Summary
FedFisher operates in a one-shot manner where clients compute local models and approximate Fisher information matrices, then send them to a central server for aggregation. The server initializes the global model as the average of local models and performs gradient descent on the FedFisher objective function, which incorporates the aggregated Fisher information. The method uses two practical approximations: diagonal Fisher for communication efficiency and K-FAC for better accuracy at the cost of some privacy. Clients perform local training for a fixed number of epochs before computing their Fisher matrices, and the server aggregates these to find the global model without further client communication.

## Key Results
- FedFisher improves accuracy by 5-10% over baselines (knowledge distillation, neuron matching, model fusion) across MNIST, FashionMNIST, SVHN, CIFAR-10, CINIC-10, and GTSRB datasets
- Diagonal Fisher variant maintains FedAvg communication cost while achieving better accuracy
- K-FAC approximation provides additional accuracy gains at the expense of requiring server access to individual client Fishers
- Method demonstrates utility in few-shot settings and when using pre-trained models
- Theoretical error bounds show FedFisher error vanishes with increasing network width and local training steps

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The global posterior decomposes into a product of local posteriors under a flat prior, enabling one-shot aggregation.
- **Mechanism**: FedFisher leverages a Bayesian reformulation of the federated learning objective, expressing the global posterior as a product of client-specific posteriors. By computing local modes and approximate Fisher information, the server can infer the global mode without further communication.
- **Core assumption**: The loss function is proportional to the negative log-likelihood of an exponential-family model (Assumption 1).
- **Evidence anchors**:
  - [abstract]: "motivated by a Bayesian perspective of FL"
  - [section]: Proposition 1 states that the global posterior decomposes into a product of local posteriors
  - [corpus]: Corpus includes "One-Shot Federated Learning from Client Global Statistics" which supports Bayesian aggregation approaches
- **Break condition**: The flat prior assumption fails or the loss function does not conform to exponential-family form.

### Mechanism 2
- **Claim**: Laplace approximation with Fisher matrices enables tractable inference of the global posterior.
- **Mechanism**: The method approximates the local posterior using a second-order Taylor expansion around the mode, replacing the Hessian with the Fisher information matrix for positive semi-definiteness and computational tractability.
- **Core assumption**: The model is over-parameterized such that the local models fit their data perfectly, making the Hessian equal to the Fisher.
- **Evidence anchors**:
  - [abstract]: "makes use of Fisher information matrices computed on local client models"
  - [section]: Equation (2) shows the Laplace approximation and Equation (4) shows the Fisher approximation
  - [corpus]: Corpus includes "One-Shot Federated Learning based on Bayesian Ensemble" supporting Bayesian approximation methods
- **Break condition**: The over-parameterization assumption fails or the Fisher approximation introduces significant error.

### Mechanism 3
- **Claim**: Gradient descent on the FedFisher objective converges to the global mode when initialized at the average of local models.
- **Mechanism**: The FedFisher objective is convex in the subspace spanned by the aggregated Fisher, and gradient descent with proper initialization and learning rate finds the global minimum.
- **Core assumption**: The step size is bounded by the inverse of the maximum eigenvalue of the aggregated Fisher.
- **Evidence anchors**:
  - [abstract]: "the error of our one-shot FedFisher global model becomes vanishingly small"
  - [section]: Lemma 1 proves convergence of gradient descent to the FedFisher global model
  - [corpus]: Corpus includes "One-Shot Federated Learning with Bayesian Pseudocoresets" supporting single-round optimization approaches
- **Break condition**: The aggregated Fisher is ill-conditioned or the initialization is too far from the solution.

## Foundational Learning

- **Concept**: Bayesian inference and posterior decomposition
  - **Why needed here**: FedFisher's theoretical foundation relies on viewing federated learning as posterior inference, enabling the decomposition that makes one-shot aggregation possible
  - **Quick check question**: Can you explain why the global posterior decomposes into a product of local posteriors under a flat prior?

- **Concept**: Fisher information matrix and its properties
  - **Why needed here**: The Fisher matrix replaces the Hessian in the Laplace approximation, providing positive semi-definiteness and enabling efficient computation through approximations like diagonal and K-FAC
  - **Quick check question**: What property of the Fisher matrix makes it preferable to the Hessian for the Laplace approximation in this context?

- **Concept**: Over-parameterized neural networks and optimization
  - **Why needed here**: The theoretical analysis assumes over-parameterization to justify the Laplace approximation and to bound the error terms in terms of network width
  - **Quick check question**: How does over-parameterization affect the relationship between the Hessian and Fisher matrices at the mode?

## Architecture Onboarding

- **Component map**:
  Server -> Clients (broadcast W0) -> Clients (local training) -> Clients (compute ˜Wi, ˜Fi) -> Server (aggregate) -> Server (gradient descent) -> Server (return W*)

- **Critical path**:
  1. Server broadcasts initial model W0 to all clients
  2. Each client performs local training for K steps to obtain ˜Wi
  3. Each client computes approximate Fisher ˜Fi (diagonal or K-FAC)
  4. Clients send ˜Wi and ˜Fi to server (with compression)
  5. Server aggregates to get initial W(0) = average of ˜Wi
  6. Server performs T steps of gradient descent on FedFisher objective
  7. Server returns final global model W*

- **Design tradeoffs**:
  - Communication vs. accuracy: Full Fisher gives best accuracy but high communication cost; diagonal/K-FAC trade some accuracy for efficiency
  - Local computation vs. server computation: More local training steps reduce local optimization error but increase Laplace approximation error
  - Privacy vs. utility: K-FAC provides better accuracy than diagonal but requires server access to individual Fishers, reducing privacy

- **Failure signatures**:
  - Poor accuracy: Indicates insufficient local training, inappropriate Fisher approximation, or data heterogeneity too high
  - Slow convergence: Suggests learning rate too small or aggregated Fisher ill-conditioned
  - High communication cost: Indicates lack of compression or use of full Fisher instead of approximations

- **First 3 experiments**:
  1. **Synthetic validation**: Replicate Figure 1 with varying width and local steps to verify theoretical predictions about error decomposition
  2. **Communication efficiency test**: Compare accuracy vs. compression level for diagonal and K-FAC variants on FashionMNIST
  3. **Privacy assessment**: Run model inversion attack as in Appendix E to measure privacy-utility tradeoff of K-FAC variant vs. baselines

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Theoretical analysis requires over-parameterized networks and perfect local fitting, which may not hold for all real-world architectures
- Assumption of exponential-family loss function limits applicability to specific model types
- Communication efficiency gains come at the cost of approximation accuracy in the Fisher matrix

## Confidence
- Mechanism 1 (Bayesian decomposition): High confidence - well-established Bayesian framework with clear assumptions
- Mechanism 2 (Laplace approximation): Medium confidence - theoretically sound but dependent on over-parameterization assumptions
- Mechanism 3 (Gradient descent convergence): Medium confidence - requires specific conditions on initialization and learning rate

## Next Checks
1. Test FedFisher's performance degradation when the exponential-family assumption is violated using non-standard loss functions
2. Empirically measure the relationship between model width and FedFisher error to validate theoretical predictions
3. Evaluate privacy-utility tradeoffs of K-FAC approximation using comprehensive privacy attacks beyond the simple model inversion test described in Appendix E