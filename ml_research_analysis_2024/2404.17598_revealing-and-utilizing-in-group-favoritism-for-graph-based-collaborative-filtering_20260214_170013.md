---
ver: rpa2
title: Revealing and Utilizing In-group Favoritism for Graph-based Collaborative Filtering
arxiv_id: '2404.17598'
source_url: https://arxiv.org/abs/2404.17598
tags:
- users
- co-clustering
- each
- graph
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Co-Clustering Wrapper (CCW) for graph-based
  collaborative filtering, which aims to improve recommendation performance by capturing
  in-group favoritism in user-item interaction data. The key idea is to apply spectral
  co-clustering to the user-item bipartite graph to identify strongly connected clusters,
  and then train separate collaborative filtering models for each cluster in addition
  to the global model.
---

# Revealing and Utilizing In-group Favoritism for Graph-based Collaborative Filtering

## Quick Facts
- arXiv ID: 2404.17598
- Source URL: https://arxiv.org/abs/2404.17598
- Reference count: 40
- Improves collaborative filtering by capturing in-group favoritism through spectral co-clustering of user-item interactions

## Executive Summary
This paper addresses a fundamental limitation in graph-based collaborative filtering: the inability to effectively capture in-group favoritism, where users tend to prefer items within their own social or preference groups. The authors propose Co-Clustering Wrapper (CCW), which applies spectral co-clustering to identify strongly connected user-item clusters and trains separate models for each cluster alongside a global model. The approach combines local and global embeddings with learned importance coefficients to make final predictions, showing consistent improvements across multiple datasets and CF models.

## Method Summary
The Co-Clustering Wrapper (CCW) works by first applying spectral co-clustering to the user-item bipartite graph to identify k clusters of strongly connected users and items. For each cluster, a local collaborative filtering model is trained alongside a global model trained on all data. During inference, the final prediction combines the global embedding with weighted local embeddings from clusters that contain the user or item, where the weights are learned importance coefficients. The number of clusters k is determined using a variance ratio metric that measures how well the clustering preserves the structure of the original graph.

## Key Results
- CCW consistently improves Recall@20 and NDCG@20 across five different CF models (MF-BPR, NGCF, GCMC, SGNN_RM, LightGCN)
- On Yelp2018 dataset, CCW improves NGCF's Recall@20 from 0.0574 to 0.0586 and NDCG@20 from 0.0466 to 0.0473
- Performance gains are demonstrated on four real-world datasets: Yelp2018, Amazon-CDs, Amazon-Book, and Amazon-Electronics
- The method shows robustness across different base CF architectures and dataset characteristics

## Why This Works (Mechanism)
The paper's approach works by explicitly capturing in-group favoritism through clustering users and items with similar interaction patterns. By training separate models for each cluster, the method can learn specialized embeddings that better represent the preferences within homogeneous groups. The combination of local and global embeddings allows the model to leverage both group-specific patterns and overall trends. The learned importance coefficients automatically adjust the contribution of each local model based on the specific user-item pair being predicted.

## Foundational Learning
- **Spectral co-clustering**: Why needed - to identify natural clusters in user-item interaction graphs; Quick check - verify clusters capture meaningful user groups
- **Graph-based collaborative filtering**: Why needed - to leverage user-item interaction structure; Quick check - ensure base CF model performance is solid
- **Ensemble learning**: Why needed - to combine local and global knowledge; Quick check - validate that combining models improves performance
- **Bipartite graph analysis**: Why needed - to understand user-item interaction patterns; Quick check - confirm graph structure supports clustering
- **Embedding combination strategies**: Why needed - to effectively merge local and global representations; Quick check - test different combination methods

## Architecture Onboarding

Component Map: Data -> Co-clustering -> Multiple CF Models -> Embedding Combination -> Prediction

Critical Path: The most critical components are the co-clustering step and the embedding combination strategy. The co-clustering must effectively identify meaningful user-item groups, while the combination weights must properly balance local and global information. Failure in either component significantly impacts performance.

Design Tradeoffs: The method trades computational efficiency for improved recommendation accuracy by training multiple models. The choice of k (number of clusters) involves balancing model specialization against overfitting and computational cost. The variance ratio metric for cluster selection assumes that graph structure correlates with recommendation quality.

Failure Signatures: Poor performance may result from: (1) co-clustering identifying meaningless clusters, (2) too few or too many clusters reducing model effectiveness, (3) learned combination weights failing to properly balance local/global contributions, or (4) base CF models not benefiting from clustering due to dataset characteristics.

First Experiments:
1. Test CCW with different values of k to find optimal cluster count for each dataset
2. Compare performance with and without local model training to isolate clustering contribution
3. Evaluate sensitivity to base CF model choice by testing all five models on each dataset

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but several emerge from the results. The most pressing is whether the improvements come from the co-clustering approach specifically or from the general practice of training multiple models and combining them. Additionally, the method's scalability for very large datasets and its performance with extremely sparse data remain unexplored areas for future research.

## Limitations
- The computational overhead of training multiple local models may be prohibitive for large-scale applications
- The method assumes user-item interactions naturally form distinct clusters, which may not hold for all recommendation scenarios
- Limited discussion of how the approach performs with very sparse datasets or highly variable interaction counts per user

## Confidence
- **High**: The methodological framework for combining local and global embeddings is sound
- **Medium**: The experimental results showing consistent improvements across datasets and models
- **Medium**: The cluster selection methodology using variance ratio

## Next Checks
1. Conduct ablation studies comparing CCW with a simple ensemble of base models without clustering to isolate the contribution of the co-clustering approach
2. Test the method on extremely sparse datasets to evaluate its robustness when user-item interactions are limited
3. Analyze the computational overhead and scalability of training multiple local models compared to the base models alone