---
ver: rpa2
title: Ontology-grounded Automatic Knowledge Graph Construction by LLM under Wikidata
  schema
arxiv_id: '2412.20942'
source_url: https://arxiv.org/abs/2412.20942
tags:
- knowledge
- ontology
- adams
- douglas
- schema
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an ontology-grounded approach for knowledge
  graph construction using LLMs under Wikidata schema. The method generates competency
  questions from documents, extracts relations, matches them with Wikidata properties,
  and grounds KG generation on the resulting ontology.
---

# Ontology-grounded Automatic Knowledge Graph Construction by LLM under Wikidata schema

## Quick Facts
- arXiv ID: 2412.20942
- Source URL: https://arxiv.org/abs/2412.20942
- Reference count: 33
- Primary result: Ontology-grounded LLM approach achieves competitive performance on KG construction tasks with improved interpretability and Wikidata interoperability

## Executive Summary
This paper presents an ontology-grounded approach for automatic knowledge graph construction using large language models under Wikidata schema constraints. The method generates competency questions from documents, extracts relations, matches them with Wikidata properties, and grounds KG generation on the resulting ontology. Experiments on Wiki-NRE, SciERC, and WebNLG datasets demonstrate competitive performance compared to fine-tuned and LLM baselines, with improvements in interpretability and interoperability with Wikidata.

## Method Summary
The approach follows a pipeline: (1) generate competency questions and answers from unstructured documents using LLMs, (2) extract relations from CQs and match them against Wikidata properties using vector similarity search with LLM validation, (3) format the matched properties into OWL ontology with inferred domain and range classes, and (4) use the ontology to guide KG construction from documents with LLMs. The system outputs KGs in RDF/Turtle format compatible with Wikidata, enabling interoperability with existing knowledge bases while maintaining flexibility through competency question scoping.

## Key Results
- Competitive performance on Wiki-NRE, SciERC, and WebNLG datasets compared to fine-tuned and LLM baselines
- Improved interpretability through competency question-driven KG construction
- Enhanced interoperability with Wikidata through schema-constrained ontology grounding
- Demonstrated potential for scalable KG construction with minimal human intervention

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Grounding KG generation on Wikidata schema reduces redundancy and improves interoperability
- Mechanism: By aligning extracted properties with Wikidata's existing property set, the system leverages the established knowledge base structure and semantics, ensuring generated KGs are compatible with Wikidata
- Core assumption: Wikidata schema covers a sufficiently broad range of domain-specific properties needed for proprietary knowledge domains
- Evidence anchors:
  - [abstract]: "We propose a novel ontology-grounded approach to LLM-based KG construction that leverages ontology based on Wikidata schema to guide the extraction and integration of knowledge from unstructured text"
  - [section 3.2]: "we extract relations from CQs and match them against Wikidata properties to better elicit model memories on Wikidata when constructing and using ontology"
  - [corpus]: Weak - no direct corpus evidence comparing Wikidata-based vs non-Wikidata approaches
- Break condition: When target domain requires properties that are not represented in Wikidata or when schema alignment fails to capture domain-specific nuances

### Mechanism 2
- Claim: Competency question generation scopes the KG construction task and ensures relevance
- Mechanism: LLMs generate domain-specific questions that define the knowledge scope, which then guides the extraction of relevant entities and relations
- Core assumption: Competency questions accurately capture the key information needs of the target domain
- Evidence anchors:
  - [abstract]: "Our approach begins by discovering the scope of knowledge through the generation of Competency Questions (CQ) and answers from unstructured documents"
  - [section 3.1]: "The first step in our pipeline is to generate a set of competency questions (CQs) and answers that capture the key information needs of the target domain"
  - [corpus]: Weak - no direct evidence of CQ quality assessment or comparison with alternative scoping methods
- Break condition: When generated CQs miss critical domain aspects or when documents contain information not captured by the CQs

### Mechanism 3
- Claim: Ontology matching and formatting creates a formal, machine-readable schema that grounds KG generation
- Mechanism: LLMs extract properties from CQs, match them with Wikidata properties, and generate OWL ontology with proper domain and range definitions
- Core assumption: LLMs can accurately infer and summarize classes for domain and range of relations
- Evidence anchors:
  - [section 3.3]: "we use LLM to generate an OWL ontology based on the matched and newly created properties... For new properties, LLM is prompted to infer and summarize classes for the domain and range of the relations"
  - [abstract]: "we ground generation of KG with the authored ontology based on extracted relations"
  - [corpus]: Weak - no corpus evidence of ontology quality or comparison with manually authored ontologies
- Break condition: When LLMs fail to accurately infer domain/range relationships or when generated ontology is inconsistent

## Foundational Learning

- Concept: Wikidata schema and property structure
  - Why needed here: Understanding Wikidata's property types, domain/range relationships, and namespace conventions is essential for proper schema alignment and ontology generation
  - Quick check question: What are the key components of a Wikidata property entry and how do they relate to KG construction?

- Concept: RDF/Turtle syntax and ontology formats
  - Why needed here: The system outputs KGs in RDF/Turtle format and generates OWL ontologies, requiring familiarity with these semantic web standards
  - Quick check question: How does the Turtle syntax represent triples and what are the key elements of an OWL ontology?

- Concept: Vector similarity search for property matching
- Concept: Competency question generation and information extraction
  - Why needed here: Understanding how to formulate effective CQs and extract relevant information from documents is crucial for scoping KG construction
  - Quick check question: What characteristics make a competency question effective for knowledge graph construction?

## Architecture Onboarding

- Component map: Document input → CQ generation (LLM) → CQ answering (LLM) → Relation extraction (LLM) → Ontology matching (vector search + LLM validation) → Ontology formatting (LLM) → KG construction (LLM) → RDF output
- Critical path: Document → CQ generation → Relation extraction → Ontology matching → KG construction
- Design tradeoffs:
  - Zero-shot approach vs fine-tuning: Zero-shot provides flexibility but may sacrifice performance
  - Wikidata constraint vs no constraint: Constraint ensures compatibility but may limit coverage
  - LLM size selection: Larger models (GPT-4o) perform better but cost more
  - Schema completeness vs strict adherence: Richer ontology captures more knowledge but may hurt precision
- Failure signatures:
  - Low precision/recall: Check ontology matching quality and LLM prompt effectiveness
  - Inconsistent ontology: Verify domain/range inference and property matching logic
  - Slow processing: Profile LLM calls and consider batching or caching strategies
  - Poor Wikidata alignment: Review property matching threshold and validation process
- First 3 experiments:
  1. Baseline comparison: Run the pipeline on a small document set with and without Wikidata schema alignment to measure interoperability benefits
  2. Ontology coverage analysis: Extract the full ontology from a document set and compare against known domain schemas to assess coverage
  3. Scalability test: Process increasing document volumes to identify performance bottlenecks and memory constraints

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed pipeline's computational efficiency compare to traditional fine-tuned approaches across different dataset sizes?
- Basis in paper: Inferred from computational resources section
- Why unresolved: The paper acknowledges the computational resource trade-offs but doesn't provide direct performance comparisons between the pipeline and fine-tuned approaches in terms of efficiency metrics like processing time or resource utilization.
- What evidence would resolve it: Empirical data comparing processing times, GPU memory usage, and computational costs between the proposed pipeline and various fine-tuned baselines across datasets of different sizes.

### Open Question 2
- Question: What is the optimal balance between ontology completeness and target schema constraint for achieving the best performance?
- Basis in paper: Inferred from performance discrepancy discussion in Section 4.3.1
- Why unresolved: The paper demonstrates the trade-off between schema completeness and strict adherence to predefined ontology but doesn't identify the optimal balance point for different use cases or domains.
- What evidence would resolve it: Systematic experiments varying the ratio of ontology expansion versus constraint, measuring performance impact across multiple domains and use cases.

### Open Question 3
- Question: How does the pipeline's performance degrade when processing documents with domain-specific terminology not present in Wikidata?
- Basis in paper: Explicit from the discussion of SciERC dataset results and Wikidata schema usage
- Why unresolved: While the paper shows performance on SciERC (which contains non-Wikidata properties), it doesn't systematically analyze performance degradation patterns when encountering domain-specific terminology.
- What evidence would resolve it: Experiments measuring performance on increasingly domain-specific documents, identifying the point where Wikidata-based ontology matching becomes insufficient.

## Limitations
- Evaluation relies on benchmark datasets that may not capture truly proprietary or domain-specific knowledge performance
- Assumes Wikidata schema coverage is sufficient for most domains without validation for proprietary ontologies
- LLM-based ontology generation lacks systematic evaluation of quality and consistency

## Confidence
- **High Confidence:** The pipeline architecture and technical implementation details are clearly described and reproducible
- **Medium Confidence:** Competitive performance claims against baselines, though evaluation is limited to specific datasets
- **Low Confidence:** Claims about scalability and applicability to truly proprietary knowledge without human intervention, as no such real-world deployment or evaluation is demonstrated

## Next Checks
1. **Schema Coverage Analysis:** Evaluate the system's performance on proprietary document sets where Wikidata properties are known to be insufficient, measuring the gap between required and available properties
2. **Ontology Quality Assessment:** Implement systematic validation of automatically generated OWL ontologies against manually curated ontologies in the same domains, measuring consistency, completeness, and logical correctness
3. **Real-world Deployment Test:** Apply the pipeline to a genuinely proprietary knowledge domain with no pre-existing KG, measuring construction accuracy, interoperability, and human intervention requirements in practice