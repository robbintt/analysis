---
ver: rpa2
title: 'Question: How do Large Language Models perform on the Question Answering tasks?
  Answer:'
arxiv_id: '2412.12893'
source_url: https://arxiv.org/abs/2412.12893
tags:
- llms
- answer
- questions
- question
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares large language models (LLMs) to fine-tuned
  models on the SQuAD2 question-answering dataset, focusing on single-inference prompting
  for unanswerable questions. The authors evaluate models including GPT-4 Turbo, various
  LLaMA variants, and fine-tuned models (Flan-T5, DistilBERT, RoBERTa) using standard
  metrics (EM, F1) and extended analysis including Levenshtein distance and interrogative
  pronoun breakdown.
---

# Question: How do Large Language Models perform on the Question Answering tasks? Answer:

## Quick Facts
- arXiv ID: 2412.12893
- Source URL: https://arxiv.org/abs/2412.12893
- Authors: Kevin Fischer; Darren Fürst; Sebastian Steindl; Jakob Lindner; Ulrich Schäfer
- Reference count: 39
- Key outcome: LLMs achieve competitive performance on SQuAD2 question-answering task, with LLaMA-3.1-70B outperforming fine-tuned models on 3 of 5 out-of-distribution datasets

## Executive Summary
This paper presents a comprehensive comparison between large language models (LLMs) and fine-tuned models on the SQuAD2 question-answering dataset, with particular focus on handling unanswerable questions through single-inference prompting. The authors evaluate multiple models including GPT-4 Turbo, various LLaMA variants, and fine-tuned models (Flan-T5, DistilBERT, RoBERTa) using standard metrics and extended analysis including Levenshtein distance. The study demonstrates that while fine-tuned models generally outperform LLMs on the benchmark, LLaMA-3.1-70B achieves competitive F1 scores and shows better generalization to out-of-distribution QA datasets. The proposed single-inference prompting strategy eliminates the need for separate handling of answerable and unanswerable questions, reducing computational cost while maintaining performance.

## Method Summary
The authors evaluate LLMs and fine-tuned models on SQuAD2 using standard metrics (EM, F1) and extended analysis including Levenshtein distance and interrogative pronoun breakdown. They implement a single-inference prompting strategy that handles both answerable and unanswerable questions without separate processing steps. The study compares multiple models including GPT-4 Turbo, various LLaMA variants, and fine-tuned models (Flan-T5, DistilBERT, RoBERTa). The evaluation extends to out-of-distribution datasets to assess generalization capabilities, with 5 additional datasets tested beyond the primary SQuAD2 benchmark.

## Key Results
- Fine-tuned models generally outperform LLMs on SQuAD2 benchmark metrics
- LLaMA-3.1-70B achieves competitive F1 scores compared to fine-tuned models
- LLaMA-3.1-70B outperforms fine-tuned models on 3 of 5 tested out-of-distribution datasets
- Single-inference prompting strategy reduces computational cost while maintaining performance

## Why This Works (Mechanism)
The effectiveness of LLMs in question-answering tasks stems from their ability to leverage pre-trained knowledge across diverse domains, enabling better generalization to out-of-distribution datasets. The single-inference prompting strategy works by conditioning the model to simultaneously determine answerability and generate responses within a single forward pass, eliminating the computational overhead of separate classification and generation steps. This approach benefits from the inherent language understanding capabilities of LLMs, which can implicitly reason about question-answerability through contextual understanding rather than explicit classification. The competitive performance of LLaMA-3.1-70B suggests that well-designed open-weight models can approach or match the capabilities of proprietary models when properly prompted, particularly for tasks requiring nuanced understanding of question-answer relationships.

## Foundational Learning
1. **Question-Answering Dataset Characteristics**
   - Why needed: Understanding the structure and challenges of SQuAD2, including unanswerable questions
   - Quick check: Verify dataset statistics (train/validation split, unanswerable question ratio)

2. **Evaluation Metrics for QA Tasks**
   - Why needed: EM and F1 metrics measure exact match and token overlap for answer quality
   - Quick check: Compare metric distributions across different model types

3. **Single-Inference Prompting Strategy**
   - Why needed: Reduces computational cost by combining answerability determination and answer generation
   - Quick check: Measure inference time reduction compared to multi-step approaches

4. **Out-of-Distribution Generalization**
   - Why needed: Critical for assessing real-world applicability beyond benchmark datasets
   - Quick check: Analyze performance drop across different dataset domains

## Architecture Onboarding
Component map: Data preprocessing -> Model inference -> Metric calculation -> Analysis pipeline
Critical path: Input question and context → Prompt engineering → Single-inference generation → Post-processing → Metric evaluation
Design tradeoffs: Single-inference approach vs. multi-step processing (speed vs. potential accuracy)
Failure signatures: Overconfident incorrect answers, failure to recognize unanswerable questions, inconsistent performance across domains
First experiments:
1. Baseline evaluation on SQuAD2 using standard multi-step approach
2. Single-inference prompting implementation with ablation studies on prompt structure
3. Cross-dataset generalization testing with controlled domain variations

## Open Questions the Paper Calls Out
None

## Limitations
- Study focuses exclusively on SQuAD2 and limited out-of-distribution datasets, potentially missing broader QA challenges
- Single-inference prompting may sacrifice accuracy for efficiency and requires careful prompt engineering
- Evaluation metrics may not fully capture qualitative aspects of answer quality like factual correctness
- Comparison doesn't account for significant differences in computational resources required between LLMs and fine-tuned models

## Confidence
High Confidence: Fine-tuned models generally outperforming LLMs on SQuAD2, LLaMA-3.1-70B achieving competitive performance
Medium Confidence: Better generalization of LLMs to out-of-distribution datasets, effectiveness of single-inference prompting strategy
Low Confidence: Claim that single-inference approach "eliminates the need" for separate handling may be overstated

## Next Checks
1. Broader dataset evaluation across diverse QA tasks and domains beyond the 5 tested out-of-distribution datasets
2. Comprehensive resource efficiency analysis comparing computational costs (training time, inference latency, energy consumption) between LLMs and fine-tuned models
3. Human evaluation studies to assess qualitative aspects of answer quality including factual accuracy and contextual appropriateness