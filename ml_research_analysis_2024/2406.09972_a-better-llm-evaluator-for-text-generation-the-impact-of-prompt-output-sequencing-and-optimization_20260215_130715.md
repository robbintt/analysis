---
ver: rpa2
title: 'A Better LLM Evaluator for Text Generation: The Impact of Prompt Output Sequencing
  and Optimization'
arxiv_id: '2406.09972'
source_url: https://arxiv.org/abs/2406.09972
tags:
- output
- json
- score
- prompt
- reasons
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines how the order of output instructions in prompts
  affects LLM scoring for text generation tasks. The authors find that placing reasons
  before scores leads to higher and more consistent scores compared to placing scores
  before reasons, likely because the autoregressive nature of LLMs allows the score
  to be influenced by the previously outputted reasons.
---

# A Better LLM Evaluator for Text Generation: The Impact of Prompt Output Sequencing and Optimization

## Quick Facts
- **arXiv ID**: 2406.09972
- **Source URL**: https://arxiv.org/abs/2406.09972
- **Reference count**: 18
- **Primary result**: Placing reasons before scores in LLM evaluation prompts leads to higher and more consistent scores, while adding task-specific rules and optimization techniques like GRIPS further improves scoring accuracy.

## Executive Summary
This paper investigates how the order of output instructions in prompts affects LLM scoring for text generation tasks. The authors demonstrate that placing reasons before scores leads to higher and more consistent scores compared to placing scores before reasons, due to the autoregressive nature of LLMs allowing scores to be influenced by preceding reasons. They also show that adding task-specific rules significantly improves the model's understanding and adherence to scoring requirements. Additionally, the authors explore prompt optimization techniques like GRIPS and OPRO, finding that GRIPS can lead to statistically significant improvements in scoring accuracy when sufficient paired data is available.

## Method Summary
The authors tested six prompt variations with different output instruction orders (score-first vs reason-first) across four LLM models. They conducted 50 trials per model and configuration, using dialogue evaluation tasks with special scoring rules. For optimization experiments, they applied GRIPS and OPRO techniques using SummEval dataset paired samples. The evaluation metrics included mean scores, standard deviations, MAE, and correlation coefficients (Pearson's r and Kendall's τ). Temperature settings were varied between score set (0.3) and test set (1.0) runs.

## Key Results
- Placing reasons before scores consistently produced higher mean scores across all tested models and configurations
- Adding task-specific rules significantly improved scoring consistency and alignment with evaluation criteria
- GRIPS optimization achieved statistically significant improvements in scoring accuracy when sufficient paired data was available
- The json output format with reason-first ordering showed the most consistent performance across different models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The order of output instructions affects LLM scoring due to autoregressive generation dynamics.
- Mechanism: When reasons are generated before the score, the LLM's autoregressive nature allows the score to be influenced by the context established in the reasons. Conversely, when the score is generated first, it lacks this contextual influence and may be less aligned with the detailed reasoning.
- Core assumption: LLMs generate tokens sequentially, and earlier tokens can influence the generation of later tokens through attention mechanisms.
- Evidence anchors:
  - [abstract]: "placing reasons before scores leads to higher and more consistent scores compared to placing scores before reasons, likely because the autoregressive nature of LLMs allows the score to be influenced by the previously outputted reasons."
  - [section 2.1]: "In both ex (·) and json (·) formats, the mean scores for the rs settings (output reasons before the score) are generally higher than their sr (output score before reasons) counterparts."

### Mechanism 2
- Claim: Including task-specific rules in the prompt significantly improves the model's understanding and adherence to scoring requirements.
- Mechanism: Task-specific rules provide explicit guidance that helps the LLM align its scoring with the desired criteria, reducing ambiguity and subjectivity in evaluation.
- Core assumption: LLMs benefit from explicit instructions that clarify the task's objectives and constraints.
- Evidence anchors:
  - [abstract]: "they also demonstrate that adding task-specific rules to the prompt significantly improves the model's understanding and adherence to the scoring requirements."
  - [section 2.1]: "when we removed the 'special rules' from the prompt, we found that most scores were lower and the distinctions between different settings became less pronounced."

### Mechanism 3
- Claim: Prompt optimization techniques like GRIPS can improve scoring alignment when sufficient paired data is available.
- Mechanism: GRIPS iteratively searches for editing commands that maximize the overall score on a given dataset, refining the prompt to better match human ratings.
- Core assumption: There exists a prompt configuration that, when optimized, yields better alignment with human evaluations.
- Evidence anchors:
  - [abstract]: "the authors investigate the effect of prompt optimization techniques like GRIPS and OPRO on improving the alignment between LLM scores and human ratings, finding that GRIPS can lead to statistically significant improvements in scoring accuracy when sufficient paired data is available."
  - [section 3.1]: "Following [15], we apply William's test to Pearson's r, and we found that the improvement with GRIPS over the initial instruction is statistically significant."

## Foundational Learning

- Concept: Autoregressive generation in LLMs
  - Why needed here: Understanding how the order of output instructions affects scoring requires knowledge of how LLMs generate text sequentially and how earlier tokens influence later ones.
  - Quick check question: How does the autoregressive nature of LLMs impact the generation of scores when reasons are placed before or after the score in the prompt?

- Concept: Prompt engineering and optimization
  - Why needed here: The paper demonstrates the importance of carefully crafting prompts and using optimization techniques to improve LLM performance on specific tasks.
  - Quick check question: What are the key considerations when designing prompts for LLM-based evaluation tasks, and how can optimization techniques like GRIPS be applied?

- Concept: Evaluation metrics and correlation analysis
  - Why needed here: Assessing the effectiveness of different prompt configurations and optimization techniques requires understanding evaluation metrics like MAE, Pearson's r, and Kendall's τ.
  - Quick check question: How do metrics like MAE and correlation coefficients help in evaluating the alignment between LLM scores and human ratings?

## Architecture Onboarding

- Component map: Data collection -> Prompt generation -> LLM scoring -> Optimization -> Analysis
- Critical path: 1) Collect dialogue data with quality issues, 2) Design prompt variations, 3) Run scoring trials, 4) Apply optimization if data available, 5) Analyze results
- Design tradeoffs: Balancing prompt complexity with model performance vs. computational cost of multiple variations
- Failure signatures: Inconsistent scoring distributions, low correlation with human ratings, optimization overfitting
- First 3 experiments: 1) Test output order effects on scoring consistency, 2) Evaluate task-specific rules impact, 3) Apply GRIPS optimization to subset data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different prompt optimization techniques compare in terms of improving LLM scoring accuracy for subjective tasks like dialogue evaluation?
- Basis in paper: Explicit
- Why unresolved: The paper compares two specific optimization techniques (GRIPS and OPRO) but does not provide a comprehensive analysis of other available methods or a direct comparison between these two.
- What evidence would resolve it: A systematic evaluation of various prompt optimization techniques on the same dialogue evaluation task, comparing their effectiveness in terms of scoring accuracy and generalization to unseen data.

### Open Question 2
- Question: What is the optimal number of exemplars to include in the prompt for improving LLM scoring performance on subjective tasks?
- Basis in paper: Inferred
- Why unresolved: The paper mentions including two randomly selected data exemplars when generating output instructions with OPRO, but does not explore the effect of varying the number of exemplars on scoring performance.
- What evidence would resolve it: An ablation study examining the impact of different numbers of exemplars (e.g., 0, 1, 2, 5, 10) on LLM scoring accuracy and consistency across multiple dialogue evaluation tasks.

### Open Question 3
- Question: How does the order of output instructions (reasons before scores vs. scores before reasons) interact with the specific task and model architecture to influence scoring outcomes?
- Basis in paper: Explicit
- Why unresolved: The paper demonstrates that placing reasons before scores generally leads to higher scores, but does not investigate the underlying reasons for this phenomenon or explore its generalizability to other tasks and model architectures.
- What evidence would resolve it: A series of controlled experiments varying the task type, model architecture, and prompt structure to isolate the factors that contribute to the observed effect of output instruction order on scoring outcomes.

## Limitations
- Limited to dialogue evaluation tasks, may not generalize to other domains like code generation or long-form content assessment
- Relies heavily on GPT-family models, leaving uncertainty about applicability to other LLM architectures
- Insufficient detail on methodological choices, particularly around derivation of "special rules" and exact optimization implementation

## Confidence
- Output order effects: Medium - based on systematic experimentation with statistical validation
- Prompt optimization results: Medium - demonstrated improvements but sensitive to data quality
- Generalization to other tasks/models: Low - limited cross-domain and cross-model testing

## Next Checks
1. Test whether the output order effects persist across different evaluation tasks (e.g., code quality, summarization coherence) to establish domain generality
2. Compare the performance of the optimized prompts against human-written rubrics for the same evaluation tasks
3. Evaluate whether the findings hold when using open-source models like Llama or Claude, which may have different autoregressive properties