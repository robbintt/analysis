---
ver: rpa2
title: 'SUN Team''s Contribution to ABAW 2024 Competition: Audio-visual Valence-Arousal
  Estimation and Expression Recognition'
arxiv_id: '2403.12609'
source_url: https://arxiv.org/abs/2403.12609
tags:
- recognition
- emotion
- challenge
- in-the-wild
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The SUN team participated in the 2024 ABAW challenge, focusing
  on audio-visual valence-arousal estimation and expression recognition. They developed
  deep learning models combining Convolutional Neural Networks (CNN) and Public Dimensional
  Emotion Model (PDEM) for video and audio modalities, respectively.
---

# SUN Team's Contribution to ABAW 2024 Competition: Audio-visual Valence-Arousal Estimation and Expression Recognition

## Quick Facts
- arXiv ID: 2403.12609
- Source URL: https://arxiv.org/abs/2403.12609
- Reference count: 40
- Primary result: Developed deep learning models combining CNN and PDEM for audio-visual emotion recognition, achieving competitive performance on the AffWild2 dataset

## Executive Summary
The SUN team participated in the 2024 ABAW challenge, focusing on audio-visual valence-arousal estimation and expression recognition. They developed deep learning models combining Convolutional Neural Networks (CNN) and Public Dimensional Emotion Model (PDEM) for video and audio modalities, respectively. The team experimented with various temporal modeling and fusion strategies using embeddings from modality-specific Deep Neural Networks (DNN). For audio, they fine-tuned PDEM with Wav2Vec2.0 and used GRU or transformer layers for classification and regression. For video, they employed EfficientNet and Visual Transformer architectures, with dynamic models using Transformer-based temporal aggregation. They also explored Kernel ELM and statistical-based methods for temporal modeling. Decision fusion strategies, including Dirichlet-based Random Weighted Fusion and Random Forests, were used to combine audio and visual predictions. The team's approach demonstrated competitive performance on the AffWild2 dataset, showcasing the potential of deep learning models for emotion recognition in unconstrained, in-the-wild settings.

## Method Summary
The SUN team's approach involved a multi-stage training process with modality-specific models. For video, they used pre-trained EfficientNet-B1/B4 and Visual Transformer (ViT-B-16) architectures, fine-tuned on the AffWild2 dataset with added classification/regression heads. For audio, they employed PDEM with Wav2Vec2.0 backbone, also fine-tuned on AffWild2. Temporal modeling was applied to static embeddings using Transformer layers or statistical functionals. Decision fusion combined audio and visual predictions using Dirichlet-based Random Weighted Fusion and Random Forests. The system processed 4-second audio windows and 5 FPS video frames, with faces detected using RetinaFace and audio preprocessed with voice activity detection and noise filtering.

## Key Results
- Fine-tuned CNN and PDEM models achieved strong per-frame feature extraction for video and audio modalities
- Transformer-based temporal modeling captured long-range dependencies in emotional expressions
- Decision-level fusion using DWF and Random Forests effectively combined complementary audio-visual information
- The approach demonstrated competitive performance on the AffWild2 dataset in the ABAW 2024 challenge

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using modality-specific fine-tuned models with task-specific heads enables accurate per-frame feature extraction before temporal aggregation.
- Mechanism: Fine-tuning pretrained models (EfficientNet-B1/B4, ViT-B-16 for video; PDEM with Wav2Vec2.0 for audio) adapts them to the in-the-wild AffWild2 dataset, while adding task-specific classification/regression heads allows the models to produce accurate predictions at the frame level. This per-frame accuracy is critical before applying temporal modeling.
- Core assumption: Fine-tuning on domain-specific data improves generalization over lab-controlled data.
- Evidence anchors:
  - [section] "we removed the last layer responsible for the classification and stacked on top of it (1) the dropout followed by deep embeddings layer (feed-forward) with 256 neurons, batch normalization, and Tanh activation function, (2) the new classification or regression layer with the corresponding number of neurons"
  - [abstract] "We particularly explore the effectiveness of architectures based on fine-tuned Convolutional Neural Networks (CNN) and Public Dimensional Emotion Model (PDEM), for video and audio modality, respectively"
- Break condition: If fine-tuning does not improve performance on the validation set, or if the domain shift between lab and in-the-wild data is too large.

### Mechanism 2
- Claim: Temporal modeling with Transformers captures long-range dependencies in emotional expressions better than statistical functionals alone.
- Mechanism: The dynamic models stack Transformer-encoder layers on top of frozen static embeddings, using self-attention to aggregate temporal context. This allows the model to capture complex temporal patterns in emotional expressions that simple functionals (mean, min, max) might miss.
- Core assumption: Emotional expressions have complex temporal dynamics that benefit from self-attention mechanisms.
- Evidence anchors:
  - [section] "To exploit this aspect, we developed dynamic FER models that take into account the temporal context during the decision-making process... the dynamic model consists of a static feature extractor and the temporal part that consists of three consecutive Transformer-encoder-based layers"
  - [abstract] "We compare alternative temporal modeling and fusion strategies using the embeddings from these multi-stage trained modality-specific Deep Neural Networks (DNN)"
- Break condition: If the temporal context is too short, or if the emotional expressions are too rapid and brief for self-attention to capture useful patterns.

### Mechanism 3
- Claim: Decision-level fusion using Dirichlet-based Random Weighted Fusion (DWF) and Random Forests combines complementary audio and visual information effectively.
- Mechanism: DWF randomly samples fusion matrices from the Dirichlet distribution and selects the best-performing one on the development set. Random Forests then use the concatenated probability vectors from base models to make final predictions. This leverages the strengths of both modalities while handling their potential conflicts.
- Core assumption: Audio and visual modalities provide complementary information for emotion recognition, and their combination improves performance.
- Evidence anchors:
  - [section] "For late fusion, we experimented with two schemes. First, we used Dirichlet-based Random Weighted Fusion (DWF)... The second decision fusion approach is based on Random Forests (RF)"
  - [section] "We selected the best-performing audio and visual models, extracted embeddings, and experimented with the attention mechanism. This approach did not outperform the video-only system on the development set."
- Break condition: If the audio and visual modalities are not sufficiently complementary, or if the fusion weights do not generalize well to the test set.

## Foundational Learning

- Concept: Transfer learning with pre-trained models
  - Why needed here: The in-the-wild nature of the AffWild2 dataset means there is limited labeled data. Transfer learning from large, diverse datasets (ImageNet, Wav2Vec2.0) allows the models to leverage prior knowledge and adapt to the specific characteristics of the emotion recognition task.
  - Quick check question: What are the benefits and potential pitfalls of using pre-trained models for a domain-specific task like emotion recognition?

- Concept: Temporal modeling in deep learning
  - Why needed here: Emotions are temporal phenomena, and their recognition requires understanding the dynamics of facial expressions and vocal cues over time. Temporal modeling techniques like RNNs and Transformers allow the models to capture these temporal dependencies.
  - Quick check question: How do different temporal modeling approaches (e.g., RNNs, Transformers, statistical functionals) compare in terms of their ability to capture long-range dependencies and handle variable-length sequences?

- Concept: Multimodal fusion strategies
  - Why needed here: Combining audio and visual modalities can provide complementary information for emotion recognition, potentially improving performance over unimodal approaches. Different fusion strategies (e.g., feature-level, decision-level) have different strengths and weaknesses.
  - Quick check question: What are the advantages and disadvantages of different multimodal fusion strategies, and how do they impact the performance of emotion recognition systems?

## Architecture Onboarding

- Component map: EfficientNet-B1/B4, ViT-B-16 (video); PDEM with Wav2Vec2.0 (audio) -> Transformer layers or statistical functionals (temporal modeling) -> DWF or Random Forests (fusion)
- Critical path: Fine-tune static models on AffWild2 -> Extract embeddings from static models -> Apply temporal modeling (Transformer or functionals) to embeddings -> Fuse audio and visual predictions using DWF or Random Forests
- Design tradeoffs: Complexity vs. performance: More complex models (e.g., Transformers) may achieve better performance but are more computationally expensive and harder to train; Modality selection: Including or excluding modalities (e.g., audio, visual) impacts performance and computational cost; Fusion strategy: Different fusion strategies (e.g., feature-level, decision-level) have different strengths and weaknesses in terms of performance and robustness.
- Failure signatures: Overfitting: High performance on the development set but poor generalization to the test set; Underfitting: Poor performance on both the development and test sets, indicating that the model is too simple or not trained adequately; Mode collapse: One modality dominates the fusion process, leading to suboptimal performance.
- First 3 experiments: 1. Fine-tune static models (EfficientNet-B1, ViT-B-16) on AffWild2 and evaluate their performance on the development set; 2. Apply temporal modeling (Transformer or functionals) to the static embeddings and compare the performance of different approaches; 3. Experiment with different fusion strategies (DWF, Random Forests) and evaluate their impact on the final performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of different fusion strategies on the performance of multi-modal emotion recognition systems, and which strategy provides the best balance between accuracy and computational efficiency?
- Basis in paper: [explicit] The paper discusses the use of Dirichlet-based Random Weighted Fusion (DWF) and Random Forests (RF) for decision fusion, but does not provide conclusive evidence on which strategy is superior.
- Why unresolved: The paper mentions that RF-based fusion overfits, even with a small number of trees, and that test set results are not yet available, preventing a definitive comparison of fusion strategies.
- What evidence would resolve it: Comparative analysis of fusion strategies using test set results, including metrics such as accuracy, computational efficiency, and robustness to overfitting.

### Open Question 2
- Question: How do different temporal context lengths affect the performance of dynamic FER models, and what is the optimal context length for capturing emotional expressions?
- Basis in paper: [explicit] The paper states that experiments were conducted with different temporal context lengths (1, 2, 3, 4, 6, and 8 seconds) but does not specify the optimal context length.
- Why unresolved: The paper mentions that the size of the temporal window can significantly influence the efficacy of the FER model, but does not provide a clear conclusion on the optimal context length.
- What evidence would resolve it: Detailed analysis of model performance across different temporal context lengths, including metrics such as accuracy, precision, and recall, to identify the optimal context length for capturing emotional expressions.

### Open Question 3
- Question: What are the limitations of current deep learning models in capturing the temporal dynamics of emotions, and how can these limitations be addressed to improve the ecological validity of emotion recognition systems?
- Basis in paper: [inferred] The paper discusses the use of Transformer-based temporal aggregation and recurrent neural networks, but does not explicitly address the limitations of these models in capturing the temporal dynamics of emotions.
- Why unresolved: The paper highlights the importance of temporal modeling in emotion recognition but does not provide a comprehensive analysis of the limitations of current models or potential solutions to improve ecological validity.
- What evidence would resolve it: Comparative analysis of different temporal modeling approaches, including their strengths and weaknesses, and proposed solutions to address limitations such as model complexity, computational efficiency, and ability to capture long-term dependencies.

## Limitations
- Limited detail on specific hyperparameters and training configurations, particularly for PDEM model layers and Transformer temporal modeling parameters
- Evaluation primarily focused on the AffWild2 dataset, limiting generalizability claims to other in-the-wild scenarios
- Lack of extensive ablation studies and cross-dataset validation

## Confidence
- **High Confidence**: The core architectural approach combining fine-tuned CNNs/Transformers with temporal modeling and multimodal fusion is well-established in the literature and logically sound for emotion recognition tasks.
- **Medium Confidence**: The reported performance metrics and comparative analysis are credible given the competitive nature of the ABAW challenge, though the lack of extensive ablation studies and cross-dataset validation introduces some uncertainty.
- **Low Confidence**: Specific implementation details, such as exact hyperparameter values and training procedures, are insufficiently described to enable exact reproduction without additional experimentation.

## Next Checks
1. **Ablation Study Replication**: Systematically vary the temporal window size, Transformer layer configurations, and fusion strategies to quantify their individual contributions to overall performance.
2. **Cross-Dataset Validation**: Evaluate the trained models on additional in-the-wild datasets (e.g., RECOLA, SEWA) to assess generalizability beyond AffWild2.
3. **Hyperparameter Sensitivity Analysis**: Conduct a grid search or Bayesian optimization over key hyperparameters (learning rates, batch sizes, Transformer parameters) to identify optimal configurations and robustness to hyperparameter choices.