---
ver: rpa2
title: 'Mix-of-Granularity: Optimize the Chunking Granularity for Retrieval-Augmented
  Generation'
arxiv_id: '2406.00456'
source_url: https://arxiv.org/abs/2406.00456
tags:
- granularity
- snippets
- router
- mogg
- medrag
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of optimizing chunk size for
  retrieval-augmented generation (RAG) systems, which is critical for precision and
  recall during retrieval. Different knowledge sources have varying data structures
  and information densities, making a uniform chunking size suboptimal.
---

# Mix-of-Granularity: Optimize the Chunking Granularity for Retrieval-Augmented Generation

## Quick Facts
- **arXiv ID**: 2406.00456
- **Source URL**: https://arxiv.org/abs/2406.00456
- **Reference count**: 18
- **Primary result**: MoG improves averaged accuracy by 5% compared to MedRAG and 8.7% compared to CoT on smaller models

## Executive Summary
This paper addresses the challenge of optimizing chunk size for retrieval-augmented generation (RAG) systems, which is critical for precision and recall during retrieval. Different knowledge sources have varying data structures and information densities, making a uniform chunking size suboptimal. The proposed Mix-of-Granularity (MoG) method dynamically determines the optimal granularity of a knowledge source based on input queries using a router. MoG is extended to MoG-Graph (MoGG), where reference documents are pre-processed as graphs, enabling the retrieval of distantly situated snippets. Experiments demonstrate that MoG and MoGG effectively predict optimal granularity levels, significantly enhancing the performance of the RAG system in downstream tasks.

## Method Summary
The Mix-of-Granularity (MoG) method uses a router module to dynamically select optimal granularity levels for retrieval based on query characteristics. The router takes query embeddings and outputs importance weights for different granularity levels, trained with soft labels to distinguish the relative importance of granularity candidates without direct top-k selection during training. MoG-Graph (MoGG) extends this by reorganizing reference documents as graphs, where documents are split into sentence-level nodes with edges created based on BM25 similarity, enabling retrieval of distantly situated snippets through "hopping ranges." The method uses soft labels (0.8 weight to most relevant, 0.2 to second most relevant) to overcome training challenges associated with top-k selection, and is evaluated on medical question-answering tasks using datasets like MMLU-Med, MedQA-US, and MedMCQA.

## Key Results
- MoG improved the averaged accuracy score by 5% compared to MedRAG and 8.7% compared to CoT on smaller models
- MoGG further improved the averaged accuracy scores, showing more significant improvement with fewer training samples
- The method demonstrates strong performance across five medical question-answering datasets with four different corpora

## Why This Works (Mechanism)

### Mechanism 1
MoG uses a router to dynamically select optimal granularity levels based on query characteristics. A multi-layer perceptron router takes query embeddings and outputs importance weights for different granularity levels. The router is trained with soft labels to distinguish the relative importance of granularity candidates without direct top-k selection during training.

### Mechanism 2
MoGG extends MoG by reorganizing reference documents as graphs to enable retrieval of distantly situated snippets. Documents are split into sentence-level nodes, edges are created based on BM25 similarity, and retrieval is performed using "hopping ranges" instead of fixed granularity levels.

### Mechanism 3
Soft labels enable effective training of the router without backward propagation issues from top-k selection. Instead of using ground truth snippets directly, soft labels assign 0.8 weight to the most relevant snippet and 0.2 to the second most relevant across different granularity levels, allowing gradient flow during training.

## Foundational Learning

- **Dual-Encoder Architecture (DEA)**: Understanding how RAG systems encode queries and documents into embeddings for similarity calculation is fundamental to grasping how MoG modifies this process. *Quick check: How does the DEA paradigm differ from traditional retrieval methods in terms of scalability and efficiency?*

- **Chunking strategies and their tradeoffs**: MoG specifically addresses the limitations of fixed-size chunking by introducing dynamic granularity selection. *Quick check: What are the precision-recall tradeoffs when choosing between fine-grained versus coarse-grained chunks?*

- **Graph-based information organization**: MoGG's core innovation relies on transforming linear text into graph structures to capture dispersed relationships. *Quick check: How does representing text as a graph change the nature of "context" compared to traditional linear document structures?*

## Architecture Onboarding

- **Component map**: Input: User query → Router: MLP that outputs granularity weights → Retriever: BM25 with multiple granularity levels → Top-k selection → LLM prompt → Response
- **Critical path**: Query → Router → Granularity selection → Retriever (BM25) → Top-k selection → LLM prompt → Response
- **Design tradeoffs**: Multiple granularity levels increase storage requirements (5× embeddings) but improve retrieval quality; soft labels provide training flexibility but introduce approximation error; graph preprocessing adds computational overhead but enables better cross-document retrieval
- **Failure signatures**: Router consistently predicts uniform weights across all granularity levels (indicates training failure); MoG performs worse than fixed granularity baseline (indicates poor router design or training); Execution time increases dramatically with granularity levels (indicates inefficient implementation)
- **First 3 experiments**: 1) Implement basic MoG with 3 granularity levels and test on a single dataset to verify router training and basic functionality; 2) Add soft label training and compare performance against ground truth-based training to validate the soft label approach; 3) Implement MoGG with graph preprocessing on a small corpus and test cross-document retrieval capability

## Open Questions the Paper Calls Out

### Open Question 1
How can the optimal granularity levels for different knowledge sources be automatically determined, rather than manually assigned? The paper states: "MoG(G)'s candidate granularity levels are manually assigned. It could be more efficient if an algorithm automatically set these granularity levels to avoid excessive grid-searching for parameter optimization."

### Open Question 2
How does incorporating additional information, such as query type or expected response length, into the router affect the performance of MoG? The paper mentions: "Incorporating more information (like query type or expected response length) into the router can potentially improve the results."

### Open Question 3
How does MoG perform in domains other than medicine, which has lower knowledge dependencies and higher error tolerances? The paper states: "We posit that significant improvements demonstrated by the tests on this knowledge-intensive field suggest MoG's potential effectiveness in other domains with lower knowledge dependencies and higher error tolerances."

## Limitations

- The router's learned patterns may not generalize well across different domains beyond the medical QA setting
- The quality of soft labels depends heavily on the RoBERTa similarity model's effectiveness in identifying truly relevant snippets
- The MoGG extension requires thorough analysis of how edge creation thresholds impact retrieval quality and computational efficiency

## Confidence

- **High Confidence**: The core premise that different knowledge sources benefit from different chunk sizes is well-supported by problem formulation and baseline comparison results
- **Medium Confidence**: The soft label training approach and its effectiveness in guiding the router without top-k selection during training
- **Low Confidence**: The MoGG extension's practical benefits beyond the specific medical QA domain

## Next Checks

1. **Cross-Domain Router Validation**: Test the trained router on a non-medical corpus (e.g., legal or technical documentation) to assess whether the learned patterns generalize or if the router requires domain-specific retraining.

2. **Granularity-Level Ablation Study**: Conduct a detailed analysis comparing retrieval precision/recall at each granularity level individually versus the MoG approach to quantify exactly how much performance gain comes from dynamic selection versus simply having more retrieval options.

3. **Graph Construction Sensitivity Analysis**: Systematically vary the edge creation threshold (Tgraph) and measure its impact on retrieval quality and computational overhead to identify optimal graph density for different corpus characteristics.