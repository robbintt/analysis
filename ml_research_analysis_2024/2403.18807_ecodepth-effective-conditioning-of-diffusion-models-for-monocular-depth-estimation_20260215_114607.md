---
ver: rpa2
title: 'ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth Estimation'
arxiv_id: '2403.18807'
source_url: https://arxiv.org/abs/2403.18807
tags: []
core_contribution: The paper proposes a new single-image depth estimation (SIDE) model
  that uses a diffusion backbone conditioned on embeddings from a pre-trained Vision
  Transformer (ViT) to provide detailed contextual information. Instead of generating
  pseudo-captions and using their CLIP embeddings, the authors argue that using ViT
  embeddings directly captures more relevant information for depth estimation.
---

# ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth Estimation

## Quick Facts
- arXiv ID: 2403.18807
- Source URL: https://arxiv.org/abs/2403.18807
- Authors: Suraj Patni; Aradhye Agarwal; Chetan Arora
- Reference count: 40
- Key result: 14% improvement in absolute relative error on NYUv2 compared to previous best

## Executive Summary
This paper introduces ECoDepth, a novel monocular depth estimation model that leverages diffusion models conditioned on Vision Transformer (ViT) embeddings. The authors argue that ViT embeddings capture richer semantic context than pseudo-caption-based embeddings for depth estimation tasks. ECoDepth achieves state-of-the-art performance on NYUv2 and KITTI datasets while demonstrating superior zero-shot transfer capabilities to four additional datasets.

## Method Summary
ECoDepth uses a diffusion model as its backbone, conditioned on embeddings from a pre-trained ViT model. The approach replaces the common practice of generating pseudo-captions and using CLIP embeddings with direct ViT embeddings, which the authors claim contain more relevant semantic information. The model includes a CIDE module that transforms ViT logits into conditioning vectors, a latent diffusion process for depth generation, and a depth regressor for final output. Training is performed on NYUv2 for 25 epochs using AdamW optimizer.

## Key Results
- Achieves 14% improvement in absolute relative error compared to previous best on NYUv2 dataset
- Demonstrates state-of-the-art performance on KITTI dataset
- Shows superior zero-shot transfer with mean relative improvements of 20%, 23%, 81%, and 25% over NeWCRFs on Sun-RGBD, iBims1, DIODE, and HyperSim datasets respectively

## Why This Works (Mechanism)

### Mechanism 1
ViT embeddings capture more detailed semantic context than pseudo-caption-based embeddings for monocular depth estimation. The CIDE module uses class-wise probability vectors from ViT logits that retain information about small background elements and a broader range of objects, conditioning the diffusion model to produce more object-specific and spatially accurate depth predictions.

### Mechanism 2
Diffusion models conditioned on semantic embeddings improve depth map accuracy through multi-scale hierarchical feature processing. The diffusion backbone denoises latent depth representations while being conditioned on ViT-derived embeddings, with hierarchical decoder features concatenated and processed to generate the final depth map, allowing both global context and fine-grained detail to influence the output.

### Mechanism 3
Zero-shot transfer performance improves because ViT embeddings provide generalizable semantic priors that are not dataset-specific. By training only on NYUv2 but conditioning with ViT embeddings learned from large-scale image datasets, the model generalizes better to unseen environments without requiring multi-dataset training.

## Foundational Learning

- **Concept**: Diffusion models as generative processes
  - **Why needed here**: The backbone relies on denoising diffusion to progressively reconstruct depth maps from noise, conditioned on image embeddings
  - **Quick check question**: What is the role of the noise schedule βt in the forward diffusion process?

- **Concept**: Vision Transformer embeddings as semantic priors
  - **Why needed here**: ViT provides rich, class-wise probability vectors that encode scene content beyond object detection, crucial for guiding depth estimation
  - **Quick check question**: How does a ViT's logit vector differ from its final classification output?

- **Concept**: Latent diffusion and VQ-VAE
  - **Why needed here**: Using a VQ-VAE encoder to map images to latent space before diffusion improves training stability and scalability
  - **Quick check question**: Why is diffusion performed in latent space rather than pixel space in this architecture?

## Architecture Onboarding

- **Component map**: RGB Image → ViT Backbone → CIDE Module → Conditional Diffusion (UNet) → Upsampling Decoder → Depth Regressor → Output Depth Map
- **Critical path**: ViT → CIDE → Conditional Diffusion → Depth Regressor
- **Design tradeoffs**: ViT-base chosen over larger variants to balance embedding quality and computational cost; learnable embeddings in CIDE (dimension 100) tuned to capture scene semantics without overfitting; latent diffusion reduces memory usage but adds complexity in mapping between image and latent spaces
- **Failure signatures**: Poor depth accuracy (check conditioning vector quality and ViT embedding relevance); unstable training (verify noise schedule and diffusion step count); overfitting to NYUv2 (evaluate zero-shot transfer metrics early and often)
- **First 3 experiments**: 1) Train baseline diffusion model without conditioning; compare depth accuracy to verify conditioning impact; 2) Replace ViT with CLIP embeddings; compare performance to confirm ViT advantage; 3) Vary learnable embedding dimension (10, 50, 100, 200); identify saturation point and optimal size

## Open Questions the Paper Calls Out
None

## Limitations
- ViT embedding generalization mechanism is not fully explored or rigorously analyzed
- CIDE module implementation details are sparse, making it difficult to assess whether improvements are due to ViT embeddings or CIDE transformation
- Zero-shot transfer claims, particularly the 81% improvement on DIODE, may be influenced by dataset-specific factors without ablation studies isolating ViT conditioning impact

## Confidence
- **High Confidence**: Diffusion model architecture and training procedure are well-defined with reproducible NYUv2 and KITTI results
- **Medium Confidence**: Zero-shot transfer improvements are plausible but require independent validation, particularly for DIODE dataset
- **Low Confidence**: Superiority of ViT embeddings over pseudo-caption-based embeddings is asserted but not empirically proven through direct comparison

## Next Checks
1. Perform ablation study training ECoDepth variants using CLIP embeddings, raw ViT logits, and no conditioning to isolate CIDE module impact
2. Quantify correlation between ViT object predictions and depth values at object boundaries to validate proposed mechanism
3. Test whether freezing ViT and retraining CIDE on each target dataset improves zero-shot transfer performance to determine if current approach overfits to NYUv2 semantics