---
ver: rpa2
title: Activation Space Selectable Kolmogorov-Arnold Networks
arxiv_id: '2408.08338'
source_url: https://arxiv.org/abs/2408.08338
tags:
- activation
- function
- s-kan
- functions
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces S-KAN (Selectable Kolmogorov-Arnold Networks),
  which addresses the limitation of standard KANs that rely on a single activation
  function space, resulting in variable performance across tasks. The proposed method
  employs an adaptive strategy to select the optimal activation function for each
  node in the KAN from a pool of 18 candidate functions (including B-splines, Chebyshev
  polynomials, wavelets, etc.).
---

# Activation Space Selectable Kolmogorov-Arnold Networks

## Quick Facts
- arXiv ID: 2408.08338
- Source URL: https://arxiv.org/abs/2408.08338
- Reference count: 40
- Primary result: S-KAN achieves significantly lower mean squared errors than standard KANs and MLPs on function fitting tasks, and leads accuracy on image classification datasets

## Executive Summary
This paper introduces S-KAN (Selectable Kolmogorov-Arnold Networks), addressing the limitation of standard KANs that rely on a single activation function space with variable task-specific performance. The proposed method employs an adaptive strategy to select the optimal activation function for each node from a pool of 18 candidate functions. Through a three-step training process involving full training, selective training, and pruning, S-KAN achieves superior performance on both function fitting tasks and image classification benchmarks compared to baseline KANs and MLPs with equivalent parameter counts.

## Method Summary
The authors propose a three-step training methodology for S-KAN. First, all 18 candidate activation functions (including B-splines, Chebyshev polynomials, and wavelets) are fully trained to establish baseline performance. Second, a selective training phase optimizes the weights while allowing activation function switching based on performance. Finally, a pruning step eliminates underperforming activation functions, retaining only those that contribute most effectively to the task. This approach enables each node in the KAN to utilize the most suitable activation function from the candidate pool, rather than being constrained to a single function type. The method is extended to S-ConvKAN for image classification by incorporating the same activation function selection mechanism into convolutional layers.

## Key Results
- S-KAN achieves mean squared errors significantly lower than baseline KANs and MLPs on seven function fitting tasks (e.g., 0.0023 vs. 0.4307 for MLP on certain functions)
- S-ConvKAN achieves leading accuracy on four image classification datasets (MNIST, Fashion MNIST, CIFAR-10, CIFAR-100) compared to comparable models
- The method demonstrates consistent performance improvements across diverse tasks while maintaining comparable parameter counts to baselines

## Why This Works (Mechanism)
The key innovation addresses the fundamental limitation of standard KANs where a single activation function must perform across all nodes and tasks, leading to suboptimal performance on certain function families. By allowing each node to select its optimal activation function from a diverse pool, S-KAN can better approximate complex function landscapes and adapt to task-specific characteristics. The three-step training process ensures thorough exploration of the activation space while the pruning mechanism prevents overfitting and maintains computational efficiency by eliminating redundant functions.

## Foundational Learning

1. **Activation Function Selection in Neural Networks**
   - *Why needed*: Standard KANs use a single activation function across all nodes, limiting adaptability to different function landscapes
   - *Quick check*: Verify that the candidate pool includes diverse function families (polynomials, wavelets, splines) that can approximate different function characteristics

2. **Three-Step Training Methodology**
   - *Why needed*: Sequential training phases allow for comprehensive activation function evaluation, weight optimization, and pruning without catastrophic forgetting
   - *Quick check*: Confirm that full training precedes selective training and that pruning occurs only after both phases complete

3. **Kolmogorov-Arnold Network Architecture**
   - *Why needed*: KANs replace fixed weight matrices with learnable univariate functions, enabling more flexible function approximation
   - *Quick check*: Understand how S-KAN modifies the standard KAN by adding activation function selection to each univariate function node

## Architecture Onboarding

**Component Map**: Input -> S-KAN Layer (with activation function selection) -> S-KAN Layer -> ... -> Output

**Critical Path**: Input features flow through multiple S-KAN layers where each node independently selects its optimal activation function, with gradients flowing through both the function selection mechanism and weight parameters during backpropagation.

**Design Tradeoffs**: The primary tradeoff involves increased computational complexity during training (due to evaluating 18 activation functions per node) versus improved task-specific performance and potentially reduced model size through pruning. The method sacrifices training efficiency for superior function approximation capabilities.

**Failure Signatures**: Poor performance may indicate inadequate candidate pool diversity, insufficient training iterations in the selective phase, or overly aggressive pruning that removes potentially useful activation functions. Convergence issues could arise from the complex optimization landscape created by the function selection mechanism.

**First Experiments**:
1. Implement the three-step training process on a simple univariate function (e.g., sin(x)) to verify activation function selection behavior
2. Compare S-KAN performance against standard KAN with a single activation function on a benchmark function fitting task
3. Analyze the distribution of selected activation functions across different node types and layers to understand selection patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Computational overhead from three-step training process, particularly the full training of all 18 activation functions
- Limited exploration of how results scale to larger models and more complex tasks
- Lack of ablation studies to isolate the contribution of activation function selection versus other architectural choices

## Confidence

| Claim | Confidence |
|-------|------------|
| Improved performance on function fitting tasks | Medium |
| Leading accuracy on image classification datasets | Low |
| Computational efficiency compared to baselines | Low |

## Next Checks

1. Conduct runtime and parameter efficiency analysis comparing S-KAN to standard KANs and MLPs across multiple scales, including wall-clock time and energy consumption metrics.

2. Perform ablation studies on the activation function pool composition to determine the impact of including versus excluding specific function families on different task types.

3. Implement statistical significance testing (e.g., paired t-tests) across multiple random seeds for all benchmark tasks to verify that performance improvements are not due to random variation.