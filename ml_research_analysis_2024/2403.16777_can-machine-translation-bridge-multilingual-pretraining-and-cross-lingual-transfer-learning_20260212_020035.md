---
ver: rpa2
title: Can Machine Translation Bridge Multilingual Pretraining and Cross-lingual Transfer
  Learning?
arxiv_id: '2403.16777'
source_url: https://arxiv.org/abs/2403.16777
tags:
- mbart
- cross-lingual
- layer
- language
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether machine translation (MT) objectives
  can improve cross-lingual transfer learning. We compare publicly available multilingual
  language models and MT systems, including models with continued pretraining (CP)
  on MT objectives.
---

# Can Machine Translation Bridge Multilingual Pretraining and Cross-lingual Transfer Learning?

## Quick Facts
- **arXiv ID:** 2403.16777
- **Source URL:** https://arxiv.org/abs/2403.16777
- **Reference count:** 0
- **Primary result:** MT as continued pretraining fails to enhance cross-lingual performance in multiple NLU tasks

## Executive Summary
This study systematically investigates whether machine translation (MT) objectives can improve cross-lingual transfer learning capabilities in multilingual language models. The authors compare various publicly available multilingual language models and MT systems, including models with continued pretraining on MT objectives. Through comprehensive experiments across multiple natural language understanding tasks, they find that MT-based continued pretraining does not enhance cross-lingual performance. Representation similarity analysis reveals that MT-continued models maintain similar representations to their source models without developing more language-agnostic features, suggesting that explicit sentence-level alignment through MT objectives may be counterproductive for cross-lingual transfer pretraining.

## Method Summary
The study employs a comparative approach using publicly available multilingual language models and machine translation systems, with some models undergoing continued pretraining (CP) on MT objectives. The evaluation framework includes multiple natural language understanding tasks to assess cross-lingual transfer performance. The authors conduct representation similarity analysis to examine whether CP models develop more language-agnostic features compared to their source models. Additionally, they analyze weight matrices to understand the impact of MT objectives on model representations, specifically examining singular values to assess output separability changes.

## Key Results
- MT as continued pretraining fails to enhance cross-lingual performance across multiple NLU tasks
- CP models maintain similar representations to source models without developing more language-agnostic features
- MT objectives lead to larger singular values in weight matrices, indicating increased output separability beneficial for MT but potentially detrimental for cross-lingual transfer

## Why This Works (Mechanism)
The study reveals that MT objectives create representations optimized for translation quality rather than cross-lingual transfer. When models are pretrained on MT tasks, they learn to map sentences between languages with high fidelity, which requires maintaining language-specific features rather than abstracting them away. This explicit alignment at the sentence level creates representations that are well-suited for translation but not for cross-lingual understanding tasks where language-agnostic features are more valuable. The increased singular values in weight matrices suggest that MT pretraining makes the output space more separable, which benefits translation accuracy but may hinder the model's ability to transfer knowledge across languages in a unified representation space.

## Foundational Learning

**Cross-lingual transfer learning** - The ability of models trained on one language to perform well on tasks in other languages. *Why needed:* Forms the core evaluation target of the study. *Quick check:* Models should show comparable performance across multiple languages when properly aligned.

**Multilingual language models** - Models trained on multiple languages simultaneously, typically using shared vocabulary and representation space. *Why needed:* The foundation for any cross-lingual transfer capability. *Quick check:* Should handle multiple languages with shared parameters.

**Continued pretraining** - Further training of pretrained models on specific objectives or domains. *Why needed:* The experimental manipulation being tested. *Quick check:* Should improve performance on the continued pretraining objective.

**Representation similarity analysis** - Techniques for comparing how different models encode information in their hidden states. *Why needed:* To determine if MT pretraining changes the fundamental nature of representations. *Quick check:* Similar representations should show high correlation across model layers.

## Architecture Onboarding

**Component map:** Multilingual base model -> MT objective pretraining -> NLU task evaluation -> Representation analysis -> Weight matrix analysis

**Critical path:** The most important pathway is Multilingual base model → MT objective pretraining → NLU task evaluation, as this determines whether MT pretraining actually helps cross-lingual transfer.

**Design tradeoffs:** The study implicitly trades off translation quality (improved by MT objectives) against cross-lingual transfer capability (decreased by MT objectives). The design favors explicit alignment through MT over implicit alignment through multilingual pretraining.

**Failure signatures:** If MT pretraining were successful, we would expect to see improved cross-lingual transfer performance alongside improved translation quality. The absence of transfer improvements despite maintained translation capabilities indicates the approach's limitations.

**First experiments:** 1) Compare base multilingual model performance across languages before any MT pretraining, 2) Evaluate MT-continued models on the same translation tasks to confirm MT capability is maintained, 3) Test cross-lingual transfer on low-resource language pairs to check for potential benefits not visible in high-resource settings.

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis focuses primarily on sentence-level alignment, leaving open questions about alternative alignment strategies at token or subword level
- Fixed set of languages and tasks may not generalize to all cross-lingual scenarios, particularly low-resource language pairs
- Interpretation of singular value analysis as "overfitting to MT" is plausible but not definitively proven

## Confidence
- **Primary conclusion (MT pretraining fails for cross-lingual transfer):** High
- **Claim (explicit sentence-level alignment is counterproductive):** Medium
- **Claim (increased singular values indicate overfitting):** Low

## Next Checks
1. Test alternative alignment strategies at finer granularity (token/subword level) to determine if explicit alignment can be beneficial when applied differently than sentence-level MT objectives
2. Expand experiments to include low-resource language pairs to assess whether findings generalize across the full spectrum of language availability and typological diversity
3. Conduct ablation studies on the weight matrix changes to distinguish whether increased singular values represent overfitting or a different form of cross-lingual representation that may benefit specific task types not covered in this study