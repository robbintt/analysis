---
ver: rpa2
title: 'Aligning Embeddings and Geometric Random Graphs: Informational Results and
  Computational Approaches for the Procrustes-Wasserstein Problem'
arxiv_id: '2405.14532'
source_url: https://arxiv.org/abs/2405.14532
tags:
- have
- which
- problem
- thus
- probability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the Procrustes-Wasserstein problem, which involves
  matching two high-dimensional point clouds in an unsupervised setting. The authors
  consider a planted model where one dataset is a noisy version of the other, up to
  an orthogonal transformation and a relabeling of data points.
---

# Aligning Embeddings and Geometric Random Graphs: Informational Results and Computational Approaches for the Procrustes-Wasserstein Problem

## Quick Facts
- **arXiv ID:** 2405.14532
- **Source URL:** https://arxiv.org/abs/2405.14532
- **Reference count:** 40
- **Key outcome:** Establishes information-theoretic bounds and proposes a Ping-Pong algorithm for unsupervised alignment of high-dimensional point clouds under planted permutation and orthogonal transformation.

## Executive Summary
This paper addresses the Procrustes-Wasserstein problem of matching two high-dimensional point clouds when one is a noisy version of the other up to an orthogonal transformation and relabeling. The authors establish information-theoretic thresholds for recovery in both high-dimensional (d ≫ log n) and low-dimensional (d ≪ log n) regimes. They propose a computationally efficient Ping-Pong algorithm that alternately estimates the orthogonal transformation and relabeling, initialized via a Frank-Wolfe convex relaxation. The method provably achieves exact or near-exact recovery under sufficient noise conditions and outperforms state-of-the-art approaches in experiments.

## Method Summary
The paper studies the Procrustes-Wasserstein problem where two point clouds X and Y ∈ R^d×n are related by a planted permutation π⋆ and orthogonal transformation Q⋆, with Y = Q⋆X(π⋆)⊤ + σZ where Z is Gaussian noise. The Ping-Pong algorithm alternately estimates Q⋆ via SVD when π⋆ is known, and estimates π⋆ via solving a linear assignment problem when Q⋆ is known. Initialization uses a Frank-Wolfe algorithm to solve a relaxed quadratic assignment problem, providing a good starting point for the alternating optimization. The method works in both high-dimensional regimes (d ≫ log n) where overlap and transport cost metrics are equivalent, and low-dimensional regimes (d ≪ log n) where transport cost enables recovery even when overlap-based methods fail.

## Key Results
- Establishes information-theoretic thresholds for exact recovery in both high-dimensional (d ≫ log n) and low-dimensional (d ≪ log n) regimes
- Proves sufficient conditions for the Ping-Pong algorithm to retrieve the planted signal after one alternating step
- Demonstrates superior performance compared to Grave et al. [2019] on synthetic datasets
- Shows that convex relaxation via Frank-Wolfe provides effective initialization for alternating optimization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The Ping-Pong algorithm achieves exact or near-exact recovery of the planted permutation and orthogonal transformation under sufficient noise conditions.
- **Mechanism:** Alternates between estimating the orthogonal transformation via SVD and estimating the permutation via solving a linear assignment problem. This alternation leverages the fact that when one variable is known, the other problem becomes tractable.
- **Core assumption:** The noise level σ is sufficiently small relative to n and d for each iteration to make progress.
- **Evidence anchors:**
  - [abstract] "We give sufficient conditions for the method to retrieve the planted signal after one single step."
  - [section 3.1] "Combining an initialization with convex relaxation, computed via Frank-Wolfe algorithm [Jaggi, 2013] and the alternate minimizations in P and Q yields the Ping-Pong algorithm."
  - [corpus] Weak - corpus papers focus on registration and geometric uncertainty, not the alternating optimization framework.
- **Break condition:** If σ is too large relative to n and d, the alternating updates may not converge to the planted signal.

### Mechanism 2
- **Claim:** The information-theoretic threshold for exact recovery depends critically on the dimensional regime (high d ≫ log n vs low d ≪ log n).
- **Mechanism:** In high dimensions, the equivalence between overlap and transport cost metrics allows simpler recovery conditions. In low dimensions, the transport cost metric enables recovery even when overlap-based methods fail.
- **Core assumption:** The dimensional regime is correctly identified and the appropriate metric is used.
- **Evidence anchors:**
  - [section 1.2] "For d ≫ log(n), the overlap and the transport cost metrics are equivalent: there exist numerical constants α, β > 0 such that w.h.p., for all permutation matrices π, π′, αc2(π, π′) ⩽ 2d(1 − ov(π, π′)) ⩽ βc2(π, π′)."
  - [section 2.2] "In the low-dimensional case when d ≪ log n, Theorem 2 below implies that if σ = o(d−1/2) then there exist estimators ˆπ, ˆQ that satisfy w.h.p."
  - [corpus] Weak - corpus papers focus on uncertainty quantification and geometric representations, not dimensional regime analysis.
- **Break condition:** If the dimensional regime is misclassified or the wrong metric is used, the recovery conditions may be violated.

### Mechanism 3
- **Claim:** The convex relaxation via Frank-Wolfe initialization provides a good starting point for the Ping-Pong algorithm.
- **Mechanism:** The relaxed quadratic assignment problem (relaxed QAP) is solved approximately using Frank-Wolfe, providing an initial estimate that is then refined through alternating optimization.
- **Core assumption:** The Frank-Wolfe algorithm converges to a solution close to the true planted permutation within the convex relaxation.
- **Evidence anchors:**
  - [section 3.1] "Estimating P ⋆ can be made via solving the QAP(5), that can be convexified into the relaxed quadratic assignment problem (relaxed QAP)... The estimate ˆPrelaxed gives a first estimate to then perform alternate minimizations in Q through an SVD – see (11) – and P through a LAP – see (10)."
  - [section 3.3] "The curve 'relaxed QAP via FW' is obtained by computing the relaxed QAP estimator with Frank-Wolfe algorithm with T = 1000 steps, enough for convergence."
  - [corpus] Weak - corpus papers don't discuss convex relaxations or Frank-Wolfe methods.
- **Break condition:** If the convex relaxation has a large gap from the true solution, the initialization may be too far from optimal to benefit from subsequent alternating optimization.

## Foundational Learning

- **Concept:** Gaussian random geometry and concentration inequalities
  - Why needed here: The proofs rely heavily on controlling deviations of random variables using concentration inequalities, particularly for chi-squared and Gaussian random variables.
  - Quick check question: What is the probability that a chi-squared random variable with k degrees of freedom exceeds its mean by more than 2√kt + 2t?

- **Concept:** Orthogonal Procrustes problem and its solution via SVD
  - Why needed here: When the permutation is known, recovering the orthogonal transformation reduces to the classical Procrustes problem with a closed-form SVD solution.
  - Quick check question: Given matrices X and Y, what is the closed-form solution for the orthogonal matrix Q that minimizes ||X - QY||_F?

- **Concept:** Linear assignment problem and Hungarian algorithm
  - Why needed here: When the orthogonal transformation is known, recovering the permutation reduces to solving a linear assignment problem.
  - Quick check question: What is the time complexity of the Hungarian algorithm for solving a linear assignment problem with n items?

## Architecture Onboarding

- **Component map:** Data generation → Frank-Wolfe initialization → Ping-Pong iterations → Evaluation
- **Critical path:** Frank-Wolfe → Ping-Pong iterations → Convergence check
- **Design tradeoffs:**
  - Computational complexity vs. recovery accuracy (Frank-Wolfe steps vs. alternating steps)
  - Dimensional regime handling (different metrics and thresholds for high vs. low d)
  - Initialization quality vs. number of alternating iterations needed
- **Failure signatures:**
  - Oscillations between permutation and orthogonal estimates
  - Slow convergence or stagnation at suboptimal solutions
  - Recovery performance degrading rapidly as noise σ increases
- **First 3 experiments:**
  1. **Dimensional regime test:** Fix n=100, vary d from 2 to 20, measure recovery performance for different noise levels σ to verify theoretical predictions.
  2. **Noise sensitivity test:** Fix d=10, n=100, vary σ from 0.1 to 1.0, measure how many Ping-Pong iterations are needed for recovery.
  3. **Initialization comparison:** Compare Frank-Wolfe initialization with random initialization, measuring the number of iterations to reach the same recovery quality.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What are the fundamental limits of the Procrustes-Wasserstein problem in high-dimensional regimes (d ≫ log n)?
- **Basis in paper:** [explicit] The paper establishes information-theoretic results in the high-dimensional regime (d ≫ log n) and shows that almost exact recovery of the planted signal is feasible under the loose assumption that the noise parameter σ → 0.
- **Why unresolved:** The paper provides a theoretical framework and proves sufficient conditions for recovery, but the exact threshold for exact recovery and the tightness of these bounds remain open questions.
- **What evidence would resolve it:** Experiments demonstrating the exact threshold for recovery in high-dimensional regimes, or tighter theoretical bounds that match or improve upon the current results.

### Open Question 2
- **Question:** How does the performance of the Ping-Pong algorithm compare to other state-of-the-art methods in low-dimensional regimes (d ≪ log n)?
- **Basis in paper:** [explicit] The paper proposes the Ping-Pong algorithm and provides experimental results comparing its performance to the state-of-the-art method of Grave et al. [2019] in various settings.
- **Why unresolved:** While the paper shows that the Ping-Pong algorithm outperforms the method of Grave et al. [2019] in the experiments conducted, a comprehensive comparison with other methods in the literature, especially in low-dimensional regimes, is lacking.
- **What evidence would resolve it:** Extensive experiments comparing the Ping-Pong algorithm with other state-of-the-art methods in low-dimensional regimes, or theoretical analysis demonstrating its superiority or limitations in these settings.

### Open Question 3
- **Question:** Can the information-theoretic bounds established in the paper be improved, and if so, how?
- **Basis in paper:** [inferred] The paper establishes information-theoretic results in both high and low-dimensional regimes, but the tightness of these bounds and the possibility of improvement are not explicitly discussed.
- **Why unresolved:** The paper focuses on establishing sufficient conditions for recovery, but does not explore the possibility of tighter bounds or the fundamental limits of the problem.
- **What evidence would resolve it:** Tighter information-theoretic bounds that match or improve upon the current results, or a proof that the established bounds are indeed tight.

## Limitations
- Theoretical analysis is primarily asymptotic with limited finite-sample guarantees
- Frank-Wolfe convex relaxation is only run for T=1 step in experiments, which may not be sufficient for convergence in practice
- Assumes i.i.d. Gaussian noise which may not hold in real-world applications where noise could be heteroscedastic or correlated

## Confidence

- **High confidence**: The dimensional regime analysis (high vs. low d) and the equivalence between overlap and transport cost metrics in high dimensions are well-supported by theoretical derivations.
- **Medium confidence**: The convergence properties of the Ping-Pong algorithm rely on specific noise-to-dimension ratios that may be difficult to verify in practice.
- **Low confidence**: The computational experiments are limited to synthetic data with specific parameter settings, making generalization to real-world scenarios uncertain.

## Next Checks
1. **Robustness to initialization**: Run the Ping-Pong algorithm with multiple random initializations to assess sensitivity to the Frank-Wolfe starting point and measure variance in recovery performance.
2. **Finite-sample performance**: Test the algorithm on finite datasets (n=50, 100, 200) to verify if theoretical bounds accurately predict practical performance.
3. **Real-world applicability**: Apply the method to actual geometric registration problems (e.g., 3D point cloud alignment) where the planted model assumptions may be violated to assess practical utility.