---
ver: rpa2
title: 'Meme-ingful Analysis: Enhanced Understanding of Cyberbullying in Memes Through
  Multimodal Explanations'
arxiv_id: '2401.09899'
source_url: https://arxiv.org/abs/2401.09899
tags:
- visual
- textual
- memes
- multimodal
- cyberbullying
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MultiBully-Ex, the first dataset for multimodal
  explanations of cyberbullying in code-mixed memes, featuring manually annotated
  textual and visual rationales. The authors propose a CLIP-based multimodal shared-private
  multitask architecture to jointly generate textual justifications and visual evidence
  for cyberbullying detection.
---

# Meme-ingful Analysis: Enhanced Understanding of Cyberbullying in Memes Through Multimodal Explanations

## Quick Facts
- arXiv ID: 2401.09899
- Source URL: https://arxiv.org/abs/2401.09899
- Reference count: 36
- Introduces MultiBully-Ex dataset with manually annotated multimodal explanations for cyberbullying detection in code-mixed memes

## Executive Summary
This paper addresses the challenge of understanding cyberbullying in memes by introducing MultiBully-Ex, the first dataset featuring both textual and visual explanations for multimodal cyberbullying detection. The authors propose a CLIP-based shared-private multitask architecture that jointly generates textual justifications and visual evidence for identifying cyberbullying content. The study demonstrates that incorporating multimodal explanations during training significantly improves model performance, with the best multitask approach achieving up to 63.07 ROUGE-L for textual explanations and 62.95 mIOU for visual explanations.

## Method Summary
The authors develop a CLIP-based multimodal architecture that employs both shared and private components for jointly learning cyberbullying detection and explanation generation tasks. The model processes both text and image modalities through separate encoders, then uses shared layers to capture common features while maintaining private layers for task-specific learning. The system is trained to simultaneously detect cyberbullying instances and generate corresponding textual rationales and visual evidence highlighting problematic regions in memes. This multitask approach leverages the interdependencies between detection and explanation tasks to improve overall performance.

## Key Results
- Best multitask model achieves 63.07 ROUGE-L for textual explanation generation
- Visual explanation task achieves 62.95 mIOU for highlighting cyberbullying regions
- Multitask learning outperforms single-task baselines in both explainability metrics
- CLIP-based architecture demonstrates effective cross-modal alignment for cyberbullying detection

## Why This Works (Mechanism)
The shared-private architecture enables the model to learn both common and task-specific representations across modalities. The shared components capture cross-modal relationships between text and images, while private components allow specialized learning for each explanation type. By jointly optimizing detection and explanation generation, the model develops richer internal representations that benefit both tasks simultaneously.

## Foundational Learning
- **Multimodal representation learning**: Why needed - memes require understanding both visual and textual content; Quick check - model correctly aligns text-image pairs
- **Code-mixed language processing**: Why needed - memes often combine multiple languages (Hindi-English); Quick check - model handles language switching appropriately
- **Visual explanation generation**: Why needed - identifying specific regions responsible for cyberbullying; Quick check - generated masks accurately highlight problematic content
- **Shared-private multitask learning**: Why needed - different explanation types require both shared and specialized knowledge; Quick check - ablation shows contribution of each component

## Architecture Onboarding

**Component Map**: Text Encoder -> Shared Layers -> Private Text Layers -> Explanation Generator; Image Encoder -> Shared Layers -> Private Image Layers -> Segmentation Module; Detection Head from Shared Layers

**Critical Path**: Text/Image input → Encoders → Shared layers → Detection head (for cyberbullying detection) AND Shared/Private layers → Explanation generators (for justifications)

**Design Tradeoffs**: The shared-private architecture balances between learning common multimodal features and maintaining task-specific capabilities. Alternative designs could use completely separate models for each task, but this would lose beneficial knowledge transfer.

**Failure Signatures**: Poor performance on either textual or visual explanations may indicate insufficient cross-modal alignment in shared layers, or inadequate task-specific learning in private components.

**First 3 Experiments**:
1. Train detection-only model to establish baseline performance
2. Train separate explanation models (text-only and image-only) to compare against multitask approach
3. Perform ablation study removing shared or private components to measure their individual contributions

## Open Questions the Paper Calls Out
The paper acknowledges several limitations including the dataset's focus on Hindi-English code-mixed memes, raising questions about performance on monolingual memes or memes in other language pairs. The study also notes that the visual explanation task remains significantly more challenging than textual generation, with lower performance metrics suggesting room for architectural improvements.

## Limitations
- Dataset limited to Hindi-English code-mixed memes, raising generalizability concerns
- Visual explanation task shows substantially lower performance than textual counterpart
- Modest absolute gains in metrics despite statistical significance
- Dataset size may be insufficient for robust training, particularly for image-based explanations

## Confidence
- **High Confidence**: Dataset creation methodology and annotation process are well-documented and follow standard practices
- **Medium Confidence**: Comparative analysis between single-task and multitask approaches shows consistent patterns
- **Medium Confidence**: CLIP-based architecture demonstrates reasonable performance for targeted task

## Next Checks
1. Evaluate model performance on monolingual memes and memes in different language pairs to assess cross-lingual generalization
2. Conduct ablation studies to determine contribution of shared vs. private layers to overall performance
3. Test model's robustness to adversarial examples and out-of-distribution memes to understand real-world applicability