---
ver: rpa2
title: '"They are uncultured": Unveiling Covert Harms and Social Threats in LLM Generated
  Conversations'
arxiv_id: '2405.05378'
source_url: https://arxiv.org/abs/2405.05378
tags: []
core_contribution: This study introduces the Covert Harms and Social Threats (CHAST)
  metrics, a set of seven indicators grounded in social science literature, to evaluate
  nuanced harms in LLM-generated conversations within recruitment contexts. By prompting
  eight LLMs to simulate hiring discussions involving race and caste identities, the
  research found that seven of the eight models produced content containing CHAST,
  with open-source models and GPT-3.5-Turbo generating significantly more harm in
  caste-based scenarios than race-based ones.
---

# "They are uncultured": Unveiling Covert Harms and Social Threats in LLM Generated Conversations

## Quick Facts
- arXiv ID: 2405.05378
- Source URL: https://arxiv.org/abs/2405.05378
- Authors: Preetam Prabhu, Srikar Dammu, Hayoung Jung, Anjali Singh, Monojit Choudhury, Tanushree Mitra
- Reference count: 40
- Key outcome: Introduces CHAST metrics framework and demonstrates that 7/8 tested LLMs generate covert harms in recruitment conversations, with open-source models showing more harm in caste vs race contexts.

## Executive Summary
This study introduces the Covert Harms and Social Threats (CHAST) metrics framework to evaluate nuanced harms in LLM-generated conversations, particularly in recruitment contexts involving race and caste identities. Through systematic prompting of eight different LLMs, the research reveals that most models produce content containing subtle forms of discrimination expressed in seemingly neutral language. The findings demonstrate that traditional toxicity detection methods fail to capture these covert threats, while a fine-tuned Vicuna-13b-16K model shows promise for scalable detection aligned with expert human judgments.

## Method Summary
The researchers prompted eight LLMs (GPT-3.5-Turbo, GPT-4-Turbo, Vicuna-7b-1.5, Vicuna-13b-1.5, Orca2-7b, MPT-7b-chat, Llama-2-7b-chat-hf, Llama2-13b-chat-hf) to generate 1,920 conversations across four occupations (Software Developer, Doctor, Nurse, Teacher) and two cultural concepts (race, caste). They developed the CHAST framework grounded in social science literature to detect seven types of covert harm. GPT-4-Turbo was used to label conversations, which were validated against expert annotations. A Vicuna-13b-16K model was fine-tuned on this labeled data using LoRA adapters (rank 32, learning rate 5e-05, batch size 8, 2 epochs) to create a scalable detection tool.

## Key Results
- Seven out of eight tested LLMs generated conversations containing CHAST metrics
- Open-source models and GPT-3.5-Turbo produced significantly more CHAST content in caste-based conversations compared to race-based ones
- Traditional toxicity baselines (Perspective API, Detoxify, ConvoKit) failed to detect these covert threats
- Fine-tuned Vicuna-13b-16K model aligned with expert human judgments for CHAST detection

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Vicuna-13b-16K can be aligned with human judgments for detecting CHAST metrics using fine-tuning
- **Mechanism**: Vicuna-13b-16K is fine-tuned on a dataset of LLM-generated conversations labeled by GPT-4-Turbo, which was validated against expert annotations
- **Core assumption**: The GPT-4-Turbo labels are sufficiently aligned with human judgments to serve as a reliable training signal
- **Evidence anchors**: Weak evidence; no direct citations of prior work validating GPT-4-Turbo's alignment with human judgments
- **Break condition**: If GPT-4-Turbo's labels diverge significantly from human judgments in new data, the fine-tuned model's performance will degrade

### Mechanism 2
- **Claim**: The CHAST metrics framework captures nuanced harms that traditional toxicity baselines miss
- **Mechanism**: CHAST metrics are grounded in social science literature (Social Identity Threat Theory, Intergroup Threat Theory) and capture subtle forms of harm expressed in seemingly neutral language
- **Core assumption**: Social science frameworks are valid and relevant for detecting harms in LLM-generated conversations
- **Evidence anchors**: No direct citations of prior work validating the CHAST framework against human judgments
- **Break condition**: If social science frameworks do not accurately capture the harms in LLM-generated conversations, the CHAST metrics will fail to detect them

### Mechanism 3
- **Claim**: GPT-3.5-Turbo and open-source models generate significantly more CHAST for caste-based conversations than race-based ones
- **Mechanism**: The models are prompted with neutral conversations about applicants with caste or race attributes, and the generated content is analyzed for CHAST metrics
- **Core assumption**: The conversation generation process accurately reflects the models' biases and stereotypes
- **Evidence anchors**: Weak evidence; no direct citations of prior work comparing CHAST scores for caste and race
- **Break condition**: If the conversation generation process does not accurately reflect the models' biases, the results will be misleading

## Foundational Learning

- **Concept**: Social Identity Threat Theory
  - Why needed here: CHAST metrics are grounded in this theory to capture various forms of harm and threat to identity groups
  - Quick check question: What are the three types of social identity threat identified in the theory?

- **Concept**: Intergroup Threat Theory
  - Why needed here: CHAST metrics incorporate metrics from this theory to capture realistic and symbolic threats between groups
  - Quick check question: What are the two types of intergroup threat identified in the theory?

- **Concept**: Fine-tuning LLMs for specific tasks
  - Why needed here: Vicuna-13b-16K is fine-tuned to detect CHAST metrics in LLM-generated conversations
  - Quick check question: What is the purpose of fine-tuning an LLM on a specific task?

## Architecture Onboarding

- **Component map**: LLM conversation generation -> CHAST metrics framework -> GPT-4-Turbo labeling -> Vicuna-13b-16K fine-tuning -> Scalable detection
- **Critical path**: Generate LLM conversations -> Label conversations with CHAST metrics -> Evaluate labels against human judgments -> Fine-tune Vicuna-13b-16K -> Deploy for scalable CHAST detection
- **Design tradeoffs**: Using GPT-4-Turbo for labeling is accurate but expensive and not open-source. Fine-tuning Vicuna-13b-16K is cheaper and open-source but may have lower performance
- **Failure signatures**: High disagreement between Vicuna-13b-16K and human judgments, low performance on test data, failure to detect CHAST in new conversations
- **First 3 experiments**:
  1. Generate LLM conversations with different prompts and evaluate for CHAST using GPT-4-Turbo
  2. Fine-tune Vicuna-13b-16K on labeled conversations and evaluate performance on test data
  3. Deploy Vicuna-13b-16K for scalable CHAST detection and monitor performance over time

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do CHAST scores vary across different occupations and identity groups beyond the four studied (software developer, doctor, nurse, teacher) and two caste/race groups (Brahmin/Dalit, White/Black)?
- Basis in paper: The study found that older occupations (doctor, nurse, teacher) showed higher CHAST scores in caste-based conversations compared to the newer occupation (software developer)
- Why unresolved: The study's scope was limited to a small set of occupations and identity groups
- What evidence would resolve it: Conducting a larger-scale study with a diverse set of occupations and identity groups, including those from other cultural contexts beyond race and caste

### Open Question 2
- Question: How do different prompt engineering techniques affect the detection of CHAST in LLM-generated conversations?
- Basis in paper: The paper explored various prompt engineering techniques, such as using different system roles, providing few-shot examples, and varying temperature settings
- Why unresolved: The study only tested a limited set of prompt engineering techniques and did not systematically investigate their impact on CHAST detection performance
- What evidence would resolve it: Conducting a comprehensive study that systematically varies different prompt engineering parameters and measures their impact on CHAST detection accuracy

### Open Question 3
- Question: How do different evaluation metrics and baselines compare in detecting CHAST in LLM-generated conversations?
- Basis in paper: The paper compared the performance of GPT-4-Turbo and a fine-tuned Vicuna-13b-16K model against popular toxicity detection baselines (Perspective API, Detoxify, ConvoKit)
- Why unresolved: The study did not explore a wider range of evaluation metrics and baselines that could potentially be more effective in detecting CHAST
- What evidence would resolve it: Evaluating the performance of the proposed CHAST metrics and detection models against a broader set of evaluation metrics and baselines

## Limitations

- The study relies on GPT-4-Turbo labels without direct validation against human annotators, creating uncertainty about the model's true evaluation capability
- The CHAST metrics framework lacks direct empirical validation against human judgments for this specific application
- The observed differences in harm generation between caste and race contexts may be influenced by the specific prompt design and cultural context of the research team

## Confidence

- **High confidence**: The empirical finding that seven out of eight tested LLMs generated conversations containing CHAST metrics across both race and caste contexts
- **Medium confidence**: The claim that traditional toxicity baselines fail to detect these covert harms
- **Low confidence**: The assertion that Vicuna-13b-16K can reliably detect CHAST metrics at scale

## Next Checks

1. Conduct a direct human evaluation study where expert annotators label a sample of LLM-generated conversations for CHAST metrics, then compare these labels against both GPT-4-Turbo and fine-tuned Vicuna-13b-16K outputs to establish ground truth alignment
2. Test the Vicuna-13b-16K CHAST detection model on conversations generated by LLMs not included in the original study (e.g., Claude, Gemini) to assess generalizability across model architectures
3. Replicate the conversation generation experiment with multiple prompt variations and cultural contexts beyond race and caste (e.g., religion, disability status) to determine if the observed harm patterns are robust across different identity dimensions and prompt formulations