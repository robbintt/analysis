---
ver: rpa2
title: 'Tokenization counts: the impact of tokenization on arithmetic in frontier
  LLMs'
arxiv_id: '2402.14903'
source_url: https://arxiv.org/abs/2402.14903
tags:
- tokenization
- figure
- digit
- when
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how number tokenization affects arithmetic performance
  in large language models. The authors find that GPT-3.5 and GPT-4 perform significantly
  better when using right-to-left tokenization (enforced by comma separation) versus
  the standard left-to-right tokenization, with accuracy improvements up to 20%.
---

# Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs

## Quick Facts
- arXiv ID: 2402.14903
- Source URL: https://arxiv.org/abs/2402.14903
- Reference count: 40
- Models perform significantly better with right-to-left tokenization vs left-to-right for arithmetic

## Executive Summary
This paper investigates how number tokenization direction affects arithmetic performance in large language models. The authors demonstrate that GPT-3.5 and GPT-4 show significantly improved accuracy (up to 20%) when problems are tokenized right-to-left (using comma separation) compared to standard left-to-right tokenization. They identify stereotyped error patterns suggesting systematic rather than approximate computation, particularly when answer length differs from addends. The models can convert between tokenization formats when prompted to repeat problems in their preferred format, recovering performance. Importantly, these tokenization-dependent effects persist across model versions but decrease with scale, suggesting larger models are better at overriding tokenization biases.

## Method Summary
The authors conducted experiments using OpenAI's GPT-3.5 and GPT-4 models across various arithmetic tasks including addition, multiplication, and multi-step reasoning problems. They systematically tested different tokenization formats - standard left-to-right (e.g., "123"), comma-separated right-to-left (e.g., "3,2,1"), and space-separated right-to-left (e.g., "3 2 1"). The study examined performance across different number ranges (1-3 digits and 4-6 digits), operation types, and problem configurations. They also tested model ability to convert between tokenization formats and assessed scale-dependent effects by comparing different model versions.

## Key Results
- Right-to-left tokenization (comma-separated) improves arithmetic accuracy by up to 20% compared to left-to-right
- Left-to-right tokenization produces stereotyped errors when answer length differs from addends
- Models can recover performance by converting problems to their preferred tokenization format
- Tokenization effects persist across model versions but decrease with scale

## Why This Works (Mechanism)
The study reveals that tokenization direction fundamentally affects how models process numerical information. Right-to-left tokenization aligns with the natural computational flow of arithmetic operations, where calculations start from the least significant digit. This alignment reduces computational overhead and prevents systematic errors that arise when the model's internal representation conflicts with the tokenization order. The persistence of effects across model versions suggests this is a fundamental architectural constraint rather than a training artifact.

## Foundational Learning
- **Tokenization schemes**: Why needed - Different schemes represent numbers in varying granularities; Quick check - Compare byte-pair vs wordpiece tokenization effects
- **Directional processing**: Why needed - Models process tokens sequentially; Quick check - Test performance with reversed token orders
- **Numerical representation**: Why needed - How numbers are encoded affects computation; Quick check - Compare integer vs floating-point tokenization
- **Systematic vs approximate computation**: Why needed - Distinguishes model reasoning patterns; Quick check - Analyze error distributions across difficulty levels
- **Scale-dependent effects**: Why needed - Shows architectural evolution; Quick check - Plot performance vs parameter count
- **Cross-format conversion**: Why needed - Reveals model flexibility; Quick check - Test bidirectional format translation accuracy

## Architecture Onboarding
**Component map**: Input tokenizer → Token sequence → Attention layers → Output projection → Token generation
**Critical path**: Tokenization → Sequential processing → Arithmetic computation → Output generation
**Design tradeoffs**: Right-to-left offers accuracy but requires preprocessing; left-to-right is standard but less effective for arithmetic
**Failure signatures**: Length mismatches between addends and answers, stereotyped error patterns
**First experiments**: 1) Test subtraction and division with different tokenizations, 2) Implement custom tokenizer preserving numerical meaning, 3) Analyze attention patterns during arithmetic operations

## Open Questions the Paper Calls Out
None

## Limitations
- Results are limited to OpenAI models, restricting generalizability to other architectures
- Only addition and multiplication were tested, leaving uncertainty about other operations
- Analysis is constrained to tested tokenization schemes without exploring alternatives

## Confidence
- High confidence in tokenization direction affecting performance for GPT-3.5 and GPT-4
- Medium confidence in the systematic error pattern interpretation
- Medium confidence in scale-dependent reduction of tokenization effects

## Next Checks
1. Test the tokenization effects across a broader range of models including open-source LLMs and different architectural families to assess generalizability
2. Extend experiments to include subtraction, division, and multi-step mathematical reasoning problems to determine if effects generalize beyond addition and multiplication
3. Implement and evaluate custom tokenization schemes that preserve numerical meaning while avoiding the identified directional biases