---
ver: rpa2
title: Parameter Efficient Diverse Paraphrase Generation Using Sequence-Level Knowledge
  Distillation
arxiv_id: '2404.12596'
source_url: https://arxiv.org/abs/2404.12596
tags:
- paraphrase
- score
- online
- generation
- diversity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a method to generate efficient paraphrase\
  \ models using sequence-level knowledge distillation from large language models\
  \ (LLMs). Three smaller models\u2014T5-small, FlanT5-small, and BART-base\u2014\
  were trained using paraphrase pairs generated by ChatGPT."
---

# Parameter Efficient Diverse Paraphrase Generation Using Sequence-Level Knowledge Distillation

## Quick Facts
- arXiv ID: 2404.12596
- Source URL: https://arxiv.org/abs/2404.12596
- Reference count: 40
- Smaller models achieve comparable paraphrase quality to 1000x larger LLM while being 1000x more parameter efficient

## Executive Summary
This paper presents a method to generate efficient paraphrase models using sequence-level knowledge distillation from large language models (LLMs). Three smaller models—T5-small, FlanT5-small, and BART-base—were trained using paraphrase pairs generated by ChatGPT. The resulting models achieved comparable paraphrase quality to the LLM teacher while being 1000 times smaller. Human evaluation showed only a 4% drop in performance relative to the teacher model, and the models exhibited both syntactic and lexical diversity. The approach offers a more efficient, cost-effective solution for paraphrase generation tasks while retaining high paraphrase quality.

## Method Summary
The authors use sequence-level knowledge distillation, where ChatGPT (gpt-3.5-turbo) generates diverse paraphrase pairs from multiple datasets (Quora, PAWS, MRPC, MSCOCO, Wiki Answer, Twitter URL). These pairs are then used to train smaller models—T5-small, FlanT5-small, and BART-base—using Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning. The distilled models maintain high paraphrase quality while being approximately 1000 times smaller than the teacher model, making them suitable for consumer hardware deployment.

## Key Results
- Smaller models (T5-small, FlanT5-small, BART-base) achieved comparable paraphrase quality to ChatGPT teacher with only 4% performance drop in human evaluation
- Models maintained both syntactic and lexical diversity while being 1000x smaller in parameter count
- LoRA enabled efficient fine-tuning on consumer hardware without sacrificing quality
- The approach proved effective across multiple diverse datasets including Quora, PAWS, and MSCOCO

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge distillation from a large teacher model to smaller student models preserves high paraphrase quality
- Mechanism: The teacher model (ChatGPT) generates diverse paraphrase pairs, which are used to train smaller models. The smaller models learn paraphrasing patterns and diversity from the teacher without needing to replicate its full complexity
- Core assumption: The teacher model can generate high-quality, diverse paraphrases that capture essential characteristics of good paraphrasing
- Evidence anchors:
  - [abstract] "These distilled models are capable of maintaining the quality of paraphrases generated by the LLM"
  - [section] "The distilled models were indeed capable of maintaining the quality and diversity of the output despite being drastically smaller"
  - [corpus] Corpus evidence through use of multiple datasets to create diverse training set

### Mechanism 2
- Claim: Sequence-level knowledge distillation is more effective than traditional fine-tuning for paraphrase generation
- Mechanism: Instead of fine-tuning on raw data, the teacher model generates synthetic paraphrase pairs, which are then used to train student models
- Core assumption: The synthetic paraphrase pairs generated by the teacher model are of high quality and diversity
- Evidence anchors:
  - [abstract] "applying a method referred to as sequence-level knowledge distillation"
  - [section] "The efficiency of sequence-level distillation lies in the fact that it only necessitates running the typically large teacher model once"
  - [corpus] The corpus evidence shows use of diverse datasets, indicating sequence-level approach can handle various paraphrasing tasks

### Mechanism 3
- Claim: Parameter-efficient fine-tuning (LoRA) allows smaller models to achieve performance comparable to teacher model
- Mechanism: LoRA reduces trainable parameters by introducing low-rank decomposition matrices, making it computationally feasible to fine-tune large models on paraphrase generation tasks
- Core assumption: The low-rank adaptation can capture essential knowledge from teacher model without requiring full fine-tuning
- Evidence anchors:
  - [abstract] "These models are approximately a thousand times smaller in terms of the number of parameters"
  - [section] "This method preserves the weights of the pre-trained model and integrates trainable rank decomposition matrices into every layer"
  - [corpus] Corpus evidence not directly related to LoRA but supports overall approach of using diverse datasets

## Foundational Learning

- Concept: Sequence-level knowledge distillation
  - Why needed here: Traditional fine-tuning requires large amounts of labeled data, which is often scarce for paraphrase generation
  - Quick check question: How does sequence-level knowledge distillation differ from traditional fine-tuning, and why is it more effective for paraphrase generation?

- Concept: Parameter-efficient fine-tuning (LoRA)
  - Why needed here: Fine-tuning large models on paraphrase generation tasks can be computationally expensive
  - Quick check question: What is the main advantage of using LoRA for fine-tuning large models, and how does it achieve this?

- Concept: Evaluation metrics for paraphrase quality
  - Why needed here: To assess effectiveness of distilled models, crucial to use variety of metrics that capture different aspects of paraphrase quality
  - Quick check question: Why is it important to use multiple evaluation metrics when assessing paraphrase generation models?

## Architecture Onboarding

- Component map:
  ChatGPT (gpt-3.5-turbo) -> Paraphrase Pair Generation -> Multiple Datasets (Quora, PAWS, MRPC, MSCOCO, Wiki Answer, Twitter URL) -> T5-small, FlanT5-small, BART-base (trained via LoRA) -> Evaluation (Human, LLM, Quantitative Metrics)

- Critical path:
  1. Generate paraphrase pairs using ChatGPT
  2. Filter and preprocess the generated data
  3. Train student models using LoRA on generated data
  4. Evaluate the models using human evaluation, LLM evaluation, and quantitative metrics

- Design tradeoffs:
  - Model size vs. performance: Smaller models are more efficient but may sacrifice some performance compared to teacher model
  - Training time vs. quality: Longer training times may lead to better performance but increase computational costs
  - Diversity vs. accuracy: Encouraging diversity in paraphrases may lead to some loss in accuracy or semantic similarity

- Failure signatures:
  - Low semantic similarity scores: Indicates paraphrases are not preserving meaning of source text
  - Poor lexical or syntactic diversity: Suggests models are not generating varied paraphrases
  - High grammatical error rates: Points to issues in model's ability to generate grammatically correct sentences

- First 3 experiments:
  1. Train T5-small model on subset of generated paraphrase pairs and evaluate performance using semantic similarity metrics
  2. Compare performance of T5-small and FlanT5-small models to assess impact of multilingual capabilities on paraphrase generation
  3. Evaluate effectiveness of different LoRA configurations (e.g., rank and alpha values) on performance of student models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model's performance on paraphrase generation tasks change when trained on datasets with different levels of noise?
- Basis in paper: [inferred] The paper mentions that datasets created using back-translation often contain significant noise including non-English source sentences
- Why unresolved: The paper does not provide detailed analysis of how model's performance is affected by noise level in training data
- What evidence would resolve it: An empirical study comparing model's performance on paraphrase generation tasks when trained on datasets with varying noise levels

### Open Question 2
- Question: Can the knowledge distillation technique used in this paper be extended to other natural language processing tasks beyond paraphrase generation?
- Basis in paper: [explicit] The paper discusses potential of knowledge distillation as strategy for leveraging power of LLMs in more efficient and accessible manner
- Why unresolved: The paper does not explore application of this technique to other NLP tasks
- What evidence would resolve it: A study applying knowledge distillation technique to other NLP tasks like machine translation or sentiment analysis

### Open Question 3
- Question: How does the model's performance on paraphrase generation tasks change when using different inference hyperparameters?
- Basis in paper: [inferred] The paper mentions that inference hyperparameters were set differently for each model to optimize performance
- Why unresolved: The paper does not provide comprehensive analysis of how different inference hyperparameters affect model's performance
- What evidence would resolve it: An empirical study comparing model's performance on paraphrase generation tasks when using different inference hyperparameters

## Limitations
- Quality of synthetic data relies heavily on automatic metrics and limited human evaluation
- Evaluation scope limited by small human evaluation sample size (100 sentences)
- Effectiveness may be specific to ChatGPT and particular student architecture combination
- Initial distillation phase still requires significant computational resources

## Confidence

**High confidence** - The claim that smaller models can achieve comparable paraphrase quality to larger models is well-supported by both quantitative metrics and human evaluation results. The 1000x parameter reduction is clearly demonstrated.

**Medium confidence** - The assertion that sequence-level knowledge distillation is more effective than traditional fine-tuning is supported but would benefit from direct comparison experiments. The evaluation framework is robust but the sample size for human evaluation is limited.

**Low confidence** - The claim about models exhibiting "syntactic and lexical diversity" relies heavily on automatic metrics without comprehensive analysis of types of diversity achieved or potential trade-offs between diversity and quality.

## Next Checks
1. **Replication with different teacher models** - Test the approach using alternative large language models (e.g., Claude, Llama) as teachers to verify that success isn't specific to ChatGPT's particular capabilities

2. **Extended human evaluation** - Conduct a larger-scale human evaluation (minimum 500 sentences) with multiple judges to validate quality assessments and examine inter-annotator agreement rates

3. **Long-term stability analysis** - Evaluate performance of distilled models over extended generation sessions to check for degradation in quality or diversity that might not appear in short evaluation sequences