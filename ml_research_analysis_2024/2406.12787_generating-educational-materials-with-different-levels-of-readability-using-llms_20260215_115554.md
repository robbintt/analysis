---
ver: rpa2
title: Generating Educational Materials with Different Levels of Readability using
  LLMs
arxiv_id: '2406.12787'
source_url: https://arxiv.org/abs/2406.12787
tags:
- text
- lexile
- readability
- llms
- educational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces the leveled-text generation task, aiming
  to rewrite educational materials to specific readability levels while preserving
  meaning. Three LLMs (GPT-3.5, LLaMA-2 70B, Mixtral 8x7B) were evaluated on 100 educational
  materials using zero-shot and few-shot prompting.
---

# Generating Educational Materials with Different Levels of Readability using LLMs

## Quick Facts
- arXiv ID: 2406.12787
- Source URL: https://arxiv.org/abs/2406.12787
- Authors: Chieh-Yang Huang; Jing Wei; Ting-Hao 'Kenneth' Huang
- Reference count: 35
- Primary result: LLaMA-2 70B best achieved target difficulty levels (MAE=172.9, match rate=22.22%), while GPT-3.5 best preserved original meaning (BERTScore=0.937)

## Executive Summary
This study introduces the leveled-text generation task, aiming to rewrite educational materials to specific readability levels while preserving meaning. Three LLMs (GPT-3.5, LLaMA-2 70B, Mixtral 8x7B) were evaluated on 100 educational materials using zero-shot and few-shot prompting. Few-shot prompting significantly improved performance in readability manipulation and information preservation. LLaMA-2 70B best achieved target difficulty levels (MAE=172.9, match rate=22.22%), while GPT-3.5 best preserved original meaning (BERTScore=0.937). Manual inspection revealed issues including misinformation introduction, inconsistent edit distribution, and a tendency to generate shorter texts regardless of target level. The findings emphasize the need for further research to ensure quality of generated educational content.

## Method Summary
The study employed a comparative evaluation of three LLMs (GPT-3.5, LLaMA-2 70B, Mixtral 8x7B) on a leveled-text generation task. Using a parallel dataset of 30K pairs of leveled texts, the researchers implemented zero-shot and few-shot prompting approaches to rewrite educational materials to target Lexile levels while preserving meaning. Generated texts were evaluated using both automated metrics (Lexile score accuracy, BERTScore, semantic similarity, normalized edit distance) and manual inspection of 200 texts. The methodology focused on assessing both the achievement of target difficulty levels and the preservation of original content meaning.

## Key Results
- LLaMA-2 70B achieved the best target difficulty levels (MAE=172.9, match rate=22.22%)
- GPT-3.5 demonstrated superior performance in preserving original meaning (BERTScore=0.937)
- Few-shot prompting significantly outperformed zero-shot prompting for both readability manipulation and information preservation
- Generated texts consistently exhibited a tendency to be shorter than originals regardless of target level

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot prompting significantly improves LLM performance on leveled-text generation tasks.
- Mechanism: By providing examples of text pairs with different Lexile levels, LLMs learn the relationship between text features (sentence length, word frequency) and readability levels, enabling them to generate text at the target level.
- Core assumption: LLMs have latent knowledge about text features and readability but need explicit examples to connect them to specific Lexile levels.
- Evidence anchors:
  - [abstract]: "few-shot prompting significantly improves performance in readability manipulation and information preservation"
  - [section]: "Our findings suggest that providing a few examples significantly improves performance, as few-shot learning outperforms zero-shot learning"
  - [corpus]: Weak - corpus neighbors are about educational content generation but don't directly address few-shot prompting effectiveness

### Mechanism 2
- Claim: LLaMA-2 70B is more effective at achieving target Lexile levels compared to GPT-3.5 and Mixtral 8x7B.
- Mechanism: LLaMA-2 70B's architecture and training may have better encoded knowledge about text complexity and linguistic features, allowing it to generate text that more closely matches the target Lexile level.
- Core assumption: LLaMA-2 70B's training data and model architecture are better suited for understanding and manipulating text complexity features.
- Evidence anchors:
  - [abstract]: "LLaMA-2 70B performs better in achieving the desired difficulty range"
  - [section]: "When comparing different LLMs, we found that LLaMA-2 70B performs best in adjusting readability"
  - [corpus]: Weak - corpus neighbors don't provide direct evidence comparing LLaMA-2 70B's performance to other models

### Mechanism 3
- Claim: GPT-3.5 is superior at preserving the original meaning of the text compared to LLaMA-2 70B and Mixtral 8x7B.
- Mechanism: GPT-3.5's training may have emphasized semantic understanding and context preservation, allowing it to maintain the original meaning better during text rewriting.
- Core assumption: GPT-3.5's training objectives and data composition prioritize semantic preservation over text complexity manipulation.
- Evidence anchors:
  - [abstract]: "GPT-3.5 excels in maintaining the meaning of the original text"
  - [section]: "GPT-3.5 demonstrates superior performance in preserving content and meaning"
  - [corpus]: Weak - corpus neighbors don't provide direct evidence about GPT-3.5's semantic preservation capabilities

## Foundational Learning

- Concept: Text readability and Lexile scale
  - Why needed here: Understanding how text complexity is measured and the factors that influence readability is crucial for developing effective leveled-text generation models.
  - Quick check question: What are the two main linguistic features that the Lexile Framework uses to assess text complexity for upper-level learners?

- Concept: Few-shot learning and prompt engineering
  - Why needed here: Effective use of few-shot learning techniques is essential for teaching LLMs the specific task of leveled-text generation and improving their performance.
  - Quick check question: How does providing examples in few-shot learning help LLMs understand the task of leveled-text generation?

- Concept: Evaluation metrics for text generation tasks
  - Why needed here: Proper evaluation of leveled-text generation models requires understanding and using appropriate metrics to assess both readability manipulation and information preservation.
  - Quick check question: What are the three metrics used in this study to evaluate whether the generated text matches the intended Lexile score?

## Architecture Onboarding

- Component map: Input processing -> Prompt generation -> LLM inference -> Output processing -> Manual inspection
- Critical path: Input → Prompt generation → LLM inference → Output processing → Evaluation
- Design tradeoffs:
  - Model selection: Balancing performance in readability manipulation vs. meaning preservation
  - Few-shot examples: Tradeoff between prompt length and learning effectiveness
  - Evaluation metrics: Balancing quantitative metrics with qualitative manual inspection
- Failure signatures:
  - Generated texts consistently shorter than source texts
  - Uneven distribution of edits within articles
  - Tendency to generate texts more complex than intended
- First 3 experiments:
  1. Compare zero-shot and few-shot performance using a small set of texts with known Lexile scores
  2. Test the effect of different numbers of few-shot examples (1-shot, 3-shot, 5-shot) on model performance
  3. Analyze the distribution of edits within generated texts to identify patterns in text manipulation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific factors cause LLMs to consistently generate shorter texts than the original regardless of whether the task is to simplify or complexify?
- Basis in paper: [explicit] The paper identifies a consistent bias where generated texts (both zero-shot and few-shot) are shorter than originals (350-500 words vs 825 words original), regardless of target difficulty direction.
- Why unresolved: The paper notes this bias appears in both zero-shot and few-shot scenarios, suggesting it's not simply due to shorter example prompts, but doesn't identify the underlying cause.
- What evidence would resolve it: Experiments testing different prompt formulations, length constraints, and training data analysis to determine if this is an inherent LLM bias or artifact of the prompting approach.

### Open Question 2
- Question: What causes the uneven distribution of edits within articles, where some paragraphs remain unchanged while others undergo significant revisions?
- Basis in paper: [explicit] The paper identifies this issue particularly in texts generated by LLaMA-2 70B, noting it appears "more frequently" in those outputs.
- Why unresolved: While the problem is identified and deemed unsuitable for educational materials, the paper doesn't investigate the underlying causes of this inconsistent editing pattern.
- What evidence would resolve it: Analysis of edit patterns across different sections/types of text, comparison of model attention mechanisms, and testing whether this varies by content type or complexity level.

### Open Question 3
- Question: How can we effectively integrate educational elements like learning objectives, vocabulary targets, and grammar points into leveled-text generation?
- Basis in paper: [explicit] The paper states "Integration of these educational elements into rewritten texts remains an unresolved challenge" and notes that determining appropriate learning objectives for different levels is "crucial yet challenging."
- Why unresolved: The paper acknowledges this as an important aspect of educational content generation but doesn't propose solutions or methods for incorporating these elements.
- What evidence would resolve it: Development and testing of frameworks that incorporate educational objectives into the generation process, evaluation of generated content against learning goals, and studies on the effectiveness of such integrated approaches.

## Limitations

- The study's evaluation relies on automated metrics that may not fully capture nuances of readability and meaning preservation
- The sample size of 100 educational materials may not adequately represent the diversity of educational content across different subject domains
- Manual inspection was limited in scope (200 texts) and may not capture systematic issues across the entire dataset

## Confidence

**High Confidence** in comparative LLM performance rankings, as these are based on consistent evaluation criteria and repeated measurements.

**Medium Confidence** in few-shot prompting effectiveness, as the study demonstrates clear improvements over zero-shot approaches but doesn't systematically explore optimal example counts.

**Low Confidence** in generalizability to real-world educational settings, as the study focuses on controlled conditions without addressing practical deployment challenges.

## Next Checks

1. **Cross-domain validation**: Test the same models and prompting strategies on educational content from different subject areas (science, history, literature) to assess whether performance differences persist across domains.

2. **Longitudinal meaning preservation**: Conduct a time-delayed evaluation where human experts assess the generated texts for factual accuracy and conceptual integrity after a period of time.

3. **Iterative refinement protocol**: Implement a human-in-the-loop system where initial LLM outputs are reviewed and used to generate improved few-shot examples, then measure whether this iterative approach produces consistently better results than static few-shot prompting across multiple refinement cycles.