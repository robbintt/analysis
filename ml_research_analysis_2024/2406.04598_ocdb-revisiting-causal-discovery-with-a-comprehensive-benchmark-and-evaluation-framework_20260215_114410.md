---
ver: rpa2
title: 'OCDB: Revisiting Causal Discovery with a Comprehensive Benchmark and Evaluation
  Framework'
arxiv_id: '2406.04598'
source_url: https://arxiv.org/abs/2406.04598
tags:
- causal
- discovery
- data
- structure
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OCDB, a comprehensive benchmark for causal
  discovery that addresses the lack of real-world data in current evaluations. It
  proposes two new metrics, CSD and CED, to measure differences in causal structures
  and effects, which are critical for interpretability in LLMs.
---

# OCDB: Revisiting Causal Discovery with a Comprehensive Benchmark and Evaluation Framework

## Quick Facts
- arXiv ID: 2406.04598
- Source URL: https://arxiv.org/abs/2406.04598
- Reference count: 40
- Primary result: OCDB introduces new metrics (CSD, CED) enabling fair comparison between DAGs and CPDAGs on 13 real-world datasets

## Executive Summary
This paper addresses the critical gap in causal discovery evaluation by introducing OCDB, a comprehensive benchmark that moves beyond synthetic data to include 13 diverse real-world datasets spanning static, time-series, and event sequence data. The authors identify that existing evaluation metrics fail to account for undirected edges in CPDAGs, making fair comparisons with DAGs impossible. To solve this, they propose two new metrics - Causal Structure Distance (CSD) and Causal Effect Distance (CED) - that naturally handle undirected edges and enable meaningful comparisons between different graph representations. Experiments reveal that existing causal discovery algorithms struggle with generalization on real-world data, highlighting the need for improved methods.

## Method Summary
OCDB provides a standardized framework for causal discovery evaluation using 13 real-world datasets. The framework includes a DataLoader for dataset management, a BaseModel parent class for algorithm implementation, and specific models including ICA-LiNGAM, DirectLiNGAM, NOTEARS, DAG-GNN, and others. Evaluation uses two novel metrics: CSD for measuring causal structure differences using adjacency matrix encoding, and CED for measuring causal effect differences by checking adjustment set validity through reachability matrices. The framework enables fair comparisons between DAGs and CPDAGs by naturally handling undirected edges through bidirectional encoding in adjacency matrices.

## Key Results
- CSD and CED metrics successfully enable fair comparisons between DAGs and CPDAGs on real-world data
- Existing algorithms show weak generalization across different datasets, with the best model varying by dataset
- Computational complexity of CED is roughly quadratic or cubic in number of variables, remaining negligible compared to discovery algorithms
- Real-world datasets reveal significant challenges not captured by synthetic data benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CSD captures causal structure differences with interpretability-aware edge encoding
- Mechanism: Uses adjacency matrix encoding where directed edges are 10/01 and undirected edges are 11, enabling Hamming distance to reflect severity of different error types
- Core assumption: The interpretability impact of different edge errors can be captured by binary distance in adjacency matrix representation
- Break condition: When the severity of different edge errors does not align with binary distance

### Mechanism 2
- Claim: CED distinguishes between structural and intervention distribution effects on causal effects
- Mechanism: Computes causal effect differences by checking if valid adjustment sets remain valid after structure changes, using reachability matrices to efficiently identify descendant relationships
- Core assumption: Valid adjustment sets can be efficiently identified using reachability matrices and the property that parents block all non-directed paths
- Break condition: When the parent set does not block all non-directed paths

### Mechanism 3
- Claim: OCDB enables fair comparison between DAGs and CPDAGs by using metrics that naturally handle undirected edges
- Mechanism: Both CSD and CED metrics are defined using adjacency matrices where undirected edges are represented as bidirectional edges, making them directly comparable across graph types
- Core assumption: Representing undirected edges as bidirectional edges in adjacency matrices preserves the essential causal information needed for comparison
- Break condition: When bidirectional representation loses critical information about edge directionality uncertainty

## Foundational Learning

- Concept: Causal structure vs causal effect distinction
  - Why needed here: The paper distinguishes between metrics for causal structure differences (CSD) and causal effect differences (CED), requiring understanding of what each represents
  - Quick check question: What is the difference between measuring differences in causal structures versus differences in causal effects?

- Concept: Markov equivalence classes and CPDAGs
  - Why needed here: The framework needs to compare DAGs and CPDAGs, which requires understanding that CPDAGs represent Markov equivalence classes
  - Quick check question: Why can't we simply compare DAGs and CPDAGs using standard DAG metrics?

- Concept: Adjustment sets in causal inference
  - Why needed here: CED computation relies on valid adjustment sets to estimate causal effects, requiring understanding of backdoor criterion
  - Quick check question: What makes a set of variables a valid adjustment set for estimating causal effects?

## Architecture Onboarding

- Component map: DataLoader -> BaseModel -> Specific models (ICA-LiNGAM, NOTEARS, etc.) -> _eval -> Metrics (CSD, CED, SHD-C, SID)

- Critical path: 1. Load dataset via DataLoader 2. Initialize model (inherits from BaseModel) 3. Call fit() to learn causal structure 4. Call _eval() to compute evaluation metrics 5. Compare results across different algorithms

- Design tradeoffs: Matrix-based metrics (CSD, CED) vs graph traversal approaches: matrix operations are faster but may use more memory; Comprehensive real data vs synthetic data: real data provides practical relevance but may have noise and confounders; Single value vs range outputs: single values (CSD, CED) are easier to compare but may lose some information

- Failure signatures: CSD/SID giving inconsistent results: indicates model performance varies significantly depending on whether DAG or CPDAG comparison is used; CED computation taking excessive time: suggests scalability issues with number of variables; Best model varying across datasets: indicates weak generalization and lack of robustness

- First 3 experiments: 1. Run ICA-LiNGAM on Sachs dataset and compare SHD-C vs CSD scores to verify fair DAG/CPDAG comparison 2. Test NOTEARS on multiple datasets to identify which datasets it performs best/worst on 3. Compute CED computation time scaling with number of variables to verify O(n²) or O(n³) complexity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed Causal Structure Distance (CSD) perform on larger causal graphs with hundreds of nodes compared to existing metrics?
- Basis in paper: The paper mentions that the computational complexity of CED is roughly quadratic or cubic in terms of the number of variables, and the actual time cost is negligible compared to causal discovery algorithms. However, it does not provide specific results for larger graphs.
- Why unresolved: The paper does not provide experimental results or analysis for larger causal graphs, focusing instead on datasets with fewer variables.
- What evidence would resolve it: Experimental results comparing CSD with existing metrics on larger causal graphs, along with computational time analysis.

### Open Question 2
- Question: Can the Causal Effect Distance (CED) be extended to handle continuous-time event sequences effectively?
- Basis in paper: The paper introduces CED for discrete-time event sequences but does not discuss its applicability to continuous-time event sequences.
- Why unresolved: The paper does not provide any analysis or experimental results for continuous-time event sequences.
- What evidence would resolve it: Implementation and evaluation of CED on continuous-time event sequences, along with a comparison to existing methods.

### Open Question 3
- Question: How does the performance of causal discovery algorithms on real-world datasets compare to their performance on synthetic datasets?
- Basis in paper: The paper states that "the fact that the best model varies across different datasets indicates a weak robustness of causal discovery algorithms" and mentions that "synthetic data is often generated based on certain assumptions or models, which may not fully reflect the complexity and diversity of the real world."
- Why unresolved: While the paper highlights the gap between synthetic and real-world data, it does not provide a detailed comparison of algorithm performance on both types of data.
- What evidence would resolve it: A comprehensive study comparing the performance of causal discovery algorithms on synthetic and real-world datasets, including quantitative metrics and qualitative analysis.

## Limitations
- Limited empirical validation of CSD and CED metrics beyond mathematical formulation
- Reliance on reachability matrices may not generalize well to complex real-world causal structures with cycles or latent confounders
- Benchmark focuses on a specific set of 13 datasets which may not fully represent the breadth of real-world causal discovery scenarios

## Confidence
- **High Confidence**: The framework's architecture for standardizing causal discovery evaluation (OCDB structure, DataLoader, BaseModel)
- **Medium Confidence**: The mathematical formulation of CSD and CED metrics - theoretically sound but requires more empirical validation
- **Medium Confidence**: The claim that existing algorithms struggle with real-world data generalization - supported by experimental results but could benefit from more systematic analysis

## Next Checks
1. **Metric Validation**: Implement CSD and CED metrics on synthetic datasets with known ground truth to verify they capture meaningful causal differences across edge types and adjustment set validity
2. **Scalability Analysis**: Measure CED computation time on datasets of increasing size (n variables) to empirically verify the claimed O(n²) or O(n³) complexity and identify practical limits
3. **Algorithm Robustness**: Systematically test each algorithm across all 13 datasets using k-fold cross-validation to quantify variance in performance and identify whether poor generalization is due to algorithm limitations or dataset-specific challenges