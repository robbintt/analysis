---
ver: rpa2
title: 'AI Insights: A Case Study on Utilizing ChatGPT Intelligence for Research Paper
  Analysis'
arxiv_id: '2403.03293'
source_url: https://arxiv.org/abs/2403.03293
tags:
- research
- papers
- gpt-4
- scope
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the effectiveness of GPT-3.5 and GPT-4 in
  automating research paper analysis for writing literature surveys on AI applications
  in breast cancer treatment. The models were used to categorize papers, detect their
  scope, and extract key information.
---

# AI Insights: A Case Study on Utilizing ChatGPT Intelligence for Research Paper Analysis

## Quick Facts
- arXiv ID: 2403.03293
- Source URL: https://arxiv.org/abs/2403.03293
- Reference count: 39
- Primary result: GPT-4 achieved 77.3% accuracy in identifying paper categories and correctly identified scope for 50% of papers in breast cancer treatment research

## Executive Summary
This study evaluated GPT-3.5 and GPT-4 for automating research paper analysis in the context of AI applications for breast cancer treatment. The researchers tested the models' ability to categorize papers, detect their scope, and extract key information from a corpus of 1,199 research papers. GPT-4 demonstrated superior performance compared to GPT-3.5, achieving 77.3% accuracy in category identification and 50% accuracy in scope detection. The study highlights both the potential and limitations of using large language models for scholarly work, including challenges with inconsistent responses and difficulties handling papers with multiple treatment focuses.

## Method Summary
The researchers collected 1,199 research papers from Google Scholar, PubMed, and Scopus related to AI applications in breast cancer treatment. They removed duplicates to create a unified corpus and used ChatGPT models with specific prompts to identify paper categories, detect scopes, and extract information. The models were evaluated against human-annotated ground truth data, with GPT-4 showing significantly better performance than GPT-3.5. Multiple prompt executions were performed for each paper, with majority responses selected to improve consistency.

## Key Results
- GPT-4 achieved 77.3% accuracy in identifying research paper categories compared to 65.15% for GPT-3.5
- GPT-4 correctly identified the scope of 50% of research papers, with partial scope identification for 22% of papers
- 67% of GPT-4's generated reasons were completely agreeable to domain experts, with 27% new words in the explanations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 achieves higher accuracy in research paper categorization when given both title and abstract, compared to GPT-3.5.
- Mechanism: The model leverages contextual information from both title and abstract to better infer the paper's objective category.
- Core assumption: The title and abstract contain sufficient information to distinguish between research categories.
- Evidence anchors:
  - [abstract] GPT-4 achieves 77.3% accuracy in identifying the research paper categories compared to 65.15% for GPT-3.5.
  - [section] GPT-4 model significantly outperformed the GPT-3.5 model in identifying research paper categories.
- Break condition: If abstracts are missing or uninformative, the model's accuracy drops significantly.

### Mechanism 2
- Claim: GPT-4 generates reasons for its categorization decisions that are largely agreeable to domain experts.
- Mechanism: The model uses its reasoning capabilities to provide justifications that align with expert interpretation.
- Core assumption: The model's reasoning process produces explanations that are interpretable and valid to human experts.
- Evidence anchors:
  - [abstract] 67% of the reasons given by the model were completely agreeable to the subject experts.
  - [section] GPT-4 produces completely agreeable reasoning most of the time (67.42%).
- Break condition: If the reasoning becomes too generic or relies heavily on copied text, expert agreement decreases.

### Mechanism 3
- Claim: GPT-4 can identify the scope of breast cancer treatment research papers by analyzing full text.
- Mechanism: The model reads entire research articles to classify them into specific treatment categories based on the taxonomy.
- Core assumption: Full-text analysis provides sufficient context to accurately determine the scope of the research.
- Evidence anchors:
  - [abstract] GPT-4 correctly identified the scope of 50% of the research papers.
  - [section] GPT-4 correctly identified the scope of 50% of the research papers, and for 22%, it identified a portion of the scope.
- Break condition: If papers cover multiple scopes ambiguously, the model struggles to provide accurate classification.

## Foundational Learning

- Concept: Taxonomy construction for breast cancer treatment
  - Why needed here: Provides the structured framework for categorizing and organizing research papers.
  - Quick check question: What are the three main oncological categories used in the taxonomy?

- Concept: Prompt engineering for specific tasks
  - Why needed here: Different prompts are required for category identification, scope detection, and information extraction.
  - Quick check question: How many options were provided in the scope detection prompt?

- Concept: Evaluation against ground truth data
  - Why needed here: Validates the model's performance by comparing its output to expert-annotated data.
  - Quick check question: What percentage of GPT-4's reasons were completely agreeable to subject experts?

## Architecture Onboarding

- Component map: Data collection from Google Scholar, PubMed, Scopus -> Duplicate removal -> GPT-4 for category identification -> GPT-4 for scope detection -> GPT-4 for information extraction
- Critical path:
  1. Collect research papers from multiple sources
  2. Remove duplicates to create unified corpus
  3. Identify relevant papers (AI in BCT)
  4. Determine scope of each relevant paper
  5. Extract key information for survey writing
- Design tradeoffs:
  - Using full-text vs. just title/abstract for analysis (accuracy vs. resource usage)
  - Multiple prompt executions vs. single execution (consistency vs. efficiency)
  - Manual vs. automated evaluation (accuracy vs. scalability)
- Failure signatures:
  - Inconsistent responses across multiple prompt executions
  - High rate of "Not sure" responses
  - Low agreement between model reasoning and expert judgment
- First 3 experiments:
  1. Test GPT-4's accuracy on a small sample of papers with both title/abstract and full-text
  2. Compare performance of different prompt structures for category identification
  3. Measure consistency of responses across multiple executions of the same prompt

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the accuracy of ChatGPT in identifying the scope of research papers be improved, particularly for papers with multiple treatment focuses?
- Basis in paper: [explicit] The paper states that GPT-4 correctly identified the scope of 50% of the research papers, and for 22%, it identified a portion of the scope annotated by subject experts.
- Why unresolved: The current model struggles with papers that have multiple scopes, either providing a narrower or broader range than what the experts identified.
- What evidence would resolve it: Improved performance metrics on a larger dataset, specifically showing higher accuracy in identifying papers with multiple scopes.

### Open Question 2
- Question: What are the limitations of using GPT models for scholarly work, and how can they be mitigated?
- Basis in paper: [explicit] The paper details several limitations, including noisy data retrieval, inconsistent responses, limited functionality, and the need for iterative prompt creation.
- Why unresolved: These limitations impact the efficiency and reliability of using GPT models for academic research and writing.
- What evidence would resolve it: Development of more robust data retrieval methods, improved model consistency, enhanced functionality, and refined prompt creation techniques.

### Open Question 3
- Question: How can the taxonomy of BCT be extended to cover all treatment types, and what impact would this have on the accuracy of scope detection?
- Basis in paper: [explicit] The paper mentions that the current taxonomy is ongoing and aims to extend it to cover all BCT types.
- Why unresolved: A more comprehensive taxonomy could potentially improve the model's ability to accurately identify the scope of research papers.
- What evidence would resolve it: Comparison of scope detection accuracy before and after extending the taxonomy, showing improved performance metrics.

## Limitations

- The study relies on a relatively small sample size of 20 research papers for evaluation, which may limit generalizability of the results.
- The methodology for handling inconsistent responses across multiple prompt executions is not fully specified, creating potential reproducibility issues.
- The study focuses specifically on AI applications in breast cancer treatment, which may not translate directly to other medical or non-medical domains.

## Confidence

- GPT-4 outperforms GPT-3.5 in research paper categorization: High confidence
- GPT-4 generates reasons largely agreeable to domain experts: Medium confidence
- GPT-4 can identify research paper scope with 50% accuracy: Low confidence

## Next Checks

1. Conduct a larger-scale evaluation using 100+ research papers across multiple medical domains to assess generalizability of GPT-4's performance.
2. Implement and test a standardized protocol for handling inconsistent responses across multiple prompt executions to establish reproducibility.
3. Perform a cross-domain validation study to evaluate whether GPT-4's effectiveness in breast cancer research extends to other medical specialties or general scientific literature.