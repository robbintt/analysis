---
ver: rpa2
title: 'REBUS: A Robust Evaluation Benchmark of Understanding Symbols'
arxiv_id: '2401.05604'
source_url: https://arxiv.org/abs/2401.05604
tags:
- rebus
- image
- wang
- puzzle
- chen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: REBUS introduces a new multimodal reasoning benchmark using 333
  original rebus puzzles across 13 categories. The benchmark requires models to integrate
  image recognition, string manipulation, hypothesis testing, and real-world knowledge
  to decode wordplay puzzles.
---

# REBUS: A Robust Evaluation Benchmark of Understanding Symbols

## Quick Facts
- arXiv ID: 2401.05604
- Source URL: https://arxiv.org/abs/2401.05604
- Authors: Andrew Gritsevskiy; Arjun Panickssery; Aaron Kirtland; Derik Kauffman; Hans Gundlach; Irina Gritsevskaya; Joe Cavanagh; Jonathan Chiang; Lydia La Roux; Michelle Hung
- Reference count: 40
- Primary result: GPT-4o achieves 42% accuracy on REBUS benchmark, significantly outperforming other models

## Executive Summary
REBUS is a new multimodal reasoning benchmark consisting of 333 original rebus puzzles across 13 categories. The benchmark requires models to integrate image recognition, string manipulation, hypothesis testing, and real-world knowledge to decode wordplay puzzles. Current models show significant limitations in multimodal reasoning, with GPT-4o achieving the highest accuracy at 42% but struggling with harder puzzles (7% accuracy). Open-source models perform much worse, with LLaVa-1.6-34B at only 2.7% accuracy. The benchmark also reveals poor calibration and lack of faithful reasoning in model outputs.

## Method Summary
The authors created REBUS by collecting 333 original rebus puzzles, each containing an image and textual description of the puzzle category. Puzzles were designed to test multimodal reasoning by combining visual elements with textual transformations requiring both perceptual and symbolic processing. The benchmark evaluates models using zero-shot inference, requiring them to generate answers in a specific format and optionally provide confidence scores and explanations. Performance is measured across different difficulty levels and puzzle categories, with particular attention to calibration and faithfulness of reasoning.

## Key Results
- GPT-4o achieves the highest accuracy at 42%, significantly outperforming other models
- Open-source models perform much worse, with LLaVa-1.6-34B at only 2.7% accuracy
- Models show poor calibration and often fail to provide faithful explanations, even when correct
- All models struggle with harder puzzles, achieving only 7% accuracy on challenging examples

## Why This Works (Mechanism)

### Mechanism 1
The benchmark effectively tests multimodal reasoning by requiring integration of image recognition, string manipulation, and world knowledge. REBUS puzzles combine visual elements with textual transformations that demand both perceptual and symbolic processing, creating a complex reasoning task that reveals limitations in current MLLMs.

### Mechanism 2
The benchmark reveals calibration issues and lack of faithfulness in model reasoning. Models often provide overconfident confidence estimates and generate plausible-sounding but incorrect explanations, even when arriving at correct answers.

### Mechanism 3
The benchmark exposes knowledge gaps in world knowledge and cultural references. Puzzles require specific real-world knowledge (celebrities, landmarks, cultural references) that current models struggle with, particularly open-source models.

## Foundational Learning

- **Concept**: Multimodal integration
  - **Why needed here**: REBUS requires combining visual perception with language understanding to solve puzzles
  - **Quick check question**: Can you describe how a rebus puzzle might require both image recognition and text manipulation to solve?

- **Concept**: Calibration and confidence estimation
  - **Why needed here**: Models show poor calibration, being overconfident in their answers even when wrong
  - **Quick check question**: What does it mean for a model to be "calibrated" in its confidence estimates?

- **Concept**: Faithful reasoning
  - **Why needed here**: Models often provide incorrect justifications even when giving correct answers
  - **Quick check question**: Why is it problematic if a model gives a correct answer but with wrong reasoning?

## Architecture Onboarding

- **Component map**: Vision encoder -> Text encoder -> Multimodal fusion -> Reasoning module -> Output layer
- **Critical path**: Image → Visual features → Multimodal fusion → Reasoning → Answer generation
- **Design tradeoffs**:
  - Fine-tuning vs. zero-shot: Benchmark uses zero-shot evaluation to test general capabilities
  - Model size vs. performance: GPT-4o outperforms much larger open-source models
  - Reasoning depth vs. computational cost: Multi-step reasoning is computationally expensive
- **Failure signatures**:
  - Overconfidence in incorrect answers
  - Faithless reasoning (incorrect explanations for correct answers)
  - Failure to recognize specific cultural references
  - Poor string manipulation despite good image recognition
- **First 3 experiments**:
  1. Test model calibration by asking for confidence scores on predictions
  2. Evaluate faithfulness by checking if explanations match the correct reasoning path
  3. Compare performance on puzzles requiring vs. not requiring world knowledge

## Open Questions the Paper Calls Out

None

## Limitations

- The benchmark's reliance on English-language rebus puzzles creates significant cultural and linguistic limitations
- The sample size of 333 puzzles may not be sufficient to fully characterize model performance across all possible rebus puzzles
- The benchmark's focus on zero-shot evaluation may underestimate the potential of fine-tuned models

## Confidence

- **High Confidence (8/10)**: GPT-4o's superior performance (42% accuracy) and the calibration analysis showing model overconfidence
- **Medium Confidence (6/10)**: Claims about the benchmark's effectiveness in testing multimodal reasoning
- **Low Confidence (4/10)**: Generalizability of findings to other types of multimodal reasoning tasks beyond rebus puzzles

## Next Checks

1. **Cross-linguistic validation**: Test whether the benchmark's insights about multimodal reasoning transfer to non-English rebus puzzles or similar visual-verbal reasoning tasks in other languages.

2. **Fine-tuning impact study**: Compare zero-shot performance against models fine-tuned on rebus puzzles to better understand the gap between general reasoning capabilities and specialized knowledge.

3. **Alternative interpretation tolerance**: Implement a system for accepting multiple valid answers to puzzles and assess how this affects model performance and evaluation metrics.