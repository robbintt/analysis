---
ver: rpa2
title: Black-Box Opinion Manipulation Attacks to Retrieval-Augmented Generation of
  Large Language Models
arxiv_id: '2407.13757'
source_url: https://arxiv.org/abs/2407.13757
tags:
- retrieval
- opinion
- manipulation
- adversarial
- black-box
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a black-box opinion manipulation attack framework
  targeting Retrieval-Augmented Generation (RAG) models. The approach involves training
  a surrogate model to mimic the black-box retriever's behavior, then using adversarial
  retrieval attacks to manipulate document rankings.
---

# Black-Box Opinion Manipulation Attacks to Retrieval-Augmented Generation of Large Language Models

## Quick Facts
- arXiv ID: 2407.13757
- Source URL: https://arxiv.org/abs/2407.13757
- Authors: Zhuo Chen; Jiawei Liu; Haotan Liu; Qikai Cheng; Fan Zhang; Wei Lu; Xiaozhong Liu
- Reference count: 30
- Primary result: Black-box opinion manipulation attacks on RAG systems achieve ASV of 0.42-0.67 and ASR up to 0.5 through surrogate modeling and adversarial retrieval attacks

## Executive Summary
This paper presents a black-box opinion manipulation attack framework targeting Retrieval-Augmented Generation (RAG) models. The approach involves training a surrogate model to mimic the black-box retriever's behavior, then using adversarial retrieval attacks to manipulate document rankings. Experiments on opinion datasets across four topics (Government, Education, Society, Health) demonstrate significant success in altering RAG-generated content opinions, with attack success rates reaching 0.5 and substantial stance variations observed in society and health topics.

## Method Summary
The attack framework operates in two stages: first, training a surrogate model using ranking data obtained from the black-box RAG system to approximate its retrieval behavior; second, employing the Pairwise Anchor-based Trigger (PAT) adversarial attack strategy on the surrogate model to generate adversarial text samples that manipulate document rankings when inserted into the retrieval corpus. The manipulated rankings influence the LLM's context selection, ultimately producing responses that align with the attacker's desired opinion.

## Key Results
- Average Stance Variation (ASV) of 0.42-0.67 across four controversial topics
- Attack Success Rate (ASR) reaching up to 0.5 in manipulating LLM responses
- Most effective manipulation observed in society and health topics
- Significant changes in Top-3 document rankings after adversarial insertion

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The surrogate model effectively mimics the black-box retriever's ranking behavior
- Mechanism: By training on ranking data obtained from the black-box RAG system, the surrogate model learns to approximate the relevance scoring function of the original retriever
- Core assumption: The black-box RAG system's ranking behavior is consistent and can be captured through supervised learning from observed outputs
- Evidence anchors:
  - [abstract]: "We train a surrogate model on the obtained retrieval ranking data to approximate the features and relevance preferences of the retriever in RAG"
  - [section]: "After obtaining the adversarial text padv, it is added to the candidate document with St. Then, the system of the black-box RAG model is queried, and the generated response is obtained"
  - [corpus]: Weak - The corpus only shows related papers but doesn't provide direct evidence of surrogate model effectiveness
- Break condition: If the black-box RAG system uses non-deterministic ranking or if the ranking behavior changes significantly over time

### Mechanism 2
- Claim: Adversarial retrieval attacks can manipulate document rankings in black-box systems through surrogate modeling
- Mechanism: The PAT (Pairwise Anchor-based Trigger) method generates adversarial text that increases the relevance score of target documents when processed by the surrogate model, and these modifications transfer to the black-box system
- Core assumption: The adversarial examples generated on the surrogate model will effectively transfer to the black-box retriever due to similar underlying ranking mechanisms
- Evidence anchors:
  - [abstract]: "By employing adversarial retrieval attack methods to the surrogate model, black-box transfer attacks on RAG are further realized"
  - [section]: "This paper employs the Pairwise Anchor-based Trigger (PAT) strategy for adversarial retrieval attacks, which is commonly used as a baseline in related research"
  - [corpus]: Weak - Related papers discuss similar attacks but don't validate transfer effectiveness specifically
- Break condition: If the black-box system has defense mechanisms against adversarial examples or uses fundamentally different ranking algorithms

### Mechanism 3
- Claim: LLMs in RAG systems can be guided to generate biased content through manipulated retrieval results
- Mechanism: By placing opinion-aligned documents at the top of retrieval results, the LLM incorporates this biased context into its generation process, producing responses that reflect the manipulated stance
- Core assumption: The LLM's generation is significantly influenced by the order and content of retrieved documents, and it follows the contextual bias present in top-ranked results
- Evidence anchors:
  - [abstract]: "Experiments conducted on opinion datasets across multiple topics show that the proposed attack strategy can significantly alter the opinion polarity of the content generated by RAG"
  - [section]: "Leveraging the strong capability of LLM for understanding and following instructions, we guide the LLM to generate responses that align with the expected opinion"
  - [corpus]: Weak - Related papers discuss RAG vulnerabilities but don't specifically validate LLM response manipulation through ranking
- Break condition: If the LLM has strong fact-checking capabilities or if it independently verifies information regardless of retrieval context

## Foundational Learning

- Concept: Black-box adversarial attacks
  - Why needed here: The attack must work without access to the target system's internal parameters or architecture
  - Quick check question: What is the fundamental difference between white-box and black-box adversarial attacks?

- Concept: Retrieval-augmented generation (RAG) architecture
  - Why needed here: Understanding how retrieval and generation components interact is crucial for identifying attack vectors
  - Quick check question: In a RAG system, what are the two main components that work together to produce responses?

- Concept: Surrogate modeling for attack transfer
  - Why needed here: Creating a white-box approximation of the black-box system enables the generation of transferable adversarial examples
  - Quick check question: Why is training a surrogate model a necessary step in black-box adversarial attacks?

## Architecture Onboarding

- Component map:
  Query input → Retriever (black-box) → Top-K documents → LLM (black-box) → Response output
  Attacker's surrogate model trained on observed retriever outputs
  Adversarial text generator (PAT) modifying target documents

- Critical path: Document ranking → Context selection → Response generation
  The attacker manipulates document ranking, which influences which documents the LLM uses for context, ultimately affecting the response

- Design tradeoffs:
  More sophisticated surrogate models may better approximate the black-box retriever but require more training data
  Aggressive adversarial modifications may be more effective but risk detection or reduce document quality
  Targeting different positions in the ranking (top-1 vs top-3) involves different attack strategies and effectiveness

- Failure signatures:
  No change in LLM response stance despite successful ranking manipulation
  Surrogate model failing to accurately predict black-box retriever rankings
  Adversarial text being filtered or rejected by the black-box system
  LLM ignoring top-ranked documents in favor of others

- First 3 experiments:
  1. Train surrogate model on black-box RAG ranking data and evaluate ranking similarity metrics (MRR, NDCG, RBO)
  2. Generate adversarial examples using PAT on surrogate model and test transfer effectiveness to black-box system
  3. Measure ASV (Average Stance Variation) by comparing LLM responses before and after document manipulation across different topics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed black-box opinion manipulation attack framework scale to larger and more complex RAG systems, including those with millions of documents in the corpus?
- Basis in paper: [explicit] The paper acknowledges that future work should "expand the scale of the experiments by including more open-source and commercial RAG systems to more comprehensively evaluate the reliability of viewpoint generation by RAG models."
- Why unresolved: The current experiments are conducted on a limited scale with specific datasets and RAG models. The scalability and effectiveness of the attack strategy in larger, more diverse, and real-world RAG systems are not yet fully explored.
- What evidence would resolve it: Conducting extensive experiments on a variety of large-scale RAG systems, including commercial ones, and evaluating the attack success rates and impact on opinion manipulation in these systems.

### Open Question 2
- Question: What are the most effective defense mechanisms against black-box opinion manipulation attacks on RAG systems, and how can they be implemented without significantly impacting the performance and usability of the system?
- Basis in paper: [explicit] The paper mentions that "future work should focus on developing more robust defense strategies" and suggests improving the robustness of retrieval algorithms, enhancing the reliability of generation models, and introducing multi-level input filtering mechanisms.
- Why unresolved: While the paper identifies potential defense strategies, it does not provide specific implementations or evaluate their effectiveness in mitigating the attack. The trade-off between defense effectiveness and system performance is also not addressed.
- What evidence would resolve it: Developing and testing specific defense mechanisms, such as robust retrieval algorithms, enhanced generation models, and input filtering techniques, and evaluating their effectiveness in mitigating the attack while maintaining system performance.

### Open Question 3
- Question: How does the opinion manipulation attack framework perform against different types of LLMs, including those with varying capabilities and training data, and how can the attack be adapted to target specific LLM characteristics?
- Basis in paper: [explicit] The paper compares the attack effectiveness on two LLMs (LLAMA3-8B and Qwen1.5-14B) and observes differences in their susceptibility to manipulation, suggesting that LLM characteristics may influence the attack success.
- Why unresolved: The current study only tests the attack on a limited set of LLMs. The generalizability of the attack across different LLM architectures, training data, and capabilities is not fully explored. Understanding how to adapt the attack to target specific LLM characteristics is also an open question.
- What evidence would resolve it: Conducting experiments on a diverse range of LLMs, including those with different architectures, training data, and capabilities, and analyzing the attack success rates and strategies for each type of LLM.

## Limitations

- Surrogate model fidelity uncertainty: The paper provides limited quantitative evidence of how well the surrogate model approximates the black-box retriever's ranking behavior, which is critical for attack effectiveness
- Transfer gap analysis missing: The framework theoretically enables black-box attacks, but lacks comprehensive analysis of how much attack effectiveness degrades when transferring from surrogate to target system
- Limited generalizability: Experiments focus on specific RAG configurations with particular retrievers and LLMs, leaving unclear how the attack performs against different architectural choices

## Confidence

**High Confidence**: The fundamental mechanism of using surrogate models for black-box attack transfer is well-established in the adversarial ML literature. The ASV metric for measuring opinion manipulation is clearly defined and methodologically sound.

**Medium Confidence**: The experimental results showing ASV scores of 0.42-0.67 and ASR up to 0.5 are presented with appropriate statistical analysis. However, the limited scope (4 topics, specific RAG configurations) reduces confidence in generalizability.

**Low Confidence**: The practical impact assessment is minimal. While the paper discusses theoretical implications for misinformation, it doesn't demonstrate real-world attack scenarios or user perception studies to validate actual harm.

## Next Checks

1. **Surrogate Model Validation**: Conduct comprehensive similarity analysis between surrogate and black-box retriever rankings using multiple metrics (MRR, NDCG, RBO) across diverse query distributions to quantify approximation quality.

2. **Cross-Architecture Transfer Testing**: Test the attack framework against different retriever types (e.g., dense retrievers like DPR vs sparse retrievers like BM25) and generation models to establish robustness boundaries and identify architectural vulnerabilities.

3. **User Perception Study**: Design a user study where participants evaluate LLM responses with and without manipulation to measure actual impact on opinion formation, establishing practical relevance beyond statistical metrics.