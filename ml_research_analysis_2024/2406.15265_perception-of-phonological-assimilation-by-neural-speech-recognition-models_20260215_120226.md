---
ver: rpa2
title: Perception of Phonological Assimilation by Neural Speech Recognition Models
arxiv_id: '2406.15265'
source_url: https://arxiv.org/abs/2406.15265
tags:
- context
- assimilation
- compensation
- phonological
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how neural ASR models process phonological
  assimilation by analyzing the Wav2Vec2 model's behavior on psycholinguistic stimuli.
  Using controlled experiments, the authors examine how the model compensates for
  place assimilation in various linguistic contexts.
---

# Perception of Phonological Assimilation by Neural Speech Recognition Models

## Quick Facts
- arXiv ID: 2406.15265
- Source URL: https://arxiv.org/abs/2406.15265
- Reference count: 40
- Neural ASR models compensate for phonological assimilation using minimal context cues, similar to humans but without semantic integration

## Executive Summary
This paper investigates how neural ASR models process phonological assimilation by analyzing the Wav2Vec2 model's behavior on psycholinguistic stimuli. Using controlled experiments, the authors examine how the model compensates for place assimilation in various linguistic contexts. Behavioral experiments show that the model is sensitive to phonological context, compensating more in viable assimilation contexts than unviable ones, similar to human behavior. However, the model does not integrate semantic context cues. Interpretability experiments reveal that the model shifts from surface to underlying interpretations in later layers, with causal interventions showing that minimal phonological cues (e.g., single phones) drive this compensation. These findings provide insights into the linguistic knowledge encoded by neural ASR models and highlight both similarities and differences with human phonological processing.

## Method Summary
The authors analyze Wav2Vec2 (facebook/wav2vec2-base-960h) finetuned on LibriSpeech using stimuli from psycholinguistic studies of place assimilation. They measure compensation rates for viable vs. unviable phonological contexts, train probing classifiers on TIMIT data to decode phonemes at different model layers, and conduct causal interventions to identify minimal context cues. The study compares model behavior to human psycholinguistic data and examines layer-wise shifts in phonological interpretation.

## Key Results
- Wav2Vec2 compensates for place assimilation in viable contexts (44% compensation) but less so in unviable contexts (40%), mirroring human sensitivity to phonological context
- The model shifts from surface to underlying interpretations in later layers, as shown by probing classifier probabilities
- Causal interventions reveal that minimal phonological context cues (single phones) are sufficient to drive compensation
- Unlike humans, Wav2Vec2 does not integrate semantic context, even when it would aid disambiguation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Wav2Vec2 shifts interpretation from surface to underlying form in later layers during place assimilation compensation
- Mechanism: The model first identifies the surface phones in early/middle layers, then integrates phonological context to infer the underlying phoneme in later layers
- Core assumption: The model learns to distinguish between surface and underlying phonemes through its pretraining and finetuning objectives
- Evidence anchors:
  - [abstract] "our probing experiments indicate that the model shifts its interpretation of assimilated sounds from their acoustic form to their underlying form in its final layers"
  - [section 4.1] "Early to middle layers seem to be identifying the surface phones that are pronounced... Later layers seem to have incorporated contextual information, so that the model can infer the phonemes underlying the surface sounds"

### Mechanism 2
- Claim: Minimal phonological context cues (single phones) drive compensation for place assimilation
- Mechanism: The model uses early MLP outputs to embed context phone information into the residual stream of the assimilated frame, which later layers use to shift interpretation
- Core assumption: Single context phones contain sufficient information for the model to determine whether assimilation is viable or not
- Evidence anchors:
  - [abstract] "causal intervention experiments suggest that the model relies on minimal phonological context cues to accomplish this shift"
  - [section 4.2] "we observe a drastic increase in the probability for the underlying consonant <n> when intervening on early MLP outputs (layers 1-5) at the context word position"

### Mechanism 3
- Claim: Lexical knowledge helps override non-words but not ambiguous words during assimilation compensation
- Mechanism: When assimilation creates a non-word, the model uses lexical knowledge to find the closest valid lexical candidate; when assimilation creates an ambiguous word, the model defaults to surface interpretation
- Core assumption: The model has learned lexical knowledge about valid English words during pretraining/finetuning
- Evidence anchors:
  - [section 3.1] "the model still demonstrates a 40 percent compensation rate in unviable assimilation contexts... These sources of information could be implicitly learned knowledge about valid English words"
  - [section 3.2] "the model compensates much more frequently if the assimilation transforms the lexical item into a non-word... rather than an alternative lexical candidate"

## Foundational Learning

- Concept: Phonological assimilation and compensation
  - Why needed here: The paper investigates how neural ASR models process place assimilation and compensate for it, so understanding these phonological concepts is fundamental
  - Quick check question: What is the difference between complete and incomplete place assimilation, and how does this affect compensation?

- Concept: Self-supervised learning in speech models
  - Why needed here: Wav2Vec2 is a self-supervised model, and understanding how it learns speech representations without explicit labels is crucial for interpreting the results
  - Quick check question: How does Wav2Vec2's pretraining objective (predicting masked frames) differ from supervised training, and what implications does this have for learned representations?

- Concept: Causal interventions in neural networks
  - Why needed here: The paper uses causal interventions to identify which model components are responsible for compensation, so understanding this methodology is essential
  - Quick check question: What is the difference between correlation-based analysis and causal intervention, and why is the latter more informative for understanding model mechanisms?

## Architecture Onboarding

- Component map: Raw waveform -> CNN layers (7 layers, 512 dimensions) -> Transformer layers (12 layers, 768 dimensions) -> Frame representations -> Character predictions -> Transcriptions
- Critical path: Raw waveform → CNN layers → Transformer layers → Frame representations → Character predictions → Transcriptions
- Design tradeoffs: The model prioritizes frame-level predictions over longer-distance context, which explains why it doesn't integrate semantic context but does integrate phonological context
- Failure signatures: Compensation rates are lower for ambiguous words than non-words; the model doesn't use semantic context even when it would be helpful
- First 3 experiments:
  1. Run behavioral experiments with different assimilation types to verify the phonological sensitivity pattern
  2. Perform layer-wise probing with different probing classifiers to confirm the surface-to-underlying shift mechanism
  3. Conduct causal interventions with different context positions to identify the minimal cues needed for compensation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do neural ASR models process complete assimilation that results in lexical ambiguity, where the assimilated form could be either a standard pronunciation or an assimilated form of another word?
- Basis in paper: [explicit] The paper discusses the example of "ru[m] picks you up" where ru[m] could be either "rum" or the assimilated form of "run", and shows that Wav2Vec2 tends to adopt the surface interpretation without considering semantic context.
- Why unresolved: The paper only examines Wav2Vec2's behavior on a limited set of stimuli and does not explore how other neural ASR models or the same model trained on different data might handle such ambiguous cases.
- What evidence would resolve it: Testing a variety of neural ASR models on a large dataset of ambiguous assimilation cases, analyzing their compensation rates and the influence of different types of semantic context.

### Open Question 2
- Question: What is the precise mechanism by which neural ASR models shift from surface to underlying interpretations of assimilated sounds in later layers, and which specific model components are involved?
- Basis in paper: [explicit] The paper uses interpretability experiments to show that Wav2Vec2 shifts its interpretation from surface to underlying forms in later layers, and identifies causal interventions that can flip the prediction from the surface to the underlying consonant.
- Why unresolved: The paper only examines a limited number of examples and does not provide a comprehensive analysis of the model components involved in this shift across different assimilation types and contexts.
- What evidence would resolve it: Conducting a systematic analysis of the model components involved in the shift for a wide range of assimilation types and contexts, potentially using automated methods to identify important model components.

### Open Question 3
- Question: How does the compensation behavior of neural ASR models for place assimilation compare to that of humans, and what are the key differences in the underlying mechanisms?
- Basis in paper: [explicit] The paper compares Wav2Vec2's compensation behavior to that of humans in psycholinguistic experiments, finding similarities in sensitivity to phonological context but differences in the influence of semantic context.
- Why unresolved: The paper only examines one neural ASR model and does not explore how other models or human listeners might differ in their compensation strategies, particularly for more challenging cases of assimilation.
- What evidence would resolve it: Conducting a comprehensive comparison of compensation behavior across multiple neural ASR models and human listeners, using a variety of assimilation types and contexts, and analyzing the underlying mechanisms using techniques such as neuroimaging and computational modeling.

## Limitations
- The exact acoustic realizations of experimental stimuli are not provided, requiring recreation that may introduce variability
- The study focuses on a single model architecture without comparing to alternative approaches or human listeners
- Probing classifier methodology may be sensitive to training data composition and classifier architecture choices

## Confidence
- High confidence: The model demonstrates sensitivity to phonological context in compensation patterns (section 3.1 findings)
- Medium confidence: The layer-wise shift from surface to underlying interpretations (section 4.1) is supported but depends on probing classifier reliability
- Medium confidence: The causal intervention findings showing minimal context cues drive compensation (section 4.2) are methodologically sound but limited to specific manipulation points

## Next Checks
1. Replicate the behavioral experiments with matched human listener data to directly compare model and human phonological processing
2. Test additional Wav2Vec2 variants (different sizes, pretraining durations) to determine if the observed patterns are architecture-specific
3. Conduct ablation studies removing lexical knowledge during fine-tuning to isolate the contribution of phonological vs. lexical processing to compensation