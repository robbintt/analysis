---
ver: rpa2
title: Accelerating Multilingual Language Model for Excessively Tokenized Languages
arxiv_id: '2401.10660'
source_url: https://arxiv.org/abs/2401.10660
tags:
- language
- multilingual
- mumo
- pre-trained
- head
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency in text generation for non-alphabetic
  languages due to excessive tokenization in multilingual language models. The proposed
  MuMo framework introduces a new language model head with a vocabulary set tailored
  to the target language and fine-tunes it while freezing other model parameters.
---

# Accelerating Multilingual Language Model for Excessively Tokenized Languages

## Quick Facts
- arXiv ID: 2401.10660
- Source URL: https://arxiv.org/abs/2401.10660
- Reference count: 26
- Primary result: 1.7x text generation speedup for Korean and Japanese with maintained monolingual performance

## Executive Summary
This paper addresses the inefficiency in text generation for non-alphabetic languages caused by excessive tokenization in multilingual language models. The authors propose MuMo, a framework that introduces a new language model head with a vocabulary tailored to the target language, fine-tuning only this component while freezing other model parameters. This approach reduces token fragmentation and accelerates generation. Experiments demonstrate a 1.7x speedup for Korean and Japanese while maintaining monolingual task performance.

## Method Summary
The MuMo framework addresses tokenization inefficiency by adding a language-specific model head with a vocabulary designed for the target language. During training, only this new head is fine-tuned while the rest of the model parameters remain frozen. This selective adaptation reduces the fragmentation of non-alphabetic languages into excessive tokens, thereby improving generation efficiency. The approach is evaluated on Korean and Japanese, showing significant speed improvements while maintaining performance on monolingual tasks.

## Key Results
- MuMo achieves 1.7x faster text generation for Korean and Japanese
- Monolingual task performance is maintained after applying MuMo
- Token fragmentation is reduced through language-specific vocabulary adaptation

## Why This Works (Mechanism)
The approach works by addressing the fundamental mismatch between general multilingual tokenizers and language-specific characteristics. Non-alphabetic languages like Korean and Japanese suffer from excessive tokenization when processed by general-purpose tokenizers designed for alphabetic languages. By introducing a language-specific vocabulary and adapting only the model head, MuMo reduces the number of tokens needed to represent text in these languages, directly translating to faster generation without retraining the entire model.

## Foundational Learning

**Tokenization** - Why needed: Understanding how text is split into discrete units for language models. Quick check: Compare token counts for same text across different languages.

**Subword tokenization** - Why needed: Most modern tokenizers use subword units to balance vocabulary size and coverage. Quick check: Examine BPE or WordPiece tokenization examples.

**Language-specific morphology** - Why needed: Different languages have different word formation rules affecting optimal tokenization. Quick check: Compare morphological complexity across Korean, Japanese, and English.

**Parameter-efficient fine-tuning** - Why needed: Techniques to adapt models without full retraining. Quick check: Review adapter-based methods and LoRA.

**Generation speed metrics** - Why needed: Quantifying efficiency improvements requires proper measurement. Quick check: Compare tokens/second across different models.

## Architecture Onboarding

**Component Map:** Input text -> Tokenizer -> Frozen multilingual model -> MuMo language-specific head -> Output generation

**Critical Path:** Text input flows through existing tokenizer, frozen multilingual encoder, and new language-specific decoder head for generation.

**Design Tradeoffs:** Freezes most parameters for efficiency but may limit cross-lingual transfer; balances speed gains against potential loss of multilingual capabilities.

**Failure Signatures:** Reduced performance on cross-lingual tasks; potential degradation in languages with similar characteristics to target languages.

**First Experiments:**
1. Compare token counts for identical text between original and MuMo tokenizers
2. Measure generation speed difference on held-out Korean/Japanese test sets
3. Evaluate monolingual task performance before and after MuMo application

## Open Questions the Paper Calls Out
None

## Limitations
- Results validated only on Korean and Japanese, limiting generalizability
- Cross-lingual task performance not evaluated
- Claim of "maintaining performance" lacks comprehensive task coverage and detailed metrics

## Confidence

**High confidence:** The experimental methodology and results for Korean and Japanese text generation speed improvements are well-documented and reproducible.

**Medium confidence:** The claim of maintaining monolingual task performance is supported but lacks comprehensive task coverage and detailed metrics.

**Low confidence:** The generalizability of the approach to other non-alphabetic languages and its impact on multilingual capabilities are not sufficiently explored.

## Next Checks

1. Conduct experiments on additional non-alphabetic languages (e.g., Thai, Vietnamese) to assess the framework's generalizability.

2. Evaluate the impact of MuMo on cross-lingual task performance to understand potential trade-offs in multilingual settings.

3. Perform a detailed analysis of token fragmentation reduction with quantitative metrics across different tokenization schemes.