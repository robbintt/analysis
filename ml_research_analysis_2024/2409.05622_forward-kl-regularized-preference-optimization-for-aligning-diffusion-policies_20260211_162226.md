---
ver: rpa2
title: Forward KL Regularized Preference Optimization for Aligning Diffusion Policies
arxiv_id: '2409.05622'
source_url: https://arxiv.org/abs/2409.05622
tags:
- diffusion
- policy
- learning
- preference
- fkpd
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes Forward KL Regularized Preference Optimization
  for aligning Diffusion policies (FKPD), a method to learn diffusion policies directly
  from preference data without relying on rewards or demonstrations. The method works
  in two stages: first, a basic diffusion policy is trained via behavior cloning from
  an offline dataset, then the policy is aligned to preferences via direct preference
  optimization with forward KL regularization.'
---

# Forward KL Regularized Preference Optimization for Aligning Diffusion Policies

## Quick Facts
- arXiv ID: 2409.05622
- Source URL: https://arxiv.org/abs/2409.05622
- Authors: Zhao Shan; Chenyou Fan; Shuang Qiu; Jiyuan Shi; Chenjia Bai
- Reference count: 40
- Primary result: Proposed FKPD method achieves state-of-the-art performance on MetaWorld and D4RL tasks by aligning diffusion policies with preferences using forward KL regularization

## Executive Summary
This paper introduces Forward KL Regularized Preference Optimization for aligning Diffusion policies (FKPD), a method that learns diffusion policies directly from preference data without requiring rewards or demonstrations. The approach operates in two stages: first training a basic diffusion policy via behavior cloning from an offline dataset, then aligning the policy to preferences using direct preference optimization with forward KL regularization. The forward KL regularization proves more effective than reverse KL in avoiding out-of-distribution issues during the alignment process. Experiments demonstrate FKPD achieves state-of-the-art performance on benchmark tasks, showing superior alignment with preferences compared to previous methods.

## Method Summary
FKPD addresses the challenge of aligning diffusion policies with human preferences by combining two key components. First, it trains an initial diffusion policy through behavior cloning on an offline dataset, providing a reasonable starting point for policy optimization. Second, it applies direct preference optimization with forward KL regularization to align the policy with preference data. The forward KL regularization is crucial as it prevents the policy from venturing too far from the initial policy distribution, thus avoiding out-of-distribution issues that commonly plague reinforcement learning approaches. This two-stage process allows the method to leverage existing offline data while still incorporating human preferences effectively.

## Key Results
- FKPD achieves state-of-the-art performance on MetaWorld and D4RL benchmark tasks
- Forward KL regularization outperforms reverse KL in maintaining policy stability during alignment
- The method demonstrates superior alignment with human preferences compared to previous approaches
- The two-stage training process effectively combines offline data with preference information

## Why This Works (Mechanism)
The method works by leveraging the mathematical properties of forward KL divergence to maintain policy stability during the alignment process. Forward KL regularization encourages the new policy to stay close to the initial policy distribution, which prevents the agent from exploring regions of state-action space that were not covered in the offline dataset. This is particularly important for diffusion policies, which can generate actions across the entire continuous space. By keeping the aligned policy within the support of the initial policy, the method avoids the out-of-distribution issues that typically arise when directly optimizing for preferences without such regularization.

## Foundational Learning
- **Diffusion Policies**: Continuous-time generative models for sequential decision-making that model action distributions conditioned on states. Needed because they provide a flexible framework for representing complex policies. Quick check: Verify the policy can generate valid action distributions for all states in the dataset.
- **Behavior Cloning**: Supervised learning approach that mimics expert demonstrations. Needed as the first stage to establish a reasonable initial policy. Quick check: Measure the initial policy's performance on validation tasks before preference alignment.
- **Direct Preference Optimization**: Method for learning from pairwise preference comparisons without explicit reward functions. Needed to align policies with human preferences directly. Quick check: Confirm the preference model can accurately predict pairwise preferences on held-out data.
- **KL Regularization**: Technique that constrains policy updates to stay close to a reference policy. Needed to prevent distributional shift during optimization. Quick check: Monitor KL divergence between successive policy updates to ensure stability.
- **Offline Reinforcement Learning**: Learning paradigm that uses pre-collected datasets without environment interaction. Needed to provide the initial dataset for behavior cloning. Quick check: Verify the offline dataset covers the relevant state-action space adequately.

## Architecture Onboarding

Component Map: Offline Dataset -> Behavior Cloning -> Initial Diffusion Policy -> Preference Optimization with Forward KL -> Aligned Diffusion Policy

Critical Path: The method follows a sequential two-stage pipeline where the quality of the initial policy directly impacts the effectiveness of the preference alignment stage. The forward KL regularization acts as a stability constraint throughout the optimization process.

Design Tradeoffs: The two-stage approach trades computational efficiency for stability and performance. While end-to-end methods might be faster, the staged approach provides better control over distributional shift and more reliable convergence.

Failure Signatures: Poor initial policy quality leads to limited preference alignment effectiveness. Excessive forward KL regularization prevents meaningful policy updates. Insufficient preference data results in noisy or unstable alignment.

First Experiments:
1. Test behavior cloning performance on a subset of the offline dataset to establish baseline capability
2. Evaluate forward vs. reverse KL regularization on a simple preference alignment task
3. Measure the impact of varying the KL regularization coefficient on policy stability and performance

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Limited ablation studies on the forward vs. reverse KL trade-off across different task complexities
- Unclear scalability to high-dimensional observation spaces and real-world robotics scenarios
- Computational overhead of the two-stage training process not fully characterized

## Confidence
- Core methodological claims: **High**
- Empirical performance claims: **Medium**
- Scalability and generalization claims: **Low**

## Next Checks
1. Conduct extensive ablation studies varying the KL regularization coefficient across a wider range of values and tasks to establish robustness
2. Test the method on high-dimensional visual input tasks to evaluate real-world applicability
3. Measure and report the computational overhead of the two-stage training process compared to end-to-end alternatives