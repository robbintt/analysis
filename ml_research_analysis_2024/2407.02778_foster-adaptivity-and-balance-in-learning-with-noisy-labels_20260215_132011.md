---
ver: rpa2
title: Foster Adaptivity and Balance in Learning with Noisy Labels
arxiv_id: '2407.02778'
source_url: https://arxiv.org/abs/2407.02778
tags: []
core_contribution: This paper addresses the problem of learning with noisy labels,
  a common issue in real-world scenarios that degrades the performance of deep neural
  networks. The authors propose SED, a method that deals with label noise in a self-adaptive
  and class-balanced manner.
---

# Foster Adaptivity and Balance in Learning with Noisy Labels

## Quick Facts
- arXiv ID: 2407.02778
- Source URL: https://arxiv.org/abs/2407.02778
- Reference count: 40
- One-line primary result: SED achieves 66.50% accuracy on CIFAR100N under 20% symmetric noise, outperforming state-of-the-art methods

## Executive Summary
This paper addresses the fundamental challenge of learning with noisy labels by proposing SED (Sample selection, Label correction, and Sample re-weighting), a method that integrates three complementary components to handle label noise in a self-adaptive and class-balanced manner. The authors demonstrate that existing methods often suffer from confirmation bias and class imbalance during label correction, leading to degraded performance on real-world datasets. SED tackles these issues by using dynamic class-specific thresholds for sample selection, confidence-based re-weighting for corrected labels, and consistency regularization on clean samples. Extensive experiments show SED's superiority over state-of-the-art methods on both synthetic and real-world datasets.

## Method Summary
SED is a self-adaptive and class-balanced framework for learning with noisy labels that operates through three integrated components: sample selection, label correction, and sample re-weighting. The method first identifies clean samples using dynamic global and local thresholds based on predicted probabilities with respect to given labels, where the global threshold adapts to overall learning progress and the local threshold normalizes class-wise predictions to ensure balance. Clean samples are then used to train a mean-teacher model that provides pseudo-labels for noisy samples, which are subsequently re-weighted according to their correction confidence using a truncated normal distribution. Additionally, consistency regularization is applied to clean samples to improve model generalization. The method uses a mean-teacher architecture with exponential moving average updates and trains only one network without requiring Mixup augmentation.

## Key Results
- On CIFAR100N with 20% symmetric noise, SED achieves 66.50% accuracy, outperforming state-of-the-art methods
- On real-world Web-Aircraft dataset, SED achieves 85.83% average test accuracy, surpassing existing approaches
- SED demonstrates robust performance across different noise types and rates, maintaining effectiveness even at high noise levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic class-specific thresholds improve clean sample selection precision while reducing class imbalance.
- Mechanism: SED uses both global and local thresholds derived from predicted probabilities w.r.t. given labels. The global threshold adapts to overall learning progress, while the local threshold normalizes class-wise predictions to ensure class balance.
- Core assumption: Sample selection based on predicted probability w.r.t. given label correlates with true label correctness and can be dynamically updated.
- Evidence anchors:
  - [abstract] "We propose to identify clean samples based on the predicted probability w.r.t. the given labels of input samples."
  - [section] "The global and local thresholds are dynamically updated during training."
  - [corpus] Weak evidence - no direct comparison of dynamic vs. static thresholds in neighbor papers.
- Break condition: If the network memorizes noisy labels early, predicted probabilities may no longer correlate with true correctness, leading to false positives in clean set.

### Mechanism 2
- Claim: Truncated normal re-weighting based on correction confidence mitigates confirmation bias from imbalanced label correction.
- Mechanism: Noisy samples corrected by mean-teacher model are re-weighted according to their confidence (predicted probability w.r.t. corrected label). Weights follow a truncated normal distribution whose parameters are updated per class.
- Core assumption: Correction confidence is a reliable proxy for label correction correctness and varies systematically across classes.
- Evidence anchors:
  - [abstract] "we propose to re-weight label-corrected noisy samples in a self-adaptive and class-balanced fashion to alleviate the confirmation bias caused by imbalanced label correction."
  - [section] "We employ the prediction probability w.r.t. the corrected label to reveal the correction confidence."
  - [corpus] Weak evidence - neighbor papers discuss re-weighting but not truncated normal distribution per class.
- Break condition: If the mean-teacher model's predictions are highly biased toward certain classes, re-weighting will amplify rather than correct this bias.

### Mechanism 3
- Claim: Consistency regularization on clean samples improves model robustness by reducing over-reliance on noisy labels.
- Mechanism: An additional loss term encourages prediction consistency between weakly and strongly augmented views of clean samples.
- Core assumption: Clean samples selected by SED are sufficiently accurate to provide meaningful consistency targets.
- Evidence anchors:
  - [abstract] "we additionally employ consistency regularization on selected clean samples to improve model generalization performance."
  - [section] "we additionally imposed consistency regularization on the clean subset to further improve model performance."
  - [corpus] Weak evidence - neighbor papers focus on label noise but not consistency regularization as stated.
- Break condition: If clean sample selection precision is low, consistency regularization will reinforce incorrect patterns.

## Foundational Learning

- Concept: Memorization effect in deep networks
  - Why needed here: Explains why networks fit clean samples first and only later memorize noisy ones, justifying dynamic threshold scheduling.
  - Quick check question: If a network has trained for many epochs with noisy labels, what is the likely state of its loss distribution for clean vs. noisy samples?

- Concept: Class imbalance in label correction
  - Why needed here: Networks learn easier classes faster, leading to biased pseudo-labels when correcting noisy samples. SED addresses this via class-balanced re-weighting.
  - Quick check question: In a balanced dataset with 10 classes, if the model's accuracy is 90% on class A and 60% on class J, which class's noisy samples are more likely to be mis-corrected?

- Concept: Semi-supervised learning paradigm for noisy labels
  - Why needed here: SED treats identified clean samples as labeled data and noisy samples as unlabeled data for self-training, enabling joint learning.
  - Quick check question: How does treating noisy samples as unlabeled data help prevent error propagation compared to direct label correction?

## Architecture Onboarding

- Component map: Sample selection (global+local thresholds) -> Label correction (mean-teacher) -> Sample re-weighting (truncated normal) -> Consistency regularization -> Loss aggregation
- Critical path: Sample selection accuracy -> Label correction reliability -> Re-weighting effectiveness -> Final model performance
- Design tradeoffs: Dynamic thresholds improve adaptivity but increase complexity; class-balanced re-weighting reduces bias but requires per-class statistics
- Failure signatures: Poor early sample selection precision -> noisy clean set -> poor mean-teacher guidance -> ineffective re-weighting -> degraded final accuracy
- First 3 experiments:
  1. Verify that global threshold T_t increases monotonically during training on CIFAR100N.
  2. Measure class-wise selection precision before and after introducing local thresholds.
  3. Test re-weighting effectiveness by comparing correction confidence distributions with and without truncated normal weighting.

## Open Questions the Paper Calls Out
- How does SED's performance scale with increasing noise rates beyond the evaluated range (e.g., 90% symmetric noise)?
- How does SED perform when applied to datasets with severe class imbalance in addition to label noise?
- What is the impact of SED's computational overhead compared to existing methods in terms of training time and memory usage?

## Limitations
- The effectiveness of dynamic class-specific thresholds is not directly validated against simpler static threshold alternatives
- The truncated normal re-weighting mechanism's sensitivity to hyperparameter choices remains unexplored
- The relative contribution of consistency regularization to overall performance gains is not isolated through ablation studies

## Confidence
- High confidence: The general framework of combining sample selection, label correction, and re-weighting is well-established in noisy label literature. The experimental superiority on benchmark datasets is supported by reported results.
- Medium confidence: The specific mechanisms (dynamic class-specific thresholds, truncated normal re-weighting) are plausible but lack direct comparative validation against simpler alternatives.
- Low confidence: The claim that class imbalance in label correction is the primary source of confirmation bias without empirical quantification of this effect.

## Next Checks
1. Compare SED performance with static thresholds to quantify the actual benefit of dynamic adaptation
2. Measure the correlation between correction confidence scores and actual correction accuracy across different noise rates
3. Monitor the precision of the clean sample set throughout training to verify it remains sufficiently high for consistency regularization to be beneficial