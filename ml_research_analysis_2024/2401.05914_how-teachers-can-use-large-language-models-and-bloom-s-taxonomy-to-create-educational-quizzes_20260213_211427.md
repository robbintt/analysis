---
ver: rpa2
title: How Teachers Can Use Large Language Models and Bloom's Taxonomy to Create Educational
  Quizzes
arxiv_id: '2401.05914'
source_url: https://arxiv.org/abs/2401.05914
tags:
- quiz
- questions
- teachers
- generated
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a study on how teachers can use large language\
  \ models (LLMs) and Bloom\u2019s taxonomy to create educational quizzes. The researchers\
  \ generated questions at different levels of Bloom\u2019s taxonomy using GPT-3.5\
  \ and compared them to simpler question-generation methods."
---

# How Teachers Can Use Large Language Models and Bloom's Taxonomy to Create Educational Quizzes

## Quick Facts
- arXiv ID: 2401.05914
- Source URL: https://arxiv.org/abs/2401.05914
- Reference count: 9
- Teachers strongly prefer Bloom's taxonomy-aligned question generations for quiz creation

## Executive Summary
This study explores how large language models (LLMs) can assist teachers in creating educational quizzes by generating questions aligned with Bloom's taxonomy. The researchers compared two prompting strategies—simple and controlled (using Bloom's taxonomy)—to generate questions for quiz writing. Teachers were asked to create quizzes using both automatically generated questions and handwritten questions. The results demonstrate that teachers strongly prefer Bloom's taxonomy-aligned questions, finding them more useful for quiz creation. While quiz quality remained comparable across methods, some metrics indicated higher quality when generated questions were used. Additionally, teachers wrote fewer handwritten questions when provided with Bloom's taxonomy-based generations, suggesting these questions better met their needs.

## Method Summary
The study used GPT-3.5 to generate questions using two prompting strategies: a simple prompt and a controlled prompt aligned with Bloom's taxonomy. Biology and machine learning passages served as input material. Teachers participated in quiz-writing experiments where they created quizzes using both automatically generated questions and handwritten questions. The generated questions were evaluated for quality using multiple metrics, including relevance, coherence, fluency, coverage, and question diversity. Teacher preferences were measured through surveys and analysis of their question selection and writing behaviors during the quiz creation process.

## Key Results
- Teachers strongly preferred Bloom's taxonomy-aligned question generations over simple generations
- Quiz quality remained comparable across all methods, with some metrics showing higher quality for quizzes with generated questions
- Teachers wrote fewer handwritten questions when provided with Bloom's taxonomy-based generations
- Generated questions improved quiz coverage of passage content

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aligning question generation with Bloom's taxonomy increases teacher preference and perceived usefulness of generated questions.
- Mechanism: Bloom's taxonomy provides a structured framework of learning objectives (remembering, understanding, applying, analyzing, evaluating, creating) that allows LLMs to generate questions targeting different cognitive levels. This alignment with pedagogical goals makes the questions more relevant and useful for teachers creating quizzes.
- Core assumption: Teachers value questions that address different cognitive levels and find them useful for creating comprehensive assessments.
- Evidence anchors:
  - [abstract] "teachers strongly prefer using questions generated with Bloom’s taxonomy, finding them more useful for quiz creation"
  - [section] "The controlled prompting strategy uses a pedagogical question taxonomy to generate questions with different learning goals in mind. Bloom’s taxonomy is a popular framework for categorizing learning objectives in educational materials"
  - [corpus] Weak evidence - corpus shows related work on Bloom's taxonomy alignment but no direct evidence of teacher preference for generated questions
- Break condition: If teachers do not value different cognitive levels or find the generated questions misaligned with their assessment goals.

### Mechanism 2
- Claim: Using automatically generated questions improves quiz coverage without sacrificing quality.
- Mechanism: Automatically generated questions, especially those aligned with Bloom's taxonomy, can cover more of the input passage content than handwritten questions alone. This increased coverage comes from systematic generation across different cognitive levels, ensuring broader representation of the material.
- Core assumption: Increased coverage of passage content is a desirable quality in quizzes and does not come at the expense of question relevance or coherence.
- Evidence anchors:
  - [abstract] "quizzes with automatically generated questions were even of higher quality"
  - [section] "The coverage results show significant improvement when teachers utilize generated questions during their quiz writing process"
  - [corpus] Moderate evidence - corpus includes work on question diversity but limited evidence on coverage improvement
- Break condition: If increased coverage leads to redundancy, irrelevance, or decreased coherence in the quiz.

### Mechanism 3
- Claim: Teachers use fewer handwritten questions when provided with Bloom's taxonomy-based generations, indicating higher quality or better suitability.
- Mechanism: When teachers have access to automatically generated questions aligned with Bloom's taxonomy, they copy more of these questions directly into their quizzes and handwrite fewer questions themselves. This behavior suggests that the generated questions are of sufficient quality or better suited to their needs.
- Core assumption: Teacher behavior (copying vs. handwriting) is a valid indicator of question quality and suitability for their purposes.
- Evidence anchors:
  - [abstract] "teachers used fewer handwritten questions when provided with Bloom’s taxonomy-based generations"
  - [section] "In both cohorts, the teachers hand write fewer questions when they have the controlled generations at their disposal"
  - [corpus] Weak evidence - corpus lacks direct evidence of teacher behavior patterns with generated questions
- Break condition: If teacher copying behavior is influenced by factors other than question quality, such as time constraints or workflow preferences.

## Foundational Learning

- Concept: Bloom's Taxonomy
  - Why needed here: Understanding Bloom's taxonomy is essential for comprehending how the question generation system works and why it produces pedagogically useful questions.
  - Quick check question: What are the six levels of Bloom's taxonomy in order from lowest to highest cognitive demand?

- Concept: Prompt Engineering
  - Why needed here: The paper relies heavily on prompt engineering techniques to control LLM output, so understanding these concepts is crucial for implementing or modifying the system.
  - Quick check question: What is the difference between a prefix-style prompt and a few-shot learning prompt?

- Concept: Question Quality Metrics
  - Why needed here: The evaluation of the generated questions and quizzes relies on specific quality metrics, so understanding these is necessary to interpret the results.
  - Quick check question: What does the "coverage" metric measure in the context of quiz evaluation?

## Architecture Onboarding

- Component map: Input passage → Prompt engineering (simple or controlled) → LLM generation → Teacher quiz creation → Quality evaluation → Analysis of teacher preferences and quiz metrics
- Critical path: Input passage → Prompt engineering (simple or controlled) → LLM generation → Teacher quiz creation → Quality evaluation → Analysis of teacher preferences and quiz metrics
- Design tradeoffs: The system trades off between the simplicity of the "simple" prompting strategy and the pedagogical alignment of the "controlled" strategy. The controlled strategy requires more sophisticated prompt engineering but produces questions that teachers find more useful.
- Failure signatures: If generated questions are irrelevant, disfluent, or unanswerable; if teachers show no preference for Bloom's taxonomy-aligned questions; if quiz quality decreases with generated questions; if teacher efficiency does not improve.
- First 3 experiments:
  1. Generate questions using both prompting strategies on a small set of passages and manually evaluate relevance and diversity.
  2. Conduct a small-scale quiz writing experiment with a few teachers to test the workflow and identify potential issues.
  3. Evaluate the generated questions using the defined quality metrics to establish baseline performance before full-scale deployment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do student learning outcomes compare when using quizzes created with Bloom's taxonomy-based question generation versus traditional methods?
- Basis in paper: [explicit] The paper notes that future work should include student goals, opinions, and performance to more comprehensively understand the implications of using automatically generated questions in the classroom.
- Why unresolved: The current study focuses on teacher preferences and quiz quality but does not include student performance metrics or learning outcomes.
- What evidence would resolve it: Controlled experiments comparing student test scores, retention rates, and engagement levels between quizzes created with and without Bloom's taxonomy-based generation.

### Open Question 2
- Question: What are the optimal prompt engineering strategies for different educational domains and complexity levels?
- Basis in paper: [inferred] The study used a limited set of domains (biology and machine learning) and specific prompt strategies, suggesting that domain-agnostic results are still to be demonstrated.
- Why unresolved: The paper only considers two domains and does not explore the full range of possible prompt engineering variations across different subjects or complexity levels.
- What evidence would resolve it: Comparative studies across multiple disciplines with systematic variations in prompt structure, context length, and example quality to identify optimal configurations.

### Open Question 3
- Question: How does the use of automatically generated questions impact long-term teacher workload and pedagogical practices?
- Basis in paper: [explicit] The paper acknowledges that the quiz writing setting is somewhat contrived and that future work should remove these constraints to better assess realistic usage patterns.
- Why unresolved: The study used a controlled experimental setting that doesn't reflect real-world teaching conditions where teachers might use additional resources or have different time constraints.
- What evidence would resolve it: Longitudinal studies tracking teacher time investment, question modification patterns, and pedagogical approaches over extended periods of actual classroom use.

## Limitations

- Small sample size with only 11 teachers across two cohorts
- Limited to biology and machine learning subject domains
- Single LLM model (GPT-3.5) used, making results potentially model-dependent

## Confidence

- Teacher preference for Bloom's taxonomy-aligned questions: High confidence
- Improved quiz coverage with generated questions: Medium confidence
- Higher quiz quality with generated questions: Low-Medium confidence
- Reduced handwritten questions when using generated content: High confidence

## Next Checks

1. Replicate with larger teacher sample and diverse subject areas - Test the findings with at least 50 teachers across humanities, social sciences, and STEM subjects to verify generalizability
2. Compare multiple LLM models and versions - Evaluate GPT-4, Claude, and open-source alternatives to determine if results are model-specific
3. Longitudinal learning outcome study - Track student performance over time using quizzes created with different generation methods to assess educational impact beyond teacher preference