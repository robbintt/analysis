---
ver: rpa2
title: Breaking the Curse of Multilinguality with Cross-lingual Expert Language Models
arxiv_id: '2401.10440'
source_url: https://arxiv.org/abs/2401.10440
tags:
- language
- languages
- training
- x-elm
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Cross-lingual Expert Language Models (X-ELM) address the performance
  degradation in multilingual language models caused by inter-language competition
  for model parameters. The core method involves training independent expert language
  models on subsets of a multilingual corpus, specialized to different languages while
  remaining effective as a multilingual ensemble.
---

# Breaking the Curse of Multilinguality with Cross-lingual Expert Language Models

## Quick Facts
- **arXiv ID**: 2401.10440
- **Source URL**: https://arxiv.org/abs/2401.10440
- **Reference count**: 10
- **Primary result**: X-ELM outperforms jointly trained multilingual models across 16 languages with up to 7.77 perplexity improvement

## Executive Summary
Cross-lingual Expert Language Models (X-ELM) address the performance degradation in multilingual language models caused by inter-language competition for model parameters. The approach involves training independent expert language models on subsets of a multilingual corpus, specialized to different languages while remaining effective as a multilingual ensemble. X-ELM demonstrates consistent improvements across all considered languages and transfers to downstream tasks, offering benefits like asynchronous training and the ability to add new experts without catastrophic forgetting.

## Method Summary
X-ELM uses a Branch-Train-Merge (BTM) paradigm with typological language similarity clustering and Hierarchical Multi-round Training (HMR) for adaptation. The method trains independent expert models (1.7B parameters each) on different language clusters, initialized from XGLM weights. Experts are trained separately and can be used individually or ensembled at inference time. The approach addresses the "curse of multilinguality" by reducing parameter competition while maintaining cross-lingual capabilities through shared initialization.

## Key Results
- X-ELM achieves up to 7.77 improvement in perplexity over dense baselines across 16 languages
- Consistent performance gains transfer to downstream tasks (XNLI, XStoryCloze, PAWS-X)
- Up to 3.76 improvement over dense baselines on downstream tasks
- Enables efficient adaptation to new languages without catastrophic forgetting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Inter-language competition for model parameters causes performance degradation in multilingual language models.
- Mechanism: When multiple languages share the same model capacity during joint training, high-resource languages dominate parameter updates, causing low-resource languages to underfit.
- Core assumption: Fixed model capacity cannot adequately represent all languages simultaneously.
- Evidence anchors:
  - [abstract] "multilingual language models often underperform monolingual ones due to inter-language competition for model parameters"
  - [section] "the many languages are represented in the same, fixed model capacity, causing performance on individual languages to degrade relative to monolingual models"
  - [corpus] Weak evidence - corpus shows related work on curse of multilinguality but no direct evidence for this specific mechanism
- Break condition: If model capacity scales proportionally with number of languages or if languages share sufficient structural similarities.

### Mechanism 2
- Claim: Independent expert training on language subsets improves multilingual performance by reducing inter-language interference.
- Mechanism: By allocating different languages to separate experts, each expert can specialize without interference from other languages, while maintaining cross-lingual capabilities through shared initialization.
- Core assumption: Specialization improves performance more than joint training despite reduced parameter sharing during training.
- Evidence anchors:
  - [abstract] "X-ELM outperforms jointly trained multilingual models across all considered languages"
  - [section] "each expert is trained on a different portion of a multilingual corpus with x-BTM, a new extension of the Branch-Train-Merge paradigm"
  - [corpus] Weak evidence - corpus mentions related work on sparse models but doesn't directly address this mechanism
- Break condition: If language similarities are too high (making separation unnecessary) or too low (making cross-lingual transfer ineffective).

### Mechanism 3
- Claim: Hierarchical Multi-Round Training enables efficient adaptation to new languages without catastrophic forgetting.
- Mechanism: By branching new experts from related existing ones based on typological similarity, new languages can be added while preserving knowledge of existing languages.
- Core assumption: Linguistic typology provides meaningful similarity measures that enable effective knowledge transfer.
- Evidence anchors:
  - [abstract] "new experts can be iteratively added, adapting X-ELM to new languages without catastrophic forgetting"
  - [section] "Hierarchical Multi-Round (HMR) training procedure... efficiently adapting trained X-ELMs to novel multilingual settings"
  - [corpus] Moderate evidence - corpus shows related work on language adaptation but limited on hierarchical approaches
- Break condition: If typological similarity doesn't correlate with useful transfer or if new languages are too dissimilar from existing ones.

## Foundational Learning

- Concept: Multilingual pretraining and the curse of multilinguality
  - Why needed here: Understanding why joint multilingual training degrades performance is fundamental to grasping why X-ELM's approach works
  - Quick check question: What happens to low-resource language performance when high-resource languages dominate parameter updates in joint multilingual training?

- Concept: Branch-Train-Merge (BTM) paradigm
  - Why needed here: X-ELM builds on BTM, so understanding this training methodology is crucial for implementation
  - Quick check question: How does BTM differ from standard dense multilingual training in terms of parameter updates and GPU communication?

- Concept: Linguistic typology and language similarity metrics
  - Why needed here: X-ELM uses typological clustering for data allocation, so understanding how languages are grouped is essential
  - Quick check question: What linguistic features are typically used to measure language similarity in the LANG2VEC framework?

## Architecture Onboarding

- Component map: XGLM seed model -> Typological clustering -> Expert initialization -> Independent expert training -> Inference (top-1 expert or ensembling)
- Critical path: Data allocation → Expert initialization → Independent expert training → Expert merging/ensembling
- Design tradeoffs: Higher performance vs. increased storage requirements; specialized experts vs. cross-lingual transfer capabilities
- Failure signatures: Degraded performance on low-resource languages, increased perplexity on previously seen languages, or ineffective adaptation to new languages
- First 3 experiments:
  1. Compare perplexity of X-ELM vs. dense baseline on a single high-resource language
  2. Test typological clustering by grouping two linguistically similar languages and measuring performance
  3. Evaluate catastrophic forgetting by adapting X-ELM to a new language and testing on original languages

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal expert count k for multilingual models of different sizes and compute budgets?
- Basis in paper: [explicit] The paper tests k=4,8,16 for a 1.7B parameter model but notes this may not generalize
- Why unresolved: The experiments only test a single model size and compute budget; scaling effects are unknown
- What evidence would resolve it: Experiments varying model size (e.g., 500M, 3B, 7B parameters) and compute budgets while measuring perplexity

### Open Question 2
- Question: How do different data quality issues in multilingual corpora affect X-ELM performance?
- Basis in paper: [explicit] The paper notes "data quality issues have been documented for mC4" but doesn't explore this
- Why unresolved: The experiments use only mC4 without investigating how data quality variations impact results
- What evidence would resolve it: Training X-ELMs on curated multilingual corpora with varying quality levels and measuring performance differences

### Open Question 3
- Question: What are the long-term effects of X-ELM specialization on cross-lingual transfer capabilities?
- Basis in paper: [inferred] The paper shows improved performance but doesn't examine how specialization affects zero-shot/cross-lingual abilities over time
- Why unresolved: The experiments focus on immediate performance gains rather than longitudinal transfer capabilities
- What evidence would resolve it: Tracking cross-lingual transfer performance across multiple rounds of X-ELM training and comparing to dense models

### Open Question 4
- Question: How does the computational overhead of X-ELM compare to dense models when scaling to 100+ languages?
- Basis in paper: [explicit] The paper notes X-ELM has "training efficiency and flexibility" benefits but doesn't scale beyond 20 languages
- Why unresolved: Experiments are limited to 16-20 languages; scaling effects are unknown
- What evidence would resolve it: Training X-ELMs on 100+ languages and comparing total training/inference costs to equivalent dense models

## Limitations
- Limited empirical validation of specific mechanisms despite strong theoretical framework
- Significant storage overhead (8x) compared to single dense model
- Experiments limited to 16-20 languages, not testing scalability to 100+ languages

## Confidence
- **High confidence**: Empirical results showing perplexity improvements across all 16 languages and downstream task performance gains are well-supported by experimental data.
- **Medium confidence**: Claim that inter-language competition causes performance degradation is theoretically sound but lacks direct empirical validation in this work.
- **Medium confidence**: Effectiveness of typological clustering for data allocation is supported by results but specific implementation details are not fully specified.

## Next Checks
1. **Mechanism isolation test**: Conduct controlled experiments where languages with high typological similarity are either grouped together or separated to quantify the specific contribution of typological clustering to performance gains.

2. **Resource scaling analysis**: Measure the exact computational overhead (GPU memory, storage, inference latency) of X-ELM compared to dense baselines across different hardware configurations to better understand practical deployment implications.

3. **Cross-lingual transfer evaluation**: Systematically test the cross-lingual transfer capabilities of X-ELM by training experts on specific language families and evaluating performance on related but unseen languages to validate the cross-lingual effectiveness claim.