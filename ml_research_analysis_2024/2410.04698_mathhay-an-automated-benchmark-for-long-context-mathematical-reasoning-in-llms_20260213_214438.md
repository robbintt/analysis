---
ver: rpa2
title: 'MathHay: An Automated Benchmark for Long-Context Mathematical Reasoning in
  LLMs'
arxiv_id: '2410.04698'
source_url: https://arxiv.org/abs/2410.04698
tags:
- reasoning
- relevant
- question
- documents
- mathematical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MathHay, an automated benchmark for evaluating
  long-context mathematical reasoning in large language models. Unlike existing benchmarks
  focusing on information retrieval, MathHay requires both information-seeking and
  complex mathematical reasoning abilities.
---

# MathHay: An Automated Benchmark for Long-Context Mathematical Reasoning in LLMs

## Quick Facts
- arXiv ID: 2410.04698
- Source URL: https://arxiv.org/abs/2410.04698
- Reference count: 33
- Key outcome: Even the best model (Gemini-1.5-Pro-002) achieves only 51.26% accuracy at 128K tokens, highlighting significant room for improvement in long-context mathematical reasoning.

## Executive Summary
MathHay is an automated benchmark designed to evaluate long-context mathematical reasoning in large language models. Unlike existing benchmarks that focus on information retrieval, MathHay requires models to perform both information-seeking and complex mathematical reasoning across varying context lengths (32K-128K tokens). The benchmark is constructed through an automated pipeline involving document collection, question generation across four difficulty levels, quality control, and haystack construction with varying input lengths. Extensive experiments with eight top-performing LLMs reveal significant challenges in long-context mathematical reasoning, with accuracy dropping substantially as context length increases.

## Method Summary
The benchmark is constructed through an automated pipeline that collects documents containing numerical values using Tavily Search, generates mathematical reasoning questions using LLM prompts across four difficulty levels (SSSD, MSSD, SSMD, MSMD), and applies quality control through solution consistency checks. The resulting questions are embedded into document "haystacks" of varying sizes (32K, 64K, 128K tokens) with relevant documents placed at different positions to test retrieval capabilities. Model performance is evaluated using GPT-4o as a judge with a multi-stage verification process.

## Key Results
- Gemini-1.5-Pro-002 achieves 51.26% accuracy at 128K tokens, significantly lower than its 85.67% accuracy at 32K tokens
- Open-source models perform significantly worse than closed-source models, particularly on multi-step tasks
- Model performance degrades substantially with longer input lengths and more noisy text
- Multi-document tasks show more pronounced accuracy drops compared to single-document tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Automated document collection and query generation enables scalable benchmark creation
- Mechanism: Uses Tavily Search to retrieve up-to-date documents based on LLM-generated queries, then filters for sufficient numerical content
- Core assumption: LLMs can reliably generate realistic queries that produce documents with adequate numerical data
- Evidence anchors: Incorporates time period constraints in prompts, selects top-ranked documents, but corpus shows weak relevance (average neighbor FMR=0.425, average citations=0.0)

### Mechanism 2
- Claim: Multi-document haystack construction with varying placement strategies tests document retrieval and reasoning abilities
- Mechanism: Inserts relevant documents into noisy text at different positions and varying input lengths (32K-128K tokens)
- Core assumption: Placement strategy and noise level sufficiently challenge models' ability to identify relevant documents
- Evidence anchors: Constructs varying-sized haystacks, shows GPT-4o-mini's instability with longer inputs

### Mechanism 3
- Claim: Automated quality control through solution consistency ensures benchmark reliability
- Mechanism: Generates two independent Python solutions for each question, retains only examples with matching answers
- Core assumption: Matching LLM-generated solutions indicate correct reasoning
- Evidence anchors: Executes Python solutions, filters inconsistent examples, but depends entirely on LLM reliability

## Foundational Learning

- Concept: Long-context modeling and KV cache management
  - Why needed here: Benchmark evaluates models up to 128K tokens, requiring understanding of attention mechanisms and memory usage
  - Quick check question: What happens to attention patterns when context length increases from 32K to 128K tokens?

- Concept: Mathematical reasoning and numerical computation
  - Why needed here: Benchmark tests models' ability to perform arithmetic operations and multi-step reasoning using extracted numerical data
  - Quick check question: How would you design a test to verify a model can correctly compute percentage changes using extracted values?

- Concept: Document retrieval and information extraction
  - Why needed here: Models must locate relevant information within noisy contexts, requiring understanding of retrieval mechanisms
  - Quick check question: What retrieval strategies might a model use when relevant information is buried in irrelevant documents?

## Architecture Onboarding

- Component map: Document Collection (Tavily Search + LLM queries) -> Question Generation (LLM prompts) -> Quality Control (Python execution + LLM judgment) -> Benchmark Construction (haystack assembly) -> Evaluation (GPT-4o judge)

- Critical path: Document Collection → Question Generation → Quality Control → Benchmark Construction → Evaluation

- Design tradeoffs:
  - Automation vs. quality: Fully automated pipeline may introduce noise, but manual review is not scalable
  - Document diversity vs. relevance: Broader topic coverage may reduce relevance of numerical content
  - Input length vs. practicality: Longer contexts are more realistic but computationally expensive

- Failure signatures:
  - Low correlation between verified and unverified data suggests quality control issues
  - Consistent performance drop with longer contexts indicates attention limitations
  - High variance across placement strategies suggests retrieval challenges

- First 3 experiments:
  1. Test correlation between verified and unverified data quality to validate automation pipeline
  2. Evaluate model performance across different placement strategies to identify retrieval bottlenecks
  3. Measure performance degradation as input length increases to establish context length limits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of irrelevant documents in MATHHAY affect LLM performance compared to benchmarks without such noise?
- Basis in paper: The paper states MATHHAY includes irrelevant documents to simulate real-world scenarios, making tasks more challenging.
- Why unresolved: While performance decreases with longer inputs and more noise, direct comparison to noise-free benchmarks is not provided.
- What evidence would resolve it: Comparative experiments between MATHHAY and benchmarks like Needle in a Haystack to quantify performance differences.

### Open Question 2
- Question: Can the automated construction method be effectively applied to other domains beyond mathematical reasoning?
- Basis in paper: The paper describes an automated method for constructing MATHHAY, but focuses on mathematical reasoning tasks.
- Why unresolved: Effectiveness for other domains (legal reasoning, medical diagnosis) is unclear.
- What evidence would resolve it: Application to other domains and evaluation of resulting benchmarks' effectiveness.

### Open Question 3
- Question: How does open-source model performance on MATHHAY compare to other long-context benchmarks, and what factors contribute to their lower performance?
- Basis in paper: Open-source models perform significantly worse than closed-source models on MATHHAY, particularly in multi-step tasks.
- Why unresolved: The paper does not explore reasons behind the performance gap or compare across benchmarks.
- What evidence would resolve it: Detailed analysis of open-source model performance across multiple benchmarks and identification of specific challenges.

## Limitations

- Automated construction pipeline may propagate systematic biases through LLM-generated queries and solutions
- Quality control relies entirely on LLM solution consistency without independent human verification
- Document retrieval mechanism may introduce selection bias toward certain sources or time periods

## Confidence

**High Confidence**: The observation that even state-of-the-art models achieve only 51.26% accuracy at 128K tokens is well-supported by experimental results and clearly demonstrates current limitations.

**Medium Confidence**: The claim that MathHay uniquely combines information-seeking with complex mathematical reasoning is supported by comparisons to existing benchmarks, but automated construction methodology introduces uncertainty about reasoning complexity capture.

**Low Confidence**: The assertion that automated quality control through solution consistency ensures benchmark reliability is the weakest claim, as it depends entirely on LLM-generated solutions being trustworthy indicators of correct reasoning.

## Next Checks

1. **Independent Solution Verification**: Have human experts solve a random sample of 100 benchmark questions and compare their answers to both LLM-generated solutions and final benchmark answers to assess automated quality control reliability.

2. **Cross-Model Performance Analysis**: Test whether performance degradation patterns are consistent across a broader range of architectures, including smaller models and different families, to determine if benchmark reveals fundamental limitations or model-specific issues.

3. **Noise Robustness Evaluation**: Systematically vary the ratio of relevant to irrelevant documents in haystacks to quantify sensitivity to noise levels and identify thresholds where mathematical reasoning becomes infeasible regardless of context length.