---
ver: rpa2
title: Multi-Criteria Comparison as a Method of Advancing Knowledge-Guided Machine
  Learning
arxiv_id: '2403.11840'
source_url: https://arxiv.org/abs/2403.11840
tags:
- criteria
- competition
- decision
- could
- multiple
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a multi-criteria model evaluation method that
  compares AI/ML models across multiple scientific, theoretical, and practical criteria
  using voting rules from computational social choice. This method addresses limitations
  of single-criterion evaluations (e.g., accuracy only) which can incentivize black-box
  models lacking generalizability, explainability, and scientific grounding.
---

# Multi-Criteria Comparison as a Method of Advancing Knowledge-Guided Machine Learning

## Quick Facts
- arXiv ID: 2403.11840
- Source URL: https://arxiv.org/abs/2403.11840
- Reference count: 21
- One-line primary result: Multi-criteria evaluation using voting rules promotes diverse, scientifically-grounded ML models over single-criterion accuracy optimization

## Executive Summary
This paper proposes a multi-criteria model evaluation method that compares AI/ML models across multiple scientific, theoretical, and practical criteria using voting rules from computational social choice. The method addresses limitations of single-criterion evaluations (e.g., accuracy only) which can incentivize black-box models lacking generalizability, explainability, and scientific grounding. The authors demonstrate the method's flexibility by applying it to a real-world ML competition in personnel selection, where a simple non-ML model outperformed most ML entrants by ranking candidates across multiple variables and selecting top performers while maintaining demographic proportions.

## Method Summary
The method quantifies multiple evaluation criteria (predictive accuracy, explainability, adverse impact, theoretical soundness) for each model, then uses Condorcet and Borda voting rules to rank models holistically. Models are ranked ordinally on each criterion, and pairwise comparisons determine if any model dominates others across a majority of criteria. If no Condorcet winner exists, Borda voting with tie-breaking rules selects the final winner. The approach provides detailed performance profiles for post-hoc analysis while promoting diverse model types beyond accuracy-optimized solutions.

## Key Results
- Multi-criteria evaluation promotes diverse model types compared to single-criterion accuracy optimization
- The method provides richer post-hoc comparisons and insights into model performance
- A simple non-ML model outperformed most ML entrants in a personnel selection competition by ranking across multiple variables

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method improves model diversity by using multi-criteria evaluation instead of single-criterion accuracy optimization.
- Mechanism: By evaluating models across multiple scientific, theoretical, and practical criteria (e.g., theoretical criteria, psychological criteria, scientific criteria, explainability criteria, ethical criteria), the method reduces the incentive for models to focus solely on predictive accuracy, which often leads to overfitting and lack of explainability.
- Core assumption: Model builders will adjust their approach when evaluation criteria include factors beyond accuracy, such as explainability and theoretical grounding.
- Evidence anchors:
  - [abstract] "This method addresses limitations of single-criterion evaluations (e.g., accuracy only) which can incentivize black-box models lacking generalizability, explainability, and scientific grounding."
  - [section] "The main theme of many of the critiques was the reliance on a single evaluative criterion; minimized prediction error (MSD in this case). Harman et al. outline how using a single evaluative criterion limits the type and variety of models entered..."
- Break condition: If the weighting between criteria is heavily skewed toward accuracy, model builders may still optimize primarily for accuracy.

### Mechanism 2
- Claim: The voting rules from computational social choice enable fair comparison of models with divergent characteristics.
- Mechanism: The method uses Condorcet and Borda voting rules to aggregate ordinal rankings across multiple criteria, allowing models with different strengths to be compared holistically rather than competing on a single dimension.
- Core assumption: Voting rules from computational social choice can effectively aggregate heterogeneous model evaluation criteria into meaningful comparisons.
- Evidence anchors:
  - [abstract] "Ordinal ranking of criteria scores are evaluated using voting rules from the field of computational social choice and allow the comparison of divergent measures and types of models in a holistic evaluation."
  - [section] "To evaluate candidate models (and select a winner for modeling competitions) Harman et al. propose a combination of Condorcet and Borda rule voting where models are ranked ordinally on each criteria."
- Break condition: If the criteria are too heterogeneous or the voting rules don't properly weight important criteria, the comparison may not produce meaningful results.

### Mechanism 3
- Claim: The method provides richer post-hoc analysis by maintaining detailed performance data across multiple criteria.
- Mechanism: By quantifying each criterion ordinally or continuously, the method creates a comprehensive performance profile for each model that enables detailed comparisons beyond simple accuracy metrics.
- Core assumption: Having detailed multi-criteria performance data enables more nuanced understanding of model strengths and weaknesses than single-metric evaluations.
- Evidence anchors:
  - [abstract] "The multi-criteria approach not only promotes more diverse model types but also provides richer post-hoc comparisons and insights into model performance."
  - [section] "In addition to allowing direct model evaluation across multiple differing criteria, this structure also provides more insight into relative model performance."
- Break condition: If the criteria quantification is too coarse or the reporting system is inadequate, the post-hoc analysis benefits may be limited.

## Foundational Learning

- Concept: Computational social choice theory and voting rules
  - Why needed here: The method relies on Condorcet and Borda voting rules to aggregate model rankings across multiple criteria, requiring understanding of how these voting systems work and their properties.
  - Quick check question: What happens when no Condorcet winner exists in the model evaluation process?

- Concept: Model evaluation taxonomy development
  - Why needed here: The method requires creating a taxonomy of desirable model characteristics specific to the application domain, which involves understanding what makes a good model in that context.
  - Quick check question: How would you modify the taxonomy for a medical diagnosis application versus a personnel selection application?

- Concept: Ordinal vs. continuous quantification methods
  - Why needed here: The method allows for both ordinal and continuous quantification of criteria, requiring understanding of when each approach is appropriate and how they affect the evaluation outcome.
  - Quick check question: What are the implications of using binary criteria versus continuous criteria in the voting process?

## Architecture Onboarding

- Component map: Criteria definition module -> Quantification module -> Voting module -> Result aggregation module -> Reporting module
- Critical path: Criteria definition → Quantification → Voting → Result aggregation → Reporting
- Design tradeoffs: The method balances comprehensiveness (more criteria provide better insights) against complexity (too many criteria can make the system unwieldy). The choice between ordinal and continuous quantification affects the sensitivity of the evaluation.
- Failure signatures: Models that appear to perform well but lack theoretical grounding, or models that are theoretically sound but perform poorly on practical criteria. Voting deadlocks when no clear winner emerges.
- First 3 experiments:
  1. Implement a simplified version with 3-4 criteria using synthetic model performance data to verify the voting logic works correctly
  2. Test with real-world model data from a simple prediction task to validate the criteria quantification approach
  3. Conduct sensitivity analysis by varying the weighting between criteria to understand how different emphasis affects model rankings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can multi-criteria evaluation methods be effectively implemented in real-world AI/ML competitions to promote knowledge-guided machine learning?
- Basis in paper: Explicit - The paper describes a multi-criteria evaluation method and its application in a modeling competition in Psychology, but notes the need for further exploration in real-world AI/ML contexts.
- Why unresolved: The method's effectiveness and adaptability to various AI/ML domains and competitions have not been fully explored or validated.
- What evidence would resolve it: Conducting multiple real-world AI/ML competitions using the multi-criteria evaluation method and analyzing the diversity and quality of models produced, as well as the insights gained from the evaluations.

### Open Question 2
- Question: What are the potential limitations or challenges of using multi-criteria evaluation methods in AI/ML model comparison, and how can they be addressed?
- Basis in paper: Inferred - While the paper highlights the advantages of the method, it does not discuss potential limitations or challenges in detail.
- Why unresolved: The paper focuses on the benefits of the method without thoroughly exploring its potential drawbacks or obstacles in implementation.
- What evidence would resolve it: Identifying and analyzing the limitations or challenges of the multi-criteria evaluation method through empirical studies or case studies, and proposing solutions to overcome these issues.

### Open Question 3
- Question: How can the multi-criteria evaluation method be adapted to assess AI/ML models in different domains or applications, such as healthcare, finance, or autonomous vehicles?
- Basis in paper: Explicit - The paper mentions the flexibility of the method and its potential adaptation to various fields, but does not provide specific examples or guidelines.
- Why unresolved: The paper suggests the adaptability of the method but does not demonstrate how it can be tailored to specific domains or applications.
- What evidence would resolve it: Developing domain-specific taxonomies of desirable model characteristics and quantifying criteria relevant to each domain, then applying the multi-criteria evaluation method to AI/ML models in those fields to assess its effectiveness and insights gained.

## Limitations

- The method is demonstrated through a single real-world case study in personnel selection, limiting generalizability to other domains
- Selection of criteria and their relative importance remains domain-specific and requires expert judgment that may introduce bias
- Effectiveness depends on having well-defined, quantifiable criteria across all evaluation dimensions, which may be challenging for some domains

## Confidence

- **High Confidence**: The core mechanism of using voting rules to aggregate multi-criteria model rankings is well-established and theoretically sound. The demonstration that single-criterion evaluation can incentivize black-box models is supported by existing literature.
- **Medium Confidence**: The claim that this method promotes more diverse model types is plausible but requires broader empirical validation across multiple domains and problem types.
- **Medium Confidence**: The assertion that the method provides richer post-hoc analysis is supported by the case study but would benefit from systematic comparison with traditional evaluation approaches.

## Next Checks

1. **Cross-domain validation**: Apply the method to at least two additional domains (e.g., medical diagnosis and financial risk assessment) to test generalizability of criteria selection and voting rule effectiveness.

2. **Sensitivity analysis**: Systematically vary the weighting between criteria to determine how robust the model rankings are to changes in evaluation priorities, particularly the balance between accuracy and other factors.

3. **Comparison study**: Conduct a controlled experiment comparing model diversity and performance outcomes between single-criterion and multi-criteria evaluation methods across multiple modeling competitions.