---
ver: rpa2
title: 'RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots'
arxiv_id: '2403.01193'
source_url: https://arxiv.org/abs/2403.01193
tags:
- context
- responses
- information
- hallucinations
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how retrieval-augmented generation (RAG)
  can reduce hallucinations in large language models (LLMs). Using academic CVs as
  ground-truth context, we compared LLM accuracy with and without context prompts.
---

# RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots

## Quick Facts
- arXiv ID: 2403.01193
- Source URL: https://arxiv.org/abs/2403.01193
- Reference count: 15
- Key result: RAG increased LLM accuracy from 7.31% to 93.95% on academic CV tasks

## Executive Summary
This study investigates how retrieval-augmented generation (RAG) can reduce hallucinations in large language models (LLMs). Using academic CVs as ground-truth context, we compared LLM accuracy with and without context prompts. Adding context increased correct responses from 7.31% to 93.95%, an 18-fold improvement. However, RAG still produced errors in 6.04% of cases due to issues like noisy context, instruction mismatch, context-based synthesis, unusual formatting, and incomplete context. These findings highlight RAG's effectiveness while revealing remaining challenges, particularly the model's tendency to generate plausible but incorrect responses when context is missing or poorly formatted.

## Method Summary
The study collected 56 academic CVs and used the OpenAI GPT-3.5-turbo-16k-0613 model to evaluate responses with and without context prompts. Academic participants reviewed their own CV data and evaluated LLM responses for accuracy, hallucinations, partial correctness, and helpfulness. The evaluation compared context-based responses against non-context-based responses to measure the impact of retrieval-augmented generation on hallucination reduction.

## Key Results
- RAG increased LLM accuracy from 7.31% to 93.95% on academic CV tasks
- Context-based responses still produced errors in 6.04% of cases
- Five distinct error categories identified: noisy context, instruction mismatch, context-based synthesis, unusual formatting, and incomplete context

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG significantly reduces LLM hallucinations by providing external context
- Mechanism: RAG retrieves relevant information from external sources and incorporates it into the prompt, allowing the LLM to base responses on verified data rather than its internal knowledge alone
- Core assumption: The retrieved context accurately represents the information needed to answer the query
- Evidence anchors:
  - [abstract] "RAG increases accuracy in some cases, but can still be misled when prompts directly contradict the model's pre-trained understanding"
  - [section 4.1] "The addition of context dramatically increases the accuracy of the LLM's responses. With context, subjects indicated that the model correctly navigated the text to produce accurate responses approximately 94% of the time"
  - [corpus] Weak evidence - neighboring papers discuss RAG effectiveness but don't provide direct experimental validation
- Break condition: When retrieved context is noisy, incomplete, or contradicts the model's training

### Mechanism 2
- Claim: Context prompts can inadvertently introduce errors through context drift and synthesis
- Mechanism: When the LLM processes lengthy context, it may incorrectly associate information from adjacent sections or generate plausible but false information to fill gaps
- Core assumption: The model's token limit and processing logic can cause it to blend information across context boundaries
- Evidence anchors:
  - [section 4.1] "we found that the generated response using context from one section sometimes continues into the next instead of terminating after retrieving all relevant information"
  - [section 4.3] "the model filled in the gaps by generating a list of ten jobs, starting with 'Volunteer' and ending with 'Researcher at Florida State University'"
  - [corpus] Moderate evidence - neighboring papers discuss RAG limitations but focus more on system configuration than generation errors
- Break condition: When context sections are too long or lack clear delimiters

### Mechanism 3
- Claim: Unusual formatting in source documents can mislead RAG systems
- Mechanism: LLMs trained on standard document formats may misinterpret non-standard layouts, particularly date and position information
- Core assumption: The model relies heavily on positional and formatting cues to correctly parse context
- Evidence anchors:
  - [section 4.4] "the date information for each job listing followed these long and detailed descriptions, which placed date information from the previous job in close proximity to the position and institution of the subsequent job"
  - [section 4.5] "The model we used did not have the capability to search online data, and as a result, had to rely on the context"
  - [corpus] Weak evidence - no neighboring papers specifically address formatting issues in RAG contexts
- Break condition: When source documents deviate significantly from training data formats

## Foundational Learning

- Concept: Token limits and context window management
  - Why needed here: Understanding how token constraints affect RAG performance is crucial for interpreting the study's results
  - Quick check question: What happens when the context exceeds the model's token limit?

- Concept: Information retrieval principles
  - Why needed here: The quality of RAG depends on how effectively relevant information is retrieved and presented
  - Quick check question: How does retrieval precision impact RAG accuracy?

- Concept: Hallucination detection and classification
  - Why needed here: The study categorizes different types of errors, requiring understanding of hallucination characteristics
  - Quick check question: What distinguishes a hallucination from an incomplete or noisy response?

## Architecture Onboarding

- Component map: Query → Retrieval → Context assembly → Prompt → LLM → Response evaluation
- Critical path: Query → Retrieval → Context assembly → Prompt → LLM → Response evaluation
- Design tradeoffs: Context comprehensiveness vs. noise introduction; retrieval accuracy vs. latency
- Failure signatures: Context drift (blending information across sections), synthesis (generating plausible but false content), formatting misinterpretation
- First 3 experiments:
  1. Test context drift by providing clearly delineated sections and measuring cross-section contamination
  2. Evaluate synthesis tendency by providing incomplete context and observing generated completions
  3. Assess formatting sensitivity by presenting the same information in different layouts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can RAG systems be designed to prevent the interpolation of plausible but inaccurate information when context is missing or distorted?
- Basis in paper: [explicit] The paper identifies "context-based synthesis" as a significant issue where the model generates believable but incorrect information when context is incomplete.
- Why unresolved: The paper highlights this as a critical area for further research but does not provide specific solutions or methodologies to address this problem.
- What evidence would resolve it: Experimental studies demonstrating effective strategies or architectural changes in RAG systems that successfully mitigate the generation of plausible but inaccurate content when context is incomplete.

### Open Question 2
- Question: What are the implications of unusual formatting in context documents on the reliability of RAG responses, and how can these be mitigated?
- Basis in paper: [explicit] The paper discusses "unusual formatting" as a source of error, where the model's responses are subtly incorrect due to deviations from common document structures.
- Why unresolved: The paper identifies this as a challenge but does not explore potential solutions or the extent of its impact on RAG system performance.
- What evidence would resolve it: Research showing the development of preprocessing techniques or model adjustments that improve the handling of variably formatted context documents.

### Open Question 3
- Question: How can user expectations regarding the capabilities of LLMs in information retrieval be managed to prevent reliance on incorrect or incomplete responses?
- Basis in paper: [explicit] The paper notes that users may expect LLMs to function as information retrieval systems, leading to issues when context is incomplete or when users rely on generated responses for information not provided in the context.
- Why unresolved: The paper suggests this as an area for future research but does not propose methods for aligning user expectations with the actual capabilities of LLMs.
- What evidence would resolve it: Studies or frameworks that effectively communicate the limitations of LLMs to users and adjust prompt engineering to better manage expectations and improve response accuracy.

## Limitations
- Sample size of 56 CVs may limit generalizability to other domains or document types
- Evaluation relies on academic participants' subjective assessments rather than automated metrics
- OpenAI GPT-3.5-turbo-16k-0613 model used may not represent latest LLM capabilities

## Confidence

- **High confidence**: The 18-fold improvement in accuracy when adding context (7.31% to 93.95%) is well-supported by empirical data
- **Medium confidence**: The identification of five error categories in context-based responses is supported by qualitative analysis
- **Low confidence**: The generalizability of formatting-related errors to other document types remains uncertain

## Next Checks

1. Implement automated metrics for hallucination detection to complement human evaluation and enable larger-scale testing across diverse document types
2. Test the same RAG pipeline across multiple LLM architectures (GPT-4, Claude, LLaMA) to determine if observed error patterns are model-specific or inherent to RAG systems
3. Systematically vary context length and document complexity to map the relationship between token limits, context quality, and hallucination rates across different document types and query complexity levels