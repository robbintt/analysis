---
ver: rpa2
title: Can We Achieve High-quality Direct Speech-to-Speech Translation without Parallel
  Speech Data?
arxiv_id: '2406.07289'
source_url: https://arxiv.org/abs/2406.07289
tags:
- latexit
- sha1
- base64
- speech
- s2tt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ComSpeech, a direct speech-to-speech translation
  (S2ST) model that integrates arbitrary pretrained speech-to-text translation (S2TT)
  and text-to-speech (TTS) models. It addresses the challenge of requiring parallel
  speech data for training by introducing a vocabulary adaptor based on connectionist
  temporal classification (CTC) to align representations between different vocabularies.
---

# Can We Achieve High-quality Direct Speech-to-Speech Translation without Parallel Speech Data?

## Quick Facts
- arXiv ID: 2406.07289
- Source URL: https://arxiv.org/abs/2406.07289
- Authors: Qingkai Fang; Shaolei Zhang; Zhengrui Ma; Min Zhang; Yang Feng
- Reference count: 21
- One-line primary result: Introduces ComSpeech, a direct S2ST model achieving state-of-the-art performance without parallel speech data through vocabulary adaptation and contrastive learning.

## Executive Summary
This paper addresses the challenge of direct speech-to-speech translation (S2ST) without requiring parallel speech data. The authors propose ComSpeech, a model that integrates arbitrary pretrained speech-to-text translation (S2TT) and text-to-speech (TTS) models through a novel vocabulary adaptor based on CTC alignment. For the zero-shot learning scenario, they introduce ComSpeech-ZS, which uses contrastive learning to align representations between modalities without parallel speech pairs. Experiments on the CVSS dataset demonstrate that ComSpeech outperforms previous two-pass models like UnitY and Translatotron 2 in both translation quality and decoding speed, while ComSpeech-ZS achieves only 0.7 ASR-BLEU lower than the supervised version.

## Method Summary
ComSpeech is built by integrating pretrained S2TT and TTS models through a vocabulary adaptor that uses CTC to align representations between different vocabularies (e.g., subword tokens and phonemes). The model consists of three main modules: an arbitrary S2TT model (F), a vocabulary adaptor (A), and an arbitrary TTS model (G). The adaptor bridges vocabulary mismatches, enabling seamless integration. For zero-shot learning, ComSpeech-ZS adds a contrastive learning objective to align latent representations without requiring parallel speech data. The training involves two stages: first pretraining on S2TT and TTS data separately, then fine-tuning the integrated model. The approach eliminates the need for parallel speech data while maintaining high translation quality.

## Key Results
- ComSpeech surpasses previous two-pass models like UnitY and Translatotron 2 in both translation quality and decoding speed on the CVSS dataset
- In zero-shot learning, ComSpeech-ZS achieves only 0.7 ASR-BLEU lower than the supervised ComSpeech
- ComSpeech-ZS outperforms cascaded systems (S2TT + G2P + TTS) while requiring no parallel speech data
- The model supports arbitrary pretrained S2TT and TTS models through the vocabulary adaptor mechanism

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ComSpeech's vocabulary adaptor bridges vocabulary mismatches between S2TT and TTS models, enabling seamless integration.
- Mechanism: A CTC-based adaptor learns to map representation sequences between different vocabularies (e.g., subword → phoneme), allowing arbitrary pretrained models to be combined.
- Core assumption: The temporal alignment learned by CTC is sufficient to handle different tokenization granularities without degrading translation quality.
- Evidence anchors:
  - [abstract]: "It facilitates the conversion of representation sequences between any vocabularies, thus enabling the integration of any pretrained S2TT and TTS models into an S2ST model."
  - [section]: "Specifically, we use W and V to denote the vocabularies of the S2TT model and the TTS model, respectively... Consequently, directly combining them into an S2ST model results in the TTS model being unable to process the representations output by the S2TT model."
  - [corpus]: Weak—no explicit neighbor paper directly discusses CTC-based vocabulary adaptors.
- Break condition: If CTC alignment fails to capture fine-grained temporal correspondences, the TTS model will misinterpret input and translation quality will drop.

### Mechanism 2
- Claim: Contrastive learning in ComSpeech-ZS aligns latent representations across modalities without parallel speech data.
- Mechanism: A representation alignment loss (MSE + contrastive) ensures that the TTS encoder produces similar outputs for both ground-truth phonemes and adaptor-generated representations, enabling zero-shot transfer.
- Core assumption: The learned alignment generalizes from text-only TTS pairs to unseen speech-text pairs.
- Evidence anchors:
  - [abstract]: "It aligns representations in the latent space through contrastive learning, allowing the speech synthesis capabilities learned from the TTS data to generalize to S2ST."
  - [section]: "We incorporate a contrastive learning objective to align representations. This objective maximizes the similarity between representations from the same input pairs and minimizes the similarity between representations from different input pairs."
  - [corpus]: Weak—neighbors focus on direct S2ST but do not describe contrastive alignment in zero-shot settings.
- Break condition: If the contrastive objective overfits to synthetic TTS pairs, it will not generalize to real speech inputs, breaking zero-shot capability.

### Mechanism 3
- Claim: Pretraining on S2TT and TTS data before S2ST finetuning significantly improves translation quality.
- Mechanism: S2TT pretraining provides strong translation priors, while TTS pretraining gives robust speech synthesis; both reduce the burden on limited S2ST data.
- Core assumption: The compositional model can effectively reuse independently trained components without catastrophic forgetting.
- Evidence anchors:
  - [abstract]: "ComSpeech comprises three modules: F, A, and G, where F denotes arbitrary S2TT model, G denotes arbitrary TTS model for the target language..."
  - [section]: "ComSpeech supports S2TT and TTS pretraining. We explore the impact of pretraining in both supervised and zero-shot learning scenarios."
  - [corpus]: Weak—no neighbor explicitly discusses staged pretraining for S2ST.
- Break condition: If pretraining objectives conflict, the combined model may lose either translation or synthesis quality during finetuning.

## Foundational Learning

- Concept: CTC (Connectionist Temporal Classification)
  - Why needed here: CTC provides a principled way to align variable-length sequences between different vocabularies without explicit segmentation.
  - Quick check question: How does CTC handle blank tokens when mapping between subword and phoneme sequences?

- Concept: Contrastive learning
  - Why needed here: Contrastive objectives encourage the model to produce similar embeddings for equivalent inputs across modalities, enabling zero-shot transfer.
  - Quick check question: What is the role of the temperature hyperparameter τ in the contrastive loss?

- Concept: Two-stage fine-tuning
  - Why needed here: Separating S2TT adaptation from joint S2TT+TTS training stabilizes learning and prevents early interference between tasks.
  - Quick check question: Why might joint fine-tuning from scratch hurt performance compared to a staged approach?

## Architecture Onboarding

- Component map: S2TT model (F) → Vocabulary Adaptor (A) → TTS Encoder (Genc) → TTS Decoder (Gdec) → Vocoder
- Critical path: Source speech → Fenc → Fdec → A → Genc → Gdec → waveform
- Design tradeoffs:
  - Using a strong TTS model improves naturalness but may slow decoding vs. discrete unit approaches.
  - Larger adaptor depth improves alignment but increases latency and overfitting risk.
- Failure signatures:
  - ASR-BLEU drops sharply → vocabulary adaptor misalignment.
  - Speech quality degrades → TTS pretraining insufficient or mismatched vocabulary.
  - Training instability → learning rate too high during fine-tuning.
- First 3 experiments:
  1. Train with only MSE alignment loss (no contrastive) to measure impact of contrastive learning.
  2. Remove pretraining entirely and train end-to-end to see pretraining benefit.
  3. Swap in a weaker TTS model to confirm synthesis bottleneck.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ComSpeech-ZS scale with extremely large amounts of TTS data beyond the 191 hours tested?
- Basis in paper: [explicit] The paper mentions that ComSpeech-ZS's performance improves with increased TTS data size, but only tests up to 191 hours.
- Why unresolved: The experiments did not explore the upper bounds of TTS data scaling on ComSpeech-ZS performance.
- What evidence would resolve it: Additional experiments with TTS data sizes significantly larger than 191 hours, measuring ASR-BLEU scores to determine performance saturation points.

### Open Question 2
- Question: Can the vocabulary adaptor be effectively used to connect more powerful non-autoregressive S2TT models with TTS models to improve ComSpeech's performance?
- Basis in paper: [explicit] The paper mentions this as a theoretical possibility for future research.
- Why unresolved: The current implementation uses a specific S2TT model and doesn't explore the potential of connecting more advanced non-autoregressive models.
- What evidence would resolve it: Experiments comparing ComSpeech's performance when using different types of S2TT models (autoregressive vs. non-autoregressive) while keeping the same TTS model.

### Open Question 3
- Question: What are the specific limitations of the current contrastive learning approach in aligning representations between different vocabularies, and how can these be addressed?
- Basis in paper: [inferred] The paper shows that contrastive learning is crucial for zero-shot learning but doesn't provide a detailed analysis of its limitations or potential improvements.
- Why unresolved: The paper focuses on the effectiveness of contrastive learning but doesn't delve into its shortcomings or possible enhancements.
- What evidence would resolve it: A detailed analysis of contrastive learning's performance in various scenarios, including cases where it fails or produces suboptimal results, along with proposed solutions to these limitations.

## Limitations

- Experimental validation is confined to the CVSS dataset with only three language pairs, limiting cross-linguistic generalizability
- No quantitative speed measurements are provided to verify claims about decoding speed improvements over previous models
- The 0.7 ASR-BLEU gap in zero-shot learning represents measurable performance degradation that may not be acceptable for all production applications
- The paper does not address robustness to real-world conditions such as noisy speech input, speaker variation, or domain mismatch

## Confidence

- **High confidence** in the core technical contribution: The vocabulary adaptor mechanism based on CTC alignment is technically sound and the experimental results showing improvement over cascaded systems are well-supported by quantitative metrics (ASR-BLEU and BLASER 2.0 scores).
- **Medium confidence** in the zero-shot learning claims: While the contrastive learning approach is theoretically justified and shows promising results, the evaluation is limited to a single dataset, and the 0.7 ASR-BLEU gap suggests some performance degradation that may not be acceptable for all use cases.
- **Low confidence** in decoding speed claims: The paper states that ComSpeech is faster than previous two-pass models but provides no concrete measurements or methodology for speed evaluation, making independent verification difficult.

## Next Checks

1. **Cross-dataset validation**: Evaluate ComSpeech on multilingual speech translation benchmarks beyond CVSS (such as CoVoST 2 or Europarl-ST) to assess generalization across different domains and language pairs.

2. **Speed benchmarking**: Implement and measure actual inference latency of ComSpeech compared to Translatotron 2 and cascaded systems using standardized hardware and batch sizes to verify the claimed speed improvements.

3. **Robustness testing**: Assess model performance under realistic conditions including noisy speech inputs, speaker variation, and out-of-domain text to determine practical deployment viability beyond controlled experimental settings.