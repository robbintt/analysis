---
ver: rpa2
title: Weber-Fechner Law in Temporal Difference learning derived from Control as Inference
arxiv_id: '2412.21004'
source_url: https://arxiv.org/abs/2412.21004
tags:
- learning
- page
- value
- which
- control
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper uncovers a nonlinear update rule in temporal difference
  (TD) learning derived from the Control as Inference framework, revealing the Weber-Fechner
  Law (WFL). WFL describes how the update magnitude in RL depends logarithmically
  on the value function scale: updates are more sensitive when values are small and
  less sensitive when values are large.'
---

# Weber-Fechner Law in Temporal Difference learning derived from Control as Inference

## Quick Facts
- **arXiv ID**: 2412.21004
- **Source URL**: https://arxiv.org/abs/2412.21004
- **Reference count**: 10
- **Primary result**: Uncovers Weber-Fechner Law in TD learning, showing nonlinear updates that accelerate early reward maximization and suppress punishments

## Executive Summary
This paper derives a novel temporal difference learning rule from the Control as Inference framework that exhibits the Weber-Fechner Law (WFL). WFL describes how update magnitudes scale logarithmically with value function intensity, being more sensitive for small values and less sensitive for large values. The authors implement this practically through a reward-punishment framework that separates positive rewards and punishments, enabling computable updates. Experiments demonstrate WFL's ability to accelerate reward learning early and suppress punishments during training, improving performance on pendulum control and robotic valve-turning tasks.

## Method Summary
The method implements WFL by modifying the TD update rule with a nonlinear term δln = ln(1-pV) - ln(1-pQ) that depends on value function bounds. A reward-punishment framework splits rewards into positive (r+) and negative (r-) components, with separate value functions V+ and V- for each. The update rule weights the TD error by (1-λβ)δ + λβδln, where β controls WFL activation. Asymmetric β+,−0 parameters tune WFL strength for rewards versus punishments. The system uses PPO-RPE for policy optimization with experience replay and target networks for stability.

## Key Results
- WFL-RP outperforms conventional methods on pendulum control, achieving faster reward maximization
- Asymmetric β tuning enables targeted acceleration of reward learning while suppressing punishments
- On D'Claw valve-turning task, WFL variants achieve higher success rates than baseline methods
- WFL's logarithmic scaling matches biological perception patterns in reinforcement learning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The nonlinear term δln = ln(1 - pV) - ln(1 - pQ) encodes Weber-Fechner Law, causing perception updates to scale logarithmically with value function magnitude.
- **Mechanism**: When λβ is large (β is small), δln dominates, growing slowly for larger V and Q values. This mimics biological WFL where perception is less sensitive to large stimuli, causing updates to shrink when values are far from bounds and amplify when close.
- **Core assumption**: Taylor expansion around bounds is valid when V and Q are near those bounds
- **Evidence anchors**: Taylor expansion shows δln ∝ -ln((R-Q)/(R-V)), matching WFL structure; no corpus evidence linking WFL to RL TD updates
- **Break condition**: If Taylor expansion is invalid (V and Q far from bounds) or β is too large (λβ→0), WFL is suppressed

### Mechanism 2
- **Claim**: Reward-punishment framework enables δln computation by assigning known bounds to separated reward streams.
- **Mechanism**: For punishments (r-≤0), upper bound 0 is known, making pV and pQ computable. For rewards (r+≥0), lower bound 0 is used with inverted optimality definition, again enabling δln computation. This allows independent WFL application.
- **Core assumption**: Environment can be reformulated to output r+ and r- separately
- **Evidence anchors**: Reward-punishment framework described in Kobayashi et al. (2019) and Wang et al. (2021); no corpus evidence for this specific framework
- **Break condition**: If environment cannot separate rewards/punishments or bounds are unknown/unreliable, δln cannot be computed

### Mechanism 3
- **Claim**: Asymmetric β+,−0 tuning controls WFL strength on rewards vs. punishments for targeted learning effects.
- **Mechanism**: Small β+0 makes WFL more active for rewards, speeding early reward maximization. Small β-0 makes WFL more active for punishments, suppressing temporary punishment during learning. This matches experimental results showing balanced exploration and safety.
- **Core assumption**: Optimal β+,−0 can be tuned per task
- **Evidence anchors**: Fig. 4 and 5 show β+,−0 tuning affects learning speed and punishment suppression; no corpus evidence for this specific asymmetric tuning
- **Break condition**: If β+,−0 too small, exploration may be overly suppressed; if too large, WFL effects vanish

## Foundational Learning

- **Concept: Temporal Difference (TD) Learning**
  - Why needed here: Builds WFL into standard TD update rule by modifying TD error weighting term
  - Quick check question: In TD learning, what is the TD error δ defined as? (Answer: δ = r + γV(s') - V(s))

- **Concept: Control as Inference Framework**
  - Why needed here: Derives standard TD rule from this framework, then identifies where uncomputable nonlinear term δln arises
  - Quick check question: In control as inference, how is the optimality variable O related to the value function? (Answer: p(O=1|s) = e^{β(V(s)-R)})

- **Concept: Reward-Punishment Framework**
  - Why needed here: Splits scalar rewards into positive and negative streams, enabling separate δln computation with known bounds
  - Quick check question: In reward-punishment framework, what are the bounds on r+ and r-? (Answer: r+ ≥ 0, r- ≤ 0)

## Architecture Onboarding

- **Component map**: Experience replay buffer -> Sample (s,a,r+,r-,s') -> Compute δ+, δ- -> Compute δln+, δln- -> Weight gradients by (1-λβ)δ + λβδln -> Update V+, V-, π+, π- with AdaTerm -> Update target networks and β+,−

- **Critical path**:
  1. Sample experience (s, a, r+, r-, s')
  2. Compute TD errors δ+ = r+ + γV+(s') - V+(s), δ- = r- + γV-(s') - V-(s)
  3. Compute δln+ = -ln(1-pV+) + ln(1-pQ+), δln- = -ln(1-pV-) + ln(1-pQ-)
  4. Weight gradients by (1-λβ)δ + λβδln for each stream
  5. Update V+, V-, π+, π- with AdaTerm
  6. Update target networks and β+,−

- **Design tradeoffs**:
  - Splitting rewards into r+, r- increases model complexity but enables WFL computation
  - Asymmetric β+,−0 allows fine-grained control but adds hyperparameter tuning burden
  - Experience replay improves sample efficiency but may slow on-policyness
  - Target networks stabilize learning but introduce lag

- **Failure signatures**:
  - Learning collapse: check if β+,− are too small or reward-punishment separation is broken
  - Delayed reward learning: check if β+0 is too small or WFL over-suppresses exploration
  - Persistent punishments: check if β-0 is too large or WFL not active enough
  - Unstable β: check ζ adaptation rate or scale estimation

- **First 3 experiments**:
  1. Pendulum-v0 with known bounds, test β+,−0 ∈ {0.1, 1, 10, ∞} to observe WFL activation
  2. Pendulum-v0 with asymmetric β+,−0 to balance reward learning and punishment suppression
  3. D'Claw simulation with WFL-R, WFL-P, WFL-RP to compare effects on rewards vs. punishments

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does Weber-Fechner Law exist in biological dopamine signals during learning tasks?
- **Basis in paper**: [explicit] The paper suggests WFL might be present in biological learning based on biological plausibility and activation under uncertain optimality, similar to organisms' decision-making conditions
- **Why unresolved**: While WFL's existence in TD learning is established, it has not been experimentally verified in biological systems
- **What evidence would resolve it**: Neuroimaging studies or neural recordings showing dopamine signal patterns matching WFL predictions during reinforcement learning tasks in animals

### Open Question 2
- **Question**: What is the optimal reward-punishment separation strategy for WFL-based RL algorithms?
- **Basis in paper**: [inferred] The paper shows WFL's behavior depends on whether applied to rewards (r+) or punishments (r-), but notes complexity of determining appropriate separation and prioritization among multiple objectives
- **Why unresolved**: The paper demonstrates separating rewards and punishments affects learning, but doesn't provide systematic method for optimal separation or handling multiple objectives
- **What evidence would resolve it**: Comparative studies showing performance across different reward-punishment separation strategies on diverse tasks, along with theoretical analysis of optimal separation criteria

### Open Question 3
- **Question**: How does WFL interact with exploration-exploitation trade-offs in RL?
- **Basis in paper**: [inferred] The paper mentions WFL's sensitivity to small rewards might suppress exploration, but doesn't systematically investigate this interaction or provide solutions
- **Why unresolved**: While the paper observes exploration suppression as WFL side effect, it doesn't analyze underlying mechanisms or propose methods to balance WFL's effects with exploration needs
- **What evidence would resolve it**: Empirical studies measuring exploration rates under WFL versus conventional methods, and theoretical analysis of how WFL's logarithmic scaling affects exploration-exploitation balance

## Limitations

- WFL update rule has limited validation beyond pendulum and robotic valve-turning tasks
- Taylor expansion assumptions require close proximity to value bounds, which may not hold in all RL environments
- Reward-punishment framework depends on environment-specific reformulation capabilities not universally applicable

## Confidence

- **High confidence**: Mathematical derivation showing WFL emerges from Control as Inference framework is sound with clear equations and Taylor expansions
- **Medium confidence**: Reward-punishment implementation and asymmetric β tuning work as described in tested environments
- **Low confidence**: Generalizability to diverse RL tasks beyond pendulum and valve-turning, particularly those without clear reward-punishment separation

## Next Checks

1. Test WFL-RP on a sparse-reward environment (e.g., MountainCar) where value functions frequently approach bounds to verify Taylor expansion validity across full state space

2. Implement WFL with single value function (no reward-punishment separation) to isolate whether WFL effects require the framework or can emerge from simpler formulations

3. Conduct ablation studies systematically varying β+,−0 across orders of magnitude to precisely map boundary between WFL activation and suppression effects on exploration-exploitation tradeoff