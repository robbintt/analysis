---
ver: rpa2
title: 'KnowledgePrompts: Exploring the Abilities of Large Language Models to Solve
  Proportional Analogies via Knowledge-Enhanced Prompting'
arxiv_id: '2412.00869'
source_url: https://arxiv.org/abs/2412.00869
tags:
- knowledge
- prompting
- prompt
- language
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a 15K MCQ dataset for proportional analogy
  completion and evaluates nine GenAI models using knowledge-enhanced prompting techniques.
  The study tests zero-shot, few-shot, structured knowledge, and targeted knowledge
  prompting.
---

# KnowledgePrompts: Exploring the Abilities of Large Language Models to Solve Proportional Analogies via Knowledge-Enhanced Prompting

## Quick Facts
- arXiv ID: 2412.00869
- Source URL: https://arxiv.org/abs/2412.00869
- Reference count: 40
- Best model achieves 55% accuracy on proportional analogy completion

## Executive Summary
This paper introduces a 15K MCQ dataset for proportional analogy completion and evaluates nine GenAI models using knowledge-enhanced prompting techniques. The study tests zero-shot, few-shot, structured knowledge, and targeted knowledge prompting. The best model achieves 55% accuracy, with targeted knowledge prompting outperforming other approaches by up to 21% compared to zero-shot and 45% compared to structured knowledge. The results show that providing specific semantic relationships and cognitive reasoning steps significantly improves model performance, while simply adding structured knowledge from external sources does not consistently enhance results. The findings indicate that proportional analogy completion remains a challenging task for current LLMs, but targeted knowledge enhancement offers a promising approach for improvement.

## Method Summary
The study evaluates nine large language models on a 15K proportional analogy multiple-choice question dataset. Six prompting techniques are tested: zero-shot, one-shot, five-shot, structured knowledge prompting (SKP), and targeted knowledge prompting (TKP). Models are configured with temperature=0.1, top_p=0.1, and repetition_penalty=1.2. External knowledge sources include Wikidata, ConceptNet, and WordNet. Performance is measured using Exact Match Accuracy (EMA) across all models and prompting techniques.

## Key Results
- Best model (GPT-3.5-Turbo) achieves 55.25% EMA using Targeted Knowledge Prompting
- Targeted Knowledge Prompting outperforms zero-shot by up to 21% and structured knowledge by up to 45%
- Structured Knowledge Prompting does not consistently improve performance due to potential noise from irrelevant knowledge paths
- Increasing few-shot exemplars from one to five decreases EMA in six out of nine models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Targeted knowledge prompting significantly improves proportional analogy completion by providing models with explicit semantic relationships and reasoning steps.
- Mechanism: TKP enhances prompts with the specific semantic relationship between the question term pair (e.g., "made of") and a cognitive process description, guiding the model's reasoning.
- Core assumption: LLMs struggle with proportional analogies due to the lack of explicit semantic relationship identification and reasoning steps.
- Evidence anchors:
  - [abstract] "Our results show that despite extensive training data, solving proportional analogies remains challenging for current LLMs, with the best model achieving an accuracy of 55%. Notably, we find that providing targeted knowledge can better assist models in completing proportional analogies compared to providing exemplars or collections of structured knowledge."
  - [section 5.1] "The highest overall performance was attained by GPT-3.5-Turbo, achieving an EMA of 55.25%. This result underscores the challenge that proportional analogies pose for current state-of-the-art GenAI models. This accuracy was obtained through Targeted Knowledge Prompting where the prompt was enhanced with targeted knowledge."
- Break condition: If the semantic relationship is not correctly identified or the cognitive process description is not aligned with the analogy structure, TKP may not improve performance.

### Mechanism 2
- Claim: Structured knowledge prompting does not consistently improve model performance due to potential noise and irrelevance of retrieved knowledge paths.
- Mechanism: SKP retrieves and integrates knowledge paths from external sources (Wikidata, ConceptNet, WordNet) into prompts, but the retrieved paths may not be directly relevant to the analogy completion task.
- Core assumption: The retrieved knowledge paths are not always semantically similar to the term pairs in the question and answer choices, leading to noise and confusion for the model.
- Evidence anchors:
  - [abstract] "Our findings indicate that completing proportional analogies is highly challenging for current LLMs and incorporating targeted knowledge significantly enhances model performance, with the best-performing model showing an improvement of approximately +21% compared to prompts without any knowledge, and around +45% relative to prompts enhanced with structured knowledge."
  - [section 5.2] "Although enhancing prompts with structured knowledge does not consistently improve model performance compared to other prompting techniques, SKP[semantic] leads to slight increases in EMA values (ranging from 0.01% to 1.32%) compared to SKP[random], across all models except GPT-2 and Mistral."
- Break condition: If the knowledge filtering mechanism is improved to ensure only relevant knowledge paths are included, SKP may potentially improve model performance.

### Mechanism 3
- Claim: Few-shot prompting with exemplars does not consistently improve model performance, and increasing the number of exemplars can even decrease accuracy.
- Mechanism: Few-shot prompting provides the model with one or five exemplars from the dataset to demonstrate the task, but the exemplars may not be semantically similar to the question, leading to confusion or distraction.
- Core assumption: The exemplars provided are not always the most relevant or helpful for the specific analogy completion task.
- Evidence anchors:
  - [section 5.3] "Brown (2020) demonstrated that the accuracy of large language models improves with an increase in the number of exemplars. However, Liu et al. (2022) found that the benefits diminish beyond 20 exemplars in certain cases. Similarly, in our study, increasing exemplars from one to five decreases EMA in six out of nine models (see Table 2), leading us to limit exemplars to a maximum of five."
  - [section 3.3.2] "We employ one-shot and five-shot prompting under the few-shot prompting strategy where one example and five examples are provided respectively. We select these quantities of exemplars to strike a balance between the models' maximum accepted context length and the computational resources required."
- Break condition: If the exemplar selection mechanism is improved to ensure only the most relevant and helpful exemplars are provided, few-shot prompting may potentially improve model performance.

## Foundational Learning

- Concept: Proportional analogies and their structure (A:B::C:D)
  - Why needed here: Understanding the structure of proportional analogies is crucial for developing effective prompting techniques and evaluating model performance on this task.
  - Quick check question: Given the analogy "Oxygen is to Gas as Aluminum is to Metal," what is the semantic relationship between "Oxygen" and "Gas"?
- Concept: Knowledge-enhanced prompting techniques (Zero-shot, Few-shot, Structured Knowledge, Targeted Knowledge)
  - Why needed here: Familiarity with different knowledge-enhanced prompting techniques is essential for understanding the study's approach and interpreting the results.
  - Quick check question: What is the difference between Structured Knowledge Prompting and Targeted Knowledge Prompting?
- Concept: Evaluation metrics for multiple-choice question answering (EMA)
  - Why needed here: Understanding the evaluation metric used in the study (Exact Match Accuracy) is crucial for interpreting the results and comparing model performance.
  - Quick check question: How is Exact Match Accuracy calculated for multiple-choice question answering tasks?

## Architecture Onboarding

- Component map: Dataset creation -> Model selection -> Prompting techniques -> Knowledge sources -> Evaluation metric
- Critical path:
  1. Create dataset of proportional analogy MCQs
  2. Select and configure GenAI models
  3. Implement prompting techniques (Zero-shot, Few-shot, SKP, TKP)
  4. Integrate knowledge sources (Wikidata, ConceptNet, WordNet)
  5. Evaluate model performance using EMA
  6. Analyze results and draw conclusions
- Design tradeoffs:
  - Prompt length vs. model performance (longer prompts may contain more information but can also lead to confusion)
  - Knowledge source selection (different sources may provide varying levels of relevance and noise)
  - Exemplar selection (more exemplars may provide better demonstration but can also lead to distraction)
- Failure signatures:
  - Low EMA across all models and prompting techniques (indicating the task is too challenging for current LLMs)
  - SKP consistently outperforming TKP (suggesting that the knowledge integration mechanism is more effective than the targeted knowledge approach)
  - Few-shot prompting consistently outperforming zero-shot prompting (indicating that the exemplars provided are highly relevant and helpful)
- First 3 experiments:
  1. Evaluate all 9 models using zero-shot prompting on the 15K dataset and calculate EMA for each model.
  2. Implement and evaluate one-shot and five-shot prompting techniques for all models and compare EMA with zero-shot prompting.
  3. Integrate structured knowledge from Wikidata, ConceptNet, and WordNet into prompts and evaluate the impact on model performance using EMA.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of knowledge-enhanced prompting techniques vary across different model sizes and architectures?
- Basis in paper: [explicit] The paper evaluates nine different models, including various sizes and architectures (e.g., Falcon-7B, FlanT5-XXL, GPT-3.5-Turbo)
- Why unresolved: The paper provides overall performance results but does not specifically analyze how model size and architecture impact the effectiveness of different prompting techniques.
- What evidence would resolve it: Detailed analysis comparing performance trends across model families (e.g., decoder-only vs. encoder-decoder) and size ranges (e.g., 7B vs. 11B+ parameters) for each prompting technique.

### Open Question 2
- Question: What is the optimal balance between structured knowledge sources (WordNet, ConceptNet, Wikidata) for proportional analogy completion?
- Basis in paper: [explicit] The paper experiments with individual and combined use of three knowledge sources but finds varying results across models
- Why unresolved: While the paper shows that combining all three sources doesn't consistently improve performance, it doesn't determine the optimal combination or weight distribution for different types of analogies.
- What evidence would resolve it: Systematic experiments varying the inclusion/exclusion and weighting of each knowledge source across different analogy types and model architectures.

### Open Question 3
- Question: How does the performance of knowledge-enhanced prompting scale with analogy complexity and diversity?
- Basis in paper: [inferred] The paper introduces a dataset with 238 distinct relation types but doesn't analyze performance variation across complexity levels
- Why unresolved: The paper provides aggregate performance metrics but doesn't investigate how different types of semantic relationships or analogy complexities affect prompting effectiveness.
- What evidence would resolve it: Performance analysis stratified by relation type complexity, dataset size, and analogy difficulty levels for each prompting technique.

## Limitations
- The knowledge integration mechanism in structured knowledge prompting may introduce noise from irrelevant knowledge paths
- Few-shot prompting effectiveness is inconsistent, with increased exemplars sometimes decreasing performance
- 55% accuracy ceiling suggests proportional analogy completion remains challenging for current LLMs

## Confidence

**High Confidence**: Targeted knowledge prompting significantly outperforms zero-shot and structured knowledge prompting (up to +21% and +45% respectively)

**Medium Confidence**: Structured knowledge prompting does not consistently improve performance due to potential noise from irrelevant knowledge paths

**Low Confidence**: Generalizability of the 55% accuracy ceiling to other proportional analogy datasets or domains

## Next Checks
1. Implement and test an explicit knowledge filtering mechanism for structured knowledge prompting that validates semantic similarity between retrieved knowledge paths and analogy term pairs, then re-evaluate model performance.

2. Design a controlled experiment varying exemplar quality (selecting only highly relevant analogies vs. random selection) while keeping quantity constant, to isolate the impact of exemplar relevance on few-shot prompting effectiveness.

3. Evaluate the same prompting techniques and models on at least two additional proportional analogy datasets from different domains to assess whether the 55% accuracy ceiling and TKP effectiveness generalize beyond the original dataset.