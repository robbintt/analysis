---
ver: rpa2
title: 'MERaLiON-SpeechEncoder: Towards a Speech Foundation Model for Singapore and
  Beyond'
arxiv_id: '2412.11538'
source_url: https://arxiv.org/abs/2412.11538
tags:
- speech
- training
- pre-training
- dataset
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The MERaLiON-SpeechEncoder is a 630M parameter speech foundation
  model pre-trained from scratch on 200,000 hours of unlabelled speech data using
  a self-supervised learning approach based on masked language modelling. The model
  was developed as part of Singapore's National Multimodal Large Language Model Programme
  to address speech processing needs in Singapore and Southeast Asia, with initial
  focus on English including Singapore-accented English and Singlish.
---

# MERaLiON-SpeechEncoder: Towards a Speech Foundation Model for Singapore and Beyond

## Quick Facts
- **arXiv ID**: 2412.11538
- **Source URL**: https://arxiv.org/abs/2412.11538
- **Reference count**: 15
- **Primary result**: 630M parameter speech foundation model achieving 2.1% WER on Librispeech test-clean

## Executive Summary
The MERaLiON-SpeechEncoder is a 630M parameter speech foundation model developed as part of Singapore's National Multimodal Large Language Model Programme. Pre-trained from scratch on 200,000 hours of unlabelled speech data, the model employs a self-supervised learning approach based on masked language modelling to address speech processing needs in Singapore and Southeast Asia, with initial focus on English including Singapore-accented English and Singlish. The model demonstrates strong performance across multiple speech processing tasks while maintaining particular strength in Singapore English and Singlish recognition.

## Method Summary
The model uses BEST-RQ (BERT-based speech pre-training with random-projection quantizer), which simplifies target computation by using random projections of input features mapped to codebook vectors, avoiding iterative k-means clustering. The architecture employs 24 Conformer layers with relative positional embeddings and was trained using a masking probability of 0.4 with 32 independent codebooks each with 2048 vocabulary size. The model was pre-trained on 200,000 hours of unlabelled speech data and evaluated across multiple benchmarks including Librispeech, TEDLIUMv3, NSC subset, and SUPERB.

## Key Results
- Achieved 2.1% WER on Librispeech test-clean and 4.3% WER test-other with 960 hours finetuning
- Significantly improved spontaneous speech recognition with 5.6% WER on TEDLIUMv3
- Strong performance on Singapore English/Singlish with 15.2% average WER on NSC subset
- Competitive SUPERB benchmark score of 82.62, comparable to HuBERT large and approaching WavLM large performance

## Why This Works (Mechanism)
The model's effectiveness stems from its foundation in self-supervised learning through masked language modelling, which enables learning rich speech representations without requiring labeled data. The BEST-RQ quantization approach provides efficient and effective speech encoding by mapping continuous features to discrete tokens through random projections, avoiding the computational complexity of iterative clustering methods. The Conformer architecture combines convolutional neural networks with self-attention mechanisms, capturing both local and global speech patterns effectively.

## Foundational Learning
- **Self-supervised learning**: Needed for training on unlabelled speech data; quick check: model can learn meaningful representations without transcriptions
- **Masked language modelling**: Essential for speech pre-training objectives; quick check: model learns to predict masked segments from context
- **Random-projection quantization**: Required for efficient discrete representation of continuous speech features; quick check: random projections capture meaningful speech patterns
- **Conformer architecture**: Combines CNN locality with transformer attention for speech processing; quick check: model handles both fine-grained and global speech patterns
- **Relative positional embeddings**: Critical for maintaining temporal relationships in speech; quick check: model preserves speech sequence information
- **Codebook-based token representation**: Enables discrete speech unit modeling; quick check: codebook vectors capture meaningful speech distinctions

## Architecture Onboarding

**Component map:**
Input speech features -> BEST-RQ quantization -> 24 Conformer layers -> Output representations

**Critical path:**
Speech input → Feature extraction → Random-projection quantization → Masked prediction task → Conformer processing → Downstream task adaptation

**Design tradeoffs:**
- BEST-RQ vs iterative quantization: Simpler computation vs potential accuracy loss
- 24 Conformer layers: Depth for representation learning vs computational efficiency
- 32 codebooks with 2048 vocab: Granularity of speech representation vs model capacity
- Masking probability 0.4: Balance between context preservation and prediction challenge

**Failure signatures:**
- Over-reliance on random projections may miss structured speech patterns
- Insufficient masking may lead to shallow representations
- Limited codebook diversity may fail to capture speech nuances
- Conformer depth may cause optimization difficulties

**3 first experiments:**
1. Test BEST-RQ quantization quality by comparing random projections to learned projections on a small dataset
2. Validate masking effectiveness by training with different masking probabilities (0.3, 0.4, 0.5)
3. Assess codebook sufficiency by varying codebook count (16, 32, 64) and vocabulary size (1024, 2048, 4096)

## Open Questions the Paper Calls Out
None

## Limitations
- Performance on Singapore English and Singlish (15.2% WER on NSC subset) may not fully capture linguistic diversity within Singapore's multilingual context
- Spontaneous speech evaluation on TEDLIUMv3 may not represent real-world conversational patterns
- SUPERB benchmark score of 82.62 still lags behind WavLM large (85.25), suggesting potential gaps in certain speech processing tasks

## Confidence
- **High confidence**: Technical architecture and training methodology (BEST-RQ with Conformer layers, masking strategy) are well-documented and align with established approaches
- **Medium confidence**: Performance claims on Singapore English/Singlish are based on NSC subset, which may not represent full linguistic variation
- **Medium confidence**: Comparisons to HuBERT and WavLM models depend on specific implementation details not fully detailed

## Next Checks
1. Evaluate the model on additional Singapore English datasets with diverse speaker demographics and conversational contexts to verify generalizability
2. Conduct ablation studies comparing BEST-RQ with alternative quantization methods to quantify the impact of random-projection approach
3. Test the model's zero-shot performance on related Southeast Asian languages and dialects to assess cross-linguistic capabilities beyond English