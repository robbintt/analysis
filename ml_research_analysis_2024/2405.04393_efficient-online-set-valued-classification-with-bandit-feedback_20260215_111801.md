---
ver: rpa2
title: Efficient Online Set-valued Classification with Bandit Feedback
arxiv_id: '2405.04393'
source_url: https://arxiv.org/abs/2405.04393
tags: []
core_contribution: This paper tackles the challenge of conformal prediction in online
  multi-class classification under bandit feedback, where only the correctness of
  predictions (not the true labels) is observed. The authors propose Bandit Class-specific
  Conformal Prediction (BCCP), which leverages an unbiased estimator of the true label
  indicator function to update the model and quantile estimates via stochastic gradient
  descent.
---

# Efficient Online Set-valued Classification with Bandit Feedback

## Quick Facts
- arXiv ID: 2405.04393
- Source URL: https://arxiv.org/abs/2405.04393
- Authors: Zhou Wang; Xingye Qiao
- Reference count: 40
- Key outcome: This paper tackles the challenge of conformal prediction in online multi-class classification under bandit feedback, where only the correctness of predictions (not the true labels) is observed.

## Executive Summary
This paper addresses the problem of conformal prediction in online multi-class classification under bandit feedback, where only the correctness of predictions is observed, not the true labels. The authors propose Bandit Class-specific Conformal Prediction (BCCP), which leverages an unbiased estimator of the true label indicator function to update the model and quantile estimates via stochastic gradient descent. This allows efficient use of all data instances, even those where incorrect predictions are made, overcoming the sparsity of labeled data in bandit settings. Theoretical results show convergence of coverage and regret at O(T^(-1/2)) rates under certain conditions. Experiments on CIFAR10, CIFAR100, and SVHN demonstrate that BCCP effectively achieves the desired class-specific coverage with competitive prediction set sizes, outperforming baseline methods, particularly when using the RAPS score function.

## Method Summary
The paper introduces Bandit Class-specific Conformal Prediction (BCCP), a method for conformal prediction in online multi-class classification under bandit feedback. BCCP uses an unbiased estimator of the true label indicator function to update the model and quantile estimates via stochastic gradient descent. This approach allows the method to efficiently utilize all data instances, even those where incorrect predictions are made, addressing the challenge of sparsity in bandit settings. The method leverages the RAPS score function and includes an ensemble approach to address the challenge of selecting an optimal learning rate for quantile estimation. Theoretical analysis proves convergence of coverage and regret at O(T^(-1/2)) rates under certain conditions.

## Key Results
- BCCP achieves desired class-specific coverage with competitive prediction set sizes on CIFAR10, CIFAR100, and SVHN datasets.
- The method outperforms baseline approaches, particularly when using the RAPS score function.
- Theoretical analysis shows convergence of coverage and regret at O(T^(-1/2)) rates under specific conditions.

## Why This Works (Mechanism)
BCCP works by leveraging an unbiased estimator of the true label indicator function, which allows the model to update using all data instances, not just correctly classified ones. This is crucial in bandit settings where labeled data is sparse. The method uses stochastic gradient descent to update both the model and quantile estimates, enabling efficient online learning. The RAPS score function helps in achieving better class-specific coverage, while the ensemble approach for learning rate selection addresses a key challenge in quantile estimation.

## Foundational Learning
- **Conformal prediction**: A method for producing prediction sets with valid coverage guarantees. Needed to ensure reliable uncertainty quantification in classification tasks.
- **Bandit feedback**: A learning scenario where only the correctness of predictions is observed, not the true labels. Crucial for this work as it addresses the challenge of limited information in online settings.
- **Unbiased estimator of true label indicator**: A technique to estimate true labels from bandit feedback. Essential for updating the model using all data instances, not just correctly classified ones.
- **Stochastic gradient descent**: An optimization algorithm for updating model parameters. Used here for efficient online learning and quantile estimation.
- **RAPS score function**: A scoring function used in conformal prediction. Chosen for its effectiveness in achieving class-specific coverage.
- **Ensemble learning**: A technique combining multiple models to improve performance. Applied here to address the challenge of learning rate selection for quantile estimation.

## Architecture Onboarding
**Component Map**: Data instances -> BCCP model (with RAPS score function) -> Prediction sets -> Bandit feedback -> Unbiased label estimator -> Model update -> Quantile estimation -> Updated BCCP model

**Critical Path**: Data instance reception -> Prediction set generation -> Bandit feedback collection -> Unbiased label estimation -> Stochastic gradient descent update -> Quantile estimation update -> Next prediction cycle

**Design Tradeoffs**: The use of RAPS score function improves class-specific coverage but may increase computational complexity. The ensemble approach for learning rate selection adds robustness but increases model complexity and memory requirements.

**Failure Signatures**: Poor coverage rates may indicate issues with the unbiased label estimator or quantile estimation. Large prediction set sizes could suggest suboptimal score function selection or learning rate issues.

**First Experiments**:
1. Verify coverage rates on CIFAR10 dataset with varying levels of bandit feedback sparsity.
2. Compare prediction set sizes using different score functions (e.g., RAPS vs. baseline) on CIFAR100.
3. Evaluate the impact of ensemble learning rate selection on convergence speed and final performance on SVHN dataset.

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical analysis relies on specific assumptions about loss functions and the use of RAPS score functions, which may not hold in all practical scenarios.
- The convergence rates of O(T^(-1/2)) for coverage and regret are proven under certain conditions, but the paper does not extensively explore the robustness of these results when assumptions are violated.
- The ensemble approach for learning rate selection is introduced but not thoroughly validated, leaving questions about its effectiveness in diverse settings.

## Confidence
Theoretical guarantees (Medium): The convergence rates and regret bounds are derived under specific assumptions, but their applicability to all practical scenarios is uncertain. More extensive empirical validation is needed to confirm these theoretical results.

Empirical performance (Medium): The experimental results on CIFAR10, CIFAR100, and SVHN are promising, but the evaluation is limited to a few datasets. Testing on a broader range of datasets and more complex models would strengthen the claims.

## Next Checks
1. Conduct experiments on additional diverse datasets (e.g., ImageNet, medical imaging datasets) to assess the generalizability of BCCP across different domains and task complexities.

2. Perform ablation studies to evaluate the impact of the ensemble approach for learning rate selection on both coverage and prediction set sizes, comparing it to alternative methods.

3. Investigate the scalability of BCCP in terms of computational efficiency and memory usage when applied to large-scale online learning scenarios with high-dimensional data.