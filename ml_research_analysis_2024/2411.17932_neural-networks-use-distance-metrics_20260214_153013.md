---
ver: rpa2
title: Neural Networks Use Distance Metrics
arxiv_id: '2411.17932'
source_url: https://arxiv.org/abs/2411.17932
tags:
- intensity
- distance
- neural
- features
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether neural networks learn distance-based
  or intensity-based representations through systematic perturbation analysis. Using
  MNIST dataset, the authors independently manipulate distance and intensity properties
  of network activations in trained models with ReLU and Absolute Value activation
  functions.
---

# Neural Networks Use Distance Metrics

## Quick Facts
- arXiv ID: 2411.17932
- Source URL: https://arxiv.org/abs/2411.17932
- Reference count: 22
- Neural networks show high sensitivity to distance-based perturbations while maintaining robustness to intensity-based perturbations

## Executive Summary
This study investigates whether neural networks learn distance-based or intensity-based representations through systematic perturbation analysis. Using the MNIST dataset, the authors independently manipulate distance and intensity properties of network activations in trained models with ReLU and Absolute Value activation functions. The findings provide empirical evidence that neural networks use distance metrics rather than intensity metrics for classification, challenging the prevailing intensity-based interpretation of neural network activations.

## Method Summary
The authors conducted systematic perturbation experiments on trained neural networks using the MNIST dataset. They independently manipulated two properties of network activations: distance (based on activation differences) and intensity (based on activation magnitudes). For each perturbation type, they applied varying magnitudes while measuring classification accuracy. This approach allowed them to isolate how networks respond to changes in either property. The experiments were conducted on networks using ReLU and Absolute Value activation functions to assess whether the representational properties are activation-dependent.

## Key Results
- Networks show high sensitivity to small distance-based perturbations while maintaining robust performance under large intensity-based perturbations
- Both ReLU and Absolute Value activation functions exhibit similar distance-based representational properties
- The findings provide empirical evidence that neural networks use distance metrics for classification rather than intensity metrics

## Why This Works (Mechanism)
The mechanism underlying this distance-based representation appears to stem from how neural networks learn to separate classes in activation space. During training, networks develop decision boundaries that are more sensitive to relative positioning (distance) between activations than to their absolute magnitudes (intensity). This distance-based learning emerges because classification depends on distinguishing between different classes, which requires encoding relational information about how far apart activations are from different categories. The absolute magnitude of activations may be less critical for this discriminative task.

## Foundational Learning
- **Activation functions**: Mathematical functions that determine whether a neuron should be activated and to what degree (why needed: fundamental building blocks of neural networks; quick check: understand ReLU, sigmoid, tanh behaviors)
- **Distance metrics**: Mathematical measures of similarity or dissimilarity between data points (why needed: core concept for understanding network representations; quick check: know Euclidean, cosine, and other common distance measures)
- **Perturbation analysis**: Method of systematically modifying inputs or representations to study system behavior (why needed: experimental approach used in the paper; quick check: understand controlled variable manipulation)
- **Classification accuracy**: Performance metric measuring correct predictions (why needed: primary evaluation metric; quick check: understand confusion matrices and error rates)
- **Representation learning**: Process by which neural networks develop internal encodings of data (why needed: central to understanding what networks learn; quick check: distinguish between raw inputs and learned features)

## Architecture Onboarding

**Component map:** Input images → Neural network layers (with ReLU/Absolute Value activations) → Activation representations → Classification output

**Critical path:** Input perturbation → Activation manipulation → Classification prediction

**Design tradeoffs:** The choice between distance-based and intensity-based representations affects how networks generalize and handle noisy inputs. Distance-based representations may be more robust to scaling variations but potentially more sensitive to relative positioning errors.

**Failure signatures:** Networks that rely on intensity-based representations would show the opposite pattern - robust to distance perturbations but sensitive to intensity changes.

**First experiments:**
1. Replicate the MNIST perturbation experiments to verify the distance-intensity sensitivity pattern
2. Apply the same perturbation methodology to a simple CNN architecture
3. Test networks with different activation functions (sigmoid, tanh) to assess activation-dependence

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis is confined to MNIST dataset and two activation functions, limiting generalizability
- Perturbation methodology may not capture all aspects of how networks process information during classification
- Does not address whether distance-based representations emerge during training or are inherent properties

## Confidence
- **High confidence**: Empirical observation of sensitivity pattern to distance vs. intensity perturbations
- **Medium confidence**: Interpretation that this indicates distance-based representations
- **Low confidence**: Claim that findings challenge prevailing intensity-based interpretations

## Next Checks
1. Replicate perturbation analysis across multiple architectures (CNNs, transformers) and datasets (CIFAR-10, ImageNet)
2. Conduct ablation studies examining intermediate layer representations to determine when distance-based properties emerge
3. Compare network performance under distance vs. intensity perturbations in adversarially trained models