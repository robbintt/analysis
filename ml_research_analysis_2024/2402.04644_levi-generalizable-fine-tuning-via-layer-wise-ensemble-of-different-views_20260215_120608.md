---
ver: rpa2
title: 'LEVI: Generalizable Fine-tuning via Layer-wise Ensemble of Different Views'
arxiv_id: '2402.04644'
source_url: https://arxiv.org/abs/2402.04644
tags:
- data
- levi
- fine-tuning
- pre-trained
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LEVI addresses the problem of out-of-distribution (OOD) generalization
  in fine-tuning pre-trained models. While existing methods focus on mitigating issues
  in fine-tuning data, LEVI recognizes that inherent problems in pre-trained models
  can also hinder generalization.
---

# LEVI: Generalizable Fine-tuning via Layer-wise Ensemble of Different Views

## Quick Facts
- arXiv ID: 2402.04644
- Source URL: https://arxiv.org/abs/2402.04644
- Authors: Yuji Roh; Qingyun Liu; Huan Gui; Zhe Yuan; Yujin Tang; Steven Euijong Whang; Liang Liu; Shuchao Bi; Lichan Hong; Ed H. Chi; Zhe Zhao
- Reference count: 40
- Primary result: LEVI improves OOD generalization by adaptively ensembling pre-trained and task-specific models layer-wise

## Executive Summary
LEVI addresses out-of-distribution (OOD) generalization challenges in fine-tuning pre-trained models by recognizing that both pre-trained models and fine-tuning data can contain problematic spurious features. The method introduces a layer-wise ensemble approach that combines a frozen pre-trained model with a small task-specific model, allowing each to contribute complementary views while suppressing spurious correlations. This architecture not only improves OOD performance but also maintains strong in-distribution accuracy while being more computationally efficient than existing ensemble methods.

## Method Summary
LEVI fine-tunes pre-trained models by freezing the pre-trained parameters and training a small randomly-initialized task-specific model alongside it. The key innovation is using multiple adapting layers that concatenate intermediate outputs from both models at different depths, then average predictions across these layers. This layer-wise ensemble captures different levels of abstraction - early layers providing general features and later layers offering task-specific information. The method trains only the small model and adapting layers, making it significantly more efficient than traditional ensemble approaches while maintaining comparable inference costs.

## Key Results
- LEVI significantly improves OOD performance while maintaining or improving in-distribution accuracy across language and vision tasks
- The method demonstrates superior efficiency with lower training parameters compared to full model ensemble baselines
- LEVI shows compatibility with efficient fine-tuning techniques like LoRA, further enhancing practical applicability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The pre-trained model and trained-from-scratch model provide complementary views that help suppress spurious features
- Mechanism: The pre-trained model captures general features from diverse data, while the trained-from-scratch model learns task-specific features from fine-tuning data. When both contain different spurious features, ensembling helps cancel out their individual spurious correlations
- Core assumption: Spurious features in pre-trained and fine-tuning data do not overlap significantly
- Evidence anchors:
  - [abstract]: "LEVI effectively suppresses problematic features in both the fine-tuning data and pre-trained model"
  - [section 3.1]: "We provide theoretical backgrounds that show the fine-tuned model can suffer from the spurious features from both pre-trained models and fine-tuning data"
  - [corpus]: Weak - no direct mention of spurious feature suppression mechanisms
- Break condition: If spurious features overlap significantly between pre-training and fine-tuning data, ensembling may not effectively suppress them

### Mechanism 2
- Claim: Layer-wise ensemble captures different levels of abstraction, with early layers being more general and later layers more specific
- Mechanism: By ensembling multiple intermediate layers, LEVI leverages early layers for robustness and later layers for task accuracy, creating a balanced representation that improves both ID and OOD performance
- Core assumption: Different intermediate layers capture progressively more task-specific information
- Evidence anchors:
  - [abstract]: "By combining two complementing models, LEVI effectively suppresses problematic features in both the fine-tuning data and pre-trained model and preserves useful features for new tasks"
  - [section 4]: "by ensembling within the model, we can utilize different information from each intermediate layer (e.g., early layers are more general, and the later layers are more specific)"
  - [corpus]: Weak - no direct mention of layer-wise abstraction benefits
- Break condition: If intermediate layers do not capture meaningful hierarchical information, layer-wise ensembling provides minimal benefit

### Mechanism 3
- Claim: Small task-specific model reduces computational cost while learning essential representations that complement pre-trained features
- Mechanism: Instead of ensembling multiple large models, LEVI uses one large pre-trained model plus a small randomly-initialized model, achieving similar performance gains with lower training and inference costs
- Core assumption: A small model can effectively learn task-specific features that complement the pre-trained model
- Evidence anchors:
  - [abstract]: "LEVI is more efficient than existing ensemble-based methods, with lower training parameters and comparable inference costs"
  - [section 5.2]: "When LEVI uses a pre-trained model, the number of training parameters is significantly lower than all state-of-the-art baselines"
  - [corpus]: Weak - no direct mention of efficiency benefits from small model ensembles
- Break condition: If task requires complex representations that small models cannot capture, performance gains may be limited

## Foundational Learning

- Concept: Spurious correlation and its impact on OOD generalization
  - Why needed here: Understanding how spurious features harm model generalization is fundamental to grasping why LEVI's approach works
  - Quick check question: Why might a model that performs well on training data fail on OOD data?

- Concept: Pre-trained model limitations and domain shift
  - Why needed here: Recognizing that pre-trained models may have inherent issues and limited features for new tasks explains the need for LEVI's dual-model approach
  - Quick check question: What happens when pre-trained features don't align well with downstream task requirements?

- Concept: Ensemble methods and their theoretical foundations
  - Why needed here: Understanding why ensembles reduce variance and improve generalization helps explain LEVI's architecture choices
  - Quick check question: How does combining diverse models theoretically improve model performance?

## Architecture Onboarding

- Component map:
  Pre-trained model (frozen) -> Trained-from-scratch model (small) -> Adapting layers (concatenate intermediate outputs) -> Final prediction (average across adapting layers)

- Critical path:
  1. Freeze pre-trained model parameters
  2. Initialize small task-specific model randomly
  3. Create adapting layers that concatenate intermediate features
  4. Train adapting layers while keeping pre-trained model frozen
  5. Use average of adapting layer outputs as final prediction

- Design tradeoffs:
  - Using multiple adapting layers vs. single layer: More layers provide richer information but increase computation
  - Small vs. large task-specific model: Smaller models are more efficient but may miss complex task features
  - Equal vs. weighted loss across layers: Equal weights are simpler but weighted approaches might optimize performance

- Failure signatures:
  - Poor OOD performance despite good ID performance: Likely issue with spurious feature suppression
  - Very slow training: May have too many adapting layers or unnecessarily large task-specific model
  - Good OOD but poor ID performance: Possibly over-emphasizing robustness at expense of accuracy

- First 3 experiments:
  1. Compare LEVI with simple ensemble of fine-tuned and trained-from-scratch models (no intermediate layers)
  2. Test different numbers of adapting layers (1, 3, 5) to find optimal balance
  3. Evaluate LEVI with different task-specific model architectures (MLP vs CNN) on appropriate tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of task-specific model architecture (e.g., MLP vs. CNN) impact LEVI's performance across different downstream tasks?
- Basis in paper: [explicit] The paper mentions LEVI's flexibility to use any model architecture as the trained-from-scratch model and provides examples of MLP for recommendation tasks and CNN for computer vision tasks.
- Why unresolved: The paper only provides specific examples for two types of tasks. It doesn't explore a wide range of task-specific architectures or their impact on performance.
- What evidence would resolve it: Empirical results comparing LEVI's performance using various task-specific architectures (e.g., RNN, Transformer, GNN) across a diverse set of downstream tasks (e.g., NLP, vision, graph-based).

### Open Question 2
- Question: Can LEVI's layer-wise ensemble approach be extended to incorporate more than two models (e.g., pre-trained model + multiple task-specific models)?
- Basis in paper: [inferred] The paper focuses on combining a pre-trained model with a single task-specific model. However, the layer-wise ensemble concept could potentially be extended to include more models.
- Why unresolved: The paper doesn't explore the possibility of incorporating multiple task-specific models or the potential benefits/drawbacks of doing so.
- What evidence would resolve it: Experiments comparing LEVI's performance with different numbers of task-specific models (e.g., 1, 2, 3) across various tasks, along with an analysis of the trade-offs in terms of performance and computational efficiency.

### Open Question 3
- Question: How does LEVI's performance scale with the size of the pre-trained model (e.g., T5-small vs. T5-large vs. T5-3B)?
- Basis in paper: [inferred] The paper uses T5-small and ViT-base architectures. However, it doesn't investigate how LEVI's performance changes with larger pre-trained models.
- Why unresolved: The paper doesn't provide results for larger pre-trained models, which are becoming increasingly common in practice.
- What evidence would resolve it: Empirical results comparing LEVI's performance using different sizes of pre-trained models (e.g., T5-small, T5-base, T5-large, T5-3B) across various tasks, along with an analysis of the trade-offs in terms of performance and computational efficiency.

## Limitations

- Limited exploration of diverse task-specific architectures and their impact on performance across different downstream tasks
- No investigation of extending the layer-wise ensemble approach to incorporate multiple task-specific models beyond the single pre-trained model setup
- Lack of empirical results showing how LEVI's performance scales with larger pre-trained model sizes (e.g., T5-large, T5-3B)

## Confidence

- Spurious feature suppression mechanism: Medium - supported by performance gains but lacks direct measurement
- Layer-wise abstraction benefits: Medium - plausible based on literature but not explicitly validated in this work
- Efficiency advantages: High - clearly demonstrated through parameter and training cost comparisons
- Cross-task generalizability: Medium - validated on language and vision tasks but limited diversity in task types

## Next Checks

1. Conduct controlled experiments measuring spurious feature correlation before and after LEVI fine-tuning to directly validate the suppression mechanism
2. Perform ablation studies testing different layer ensemble configurations (number of layers, weight distributions) to quantify the contribution of layer-wise benefits
3. Evaluate LEVI on additional task types (e.g., tabular, multimodal) and diverse OOD scenarios to test generalizability limits