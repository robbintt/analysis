---
ver: rpa2
title: 'AgentMD: Empowering Language Agents for Risk Prediction with Large-Scale Clinical
  Tool Learning'
arxiv_id: '2402.13225'
source_url: https://arxiv.org/abs/2402.13225
tags:
- agentmd
- calculators
- tool
- clinical
- riskcalcs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AgentMD, a language agent framework that
  automatically curates and applies clinical calculators from biomedical literature.
  The authors present RiskCalcs, a collection of 2,164 executable clinical calculators
  extracted from PubMed articles using GPT-4 for tool generation and verification.
---

# AgentMD: Empowering Language Agents for Risk Prediction with Large-Scale Clinical Tool Learning

## Quick Facts
- arXiv ID: 2402.13225
- Source URL: https://arxiv.org/abs/2402.13225
- Reference count: 0
- Primary result: AgentMD achieves 87.7% accuracy on RiskQA clinical vignettes using automatically curated clinical calculators

## Executive Summary
AgentMD is a language agent framework that enhances clinical risk prediction by automatically curating and applying executable clinical calculators extracted from biomedical literature. The system introduces RiskCalcs, a collection of 2,164 Python-based clinical calculators mined from PubMed abstracts using GPT-4. When evaluated on RiskQA, a novel benchmark of clinical vignettes, AgentMD significantly outperforms chain-of-thought prompting with GPT-4 (87.7% vs 40.9% accuracy). The approach demonstrates the potential of combining language models with executable tools for healthcare analytics, achieving over 80% quality metrics on manual evaluation of 100 tools and providing population-level insights when applied to 9,822 MIMIC-III ICU patient notes.

## Method Summary
AgentMD employs a three-step pipeline to create clinical calculators: screening PubMed abstracts using Boolean queries and GPT-3.5 filtering, drafting calculator tools with GPT-4, and verifying them through structured validation. The system uses MedCPT for semantic retrieval of relevant calculators, then applies an LLM-based selection process to identify the most appropriate tool. Calculations are executed using a Python interpreter rather than relying solely on LLM reasoning. The framework is evaluated on both a novel RiskQA benchmark of 350 clinical vignettes and real-world MIMIC-III ICU data.

## Key Results
- AgentMD achieves 87.7% accuracy on RiskQA benchmark versus 40.9% for chain-of-thought prompting with GPT-4
- Manual evaluation of 100 RiskCalcs tools shows over 80% accuracy across three quality metrics
- System identifies relevant risks and provides population-level insights when applied to 9,822 MIMIC-III ICU patient notes
- Dense retrieval with MedCPT achieves top-1 accuracy of 0.723, with LLM selection further improving tool choice

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AgentMD achieves higher accuracy by using automatically generated and verified clinical calculators rather than relying solely on LLM reasoning.
- Mechanism: The system generates Python-based clinical calculators from PubMed abstracts using GPT-4, then verifies them through a structured validation process. These executable tools provide precise computational results when combined with a Python interpreter.
- Core assumption: Automatically generated clinical calculators maintain sufficient accuracy when validated by LLMs.
- Evidence anchors:
  - Manual evaluations show that RiskCalcs tools achieve an accuracy of over 80% on three quality metrics.
  - Manual evaluations on a representative subset of 100 calculators have demonstrated the high-quality of RiskCalcs, with 67.0% of the calculators evaluated as 'Correct' in all three aspects.
- Break condition: If verification fails to catch critical errors in calculator logic, or if the generation process introduces systematic biases that compromise accuracy.

### Mechanism 2
- Claim: AgentMD's two-stage tool selection process (MedCPT retrieval + LLM selection) improves accuracy over using either approach alone.
- Mechanism: First-stage retrieval uses MedCPT to find the top 10 most relevant calculators based on semantic similarity, then second-stage selection uses an LLM to choose the most appropriate tool from those candidates.
- Core assumption: The combination of dense retrieval and LLM reasoning provides better tool selection than either method alone.
- Evidence anchors:
  - AgentMD significantly outperforms chain-of-thought prompting with GPT-4 (87.7% vs. 40.9% in accuracy).
  - Dense retrieval with MedCPT achieves a top-1 accuracy of 0.723 and GPT-4-based AgentMD can better select the required tool than MedCPT.
- Break condition: If the initial MedCPT retrieval fails to include the relevant calculator in the top-10 results, or if the LLM selection becomes unreliable with more diverse tool sets.

### Mechanism 3
- Claim: Implementing clinical calculators as executable Python functions rather than raw text descriptions improves accuracy.
- Mechanism: Calculators are represented as reusable Python functions that can be executed with specific parameters, rather than relying on LLMs to parse and compute from textual descriptions.
- Core assumption: Executable code is more reliable for computation than natural language descriptions.
- Evidence anchors:
  - AgentMD can automatically select and apply the relevant RiskCalcs tools given any patient description.
  - AgentMD uses the calculator by interacting with a Python interpreter and these results suggest that clinical calculators are better implemented with code snippets and utilized with a Python interpreter.
- Break condition: If the Python interpreter introduces latency or computational overhead that makes the system impractical for real-time clinical use.

## Foundational Learning

- Concept: Understanding of clinical calculators and their role in medical decision-making
  - Why needed here: The system is built around curating and applying clinical calculators, so understanding their purpose and limitations is essential
  - Quick check question: What are the key limitations of traditional clinical calculators that AgentMD aims to address?

- Concept: Familiarity with PubMed search and biomedical literature structure
  - Why needed here: The system mines PubMed abstracts to generate clinical calculators, requiring knowledge of how medical literature is organized
  - Quick check question: What types of information are typically found in PubMed abstracts that would be useful for creating clinical calculators?

- Concept: Understanding of retrieval-augmented generation (RAG) and dense vector representations
  - Why needed here: AgentMD uses MedCPT for semantic retrieval of relevant calculators
  - Quick check question: How does dense retrieval with MedCPT differ from traditional keyword-based search in terms of finding relevant clinical calculators?

## Architecture Onboarding

- Component map: PubMed screening pipeline (Boolean query → GPT-3.5 filtering → GPT-4 tool generation) → RiskCalcs repository (2,164 verified clinical calculators as Python functions) → MedCPT semantic index (dense vector representations of calculators) → AgentMD framework (tool selection, computation with Python interpreter, summarization) → Evaluation components (RiskQA benchmark, MIMIC-III validation)

- Critical path: PubMed screening → Calculator generation and verification → RiskCalcs creation → Tool selection (MedCPT + LLM) → Python computation → Result summarization

- Design tradeoffs:
  - Accuracy vs. coverage: More permissive screening might increase coverage but decrease accuracy
  - Computation time vs. precision: Using Python interpreter ensures precision but adds computational overhead
  - LLM model choice: GPT-4 provides better tool selection but at higher cost than GPT-3.5

- Failure signatures:
  - Tool selection failures: Wrong calculator chosen despite relevant options in top-10
  - Computation failures: Python interpreter errors or incorrect parameter extraction
  - Summarization failures: Inconsistent or incomplete result interpretation
  - Generation failures: Hallucinations in calculator creation that pass verification

- First 3 experiments:
  1. Test tool selection accuracy on a small, controlled set of patient scenarios with known correct calculators
  2. Verify Python interpreter integration by running sample calculations with known inputs/outputs
  3. Evaluate the complete pipeline on RiskQA benchmark to establish baseline performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do alternative, open-source LLMs (e.g., Llama) compare to GPT-4 in terms of tool selection accuracy and overall performance on RiskQA?
- Basis in paper: The paper mentions that GPT-4's high operational costs and deployment challenges raise concerns about data privacy and security, suggesting the potential benefits of investigating alternative LLMs like Llama.
- Why unresolved: The study exclusively uses GPT-3.5 and GPT-4 as backbone LLMs. While it acknowledges the limitations of GPT-4, it does not explore or compare the performance of other LLMs, including open-source options.
- What evidence would resolve it: Conducting experiments using alternative LLMs like Llama on the RiskQA benchmark and comparing their tool selection accuracy, overall performance, and computational efficiency to GPT-4.

### Open Question 2
- Question: How does the accuracy of AgentMD's risk predictions vary across different clinical contexts and patient populations?
- Basis in paper: The paper states that "larger-scale clinical studies on a diverse set of patients are imperative to validate the effectiveness of AgentMD."
- Why unresolved: The current study primarily evaluates AgentMD on a relatively small sample of 100 clinical calculators and the RiskQA dataset of 350 USMLE-style questions. While it applies AgentMD to MIMIC-III data, the evaluation focuses on cohort-level insights rather than individual patient predictions.
- What evidence would resolve it: Conducting large-scale clinical studies with diverse patient populations and evaluating AgentMD's accuracy in predicting risks across different clinical contexts, such as various diseases, patient demographics, and healthcare settings.

### Open Question 3
- Question: How does the inclusion of full-text articles, beyond PubMed abstracts, impact the quality and coverage of RiskCalcs?
- Basis in paper: The paper mentions that "the creation of calculator tools was restricted to PubMed abstracts, overlooking detailed descriptions available in full-text articles."
- Why unresolved: The study only uses PubMed abstracts as the knowledge source for curating RiskCalcs. It does not explore the potential benefits of incorporating full-text articles, which might contain more detailed information about clinical calculators.
- What evidence would resolve it: Extending the data collection process to include full-text articles and comparing the quality and coverage of the resulting tool collection to RiskCalcs. This could involve assessing the accuracy of tool descriptions, the number of new tools discovered, and the overall performance of AgentMD when using the expanded tool collection.

## Limitations

- Tool quality concerns: Only 67.0% of RiskCalcs tools are fully correct across all three quality metrics, indicating potential errors in a significant portion of calculators
- Generalizability limitations: Performance is demonstrated on MIMIC-III ICU data and clinical vignettes but not validated across diverse clinical settings and patient populations
- Computational overhead: The approach requires multiple LLM calls and Python interpreter execution, raising questions about real-world latency and cost implications

## Confidence

- High Confidence: The core mechanism of using executable Python functions for clinical calculators rather than pure LLM reasoning is well-supported, with substantial accuracy improvement from 40.9% to 87.7% on RiskQA
- Medium Confidence: The two-stage tool selection process (MedCPT retrieval + LLM selection) is supported by comparative results, though the specific contribution of each component is not isolated through ablation studies
- Medium Confidence: The quality of RiskCalcs tools (over 80% accuracy on quality metrics) is based on manual evaluation of 100 tools, which provides reasonable but not comprehensive validation

## Next Checks

1. Conduct an ablation study to test AgentMD performance with only MedCPT retrieval (no LLM selection) and with only LLM selection (no MedCPT retrieval) to quantify the individual contributions of each component to the overall 87.7% accuracy.

2. Systematically test a larger random sample of RiskCalcs tools (e.g., 500 calculators) across diverse clinical domains to better characterize the 67.0% fully correct rate and identify systematic error patterns.

3. Evaluate AgentMD's performance on a held-out test set of MIMIC-III notes or external clinical data that was not used in the original development or validation to assess generalizability and real-world applicability.