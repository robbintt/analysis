---
ver: rpa2
title: American Sign Language Video to Text Translation
arxiv_id: '2402.07255'
source_url: https://arxiv.org/abs/2402.07255
tags: []
core_contribution: This paper replicates and improves upon a recent study on American
  Sign Language (ASL) video-to-text translation using the How2Sign dataset. The authors
  implement a transformer-based encoder-decoder architecture with I3D feature extraction
  and evaluate using BLEU and rBLEU metrics.
---

# American Sign Language Video to Text Translation

## Quick Facts
- arXiv ID: 2402.07255
- Source URL: https://arxiv.org/abs/2402.07255
- Reference count: 20
- Authors: Parsheeta Roy; Ji-Eun Han; Srishti Chouhan; Bhaavanaa Thumu
- Key outcome: Improves ASL video-to-text translation baseline with rBLEU 3.29 (vs 2.79) and BLEU 9.39 (vs 8.89) on validation set

## Executive Summary
This paper addresses the challenge of translating American Sign Language videos to text using the How2Sign dataset. The authors implement a transformer-based encoder-decoder architecture with I3D feature extraction and conduct extensive ablation studies to identify optimal hyperparameters. Through systematic experimentation with optimizers, activation functions, and regularization techniques, they achieve significant improvements over the baseline model. The study demonstrates that careful hyperparameter tuning and regularization can substantially enhance translation quality in low-resource sign language translation tasks.

## Method Summary
The authors use a transformer encoder-decoder architecture with I3D features extracted from RGB frames of ASL videos. The model employs 6 encoder and 6 decoder layers with embedding dimensions of 256 and 512 respectively, trained using AdamW optimizer with cosine learning rate scheduling. They incorporate label smoothing during training and evaluate performance using BLEU and rBLEU metrics. The system uses SentencePiece tokenization with a 7000 subword vocabulary and includes various regularization techniques including dropout and weight decay.

## Key Results
- Best configuration achieves rBLEU score of 3.29 (vs 2.79 baseline) on validation set
- Best configuration achieves BLEU score of 9.39 (vs 8.89 baseline) on validation set
- Model performance significantly influenced by optimizers, activation functions, and label smoothing
- Increasing decoder layers improves performance but requires additional regularization to prevent overfitting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: I3D features enable the model to capture both spatial and temporal dynamics of sign language videos effectively
- Mechanism: I3D inflates 2D CNN filters into 3D to process video frames, capturing motion patterns across time while retaining spatial information
- Core assumption: Pre-trained I3D network extracts sufficiently discriminative features for sign language translation
- Evidence anchors: [section] "I3D features are used to extract video representations directly from RGB frames...features are extracted from the 1024-dimensional activation before the pooling layer"
- Break condition: If I3D features don't capture sufficient motion or spatial details relevant to sign language semantics

### Mechanism 2
- Claim: Label smoothing improves generalization by preventing overconfident predictions on training data
- Mechanism: Label smoothing modifies target distribution by assigning small probability to incorrect classes
- Core assumption: Limited training dataset makes overfitting likely; label smoothing helps generalization
- Evidence anchors: [section] "training process incorporates cross-entropy loss with label smoothing to enhance model's generalization"
- Break condition: If label smoothing is too aggressive, causing underfitting

### Mechanism 3
- Claim: Increasing decoder layers improves translation quality by providing more capacity to model complex language generation
- Mechanism: Additional decoder layers allow building more abstract representations of target language
- Core assumption: Baseline decoder is under-capacity for ASL-to-English translation complexity
- Evidence anchors: [section] "Increasing the decoder layers of the best baseline Model 18 inherently induces overfitting"
- Break condition: If model overfits due to increased parameters without sufficient regularization

## Foundational Learning

- Concept: Cross-entropy loss with label smoothing
  - Why needed here: Prevents overconfident predictions on limited data and encourages smoother probability distributions
  - Quick check question: What happens to the target distribution when label smoothing with ε=0.1 is applied?

- Concept: Transformer encoder-decoder architecture
  - Why needed here: Handles sequence-to-sequence translation by capturing temporal dependencies in video input and generating coherent text output
  - Quick check question: Why does the transformer use positional embeddings in both encoder and decoder?

- Concept: I3D feature extraction
  - Why needed here: Provides spatio-temporal features from video frames that encode both motion and appearance for sign language understanding
  - Quick check question: How does inflating 2D CNN filters into 3D help capture motion in videos?

## Architecture Onboarding

- Component map: Sign language video → I3D feature extractor (1024-dim) → Transformer encoder (6 layers) → Cross-attention with decoder → Transformer decoder (3-6 layers) → Output vocabulary (7000 subwords)
- Critical path: Video preprocessing (I3D extraction) → Tokenization → Encoder forward pass → Decoder autoregressive generation → Post-processing → Evaluation
- Design tradeoffs:
  - Model size vs. overfitting: Larger models (6-6 layers) require stronger regularization (dropout, weight decay, label smoothing)
  - Vocabulary size vs. sequence length: 7000 subwords balances rare word representation with manageable sequence lengths
  - Pre-trained features vs. end-to-end learning: I3D features provide strong initialization but may limit adaptability to sign-specific patterns
- Failure signatures:
  - BLEU improves but rBLEU stagnates: Model memorizing frequent patterns rather than learning semantics
  - Training loss decreases but validation loss increases: Overfitting, likely need more regularization
  - Both metrics improve slowly: Learning rate too low or insufficient model capacity
- First 3 experiments:
  1. Reproduce baseline with exact hyperparameters from paper (6-3 layers, ReLU, label smoothing 0.1, dropout 0.3, weight decay 0.1)
  2. Test impact of activation function by replacing ReLU with GeLU while keeping other parameters fixed
  3. Evaluate effect of increasing decoder layers from 3 to 6 with proportional regularization adjustments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do alternative visual feature extraction methods (e.g., 2D pose estimation or other 3D CNN architectures) compare to I3D features in terms of translation performance and computational efficiency for ASL video-to-text translation?
- Basis in paper: [explicit] The paper mentions that various feature extraction techniques exist, including 2D CNNs, 3D ConvNets (I3D), and pose-based methods like Pose-TGCN, but only uses I3D in their experiments
- Why unresolved: The paper only evaluates I3D features and does not explore other feature extraction methods
- What evidence would resolve it: A systematic ablation study comparing I3D with alternative feature extraction methods on the same dataset and evaluation metrics

### Open Question 2
- Question: What is the optimal balance between model depth (encoder/decoder layers) and width (embedding dimensions, FFN size) for ASL video-to-text translation under varying data conditions?
- Basis in paper: [explicit] The ablation study shows that model performance is significantly influenced by the number of encoder/decoder layers and embedding dimensions
- Why unresolved: While the paper explores different configurations, it doesn't systematically investigate how depth and width interact
- What evidence would resolve it: A comprehensive study varying both depth and width parameters across multiple dataset sizes

### Open Question 3
- Question: How do different regularization techniques (dropout, weight decay, label smoothing) interact with each other and affect model performance at different stages of training?
- Basis in paper: [explicit] The paper demonstrates that regularization techniques significantly improve performance, particularly for larger models
- Why unresolved: The ablation study adjusts regularization parameters individually but doesn't examine their combined effects
- What evidence would resolve it: Experiments testing various combinations of regularization techniques at different training stages

## Limitations

- Dataset specificity and generalization: Results limited to How2Sign dataset (cooking videos), may not generalize to other ASL contexts or dialects
- Feature extraction dependency: Entire pipeline relies on I3D features from pre-trained models, optimal feature extraction for ASL remains untested
- Limited ablation scope: Doesn't explore alternative feature extraction methods or different tokenization strategies beyond SentencePiece

## Confidence

**High confidence** in architectural approach: Transformer encoder-decoder with I3D features is well-established for video-to-text translation, supported by systematic ablation studies

**Medium confidence** in claimed improvements: Reported gains are statistically measurable but practically modest, stemming from well-understood regularization techniques

**Low confidence** in generalization claims: Limited evidence that findings extend beyond How2Sign dataset, ablation studies stay within transformer + I3D paradigm

## Next Checks

**Validation check 1**: Reproduce the baseline model (6-6 layers, ReLU activation, label smoothing 0.1, dropout 0.3, weight decay 0.1) on How2Sign dataset and verify reported BLEU (8.89) and rBLEU (2.79) scores

**Validation check 2**: Train identical transformer architecture but replace I3D features with 3D CNN features trained from scratch on How2Sign dataset, compare performance

**Validation check 3**: Conduct human evaluation study where native ASL signers assess semantic accuracy and fluency of model outputs, compare with BLEU/rBLEU metrics