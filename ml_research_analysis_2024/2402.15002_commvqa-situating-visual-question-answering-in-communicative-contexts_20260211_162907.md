---
ver: rpa2
title: 'CommVQA: Situating Visual Question Answering in Communicative Contexts'
arxiv_id: '2402.15002'
source_url: https://arxiv.org/abs/2402.15002
tags:
- image
- questions
- contextual
- description
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CommVQA is a new dataset for visual question answering (VQA) that
  situates the task in communicative contexts. It contains 1000 images, 8,949 question-answer
  pairs, image descriptions, and real-world scenarios where the images might appear.
---

# CommVQA: Situating Visual Question Answering in Communicative Contexts

## Quick Facts
- arXiv ID: 2402.15002
- Source URL: https://arxiv.org/abs/2402.15002
- Authors: Nandita Shankar Naik; Christopher Potts; Elisa Kreiss
- Reference count: 18
- 1000 images with 8,949 question-answer pairs in communicative contexts

## Executive Summary
CommVQA introduces a new dataset for visual question answering that situates the task in real-world communicative contexts. Unlike traditional VQA datasets where questions are elicited directly from images, CommVQA questions are prompted by scenarios and descriptions that reflect how people encounter visual content online. The dataset contains 1000 images paired with image descriptions, real-world scenarios, and question-answer pairs that vary significantly across different contexts. The work demonstrates that incorporating contextual information improves model performance, with IDEFICS achieving the highest scores. However, even the best models generate high rates of hallucinations and struggle to identify unanswerable questions, highlighting the importance of contextual grounding for reliable VQA systems.

## Method Summary
The CommVQA dataset was created by first collecting images from existing datasets and having crowdworkers write image descriptions and plausible real-world scenarios where these images might appear. Questions were then elicited from crowdworkers based on the scenario and description rather than the image itself, simulating how people ask questions when encountering visual content online. The dataset contains 1000 images with 8,949 question-answer pairs, of which 50 are labeled as unanswerable. Models are evaluated on their ability to answer questions given the image, description, and scenario using reference-based metrics (BLEU, METEOR, ROUGE, CIDEr) and CLIPScore, along with human evaluation for hallucination detection.

## Key Results
- Access to contextual information (scenario + description) significantly improves VQA model performance, with IDEFICS achieving the highest scores across all metrics
- Questions vary significantly across scenarios, with statistical tests showing scenario-dependent question distributions (e.g., "Who" questions are more likely in social media scenarios)
- Current models struggle with unanswerable questions, with IDEFICS generating incorrect information at high rates (23% clearly erroneous, 22% unverifiable)
- CLIPScore correlates poorly with reference-based metrics and human judgments, suggesting it may not be a reliable evaluation metric for this task

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Contextual information improves VQA model performance by aligning generated answers more closely with human expectations.
- **Mechanism**: When models are provided with the scenario and description, they generate answers that are more aligned with the ground-truth references as measured by BLEU, METEOR, ROUGE, and CIDEr metrics.
- **Core assumption**: The scenario and description provide sufficient contextual grounding for the model to understand the user's information needs.
- **Evidence anchors**:
  - [abstract]: "Overall, we show that access to contextual information is essential for solving CommVQA, leading to the highest performing VQA model"
  - [section]: "IDEFICS (contextual) achieved the highest scores across all metrics"
  - [corpus]: Weak - no direct corpus evidence linking context to performance
- **Break condition**: If the scenario and description are too vague or misaligned with the image, the model may generate irrelevant answers despite having contextual information.

### Mechanism 2
- **Claim**: Different scenarios elicit distinct question types, making VQA performance scenario-dependent.
- **Mechanism**: Questions vary significantly across scenarios (e.g., "Who" questions are more likely in social media, "Where" questions in travel), and models must adapt to these variations.
- **Core assumption**: The types of questions asked are genuinely shaped by the scenario rather than being random variations.
- **Evidence anchors**:
  - [section]: "BERT achieved an accuracy of 56% on this task, a significant improvement over random choice (16%)"
  - [section]: "Welch's t-test demonstrated that, for example, compared to all other scenarios, 'Who' questions are significantly more likely to appear in the social media scenario"
  - [corpus]: Weak - corpus shows related VQA work but doesn't specifically address scenario-based question variation
- **Break condition**: If scenario differences are superficial and don't genuinely affect information needs, models won't need to adapt across scenarios.

### Mechanism 3
- **Claim**: Models struggle with identifying unanswerable questions, leading to hallucinations even when context is provided.
- **Mechanism**: Even the best-performing model (IDEFICS contextual) generates incorrect information at high rates (23% clearly erroneous, 22% unverifiable), and has low abstention rates for unanswerable questions.
- **Core assumption**: The presence of unanswerable questions in the dataset is a valid measure of model reliability.
- **Evidence anchors**:
  - [abstract]: "Error analyses and a human-subjects study suggest that generated answers still contain high rates of hallucinations, fail to fittingly address unanswerable questions"
  - [section]: "participants indicated that 23% of the model-generated answers contained clearly erroneous information"
  - [corpus]: Weak - corpus contains medical VQA work but not specifically about hallucination rates or unanswerable questions
- **Break condition**: If models could be better trained to recognize unanswerable questions or if unanswerable questions were removed from evaluation, hallucination rates would appear lower.

## Foundational Learning

- **Concept**: Pragmatic Bayesian view of communication
  - Why needed here: Explains why questions depend on informational needs and prior knowledge about image content
  - Quick check question: Why would someone browsing a shopping site ask different questions about an image than someone browsing a news site?

- **Concept**: Visual Question Answering as multimodal reasoning
  - Why needed here: CommVQA requires synthesizing visual, textual, and contextual information
  - Quick check question: What are the three types of information a model needs to answer questions in CommVQA?

- **Concept**: Hallucination detection and abstention strategies
  - Why needed here: The dataset includes unanswerable questions and high hallucination rates, requiring models to know when to decline answering
  - Quick check question: What percentage of CommVQA questions are labeled as unanswerable?

## Architecture Onboarding

- **Component map**: Image encoder -> Text encoder -> Context integration module -> Cross-modal fusion layer -> Answer decoder/generator -> Hallucination detection module

- **Critical path**:
  1. Encode image, question, scenario, and description
  2. Fuse multimodal representations
  3. Generate answer conditioned on all inputs
  4. Evaluate answer quality and hallucination risk

- **Design tradeoffs**:
  - Context integration: Early fusion (before reasoning) vs. late fusion (after reasoning)
  - Answer generation: Autoregressive vs. non-autoregressive decoding
  - Evaluation: Reference-based metrics vs. human evaluation for hallucination detection

- **Failure signatures**:
  - High CLIPScore with low reference-based metrics → model is repeating description rather than answering question
  - Low abstention rate on unanswerable questions → model hallucinates instead of declining
  - Performance degradation when context is removed → model is not genuinely reasoning about image

- **First 3 experiments**:
  1. Benchmark IDEFICS with and without contextual prompt on CommVQA subset to confirm performance gap
  2. Analyze CLIPScore vs. reference-based metrics correlation to detect description repetition
  3. Test abstention performance on unanswerable questions with explicit "say unanswerable" prompt

## Open Questions the Paper Calls Out

- How can we improve models' ability to identify unanswerable questions and generate appropriate abstention responses?
- What is the relationship between CLIPScore and other evaluation metrics, and is CLIPScore a reliable measure for this task?
- How can we reduce hallucination rates in VQA systems while maintaining answer quality for answerable questions?

## Limitations

- The dataset size (1000 images, ~9K question-answer pairs) is relatively small compared to other VQA benchmarks, limiting generalizability
- Evaluation relies heavily on reference-based metrics that may not fully capture answer quality in communicative contexts
- Human evaluation study involved only 3 participants per question, which may not provide statistically robust assessments

## Confidence

- **High confidence**: Contextual information improves model performance; models struggle with unanswerable questions and generate high hallucination rates
- **Medium confidence**: Questions vary significantly across scenarios; IDEFICS superiority may be dataset-specific
- **Low confidence**: Models cannot identify unanswerable questions effectively; specific hallucination rates may not generalize

## Next Checks

1. **Scale-up evaluation**: Conduct the same human evaluation study with 30+ participants per question to establish more robust hallucination rate estimates and unanswerable question detection performance.

2. **Cross-dataset validation**: Test whether the performance improvements from contextual information transfer to other VQA datasets (e.g., VQA-v2, GQA) by adding scenario descriptions to questions and measuring performance changes.

3. **Model ablation study**: Perform a systematic ablation of different contextual components (scenario vs. description) to determine which type of contextual information is most critical for performance improvements and hallucination reduction.