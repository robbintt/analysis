---
ver: rpa2
title: Investigating grammatical abstraction in language models using few-shot learning
  of novel noun gender
arxiv_id: '2403.10338'
source_url: https://arxiv.org/abs/2403.10338
tags: []
core_contribution: Humans learn novel words and their grammatical properties from
  very few examples, demonstrating an abstract understanding of linguistic properties
  like grammatical gender and agreement rules that can be applied to novel syntactic
  contexts and words. Drawing inspiration from psycholinguistics, we conduct a noun
  learning experiment to assess whether LSTM and transformer language models can achieve
  human-like abstraction of grammatical gender in French.
---

# Investigating grammatical abstraction in language models using few-shot learning of novel noun gender

## Quick Facts
- **arXiv ID**: 2403.10338
- **Source URL**: https://arxiv.org/abs/2403.10338
- **Reference count**: 35
- **Primary result**: Language models can learn novel noun gender from few examples and generalize across agreement contexts, though with masculine gender bias similar to humans

## Executive Summary
This paper investigates whether language models can learn grammatical gender of novel nouns from few examples in a manner similar to humans. Using French as a test case, the authors conduct few-shot learning experiments where models learn the gender of novel nouns from one or two examples in one agreement context and then predict agreement in a different, unseen context. Both LSTM and transformer models successfully generalize novel noun gender, demonstrating what appears to be abstract representation of grammatical gender. However, both models and humans exhibit a persistent masculine gender bias, suggesting limitations in their learning mechanisms.

## Method Summary
The experiment involves training language models to learn the gender of novel nouns embedded in French sentences. Models receive few-shot examples showing a novel noun with its correct gender agreement (e.g., "le munip" for masculine, "la munip" for feminine) in one syntactic context (either noun-adjective or noun-relative-pronoun agreement). After learning, models are tested on predicting agreement in a different context that was not seen during training. The key innovation is restricting gradient updates to only the embedding layer, testing whether gender information can be encoded purely in word embeddings. Human participants performed an analogous task to compare learning patterns.

## Key Results
- Both LSTM and transformer models successfully generalize novel noun gender from 1-2 learning examples
- Models apply learned gender across different agreement contexts (noun-adjective to noun-relative-pronoun and vice versa)
- Both models and humans exhibit a significant masculine gender bias in their predictions
- Restricting updates to embedding layers still enables effective gender learning, suggesting gender information is encoded in the embedding space

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot updates to embedding layers enable models to learn novel noun gender without retraining full model
- Mechanism: The gender of a novel noun is encoded as a vector in the embedding space, which can be adjusted with minimal gradient updates. This allows the model to represent gender as an abstract property of the noun rather than memorizing specific syntactic contexts.
- Core assumption: Grammatical gender is sufficiently encoded in the embedding layer and can be generalized across different syntactic agreement contexts
- Evidence anchors:
  - [abstract] "Importantly, the few-shot updates were only applied to the embedding layers, demonstrating that models encode sufficient gender information within the word embedding space."
  - [section] "Both language models seem to acquire abstract gender properties of novel nouns from few-shot learning... This aligns with how humans are believed to learn words, which only requires an incremental update to the knowledge of nouns during acquisition while maintaining an abstract understanding of grammatical gender and agreement rules."
  - [corpus] Weak - the corpus evidence does not directly support the claim about gender encoding in embeddings
- Break condition: If gender information requires hidden layer representations beyond embeddings, this mechanism fails

### Mechanism 2
- Claim: Models develop abstract gender representations that generalize across syntactic contexts
- Mechanism: The model learns gender not just from specific agreement patterns but as a generalizable feature that can be applied to novel syntactic contexts (e.g., from noun-adjective to noun-relative-pronoun agreement)
- Core assumption: Gender abstraction is not tied to specific syntactic constructions but exists as a property of the noun itself
- Evidence anchors:
  - [abstract] "We find that both language models effectively generalise novel noun gender from one to two learning examples and apply the learnt gender across agreement contexts"
  - [section] "Language models were tasked with learning the gender of a novel noun embedding from a few examples in one grammatical agreement context and predicting agreement in another, unseen context."
  - [corpus] Weak - corpus evidence does not directly test generalization across contexts
- Break condition: If models fail to generalize gender across different agreement contexts beyond the learning examples

### Mechanism 3
- Claim: Transformer models encode gender through co-occurrence patterns rather than noun-centric representation
- Mechanism: The transformer updates not just the novel noun embedding but also related gender-marked words (like determiners), suggesting gender is represented through relationships between word types rather than as a property of nouns alone
- Core assumption: Gender information emerges from co-occurrence patterns in the embedding space rather than being a core noun property
- Evidence anchors:
  - [section] "Notably, the transformer model also significantly adjusted the weight for the masculine article 'le', even if it was not in the learning sentences, and with the feminine learning condition."
  - [section] "This suggests that the trained language model may not represent gender as critically hosted by nouns, and gender agreement as triggered by nouns alone."
  - [corpus] Weak - corpus evidence does not directly support this co-occurrence hypothesis
- Break condition: If weight updates are primarily noun-centric and do not affect related gender-marked words

## Foundational Learning

- Concept: Grammatical gender as an abstract linguistic property
  - Why needed here: Understanding that grammatical gender is not semantically interpretable but follows arbitrary rules that must be learned
  - Quick check question: What makes grammatical gender different from number agreement in terms of semantic interpretability?

- Concept: Few-shot learning and gradient updates
  - Why needed here: The experimental method relies on updating only embedding layers with minimal examples, requiring understanding of how gradient-based learning works
  - Quick check question: How does restricting gradient updates to embedding layers affect the model's ability to learn new concepts?

- Concept: Syntactic agreement and long-distance dependencies
  - Why needed here: The experiment tests agreement across varying distances between noun and target, requiring understanding of syntactic structure versus linear word order
  - Quick check question: Why does grammatical agreement depend on syntactic structure rather than simple word proximity?

## Architecture Onboarding

- Component map: Embedding layer → LSTM/Transformer hidden layers → output layer → gradient computation → embedding layer updates
- Critical path: Embedding layer → hidden layers → output layer → gradient computation → embedding layer updates
- Design tradeoffs: Embedding-only updates vs. full model updates (computational efficiency vs. learning capacity)
- Failure signatures: Gender bias persistence, failure to generalize across contexts, incorrect weight updates to non-relevant tokens
- First 3 experiments:
  1. Test baseline gender agreement on existing nouns to verify model can perform the task before few-shot learning
  2. Apply few-shot learning with 1-2 examples and test across all four agreement conditions to measure generalization
  3. Analyze weight changes in embedding layer to identify which tokens are most affected by learning updates

## Open Questions the Paper Calls Out

- How exactly is grammatical gender implemented in language models - as an abstract categorical property or through statistical co-occurrence patterns?
- Why do both models and humans exhibit a masculine gender bias, and does this indicate similar underlying mechanisms?
- Can language models learn truly arbitrary gender systems where masculine is not the default category?
- Do transformer models represent gender differently than LSTM models, as suggested by their different weight update patterns?

## Limitations

- The masculine gender bias observed in both models and humans suggests the learning mechanism may be based on frequency statistics rather than genuine categorical abstraction
- Corpus evidence is weak for several key claims, particularly regarding whether gender information is sufficiently encoded in embeddings alone
- The experimental design cannot definitively distinguish between abstract categorical representations and sophisticated pattern matching

## Confidence

- **Core finding (models learn novel gender from few examples)**: Medium
- **Human-like abstraction claim**: Low
- **Embedding-only representation claim**: Medium

## Next Checks

1. Test models with artificially constructed gender systems where masculine is not the default to determine if the bias persists or if models can learn truly arbitrary gender categories
2. Conduct ablation studies removing specific hidden layer components while preserving embedding updates to isolate whether gender abstraction requires deep representations beyond embeddings
3. Compare model learning curves with human behavioral data across multiple gender categories and learning conditions to identify whether the statistical learning patterns match or diverge fundamentally from human cognitive processes