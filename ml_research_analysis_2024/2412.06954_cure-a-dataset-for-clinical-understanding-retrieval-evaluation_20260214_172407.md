---
ver: rpa2
title: 'CURE: A Dataset for Clinical Understanding & Retrieval Evaluation'
arxiv_id: '2412.06954'
source_url: https://arxiv.org/abs/2412.06954
tags:
- query
- retrieval
- passages
- cure
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CURE, a novel retrieval test dataset for
  healthcare practitioners. CURE addresses the lack of domain-specific evaluation
  tools for retrieval systems in medical settings by providing 2000 queries spanning
  10 medical domains with both monolingual and cross-lingual conditions.
---

# CURE: A Dataset for Clinical Understanding & Retrieval Evaluation

## Quick Facts
- arXiv ID: 2412.06954
- Source URL: https://arxiv.org/abs/2412.06954
- Authors: Nadia Athar Sheikh; Daniel Buades Marcos; Anne-Laure Jousse; Akintunde Oladipo; Olivier Rousseau; Jimmy Lin
- Reference count: 8
- Primary result: Novel retrieval test dataset for healthcare practitioners with 2000 queries spanning 10 medical domains

## Executive Summary
CURE addresses the critical gap in domain-specific evaluation tools for retrieval systems in medical settings. The dataset provides 2000 queries across 10 medical domains with both monolingual and cross-lingual conditions, created through collaboration with medical professionals. It enables rigorous evaluation of retrieval systems for healthcare applications, supporting the development of reliable tools that can assist practitioners in accessing relevant biomedical literature.

## Method Summary
The dataset construction involved medical professionals generating natural language queries based on real clinical scenarios and identifying relevant passages from biomedical literature. The team ensured comprehensive medical domain coverage while maintaining query naturalness and relevance criteria. The resulting dataset includes both English queries with cross-lingual evaluation capabilities, making it suitable for assessing retrieval systems in diverse healthcare settings.

## Key Results
- Baseline results show BM25, GTE Multilingual Base, and NV-Embed V2 performance on CURE dataset
- Dataset demonstrates effectiveness for evaluating retrieval systems in healthcare contexts
- CURE available under Creative Commons license for research and development purposes

## Why This Works (Mechanism)
The dataset works by providing realistic, clinically-grounded queries that reflect actual healthcare practitioner information needs. By involving medical professionals in query generation and relevance assessment, CURE ensures that evaluation captures meaningful retrieval scenarios rather than artificial test cases. The cross-lingual capability enables assessment of systems that must serve diverse patient populations and international medical communities.

## Foundational Learning
- **Medical domain terminology**: Understanding specialized healthcare vocabulary is essential for accurate query processing and relevance assessment
- **Cross-lingual retrieval concepts**: Necessary for evaluating systems that must handle queries in multiple languages while maintaining retrieval quality
- **Clinical information needs**: Critical for designing queries that reflect genuine practitioner requirements rather than academic exercises
- **Biomedical literature structure**: Important for understanding how medical knowledge is organized and accessed
- **Evaluation methodology for IR systems**: Required for proper assessment of retrieval performance metrics
- **Creative Commons licensing**: Relevant for understanding dataset usage rights and research implications

## Architecture Onboarding

**Component Map**: Medical professionals -> Query generation -> Relevance assessment -> Dataset compilation -> Baseline evaluation -> Distribution

**Critical Path**: Query generation → Relevance assessment → Dataset compilation → Baseline evaluation

**Design Tradeoffs**: Natural language queries vs. controlled vocabulary; domain specificity vs. general applicability; monolingual vs. cross-lingual coverage

**Failure Signatures**: Poor medical domain coverage, unrealistic query formulation, insufficient relevance criteria, limited language support

**First Experiments**:
1. Evaluate baseline retrieval models (BM25, neural embeddings) on small CURE subset
2. Compare cross-lingual performance across different language pairs
3. Analyze query difficulty distribution across medical domains

## Open Questions the Paper Calls Out
None

## Limitations
- Relatively small scale (2000 queries) compared to general-domain benchmarks
- Single annotator judgments may introduce individual bias without redundancy checks
- English-only queries may not capture full linguistic diversity of global healthcare practitioners
- Baseline models used are standard approaches that may not represent state-of-the-art capabilities

## Confidence
- **High confidence**: Medical domain coverage and annotation methodology are well-documented and sound
- **Medium confidence**: Baseline performance results are reliable but may not represent cutting-edge capabilities
- **Medium confidence**: Claim about addressing critical gap in medical retrieval evaluation is well-supported, though long-term impact remains to be seen

## Next Checks
1. Conduct inter-annotator agreement studies on subset of queries to quantify annotation consistency
2. Expand dataset with additional queries and domains to improve statistical power and coverage
3. Benchmark state-of-the-art retrieval models (e.g., ColBERT, SPLADE) to establish comprehensive baseline performance