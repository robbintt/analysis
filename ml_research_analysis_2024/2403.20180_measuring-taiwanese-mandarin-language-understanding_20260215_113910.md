---
ver: rpa2
title: Measuring Taiwanese Mandarin Language Understanding
arxiv_id: '2403.20180'
source_url: https://arxiv.org/abs/2403.20180
tags:
- tmlu
- arxiv
- llms
- preprint
- chinese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces TMLU, a comprehensive evaluation suite for
  assessing large language models in the context of Taiwanese Mandarin. TMLU covers
  37 subjects across social science, STEM, humanities, Taiwan-specific content, and
  others, ranging from middle school to professional levels.
---

# Measuring Taiwanese Mandarin Language Understanding
## Quick Facts
- arXiv ID: 2403.20780
- Source URL: https://arxiv.org/abs/2403.20780
- Authors: Po-Heng Chen; Sijia Cheng; Wei-Lin Chen; Yen-Ting Lin; Yun-Nung Chen
- Reference count: 18
- Primary result: Proprietary multilingual models outperform open-weight models on Taiwanese Mandarin tasks, with room for improvement in localized models

## Executive Summary
This work introduces TMLU, a comprehensive evaluation suite for assessing large language models in the context of Taiwanese Mandarin. TMLU covers 37 subjects across social science, STEM, humanities, Taiwan-specific content, and others, ranging from middle school to professional levels. It includes manually curated chain-of-thought-like explanations to evaluate complex reasoning skills. Experiments on 24 advanced LLMs show that proprietary multilingual models generally outperform open-weight models, with those tailored for Taiwanese Mandarin lagging behind Simplified-Chinese counterparts. The results indicate significant room for improvement and emphasize TMLU's goal to foster the development of localized Taiwanese-Mandarin LLMs.

## Method Summary
The paper evaluates 24 large language models on TMLU, a benchmark consisting of 2,981 multiple-choice questions across 37 subjects in Taiwanese Mandarin. The evaluation uses few-shot prompting with 5 demonstrations per subject, employing both direct answer and Chain-of-Thought (CoT) prompting strategies. Proprietary models use generation-based answer extraction while open-source models use likelihood-based extraction for direct answer and generation-based for CoT. Data is collected from PDF and Microsoft Word documents to minimize contamination risk, and MIN-K% PROB is applied to detect potential data leakage.

## Key Results
- Proprietary multilingual models outperform open-weight models on Taiwanese Mandarin tasks
- Models tailored specifically for Taiwanese Mandarin lag behind Simplified-Chinese counterparts
- CoT prompting shows performance gains for flagship models on complex reasoning tasks
- Significant room for improvement identified in Taiwanese Mandarin language model capabilities

## Why This Works (Mechanism)
### Mechanism 1
- Claim: Traditional Chinese is underrepresented in existing benchmarks
- Mechanism: The paper identifies a gap in current evaluation frameworks that focus predominantly on Simplified Chinese, creating a need for localized Traditional Chinese benchmarks
- Core assumption: Traditional Chinese usage in Taiwan has distinct linguistic and cultural characteristics that differ from Simplified Chinese
- Evidence anchors:
  - [abstract] "Traditional Chinese, often referred to as Traditional Mandarin, has been notably underrepresented"
  - [section] "TC-Eval consists of a collection of existing datasets, mainly from conventional NLP tasks, and a newly introduced, MMLU-like multiple-choice dataset"
  - [corpus] Weak evidence - corpus shows related works but no direct comparison of Traditional vs Simplified Chinese representation
- Break condition: If the linguistic differences between Traditional and Simplified Chinese are minimal or if localization benefits are negligible

### Mechanism 2
- Claim: Data contamination is a significant concern for LLM evaluation
- Mechanism: The paper implements data collection from PDF and Microsoft Word documents rather than web text to minimize pre-training overlap
- Core assumption: Web-scraped data is more likely to be present in LLM pre-training corpora than document-based sources
- Evidence anchors:
  - [abstract] "To minimize the risk of test data contamination, we follow Huang et al. 2023 by collecting data sourced in PDF and Microsoft Word documents from the internet"
  - [section] "We apply MIN-K% PROB on 2000 sampled instances from TMLU and TMMLU-plus with 6 different base models"
  - [corpus] Weak evidence - corpus shows related works on LLM evaluation but limited focus on contamination specifically
- Break condition: If the document-based approach doesn't significantly reduce contamination compared to web sources

### Mechanism 3
- Claim: Chain-of-thought prompting improves reasoning performance for large models
- Mechanism: Manual CoT explanations are curated and applied to evaluation, showing performance gains for flagship models
- Core assumption: Complex reasoning tasks benefit from step-by-step reasoning chains
- Evidence anchors:
  - [abstract] "we curate chain-of-thought-like few-shot explanations for each subject to facilitate the evaluation of complex reasoning skills"
  - [section] "Compared to direct answer, CoT prompting includes an explanation... before the final answer prediction"
  - [corpus] Weak evidence - corpus shows related works on CoT but limited specific evidence for Traditional Chinese context
- Break condition: If smaller models or simpler reasoning tasks don't benefit from CoT prompting

## Foundational Learning
- Concept: Multiple-choice question evaluation format
  - Why needed here: TMLU uses multiple-choice questions to assess knowledge and reasoning
  - Quick check question: How does multiple-choice format differ from open-ended question evaluation in terms of model assessment?

- Concept: Chain-of-thought prompting methodology
  - Why needed here: CoT explanations are manually curated to evaluate complex reasoning capabilities
  - Quick check question: What is the difference between standard prompting and chain-of-thought prompting in LLM evaluation?

- Concept: Data contamination detection techniques
  - Why needed here: The paper uses MIN-K% PROB to detect potential data leakage in evaluation benchmarks
  - Quick check question: How does MIN-K% PROB work to identify potential pre-training data overlap?

## Architecture Onboarding
- Component map: Data collection → Processing → Benchmark creation → Model evaluation → Analysis
- Critical path: Document sourcing → Question curation → CoT explanation creation → Evaluation pipeline → Result analysis
- Design tradeoffs: Traditional vs Simplified Chinese representation, document-based vs web-scraped data, manual vs automated CoT creation
- Failure signatures: Contamination in test data, incorrect LaTeX formatting, insufficient CoT explanations, model evaluation inconsistencies
- First 3 experiments:
  1. Test MIN-K% PROB contamination detection on a small subset of questions
  2. Verify LaTeX formatting consistency across all STEM questions
  3. Evaluate a baseline model on direct answer vs CoT prompting to confirm expected performance differences

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What is the optimal model architecture for Traditional Chinese LLMs?
- Basis in paper: explicit
- Why unresolved: The paper shows that current open-weight models tailored for Taiwanese Mandarin lag behind Simplified-Chinese counterparts, but doesn't identify specific architectural improvements needed.
- What evidence would resolve it: Systematic comparison of different model architectures (e.g., different attention mechanisms, tokenizers, pre-training objectives) on Traditional Chinese benchmarks.

### Open Question 2
- Question: How does test data contamination affect evaluation of Chinese LLMs?
- Basis in paper: explicit
- Why unresolved: While the paper investigates data contamination using MIN-K% PROB, it doesn't quantify the impact on model performance or establish guidelines for contamination thresholds.
- What evidence would resolve it: Controlled experiments measuring performance degradation as contamination increases, plus analysis of contamination sources in existing benchmarks.

### Open Question 3
- Question: What is the relationship between model size and performance on Traditional Chinese tasks?
- Basis in paper: explicit
- Why unresolved: The paper shows that larger models generally perform better, but doesn't establish whether this relationship holds for Traditional Chinese specifically or identify potential saturation points.
- What evidence would resolve it: Scaling studies comparing models of different sizes trained on the same Traditional Chinese data, examining performance across different task complexities.

### Open Question 4
- Question: How do different tokenization strategies affect Traditional Chinese LLM performance?
- Basis in paper: inferred
- Why unresolved: The paper doesn't discuss tokenization choices, though these could significantly impact performance given the differences between Traditional and Simplified Chinese.
- What evidence would resolve it: Controlled experiments comparing different tokenization approaches (character-level, word-level, subword) on Traditional Chinese benchmarks.

## Limitations
- Limited quantitative evidence demonstrating the extent of Traditional Chinese underrepresentation in existing benchmarks
- No empirical validation comparing contamination rates between document-based and web-scraped data collection methods
- CoT explanation curation process may introduce bias without systematic evaluation of explanation quality across different reasoning types

## Confidence
- High Confidence: Experimental methodology for evaluating 24 LLMs on TMLU is clearly described and reproducible
- Medium Confidence: Core finding that proprietary models outperform open-weight models is supported, but analysis could be deeper regarding capability differences
- Low Confidence: Assertion that Traditional Chinese is "notably underrepresented" lacks quantitative comparative evidence

## Next Checks
1. Run MIN-K% PROB analysis on a subset of TMLU questions using multiple base models and compare contamination rates against a web-scraped benchmark of similar size
2. Conduct controlled experiments comparing model performance on Traditional vs Simplified Chinese versions of identical content to measure actual linguistic differences
3. Perform ablation studies on CoT explanations by testing model performance with different levels of explanation detail across various reasoning task types