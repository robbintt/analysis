---
ver: rpa2
title: Where Do We Go from Here? Multi-scale Allocentric Relational Inference from
  Natural Spatial Descriptions
arxiv_id: '2402.16364'
source_url: https://arxiv.org/abs/2402.16364
tags:
- instructions
- spatial
- goal
- task
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the Rendezvous (RVS) dataset, which includes
  10,404 examples of English geospatial instructions for reaching a target location
  using map-knowledge. The dataset focuses on instructions based on survey knowledge,
  which provide a complete view of the environment and capture its overall structure.
---

# Where Do We Go from Here? Multi-scale Allocentric Relational Inference from Natural Spatial Descriptions

## Quick Facts
- arXiv ID: 2402.16364
- Source URL: https://arxiv.org/abs/2402.16364
- Reference count: 19
- Key outcome: Introduces RVS dataset with 10,404 examples of English geospatial instructions using survey knowledge, showing richer allocentric relations and requiring simultaneous resolution of multiple spatial relations

## Executive Summary
This paper introduces the Rendezvous (RVS) dataset, which contains 10,404 examples of English geospatial instructions for navigating to target locations using map knowledge. Unlike traditional navigation datasets that rely on route knowledge (step-by-step egocentric directions), RVS focuses on survey knowledge that provides a complete view of the environment and captures its overall structure. The dataset is specifically designed to test models' ability to reason about allocentric spatial relations - those that describe landmark positions relative to each other rather than relative to the agent. The authors demonstrate that RVS exhibits a richer use of spatial allocentric relations and requires resolving more spatial relations simultaneously compared to previous text-based navigation benchmarks.

## Method Summary
The approach uses a T5-based sequence-to-sequence model that generates paths through S2-cells representing geographic coordinates. The model can operate with text-only input or incorporate a graph-based representation of the environment. The graph connects location nodes to S2-cells using node2vec embeddings, creating a joint embedding space for semantic entities and geographic positions. The model is trained on the RVS dataset by maximizing the log-likelihood of high-level paths. A novel zero-shot city split evaluation is introduced where models are trained on one city and tested on another unseen city, creating a challenging testbed for generalization to new environments.

## Key Results
- RVS dataset shows significantly richer use of allocentric spatial relations compared to previous navigation benchmarks
- Models trained on RVS achieve 65.8% 100m accuracy on the seen-city split, outperforming text-only baselines
- Zero-shot city split evaluation reveals substantial out-of-vocabulary challenges, with models achieving only 40.3% 100m accuracy on unseen cities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Survey knowledge-based instructions enable more accurate geospatial inference than egocentric route knowledge.
- Mechanism: Survey knowledge captures allocentric spatial relations and global structure, allowing models to reason about landmark positions relative to each other rather than relative to the agent's current position.
- Core assumption: The environment representation (map graph) adequately encodes spatial relationships between landmarks that can be extracted from survey-based instructions.
- Evidence anchors:
  - [abstract]: "descriptions based on knowledge acquired through a map provide a complete view of the environment and capture its overall structure"
  - [section]: "instructions based on survey knowledge use allocentric rather than egocentric spatial relations"
  - [corpus]: Weak evidence - corpus shows related work on allocentric navigation but lacks direct comparison to survey vs route knowledge performance
- Break condition: If the map graph fails to represent complex spatial relationships (e.g., non-linear arrangements, irregular shapes), the survey knowledge advantage disappears.

### Mechanism 2
- Claim: Graph-based environmental representation improves grounding of spatial language to physical locations.
- Mechanism: The heterogeneous graph connects semantic entities (landmarks) to positional S2-cells, creating a joint embedding space that allows language models to map textual descriptions to geographic coordinates.
- Core assumption: Random walks on the graph can learn meaningful semantic-positional relationships that transfer to instruction interpretation.
- Evidence anchors:
  - [section]: "To learn a joint embedding space for locations and S2-cells, we compute random walks on the graph using node2vec algorithm"
  - [abstract]: "The World as a Graph" section describes connecting location nodes to S2-cells
  - [corpus]: Weak evidence - corpus mentions related work on graph embeddings but doesn't show performance gains from this specific approach
- Break condition: If the graph construction fails to capture relevant connections (e.g., missing edges between nearby but non-adjacent landmarks), the embedding becomes ineffective.

### Mechanism 3
- Claim: Zero-shot city split creates a challenging generalization test that exposes limitations in current spatial reasoning models.
- Mechanism: Training on one city and testing on another forces models to handle novel entities and spatial relations without relying on memorization of specific landmarks.
- Core assumption: The distribution of spatial relations and entity types is sufficiently different between cities to require true generalization rather than pattern matching.
- Evidence anchors:
  - [section]: "Our out-of-vocabulary (OOV) analysis shows that, unlike previous navigation datasets, RVS presents a challenge with novel entities in a city-split setup"
  - [abstract]: "This new zero-shot setup is a challenging testbed for models' ability to generalize to new environments"
  - [corpus]: Moderate evidence - corpus shows zero-shot learning is an active research area but doesn't provide specific results for this dataset
- Break condition: If cities share too many similar spatial patterns or if the model can rely on generic spatial reasoning without city-specific knowledge, the zero-shot challenge diminishes.

## Foundational Learning

- Concept: Allocentric vs. Egocentric spatial relations
  - Why needed here: The RVS task requires understanding directions like "north of" rather than "on your right," which fundamentally changes how spatial reasoning is performed
  - Quick check question: Given "The park is east of the library," what direction would the park be from the library's perspective? (Answer: West)

- Concept: Graph representation learning
  - Why needed here: The model uses a heterogeneous graph to connect semantic landmarks to their geographic positions, requiring understanding of how graph embeddings capture relationships
  - Quick check question: If landmark A is connected to S2-cell X and S2-cell X is connected to S2-cell Y, what relationship might the model learn between A and Y? (Answer: Proximity or directional relationship)

- Concept: Zero-shot generalization
  - Why needed here: The city-split evaluation tests whether models can handle completely new environments without retraining, which is critical for real-world deployment
  - Quick check question: If a model trained on Manhattan sees "near the Liberty Bell" in Philadelphia instructions, what knowledge must it lack to demonstrate zero-shot capability? (Answer: Any specific knowledge about Philadelphia landmarks)

## Architecture Onboarding

- Component map: Text instruction → T5 encoder → Graph embeddings → Decoder → Geographic coordinates
- Critical path: Text instruction → T5 encoder → Graph embeddings → Decoder → Geographic coordinates
- Design tradeoffs: Using S2-cells provides global uniformity but may lack granularity for distinguishing close landmarks; graph embeddings capture relationships but add complexity
- Failure signatures: High mean error with low 100m accuracy suggests poor localization; good 250m accuracy but poor 100m suggests coarse location but poor fine-grained reasoning
- First 3 experiments:
  1. Test STOP baseline on development set to establish minimum performance
  2. Train T5 model on seen-city split to verify basic functionality
  3. Train T5+GRAPH model and compare to T5 to validate graph contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the T5+GRAPH model on the RVS task compare to human performance in terms of 100m and 250m accuracy, mean error, median error, and maximum error?
- Basis in paper: Explicit, Section 7 and Table 5
- Why unresolved: The paper provides human performance as an upper bound for the RVS task, but it does not directly compare the T5+GRAPH model's performance to human performance in terms of specific metrics like 100m and 250m accuracy, mean error, median error, and maximum error.
- What evidence would resolve it: A direct comparison of the T5+GRAPH model's performance to human performance in terms of 100m and 250m accuracy, mean error, median error, and maximum error.

### Open Question 2
- Question: How does the performance of the T5+GRAPH model on the RVS task change when using a higher level of S2-Cells (e.g., level 18 instead of level 16)?
- Basis in paper: Inferred, Section 5 and Table 5
- Why unresolved: The paper mentions that the S2-Cell level was searched in [15, 16, 17, 18] and 16 was chosen, but it does not provide information on how the model's performance would change with a higher level of S2-Cells.
- What evidence would resolve it: Results showing the performance of the T5+GRAPH model on the RVS task using different levels of S2-Cells, particularly level 18.

### Open Question 3
- Question: How does the performance of the T5+GRAPH model on the RVS task change when using a different number of clusters for the quantization process (e.g., 100 instead of 150)?
- Basis in paper: Inferred, Section 5 and Table 5
- Why unresolved: The paper mentions that the number of clusters for the quantization process was searched in [50, 100, 150, 200, 250] and 150 was chosen, but it does not provide information on how the model's performance would change with a different number of clusters.
- What evidence would resolve it: Results showing the performance of the T5+GRAPH model on the RVS task using different numbers of clusters for the quantization process, particularly 100.

### Open Question 4
- Question: How does the performance of the T5+GRAPH model on the RVS task change when using a different number of quantization layers (e.g., 1 instead of 2)?
- Basis in paper: Inferred, Section 5 and Table 5
- Why unresolved: The paper mentions that 2 quantization layers were used, but it does not provide information on how the model's performance would change with a different number of quantization layers.
- What evidence would resolve it: Results showing the performance of the T5+GRAPH model on the RVS task using different numbers of quantization layers, particularly 1.

### Open Question 5
- Question: How does the performance of the T5+GRAPH model on the RVS task change when using a different learning rate (e.g., 1e-3 instead of 1e-4)?
- Basis in paper: Inferred, Section 5 and Table 5
- Why unresolved: The paper mentions that a learning rate of 1e-4 was used, but it does not provide information on how the model's performance would change with a different learning rate.
- What evidence would resolve it: Results showing the performance of the T5+GRAPH model on the RVS task using different learning rates, particularly 1e-3.

## Limitations

- The performance gains from the graph-based approach are modest (57.8% to 65.8% 100m accuracy), raising questions about whether the added complexity is justified
- Direct empirical comparisons between survey knowledge and route knowledge performance are lacking, making it unclear if the allocentric focus translates to better spatial reasoning
- The zero-shot generalization evaluation may not fully test true generalization, as the nature of the generalization barrier (novel entities vs. spatial patterns) remains unclear

## Confidence

- High Confidence: The RVS dataset contains more allocentric spatial relations than existing navigation datasets
- Medium Confidence: Survey knowledge-based instructions enable more accurate geospatial inference than egocentric route knowledge
- Low Confidence: The heterogeneous graph representation significantly improves model performance on the RVS task

## Next Checks

1. **Direct Dataset Comparison**: Train identical models on both RVS and a traditional navigation dataset (e.g., Touchdown or R2R) to quantify whether the allocentric focus in RVS translates to measurable performance differences in spatial reasoning tasks.

2. **Graph Necessity Analysis**: Implement a simplified spatial reasoning model that uses only location coordinates and basic spatial relationships (without the full heterogeneous graph) and compare its performance to the T5+GRAPH model to determine if the graph complexity is essential.

3. **Spatial Pattern Generalization**: Analyze whether models struggle more with novel entities or with novel spatial configurations in the zero-shot city split by creating controlled experiments where either entities or spatial patterns are held constant while varying the other factor.