---
ver: rpa2
title: 'Towards Better Understanding of Cybercrime: The Role of Fine-Tuned LLMs in
  Translation'
arxiv_id: '2404.01940'
source_url: https://arxiv.org/abs/2404.01940
tags:
- translation
- russian
- messages
- fine-tuned
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes fine-tuning large language models (LLMs) to
  translate Russian cybercrime communications more accurately than existing machine
  translation methods. The authors applied their approach to NoName057(16) hacktivist
  Telegram chats, comparing a fine-tuned GPT-3.5-turbo model against human translators
  and standard MT tools.
---

# Towards Better Understanding of Cybercrime: The Role of Fine-Tuned LLMs in Translation

## Quick Facts
- arXiv ID: 2404.01940
- Source URL: https://arxiv.org/abs/2404.01940
- Reference count: 40
- Key outcome: Fine-tuned LLM outperforms standard MT tools for translating Russian cybercrime communications, with 64% human preference and 430-23,000× cost reduction

## Executive Summary
This paper proposes fine-tuning large language models (LLMs) to translate Russian cybercrime communications more accurately than existing machine translation methods. The authors applied their approach to NoName057(16) hacktivist Telegram chats, comparing a fine-tuned GPT-3.5-turbo model against human translators and standard MT tools. In blind human evaluations, the fine-tuned model was preferred 64% of the time, and BLEU/METEOR/TER metrics also favored it over the base model. The method achieves high-fidelity translations at 430-23,000× lower cost than human translators, with better handling of URLs, emojis, slang, and humor.

## Method Summary
The study collected 130 messages from NoName057(16) hacktivist group Telegram chats, split into 100 for training/validation and 30 for testing. Ground truth translations were created by native Russian speakers with cybersecurity expertise. The base GPT-3.5-turbo-0125 model was fine-tuned with curated translations and vocabulary corrections using a prompt emphasizing respect for URLs, names, dates, and slang. Evaluation involved human blind tests with 7 respondents and automatic metrics (BLEU, METEOR, TER).

## Key Results
- Fine-tuned LLM preferred over base model by human translators in 64.08% of blind comparisons
- Automatic metrics showed improvements: BLEU 0.347, METEOR 0.711, TER 47.792
- Cost reduction of 430-23,000× compared to human translators
- Better handling of URLs, emojis, slang, and humor compared to traditional MT tools

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning a pre-trained LLM on cybercrime-specific Russian text improves translation quality by adapting the model to specialized jargon and contextual nuances.
- Mechanism: The base LLM is trained on general data but lacks domain-specific knowledge. Fine-tuning injects domain knowledge through a curated dataset of 130 messages with expert translations and vocabulary corrections, allowing the model to learn patterns unique to cybercrime communication.
- Core assumption: The fine-tuning dataset is representative of linguistic patterns in cybercrime communications and that 130 messages provide sufficient signal to adapt the model without overfitting.
- Evidence anchors:
  - [abstract]: "Our results show that our fine-tuned LLM model is better, faster, more accurate, and able to capture nuances of the language."
  - [section]: "The Russian expert selected an LLM model as the best model in the training dataset. We decided to fine-tune it to adapt it to the nuances of the Russian language cybercriminal world."
  - [corpus]: Weak evidence. The corpus shows related work on LLM use in cybercrime contexts but no direct validation of the fine-tuning approach's effectiveness.

### Mechanism 2
- Claim: Human evaluation by native Russian speakers with cybersecurity knowledge provides more reliable assessment of translation quality than automatic metrics.
- Mechanism: The study employed 7 respondents who self-assessed as C1/C2 English speakers and intermediate-to-expert in cybersecurity. They compared base vs. fine-tuned translations in blind tests, selecting the better translation.
- Core assumption: The evaluators' expertise and language proficiency are sufficient to judge translation quality accurately, and their subjective preferences align with objective translation quality.
- Evidence anchors:
  - [abstract]: "In a blind test, human translators choose the fine-tuned model as the best translation in 64.08% of the cases."
  - [section]: "Our first evaluation of whether our fine-tuned LLM model is better than the non-fine-tuned LLM model was done using a new test group of human translators."
  - [corpus]: Weak evidence. The corpus includes related work on human vs. machine translation but no direct evidence supporting this specific human evaluation methodology.

### Mechanism 3
- Claim: Fine-tuned LLMs outperform traditional machine translation (MT) methods in handling specialized content like URLs, emojis, slang, and humor.
- Mechanism: Traditional MT methods often fail with non-literal content. The fine-tuned LLM was explicitly trained to respect URLs, names, dates, and informal language.
- Core assumption: The fine-tuning prompt and dataset effectively taught the model to handle these specific challenges better than general MT.
- Evidence anchors:
  - [abstract]: "Our method shows it is possible to achieve high-fidelity translations and significantly reduce costs by a factor ranging from 430 to 23,000 compared to a human translator."
  - [section]: "When analysing the human evaluation, we found that it was not an easy task for them... This further highlights the importance of having automated tools that are not affected by such emotions, take no sides, and do not get tired."
  - [corpus]: Weak evidence. The corpus mentions LLM use in cybercrime translation but doesn't directly validate this specific comparison with traditional MT.

## Foundational Learning

- Concept: Domain-specific fine-tuning of language models
  - Why needed here: General LLMs lack knowledge of cybercrime-specific Russian terminology, cultural references, and communication styles. Fine-tuning adapts the model to this specialized domain.
  - Quick check question: What is the minimum size of a fine-tuning dataset needed to achieve meaningful improvement without overfitting?

- Concept: Human evaluation vs. automatic metrics in translation quality assessment
  - Why needed here: BLEU, METEOR, and TER may not capture subjective aspects like fluency, cultural appropriateness, or domain-specific accuracy that human evaluators can judge.
  - Quick check question: When would human evaluation be preferred over automatic metrics in translation quality assessment?

- Concept: Cost-benefit analysis of AI translation vs. human translation
  - Why needed here: The paper claims 430-23,000× cost reduction. Understanding the methodology behind this calculation is crucial for evaluating the practical impact.
  - Quick check question: What factors should be considered when comparing the cost of AI translation to human translation services?

## Architecture Onboarding

- Component map: Telegram API scraping (Spylegram tool) -> SQLite database -> 130 message selection -> Human expert translation (ground truth) -> 8 baseline models -> Expert selection of best model -> Fine-tuning with 125 messages -> Human blind test (7 respondents) + Automatic metrics (BLEU, METEOR, TER)
- Critical path: Data collection -> Dataset creation -> Fine-tuning -> Human evaluation -> Result analysis
- Design tradeoffs:
  - Dataset size vs. quality: 130 messages may be small but ensures high-quality ground truth
  - Human vs. automatic evaluation: Humans provide nuanced feedback but are slower and costlier
  - Closed-source vs. open-source models: Better performance but less transparency and control
- Failure signatures:
  - Poor human evaluation results: Model doesn't capture domain-specific nuances
  - Low automatic metric scores: Model fails on general translation quality
  - High variance in human responses: Evaluation methodology may be flawed or respondents lack consistency
- First 3 experiments:
  1. Test the fine-tuned model on a held-out test set of 30 messages not used in training to measure generalization
  2. Compare the fine-tuned model against a randomly initialized model fine-tuned on the same data to validate the base model's importance
  3. Conduct ablation studies removing different components of the fine-tuning prompt to identify which instructions matter most

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the fine-tuned LLM model perform when translating other types of cybercrime-related communications beyond hacktivist Telegram chats, such as underground forum posts or dark web marketplaces?
- Basis in paper: [inferred] The paper focuses on translating hacktivist Telegram chats, but mentions that the methodology could be applied to other cybercrime-related communications.
- Why unresolved: The paper does not provide evidence of the model's performance on other types of cybercrime communications, which may have different linguistic characteristics and jargon.
- What evidence would resolve it: Testing the fine-tuned LLM model on a diverse dataset of cybercrime-related communications from various sources and comparing its performance to existing machine translation methods would provide insights into its generalizability and effectiveness.

### Open Question 2
- Question: What are the potential biases and limitations of using closed-source LLM models like OpenAI's GPT-3.5-turbo for translating cybercrime communications, and how can these be addressed?
- Basis in paper: [explicit] The paper mentions that using closed platforms like OpenAI has challenges, including automatic cancellations due to violations of terms and conditions and the inability to share fine-tuned models openly with the community.
- Why unresolved: The paper does not delve into the specific biases and limitations of using closed-source LLM models for this purpose, nor does it propose solutions to address these issues.
- What evidence would resolve it: Conducting a thorough analysis of the biases and limitations of using closed-source LLM models for translating cybercrime communications, along with exploring alternative approaches such as open-source models or hybrid human-AI systems, would help address these concerns.

### Open Question 3
- Question: How can the fine-tuned LLM model be integrated into a real-time intelligence analysis pipeline to provide timely and accurate translations of cybercrime communications for defense purposes?
- Basis in paper: [explicit] The paper mentions that future work includes the use of the output in a pipeline of intelligence analysis to produce timely intelligence and analysis for defense.
- Why unresolved: The paper does not provide details on how the fine-tuned LLM model can be integrated into a real-time intelligence analysis pipeline or the specific challenges and requirements for such integration.
- What evidence would resolve it: Developing a proof-of-concept system that integrates the fine-tuned LLM model into a real-time intelligence analysis pipeline, along with evaluating its performance, scalability, and security implications, would demonstrate its practical applicability for defense purposes.

## Limitations
- Small fine-tuning dataset (130 messages) raises concerns about overfitting and generalizability
- Limited to one hacktivist group (NoName057(16)), may not capture full diversity of Russian cybercrime communications
- Human evaluation involved only 7 respondents with varying expertise levels, introducing potential subjectivity

## Confidence
- High Confidence: The 430-23,000× cost reduction claim is well-supported by the methodology and calculations presented
- Medium Confidence: The claim that the fine-tuned model is "better, faster, more accurate" has moderate support from 64.08% human preference but mixed automatic metric results
- Low Confidence: The generalizability claim to "a broad range of languages and contexts" extends beyond what the study demonstrates

## Next Checks
1. Evaluate the fine-tuned model on a held-out test set of 30+ messages from different Russian cybercrime sources to assess generalization beyond NoName057(16)
2. Conduct ablation studies removing different components of the fine-tuning prompt to determine which instructions contribute most to performance improvements
3. Replicate the 430-23,000× cost reduction calculation using different assumptions about human translator rates and model API costs to verify economic claims