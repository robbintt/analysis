---
ver: rpa2
title: 'Mission Impossible: A Statistical Perspective on Jailbreaking LLMs'
arxiv_id: '2408.01420'
source_url: https://arxiv.org/abs/2408.01420
tags:
- arxiv
- harmful
- prompt
- preprint
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of jailbreaking large language
  models (LLMs), where adversaries can manipulate input prompts to elicit harmful
  behavior despite safety alignment. The authors propose a theoretical framework that
  decomposes prompts into query and concept pairs, allowing them to quantify the strength
  of potential adversaries.
---

# Mission Impossible: A Statistical Perspective on Jailbreaking LLMs

## Quick Facts
- arXiv ID: 2408.01420
- Source URL: https://arxiv.org/abs/2408.01420
- Reference count: 40
- Authors: Jingtong Su, Julia Kempe, Karen Ullrich

## Executive Summary
This paper addresses the fundamental problem of jailbreaking large language models (LLMs), where adversaries can manipulate input prompts to elicit harmful behavior despite safety alignment. The authors propose a theoretical framework that decomposes prompts into query and concept pairs, allowing them to quantify the strength of potential adversaries. They prove that pretrained LLMs will mimic harmful behavior if present in the training corpus, and that jailbreaking is unpreventable even after safety alignment. To improve safety alignment, they introduce a simple modification to the RLHF objective, called E-RLHF, which aims to increase the likelihood of safe responses.

## Method Summary
The authors propose a novel statistical framework for analyzing LLM safety that decomposes prompts into query-concept pairs and introduces a theoretical notion of alignment. They prove that pretrained LLMs will generalize harmful behavior from their training corpus and that jailbreaking remains possible even after safety alignment. To address this, they modify the RLHF objective by incorporating a safe prior for harmful prompts, using safe prefixes to transform harmful concepts into safe ones during training. The method involves three phases: collecting preference data, training with modified DPO objective using safe prefixes for harmful prompts, and evaluating on standard safety benchmarks.

## Key Results
- E-RLHF outperforms RLHF on HarmBench and AdvBench safety benchmarks without sacrificing MT-Bench performance
- Theoretical proof that pretrained LLMs will mimic harmful behavior if present in training corpus
- Demonstration that jailbreaking probability is bounded away from zero under reasonable assumptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pretrained LLMs will mimic harmful behavior if present in the training corpus
- Mechanism: PAC-Bayesian generalization bound shows that if pretraining reduces empirical loss on harmful data, the model must generalize to produce harmful explanations on harmful prompts
- Core assumption: Framework's decomposition of prompts into query/concept pairs and assumption that concepts uniquely determine explanation support
- Evidence anchors: Theorem 1 shows language models mimic world well on direct prompts when pretraining reduces training loss

### Mechanism 2
- Claim: Jailbreaking is unpreventable even after safety alignment
- Mechanism: Statistical notion of alignment fails because output distribution doesn't concentrate on safe responses, allowing adversary to move output to harmful zone
- Core assumption: Induced posterior distribution over language models isn't concentrated in extremely safe area of output simplex
- Evidence anchors: Lower-bound on jailbreaking probability shows it's unpreventable under reasonable assumptions

### Mechanism 3
- Claim: E-RLHF improves safety by expanding safety zone
- Mechanism: Modifying RLHF objective to use safe prior for harmful prompts increases likelihood of safe responses by erasing harmful explanations and adding safe explanations to support
- Core assumption: Safe transformation can be done by appending prefix to harmful prompts
- Evidence anchors: LE-DPO objective uses safe concept replacements during training to improve safety

## Foundational Learning

- Concept: PAC-Bayesian generalization bounds
  - Why needed here: Provides theoretical foundation for why pretrained LLMs will produce harmful behavior if trained on harmful data
  - Quick check question: What does the PAC-Bayesian bound tell us about the relationship between training loss and generalization for language models?

- Concept: Adversarial robustness
  - Why needed here: Framework borrows concepts from adversarial attacks to bound strength of jailbreaking adversaries
  - Quick check question: How does notion of "ϵ-bounded adversary" in jailbreaking context relate to adversarial examples in computer vision?

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: Understanding current alignment strategy that paper proposes to modify
  - Quick check question: What are three phases of typical RLHF process and how does E-RLHF modify third phase?

## Architecture Onboarding

- Component map: Prompt → Query-Concept Decomposition → Language Model → Distribution over Explanations → Safety Analysis
- Critical path: Pretraining → Alignment (RLHF/E-RLHF) → Jailbreaking analysis → Safety evaluation
- Design tradeoffs: Framework trades model expressiveness for theoretical tractability by decomposing prompts into query/concept pairs and assuming concepts uniquely determine explanation support
- Failure signatures: If induced posterior distribution is concentrated in safety zone, if training corpus doesn't contain harmful examples, or if safe prefix doesn't effectively transform harmful prompts
- First 3 experiments:
  1. Evaluate ASR on HarmBench and AdvBench datasets for DPO vs E-DPO models
  2. Ablate effect of different safe prefixes on model safety
  3. Evaluate MT-Bench scores to ensure E-DPO doesn't sacrifice helpfulness for safety

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does E-RLHF approach scale when applied to larger language models with significantly more parameters?
- Basis in paper: [inferred] Paper demonstrates effectiveness on specific models but does not discuss scaling to larger models
- Why unresolved: Authors do not provide theoretical or empirical analysis on how E-RLHF performs as model size increases
- What evidence would resolve it: Experiments showing E-RLHF performance across models of varying sizes, particularly comparing smaller models to those with billions of parameters

### Open Question 2
- Question: What are theoretical limitations of using single safe prefix versus individualized prompt transformations in E-RLHF?
- Basis in paper: [explicit] Paper mentions that individualized transformations could improve results but only tests universal prefix approach
- Why unresolved: Authors acknowledge potential improvements but do not provide quantitative comparisons between universal and individualized approaches
- What evidence would resolve it: Comparative studies showing safety metrics for both universal prefix and individualized transformation approaches across various harmful prompts

### Open Question 3
- Question: How does jailbreaking probability change under different distance metrics beyond ℓp and Jensen-Shannon Divergence?
- Basis in paper: [explicit] Authors mention these metrics but acknowledge analysis could extend to other distance measures
- Why unresolved: Paper only proves results for specific distance metrics, leaving open questions about robustness across alternative measures
- What evidence would resolve it: Formal proofs extending Theorem 2 to other distance metrics, or empirical validation showing jailbreaking probabilities under various distance measures

## Limitations

- Theoretical framework relies on simplifying assumptions about prompt decomposition and concept-explanation relationships that may not capture full complexity of real-world jailbreaking attacks
- Empirical evaluation only tests against subset of jailbreaking techniques documented in literature, limiting generalizability of safety claims
- Analysis assumes static training corpus, but modern LLMs are frequently updated with new data that could introduce harmful behaviors not accounted for in original theoretical bounds

## Confidence

**High Confidence**: Theoretical foundation showing pretrained LLMs will mimic harmful behavior if present in training corpus is well-supported by PAC-Bayesian generalization theory

**Medium Confidence**: Claim that jailbreaking is unpreventable under reasonable assumptions is theoretically supported but relies on assumptions about posterior distribution concentration

**Medium Confidence**: Empirical results showing E-RLHF improves safety while maintaining performance are promising but based on limited set of benchmarks

## Next Checks

1. **Cross-Attack Robustness Test**: Evaluate E-RLHF models against comprehensive suite of jailbreaking techniques beyond HarmBench and AdvBench, including adaptive attacks targeting safe prefix mechanism

2. **Longitudinal Safety Analysis**: Monitor model safety over extended training periods and after fine-tuning on new data to assess whether theoretical bounds hold in dynamic deployment scenarios

3. **Real-World Red Teaming**: Deploy E-RLHF model in controlled red teaming exercise with human adversaries to test whether theoretical safety improvements translate to practical robustness against creative jailbreaking attempts