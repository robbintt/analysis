---
ver: rpa2
title: 'LayerMerge: Neural Network Depth Compression through Layer Pruning and Merging'
arxiv_id: '2406.12837'
source_url: https://arxiv.org/abs/2406.12837
tags:
- layer
- network
- latency
- pruning
- depth
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LayerMerge addresses the kernel size expansion problem in existing
  depth compression methods, where merging consecutive convolution layers leads to
  significantly larger kernels that undermine latency gains. The proposed method jointly
  optimizes the selection of both convolution layers and activation layers to remove,
  thereby bypassing the kernel size issue.
---

# LayerMerge: Neural Network Depth Compression through Layer Pruning and Merging

## Quick Facts
- arXiv ID: 2406.12837
- Source URL: https://arxiv.org/abs/2406.12837
- Reference count: 29
- Primary result: Addresses kernel size expansion in depth compression through joint optimization of convolution and activation layers

## Executive Summary
LayerMerge presents a novel approach to neural network depth compression that addresses a critical limitation in existing layer merging methods: kernel size expansion. Traditional layer merging techniques combine consecutive convolution layers, resulting in significantly larger kernels that undermine latency gains. LayerMerge overcomes this by jointly optimizing the selection of both convolution and activation layers to remove, effectively bypassing the kernel size issue while maintaining model performance.

The method employs a surrogate optimization framework that maximizes importance values under latency constraints, solved efficiently using dynamic programming. This approach constructs latency and importance lookup tables that leverage the combinatorial structure of the problem. Experimental results demonstrate consistent improvements across image classification tasks (ResNet-34, MobileNetV2) and generation tasks (DDPM), achieving better speed-accuracy tradeoffs than existing depth compression and layer pruning methods.

## Method Summary
LayerMerge addresses the fundamental challenge in neural network depth compression where merging consecutive convolution layers leads to kernel size expansion, negating latency improvements. The method introduces a novel formulation that jointly optimizes both convolution and activation layer removal decisions. Rather than merging layers, LayerMerge strategically removes entire layers while maintaining model functionality.

The core innovation lies in formulating a surrogate optimization problem that maximizes importance values subject to latency constraints. This problem is solved exactly using dynamic programming, which enables efficient computation of the optimal layer removal configuration. The approach constructs lookup tables for latency and importance values, exploiting the combinatorial structure inherent in layer selection problems. This enables LayerMerge to achieve superior speed-accuracy tradeoffs compared to traditional layer merging approaches.

## Key Results
- ResNet-34: 1.36× speedup with 74.26% accuracy versus 1.24× speedup with 73.49% for baseline
- MobileNetV2-1.4: 1.99× speedup with 74.91% accuracy versus 1.93× speedup with 74.68% for baseline
- DDPM: 1.13× speedup with 4.16 FID versus 1.04× speedup with 4.21 FID for baseline

## Why This Works (Mechanism)
LayerMerge works by fundamentally changing the approach to depth compression. Instead of merging layers (which causes kernel size expansion), it removes entire layers while maintaining model functionality through strategic selection. The joint optimization of convolution and activation layers allows the method to bypass the kernel size problem entirely. By formulating the problem as maximizing importance under latency constraints and solving it with dynamic programming, LayerMerge finds optimal configurations that existing methods miss. The lookup table approach efficiently handles the combinatorial complexity of layer selection decisions.

## Foundational Learning
- Dynamic Programming: Why needed - Efficiently solves the combinatorial optimization problem for layer selection; Quick check - Verify the recurrence relations correctly capture layer dependencies
- Surrogate Optimization: Why needed - Transforms the intractable layer selection problem into a tractable form; Quick check - Confirm the surrogate objective correlates with actual model performance
- Importance Scoring: Why needed - Provides quantitative measure for layer significance in model performance; Quick check - Validate importance scores predict actual impact of layer removal
- Latency Estimation: Why needed - Enables optimization under practical deployment constraints; Quick check - Compare estimated vs measured latency on target hardware
- Combinatorial Structure: Why needed - Understanding layer interactions is crucial for optimal selection; Quick check - Verify the lookup table construction correctly enumerates valid configurations

## Architecture Onboarding

**Component Map:**
Input Model -> Importance Scoring -> Latency Estimation -> Dynamic Programming Optimization -> Compressed Model

**Critical Path:**
The critical path flows from importance scoring through latency estimation to dynamic programming optimization. Each component must be accurate for the final compressed model to achieve desired speed-accuracy tradeoffs.

**Design Tradeoffs:**
- Layer removal vs. merging: Removal avoids kernel expansion but may lose more information
- Exact vs. approximate optimization: Dynamic programming provides optimal solutions but requires careful implementation
- Importance vs. latency weighting: Balancing compression goals with performance targets

**Failure Signatures:**
- Suboptimal speed-accuracy tradeoffs indicating poor importance scoring
- Excessive accuracy degradation suggesting too aggressive layer removal
- Failed optimization suggesting issues with dynamic programming formulation

**First Experiments:**
1. Verify importance scoring correlates with actual layer impact by removing high/low importance layers
2. Test dynamic programming solution on small models to validate correctness
3. Compare estimated vs measured latency on a single layer removal to validate latency estimation

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Evaluation limited to standard benchmark models (ResNet-34, MobileNetV2) and single generative model (DDPM)
- Hardware-specific optimizations (GPU/TPU efficiency, mobile deployment) not quantified
- Latency measurements appear to be simulated/estimated rather than measured on actual hardware

## Confidence
- High confidence: Core algorithmic contribution and dynamic programming formulation
- Medium confidence: Experimental results showing consistent improvements on tested architectures
- Low confidence: Generalization to other model types, real-world deployment performance, hardware-specific efficiency gains

## Next Checks
1. Validate effectiveness on larger-scale models (ResNet-50/101, EfficientNet variants) and diverse model families
2. Conduct hardware measurements on actual target devices (mobile, edge devices) to verify simulated latency improvements
3. Evaluate performance on additional generative models and vision transformers to assess cross-task generalization