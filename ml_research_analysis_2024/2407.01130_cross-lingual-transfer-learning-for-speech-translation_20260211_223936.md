---
ver: rpa2
title: Cross-Lingual Transfer Learning for Speech Translation
arxiv_id: '2407.01130'
source_url: https://arxiv.org/abs/2407.01130
tags:
- speech
- languages
- translation
- whisper
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper demonstrates that Whisper's speech encoder produces language-invariant
  embeddings that map utterances from different languages into a shared semantic space,
  enabling cross-lingual transfer for speech translation. This is validated through
  speech-to-speech retrieval experiments showing high recall rates across language
  pairs, and by fine-tuning the decoder on English-to-Chinese data to improve translation
  quality for multiple source languages into Chinese.
---

# Cross-Lingual Transfer Learning for Speech Translation

## Quick Facts
- arXiv ID: 2407.01130
- Source URL: https://arxiv.org/abs/2407.01130
- Reference count: 23
- The paper demonstrates that Whisper's speech encoder produces language-invariant embeddings that enable cross-lingual transfer for speech translation

## Executive Summary
This paper investigates the cross-lingual transfer capabilities of Whisper's speech encoder by analyzing whether it produces language-invariant embeddings that map utterances from different languages into a shared semantic space. Through speech-to-speech retrieval experiments, the authors show that semantically similar utterances from different languages achieve high recall rates, indicating successful cross-lingual alignment. The study further demonstrates that fine-tuning Whisper's decoder on English-to-Chinese data improves translation performance for multiple source languages into Chinese, and that the model can translate previously unseen languages into English despite not being trained on them.

## Method Summary
The authors evaluate Whisper's cross-lingual transfer capabilities using speech-to-speech retrieval with the SeqSim similarity metric, fine-tuning the decoder on English-to-Chinese translation data while freezing the encoder, and testing zero-shot translation on previously unseen languages. They use three datasets: FLEURS (5 languages), CoV oST 2 (English-to-Chinese data), and MaSS (7 languages). The experiments measure BLEU and COMET scores for translation quality and R@1 recall rates for retrieval tasks. The approach demonstrates that cross-lingual transfer can expand Whisper's speech translation capabilities with limited data by leveraging the shared semantic space created by the encoder.

## Key Results
- Speech-to-speech retrieval shows R@1 recall rates significantly above random baseline (0.2%) across all 20 language pairs, indicating language-invariant embeddings
- Fine-tuning Whisper's decoder on English-to-Chinese data improves translation performance for multiple source languages (French, German, Japanese) into Chinese
- Whisper can translate previously unseen low-resource languages (Kabuverdianu, Asturian) into English with surprisingly good BLEU scores, despite never seeing these languages during training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Whisper's encoder produces language-invariant embeddings that map semantically similar utterances from different languages into a shared semantic space
- Mechanism: Speech-to-speech retrieval task shows high recall rates across language pairs, indicating embeddings are close in shared space
- Core assumption: Pre-training objective implicitly aligns embeddings across languages through exposure to diverse multilingual audio
- Evidence anchors:
  - [abstract] "Using speech-to-speech retrieval to analyse the audio representations generated by the encoder, we show that utterances from different languages are mapped to a shared semantic space."
  - [section 4.2] "On all 20 language pairs, SeqSim consistently achieved remarkably higher recall rates compared to a random baseline of 0.2%."
- Break condition: If retrieval performance drops to random baseline levels, the shared embedding space assumption would be invalid

### Mechanism 2
- Claim: Fine-tuning Whisper's decoder on English-to-Chinese data improves translation performance for multiple source languages into Chinese through cross-lingual transfer
- Mechanism: Shared encoder embeddings provide common representation space, allowing fine-tuned decoder to generalize across source languages
- Core assumption: Decoder can leverage shared semantic space to transfer knowledge from fine-tuning data to unseen source language pairs
- Evidence anchors:
  - [abstract] "By fine-tuning the Whisper decoder with only English-to-Chinese speech translation data, improved performance for translation to Chinese can be obtained for multiple languages, in addition to English."
  - [section 4.3.2] "Testing French, German and Japanese utterances from FLEURS revealed that fine-tuning also improved BLEU and COMET scores for these languages."
- Break condition: If fine-tuning causes catastrophic forgetting of original English translation capability

### Mechanism 3
- Claim: Whisper can translate speech from previously unseen languages into English by mapping their embeddings into the shared semantic space
- Mechanism: Low-resource languages sharing acoustic features with training languages can have embeddings mapped to shared space
- Core assumption: Acoustic similarity between languages is sufficient for embedding alignment, even without lexical overlap
- Evidence anchors:
  - [abstract] "for languages related to those seen in training it is possible to perform speech translation, despite the model never seeing the language in training"
  - [section 4.4] "Utilising these speech embeddings, the Whisper decoder can translate these languages into English. The results in Table 3 reveal surprisingly good BLEU scores for languages like Kabuverdianu and Asturian."
- Break condition: If ASR performance remains poor and translation BLEU scores don't improve beyond random chance for unseen languages

## Foundational Learning

- Concept: Speech-to-speech retrieval evaluation
  - Why needed here: This task directly measures whether embeddings from different languages occupy similar regions in the semantic space, which is the foundation for cross-lingual transfer
  - Quick check question: If you retrieve utterances in language B that are semantically similar to a query in language A, what does this tell you about the embedding space?

- Concept: Cosine similarity for sequence embeddings
  - Why needed here: The SeqSim metric uses cosine similarity between frame-level embeddings to measure semantic similarity between utterances, crucial for both retrieval and transfer learning
  - Quick check question: Why might simple average pooling of embeddings be insufficient for comparing semantically similar but phonetically different utterances?

- Concept: Catastrophic forgetting in fine-tuning
  - Why needed here: Fine-tuning on one language pair can degrade performance on previously supported tasks, observed when English translation performance drops after fine-tuning on English-to-Chinese data
  - Quick check question: What happens to a model's performance on task A when you fine-tune it extensively on task B, and why?

## Architecture Onboarding

- Component map: Audio → Encoder (frame-level embeddings) → Similarity computation (SeqSim) → Retrieval/Translation → Decoder (task-specific output)
- Critical path: Audio → Encoder (frame-level embeddings) → Similarity computation (SeqSim) → Retrieval/Translation → Decoder (task-specific output)
- Design tradeoffs: Freezing the encoder preserves cross-lingual alignment but limits adaptation to language-specific acoustic features. Using frame-level embeddings provides fine-grained representations but increases computational cost compared to utterance-level pooling.
- Failure signatures: Poor retrieval performance (low R@1) indicates weak cross-lingual alignment. Significant BLEU score drops for original language pairs after fine-tuning indicates catastrophic forgetting. High WER for unseen languages suggests insufficient acoustic similarity for embedding alignment.
- First 3 experiments:
  1. Implement speech-to-speech retrieval using SeqSim on a small set of parallel utterances from two languages to verify cross-lingual alignment
  2. Fine-tune the decoder on English-to-Chinese data and evaluate translation performance for French→Chinese and German→Chinese to test cross-lingual transfer
  3. Test translation of a previously unseen low-resource language into English using the baseline model to validate zero-shot capabilities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust is cross-lingual transfer for low-resource languages that are linguistically distant from the training languages?
- Basis in paper: [inferred] The paper shows promising results for some unseen languages (like Kabuverdianu and Asturian) but notes room for improvement with others (like Irish). The experiments primarily focused on languages related to those seen in training.
- Why unresolved: The study did not systematically explore the limits of cross-lingual transfer for linguistically distant low-resource languages, nor did it investigate what acoustic or linguistic features are most critical for successful transfer.
- What evidence would resolve it: Experiments testing transfer to a diverse set of low-resource languages spanning different language families, measuring both retrieval performance and translation quality, would clarify the robustness and limitations of cross-lingual transfer.

### Open Question 2
- Question: Can catastrophic forgetting be effectively mitigated when fine-tuning Whisper for new target languages while preserving existing translation capabilities?
- Basis in paper: [explicit] The paper explicitly mentions catastrophic forgetting as a limitation, showing that fine-tuning on en→zh data degrades performance on X→en translations, especially for languages similar to Chinese.
- Why unresolved: The paper only mentions the problem and proposes elastic weight consolidation (EWC) as a future direction, but does not implement or test this solution.
- What evidence would resolve it: Implementing and evaluating EWC or other regularization techniques during fine-tuning, then comparing translation performance across all language pairs before and after fine-tuning, would demonstrate whether catastrophic forgetting can be effectively mitigated.

### Open Question 3
- Question: How does the choice of source language in fine-tuning affect cross-lingual transfer performance to new target languages?
- Basis in paper: [explicit] The paper shows in Appendix D that when fine-tuned with Japanese→zh pairs, the model transfers poorly to other languages due to the substantial difference between Japanese and European languages, highlighting the importance of source language similarity.
- Why unresolved: While the paper demonstrates this effect with one example, it does not systematically investigate which source languages are optimal for different target languages or what linguistic features drive successful transfer.
- What evidence would resolve it: Systematic experiments fine-tuning with different source languages for various target languages, measuring transfer performance to multiple other languages, would identify patterns in which source-target combinations yield the best cross-lingual transfer.

## Limitations

- The evaluation relies heavily on datasets with overlapping language pairs, particularly FLEURS which covers only 5 languages across all experiments
- The mechanism for cross-lingual alignment remains implicit - while retrieval performance is strong, the paper doesn't establish whether this results from phonetic similarity, shared semantic content, or a combination of both
- The zero-shot translation capability for unseen languages shows promising BLEU scores but these languages are Romance languages closely related to Spanish/Portuguese, which were in training

## Confidence

**High Confidence**: The claim that Whisper's encoder produces language-invariant embeddings is strongly supported by consistent retrieval performance across all 20 language pairs with R@1 scores significantly above random baseline (0.2%). The retrieval experiments are straightforward to reproduce and validate, and the results are statistically robust.

**Medium Confidence**: The claim that fine-tuning improves translation to Chinese for multiple source languages is supported by empirical results but requires careful interpretation. While French, German, and Japanese show improvements, the paper doesn't establish whether this generalizes to languages outside the FLEURS set or whether the improvements are consistent across different target languages.

**Low Confidence**: The claim that Whisper can translate completely unseen languages into English, despite never being trained on them, requires more rigorous validation. The tested languages (Kabuverdianu, Asturian, Aragonese) are all closely related to Spanish/Portuguese, which were in training. Without testing on linguistically distant languages like Mandarin, Arabic, or Swahili, this claim remains speculative about true zero-shot capabilities.

## Next Checks

1. **Cross-Lingual Retrieval Robustness Test**: Evaluate retrieval performance using MaSS dataset which contains parallel speech in 7 diverse languages (Basque, Cantonese, English, Hungarian, Japanese, Mandarin, Tamil). This would test whether the shared embedding space holds across language families with different scripts and phonologies, not just Romance languages. Measure R@1 scores across all language pairs and compare to the FLEURS results.

2. **Fine-Tuning Generalization Evaluation**: After fine-tuning on English-to-Chinese data, evaluate translation performance for source languages not present in FLEURS (e.g., Russian, Arabic, or Hindi) to test whether cross-lingual transfer generalizes beyond the narrow set of tested languages. This would reveal whether the transfer capability is robust or limited to specific language families.

3. **Zero-Shot Capability on Distant Languages**: Test translation of a truly unseen language from a different language family (e.g., Swahili, Turkish, or Vietnamese) into English. This would validate whether the zero-shot translation capability extends beyond closely related languages to demonstrate genuine cross-lingual generalization, or whether it's primarily effective for languages sharing acoustic/phonetic features with training languages.