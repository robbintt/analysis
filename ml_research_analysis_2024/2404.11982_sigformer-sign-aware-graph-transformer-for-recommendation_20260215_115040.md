---
ver: rpa2
title: 'SIGformer: Sign-aware Graph Transformer for Recommendation'
arxiv_id: '2404.11982'
source_url: https://arxiv.org/abs/2404.11982
tags:
- graph
- negative
- recommendation
- signed
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes SIGformer, a novel method for sign-aware graph-based
  recommendation that leverages the transformer architecture to integrate both positive
  and negative user feedback. The key innovation lies in two novel positional encodings:
  Sign-aware Spectral Encoding (SSE) that captures spectral properties of the signed
  graph, and Sign-aware Path Encoding (SPE) that encodes path patterns.'
---

# SIGformer: Sign-aware Graph Transformer for Recommendation

## Quick Facts
- arXiv ID: 2404.11982
- Source URL: https://arxiv.org/abs/2404.11982
- Authors: Sirui Chen; Jiawei Chen; Sheng Zhou; Bohao Wang; Shen Han; Chanfei Su; Yuqing Yuan; Can Wang
- Reference count: 40
- Primary result: SIGformer achieves significant improvements in Recall@20 and NDCG@20 metrics over state-of-the-art methods on five real-world datasets by leveraging both positive and negative user feedback

## Executive Summary
This paper introduces SIGformer, a novel transformer-based method for sign-aware graph recommendation that effectively incorporates both positive and negative user feedback. The key innovation lies in two novel positional encodings: Sign-aware Spectral Encoding (SSE) that captures spectral properties of signed graphs using low-frequency eigenvectors, and Sign-aware Path Encoding (SPE) that encodes path patterns to capture node similarity. Extensive experiments on five real-world datasets demonstrate SIGformer's superiority over state-of-the-art methods, achieving significant improvements in recommendation performance while maintaining comparable efficiency.

## Method Summary
SIGformer is a transformer-based model that integrates both positive and negative user feedback for recommendation tasks. It uses two novel positional encodings: SSE leverages eigenvectors of the signed graph Laplacian to encode spectral properties, while SPE captures path patterns between nodes. The model employs a random walk-based sampling strategy to enable efficient attention computation on large graphs. The architecture consists of an embedding module, stacked transformer layers with SSE and SPE positional encodings, and a prediction module that aggregates outputs from different layers. The model is trained using BPR loss with negative sampling to optimize recommendation quality.

## Key Results
- SIGformer achieves significant improvements in Recall@20 and NDCG@20 metrics compared to both unsigned graph-based and sign-aware graph-based baselines
- The method demonstrates effectiveness across five diverse real-world datasets including Amazon-CDs, Amazon-Music, Epinions, KuaiRec, and KuaiRand
- Ablation studies validate the importance of both positional encodings (SSE and SPE) and negative interactions in improving recommendation performance
- SIGformer maintains comparable efficiency to existing methods while providing superior recommendation quality through better exploitation of collaborative information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The spectral encoding leverages low-frequency eigenvectors to draw embeddings of user-item pairs with positive feedback closer and distance those with negative feedback.
- Mechanism: The signed graph's Laplacian matrix is decomposed into eigenvectors, and the low-frequency components (corresponding to smallest eigenvalues) are used as positional encodings. This creates a low-pass filter effect where the transformer preferentially preserves signals that minimize distances between positively connected nodes and maximize distances between negatively connected nodes.
- Core assumption: The low-frequency eigenvectors of the signed graph Laplacian naturally encode the desired proximity relationships between nodes based on their positive and negative connections.
- Evidence anchors:
  - [abstract] "The transformer equipped with SSE can be interpreted as a low-pass filter, bringing the embeddings of user-item pairs with positive feedback closer and distancing those with negative feedback."
  - [section] "The low-frequency components (‚Ñé1, ‚Ñé2, ¬∑ ¬∑ ¬∑, ‚Ñéùëë‚Ñé ) optimizes the following objective function: [equation showing minimization of distances between positive edges and maximization between negative edges]"
  - [corpus] Weak - no direct evidence in related papers about this specific spectral approach.
- Break condition: If the signed graph is not approximately balanced, the spectral properties may not capture meaningful collaborative information, or if the eigenvector spectrum is flat, the low-frequency components won't provide useful discrimination.

### Mechanism 2
- Claim: Path encoding captures node similarity by learning parameters that weight different path types based on their lengths and edge sign patterns.
- Mechanism: All path types up to a maximum length are enumerated, and each path type gets a learnable parameter that represents the affinity between nodes connected by that path type. The path relationship vector for each node pair is multiplied by these parameters to produce the path encoding.
- Core assumption: Different path patterns (combinations of positive and negative edges) in the signed graph reflect varying levels of user-item affinity, and these can be captured by learning parameters.
- Evidence anchors:
  - [abstract] "Sign-aware Path Encoding (SPE) that encodes path patterns" and "path types reflect varying levels of similarity"
  - [section] "We encode the distance and the signs of edges along these paths into learnable parameters to capture the affinity between nodes connected by these paths"
  - [corpus] Weak - no direct evidence in related papers about this specific path encoding approach.
- Break condition: If path lengths beyond 1-2 hops provide diminishing returns or noise, or if the number of path types becomes too large relative to available data, the encoding may become ineffective or overfit.

### Mechanism 3
- Claim: Transformer architecture naturally aligns with collaborative filtering by estimating similarity between users and items based on historical feedback and aggregating information from similar entities.
- Mechanism: The self-attention mechanism computes similarity scores between all node pairs using their projected features, then uses these scores to weight the aggregation of information from other nodes, with more similar nodes contributing more.
- Core assumption: The dot-product similarity in self-attention effectively captures the relevance of other users/items for recommendation purposes.
- Evidence anchors:
  - [abstract] "the transformer initially estimates the similarity between users and items based on their projected features, then aggregates information from other entities according to this similarity"
  - [section] "Connecting with Collaborative Filtering. The transformer architecture aligns closely with the fundamental principle of collaborative filtering"
  - [corpus] Moderate - related papers like GFormer show transformer success in unsigned graph recommendation, though not specifically for signed graphs.
- Break condition: If the similarity estimation in self-attention doesn't capture the nuanced relationships in signed graphs, or if the attention mechanism becomes too diffuse with many nodes.

## Foundational Learning

- Concept: Signed graphs and their spectral properties
  - Why needed here: Understanding how to construct and analyze signed graphs is fundamental to implementing both spectral and path encodings
  - Quick check question: What is the relationship between the signed Laplacian and the positive/negative subgraphs, and how do you compute its eigenvectors?

- Concept: Graph transformer architectures and positional encodings
  - Why needed here: The core innovation builds on transformer models adapted for graph-structured data
  - Quick check question: How do positional encodings in graph transformers differ from those in standard transformers, and why are they necessary?

- Concept: Collaborative filtering principles and graph-based recommendation
  - Why needed here: Understanding the connection between transformer attention and collaborative filtering helps explain why this approach works
  - Quick check question: How does the self-attention mechanism in transformers relate to the similarity estimation in traditional collaborative filtering methods?

## Architecture Onboarding

- Component map: Embedding module -> Transformer layers (with SSE/SPE positional encodings) -> Aggregated embeddings -> Prediction module

- Critical path: Input embeddings ‚Üí Transformer layers (with SSE/SPE) ‚Üí Aggregated embeddings ‚Üí Prediction

- Design tradeoffs:
  - Using transformer vs. GNN: Transformers offer more expressive power but higher computational cost; GNNs are more efficient but may not handle negative edges well
  - Sampling vs. full attention: Sampling enables scalability but introduces variance; full attention is more accurate but computationally prohibitive
  - Path length threshold: Longer paths capture more context but increase complexity and may add noise

- Failure signatures:
  - Poor performance on datasets with weak negative feedback signals
  - Overfitting when path length threshold is too high relative to dataset size
  - Computational issues when scaling to very large graphs due to sampling variance

- First 3 experiments:
  1. Ablation study: Remove SSE and measure performance drop to validate spectral encoding's contribution
  2. Ablation study: Remove SPE and measure performance drop to validate path encoding's contribution
  3. Hyperparameter sweep: Test different values of Œ± (negative graph influence parameter) to find optimal balance between positive and negative feedback

## Open Questions the Paper Calls Out

- **Question**: How does the choice of Œ± affect the model's performance across different types of feedback (ratings vs clicks vs view duration)?
  - Basis in paper: [explicit] The paper discusses Œ±'s role in balancing positive and negative feedback, with optimal values varying by dataset (e.g., minor negative values for Amazon datasets, larger negative values for KuaiRec)
  - Why unresolved: The paper doesn't systematically analyze how different feedback types require different Œ± values or provide theoretical justification for why this relationship exists
  - What evidence would resolve it: Controlled experiments testing various Œ± values across datasets with different feedback types, combined with theoretical analysis of the relationship between feedback type characteristics and optimal Œ±

- **Question**: Can the SSE and SPE positional encodings be generalized to heterogeneous signed graphs with multiple types of nodes and edges?
  - Basis in paper: [inferred] The paper focuses on bipartite user-item signed graphs; while SSE uses signed Laplacian eigenvectors and SPE uses path patterns, neither is explicitly tested on heterogeneous graphs
  - Why unresolved: The paper doesn't explore how these encodings would handle more complex graph structures with multiple node/edge types
  - What evidence would resolve it: Experiments applying SIGformer to heterogeneous signed graphs (e.g., user-item-merchant with different interaction types) and analysis of how positional encodings adapt to multiple edge types

- **Question**: What is the theoretical relationship between path length thresholds (ùêøùëù) and the diminishing returns in collaborative information captured?
  - Basis in paper: [explicit] The paper observes that performance initially improves with longer path lengths but then declines, suggesting weaker correlations for higher-order neighbors
  - Why unresolved: The paper doesn't provide a theoretical framework explaining the relationship between path length, information quality, and computational cost
  - What evidence would resolve it: Mathematical analysis of information decay in signed paths, combined with empirical studies showing the optimal tradeoff between path length and collaborative signal quality across different datasets

## Limitations

- The evaluation is restricted to five rating-based datasets, which may not represent the diversity of real-world recommendation scenarios
- The method's scalability to extremely large graphs remains untested, as random walk sampling may introduce variance that increases with graph size
- The theoretical grounding for why specific spectral properties of signed graphs lead to better recommendations is primarily empirical rather than rigorously proven

## Confidence

- **High confidence**: The transformer architecture's alignment with collaborative filtering principles and the general effectiveness of incorporating negative feedback
- **Medium confidence**: The specific design choices for SSE and SPE positional encodings, as their theoretical justification is limited
- **Medium confidence**: The ablation study results, though the comparison against unsigned graph baselines is convincing

## Next Checks

1. **Theoretical validation**: Prove or disprove whether the low-frequency eigenvectors of signed graph Laplacians actually optimize the objective of bringing positive edges closer and negative edges farther apart.

2. **Scalability analysis**: Test SIGformer on datasets with 10-100x more nodes to evaluate how sampling variance affects recommendation quality and whether computational benefits persist at scale.

3. **Alternative graph construction**: Evaluate SIGformer on datasets where negative feedback is defined differently (e.g., skips, explicit dislikes, or implicit negative feedback) to test robustness to feedback definition variations.