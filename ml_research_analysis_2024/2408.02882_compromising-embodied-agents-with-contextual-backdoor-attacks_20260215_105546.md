---
ver: rpa2
title: Compromising Embodied Agents with Contextual Backdoor Attacks
arxiv_id: '2408.02882'
source_url: https://arxiv.org/abs/2408.02882
tags:
- attack
- backdoor
- agent
- arxiv
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces contextual backdoor attacks, a novel method
  to compromise code-driven embodied agents with large language models (LLMs). The
  attack poisons a few contextual demonstrations to induce LLMs to generate programs
  with context-dependent defects, which can activate and induce unintended behaviors
  when the agent encounters specific triggers.
---

# Compromising Embodied Agents with Contextual Backdoor Attacks

## Quick Facts
- arXiv ID: 2408.02882
- Source URL: https://arxiv.org/abs/2408.02882
- Reference count: 40
- Key outcome: Introduces contextual backdoor attacks that compromise code-driven embodied agents by poisoning LLM contextual demonstrations with dual-modality triggers

## Executive Summary
This paper presents contextual backdoor attacks, a novel method to compromise embodied agents powered by large language models (LLMs) without retraining. The attack works by poisoning a small number of contextual demonstrations to induce LLMs to generate backdoor-embedded programs. These programs remain dormant until specific textual and visual triggers are encountered, at which point they induce unintended behaviors. The method employs adversarial in-context generation to optimize poisoned demonstrations and a dual-modality activation strategy to control when defects activate. Experiments demonstrate high attack success rates while maintaining the original functionality of the LLM across multiple tasks including robot planning, manipulation, and compositional visual reasoning.

## Method Summary
The attack methodology leverages in-context learning to poison contextual demonstrations without modifying LLM weights. Attackers craft poisoned demonstrations through an iterative optimization process using a two-player adversarial game between a judge LLM and a modifier LLM, enhanced with chain-of-thought reasoning. The dual-modality activation strategy employs both textual triggers (inserted into user prompts) and visual triggers (placed in the environment) to control when the backdoor activates. This approach allows attackers to induce context-dependent behaviors in downstream agents while maintaining the original functionality of the LLM under normal conditions.

## Key Results
- Achieved high attack success rates while maintaining original LLM functionality across multiple tasks
- Demonstrated effectiveness of dual-modality activation strategy (textual + visual triggers)
- Showed potential impact on real-world autonomous driving systems
- Validated attack transferability across different LLM architectures (GPT-3.5-turbo, Davinci-002, Gemini)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The attack leverages in-context learning (ICL) to poison a few contextual demonstrations, causing LLMs to generate backdoor-embedded programs without retraining.
- Mechanism: Attackers craft poisoned demonstrations by iteratively optimizing natural language prompts via a two-player adversarial game. An LLM judge evaluates poisoned prompts, and a modifier LLM refines them using chain-of-thought reasoning to maximize backdoor effectiveness while maintaining original functionality.
- Core assumption: LLMs rely heavily on demonstration examples during ICL, and poisoned examples can bias the generated code without altering the model weights.
- Evidence anchors:
  - [abstract] "By poisoning just a few contextual demonstrations, attackers can covertly compromise the contextual environment of a black-box LLM..."
  - [section] "Therefore, we propose the Contextual Backdoor Attack and initiate the process of contextual backdooring code-driven embodied agents with LLMs..."
  - [corpus] Weak - corpus mentions similar backdoor threats but lacks specifics on ICL-based poisoning methods.
- Break condition: If the target LLM applies strong filtering or detection on contextual samples, the poisoned demonstrations may be discarded or neutralized.

### Mechanism 2
- Claim: The dual-modality activation strategy enables context-dependent behavior by controlling both code generation and execution through textual and visual triggers.
- Mechanism: Textual triggers inserted into user prompts cause the LLM to generate malicious code. Visual triggers placed in the environment activate the backdoor logic during program execution, ensuring the attack remains stealthy until specific conditions are met.
- Core assumption: Embodied agents can perceive visual triggers in their environment, and the generated code can branch logic based on such perceptions.
- Evidence anchors:
  - [abstract] "To enable context-dependent behaviors in downstream agents, we implement a dual-modality activation strategy..."
  - [section] "To induce contextual-dependent behaviors for the downstream agents, we design a dual-modality activation strategy..."
  - [corpus] Weak - corpus mentions visual backdoor attacks but does not detail dual-modality trigger coordination.
- Break condition: If the agent's perception system is robust to adversarial visual inputs or if visual triggers are filtered/blocked, the backdoor execution fails.

### Mechanism 3
- Claim: Adversarial in-context generation optimizes poisoned demonstrations to maximize attack success while preserving original LLM functionality.
- Mechanism: A two-player adversarial game between a judge LLM and a modifier LLM iteratively refines poisoned demonstrations. Chain-of-thought reasoning is used to enhance the quality of poisoned prompts, ensuring they appear natural while embedding backdoor logic.
- Core assumption: The iterative refinement process can produce poisoned demonstrations that are indistinguishable from clean ones to both the judge and the target LLM.
- Evidence anchors:
  - [abstract] "we employ adversarial in-context generation to optimize poisoned demonstrations..."
  - [section] "we employ an LLM judge to evaluate the poisoned prompts and report to an additional LLM to optimize the demonstration iteratively in a two-player adversarial game manner..."
  - [corpus] Weak - corpus discusses adversarial learning but not specifically for ICL optimization.
- Break condition: If the judge LLM becomes too strict or if the modifier LLM fails to produce high-quality refinements, the poisoned demonstrations may be ineffective or detectable.

## Foundational Learning

- Concept: In-context learning (ICL)
  - Why needed here: ICL allows the attack to poison LLM behavior without retraining, making it stealthy and applicable to black-box models.
  - Quick check question: How does ICL differ from fine-tuning in terms of model modification?

- Concept: Adversarial machine learning
  - Why needed here: The attack uses adversarial techniques to craft poisoned demonstrations that evade detection while maximizing backdoor effectiveness.
  - Quick check question: What is the role of the two-player adversarial game in optimizing poisoned demonstrations?

- Concept: Chain-of-thought reasoning
  - Why needed here: CoT reasoning enhances the modifier LLM's ability to generate high-quality poisoned prompts by providing step-by-step explanations.
  - Quick check question: How does CoT reasoning improve the stealthiness of poisoned demonstrations?

## Architecture Onboarding

- Component map:
  Target LLM <- Judge LLM -> Modifier LLM -> Embodied agent <- Environment

- Critical path:
  1. Craft poisoned demonstrations using adversarial in-context generation
  2. Provide poisoned demonstrations to the target LLM via ICL
  3. Target LLM generates backdoor-embedded programs
  4. Embodied agent executes the programs
  5. Visual triggers in the environment activate the backdoor logic

- Design tradeoffs:
  - Poisoning ratio: Higher ratios increase attack success but risk detection and functionality loss
  - Trigger stealth: More subtle triggers are harder to detect but may reduce activation reliability
  - Program complexity: More complex backdoors are harder to detect but increase the risk of errors

- Failure signatures:
  - Low attack success rate: Poisoned demonstrations are ineffective or detected
  - High false-positive rate: LLM generates backdoor programs without triggers
  - Agent malfunction: Backdoor logic causes unintended behavior or crashes

- First 3 experiments:
  1. Test attack effectiveness on a simple household task (e.g., microwave operation) with a single textual and visual trigger
  2. Evaluate the impact of different poisoning ratios on attack success and functionality preservation
  3. Assess the attack's transferability across different LLM architectures (e.g., GPT-3.5-turbo, Davinci-002)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can contextual backdoor attacks be mitigated in multi-modal learning environments where agents rely on both textual and visual inputs?
- Basis in paper: [explicit] The paper demonstrates a dual-modality activation strategy for attacks but does not discuss comprehensive defenses against such multi-modal threats.
- Why unresolved: The paper focuses on demonstrating attack effectiveness but leaves open the question of how to effectively detect and prevent these attacks when they exploit both textual and visual triggers.
- What evidence would resolve it: Developing and testing robust multi-modal anomaly detection systems that can identify both textual and visual trigger-based anomalies in real-time.

### Open Question 2
- Question: To what extent can adversarial in-context generation be used to optimize defenses against contextual backdoor attacks?
- Basis in paper: [inferred] The paper employs adversarial in-context generation for crafting poisoned demonstrations, suggesting a potential avenue for using similar techniques defensively.
- Why unresolved: The paper does not explore whether adversarial in-context generation can be adapted to create robust defenses by generating countermeasures or identifying poisoned samples.
- What evidence would resolve it: Experimental results showing whether adversarial in-context generation can effectively identify and neutralize poisoned demonstrations in LLM-based systems.

### Open Question 3
- Question: How do contextual backdoor attacks scale across different types of embodied agents beyond visual and robotic tasks, such as those in healthcare or finance?
- Basis in paper: [explicit] The paper evaluates attacks on visual reasoning, robot planning, and manipulation tasks but does not explore applications in other domains like healthcare or finance.
- Why unresolved: The potential impact of contextual backdoor attacks on non-visual embodied agents is not addressed, leaving open questions about their broader applicability.
- What evidence would resolve it: Studies demonstrating the effectiveness of contextual backdoor attacks on embodied agents in diverse fields such as autonomous medical devices or financial trading systems.

## Limitations

- The effectiveness of the two-player adversarial game for optimizing poisoned demonstrations is not fully validated
- The transferability of the attack across different LLM architectures is demonstrated but not thoroughly analyzed
- The stealthiness claims rely heavily on qualitative descriptions rather than quantitative stealth metrics

## Confidence

*High Confidence:* The core mechanism of using poisoned demonstrations to influence LLM behavior through in-context learning is well-supported. The dual-modality activation strategy (textual and visual triggers) is clearly defined and experimentally validated.

*Medium Confidence:* The adversarial in-context generation process shows promise but lacks detailed analysis of optimization dynamics. The reported attack success rates are promising but may not generalize to all embodied agent scenarios.

*Low Confidence:* The real-world impact claims, particularly for autonomous driving applications, are largely theoretical. The paper presents the attack methodology but does not demonstrate successful real-world deployment or comprehensive safety analysis.

## Next Checks

1. Conduct ablation studies to quantify the contribution of each attack component (textual vs visual triggers, judge vs modifier roles) to overall attack success
2. Test attack robustness against common detection methods such as trigger detection, program analysis, and behavior monitoring systems
3. Evaluate the attack's effectiveness across a broader range of embodied agent types and real-world scenarios beyond the three benchmark tasks presented