---
ver: rpa2
title: 'Varying Shades of Wrong: Aligning LLMs with Wrong Answers Only'
arxiv_id: '2410.11055'
source_url: https://arxiv.org/abs/2410.11055
tags:
- blank
- wrong-over-wrong
- node
- alignment
- answers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates whether LLMs can reliably distinguish between
  varying shades of wrong answers and whether aligning with such wrong-over-wrong
  preferences is helpful. The authors employ methods based on self-consistency, token
  probabilities, and LLM-as-a-judge to elicit wrong-over-wrong preferences, and fine-tune
  language models using these synthesized preferences.
---

# Varying Shades of Wrong: Aligning LLMs with Wrong Answers Only

## Quick Facts
- arXiv ID: 2410.11055
- Source URL: https://arxiv.org/abs/2410.11055
- Reference count: 40
- Primary result: LLMs can distinguish shades of wrong answers with up to 20.9% higher accuracy than random guessing

## Executive Summary
This work investigates whether large language models can reliably distinguish between varying shades of wrong answers and whether aligning with such wrong-over-wrong preferences improves model performance. The authors develop methods to elicit wrong-over-wrong preferences using self-consistency, token probabilities, pairwise comparison, and LLM-as-a-judge approaches. They demonstrate that LLMs possess preliminary capability to distinguish wrong answers, achieving up to 70.9% accuracy on wrong-over-wrong judgments. Fine-tuning language models using these synthesized preferences leads to models that produce less wrong and sometimes outright correct answers, while improving overall calibration.

## Method Summary
The authors sample multiple answers per question from various LLMs, filter out correct answers, and elicit wrong-over-wrong preferences using different methods. They employ score-based LLM-as-a-judge with margin filtering to identify high-quality preference pairs. These preferences are used to fine-tune LLLaMA3-8B using Direct Preference Optimization (DPO). The evaluation measures improvements across three objectives: less wrong (proxy score), more correct (accuracy), and better calibration (ECE). Experiments are conducted across eight datasets using seven different LLMs for both generation and evaluation.

## Key Results
- LLMs achieve up to 20.9% higher performance than random guess in distinguishing wrong answers
- Wrong-over-wrong alignment produces up to 9.0% less wrong answers and up to 7.0% more correct answers
- Model calibration improves with Expected Calibration Error reduced by up to 9.4%
- Score-based methods with margin filtering are most effective for preference elicitation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can distinguish varying shades of wrongness by leveraging internal representations of correctness that extend beyond binary right/wrong distinctions.
- Mechanism: The model encodes partial correctness as continuous latent variables in its hidden states, allowing it to rank answers along a wrongness continuum based on proximity to correct solutions.
- Core assumption: LLMs develop richer internal representations of correctness during pretraining than simple right/wrong distinctions.
- Evidence anchors: LLMs achieve up to 70.9% accuracy in wrong-over-wrong judgments; related preference learning papers don't address wrongness gradients.
- Break condition: When wrongness proxies are poorly defined or when answers are too similar in wrongness to be reliably distinguished.

### Mechanism 2
- Claim: Score-based elicitation methods outperform other approaches because they provide continuous feedback rather than binary comparisons.
- Mechanism: By assigning scalar scores to answers, LLMs can more precisely differentiate between answers of similar wrongness levels, and margin filtering helps identify the most separable pairs.
- Core assumption: Continuous feedback is more informative than binary comparisons for learning wrongness distinctions.
- Evidence anchors: Score-based methods achieve the least wrong and most correct answers; pairwise comparison achieves the best calibration.
- Break condition: When score margins are consistently small across answer pairs, making discrimination difficult.

### Mechanism 3
- Claim: Wrong-over-wrong alignment improves calibration by forcing the model to develop more nuanced confidence estimates.
- Mechanism: Training on wrongness distinctions requires the model to refine its probability estimates, reducing over-confidence on incorrect answers and improving overall calibration.
- Core assumption: Learning to distinguish shades of wrongness requires more precise confidence estimation than simple right/wrong classification.
- Evidence anchors: Alignment reduces ECE by up to 9.4% and produces less wrong answers; related alignment papers don't specifically address wrong-over-wrong calibration.
- Break condition: When wrongness proxies are unreliable or when the model overfits to synthetic wrongness distinctions.

## Foundational Learning

- Concept: Preference optimization
  - Why needed here: The entire alignment process relies on learning from pairwise preferences rather than supervised labels
  - Quick check question: What is the key difference between preference optimization and supervised learning in the context of this work?

- Concept: Confidence calibration
  - Why needed here: The work measures improvements in model calibration as a key outcome, requiring understanding of how token probabilities relate to actual correctness
  - Quick check question: How does Expected Calibration Error (ECE) measure calibration quality?

- Concept: Wrongness proxies
  - Why needed here: The evaluation relies on proxy functions to measure wrongness since ground truth is unavailable for training
  - Quick check question: What makes a good wrongness proxy for evaluating wrong-over-wrong alignment?

## Architecture Onboarding

- Component map:
  Generator LLMs (Llama3-8B, GPT-3.5, GPT-4o) → Answer sampling → Evaluator LLMs (same models) → Wrongness preference elicitation → Preference optimization module (DPO implementation) → Model alignment

- Critical path:
  1. Sample answers from generator LLM
  2. Filter out correct answers
  3. Elicit wrong-over-wrong preferences using evaluator LLM
  4. Construct preference dataset
  5. Fine-tune with DPO
  6. Evaluate improvements in wrongness, correctness, and calibration

- Design tradeoffs:
  - Generator vs evaluator model selection (self-evaluation vs cross-evaluation)
  - Score margin threshold (M10 vs M50) balancing precision vs recall of separable pairs
  - Batch size for scoring (tradeoff between efficiency and comparison quality)

- Failure signatures:
  - Low AccWoW (below 0.5) indicating poor wrongness discrimination
  - Negative improvements in any of the three evaluation metrics
  - High variance in results across different datasets or methods

- First 3 experiments:
  1. Compare AccWoW across different elicitation methods (consistency, logits, pairwise, score-based) on a single dataset
  2. Test the effect of margin filtering (M10 vs M50) on preference quality
  3. Evaluate whether mixing generators improves wrong-over-wrong alignment compared to self-generated preferences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do wrong-over-wrong preferences impact model generalization across different domains?
- Basis in paper: [explicit] The paper mentions generalization experiments to unseen tasks within the same domain.
- Why unresolved: While the paper shows some generalization to unseen datasets, it doesn't fully explore cross-domain generalization or the factors that might influence it.
- What evidence would resolve it: Experiments testing wrong-over-wrong alignment on tasks from completely different domains, with analysis of which factors (e.g., task similarity, dataset size) affect generalization.

### Open Question 2
- Question: What is the optimal batch size for score-based wrong-over-wrong preference elicitation?
- Basis in paper: [explicit] The paper investigates batch size effects on score-based eliciting but doesn't find a clear optimal value.
- Why unresolved: The paper only tests a limited range of batch sizes (1, 2, 5, 10) and doesn't provide a definitive answer on the optimal batch size.
- What evidence would resolve it: A comprehensive study testing a wider range of batch sizes with statistical analysis to determine the optimal value for different task types.

### Open Question 3
- Question: How does wrong-over-wrong alignment affect model performance on tasks without clear right-wrong distinctions?
- Basis in paper: [explicit] The paper mentions this limitation in the discussion section, noting that wrong-over-wrong alignment may not be universally helpful in domains without clear right-wrong distinctions.
- Why unresolved: The paper only briefly mentions this limitation without providing empirical evidence or exploring potential solutions.
- What evidence would resolve it: Experiments testing wrong-over-wrong alignment on tasks without clear right-wrong distinctions, with analysis of performance and exploration of alternative approaches.

## Limitations

- The work heavily relies on proxy functions to measure wrongness and correctness, which may not accurately reflect human judgments
- The effectiveness of wrong-over-wrong alignment across entirely new domains remains uncertain despite testing on eight diverse datasets
- Results are primarily demonstrated using Llama3-8B, and generalization to other model architectures or scales is not thoroughly explored

## Confidence

**High Confidence**: LLMs can distinguish varying shades of wrong answers (achieving up to 20.9% above random guess) is well-supported by empirical results across multiple datasets.

**Medium Confidence**: Wrong-over-wrong alignment improves calibration is supported by ECE reductions, but evaluated only through proxy metrics.

**Low Confidence**: Score-based methods with margin filtering are "best" for preference elicitation lacks comprehensive comparison, as different methods show advantages for different objectives.

## Next Checks

1. **Human Evaluation Validation**: Conduct human studies to validate whether the proxy functions used to measure wrongness and correctness align with human judgments of answer quality.

2. **Cross-Domain Transfer Test**: Apply the wrong-over-wrong aligned models to entirely new task domains not seen during training or evaluation to measure transfer of wrongness discrimination capabilities.

3. **Ablation of Margin Filtering**: Systematically vary the margin filtering thresholds (M10, M50, and additional intermediate values) to clarify whether current margin choices represent optimal tradeoffs.