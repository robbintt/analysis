---
ver: rpa2
title: 'Stochastic Amortization: A Unified Approach to Accelerate Feature and Data
  Attribution'
arxiv_id: '2401.15866'
source_url: https://arxiv.org/abs/2401.15866
tags:
- data
- amortization
- correlation
- error
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces stochastic amortization, a method for accelerating
  computationally expensive tasks in explainable machine learning by training amortized
  models with noisy, unbiased labels. The key insight is that amortization can tolerate
  significant label noise when the estimates are unbiased, enabling faster training
  compared to using exact labels.
---

# Stochastic Amortization: A Unified Approach to Accelerate Feature and Data Attribution

## Quick Facts
- arXiv ID: 2401.15866
- Source URL: https://arxiv.org/abs/2401.15866
- Reference count: 40
- Key outcome: Stochastic amortization accelerates expensive explainable ML tasks by training amortized models with noisy, unbiased labels, achieving up to 10x speedups while maintaining accuracy.

## Executive Summary
This paper introduces stochastic amortization, a method that accelerates computationally expensive tasks in explainable machine learning by training amortized models using noisy but unbiased labels. The key insight is that amortization can tolerate significant label noise when estimates are unbiased, enabling faster training compared to using exact labels. The authors demonstrate this approach across multiple tasks including Shapley value feature attribution, Banzhaf values, LIME, and data valuation methods like Data Shapley. Experimental results show that stochastic amortization consistently achieves lower estimation error than existing per-example approximations, with speedups of up to an order of magnitude. The approach is particularly effective for large datasets and can generalize to unseen data points, making it promising for scaling data valuation methods beyond previous limits of a few thousand examples.

## Method Summary
Stochastic amortization accelerates XML tasks by training amortized models with noisy but unbiased labels generated by Monte Carlo estimators. The method uses pretrained base models (ViT, ResNet, FCN) with additional amortization heads that map features to target outputs like feature attributions or data values. Training involves forward passes through the amortization model using noisy labels as targets, with optimization via Adam/AdamW. The approach works for both feature attribution (images) and data valuation (tabular data) tasks, showing that unbiased label noise slows convergence but doesn't prevent learning the correct function, enabling significant speedups over per-example computation methods.

## Key Results
- Stochastic amortization achieves lower estimation error than existing per-example approximations for feature attribution and data valuation tasks
- Speedups of up to an order of magnitude compared to exact computation methods like KernelSHAP and exact Data Shapley
- Amortized models generalize effectively to unseen data points, enabling practical scaling of data valuation to datasets beyond a few thousand examples
- The approach works across multiple XML tasks including Shapley values, Banzhaf values, LIME, and Data Shapley with consistent performance improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Amortized models can tolerate significant label noise when the estimates are unbiased
- Mechanism: When the noisy oracle provides unbiased estimates of ground truth labels, the expected value of the noisy objective matches the exact objective, allowing the amortized model to converge to the correct function despite label variance
- Core assumption: The label noise has zero systematic bias (E[˜a(b) | b] = a(b))
- Evidence anchors:
  - [abstract]: "training amortized models with noisy, unbiased labels"
  - [section 3]: "we show theoretically that it is sufficient to use unbiased estimates of the ground truth labels"
  - [corpus]: No direct evidence in related papers about unbiased noise tolerance
- Break condition: If the noisy oracle has non-zero bias, the amortized model will learn an incorrect function as shown in Eq. (4)

### Mechanism 2
- Claim: Variance in the noisy labels slows convergence but doesn't prevent learning the correct function
- Mechanism: Label variance appears in the SGD convergence bound as a weighted term Nq(˜a), which increases the number of samples needed for convergence but doesn't affect the limiting behavior when bias is zero
- Core assumption: The optimization algorithm (SGD) can handle the increased variance without diverging
- Evidence anchors:
  - [section 3]: "variance in the labels plays a more benign role of slowing optimization"
  - [section 3, Theorem 1]: Explicit convergence bound showing variance term in numerator
  - [corpus]: No direct evidence about variance effects in related amortized models
- Break condition: If variance becomes too large relative to signal strength, optimization may become unstable or require impractical amounts of data

### Mechanism 3
- Claim: Amortization is more efficient than per-example computation for datasets above a moderate size threshold
- Mechanism: The amortized model incurs training cost once but provides cheap inference for all examples, while per-example computation must be repeated for each data point
- Core assumption: The amortized model can be trained efficiently relative to the cost of per-example computation
- Evidence anchors:
  - [section 5]: "amortization becomes more effective as the dataset grows"
  - [section 5, Figure 4]: Direct comparison showing amortized models achieve lower error for datasets >1K points
  - [corpus]: Related papers mention high computational cost but don't directly compare amortized vs per-example efficiency
- Break condition: For very small datasets, the amortized model's training cost exceeds the benefit from avoiding per-example computation

## Foundational Learning

- Concept: Unbiased estimation
  - Why needed here: The entire approach relies on using noisy labels that don't systematically over/underestimate the true values
  - Quick check question: If a noisy estimator has expected value E[ˆθ] = θ + c for some constant c ≠ 0, is it unbiased?

- Concept: Amortized learning
  - Why needed here: The method replaces expensive per-example computation with a learned model that can generalize across examples
  - Quick check question: What's the key difference between regression-based amortization and objective-based amortization?

- Concept: Monte Carlo estimation
  - Why needed here: Most XML tasks use Monte Carlo methods to approximate intractable expectations, providing the noisy labels for training
  - Quick check question: Why does increasing the number of Monte Carlo samples reduce the variance of the estimate?

## Architecture Onboarding

- Component map:
  Base model (ViT/ResNet/FCN) -> Amortization head (additional layers) -> Output (feature attributions/data values) <- Noisy oracle (Monte Carlo estimator)

- Critical path:
  1. Load pretrained base model
  2. Initialize amortization head with fresh weights
  3. For each training batch:
     - Generate noisy labels using Monte Carlo estimator
     - Forward pass through amortization model
     - Compute loss and backpropagate
  4. Validate periodically using independent noisy estimates
  5. Save best model based on validation loss

- Design tradeoffs:
  - Label quality vs computation: More Monte Carlo samples = better labels but slower training
  - Model capacity vs overfitting: Larger amortization heads can fit noise better but may generalize worse
  - Training set size vs performance: More training examples improve generalization but increase initial cost

- Failure signatures:
  - High validation loss that doesn't decrease: Labels may be too noisy or model capacity mismatched
  - Model predictions match noisy labels but not ground truth: Labels likely have significant bias
  - Amortized model performs worse than per-example computation: Training set too small or amortization head too simple

- First 3 experiments:
  1. Implement amortization for Shapley values using permutation sampling with 100 samples per point
  2. Compare amortized predictions to noisy labels and ground truth for a small validation set
  3. Scale up to full dataset and measure speedup vs per-example KernelSHAP computation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the maximum dataset size at which stochastic amortization remains effective for data valuation?
- Basis in paper: [inferred] The paper mentions "scaling to datasets with millions of examples" as a future research direction and shows improvements up to 10K data points, but does not test larger scales.
- Why unresolved: The paper's experiments only test up to 10K data points, and the authors explicitly note that testing larger scales is future work.
- What evidence would resolve it: Experiments showing performance degradation or maintenance of effectiveness when training on datasets of 100K, 1M, and 10M examples.

### Open Question 2
- Question: How does stochastic amortization perform when the noisy oracle has systematic bias rather than just variance?
- Basis in paper: [explicit] The paper discusses bias vs. variance trade-offs in Section 3 and shows that SGD-Shapley (which has bias) performs poorly in Figure 13, but doesn't systematically explore different bias levels.
- Why unresolved: While the paper shows that high bias leads to incorrect function learning, it only tests one biased estimator and doesn't explore how different bias levels affect performance.
- What evidence would resolve it: Experiments varying the bias level of the noisy oracle while controlling for variance, showing how different bias levels affect amortized model accuracy.

### Open Question 3
- Question: Can stochastic amortization be extended to non-Euclidean domains like graphs or sequences?
- Basis in paper: [inferred] The paper assumes Euclidean spaces for context and output variables but mentions "arbitrary domains" as a possibility in Section 2.1.
- Why unresolved: The paper focuses on image and tabular data where inputs can be embedded in Euclidean space, but doesn't explore non-Euclidean structures.
- What evidence would resolve it: Successful implementation of stochastic amortization on graph-structured data (like molecular graphs) or sequential data (like time series) with appropriate model architectures.

## Limitations
- Theoretical claims about unbiased noise tolerance are sound but practical tolerance threshold for label variance is not quantified
- Experimental comparisons are primarily against per-example computation methods rather than other amortized approaches
- Generalization claims to unseen data points are demonstrated but extent of generalization beyond training distribution is unclear

## Confidence
- Theoretical mechanism of unbiased noise tolerance: High - supported by explicit convergence bounds
- Experimental speedups and accuracy improvements: Medium - results consistent across tasks but limited to specific datasets
- Generalization claims: Medium - demonstrated for external data but limited exploration of distribution shift
- Practical applicability to real-world problems: Low-Medium - results show promise but real-world deployment challenges not addressed

## Next Checks
1. Systematically measure the effect of increasing label variance on amortized model performance to establish practical tolerance thresholds for different tasks and architectures.
2. Test generalization to datasets with different characteristics (size, class balance, feature distributions) to understand the limits of cross-dataset transfer.
3. Evaluate performance when amortized models are trained on partial datasets (e.g., 50% of data) and tested on the remainder to assess robustness to training set size.