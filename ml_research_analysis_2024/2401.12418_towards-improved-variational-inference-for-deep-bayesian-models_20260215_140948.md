---
ver: rpa2
title: Towards Improved Variational Inference for Deep Bayesian Models
arxiv_id: '2401.12418'
source_url: https://arxiv.org/abs/2401.12418
tags:
- posterior
- inducing
- deep
- which
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The thesis develops improved variational inference for Bayesian
  deep learning. It identifies overfitting pitfalls in deep kernel learning when optimizing
  hyperparameters via marginal likelihood; a fully Bayesian approach using sampling
  (HMC/SGLD) alleviates this.
---

# Towards Improved Variational Inference for Deep Bayesian Models

## Quick Facts
- **arXiv ID:** 2401.12418
- **Source URL:** https://arxiv.org/abs/2401.12418
- **Reference count:** 40
- **Key outcome:** Introduces global inducing point variational posteriors for deep Bayesian models and deep Wishart processes for Gram matrix-based inference, achieving state-of-the-art performance without data augmentation.

## Executive Summary
This thesis develops improved variational inference techniques for deep Bayesian models. It identifies and addresses overfitting issues in deep kernel learning when optimizing hyperparameters via marginal likelihood, demonstrating that a fully Bayesian approach using sampling alleviates this problem. The work introduces global inducing point variational posteriors that create dependencies across layers in both Bayesian neural networks and deep Gaussian processes, achieving 86.7% test accuracy on CIFAR-10 without data augmentation or tempering. Finally, it proposes deep Wishart processes that operate directly on Gram matrices, enabling tractable inference that consistently outperforms equivalent deep Gaussian process models with lower computational cost.

## Method Summary
The thesis presents three main contributions to variational inference for deep Bayesian models. First, it analyzes deep kernel learning with marginal likelihood optimization, identifying overfitting when the model has many learnable hyperparameters, and shows that stochastic minibatching or fully Bayesian sampling methods (HMC/SGLD) can mitigate this issue. Second, it develops a global inducing point variational posterior that propagates a set of global inducing inputs through all layers of the network, creating cross-layer dependencies that yield tighter ELBOs and better predictive performance than factorized or local inducing point methods. Third, it introduces deep Wishart processes that operate directly on Gram matrices, removing rotational symmetries present in the true posterior of isotropic kernel models and enabling more flexible unimodal variational families. The methods are evaluated across UCI regression datasets, CIFAR-10 classification, and synthetic problems, demonstrating consistent improvements in ELBO values and predictive performance.

## Key Results
- Global inducing point BNNs achieve 86.7% test accuracy on CIFAR-10 without data augmentation or tempering
- Deep Wishart processes consistently outperform equivalent deep Gaussian processes with lower computational cost
- Deep kernel learning with marginal likelihood optimization overfits for models with many hyperparameters, while fully Bayesian approaches alleviate this
- ELBOs from global inducing methods are superior to baseline factorized and local inducing point approaches across multiple datasets

## Why This Works (Mechanism)

### Mechanism 1: Deep Kernel Learning with Marginal Likelihood Optimization
Combining neural network feature extraction with GP prediction allows the model to learn data representations while maintaining GP-like uncertainty. Inputs pass through a deterministic NN preprocessor, producing intermediate features that become inputs to a GP with a base kernel. The marginal likelihood/ELBO objective automatically optimizes both kernel hyperparameters and NN weights. However, for models with many learnable hyperparameters, the marginal likelihood can overfit by over-correlating all data points, not just those that should be correlated. This manifests as pathological kernel matrices with near-diagonal correlation patterns and worse performance than deterministic NN training. Stochastic minibatching is crucial to mitigate this overfitting.

### Mechanism 2: Global Inducing Points for Cross-Layer Correlation
Variational posteriors that induce correlations across all layers using globally shared inducing points yield tighter ELBOs and better predictive performance. A set of global inducing inputs is propagated through the network to obtain inducing representations at each layer. The conditional posterior over weights given previous layer inducing points is derived from Bayesian linear regression with pseudo-outputs and precisions. This creates dependencies across layers through shared inducing points, capturing the overall input-output transformation rather than individual layers. Computationally expensive for wide layers or convolutional architectures due to large number of induced patches.

### Mechanism 3: Deep Wishart Processes on Gram Matrices
For DGPs with isotropic kernels, re-framing the prior over features as a prior over Gram matrices removes rotational symmetries of the true posterior. The Gram matrix at each layer follows a Wishart distribution given the previous layer's Gram matrix. Inference is performed entirely on these PSD matrices using a generalized singular Wishart approximate posterior derived from a modified Bartlett decomposition. Doubly-stochastic inference uses global inducing points to propagate inducing and training/test points separately. This implicitly respects rotational symmetries through the data processing inequality, ensuring KL to true posterior cannot increase.

## Foundational Learning

- **Concept: Evidence Lower Bound (ELBO)**
  - *Why needed here:* All three mechanisms rely on maximizing the ELBO as a computationally tractable proxy for the marginal likelihood. Understanding its decomposition is essential to see why better approximations matter for hyperparameter learning.
  - *Quick check question:* For a given model, what happens to the ELBO if the approximate posterior becomes more flexible? Does it always increase or can it decrease? Why?

- **Concept: Inducing Point Methods for Gaussian Processes**
  - *Why needed here:* Both DKL and the global inducing posteriors use inducing points to reduce computational cost. Understanding how inducing points approximate the kernel matrix and how they integrate into the ELBO is necessary to grasp the derivations.
  - *Quick check question:* Why does adding an inducing point always improve (or not worsen) the ELBO in sparse GP regression, and why might this not hold for a DWP?

- **Concept: Symmetries in Bayesian Neural Networks and Gaussian Processes**
  - *Why needed here:* Mechanism 3 explicitly addresses rotational and permutation symmetries. Recognizing that permuting neurons or rotating feature dimensions does not change the function but drastically changes the weight representation explains why unimodal posteriors can be biased.
  - *Quick check question:* Consider a two-layer BNN with weight matrices W1, W2. After multiplying by a permutation matrix on the first layer's output, how must you modify the second layer's weights to keep the overall function unchanged?

## Architecture Onboarding

### Component map
Input -> Neural network feature extractor (optional for DGP/DWP) -> GP layer(s) with kernel or Wishart transition on Gram matrices -> Global inducing inputs propagated through layers -> Variational parameters (pseudo-outputs, precisions, Wishart parameters) -> ELBO computation

### Critical path
1. Define prior: For BNNs, factorized Gaussian with chosen covariance; for DGPs/DWPs, zero-mean GP/DWP with isotropic SE kernels.
2. Choose variational family: For global inducing BNN/DGP, propagate inducing inputs to all layers and compute posterior as Gaussian; for DWP, define posterior as generalized singular Wishart with trainable parameters.
3. Derive ELBO for the chosen factorization, handling the final predictive layer appropriately for each model type.
4. Implement reparameterization for sampling using appropriate techniques for Gaussians or Wishart distributions.
5. Optimize with Adam, optionally using learning rate schedules and tempering.

### Design tradeoffs
- **Flexibility vs. cost:** Global inducing introduces correlations but requires propagating inducing inputs and computing full-covariance precisions, which is computationally expensive for wide layers or convolutional architectures.
- **Prior choice:** Fixed priors avoid learning prior hyperparameters but can be overly restrictive; learnable priors add flexibility but require care to avoid overfitting.
- **Wishart generalization:** Basic generalized singular Wishart may underfit; A- and AB-generalizations improve flexibility but add parameters and computational cost.
- **Tempering & data augmentation:** Used in practice for deep models but cloud the Bayesian interpretation; the thesis demonstrates strong performance without them.

### Failure signatures
1. **DKL overfitting:** Marginal likelihood training curves spike, test performance is much worse than train, kernel correlation plots show near-unity correlations for all points, ELBO high but predictive performance poor. Remedy: use full Bayesian via SGMCMC, add stochastic minibatching, or switch to global inducing DGP.
2. **Factorized VI underfitting:** ELBO stuck near baseline uniform predictive distribution for small datasets; test LL near log(1/K) for K classes; posterior variance within layers collapses to zero. Remedy: use global inducing or increase inducing points.
3. **DWP missing DGP performance:** DWP ELBO lower than global inducing DGP; worse test LL/RMSE. Likely cause: insufficiently flexible Wishart posterior or non-isotropic/ARD kernels in intermediate layers.
4. **Numerical instability:** In Wishart sampling, Bartlett factor can hit numerical issues if precision matrix poorly conditioned; use Cholesky jitter. For convolutions, ensure patches extraction does not exceed memory.

### First 3 experiments
1. **Toy 1D regression:** Verify that global inducing captures in-between uncertainty akin to HMC while factorized/local inducing do not. Check ELBO values and inspect sampled features for DWP vs DGP.
2. **UCI regression benchmark:** For small datasets like Boston, ensure global inducing (BNN/DGP) or DWP achieves higher ELBOs and competitive/test LLs vs factorized/local inducing baselines. Depths 2-5 should show stable ELBOs without sharp degradation with depth.
3. **CIFAR-10 classification without augmentation/tempering:** With ResNet-like architecture, global inducing BNN with SpatialIWPrior should achieve ~86% accuracy, outperforming factorized and local inducing methods. Check calibration and OOD entropy ratio.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How many hyperparameters can the marginal likelihood handle before overfitting becomes an issue?
- Basis: Section 6.2 directly asks "how many hyperparameters can the marginal likelihood handle before overfitting becomes an issue?"
- Why unresolved: The thesis shows overfitting in deep kernel learning with many hyperparameters; the theoretical threshold at which marginal likelihood fails is unknown.
- What evidence would resolve it: Develop theoretical bounds on overfitting as a function of hyperparameter count, or conduct extensive empirical studies varying hyperparameter numbers across models and datasets.

### Open Question 2
- Question: How can we decide which modes are most important in a multimodal approximate posterior, and is there a point where adding more modes ceases to improve performance?
- Basis: Section 6.2 states: "the most pressing research question becomes: how can we decide which modes are the most important to include? Moreover, is it possible to reach a feasible point such that adding additional modes to a mixture posterior doesn't improve the performance?"
- Why unresolved: High-dimensional posteriors contain many modes; mixture posteriors are computationally expensive and require a criterion to choose which modes to include.
- What evidence would resolve it: An efficient algorithm for mode selection or a theoretical guarantee that a limited set of modes suffices; empirical validation on problems with known posteriors.

### Open Question 3
- Question: Can the global inducing point approach be scaled to non-fully-connected architectures (e.g., CNNs, DGPs) via a framework of global interdomain inducing points?
- Basis: Section 6.2 notes that to resolve the limitations of global inducing points for CNNs/DGPs, "a framework for global interdomain inducing points would need to be developed."
- Why unresolved: The current implementation requires extracting patches, which is memory-inefficient for high-dimensional data; interdomain inducing points could reduce complexity but have not been adapted to these models.
- What evidence would resolve it: Implement a global interdomain inducing point scheme for CNNs/DGPs and demonstrate comparable or better performance with reduced computational cost.

## Limitations

- **Computational scaling:** Global inducing methods scale poorly with input dimensionality and dataset size, limiting practical applicability to large-scale problems.
- **Kernel restrictions:** Deep Wishart processes are currently limited to isotropic kernels and cannot handle ARD kernels or skip connections due to intractability of non-central Wishart distributions.
- **Architecture constraints:** The global inducing framework for convolutional networks requires patch extraction, which is memory-inefficient, and skip connections are not yet supported in the DWP extension.

## Confidence

- **High confidence:** The mechanisms for global inducing in BNNs and the overfitting behavior of DKL marginal likelihood optimization are well-supported by both theory and extensive experiments across multiple datasets.
- **Medium confidence:** The DWP framework's superiority in removing posterior symmetries and improving ELBO is demonstrated, but the evidence is primarily on smaller UCI datasets; CIFAR-10 results would strengthen this claim.
- **Medium confidence:** The claim that global inducing creates tighter ELBOs than local inducing or factorized posteriors is well-supported, but the magnitude of improvement varies significantly across datasets.

## Next Checks

1. Test DWP on larger-scale vision datasets (e.g., CIFAR-10, ImageNet subsets) to verify scalability and confirm whether it maintains ELBO advantages over DGP in high-dimensional settings.
2. Evaluate the proposed methods on non-isotropic kernels (e.g., ARD kernels) to determine whether the DWP approach can be extended or whether alternative approximations are needed.
3. Conduct ablation studies on the global inducing framework by varying the number of inducing points and layers to identify when the cross-layer correlation benefits saturate or degrade due to computational constraints.