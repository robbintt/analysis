---
ver: rpa2
title: A Rationale-centric Counterfactual Data Augmentation Method for Cross-Document
  Event Coreference Resolution
arxiv_id: '2404.01921'
source_url: https://arxiv.org/abs/2404.01921
tags:
- event
- data
- system
- coreferential
- mention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a rationale-centric counterfactual data augmentation
  method to enhance the causal reasoning capability of the event coreference resolution
  system. The method uses large language models to intervene on event triggers and
  context, generating counterfactual augmented data that emphasizes causal associations
  while mitigating spurious patterns like lexical trigger matching.
---

# A Rationale-centric Counterfactual Data Augmentation Method for Cross-Document Event Coreference Resolution

## Quick Facts
- **arXiv ID:** 2404.01921
- **Source URL:** https://arxiv.org/abs/2404.01921
- **Reference count:** 40
- **Primary result:** Enhances cross-document event coreference resolution through counterfactual data augmentation targeting causal reasoning

## Executive Summary
This paper introduces a rationale-centric counterfactual data augmentation method to improve causal reasoning in cross-document event coreference resolution systems. The approach uses large language models to intervene on event triggers and context, generating counterfactual examples that emphasize causal associations while reducing reliance on spurious patterns like lexical trigger matching. The method is evaluated on three benchmark datasets, showing state-of-the-art performance with 1.8-2.3 CoNLL F1 improvements over baseline systems.

## Method Summary
The proposed method intervenes on event triggers and their surrounding context using large language models to generate counterfactual augmented data. By creating variations that preserve causal relationships while altering surface features, the approach aims to enhance the system's ability to reason about event coreference beyond simple lexical matching. The counterfactual examples are integrated into training to help the model learn more robust representations that generalize better to out-of-domain scenarios.

## Key Results
- Achieves state-of-the-art performance on three benchmark datasets for cross-document event coreference resolution
- Improves CoNLL F1 scores by 1.8-2.3 points over baseline systems
- Demonstrates enhanced robustness in out-of-domain scenarios compared to standard approaches
- Successfully reduces reliance on spurious patterns like lexical trigger matching

## Why This Works (Mechanism)
The method works by generating counterfactual examples that preserve causal relationships while modifying surface features. This intervention forces the model to learn deeper causal associations rather than relying on superficial lexical patterns. By using large language models to create diverse counterfactual scenarios, the approach exposes the system to a broader range of valid coreference relationships that share underlying causal structures but differ in their surface manifestations.

## Foundational Learning
- **Counterfactual data augmentation** - why needed: To expose models to diverse scenarios while preserving causal relationships; quick check: Compare performance with and without counterfactual examples
- **Cross-document event coreference resolution** - why needed: To identify events across multiple documents that refer to the same real-world occurrence; quick check: Evaluate coreference linking accuracy across document boundaries
- **Causal reasoning in NLP** - why needed: To move beyond pattern matching to understanding underlying relationships; quick check: Test robustness to adversarial examples with spurious correlations
- **Large language model interventions** - why needed: To generate high-quality counterfactual examples that maintain semantic coherence; quick check: Assess quality and diversity of generated counterfactuals

## Architecture Onboarding

**Component Map:**
Data Input -> LLM Intervention (Trigger/Context) -> Counterfactual Generation -> Training Data Augmentation -> Coreference Resolution Model

**Critical Path:**
Original training data → LLM-generated counterfactuals → Augmented training set → Fine-tuned coreference resolution model → Improved inference

**Design Tradeoffs:**
- Quality vs. quantity of counterfactual examples (LLM generation cost vs. training benefit)
- Specificity vs. generality of interventions (targeted vs. broad counterfactual generation)
- Computational overhead vs. performance gains (additional LLM calls during training)

**Failure Signatures:**
- Overfitting to LLM-generated patterns
- Loss of original data distribution characteristics
- Inconsistent counterfactual quality affecting training stability

**First 3 Experiments to Run:**
1. Ablation study comparing counterfactual augmentation against random data augmentation
2. Input perturbation analysis to measure spurious pattern reliance before/after augmentation
3. Adversarial testing with deliberately introduced spurious correlations

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies primarily on CoNLL F1 metric without deeper analysis of causal reasoning improvements
- Potential biases from LLM-generated counterfactuals are not characterized or quantified
- Lack of ablation studies to isolate the specific contribution of counterfactual generation versus additional training data
- Limited out-of-domain testing across diverse domains to validate generalizability claims

## Confidence
- **High confidence** in reported CoNLL F1 improvements on the three benchmark datasets
- **Medium confidence** in claims about spurious pattern mitigation without direct validation
- **Low confidence** in generalizability claims due to limited out-of-domain testing

## Next Checks
1. Conduct ablation studies comparing counterfactual augmentation against random data augmentation with equal amounts of additional training data to isolate the specific benefit of the rationale-centric approach.

2. Perform controlled experiments measuring reliance on spurious features (like lexical trigger matching) before and after applying the counterfactual method, using techniques such as input perturbation analysis or feature importance visualization.

3. Evaluate the system's robustness to adversarial examples where spurious correlations are deliberately introduced, to test whether the counterfactual augmentation actually builds resilience against such patterns.