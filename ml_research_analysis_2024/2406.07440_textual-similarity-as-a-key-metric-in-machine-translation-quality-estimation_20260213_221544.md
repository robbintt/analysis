---
ver: rpa2
title: Textual Similarity as a Key Metric in Machine Translation Quality Estimation
arxiv_id: '2406.07440'
source_url: https://arxiv.org/abs/2406.07440
tags:
- human
- score
- similarity
- translation
- textual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces "textual similarity" as a new metric for
  Machine Translation Quality Estimation (QE), using sentence transformers and cosine
  similarity to measure semantic closeness. Analyzing data from the MLQE-PE dataset,
  we found that textual similarity exhibits stronger correlations with human scores
  than traditional metrics (hter, model evaluation, sentence probability, etc.).
---

# Textual Similarity as a Key Metric in Machine Translation Quality Estimation

## Quick Facts
- arXiv ID: 2406.07440
- Source URL: https://arxiv.org/abs/2406.07440
- Reference count: 40
- Primary result: Textual similarity outperforms traditional metrics in predicting human quality scores for machine translation

## Executive Summary
This study introduces "textual similarity" as a new metric for Machine Translation Quality Estimation (QE), using sentence transformers and cosine similarity to measure semantic closeness. Analyzing data from the MLQE-PE dataset, we found that textual similarity exhibits stronger correlations with human scores than traditional metrics (hter, model evaluation, sentence probability, etc.). Employing GAMMs as a statistical tool, we demonstrated that textual similarity consistently outperforms other metrics across multiple language pairs in predicting human scores. We also found that "hter" actually failed to predict human scores in QE.

## Method Summary
The study computes textual similarity between source and translated sentences using multilingual sentence transformers and cosine similarity. GAMM regression analysis is applied to model relationships between QE metrics and human scores, with AIC values used for model comparison. The MLQE-PE dataset (11 language pairs) and PreQuEL dataset (English-Chinese and English-German) serve as primary data sources.

## Key Results
- Textual similarity shows stronger correlations with human scores than traditional metrics across multiple language pairs
- GAMM analysis demonstrates textual similarity consistently outperforms ML_eval in predicting human scores
- hter fails to predict human scores in most cases, as it measures edit effort rather than semantic fidelity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Textual similarity captures semantic closeness between source and translated sentences better than traditional metrics.
- Mechanism: Sentence transformers map both source and translated text into a shared embedding space, where cosine similarity measures semantic alignment. This directly reflects translation quality because high semantic similarity implies accurate meaning transfer.
- Core assumption: The semantic content of the source and translated sentences can be accurately encoded by multilingual sentence transformers, and the cosine similarity of these embeddings correlates with human quality judgments.
- Evidence anchors:
  - [abstract] "using sentence transformers and cosine similarity to measure semantic closeness"
  - [section] "Pretrained sentence transformers are highly effective for generating sentence or textual embeddings due to their ability to capture deep semantic meanings"
  - [corpus] Weak: No corpus neighbor explicitly discusses sentence transformer semantic similarity for QE.
- Break condition: If the transformer model is trained on data that underrepresents the language pair or domain, the embeddings may fail to capture true semantic similarity, breaking the correlation with human scores.

### Mechanism 2
- Claim: Textual similarity outperforms ML_eval and htr in predicting human scores across language pairs.
- Mechanism: GAMM analysis shows that including textual similarity as a predictor yields lower AIC values compared to models using only ML_eval or htr, indicating better statistical fit to human judgments.
- Core assumption: Lower AIC reliably indicates superior predictive power for human quality scores, and GAMM adequately models the complex, non-linear relationships among metrics and human scores.
- Evidence anchors:
  - [abstract] "Employing GAMMs as a statistical tool, we demonstrated that textual similarity consistently outperforms other metrics across multiple language pairs in predicting human scores"
  - [section] "The contribution from 'textual similarity' outperforms that from 'ML_eval'"
  - [corpus] Weak: No corpus neighbor explicitly reports GAMM or AIC comparisons for QE metrics.
- Break condition: If human scores are influenced by factors not captured by textual similarity (e.g., fluency, style), then AIC improvement may not translate into better real-world quality prediction.

### Mechanism 3
- Claim: htr fails to predict human scores because it measures edit effort rather than semantic fidelity.
- Mechanism: Human post-editing effort correlates poorly with perceived translation quality when translations are semantically accurate but contain minor errors (e.g., typos) that require edits but do not degrade meaning.
- Core assumption: Human evaluators prioritize semantic accuracy over surface-level correctness, and edit effort does not always align with meaning preservation.
- Evidence anchors:
  - [section] "hter... measures the number of edits required... 'hter' does not show a significant impact on human scores in the most cases"
  - [section] "a translation maybe very close in meaning to the original text but contain some misspellings... 'hter' focuses on the number of edits rather than the nature of the changes"
  - [corpus] Weak: No corpus neighbor explicitly discusses htr limitations in QE.
- Break condition: If human evaluators heavily weight surface correctness or if edit effort is strongly correlated with meaning loss, htr could regain predictive power.

## Foundational Learning

- Concept: Cosine similarity in high-dimensional embedding space
  - Why needed here: Textual similarity relies on comparing sentence embeddings via cosine similarity; understanding vector geometry is essential to interpret metric values.
  - Quick check question: If two sentence embeddings are orthogonal (90Â° angle), what is their cosine similarity value?

- Concept: GAMM (Generalized Additive Mixed Models)
  - Why needed here: The study uses GAMM to model non-linear relationships between QE metrics and human scores while accounting for random effects like language pairs; knowing GAMM assumptions is critical for correct interpretation.
  - Quick check question: In a GAMM, what does a smooth term (s()) represent compared to a linear term?

- Concept: Sentence transformers and multilingual embeddings
  - Why needed here: Textual similarity is computed using pretrained multilingual sentence transformers; understanding how these models encode semantics is key to evaluating their suitability for QE.
  - Quick check question: What is the primary difference between sentence transformers and word-level embeddings like Word2Vec?

## Architecture Onboarding

- Component map: Source text -> Multilingual sentence transformer -> Source embedding; Translated text -> Multilingual sentence transformer -> Translated embedding; Cosine similarity -> Textual similarity score; Human score -> Target variable; GAMM model -> Statistical evaluation
- Critical path: Text input -> Embedding generation -> Cosine similarity computation -> Metric value -> Correlation/GAMM analysis -> Performance evaluation
- Design tradeoffs: Using multilingual transformers increases coverage but may sacrifice language-specific nuance; GAMM allows flexible modeling but is computationally heavier than linear regression
- Failure signatures: Low correlation between textual similarity and human scores; high AIC when textual similarity is included; unstable embedding similarity across language pairs
- First 3 experiments:
  1. Compute textual similarity between source and translated sentences in MLQE-PE and correlate with human scores; compare correlation strength to ML_eval and htr
  2. Fit GAMM with textual similarity, ML_eval, htr, and random effects; compare AIC values with models excluding each metric
  3. Repeat correlation and GAMM analysis on PreQuEL dataset; check if trigram sentence probability outperforms textual similarity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the "textual similarity" metric perform compared to other emerging metrics like BERTScore or chrF in machine translation quality estimation?
- Basis in paper: [inferred] The paper introduces "textual similarity" as a robust metric for MT QE, outperforming traditional metrics, but does not compare it directly with other emerging metrics like BERTScore or chrF.
- Why unresolved: The study focuses on comparing "textual similarity" with traditional metrics and does not explore its performance against newer, semantically-focused metrics.
- What evidence would resolve it: Empirical studies comparing "textual similarity" with metrics like BERTScore and chrF across various language pairs and datasets to determine relative performance.

### Open Question 2
- Question: What are the specific limitations of "textual similarity" when applied to low-resource languages in machine translation quality estimation?
- Basis in paper: [explicit] The paper mentions that the dataset covers both high- and low-resource languages but does not detail the performance of "textual similarity" in low-resource scenarios.
- Why unresolved: While the study includes low-resource languages, it does not specifically analyze or highlight the challenges or limitations of "textual similarity" in these contexts.
- What evidence would resolve it: Detailed analysis and results from applying "textual similarity" to low-resource language pairs, highlighting any performance degradation or specific challenges encountered.

### Open Question 3
- Question: How can the integration of "textual similarity" into machine translation systems impact the training and fine-tuning processes of these systems?
- Basis in paper: [inferred] The paper suggests that "textual similarity" should be incorporated into MT system training for improved accuracy and usability, but does not explore the practical implications of this integration.
- Why unresolved: The study recommends integration but does not provide insights into how this metric affects the training dynamics, convergence, or overall performance of MT systems.
- What evidence would resolve it: Experimental studies demonstrating the effects of incorporating "textual similarity" into MT training pipelines, including changes in model performance, training efficiency, and quality estimation accuracy.

## Limitations
- Reliance on sentence transformers introduces potential domain mismatch risks for specialized terminology
- Analysis focuses primarily on MLQE-PE and PreQuEL datasets, limiting generalizability to other domains
- Study doesn't explicitly evaluate performance on predicting specific error types or translation adequacy beyond overall quality

## Confidence
- Textual similarity outperforming traditional metrics (High): Strong statistical evidence from GAMM analysis across multiple language pairs supports this claim
- Textual similarity's mechanism effectiveness (Medium): While theoretically sound, the assumption that semantic similarity always aligns with human quality judgments needs further validation
- htr's failure as predictor (Medium): The evidence is clear for the MLQE-PE dataset, but may not generalize to all QE scenarios

## Next Checks
1. Test textual similarity performance on specialized domains (medical, legal) where semantic precision is critical
2. Evaluate whether combining textual similarity with surface-level metrics (like htr) improves overall QE performance
3. Assess textual similarity's effectiveness on low-resource language pairs not represented in the MLQE-PE dataset