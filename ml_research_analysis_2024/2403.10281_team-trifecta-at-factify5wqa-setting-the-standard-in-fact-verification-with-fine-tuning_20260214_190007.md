---
ver: rpa2
title: 'Team Trifecta at Factify5WQA: Setting the Standard in Fact Verification with
  Fine-Tuning'
arxiv_id: '2403.10281'
source_url: https://arxiv.org/abs/2403.10281
tags:
- claim
- evidence
- text
- question
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Pre-CoFactv3, a fact verification framework
  combining question answering and text classification components. The method leverages
  fine-tuned large language models (LLMs), including DeBERTaV3, and introduces the
  FakeNet model for enhanced feature extraction and classification.
---

# Team Trifecta at Factify5WQA: Setting the Standard in Fact Verification with Fine-Tuning

## Quick Facts
- arXiv ID: 2403.10281
- Source URL: https://arxiv.org/abs/2403.10281
- Reference count: 40
- Achieved first place in AAAI-24 Factify 3.0 Workshop with 86% internal validation accuracy

## Executive Summary
This paper presents Pre-CoFactv3, a fact verification framework that combines question answering and text classification components for improved fact verification performance. The method employs fine-tuned large language models, particularly DeBERTaV3, and introduces a novel FakeNet model for enhanced feature extraction and classification. The Trifecta team achieved first place in the AAAI-24 Factify 3.0 Workshop, demonstrating significant improvements over baseline methods and establishing a new standard in fact verification accuracy.

## Method Summary
The proposed framework leverages fine-tuned LLMs with a two-stage approach: first extracting relevant information through question answering, then classifying claims using both extracted features and original text. The FakeNet model serves as a specialized feature extractor that enhances classification performance through improved feature representation. The system employs ensemble methods combining multiple fine-tuned LLMs with FakeNet to achieve robust verification performance across diverse claim types.

## Key Results
- Achieved 86% accuracy on internal validation set
- 69.56% accuracy on external testing set
- 103% improvement over baseline accuracy
- 70% lead over second-place competitor in workshop competition

## Why This Works (Mechanism)
The framework's success stems from combining complementary approaches: question answering extracts relevant evidence from supporting documents, while text classification evaluates the relationship between claims and evidence. Fine-tuning pre-trained LLMs like DeBERTaV3 on domain-specific data allows the models to capture nuanced patterns in fact verification tasks. The ensemble approach leverages the strengths of multiple models while mitigating individual weaknesses, resulting in more robust predictions across diverse claim types and evidence sources.

## Foundational Learning
- **Fine-tuning LLMs**: Adapting pre-trained models to specific tasks by continuing training on domain-relevant data; needed to capture task-specific patterns and improve performance beyond generic pre-training
- **Ensemble methods**: Combining predictions from multiple models to achieve better performance than any single model; needed to reduce variance and leverage complementary strengths
- **Feature extraction**: Transforming raw text into meaningful representations for classification; needed to capture semantic relationships between claims and evidence
- **Text classification**: Assigning categorical labels to text inputs based on learned patterns; needed as the core verification decision mechanism
- **Question answering**: Extracting relevant information from text given specific queries; needed to identify supporting evidence for claims
- **Pre-trained models**: Using models trained on large corpora as starting points for downstream tasks; needed to leverage existing knowledge and reduce training requirements

## Architecture Onboarding

Component Map:
Input Claims -> Question Answering Module -> Evidence Extraction -> FakeNet Feature Extractor -> Text Classification Module -> Verification Output

Critical Path:
1. Claim processing through QA module to extract evidence
2. Feature extraction using FakeNet from both claim and evidence
3. Classification decision combining original text and extracted features

Design Tradeoffs:
- Fine-tuning vs. zero-shot approaches: Fine-tuning provides better task-specific performance but requires labeled data and computational resources
- Ensemble size vs. efficiency: Larger ensembles improve accuracy but increase computational overhead
- Feature complexity vs. generalization: More complex features may capture nuances but risk overfitting to training data

Failure Signatures:
- Over-reliance on lexical matching rather than semantic understanding
- Sensitivity to input formatting or noise in evidence documents
- Degradation when claims reference information outside training distribution

First Experiments:
1. Compare single LLM vs. ensemble performance on held-out validation set
2. Ablation study removing FakeNet to quantify its contribution
3. Test model robustness with adversarial claim-evidence pairs

## Open Questions the Paper Calls Out
None

## Limitations
- Limited reproducibility due to missing hyperparameter details and training data specifications
- Performance generalization to other fact verification datasets remains unverified
- Statistical significance of claimed improvements not formally tested

## Confidence
- First-place workshop ranking: High
- Methodology transparency: Low
- Reproducibility: Medium
- Generalizability: Medium

## Next Checks
1. Request and examine complete hyperparameter configuration, training data splits, and evaluation protocols used in competition
2. Conduct ablation studies to quantify individual contributions of DeBERTaV3 fine-tuning, FakeNet architecture, and ensemble methods
3. Replicate experiments on publicly available fact verification datasets (FEVER, SciFact) to verify generalizability beyond competition setting