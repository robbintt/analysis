---
ver: rpa2
title: 'The Colorful Future of LLMs: Evaluating and Improving LLMs as Emotional Supporters
  for Queer Youth'
arxiv_id: '2402.11886'
source_url: https://arxiv.org/abs/2402.11886
tags:
- your
- support
- response
- queer
- chatgpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the potential of large language models (LLMs)
  to provide emotional support to queer youth. The authors develop a ten-question
  scale based on psychological guidelines to assess LLM responses to queer-related
  posts from Reddit.
---

# The Colorful Future of LLMs: Evaluating and Improving LLMs as Emotional Supporters for Queer Youth

## Quick Facts
- arXiv ID: 2402.11886
- Source URL: https://arxiv.org/abs/2402.11886
- Reference count: 40
- Primary result: LLMs provide supportive but generic emotional support to queer youth, with dedicated prompts improving performance but still falling short of human responses

## Executive Summary
This paper evaluates large language models as potential emotional supporters for queer youth, developing a novel ten-question scale based on psychological standards to assess LLM responses to Reddit posts from the r/LGBTeens forum. The study finds that while LLMs demonstrate inclusivity and supportiveness, they tend to provide generic, less empathetic responses compared to humans, often lacking personalization and reliability. The authors demonstrate that a dedicated "Guided Supporter" prompt can significantly improve LLM performance across most evaluation questions, though challenges remain in achieving the authenticity and nuanced understanding that human supporters provide.

## Method Summary
The researchers developed a ten-question evaluation questionnaire in collaboration with psychologists and psychiatrists to assess emotional support quality. They collected 1,000 posts from Reddit's r/LGBTeens forum containing sensitive queer-related topics and generated responses using eight leading LLMs (ChatGPT, BARD, GPT3.5, GPT4, Orca, Mistral, NeuralChat) with various prompts. Human evaluation was conducted by three trained queer evaluators who scored responses for 80 sampled posts, while automatic evaluation used GPT3.5 and GPT4 as evaluators for 1,000 posts, generating over 11,000 total responses.

## Key Results
- LLMs scored higher than humans on LGBTQ+ inclusiveness, sensitivity, and emotional validation
- Human responses outperformed LLMs on personalization, reliability, and authenticity
- A dedicated "Guided Supporter" prompt significantly improved LLM performance across most evaluation questions
- All evaluators could easily distinguish between LLM and human responses, citing LLM responses as lengthy, generic, and synthetic

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A dedicated prompt can significantly improve LLM performance in providing emotional support to queer youth.
- **Mechanism:** By providing explicit guidelines and instructions, the LLM can better understand the desired response traits and tailor its output accordingly.
- **Core assumption:** The LLM has sufficient capacity to incorporate and follow the provided guidelines.
- **Evidence anchors:**
  - [abstract]: "We find that LLM responses are supportive and inclusive, outscoring humans. However, they tend to be generic, not empathetic enough, and lack personalization, resulting in nonreliable and potentially harmful advice. We discuss these challenges, demonstrate that a dedicated prompt can improve the performance..."
  - [section]: "Our research contributes to enhancing LLMs performance through the 'Guided Supporter' prompt, significantly improving responses across most questions."
- **Break condition:** The LLM lacks the capacity to effectively incorporate the provided guidelines or the guidelines are not specific enough to guide the LLM's response generation.

### Mechanism 2
- **Claim:** Human evaluators can distinguish between LLM and human responses, despite the LLM's attempts to mimic human-like empathy.
- **Mechanism:** Human evaluators can identify the synthetic, generic, and templated nature of LLM responses, which lacks the authenticity and personalization of human responses.
- **Core assumption:** Human evaluators are sensitive to the nuances of human communication and can detect the lack of authenticity in LLM responses.
- **Evidence anchors:**
  - [section]: "All evaluators mentioned they can easily distinguish between the LLM and human responses, although we did not disclose this information. They also mentioned that LLM responses are lengthy, boring, repetitive, generic, monotonic, and feel synthetic."
- **Break condition:** The LLM's response generation improves to the point where it can effectively mimic human-like empathy and authenticity, making it difficult for human evaluators to distinguish between LLM and human responses.

### Mechanism 3
- **Claim:** Automatic evaluation using LLMs can identify trends in model performance and compare different models and prompts.
- **Mechanism:** By using a separate LLM to evaluate the responses generated by other LLMs, we can assess the relative performance of different models and prompts based on their agreement with human evaluations.
- **Core assumption:** The LLM used for automatic evaluation can accurately assess the quality of responses based on the provided guidelines and annotation criteria.
- **Evidence anchors:**
  - [section]: "Table 4 in the appendix presents the average results of these measures. For almost all questions, there is an agreement between human and automatic rankings over 80% of the time."
- **Break condition:** The LLM used for automatic evaluation lacks the ability to accurately assess the quality of responses, leading to unreliable comparisons between models and prompts.

## Foundational Learning

- **Concept: Emotional support**
  - **Why needed here:** Understanding the nature and importance of emotional support is crucial for developing AI systems that can effectively provide such support to queer youth.
  - **Quick check question:** What are the key components of effective emotional support, and how do they differ from general advice or information provision?

- **Concept: Queer identity and experiences**
  - **Why needed here:** Developing AI systems that can provide appropriate support to queer youth requires an understanding of the unique challenges and experiences faced by this population.
  - **Quick check question:** What are some of the specific mental health risks and barriers to support faced by queer youth, and how do these differ from the general population?

- **Concept: AI ethics and responsible development**
  - **Why needed here:** Ensuring that AI systems are developed and deployed in an ethical and responsible manner is crucial, especially when dealing with sensitive topics like emotional support for vulnerable populations.
  - **Quick check question:** What are some of the key ethical considerations when developing AI systems for emotional support, and how can these be addressed in the design and implementation process?

## Architecture Onboarding

- **Component map:** Reddit posts -> LLMs (ChatGPT, BARD, GPT3.5, GPT4, Orca, Mistral, NeuralChat) -> Human evaluators / Automatic evaluators -> Ten-question scale scores
- **Critical path:** Collect Reddit posts → Generate responses using various LLMs and prompts → Human evaluation based on questionnaire → Automatic evaluation using LLMs → Analyze and compare results
- **Design tradeoffs:** Balancing detailed, personalized responses with risk of inaccurate/harmful advice; ensuring privacy while gathering sufficient context; weighing human evaluation benefits against automatic evaluation scalability
- **Failure signatures:** Generic, synthetic LLM responses lacking authenticity; automatic evaluation misalignment with human evaluation; inconsistent human evaluator application of guidelines
- **First 3 experiments:**
  1. Compare performance of different LLMs and prompts on small post subset using both human and automatic evaluation
  2. Analyze specific response traits where LLMs underperform and develop targeted prompts to address weaknesses
  3. Conduct user study gathering feedback on perceived quality/effectiveness of LLM vs human responses

## Open Questions the Paper Calls Out

- **How can LLM evaluators reliably assess empathy in responses without conflating it with generic, socially expected language?**
- **What is the optimal balance between personalization and safety when LLMs seek additional context from users?**
- **How can LLMs be evaluated for cultural competence across diverse geographical and religious contexts?**
- **What is the relationship between interaction frequency and perceived authenticity in LLM emotional support?**

## Limitations

- Human evaluation relied on only three annotators, all queer, limiting generalizability
- Evaluation used static Reddit dataset that may not represent all real-world scenarios
- Automatic evaluation using GPT-3.5 and GPT-4 as evaluators represents a black-box assessment

## Confidence

**High Confidence:** Core finding that LLMs provide generic, less empathetic responses than humans is well-supported
**Medium Confidence:** Automatic evaluation results showing >80% agreement with human evaluations are encouraging but need more validation
**Low Confidence:** Claim that LLM responses are "potentially harmful" needs more specific evidence

## Next Checks

1. Expand human evaluation to larger, more diverse pool of annotators (different ages, cultural backgrounds, mental health professionals)
2. Test temporal validity by re-running evaluation on new Reddit posts collected six months later
3. Apply evaluation framework to support-seeking posts from other platforms (Discord, specialized forums) to test generalizability