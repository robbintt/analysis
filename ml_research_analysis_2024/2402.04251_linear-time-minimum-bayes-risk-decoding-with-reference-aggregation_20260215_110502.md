---
ver: rpa2
title: Linear-time Minimum Bayes Risk Decoding with Reference Aggregation
arxiv_id: '2402.04251'
source_url: https://arxiv.org/abs/2402.04251
tags:
- reference
- utility
- aggregation
- metric
- chrf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the high computational cost of Minimum Bayes
  Risk (MBR) decoding in neural machine translation, which has quadratic complexity
  due to pairwise metric calculations between sampled hypotheses. The authors propose
  reference aggregation, which approximates pairwise metric scores by calculating
  scores against aggregated reference representations.
---

# Linear-time Minimum Bayes Risk Decoding with Reference Aggregation

## Quick Facts
- arXiv ID: 2402.04251
- Source URL: https://arxiv.org/abs/2402.04251
- Reference count: 27
- This paper addresses the high computational cost of MBR decoding in NMT, proposing reference aggregation to reduce complexity from O(n²) to O(n) while preserving most quality gains.

## Executive Summary
This paper tackles the computational bottleneck in Minimum Bayes Risk (MBR) decoding for neural machine translation, which requires O(n²) pairwise metric calculations between sampled hypotheses. The authors propose reference aggregation, which approximates pairwise metric scores by calculating scores against aggregated reference representations. This approach dramatically reduces computation time - achieving 99.5% speedup for CHRF and 95-99% for COMET while maintaining translation quality. The method makes MBR decoding practical for large-scale translation systems and narrows the quality gap with beam search decoding.

## Method Summary
The authors propose reference aggregation to address MBR decoding's quadratic complexity. By observing that most MBR metrics use averageable representations (n-gram statistics for CHRF, embeddings for COMET), they compute an aggregate reference representation by averaging these representations across all references. Each hypothesis is then scored against this single aggregate instead of all references pairwise, reducing complexity from O(n²) to O(n). The method is evaluated on EN-DE, DE-EN, EN-RU, and RU-EN translation directions using newstest21/22 datasets, comparing against standard MBR with 1024 samples per segment using epsilon sampling.

## Key Results
- Reference aggregation speeds up utility estimation by 99.5% for CHRF and 95.1% for COMET-22
- Total translation time reduced by 75.5% (CHRF) and 78.8% (COMET-22)
- Translation quality preserved for CHRF (near-identical results) and maintained for COMET models
- Reference aggregation makes MBR decoding computationally feasible for practical deployment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reference aggregation reduces MBR decoding complexity from O(n²) to O(n) by replacing pairwise metric calculations with single metric calculations against an aggregated reference representation.
- Mechanism: The authors observe that most MBR metrics (CHRF, COMET) use averageable representations (n-gram statistics, sentence embeddings). By averaging these representations across all references, they create a single aggregate reference. Each hypothesis is then scored against this single aggregate instead of all references pairwise.
- Core assumption: The averaged reference representation preserves enough information to approximate the expected utility of a hypothesis when compared to the full set of references.
- Evidence anchors:
  - [abstract] "We propose to approximate pairwise metric scores with scores calculated against aggregated reference representations. This changes the complexity of utility estimation from O(n2) to O(n)"
  - [section 3] "We approximate the expected the utility of a sampled hypothesis by calculating a single metric score against this aggregate representation"
  - [corpus] Found 25 related papers, but only 5 directly relevant to MBR efficiency methods. Average neighbor FMR=0.479, suggesting moderate relatedness to the current work.

### Mechanism 2
- Claim: Reference aggregation maintains most of the quality gains of MBR decoding while achieving significant computational speedup.
- Mechanism: By using the aggregated reference representation, MBR decoding can still select hypotheses that maximize expected utility, but with much fewer computations. The experimental results show that for CHRF, reference aggregation reduces computation time by 99.5% without affecting translation quality.
- Core assumption: The quality of the selected hypothesis depends more on the relative ranking of utilities than on the absolute values, and the aggregated reference preserves the ranking sufficiently well.
- Evidence anchors:
  - [abstract] "empirically preserving most of the quality gains of MBR decoding"
  - [section 5.2] "Reference aggregation speeds up utility estimation by 99.5% for CHRF and 95.1% for COMET-22, reducing the total time needed for translation by 75.5% and 78.8%, respectively"
  - [corpus] Moderate relatedness to current work suggests this mechanism is not yet fully explored in the literature.

### Mechanism 3
- Claim: Reference aggregation can be interpreted as Monte Carlo sampling of the "true" reference representation in the feature space of the utility metric.
- Mechanism: Instead of computing an MC estimate of the expected utility (pairwise comparisons), reference aggregation computes an MC estimate of the true reference representation in the metric's feature space. This estimate is computed once and reused for all hypotheses.
- Core assumption: The average of sampled reference representations is a good approximation of the true reference in the metric's feature space.
- Evidence anchors:
  - [section 3] "By averaging over representations of sampled references, we estimate a representation of the 'true' reference, which we then use for approximating the expected utility of each sampled hypothesis"
  - [corpus] Limited direct evidence in related papers; this appears to be a novel interpretation.

## Foundational Learning

- Concept: Monte Carlo sampling
  - Why needed here: MBR decoding uses MC sampling to approximate expected utility by sampling hypotheses and references from the model distribution
  - Quick check question: If you sample 1024 hypotheses and 1024 references, how many pairwise utility calculations does standard MBR require versus reference aggregation?

- Concept: Averageable representations
  - Why needed here: The effectiveness of reference aggregation depends on the utility metric using representations that can be meaningfully averaged (n-gram statistics, embeddings)
  - Quick check question: Which of these metrics can be used with reference aggregation: CHRF, COMET, BLEURT, BLEU? Why or why not?

- Concept: Computational complexity analysis
  - Why needed here: Understanding why reference aggregation reduces complexity from O(n²) to O(n) requires analyzing the number of operations in both approaches
  - Quick check question: If standard MBR requires n×m pairwise calculations and reference aggregation requires n+m operations, what is the speedup factor when n=m=1024?

## Architecture Onboarding

- Component map: MBR decoding pipeline → sampling hypotheses → sampling references → utility estimation → hypothesis selection. Reference aggregation modifies the utility estimation step by introducing reference representation averaging before scoring.
- Critical path: Sampling → Reference aggregation (if used) → Utility scoring → Hypothesis selection. The bottleneck shifts from utility scoring to sampling when reference aggregation is used.
- Design tradeoffs: Reference aggregation trades some metric accuracy for significant computational speedup. The tradeoff is favorable for metrics like CHRF but may be less optimal for some COMET models.
- Failure signatures: Quality degradation without speedup (aggregation not effective), speedup without quality preservation (aggregation too aggressive), or neither (implementation issues).
- First 3 experiments:
  1. Implement reference aggregation for CHRF and verify the 99.5% speedup on a small translation task
  2. Compare top-1 accuracy of reference aggregation versus standard MBR with varying numbers of effective references
  3. Test aggregate-to-fine MBR approach by first pruning with aggregated reference, then refining with standard MBR

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does reference aggregation perform with other utility metrics that use averageable representations beyond CHRF and COMET, such as other n-gram overlap metrics or embedding-based metrics?
- Basis in paper: [explicit] The paper mentions that reference aggregation could be applied to other lexical overlap metrics like CHRF++ and BLEU, and that the effectiveness for other trained metrics needs empirical validation
- Why unresolved: The paper only tests reference aggregation on CHRF and COMET models, leaving open whether the technique generalizes to other metrics with averageable representations
- What evidence would resolve it: Empirical studies applying reference aggregation to additional metrics like BLEU, BERTScore, or other trained metrics with averageable representations, measuring both efficiency gains and quality preservation

### Open Question 2
- Question: What is the optimal trade-off between the number of effective references (s) and quality preservation for different utility metrics?
- Basis in paper: [explicit] The paper experiments with partial aggregation by varying the number of effective references and finds different patterns for CHRF versus COMET, but doesn't provide a general framework for determining optimal s
- Why unresolved: While the paper shows that reference aggregation maintains near-perfect accuracy for CHRF even with s=1, COMET shows quality degradation that varies with s, and the optimal s likely depends on the specific metric and task
- What evidence would resolve it: Systematic studies across multiple metrics and tasks identifying patterns in how quality preservation scales with s, potentially leading to guidelines for selecting optimal reference aggregation parameters

### Open Question 3
- Question: Can reference aggregation be adapted for utility metrics based on cross-encoder architectures that don't naturally produce averageable reference representations?
- Basis in paper: [explicit] The limitations section explicitly notes that metrics like BLEURT use cross-encoder architectures that create joint representations for each hypothesis-reference pair, making reference aggregation challenging
- Why unresolved: The paper acknowledges this as a limitation but doesn't explore whether reference aggregation could be reformulated for cross-encoder metrics, leaving open whether the technique has fundamental constraints
- What evidence would resolve it: Research exploring whether cross-encoder metrics could be reformulated to produce averageable intermediate representations, or developing alternative aggregation strategies for such metrics

## Limitations
- Reference aggregation is less effective for COMET models compared to CHRF, with some quality degradation observed
- The method cannot be directly applied to cross-encoder metrics like BLEURT that don't produce averageable reference representations
- Optimal parameters for reference aggregation (number of effective references) appear to be metric-dependent and require empirical tuning

## Confidence
- **High Confidence**: The computational complexity reduction from O(n²) to O(n) is mathematically sound and clearly demonstrated
- **Medium Confidence**: The quality preservation claims are supported by empirical results but show metric-dependent variability
- **Medium Confidence**: The effectiveness across different COMET variants is demonstrated but requires further validation for less common variants

## Next Checks
1. **Metric-specific validation**: Systematically test reference aggregation across a broader range of COMET variants (including less common ones) to establish when quality preservation degrades and why
2. **Sampling sensitivity analysis**: Evaluate how the number of samples (n) affects the tradeoff between speedup and quality for both CHRF and COMET metrics
3. **Cross-domain robustness**: Test reference aggregation on low-resource language pairs and domain-specific translation tasks to assess generalizability beyond the WMT benchmark scenarios used in the paper