---
ver: rpa2
title: Efficient course recommendations with T5-based ranking and summarization
arxiv_id: '2406.19018'
source_url: https://arxiv.org/abs/2406.19018
tags:
- course
- brightfit
- user
- skill
- courses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently recommending
  online courses for specific skill-occupation pairs in a real-time system with long
  and noisy course descriptions. The authors develop a two-stage retrieval pipeline
  combining GTR-based retrieval and RankT5 re-ranking, along with summarization of
  course descriptions using LongT5 and Vicuna models.
---

# Efficient course recommendations with T5-based ranking and summarization

## Quick Facts
- arXiv ID: 2406.19018
- Source URL: https://arxiv.org/abs/2406.19018
- Reference count: 37
- Two-stage retrieval pipeline with GTR-based retrieval, RankT5 re-ranking, and summarization improves nDCG@10 from 0.482 to 0.684 and from 0.447 to 0.844

## Executive Summary
This paper tackles the challenge of efficiently recommending online courses for specific skill-occupation pairs in a real-time system, addressing the issues of long, noisy course descriptions and the need for high-speed inference. The authors propose a two-stage retrieval pipeline that combines GTR-based retrieval with RankT5 re-ranking, complemented by summarization of course descriptions using LongT5 and Vicuna models. Quantization is applied to RankT5 to further enhance inference speed. While offline evaluations show substantial improvements in ranking effectiveness (nDCG@10), an online A/B test reveals higher click-through rates for the BM25 ranker, suggesting that real-world user preferences may be influenced by factors beyond pure relevance, such as system speed and interpretability.

## Method Summary
The authors develop a two-stage retrieval pipeline for online course recommendation, addressing the challenge of long and noisy course descriptions in a real-time system. First, GTR-based retrieval is used to identify a candidate set of courses. These candidates are then re-ranked using RankT5, a T5-based model fine-tuned for ranking tasks. To handle the long input texts, course descriptions are summarized using LongT5 and Vicuna models. Additionally, quantization is applied to RankT5 to improve inference speed without significant loss in effectiveness. The approach is evaluated on two newly labeled datasets, demonstrating substantial improvements in nDCG@10 over the in-production BM25 ranker.

## Key Results
- Offline nDCG@10 improves from 0.482 to 0.684 and from 0.447 to 0.844 on two labeled datasets
- Quantization of RankT5 achieves a 40% speed-up with minimal effectiveness loss
- Online A/B test shows higher click-through rates for BM25, despite lower offline nDCG scores

## Why This Works (Mechanism)
The proposed approach works by combining the strengths of different models at each stage of the recommendation pipeline. GTR-based retrieval efficiently identifies relevant candidates from a large corpus, while RankT5 re-ranking leverages the T5 architecture's ability to capture complex relevance signals from the skill-occupation and course description pairs. Summarization with LongT5 and Vicuna reduces the noise and length of course descriptions, allowing the re-ranking model to focus on the most salient information. Quantization further accelerates inference, making the system viable for real-time deployment. The offline improvements in nDCG@10 suggest that this multi-stage approach effectively models the complex relationship between user needs and course content.

## Foundational Learning

**Retrieval models (BM25, GTR)** - Why needed: To efficiently identify relevant courses from a large corpus based on skill-occupation queries. Quick check: Compare recall@K of BM25 vs GTR on a sample query.

**RankT5 re-ranking** - Why needed: To refine the initial retrieval results by capturing more nuanced relevance signals using a transformer-based model. Quick check: Evaluate rank correlation between BM25 and RankT5 on a set of queries.

**Text summarization (LongT5, Vicuna)** - Why needed: To handle long and noisy course descriptions, extracting the most salient information for re-ranking. Quick check: Measure ROUGE scores of summaries against reference abstracts.

**Quantization** - Why needed: To accelerate inference speed of RankT5 without significant loss in effectiveness, enabling real-time recommendations. Quick check: Compare latency and nDCG@10 of quantized vs full-precision RankT5.

## Architecture Onboarding

**Component map**: User query (skill-occupation) -> GTR retrieval -> Candidate filtering -> LongT5/Vicuna summarization -> RankT5 re-ranking -> Recommendations

**Critical path**: The critical path for a recommendation request is: query -> GTR retrieval -> candidate filtering -> summarization -> RankT5 re-ranking. Bottlenecks may occur in the summarization or re-ranking stages due to the computational complexity of transformer models.

**Design tradeoffs**: The two-stage pipeline trades off some initial retrieval recall for improved ranking precision. Summarization reduces noise but may lose some fine-grained details. Quantization speeds up inference but may slightly degrade model quality.

**Failure signatures**: Low diversity in recommendations may indicate overly aggressive candidate filtering. High latency may suggest bottlenecks in summarization or re-ranking. Poor relevance may indicate issues with the RankT5 model or summarization quality.

**First experiments**:
1. Evaluate the recall@K of GTR retrieval compared to BM25 on a sample query set.
2. Measure the latency of each stage in the pipeline (retrieval, summarization, re-ranking) to identify bottlenecks.
3. Conduct a qualitative analysis of RankT5 recommendations vs BM25, comparing relevance and diversity.

## Open Questions the Paper Calls Out
None

## Limitations
- Offline relevance metrics may not fully capture real user behavior, as evidenced by higher click-through rates for BM25 in online A/B testing
- Lack of detailed error analysis and user satisfaction metrics to understand the discrepancy between offline and online results
- Limited trade-off analysis of quantization impacts beyond nDCG@10, without examining effects on summary quality or retrieval diversity

## Confidence

**High confidence**: Two-stage retrieval pipeline architecture and its general effectiveness; quantization achieving stated speedup
**Medium confidence**: nDCG@10 improvements; RankT5 re-ranking superiority over BM25 in offline settings
**Low confidence**: Real-world user preference based on click-through rates; general applicability to other skill-occupation domains

## Next Checks

1. Conduct a detailed qualitative analysis comparing BM25 and RankT5 recommendations to understand why users click more on BM25 results despite lower nDCG scores
2. Measure user satisfaction, task completion rates, and perceived recommendation quality in a controlled user study to complement click-through metrics
3. Evaluate the impact of quantized RankT5 on summary quality and downstream task performance across multiple metrics to ensure the 40% speedup doesn't degrade other aspects of the system