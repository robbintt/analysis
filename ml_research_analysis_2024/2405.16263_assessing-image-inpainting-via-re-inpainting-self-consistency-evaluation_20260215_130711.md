---
ver: rpa2
title: Assessing Image Inpainting via Re-Inpainting Self-Consistency Evaluation
arxiv_id: '2405.16263'
source_url: https://arxiv.org/abs/2405.16263
tags: []
core_contribution: This paper proposes a novel self-supervised evaluation framework
  for image inpainting based on multi-pass re-inpainting consistency. The core idea
  is to inpaint a masked image, then apply random patch masks and re-inpaint the results,
  and finally compare the original inpainted outputs with the re-inpainted ones using
  LPIPS.
---

# Assessing Image Inpainting via Re-Inpainting Self-Consistency Evaluation

## Quick Facts
- **arXiv ID:** 2405.16263
- **Source URL:** https://arxiv.org/abs/2405.16263
- **Reference count:** 36
- **Primary result:** Novel self-supervised evaluation framework for image inpainting based on multi-pass re-inpainting consistency

## Executive Summary
This paper introduces a self-supervised evaluation framework for image inpainting that leverages multi-pass re-inpainting consistency. The method avoids the bias inherent in traditional metrics that compare inpainted results to ground truth by instead comparing the consistency between first-pass inpainted images and second-pass re-inpainted versions. Using LPIPS as the similarity metric, the approach shows strong correlation with human judgment on Places2 dataset, outperforming conventional metrics like PSNR and SSIM.

## Method Summary
The proposed evaluation method works by first inpainting masked images using various inpainting models, then applying random patch masks to these results and re-inpainting them. The LPIPS distance between the original inpainted images and the re-inpainted versions serves as the consistency score. The framework uses 100 images from Places2 at 512×512 resolution, with first mask ratios of 20%-40% and second mask ratio of 40%. Ten iterations (K=10) are performed to calculate the final metric, testing inpainting models including DeepFillv2, EdgeConnect, CoModGAN, StableDiffusion, and LaMa.

## Key Results
- The re-inpainting consistency metric shows strong alignment with human judgment compared to traditional metrics
- Method remains stable across different dataset sizes and hyperparameter settings
- Works effectively for both synthesized low-quality inpaints and high-quality results
- Outperforms PSNR, SSIM, and direct LPIPS comparison with ground truth on Places2

## Why This Works (Mechanism)
The method works by exploiting the consistency of inpainting solutions across multiple passes. When an inpainting model produces a plausible result, applying another inpainting pass with different masks should yield similar outputs. The LPIPS metric captures perceptual similarity between these passes, providing a measure of quality without requiring ground truth. This self-consistency approach avoids the inherent bias of comparing to ground truth, which can penalize diverse but valid inpainting solutions.

## Foundational Learning
- **Image inpainting basics** - Understanding how masked regions are filled with plausible content
  - Why needed: Forms the foundation for understanding the evaluation task
  - Quick check: Can explain how diffusion models vs CNN-based models approach inpainting differently

- **LPIPS metric** - Learned perceptual image patch similarity
  - Why needed: Core similarity metric used to measure consistency between inpainted versions
  - Quick check: Understand that LPIPS uses deep features rather than pixel values for comparison

- **Mask generation strategies** - Random vs structured mask patterns
  - Why needed: Different mask patterns affect inpainting difficulty and consistency
  - Quick check: Can explain how patch vs irregular masks influence inpainting results

## Architecture Onboarding

**Component Map:** Original Image -> First Mask -> First Inpainting (F1) -> Second Mask -> Second Inpainting (F2) -> LPIPS Comparison

**Critical Path:** The evaluation pipeline requires sequential execution: mask generation → first inpainting → second mask application → second inpainting → LPIPS calculation. Each step must complete successfully for the final score.

**Design Tradeoffs:** The method trades computational cost (two inpainting passes per image) for unbiased evaluation. Using patch masks for the second pass balances the need for meaningful re-inpainting while maintaining consistency. The choice of LPIPS over pixel-based metrics prioritizes perceptual quality over pixel-perfect reconstruction.

**Failure Signatures:** 
- High variance in LPIPS scores across different seeds suggests instability in inpainting models
- Very low LPIPS scores might indicate over-smoothed or overly consistent inpainting that lacks diversity
- Scores that don't correlate with human judgment suggest issues with the re-inpainting process or LPIPS implementation

**First Experiments:**
1. Run the full pipeline with a single image and visualize both inpainted and re-inpainted results
2. Compare LPIPS scores when using the same inpainting model for both passes vs different models
3. Test the sensitivity of the metric to different patch sizes in the second mask generation

## Open Questions the Paper Calls Out
### Open Question 1
- **Question:** How does the proposed multi-pass re-inpainting consistency evaluation perform on image datasets other than Places2, particularly on datasets with different visual characteristics or domain-specific content?
- **Basis in paper:** [explicit] The authors state they evaluate their method on Places2, but do not explore other datasets.
- **Why unresolved:** The paper does not provide results or analysis on datasets outside Places2, leaving uncertainty about generalizability.
- **What evidence would resolve it:** Empirical results showing the metric's stability and correlation with human judgment across multiple datasets (e.g., ImageNet, medical imaging, satellite imagery).

### Open Question 2
- **Question:** What is the impact of using different patch sizes in the patch mask generator on the stability and sensitivity of the proposed evaluation metric?
- **Basis in paper:** [inferred] The authors mention using a fixed patch size S but do not analyze the sensitivity to this hyperparameter.
- **Why unresolved:** The paper does not explore how varying patch size affects the metric's performance or consistency across different inpainting methods.
- **What evidence would resolve it:** A study comparing LPIPS scores and human alignment for multiple patch sizes (e.g., S=8, 16, 32) across different inpainting methods and mask ratios.

### Open Question 3
- **Question:** How does the proposed self-consistency metric behave when the second inpainting network is significantly weaker or stronger than the first inpainting network?
- **Basis in paper:** [explicit] The authors test multiple second inpainting networks but do not discuss extreme performance gaps between F1 and F2.
- **Why unresolved:** The paper focuses on comparing similar-quality models, leaving the metric's behavior in asymmetric scenarios unexplored.
- **What evidence would resolve it:** Quantitative results showing how the metric scores change when pairing a strong first network (e.g., StableDiffusion) with a weak second network (e.g., basic diffusion-based method), and vice versa.

## Limitations
- Experimental validation primarily focuses on synthetic low-quality inpaints and limited 100 images from Places2
- Performance across diverse real-world scenarios and other inpainting tasks remains untested
- The method requires two inpainting passes, increasing computational cost compared to traditional metrics

## Confidence
- **High confidence** in the theoretical soundness of the re-inpainting consistency approach as an unbiased evaluation metric
- **Medium confidence** in the experimental results showing better alignment with human judgment compared to traditional metrics
- **Low confidence** in the generalizability of the findings to other inpainting tasks and datasets beyond Places2

## Next Checks
1. Validate the method's performance on a larger, more diverse dataset including real-world inpainting scenarios
2. Test the stability of the metric across different random seeds and hardware configurations to ensure reproducibility
3. Evaluate the method's effectiveness for other image restoration tasks beyond inpainting, such as denoising or super-resolution