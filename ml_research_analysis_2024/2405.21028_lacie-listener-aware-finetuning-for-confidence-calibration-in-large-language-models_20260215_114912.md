---
ver: rpa2
title: 'LACIE: Listener-Aware Finetuning for Confidence Calibration in Large Language
  Models'
arxiv_id: '2405.21028'
source_url: https://arxiv.org/abs/2405.21028
tags:
- answer
- listener
- lacie
- confidence
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LACIE, a listener-aware fine-tuning method
  for calibrating confidence in large language models. The core idea is to train a
  speaker model not only on whether its answers are correct, but also on whether listeners
  would accept them based on how confidently they are expressed.
---

# LACIE: Listener-Aware Finetuning for Confidence Calibration in Large Language Models

## Quick Facts
- **arXiv ID**: 2405.21028
- **Source URL**: https://arxiv.org/abs/2405.21028
- **Authors**: Elias Stengel-Eskin; Peter Hase; Mohit Bansal
- **Reference count**: 17
- **Primary result**: Listener-aware fine-tuning improves LLM confidence calibration, reducing incorrect answer acceptance by 47% in human evaluations while maintaining acceptance of correct answers.

## Executive Summary
This paper introduces LACIE, a listener-aware fine-tuning method for calibrating confidence in large language models. The core idea is to train a speaker model not only on whether its answers are correct, but also on whether listeners would accept them based on how confidently they are expressed. This is achieved through a two-agent game where a speaker model generates responses and a listener model decides whether to accept or reject them based on perceived confidence. Using this setup, the authors create preference data and fine-tune three models (Mistral-7B, Llama3-8B, Llama3-70B) with Direct Preference Optimization. The resulting LACIE models show significantly better calibration with a simulated listener, reflected in 20.7 point gains in AUROC and 7.8 point decreases in calibration error. Crucially, this improvement transfers to human listeners: in a human evaluation, LACIE reduces the rate of incorrect answers being accepted by 47% while maintaining acceptance rates for correct answers. The method also generalizes to out-of-domain data, improving truthfulness on TruthfulQA by 28%. Qualitatively, LACIE leads to more hedging and abstention when uncertain, and more authoritative tone when correct.

## Method Summary
LACIE uses a two-agent game framework where a speaker model generates answers to questions and a listener model evaluates whether to accept or reject each answer based on perceived confidence. The method generates 10 diverse responses per question using a base speaker model, then has a listener model assign acceptance probabilities to each response. Preference pairs are created based on correctness plus listener decision using a conservative preference function that favors false rejections over false acceptances. The speaker model is then fine-tuned with Direct Preference Optimization (DPO) using these synthetic preference pairs. The approach is evaluated on TriviaQA for calibration and TruthfulQA for generalization, with human evaluation confirming that the calibration improvements transfer to real listeners.

## Key Results
- LACIE achieves 20.7 point gains in AUROC and 7.8 point decreases in ECE when evaluated on a simulated listener
- In human evaluation, LACIE reduces incorrect answer acceptance by 47% while maintaining correct answer acceptance rates
- The method generalizes to out-of-domain data, improving TruthfulQA performance by 28%
- Qualitatively, LACIE produces more hedging and abstention when uncertain, and more authoritative tone when correct

## Why This Works (Mechanism)
LACIE works by optimizing for listener perception rather than just answer correctness. By incorporating a pragmatic component through the listener model, the speaker learns to express confidence appropriately - hedging when uncertain and speaking authoritatively when confident. The two-agent game creates synthetic preference data that captures the relationship between how answers are expressed and whether listeners accept them, addressing the fundamental problem that LLM confidences are often uncalibrated with respect to human perception.

## Foundational Learning
- **Two-agent game framework**: Needed to create synthetic preference data that captures pragmatic aspects of communication; quick check is whether preference pairs can be generated consistently
- **Direct Preference Optimization (DPO)**: Required for fine-tuning on the preference pairs; quick check is whether the model can learn from synthetic preferences
- **Confidence calibration metrics**: ECE and AUROC are needed to evaluate calibration performance; quick check is whether calibration improves on held-out data
- **Listener model evaluation**: The listener must accurately assess confidence in generated responses; quick check is listener model performance on annotated confidence data
- **Preference function design**: Conservative preference function needed to encode desired calibration behavior; quick check is whether the function produces sensible preference pairs

## Architecture Onboarding

**Component map**: Question -> Speaker Model -> Multiple Responses -> Listener Model -> Acceptance Probabilities -> Preference Pairs -> DPO Fine-tuning -> Calibrated Speaker Model

**Critical path**: The listener model evaluation is the critical path - if the listener cannot accurately assess confidence, the entire synthetic preference generation fails and the fine-tuning will not produce calibrated models.

**Design tradeoffs**: Conservative preference function vs. diverse response generation; synthetic data quality vs. computational cost of generating multiple responses per question.

**Failure signatures**: 
- Listener model overfitting to its own generations
- Poor abstention calibration (model either never abstains or abstains too frequently)
- Synthetic preference pairs that don't capture meaningful differences in confidence expression

**3 first experiments**:
1. Verify listener model can distinguish between confident and uncertain responses on a held-out test set
2. Check that generated preference pairs show meaningful patterns (correct+accepted > correct+rejected > incorrect+accepted > incorrect+rejected)
3. Evaluate baseline speaker model calibration on a small dev set before fine-tuning

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several emerge from the work:
- How does listener model choice affect calibration performance?
- Does the method generalize to open-ended questions and complex reasoning tasks?
- What is the optimal listener threshold for binarizing acceptance probabilities?
- How does the conservative utility function affect real-world usability?

## Limitations
- Synthetic listener data may not fully capture human perception of confidence
- Conservative preference function may lead to overly cautious models
- Limited evaluation on non-factual question answering tasks
- No analysis of cultural or linguistic variations in confidence interpretation

## Confidence
- **High confidence**: Core experimental results showing improved calibration on simulated listener and maintained TruthfulQA performance
- **Medium confidence**: Claims about increased hedging behavior (lacks quantitative metrics) and out-of-domain generalization (small TruthfulQA evaluation set)
- **Low confidence**: Assumption that listener perception directly correlates with human acceptance across all contexts

## Next Checks
1. Measure quantitative metrics for hedging behavior (frequency of hedging phrases) before and after LACIE fine-tuning
2. Test LACIE's calibration performance on non-factual question answering tasks (creative writing, opinion questions)
3. Evaluate whether listener model's calibration changes during preference generation process