---
ver: rpa2
title: Comparative Analysis of Open-Source Language Models in Summarizing Medical
  Text Data
arxiv_id: '2405.16295'
source_url: https://arxiv.org/abs/2405.16295
tags:
- llms
- medical
- summarization
- evaluation
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates open-source LLMs for medical text summarization
  tasks using GPT-4 as an assessor. The authors compare Llama2-70B and Mistral-7B
  against GPT-3.5 across five medical datasets using a pairwise comparison approach
  with GPT-4 as judge.
---

# Comparative Analysis of Open-Source Language Models in Summarizing Medical Text Data

## Quick Facts
- arXiv ID: 2405.16295
- Source URL: https://arxiv.org/abs/2405.16295
- Reference count: 19
- Llama2-70B outperformed other models with win rates over 40% across all medical summarization tasks

## Executive Summary
This paper evaluates open-source LLMs for medical text summarization tasks using GPT-4 as an assessor. The authors compare Llama2-70B and Mistral-7B against GPT-3.5 across five medical datasets using a pairwise comparison approach with GPT-4 as judge. Llama2-70B outperformed other models, achieving win rates over 40% across all tasks, while Mistral-7B only surpassed GPT-3.5 in one dataset. The study addresses the need for systematic evaluation of LLMs in domain-specific medical tasks, providing insights for selecting appropriate models in digital health applications.

## Method Summary
The study employs a two-stage pipeline to evaluate open-source LLMs (Llama2-70B and Mistral-7B) on medical text summarization tasks. Models generate summaries using uniform prompts, which are then evaluated by GPT-4 through a pairwise comparison approach. An adversarial assessment strategy is introduced where all models are compared against a single target model (GPT-3.5) rather than conducting all possible pairwise comparisons. GPT-4 evaluates summaries across four dimensions: coherence, consistency, fluency, and relevance. Position swapping is used to mitigate potential bias in the pairwise evaluation process.

## Key Results
- Llama2-70B achieved win rates exceeding 40% against GPT-3.5 across all five medical datasets
- Mistral-7B only surpassed GPT-3.5 in one dataset, demonstrating significantly lower performance than Llama2-70B
- The adversarial assessment strategy reduced evaluation complexity from O(n²) to O(n) while maintaining comparative validity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 pairwise comparison is more effective than sequential grading because it reduces variance by forcing direct comparison between models on identical inputs.
- Mechanism: Instead of having GPT-4 independently score each model's output, the pairwise approach presents two summaries simultaneously and asks GPT-4 to choose the better one, focusing evaluation on relative performance rather than absolute scores.
- Core assumption: GPT-4's judgment consistency is sufficient to reliably distinguish between two summaries when they are presented side-by-side.
- Evidence anchors:
  - [abstract] "Rather than employing GPT-4 to grade the summaries of each LLM sequentially, we instruct GPT-4 to perform a pairwise comparison"
  - [section] "To avoid position bias, we apply the swapping positions approach, employing GPT-4 to do the evaluation twice by swapping the order of two answers"
- Break condition: If GPT-4's pairwise judgments become inconsistent or show position bias that swapping doesn't mitigate, the pairwise approach loses its advantage over sequential grading.

### Mechanism 2
- Claim: The adversarial assessment strategy with a single target model reduces evaluation complexity from O(n²) to O(n) while maintaining comparative validity.
- Mechanism: By selecting one model (GPT-3.5) as the baseline and comparing all other models against it, the study avoids the exponential growth in comparisons that would occur with all-pairwise evaluations.
- Core assumption: Comparing all models against a single baseline provides sufficient comparative information to rank models effectively.
- Evidence anchors:
  - [abstract] "To enhance efficiency, we introduce a novel adversarial assessment strategy that eliminates the need to evaluate all possible pairs by selecting a target LLM and comparing all other LLMs against this target"
  - [section] "This methodology reduces the number of evaluations per data sample to n−1, where n represents the total number of models under evaluation"
- Break condition: If the target model (GPT-3.5) is not representative or if the relative performance depends heavily on the specific baseline chosen, this approach may miss important pairwise relationships.

### Mechanism 3
- Claim: Focusing GPT-4 evaluation on coherence, consistency, fluency, and relevance dimensions aligns the assessment with human judgment criteria for medical summaries.
- Mechanism: The prompt design explicitly instructs GPT-4 to evaluate summaries across these four dimensions rather than relying on automatic metrics like ROUGE or BERTScore that may not capture these qualities.
- Core assumption: These four dimensions comprehensively capture what makes a medical summary useful and accurate for clinical applications.
- Evidence anchors:
  - [abstract] "employing GPT-4 as an assessor focusing on coherence, consistency, fluency, and relevance"
  - [section] "We enhance the prompt to ensure that GPT-4's evaluation focuses on four key dimensions commonly used by humans: coherence, consistency, fluency, and relevance"
- Break condition: If these dimensions miss critical aspects of medical summary quality (such as clinical accuracy or completeness) or if GPT-4's interpretation of these dimensions diverges from human experts, the evaluation framework becomes less reliable.

## Foundational Learning

- Concept: Pairwise comparison methodology in NLP evaluation
  - Why needed here: Understanding why pairwise comparisons can be more reliable than absolute scoring is crucial for interpreting the study's methodology
  - Quick check question: Why might a pairwise comparison between two summaries be more reliable than having GPT-4 score each summary independently on a scale?

- Concept: Domain-specific LLM evaluation
  - Why needed here: The study focuses on medical text summarization, which requires understanding how general LLM capabilities translate to specialized domains
  - Quick check question: What makes evaluating LLMs on medical text different from evaluating them on general text?

- Concept: Prompt engineering for evaluation tasks
  - Why needed here: The study's effectiveness depends heavily on how prompts are designed to elicit consistent judgments from GPT-4
  - Quick check question: What elements should be included in a prompt to ensure consistent evaluation of text summaries across different dimensions?

## Architecture Onboarding

- Component map: Summarization Models (Llama2-70B, Mistral-7B, GPT-3.5) -> GPT-4 Evaluator -> Result Aggregation
- Critical path: Medical text sample → Model summary generation → GPT-4 pairwise comparison → Win/tie determination → Aggregate results
- Design tradeoffs: Pairwise comparison increases API calls but reduces variance; adversarial strategy reduces complexity but may miss nuanced relationships
- Failure signatures: Position bias in GPT-4 evaluation, inconsistent pairwise judgments, win rates consistently near 50%
- First 3 experiments:
  1. Verify that GPT-4's pairwise judgments are consistent by running the same pairs multiple times and checking for agreement
  2. Test the adversarial strategy by comparing results against a full all-pairs evaluation on a small subset to validate the approach
  3. Validate that the four evaluation dimensions (coherence, consistency, fluency, relevance) are being applied consistently by having GPT-4 explain its choices for a sample of comparisons

## Open Questions the Paper Calls Out

- How does the performance of open-source LLMs in medical text summarization compare when fine-tuned on domain-specific medical data versus using zero-shot or few-shot settings?
  - Basis in paper: [inferred] The paper uses zero-shot settings for open-source models (Llama2-70B and Mistral-7B) and compares them to GPT-3.5, but does not explore fine-tuning these models on medical data.
  - Why unresolved: The study does not investigate the impact of domain-specific fine-tuning on the performance of open-source LLMs in medical summarization tasks.
  - What evidence would resolve it: Comparative evaluation of fine-tuned open-source LLMs versus zero-shot/few-shot settings on the same medical summarization tasks, showing performance differences and identifying optimal training approaches.

- How sensitive is GPT-4 as an evaluator to the length and structure of medical summaries when performing pairwise comparisons?
  - Basis in paper: [explicit] The authors acknowledge that GPT-4 lacks sensitivity in detecting the length of summaries during their discussion section.
  - Why unresolved: The study identifies this limitation but does not systematically investigate how summary length and structure affect GPT-4's evaluation consistency and reliability.
  - What evidence would resolve it: Systematic experiments varying summary lengths and structures while keeping content quality constant, measuring GPT-4's evaluation consistency and identifying any biases.

- How well does GPT-4's assessment of medical summaries align with human expert evaluations in terms of coherence, consistency, fluency, and relevance?
  - Basis in paper: [explicit] The authors propose to address the alignment between human intent and GPT-4 evaluator in future work, suggesting this has not been investigated.
  - Why unresolved: The study uses GPT-4 as an assessor but does not validate its judgments against human experts, leaving uncertainty about the reliability of this evaluation approach.
  - What evidence would resolve it: Direct comparison studies where human experts and GPT-4 independently evaluate the same medical summaries, with statistical analysis of agreement rates and identification of systematic discrepancies.

## Limitations

- The study relies entirely on GPT-4 as the sole evaluator, which may introduce bias and lacks validation against human expert judgment
- The adversarial assessment strategy may miss nuanced pairwise relationships between non-baseline models
- The study does not explicitly assess clinical accuracy or completeness of medical information, focusing only on coherence, consistency, fluency, and relevance

## Confidence

- **High Confidence**: Llama2-70B consistently outperforming Mistral-7B and GPT-3.5 across multiple medical datasets and tasks
- **Medium Confidence**: The generalizability of results across different medical domains and tasks, given the limited number of datasets
- **Medium Confidence**: The adequacy of the four evaluation dimensions in capturing all aspects of medical summary quality

## Next Checks

1. Conduct human expert validation on a subset of summaries to verify alignment between GPT-4 judgments and clinical domain expertise
2. Perform a full all-pairs evaluation on a small subset of data to validate that the adversarial assessment strategy does not miss critical pairwise relationships
3. Test the evaluation framework on additional medical domains and task types not covered in the current study to assess generalizability of findings