---
ver: rpa2
title: Scaling Sparse Fine-Tuning to Large Language Models
arxiv_id: '2401.16405'
source_url: https://arxiv.org/abs/2401.16405
tags:
- fine-tuning
- sparse
- memory
- spiel
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces SpIEL, a novel sparse fine-tuning method that
  scales to state-of-the-art LLMs like LLaMA 2 7B and 13B. SpIEL maintains an array
  of parameter indices and their deltas relative to pretrained values, iteratively
  updating deltas, pruning obsolete indices, and regrowing new ones based on accumulated
  gradients or approximate momenta.
---

# Scaling Sparse Fine-Tuning to Large Language Models

## Quick Facts
- arXiv ID: 2401.16405
- Source URL: https://arxiv.org/abs/2401.16405
- Reference count: 40
- This work introduces SpIEL, a novel sparse fine-tuning method that scales to state-of-the-art LLMs like LLaMA 2 7B and 13B.

## Executive Summary
This paper introduces SpIEL, a sparse fine-tuning method for large language models that maintains parameter indices and their deltas relative to pretrained values. The method iteratively updates active deltas, prunes obsolete indices, and regrows new ones based on accumulated gradients or approximate momenta. SpIEL achieves comparable performance to LoRA while offering memory efficiency and compatibility with quantization and efficient optimizers, enabling scaling to larger models.

## Method Summary
SpIEL is a sparse fine-tuning method that maintains an array of parameter indices and their deltas relative to pretrained values. The method iterates over updating active deltas, pruning indices based on delta magnitude changes, and regrowing indices using either accumulated gradients over multiple training steps or approximate momenta estimated using the SM3 optimizer. This approach allows SpIEL to achieve memory efficiency by storing only O(dϕ) parameters rather than O(dθ) for the full model, while maintaining performance comparable to or better than LoRA on various instruction-tuning benchmarks.

## Key Results
- SpIEL often outperforms LoRA and other PEFT methods in performance while being comparable in runtime
- The method maintains memory efficiency by storing only active parameter indices and deltas
- SpIEL is compatible with quantization and efficient optimizers, enabling scaling to larger models with modest memory overhead

## Why This Works (Mechanism)

### Mechanism 1
- SpIEL reduces memory overhead by maintaining only active indices and deltas instead of full parameter masks
- Instead of storing a binary mask of size O(dθ), SpIEL stores only the indices and deltas of modified parameters, scaling memory with O(dϕ)
- Core assumption: The number of modified parameters (dϕ) is much smaller than total parameters (dθ), making the sparse representation efficient
- Break condition: If dϕ approaches dθ, the memory advantage disappears and the sparse representation becomes inefficient

### Mechanism 2
- Accumulated gradients across multiple iterations improve weight selection compared to single-batch gradients
- SpIEL-AG averages gradients over γ training steps before making growth decisions
- Core assumption: Averaging gradients over multiple steps reduces variance and leads to better selection of important parameters to modify
- Break condition: If the training dynamics are highly non-stationary, accumulated gradients may become stale and lead to poor growth decisions

### Mechanism 3
- Momentum approximation provides a memory-efficient alternative to accumulated gradients when extreme memory efficiency is required
- SpIEL-MA uses SM3 optimizer's row-wise and column-wise summary metrics to approximate parameter momenta
- Core assumption: The outer product of row and column summaries provides a reasonable proxy for parameter importance even with reduced quality
- Break condition: If the locality assumption breaks down (importance of parameters is not correlated within rows/columns), the approximation becomes unreliable

## Foundational Learning

- **Parameter-efficient fine-tuning (PEFT)**: Understanding PEFT is essential to grasp why SpIEL's approach to sparse fine-tuning is novel and valuable
  - Why needed: PEFT methods aim to reduce memory and computational overhead when fine-tuning large models
  - Quick check: What distinguishes SpIEL from LoRA in terms of parameter storage and memory efficiency?

- **Sparse tensor operations and their hardware limitations**: SpIEL's memory efficiency comes from avoiding sparse tensor operations that are inefficient on current hardware
  - Why needed: Understanding hardware constraints helps explain why SpIEL's approach is necessary
  - Quick check: Why does SpIEL side-step the "hardware lottery" problem mentioned in the paper?

- **Momentum-based optimization and its approximations**: Understanding how SM3 approximates momenta is crucial for grasping the SpIEL-MA variant
  - Why needed: The momentum approximation is a key component of one of SpIEL's variants
  - Quick check: How does SM3's row-wise and column-wise summary approach approximate full parameter momenta?

## Architecture Onboarding

- **Component map**: SpIEL consists of three main components: (1) delta storage and update mechanism, (2) pruning criteria based on delta magnitude changes, and (3) growth criteria (either accumulated gradients or momentum approximation). The system maintains arrays of parameter indices and their corresponding deltas relative to pretrained values.

- **Critical path**: Forward pass computes scatter-add of deltas onto frozen weights, backward pass computes gradients only for active indices, pruning and growth phases update the active index set every S steps based on the chosen criteria.

- **Design tradeoffs**: SpIEL trades off some selection quality (compared to full gradient information) for significant memory savings. The choice between AG and MA variants involves a tradeoff between selection quality and memory efficiency.

- **Failure signatures**: If the density level is set too high, memory savings diminish. If the pruning/growth schedule is too aggressive, important parameters may be dropped. If the approximation quality in SpIEL-MA is too poor, performance degrades significantly.

- **First 3 experiments**:
  1. Implement the basic SpIEL-AG with a small toy model to verify the scatter-add mechanism and gradient computation for active indices
  2. Compare memory usage and performance of SpIEL-AG vs LoRA on a medium-sized model (e.g., 1B parameters) with varying density levels
  3. Implement SpIEL-MA and verify that it produces similar results to SpIEL-AG while using less memory during training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the SpIEL-MA method's assumption of parameter locality (importance correlation within rows and columns) hold for all parameter matrices in LLMs, or are there specific layers where this assumption breaks down?
- Basis in paper: [explicit] The paper states that SpIEL-MA "assumes locality, i.e., that the importance of a parameter is correlated to those in the same row and column" and notes this is "reminiscent of the locality-driven synaptic growth in human brains."
- Why unresolved: The paper only mentions this assumption but doesn't empirically test whether it holds across different parameter matrices or layers in LLMs. The locality assumption is critical to SpIEL-MA's effectiveness but hasn't been validated.
- What evidence would resolve it: Detailed analysis of parameter importance patterns across different layer types (attention heads, feed-forward networks, etc.) in LLMs, showing whether parameters within the same row/column are indeed more correlated in importance than random parameters.

### Open Question 2
- Question: How does the performance of SpIEL compare to full fine-tuning when instruction-tuning LLMs on datasets with longer sequence lengths (e.g., 8,192 tokens) rather than the 2,048 tokens used in the experiments?
- Basis in paper: [inferred] The paper notes that "the maximum sequence length we use during fine-tuning is 2,048, whereas Ivison et al. (2023) use 8,192 for full fine-tuning" and observes that "it is possible that the shorter sequence length for PEFT has a significant effect on downstream performance for open-ended generation tasks."
- Why unresolved: The experiments were limited to 2,048 token sequences due to computational constraints, leaving open whether SpIEL's performance gap on tasks like GSM and HumanEval would persist with longer sequences.
- What evidence would resolve it: Experiments comparing SpIEL and full fine-tuning on the same datasets but with 8,192 token sequences, particularly focusing on the GSM and HumanEval benchmarks where SpIEL underperformed.

### Open Question 3
- Question: What is the theoretical minimum memory overhead achievable for sparse fine-tuning methods, and how close does SpIEL come to this bound compared to other PEFT methods?
- Basis in paper: [explicit] The paper states that "we wish for the memory use during training (beyond that required to store the pretrained model weights) to scale linearly with the number of SFT parameters O(dϕ) rather than LLM parameters O(dθ)" and proposes SpIEL as achieving this goal.
- Why unresolved: While SpIEL claims to achieve O(dϕ) memory overhead, the paper doesn't provide a theoretical analysis of the absolute minimum memory requirements for sparse fine-tuning or quantify how much of this theoretical minimum SpIEL actually achieves.
- What evidence would resolve it: A theoretical analysis of memory requirements for sparse fine-tuning (considering gradient storage, optimizer states, parameter indices, etc.) followed by empirical measurements showing how close SpIEL comes to this theoretical minimum compared to other PEFT methods.

## Limitations

- Hardware efficiency claims lack empirical evidence showing actual runtime performance comparisons across different hardware architectures
- Memory efficiency advantages depend critically on maintaining low density of active parameters, with no analysis of scaling as density increases
- All experiments focus on instruction-tuning scenarios, leaving effectiveness for other fine-tuning paradigms unexplored

## Confidence

**High confidence**: The core mechanism of maintaining deltas relative to pretrained weights and the basic pruning/growth algorithm is well-specified and theoretically sound. The memory efficiency claims for low-to-moderate density levels are supported by the described implementation.

**Medium confidence**: The performance comparisons against LoRA and other PEFT methods are reasonably convincing but limited to specific datasets and model sizes. The choice of hyperparameters and density levels appears somewhat arbitrary without systematic ablation studies.

**Low confidence**: Claims about SpIEL's compatibility with quantization and extreme memory-efficient optimizers lack empirical validation. The momentum approximation quality in SpIEL-MA is asserted but not rigorously evaluated against ground truth.

## Next Checks

1. **Memory scaling analysis**: Implement SpIEL with varying density levels from 0.1% to 50% on LLaMA 2 7B and measure actual memory consumption, comparing against theoretical predictions and LoRA baselines across different hardware platforms.

2. **Runtime efficiency benchmark**: Profile the computational overhead of SpIEL's scatter-add operations and dynamic index management across different sparsity levels, comparing wall-clock time against standard PEFT methods on both GPU and CPU architectures.

3. **Ablation study of growth criteria**: Systematically compare SpIEL-AG and SpIEL-MA across multiple tasks and density levels, measuring both performance and memory efficiency to quantify the tradeoff between selection quality and memory savings.