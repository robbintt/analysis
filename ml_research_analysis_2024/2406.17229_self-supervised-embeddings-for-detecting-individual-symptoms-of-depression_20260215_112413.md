---
ver: rpa2
title: Self-Supervised Embeddings for Detecting Individual Symptoms of Depression
arxiv_id: '2406.17229'
source_url: https://arxiv.org/abs/2406.17229
tags:
- speech
- depression
- symptoms
- symptom
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study leverages self-supervised learning (SSL) to identify
  individual symptoms of depression from speech. Unlike prior work that focuses on
  detecting depression or predicting its severity, we train models to detect specific
  symptoms while also predicting overall depression severity using the MADRS scale.
---

# Self-Supervised Embeddings for Detecting Individual Symptoms of Depression

## Quick Facts
- arXiv ID: 2406.17229
- Source URL: https://arxiv.org/abs/2406.17229
- Reference count: 0
- Key outcome: SSL-based models significantly outperform conventional speech features for detecting individual depressive symptoms

## Executive Summary
This study investigates the use of self-supervised learning (SSL) to detect individual symptoms of depression from speech, moving beyond traditional approaches that focus on overall depression severity. The research compares various SSL-based speech models (HuBERT, WavLM, BEATS, ContentVec, RDINO, BYOL-Audio, AudioMAE) with conventional speech features like spectrograms, eGeMAPS, and COV AREP. Results demonstrate that SSL embeddings significantly improve symptom detection performance, with multi-task learning providing comparable or better results while being more computationally efficient. The study also shows that combining embeddings from different SSL models that encode semantic, speaker, or prosodic information further enhances detection accuracy.

## Method Summary
The study uses a dataset of 505 participants with speech recordings labeled using the Montgomery-Åsberg Depression Rating Scale (MADRS) for 10 individual symptoms and overall severity. Speech segments of 10 seconds are processed through frozen SSL models to extract embeddings, which are then passed through feed-forward layers for symptom classification and severity regression. The approach is compared against CNNs trained on conventional speech features. Both single-task and multi-task learning setups are evaluated, with multi-task learning using 10 symptom classification heads plus a regression head for overall severity. Performance is measured using macro F-scores for symptom detection and RMSE for severity prediction.

## Key Results
- SSL-based models significantly outperform conventional speech features (spectrograms, eGeMAPS, COV AREP) for most depressive symptoms
- Multi-task learning performs comparably or better than single-task learning while being more computationally efficient
- Combining embeddings from multiple SSL models that encode different speech aspects (semantic, speaker, prosodic) further improves detection performance
- Models like WavLM and HuBERT (semantic-focused) excel at detecting symptoms like sadness and reduced interest, while RDINO (speaker-focused) performs better on symptoms like increased sleep and appetite changes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SSL-based speech models outperform conventional features because they encode richer semantic, speaker, and prosodic information that correlates with depressive symptoms
- Core assumption: SSL pretraining objectives capture speech patterns that overlap meaningfully with depression-related acoustic markers
- Evidence: Study shows SSL models outperform conventional features, with eGeMAPS performing best among conventional approaches
- Break condition: If SSL embeddings don't capture depression-relevant patterns, performance would not exceed conventional features

### Mechanism 2
- Claim: Multi-task learning performs comparably or better than single-task while being more computationally efficient
- Core assumption: Depression symptoms share underlying acoustic patterns that benefit from joint representation learning
- Evidence: Multi-task systems perform comparably or better than single-task systems according to experimental results
- Break condition: If symptom-specific patterns are too distinct, joint training could hurt performance

### Mechanism 3
- Claim: Combining embeddings from multiple SSL models that encode different information types improves symptom detection
- Core assumption: Semantic, speaker, and prosodic information each contribute uniquely to detecting different depression symptoms
- Evidence: Different SSL models excel at detecting different symptoms based on their information encoding focus
- Break condition: If different information types are redundant or conflicting, combining embeddings could degrade performance

## Foundational Learning

- Concept: Self-supervised learning (SSL) in speech processing
  - Why needed: SSL allows leveraging large unlabeled speech datasets to learn rich representations that capture depression-relevant acoustic patterns without requiring extensive manual annotations
  - Quick check: What is the key difference between SSL pretraining objectives like MLM and traditional supervised training?

- Concept: Depression rating scales (MADRS) and symptom-based assessment
  - Why needed: Clinical framework focuses on individual symptoms rather than just overall severity, requiring models to detect specific symptom presence/absence
  - Quick check: How does the MADRS scale differ from self-reported questionnaires like PHQ-8 in terms of clinical validity?

- Concept: Multi-task learning and representation sharing
  - Why needed: Joint training on multiple related depression symptoms can improve generalization by sharing learned representations across symptom detection tasks
  - Quick check: What is the main advantage of multi-task learning over training separate models for each symptom?

## Architecture Onboarding

- Component map: Speech → SSL embeddings → Average pooling → Feed-forward layers → Symptom classification + severity prediction
- Critical path: Speech segments are processed through frozen SSL models, embeddings are averaged and passed through feed-forward layers to produce symptom classifications and severity predictions
- Design tradeoffs:
  - Single vs. multiple SSL models: Combining provides richer representations but increases computational cost
  - Single-task vs. multi-task learning: Multi-task is more efficient but may hurt performance if symptom patterns are too distinct
  - Frozen vs. fine-tuned SSL weights: Freezing reduces training time but may limit adaptation to depression-specific patterns
- Failure signatures:
  - Performance near majority class baseline indicates SSL embeddings not capturing relevant patterns
  - Multi-task underperformance on most symptoms suggests symptom patterns are too distinct
  - Performance degradation when combining SSL models indicates conflicting or redundant information
- First 3 experiments:
  1. Train baseline CNN on conventional features to establish performance floor
  2. Train single SSL model (HuBERT) in single-task setting to verify SSL advantage
  3. Train same SSL model in multi-task setting to evaluate efficiency gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal combination of SSL models for detecting individual depressive symptoms, and how does this vary across symptoms?
- Basis: Paper shows that combining certain SSL models improves performance but doesn't determine optimal combinations for each symptom
- Why unresolved: Effectiveness of different combinations may vary depending on the symptom being detected
- What evidence would resolve it: Systematic experiments testing various combinations for each individual symptom with statistical analysis

### Open Question 2
- Question: How do semantic, speaker, and prosodic information contribute differently to the detection of various depressive symptoms?
- Basis: Paper analyzes different SSL models to assess significance of each information type but doesn't quantify exact contributions
- Why unresolved: Paper provides initial insights but lacks detailed quantification of each information type's contribution
- What evidence would resolve it: Detailed ablation studies systematically removing or isolating each information type

### Open Question 3
- Question: How does multi-task learning performance compare to single-task across different depressive symptoms and overall severity prediction?
- Basis: Paper shows general effectiveness of multi-task learning but lacks comprehensive analysis across individual symptoms and severity prediction
- Why unresolved: Paper demonstrates effectiveness but doesn't provide detailed comparison across all symptoms and severity prediction
- What evidence would resolve it: Detailed performance metrics for each symptom and severity prediction using both approaches with statistical analysis

## Limitations

- Dataset generalization: Results rely on a single internal dataset that may not generalize to other clinical populations or speech conditions
- SSL model selection: Study uses frozen embeddings without exploring fine-tuning or adaptation for depression detection
- Symptom correlation analysis: Paper lacks detailed analysis of how individual symptoms correlate with speech patterns across different demographic groups

## Confidence

- High Confidence: SSL-based models outperform conventional speech features is well-supported by experimental results
- Medium Confidence: Multi-task learning being computationally efficient while maintaining performance is supported but needs larger-scale validation
- Medium Confidence: Combining different SSL model types improves performance is supported by ablation studies but optimal combinations not thoroughly explored

## Next Checks

1. Cross-dataset validation: Test multi-task SSL model on external depression datasets with different speech conditions and demographic distributions
2. Fine-tuning analysis: Compare frozen SSL embeddings with fine-tuned SSL models to determine if adaptation improves performance
3. Demographic bias assessment: Analyze model performance across different age groups, genders, and cultural backgrounds to identify potential biases