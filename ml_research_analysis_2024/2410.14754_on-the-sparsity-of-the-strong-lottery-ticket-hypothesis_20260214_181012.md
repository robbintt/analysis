---
ver: rpa2
title: On the Sparsity of the Strong Lottery Ticket Hypothesis
arxiv_id: '2410.14754'
source_url: https://arxiv.org/abs/2410.14754
tags:
- log2
- theorem
- network
- which
- random
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the Strong Lottery Ticket Hypothesis (SLTH)
  by providing the first rigorous proofs with guarantees on the sparsity of winning
  tickets for classical neural network architectures. The authors introduce the Random
  Fixed-Size Subset Sum (RFSS) problem, a refinement of the Random Subset Sum (RSS)
  problem, where subsets must have a fixed size rather than being of size up to a
  given value.
---

# On the Sparsity of the Strong Lottery Ticket Hypothesis

## Quick Facts
- **arXiv ID**: 2410.14754
- **Source URL**: https://arxiv.org/abs/2410.14754
- **Reference count**: 40
- **Primary result**: First rigorous proofs with guarantees on sparsity of winning tickets for classical neural network architectures

## Executive Summary
This paper addresses the Strong Lottery Ticket Hypothesis (SLTH) by providing the first rigorous proofs with guarantees on the sparsity of winning tickets for classical neural network architectures. The authors introduce the Random Fixed-Size Subset Sum (RFSS) problem, a refinement of the Random Subset Sum (RSS) problem where subsets must have fixed size rather than being of size up to a given value. By leveraging RFSS, the paper proves that random neural networks can be pruned to obtain sparse subnetworks that approximate target networks up to a given error, with the sparsity determined by an overparameterization parameter. The results show that an overparameterized network with m parameters can approximate any target network with mt parameters using a subnetwork with sparsity 1-γ, where γ = O(ε/mt) when the overparameterization is Θ(m²t/ε²).

## Method Summary
The core method introduces the Random Fixed-Size Subset Sum (RFSS) problem and proves that it can be solved with high probability when the overparameterization ratio scales appropriately. The proof uses the second moment method to bound the probability that random subsets can approximate any target value, combined with concentration bounds for sum-bounded distributions. This theoretical framework is then applied to prove the existence of sparse winning tickets in Dense Neural Networks and equivariant networks (including CNNs). The approach shows that by carefully controlling the size of pruned subsets in each layer, one can achieve uniform sparsity guarantees across the entire network.

## Key Results
- Proves the first rigorous bounds on sparsity of winning tickets in SLTH for Dense Neural Networks with density γ = O(ε/mt)
- Introduces RFSS problem as a key technical contribution that enables uniform sparsity control across network layers
- Shows that overparameterization requirements are essentially tight with matching lower bounds
- Extends results to equivariant networks including CNNs as a special case

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Fixed-size subset selection enables uniform sparsity control across network layers.
- **Mechanism**: By constraining pruning to fixed-size subsets (RFSS), each layer in the overparameterized network can be pruned independently while maintaining a consistent density parameter γ across all layers. This uniformity ensures the resulting subnetwork preserves the structural properties needed for accurate function approximation.
- **Core assumption**: The RFSS problem can be solved with high probability when the overparameterization ratio scales as O(m²t/ε²) for a given sparsity γ.
- **Evidence anchors**:
  - [abstract]: "Central to our results, is the proof of an essentially tight bound on the Random Fixed-Size Subset Sum Problem (RFSS), a variant of the RSS Problem in which we only ask for subsets of a given size"
  - [section]: "By focusing on subsets of fixed size rather than subsets of size up to a given value for two main reasons. From a theoretical point of view, it is a stronger requirement, and practically speaking, using fixed-size subsets enables us to achieve SLTH results where the layers of the lottery ticket exhibit a uniform structure"
  - [corpus]: Weak evidence - related papers focus on general SLTH but not specifically on fixed-size constraints
- **Break condition**: If the required overparameterization exceeds practical limits or if the RFSS bound becomes too loose for high sparsity regimes (small k/n).

### Mechanism 2
- **Claim**: Sum-bounded distributions ensure sufficient diversity in random weights for accurate approximation.
- **Mechanism**: The paper leverages distributions (like Uniform[-1,1]) that are sum-bounded, meaning their sums maintain predictable density bounds. This property guarantees that random weight combinations can approximate any target value within the error tolerance ε, which is essential for proving the existence of winning tickets.
- **Core assumption**: The target network weights and the random network weights are drawn from sum-bounded distributions.
- **Evidence anchors**:
  - [section]: "We say that a probability density function f is sum-bounded if there exist positive constants cl and cu such that, for all k ∈ N, given k independent samples X1, ..., Xk with density f, the density of their sum fΣ[k] satisfies cl√k ≤ fΣ[k](x) ≤ cu√k"
  - [section]: "Previous SLTH results rely on a classical resampling argument by [18, Corollary 3.3], which shows how RSS results for Uniform [−1, 1] independent random variables naturally extend to independent random variables that contains a uniform distribution"
  - [corpus]: No direct evidence - this is a novel technical contribution specific to this paper
- **Break condition**: If the target network uses activation functions or weight distributions that violate the sum-bounded condition.

### Mechanism 3
- **Claim**: Second moment method provides tight bounds on the probability of successful subset selection.
- **Mechanism**: The proof uses the second moment method to bound the probability that random subsets can approximate any target value. By carefully analyzing the variance of the subset sum process and using combinatorial bounds, the authors show that with sufficient overparameterization, the probability of finding suitable subsets approaches 1.
- **Core assumption**: The variance analysis correctly captures the concentration properties of the subset sum process.
- **Evidence anchors**:
  - [section]: "We exploit the second moment method for RFSS, generalising it to arbitrary k... Pr(Y > 0) ≥ (E[Y])²/E[Y²]"
  - [section]: "The proof of Theorem 2 is given in Section 3.1, and it actually holds for any 1 ≤ k ≤ λn, for an arbitrary λ ∈ [1/n, 1)"
  - [corpus]: No direct evidence - this is a novel technical contribution specific to this paper
- **Break condition**: If the variance bounds become too loose for very high overparameterization ratios or if the target approximation error ε becomes too small relative to the network size.

## Foundational Learning

- **Concept**: Random Subset Sum (RSS) Problem
  - **Why needed here**: RSS forms the theoretical foundation for understanding how random weight combinations can approximate target values. The SLTH proof strategy relies on showing that random neural networks contain subnetworks that can approximate any smaller target network.
  - **Quick check question**: If you have n random variables uniformly distributed in [-1,1], what is the minimum n needed to guarantee that some subset of size k can approximate any target value in [-√k, √k] within error ε?

- **Concept**: Second Moment Method in Probabilistic Combinatorics
  - **Why needed here**: This technique is used to prove the existence of combinatorial structures (in this case, suitable subsets) by showing that the expected number of such structures is large and their variance is controlled.
  - **Quick check question**: Given a random variable Y representing the number of "good" subsets, how would you use the inequality Pr(Y > 0) ≥ (E[Y])²/E[Y²] to prove that at least one good subset exists with high probability?

- **Concept**: Concentration of Measure for Sums of Random Variables
  - **Why needed here**: Understanding how sums of random variables concentrate around their mean is crucial for bounding the approximation error and proving that the random subsets can achieve the desired accuracy.
  - **Quick check question**: If X1,...,Xk are independent random variables with mean 0 and variance σ², what is the typical size of |ΣXi| for large k, and how does this relate to the approximation error ε?

## Architecture Onboarding

- **Component map**: RFSS problem -> Sum-bounded distribution analysis -> Second moment method for probability bounds -> Application to Dense Neural Networks -> Extension to equivariant networks

- **Critical path**: The critical path for understanding and extending this work is: grasp the RSS problem → understand why fixed-size subsets are needed → follow the second moment method proof → apply the results to neural network pruning. Missing any step makes it difficult to see how the sparsity guarantees are derived.

- **Design tradeoffs**: The main tradeoff is between sparsity (smaller γ) and overparameterization (larger m). Higher sparsity requires quadratically more parameters in the overparameterized network. Another tradeoff is between the generality of the result (applying to various network architectures) and the tightness of the bounds (which may be loose for specific architectures).

- **Failure signatures**: Common failure modes include: (1) attempting to apply the results to networks with non-sum-bounded weight distributions, (2) expecting practical algorithms for finding the winning tickets (the paper only proves existence), (3) misunderstanding that the sparsity guarantees are asymptotic and may not hold for small networks, (4) overlooking that the bounds are worst-case and may be pessimistic for specific target functions.

- **First 3 experiments**:
  1. **Verify sum-boundedness**: Take a simple case with Uniform[-1,1] weights and empirically verify that the density of their sum follows the cl√k ≤ f(x,k) ≤ cu√k bounds for various k values.
  2. **Test RFSS empirically**: For small n and k, generate random subsets and measure the probability of approximating various target values within ε, comparing against the theoretical bounds.
  3. **Apply to simple networks**: Take a small overparameterized network and a target network, then use the pruning strategy outlined in the paper to find a sparse subnetwork, measuring the actual approximation error versus the theoretical guarantee.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the RFSS problem be extended to the multidimensional case where random samples and targets are vectors in Rd?
- **Basis in paper**: [explicit] The authors mention this as a challenging open problem in the conclusions, noting that previous extensions of RSS to Multidimensional RSS have allowed proving structured-pruning versions of the SLTH.
- **Why unresolved**: The current RFSS result only handles scalar targets and samples, while practical applications often involve multidimensional data.
- **What evidence would resolve it**: A proof of Multidimensional RFSS showing how the overparameterization requirement scales with dimension d and sparsity, with empirical validation on structured neural network pruning tasks.

### Open Question 2
- **Question**: Can the success probability of RFSS be improved from a constant to 1-ε, eliminating the extra logarithmic factor in the corollaries?
- **Basis in paper**: [explicit] The authors note in the conclusions that improving the success probability from constant to 1-ε would allow avoiding the "shaving off" of the extra log2(1/ε) factor.
- **Why unresolved**: The current proof technique using the second moment method only guarantees constant success probability.
- **What evidence would resolve it**: A refined analysis of the second moment method or an alternative proof technique that achieves probability 1-ε, with corresponding improvements in the SLTH bounds.

### Open Question 3
- **Question**: Can efficient algorithms be developed to reliably find sparse strong lottery tickets of desired sparsity levels?
- **Basis in paper**: [explicit] The authors discuss in the limitations section that despite theoretical existence proofs, it remains unclear if lottery tickets can be found efficiently, though empirical evidence suggests efficient algorithms exist.
- **Why unresolved**: Current training-by-pruning methods like [30, 27] are computationally expensive and don't guarantee finding tickets of specific sparsity levels.
- **What evidence would resolve it**: Development of new training-by-pruning algorithms or theoretical insights showing efficient (polynomial-time) algorithms exist for finding sparse strong lottery tickets, with experimental validation showing reliable discovery of tickets at various sparsity levels.

## Limitations
- Results are asymptotic and may not hold for small networks
- Overparameterization requirements (Θ(m²t/ε²)) may be prohibitive in practice
- Paper only proves existence of sparse winning tickets, not efficient algorithms to find them
- Bounds may be loose for specific architectures or target functions

## Confidence
- **Dense Neural Networks**: High - The theoretical framework is well-established and the proofs are rigorous
- **Equivariant Networks**: Medium - Relies on additional assumptions about equivariant basis and may have looser bounds
- **Practical Applicability**: Medium - Theoretical bounds may be loose for small networks and overparameterization requirements are high

## Next Checks
1. Empirically verify the sum-bounded distribution conditions for common initialization schemes
2. Test the practical overparameterization requirements on small networks to assess the tightness of theoretical bounds
3. Apply the pruning strategy to a convolutional network and measure actual sparsity achieved versus theoretical guarantees