---
ver: rpa2
title: 'TransportationGames: Benchmarking Transportation Knowledge of (Multimodal)
  Large Language Models'
arxiv_id: '2401.04471'
source_url: https://arxiv.org/abs/2401.04471
tags:
- tasks
- llms
- transportation
- traffic
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces TransportationGames, a benchmark for evaluating
  multimodal large language models in the transportation domain. The benchmark includes
  10 tasks categorized into three levels of Bloom''s Taxonomy: memorization, understanding,
  and applying transportation knowledge.'
---

# TransportationGames: Benchmarking Transportation Knowledge of (Multimodal) Large Language Models

## Quick Facts
- **arXiv ID**: 2401.04471
- **Source URL**: https://arxiv.org/abs/2401.04471
- **Authors**: Xue Zhang; Xiangyu Shi; Xinyue Lou; Rui Qi; Yufeng Chen; Jinan Xu; Wenjuan Han
- **Reference count**: 6
- **Primary result**: Introduces TransportationGames benchmark with 10 transportation tasks across 3 Bloom's Taxonomy levels, revealing room for improvement in multimodal LLM performance

## Executive Summary
This paper introduces TransportationGames, a comprehensive benchmark for evaluating multimodal large language models in the transportation domain. The benchmark consists of 10 tasks categorized into three cognitive levels based on Bloom's Taxonomy: memorization, understanding, and applying transportation knowledge. The evaluation covers both text-only and multimodal tasks involving traffic concepts, regulations, signs, accidents, and emergency planning. Results show that while some models perform well on specific tasks, overall performance indicates significant room for improvement, particularly in multimodal reasoning and long text generation.

## Method Summary
The authors collected text-only and multimodal datasets for transportation-related tasks, categorized them according to Bloom's Taxonomy levels (memorization, understanding, applying), and evaluated various (M)LLMs using zero-shot settings. The evaluation employed multiple metrics including accuracy for multiple-choice questions, ROUGE scores for text generation tasks, and GPT-4-Eval for assessing generated answers' accuracy, redundancy, fluency, and completeness. The benchmark specifically targets Chinese transportation scenarios while maintaining relevance to general transportation knowledge.

## Key Results
- Model performance varies significantly across different cognitive levels, with memorization tasks generally showing better results than understanding and application tasks
- Multimodal tasks present greater challenges than text-only tasks, indicating limitations in current models' visual-textual integration capabilities
- BaseModel selection proves critical for performance, with models having relevant pre-training data showing superior results regardless of instruction tuning quality

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Bloom's Taxonomy provides a structured framework for categorizing transportation-related tasks into progressively complex cognitive levels
- **Mechanism**: The authors map tasks into three levels—memorization, understanding, and applying—allowing for systematic evaluation of LLM capabilities across cognitive complexity
- **Core assumption**: Cognitive complexity in transportation knowledge follows a predictable progression from simple recall to practical application
- **Evidence anchors**:
  - [abstract]: "By comprehensively considering the applications in real-world scenarios and referring to the first three levels in Bloom's Taxonomy..."
  - [section]: "Inspired by Fei et al. (2023), we adopt Bloom's cognitive model for task classification, aiming to capture the models' capabilities at a higher level."
- **Break condition**: If transportation tasks cannot be meaningfully categorized into these cognitive levels, or if the progression doesn't reflect actual knowledge acquisition patterns

### Mechanism 2
- **Claim**: Multi-modal evaluation is essential for assessing real-world transportation domain performance
- **Mechanism**: By including both text-only and multimodal (text + image) tasks, the benchmark captures the full range of inputs transportation professionals encounter
- **Core assumption**: Real-world transportation tasks frequently involve interpreting visual information alongside textual data
- **Evidence anchors**:
  - [abstract]: "The evaluation results show that although some models perform well in some tasks, there is still much room for improvement overall."
  - [section]: "We take into account diverse sub-domains in transportation and the varying needs of different people, including the general public and industry practitioners..."
- **Break condition**: If visual information adds negligible value to transportation knowledge assessment, or if models cannot effectively process multimodal inputs

### Mechanism 3
- **Claim**: BaseModel selection is a critical determinant of model performance in domain-specific tasks
- **Mechanism**: The quality and scope of pre-training data directly influences the upper bound of a model's performance on specialized benchmarks
- **Core assumption**: Pre-training provides foundational knowledge that instruction tuning can only build upon, not replace
- **Evidence anchors**:
  - [section]: "We can observe from Table 3 and Table 4 that the performance of some small-scale models can even outperform that of many large-scale models... Additionally, due to the limited amount of Chinese corpus learned by LLaMa during the pre-training stage, the performance of the LLaMa series models is unsatisfactory..."
  - [corpus]: FMR scores indicate moderate relatedness to transportation domain, suggesting pre-training data may not fully cover transportation knowledge
- **Break condition**: If instruction tuning or domain-specific fine-tuning can overcome limitations in pre-training data, or if the relationship between pre-training and downstream performance is weaker than assumed

## Foundational Learning

- **Concept**: Bloom's Taxonomy and cognitive complexity levels
  - Why needed here: The benchmark uses Bloom's Taxonomy to structure tasks by cognitive complexity, requiring understanding of how learning objectives progress from basic recall to complex application
  - Quick check question: Can you explain the difference between "understanding" and "applying" levels in Bloom's Taxonomy and why this distinction matters for transportation tasks?

- **Concept**: Multimodal learning and cross-modal reasoning
  - Why needed here: The benchmark includes both text and image inputs, requiring models to integrate visual and textual information for transportation tasks
  - Quick check question: What are the key challenges in developing models that can effectively combine visual and textual information for complex reasoning tasks?

- **Concept**: Domain-specific benchmarking methodology
  - Why needed here: Creating effective benchmarks requires understanding how to select representative tasks, ensure data quality, and design appropriate evaluation metrics for specialized domains
  - Quick check question: What factors should be considered when selecting tasks for a domain-specific benchmark to ensure comprehensive coverage of the field?

## Architecture Onboarding

- **Component map**: Data Collection Pipeline: Web scraping → Text extraction → Manual verification → Data cleaning
- **Critical path**: Task selection → Data collection → Model evaluation → Result analysis → Performance insights
- **Design tradeoffs**:
  - Task difficulty vs. coverage breadth: More challenging tasks provide better differentiation but may limit participation
  - Data quality vs. quantity: Manual verification ensures quality but limits scale
  - Evaluation complexity vs. interpretability: More sophisticated metrics capture nuance but may be harder to interpret

- **Failure signatures**:
  - Poor performance across all tasks suggests fundamental limitations in model architecture or training approach
  - Disproportionate difficulty with multimodal tasks indicates insufficient visual processing capabilities
  - Strong memorization but weak application suggests overfitting to specific patterns rather than genuine understanding

- **First 3 experiments**:
  1. Evaluate a small set of models on a single task type to establish baseline performance and identify immediate challenges
  2. Test the impact of BaseModel selection by comparing models with different pre-training data but similar instruction tuning
  3. Analyze the contribution of multimodal information by comparing performance on text-only vs. multimodal versions of the same task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of (M)LLMs on transportation tasks change with increasing model size and what is the upper limit of this improvement?
- Basis in paper: [explicit] The paper mentions that scaling up the model size improves performance with similar BaseModels
- Why unresolved: The paper only tested a limited number of models and did not explore the full potential of scaling up
- What evidence would resolve it: Comprehensive testing of (M)LLMs across a wider range of sizes and model types

### Open Question 2
- Question: How can (M)LLMs be effectively evaluated for long text generation tasks in the transportation domain?
- Basis in paper: [inferred] The paper mentions the difficulty of evaluating long text generation tasks and uses ROUGE-L and GPT-4-Eval together
- Why unresolved: The paper acknowledges the non-uniqueness of answers and the difficulty in ensuring the same effect as manual evaluation
- What evidence would resolve it: Development of new evaluation metrics or methods specifically designed for long text generation tasks in the transportation domain

### Open Question 3
- Question: How can (M)LLMs be adapted to handle tasks that require connecting to external databases, such as real-time road condition queries and traffic flow prediction?
- Basis in paper: [explicit] The paper mentions that their TransportationGames does not include tasks that require connecting to external databases
- Why unresolved: The paper states that this type of task is not included in their benchmark due to the need for external databases
- What evidence would resolve it: Development of new (M)LLM architectures or training methods that can effectively handle tasks requiring external database connections

## Limitations

- Benchmark primarily focuses on Chinese transportation contexts without validation of cross-cultural applicability
- Heavy reliance on GPT-4-Eval for automated evaluation without thorough examination of potential biases or reliability issues
- Limited transparency in data collection methods and potential dataset biases affecting benchmark validity

## Confidence

- **High confidence**: The structured task categorization using Bloom's Taxonomy and the systematic evaluation approach
- **Medium confidence**: The generalizability of findings across different transportation contexts and cultures
- **Medium confidence**: The reliability of GPT-4-Eval as the primary evaluation metric for generated responses

## Next Checks

1. Conduct cross-cultural validation by testing models on transportation tasks from multiple countries to assess the benchmark's generalizability beyond Chinese contexts
2. Implement human evaluation studies alongside GPT-4-Eval to validate the automated assessment methodology and identify potential biases
3. Perform ablation studies on task difficulty by systematically removing visual components from multimodal tasks to quantify the contribution of multimodal reasoning to overall performance