---
ver: rpa2
title: Task Oriented Dialogue as a Catalyst for Self-Supervised Automatic Speech Recognition
arxiv_id: '2401.02417'
source_url: https://arxiv.org/abs/2401.02417
tags:
- learning
- speech
- dialogue
- current
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CLC, a family of self-supervised fine-tuning
  approaches that improve contextual automated speech recognition in task-oriented
  dialogues by learning from both successful and unsuccessful conversational interactions.
  The method uses two auxiliary contrastive losses: one that maximizes mutual information
  between current, past, and future utterances in a dialogue, and another that encourages
  the model to avoid high-confidence errors when a user repeats or rephrases a previous
  utterance.'
---

# Task Oriented Dialogue as a Catalyst for Self-Supervised Automatic Speech Recognition

## Quick Facts
- arXiv ID: 2401.02417
- Source URL: https://arxiv.org/abs/2401.02417
- Reference count: 0
- Key outcome: CLC improves contextual ASR in task-oriented dialogues using contrastive losses, achieving up to 19.2% WER reduction on OD3 dataset

## Executive Summary
This paper introduces CLC, a self-supervised fine-tuning approach that enhances contextual ASR in task-oriented dialogues by learning from conversational interactions. The method employs two auxiliary contrastive losses: one that maximizes mutual information between current, past, and future utterances, and another that helps the model avoid high-confidence errors when users repeat or rephrase. Evaluated on OD3, a large semi-synthetic dialogue dataset, and internal assistant data, CLC demonstrates significant WER improvements over baselines, showing that learning from conversational context—even when containing errors—substantially benefits ASR performance.

## Method Summary
CLC fine-tunes pre-trained conformer-based ASR models using two auxiliary contrastive losses. The Past-Future loss maximizes mutual information between current, past, and future utterances within a dialogue session, encouraging the model to produce similar embeddings for related turns. The N-best loss uses supervised contrastive learning to improve predictions when users repeat or rephrase, treating the top-1 prediction as a negative example in such cases. The approach is trained on OD3, a semi-synthetic dataset created by injecting synthetic errors, repeats, and rephrases into existing task-oriented dialogue data, totaling 63K conversations and 1,172 hours of audio.

## Key Results
- On OD3 dataset: CLC achieves up to 19.2% WER reduction compared to baseline models
- On internal conversational assistant data: Up to 6.7% WER improvement over baselines
- Ablation studies show both contrastive losses contribute to performance gains, with Past-Future loss providing the largest individual improvement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The past-future contrastive loss improves ASR by increasing mutual information between current, past, and future utterances within a dialogue session.
- Mechanism: The method encodes current, past, and future utterances into embeddings, then applies contrastive learning to make embeddings from the same dialogue closer in latent space while pushing embeddings from different dialogues apart.
- Core assumption: Utterances within the same dialogue session contain more semantic and representational overlap than utterances from different dialogues.
- Evidence anchors:
  - [abstract] "one that maximizes mutual information between current, past, and future utterances in a dialogue"
  - [section] "the speech encoder representations of audio within a session should be closer in the latent space (on average) than the representations between sessions"
  - [corpus] Weak evidence - no direct corpus studies cited for this specific contrastive approach
- Break condition: If the assumption about semantic overlap within dialogues doesn't hold (e.g., in very long dialogues where context becomes diluted), or if the encoder fails to capture meaningful temporal dependencies.

### Mechanism 2
- Claim: The N-best contrastive loss improves ASR by learning from failed conversational turns where users repeat or rephrase previous utterances.
- Mechanism: When a user repeats or rephrases, the method treats the top-1 ASR prediction as a negative example and encourages the model to produce different outputs, while treating non-repetition cases as positive examples to reinforce correct predictions.
- Core assumption: The model's top-1 prediction is often wrong when a repeat/rephrase occurs, and the top-k set contains a better alternative.
- Evidence anchors:
  - [abstract] "another that encourages the model to avoid high-confidence errors when a user repeats or rephrases a previous utterance"
  - [section] "we can use supervised contrastive learning [20] to improve the model" and "we observe empirically that our models have high oracle WER, allowing this method to achieve a weak approximation to oracle re-ranking"
  - [corpus] Weak evidence - no direct corpus studies cited for the effectiveness of this specific N-best approach
- Break condition: If the top-k set doesn't contain a better alternative, or if the model's oracle WER is too low for this approach to be effective.

### Mechanism 3
- Claim: Training on flawed conversational data (with errors, repeats, and rephrases) improves ASR robustness compared to training only on clean data.
- Mechanism: The OD3 dataset is constructed by adding synthetic errors, repeats, and rephrases to existing task-oriented dialogue datasets, creating a challenging training environment that forces the model to handle real-world conversational noise.
- Core assumption: Exposure to conversational errors during training improves the model's ability to handle similar errors at inference time.
- Evidence anchors:
  - [abstract] "learning from conversational context, even when containing errors, significantly benefits ASR performance"
  - [section] "we synthetically introduce errors and noisy conversations into the data" and "We consider conversational turns with WER higher than 15% candidates for the injection of either a repeat, or a rephrase"
  - [corpus] Weak evidence - no direct corpus studies cited for the effectiveness of this data augmentation approach
- Break condition: If the synthetic errors don't match the distribution of real-world errors, or if the model overfits to the specific types of synthetic errors used.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: To create a loss function that pulls together semantically related utterances (same dialogue) while pushing apart unrelated ones (different dialogues)
  - Quick check question: What is the main difference between supervised and unsupervised contrastive learning?

- Concept: Mutual information maximization
  - Why needed here: To ensure that the model learns representations that capture the shared semantic content across related utterances
  - Quick check question: How does maximizing mutual information between current and past utterances help with ASR?

- Concept: N-best list rescoring
  - Why needed here: To leverage the model's own predictions as additional training signals, especially for handling conversational errors
  - Quick check question: Why might the top-1 prediction be a negative example when a user repeats themselves?

## Architecture Onboarding

- Component map: Conformer encoder → Pooling heads (past, current, future) → Contrastive losses → LSTM decoder → Vocabulary
- Critical path: Audio input → Conformer encoder → Pooling heads → Contrastive loss computation → Gradients → Encoder/decoder updates
- Design tradeoffs: The past-future loss captures long-range dependencies but may dilute local context; the N-best loss is more targeted but depends on the quality of beam search predictions
- Failure signatures: Poor performance on conversational data but good on single-turn data suggests the past-future loss isn't working; good performance on clean data but poor on error-prone data suggests the N-best loss isn't effective
- First 3 experiments:
  1. Ablation study: Train with only the baseline ASR loss, then add each contrastive loss separately to measure individual contributions
  2. Hyperparameter sweep: Systematically vary α, β, γ, and κ to find the optimal balance between the different loss components
  3. Data augmentation study: Train on clean data vs. data with synthetic errors to measure the impact of the OD3-style augmentation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of CLC scale with increasing model size and complexity?
- Basis in paper: [inferred] The authors note that it remains an open question how the approaches scale with model parameters, suggesting uncertainty about performance improvements with larger models.
- Why unresolved: The paper does not provide experimental results or analysis on how CLC's effectiveness changes with model size, leaving this relationship unexplored.
- What evidence would resolve it: Systematic experiments varying model size (e.g., number of parameters, layers) while keeping other factors constant, measuring CLC's impact on WER across different scales.

### Open Question 2
- Question: What is the impact of different pre-training data mixes on CLC's performance?
- Basis in paper: [explicit] The authors explicitly state it is an open question to what extent different mixes of pre-training data alter the performance of the model.
- Why unresolved: The paper does not investigate how varying the composition or domain of pre-training data affects CLC's ability to improve ASR performance.
- What evidence would resolve it: Controlled experiments training CLC models on different pre-training datasets (varying in domain, size, quality) and measuring the resulting WER improvements.

### Open Question 3
- Question: Can alternative functional implementations (e.g., softmax) reduce the gradient variance in the N-best contrastive loss term?
- Basis in paper: [explicit] The authors suggest that the high variance in the max term of the N-best loss may destabilize training, and propose exploring functional implementations like softmax to reduce this variance.
- Why unresolved: The paper does not experiment with alternative implementations of the N-best loss, leaving the potential benefits of such changes untested.
- What evidence would resolve it: Empirical comparison of training stability and WER performance using different implementations of the N-best loss term (e.g., max vs. softmax) across multiple datasets and model architectures.

## Limitations

- The N-best contrastive loss mechanism depends heavily on the quality of beam search predictions and the assumption that top-1 predictions are often incorrect during repeats/rephrases
- The synthetic error injection in OD3 may not fully capture the complexity and distribution of real-world conversational errors
- The paper doesn't validate whether the assumptions about semantic overlap within dialogues hold across different dialogue lengths and domains

## Confidence

- **High confidence**: The core contribution of learning from conversational context (both successful and unsuccessful interactions) is well-supported by the significant WER improvements on both OD3 (up to 19.2%) and internal data (up to 6.7%). The methodology for constructing OD3 and the evaluation framework are clearly specified.
- **Medium confidence**: The individual mechanisms (past-future contrastive loss and N-best contrastive loss) are theoretically sound and show improvements in ablation studies, but their effectiveness depends on specific assumptions about semantic overlap within dialogues and the quality of beam search predictions that weren't fully validated across different scenarios.
- **Low confidence**: The claim that the synthetic error injection in OD3 perfectly mimics real-world conversational errors is not empirically validated. The paper doesn't compare model performance on OD3 synthetic errors versus real-world error distributions.

## Next Checks

1. **Oracle WER validation**: Systematically measure the oracle WER of beam search predictions across different dialogue turns (clean, repeat, rephrase) to verify that the assumption about high-confidence errors in repeat/rephrase scenarios holds consistently. This would validate whether the N-best contrastive loss is working as intended.

2. **Real vs. synthetic error comparison**: Evaluate model performance on a held-out set of real conversational errors (from the internal data) versus the synthetic errors in OD3 to quantify how well the synthetic augmentation matches real-world error distributions.

3. **Cross-architecture transferability**: Apply the CLC approach to a different ASR architecture (e.g., transformer-based) to test whether the improvements are architecture-specific or represent a general principle for leveraging conversational context in ASR.