---
ver: rpa2
title: Towards Dialogues for Joint Human-AI Reasoning and Value Alignment
arxiv_id: '2405.18073'
source_url: https://arxiv.org/abs/2405.18073
tags:
- arxiv
- reasoning
- language
- dialogues
- argument
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues for the use of argumentation-based inquiry dialogues
  to support joint human-AI reasoning, ensuring AI decisions align with human values
  and preferences. The authors propose replacing traditional persuasion dialogues
  with inquiry dialogues that accommodate ethical reasoning and value alignment.
---

# Towards Dialogues for Joint Human-AI Reasoning and Value Alignment

## Quick Facts
- arXiv ID: 2405.18073
- Source URL: https://arxiv.org/abs/2405.18073
- Reference count: 24
- Primary result: Proposes argumentation-based inquiry dialogues to support joint human-AI reasoning for value alignment, replacing traditional persuasion dialogues

## Executive Summary
This paper argues for the use of argumentation-based inquiry dialogues as a mechanism for achieving value alignment between humans and AI systems. The authors propose replacing traditional persuasion-focused dialogue protocols with inquiry dialogues that accommodate ethical reasoning and collaborative construction of arguments. The framework aims to ensure AI decisions align with human values through structured reasoning processes that allow multiple agents to challenge and refine each other's reasoning.

The paper outlines key requirements for implementing such dialogues, including strategic considerations for choosing argument schemes, handling incomplete arguments (enthymemes), and incorporating specialized ethical and metalevel reasoning schemes. The authors identify significant challenges for implementing these dialogues using large language models, including the need for improved argument classification, conversational history retention, and development of reasoning capacities for complex dialectical moves. They provide a roadmap for future research to address these challenges and realize the goal of joint human-LLM inquiry for value alignment.

## Method Summary
The paper proposes an argumentation-based framework for joint human-AI reasoning that replaces persuasion dialogues with inquiry dialogues. The method involves extending the ASPIC+ framework to accommodate metalevel reasoning about preferences and values, developing specialized argument schemes and critical questions for ethical reasoning, and exploring prompt engineering techniques for LLMs to instantiate these components. The approach aims to create sound and complete inquiry dialogues that support joint reasoning tasks requiring ethical considerations and value-aligned decisions.

## Key Results
- Inquiry dialogues enable collaborative construction of arguments between humans and AI, ensuring ethical considerations are incorporated into decisions
- LLMs show preliminary capability to classify arguments according to argumentation schemes and identify critical questions, though challenges remain
- The framework identifies specific requirements for implementing value-aligned dialogues, including handling enthymemes and developing metalevel reasoning schemes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Argumentation-based inquiry dialogues enable joint human-AI reasoning that supports value alignment.
- Mechanism: By replacing persuasion-focused dialogue protocols with inquiry dialogues, the framework allows multiple agents to collaboratively construct arguments and challenge each other's reasoning, ensuring ethical considerations are incorporated into decisions.
- Core assumption: Dialogue protocols can be designed to ensure sound and complete joint reasoning where a claim is established if and only if it is supported by a justified argument in the constructed argumentation framework.
- Evidence anchors:
  - [abstract] "enabling human-AI dialogue, purposed to support joint reasoning (i.e., 'inquiry'), is important for ensuring that AI decision making is aligned with human values and preferences."
  - [section] "one obtains (given the aforementioned soundness and completeness results for argumentative characterisation s of nm consequence relations) communicative accounts of distributed, i.e., 'joint', nm reasoning"
- Break condition: If the dialogue protocol cannot ensure soundness and completeness, or if agents cannot reliably construct and evaluate arguments in real-time, the joint reasoning mechanism fails.

### Mechanism 2
- Claim: Large Language Models can be enhanced to instantiate argumentation schemes and critical questions for ethical reasoning.
- Mechanism: LLMs can be prompted and fine-tuned to classify arguments according to argumentation schemes, identify supporting and attacking relationships, and complete enthymemes into fully formed arguments, thereby participating in inquiry dialogues.
- Core assumption: LLMs have sufficient reasoning capabilities to understand and apply argumentation schemes and critical questions in context, and can retain conversational histories to maintain coherence across extended dialogues.
- Evidence anchors:
  - [section] "our preliminary investigations suggest that not only are LLMs capable of classifying arguments according to the AS they instantiate, but are also able to articulate why said AS have been identified."
  - [corpus] Weak evidence - no direct citations of successful LLM-based argumentation scheme instantiation in the corpus.
- Break condition: If LLMs cannot reliably classify arguments, identify critical questions, or retain conversational context, the mechanism for LLM participation in inquiry dialogues breaks down.

### Mechanism 3
- Claim: Inquiry dialogues can facilitate human refinement of ethical preferences and values through exposure to alternative viewpoints.
- Mechanism: By structuring dialogues to explore the space of possible ethical evaluations (e.g., rights-based vs. consequentialist reasoning), users are exposed to alternative perspectives and can refine their ethical positions, with AI acting as a facilitator rather than a persuader.
- Core assumption: Humans are willing and able to engage in reflective ethical reasoning when exposed to alternative viewpoints, and AI can effectively present these alternatives without biasing the user toward a particular position.
- Evidence anchors:
  - [section] "the implication is that a user might be encouraged to reﬁne or even revise their ethical preferences, with the proviso that the AI interlocutor’s role is no t to persuade the user to commit to a particular ethical judgement, but rather to explore the space of ethical reasoning."
  - [corpus] No direct evidence - this is a theoretical claim based on philosophical approaches to moral uncertainty.
- Break condition: If users are not receptive to alternative viewpoints, or if the AI cannot effectively present alternatives without persuasion, the mechanism for preference refinement fails.

## Foundational Learning

- Concept: Argumentation schemes and critical questions
  - Why needed here: These provide the templates and evaluation criteria for constructing and challenging arguments in inquiry dialogues, enabling structured reasoning about ethical decisions.
  - Quick check question: Can you explain the difference between an argumentation scheme and a critical question, and give an example of how they work together?

- Concept: Non-monotonic reasoning
  - Why needed here: This type of reasoning allows conclusions to be retracted when new information is introduced, which is essential for dynamic ethical reasoning where preferences and values may change based on new evidence.
  - Quick check question: How does non-monotonic reasoning differ from classical deductive reasoning, and why is it more suitable for ethical decision-making?

- Concept: Dialogue protocols and game theory
  - Why needed here: These provide the formal rules for how agents can exchange locutions (arguments, claims, questions) in inquiry dialogues, ensuring that the dialogue remains productive and leads to justified conclusions.
  - Quick check question: What are the key differences between persuasion dialogues and inquiry dialogues in terms of their goals and rules?

## Architecture Onboarding

- Component map: LLM Core -> Argumentation Scheme Classifier -> Critical Question Generator -> Dialogue Manager -> User Interaction -> Preference Aggregator -> Learning Module
- Critical path: LLM → Scheme Classifier → CQ Generator → Dialogue Manager → User Interaction → Preference Refinement → Learning Module
- Design tradeoffs:
  - Prompt complexity vs. LLM performance: More complex prompts may improve reasoning but reduce response quality
  - Real-time processing vs. thorough reasoning: Faster responses may sacrifice depth of ethical analysis
  - User guidance vs. user autonomy: More AI guidance may improve alignment but reduce user agency
- Failure signatures:
  - LLM fails to classify arguments correctly
  - Dialogue becomes circular or unproductive
  - User expresses frustration with AI's ethical reasoning
  - System cannot handle complex enthymemes or metalevel reasoning
- First 3 experiments:
  1. Test LLM's ability to classify simple arguments according to basic argumentation schemes (e.g., Argument from Expert Opinion)
  2. Implement a simple dialogue manager that enforces basic inquiry protocol rules and test with human users on simple ethical dilemmas
  3. Develop a preference aggregator that can evaluate arguments under two ethical frameworks (e.g., rights-based and consequentialist) and test its ability to present alternative viewpoints to users

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLM-based dialogue systems effectively instantiate argument schemes (AS) and critical questions (CQs) for ethical reasoning while maintaining coherent conversational histories across extended dialogues?
- Basis in paper: [explicit] The paper explicitly identifies this as a key challenge, noting that "LLMs need to be able to classify arguments using AS and to identify how one argument relates to another, through the use of their CQs" and that "LLMs struggle to differentiate between similar AS" while also needing to "retain conversational histories over prolonged interactions."
- Why unresolved: Current LLMs show limited ability to differentiate between similar argument schemes and lack robust mechanisms for maintaining long-term conversational context. The paper identifies this as an active research area but notes no definitive solutions have been demonstrated.
- What evidence would resolve it: Empirical demonstrations of LLM dialogue systems successfully instantiating appropriate AS/CQs in extended ethical reasoning dialogues while maintaining coherent conversation histories, with measurable improvements over baseline systems.

### Open Question 2
- Question: How can LLM-based dialogue systems develop reasoning capacities for complex dialectical moves beyond simple argument exchange, including Socratic questioning and metalevel reasoning about preferences and values?
- Basis in paper: [explicit] The paper explicitly requires that "Dialogues accommodate dialectical moves other than the moving of attacking/supporting arguments, and results in changes to the beliefs of interlocutors (RDia)" and discusses the need for "metalevel schemes and critical questions specialised for metalevel reasoning."
- Why unresolved: Current LLM architectures and training paradigms are not designed for the recursive, reflective reasoning required for metalevel argumentation and belief revision. The paper notes this requires development of "reasoning capacities enabling more complex dialectical moves" but doesn't provide concrete solutions.
- What evidence would resolve it: Demonstrated LLM systems capable of performing sophisticated dialectical moves like revealing contradictions in interlocutor beliefs, engaging in metalevel argumentation about preferences, and causing belief changes through dialogue, with measurable success rates.

### Open Question 3
- Question: What are the optimal prompt engineering techniques for enabling LLMs to engage in structured inquiry dialogues that balance exploration of ethical reasoning space with task completion?
- Basis in paper: [inferred] While the paper discusses prompt engineering as a promising technique ("Chain of thought (CoT) prompting...holds promise for addressing the challenges we have identified"), it doesn't specify optimal prompt patterns for structured ethical inquiry dialogues or how to balance exploration versus task completion.
- Why unresolved: The paper acknowledges prompt engineering's potential but notes it's an "active research area" without providing specific guidelines for structuring ethical inquiry dialogues or measuring the exploration-completion tradeoff.
- What evidence would resolve it: Systematic evaluation of different prompt engineering approaches for structured ethical dialogues, with comparative results showing which techniques best balance comprehensive ethical exploration with practical decision-making efficiency.

## Limitations

- The paper presents a theoretical framework without comprehensive empirical validation of LLM capabilities for argument classification and critical question generation
- Specialized ethical reasoning schemes and metalevel reasoning mechanisms are described conceptually but lack concrete implementation specifications
- The approach assumes users will engage productively with alternative ethical viewpoints, but doesn't address potential user resistance or comprehension challenges

## Confidence

**High Confidence:** The theoretical foundation connecting inquiry dialogues to value alignment through structured argumentation is well-established in philosophy and AI ethics literature. The identification of dialogue protocols as a mechanism for ensuring sound and complete joint reasoning is supported by existing argumentation theory.

**Medium Confidence:** The feasibility of enhancing LLMs to participate effectively in inquiry dialogues shows promise based on preliminary investigations, but requires significant engineering efforts and empirical validation. The approach of using prompt engineering to instantiate argumentation schemes is theoretically sound but practically challenging.

**Low Confidence:** The proposed specialized ethical reasoning schemes and their integration with LLMs remain largely conceptual. While the framework identifies the need for such schemes, their practical implementation and effectiveness in supporting value alignment have not been demonstrated.

## Next Checks

1. **Argument Classification Benchmark:** Develop a benchmark dataset of ethical arguments and test multiple LLMs' ability to classify them according to standard argumentation schemes with measurable accuracy rates, including both correct classifications and explanations of reasoning.

2. **Dialogue Coherence Test:** Implement a prototype dialogue manager and conduct controlled experiments measuring LLMs' ability to maintain coherent conversational histories across 10+ turn dialogues on ethical topics, tracking consistency and context retention.

3. **Value Alignment Evaluation:** Design a user study comparing decision outcomes from traditional persuasive AI interactions versus inquiry-based dialogues, measuring alignment with user values, user satisfaction, and perceived autonomy across both approaches.