---
ver: rpa2
title: On the Expressive Power of Spectral Invariant Graph Neural Networks
arxiv_id: '2406.04336'
source_url: https://arxiv.org/abs/2406.04336
tags:
- graph
- spectral
- color
- expressive
- power
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive theoretical analysis of the
  expressive power of spectral invariant Graph Neural Networks (GNNs). The authors
  introduce a unified framework, Eigenspace Projection GNN (EPNN), which incorporates
  spectral information through eigenspace projection matrices.
---

# On the Expressive Power of Spectral Invariant Graph Neural Networks

## Quick Facts
- arXiv ID: 2406.04336
- Source URL: https://arxiv.org/abs/2406.04336
- Authors: Bohang Zhang; Lingxiao Zhao; Haggai Maron
- Reference count: 40
- Key outcome: EPNN framework unifies spectral invariant GNNs and proves they are strictly less expressive than 3-WL

## Executive Summary
This paper provides a comprehensive theoretical analysis of spectral invariant Graph Neural Networks (GNNs), introducing the Eigenspace Projection GNN (EPNN) framework that unifies various spectral invariant architectures through eigenspace projection matrices. The authors establish that EPNN is strictly less expressive than 3-WL and subsumes prior architectures like BasisNet and SPE. Through rigorous analysis, they reveal fundamental limitations of distance-based spectral GNNs and highlight the crucial role of message-passing in enhancing expressive power. The work also explores the potential of higher-order spectral features for boosting expressiveness, significantly advancing our understanding of spectral invariant GNNs.

## Method Summary
The paper introduces EPNN as a unified framework that encodes spectral information through eigenspace projection matrices, which are used as edge features in a standard message-passing GNN on a fully connected graph. The authors establish expressiveness bounds by connecting EPNN to Subgraph GNNs (PSWL) and comparing against WL hierarchy benchmarks. Theoretical analysis focuses on color refinement algorithms to determine what graph structures can be distinguished by different spectral invariant architectures. The framework incorporates various graph matrices (A, L, L̂) and examines their impact on expressiveness.

## Key Results
- EPNN unifies all spectral invariant architectures through eigenspace projection matrices
- All spectral invariant architectures are strictly less expressive than 3-WL
- Message-passing aggregation is crucial for enhancing spectral feature expressiveness
- EPNN with normalized Laplacian (L̂) can encode all distances studied, while other matrices have limitations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EPNN unifies all spectral invariant architectures through eigenspace projection matrices
- Mechanism: EPNN encodes spectral information as edge features via projection matrices, then uses standard message-passing on a fully connected graph
- Core assumption: Projection matrices are invariant to eigenvector ambiguity and can be used as reliable edge features
- Evidence anchors:
  - [abstract] "EPNN is very simple: it encodes all spectral information for a node pair (u, v) as a set containing the values of all projection matrices on that node pair"
  - [section 4] "P_M G(u,v) essentially encodes the relation between vertices u and v in graph G and can thus be treated as a form of 'edge feature'"
  - [corpus] Weak evidence - only 1 neighbor paper mentions spectral invariants

### Mechanism 2
- Claim: Message-passing aggregation is crucial for enhancing spectral feature expressiveness
- Mechanism: Iterative refinement enriches node representations beyond what raw spectral features can provide
- Core assumption: The aggregation process can extract sufficient information from projection matrices to determine various distances
- Evidence anchors:
  - [abstract] "Our analysis underscores the crucial role of message-passing in enhancing the expressive power of spectral features"
  - [section 5] "The refinement continuously enriches the information embedded in node colors χ(l)_G(v), so that the tuple (χ(l)_G(u), χ(l)_G(v), P̂_L_G(u,v)) eventually encompasses sufficient information to determine any distance dG(u,v)"
  - [corpus] Weak evidence - no neighbor papers directly address message-passing enhancement

### Mechanism 3
- Claim: EPNN is strictly less expressive than 3-WL due to fundamental limitations
- Mechanism: The connection between EPNN and Subgraph GNNs (PSWL) reveals an expressiveness ceiling
- Core assumption: The color refinement in EPNN cannot capture all graph structures that 3-WL can
- Evidence anchors:
  - [abstract] "we prove that EPNN itself is bounded by a recently proposed class of Subgraph GNNs, implying that all these spectral invariant architectures are strictly less expressive than 3-WL"
  - [section 4.1] "combined with the result that PSWL is strictly bounded by 3-WL (Zhang et al., 2023a; 2024), we obtain the concluding corollary: For any graph matrix M ∈ {A, L, L̂}, the expressive power of EPWL is strictly bounded by 3-WL"
  - [corpus] Weak evidence - only 1 neighbor paper mentions WL tests

## Foundational Learning

- Concept: Graph spectra and projection matrices
  - Why needed here: Understanding how eigenvalues and eigenvectors relate to graph structure is fundamental to EPNN
  - Quick check question: What is the relationship between the projection matrix Pi and any orthogonal basis of unit eigenvectors {zi,1, ..., zi,Ji}?

- Concept: Message-passing neural networks
  - Why needed here: EPNN uses standard message-passing framework with spectral features as edge attributes
  - Quick check question: How does the aggregation function g(l+1) in Equation (1) differ from standard GNN message-passing?

- Concept: Weisfeiler-Lehman hierarchy
  - Why needed here: The paper uses WL tests as benchmarks for expressiveness comparison
  - Quick check question: Why is 3-WL considered more expressive than 1-WL for graph isomorphism testing?

## Architecture Onboarding

- Component map: Graph -> Spectrum/Eigenvectors -> Projection Matrices -> Edge Features -> Message-passing GNN -> Graph Representation

- Critical path:
  1. Compute graph spectrum and projection matrices
  2. Encode projection matrices as edge features in fully connected graph
  3. Apply message-passing with spectral edge features
  4. Perform global pooling for graph-level representation

- Design tradeoffs:
  - Memory vs. Expressiveness: Storing all projection matrices increases memory but provides more information
  - Message-passing vs. Raw Features: Aggregation enhances expressiveness but adds computational complexity
  - Graph Matrix Choice: Different matrices (A, L, L̂) capture different spectral properties

- Failure signatures:
  - Poor performance on graphs with high eigenvalue multiplicity
  - Inability to distinguish graphs that differ only in high-order structures
  - Computational bottlenecks when graph size increases

- First 3 experiments:
  1. Compare EPNN with BasisNet on synthetic graphs with known spectral properties
  2. Test EPNN with different graph matrices (A, L, L̂) on benchmark datasets
  3. Evaluate EPNN's ability to encode various distance metrics on graph isomorphism tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can using normalized Laplacian matrix (ˆL) instead of adjacency matrix (A) or standard Laplacian (L) provide strictly more expressive power for spectral invariant GNNs?
- Basis in paper: [inferred] The paper mentions that EPNN with ˆL can encode all distances studied, but it's unclear if other graph matrices can do the same. It also suggests that using ˆL might be more beneficial than other matrices.
- Why unresolved: The paper provides evidence that EPNN with ˆL can encode all distances, but it does not provide a formal proof or comparison for other graph matrices.
- What evidence would resolve it: A formal proof showing that EPNN with ˆL is strictly more expressive than EPNN with A or L, or counterexample graphs that can be distinguished by EPNN with ˆL but not with A or L.

### Open Question 2
- Question: What is the expressive power of higher-order spectral features, such as those obtained using token graphs?
- Basis in paper: [explicit] The paper discusses the potential of higher-order spectral features and mentions that using higher-order projection tensors could boost the expressive power of higher-order GNNs. It also references prior work on the spectra of symmetric powers of graphs.
- Why unresolved: The paper does not provide a tight lower/upper bound for the expressive power of higher-order spectral features in relation to higher-order WL tests. It leaves the architectural design and expressiveness analysis as an open direction for future study.
- What evidence would resolve it: A formal proof establishing the expressive power of higher-order spectral features in relation to higher-order WL tests, or a counterexample graph that can be distinguished by higher-order spectral features but not by existing spectral invariant GNNs.

### Open Question 3
- Question: Can the expressiveness of spectral invariant GNNs be further enhanced by allowing interactions between eigenspaces?
- Basis in paper: [explicit] The paper shows that Siamese IGN, which processes each eigenspace independently, is strictly less expressive than Spectral IGN, which allows interactions between eigenspaces. It also demonstrates that Siamese IGN cannot fully encode any graph distance, even the basic shortest path distance.
- Why unresolved: The paper provides theoretical evidence for the importance of interactions between eigenspaces, but it does not explore all possible ways to enhance expressiveness through such interactions or provide a comprehensive analysis of their impact.
- What evidence would resolve it: A formal proof showing that allowing interactions between eigenspaces can significantly enhance the expressive power of spectral invariant GNNs, or a counterexample graph that can be distinguished by a spectral invariant GNN with interactions but not by one without interactions.

## Limitations

- Theoretical bounds rely heavily on Subgraph GNN framework which may not capture all possible GNN architectures
- Analysis assumes ideal conditions for eigenvalue decomposition and doesn't account for numerical precision issues
- Claims about message-passing importance could benefit from more empirical validation on real-world datasets

## Confidence

- **High**: EPNN framework correctly unifies spectral invariant architectures
- **Medium**: Expressiveness comparison between EPNN and 3-WL is valid
- **Medium**: Message-passing enhances spectral feature expressiveness

## Next Checks

1. Implement EPNN with different graph matrices (A, L, L̂) and test on benchmark datasets to empirically verify expressiveness claims
2. Conduct ablation studies removing message-passing to quantify its contribution to expressiveness
3. Test EPNN on graphs with high eigenvalue multiplicity to identify practical limitations of the framework