---
ver: rpa2
title: Learning Cognitive Maps from Transformer Representations for Efficient Planning
  in Partially Observed Environments
arxiv_id: '2401.05946'
source_url: https://arxiv.org/abs/2401.05946
tags: []
core_contribution: This paper addresses the challenge of planning and navigation in
  partially observed environments where agents receive perceptually aliased observations.
  The authors propose TDB (Transformer with Discrete Bottlenecks), which adds discrete
  bottlenecks to compress transformer outputs into a finite number of latent codes.
---

# Learning Cognitive Maps from Transformer Representations for Efficient Planning in Partially Observed Environments

## Quick Facts
- arXiv ID: 2401.05946
- Source URL: https://arxiv.org/abs/2401.05946
- Reference count: 40
- This paper addresses planning and navigation in partially observed environments with perceptually aliased observations.

## Executive Summary
This paper tackles the challenge of planning and navigation in partially observed environments where agents receive perceptually aliased observations. The authors propose TDB (Transformer with Discrete Bottlenecks), which adds discrete bottlenecks to compress transformer outputs into a finite number of latent codes. After training TDB to predict future observations, interpretable cognitive maps are extracted from the bottleneck indices and paired with external solvers for path planning. Experiments demonstrate that TDB retains near-perfect predictive performance while solving shortest path problems exponentially faster than vanilla transformers or LSTMs.

## Method Summary
The method introduces Transformer with Discrete Bottlenecks (TDB) by adding discrete bottlenecks to compress transformer outputs into latent codes via vector quantization. The model is trained to predict future observations, and after training, cognitive maps are extracted from bottleneck indices by tracking empirical transitions between these indices given actions. These maps are then paired with external solvers to solve constrained path planning problems. The approach uses augmented training objectives (multi-step prediction or next encoding prediction) to encourage disambiguation of aliased observations.

## Key Results
- TDB achieves test accuracies above 99% while solving over 99% of path planning problems optimally
- TDB solves shortest path problems exponentially faster than vanilla transformers or LSTMs
- The learned cognitive maps are nearly isomorphic to ground truth maps, with normalized graph edit distances below 0.1

## Why This Works (Mechanism)

### Mechanism 1: Discrete Bottlenecks for Disambiguation
Discrete bottlenecks enable the transformer to disambiguate aliased observations by compressing history into a finite set of latent codes that encode spatial positions. The vector quantizer maps each transformer output to the nearest latent code, forcing the model to represent distinct spatial positions with distinct codes even when observations are perceptually identical. This mechanism relies on augmented training objectives to encourage disambiguation of positions with identical neighbors.

### Mechanism 2: Cognitive Map Extraction
The learned cognitive map from bottleneck indices models the environment's dynamics by tracking empirical transitions between bottleneck indices given actions. A count tensor records these transitions, and thresholding builds a graph where nodes represent spatial positions and edges represent actions. This graph can be queried by external solvers for path planning, assuming the empirical transitions accurately reflect ground truth dynamics.

### Mechanism 3: Multiple Discrete Bottlenecks for Training Acceleration
Having multiple discrete bottlenecks accelerates training by parallelizing the learning process and providing redundant representations that capture different aspects of the data. This leads to faster convergence and better generalization, though the bottlenecks may learn highly redundant representations rather than truly disentangled factors.

## Foundational Learning

- **Partially Observed Environments (POEs)**: Understanding POEs is crucial because the problem addresses planning in environments where observations are aliased, making it impossible to determine spatial position from a single observation. Quick check: What is perceptual aliasing and why does it make path planning hard?

- **Vector Quantization**: Vector quantization is the core mechanism that compresses transformer outputs into discrete latent codes, enabling the extraction of interpretable cognitive maps. Quick check: How does vector quantization work and why is it used in this model?

- **Graph Edit Distance**: Graph edit distance is used to evaluate the similarity between the learned cognitive map and the ground truth map, providing a quantitative measure of the model's performance. Quick check: What is graph edit distance and how is it used to evaluate the learned cognitive map?

## Architecture Onboarding

- **Component map**: Observations and actions -> Transformer -> Discrete bottlenecks -> Cognitive map -> External solver -> Path planning
- **Critical path**: The transformer processes observations and actions, discrete bottlenecks compress outputs via vector quantization, cognitive maps are extracted from bottleneck indices, and external solvers use these maps for path planning
- **Design tradeoffs**: Single vs. multiple discrete bottlenecks (speed vs. potential redundancy), single-step vs. multi-step prediction (simplicity vs. disambiguating power), using next encoding prediction vs. multi-step prediction (complexity vs. performance)
- **Failure signatures**: Poor test accuracy (model not learning), inability to solve path planning problems (model not disambiguating aliased observations), high graph edit distance (learned map not isomorphic to ground truth)
- **First 3 experiments**:
  1. Train TDB on a simple 2D aliased room and visualize the learned cognitive map
  2. Compare the performance of TDB with single vs. multiple discrete bottlenecks on a 3D simulated environment
  3. Test TDB's in-context learning capabilities on a new, unseen aliased room

## Open Questions the Paper Calls Out

- **High-dimensional continuous observations**: How can TDB be extended to handle high-dimensional continuous observations (e.g., images) rather than categorical inputs? The authors note this as a limitation and express interest in modifying the architecture to handle image data.

- **Disentangled representations**: Can TDB be modified to learn disentangled representations with multiple discrete bottlenecks? The authors show that multiple discrete bottlenecks do not learn disentangled representations and have high redundancy, which they identify as a limitation.

- **Rich environment integration**: How can TDB be integrated into a broader framework for building planning-compatible world models in rich, complex environments? The authors express interest in extending TDB to learn "disentangled latent dynamics in factored Markov decision processes" and using it to build world models for planning.

## Limitations

- The claims about disambiguating aliased observations rely heavily on augmented training objectives, and it's unclear whether the discrete bottleneck mechanism alone is sufficient
- The evaluation focuses on specific path planning tasks and doesn't address how cognitive maps would generalize to novel environments or handle dynamic conditions
- The comparison doesn't explore whether alternative architectures with discrete components might achieve similar or better performance, and computational overhead of maintaining and querying cognitive maps is not discussed in detail

## Confidence

- **High confidence**: Core empirical results showing TDB's superior path planning performance compared to vanilla transformers and LSTMs, supported by quantitative metrics (test accuracy >99%, ImpFallback >99%, RatioSP near 1.0)
- **Medium confidence**: Claim that discrete bottlenecks enable disambiguation of aliased observations, as the mechanism is theoretically plausible but not fully isolated in experiments
- **Medium confidence**: Assertion that multiple discrete bottlenecks accelerate training, as evidence is primarily from controlled experiments with specific hyperparameters and architectures

## Next Checks

1. **Ablation study on augmented objectives**: Train TDB variants with only single-step prediction (no augmentation) and compare their ability to disambiguate aliased observations and solve path planning problems to isolate whether the vector quantization mechanism alone is sufficient or if augmented objectives are essential.

2. **Generalization to novel environments**: Test whether cognitive maps learned on one environment can be transferred or adapted to solve path planning problems in structurally similar but unseen environments to validate practical utility beyond the training distribution.

3. **Alternative discrete bottleneck architectures**: Implement TDB using different discrete bottleneck mechanisms (such as Gumbel-Softmax or hierarchical VQ) to determine whether observed benefits are specific to the current vector quantization approach or are general properties of discrete bottlenecks in transformers.