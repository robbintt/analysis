---
ver: rpa2
title: A Comparative Study of Pre-training and Self-training
arxiv_id: '2409.02751'
source_url: https://arxiv.org/abs/2409.02751
tags:
- pre-training
- self-training
- data
- fine-tuning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive empirical study comparing pre-training
  and self-training approaches for semi-supervised learning. The authors systematically
  investigate all feasible training paradigms that combine pre-training, self-training,
  and fine-tuning using consistent foundational settings with language models.
---

# A Comparative Study of Pre-training and Self-training

## Quick Facts
- arXiv ID: 2409.02751
- Source URL: https://arxiv.org/abs/2409.02751
- Authors: Yiheng Wang; Jiayu Lin; Zuoquan Lin
- Reference count: 40
- Primary result: Pre-training and fine-tuning paradigm consistently outperforms self-training across sentiment analysis and NLI tasks

## Executive Summary
This paper provides a comprehensive empirical study comparing pre-training and self-training approaches for semi-supervised learning in NLP. The authors systematically investigate all feasible training paradigms that combine pre-training, self-training, and fine-tuning using consistent foundational settings with language models. Experiments conducted on six datasets reveal that the pre-training and fine-tuning paradigm yields the best overall performance. Additionally, the study finds that self-training offers no additional benefits when combined with semi-supervised pre-training.

## Method Summary
The study evaluates nine different training paradigms combining pre-training, self-training, and fine-tuning approaches using BERT-medium and BERT-base models. Experiments were conducted on six datasets (IMDB, SST, AG News, Elec, SNLI, MultiNLI) with varying labeled/unlabeled splits. The authors implemented self-paced pseudo-labeling for self-training and tested multiple data augmentation strategies (DA0-DA4) including natural noise, conditional BERT, and back-translation. Performance was measured using accuracy on sentiment analysis and natural language inference tasks.

## Key Results
- Pre-training and fine-tuning paradigm (PF) consistently outperforms all other approaches across all datasets
- Self-training provides no additional benefits when combined with semi-supervised pre-training
- Data augmentation effectiveness follows an inverted-U curve - moderate augmentation improves performance while excessive augmentation degrades it
- On imbalanced datasets, PF maintains consistent performance while other paradigms show significant drops

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training followed by fine-tuning consistently outperforms self-training across different data augmentation intensities
- Core assumption: The pre-trained model has learned transferable representations that can be effectively adapted through fine-tuning
- Evidence anchors: Pre-training and fine-tuning paradigm yields the best overall performance; PF demonstrates the best performance across all datasets

### Mechanism 2
- Claim: Self-training offers no additional benefits when combined with semi-supervised pre-training
- Core assumption: The labeled data provides sufficient information for the model to learn task-specific patterns
- Evidence anchors: Self-training offers no additional benefits when combined with semi-supervised pre-training; Fine-tuning has either resulted in negligible improvement or a slight decline in the performance of S, PS, and PFS

### Mechanism 3
- Claim: Data augmentation effectiveness follows a curve - initial improvements followed by diminishing returns or degradation with excessive augmentation
- Core assumption: There exists an optimal level of data augmentation that balances diversity and noise
- Evidence anchors: Accuracy initially rises and then declines as the extent of data augmentation grows; moderate data augmentation enhances performance, whereas excessive augmentation is ineffective

## Foundational Learning

- Concept: Semi-supervised learning
  - Why needed here: The paper compares two semi-supervised learning approaches
  - Quick check question: What are the two main sources of information used in semi-supervised learning?

- Concept: Pre-training and fine-tuning paradigm
  - Why needed here: This is identified as the best-performing approach in the study
  - Quick check question: In what order do the model processes data in the pre-training and fine-tuning paradigm?

- Concept: Self-training and pseudo-labeling
  - Why needed here: Self-training is one of the approaches being compared
  - Quick check question: What is the key challenge in self-training that the paper addresses through self-paced curriculum?

## Architecture Onboarding

- Component map: BERT model -> Linear classifier -> Self-training loop with confidence-based pseudo-label selection -> Data augmentation pipeline

- Critical path: 1) Load pre-trained BERT, 2) Fine-tune on labeled data, 3) Evaluate performance, 4) (Optional) Run self-training iterations

- Design tradeoffs: Using stronger pre-trained models improves performance but may widen the gap with self-training; more aggressive data augmentation can help self-training but may harm pre-training + fine-tuning

- Failure signatures: Self-training performance plateaus or degrades after initial iterations; Pre-training + fine-tuning shows minimal improvement with additional labeled data; Data augmentation causes performance degradation beyond a certain intensity

- First 3 experiments: 1) Implement and compare F vs P as baselines, 2) Implement and compare PF vs S on a small dataset, 3) Test the effect of different data augmentation intensities on PF and S paradigms

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does self-training provide any benefits when combined with pre-trained models in domains beyond NLP and computer vision?
- Basis in paper: The authors conclude that self-training offers no additional benefits when combined with semi-supervised pre-training across six NLP datasets, but note their study is limited to NLP tasks
- Why unresolved: The study only examined NLP tasks and did not explore other domains where self-training might be more effective
- What evidence would resolve it: Empirical studies comparing self-training with pre-training in other domains like speech recognition or bioinformatics

### Open Question 2
- Question: What is the optimal strategy for combining self-training and pre-training when dealing with highly imbalanced data?
- Basis in paper: The authors found that PF maintains consistent performance on imbalanced data while other paradigms show significant drops, but they don't explore hybrid strategies
- Why unresolved: The study identified the problem but didn't investigate whether specific modifications to self-training or pre-training could improve performance on imbalanced datasets
- What evidence would resolve it: Experiments testing modified self-training algorithms combined with pre-training on severely imbalanced datasets

### Open Question 3
- Question: How does the scale of pre-training (model size, data size) affect the relative performance of PF versus PFSF paradigms?
- Basis in paper: The authors used BERT-base and BERT-medium and found PFSF performed worse than PF, but didn't test with larger pre-trained models
- Why unresolved: The study used relatively small language models, while modern approaches typically use much larger pre-trained models
- What evidence would resolve it: Systematic experiments comparing PF and PFSF across multiple scales of pre-training

## Limitations

- Focus on BERT-based models and English-language NLP tasks may not generalize to other model architectures or languages
- Comparative analysis assumes access to reasonably large unlabeled datasets, which may not hold in all real-world scenarios
- Does not explore the computational costs of different paradigms, which could be significant in practical applications

## Confidence

- Pre-training + fine-tuning superiority: High
- Self-training provides no additional benefits: Medium
- Data augmentation effectiveness curve: Medium

## Next Checks

1. Test the same comparative framework with different pre-trained models (e.g., RoBERTa, DeBERTa) to assess architecture dependency
2. Evaluate the training paradigms on low-resource languages or domains with limited unlabeled data availability
3. Conduct a computational efficiency analysis comparing the training time and resource requirements of each paradigm across different dataset sizes