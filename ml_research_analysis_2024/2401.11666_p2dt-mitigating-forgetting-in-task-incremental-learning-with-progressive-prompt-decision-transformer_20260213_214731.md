---
ver: rpa2
title: 'P2DT: Mitigating Forgetting in task-incremental Learning with progressive
  prompt Decision Transformer'
arxiv_id: '2401.11666'
source_url: https://arxiv.org/abs/2401.11666
tags:
- learning
- task
- p2dt
- tasks
- decision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Catastrophic forgetting poses a significant challenge in continual
  learning, where intelligent agents experience performance degradation when adapting
  to new tasks. This paper introduces the Progressive Prompt Decision Transformer
  (P2DT), a novel approach that dynamically appends decision tokens during new task
  training to foster task-specific policies.
---

# P2DT: Mitigating Forgetting in task-incremental Learning with progressive prompt Decision Transformer

## Quick Facts
- arXiv ID: 2401.11666
- Source URL: https://arxiv.org/abs/2401.11666
- Reference count: 0
- Primary result: P2DT reduces catastrophic forgetting in continual learning by dynamically appending task-specific prompts and using FIM-based regularization

## Executive Summary
Catastrophic forgetting poses a significant challenge in continual learning, where intelligent agents experience performance degradation when adapting to new tasks. This paper introduces the Progressive Prompt Decision Transformer (P2DT), a novel approach that dynamically appends decision tokens during new task training to foster task-specific policies. The method leverages trajectories from all tasks and generates new task-specific tokens during training, thereby retaining knowledge from previous studies.

## Method Summary
P2DT extends the Decision Transformer framework by introducing General Attention Blocks (GAB) for task-invariant knowledge and Expert Attention Blocks (EAB) for task-specific learning. During training on each new task, the model dynamically adds a small set of learnable embeddings (prompts) that are processed only by the EABs. The model is trained using a modified loss function that includes a regularization term based on the Fisher Information Matrix (FIM) to penalize changes to parameters crucial for prior task performance. Trajectories from all tasks are used during training to support knowledge consolidation.

## Key Results
- P2DT effectively alleviates catastrophic forgetting in continuous control environments (HalfCheetah, Hopper, and Walker2D)
- The approach maintains high performance with minimal reduction across tasks compared to baseline methods
- P2DT scales well with increasing task environments and exhibits negligible overhead in parameter expansion

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task-specific prompts appended during training allow the model to learn new tasks without overwriting prior knowledge.
- Mechanism: The model dynamically adds a small set of learnable embeddings (prompts) for each new task. These are placed at the end of the input sequence and processed only by the Expert Attention Blocks (EABs), which are task-specific. The General Attention Blocks (GABs) continue to handle task-invariant knowledge, so new tasks do not interfere with existing task representations.
- Core assumption: Task prompts can be learned efficiently without significantly altering the general model parameters, and they are sufficient to capture task-specific dynamics.
- Evidence anchors:
  - [abstract] "dynamically appending decision tokens during new task training, thus fostering task-specific policies"
  - [section 2.2] "task prompts are dynamically added to the network and placed at the end of the input sequence for the transformer's expert block"
  - [corpus] Weak: no direct citations to this prompt-based mechanism; described only in this paper.
- Break condition: If the prompt embeddings grow too large or interact with GABs, the task separation breaks down and forgetting resumes.

### Mechanism 2
- Claim: Regularization via Fisher Information Matrix (FIM) penalizes changes to parameters important for prior tasks, reducing forgetting.
- Mechanism: After learning task i, the FIM measures each parameter's importance to that task. When training on a new task, the loss includes a term λ/2 Σ Mi,k(wk - w*i,k)², which penalizes deviations from the old parameter values weighted by their importance. This preserves performance on earlier tasks.
- Core assumption: FIM provides an accurate estimate of parameter importance and the regularization strength λ is tuned appropriately.
- Evidence anchors:
  - [section 2.3] "We compute the FIM for each model parameter to measure their importance for a given task"
  - [section 2.3] "The regularization term is directly linked to the square of the difference between each parameter's current value and its value when the previous task was learned"
  - [corpus] Weak: only mentioned in this paper; no external validation cited.
- Break condition: If λ is too small, regularization is ineffective; if too large, the model cannot adapt to new tasks.

### Mechanism 3
- Claim: Using trajectories from all tasks during training of new tasks helps retain old knowledge.
- Mechanism: The model is trained on data from previous tasks combined with new task data, so the decision transformer continues to see examples of earlier tasks. This replay-like effect supports consolidation of prior knowledge while learning new tasks.
- Core assumption: Storing and retraining on old trajectories is feasible and does not cause data imbalance or overfitting to old tasks.
- Evidence anchors:
  - [abstract] "P2DT leverages trajectories collected via traditional reinforcement learning from all tasks and generates new task-specific tokens during training"
  - [section 2.2] "Our method optimizes by utilizing trajectories collected from all tasks for improved policy learning"
  - [corpus] Weak: described as part of the method but no external empirical support cited.
- Break condition: If trajectory storage or replay is incomplete, forgetting can occur for underrepresented tasks.

## Foundational Learning

- Concept: Transformer-based decision modeling (Decision Transformer)
  - Why needed here: Provides the autoregressive sequence modeling framework that P2DT extends; without it, the token-based prompt mechanism would not apply.
  - Quick check question: How does the Decision Transformer represent trajectories, and what is the role of the return-to-go in its input?
- Concept: Catastrophic forgetting in continual learning
  - Why needed here: The central problem P2DT addresses; understanding how and why forgetting occurs is essential to designing mitigation strategies.
  - Quick check question: What is the difference between regularization-based and rehearsal-based continual learning approaches?
- Concept: Fisher Information Matrix (FIM) for importance weighting
  - Why needed here: FIM is used to weight the regularization term, making it crucial to understand its role in parameter importance estimation.
  - Quick check question: How is the Fisher Information Matrix estimated for a parameter in a neural network?

## Architecture Onboarding

- Component map: Trajectory → Embedding Layer → GAB → EAB with Task Prompt → Output Layer
- Critical path:
  1. Embed trajectory → pass through GAB → pass through EAB with task prompt → predict next token.
  2. Backpropagate loss through both GAB and EAB, update task prompt.
  3. Store FIM and parameter snapshot after each task for regularization.
- Design tradeoffs:
  - Adding EABs increases parameter count linearly with tasks, but prompts are small so overhead is negligible.
  - Using all trajectories for training new tasks increases memory usage but improves retention.
  - FIM-based regularization adds computation but stabilizes old task performance.
- Failure signatures:
  - Performance drop on earlier tasks after training new ones (forgetting).
  - Degraded performance on new tasks (over-regularization).
  - Slow convergence or instability in training (prompt/task block interference).
- First 3 experiments:
  1. Train on HalfCheetah, evaluate on HalfCheetah, then train on Hopper, evaluate both tasks; compare to baseline DT.
  2. Vary prompt length (5, 10, 20) and measure accuracy saturation and parameter overhead.
  3. Test FIM regularization strength λ on a toy continual task to see effect on forgetting vs plasticity.

## Open Questions the Paper Calls Out
- No open questions were explicitly called out in the paper.

## Limitations
- The evaluation is limited to three continuous control environments with similar state-action spaces, raising questions about generalization to more heterogeneous tasks.
- The paper lacks ablation studies to quantify the relative contribution of each component (prompts, FIM regularization, trajectory replay) to performance.
- Memory overhead of storing trajectories from all tasks is mentioned as "negligible" but not quantified, and computational cost of FIM computation is not discussed.

## Confidence
- **High confidence**: The architectural description of P2DT with GAB and EAB layers, and the general mechanism of appending task-specific prompts are clearly specified and internally consistent.
- **Medium confidence**: The empirical results showing reduced forgetting on the three tested environments are plausible given the methodology, but the lack of statistical significance testing and limited task diversity reduces confidence in broader applicability.
- **Low confidence**: The claim of "negligible overhead" in parameter expansion is not substantiated with quantitative analysis, and the effectiveness of FIM-based regularization strength tuning is not demonstrated through sensitivity analysis.

## Next Checks
1. **Ablation study validation**: Remove each component (task prompts, FIM regularization, trajectory replay) individually and evaluate the performance drop on all three tasks to quantify the contribution of each mechanism to forgetting mitigation.
2. **Task diversity stress test**: Evaluate P2DT on a benchmark with heterogeneous task types (e.g., combining vision-based and state-based control tasks, or tasks with different dimensionalities) to assess robustness beyond the current continuous control focus.
3. **Memory and computation overhead analysis**: Measure and report the exact memory footprint of storing all task trajectories, the computational cost of FIM calculation per parameter, and the wall-clock training time increase compared to baseline DT without continual learning considerations.