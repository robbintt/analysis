---
ver: rpa2
title: 'FAQ-Gen: An automated system to generate domain-specific FAQs to aid content
  comprehension'
arxiv_id: '2402.05812'
source_url: https://arxiv.org/abs/2402.05812
tags:
- question
- system
- generation
- answer
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an end-to-end automated system for generating
  domain-specific Frequently Asked Questions (FAQs) to aid content comprehension.
  The system leverages text-to-text transformation models, specifically T5, to perform
  domain identification, question generation, answer keyword extraction, answer completion,
  and FAQ ranking.
---

# FAQ-Gen: An automated system to generate domain-specific FAQs to aid content comprehension

## Quick Facts
- arXiv ID: 2402.05812
- Source URL: https://arxiv.org/abs/2402.05812
- Authors: Sahil Kale; Gautam Khaire; Jay Patankar
- Reference count: 11
- Primary result: Human evaluation shows high scores for generated FAQs across 17 domains (syntactic quality averaging 8.1/10 for questions and 8.0/10 for answers)

## Executive Summary
This paper presents FAQ-Gen, an end-to-end automated system for generating domain-specific Frequently Asked Questions from PDF documents. The system employs a modular pipeline using T5 text-to-text transformation models for domain identification, question generation, answer keyword extraction, answer completion, and FAQ ranking. By creating custom domain-specific datasets and using specialized models for each subtask, the system produces readable, contextually relevant, and comprehensive FAQs. Human evaluation across 17 domains demonstrates high quality in syntactic well-formedness, relevance, and comprehensiveness.

## Method Summary
FAQ-Gen processes PDF documents through a modular pipeline: text extraction and chunking, domain identification, question generation using domain-specific T5 models, answer keyword extraction, answer completion and elaboration, and FAQ ranking. The system extends SQuAD with domain-relevant content to create custom datasets for fine-tuning 17 separate question generation models. A two-metric ranking algorithm combines semantic similarity and keyword matching scores to prioritize Q&A pairs. The entire system is implemented using Python and HuggingFace transformers, with human evaluation assessing generated FAQ quality across syntactic, relevance, and comprehensiveness dimensions.

## Key Results
- Human evaluation scores averaged 8.1/10 for question syntactic quality and 8.0/10 for answer quality
- System successfully generates FAQs across 17 distinct domains with high relevance scores
- Modular architecture enables flexible domain-specific tuning while maintaining overall coherence
- Custom domain-specific datasets improve question generation quality compared to general models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modular architecture enables specialized model tuning for each FAQ generation subtask, improving overall accuracy.
- Mechanism: Each subtask (domain identification, question generation, answer keyword extraction, answer completion, and ranking) is handled by a dedicated model fine-tuned for that specific transformation. This specialization allows each model to focus on the nuances of its task, such as domain-specific vocabulary for question generation or grammatical correctness for answer completion.
- Core assumption: Fine-tuning a single model for multiple diverse subtasks would result in poorer performance than having specialized models for each task.
- Evidence anchors:
  - [abstract] "The system leverages text-to-text transformation models, specifically T5, to perform domain identification, question generation, answer keyword extraction, answer completion, and FAQ ranking."
  - [section] "From an accuracy standpoint, each step tackles a specific subtask of the problem, making it significantly easier to fine-tune high-performing models specialised in a particular text-to-text transformation task."
- Break condition: If the overhead of managing multiple models and datasets outweighs the performance gains, or if the tasks are not sufficiently distinct to benefit from specialization.

### Mechanism 2
- Claim: Domain-specific question generation models improve the relevance and quality of generated questions.
- Mechanism: The system uses 17 separate T5 models, each fine-tuned on a dataset of questions and contexts from a specific domain (e.g., Science and Technology, Arts and Culture). This allows each model to learn the unique terminology, phrasing, and knowledge structures of its domain, resulting in more relevant and contextually appropriate questions.
- Core assumption: Questions generated by a general model would lack the depth and specificity of those generated by a domain-expert model.
- Evidence anchors:
  - [abstract] "To improve domain-specificity, the authors create custom datasets by extending SQuAD with domain-relevant content."
  - [section] "Using domain-specific datasets described in Section 3.2.1, we ensure that each question-generation model has an in-depth understanding of the nuanced language, terminology, and context inherent to that particular domain."
- Break condition: If the domain-specific datasets are too small or of poor quality, leading to overfitting or generation of irrelevant questions.

### Mechanism 3
- Claim: The two-metric ranking algorithm (semantic similarity + keyword matching) produces more relevant and comprehensive FAQs.
- Mechanism: The ranking algorithm combines a semantic similarity score (cosine similarity between context and Q&A pair) with a keyword matching score (exact keyword matches with a size penalty). This dual approach balances the importance of semantic relevance with the need for factual accuracy and completeness.
- Core assumption: A single metric (e.g., only semantic similarity) would not adequately capture both the relevance and the factual coverage of a Q&A pair.
- Evidence anchors:
  - [section] "The ranking algorithm calculates a score based on two metrics that combine structural and semantic importance for each question-answer pair as follows: Semantic Similarity Score and Keyword Matching Score."
- Break condition: If the semantic similarity measure fails to capture domain-specific nuances, or if the keyword matching score becomes too dominant, leading to overly simplistic Q&A pairs.

## Foundational Learning

- Concept: Transfer learning and fine-tuning of pre-trained models.
  - Why needed here: The system relies on T5, a pre-trained text-to-text transformer, which is fine-tuned for each specific subtask. This approach leverages the general language understanding of T5 while adapting it to the specific needs of FAQ generation.
  - Quick check question: What are the key differences between training a model from scratch and fine-tuning a pre-trained model for a specific task?

- Concept: Text chunking and context management.
  - Why needed here: The input document is divided into chunks of approximately 250 words to ensure manageable context sizes for the models and to maintain the coherence of the generated FAQs. The system uses structural partitioning to create chunks of roughly equal size.
  - Quick check question: How does the choice of chunk size and partitioning strategy (semantic vs. structural) impact the quality and relevance of the generated FAQs?

- Concept: Natural Language Generation (NLG) and evaluation.
  - Why needed here: The system generates questions and answers, which are inherently subjective and difficult to evaluate automatically. The paper uses human evaluation to assess the quality of the generated FAQs based on criteria like syntactic well-formedness, relevance, and comprehensiveness.
  - Quick check question: What are the challenges in evaluating the quality of generated text, and how can human evaluation be designed to be fair and consistent?

## Architecture Onboarding

- Component map:
  - Text Extraction and Chunking -> Domain Identification -> Question Generation -> Answer Keyword Extraction -> Answer Completion -> FAQ Compilation and Ranking -> Output

- Critical path: Text Extraction -> Domain Identification -> Question Generation -> Answer Keyword Extraction -> Answer Completion -> FAQ Ranking -> Output

- Design tradeoffs:
  - Model specialization vs. model simplicity: Using 17 domain-specific question generation models increases complexity but improves quality.
  - Chunk size vs. context coherence: Smaller chunks are easier to process but may lose important context.
  - Ranking metric complexity vs. interpretability: The two-metric ranking algorithm is more complex but provides a better balance of relevance and accuracy.

- Failure signatures:
  - Poor domain identification leads to irrelevant questions and answers.
  - Insufficient training data for a domain results in low-quality question generation.
  - Overly aggressive chunking breaks up important context, leading to incomplete or irrelevant Q&A pairs.
  - Ranking algorithm overemphasizes one metric, leading to either overly complex or overly simplistic FAQs.

- First 3 experiments:
  1. Test the domain identification model on a sample of contexts from each domain to ensure accurate classification.
  2. Generate questions for a single context using the domain-specific question generation model and manually evaluate their relevance and quality.
  3. Test the entire pipeline on a small document (e.g., 500 words) from a single domain, evaluating the final FAQ set for syntactic quality, relevance, and comprehensiveness.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- Human evaluation relies on subjective scoring (0-10 scale) that may not fully capture FAQ quality across domains.
- Ranking algorithm's dual-metric design (semantic similarity + keyword matching) is not validated against alternative ranking methods.
- Domain identification accuracy directly impacts downstream quality but is not independently reported.
- System scalability to longer documents or more domains remains untested.

## Confidence
- Overall approach: Medium
- Ranking algorithm superiority: Low

## Next Checks
1. Report domain classification accuracy on a held-out test set to validate the domain identification component.
2. Conduct ablation tests comparing single vs. dual-metric ranking to assess the effectiveness of the ranking algorithm.
3. Validate model robustness by generating FAQs from documents in domains not seen during fine-tuning.