---
ver: rpa2
title: Zero-Shot Action Recognition in Surveillance Videos
arxiv_id: '2410.21113'
source_url: https://arxiv.org/abs/2410.21113
tags:
- sampling
- surveillance
- video
- self-res
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of action recognition in surveillance
  videos using zero-shot learning. The authors propose leveraging Large Vision-Language
  Models (LVLMs) and an improved token-level sampling method, Self-Reflective Sampling
  (Self-ReS), to address the limitations of traditional computer vision models in
  surveillance settings.
---

# Zero-Shot Action Recognition in Surveillance Videos

## Quick Facts
- arXiv ID: 2410.21113
- Source URL: https://arxiv.org/abs/2410.21113
- Authors: Joao Pereira; Vasco Lopes; David Semedo; Joao Neves
- Reference count: 10
- Key outcome: VideoLLaMA2 achieves 20% improvement in zero-shot performance over baseline; Self-ReS increases zero-shot action recognition to 44.6%

## Executive Summary
This paper addresses the challenge of zero-shot action recognition in surveillance videos by leveraging Large Vision-Language Models (LVLMs) and an improved token-level sampling method called Self-Reflective Sampling (Self-ReS). The authors demonstrate that VideoLLaMA2, a state-of-the-art LVLM, significantly outperforms traditional vision-language models like CLIP on the UCF-Crime dataset. Their approach addresses the limitations of linear frame sampling by focusing on fine-grained spatiotemporal regions through iterative token pruning, achieving a 44.6% zero-shot accuracy.

## Method Summary
The proposed method uses VideoLLaMA2, a Large Vision-Language Model that leverages pretrained vision-language encoders like CLIP to extract language-aligned visual features. The model employs two sampling approaches: Linear Frame Sampling (selecting 16 frames) and Self-ReS, which iteratively prunes less relevant visual tokens based on their similarity to the final input token. This token-level selection allows the model to capture fine-grained spatial and temporal details in surveillance videos where events often occupy small spatiotemporal regions and exhibit non-linear patterns.

## Key Results
- VideoLLaMA2 achieves a 20% improvement in zero-shot performance over CLIP baseline
- Self-ReS increases zero-shot action recognition performance to 44.6% on UCF-Crime
- Performance is still limited by low video quality in UCF-Crime dataset, with LVLMs benefiting from higher resolution input

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LVLMs improve zero-shot action recognition by leveraging strong visual-language alignment from pretraining
- Mechanism: LVLMs use pretrained vision-language encoders like CLIP to extract language-aligned visual features, which are then projected into the LLM's embedding space
- Core assumption: The pretrained vision-language encoder has learned transferable representations that generalize across domains
- Evidence anchors: [abstract] "Large Vision-Language Models (LVLMs), known for their strong zero and few-shot generalization"; [section] "Large Vision and Language Models... use pretrained vision-language encoder such as CLIP"
- Break condition: If visual-language alignment learned from pretraining does not transfer to low-quality, diverse surveillance domain

### Mechanism 2
- Claim: Self-ReS improves performance by focusing on relevant spatiotemporal regions through iterative token pruning
- Mechanism: Self-ReS increases frame sampling density, splits videos into segments, and uses self-reflective LLM layers to iteratively prune less relevant visual tokens
- Core assumption: The LLM can effectively identify and attend to the most relevant visual tokens for answering the query
- Evidence anchors: [abstract] "Self-ReS additionally increases zero-shot action recognition performance to 44.6%"; [section] "Self-ReS iteratively updates the set of visual tokens"
- Break condition: If the LLM cannot reliably identify relevant tokens, leading to pruning of important information

### Mechanism 3
- Claim: Improved sampling granularity is critical for surveillance videos with non-linear events and small spatiotemporal regions
- Mechanism: Token-level sampling allows capture of fine-grained spatial and temporal details missed by linear frame sampling
- Core assumption: Surveillance events are non-linear and occupy small spatiotemporal regions requiring fine-grained sampling
- Evidence anchors: [abstract] "we propose leveraging... an improved token-level sampling method, Self-ReS"; [section] "token-level selection is a superior approach for surveillance use cases"
- Break condition: If surveillance videos contain mostly linear, temporally spread-out events that don't benefit from fine-grained sampling

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: The paper explicitly aims for zero-shot action recognition without finetuning on surveillance data
  - Quick check question: Can the model recognize actions it hasn't been explicitly trained on for surveillance videos?

- Concept: Vision-language alignment
  - Why needed here: LVLMs rely on pretrained vision-language models like CLIP to align visual and textual features
  - Quick check question: How does the model convert visual features into a format the LLM can process?

- Concept: Token pruning and attention mechanisms
  - Why needed here: Self-ReS uses iterative token pruning based on attention scores to focus on relevant information
  - Quick check question: What criteria does the model use to decide which tokens to keep during self-reflective sampling?

## Architecture Onboarding

- Component map: Video frames -> Vision encoder (CLIP-based) -> Linear projection -> LLM with self-reflective layers -> Answer generation

- Critical path: Video -> Frame sampling -> Vision encoder -> Token projection -> Self-ReS processing -> LLM reasoning -> Answer

- Design tradeoffs:
  - Frame sampling rate vs. computational cost: Higher sampling rates capture more detail but increase processing time
  - Number of self-reflective layers vs. performance: More layers allow finer pruning but may over-prune important information
  - Model size vs. zero-shot capability: Larger models have better generalization but are more resource-intensive

- Failure signatures:
  - Poor performance on similar action categories suggests language model confusion
  - No improvement from Self-ReS indicates token selection isn't working effectively
  - Large performance gap vs. finetuned models suggests domain mismatch or low-quality input

- First 3 experiments:
  1. Compare linear frame sampling (16 frames) vs. baseline methods (CLIP) on UCF-Crime to establish LVLM advantage
  2. Test Self-ReS with different pruning factors (e.g., 3, 5, 8) to find optimal balance between detail and efficiency
  3. Evaluate performance on high vs. low quality frames to assess model sensitivity to input resolution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do Large Vision-Language Models perform on surveillance datasets with higher video quality than UCF-Crime?
- Basis in paper: [explicit] The paper mentions that LVLM performance is "still capped by the fact that image quality in UCF-Crime is very low, and LVLMs benefit from high resolution input"
- Why unresolved: The experiments were only conducted on UCF-Crime, which has low video quality
- What evidence would resolve it: Conducting experiments on surveillance datasets with higher resolution video quality and comparing performance to the UCF-Crime results

### Open Question 2
- Question: Can in-context learning improve LVLM performance in distinguishing between similar surveillance action categories?
- Basis in paper: [explicit] The authors mention that "more mature solutions will be able to leverage in context learning and provide the LLM with examples that help it distinguish between the different categories"
- Why unresolved: The paper only evaluates zero-shot performance without providing any in-context examples
- What evidence would resolve it: Experiments comparing zero-shot LVLM performance versus performance when provided with a small set of in-context examples

### Open Question 3
- Question: How does Self-ReS performance vary with different video lengths and temporal patterns in surveillance scenarios?
- Basis in paper: [inferred] The paper argues that "token-level selection is a superior approach for surveillance use cases, where capturing relevant events requires... dealing with nonlinear events over long videos" but only tests on UCF-Crime videos
- Why unresolved: The experiments were limited to UCF-Crime videos with fixed temporal patterns
- What evidence would resolve it: Testing Self-ReS on surveillance datasets with varying video lengths and different temporal event distributions

## Limitations

- Performance is substantially below supervised models, suggesting domain mismatch or inherent limitations in the LVLM approach
- The paper does not address potential performance degradation on low-resolution or nighttime surveillance footage
- Claims about Self-ReS effectiveness lack strong empirical validation through ablation studies or comparisons with alternative sampling methods

## Confidence

- **High Confidence**: The general framework of using LVLMs for zero-shot action recognition is technically sound and well-established
- **Medium Confidence**: The claim that VideoLLaMA2 achieves 20% improvement over CLIP baseline is supported by experimental results
- **Low Confidence**: The Self-ReS method's effectiveness and the claim that token-level sampling is superior for surveillance videos are based on limited experimental evidence

## Next Checks

1. **Ablation Study on Self-ReS Parameters**: Systematically vary pruning factors (e.g., test factors of 2, 3, 5, and 8) and number of self-reflective layers to determine optimal configurations and assess whether claimed improvements are robust to these hyperparameters.

2. **Cross-Domain Generalization Test**: Evaluate the model on multiple surveillance datasets with varying video quality (e.g., low-light conditions, different camera resolutions) to verify whether the LVLM approach generalizes beyond UCF-Crime and identify performance degradation patterns.

3. **Fine-grained Error Analysis**: Conduct detailed error analysis focusing on similar action categories (e.g., Burglary vs. Stealing vs. Robbery) to determine whether confusion stems from language model limitations or insufficient visual discrimination, and test whether prompting strategies can mitigate these confusions.