---
ver: rpa2
title: Interpreting Equivariant Representations
arxiv_id: '2401.12588'
source_url: https://arxiv.org/abs/2401.12588
tags:
- invariant
- equivariant
- latent
- representations
- representation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of correctly interpreting and
  analyzing latent representations from equivariant neural networks. The key issue
  is that standard latent space analysis methods applied to equivariant representations
  can lead to incorrect conclusions and decreased performance, as each input has multiple
  equivalent representations in the latent space.
---

# Interpreting Equivariant Representations

## Quick Facts
- arXiv ID: 2401.12588
- Source URL: https://arxiv.org/abs/2401.12588
- Authors: Andreas Abildtrup Hansen; Anna Calissano; Aasa Feragen
- Reference count: 14
- Primary result: Standard latent space analysis methods applied to equivariant representations can lead to incorrect conclusions and decreased performance.

## Executive Summary
This paper addresses the problem of correctly interpreting and analyzing latent representations from equivariant neural networks. The key issue is that standard latent space analysis methods applied to equivariant representations can lead to incorrect conclusions and decreased performance, as each input has multiple equivalent representations in the latent space. The authors propose using invariant projections of equivariant latent representations to obtain unambiguous invariant representations, demonstrating their effectiveness in two common examples: a permutation equivariant VAE for molecule graph generation and a rotation-equivariant representation for image classification.

## Method Summary
The method involves training equivariant models to obtain latent representations, then applying invariant projections to these latents to create well-defined, interpretable representations. For permutation equivariance, sorting coordinates provides an isometric cross-section, while for rotation equivariance, random invariant projections can be used. The approach is post-hoc, allowing models to be trained for performance first, then extracting interpretable representations. The authors demonstrate this on a permutation equivariant VAE for molecular graphs and a rotation-equivariant image classifier, showing improved performance in downstream tasks like k-NN classification and regression.

## Key Results
- Invariant projections can be designed to incur no loss of information for permutation equivariant representations
- Random invariant projections provide informative representations for rotation equivariant models
- Analysis of invariant latent representations proves superior to equivariant counterparts in visualization, interpolation, and downstream task performance
- Equivariant latent representations show similar ambiguity behavior in standard neural networks trained with augmentation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Equivariant latent representations are ambiguous because multiple group transformations can produce different but equivalent latent vectors for the same input.
- Mechanism: In an equivariant model, if input x transforms under group element g to give g·x, the latent representation transforms as g·f(x). This means each data point has multiple equivalent latent representations {g·f(x) | g ∈ G}, making distances and neighborhoods ill-defined.
- Core assumption: The latent space Z has a group action compatible with the input space X, and the equivariant encoder f preserves this structure.
- Evidence anchors:
  - [abstract]: "each input has multiple equivalent representations in the latent space"
  - [section 2]: "any latent code z = f(x) ∈ Z will be equivalent to the – often different – representation g·z = g·f(x) = f(g·x) of the transformed, but equivalent, input g·x"
  - [corpus]: Weak evidence - corpus neighbors mention equivariant networks but don't directly address latent representation ambiguity.

### Mechanism 2
- Claim: Invariant projections can recover well-defined latent representations by mapping equivariant representations to a space where each orbit has a unique representative.
- Mechanism: An invariant function s: Z → Z_s collapses each orbit G·z to a single point s(z), creating a Euclidean invariant representation where distances and neighborhoods are meaningful. For permutation equivariance, sorting coordinates provides an isometric cross-section.
- Core assumption: The invariant projection s is surjective and induces a well-defined map on the quotient space Z/G.
- Evidence anchors:
  - [section 3]: "we aim to find an invariant map s: Z → Z_s, and use this to extract invariant latent features"
  - [section 3.1]: "we show, that we can choose an invariant maps which induces an isometric cross section Z/G → Z_s ⊂ Z"
  - [corpus]: Weak evidence - corpus neighbors discuss invariant maps but not specifically for equivariant latent representations.

### Mechanism 3
- Claim: Random invariant linear projections can provide informative invariant representations when no isometric cross-section exists.
- Mechanism: By initializing invariant linear layers randomly (e.g., using permutation-invariant or E(n)-invariant bases), we obtain random projections that preserve local geometric structure while being computationally efficient. These can be used for visualization and downstream analysis.
- Core assumption: Random invariant projections retain sufficient information about the quotient structure for practical use.
- Evidence anchors:
  - [section 3.2]: "Random projections are often available: Ma et al. (2018); Maron et al. (2019) propose a basis for all permutation invariant linear maps"
  - [section 5.2]: "we demonstrate random invariant projections of equivariant latent features for a rotation invariant classifier"
  - [corpus]: Weak evidence - corpus neighbors don't mention random invariant projections specifically.

## Foundational Learning

- Quotient spaces and group actions
  - Why needed here: Understanding quotient spaces Z/G is crucial because equivariant representations implicitly encode this structure, and the ambiguity arises from multiple representatives of the same orbit.
  - Quick check question: If X has a group G acting on it, what is the quotient space X/G and how does it relate to orbits of group actions?

- Equivariant vs invariant functions
  - Why needed here: The distinction between equivariant (h(g·x) = g·h(x)) and invariant (h(g·x) = h(x)) functions is fundamental to understanding why equivariant latent spaces are ambiguous and how invariant projections fix this.
  - Quick check question: Given a group G and space X, write the mathematical condition for a function h: X → Y to be G-equivariant.

- Cross-sections and invariants
  - Why needed here: The concept of an isometric cross-section (a subset of Z that intersects each orbit exactly once) is key to understanding when we can recover a well-defined latent representation without loss of information.
  - Quick check question: For permutation equivariant representations in R^n, what operation creates an invariant representation that forms an isometric cross-section?

## Architecture Onboarding

- Component map: Equivariant encoder f -> Invariant projection s -> Decoder k
- Critical path:
  1. Train equivariant model to get f
  2. Apply invariant projection s to latent codes f(x)
  3. Use invariant representations for downstream tasks (visualization, kNN, interpolation)
  4. Evaluate improvement over equivariant representations
- Design tradeoffs:
  - Isometric cross-sections (like sorting) preserve all information but only work for specific group actions
  - Random invariant projections are more general but lossy
  - Post-hoc vs intrinsic invariance: Post-hoc allows training models for performance first, then extracting interpretable representations
- Failure signatures:
  - Equivariant representations show no clear structure in visualization (e.g., t-SNE plots)
  - kNN classification/regression performance is poor on equivariant latents
  - Interpolations in equivariant space produce unstable or nonsensical outputs
  - No improvement when applying invariant projections (suggesting the equivariant model already learned invariance)
- First 3 experiments:
  1. Train equivariant VAE on molecular graphs, compare t-SNE visualization of equivariant vs sorted invariant representations
  2. Apply kNN regression on equivariant vs invariant latent codes for molecular properties
  3. Test interpolation stability in equivariant vs invariant latent space for molecule generation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we systematically design high-quality invariant mappings for different equivariant architectures beyond the sorting function and random projections?
- Basis in paper: [explicit] The paper states that "suggesting suitable high-quality invariant mappings relevant for widely applied equivariant architectures is an obvious path for future work."
- Why unresolved: The authors acknowledge that the choice of invariant mapping may depend greatly on the model architecture, but do not provide a general framework for designing such mappings.
- What evidence would resolve it: A comprehensive study comparing the performance of various invariant mappings across different equivariant architectures and datasets, along with theoretical insights into their design principles.

### Open Question 2
- Question: How do the equivariant latent representations behave for other group actions beyond permutations and rotations, such as reflections, translations, or scaling?
- Basis in paper: [inferred] The paper focuses on permutation equivariant VAEs and rotation-equivariant image classifiers, but does not explore other group actions.
- Why unresolved: The authors only provide empirical evidence for two specific types of group actions, leaving open the question of how the observed phenomena generalize to other groups.
- What evidence would resolve it: Empirical studies applying the proposed methods to equivariant models with different group actions, comparing the performance of invariant projections across groups.

### Open Question 3
- Question: Can the proposed invariant projections be extended to non-Euclidean latent spaces, such as those arising from quotient spaces with non-trivial geometry?
- Basis in paper: [inferred] The authors mention that quotient spaces often have non-Euclidean structure, but do not explore how this affects the proposed invariant projections.
- Why unresolved: The paper focuses on Euclidean latent spaces and does not address the challenges of applying invariant projections to non-Euclidean spaces.
- What evidence would resolve it: A theoretical analysis of how the proposed invariant projections can be adapted to non-Euclidean latent spaces, along with empirical studies on equivariant models with non-Euclidean latent representations.

## Limitations
- The theoretical framework assumes well-behaved group actions and latent spaces, which may not hold for all equivariant architectures
- Performance improvements depend heavily on the specific group action and available invariant projections
- The method is post-hoc and requires access to intermediate latent representations, which may not always be practical

## Confidence
- Mechanism 1 (equivariant ambiguity): High - well-established group theory foundation
- Mechanism 2 (invariant projections): Medium - works well for permutation and SO(2) but not proven for all groups
- Mechanism 3 (random projections): Medium-Low - empirical results shown but theoretical guarantees limited

## Next Checks
1. Test invariant projection method on equivariant models with different group actions (e.g., E(n) or general orthogonal groups) to assess generality
2. Compare performance with alternative approaches like disentanglement or invariant learning from scratch
3. Evaluate the method's effectiveness on larger-scale equivariant models and real-world datasets beyond QM9 and MNIST