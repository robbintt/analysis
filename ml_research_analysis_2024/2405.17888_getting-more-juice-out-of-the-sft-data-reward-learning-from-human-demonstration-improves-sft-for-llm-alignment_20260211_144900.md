---
ver: rpa2
title: 'Getting More Juice Out of the SFT Data: Reward Learning from Human Demonstration
  Improves SFT for LLM Alignment'
arxiv_id: '2405.17888'
source_url: https://arxiv.org/abs/2405.17888
tags:
- reward
- data
- learning
- algorithm
- spin
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new approach to fine-tuning large language
  models (LLMs) using Inverse Reinforcement Learning (IRL) to learn a reward model
  from demonstration data, which improves alignment over standard supervised fine-tuning.
  Instead of directly imitating human demonstrations, the method jointly learns a
  reward function and a policy by framing the problem as a maximum likelihood IRL
  formulation.
---

# Getting More Juice Out of the SFT Data: Reward Learning from Human Demonstration Improves SFT for LLM Alignment

## Quick Facts
- arXiv ID: 2405.17888
- Source URL: https://arxiv.org/abs/2405.17888
- Reference count: 40
- This paper introduces a new approach to fine-tuning large language models (LLMs) using Inverse Reinforcement Learning (IRL) to learn a reward model from demonstration data, which improves alignment over standard supervised fine-tuning.

## Executive Summary
This paper presents an innovative approach to aligning large language models (LLMs) by leveraging Inverse Reinforcement Learning (IRL) to learn reward functions directly from human demonstration data. Instead of traditional supervised fine-tuning (SFT) that merely imitates demonstrations, the proposed method jointly learns a reward function and a policy, leading to improved alignment performance. The approach is theoretically grounded with convergence guarantees and demonstrates empirical improvements across both 1B and 7B parameter models on multiple benchmarks.

## Method Summary
The authors introduce a maximum likelihood formulation of IRL applied to LLM alignment, proposing two algorithms: Reward Fine-Tuning (RFT) that explicitly learns the reward function, and Implicit Reward Fine-Tuning (IRFT) that learns the reward implicitly. The IRFT method connects to recent self-play fine-tuning approaches like SPIN. Both algorithms are shown to converge to stationary points of the IRL objective. The key innovation is learning a reward model from demonstration data rather than directly imitating it, which the authors argue leads to better generalization and alignment.

## Key Results
- RFT and IRFT methods outperform standard SFT on HuggingFace's Open LLM Leaderboard
- Average score improvements from 59.47% to 61.03% across benchmarks
- Improvements demonstrated on both 1B and 7B parameter models
- Theoretical convergence guarantees provided for both algorithms

## Why This Works (Mechanism)
The approach works by learning a reward function that explains human demonstrations rather than simply copying them. This allows the model to capture the underlying preferences and intentions behind demonstrations, leading to more robust and generalizable behavior. The IRL framework provides a principled way to extract reward signals from demonstration data, which can then be used to guide fine-tuning in a more aligned direction than pure imitation learning.

## Foundational Learning
- **Inverse Reinforcement Learning (IRL)**: A framework for learning reward functions from expert demonstrations; needed to extract preference signals from human data; quick check: can the learned reward predict human preferences on held-out data?
- **Maximum Likelihood IRL**: A specific formulation that maximizes the likelihood of demonstrations under the learned reward; needed for tractable optimization; quick check: does the likelihood converge during training?
- **Self-Play Fine-Tuning**: Recent methods that use model-generated data for training; needed for context on IRFT's connection to SPIN; quick check: how does IRFT differ from pure self-play approaches?
- **Policy Optimization**: The process of updating model parameters to maximize expected reward; needed for the joint learning of reward and policy; quick check: do policy updates improve reward-weighted log-likelihood?
- **Stationary Point Convergence**: Theoretical guarantee that algorithms converge to points where gradients are zero; needed for ensuring stable training; quick check: do training curves show convergence without oscillation?
- **Demonstration Data Quality**: The importance of high-quality human demonstrations for learning effective reward functions; needed because IRL is sensitive to demonstration quality; quick check: how does performance vary with demonstration quality?

## Architecture Onboarding
- **Component Map**: Human Demonstrations -> IRL Reward Learning -> Reward-Weighted Policy Update -> Fine-Tuned LLM
- **Critical Path**: Demonstration Data → Reward Model Training → Policy Fine-Tuning → Evaluation
- **Design Tradeoffs**: Explicit reward learning (RFT) vs. implicit reward learning (IRFT); computational cost vs. alignment quality; theoretical guarantees vs. practical performance
- **Failure Signatures**: Poor reward learning leading to misaligned outputs; overfitting to demonstration distribution; instability in joint optimization of reward and policy
- **First 3 Experiments**: 1) Compare RFT vs. IRFT performance on standard benchmarks; 2) Ablation study on reward learning contribution; 3) Test scalability to larger model sizes

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on curated demonstration datasets rather than preference data limits generalizability to real-world alignment scenarios
- Evaluation scope is narrow, focusing on public benchmarks without extensive testing on task-specific or safety-critical domains
- Computational overhead of IRL training versus standard SFT is not thoroughly characterized

## Confidence
- High confidence in theoretical framework and convergence analysis
- Medium confidence in empirical improvements given limited model scales (1B and 7B) and evaluation datasets
- Low confidence in scalability and robustness for frontier LLMs or safety-critical applications

## Next Checks
1. Test the algorithms on larger model scales (e.g., 70B+ parameters) and diverse instruction-following datasets to assess scalability
2. Conduct ablation studies isolating the contribution of reward learning versus policy updates to quantify efficiency gains
3. Evaluate alignment robustness on safety and bias benchmarks to determine practical utility beyond performance metrics