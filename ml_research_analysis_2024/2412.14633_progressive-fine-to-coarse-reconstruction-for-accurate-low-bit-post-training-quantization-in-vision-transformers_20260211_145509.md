---
ver: rpa2
title: Progressive Fine-to-Coarse Reconstruction for Accurate Low-Bit Post-Training
  Quantization in Vision Transformers
arxiv_id: '2412.14633'
source_url: https://arxiv.org/abs/2412.14633
tags:
- reconstruction
- quantization
- performance
- granularity
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of post-training quantization
  (PTQ) for vision transformers (ViTs) when quantized into low-bit representations,
  which often results in significant performance drops. The authors propose a Progressive
  Fine-to-Coarse Reconstruction (PFCR) method to mitigate this issue.
---

# Progressive Fine-to-Coarse Reconstruction for Accurate Low-Bit Post-Training Quantization in Vision Transformers

## Quick Facts
- arXiv ID: 2412.14633
- Source URL: https://arxiv.org/abs/2412.14633
- Reference count: 9
- Achieves 75.61% top-1 accuracy for 3-bit quantized ViT-B in PTQ on ImageNet

## Executive Summary
This paper addresses the challenge of post-training quantization (PTQ) for vision transformers (ViTs) when quantized into low-bit representations, which often results in significant performance drops. The authors propose a Progressive Fine-to-Coarse Reconstruction (PFCR) method to mitigate this issue. PFCR progressively reconstructs the quantized model from fine granularity to coarse granularity, starting with multi-head self-attention (MHSA) and multi-layer perceptron (MLP) modules as the finest reconstruction units, and then iteratively combining and reconstructing coarser blocks. Additionally, a Progressive Optimization Strategy (POS) is introduced to further improve model performance. Experimental results on the ImageNet dataset demonstrate that the proposed method achieves state-of-the-art performance, particularly attaining 75.61% top-1 accuracy for 3-bit quantized ViT-B in PTQ.

## Method Summary
The proposed method introduces Progressive Fine-to-Coarse Reconstruction (PFCR) for low-bit post-training quantization of vision transformers. The approach works by first treating MHSA and MLP modules as the finest reconstruction units, then progressively merging these into larger blocks (layers, stages) and reconstructing them iteratively. A Progressive Optimization Strategy (POS) complements this by gradually optimizing the model through stages. The method addresses the performance degradation typically seen in low-bit quantization by preserving more structural information during the quantization process.

## Key Results
- Achieves 75.61% top-1 accuracy for 3-bit quantized ViT-B in PTQ on ImageNet
- State-of-the-art performance on low-bit PTQ for vision transformers
- Validated effectiveness on COCO dataset for object detection and instance segmentation tasks

## Why This Works (Mechanism)
The progressive fine-to-coarse reconstruction preserves critical information during quantization by maintaining finer structural granularity early in the process before gradually coarsening. This approach prevents the loss of important feature relationships that typically occurs in traditional quantization methods. The progressive optimization strategy further refines the model by allowing gradual adaptation to the quantized representation, preventing catastrophic performance drops that occur when optimizing the entire model simultaneously in low-bit space.

## Foundational Learning
- Post-training quantization (PTQ): A quantization approach that doesn't require full retraining, making it more practical for deployment
  - Why needed: Full quantization-aware training is computationally expensive and requires access to original training data
  - Quick check: Compare with quantization-aware training (QAT) in terms of accuracy and computational requirements

- Vision Transformer (ViT) architecture: Uses self-attention mechanisms instead of convolutional layers for image processing
  - Why needed: Understanding ViT components is crucial since the method targets specific modules (MHSA and MLP)
  - Quick check: Identify the key differences between ViT and convolutional neural networks

- Multi-head self-attention (MHSA): Allows the model to focus on different positions and capture various relationships
  - Why needed: MHSA modules are treated as the finest reconstruction units in PFCR
  - Quick check: Explain how multiple attention heads contribute to feature extraction

- Progressive optimization: A strategy that gradually optimizes model parameters through stages
  - Why needed: Prevents optimization difficulties that arise from directly optimizing in low-bit space
  - Quick check: Compare with end-to-end optimization in terms of convergence behavior

## Architecture Onboarding
**Component map:** Input image -> Patch embedding -> Transformer encoder (with progressive reconstruction) -> Classification head

**Critical path:** The progressive reconstruction process flows from MHSA/MLP modules -> merged layers -> merged stages -> final output

**Design tradeoffs:** The method trades computational overhead during reconstruction for better accuracy in low-bit quantization, prioritizing deployment efficiency over training speed

**Failure signatures:** Performance degradation occurs when progressive stages are skipped or when the merging strategy fails to preserve critical information between granularity levels

**First experiments:**
1. Apply PFCR to a pre-trained ViT-B model with 3-bit quantization on ImageNet
2. Compare progressive reconstruction with traditional post-training quantization baseline
3. Evaluate the contribution of POS by comparing with PFCR without progressive optimization

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness appears primarily validated on ViT architectures and may not generalize seamlessly to other vision transformer variants or architectures with different attention mechanisms
- Computational overhead of the progressive reconstruction process and progressive optimization strategy is not thoroughly characterized
- Benefits for higher bit-widths (5-8 bits) remain unexplored, limiting understanding of the method's full applicability range

## Confidence
- **High confidence** in the core methodology and reported ImageNet results, given the clear experimental setup and comparative analysis
- **Medium confidence** in the generalization claims to object detection and instance segmentation, as these results are less extensively validated
- **Low confidence** in computational efficiency claims due to lack of detailed complexity analysis

## Next Checks
1. Evaluate PFCR performance on Swin Transformer and other attention-based architectures to verify architectural generalization
2. Conduct ablation studies isolating the contribution of POS from the progressive reconstruction framework
3. Measure and report wall-clock time and memory overhead during both training and inference phases to assess practical deployment feasibility