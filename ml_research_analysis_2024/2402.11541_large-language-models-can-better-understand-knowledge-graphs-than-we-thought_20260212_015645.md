---
ver: rpa2
title: Large Language Models Can Better Understand Knowledge Graphs Than We Thought
arxiv_id: '2402.11541'
source_url: https://arxiv.org/abs/2402.11541
tags:
- llms
- language
- knowledge
- triples
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper empirically investigates how large language models (LLMs)
  process and interpret knowledge graph (KG) information across various input formats.
  The authors compare unordered linearized triples with fluent natural language text
  at both literal and attention distribution levels, finding that LLMs prefer linearized
  triples for fact-intensive questions.
---

# Large Language Models Can Better Understand Knowledge Graphs Than We Thought

## Quick Facts
- arXiv ID: 2402.11541
- Source URL: https://arxiv.org/abs/2402.11541
- Authors: Xinbang Dai; Yuncheng Hua; Tongtong Wu; Yang Sheng; Qiu Ji; Guilin Qi
- Reference count: 40
- Primary result: LLMs prefer unordered linearized triples over fluent natural language text for fact-intensive questions

## Executive Summary
This paper empirically investigates how large language models process and interpret knowledge graph information across various input formats. The authors compare unordered linearized triples with fluent natural language text at both literal and attention distribution levels, finding that LLMs consistently achieve better performance with linearized triples for fact-intensive questions. The study also examines how different organizational formats of triples impact performance and evaluates LLM robustness to noisy or incomplete KGs. Key findings include: linearized triples outperform NL text for multi-hop questions; different LLMs show varying preferences for prompt strategies; and larger models are more susceptible to noisy or incomplete subgraphs.

## Method Summary
The study uses Wikidata-based KGQA datasets (QALD-7, LC-QuAD 2.0, KQAPro) and extracts subgraphs using SPARQL queries. The researchers prepare input formats by generating linearized triples, natural language text (both rule-based and model-based), and testing different organizational strategies. Experiments are conducted with multiple LLMs (ChatGPT, Vicuna 7B/13B) and evaluated using Exact Match metrics. The study compares performance across different input formats, analyzes attention distributions, and tests robustness by introducing noise and incompleteness to KG subgraphs.

## Key Results
- Linearized triples outperform natural language text for multi-hop questions
- Larger models show greater performance degradation with noisy or incomplete subgraphs
- Different LLMs exhibit varying preferences for organizational formats of triples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs prefer unordered linearized triples over fluent natural language text for fact-intensive questions
- Mechanism: Linearized triples preserve explicit, structured relational information that LLMs can directly match to question-answer patterns without parsing through semantic noise introduced by text generation
- Core assumption: The structured nature of triples allows LLMs to more efficiently extract answer-relevant information compared to processing narrative text
- Evidence anchors:
  - [abstract] "linearized triples outperform NL text for multi-hop questions"
  - [section 3.1.2] "compared to NL text, LLMs achieve significant improvements when using linearized triples"
  - [corpus] Weak evidence - related papers discuss KG integration but don't specifically address triple vs text format preference
- Break condition: If LLMs are trained specifically on graph-to-text generation tasks, they might develop stronger text processing capabilities for KG information

### Mechanism 2
- Claim: Larger LLMs are more susceptible to noisy or incomplete subgraphs
- Mechanism: Larger models with more parameters have higher capacity to memorize training patterns, making them more sensitive to inconsistencies between expected and actual input patterns
- Core assumption: The increased parameter count in larger models creates stronger expectations about input consistency
- Evidence anchors:
  - [abstract] "LLMs with larger scales are more susceptible to noisy, incomplete subgraphs"
  - [section 5.3] "ChatGPT shows greater performance degradation on replacement and deletion compared to the smaller parameter models"
  - [corpus] No direct evidence - corpus papers focus on KG integration rather than model size effects on robustness
- Break condition: If larger models are specifically fine-tuned on noisy KG data, they might develop better robustness to input inconsistencies

### Mechanism 3
- Claim: Different LLMs exhibit varying preferences for organizational formats of triples
- Mechanism: Different training corpora and architectural choices lead to varying internal representations and processing preferences for structured information
- Core assumption: The diversity in model training data and architectures creates different optimal input formats for each model
- Evidence anchors:
  - [abstract] "Different LLMs exhibit varying preferences for different organizational formats of triples"
  - [section 4.4] "ChatGPT favors the prompt method incorporating relevance scores, while the Vicuna series prefers ranking strategies"
  - [corpus] Weak evidence - related papers discuss KG integration but don't specifically address organizational format preferences
- Break condition: If models are fine-tuned on diverse organizational formats, they might develop more uniform preferences

## Foundational Learning

- Concept: Knowledge Graph (KG) structure and triple representation
  - Why needed here: Understanding how KGs are represented as subject-predicate-object triples is fundamental to grasping why linearized triples work better
  - Quick check question: What are the three components of a KG triple and how do they represent relationships?

- Concept: Prompt engineering and input format sensitivity
  - Why needed here: The paper's core finding is about how different input formats affect LLM performance
  - Quick check question: How might the format of input information (structured vs natural language) affect an LLM's ability to extract relevant facts?

- Concept: Attention mechanisms in transformers
  - Why needed here: The paper analyzes attention distributions to understand why triples work better
  - Quick check question: How do attention mechanisms help LLMs focus on relevant information in their input?

## Architecture Onboarding

- Component map: KG Subgraph Extraction -> Triple Linearization -> Prompt Formatting -> LLM Inference -> Answer Generation
- Critical path: The process of converting KG subgraphs into different formats and measuring LLM performance on fact-intensive questions
- Design tradeoffs:
  - Structured vs. natural language representation: Structured triples provide clarity but lack fluency
  - Model size vs. robustness: Larger models perform better but are more sensitive to noise
  - Prompt complexity vs. universal applicability: More sophisticated prompts may work better for specific models but lack generalizability
- Failure signatures:
  - Performance degradation when using natural language text vs. triples
  - Larger models showing more significant performance drops with noisy data
  - Inconsistent performance across different organizational formats of triples
- First 3 experiments:
  1. Compare LLM performance on fact-intensive questions using linearized triples vs. natural language text representations of the same KG subgraph
  2. Test robustness by gradually corrupting KG subgraphs with noise and measuring performance degradation across different model sizes
  3. Evaluate different organizational formats of linearized triples (unordered, ranked by relevance, grouped by relevance) to identify optimal prompt strategies for different LLM architectures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different large language models (LLMs) compare in their ability to understand and process knowledge graph (KG) information across various input formats?
- Basis in paper: [explicit] The paper mentions that "Different LLMs exhibit varying preferences for different organizational formats of triples" and that "different series of LLMs do not necessarily rely on order" when processing linearized triples.
- Why unresolved: The paper only tests a limited set of LLMs (ChatGPT, Vicuna 7B, Vicuna 13B) and doesn't provide a comprehensive comparison across different model architectures or sizes.
- What evidence would resolve it: Systematic testing of a wider range of LLMs (including different architectures like BERT, GPT-3, LLaMA, etc.) across various KG input formats and prompt strategies.

### Open Question 2
- Question: What are the underlying mechanisms driving LLMs' preferences for different KG input formats at the attention distribution level?
- Basis in paper: [explicit] The paper states that at the attention level, "we discuss the underlying mechanisms driving these preferences" and observes that LLMs consistently demonstrate a greater capacity to capture answer information from linearized triples.
- Why unresolved: While the paper observes attention distribution differences, it doesn't fully explain the neural mechanisms or provide detailed analysis of how attention heads process different KG formats.
- What evidence would resolve it: Detailed attention head analysis, visualization of attention patterns, and ablation studies showing which attention mechanisms are most important for different KG formats.

### Open Question 3
- Question: How can we design universal prompting techniques that work effectively across different LLM architectures for KG-related tasks?
- Basis in paper: [explicit] The paper finds that "different versions of LLMs exhibit distinct preferences for these strategies" and suggests that "when designing a prompt method, the universal applicability of a given prompt strategy across multiple LLMs should be considered."
- Why unresolved: The paper identifies this as a challenge but doesn't provide a solution or framework for creating universal prompting strategies that work across different LLM architectures.
- What evidence would resolve it: Development and testing of a meta-learning approach for prompt optimization, or creation of a framework that can automatically adapt prompts to different LLM architectures based on their characteristics.

## Limitations

- Domain specificity: All experiments use Wikidata-based knowledge graphs, which may not generalize to other KG domains
- Single-turn evaluation: The study focuses on single-turn question answering, not evaluating multi-turn conversational contexts
- Black-box analysis: While attention distributions are analyzed, the study cannot fully explain why LLMs prefer certain formats

## Confidence

- High confidence: LLMs achieve better performance with linearized triples than NL text for fact-intensive questions
- Medium confidence: Larger models are more susceptible to noisy KG subgraphs
- Medium confidence: Different LLMs exhibit varying preferences for organizational formats of triples

## Next Checks

1. Cross-domain validation: Test the triple vs. NL text preference using biomedical knowledge graphs like UMLS or MeSH to determine if the findings generalize beyond Wikidata's encyclopedic knowledge.

2. Multi-turn evaluation: Design experiments where LLMs must reference KG information across multiple conversational turns to assess whether format preferences persist in sustained interactions.

3. Noise type variation: Expand the robustness testing to include different types of KG noise (structural inconsistencies, entity resolution errors, temporal mismatches) to better understand model sensitivity patterns across different failure modes.