---
ver: rpa2
title: Lightweight Modular Parameter-Efficient Tuning for Open-Vocabulary Object Detection
arxiv_id: '2408.10787'
source_url: https://arxiv.org/abs/2408.10787
tags:
- object
- detection
- mdetr
- image
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: UniProj-Det introduces a lightweight modular framework for parameter-efficient
  open-vocabulary object detection by freezing pretrained backbones and training only
  a small Universal Projection module with a learnable modality token. Applied to
  MDETR, it trains only ~2-5% of parameters while achieving competitive or superior
  performance on phrase grounding, referring expression comprehension, and segmentation
  tasks.
---

# Lightweight Modular Parameter-Efficient Tuning for Open-Vocabulary Object Detection

## Quick Facts
- arXiv ID: 2408.10787
- Source URL: https://arxiv.org/abs/2408.10787
- Authors: Bilal Faye; Hanane Azzag; Mustapha Lebbah
- Reference count: 31
- Primary result: UniProj-Det achieves competitive performance on phrase grounding and referring expression tasks while training only 2-5% of parameters

## Executive Summary
UniProj-Det introduces a lightweight modular framework for parameter-efficient open-vocabulary object detection by freezing pretrained backbones and training only a small Universal Projection module with a learnable modality token. Applied to MDETR, it trains only ~2-5% of parameters while achieving competitive or superior performance on phrase grounding, referring expression comprehension, and segmentation tasks. On RefCOCO, RefCOCO+, and RefCOCOg, LightMDETR achieves up to 85.92% P@1 and 95.52% P@5, matching or exceeding MDETR despite the drastic reduction in trainable parameters.

## Method Summary
UniProj-Det is a parameter-efficient tuning framework that freezes pretrained vision and language backbones while introducing a Universal Projection module and learnable modality token. The approach applies to MDETR architecture, where the projection module is trained to bridge frozen features between modalities. This modular design enables efficient adaptation to open-vocabulary detection tasks without extensive fine-tuning of the entire model, achieving competitive performance while training only 2-5% of total parameters.

## Key Results
- Achieves up to 85.92% P@1 and 95.52% P@5 on RefCOCO datasets
- Matches or exceeds MDETR performance despite training only 2-5% of parameters
- Demonstrates competitive results on phrase grounding, referring expression comprehension, and segmentation tasks

## Why This Works (Mechanism)
The approach leverages frozen pretrained backbones to maintain rich visual and linguistic representations while the Universal Projection module learns task-specific alignments. The learnable modality token acts as a bridge between vision and language modalities, enabling efficient cross-modal reasoning without full fine-tuning. By focusing training on only the projection components, the method maintains generalization capabilities while adapting to new detection tasks efficiently.

## Foundational Learning
- Cross-modal alignment: Essential for bridging vision and language representations; quick check: verify alignment metrics between modalities
- Parameter-efficient tuning: Reduces computational cost while maintaining performance; quick check: compare parameter counts with baseline models
- Modality token learning: Enables adaptive feature fusion across modalities; quick check: ablation study with and without modality token
- Open-vocabulary detection: Extends object detection beyond predefined categories; quick check: test on novel object classes
- Modular architecture design: Facilitates independent component updates; quick check: assess impact of freezing different modules

## Architecture Onboarding

Component map: Input images/text -> Vision Backbone -> Universal Projection -> Modality Token -> Language Backbone -> Output predictions

Critical path: Vision Backbone (frozen) -> Universal Projection (trainable) -> Modality Token (trainable) -> Language Backbone (frozen) -> Detection outputs

Design tradeoffs: High parameter efficiency vs. potential performance limitations from frozen backbones; modular design enables easy adaptation but may miss fine-grained cross-modal interactions

Failure signatures: Degraded performance on rare object categories, reduced cross-modal alignment quality, increased sensitivity to initialization of projection module

First experiments:
1. Baseline comparison: Full fine-tuning vs. UniProj-Det parameter efficiency
2. Ablation study: Remove modality token to quantify contribution
3. Cross-dataset evaluation: Test generalization beyond RefCOCO series

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability across different vision-language architectures remains unverified beyond MDETR
- Computational overhead and inference speed comparisons with full fine-tuning are not provided
- Impact of modality token learning on model interpretability and potential bias introduction is unexplored

## Confidence
High: Performance claims on RefCOCO datasets (85.92% P@1, 95.52% P@5)
Medium: Generalization to other tasks and architectures beyond MDETR
Medium: Efficiency claims without comprehensive speed and resource usage comparisons

## Next Checks
1. Evaluate UniProj-Det on additional object detection benchmarks beyond RefCOCO series, including MSCOCO and LVIS, to assess cross-dataset generalization
2. Conduct ablation studies removing the modality token component to quantify its specific contribution to performance improvements
3. Compare inference latency and memory consumption between UniProj-Det and full fine-tuning baselines under identical hardware conditions