---
ver: rpa2
title: 'Husky: A Unified, Open-Source Language Agent for Multi-Step Reasoning'
arxiv_id: '2406.06469'
source_url: https://arxiv.org/abs/2406.06469
tags:
- step
- solution
- question
- husky
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Husky is an open-source language agent designed to tackle multi-step
  reasoning tasks across numerical, tabular, and knowledge-based domains. It operates
  by iteratively generating and executing actions using specialized tools such as
  code generation, math reasoning, search, and commonsense modules.
---

# Husky: A Unified, Open-Source Language Agent for Multi-Step Reasoning

## Quick Facts
- **arXiv ID**: 2406.06469
- **Source URL**: https://arxiv.org/abs/2406.06469
- **Reference count**: 40
- **Primary result**: Husky outperforms prior open-source agents and matches GPT-4 on multi-step reasoning benchmarks.

## Executive Summary
Husky is an open-source language agent designed for multi-step reasoning across numerical, tabular, and knowledge-based domains. It operates through a modular pipeline where an action generator predicts the next reasoning step and tool, then specialized expert models execute the action. Trained on high-quality synthetic data from teacher model trajectories, Husky demonstrates strong generalization and outperforms previous open-source agents while matching proprietary models on mixed-tool reasoning tasks.

## Method Summary
Husky employs a two-stage pipeline: an action generator predicts the next high-level step and associated tool from the task instruction and solution history, then specialized expert models execute the action and update the solution state. The agent is trained on synthetic trajectories generated by a teacher LLM, with fine-tuned models for code generation, math reasoning, search, and commonsense reasoning. This modular design allows Husky to handle diverse reasoning tasks without task-specific tools, using only 7B models for efficiency and scalability.

## Key Results
- Husky achieves state-of-the-art performance among open-source agents on mixed-tool reasoning benchmarks like Husky QA.
- On DROP* and IIRC*, Husky matches or exceeds proprietary models like GPT-4 while outperforming previous open-source approaches.
- The modular design with specialized expert models enables strong performance across numerical, tabular, and knowledge-based domains.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Husky achieves strong performance through a modular two-stage pipeline with joint action and tool prediction.
- **Mechanism**: The action generator receives task instruction and solution history, predicts the next step and tool, then routes to the corresponding expert model for execution and state update.
- **Core assumption**: Joint prediction of step and tool by a single module, combined with modular expert execution, generalizes across diverse reasoning tasks without task-specific tools.
- **Evidence anchors**:
  - [abstract]: "Husky iterates between two stages: 1) generating the next action to take towards solving a given task and 2) executing the action using expert models and updating the current solution state."
  - [section]: "The first module in Husky is the action generator. Given the input question and the solution generated so far, the action generator jointly predicts the next high-level step to take and the associated tool."
- **Break condition**: Pipeline fails if the action generator cannot predict correct next steps or tools, or if expert models fail to execute.

### Mechanism 2
- **Claim**: Husky's performance scales with the quality and specificity of expert models for each tool type.
- **Mechanism**: Each tool ([code], [math], [search], [commonsense]) is paired with a finetuned expert model (Mc, Mm, Mq, Mr) using specialized models like DeepSeekCoder-7B-Instruct-v1.5 for code and DeepSeekMath-7B-Instruct for math.
- **Core assumption**: Specialized expert models outperform general-purpose models on their respective tool tasks, and high-quality finetuning data leads to better generalization.
- **Evidence anchors**:
  - [section]: "For code generators, we confirm that DeepSeekCoder-7B-Instruct-v1.5 provides the best initialization for Mc with its state-of-the-art coding capabilities."
  - [section]: "For math reasoners, we also find that DeepSeekMath-7B-Instruct results in the best performance for the agent."
- **Break condition**: Performance drops if general-purpose models replace specialized ones, especially on tasks requiring deep domain expertise.

### Mechanism 3
- **Claim**: Husky's training data synthesis via teacher model trajectories enables learning of multi-step reasoning and tool use without manual data curation.
- **Mechanism**: A teacher LLM generates tool-integrated solution trajectories for seed training tasks, which are parsed to extract training instances for the action generator and expert models.
- **Core assumption**: A capable teacher model can generate high-quality, diverse trajectories covering multi-step reasoning, automatically transformed into training data.
- **Evidence anchors**:
  - [section]: "We use a simplified and generalizable pipeline where we few-shot prompt a teacher model to generate tool-integrated solution trajectories, which are crucial to endowing tool use abilities to Husky."
  - [section]: "The solution trajectory is then used as a seed dataset from which training instances for A, Mm, Mc and Mq are extracted."
- **Break condition**: Training data quality degrades if the teacher model cannot generate correct or diverse trajectories.

## Foundational Learning

- **Concept**: Modular decomposition of multi-step reasoning tasks into high-level steps and tool assignments.
  - **Why needed here**: Husky must generalize across diverse tasks without task-specific code. Understanding modular decomposition enables design of a unified action space and routing mechanism.
  - **Quick check question**: Can you explain how a single action generator can route to different expert models based on predicted tool tokens?

- **Concept**: Tool-integrated solution trajectory synthesis from teacher model outputs.
  - **Why needed here**: Husky's training relies on automatically generating synthetic trajectories for learning, avoiding manual annotation. Understanding this process is key to scaling Husky to new tasks.
  - **Quick check question**: How does Husky transform a tool-integrated trajectory into training data for both the action generator and expert models?

- **Concept**: Evaluation of multi-tool reasoning versus single-tool or chain-of-thought baselines.
  - **Why needed here**: Husky is designed for mixed-tool reasoning, so understanding the difference between evaluating on single-tool tasks (like GSM-8K) versus mixed-tool tasks (like Husky QA) is crucial for proper benchmarking.
  - **Quick check question**: What is the difference between Husky's evaluation on DROP* and GSM-8K in terms of tool use requirements?

## Architecture Onboarding

- **Component map**: Task instruction + solution history → Action Generator → step + tool → Expert Model (Mc/Mm/Mq/Mr) → action execution → update solution history → repeat until [finish] token → return final answer

- **Critical path**: 1) Task instruction + initial history → Action Generator → step + tool. 2) Tool token → Expert Model → action execution → natural language output. 3) Update solution history → repeat until [finish] token. 4) Return final answer.

- **Design tradeoffs**:
  - Joint action generator vs. separate step and tool predictors: Joint model reduces inference steps and enforces coherence, but may be harder to train.
  - Specialized expert models vs. single general-purpose model: Specialized models yield better performance on their domains, but require more resources and careful selection.
  - Synthetic trajectory generation vs. manual data curation: Synthetic data scales easily and avoids annotation costs, but depends on teacher model quality.

- **Failure signatures**:
  - Action generator repeatedly predicts wrong tools or steps → check training data diversity and quality.
  - Expert models produce invalid or non-executable outputs → verify model finetuning and tool interfaces.
  - Husky gets stuck in loops or exceeds iteration limits → check stopping conditions and action generator's ability to detect final answers.

- **First 3 experiments**:
  1. Run Husky on a simple GSM-8K problem (e.g., "What is 5 + 7?") and inspect the action generator's step and tool prediction, then the code/math model's output.
  2. Replace the DeepSeekMath-7B-Instruct with a general-purpose model (e.g., Llama-3-8B) and compare performance on a set of MATH problems to measure the impact of tool model choice.
  3. Generate a small set of synthetic trajectories using a teacher model and manually inspect the quality and diversity of the resulting training data for the action generator.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does Husky's performance scale with increasing action space size and number of expert models?
  - **Basis in paper**: [inferred]
  - **Why unresolved**: The paper mentions that Husky can be extended to address a wider variety of tasks by scaling the action space and number of expert models, but does not provide empirical evidence or analysis on the performance gains or limitations of such scaling.
  - **What evidence would resolve it**: Conducting experiments with progressively larger action spaces and additional expert models, evaluating performance on a diverse set of tasks, and analyzing the trade-offs between model complexity and reasoning accuracy.

- **Open Question 2**: How robust is Husky to noisy or incomplete tool-integrated solution trajectories during training?
  - **Basis in paper**: [inferred]
  - **Why unresolved**: Husky relies on high-quality, synthetic data derived from tool-integrated solution trajectories for training. The paper does not investigate how the model performs when the training data contains errors, ambiguities, or missing steps in the solution trajectories.
  - **What evidence would resolve it**: Systematically introducing noise or incompleteness in the training data, training Husky on these modified datasets, and comparing the performance to the original model on the same evaluation tasks.

- **Open Question 3**: Can Husky's action generator learn to decompose tasks into subtasks without explicit step-by-step annotations in the training data?
  - **Basis in paper**: [inferred]
  - **Why unresolved**: Husky is trained using tool-integrated solution trajectories that explicitly break down tasks into steps. The paper does not explore whether the action generator can learn to decompose tasks autonomously, without such step-by-step annotations, which would be a more general and scalable approach.
  - **What evidence would resolve it**: Training Husky on datasets where the solution trajectories only contain the final answer and the intermediate steps are not provided, and evaluating whether the action generator can still learn to decompose tasks effectively.

## Limitations
- The quality and diversity of synthetic training data depends heavily on the teacher model used for trajectory generation, which is not fully specified.
- Husky may fail on tasks requiring deep domain expertise if specialized expert models are not used or properly finetuned.
- The action generator's ability to handle novel task structures or noisy training data is not extensively validated.

## Confidence

- **High Confidence**: Husky's modular two-stage pipeline with joint action generation and specialized expert models is well-defined and supported by clear experimental results, particularly in outperforming open-source baselines on mixed-tool reasoning tasks.
- **Medium Confidence**: The claim that Husky matches or exceeds proprietary models like GPT-4 is supported by benchmark results, but the reliance on teacher model quality for synthetic data generation introduces uncertainty in performance consistency.
- **Low Confidence**: The generalizability of Husky to new tasks beyond those evaluated is uncertain, as the paper does not provide extensive ablation studies on the impact of training data diversity or the robustness of the action generator to novel task structures.

## Next Checks

1. **Reproduce Synthetic Data Generation**: Use the provided few-shot prompts (Appendix J) to generate tool-integrated solution trajectories with a teacher model (e.g., GPT-4). Manually inspect the generated trajectories for correctness, diversity, and adherence to the required format. Adjust prompts if necessary to improve quality.

2. **Evaluate Tool Model Impact**: Replace the specialized DeepSeekMath-7B-Instruct with a general-purpose model (e.g., Llama-3-8B) and compare Husky's performance on MATH problems. Measure the drop in accuracy to quantify the benefit of specialized expert models.

3. **Stress Test Action Generator**: Run Husky on a set of tasks with known complexity (e.g., a mix of GSM-8K, TabMWP, and HotpotQA) and monitor for failure modes such as loop behavior, incorrect tool prediction, or failure to reach a final answer. Analyze the action generator's predictions and expert model outputs to identify bottlenecks.