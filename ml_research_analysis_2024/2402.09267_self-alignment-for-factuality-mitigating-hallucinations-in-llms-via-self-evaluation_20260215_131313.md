---
ver: rpa2
title: 'Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via Self-Evaluation'
arxiv_id: '2402.09267'
source_url: https://arxiv.org/abs/2402.09267
tags: []
core_contribution: This paper introduces a self-alignment approach for mitigating
  hallucinations in LLMs by leveraging the model's own factuality evaluation capability.
  The key idea is to use an LLM to evaluate the factuality of its own generated responses
  based on internal knowledge, then use these self-annotated responses to fine-tune
  the model via Direct Preference Optimization.
---

# Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via Self-Evaluation

## Quick Facts
- arXiv ID: 2402.09267
- Source URL: https://arxiv.org/abs/2402.09267
- Authors: Xiaoying Zhang; Baolin Peng; Ye Tian; Jingyan Zhou; Lifeng Jin; Linfeng Song; Haitao Mi; Helen Meng
- Reference count: 40
- Key outcome: Self-alignment approach using LLM self-evaluation significantly improves factual accuracy over Llama models on knowledge-intensive tasks.

## Executive Summary
This paper introduces a self-alignment approach for mitigating hallucinations in LLMs by leveraging the model's own factuality evaluation capability. The method uses an LLM to evaluate the factuality of its own generated responses based on internal knowledge, then uses these self-annotated responses to fine-tune the model via Direct Preference Optimization. The authors also propose Self-Knowledge Tuning (SK-Tuning) to improve the model's confidence estimation and calibration, thereby enhancing its self-evaluation ability. Experiments on three knowledge-intensive tasks using TruthfulQA and BioGEN datasets show that the proposed self-alignment approach significantly improves factual accuracy over Llama family models.

## Method Summary
The method consists of two main components: SELF-EVAL for factuality self-evaluation and Direct Preference Optimization (DPO) fine-tuning on self-annotated preference data. First, the LLM generates candidate responses for prompts using 5-shot prompting. Then, SELF-EVAL-SKT evaluates factuality by extracting atomic claims from responses, converting them to questions, and scoring each claim's factuality based on internal knowledge. The responses are ranked by factuality scores and the top responses are selected as preferred responses. Finally, the LLM is fine-tuned using DPO on the preference data. Additionally, SK-Tuning is applied to improve confidence estimation and calibration, which enhances the self-evaluation ability.

## Key Results
- Self-Alignment for Factuality significantly improves Accuracy by roughly 13% on TruthfulQA (MC) task compared to Llama-7B
- SK-Tuning consistently outperforms SELF-EVAL-P(TRUE) by a substantial margin in terms of Accuracy for the selection task and AUROC for the discrimination task
- The proposed approach notably outperforms representative decoding-based methods and a recent approach with consistency-based confidence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can self-evaluate factuality better than they can generate factual content.
- Mechanism: The model's internal knowledge enables it to assess the truthfulness of its own generated responses through prompting, even when its generation may be flawed.
- Core assumption: Self-evaluation is easier than self-generation for LLMs due to different cognitive processes.
- Evidence anchors:
  - [abstract] "LLMs may generate inaccurate responses, even though they demonstrate the ability to provide precise answers during different inference periods"
  - [section 3.2] "evaluation is easier than generation" and "LLMs have demonstrated capabilities in discerning the extent of their knowledge"
  - [corpus] Weak - no direct corpus evidence supporting this specific mechanism
- Break condition: If the model lacks relevant internal knowledge for a given prompt, self-evaluation accuracy degrades significantly.

### Mechanism 2
- Claim: Fine-tuning on self-annotated preference data via DPO improves factual accuracy.
- Mechanism: The model learns to prefer responses with higher self-evaluated factuality scores, aligning its outputs with its internal knowledge.
- Core assumption: The self-evaluated factuality scores are reliable enough to serve as training signals.
- Evidence anchors:
  - [abstract] "we utilize these self-annotated responses to fine-tune the model via Direct Preference Optimization algorithm"
  - [section 3.1] "we rank the candidate responses according to the factually scores acquired... we select the top α responses as the preferred responses"
  - [section 4.2] "Self-Alignment for Factuality significantly improves Accuracy by roughly 13% on TruthfulQA (MC) task"
- Break condition: If the preference data is poorly constructed or the self-evaluation is systematically biased, fine-tuning may reinforce incorrect patterns.

### Mechanism 3
- Claim: SK-Tuning improves the model's confidence estimation and calibration, leading to better self-evaluation.
- Mechanism: Fine-tuning on heterogeneous knowledge tasks enhances the model's ability to accurately assess its own knowledge and assign appropriate confidence scores.
- Core assumption: Confidence calibration is crucial for effective self-evaluation and can be improved through targeted fine-tuning.
- Evidence anchors:
  - [section 3.2] "we introduce Self-Knowledge Tuning (SK-Tuning) to augment the LLM's self-evaluation ability by improving the model's confidence estimation and calibration"
  - [section 5.2] "SK-Tuning consistently outperforms SELF-EVAL-P(TRUE) by a substantial margin in terms of Accuracy for the selection task and AUROC for the discrimination task"
  - [corpus] Weak - limited corpus evidence specifically on SK-Tuning effectiveness
- Break condition: If the SK-Tuning dataset is not representative of the target tasks or if the fine-tuning process overfits, the benefits may not transfer.

## Foundational Learning

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: DPO is used to fine-tune the model on the self-annotated preference data, aligning it towards higher factuality.
  - Quick check question: How does DPO differ from traditional RLHF in terms of training objective and data requirements?

- Concept: Confidence calibration
  - Why needed here: Proper confidence calibration is essential for accurate self-evaluation, which is the core of the proposed approach.
  - Quick check question: What is the difference between confidence calibration and confidence estimation in the context of LLMs?

- Concept: Factuality self-evaluation
  - Why needed here: The ability to self-evaluate factuality is the key mechanism that enables the proposed self-alignment approach.
  - Quick check question: How does the SELF-EVAL component use prompting to elicit factuality confidence scores from the LLM?

## Architecture Onboarding

- Component map: Base LLM → Self-Evaluation Component (SELF-EVAL/SELF-EVAL-SKT) → Preference Data Generation → DPO Fine-tuning → Aligned LLM
- Critical path: Generate responses → Self-evaluate factuality → Create preference pairs → Fine-tune with DPO
- Design tradeoffs: Using the model's internal knowledge avoids external data needs but may be limited by the model's existing knowledge; SK-Tuning improves self-evaluation but adds fine-tuning overhead.
- Failure signatures: Poor factuality improvement despite fine-tuning may indicate issues with self-evaluation accuracy or preference data quality; overconfident but incorrect responses suggest calibration problems.
- First 3 experiments:
  1. Implement SELF-EVAL on a simple MCQA task to verify self-evaluation accuracy.
  2. Apply DPO fine-tuning on self-annotated preference data and measure factuality improvement.
  3. Implement SK-Tuning and compare self-evaluation accuracy before and after.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the self-alignment approach scale with model size beyond 7B parameters?
- Basis in paper: The authors mention that larger models tend to have better self-evaluation abilities based on findings from Kadavath et al. (2022), and they anticipate even greater success for larger variants like 13B and 70B.
- Why unresolved: The experiments were conducted only on 7B-scale models from the LLAMA family. The authors did not test the approach on larger models to validate their hypothesis about improved performance.
- What evidence would resolve it: Conducting the same experiments on 13B and 70B variants of LLAMA or other large models would provide empirical evidence of how the self-alignment approach scales with model size.

### Open Question 2
- Question: How does combining the self-alignment framework with high-performing decoding-based approaches like DOLA affect factual accuracy?
- Basis in paper: The authors suggest that combining their self-alignment framework with high-performing approaches like DOLA has the potential to yield even more accurate and factual improvements in LLMs.
- Why unresolved: The authors did not conduct experiments to combine their self-alignment framework with other decoding-based approaches. They only compared their method against existing approaches individually.
- What evidence would resolve it: Implementing a hybrid approach that combines the self-alignment framework with DOLA or other high-performing decoding-based methods, and then evaluating its performance on the same tasks would provide concrete evidence of the benefits of such combination.

### Open Question 3
- Question: What is the impact of different confidence estimation and calibration methods on the performance of the self-alignment framework?
- Basis in paper: The authors mention that adopting various factuality estimation approaches substantially influences the performance of their self-alignment framework. They also note that while their proposed SK-TUNING is effective, future research may benefit from exploring more efficient confidence estimation and calibration methods.
- Why unresolved: The authors only implemented and evaluated their proposed SK-TUNING method for confidence estimation and calibration. They did not compare it with other existing methods or explore the impact of different approaches on the framework's performance.
- What evidence would resolve it: Implementing and comparing the self-alignment framework with different confidence estimation and calibration methods, such as those mentioned in the paper (Guo et al., 2017; Tian et al., 2023b; Zhu et al., 2023; Chen et al., 2023a; Shrivastava et al., 2023; Liu et al., 2023a), would provide empirical evidence of the impact of these methods on the framework's performance.

## Limitations
- Limited external validation - evaluation primarily self-referential, comparing proposed method to baselines on the same datasets used for training
- SK-Tuning effectiveness ambiguity - unclear whether improvements come from better calibration specifically or simply from additional fine-tuning on knowledge tasks
- Unknown dataset construction - exact prompt formats for different task formats and SELF-EVAL are only partially described

## Confidence

**High confidence** in: The core claim that LLMs can self-evaluate factuality of their own responses. The paper provides clear experimental evidence showing that SELF-EVAL-SKT consistently outperforms random selection and baseline methods across multiple datasets and tasks.

**Medium confidence** in: The effectiveness of SK-Tuning for improving confidence calibration. While experimental results show improvements, the underlying mechanism (calibration vs. general knowledge enhancement) is not definitively established.

**Medium confidence** in: The DPO fine-tuning approach on self-annotated preference data. The improvements are statistically significant but the paper doesn't provide detailed analysis of whether the learned preferences generalize beyond the training distribution.

## Next Checks
1. Ablation study on SK-Tuning: Run experiments with SK-Tuning but without the confidence calibration component to determine if improvements come from better calibration specifically versus general knowledge enhancement.

2. Cross-domain generalization test: Evaluate the aligned models on completely held-out domains not seen during any training phase to assess whether improvements transfer beyond the training distribution.

3. Human evaluation correlation: Compare the SELF-EVAL-SKT confidence scores against human-annotated factuality assessments to validate whether the model's self-evaluation aligns with human judgment, particularly for edge cases where internal knowledge may be limited.