---
ver: rpa2
title: H2O-Danube-1.8B Technical Report
arxiv_id: '2401.16818'
source_url: https://arxiv.org/abs/2401.16818
tags:
- arxiv
- tokens
- h2o-danube-1
- shot
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: H2O-Danube is a series of small 1.8B parameter language models
  trained on 1T tokens (H2O-Danube-1.8B) and 3T tokens total (H2O-Danube2-1.8B). The
  models follow a decoder architecture based on Llama 2 and Mistral, with sliding
  window attention, rotary positional embeddings, and grouped-query attention.
---

# H2O-Danube-1.8B Technical Report

## Quick Facts
- arXiv ID: 2401.16818
- Source URL: https://arxiv.org/abs/2401.16818
- Reference count: 40
- Key outcome: H2O-Danube2-1.8B achieves state-of-the-art results on Open LLM Leaderboard for models under 2B parameters, with an average score of 48.72 across 8 benchmarks.

## Executive Summary
H2O-Danube is a series of small 1.8B parameter language models trained on 1T tokens (H2O-Danube-1.8B) and 3T tokens total (H2O-Danube2-1.8B). The models follow a decoder architecture based on Llama 2 and Mistral, with sliding window attention, rotary positional embeddings, and grouped-query attention. Training used 8xH100 GPUs with AdamW optimizer and cosine learning rate scheduling. H2O-Danube2-1.8B achieves state-of-the-art results on Open LLM Leaderboard for models under 2B parameters, with an average score of 48.72 across 8 benchmarks. The models also show strong performance on commonsense reasoning, world knowledge, and reading comprehension tasks. Chat variants were fine-tuned using supervised fine-tuning and direct preference optimization, with the H2O-Danube2-1.8B-Chat variant scoring 5.79 on MT-Bench. All models are released under Apache 2.0 license.

## Method Summary
The H2O-Danube models are 1.8B parameter language models based on a decoder architecture derived from Llama 2 and Mistral. The models use sliding window attention, rotary positional embeddings, and grouped-query attention. Training was conducted on 8xH100 GPUs using AdamW optimizer with cosine learning rate scheduling. H2O-Danube-1.8B was trained on 1T tokens, while H2O-Danube2-1.8B was trained on an additional 2T tokens (total 3T). Chat variants were fine-tuned using supervised fine-tuning with datasets like OpenOrca and MetaMathQA, followed by direct preference optimization using preference datasets such as UltraFeedback Binarized and Orca DPO Pairs.

## Key Results
- H2O-Danube2-1.8B achieves state-of-the-art results on Open LLM Leaderboard for models under 2B parameters, with an average score of 48.72 across 8 benchmarks.
- H2O-Danube2-1.8B-Chat scores 5.79 on MT-Bench, demonstrating strong performance in conversational tasks.
- The models show strong performance on commonsense reasoning, world knowledge, and reading comprehension tasks, indicating their versatility and effectiveness.

## Why This Works (Mechanism)
The H2O-Danube models leverage a decoder architecture based on Llama 2 and Mistral, which provides a strong foundation for language understanding. The use of sliding window attention, rotary positional embeddings, and grouped-query attention enables efficient processing of long sequences and improves the model's ability to capture long-range dependencies. The large-scale training on 1T-3T tokens allows the models to learn rich representations from diverse data sources, including web documents, encyclopedias, and public knowledge databases. The fine-tuning process using supervised fine-tuning and direct preference optimization further enhances the models' performance on specific tasks and conversational abilities.

## Foundational Learning
- **Decoder Architecture**: The decoder architecture is essential for autoregressive language modeling, allowing the model to predict the next token in a sequence based on previous tokens. Quick check: Verify that the model can generate coherent text by sampling from the model.
- **Sliding Window Attention**: Sliding window attention enables efficient processing of long sequences by attending to a fixed-size window of previous tokens. Quick check: Evaluate the model's performance on tasks requiring long-range dependencies, such as document summarization.
- **Rotary Positional Embeddings**: Rotary positional embeddings provide a way to incorporate positional information into the model without increasing the number of parameters. Quick check: Compare the model's performance with and without rotary positional embeddings on tasks requiring position-aware understanding.
- **Grouped-Query Attention**: Grouped-query attention reduces the computational cost of multi-head attention by grouping queries and keys, improving efficiency without sacrificing performance. Quick check: Measure the model's inference speed with and without grouped-query attention.
- **AdamW Optimizer**: AdamW optimizer is a variant of the Adam optimizer that decouples weight decay from the optimization process, leading to better regularization and improved generalization. Quick check: Monitor the model's training and validation loss curves to ensure proper convergence.
- **Cosine Learning Rate Scheduling**: Cosine learning rate scheduling gradually reduces the learning rate following a cosine curve, allowing for fine-grained control over the optimization process and potentially improving convergence. Quick check: Compare the model's performance with different learning rate schedules, such as step decay or linear decay.

## Architecture Onboarding
- **Component Map**: Data (Web documents, encyclopedia, public knowledge databases) -> Pre-training (1T-3T tokens, 8xH100 GPUs) -> Fine-tuning (SFT, DPO) -> Chat Model
- **Critical Path**: The critical path involves pre-training the base model on a large corpus of data, followed by fine-tuning on task-specific datasets using supervised fine-tuning and direct preference optimization.
- **Design Tradeoffs**: The choice of a decoder architecture based on Llama 2 and Mistral provides a good balance between performance and efficiency. The use of sliding window attention and rotary positional embeddings enables efficient processing of long sequences, while grouped-query attention improves inference speed.
- **Failure Signatures**: Poor convergence during training may indicate issues with learning rate scheduling or batch size. Chat model performance degradation after DPO fine-tuning could suggest overfitting on preference datasets.
- **First Experiments**: 1) Evaluate the base model's performance on standard language modeling benchmarks to assess its overall capabilities. 2) Fine-tune the base model on a specific task (e.g., question answering) and evaluate its performance to gauge the effectiveness of the fine-tuning process. 3) Perform an ablation study by removing or modifying key architectural components (e.g., sliding window attention, rotary positional embeddings) to understand their impact on the model's performance.

## Open Questions the Paper Calls Out
- **Open Question 1**: How does the performance of H2O-Danube2-1.8B scale with additional training tokens beyond 3T? The paper only reports results for training up to 3T tokens, leaving the question of performance at higher token counts unanswered. Training H2O-Danube2-1.8B on additional tokens (e.g., 4T, 5T) and evaluating performance on benchmarks would help determine if further scaling improves results.
- **Open Question 2**: How does the removal of sliding window attention and increased context length to 8,192 tokens impact the model's performance on long-context tasks? While the paper claims improved long-context behavior, it does not provide specific benchmarks or tasks to demonstrate this improvement. Evaluating H2O-Danube2-1.8B on tasks specifically designed to test long-context understanding (e.g., document summarization, multi-hop reasoning) and comparing results to models with shorter context lengths would provide insights into the impact of these changes.
- **Open Question 3**: How does the quality of the training data affect the performance of H2O-Danube2-1.8B compared to models trained on larger but potentially lower-quality datasets? The paper mentions using heuristics and small models to predict the quality of training samples, suggesting that data quality is a consideration. Training multiple models of similar size on datasets with varying quality levels (e.g., curated vs. web-scraped) and evaluating their performance on benchmarks would help determine the impact of data quality on model performance.

## Limitations
- The technical report does not provide detailed information about the composition and curation process of the training datasets, making it challenging to reproduce the exact results.
- The specific hyperparameters for the fine-tuning stages, such as learning rates for SFT and DPO, are not fully documented, limiting the ability to achieve identical results.
- The paper does not provide direct comparisons of models trained on datasets of varying quality but similar size, leaving the question of how data quality impacts performance unanswered.

## Confidence
- High: The model architecture and training methodology are well-specified, allowing for faithful reproduction of the base model.
- Medium: The fine-tuning process and hyperparameters are partially documented, requiring some experimentation to achieve optimal results.
- Low: The lack of detailed information about the training data composition and curation process creates significant uncertainty in replicating the model's performance.

## Next Checks
1. Reconstruct and analyze