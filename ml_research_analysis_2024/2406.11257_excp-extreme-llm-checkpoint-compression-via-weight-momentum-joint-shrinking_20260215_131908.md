---
ver: rpa2
title: 'ExCP: Extreme LLM Checkpoint Compression via Weight-Momentum Joint Shrinking'
arxiv_id: '2406.11257'
source_url: https://arxiv.org/abs/2406.11257
tags: []
core_contribution: This paper introduces ExCP, a framework for extreme compression
  of large language model (LLM) training checkpoints. ExCP exploits the sparsity of
  weight residuals between adjacent checkpoints and jointly prunes both model weights
  and optimizer momentum states to significantly reduce storage requirements.
---

# ExCP: Extreme LLM Checkpoint Compression via Weight-Momentum Joint Shrinking

## Quick Facts
- arXiv ID: 2406.11257
- Source URL: https://arxiv.org/abs/2406.11257
- Authors: Wenshuo Li; Xinghao Chen; Han Shu; Yehui Tang; Yunhe Wang
- Reference count: 38
- Achieves up to 70× compression ratio on Pythia-410M while maintaining nearly lossless performance

## Executive Summary
This paper introduces ExCP, a framework for extreme compression of large language model training checkpoints. The method exploits the sparsity of weight residuals between consecutive checkpoints and jointly prunes both model weights and optimizer momentum states to significantly reduce storage requirements. ExCP achieves up to 70× compression ratio while maintaining nearly lossless performance across various downstream tasks, offering substantial storage savings for LLM training without requiring retraining or fine-tuning.

## Method Summary
ExCP applies a three-stage compression pipeline to LLM checkpoints: (1) residual checkpoint calculation to capture sparse weight differences between consecutive checkpoints, (2) weight-momentum joint pruning using magnitude-based thresholds that consider both weight changes and momentum importance, and (3) non-uniform quantization with K-means clustering to further compress remaining values. The framework is evaluated on models ranging from 410M to 7B parameters, demonstrating effectiveness across both vision and language tasks.

## Key Results
- Achieves 70× compression ratio on Pythia-410M with performance matching the original model
- Maintains nearly lossless performance on downstream tasks including ImageNet-1K and various NLP benchmarks
- Effective across model scales from 410M to 7B parameters (ViT-L32, Pythia-410M, PanGu-π series)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Residual checkpointing reduces storage by exploiting sparsity between consecutive model weight updates
- Mechanism: Instead of storing full model weights at each checkpoint, store only the difference (residual) between current and previous weights. Since consecutive weight updates are often small, residuals are sparse and compressible.
- Core assumption: Weight changes between consecutive checkpoints are small and sparse enough to be efficiently compressed.
- Evidence anchors:
  - [abstract]: "We first calculate the residuals of adjacent checkpoints to obtain the essential but sparse information for higher compression ratio."
  - [section]: "The model weights will be updated upon previous ones according to the gradient during training, thus the difference between adjacent model weights is mostly to be sparse, which is more suitable for compression."
  - [corpus]: No direct evidence found in corpus; this is a core assumption of the method.
- Break condition: If training involves large, abrupt weight changes between checkpoints, residuals may not be sparse, reducing compression effectiveness.

### Mechanism 2
- Claim: Joint pruning of weights and momentum states achieves better compression without significant accuracy loss
- Mechanism: Instead of pruning weights and momentum separately, use a unified criterion that considers both weight magnitude changes (second-order momentum) and momentum importance. This allows more aggressive pruning while preserving critical information.
- Core assumption: Weights and momentum states are correlated, so joint pruning can remove redundant information more effectively than separate pruning.
- Evidence anchors:
  - [abstract]: "To further excavate the redundancy parameters in checkpoints, we then propose a weight-momentum joint shrinking method to utilize another important information during the model optimization, i.e., momentum."
  - [section]: "For momentum pruning, we use the first-order moment as an indicator to determine whether to prune this momentum states or not... if a specific location of weights is pruned, intuitively it is not important to preserve the corresponding momentum states."
  - [corpus]: No direct evidence found in corpus; this is a novel contribution of the paper.
- Break condition: If the correlation between weights and momentum is weak or non-existent, joint pruning may remove important information and degrade model performance.

### Mechanism 3
- Claim: Non-uniform quantization further compresses checkpoints while maintaining accuracy
- Mechanism: After pruning, cluster the remaining weight and momentum values into a small number of quantization centers using K-means. Store only the cluster centers and indices, reducing storage requirements.
- Core assumption: The distribution of weight and momentum values can be approximated by a small number of clusters without significant information loss.
- Evidence anchors:
  - [abstract]: "Furthermore, we utilize non-uniform quantization to further compress the storage of checkpoints."
  - [section]: "We quantize weights and momentum states separately... We leave the pruned weights or momentum states to zero, and apply K-means algorithm on other weights or momentum states to cluster them to 2^n - 1 cluster centers."
  - [corpus]: No direct evidence found in corpus; this is a standard technique applied in this context.
- Break condition: If the value distribution is too complex to be well-approximated by a small number of clusters, quantization may introduce significant errors and degrade model performance.

## Foundational Learning

- Concept: Understanding of neural network training and checkpointing
  - Why needed here: To understand why checkpoint compression is important and how the proposed methods work
  - Quick check question: What is the purpose of saving checkpoints during neural network training?

- Concept: Familiarity with optimization algorithms, particularly Adam
  - Why needed here: To understand the role of momentum states in the compression method and the convergence analysis
  - Quick check question: What are the first-order and second-order moments in the Adam optimizer, and why are they important?

- Concept: Knowledge of data compression techniques (pruning, quantization)
  - Why needed here: To understand how the proposed methods achieve compression and their limitations
  - Quick check question: How do pruning and quantization reduce the storage requirements of neural network models?

## Architecture Onboarding

- Component map:
  Residual calculation -> Joint pruning -> Non-uniform quantization -> 7zip compression

- Critical path: Residual calculation → Joint pruning → Non-uniform quantization → 7zip compression

- Design tradeoffs:
  - Higher compression ratio vs. potential accuracy loss
  - Joint pruning vs. separate pruning of weights and momentum
  - Non-uniform quantization vs. uniform quantization

- Failure signatures:
  - Significant accuracy degradation after compression
  - Inability to reconstruct checkpoints accurately
  - Training divergence when resuming from compressed checkpoints

- First 3 experiments:
  1. Apply residual checkpointing to a small model and measure compression ratio and accuracy
  2. Implement joint pruning and compare results with separate pruning
  3. Evaluate the impact of non-uniform quantization on compression ratio and accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed ExCP framework perform when applied to multi-modal large models and visual large models?
- Basis in paper: [explicit] The authors mention in the conclusion that they plan to extend experiments to different tasks such as multi-modal large models and visual large models.
- Why unresolved: The current study only evaluates the framework on language models (ViT-L32, Pythia-410M, and PanGu-π series). There is no empirical evidence on how the framework performs on other types of models.
- What evidence would resolve it: Experiments comparing the performance of ExCP on multi-modal and visual large models against baseline methods would provide insights into the framework's effectiveness across different model types.

### Open Question 2
- Question: What is the impact of different types of neural networks (e.g., CNNs, RNNs) on the performance of the ExCP framework?
- Basis in paper: [explicit] The authors state in the conclusion that they would consider different types of neural networks in the future, including transformers, CNNs, and RNNs.
- Why unresolved: The current study focuses on transformer-based models. The behavior of ExCP on other neural network architectures remains unexplored.
- What evidence would resolve it: Comparative experiments on CNNs and RNNs using ExCP and baseline methods would reveal how the framework adapts to different neural network structures.

### Open Question 3
- Question: How does the choice of hyperparameters (e.g., α, β, n) affect the compression ratio and model performance in the ExCP framework?
- Basis in paper: [explicit] The authors set hyperparameters α and β in the weight-momentum joint pruning method and n in the non-uniform quantization. However, they do not explore the sensitivity of these parameters.
- Why unresolved: The paper does not provide a detailed analysis of how varying these hyperparameters influences the trade-off between compression ratio and model accuracy.
- What evidence would resolve it: A systematic study varying the hyperparameters and measuring the resulting compression ratios and model performances would clarify their impact and help optimize the framework.

## Limitations

- The exact hyperparameter search procedure for pruning thresholds (α = 5e-5, β = 2.0) is not specified, raising questions about generalizability
- Limited downstream task diversity with ViT-L32 tested only on ImageNet-1K and unspecified LLM tasks
- Scalability assumptions about weight residual sparsity may not hold for models with complex optimization dynamics

## Confidence

**High confidence** in the fundamental mechanism: Residual checkpointing is a well-established technique, and the paper's implementation follows logical principles. The joint pruning approach, while novel, builds on reasonable assumptions about weight-momentum correlation.

**Medium confidence** in claimed compression ratios: The results are internally consistent and show clear improvement over baseline methods, but the lack of detailed hyperparameter search and limited task diversity reduce confidence in generalizability.

**Low confidence** in long-term stability claims: The paper doesn't address extended training scenarios, fine-tuning from compressed checkpoints, or potential degradation over multiple training cycles.

## Next Checks

1. **Ablation study on pruning thresholds**: Systematically vary α and β across multiple orders of magnitude to determine sensitivity and establish guidelines for threshold selection on new models.

2. **Extended downstream evaluation**: Test compressed checkpoints on additional tasks including reasoning benchmarks, code generation, and multilingual evaluations to verify the "nearly lossless" claim across diverse use cases.

3. **Longitudinal training stability**: Resume training from compressed checkpoints and continue training for multiple epochs, measuring any gradual performance degradation or training instability that might emerge over extended use.