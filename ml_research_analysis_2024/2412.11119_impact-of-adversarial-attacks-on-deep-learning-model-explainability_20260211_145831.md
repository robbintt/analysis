---
ver: rpa2
title: Impact of Adversarial Attacks on Deep Learning Model Explainability
arxiv_id: '2412.11119'
source_url: https://arxiv.org/abs/2412.11119
tags:
- adversarial
- explanation
- images
- learning
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how adversarial attacks affect the explainability
  of deep learning models using techniques like GradCAM, SmoothGrad, and LIME. The
  authors generate adversarial images using FGSM and BIM methods, which significantly
  reduce model accuracy from 89.94% to 58.73% and 45.50% respectively, yet explanation
  metrics (IoU and RMSE) show negligible changes.
---

# Impact of Adversarial Attacks on Deep Learning Model Explainability

## Quick Facts
- arXiv ID: 2412.11119
- Source URL: https://arxiv.org/abs/2412.11119
- Authors: Gazi Nazia Nur; Mohammad Ahnaf Sadat
- Reference count: 7
- Primary result: Adversarial attacks significantly reduce model accuracy but explanation metrics (IoU, RMSE) remain largely unchanged

## Executive Summary
This paper investigates how adversarial attacks affect the explainability of deep learning models using GradCAM, SmoothGrad, and LIME. The authors generate adversarial images using FGSM and BIM methods, which significantly reduce model accuracy from 89.94% to 58.73% and 45.50% respectively, yet explanation metrics show negligible changes. This suggests current explainability metrics may not be sensitive enough to detect adversarial perturbations. The study highlights a gap in benchmarks for evaluating adversarial attacks on explanation methods and proposes future work to include more attack methods, metrics, and datasets for comprehensive evaluation.

## Method Summary
The study evaluates deep learning model explainability under adversarial attacks using three explanation techniques (GradCAM, SmoothGrad, LIME) on CIFAR-10 dataset. The authors generate adversarial images using FGSM and BIM attack methods and measure their impact on model accuracy and explanation quality using Intersection over Union (IoU) and Root Mean Square Error (RMSE) metrics. The evaluation pipeline processes original and adversarial images through both the model and explanation techniques, comparing the consistency of explanations before and after attacks.

## Key Results
- Model accuracy drops from 89.94% to 58.73% (FGSM) and 45.50% (BIM) under adversarial attacks
- IoU and RMSE metrics for explanation methods remain largely unchanged despite accuracy degradation
- SmoothGrad and LIME show slight variations in IoU values, but GradCAM remains stable across attack types
- Results indicate current explainability metrics are insensitive to adversarial perturbations affecting model predictions

## Why This Works (Mechanism)
The paper demonstrates that adversarial attacks can successfully fool deep learning models into making incorrect predictions while the explanation methods continue to produce similar quality explanations. This occurs because explanation techniques extract information from the same internal model representations that are being subtly manipulated by the attacks. The attacks are designed to be imperceptible to human observers and maintain enough similarity to original inputs that explanation algorithms cannot detect the perturbation-induced changes in model behavior.

## Foundational Learning
- Adversarial attacks (FGSM, BIM): These are techniques to generate malicious inputs that cause models to make wrong predictions while appearing normal to humans. Needed to understand how model vulnerabilities can be exploited while maintaining visual similarity.
- GradCAM: A gradient-based visualization method that highlights important regions in input images for model predictions. Quick check: Verify it produces heatmaps showing pixel importance for classification decisions.
- SmoothGrad: An explanation technique that reduces noise in gradient-based explanations by averaging multiple noisy samples. Quick check: Confirm it produces smoother, more stable visualizations compared to raw gradients.
- LIME: Local Interpretable Model-agnostic Explanations that approximate model behavior with interpretable models locally. Quick check: Validate it generates sparse linear models explaining individual predictions.
- IoU (Intersection over Union): A metric measuring overlap between predicted and ground truth regions. Quick check: Ensure values range from 0 (no overlap) to 1 (perfect overlap).
- RMSE (Root Mean Square Error): A metric measuring average magnitude of errors between predicted and actual values. Quick check: Lower values indicate better prediction accuracy.

## Architecture Onboarding

**Component Map**: Input Images -> Adversarial Attack Generator -> Deep Learning Model -> GradCAM/SmoothGrad/LIME -> IoU/RMSE Metrics

**Critical Path**: Original Image → Model → Explanation Method → IoU/RMSE Calculation (baseline)
Adversarial Image → Model → Explanation Method → IoU/RMSE Calculation (comparison)

**Design Tradeoffs**: The study prioritizes sensitivity of explanation metrics over computational efficiency by testing multiple explanation methods. Using simpler attack methods (FGSM, BIM) enables clearer analysis but may miss more sophisticated attack patterns that could affect explanations differently.

**Failure Signatures**: When explanation metrics remain stable despite significant accuracy drops, it indicates the metrics are not sensitive enough to detect adversarial perturbations. Large discrepancies between original and adversarial explanation quality would suggest metric sensitivity.

**First Experiments**: 1) Generate adversarial images using FGSM and BIM on CIFAR-10 test set 2) Run GradCAM on both original and adversarial images to visualize differences 3) Calculate IoU and RMSE between original and adversarial explanations to quantify changes

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses exclusively on FGSM and BIM attacks, representing only a narrow subset of adversarial attack methods
- Evaluation is limited to CIFAR-10 dataset, restricting generalizability to other domains
- Only three explanation techniques are tested, potentially missing sensitivity patterns in other methods

## Confidence
The paper presents important findings about the robustness of explainability metrics under adversarial attacks, but several limitations affect confidence in the conclusions. Confidence: Medium

## Next Checks
1. Test the same methodology with more advanced adversarial attacks (CW, PGD) to verify if explainability metrics remain insensitive
2. Evaluate the approach on additional datasets (ImageNet, medical imaging) to assess cross-domain robustness
3. Include more explainability methods (SHAP, Integrated Gradients) and compare their sensitivity to adversarial perturbations against GradCAM, SmoothGrad, and LIME