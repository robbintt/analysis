---
ver: rpa2
title: 'OTTER: Effortless Label Distribution Adaptation of Zero-shot Models'
arxiv_id: '2404.08461'
source_url: https://arxiv.org/abs/2404.08461
tags:
- label
- distribution
- otter
- zero-shot
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces OTTER, a method for adapting zero-shot models
  to handle mismatched label distributions in downstream tasks. The core idea is to
  use optimal transport to rebalance model predictions based on an estimated target
  label distribution, without requiring labeled data or access to the pretraining
  distribution.
---

# OTTER: Effortless Label Distribution Adaptation of Zero-shot Models

## Quick Facts
- arXiv ID: 2404.08461
- Source URL: https://arxiv.org/abs/2404.08461
- Authors: Changho Shin; Jitian Zhao; Sonia Cromp; Harit Vishwakarma; Frederic Sala
- Reference count: 40
- Key outcome: OTTER adapts zero-shot models to mismatched label distributions using optimal transport, improving accuracy by 4.8% on average in image classification and 15.9% in text classification across 21 datasets.

## Executive Summary
OTTER addresses a fundamental challenge in deploying zero-shot models: label distribution shifts between pretraining and target domains. By estimating the target label distribution and using optimal transport to rebalance model predictions, OTTER adapts models without requiring labeled data or access to the pretraining distribution. The method shows strong theoretical guarantees, recovering the Bayes-optimal classifier under label shift and demonstrating robustness to certain perturbations.

## Method Summary
OTTER estimates the target label distribution from unlabeled target data and uses optimal transport to compute a transport map that rebalances the model's predictions. The method leverages the zero-shot model's ability to score any input-label pair, enabling estimation of the target label distribution through rejection sampling. By solving an optimal transport problem between the estimated source and target label distributions, OTTER computes a transport matrix that reweights predictions to match the target distribution.

## Key Results
- OTTER improves accuracy by 4.8% on average in image classification across 21 datasets
- OTTER improves accuracy by 15.9% in text classification across 21 datasets
- OTTER extends to few-shot learning and mitigates LLM selection bias

## Why This Works (Mechanism)
OTTER works by recognizing that zero-shot models are trained on one label distribution but must perform well on another. The method estimates the target distribution from unlabeled data, then uses optimal transport to find the optimal way to transform predictions from the source to target distribution. This transport map is applied to model outputs, effectively rebalancing predictions to match the target domain without requiring any labeled examples.

## Foundational Learning

### Optimal Transport
- Why needed: Provides a principled way to measure and transform between probability distributions
- Quick check: Understand Wasserstein distance and transport plans

### Label Shift
- Why needed: The core problem OTTER addresses - mismatch between source and target label distributions
- Quick check: Recognize when accuracy metrics can be misleading under label shift

### Rejection Sampling
- Why needed: Enables estimation of target label distribution from unlabeled data using zero-shot model scores
- Quick check: Understand how to use model predictions as proposal distribution

## Architecture Onboarding

### Component Map
Zero-shot model -> Rejection sampler -> Target distribution estimator -> Optimal transport solver -> Prediction reweighting

### Critical Path
Unlabeled target data → Rejection sampling using zero-shot model → Target distribution estimation → Optimal transport between source/target distributions → Reweighted predictions

### Design Tradeoffs
- Accuracy vs. computational cost: Solving optimal transport can be expensive for large label spaces
- Assumptions vs. generality: OTTER assumes access to unlabeled target data, which may not always be available
- Simplicity vs. performance: The method is straightforward but may be outperformed by more complex approaches

### Failure Signatures
- Poor performance when target label distribution cannot be accurately estimated
- Limited effectiveness under extreme distribution shifts
- Computational bottlenecks with large label spaces

### First Experiments
1. Test OTTER on a simple synthetic dataset with known label distribution shift
2. Compare OTTER against no-adaptation baseline on a standard benchmark
3. Evaluate OTTER's sensitivity to errors in target distribution estimation

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions beyond those addressed in the limitations section.

## Limitations

- Assumes access to unlabeled target dataset for distribution estimation, which may not be feasible in all deployment scenarios
- Effectiveness under extreme distribution mismatches remains unclear
- Computational overhead of solving optimal transport problems for large label spaces could limit scalability

## Confidence

- Claim: OTTER improves accuracy by 4.8% on average in image classification - High
- Claim: OTTER improves accuracy by 15.9% in text classification - Medium
- Claim: OTTER is robust to certain perturbations - High
- Claim: OTTER recovers the Bayes-optimal classifier under label shift - High
- Claim: OTTER extends to few-shot learning and mitigates LLM selection bias - Medium

## Next Checks

1. Test OTTER's performance on datasets with extreme label distribution shifts to assess the limits of its robustness
2. Conduct experiments measuring the computational overhead of OTTER on large-scale classification problems with thousands of classes
3. Evaluate OTTER in a true deployment scenario where only unlabeled data is available at inference time, without access to any target domain data for distribution estimation