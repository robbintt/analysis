---
ver: rpa2
title: 'Generalized Prompt Tuning: Adapting Frozen Univariate Time Series Foundation
  Models for Multivariate Healthcare Time Series'
arxiv_id: '2411.12824'
source_url: https://arxiv.org/abs/2411.12824
tags:
- time
- latexit
- series
- prompt
- foundation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Generalized Prompt Tuning (Gen-P-Tuning), a
  method to adapt frozen univariate time series foundation models for multivariate
  time series prediction in healthcare applications. The approach introduces a Prompt
  Module that summarizes information across channels of multivariate time series and
  attaches it as a prefix to each channel's preprocessed representation before feeding
  it to the frozen foundation model.
---

# Generalized Prompt Tuning: Adapting Frozen Univariate Time Series Foundation Models for Multivariate Healthcare Time Series

## Quick Facts
- arXiv ID: 2411.12824
- Source URL: https://arxiv.org/abs/2411.12824
- Reference count: 16
- This paper proposes Generalized Prompt Tuning (Gen-P-Tuning), a method to adapt frozen univariate time series foundation models for multivariate time series prediction in healthcare applications

## Executive Summary
This paper introduces Generalized Prompt Tuning (Gen-P-Tuning), a parameter-efficient method for adapting frozen univariate time series foundation models to multivariate healthcare time series prediction tasks. The approach uses a trainable Prompt Module that summarizes information across channels and attaches this summary as a prefix to each channel's input before feeding it to the frozen foundation model. Experiments on MIMIC-III classification tasks and influenza forecasting demonstrate that Gen-P-Tuning is competitive with various fine-tuning baselines while requiring minimal trainable parameters, making it particularly suitable for low-data regimes common in healthcare applications.

## Method Summary
Gen-P-Tuning treats a pretrained univariate time series foundation model as frozen and adds a trainable Prompt Module that processes multivariate input by first separating channels, applying the foundation model's preprocessing function to each channel independently, then aggregating the resulting representations through a neural network to create a channel-aware summary prompt. This prompt is attached as a prefix to each channel's preprocessed representation before feeding to the frozen transformer layers. The method generalizes standard prompt tuning by making prompts context-aware rather than static, and can be configured as special cases including channel-independent linear probing or standard P-tuning v2 by adjusting the Prompt Module's architecture.

## Key Results
- Gen-P-Tuning achieves competitive performance with full fine-tuning, LoRA, linear probing, and standard prompt tuning on MIMIC-III mortality prediction and phenotyping tasks
- The method is particularly effective in low-data regimes where labeled data is scarce, which is common in medical applications
- Different Prompt Module architectures (transformer, RNN, and MLP) yield comparable results on the tested tasks
- Channel-independent strategies sometimes perform well, suggesting foundation models' learned representations are already quite powerful

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Prompt Module enables the frozen univariate foundation model to process multivariate information by creating channel-aware summaries that are prepended to each channel's input
- Mechanism: The method processes each channel independently through the frozen foundation model's preprocessing function, then aggregates information across channels using a trainable neural network (the Prompt Module) to create a summary tensor. This summary is attached as a prefix to each channel's preprocessed representation before feeding it to the transformer layers, effectively providing cross-channel context without modifying the frozen backbone
- Core assumption: Information from different channels can be meaningfully summarized into a fixed-size prompt that enhances the model's predictions when prepended to each channel's input
- Evidence anchors:
  - [abstract] "Our approach provides a way to combine information across channels (variables) of multivariate time series"
  - [section 3.1] "the prompt P should encode summary information across channels"
- Break condition: If the channel relationships are too complex or non-linear for the Prompt Module to capture effectively, or if the fixed prompt size K is insufficient to represent the necessary cross-channel information

### Mechanism 2
- Claim: The method achieves competitive performance by preserving the pre-trained foundation model's capabilities while adding minimal trainable parameters
- Mechanism: By treating the foundation model as frozen and only training the Prompt Module and prediction head, the approach maintains the model's learned representations from large-scale pretraining while adapting it to new tasks. The channel-independent strategy (linear probing) sometimes performs well, suggesting that the foundation model's learned representations are already quite powerful
- Core assumption: The foundation model's pre-trained representations contain sufficient general knowledge that can be adapted with minimal fine-tuning
- Evidence anchors:
  - [section 4] "Full fine-tuning sometimes does not perform well, especially for forecasting experiments. This might be a sign of catastrophic forgetting"
  - [section 4] "Linear probing sometimes performs very well, which suggests that a channel-independent strategy is sometimes sufficient"
- Break condition: If the task requires substantial domain-specific adaptation that cannot be captured by the small trainable components, or if catastrophic forgetting is severe despite freezing most parameters

### Mechanism 3
- Claim: The prompt-tuning-inspired approach generalizes standard prompt tuning by incorporating channel information into the prompt generation
- Mechanism: Standard prompt tuning treats prompts as purely trainable parameters. Gen-P-Tuning generalizes this by making prompts depend on the input through the Prompt Module, which processes the channel-specific preprocessed representations. This allows the prompts to be context-aware rather than static
- Core assumption: Context-aware prompts generated from channel information are more effective than static prompts for multivariate time series
- Evidence anchors:
  - [section 3.1] "If the Prompt Module fprompt is defined to not actually depend on E and instead just output an array of (C·K)-by-D numbers that are all treated as trainable parameters, then we recover a popular prompt tuning strategy called P-tuning v2"
  - [section 4] "We can choose between these special cases by whichever one achieves the best validation loss"
- Break condition: If the additional complexity of generating context-aware prompts doesn't provide sufficient performance gains over simpler static prompts for the given task

## Foundational Learning

- Concept: Time series foundation models and their pretraining objectives
  - Why needed here: Understanding how foundation models are pretrained on large datasets and what patterns they learn is crucial for appreciating why they can be adapted with minimal fine-tuning
  - Quick check question: What is the key difference between how time series foundation models and large language models are pretrained?

- Concept: Parameter-efficient fine-tuning (PEFT) methods
  - Why needed here: The paper builds on existing PEFT techniques like LoRA and prompt tuning, so understanding these methods is essential for grasping how Gen-P-Tuning fits into the broader landscape
  - Quick check question: How does LoRA modify transformer layers differently from standard fine-tuning?

- Concept: Multivariate vs univariate time series modeling
  - Why needed here: The core contribution is adapting univariate models for multivariate tasks, so understanding the distinction and challenges is fundamental
  - Quick check question: What is the key limitation of treating multivariate time series with channel independence?

## Architecture Onboarding

- Component map:
  Input: Multivariate time series X ∈ R^C×T → Channel separation and foundation model preprocessing function femb → Prompt Module (trainable neural network) → Prefix attachment (concatenating prompt P to each channel's preprocessed representation) → Frozen backbone (foundation model transformer layers) → Prediction head (trainable component) → Output (predicted target Y)

- Critical path: Input → Channel separation → femb preprocessing → Prompt Module → Prefix attachment → Frozen transformer layers → Prediction head → Output

- Design tradeoffs:
  - Prompt size K: Larger values may capture more cross-channel information but increase computational cost and risk overfitting
  - Prompt Module architecture: Transformer-based (more powerful but computationally expensive) vs RNN/MLP (faster but potentially less expressive)
  - Freezing vs fine-tuning: Freezing preserves pretraining benefits but may limit adaptation; fine-tuning increases parameter count and risk of catastrophic forgetting

- Failure signatures:
  - Poor performance despite training: Likely issues with Prompt Module architecture or prompt size being insufficient
  - Overfitting on small datasets: May need regularization or smaller prompt size
  - Computational inefficiency: Large prompt size or complex Prompt Module architecture

- First 3 experiments:
  1. Implement the channel-independent baseline (linear probing) to establish a performance reference point
  2. Test Gen-P-Tuning with minimal prompt size (K=1) to verify the basic mechanism works before scaling up
  3. Compare different Prompt Module architectures (transformer vs RNN vs MLP) on a validation set to select the most effective approach

## Open Questions the Paper Calls Out

- Open Question: How does Gen-P-Tuning perform on healthcare time series with categorical variables that change over time?
  - Basis in paper: [inferred] The paper notes that healthcare time series routinely consist of categorical variables that change over time, and the authors suspect these do not closely resemble the sort of time series that time series foundation models are typically trained on, but they have not investigated how fine-tuning time series foundation models copes with these categorical variables.
  - Why unresolved: The experiments were limited to continuous clinical variables and public health data, without evaluating performance on categorical time-varying features common in healthcare.
  - What evidence would resolve it: Experimental results comparing Gen-P-Tuning performance on datasets with significant categorical time-varying variables (e.g., diagnosis codes, treatment status) versus datasets with only continuous variables, along with ablation studies showing how different encoding strategies for categorical variables affect performance.

- Open Question: Can the learned Prompt Module in Gen-P-Tuning be interpreted to understand how it combines information across channels?
  - Basis in paper: [explicit] The discussion section explicitly raises this as an open question, noting that while attention weights of the Prompt Module could be visualized, there is debate on whether attention weights are interpretable, and suggesting synthetic experiments as a promising direction.
  - Why unresolved: The authors acknowledge that interpreting attention weights is controversial and have not yet conducted synthetic experiments to validate whether the Prompt Module recovers ground truth channel mixing strategies.
  - What evidence would resolve it: Results from synthetic experiments where ground truth channel mixing is known, demonstrating whether the Prompt Module can recover these mixing patterns, or a rigorous interpretability analysis showing which channels contribute to which aspects of the prompt across different tasks.

- Open Question: What is the optimal prompt size hyperparameter K for different types of time series tasks and datasets?
  - Basis in paper: [explicit] The paper presents an experiment showing that performance generally improves as prompt size increases for the MIMIC-III mortality prediction task, but does not systematically explore the relationship between prompt size and task characteristics across multiple datasets.
  - Why unresolved: The paper only tested a few prompt sizes on a limited set of tasks, without exploring whether there are systematic patterns in optimal prompt size selection based on dataset characteristics, task complexity, or foundation model architecture.
  - What evidence would resolve it: A comprehensive study across multiple diverse time series datasets showing how optimal prompt size varies with factors like number of channels, temporal resolution, task type (classification vs forecasting), and dataset size, potentially leading to guidelines or automated methods for prompt size selection.

## Limitations
- The method's effectiveness depends heavily on the quality of the frozen foundation model's pretraining and the capacity of the Prompt Module to capture cross-channel relationships
- Different Prompt Module architectures yield comparable results, raising questions about whether the specific architecture choice is truly unimportant or whether the evaluation tasks are not sufficiently challenging to distinguish between them
- The paper only demonstrates effectiveness on continuous clinical variables and public health data, without evaluating performance on categorical time-varying features common in healthcare

## Confidence
- **High Confidence**: The core mechanism of attaching channel-summarized prompts to each channel's input is technically sound and well-implemented
- **Medium Confidence**: The claim that Gen-P-Tuning is competitive with other parameter-efficient fine-tuning methods, as results vary significantly across tasks and the margins of difference are often small
- **Low Confidence**: The assertion that Gen-P-Tuning is particularly effective in low-data regimes, as the experiments only compare against a limited set of alternatives and don't systematically explore very small sample sizes

## Next Checks
1. **Systematic ablation study on prompt size and module capacity**: Vary K systematically from very small (K=1) to large values, and test Prompt Modules with increasing capacity to identify whether there's a clear optimal configuration or whether performance plateaus early.

2. **Extreme low-data regime evaluation**: Test Gen-P-Tuning with severely limited labeled data (e.g., 10-50 samples) compared to standard fine-tuning and LoRA to rigorously verify the claimed advantage in data-scarce settings.

3. **Cross-domain generalization test**: Evaluate whether foundation models pretrained on one type of time series (e.g., general time series data) can be effectively adapted to healthcare domains using Gen-P-Tuning, or whether healthcare-specific pretraining is essential for good performance.