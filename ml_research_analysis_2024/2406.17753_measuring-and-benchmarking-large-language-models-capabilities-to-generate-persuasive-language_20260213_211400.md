---
ver: rpa2
title: Measuring and Benchmarking Large Language Models' Capabilities to Generate
  Persuasive Language
arxiv_id: '2406.17753'
source_url: https://arxiv.org/abs/2406.17753
tags:
- persuasive
- language
- more
- text
- less
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper measures and benchmarks large language models' (LLMs)
  capabilities to generate persuasive language. It constructs a new dataset, Persuasive-Pairs,
  of 2,697 short text pairs annotated for relative persuasive language on a six-point
  scale.
---

# Measuring and Benchmarking Large Language Models' Capabilities to Generate Persuasive Language

## Quick Facts
- arXiv ID: 2406.17753
- Source URL: https://arxiv.org/abs/2406.17753
- Reference count: 40
- Key outcome: This paper measures and benchmarks LLMs' capabilities to generate persuasive language using a new dataset and regression model

## Executive Summary
This paper addresses the challenge of measuring and benchmarking large language models' (LLMs) capabilities to generate persuasive language. The researchers construct a new dataset called Persuasive-Pairs containing 2,697 short text pairs annotated for relative persuasive language on a six-point scale. They train a regression model on this data to predict persuasive language scores for new text pairs, enabling systematic benchmarking of LLMs across different settings and domains. The analysis reveals that different personas in LLaMA3's system prompt significantly affect persuasive language generation, even when only instructed to paraphrase, and that LLMs tend to reduce persuasive elements when simply asked to paraphrase without explicit persuasion instructions.

## Method Summary
The researchers constructed a novel dataset called Persuasive-Pairs consisting of pairs of short texts and their rewrites by LLMs to amplify or diminish persuasive language. Human annotators then scored these pairs on a six-point relative scale for persuasive language. A DeBERTa-v3 regression model was trained on this data to predict persuasion scores for new text pairs. The model was evaluated using cross-validation to test generalization across domains. The researchers then used this model to benchmark LLMs by having them paraphrase texts under different system prompt conditions and comparing the predicted persuasion scores.

## Key Results
- Different personas in LLaMA3's system prompt significantly change persuasive language generation, even when only instructed to paraphrase
- When LLMs are merely asked to paraphrase (without instruction on persuasiveness), they tend to reduce the degree of persuasive language in already persuasive text
- The regression model trained on relative persuasiveness scores generalizes across domains and enables benchmarking of new LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The regression model generalizes across domains because it learns relative persuasiveness scores rather than absolute persuasiveness categories
- Mechanism: By training on pairwise comparisons and predicting relative differences, the model avoids the need for domain-specific persuasion categories and captures universal linguistic features of persuasion
- Core assumption: Persuasive language has common linguistic features across different domains and genres
- Evidence anchors:
  - [abstract] "We construct the new dataset PERSUASIVE -PAIRS of pairs of a short text and its rewrite by an LLM to amplify or diminish persuasive language"
  - [section] "The model allows us to score and benchmark new LLMs in different settings, e.g. varying the prompt and system prompt, and on various texts and domains"
- Break condition: If persuasive language features are domain-specific rather than universal, the model would fail to generalize

### Mechanism 2
- Claim: Different personas in system prompts significantly affect persuasive language generation even when not explicitly instructed to be persuasive
- Mechanism: The persona setting acts as a contextual frame that influences the language style and rhetorical choices of the LLM, affecting persuasive elements like emotional appeals, credibility markers, and rhetorical devices
- Core assumption: LLMs incorporate persona context into their generation process beyond just following explicit instructions
- Evidence anchors:
  - [abstract] "In our analysis, we find that different 'personas' in LLaMA3's system prompt change persuasive language substantially, even when only instructed to paraphrase"
  - [section] "We observe significant differences in persuasive language use depending on whether the system prompt was set as a 'right-wing' or 'centre-right' politician"
- Break condition: If LLMs only follow explicit instructions and ignore persona context, persona changes would not affect persuasive language generation

### Mechanism 3
- Claim: Human annotators can reliably judge relative persuasiveness between text pairs even without domain-specific persuasion taxonomies
- Mechanism: Humans have intuitive understanding of persuasive language that allows them to make relative judgments about which text is more persuasive, even across different domains and purposes
- Core assumption: Human intuition about persuasive language is consistent enough across annotators to enable reliable relative scoring
- Evidence anchors:
  - [abstract] "We multi-annotate the pairs on a relative scale for persuasive language"
  - [section] "We obtain a good level of human consensus in choosing the most persuasive language, and in scoring how much more, but with differences in sources and models â€“ we get an inter-annotator agreement on the ordinary 6-point scale using Krippendorfs alpha of 0.61"
- Break condition: If human intuitions about persuasive language are too subjective or inconsistent, relative scoring would not be reliable

## Foundational Learning

- Concept: Relative scoring vs absolute scoring
  - Why needed here: Absolute scoring requires defining universal categories of persuasive language, which is difficult across domains. Relative scoring allows comparing texts without needing to define absolute persuasiveness levels
  - Quick check question: Why did the researchers choose to use relative scoring rather than asking annotators to rate each text on an absolute persuasiveness scale?

- Concept: Cross-validation for model evaluation
  - Why needed here: The dataset is limited in size, and we want to ensure the model generalizes to new domains and models rather than just memorizing training data patterns
  - Quick check question: What evaluation method did the researchers use to test whether their model would generalize to new domains and models?

- Concept: Mann-Whitney U test for non-parametric comparison
  - Why needed here: The distributions of predicted persuasion scores are not normally distributed, so parametric tests like t-tests would not be appropriate for comparing LLM performance
  - Quick check question: Why did the researchers choose the Mann-Whitney U test instead of a t-test to compare distributions of predicted persuasion scores?

## Architecture Onboarding

- Component map: Source texts -> LLM generation (more/less persuasive) -> Human annotation -> Dataset creation -> DeBERTa-v3 regression model -> Persuasion score prediction -> New text samples -> LLM paraphrasing -> Score prediction -> Statistical comparison

- Critical path: The most time-consuming step is human annotation, which requires careful quality control and multiple annotators per sample. The model training is relatively fast once annotated data is available

- Design tradeoffs: The researchers chose relative scoring over absolute scoring to avoid domain-specific persuasion taxonomies, but this means the model can only compare pairs rather than rate absolute persuasiveness levels

- Failure signatures: Low inter-annotator agreement (Krippendorff's alpha below 0.4) would indicate the relative scoring approach isn't working. Poor generalization in cross-validation would suggest the model is overfitting to specific domains

- First 3 experiments:
  1. Test the model on pairs from a new domain not in the training data to verify generalization
  2. Compare persuasion scores when using different persona system prompts with the same instruction to see if persona affects scores
  3. Test whether relaxing text length restrictions affects persuasion scores for a given LLM

## Open Questions the Paper Calls Out

None

## Limitations

- Dataset size and representativeness may limit the model's ability to capture the full complexity of persuasive language across diverse domains
- The specific mechanism by which persona context influences persuasive language generation is not fully explained
- It's unclear whether the observed persona effects would replicate with other LLM architectures beyond LLaMA3

## Confidence

**High Confidence**:
- The regression model can predict relative persuasiveness scores for text pairs
- Human annotators can reliably judge relative persuasiveness between text pairs (Krippendorff's alpha = 0.61)
- LLMs tend to reduce persuasive language when merely asked to paraphrase

**Medium Confidence**:
- The model generalizes across domains due to learning relative rather than absolute scores
- Different personas in system prompts significantly affect persuasive language generation
- The dataset construction methodology produces reliable pairwise comparisons

**Low Confidence**:
- The specific mechanism by which persona context influences persuasive language generation
- Whether the observed effects would replicate with other LLM architectures
- The extent to which the model captures domain-specific persuasive strategies

## Next Checks

1. Evaluate the model on persuasive text pairs from entirely new domains (e.g., scientific abstracts, legal briefs) that were not represented in the original training data to verify true cross-domain generalization

2. Test whether the persona-based persuasion effects replicate across multiple LLM architectures (e.g., GPT-4, Claude, Mistral) and different persona configurations to determine if this is a universal phenomenon

3. Conduct a targeted study measuring inter-annotator agreement on texts with known persuasive strategies to better understand the reliability of human judgments across different types of persuasive language