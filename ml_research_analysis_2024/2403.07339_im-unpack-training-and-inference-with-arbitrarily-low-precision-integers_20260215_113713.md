---
ver: rpa2
title: 'IM-Unpack: Training and Inference with Arbitrarily Low Precision Integers'
arxiv_id: '2403.07339'
source_url: https://arxiv.org/abs/2403.07339
tags:
- bit-width
- matrix
- integer
- training
- integers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of achieving high efficiency
  in deep learning models by using low bit-width integer matrix multiplication (GEMM)
  while maintaining accuracy. The authors verify that integers are sufficient for
  all GEMMs in Transformer-based models for both training and inference, without needing
  sophisticated techniques.
---

# IM-Unpack: Training and Inference with Arbitrarily Low Precision Integers

## Quick Facts
- arXiv ID: 2403.07339
- Source URL: https://arxiv.org/abs/2403.07339
- Authors: Zhanpeng Zeng; Karthikeyan Sankaralingam; Vikas Singh
- Reference count: 40
- This paper addresses the challenge of achieving high efficiency in deep learning models by using low bit-width integer matrix multiplication (GEMM) while maintaining accuracy.

## Executive Summary
This paper tackles the challenge of achieving high efficiency in deep learning models by using low bit-width integer matrix multiplication (GEMM) while maintaining accuracy. The authors verify that integers are sufficient for all GEMMs in Transformer-based models for both training and inference, without needing sophisticated techniques. However, a few heavy-hitter entries in matrices make it difficult to achieve efficiency gains using only low bit-width integers. To address this issue, the authors propose IM-Unpack, an algorithm that unpacks a matrix with large integer entries into a larger matrix with entries within the representable range of arbitrarily low bit-width integers. This allows for exact results using only low bit-width integer GEMMs, with a small overhead.

## Method Summary
The paper presents IM-Unpack, an algorithm that enables exact matrix multiplication using only low bit-width integer GEMMs. The method works by first quantizing floating point matrices to integers using a percentile-based scaling approach (RTN quantization). For matrices containing heavy hitter entries that exceed the representable range of low bit-width integers, IM-Unpack decomposes these large values into sums of smaller integers that fit within the low bit-width range. The GEMM is then performed using these unpacked matrices, followed by scaling and accumulation to obtain the exact result. The approach is validated on various Transformer-based models including LLaMA and ViT, demonstrating comparable performance to full-precision counterparts while significantly improving efficiency.

## Key Results
- IM-Unpack achieves parity with full-precision GEMMs for Transformer-based models without sophisticated techniques
- Experimental results on LLaMA and ViT models show comparable performance to full-precision counterparts
- The unpacking overhead remains small (below 2.4x) while enabling the use of arbitrarily low bit-width integers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Integer GEMMs can achieve parity with floating point counterparts without sophisticated techniques when the low bit-width restriction is removed.
- Mechanism: By allowing integers to represent the full range of matrix entries rather than restricting to low bit-width integers, rounding errors are minimized and the exact result of the original GEMM can be obtained.
- Core assumption: The rounding error from converting floating point to integers is small enough when using full integer range, and does not significantly impact model performance.
- Evidence anchors:
  - [abstract] "No sophisticated techniques are needed... integers are sufficient for all GEMMs need – for both training and inference stages, and can achieve parity with floating point counterparts."
  - [section 2.1] "we first verify/check that the aforementioned hypothesis – that integer GEMM may work – is true"
  - [corpus] Weak evidence; related works focus on mixed-precision and quantization but not direct integer GEMM parity verification.

### Mechanism 2
- Claim: The majority of matrix entries can be represented by low bit-width integers, but a few heavy hitter entries prevent achieving efficiency gains using only low bit-width GEMMs.
- Mechanism: Most entries in matrices are small and can be efficiently represented with low bit-width integers. However, a small number of large entries (heavy hitters) require higher bit-widths, preventing the use of efficient low bit-width arithmetic throughout.
- Core assumption: The distribution of matrix entries has a long tail of large values that cannot be represented by low bit-width integers.
- Evidence anchors:
  - [abstract] "we find that while a large majority of entries in matrices (encountered in such models) can be easily represented by low bit-width integers, the existence of a few heavy hitter entries make it difficult to achieve efficiency gains via the exclusive use of low bit-width GEMMs alone."
  - [section 3] "we calculate the ratio α100(·)/α95(·) between the maximum and 95th-percentile of the magnitude of each matrix... We see extremely large values across both training and inference"
  - [corpus] Weak evidence; related works mention outliers but do not quantify their impact on bit-width requirements.

### Mechanism 3
- Claim: IM-Unpack can unpack a matrix with large integer entries into a larger matrix with entries within the representable range of arbitrarily low bit-width integers, enabling exact results using only low bit-width integer GEMMs.
- Mechanism: Large integers are decomposed into sums of smaller integers that fit within the low bit-width range. The GEMM is then performed using these unpacked matrices, followed by scaling and accumulation to obtain the exact result.
- Core assumption: Any integer can be represented as a sum of smaller integers scaled by powers of the base (e.g., 2^b), and the additional computational overhead is acceptable.
- Evidence anchors:
  - [abstract] "we develop a simple algorithm, Integer Matrix Unpacking (IM-Unpack), to unpack a matrix with large integer entries into a larger matrix whose entries all lie within the representable range of arbitrarily low bit-width integers."
  - [section 4] "our simple procedure, IM-Unpack, allows representing heavy hitters using low bit-width integers... The key outcome is that the calculation can be carried out entirely using low bit-width integer arithmetic"
  - [corpus] Weak evidence; no directly comparable unpacking algorithm found in related works.

## Foundational Learning

- Concept: General Matrix Multiply (GEMM) operation
  - Why needed here: GEMM is the central operation in deep learning models and the primary target for efficiency improvements through low bit-width integer arithmetic.
  - Quick check question: What is the mathematical definition of GEMM for matrices A (n×d) and B (h×d)?

- Concept: Quantization and rounding errors
  - Why needed here: Understanding how rounding to integers introduces errors and how restricting to low bit-width integers exacerbates these errors is crucial for appreciating the problem IM-Unpack addresses.
  - Quick check question: How does the Rounding To Nearest (RTN) quantization scheme map floating point values to integers, and what is the impact on the representable range?

- Concept: Bit-width and representable range of integers
  - Why needed here: The relationship between bit-width and the range of representable integers determines the efficiency and accuracy of low bit-width integer GEMMs.
  - Quick check question: For a signed integer with b bits, what is the range of values that can be represented, and how does this affect the choice of b for GEMM operations?

## Architecture Onboarding

- Component map: Input matrices A and B -> Quantization step -> IM-Unpack algorithm -> Low bit-width GEMM -> Scaling and accumulation -> Output matrix C
- Critical path: Quantization → IM-Unpack → Low bit-width GEMM → Scaling and accumulation → Output matrix C
- Design tradeoffs:
  - Higher bit-width allows more accurate representation but reduces efficiency gains
  - Larger unpacked matrices increase computational overhead but enable the use of low bit-width arithmetic
  - Choice of unpacking strategy (row-wise, column-wise, or both) affects the efficiency and effectiveness of the algorithm
- Failure signatures:
  - Significant performance degradation when using low bit-width integers due to rounding errors
  - Large increase in computational overhead from unpacking, negating efficiency gains
  - Inability to represent certain matrix entries even after unpacking, requiring fallback to higher bit-widths
- First 3 experiments:
  1. Verify parity of integer GEMMs with floating point counterparts for a simple model (e.g., a small Transformer) without low bit-width restrictions
  2. Measure the distribution of matrix entries and identify heavy hitters that prevent the use of low bit-width integers
  3. Apply IM-Unpack to a matrix with known heavy hitters and verify that the exact GEMM result can be obtained using only low bit-width integer arithmetic

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of IM-Unpack scale with increasing model size and complexity, especially for very large language models (e.g., GPT-4 scale)?
- Basis in paper: [inferred] The paper demonstrates effectiveness on LLaMA-7B, LLaMA-13B, and Mistral-7B, but doesn't explore larger models.
- Why unresolved: The computational cost of testing extremely large models would be prohibitive, and the paper focuses on demonstrating the concept rather than exhaustive scaling.
- What evidence would resolve it: Empirical results showing performance degradation or maintenance as model parameters increase by orders of magnitude.

### Open Question 2
- Question: Can the unpacking overhead be further reduced by incorporating more sophisticated strategies that exploit specific patterns in the distribution of heavy-hitters?
- Basis in paper: [explicit] The paper acknowledges that the current unpacking strategies are not optimal for matrices where outliers concentrate on the diagonal (like self-attention matrices).
- Why unresolved: Developing and validating new strategies would require extensive experimentation and potentially new algorithmic insights.
- What evidence would resolve it: A new unpacking algorithm that consistently achieves lower unpack ratios than the current methods across diverse model architectures.

### Open Question 3
- Question: How does IM-Unpack perform when applied to other types of neural network architectures beyond Transformers, such as convolutional neural networks (CNNs) or recurrent neural networks (RNNs)?
- Basis in paper: [inferred] The paper focuses exclusively on Transformer-based models, leaving the applicability to other architectures unexplored.
- Why unresolved: Different architectures have different computational patterns and may exhibit different characteristics in their weight distributions.
- What evidence would resolve it: Successful application of IM-Unpack to achieve similar efficiency gains on CNNs or RNNs without significant modifications to the algorithm.

## Limitations

- The unpacking overhead could become prohibitive for matrices with extreme outlier distributions or specific patterns, potentially negating efficiency gains
- The algorithm's effectiveness depends on maintaining small unpacking ratios (below 2.4x), which may not hold for all model architectures
- While demonstrated on Transformer-based models, the generalizability to other neural network architectures remains uncertain

## Confidence

**Confidence Level: Low** - The paper's claims about achieving parity with floating point GEMMs using integer-only operations are based on empirical verification rather than theoretical guarantees. The verification is limited to specific Transformer-based models and may not generalize to all deep learning architectures.

**Confidence Level: Medium** - The IM-Unpack algorithm's effectiveness depends on the unpacking ratio remaining small (below 2.4x). For matrices with extreme outlier distributions or specific patterns, the unpacking overhead could become prohibitive, potentially negating the efficiency gains from using low bit-width integers.

**Confidence Level: Medium** - While the paper demonstrates effectiveness on LLaMA and ViT models, the generalizability to other model architectures (CNNs, RNNs, multimodal models) remains uncertain. The algorithm's performance characteristics may vary significantly depending on the matrix structure and sparsity patterns of different model types.

## Next Checks

1. **Cross-Architecture Generalization Test**: Apply IM-Unpack to diverse model architectures beyond Transformers, including CNNs (ResNet, EfficientNet), RNNs (LSTM, GRU), and multimodal models. Measure unpacking ratios and accuracy preservation across at least 10 different architectures to establish broader applicability.

2. **Extreme Distribution Analysis**: Systematically generate matrices with controlled heavy hitter distributions (varying outlier ratios from 0.1% to 10%, different magnitude scales) and measure IM-Unpack's performance degradation. Establish clear thresholds where the algorithm becomes impractical and document the conditions that lead to high unpacking ratios.

3. **Training Stability Validation**: Conduct extended training experiments using IM-Unpack for all GEMM operations in the backward pass, not just inference. Monitor for training divergence, convergence speed differences, and final accuracy compared to full-precision training across multiple random seeds and learning rate schedules.