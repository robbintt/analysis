---
ver: rpa2
title: 'SemRoDe: Macro Adversarial Training to Learn Representations That are Robust
  to Word-Level Attacks'
arxiv_id: '2403.18423'
source_url: https://arxiv.org/abs/2403.18423
tags:
- adversarial
- sample
- training
- word
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a macro adversarial training strategy called
  SemRoDe to enhance the robustness of language models against word-level attacks.
  The key idea is to learn a robust representation that bridges the base and adversarial
  domains by incorporating a distance-based objective to align their high-level output
  features.
---

# SemRoDe: Macro Adversarial Training to Learn Representations That are Robust to Word-Level Attacks

## Quick Facts
- **arXiv ID:** 2403.18423
- **Source URL:** https://arxiv.org/abs/2403.18423
- **Reference count:** 40
- **Key outcome:** Proposes SemRoDe, a macro adversarial training strategy that uses distance-based objectives to align base and adversarial feature distributions, achieving state-of-the-art robustness across three datasets with significant after-attack accuracy improvements.

## Executive Summary
This paper addresses the vulnerability of language models to word-level adversarial attacks by proposing SemRoDe, a macro adversarial training approach that learns robust representations through distribution alignment. The method generates adversarial samples via word substitutions and incorporates a distance-based regularizer to align the high-level feature distributions of base and adversarial samples. Experiments demonstrate significant improvements in robustness across multiple attack methods and datasets compared to strong baselines like InfoBERT, FreeLB++, and DSRM.

## Method Summary
SemRoDe employs a macro adversarial training strategy that generates adversarial samples offline using word substitution attacks, then trains the model with a combined loss of cross-entropy and a distance-based regularizer (MMD, CORAL, or Optimal Transport). The regularizer aligns the high-level feature distributions of base and adversarial samples, forcing the model to learn invariant representations that generalize across attack methods. The approach is evaluated on BERT and RoBERTa models across three classification datasets, showing significant improvements in after-attack accuracy.

## Key Results
- Achieves state-of-the-art robustness across three datasets (MR, SST-2, AG-News) compared to InfoBERT, FreeLB++, and DSRM
- MMD regularizer consistently outperforms CORAL and Optimal Transport in robustness gains
- Significant improvements in after-attack accuracy while maintaining clean accuracy
- Effective across different word embeddings and attack generation methods

## Why This Works (Mechanism)

### Mechanism 1
Word substitutions create a large domain shift in the feature space, measurable by high Wasserstein distance. The substitution forms a new adversarial domain distribution distinct from the base domain, allowing the distance-based regularizer to effectively align them. Core assumption: adversarial samples form a separate distribution measurable by optimal transport.

### Mechanism 2
Minimizing the distance between base and adversarial feature distributions improves generalization to unseen attacks. The distance regularizer acts as a domain alignment objective, pulling adversarial distributions closer to base so the classifier learns invariant features. Core assumption: aligning statistical properties leads to robust internal representations.

### Mechanism 3
High-quality word embeddings for attack generation lead to better alignment than embedding-space perturbations. Word substitution attacks explore broader attack spaces than small perturbations, making base/adversarial separation more pronounced and amenable to alignment. Core assumption: substitution quality affects the alignment objective's effectiveness.

## Foundational Learning

- **Wasserstein distance and optimal transport**: Needed to measure distributional shift between base and adversarial domains. Quick check: What is the difference between MMD and Wasserstein distance in measuring distributional shift?

- **Adversarial training and saddle-point optimization**: Needed to understand how macro adversarial training differs from standard embedding perturbation. Quick check: How does the macro adversarial objective differ from standard embedding perturbation in adversarial training?

- **Domain adaptation and feature alignment**: Needed to understand how distance-based regularizers align feature distributions. Quick check: What is the role of second-order statistics (CORAL) vs. mean statistics (MMD) in feature alignment?

## Architecture Onboarding

- **Component map:** Tokenizer → Base model → Pooling layer → Classifier head; Separate adversarial sample generator (TextFooler, BERTAttack, etc.); Distance regularizer module (MMD/CORAL/OT) operating on pooled features

- **Critical path:** Forward pass with base samples → Compute loss + distance regularizer; Forward pass with adversarial samples → Compute adversarial loss + distance regularizer; Backward pass updates weights to minimize both classification and distance objectives

- **Design tradeoffs:** Choice of distance metric (MMD stable/effective, OT expressive/unstable, CORAL simple/less powerful); Regularizer strength λ (too high causes overfitting, too low ineffective); Attack strength ϵ (controls semantic similarity, affects alignment quality)

- **Failure signatures:** High variance in training loss (likely OT instability); No improvement in after-attack accuracy (distance regularizer too weak/misaligned); Drop in clean accuracy (regularizer too strong/conflicting with classification objective)

- **First 3 experiments:** 1) Run with MMD regularizer only (λ > 0, no adversarial samples) to check distance objective effects; 2) Compare MMD vs CORAL vs OT on small dataset to identify most stable metric; 3) Test with different λ values (0.1, 1, 5) to find optimal regularizer strength

## Open Questions the Paper Calls Out

- **How can the distribution alignment technique be extended to generative models to improve their robustness against adversarial attacks?** The paper mentions this as a limitation since it focuses on sequence classification tasks and does not explore generative models.

- **How does the choice of word embeddings (e.g., GloVe, Counter-Fitted GloVe) impact the effectiveness of the distribution alignment technique?** While various embeddings are tested, the paper does not provide detailed analysis of how embedding choice impacts alignment process and robustness gains.

- **How does the distribution alignment technique perform against adaptive attacks that specifically target the alignment process?** The paper only provides initial results against adaptive attacks using character and punctuation insertions, not exploring other adaptive attack strategies.

## Limitations

- The assumption that word substitution attacks create a statistically distinct domain distribution may not hold for all attack methods or datasets
- Semantic similarity threshold of 0.5 may be too permissive, allowing attacks that significantly alter meaning while still being considered "robust"
- Optimal transport implementation details are not specified, raising concerns about stability and reproducibility

## Confidence

- **High:** The core mechanism of using distance-based regularizers to align base and adversarial feature distributions
- **Medium:** The claim that MMD outperforms other distance metrics in this setting
- **Medium:** The generalizability across different word embeddings and attack methods

## Next Checks

1. Conduct ablation studies comparing MMD, CORAL, and OT across multiple datasets to verify the claimed superiority of MMD
2. Test the method with different semantic similarity thresholds (0.3, 0.5, 0.7) to understand the tradeoff between robustness and semantic preservation
3. Evaluate performance on additional attack methods not seen during training to test true generalization capabilities