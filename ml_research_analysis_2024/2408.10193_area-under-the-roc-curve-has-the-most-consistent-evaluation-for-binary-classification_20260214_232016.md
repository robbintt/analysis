---
ver: rpa2
title: Area under the ROC Curve has the Most Consistent Evaluation for Binary Classification
arxiv_id: '2408.10193'
source_url: https://arxiv.org/abs/2408.10193
tags:
- prevalence
- metrics
- evaluation
- different
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the consistency of 18 binary classification
  metrics across different prevalence levels using statistical simulation. The research
  examines how well metrics rank five common machine learning models and a random
  guess model as data prevalence varies from 8% to 83%.
---

# Area under the ROC Curve has the Most Consistent Evaluation for Binary Classification

## Quick Facts
- arXiv ID: 2408.10193
- Source URL: https://arxiv.org/abs/2408.10193
- Authors: Jing Li
- Reference count: 22
- This study evaluates the consistency of 18 binary classification metrics across different prevalence levels using statistical simulation, finding that AUC has the smallest variance in model rankings (0.271) and should be preferred for consistent model comparison across varying prevalence levels.

## Executive Summary
This study systematically evaluates 18 binary classification metrics across varying prevalence levels to determine which provides the most consistent model ranking. Using statistical simulation, the research examines how well different metrics rank five common machine learning models and a random guess model as data prevalence varies from 8% to 83%. The results demonstrate that metrics heavily influenced by prevalence (like F1 score, precision, and recall) perform poorly at ranking models consistently, while prevalence-independent metrics like AUC and MCC maintain stable rankings across all prevalence levels. AUC emerges as the most reliable metric for comparing model performance across different data distributions.

## Method Summary
The study uses the Broward County recidivism dataset (6,214 observations) and applies 5 ML models (GLM, RF, KNN, LDA, GBM) plus a random guess model. Models are trained using 10-fold cross-validation with 80/20 train/test splits. Prevalence levels are manipulated by iteratively adjusting the balance between positive and negative classes while maintaining constant sample size. All 18 evaluation metrics are calculated across test sets with varying prevalence levels (8% to 83%), and variance in model rankings is computed to determine consistency.

## Key Results
- AUC had the smallest variance in model rankings across prevalence levels (0.271), making it the most reliable metric for consistent model comparison
- Prevalence-dependent metrics like F1 score, precision, and recall show poor consistency in model rankings across different prevalence levels
- MCC provides the second most stable rankings and is best for assessing overall label alignment and accuracy for probability alignment
- Accuracy, precision, F1 score, and Fβ scores are all heavily influenced by prevalence and should be avoided when consistent comparison across varying prevalence levels is needed

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AUC maintains stable rankings across varying prevalence levels because it evaluates performance across all classification thresholds rather than a single fixed threshold.
- Mechanism: By integrating performance across all possible decision thresholds, AUC captures the full trade-off between true positive and false positive rates, which are inversely related to each other. This comprehensive evaluation smooths out the effects of class imbalance that would otherwise distort metrics based on a single threshold.
- Core assumption: The relationship between true positive rate and false positive rate remains relatively stable across different prevalence levels when evaluating the same model.
- Evidence anchors:
  - [abstract] "AUC which takes all decision thresholds into account when evaluating models has the smallest variance in evaluating individual models and smallest variance in ranking of a set of models."
  - [section 2.3] "As Area under the ROC Curve (AUC) assesses the performance of a classifier at all possible classification thresholds, as a result, it cannot be constructed from a single confusion matrix based on one classification threshold"
- Break condition: If the model's decision function changes its shape significantly across different prevalence levels, the threshold-invariant property of AUC might not hold.

### Mechanism 2
- Claim: Prevalence-dependent metrics like F1 score and precision fluctuate because they depend on both the true positive rate and the prevalence level.
- Mechanism: These metrics have denominators that include terms directly proportional to prevalence (like TP + FP for precision). When prevalence changes, both the numerator and denominator change, but not necessarily in the same proportion, leading to unstable metric values.
- Core assumption: The mathematical expressions for these metrics contain terms that are directly proportional to prevalence.
- Evidence anchors:
  - [section 2.2] "PPV = TPR · ϕ / (TPR · ϕ + (1 − TNR) · (1 − ϕ))" showing prevalence ϕ directly in the formula
  - [section 3.1] "As TPR, TNR, PPV and NPV mostly concern prediction accuracy for one class only, they have a monotonic relationship with prevalence"
- Break condition: If the model's true positive rate and true negative rate change in perfect proportion to prevalence changes, some stability might be maintained.

### Mechanism 3
- Claim: MCC provides stable rankings because it measures the correlation between predicted and actual class labels, which is less sensitive to class imbalance.
- Mechanism: MCC is essentially a correlation coefficient that measures the quality of binary classifications. Correlation measures are designed to be scale-invariant, which helps them maintain stability across different prevalence levels.
- Core assumption: Correlation-based metrics are inherently less sensitive to changes in class distribution than metrics based on absolute counts or proportions.
- Evidence anchors:
  - [abstract] "Matthew's correlation coefficient as a more strict measure of model performance has the second smallest variance."
  - [section 2.1] "Matthew's correlation coefficient (MCC): MCC = (TP · TN − FP · FN) / √((TP + FP) · (TP + FN) · (TN + FP) · (TN + FN))"
- Break condition: If the correlation between predicted and actual labels changes systematically with prevalence, MCC stability would break down.

## Foundational Learning

- Concept: Prevalence and its effect on binary classification metrics
  - Why needed here: The paper's core finding is that different metrics respond differently to changes in prevalence, so understanding this relationship is fundamental to interpreting the results.
  - Quick check question: If a dataset has 90% negative cases and 10% positive cases, which class dominates the accuracy metric?

- Concept: Confusion matrix and its derived metrics
  - Why needed here: All 18 metrics evaluated in the paper are derived from the confusion matrix, so understanding how TP, FP, TN, and FN relate to each metric is essential.
  - Quick check question: What are the four possible outcomes when a binary classifier makes predictions on a test set?

- Concept: Statistical simulation for evaluating metric consistency
  - Why needed here: The paper uses simulation to systematically vary prevalence while keeping other factors constant, which is key to understanding how the results were generated.
  - Quick check question: Why might simulation be preferred over using real datasets when studying the effect of prevalence on metric consistency?

## Architecture Onboarding

- Component map: Data simulation → Model training (with cross-validation) → Metric calculation at each prevalence level → Ranking computation → Variance analysis
- Critical path: Data simulation → Model training (with cross-validation) → Metric calculation at each prevalence level → Ranking computation → Variance analysis
- Design tradeoffs: The simulation approach provides comprehensive coverage of prevalence levels but may not capture all real-world data complexities. Using multiple ML algorithms provides robustness but increases computational cost.
- Failure signatures: If variance calculations show high values across all metrics, this might indicate issues with the simulation design or model training process. If only certain metrics show high variance, this supports the paper's hypothesis about prevalence sensitivity.
- First 3 experiments:
  1. Replicate the crime recidivism case study to verify the ranking patterns observed in Table 2
  2. Run the simulation with just one ML model and all 18 metrics to verify the prevalence-accuracy patterns shown in Figure 1
  3. Compare the variance results from the simulation with those reported in Table 3 to ensure the ranking consistency analysis is working correctly

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do model evaluation metrics behave in multi-class classification tasks compared to binary classification?
- Basis in paper: [explicit] The paper suggests future research could explore similar analysis for datasets with multi-class outcomes, implying this area remains unexplored.
- Why unresolved: The current study focuses exclusively on binary classification, leaving multi-class scenarios unexamined.
- What evidence would resolve it: Empirical studies applying the same simulation approach to multi-class datasets, comparing metric consistency across varying class distributions.

### Open Question 2
- Question: Can novel classification measures be developed that account for sample size, prevalence, TPR, and TNR while improving consistency across prevalence levels?
- Basis in paper: [explicit] The paper concludes by suggesting developing novel classification measures that take these four quantities into account as a viable path for future research.
- Why unresolved: While the paper identifies these four quantities as foundational, it does not propose or test new metrics based on them.
- What evidence would resolve it: Creation and validation of new metrics using these four quantities, followed by simulation studies comparing their consistency to existing metrics.

### Open Question 3
- Question: How do different machine learning model types influence metric consistency across prevalence levels?
- Basis in paper: [inferred] The paper notes that GBM and Random Forest models appear to have less variance in rankings compared to GLM, KNN, or LDA, suggesting model type may affect metric consistency.
- Why unresolved: The paper only briefly mentions this observation without systematic investigation of how different model architectures impact metric performance.
- What evidence would resolve it: Comparative studies testing various model types (e.g., neural networks, SVMs, ensemble methods) across diverse prevalence scenarios to quantify their effect on metric consistency.

## Limitations

- The simulation-based approach, while systematic, may not fully capture real-world data complexities and model behavior
- Limited to 5 ML algorithms and one specific dataset, potentially limiting generalizability
- The paper doesn't address how these findings might change with multi-class problems or imbalanced cost scenarios

## Confidence

- High Confidence: AUC as the most consistent metric for model ranking across prevalence levels - supported by clear variance analysis and theoretical foundation
- Medium Confidence: MCC as the second most stable metric - while variance results support this, the underlying mechanism could be explored further
- Low Confidence: Specific prevalence manipulation algorithm details - not fully specified in the paper, which could affect reproducibility

## Next Checks

1. **Replicate on synthetic data**: Generate controlled datasets with known properties to verify that AUC consistently maintains stable rankings while prevalence-dependent metrics fluctuate
2. **Test with additional algorithms**: Include deep learning models and other ML algorithms not in the original study to assess generalizability of findings
3. **Cross-dataset validation**: Apply the same evaluation framework to multiple binary classification datasets from different domains to confirm the robustness of AUC's consistency advantage