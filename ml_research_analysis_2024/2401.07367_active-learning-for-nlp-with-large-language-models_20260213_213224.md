---
ver: rpa2
title: Active Learning for NLP with Large Language Models
arxiv_id: '2401.07367'
source_url: https://arxiv.org/abs/2401.07367
tags:
- samples
- gpt-3
- learning
- accuracy
- active
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the accuracy and cost of using large language
  models (LLMs) like GPT-3.5 and GPT-4 to label samples in active learning for NLP
  tasks. A consistency-based strategy is proposed to select samples that are potentially
  incorrectly labeled, allowing human annotations to be used for those samples.
---

# Active Learning for NLP with Large Language Models

## Quick Facts
- arXiv ID: 2401.07367
- Source URL: https://arxiv.org/abs/2401.07367
- Authors: Xuesong Wang
- Reference count: 20
- Primary result: Mixed annotations (LLM + human) achieve similar or better results than human-only annotations on AG's News and Rotten Tomatoes datasets

## Executive Summary
This paper investigates the accuracy and cost of using large language models (LLMs) like GPT-3.5 and GPT-4 to label samples in active learning for NLP tasks. A consistency-based strategy is proposed to select samples that are potentially incorrectly labeled, allowing human annotations to be used for those samples. The accuracy of active learning models under three query strategies is reported on three text classification datasets. On AG's News and Rotten Tomatoes, models trained with the mixed annotation strategy (LLM + human) achieve similar or better results compared to those with human annotations only. The method reveals great potential for LLMs as annotators in terms of accuracy and cost efficiency in active learning settings.

## Method Summary
The study uses GPT-3.5 and GPT-4 to annotate samples in active learning for NLP, with a consistency-based strategy to select potentially incorrectly labeled samples for human annotation. Three text classification datasets (AG's News, TREC-6, and Rotten Tomatoes) are used, with three query strategies (random, least confidence, and breaking ties) evaluated. The DistilRoBERTa model is trained with mixed annotations (LLM + human) and compared to human-only annotations in terms of accuracy and cost.

## Key Results
- GPT-4 achieves higher accuracy than GPT-3.5 on difficult text classification tasks like TREC-6
- Consistency-based selection identifies samples where LLMs are uncertain by detecting inconsistent outputs across multiple generations
- Mixed annotations (LLM + human) achieve similar or better results than human-only annotations on AG's News and Rotten Tomatoes datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 achieves higher accuracy than GPT-3.5 on difficult text classification tasks like TREC-6.
- Mechanism: GPT-4's larger model size and training provides better understanding of complex linguistic structures and context, leading to more accurate classifications on challenging datasets.
- Core assumption: GPT-4's architectural improvements directly translate to better performance on harder NLP tasks.
- Evidence anchors:
  - [abstract] "On AG's News and Rotten Tomatoes, the models trained with the mixed annotation strategy achieves similar or better results compared to that with human annotations."
  - [section] "On the other datasets, GPT-4 has slightly lower inconsistency rate than GPT-3.5."
  - [corpus] Weak evidence - corpus doesn't contain direct comparisons of GPT-4 vs GPT-3.5 on difficult tasks.
- Break condition: If GPT-4's cost becomes prohibitively high relative to accuracy gains, or if it performs worse on certain datasets.

### Mechanism 2
- Claim: Consistency-based selection identifies samples where LLMs are uncertain.
- Mechanism: By generating multiple responses with temperature variation, samples showing inconsistent outputs across generations are flagged as uncertain and sent for human annotation.
- Core assumption: Inconsistent outputs from the same LLM indicate model uncertainty about the correct label.
- Evidence anchors:
  - [abstract] "A consistency-based strategy is proposed to select samples that are potentially incorrectly labeled so that human annotations can be used for those samples in AL settings"
  - [section] "If there is any inconsistency among the n results on any samples, those samples will be marked as inconsistent"
  - [corpus] Weak evidence - corpus mentions related work but not specific consistency-based methods.
- Break condition: If the inconsistency detection threshold is too high (missing uncertain samples) or too low (over-flagging confident samples).

### Mechanism 3
- Claim: Mixed annotations (LLM + human) achieve similar or better results than human-only annotations.
- Mechanism: Using LLMs for the majority of annotations reduces cost while the consistency-based approach ensures humans correct the most uncertain cases, maintaining or improving accuracy.
- Core assumption: The cost savings from LLM annotations outweigh any minor accuracy loss, especially when human annotations correct the most uncertain cases.
- Evidence anchors:
  - [abstract] "On AG's News and Rotten Tomatoes, the models trained with the mixed annotation strategy achieves similar or better results compared to that with human annotations."
  - [section] "we have demonstrated the potential of the mixed annotations over the human annotations."
  - [corpus] Moderate evidence - related papers discuss LLM-in-the-loop active learning but not the specific mixed annotation approach.
- Break condition: If LLM annotations are consistently wrong on certain dataset types, or if the cost of human corrections exceeds savings from LLM annotations.

## Foundational Learning

- Concept: Active Learning Query Strategies
  - Why needed here: Understanding how different query strategies (random, least confidence, breaking ties) affect model performance is crucial for comparing the mixed annotation approach.
  - Quick check question: What is the key difference between least confidence and breaking ties query strategies?

- Concept: Text Classification Datasets
  - Why needed here: Familiarity with AG's News, TREC-6, and Rotten Tomatoes datasets is necessary to understand the experimental setup and results.
  - Quick check question: How many classes does the TREC-6 dataset have, and what type of text does it classify?

- Concept: Large Language Model In-context Learning
  - Why needed here: Understanding how GPT-3.5 and GPT-4 use demonstration examples to perform zero-shot or few-shot classification is key to the annotation approach.
  - Quick check question: What role do demonstration examples play in LLM-based annotation?

## Architecture Onboarding

- Component map:
  - Data Preparation: Dataset loading, initial stratified sampling
  - LLM Annotation: OpenAI Chat Completion API calls with demonstration examples
  - Consistency Check: Multiple generations with temperature variation
  - Active Learning Loop: Query strategy implementation, model training with mixed annotations
  - Evaluation: Accuracy calculation, cost tracking

- Critical path: Dataset → Initial Annotation (LLM) → Consistency Check → Mixed Annotation Set → Active Learning → Model Training → Evaluation

- Design tradeoffs:
  - Cost vs. Accuracy: More LLM annotations reduce cost but may decrease accuracy if not properly filtered
  - Consistency Check Parameters: Higher n and temperature improve uncertainty detection but increase token usage and cost
  - Query Strategy Choice: Different strategies may perform better with mixed annotations vs. human-only

- Failure signatures:
  - High inconsistency rate indicates LLM struggles with the dataset
  - Low accuracy improvement in active learning suggests the mixed annotation approach isn't effective
  - Unexpected cost increases may indicate inefficient token usage in consistency checks

- First 3 experiments:
  1. Test annotation accuracy and cost for GPT-3.5 and GPT-4 on a small subset of each dataset with different demonstration sizes
  2. Run consistency checks on LLM annotations to determine appropriate n and temperature parameters
  3. Implement and test the mixed annotation approach on one dataset with one query strategy before scaling to all datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of demonstration examples (N) per class for GPT-3.5 and GPT-4 to maximize annotation accuracy while minimizing cost in active learning settings?
- Basis in paper: [inferred] The paper tests N = 1, 2, 3, 4, 5 but does not find a definite trend in accuracy improvement, suggesting that the optimal number may depend on dataset characteristics or task difficulty.
- Why unresolved: The experiments show varying results across datasets and models, and the paper does not explore a broader range of N values or conduct a systematic analysis to identify a clear optimal value.
- What evidence would resolve it: A comprehensive study varying N across a wider range and analyzing the trade-off between accuracy gains and cost increases for different dataset types and model sizes.

### Open Question 2
- Question: How do different demonstration example selection strategies (random, min token, max similarity) impact the consistency and accuracy of LLM annotations in active learning, and under what conditions does each strategy perform best?
- Basis in paper: [explicit] The paper compares three strategies but finds that min token often achieves highest or close to highest accuracy, though it also has the highest inconsistency rate, indicating a trade-off that needs further exploration.
- Why unresolved: The paper does not provide a detailed analysis of why certain strategies perform better for specific datasets or tasks, nor does it explore hybrid or adaptive strategies that could combine the strengths of each approach.
- What evidence would resolve it: Experiments that systematically test each strategy across diverse datasets and tasks, including analysis of when and why certain strategies outperform others, potentially leading to the development of adaptive selection methods.

### Open Question 3
- Question: How can we effectively detect and correct consistently mislabeled samples by LLMs in active learning without incurring significant additional costs?
- Basis in paper: [explicit] The paper acknowledges that consistently mislabeled samples are not detectable with the current consistency-based method and suggests randomly sampling suspected correct samples for human verification, but notes this could increase costs.
- Why unresolved: The paper does not propose a concrete solution for identifying consistent errors, and the suggested approach of random sampling may not be efficient or scalable for large datasets.
- What evidence would resolve it: Development and testing of methods to estimate the confidence or reliability of LLM annotations, such as using ensemble models, uncertainty measures, or meta-learning approaches, to identify potential consistent errors without relying on ground truth labels.

## Limitations
- Evaluation covers only three datasets, limiting generalizability to other text classification tasks
- Study focuses on cost efficiency relative to human-only annotation but doesn't benchmark against other LLM-based active learning approaches
- Consistency-based uncertainty detection mechanism may not capture all types of model uncertainty, particularly for nuanced or domain-specific text classification tasks

## Confidence

- **High Confidence**: The claim that GPT-4 achieves higher accuracy than GPT-3.5 on challenging tasks like TREC-6 is supported by the observed performance differences and aligns with known capabilities of these models.
- **Medium Confidence**: The assertion that mixed annotations achieve similar or better results than human-only annotations is supported by results on AG's News and Rotten Tomatoes but requires further validation across diverse datasets and tasks.
- **Low Confidence**: The consistency-based selection mechanism's effectiveness in identifying truly uncertain samples may vary significantly depending on the specific characteristics of different text classification tasks.

## Next Checks

1. **Dataset Diversity Test**: Evaluate the mixed annotation approach on additional text classification datasets with varying complexity, domain specificity, and class balance to assess generalizability.

2. **Consistency Threshold Optimization**: Systematically vary the consistency check parameters (n, temperature) across different dataset types to determine optimal settings for uncertainty detection and cost efficiency.

3. **Comparison with Alternative Methods**: Benchmark the consistency-based approach against other uncertainty sampling methods (e.g., entropy-based, margin sampling) when used in conjunction with LLM annotations to validate its effectiveness.