---
ver: rpa2
title: 'Contrasting Adversarial Perturbations: The Space of Harmless Perturbations'
arxiv_id: '2402.02095'
source_url: https://arxiv.org/abs/2402.02095
tags:
- harmless
- perturbations
- layer
- perturbation
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reveals the existence of harmless perturbation spaces
  within deep neural networks (DNNs), where perturbations drawn from these spaces,
  regardless of magnitude, leave network outputs unchanged. These harmless perturbations
  arise from the usage of non-injective functions in DNNs, particularly in linear
  layers where input dimensions exceed output dimensions.
---

# Contrasting Adversarial Perturbations: The Space of Harmless Perturbations

## Quick Facts
- **arXiv ID**: 2402.02095
- **Source URL**: https://arxiv.org/abs/2402.02095
- **Authors**: Lu Chen; Shaofeng Li; Benhao Huang; Fan Yang; Zheng Li; Jie Li; Yuan Luo
- **Reference count**: 38
- **Key outcome**: This paper reveals the existence of harmless perturbation spaces within deep neural networks (DNNs), where perturbations drawn from these spaces, regardless of magnitude, leave network outputs unchanged.

## Executive Summary
This paper introduces the concept of harmless perturbations in deep neural networks, which are fundamentally different from adversarial examples. While adversarial perturbations cause networks to misclassify inputs, harmless perturbations leave network outputs unchanged regardless of their magnitude. These harmless perturbations arise from the non-injective nature of functions used in DNNs, particularly in linear layers where input dimensions exceed output dimensions. The study demonstrates that harmless perturbations can be used for privacy-preserving data transformation while maintaining model performance, offering new insights into DNN robustness and potential applications in privacy protection and model fingerprinting.

## Method Summary
The paper analyzes the mathematical properties of DNNs to identify harmless perturbation spaces. The authors first prove that harmless perturbations exist due to the non-injective nature of linear layers when input dimensions exceed output dimensions. They then derive a closed-form solution to calculate these harmless perturbations for fully connected and convolutional layers. The methodology involves computing the null space of the transformation matrices in linear layers and using these to generate perturbations that do not affect network outputs. The authors validate their theoretical findings through extensive experiments on MNIST and CIFAR-10 datasets using various network architectures, demonstrating that harmless perturbations can obscure sensitive information while maintaining classification accuracy.

## Key Results
- Harmless perturbations can obscure sensitive image data without affecting network performance, achieving similar privacy preservation to Gaussian noise
- The space of harmless perturbations is distinct from adversarial examples, with different properties and origins
- Harmless perturbations can be used for privacy-preserving data transformation and model fingerprinting applications

## Why This Works (Mechanism)
The paper demonstrates that harmless perturbations work because DNNs use non-injective functions, particularly in linear layers where input dimensions exceed output dimensions. When a matrix has more columns than rows, there exist non-zero vectors in its null space that, when added to inputs, do not change the output. This mathematical property creates spaces of perturbations that the network treats as equivalent, regardless of their magnitude.

## Foundational Learning
- **Null space of matrices**: Understanding the set of vectors that map to zero under a linear transformation is crucial for identifying harmless perturbations. Quick check: Verify that for a matrix A with more columns than rows, there exists at least one non-zero vector x such that Ax = 0.
- **Non-injective functions**: Functions that map multiple inputs to the same output create equivalence classes of inputs. Quick check: Confirm that linear layers with dimensionality reduction are non-injective by finding distinct inputs that produce identical outputs.
- **DNN layer transformations**: Knowledge of how fully connected and convolutional layers transform inputs is essential for computing harmless perturbations. Quick check: Trace how a single input propagates through a linear layer to verify the transformation properties.
- **Privacy-utility trade-off**: Understanding the balance between data privacy and model performance is key for evaluating harmless perturbation applications. Quick check: Measure classification accuracy before and after applying harmless perturbations to ensure functionality is preserved.
- **Adversarial vs. harmless perturbations**: Distinguishing between perturbations that change outputs versus those that preserve them is fundamental to the paper's contribution. Quick check: Compare the effects of adversarial and harmless perturbations on the same network to observe their contrasting behaviors.

## Architecture Onboarding

**Component map**: Input -> Linear/Convolutional Layers -> Activation Functions -> Output Layers -> Classification Result

**Critical path**: The critical path for harmless perturbations is through linear layers where input dimensionality exceeds output dimensionality, as these create the null spaces necessary for harmless perturbations to exist.

**Design tradeoffs**: The paper highlights a fundamental tradeoff between model expressivity (using high-dimensional inputs with lower-dimensional outputs) and the existence of harmless perturbation spaces. While these spaces enable privacy-preserving applications, they also represent redundant capacity in the network.

**Failure signatures**: Harmless perturbations fail when networks use only injective transformations (where input dimensionality â‰¤ output dimensionality) or when non-linear activation functions break the linear structure necessary for null space calculations.

**3 first experiments**:
1. Compute the null space of a random matrix with more columns than rows and verify that adding vectors from this space to inputs does not change the output
2. Apply harmless perturbations to a trained network and measure classification accuracy to confirm preservation of functionality
3. Compare the effect of harmless perturbations versus Gaussian noise on both privacy (visual obfuscation) and utility (classification accuracy)

## Open Questions the Paper Calls Out
None

## Limitations
- The analysis focuses predominantly on linear layers where input dimensions exceed output dimensions, leaving open questions about prevalence across different architectural components
- Experimental validation primarily uses standard benchmark datasets with relatively simple network architectures
- The paper does not extensively explore edge cases or potential information leakage through other channels when harmless perturbations are applied

## Confidence
- **Existence of harmless perturbations**: High confidence based on mathematical proofs
- **Privacy preservation claims**: Medium confidence based on benchmark experiments
- **Fundamental difference from adversarial examples**: High confidence supported by theoretical arguments

## Next Checks
1. Test harmless perturbation effectiveness across diverse architectures including transformers, residual networks, and models with varying activation functions to establish generalizability
2. Evaluate potential information leakage through adversarial transferability and model inversion attacks when harmless perturbations are applied
3. Conduct extensive privacy-utility trade-off analysis across multiple datasets with varying levels of sensitivity to establish practical bounds for real-world deployment