---
ver: rpa2
title: Leveraging Approximate Model-based Shielding for Probabilistic Safety Guarantees
  in Continuous Environments
arxiv_id: '2402.00816'
source_url: https://arxiv.org/abs/2402.00816
tags:
- policy
- safety
- safe
- ambs
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends approximate model-based shielding (AMBS) to
  continuous state and action spaces, using Safety Gym to benchmark against constrained
  RL algorithms. AMBS shields learned policies by simulating future trajectories in
  a world model and overriding unsafe actions.
---

# Leveraging Approximate Model-based Shielding for Probabilistic Safety Guarantees in Continuous Environments

## Quick Facts
- arXiv ID: 2402.00816
- Source URL: https://arxiv.org/abs/2402.00816
- Reference count: 40
- One-line primary result: AMBS with penalty techniques reduces cumulative safety violations from ~25k to ~9k in PointGoal1 while maintaining competitive returns

## Executive Summary
This paper extends Approximate Model-based Shielding (AMBS) to continuous state and action spaces, using Safety Gym to benchmark against constrained RL algorithms. AMBS shields learned policies by simulating future trajectories in a world model and overriding unsafe actions. The authors introduce three novel penalty techniques (PENL, PLPG, COPT) that modify the policy gradient to provide safety information, reducing the policy–shield conflict that harms convergence. Experiments on PointGoal1, PointGoal2, and CarGoal1 environments show that AMBS with penalties dramatically reduces cumulative safety violations while maintaining competitive returns, with more principled penalties (PLPG, COPT) providing stable convergence over longer training. Probabilistic safety guarantees are also established for the continuous setting.

## Method Summary
The paper extends AMBS to continuous environments by learning a DreamerV3 world model that predicts both rewards and safety costs. The method uses this model to simulate future trajectories and estimate safety violation probabilities, overriding unsafe actions with a safe backup policy. Three novel penalty techniques modify the policy gradient: PENL uses a simple penalty critic, PLPG directly optimizes a shielded policy by re-normalizing action probabilities based on safety estimates, and COPT identifies counter-example traces and scales down their gradients. The approach provides probabilistic safety guarantees through bounding the KL divergence between true and approximate transition systems.

## Key Results
- Cumulative safety violations reduced from ~25k to ~9k in PointGoal1 environment
- PLPG and COPT penalty techniques show more stable convergence than simple PENL
- Safety performance maintained while achieving competitive episode returns compared to constrained RL baselines
- Theoretical safety guarantees established with sample complexity bounds for continuous settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AMBS with penalty techniques reduces safety violations by overriding unsafe actions and modifying the policy gradient to penalize unsafe behavior.
- Mechanism: AMBS samples future trajectories from a learned world model, estimates safety violation probabilities, and overrides unsafe actions with a safe backup policy. Penalty techniques like PENL, PLPG, and COPT modify the policy gradient to provide safety information, reducing the conflict between the reward-seeking policy and the safety shield.
- Core assumption: The learned world model accurately approximates the true dynamics in safety-relevant regions of the state space, and the backup policy can provide safe actions when needed.
- Evidence anchors:
  - [abstract]: "AMBS shields learned policies by simulating future trajectories in a world model and overriding unsafe actions."
  - [section 4.1]: "The penalty critic estimates the discounted accumulated cost from a given state, taking the expectation under the task policy."
  - [corpus]: Weak evidence for this specific claim. Similar approaches exist in related works, but direct comparison is lacking.

### Mechanism 2
- Claim: PLPG directly optimizes a shielded policy by re-normalizing action probabilities based on safety estimates, ensuring convergence guarantees.
- Mechanism: PLPG uses bounded safety semantics to specify a re-normalizing coefficient for the policy gradient, increasing the probability of safer actions and decreasing unsafe ones. It also adds a log probability penalty term to discourage unsafe states.
- Core assumption: The safety estimates used for re-normalization are accurate and reliable, and the policy can converge to a balance between reward and safety.
- Evidence anchors:
  - [section 4.2]: "The PLPG directly optimises a shielded policy which re-normalises the probabilities of the actions of a given base policy."
  - [section 4.2]: "The key motivation of this probabilistic shielding framework, is that it comes with the same convergence guarantees as standard policy gradient methods."
  - [corpus]: Weak evidence. PLPG is mentioned in the corpus, but its specific mechanism and convergence guarantees are not detailed.

### Mechanism 3
- Claim: COPT identifies counter-example traces (unsafe trajectories) and scales down their gradients to prevent encouraging unsafe behavior.
- Mechanism: COPT samples traces from the world model, computes their discounted cost, and identifies those exceeding a threshold as counter-examples. It then scales the gradients of these counter-examples to zero, preventing the policy from learning unsafe behaviors.
- Core assumption: The cost threshold for identifying counter-examples is appropriately set, and the scaling of gradients effectively prevents the learning of unsafe behaviors.
- Evidence anchors:
  - [section 4.3]: "Our goal is to identify counter-example traces and use them to directly modify the policy gradient in such a way that unsafe (but possibly rewardful) behaviours are not encouraged."
  - [section 4.3]: "If the cost of a trace is above a particular threshold then it is considered a counter-example."
  - [corpus]: Weak evidence. COPT is introduced in this paper, but its specific mechanism and effectiveness are not detailed in related works.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: AMBS operates in POMDPs, where the agent receives partial observations of the environment state. Understanding MDPs and POMDPs is crucial for grasping the problem setup and the safety guarantees provided by AMBS.
  - Quick check question: What is the key difference between an MDP and a POMDP, and how does this difference affect the agent's ability to learn and make decisions?

- Concept: Reinforcement Learning (RL) and Policy Optimization
  - Why needed here: AMBS is an RL algorithm that optimizes policies to maximize reward while ensuring safety. Understanding RL concepts like policy gradients, value functions, and exploration-exploitation tradeoff is essential for comprehending the algorithm's design and implementation.
  - Quick check question: What is the policy gradient theorem, and how is it used to optimize policies in RL?

- Concept: Model-based RL and World Models
  - Why needed here: AMBS leverages a learned world model to simulate future trajectories and estimate safety violation probabilities. Understanding model-based RL and world models is crucial for grasping the algorithm's core mechanism and its ability to provide safety guarantees.
  - Quick check question: What is a world model in RL, and how does it differ from a traditional model-based approach?

## Architecture Onboarding

- Component map: World Model -> Safety Critic -> Task Policy -> Safe Policy
- Critical path:
  1. Learn a world model of the environment dynamics.
  2. Use the world model to simulate future trajectories and estimate safety violation probabilities.
  3. Override unsafe actions proposed by the task policy with actions from the safe policy.
  4. Modify the policy gradient using penalty techniques to provide safety information.
  5. Optimize the task policy to maximize reward while minimizing safety violations.

- Design tradeoffs:
  - Accuracy vs. Efficiency: A more accurate world model leads to better safety estimates but requires more computational resources.
  - Safety vs. Performance: Stricter safety constraints may lead to lower performance in terms of reward maximization.
  - Exploration vs. Exploitation: Balancing exploration of the environment with exploitation of learned safe policies.

- Failure signatures:
  - High safety violation rate: Indicates that the world model is not accurately predicting safety-relevant dynamics or the safe policy is not effective.
  - Poor performance: Suggests that the penalty techniques are overly restrictive or the safe policy is not optimal.
  - Slow convergence: May indicate that the world model is not learning effectively or the penalty techniques are hindering exploration.

- First 3 experiments:
  1. Run AMBS without any penalty techniques on a simple environment to establish a baseline for safety violations and performance.
  2. Add the PENL penalty technique and compare its effectiveness in reducing safety violations and improving performance.
  3. Test the PLPG and COPT penalty techniques and compare their performance to PENL and the baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do PLPG and COPT penalty techniques compare in terms of convergence speed and stability to the baseline Lagrangian method (LAG) over longer training runs?
- Basis in paper: [explicit] The paper states that PLPG and COPT methods maintain more stable convergence properties during longer training runs compared to the simple penalty critic, but notes that further investigation is needed.
- Why unresolved: The paper only provides preliminary evidence of PLPG and COPT stability in longer runs and lacks a comprehensive comparison with LAG.
- What evidence would resolve it: A detailed comparative study of PLPG, COPT, and LAG over extended training periods (e.g., 10M+ frames) with metrics on convergence speed, stability, and safety performance.

### Open Question 2
- Question: What are the theoretical guarantees for the convergence of the PLPG and COPT penalty techniques?
- Basis in paper: [explicit] The paper mentions that PLPG comes with convergence guarantees, but does not provide specific theoretical results for COPT.
- Why unresolved: The paper introduces COPT as a novel method but does not establish its theoretical convergence properties.
- What evidence would resolve it: A formal theoretical analysis of COPT, including proof of convergence under certain conditions, similar to the guarantees provided for PLPG.

### Open Question 3
- Question: How does the performance of AMBS with penalty techniques vary across different continuous state and action space environments?
- Basis in paper: [inferred] The paper tests AMBS with penalty techniques on three Safety Gym environments but does not explore a broader range of continuous environments.
- Why unresolved: The experiments are limited to specific environments, and the generalizability of the results to other continuous settings is unclear.
- What evidence would resolve it: Experiments applying AMBS with penalty techniques to a diverse set of continuous environments, such as those with varying complexity, dimensionality, and dynamics, to assess performance and adaptability.

## Limitations
- Empirical evaluation limited to three Safety Gym environments with only one baseline comparison
- Theoretical safety guarantees rely on strong assumptions about world model accuracy
- No comprehensive analysis of relative strengths and weaknesses of the three penalty techniques

## Confidence
- High Confidence: Core AMBS mechanism and basic framework of penalty techniques
- Medium Confidence: Probabilistic safety guarantees and sample complexity bounds
- Low Confidence: Generalizability to other continuous control domains and long-term stability

## Next Checks
1. Replicate experiments with additional constrained RL baselines and alternative world model architectures
2. Conduct systematic ablation studies varying key hyperparameters (α coefficients, safety level Δ, cost threshold)
3. Test AMBS on more complex continuous control tasks to evaluate generalizability and identify potential failure modes