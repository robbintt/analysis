---
ver: rpa2
title: 'ParaFusion: A Large-Scale LLM-Driven English Paraphrase Dataset Infused with
  High-Quality Lexical and Syntactic Diversity'
arxiv_id: '2404.12010'
source_url: https://arxiv.org/abs/2404.12010
tags:
- paraphrase
- dataset
- source
- diversity
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ParaFusion, a large-scale English paraphrase
  dataset developed using Large Language Models to address the limitations of existing
  datasets in terms of syntactic and lexical diversity. The dataset was created by
  augmenting existing datasets with high-quality data generated using the ChatGPT
  (gpt-3.5-turbo) LLM, resulting in a significant improvement in both lexical and
  syntactic diversity while maintaining strong semantic similarity.
---

# ParaFusion: A Large-Scale LLM-Driven English Paraphrase Dataset Infused with High-Quality Lexical and Syntactic Diversity

## Quick Facts
- arXiv ID: 2404.12010
- Source URL: https://arxiv.org/abs/2404.12010
- Reference count: 40
- ParaFusion improves lexical and syntactic diversity by at least 25% compared to existing datasets

## Executive Summary
ParaFusion is a novel large-scale English paraphrase dataset developed using Large Language Models (LLMs) to address the limitations of existing datasets in terms of syntactic and lexical diversity. The dataset was created by augmenting existing paraphrase datasets with high-quality data generated using ChatGPT (gpt-3.5-turbo), resulting in significant improvements in both lexical and syntactic diversity while maintaining strong semantic similarity. The authors present a comprehensive evaluation strategy that sets a new gold standard for paraphrase evaluation, combining multiple metrics to assess quality across different dimensions.

## Method Summary
The ParaFusion dataset was constructed by leveraging ChatGPT to augment existing paraphrase datasets including PAWS, ParaBank2, ParaSCI, and ParaSCI-Biology. The authors employed a multi-step process where they first filtered and cleaned existing datasets, then used carefully crafted prompts to generate diverse paraphrases. The LLM generation process was optimized to balance diversity with semantic preservation. The resulting dataset contains approximately 1.6 million unique paraphrase pairs, significantly expanding the diversity of existing paraphrase resources while maintaining high-quality semantic relationships between source and target sentences.

## Key Results
- At least 25% improvement in both lexical and syntactic diversity compared to existing datasets
- Strong semantic similarity preservation as measured by multiple metrics (BLEU, METEOR, BertScore)
- Comprehensive evaluation framework combining automated metrics for quality assessment
- Demonstrated effectiveness across multiple existing benchmark datasets

## Why This Works (Mechanism)
The approach works by leveraging the generative capabilities of LLMs to create diverse paraphrases that go beyond the limited patterns found in human-curated datasets. By using temperature-controlled generation with carefully designed prompts, the system can explore the paraphrase space more broadly while maintaining semantic fidelity through the model's inherent understanding of language relationships.

## Foundational Learning

**Paraphrase Diversity Metrics**: Why needed - To quantify improvements beyond simple accuracy measures; Quick check - Verify that diversity metrics show consistent improvement across multiple dimensions.

**Semantic Similarity Evaluation**: Why needed - To ensure diversity gains don't sacrifice meaning preservation; Quick check - Confirm high scores on multiple semantic similarity metrics.

**LLM Prompt Engineering**: Why needed - To guide generation toward desired diversity patterns; Quick check - Test different prompt variations to optimize output quality.

**Dataset Augmentation Strategies**: Why needed - To effectively scale existing resources; Quick check - Measure coverage expansion relative to original dataset sizes.

## Architecture Onboarding

**Component Map**: Raw datasets -> LLM Generation (ChatGPT) -> Diversity Filtering -> Quality Assessment -> ParaFusion

**Critical Path**: The most critical path involves the LLM generation and subsequent diversity filtering, as these steps directly determine the quality and utility of the final dataset.

**Design Tradeoffs**: The authors balanced diversity against semantic preservation by adjusting generation parameters and implementing post-generation filtering, accepting some computational overhead for improved output quality.

**Failure Signatures**: Potential failures include semantic drift in generated paraphrases, overfitting to the LLM's training distribution, and contamination between training and test splits.

**First Experiments**: 
1. Generate paraphrases for a small subset of existing dataset and measure diversity improvements
2. Compare semantic similarity scores between original and generated paraphrases
3. Evaluate downstream task performance using augmented versus original datasets

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy reliance on automated metrics without human evaluation to verify semantic preservation
- Potential biases introduced from the LLM's training data distribution
- Lack of analysis on dataset leakage or contamination between splits
- Claims of "at least 25% improvement" should be interpreted cautiously relative to varying baseline qualities

## Confidence

| Claim | Confidence |
|-------|------------|
| Dataset diversity improvements | High confidence |
| Semantic preservation claims | Medium confidence |
| Generalizability to downstream tasks | Medium confidence |

## Next Checks

1. Conduct human evaluation studies to verify that improved diversity metrics correspond to meaningful linguistic variation perceivable by humans and that semantic preservation holds under human judgment.

2. Perform ablation studies varying prompt strategies and temperature settings in the LLM generation process to determine optimal parameters for balancing diversity and quality.

3. Test downstream model performance using ParaFusion compared to existing datasets on multiple NLP tasks (paraphrase generation, semantic textual similarity, and robustness evaluation) to validate practical utility beyond diversity metrics.