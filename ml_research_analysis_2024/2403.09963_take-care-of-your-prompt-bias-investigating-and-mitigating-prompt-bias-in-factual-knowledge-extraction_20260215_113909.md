---
ver: rpa2
title: Take Care of Your Prompt Bias! Investigating and Mitigating Prompt Bias in
  Factual Knowledge Extraction
arxiv_id: '2403.09963'
source_url: https://arxiv.org/abs/2403.09963
tags:
- bias
- prompt
- prompts
- knowledge
- lama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates and mitigates "prompt bias" in factual
  knowledge extraction from pre-trained language models (PLMs). Prompt bias refers
  to the tendency of prompts to introduce biases towards specific labels, which can
  undermine the reliability of benchmarks and impair the knowledge retrieval capabilities
  of PLMs.
---

# Take Care of Your Prompt Bias! Investigating and Mitigating Prompt Bias in Factual Knowledge Extraction

## Quick Facts
- arXiv ID: 2403.09963
- Source URL: https://arxiv.org/abs/2403.09963
- Authors: Ziyang Xu; Keqin Peng; Liang Ding; Dacheng Tao; Xiliang Lu
- Reference count: 0
- One-line primary result: Representation-based debiasing improves factual knowledge extraction by up to 10% by removing prompt-induced label bias

## Executive Summary
This paper identifies and addresses "prompt bias" in factual knowledge extraction from pre-trained language models (PLMs). Prompt bias occurs when prompts themselves introduce biases toward specific labels, leading to inflated benchmark performance that doesn't reflect true knowledge retrieval capabilities. The authors propose a representation-based debiasing approach that estimates and removes biased components from model representations, improving both benchmark reliability and knowledge retrieval performance across various PLMs, prompts, and benchmarks.

The study demonstrates that prompt bias significantly affects factual knowledge extraction, with gradient-based prompts showing higher bias levels than manual or paraphrase-based prompts. Through experiments on LAMA, LAMA-UHN, and WIKI-UNI benchmarks using BERT and RoBERTa models, the proposed debiasing method successfully rectifies overfitted performance and achieves up to 10% absolute improvement in prompt retrieval capability, particularly on imbalanced datasets.

## Method Summary
The paper proposes a representation-based approach to mitigate prompt bias in factual knowledge extraction. The method works by first constructing prompt-only queries (replacing subject information with meaningless tokens) to probe the inherent bias of prompts. The model's output distribution for these prompt-only queries reveals the prompt's intrinsic bias toward certain labels. The debiasing process then estimates the biased representation using these prompt-only queries and removes it from the model's internal representations through vector subtraction, generating debiased representations that preserve knowledge components while eliminating prompt-induced bias. These debiased representations are used for final predictions, improving both the reliability of benchmark assessments and the model's knowledge retrieval capabilities.

## Key Results
- Gradient-based prompts (AutoPrompt, OptiPrompt) show significantly higher bias levels compared to manual and paraphrase-based prompts
- Prompt bias achieves non-trivial performance on imbalanced benchmarks (LAMA, LAMA-UHN) by exploiting label distribution skew
- The representation-based debiasing approach achieves up to 10% absolute performance gain in prompt retrieval capability
- Debiased representations improve reliability of benchmark assessments and knowledge retrieval across various PLMs and prompts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt-only querying reveals inherent label bias by replacing subject information with meaningless tokens.
- Mechanism: The model's output distribution when given only a prompt (no valid subject) reflects the prompt's intrinsic bias toward certain labels.
- Core assumption: The model's predictions are influenced by the prompt structure itself, independent of actual subject knowledge.
- Evidence anchors:
  - [abstract] "prompt-only querying to probe PLMs... we can observe the inherent bias of the prompt"
  - [section 2.1] "prompt-only querying should exhibit a uniform distribution... however, the prompts... show a severe bias towards specific labels"
  - [corpus] Weak evidence - no direct neighbor citations, but FMR score suggests moderate relevance

### Mechanism 2
- Claim: Representation subtraction removes bias while preserving knowledge-related components.
- Mechanism: By subtracting the prompt-only representation (which captures bias) from the full query representation, the debiased representation retains knowledge but loses prompt-induced bias.
- Core assumption: The representation space separates bias and knowledge components in a linearly separable way.
- Evidence anchors:
  - [abstract] "we first estimate the biased representation using prompt-only querying, and then remove it from the model's internal representations"
  - [section 3] "the internal representation vectors of PLMs encompass their preference for answer labels, which incorporate both internal knowledge and prompt bias"
  - [corpus] Moderate evidence - related work on debiasing but not specifically representation-based methods

### Mechanism 3
- Claim: Prompt bias causes overfitting on imbalanced benchmarks, inflating performance metrics.
- Mechanism: Prompts with strong label bias can achieve high accuracy by exploiting label distribution skew rather than actual knowledge retrieval.
- Core assumption: Benchmark performance improvements from biased prompts reflect statistical exploitation rather than genuine capability.
- Evidence anchors:
  - [abstract] "prompt bias can amplify benchmark accuracy unreasonably by overfitting the test datasets"
  - [section 2.3] "prompt bias achieves non-trivial performance on LAMA and LAMA-UHN... which are widely used benchmarks with imbalanced data distributions"
  - [corpus] Strong evidence - multiple neighbors discuss bias in PLMs and factual knowledge extraction

## Foundational Learning

- Concept: Jensen-Shannon divergence as bias quantification
  - Why needed here: Provides a symmetric, bounded measure of how far prompt bias distribution deviates from uniform
  - Quick check question: Why choose J-S divergence over KL divergence for measuring prompt bias?
  - Answer: J-S divergence is symmetric and bounded, avoiding infinite values when probabilities are zero

- Concept: Representation layer vs output layer debiasing
  - Why needed here: Working in representation space preserves more information for downstream tasks compared to output probability manipulation
  - Quick check question: What's the key advantage of debiasing in representation space rather than output probabilities?
  - Answer: Generated debiased representations can be used in broader contexts like sentence embeddings

- Concept: Typed vs untyped querying paradigms
  - Why needed here: Typed querying restricts candidate space, making bias effects more measurable and manageable
  - Quick check question: Why focus on typed querying rather than untyped querying in this study?
  - Answer: Typed querying provides a controlled candidate set, making bias measurement and mitigation more tractable

## Architecture Onboarding

- Component map: Input prompt + subject tuple -> Query construction (vanilla and prompt-only variants) -> PLM inference (generate representations) -> Subtraction module (compute debiased representation) -> Output embedding (generate final logits) -> Label selection (choose highest-scoring label)

- Critical path: Input → Query construction → PLM inference → Subtraction → Output embedding → Label selection

- Design tradeoffs:
  - Using final layer representations vs intermediate layers (final layer chosen for being closest to output)
  - Single subtraction vs learned bias removal (subtraction chosen for simplicity and interpretability)
  - Static prompt-only template vs dynamic template generation (static chosen for consistency)

- Failure signatures:
  - Debiasing degrades performance on balanced datasets (suggests over-correction)
  - Debiasing fails to improve performance on imbalanced datasets (suggests insufficient bias removal)
  - Performance becomes highly sensitive to prompt-only template choice (suggests unstable bias estimation)

- First 3 experiments:
  1. Run vanilla and prompt-only queries on a simple prompt to verify bias detection
  2. Apply subtraction debiasing and measure representation similarity changes
  3. Test on a small imbalanced dataset to verify overfitting correction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the debiasing approach affect the performance of large language models (LLMs) with billions of parameters beyond Llama2 7B?
- Basis in paper: [explicit] The paper evaluates the debiasing strategy on Llama2 7B and shows positive results, but does not explore larger LLMs.
- Why unresolved: The study only tests the debiasing method on a single, relatively small LLM (Llama2 7B). The generalizability of the approach to much larger models remains unknown.
- What evidence would resolve it: Experiments applying the debiasing technique to larger LLMs (e.g., GPT-3, GPT-4, PaLM) and comparing performance across different scales.

### Open Question 2
- Question: What are the underlying mechanisms that cause gradient-based prompts like AutoPrompt and OptiPrompt to exhibit higher levels of bias compared to manual or paraphrase-based prompts?
- Basis in paper: [explicit] The paper observes that gradient-based prompts show significantly higher levels of bias but does not provide a detailed explanation for this phenomenon.
- Why unresolved: The study identifies the correlation between prompt type and bias level but does not investigate the root causes of this difference.
- What evidence would resolve it: A thorough analysis of the optimization processes and resulting representations of different prompt types, potentially involving ablation studies or visualization of learned patterns.

### Open Question 3
- Question: How does the debiasing approach perform on knowledge extraction tasks beyond factual knowledge, such as commonsense reasoning or sentiment analysis?
- Basis in paper: [inferred] The paper focuses on factual knowledge extraction but mentions that the approach could be applied to other prompt-based tasks where prompt bias occurs.
- Why unresolved: The study does not explore the effectiveness of the debiasing method on different types of knowledge or reasoning tasks.
- What evidence would resolve it: Experiments applying the debiasing technique to various NLP benchmarks that require different types of knowledge or reasoning, such as SocialIQA for commonsense reasoning or SST-2 for sentiment analysis.

## Limitations

- The debiasing approach assumes linear separability between bias and knowledge components in model representations, which may not hold for all PLMs or prompt types
- The method is specifically designed for typed querying with restricted candidate sets, limiting its applicability to untyped or open-ended knowledge extraction tasks
- The evaluation focuses primarily on factual knowledge extraction using LAMA benchmarks, with limited exploration of how the approach generalizes to other types of knowledge or reasoning tasks

## Confidence

**High Confidence:** The identification of prompt bias as a significant issue in factual knowledge extraction, and the empirical demonstration that biased prompts can achieve non-trivial performance on imbalanced benchmarks through label distribution exploitation rather than genuine knowledge retrieval.

**Medium Confidence:** The representation-based debiasing approach effectively removes prompt bias while preserving knowledge components, as evidenced by improved performance on both imbalanced and balanced benchmarks. The J-S divergence quantification provides a reliable measure of prompt bias severity.

**Low Confidence:** The claim that the proposed approach can improve prompt retrieval capability by up to 10% absolute performance gain across all tested PLMs and prompts. This appears optimistic given the variability in results across different model sizes and prompt types, and may be an upper bound rather than typical performance improvement.

## Next Checks

1. **Ablation Study on Representation Layers:** Test the debiasing approach using different internal representation layers (not just the final layer) to determine if the choice of layer significantly impacts debiasing effectiveness. This would validate whether the final layer is optimal for separating bias from knowledge components.

2. **Zero-Shot Scenario Evaluation:** Extend the evaluation to untyped querying scenarios where the candidate set is not restricted, to test whether the representation-based debiasing approach generalizes beyond the typed querying paradigm used in the current experiments.

3. **Cross-Model Transferability Test:** Apply the debiasing method trained on one PLM (e.g., BERT) to another PLM (e.g., RoBERTa) without retraining, to assess whether the bias patterns are model-specific or can be transferred across architectures.