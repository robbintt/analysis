---
ver: rpa2
title: Robust Entropy Search for Safe Efficient Bayesian Optimization
arxiv_id: '2405.19059'
source_url: https://arxiv.org/abs/2405.19059
tags:
- function
- robust
- optimization
- parameters
- optimum
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of finding adversarially robust
  optima in Bayesian optimization, where some parameters are uncontrollable and can
  be adversarially perturbed at application time. The proposed method, Robust Entropy
  Search (RES), is an information-theoretic acquisition function that considers the
  properties of the robust optimum by involving the noiseless robust optimal value,
  the argmax function, and its corresponding function values.
---

# Robust Entropy Search for Safe Efficient Bayesian Optimization

## Quick Facts
- **arXiv ID**: 2405.19059
- **Source URL**: https://arxiv.org/abs/2405.19059
- **Reference count**: 40
- **Primary result**: RES reliably finds robust optima in adversarial settings, outperforming state-of-the-art algorithms including StableOpt, MES, UCB, KG, and EI.

## Executive Summary
This paper addresses the challenge of finding adversarially robust optima in Bayesian optimization where some parameters are uncontrollable and can be adversarially perturbed at application time. The proposed Robust Entropy Search (RES) method is an information-theoretic acquisition function that integrates worst-case optimization into the acquisition process by considering the noiseless robust optimal value, the argmax function, and corresponding function values. RES is evaluated on synthetic and real-life data including engineering and robotics applications, demonstrating superior performance compared to state-of-the-art algorithms.

## Method Summary
RES is an information-theoretic acquisition function that extends entropy search to the adversarial robustness setting. It computes the mutual information between the next observation and the robustness characteristics (robust optimum, argmax function, and maximizing function) by sampling functions from a GP posterior, computing worst-case values over uncontrollable parameters, and conditioning the posterior accordingly. The method uses a Sparse Spectrum Gaussian Process approximation for efficient sampling and Expectation Propagation for conditioning on robustness constraints.

## Key Results
- RES reliably finds robust optima across various benchmark functions and real-world problems
- Outperforms state-of-the-art algorithms (StableOpt, MES, UCB, KG, EI) in adversarial robustness settings
- Demonstrates superior performance on engineering applications including finite element method simulation parameter calibration and robust robot pushing
- Shows robustness to hyperparameter choices with number of samples C having minimal impact on performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RES integrates worst-case optimization into information-theoretic Bayesian optimization
- Mechanism: Conditions the predictive posterior on worst-case maximizing function values and robust optimal value
- Core assumption: Argmax over uncontrollable parameters can be computed efficiently
- Evidence anchors: Abstract and section 4.1 explicitly describe conditioning on robustness characteristics
- Break condition: Intractable argmax over uncontrollable parameters prevents method application

### Mechanism 2
- Claim: SSGP approximations enable efficient sampling and conditioning
- Mechanism: Provides closed-form function samples for gradient-based optimization and EP conditioning
- Core assumption: 500 Fourier features provide sufficient GP posterior approximation
- Evidence anchors: Section 4.2.1 describes SSGP implementation with 500 Fourier features
- Break condition: Coarse SSGP approximation leads to inaccurate conditioning and degraded performance

### Mechanism 3
- Claim: Three-step conditioning process approximates needed conditional distribution
- Mechanism: EP enforces constraints at training points, predictive distribution is created, then truncated to satisfy constraints
- Core assumption: Doubly truncated Gaussian moments can be computed accurately enough
- Evidence anchors: Section 4.2.2 describes the three-step EP and truncation approach
- Break condition: Truncation introduces significant bias or numerical instability

## Foundational Learning

- **Gaussian Process regression and conditioning**: RES relies on GP surrogate models and conditioning them on robustness constraints. Quick check: Can you write the posterior mean and variance of a GP after conditioning on a set of observations?
- **Information-theoretic acquisition functions**: RES extends entropy search methods. Quick check: What is the difference between Expected Improvement and Entropy Search in terms of what they optimize?
- **Expectation Propagation for linear constraints**: EP is used to enforce robustness constraints. Quick check: How does EP approximate a posterior with linear inequality constraints?

## Architecture Onboarding

- **Component map**: GP surrogate model -> SSGP sampler -> Robustness characteristic calculator -> EP conditioner -> Predictive conditioner -> Acquisition optimizer
- **Critical path**: 1) Sample functions from GP posterior (SSGP) 2) Compute robustness characteristics for each sample 3) Condition GP on robustness at training data (EP) 4) For each candidate point: create predictive distribution, condition on robustness (truncation), compute entropy and acquisition value 5) Optimize acquisition to select next point
- **Design tradeoffs**: Number of function samples C vs. accuracy of robustness approximation; Number of Fourier features F vs. SSGP fidelity; Choice of optimization method for robustness characteristics vs. runtime
- **Failure signatures**: Poor GP fit leading to inaccurate robustness characteristics; Numerical instability in EP or truncation; Slow convergence due to high-dimensional argmax
- **First 3 experiments**: 1) Verify SSGP sampling matches GP posterior (visualize samples) 2) Test conditioning on synthetic robustness constraints (check truncation bounds) 3) Run on a simple 1D adversarial robust problem (compare to brute-force)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does RES compare to other information-based acquisition functions like MES and JES on adversarially robust optimization problems?
- Basis: Paper compares RES to state-of-the-art algorithms but doesn't specifically compare to other information-based acquisition functions
- Why unresolved: Paper focuses on comparing RES to a subset of algorithms without including other information-based acquisition functions
- What evidence would resolve it: Experiments comparing RES to other information-based acquisition functions like JES on adversarially robust optimization problems

### Open Question 2
- Question: How does the choice of number of samples (C) in RES affect its performance?
- Basis: Paper mentions C only slightly impacts performance and sets it to 1 for all other experiments
- Why unresolved: Paper doesn't explore impact of different C values in detail
- What evidence would resolve it: Experiments with different C values in RES and comparing results

### Open Question 3
- Question: How does RES perform on higher-dimensional spaces or more complex objective functions?
- Basis: Paper evaluates RES on synthetic and real-life data but doesn't mention higher-dimensional experiments
- Why unresolved: Paper doesn't provide information on performance with higher-dimensional spaces or complex functions
- What evidence would resolve it: Experiments with higher-dimensional spaces or complex objective functions and comparison to other algorithms

## Limitations

- Method's reliance on efficient argmax computation over uncontrollable parameters may not hold for high-dimensional or complex constraint sets
- SSGP approximation with 500 Fourier features may introduce approximation errors, particularly in tails where robustness constraints are most relevant
- Truncation step in conditioning process could introduce bias or numerical instability in entropy calculations

## Confidence

- **High Confidence**: Fundamental mechanism of integrating worst-case optimization into information-theoretic Bayesian optimization is well-supported by mathematical formulation and experimental results
- **Medium Confidence**: SSGP approximation's accuracy and impact on conditioning steps is reasonably supported but needs sensitivity analysis
- **Medium Confidence**: Three-step conditioning approach appears theoretically sound though practical implications of approximation errors need exploration

## Next Checks

1. **Sensitivity Analysis**: Systematically vary the number of Fourier features (F) in SSGP approximation and measure impact on optimization performance across different problem dimensions
2. **Constraint Complexity Test**: Evaluate RES on problems with increasingly complex constraint sets for uncontrollable parameters to identify when argmax computation becomes intractable
3. **Approximation Error Quantification**: Compare entropy estimates from RES with ground truth values on small problems where exact computation is feasible to quantify impact of SSGP and truncation approximations