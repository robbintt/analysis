---
ver: rpa2
title: Parameter Efficient Reinforcement Learning from Human Feedback
arxiv_id: '2403.10704'
source_url: https://arxiv.org/abs/2403.10704
tags:
- learning
- lora
- arxiv
- reward
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Parameter Efficient Reinforcement Learning from Human Feedback
  (PE-RLHF) reduces memory usage by up to 50% and speeds up training by up to 90%
  compared to standard RLHF while maintaining comparable performance across diverse
  tasks like summarization, harmlessness, helpfulness, UI automation, and visual question
  answering. The approach uses LoRA to fine-tune only a small fraction of model parameters
  during both reward model and policy training, enabling efficient alignment of large
  language and vision-language models with human preferences.
---

# Parameter Efficient Reinforcement Learning from Human Feedback

## Quick Facts
- arXiv ID: 2403.10704
- Source URL: https://arxiv.org/abs/2403.10704
- Reference count: 25
- Reduces memory usage by up to 50% and speeds up training by up to 90% compared to standard RLHF

## Executive Summary
Parameter Efficient Reinforcement Learning from Human Feedback (PE-RLHF) introduces a memory-efficient approach to aligning large language and vision-language models with human preferences. By leveraging LoRA (Low-Rank Adaptation) to fine-tune only a small fraction of model parameters during both reward model and policy training, PE-RLHF achieves substantial computational savings without sacrificing alignment quality. The method is validated across diverse tasks including summarization, harmlessness, helpfulness, UI automation, and visual question answering, demonstrating performance parity with standard RLHF while significantly reducing resource requirements.

## Method Summary
PE-RLHF employs LoRA-based fine-tuning for both the reward model and the reinforcement learning policy, contrasting with standard RLHF which typically requires full fine-tuning. The approach uses LoRA adapters with varying ranks (1, 4, 8, 16) to approximate the full parameter updates during both stages of RLHF. The method is tested on six diverse datasets using PaLM 2 and Gemini Pro models of varying sizes (XXS to L). Training follows a two-stage process: first training a LoRA-based reward model using pairwise comparisons, then performing LoRA-based RL policy optimization using REINFORCE with KL regularization. Comprehensive ablations examine the effects of model size and LoRA rank on both efficiency and performance.

## Key Results
- Achieves 50% memory reduction and 90% speed improvements compared to standard RLHF
- Maintains comparable performance across summarization (win rates), harmlessness, helpfulness, UI automation accuracy, and VQA tasks
- Performance parity with full fine-tuning improves as model size increases, with LoRA rank 1 sufficient for XXS/S models and rank 4 needed for L models

## Why This Works (Mechanism)
PE-RLHF works by constraining the parameter updates during both reward modeling and policy optimization to low-rank subspaces via LoRA adapters. This dramatically reduces the number of parameters that need to be updated and stored during training, leading to the observed efficiency gains. The key insight is that the alignment-relevant information can be captured in these low-rank updates without requiring modification of the full model, allowing the method to maintain performance while achieving substantial computational savings.

## Foundational Learning
- **LoRA (Low-Rank Adaptation)**: A parameter-efficient fine-tuning technique that approximates weight updates using low-rank matrices; needed to reduce memory and compute requirements while maintaining performance.
- **Reward Modeling in RLHF**: Training a model to predict human preferences between pairs of responses; required to provide feedback signals for policy optimization.
- **Reinforcement Learning from Human Feedback**: The two-stage process of first training a reward model then optimizing a policy against it using RL; the baseline that PE-RLHF improves upon.
- **JAX and TPU acceleration**: The computational framework used for efficient training; essential for handling the large models and datasets involved.
- **Judge models for evaluation**: Using a separate model (PaLM 2 L) to evaluate alignment quality; necessary for automated win rate calculations across tasks.
- **KL regularization in RL**: Penalizing deviation from the original policy to prevent mode collapse; important for maintaining safe and useful outputs.

## Architecture Onboarding

Component Map:
Input Datasets -> Reward Model (LoRA) -> Reward Model Evaluation -> Policy Model (LoRA) -> Policy Evaluation (Judge Model) -> Final PE-RLHF Model

Critical Path:
The critical path is the sequential execution of reward model training followed by policy optimization. Each stage depends on the successful completion of the previous one, with LoRA adapters being initialized, trained, and merged with the backbone model. The reward model must achieve sufficient pairwise accuracy before policy training can begin, and the policy training must produce a model that the judge model can evaluate.

Design Tradeoffs:
The primary tradeoff is between efficiency and performance fidelity. Lower LoRA ranks provide greater efficiency but may compromise the ability to capture complex alignment signals. The method sacrifices some parameter flexibility to achieve computational gains, betting that the low-rank subspace contains sufficient information for effective alignment. This contrasts with full fine-tuning which maintains all parameter flexibility at the cost of higher resource requirements.

Failure Signatures:
- Reward model achieving low pairwise accuracy (<70%) indicating failure to learn human preferences
- Policy training producing degenerate outputs or failing to improve over the base model
- Judge model evaluations showing high variance or systematic bias
- Memory usage not decreasing as expected due to improper LoRA implementation

Three First Experiments:
1. Verify LoRA adapter initialization and merging works correctly by comparing outputs of LoRA-finetuned vs full-finetuned models on a small dataset
2. Test reward model pairwise accuracy on a held-out validation set before proceeding to policy training
3. Run a single RL training iteration with LoRA and verify KL divergence remains within acceptable bounds

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- No ablation studies on the KL regularization coefficient β which could impact efficiency and alignment quality
- Reliance on PaLM 2 L as judge model introduces potential evaluation biases
- Limited exploration of scalability limits for extremely large models beyond PaLM 2 L
- Lack of long-term stability testing across multiple training seeds and extended durations

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Memory usage and training speed improvements | High |
| Performance parity with full fine-tuning | Medium |
| Scalability observations with model size | Medium |

## Next Checks
1. Replicate memory and speed measurements using different hardware configurations (A100 vs H100 GPUs) to verify improvements are hardware-agnostic
2. Conduct extended stability tests by evaluating model performance across multiple training seeds and longer training durations
3. Perform ablation studies varying the KL regularization coefficient β to determine its impact on efficiency and alignment quality