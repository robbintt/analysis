---
ver: rpa2
title: 'Emotional Voice Messages (EMOVOME) database: emotion recognition in spontaneous
  voice messages'
arxiv_id: '2402.17496'
source_url: https://arxiv.org/abs/2402.17496
tags: []
core_contribution: This paper introduces the Emotional Voice Messages (EMOVOME) database,
  a spontaneous speech corpus containing 999 audio messages from real conversations
  on a messaging app from 100 Spanish speakers. The database includes demographic
  details for the speakers and annotators, and provides transcriptions of the audio
  messages.
---

# Emotional Voice Messages (EMOVOME) database: emotion recognition in spontaneous voice messages

## Quick Facts
- arXiv ID: 2402.17496
- Source URL: https://arxiv.org/abs/2402.17496
- Reference count: 33
- Primary result: Spanish spontaneous speech database with 999 audio messages from 100 speakers

## Executive Summary
This paper introduces the EMOVOME database, a spontaneous speech corpus containing 999 audio messages from real conversations on a messaging app, recorded from 100 Spanish speakers. The database includes demographic details for both speakers and annotators, along with transcriptions of all audio messages. The authors implemented emotion recognition models using both speech features (eGeMAPS) with SVM classifiers and text transcriptions with a fine-tuned multilingual BERT model. The database aims to fill a gap in emotion recognition research by providing naturally occurring, spontaneous speech data for the Spanish language.

## Method Summary
The EMOVOME database was created by collecting 999 spontaneous voice messages from real conversations on a messaging app from 100 Spanish speakers. Demographic information was gathered for both speakers and annotators. Audio messages were transcribed and emotion annotations were collected using two approaches: 6 annotators per message for audio-based annotations and 6 annotators per sample for text-based annotations. The authors then implemented baseline emotion recognition models using eGeMAPS acoustic features with SVM classifiers for valence and arousal prediction, and a fine-tuned multilingual BERT model for text-based emotion recognition. Performance was evaluated using unweighted accuracy metrics.

## Key Results
- Achieved 49.27% unweighted accuracy for valence and 44.71% for arousal using eGeMAPS features with SVM classifiers
- Obtained 61.15% unweighted accuracy for valence and 47.43% for arousal using fine-tuned multilingual BERT on text transcriptions
- Created the first Spanish spontaneous speech database for emotion recognition with 999 audio messages from 100 speakers

## Why This Works (Mechanism)
None provided

## Foundational Learning
- Spontaneous speech characteristics: Natural conversations contain disfluencies, varied speaking styles, and contextual dependencies that differ from read speech; understanding these differences is crucial for developing robust emotion recognition systems
- eGeMAPS feature extraction: Standardized acoustic feature set designed for paralinguistic analysis that captures voice quality, prosody, and spectral characteristics relevant to emotion expression
- Multilingual BERT fine-tuning: Transfer learning approach that adapts pre-trained language models to specific downstream tasks with limited training data, leveraging cross-lingual representations

## Architecture Onboarding
Component map: Audio recording -> Transcription -> Emotion annotation -> Feature extraction (eGeMAPS/BERT) -> Classification (SVM/BERT) -> Performance evaluation

Critical path: The core pipeline involves collecting spontaneous voice messages, transcribing them, extracting acoustic features (eGeMAPS) or text features (BERT embeddings), and training classifiers for valence and arousal prediction.

Design tradeoffs: The choice between audio-based (eGeMAPS + SVM) and text-based (BERT) approaches represents a tradeoff between capturing paralinguistic cues versus linguistic content. The database's spontaneous nature provides ecological validity but introduces variability that may challenge systematic analysis.

Failure signatures: Performance limitations may stem from: 1) Limited training data size for deep learning approaches, 2) Language-specific constraints limiting cross-lingual generalization, 3) Demographic biases in speaker or annotator populations affecting model fairness, 4) Annotation inconsistencies due to different approaches for audio vs. text.

First experiments:
1. Test model performance on different demographic subgroups within the database
2. Compare eGeMAPS feature performance against deep learning-based acoustic features
3. Evaluate cross-lingual transfer by testing models on non-Spanish emotion databases

## Open Questions the Paper Calls Out
None

## Limitations
- Database restricted to Spanish speakers, limiting generalizability to other languages and cultures
- Dataset size of 999 messages from 100 speakers may be relatively small for training robust deep learning models
- Different annotation approaches (6 annotators per message vs. 6 annotators per sample) create potential inconsistencies in the annotation scheme

## Confidence
High confidence: Database creation methodology, demographic data collection, and transcription process are clearly described and reproducible. Baseline results using eGeMAPS features with SVM and multilingual BERT with text transcriptions appear methodologically sound.

Medium confidence: Claim that database will significantly contribute to research on emotion recognition in the wild is reasonable but needs broader community validation. Reported performance metrics are valid for specific models used but may not generalize to all emotion recognition approaches.

Low confidence: Assertion that this is a "unique" resource for Spanish emotion recognition data is difficult to verify without comprehensive knowledge of all existing Spanish emotion databases.

## Next Checks
1. Conduct a replication study using the EMOVOME database with different feature extraction methods (e.g., deep learning-based features) to verify if reported performance is consistent across different approaches
2. Perform demographic bias analysis to assess whether models perform differently across various age groups, genders, and other demographic factors present in the speaker population
3. Expand emotion recognition experiments to include additional languages or cross-lingual validation to test generalizability of findings from this Spanish-specific database