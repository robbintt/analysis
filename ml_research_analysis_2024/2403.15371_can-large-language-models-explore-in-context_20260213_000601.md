---
ver: rpa2
title: Can large language models explore in-context?
arxiv_id: '2403.15371'
source_url: https://arxiv.org/abs/2403.15371
tags:
- learning
- failures
- which
- in-context
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) can
  explore in multi-armed bandit (MAB) settings, a core capability for decision-making
  agents. The authors deploy GPT-3.5, GPT-4, and Llama2 as agents in simple MAB environments,
  specifying the environment and interaction history entirely within the LLM prompt.
---

# Can large language models explore in-context?

## Quick Facts
- arXiv ID: 2403.15371
- Source URL: https://arxiv.org/abs/2403.15371
- Reference count: 40
- Primary result: Most LLM configurations fail to explore robustly in multi-armed bandits; only GPT-4 with summarized history and reinforced CoT succeeds

## Executive Summary
This paper investigates whether large language models can perform exploration in multi-armed bandit (MAB) settings, a fundamental capability for decision-making agents. The authors deploy GPT-3.5, GPT-4, and Llama2 as agents in simple MAB environments, specifying all information through LLM prompts. They experiment with various prompt designs including different scenarios, framings, history presentations, and chain-of-thought reasoning. The primary finding is that most LLM configurations fail to robustly explore, either committing to suboptimal arms or selecting all arms uniformly. Only one configuration succeeds: GPT-4 with suggestive framing, externally summarized history, and reinforced chain-of-thought reasoning. This suggests that while LLMs can explore with careful prompting, external summarization may be crucial, hinting at the need for further algorithmic interventions in more complex settings.

## Method Summary
The paper treats LLMs as black-box agents in multi-armed bandit environments, specifying the environment and interaction history entirely within the LLM prompt. The authors experiment with different prompt designs including scenario (buttons/advertisements), framing (neutral/suggestive), history presentation (raw vs. summarized), and chain-of-thought reasoning. They evaluate performance using GPT-3.5, GPT-4, and Llama2 on simple MAB instances with 5 arms and Bernoulli rewards. The evaluation focuses on suffix failures (committing to suboptimal arms) and uniform-like failures (selecting all arms equally). Performance is compared against standard bandit algorithms like UCB and Thompson Sampling.

## Key Results
- Most LLM configurations fail to explore robustly, with 70-100% failure rates across models
- GPT-4 with suggestive framing, externally summarized history, and reinforced chain-of-thought succeeds in exploration
- Temperature 0 is critical for deterministic reasoning-based selection rather than random sampling
- External summarization of interaction history appears crucial for enabling successful exploration

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 with reinforced CoT and summarized history succeeds because it can effectively process compressed information and apply deliberate reasoning to exploration decisions.
- Mechanism: The summarized history reduces cognitive load by presenting arm statistics directly, while reinforced CoT prompts explicit reasoning about exploration-exploitation tradeoffs. Together, these interventions enable the model to recognize and act on patterns that lead to optimal arm selection.
- Core assumption: GPT-4 has sufficient reasoning capacity to interpret summarized statistics and apply algorithmic principles when prompted with chain-of-thought reasoning.
- Evidence anchors: [abstract] "only one configuration resulted in satisfactory exploratory behavior: Gpt-4 with chain-of-thought reasoning and an externally summarized interaction history"
- Break condition: If the model cannot perform basic arithmetic on summarized statistics, or if the chain-of-thought reasoning is not sufficiently reinforced to trigger algorithmic thinking.

### Mechanism 2
- Claim: Temperature 0 ensures deterministic selection based on the model's reasoning rather than random exploration.
- Mechanism: By setting temperature to 0, the LLM must rely on its own deliberative process to select arms, rather than sampling from a probability distribution. This isolates the model's inherent ability to balance exploration and exploitation.
- Core assumption: The model's reasoning process, when deterministic, produces better exploration behavior than random sampling.
- Evidence anchors: [abstract] "The single configuration that succeeds... involves... temperature 0"
- Break condition: If the model's deterministic reasoning consistently fails to explore, or if the temperature parameter has no effect on selection behavior.

### Mechanism 3
- Claim: The suggestive framing helps activate the model's knowledge of bandit algorithms and exploration strategies.
- Mechanism: By providing a suggestive hint about balancing exploration and exploitation, the prompt activates relevant knowledge from the training corpus, guiding the model toward algorithmic thinking rather than naive decision-making.
- Core assumption: GPT-4 has been exposed to bandit algorithms and exploration-exploitation tradeoffs in its training data.
- Evidence anchors: [abstract] "the single configuration that succeeds involves... an 'enhanced' prompt that (a) provides a suggestive hint to explore"
- Break condition: If the suggestive framing does not consistently improve performance across different MAB instances, or if the model fails to activate relevant knowledge.

## Foundational Learning

- Concept: Multi-armed bandit problem structure
  - Why needed here: Understanding the exploration-exploitation tradeoff and the role of external randomness in bandit problems is crucial for interpreting LLM behavior
  - Quick check question: What is the fundamental difference between a greedy algorithm and a bandit algorithm in this context?

- Concept: In-context learning and prompt engineering
  - Why needed here: The success of LLM agents depends heavily on how information is presented in the prompt and how the model is guided to reason about it
  - Quick check question: How does the format of the interaction history (raw vs. summarized) affect the model's ability to make decisions?

- Concept: Chain-of-thought reasoning
  - Why needed here: CoT prompts explicit reasoning steps, which can help the model apply algorithmic thinking to exploration problems
  - Quick check question: What is the difference between a zero-shot CoT prompt and a reinforced CoT prompt?

## Architecture Onboarding

- Component map: LLM agent (GPT-3.5, GPT-4, Llama2) → prompt template (scenario, framing, history format, CoT, distribution output) → MAB environment (arms, means, horizon) → performance metrics (suffix failures, uniform-like failures, reward)
- Critical path: Generate prompt → send to LLM → receive arm selection → update history → repeat for T rounds → analyze suffix failures and uniform-like failures
- Design tradeoffs: Longer histories provide more information but increase token costs; summarized histories reduce information but improve processing; temperature settings trade off determinism for randomness
- Failure signatures: High suffix failure frequency (>15% at T/2), high K*MinFrac indicating uniform-like behavior, reward significantly below baseline algorithms
- First 3 experiments:
  1. Test basic configuration (BNRN0) on hard MAB instance with N=10 replicates, T=100
  2. Test reinforced CoT configuration (BSS eC0) on hard MAB instance with N=10 replicates, T=100
  3. Test temperature variation (BNRN0 vs BNRN1) on hard MAB instance with N=10 replicates, T=100

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLM-based agents perform in contextual bandits with high-dimensional contexts?
- Basis in paper: [explicit] The paper notes that external summarization may not be possible in more complex settings like contextual bandits with many contexts, and that even for linear contextual bandits, the approach may not be applicable without substantial algorithmic intervention.
- Why unresolved: The paper only experiments with simple multi-armed bandits and does not test contextual bandits.
- What evidence would resolve it: Experiments with LLMs in contextual bandit settings with varying context dimensions, comparing performance with and without external summarization or algorithmic interventions.

### Open Question 2
- Question: What is the impact of different pre-training datasets on LLMs' ability to explore in bandit tasks?
- Basis in paper: [inferred] The paper suggests that training interventions, such as dataset curation, may be required to improve LLM exploration capabilities. It also mentions the need for methodological advancements to enable cost-effective diagnosis of LLM-agent behavior.
- Why unresolved: The paper does not investigate the effect of different pre-training datasets on exploration behavior.
- What evidence would resolve it: Experiments training LLMs on different datasets (e.g., with and without exploration examples) and testing their performance on bandit tasks.

### Open Question 3
- Question: Can fine-tuning LLMs on bandit-specific data improve their exploration capabilities?
- Basis in paper: [explicit] The paper concludes that non-trivial algorithmic interventions, such as fine-tuning, may be required to empower LLM-based decision making agents in complex settings.
- Why unresolved: The paper only evaluates "native" LLM performance without training interventions.
- What evidence would resolve it: Experiments fine-tuning LLMs on bandit-specific data (e.g., expert demonstrations or reinforcement learning trajectories) and comparing their exploration performance to baseline algorithms and non-fine-tuned LLMs.

## Limitations

- Findings hinge on a single successful configuration (GPT-4 with summarized history and reinforced CoT), raising questions about generalizability
- Lack of direct evidence about how summarization and CoT reasoning interact suggests the mechanism may be more fragile than presented
- Paper doesn't explore whether prompting techniques scale to more complex bandit environments or transfer to other decision-making tasks

## Confidence

- Medium: The finding that most LLM configurations fail to explore robustly is well-supported by experimental results, though failure modes could benefit from deeper analysis
- Low: The claim that external summarization is "crucial" for successful exploration rests on a single successful configuration and lacks mechanistic explanation
- Medium: The demonstration that careful prompt engineering can enable exploration in LLMs is compelling but needs validation across different model families and problem complexities

## Next Checks

1. Cross-model validation: Test the successful GPT-4 configuration (summarized history + reinforced CoT) on GPT-3.5 and Llama2 to determine if the approach generalizes across model architectures

2. Mechanism ablation: Systematically vary the summarization format (different statistics, different levels of detail) while keeping CoT constant to isolate the contribution of external summarization to successful exploration

3. Scaling test: Increase the number of arms (e.g., 10 arms instead of 5) and/or arm reward gaps to evaluate whether the prompting techniques scale to more challenging bandit problems where exploration is more critical