---
ver: rpa2
title: 'Inclusivity in Large Language Models: Personality Traits and Gender Bias in
  Scientific Abstracts'
arxiv_id: '2406.19497'
source_url: https://arxiv.org/abs/2406.19497
tags:
- llms
- features
- scientific
- gender
- abstracts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates inclusivity in large language models (LLMs)\
  \ by evaluating their performance on generating scientific abstracts and identifying\
  \ gender biases. Three prominent LLMs\u2014Claude 3 Opus, Mistral AI Large, and\
  \ Gemini 1.5 Flash\u2014are analyzed using the Linguistic Inquiry and Word Count\
  \ (LIWC) framework to extract lexical, psychological, and social features."
---

# Inclusivity in Large Language Models: Personality Traits and Gender Bias in Scientific Abstracts

## Quick Facts
- arXiv ID: 2406.19497
- Source URL: https://arxiv.org/abs/2406.19497
- Authors: Naseela Pervez; Alexander J. Titus
- Reference count: 31
- Key outcome: LLMs amplify gender disparities in scientific writing style, reflecting and exaggerating gender-specific linguistic patterns.

## Executive Summary
This study investigates inclusivity and gender bias in large language models (LLMs) by analyzing their performance on generating scientific abstracts. Using the LIWC framework, the research evaluates lexical, psychological, and social features across three prominent LLMs—Claude 3 Opus, Mistral AI Large, and Gemini 1.5 Flash. The findings reveal that while LLMs closely align with human-authored text in terms of personality traits, they also amplify gender-based stylistic differences, potentially perpetuating stereotypes in scientific writing.

## Method Summary
The study uses 3,390 scientific abstracts from the CORE dataset, with author genders assigned via the gender-extractor library. Abstracts are rewritten using three LLMs, and LIWC-22 features are extracted for both human and LLM-generated texts. Statistical analyses include Pearson correlation to assess feature alignment and two-sample t-tests to detect gender-based differences across 35 LIWC features.

## Key Results
- LLMs closely align with human personality traits in scientific writing, showing strong positive correlations in LIWC features.
- Gender gaps in politeness, conflict, and risk-related language are present in both human and LLM-generated abstracts, with LLMs amplifying these disparities.
- Statistical tests reveal significant differences between male and female authors in 15 out of 35 analyzed LIWC features.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs amplify gender disparities in scientific writing style.
- Mechanism: LLMs preserve and exaggerate gender-specific linguistic patterns from human-authored texts, such as differences in politeness, conflict-related language, and risk-associated words.
- Core assumption: LLMs are trained on human-written data containing gender-biased patterns.
- Evidence anchors:
  - [section] "LLMs not only reflect but also magnify these differences, potentially perpetuating gender stereotypes in scientific writing."
  - [section] "However, this distinction is underestimated in LLM-generated abstracts, contributing to a gender gap in the portrayal of politeness in writing styles (see table 3)."
- Break condition: If LLMs were trained on debiased datasets or if bias mitigation techniques were applied during fine-tuning.

### Mechanism 2
- Claim: LLMs align closely with human personality traits in scientific writing.
- Mechanism: LLMs capture lexical, psychological, and social features of human-authored abstracts using frameworks like LIWC.
- Core assumption: Training data includes sufficient human-authored scientific abstracts reflecting diverse personality traits.
- Evidence anchors:
  - [abstract] "The study reveals that while these models generally produce text closely resembling human-authored content, variations in stylistic features suggest significant gender biases."
  - [section] "Our findings demonstrate a strong positive correlation between LIWC features in human-written and LLM-generated scientific abstracts across all three LLM models."
- Break condition: If LLMs were fine-tuned on a non-scientific corpus or if LIWC is not a reliable measure for scientific text.

### Mechanism 3
- Claim: Gender gaps in scientific writing are quantifiable using LIWC features.
- Mechanism: Statistical tests (t-tests) reveal significant differences between male and female authors in features like politeness, conflict, and risk, which are then reflected (and sometimes amplified) by LLMs.
- Core assumption: LIWC features are valid indicators of gender-based writing style differences.
- Evidence anchors:
  - [section] "We conducted a two-sample t-test to compare the LIWC features between male and female authors. Among the 35 features analyzed, 15 features showed a statistically significant t-statistic value (p-value < 0.05)."
  - [section] "male authors tend to adopt a significantly more polite writing style, approximately five times more polite on average than females that is consistent with existing literature [23]."
- Break condition: If the LIWC dictionary is not suited for scientific abstracts or if the sample size is too small to detect real differences.

## Foundational Learning

- Concept: Linguistic Inquiry and Word Count (LIWC)
  - Why needed here: LIWC provides a standardized way to quantify lexical, psychological, and social features of text, which is essential for comparing human and LLM-generated scientific writing.
  - Quick check question: What are the three main categories of features that LIWC analyzes in text?

- Concept: Pearson correlation coefficient
  - Why needed here: It measures the strength and direction of the linear relationship between LIWC features of human and LLM-generated texts, indicating alignment.
  - Quick check question: What does a Pearson correlation coefficient of 0.8 between two LIWC features suggest about their relationship?

- Concept: Two-sample t-test
  - Why needed here: It tests whether there are significant differences in LIWC feature means between male and female authors, revealing gender-based writing style gaps.
  - Quick check question: What p-value threshold is typically used to determine statistical significance in this study?

## Architecture Onboarding

- Component map:
  - Data collection: CORE dataset abstracts → gender assignment via gender-extractor library
  - LLM generation: Claude 3 Opus, Mistral AI Large, Gemini 1.5 Flash → prompt-based rewriting
  - Analysis framework: LIWC-22 → feature extraction for lexical, psychological, social traits
  - Statistical analysis: Pearson correlation → alignment assessment; t-tests → gender bias detection
  - Output: Quantitative comparison tables and visualizations

- Critical path:
  1. Extract and gender-label scientific abstracts
  2. Generate rewritten abstracts using each LLM
  3. Compute LIWC features for all texts
  4. Perform correlation analysis (human vs LLM)
  5. Perform t-tests (male vs female, human vs LLM)
  6. Interpret results for bias and alignment

- Design tradeoffs:
  - Using only abstracts limits context but ensures consistency and manageability.
  - LIWC may not capture all nuances of scientific language but offers standardized metrics.
  - Three LLMs provide breadth but may miss newer or smaller models with different biases.

- Failure signatures:
  - Low or inconsistent Pearson correlations suggest LLMs do not capture human writing style.
  - Non-significant t-test results could indicate insufficient sample size or lack of real gender differences.
  - Overestimation or underestimation of certain LIWC features by LLMs signals bias amplification.

- First 3 experiments:
  1. Run LIWC on a small set of human abstracts and verify feature extraction works as expected.
  2. Generate LLM-rewritten abstracts for a subset and compare LIWC features to originals for correlation.
  3. Perform t-tests on a balanced gender sample to check for initial gender-based differences.

## Open Questions the Paper Calls Out
None

## Limitations
- The study relies on abstracts only, which may not fully represent the broader context and style of scientific writing.
- Gender assignment using the gender-extractor library may introduce inaccuracies, particularly for non-Western names or ambiguous cases.
- LIWC, while standardized, may not fully capture the nuances of scientific language or account for evolving writing norms.

## Confidence
- Alignment with human personality traits: Medium
- Gender bias amplification: Medium
- Statistical significance of findings: Medium

## Next Checks
1. Validate gender assignment accuracy by manually reviewing a random sample of abstracts to assess the reliability of the gender-extractor library.
2. Test LIWC feature extraction on a diverse set of scientific texts to confirm its suitability for capturing relevant stylistic and psychological traits in this domain.
3. Conduct a pilot study with a balanced gender sample to ensure sufficient statistical power for detecting meaningful gender-based differences in LIWC features.