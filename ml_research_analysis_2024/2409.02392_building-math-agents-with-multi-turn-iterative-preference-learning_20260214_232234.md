---
ver: rpa2
title: Building Math Agents with Multi-Turn Iterative Preference Learning
arxiv_id: '2409.02392'
source_url: https://arxiv.org/abs/2409.02392
tags:
- arxiv
- learning
- iterative
- preference
- math
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multi-turn direct preference learning framework
  to enhance the mathematical problem-solving capabilities of large language models
  (LLMs) by integrating external tools like code interpreters and leveraging trajectory-level
  feedback. The proposed framework includes multi-turn DPO and multi-turn KTO algorithms,
  designed to handle the complexities of multi-turn reasoning and external tool integration,
  which existing single-turn methods cannot address.
---

# Building Math Agents with Multi-Turn Iterative Preference Learning

## Quick Facts
- arXiv ID: 2409.02392
- Source URL: https://arxiv.org/abs/2409.02392
- Reference count: 40
- Primary result: Multi-turn preference learning framework improves math reasoning accuracy from 77.5% to 83.9% on GSM8K and 46.1% to 51.2% on MATH datasets.

## Executive Summary
This paper introduces a multi-turn direct preference learning framework to enhance the mathematical problem-solving capabilities of large language models (LLMs) by integrating external tools like code interpreters and leveraging trajectory-level feedback. The proposed framework includes multi-turn DPO and multi-turn KTO algorithms, designed to handle the complexities of multi-turn reasoning and external tool integration, which existing single-turn methods cannot address. The effectiveness of the framework is validated on the GSM8K and MATH datasets, demonstrating substantial improvements in model performance: a supervised fine-tuned Gemma-1.1-it-7B model's accuracy increased from 77.5% to 83.9% on GSM8K and from 46.1% to 51.2% on MATH, while a Gemma-2-it-9B model improved from 84.1% to 86.3% on GSM8K and from 51.0% to 54.5% on MATH.

## Method Summary
The method employs a two-stage approach: supervised fine-tuning on an Open-MathInstruct dataset followed by iterative preference learning. The iterative process uses multi-turn DPO and KTO algorithms with trajectory-level preference signals, where trajectories are collected using mixture sampling (20 from current model, 10 from previous) and annotated based on final answer correctness. Key innovations include masking user-turn tokens during training to handle deterministic external tool responses, dynamically updating the reference model at each iteration for better in-domain performance, and using KL regularization to balance exploration and exploitation. The framework is evaluated on GSM8K and MATH datasets with Gemma and Mistral model families.

## Key Results
- Gemma-1.1-it-7B model accuracy improved from 77.5% to 83.9% on GSM8K
- Gemma-1.1-it-7B model accuracy improved from 46.1% to 51.2% on MATH
- Gemma-2-it-9B model accuracy improved from 84.1% to 86.3% on GSM8K
- Gemma-2-it-9B model accuracy improved from 51.0% to 54.5% on MATH

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Masking out user-turn tokens and external messages during training allows the model to focus on learning the reasoning step without being penalized for predicting off-policy external observations.
- **Mechanism:** The multi-turn DPO loss excludes log probabilities of user-turn tokens by setting their labels to -100, preventing the gradient from trying to fit deterministic external tool responses. This avoids the mismatch between the reference model (which generates the external message) and the current policy (which should only reason about it).
- **Core assumption:** External tool messages are deterministic given the history; i.e., term (C) in the derivation vanishes.
- **Evidence anchors:**
  - [abstract] "the primary modification is to mask out irrelevant tokens during training"
  - [section 2.2] "we only assume access to the trajectory-level preference, but not an action-level one"
  - [corpus] weak; the cited works do not discuss this masking explicitly, only the present paper does.
- **Break condition:** If external messages become stochastic (e.g., human feedback in chat), term (C) ≠ 0 and the simple masking no longer yields the correct optimality condition.

### Mechanism 2
- **Claim:** Updating the reference model at each iteration (non-regularized target) allows the policy to move farther from the SFT initialization, improving in-domain reasoning performance at the expense of diversity.
- **Mechanism:** By setting πₜ, ref = π₁,ₜ₋₁, the KL penalty between consecutive policies vanishes in the limit, so the objective becomes pure reward maximization (eq. 15) rather than staying close to π₀.
- **Core assumption:** The reward signal (final answer correctness) is sufficiently accurate and concentrated on a small set of high-quality trajectories, making exploration less critical than exploitation.
- **Evidence anchors:**
  - [abstract] "optimizing the second target in (15) leads to better performance on in-domain test sets"
  - [section 2.3] "this dynamic approach optimizes the non-regularized reward"
  - [section 3.3] "we observe that if we update the reference model at each iteration, the final model outperforms the one with a fixed reference model π₀"
- **Break condition:** If the reward is noisy or the solution space is highly multimodal, the lack of KL regularization can cause premature collapse to suboptimal policies.

### Mechanism 3
- **Claim:** Mixture sampling (current model + last-iteration model) preserves trajectory diversity, preventing collapse of the contrastive preference signal and enabling continued gains across iterations.
- **Mechanism:** By collecting 20 trajectories from the current policy and 10 from the previous one, the dataset contains both high-reward and exploratory paths, maintaining a useful signal for the pairwise ranking loss.
- **Core assumption:** The last-iteration model still produces sufficiently diverse and occasionally better trajectories than the current one, providing useful negative examples.
- **Evidence anchors:**
  - [section 3.3] "we explored two data collection strategies: (1) on-policy sampling … (2) mixture sampling … the final model considerably outperform the one with only on-policy sampling"
  - [section 3.3] "we observe that on-policy sampling fails to improve MATH test accuracy in the third iteration"
  - [corpus] weak; similar mixture strategies are mentioned in Dong et al. (2024) but not validated in the same mathematical reasoning context.
- **Break condition:** If the last-iteration model becomes too poor or if computational budget forces N to be very small, mixture sampling may add noise rather than useful diversity.

## Foundational Learning

- **Concept:** Markov Decision Process (MDP) formulation of multi-turn reasoning with external tools.
  - **Why needed here:** Provides a formal framework to derive optimality conditions for preference learning that account for deterministic tool responses and trajectory-level rewards.
  - **Quick check question:** In the MDP, what are the states, actions, and observations in the math reasoning task?

- **Concept:** Bradley-Terry model for trajectory-level pairwise preferences.
  - **Why needed here:** Allows the use of relative feedback (win/lose) rather than absolute rewards, which is more practical when only final answers are labeled.
  - **Quick check question:** How does the BT model compute the probability that one trajectory is preferred over another?

- **Concept:** Gibbs distribution as the optimal policy under KL-regularized RL.
  - **Why needed here:** Justifies the form of the policy update in M-DPO and M-KTO and connects them to the optimal Q-values.
  - **Quick check question:** What is the closed-form solution for the optimal policy in a KL-regularized MDP?

## Architecture Onboarding

- **Component map:** Prompt generator -> SFT checkpoint -> Online iterative loop: Trajectory collector (mixture sampling) -> Preference annotator (final answer check) -> M-DPO/M-KTO trainer -> New checkpoint

- **Critical path:**
  1. Fine-tune base model on Open-MathInstruct (SFT) -> 3 epochs max
  2. For each iteration:
     - Generate N=30 trajectories per prompt (20 current + 10 last)
     - Filter to keep only trajectories with different final-answer correctness
     - Mask user-turn tokens in loss computation
     - Train 1 epoch M-DPO/M-KTO with learning rate 4e-7 (Gemma-1.1) or 2e-7 (Gemma-2/Mistral)
     - Evaluate on held-out validation set, select best checkpoint

- **Design tradeoffs:**
  - KL coefficient η: lower -> larger moves but risk collapse; higher -> stable but slow improvement
  - Reference model update frequency: fixed π₀ -> more diversity; dynamic -> better in-domain reward
  - N in best-of-N: larger -> better coverage of hard problems but higher cost

- **Failure signatures:**
  - Training loss collapses to near zero while validation accuracy plateaus -> over-regularization or insufficient diversity
  - Model rapidly forgets SFT reasoning patterns -> learning rate too high or reference too far
  - No improvement across iterations -> on-policy sampling only, no mixture, or N too small

- **First 3 experiments:**
  1. Run SFT for 2 epochs, checkpoint, evaluate pass@1 on GSM8K validation.
  2. First M-DPO iteration with η=0.1, mixture sampling, evaluate after 200 steps; compare to SFT baseline.
  3. Second iteration with dynamic reference, same η, measure improvement; if no gain, switch to fixed reference and re-run.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can preference learning algorithms be extended to handle stochastic observations in multi-turn reasoning tasks with external tools?
- Basis in paper: [explicit] The paper explicitly identifies that the current multi-turn DPO and M-KTO losses are only valid for deterministic transitions, and suggests that a more complex algorithm is needed for stochastic observations, involving constructing a value network and using Monte-Carlo methods to estimate the bias term (C).
- Why unresolved: The paper only briefly mentions the need for a more complex algorithm for stochastic observations but does not provide a detailed solution or implementation.
- What evidence would resolve it: A concrete algorithm that can handle stochastic observations in multi-turn reasoning tasks, along with empirical results demonstrating its effectiveness compared to the deterministic case.

### Open Question 2
- Question: What is the impact of different exploration strategies (e.g., west-of-n sampling, MCTS) on the performance of online iterative multi-turn preference learning in mathematical reasoning tasks?
- Basis in paper: [explicit] The paper mentions that advanced exploration strategies like west-of-n sampling and MCTS could be used to prevent diversity collapse and provide more meaningful signals for iterative preference learning, but does not explore these strategies in detail.
- Why unresolved: The paper only briefly mentions the potential benefits of advanced exploration strategies but does not conduct experiments or provide a theoretical analysis of their impact.
- What evidence would resolve it: Experiments comparing the performance of online iterative multi-turn preference learning with different exploration strategies (e.g., west-of-n sampling, MCTS) on mathematical reasoning tasks, along with a theoretical analysis of their exploration efficiency.

### Open Question 3
- Question: How can preference learning algorithms be adapted to leverage step-wise supervision signals in multi-turn reasoning tasks?
- Basis in paper: [explicit] The paper mentions that while the focus is on trajectory-level comparison, the preference signal itself can be generated in a step-by-step supervision manner, and the algorithmic design can be readily combined with MCTS-based data collection strategies to learn step-level signals.
- Why unresolved: The paper does not provide a concrete algorithm or empirical results for adapting preference learning to step-wise supervision signals.
- What evidence would resolve it: A concrete algorithm that can leverage step-wise supervision signals in multi-turn reasoning tasks, along with empirical results demonstrating its effectiveness compared to trajectory-level comparison.

### Open Question 4
- Question: What is the relationship between the choice of KL regularization coefficient and the trade-off between per-iteration improvement and exploration efficiency in online iterative multi-turn preference learning?
- Basis in paper: [explicit] The paper conducts an ablation study on the impact of the KL regularization coefficient and observes that a moderate coefficient of 0.1 leads to the best performance, balancing per-iteration improvement and exploration efficiency.
- Why unresolved: The paper provides empirical evidence but does not offer a theoretical explanation for the observed relationship between the KL regularization coefficient and the trade-off.
- What evidence would resolve it: A theoretical analysis of the relationship between the KL regularization coefficient and the trade-off between per-iteration improvement and exploration efficiency, supported by empirical results.

### Open Question 5
- Question: How does the performance of preference learning algorithms scale with the size and quality of the SFT dataset in multi-turn reasoning tasks?
- Basis in paper: [explicit] The paper observes that the success of preference learning is on top of a well-trained SFT model and that the final model performance can be further improved with more high-quality SFT data.
- Why unresolved: The paper does not conduct experiments to systematically investigate the relationship between the size and quality of the SFT dataset and the performance of preference learning algorithms.
- What evidence would resolve it: Experiments varying the size and quality of the SFT dataset and measuring the corresponding performance of preference learning algorithms on multi-turn reasoning tasks.

## Limitations

- The masking mechanism for user-turn tokens only works when external tool responses are deterministic, limiting applicability to scenarios with stochastic or human-generated feedback.
- Performance gains are demonstrated primarily on in-domain test sets without quantifying the trade-off between accuracy improvements and reasoning diversity.
- The framework's robustness to non-deterministic external tools and performance on out-of-distribution mathematical problems remain untested assumptions.

## Confidence

- **High confidence**: The mathematical derivations of the multi-turn DPO and KTO algorithms are sound and follow established RL and preference learning theory. The implementation details for masking user-turn tokens and the mixture sampling strategy are clearly specified.

- **Medium confidence**: The empirical improvements on GSM8K and MATH datasets are well-documented, but the relative contribution of each mechanism (masking, reference model updates, mixture sampling) to the overall performance gain is not isolated through controlled ablations.

- **Low confidence**: The framework's robustness to non-deterministic external tools and its performance on out-of-distribution mathematical reasoning tasks remain untested assumptions based on the current experimental design.

## Next Checks

1. Test the masking mechanism's effectiveness when external tool responses become stochastic or include human feedback, measuring the impact on learning stability and final accuracy.

2. Conduct ablation studies isolating the contributions of multi-turn masking, reference model updating, and mixture sampling to determine which components drive the most significant performance improvements.

3. Evaluate the iterative framework on out-of-distribution mathematical reasoning tasks to assess whether the performance gains generalize beyond the GSM8K and MATH datasets used for training.