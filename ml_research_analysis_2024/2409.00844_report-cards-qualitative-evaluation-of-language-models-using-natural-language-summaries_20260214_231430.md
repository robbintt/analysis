---
ver: rpa2
title: 'Report Cards: Qualitative Evaluation of Language Models Using Natural Language
  Summaries'
arxiv_id: '2409.00844'
source_url: https://arxiv.org/abs/2409.00844
tags:
- report
- cards
- card
- student
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Report Cards, a method for generating interpretable
  natural language summaries of LLM capabilities. The approach uses an iterative PRESS
  algorithm to create concise bullet-point summaries of model behavior across specific
  skills or topics, addressing the limitations of purely quantitative benchmarks.
---

# Report Cards: Qualitative Evaluation of Language Models Using Natural Language Summaries

## Quick Facts
- arXiv ID: 2409.00844
- Source URL: https://arxiv.org/abs/2409.00844
- Authors: Blair Yang; Fuyang Cui; Keiran Paster; Jimmy Ba; Pashootan Vaezipoor; Silviu Pitis; Michael R. Zhang
- Reference count: 40
- Key outcome: PRESS-generated Report Cards achieve 69% contrastive accuracy versus 61% for few-shot baselines

## Executive Summary
This paper introduces Report Cards, a method for generating interpretable natural language summaries of LLM capabilities that address the limitations of purely quantitative benchmarks. The approach uses an iterative PRESS algorithm to create concise bullet-point summaries of model behavior across specific skills or topics. The evaluation framework assesses Report Cards on three criteria: specificity (measured via contrastive accuracy), faithfulness (measured via Elo rating correlation), and interpretability (measured via human ratings). Experimental results show that PRESS-generated Report Cards outperform few-shot baselines and provide more nuanced insights into model capabilities compared to traditional evaluation approaches.

## Method Summary
The method uses an iterative PRESS algorithm that progressively refines Report Cards through multiple rounds of evaluation on subsets of data. The evaluator model generates temporary summaries on quiz subsets and merges them with previous summaries, allowing capture of nuanced behaviors that might be missed in a single pass. Report Cards are evaluated using three metrics: contrastive accuracy (measuring specificity by testing if cards can distinguish between models), Card Elo correlation with ground-truth Elo (measuring faithfulness), and human ratings on relevance, informativeness, and clarity (measuring interpretability).

## Key Results
- PRESS-generated Report Cards achieve 69% contrastive accuracy versus 61% for few-shot baselines
- Strong faithfulness correlations with R² scores between Card Elo and ground-truth Elo
- Human ratings for relevance, informativeness, and clarity average above 4/5
- PRESS-generated cards are robust to stylistic variations in model outputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PRESS generates more faithful and specific Report Cards than one-pass prompting by iteratively refining summaries on subsets of data
- Mechanism: PRESS uses progressive refinement where the evaluator generates temporary summaries on quiz subsets and merges them with previous summaries, allowing capture of nuanced behaviors that might be missed in a single pass over all data
- Core assumption: Model capabilities can be better captured by synthesizing multiple partial summaries than by one comprehensive summary
- Evidence anchors:
  - [abstract] "We also propose an iterative algorithm for generating Report Cards without human supervision and explore its efficacy by ablating various design choices"
  - [section 2.3] "We call our approach Progressive Refinement for Effective Skill Summarization (PRESS)"
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.535, average citations=0.0. Weak corpus support for PRESS-specific mechanism
- Break condition: If the merging step fails to preserve critical information or introduces contradictions between iterations

### Mechanism 2
- Claim: Contrastive accuracy metric effectively measures specificity by testing if Report Cards can distinguish between models using a guesser
- Mechanism: A guesser LLM is given a quiz, model completions, and Report Cards in randomized order, and must correctly match completions to Report Cards; accuracy indicates how well cards capture model-specific behaviors
- Core assumption: A successful guesser that can match completions to cards indicates the cards capture distinctive model characteristics
- Evidence anchors:
  - [abstract] "We quantify specificity using a contrastive metric, which measures how effectively Report Cards can be used to differentiate between models"
  - [section 2.2] "We measure the specificity of Report Cards using a contrastive accuracy metric, which assesses how well two student models can be distinguished given their Report Cards and a quizQ of k test questions"
  - [corpus] Weak corpus support for contrastive accuracy in LLM evaluation specifically
- Break condition: If guesser succeeds by matching stylistic features rather than substantive capabilities

### Mechanism 3
- Claim: Card Elo provides a faithful measure by correlating with ground-truth Elo derived from direct model comparisons
- Mechanism: Elo ratings are computed for models based on either direct completion comparisons (ground-truth) or Report Card comparisons (card-based); high correlation indicates faithfulness of cards to actual capabilities
- Core assumption: If Report Cards accurately represent model capabilities, Elo ratings derived from them should correlate with Elo ratings from direct comparisons
- Evidence anchors:
  - [abstract] "We assess faithfulness by comparing estimates of model performance derived from Report Cards to those based on direct output comparisons"
  - [section 2.2] "We use an Elo rating derived from pairwise comparisons of Report Cards" and "compute the Coefficient of determination (R2) between the two sets of Elo ratings"
  - [corpus] No direct corpus evidence for Elo-based evaluation of LLM summaries
- Break condition: If correlation breaks down for specific topics or model pairs

## Foundational Learning

- Concept: Elo rating system
  - Why needed here: Provides a principled way to quantify and compare model capabilities based on pairwise comparisons
  - Quick check question: How does the expected outcome E in Elo rating calculation relate to the rating difference between two models?

- Concept: Contrastive evaluation
  - Why needed here: Allows measuring if Report Cards capture distinctive model behaviors that can be used to differentiate between models
  - Quick check question: Why does the contrastive accuracy metric use randomized ordering of model completions and Report Cards?

- Concept: Few-shot prompting
  - Why needed here: Serves as a baseline for comparison and helps understand if Report Cards provide more value than simply showing example completions
  - Quick check question: What limitation does the few-shot baseline have compared to PRESS-generated Report Cards?

## Architecture Onboarding

- Component map: Student models (evaluated) ← Evaluator model (generates Report Cards via PRESS) ← Judge/guesser models (assess Report Cards) ← Human annotators (validate interpretability)
- Critical path: PRESS algorithm takes student model completions → generates Report Cards → contrastive accuracy and Elo metrics evaluate quality → human validation confirms interpretability
- Design tradeoffs: Concise bullet-point format vs. more detailed hierarchical/paragraph formats; quiz length vs. coverage of capabilities; number of PRESS iterations vs. summary quality
- Failure signatures: Low contrastive accuracy indicates cards don't capture distinguishing features; low Elo correlation indicates poor faithfulness; low human scores indicate poor interpretability
- First 3 experiments:
  1. Generate Report Cards using PRESS and compare contrastive accuracy against few-shot baseline
  2. Compute Card Elo vs Ground-truth Elo correlation to measure faithfulness
  3. Collect human ratings for a subset of Report Cards to validate interpretability claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Report Cards vary across different model sizes and architectures (e.g., transformer-based vs. other architectures)?
- Basis in paper: Inferred from the discussion of various models used (Llama-3.1-8B, Llama-3.1-70B, Llama-3.1-405B, Mistral-7B, Mixtral-8x7B) but no explicit comparison of performance across architectures is provided.
- Why unresolved: The paper tests multiple models but does not systematically analyze how Report Card quality differs based on model size or architectural differences.
- What evidence would resolve it: A controlled experiment comparing Report Card quality across models of different sizes and architectures using the same evaluation metrics.

### Open Question 2
- Question: What is the optimal quiz length (k) for generating high-quality Report Cards, and how does it trade off against computational cost?
- Basis in paper: Inferred from the PRESS algorithm which uses quizzes of length k, and the contrastive accuracy experiments which use 3-question quizzes, but no systematic exploration of different k values is provided.
- Why unresolved: The paper uses k=3 for contrastive experiments and k=8 for PRESS progression, but doesn't explore how varying k affects Report Card quality or computational efficiency.
- What evidence would resolve it: An ablation study varying k values and measuring the resulting Report Card quality and generation time.

### Open Question 3
- Question: How do Report Cards perform when evaluating models on open-ended tasks versus multiple-choice questions?
- Basis in paper: Explicit mention that experiments focus on specific topics with existing benchmarks, and the CN Grammar dataset is described as "open-ended" but is limited to grammar correction.
- Why unresolved: The paper evaluates Report Cards primarily on structured datasets (MMLU, Adv. AI Risk, CN Grammar) but doesn't test them on truly open-ended tasks like creative writing or reasoning.
- What evidence would resolve it: Application of Report Cards to diverse open-ended tasks and comparison of their effectiveness versus traditional evaluation methods.

## Limitations
- PRESS algorithm effectiveness depends heavily on iterative refinement but exact prompts are not fully specified
- Contrastive accuracy metric may be sensitive to guesser model capabilities and could be gamed by stylistic features
- Human evaluation sample size is relatively small (12 participants rating 18 Report Cards)

## Confidence

- PRESS algorithm effectiveness: **Medium** - Strong experimental support but limited prompt specification
- Contrastive accuracy as specificity measure: **Medium** - Innovative approach but potential for stylistic matching
- Elo correlation as faithfulness measure: **High** - Principled statistical approach with clear methodology
- Human evaluation results: **Medium** - Limited sample size but consistent ratings across criteria

## Next Checks
1. Test contrastive accuracy robustness by varying guesser model sizes and measuring performance differences
2. Conduct ablation study on PRESS iteration count to determine optimal number of refinement steps
3. Evaluate Report Card performance on additional domains beyond MMLU and AI risk to assess generalizability