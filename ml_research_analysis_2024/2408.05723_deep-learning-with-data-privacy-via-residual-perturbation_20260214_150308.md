---
ver: rpa2
title: Deep Learning with Data Privacy via Residual Perturbation
arxiv_id: '2408.05723'
source_url: https://arxiv.org/abs/2408.05723
tags:
- uni00000013
- uni00000011
- uni00000018
- uni00000048
- privacy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes residual perturbation for privacy-preserving
  deep learning by injecting Gaussian noise into residual mappings of ResNets. The
  method is motivated by stochastic differential equation theory and provides differential
  privacy guarantees.
---

# Deep Learning with Data Privacy via Residual Perturbation

## Quick Facts
- arXiv ID: 2408.05723
- Source URL: https://arxiv.org/abs/2408.05723
- Authors: Wenqi Tao; Huaming Ling; Zuoqiang Shi; Bao Wang
- Reference count: 40
- Primary result: Residual perturbation achieves differential privacy by injecting Gaussian noise into ResNet residual mappings, improving privacy-utility tradeoff over DPSGD

## Executive Summary
This paper introduces residual perturbation, a novel method for privacy-preserving deep learning that injects Gaussian noise into the residual mappings of ResNets. Motivated by stochastic differential equation theory, this approach provides differential privacy guarantees while maintaining or improving classification accuracy compared to traditional methods like DPSGD. The technique leverages the inherent irreversibility of skip connections in ResNets to enhance privacy protection, and an ensemble of noise-injected ResNets further improves both privacy and accuracy. Experiments on MNIST, CIFAR10/100, and IDC datasets demonstrate the method's effectiveness in protecting membership privacy.

## Method Summary
The method injects Gaussian noise into residual mappings of ResNets, motivated by stochastic differential equation theory. Two strategies are proposed: Strategy I injects noise with calibrated variance into each residual mapping, while Strategy II uses multiplicative noise. An ensemble of noise-injected ResNets is also employed to enhance both privacy and accuracy. The approach is evaluated on image classification tasks (MNIST, CIFAR10/100, and IDC datasets) and compared against DPSGD using membership inference attacks to measure privacy protection.

## Key Results
- Residual perturbation achieves better privacy-utility tradeoff than DPSGD on MNIST, CIFAR10/100, and IDC datasets
- Ensemble of noise-injected ResNets further enhances both privacy and accuracy
- The method is computationally more efficient than DPSGD as it doesn't require computing per-sample gradients
- Residual perturbation reduces the generalization gap, which helps mitigate privacy leakage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Injecting Gaussian noise into residual mappings of ResNets guarantees differential privacy and reduces generalization gap
- Mechanism: Gaussian noise is injected into each residual mapping of ResNets, motivated by stochastic differential equation theory. This noise injection protects membership privacy and improves classification accuracy.
- Core assumption: The skip connections in ResNets provide irreversibility crucial for privacy protection, and the noise injection effectively disrupts the ability to reverse-engineer the original data from the features
- Evidence anchors: [abstract] "theoretically motivated by stochastic differential equation (SDE) theory" and "residual perturbation guarantees differential privacy (DP) and reduces the generalization gap of DL"

### Mechanism 2
- Claim: Ensemble of noise-injected ResNets enhances both privacy and accuracy
- Mechanism: Multiple ResNets with residual perturbation are trained and their outputs are combined, leading to improved classification accuracy and enhanced membership privacy protection
- Core assumption: The diversity introduced by noise injection in each ResNet contributes to better generalization and privacy, and the ensemble averaging reduces the impact of individual model noise
- Evidence anchors: [abstract] "The ensemble of noise-injected ResNets further enhances both privacy and accuracy"

### Mechanism 3
- Claim: Residual perturbation reduces reversibility of the forward propagation, protecting data privacy
- Mechanism: The continuous limit of ResNets (ODE) is reversible, but injecting noise makes it harder to reverse-engineer the input from the output, thus protecting privacy
- Core assumption: The added noise creates a non-invertible mapping between the input and output, making it difficult to reconstruct the original data
- Evidence anchors: [abstract] "The method's effectiveness relies on skip connections in ResNets, which provide irreversibility crucial for privacy protection"

## Foundational Learning

- Concept: Differential Privacy (DP)
  - Why needed here: DP is the formal privacy guarantee that the residual perturbation method aims to achieve
  - Quick check question: What is the definition of (ϵ, δ)-DP and how does it relate to the privacy guarantees of residual perturbation?

- Concept: Stochastic Differential Equations (SDEs)
  - Why needed here: SDEs provide the theoretical foundation for the noise injection strategy in residual perturbation
  - Quick check question: How do SDEs relate to the continuous limit of ResNets, and how does noise injection in SDEs translate to privacy protection?

- Concept: Generalization Gap
  - Why needed here: Reducing the generalization gap is crucial for improving the utility of privacy-preserving models
  - Quick check question: How does residual perturbation reduce the generalization gap, and why is this important for privacy protection?

## Architecture Onboarding

- Component map: Residual Perturbation Module -> Ensemble Module -> Privacy Analysis Module
- Critical path: Inject noise into residual mappings → Train perturbed ResNets → Ensemble predictions → Analyze privacy guarantees
- Design tradeoffs: Noise level vs. accuracy (higher noise provides better privacy but may reduce accuracy); Ensemble size vs. computational cost (larger ensembles improve accuracy but increase computational overhead)
- Failure signatures: High membership inference attack success rate (indicates insufficient privacy protection); Significant accuracy degradation (suggests noise injection is too aggressive)
- First 3 experiments: 1) Baseline: Train standard ResNet and evaluate accuracy and privacy; 2) Residual Perturbation: Train ResNet with residual perturbation and compare to baseline; 3) Ensemble: Train ensemble of noise-injected ResNets and evaluate improvement in accuracy and privacy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical DP guarantee for Strategy II (multiplicative noise) and how tight is it compared to Strategy I and DPSGD?
- Basis in paper: [explicit] The paper mentions that the privacy guarantee for Strategy II is provided in Theorem 2, but notes that a tight DP guarantee for Strategy II remains as future work
- Why unresolved: The paper explicitly states that the tight DP guarantee for Strategy II is left as future work
- What evidence would resolve it: A rigorous proof of the DP guarantee for Strategy II with a comparison to the DP guarantees of Strategy I and DPSGD

### Open Question 2
- Question: How does the number of models in the ensemble affect the privacy-utility tradeoff in residual perturbation, and what is the optimal number of models?
- Basis in paper: [explicit] The paper mentions that tuning the noise coefficient and the number of models in the ensemble is crucial to optimize the tradeoff between accuracy and privacy
- Why unresolved: While the paper shows that the number of models affects the tradeoff, it does not provide a clear answer on the optimal number of models for different scenarios or datasets
- What evidence would resolve it: Experimental results showing the effect of varying the number of models in the ensemble on the privacy-utility tradeoff for different datasets and noise coefficients

### Open Question 3
- Question: How does the irreversibility of the SDE models (3) and (7) contribute to the privacy protection in residual perturbation, and can this be quantified?
- Basis in paper: [explicit] The paper mentions that the irreversibility of the SDEs (3) and (7) is crucial for the success of residual perturbation for privacy protection
- Why unresolved: The paper does not provide a quantitative measure of the irreversibility of the SDE models or its direct contribution to privacy protection
- What evidence would resolve it: A quantitative analysis of the irreversibility of the SDE models and its correlation with the privacy protection achieved by residual perturbation

## Limitations

- The theoretical connection between SDEs and ResNets is not fully validated empirically
- The practical impact of membership inference attacks is not thoroughly evaluated
- Computational efficiency claims compared to DPSGD are not substantiated with detailed benchmarks

## Confidence

- **High Confidence**: The theoretical motivation using SDE theory and the basic mechanism of injecting noise into residual mappings are well-established concepts
- **Medium Confidence**: The empirical results showing improved privacy-utility tradeoff are promising but may be dataset-specific and require further validation
- **Low Confidence**: The claim that residual perturbation reduces the generalization gap is theoretically motivated but lacks empirical evidence and clear connection to privacy protection

## Next Checks

1. Conduct experiments to directly measure and compare the generalization gap of standard ResNets, ResNets with residual perturbation, and DPSGD-trained models on multiple datasets
2. Evaluate the effectiveness of residual perturbation against various membership inference attacks, including adaptive attacks, on multiple datasets
3. Perform detailed benchmarks comparing the computational cost and memory usage of residual perturbation with DPSGD across different model architectures and dataset sizes