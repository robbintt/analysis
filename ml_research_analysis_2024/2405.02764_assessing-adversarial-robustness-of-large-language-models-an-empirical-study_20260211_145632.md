---
ver: rpa2
title: 'Assessing Adversarial Robustness of Large Language Models: An Empirical Study'
arxiv_id: '2405.02764'
source_url: https://arxiv.org/abs/2405.02764
tags:
- adversarial
- robustness
- language
- llms
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the adversarial robustness of large language
  models (LLMs) using a novel white-box attack approach based on geometric adversarial
  attacks. The method leverages output logits and gradients to generate adversarial
  examples through word replacement while maintaining semantic similarity.
---

# Assessing Adversarial Robustness of Large Language Models: An Empirical Study

## Quick Facts
- **arXiv ID**: 2405.02764
- **Source URL**: https://arxiv.org/abs/2405.02764
- **Reference count**: 40
- **Primary result**: White-box geometric adversarial attacks cause 15-80% accuracy drops in LLMs across classification tasks

## Executive Summary
This empirical study evaluates the adversarial robustness of large language models (LLMs) using a novel white-box attack approach based on geometric adversarial attacks. The method leverages output logits and gradients to generate adversarial examples through word replacement while maintaining semantic similarity. Experiments across five text classification datasets with models ranging from 60M to 13B parameters demonstrate that LLMs are significantly vulnerable to adversarial attacks, with accuracy drops ranging from 15% to 80% depending on model size and dataset characteristics.

## Method Summary
The study employs a geometric adversarial attack method that utilizes output logits and gradients to identify vulnerable words in input text. The attack generates adversarial examples through word replacement operations while constraining semantic similarity to maintain text coherence. The evaluation framework tests various LLM architectures including T5, OPT, and Llama across five text classification datasets. The methodology systematically varies model size, architecture type (encoder-decoder vs decoder-only), and fine-tuning techniques to assess their impact on adversarial robustness.

## Key Results
- LLMs experience accuracy drops of 15-80% under white-box adversarial attacks
- Decoder-only architectures (OPT, Llama) demonstrate better robustness than encoder-decoder models (T5)
- Model size generally improves robustness up to a saturation point
- Fine-tuning techniques like LoRA and quantization do not significantly affect adversarial resilience

## Why This Works (Mechanism)
The attack exploits the gradient information of LLM output logits to identify input perturbations that maximally disrupt the model's decision boundaries. By replacing words with semantically similar alternatives that induce large changes in logits, the attack creates examples that appear natural but cause misclassification. The geometric approach operates in the continuous embedding space of the model, allowing precise control over perturbation magnitude while maintaining text fluency.

## Foundational Learning

**Geometric Adversarial Attacks**
*Why needed*: Understanding how continuous optimization techniques can be applied to discrete text inputs
*Quick check*: Verify gradient flow through embedding layers and softmax operations

**Text Classification Metrics**
*Why needed*: Establishing baseline performance and measuring attack impact
*Quick check*: Compare accuracy, precision, and recall across datasets

**Semantic Similarity Constraints**
*Why needed*: Ensuring adversarial examples remain meaningful while effective
*Quick check*: Validate semantic preservation using similarity metrics

## Architecture Onboarding

**Component Map**: Input Text -> Embedding Layer -> Transformer Blocks -> Classification Head -> Output Logits

**Critical Path**: The path from input embeddings through transformer layers to final logits, where gradient information flows backward during attack generation

**Design Tradeoffs**: 
- White-box access enables stronger attacks but may overestimate real-world vulnerability
- Semantic similarity constraints balance attack effectiveness with natural language preservation
- Model size vs. efficiency tradeoff in achieving robustness

**Failure Signatures**: 
- Sharp accuracy drops on specific word substitutions
- Sensitivity to high-impact words identified by gradient magnitudes
- Inconsistent robustness across different dataset domains

**First Experiments**:
1. Baseline accuracy measurement across all models and datasets
2. Gradient magnitude analysis to identify most vulnerable input positions
3. Ablation study removing semantic similarity constraints to measure impact

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses only on classification tasks, leaving generative task robustness unexplored
- White-box attack methodology may overestimate vulnerability compared to black-box scenarios
- Limited dataset selection (5 text classification datasets) may restrict generalizability
- Fine-tuning technique evaluation is limited to LoRA and quantization without comprehensive defense analysis

## Confidence

**Major Claim Clusters**:
- LLMs are vulnerable to adversarial attacks (High)
- Decoder-only models are more robust than encoder-decoder models (Medium)
- Model size improves robustness up to saturation (Medium)
- Fine-tuning techniques don't significantly affect robustness (Low)

## Next Checks

1. Replicate the attack methodology under black-box constraints where gradients are not directly accessible, comparing vulnerability levels between white-box and black-box scenarios

2. Extend evaluation to generative tasks (story completion, question answering) to assess whether classification-specific vulnerabilities generalize to broader LLM applications

3. Test additional defense mechanisms beyond LoRA and quantization, including adversarial training, input preprocessing, and ensemble methods, to identify potential robustness improvements