---
ver: rpa2
title: Interweaving Memories of a Siamese Large Language Model
arxiv_id: '2412.17383'
source_url: https://arxiv.org/abs/2412.17383
tags:
- imsm
- peft
- arxiv
- llms
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a parameter-efficient fine-tuning framework
  called IMSM that addresses catastrophic forgetting in large language models (LLMs).
  The method uses a siamese LLM with original and fine-tuned parameters to generate
  two distinct memories, then interweaves them using a query-aware gate mechanism
  to generate the next token.
---

# Interweaving Memories of a Siamese Large Language Model

## Quick Facts
- **arXiv ID**: 2412.17383
- **Source URL**: https://arxiv.org/abs/2412.17383
- **Authors**: Xin Song; Zhikai Xue; Guoxiu He; Jiawei Liu; Wei Lu
- **Reference count**: 23
- **Key outcome**: IMSM outperforms classical PEFT methods (LoRA, (IA)3, AdaLoRA) and state-of-the-art methods (DoRA, LoRAMoE) in both downstream task performance and mitigation of catastrophic forgetting while maintaining comparable time and space efficiency.

## Executive Summary
This paper introduces IMSM (Interweaving Memories of a Siamese Model), a parameter-efficient fine-tuning framework designed to address catastrophic forgetting in large language models. The method uses a siamese architecture with original and fine-tuned parameters to generate distinct memories, then interweaves them using a query-aware gate mechanism to generate the next token. IMSM is evaluated across four datasets and four different LLMs, demonstrating superior performance compared to existing PEFT methods.

## Method Summary
IMSM creates a siamese LLM architecture with two identical models - one with frozen pre-trained parameters and one with fine-tuned parameters. During inference, both models generate hidden states for the same input, which are then combined using a query-aware gate mechanism. The gate takes averaged query representations from both models, concatenated with current token hidden states, and uses element-wise multiplication to determine the optimal contribution from each memory source. This approach preserves original knowledge while incorporating task-specific enhancements.

## Key Results
- IMSM achieves 99.05% performance on target datasets compared to 96.91% for standard PEFT methods
- Reduces catastrophic forgetting by 17.6% compared to existing PEFT approaches
- Maintains comparable time and space efficiency despite doubling model parameters
- Outperforms state-of-the-art methods (DoRA, LoRAMoE) across all evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The siamese LLM structure preserves original knowledge by maintaining a frozen copy while fine-tuning another copy.
- Mechanism: Two identical LLMs - one frozen with pre-trained parameters, one fine-tuned with modified parameters. Both generate hidden states that are combined via gate mechanism.
- Core assumption: Final hidden states can serve as reliable "memories" capturing model understanding.
- Evidence anchors: [abstract] "generates two distinct memories based on the pre-trained and fine-tuned parameters"; [section] "Let M represent the original LLM with parameters Wo, while M′ denotes the fine-tuned LLM with parameters W"
- Break condition: Gate mechanism fails to learn meaningful weights, degrading performance.

### Mechanism 2
- Claim: Query-aware gate dynamically adjusts balance between original and fine-tuned knowledge.
- Mechanism: Gate takes averaged query representations from both LLMs, concatenated with current token hidden states, determining optimal contribution from each memory source.
- Core assumption: Model can learn to recognize when queries require original vs. fine-tuned knowledge.
- Evidence anchors: [abstract] "Trainable parameters are marked in red"; [section] "gate = sigmoid(f(hqM ⊕ htM ⊕ htM′ ⊕ hqM′))"
- Break condition: Query features not informative enough, gate defaults to suboptimal weights.

### Mechanism 3
- Claim: Interweaving memories achieves finer-grained control than simple addition or averaging.
- Mechanism: Uses element-wise multiplication with gate values instead of arithmetic operations, allowing independent weighting of each hidden state dimension.
- Core assumption: Element-wise multiplication provides more nuanced control than simple operations.
- Evidence anchors: [abstract] "The next token is generated from the intertwined memory"; [section] "htN = gate ◦ htM + (1 − gate) ◦ htM′"
- Break condition: Gate values become saturated (close to 0 or 1), losing ability to perform nuanced blending.

## Foundational Learning

- Concept: Siamese Network Architecture
  - Why needed here: Allows maintaining two versions of the same model - one frozen with original knowledge and one fine-tuned for the task.
  - Quick check question: What is the key difference between a siamese network and an ensemble of different models?

- Concept: Catastrophic Forgetting
  - Why needed here: Understanding why standard PEFT methods cause catastrophic forgetting is crucial to appreciate the problem IMSM solves.
  - Quick check question: In what way does catastrophic forgetting manifest differently in PEFT compared to full fine-tuning?

- Concept: Parameter-Efficient Fine-Tuning (PEFT)
  - Why needed here: IMSM builds on existing PEFT methods, so understanding how LoRA, (IA)3, and other methods work is necessary to grasp the framework's innovations.
  - Quick check question: How does the parameter count of PEFT methods compare to full fine-tuning, and what are the trade-offs?

## Architecture Onboarding

- Component map: Input → Siamese LLM processing → Gate mechanism → Memory interweaving → Logits generation → Output token

- Critical path: Input → Siamese LLM processing → Gate mechanism → Memory interweaving → Logits generation → Output token

- Design tradeoffs: 
  - Memory overhead: Doubling model size vs. catastrophic forgetting
  - Computational cost: Parallel inference vs. single-pass inference
  - Parameter efficiency: Additional gate parameters vs. preserving original knowledge
  - Complexity: Query-aware gate vs. simpler fusion methods

- Failure signatures:
  - Poor performance on fine-tuning tasks: Gate may be overly conservative, relying too much on frozen knowledge
  - Poor performance on general tasks: Gate may be overly aggressive, relying too much on fine-tuned knowledge
  - Unstable training: Learning rate or rank hyperparameter issues
  - Memory inefficiency: Gate mechanism not properly optimized

- First 3 experiments:
  1. Ablation test: Remove gate mechanism and directly add two hidden states to confirm gate's contribution
  2. Sensitivity analysis: Vary gate rank parameter (r) to find optimal configuration for different PEFT methods
  3. Cross-dataset evaluation: Fine-tune on one dataset and evaluate on others to measure catastrophic forgetting mitigation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does IMSM perform when applied to other types of PEFT methods not evaluated in this study, such as BitFit or prompt tuning?
- Basis in paper: [explicit] The paper states that IMSM is "theoretically applicable to all open-source LLMs and existing PEFT methods" but only tests it with LoRA, (IA)3, AdaLoRA, DoRA, and LoRAMoE.
- Why unresolved: Experiments are limited to specific PEFT methods, leaving uncertainty about IMSM's effectiveness with other approaches.
- What evidence would resolve it: Experiments applying IMSM to additional PEFT methods like BitFit, prompt tuning, or other hybrid approaches would demonstrate its generalizability.

### Open Question 2
- Question: What is the optimal gate rank (r) value for different types of downstream tasks and model sizes?
- Basis in paper: [explicit] The paper conducts a hyperparameter analysis showing that different PEFT methods benefit from different rank values but doesn't explore optimal values across task types.
- Why unresolved: Analysis only varies rank values within the same task and doesn't examine how task complexity or model size affects optimal rank.
- What evidence would resolve it: Systematic experiments varying rank values across different task types and model sizes would identify optimal configurations.

### Open Question 3
- Question: How does IMSM perform in more challenging catastrophic forgetting scenarios, such as sequential fine-tuning on multiple tasks or domain shifts?
- Basis in paper: [inferred] Evaluation focuses on single fine-tuning task followed by testing on general knowledge benchmarks, not reflecting real-world scenarios.
- Why unresolved: Evaluation methodology is limited to simple case of catastrophic forgetting.
- What evidence would resolve it: Experiments with sequential fine-tuning on multiple tasks or with significant domain shifts would reveal IMSM's effectiveness in more challenging scenarios.

### Open Question 4
- Question: What is the impact of the gate mechanism on the model's ability to perform zero-shot or few-shot learning on tasks outside the fine-tuning domain?
- Basis in paper: [explicit] Paper mentions IMSM allows "flexible fusion of the last hidden states" but doesn't specifically test zero-shot or few-shot capabilities.
- Why unresolved: Evaluation focuses on fine-tuned task performance and general knowledge retention, not examining zero-shot/few-shot adaptation.
- What evidence would resolve it: Direct comparison of IMSM's zero-shot and few-shot performance against vanilla PEFT methods on novel tasks would quantify the impact.

## Limitations

- The siamese architecture doubles model parameters, introducing significant memory overhead despite claimed efficiency gains.
- Evaluation focuses primarily on classification and reasoning tasks with limited testing on generation or multimodal applications.
- Long-term stability under multiple sequential fine-tuning tasks is not thoroughly investigated.

## Confidence

- **High Confidence**: The core mechanism of using a siamese architecture with gate-based interweaving is well-defined and theoretically sound. Experimental results showing improved performance are robust across multiple datasets and model sizes.
- **Medium Confidence**: Efficiency claims are supported by methodology but would benefit from more granular benchmarking. Generalization across different LLM architectures is demonstrated but not extensively explored.
- **Low Confidence**: Long-term stability under multiple sequential fine-tuning tasks is not thoroughly investigated. Impact of additional gate parameters on overall model compactness is mentioned but not quantified in detail.

## Next Checks

1. **Ablation Study on Gate Complexity**: Conduct experiments varying the rank parameter (r) of the gate mechanism across different backbone LLMs to determine optimal configurations and establish whether the query-aware gate provides consistent benefits across architectures.

2. **Sequential Fine-tuning Evaluation**: Implement a multi-task sequential fine-tuning protocol where IMSM is repeatedly fine-tuned on different datasets, measuring both task-specific performance and retention of previously learned knowledge over multiple iterations.

3. **Memory Efficiency Analysis**: Quantify the actual memory overhead introduced by the siamese architecture and gate mechanism, comparing it against claimed efficiency gains through detailed profiling of GPU memory usage during both training and inference phases.