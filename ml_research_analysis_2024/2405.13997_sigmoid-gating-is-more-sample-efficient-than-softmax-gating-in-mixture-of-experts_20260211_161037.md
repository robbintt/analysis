---
ver: rpa2
title: Sigmoid Gating is More Sample Efficient than Softmax Gating in Mixture of Experts
arxiv_id: '2405.13997'
source_url: https://arxiv.org/abs/2405.13997
tags:
- experts
- gating
- expert
- function
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a rigorous theoretical analysis of the sample
  efficiency of sigmoid gating in mixture of experts (MoE) models compared to the
  widely-used softmax gating. Under a regression framework where the true regression
  function is modeled as a mixture of experts, the authors study the convergence rates
  of least squares estimators when the number of fitted experts exceeds the true number.
---

# Sigmoid Gating is More Sample Efficient than Softmax Gating in Mixture of Experts

## Quick Facts
- arXiv ID: 2405.13997
- Source URL: https://arxiv.org/abs/2405.13997
- Reference count: 40
- Primary result: Sigmoid gating achieves O(n^(-1/2)) convergence rates for expert estimation versus O(n^(-1/4)) for softmax gating with ReLU/GELU experts

## Executive Summary
This paper provides a rigorous theoretical analysis of sample efficiency in mixture of experts (MoE) models, comparing sigmoid gating to the widely-used softmax gating. Under a regression framework with least squares estimation, the authors identify two gating regimes based on whether over-specified parameters are zero or non-zero. They establish identifiability conditions for expert functions and prove that sigmoid gating delivers faster convergence rates for expert parameter estimation, particularly when the number of fitted experts exceeds the true number.

## Method Summary
The authors analyze least squares estimators in MoE models with over-specified experts, establishing identifiability conditions and convergence rates for both sigmoid and softmax gating functions. They decompose the regression estimation error using Voronoi loss functions and study two regimes based on the structure of over-specified parameters. The analysis covers both parametric experts and feed-forward networks with ReLU or GELU activations, providing theoretical convergence rates and empirical validation through synthetic data experiments.

## Key Results
- Sigmoid gating achieves parametric convergence rates of O(n^(-1/2)) for expert estimation under regime 2 (at least one over-specified parameter non-zero)
- Softmax gating converges at O(n^(-1/4)) with ReLU/GELU experts and potentially O(1/log(n)) with polynomial experts
- Sigmoid gating avoids representation collapse caused by competition among experts that occurs with softmax gating
- The results hold across both parametric experts and feed-forward networks with common activation functions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Sigmoid gating avoids the representation collapse caused by softmax competition.
- **Mechanism**: Softmax gating forces weights to sum to one, creating zero-sum competition where increasing one expert's weight decreases others'. Sigmoid gating allows independent activation weights, preventing this competition.
- **Core assumption**: Independence of sigmoid gating weights leads to more stable expert representation.
- **Evidence anchors**:
  - [abstract] "the softmax gating may lead to unnecessary competition among experts, potentially causing the undesirable phenomenon of representation collapse due to its inherent structure"
  - [section] "the use of softmax function might introduce an unexpected competition among experts, which leads to the representation collapse issue"
  - [corpus] Weak evidence: Only mentions representation collapse as known issue without quantitative analysis
- **Break condition**: If experts are perfectly correlated or gating becomes saturated (weights near 0 or 1), benefits may diminish.

### Mechanism 2
- **Claim**: Sigmoid gating provides faster expert estimation convergence rates.
- **Mechanism**: Under regime 2 (at least one over-specified parameter non-zero), sigmoid gating achieves parametric rates of O(n^(-1/2)) for expert estimation versus O(n^(-1/4)) for softmax gating with ReLU/GELU experts.
- **Core assumption**: Gating function choice directly impacts statistical efficiency of parameter estimation.
- **Evidence anchors**:
  - [abstract] "sigmoid gating achieves faster convergence rates for expert estimation than softmax gating, particularly when experts are formulated as feed-forward networks with ReLU or GELU activations"
  - [section] "sigmoid gating delivers superior sample efficiency for estimating the model parameters"
  - [corpus] Moderate evidence: Related papers confirm sample efficiency benefits but with different experimental setups
- **Break condition**: If true regression function doesn't follow mixture of experts structure, or number of experts is severely mis-specified.

### Mechanism 3
- **Claim**: Sigmoid gating is compatible with a broader class of expert functions.
- **Mechanism**: Sigmoid gating allows faster convergence rates with polynomial experts (O(n^(-1/2))) compared to softmax gating (potentially O(1/log(n))).
- **Core assumption**: Gating function's mathematical properties interact favorably with different expert function structures.
- **Evidence anchors**:
  - [abstract] "experts formulated as feed-forward networks with commonly used activation such as ReLU and GELU enjoy faster convergence rates under the sigmoid gating than those under softmax gating"
  - [section] "the sigmoid gating is compatible with a broader class of experts than the softmax gating"
  - [corpus] Weak evidence: Limited direct comparison of polynomial experts between gating types
- **Break condition**: If expert functions have very specific structural properties that don't interact well with sigmoid gating's independence assumptions.

## Foundational Learning

- **Concept**: Identifiability conditions for expert functions
  - **Why needed here**: To determine which expert functions will have fast convergence rates under different gating regimes
  - **Quick check question**: Can you explain why input-independent experts (a=0d) fail the strong identifiability condition?

- **Concept**: Two gating regimes (over-specified parameters zero vs non-zero)
  - **Why needed here**: The convergence behavior differs dramatically between regimes, affecting which gating function is more sample efficient
  - **Quick check question**: What happens to the convergence rate when all over-specified parameters are zero versus when at least one is non-zero?

- **Concept**: Voronoi loss function construction
  - **Why needed here**: Used to establish the relationship between regression estimation error and parameter estimation error
  - **Quick check question**: How does the Voronoi loss help in decomposing the difference between estimated and true regression functions?

## Architecture Onboarding

- **Component map**: Input features → Gating network (sigmoid vs softmax) → Expert networks → Weighted combination
- **Critical path**: 1) Data generation with true MoE parameters, 2) Parameter initialization near true values, 3) Least squares optimization via SGD, 4) Convergence rate analysis using Voronoi loss bounds
- **Design tradeoffs**: Sigmoid: No competition, broader expert compatibility, faster convergence in regime 2; Softmax: Normalized weights, simpler interpretation, faster in regime 1 when over-specified parameters are zero
- **Failure signatures**: Slow convergence (may indicate regime 1 with all over-specified parameters zero), parameter explosion (poor initialization or learning rate issues), representation collapse (softmax gating with highly correlated experts)
- **First 3 experiments**: 1) Compare sigmoid vs softmax with ReLU experts under regime 2 (expect sigmoid to be faster), 2) Test polynomial experts with both gating functions (expect sigmoid to be significantly faster), 3) Vary number of over-specified parameters to see regime transitions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the sample efficiency of sigmoid gating compare to other gating functions beyond softmax, such as tanh or piecewise linear gates?
- Basis in paper: [explicit] The paper focuses on comparing sigmoid gating to softmax gating, noting that sigmoid gating is more sample efficient for expert estimation in mixture of experts models.
- Why unresolved: The analysis only covers sigmoid and softmax gating functions. Other gating functions are not explored or compared.
- What evidence would resolve it: Empirical and theoretical comparisons of sigmoid gating to other gating functions (e.g., tanh, piecewise linear) in terms of sample efficiency for expert estimation in MoE models.

### Open Question 2
- Question: What are the implications of the input-dependent gating parameters under Regime 2 for the overall model interpretability and explainability?
- Basis in paper: [explicit] The paper discusses two gating regimes based on whether over-specified parameters are zero or non-zero, with Regime 2 being more practical and input-dependent.
- Why unresolved: While the paper identifies the regimes and their effects on convergence rates, it does not delve into the broader implications for model interpretability and explainability.
- What evidence would resolve it: Studies analyzing the interpretability and explainability of MoE models under different gating regimes, particularly focusing on the practical Regime 2.

### Open Question 3
- Question: How do the findings on sample efficiency translate to real-world applications beyond synthetic data, particularly in domains with high-dimensional data?
- Basis in paper: [explicit] The paper provides theoretical and empirical evidence on the sample efficiency of sigmoid gating using synthetic data.
- Why unresolved: The experiments are conducted on synthetic data, and the paper does not address how these findings apply to real-world, high-dimensional data scenarios.
- What evidence would resolve it: Case studies and experiments applying sigmoid gating to real-world datasets, especially those with high-dimensional features, to validate the sample efficiency claims.

## Limitations
- Theoretical analysis assumes least squares estimation, which may not reflect practical training with cross-entropy loss common in deep learning
- Study focuses on regression problems, limiting direct applicability to classification tasks where softmax gating is traditionally preferred
- Identifiability conditions are quite restrictive, requiring specific relationships between input features and expert parameters that may not hold in real-world data

## Confidence
- **High confidence**: The fundamental mathematical comparison between sigmoid and softmax gating structures is sound, and the representation collapse issue with softmax is well-documented
- **Medium confidence**: The convergence rate comparisons are theoretically rigorous but may not fully capture practical performance due to simplified assumptions about expert functions and optimization procedures
- **Low confidence**: The broader applicability to non-regression problems and the practical significance of the sample efficiency gains in real-world scenarios

## Next Checks
1. **Empirical validation across architectures**: Test the theoretical findings on modern deep learning architectures with both regression and classification objectives to verify if sigmoid gating consistently outperforms softmax in sample efficiency
2. **Robustness to initialization and hyperparameters**: Systematically study how sensitive the convergence rates are to initialization strategies, learning rates, and the number of experts to understand practical limitations
3. **Comparison with alternative gating mechanisms**: Evaluate sigmoid gating against other gating functions like sparsemax or entmax that also aim to reduce competition among experts