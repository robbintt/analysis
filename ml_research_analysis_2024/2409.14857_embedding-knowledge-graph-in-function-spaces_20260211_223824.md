---
ver: rpa2
title: Embedding Knowledge Graph in Function Spaces
arxiv_id: '2409.14857'
source_url: https://arxiv.org/abs/2409.14857
tags:
- fmult
- knowledge
- functions
- function
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes embedding knowledge graphs in function spaces
  instead of traditional finite vector spaces. The core idea is to represent entities
  and relations as polynomial, trigonometric, or neural network functions, enabling
  richer and more expressive representations.
---

# Embedding Knowledge Graph in Function Spaces

## Quick Facts
- arXiv ID: 2409.14857
- Source URL: https://arxiv.org/abs/2409.14857
- Authors: Louis Mozart Kamdem Teyou; Caglar Demir; Axel-Cyrille Ngonga Ngomo
- Reference count: 40
- Primary result: Functional representations (FMult, FMultn) outperform state-of-the-art models on several benchmarks

## Executive Summary
This paper introduces a novel approach to knowledge graph embedding by representing entities and relations as functions in function spaces rather than traditional finite-dimensional vectors. The authors propose three variants: FMultn (polynomial functions), FMultin (trigonometric functions), and FMult (neural networks). By leveraging the expressive power of function spaces, these models can capture complex, non-linear relationships in knowledge graphs through operations like composition, integration, and differentiation. The experimental results demonstrate that FMult and FMultn outperform existing models on benchmark datasets, particularly in scenarios with hierarchical or symmetric relationships.

## Method Summary
The core methodology involves embedding knowledge graph entities and relations as functions in function spaces, specifically L² spaces. Three approaches are proposed: FMultn uses polynomial functions of varying degrees, FMultin employs trigonometric functions, and FMult leverages neural networks as function representations. The scoring function is computed by combining entity and relation functions through multiplication and integration over a bounded domain. Training uses binary cross-entropy loss with negative sampling, optimized via Adam with a learning rate of 0.02 for 500 epochs. The approach enables more expressive representations by utilizing operations like function composition and integration that are unavailable in traditional vector space embeddings.

## Key Results
- FMult achieves state-of-the-art performance on NELL-995-h100 dataset
- FMultn with degree-3 polynomials excels at capturing symmetric relationships in KINSHIP dataset
- FMultn with degree-15 polynomials shows superior performance on COUNTRIES dataset
- The functional representation framework demonstrates consistent improvements over traditional embedding methods across multiple benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Polynomial functions can approximate complex knowledge graph relationships through non-linear interactions
- Core assumption: Weierstrass approximation theorem enables polynomials to approximate any continuous function
- Evidence anchors: Abstract mentions polynomial functions; section 4.1 discusses functional representations; no direct corpus support
- Break condition: Discontinuous or non-analytic relationships cannot be accurately approximated by polynomials

### Mechanism 2
- Claim: Neural networks as embeddings enable adaptive learning of complex knowledge graph relationships
- Core assumption: Universal approximation theorem justifies neural network's ability to model any continuous function
- Evidence anchors: Abstract highlights neural networks' dynamic adaptation; section 4.3.2 shows function composition; no direct corpus support
- Break condition: Insufficient network depth or capacity fails to capture relationship complexity

### Mechanism 3
- Claim: Function space operations (composition, derivatives, integration) provide richer mathematical expressiveness
- Core assumption: Additional mathematical operations meaningfully represent knowledge graph dynamics
- Evidence anchors: Abstract emphasizes enhanced expressiveness; section 1 discusses compositionality; no direct corpus support
- Break condition: Operations don't provide meaningful information about KG structure

## Foundational Learning

- Concept: Function spaces and L^p spaces
  - Why needed here: Essential for understanding how functions serve as vector space elements with norms
  - Quick check question: Explain the difference between L^2 and L^p spaces and why L^2 is useful here

- Concept: Polynomial approximation and Weierstrass theorem
  - Why needed here: Theoretical justification for using polynomials to approximate KG relationships
  - Quick check question: What does Weierstrass approximation theorem state and how does it relate to KGE?

- Concept: Neural network universal approximation
  - Why needed here: Justifies neural network approach for function space embeddings
  - Quick check question: What is the universal approximation theorem for neural networks and its limitations?

## Architecture Onboarding

- Component map: Data preprocessing -> Embedding layer (polynomial/neural network) -> Scoring function (composition/integration) -> Loss function -> Optimizer (Adam) -> Regularization (L2)

- Critical path: Initialize function embeddings → Compute scoring function for triples → Generate negative samples → Calculate loss → Backpropagate gradients → Update parameters via Adam → Repeat for 500 epochs

- Design tradeoffs:
  - Polynomial degree vs. complexity: Higher degrees capture more complexity but increase computational cost and overfitting risk
  - Neural network depth vs. stability: Deeper networks model complexity better but may suffer from vanishing gradients
  - Function space choice: Polynomial, trigonometric, and neural network approaches each have strengths for different KG structures

- Failure signatures:
  - Poor performance on symmetric relationships: Polynomial embeddings struggle with symmetry per Theorem 4.2
  - Training instability: Neural networks sensitive to initialization requiring careful hyperparameter tuning
  - Computational inefficiency: Function operations (integration, composition) expensive for large KGs

- First 3 experiments:
  1. Train FMultn on UMLS with degree 0, compare to DistMult
  2. Train FMult on KINSHIP with single layer, compare to ComplEx
  3. Vary polynomial degrees (0, 1, 3, 7) on KINSHIP to study symmetric relationship impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does activation function choice in FMult affect performance across datasets?
- Basis: Paper only tested tanh activation without exploring alternatives
- Why unresolved: Limited activation function experimentation
- What evidence would resolve it: Experiments comparing multiple activations (ReLU, sigmoid) across datasets

### Open Question 2
- Question: Can FMultin be optimized to outperform other approaches on specific datasets?
- Basis: Paper states FMultin potential remains unexplored for future work
- Why unresolved: FMultin only briefly mentioned without validation
- What evidence would resolve it: Comprehensive FMultin optimization and benchmarking

### Open Question 3
- Question: How does polynomial degree impact hierarchical relationship modeling?
- Basis: Paper shows degree effects but lacks systematic analysis
- Why unresolved: Limited degree range tested without systematic study
- What evidence would resolve it: Systematic degree variation across hierarchical datasets

## Limitations
- Theoretical analysis incomplete with only one proven theorem about symmetric relationships
- Computational complexity characterization missing for function space operations
- Limited empirical evaluation lacking ablation studies to isolate function space contribution

## Confidence
- High confidence: Mathematical framework and basic implementation approach
- Medium confidence: Empirical performance claims due to limited dataset coverage and no ablation studies
- Low confidence: Theoretical guarantees beyond single theorem on symmetric relationships

## Next Checks
1. Conduct ablation studies comparing function space embeddings against equivalent static embedding models with identical neural network architectures
2. Perform scaling analysis on progressively larger knowledge graphs to characterize computational complexity and identify bottlenecks
3. Extend theoretical analysis to prove approximation bounds for knowledge graph relationships under different function space representations