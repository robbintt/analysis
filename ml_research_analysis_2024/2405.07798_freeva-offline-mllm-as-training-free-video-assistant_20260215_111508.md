---
ver: rpa2
title: 'FreeVA: Offline MLLM as Training-Free Video Assistant'
arxiv_id: '2405.07798'
source_url: https://arxiv.org/abs/2405.07798
tags:
- video
- mllms
- image
- performance
- tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FreeVA explores the potential of using existing image-based MLLMs
  directly for video understanding without additional training. It introduces a simple
  temporal aggregation mechanism that concatenates visual tokens from sampled video
  frames and feeds them into the language model.
---

# FreeVA: Offline MLLM as Training-Free Video Assistant

## Quick Facts
- **arXiv ID**: 2405.07798
- **Source URL**: https://arxiv.org/abs/2405.07798
- **Reference count**: 40
- **Primary result**: Simple temporal aggregation of video frames with existing image-based MLLMs outperforms video instruction-tuned models on zero-shot video QA tasks

## Executive Summary
FreeVA demonstrates that existing image-based multimodal large language models (MLLMs) can perform video understanding tasks without any additional training by using a simple temporal aggregation mechanism. The approach samples frames from videos, processes them through a frozen image encoder, aggregates the resulting visual tokens, and feeds them to a language model. Surprisingly, this training-free method achieves state-of-the-art performance on zero-shot video question-answering benchmarks like MSVD-QA, ActivityNet-QA, and MSRVTT-QA, even outperforming models that undergo video instruction tuning.

The study also reveals critical insights about current video MLLM practices: fine-tuning with the widely-used VideoInstruct-100K dataset actually degrades performance compared to zero-shot approaches, suggesting limitations in current video instruction datasets. Additionally, the research highlights that evaluation metrics are significantly influenced by GPT API version changes over time, raising important concerns about the reproducibility and comparability of video MLLM results across different research efforts.

## Method Summary
FreeVA extends image-based MLLMs to video understanding without any training by implementing a simple temporal aggregation mechanism. The method samples T frames uniformly from input videos (default T=4), processes each frame through a frozen image encoder to extract visual tokens, then aggregates these tokens across frames using concatenation or pooling. The aggregated tokens are fed directly to the language model, which generates answers for video question-answering tasks. The approach requires only minor adjustments to existing image MLLM inference pipelines and maintains the original model parameters.

## Key Results
- Zero-shot FreeVA outperforms state-of-the-art video MLLMs that use video instruction tuning on MSVD-QA, ActivityNet-QA, and MSRVTT-QA benchmarks
- Fine-tuning LLaVA with VideoInstruct-100K degrades performance compared to using the original image MLLM without training
- GPT API version changes significantly impact evaluation metrics, with different versions producing substantially different scores
- Increasing frame count from 1 to 4 improves performance substantially, but further increases yield diminishing returns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Image-based MLLMs already contain sufficient visual knowledge to understand video frames directly without video-specific training
- Mechanism: Visual tokens extracted from sampled video frames retain the spatial and semantic information captured during original image training, which the language model can interpret when provided with temporal aggregation
- Core assumption: The visual encoder's learned representations are sufficiently general to encode video frames without temporal modeling in the encoder itself
- Evidence anchors:
  - [abstract] "FreeVA, leveraging only offline image-based MLLM without additional training, excels in zero-shot video question-answering (e.g., MSVD-QA, ActivityNet-QA, and MSRVTT-QA), even surpassing state-of-the-art methods that involve video instruction tuning."
  - [section 3.2] "Compared to the inference process of image MLLM, our FreeVA requires only minor adjustments... design a parameter-free temporal aggregation mechanism to aggregate visual tokens across frames."
- Break condition: If visual encoder representations are too domain-specific to static images and cannot generalize to video motion dynamics or temporal context

### Mechanism 2
- Claim: Simple temporal aggregation methods can effectively combine information across video frames without complex video-specific architectures
- Mechanism: Concatenating or pooling visual tokens from multiple frames provides the language model with sufficient temporal context for video understanding tasks
- Core assumption: The language model can integrate temporal information when visual tokens from multiple frames are presented together, even without explicit temporal modeling in the vision encoder
- Evidence anchors:
  - [section 3.2] "Specifically, video sampler is used to capture multiple frames of the video as inputs and design a parameter-free temporal aggregation mechanism to aggregate visual tokens across frames."
  - [section 4.2] "using 4 frames yielded the best results on both datasets" and "increasing the number of video frames significantly improves performance, surpassing the performance achieved with sparse temporal aggregation by a large margin."
- Break condition: If temporal dependencies in videos are too complex for simple aggregation to capture, requiring specialized temporal modeling architectures

### Mechanism 3
- Claim: The widely used VideoInstruct-100K dataset may not provide meaningful improvements over zero-shot approaches for video MLLM fine-tuning
- Mechanism: Fine-tuning with VideoInstruct-100K can actually degrade performance compared to using the original image MLLM directly, suggesting the dataset may not contain valuable video-specific knowledge
- Core assumption: The quality or relevance of video instruction data in VideoInstruct-100K is insufficient to improve upon the knowledge already present in image-trained models
- Evidence anchors:
  - [abstract] "2) While mainstream video-based MLLMs typically initialize with an image-based MLLM (e.g., LLaVA) and then fine-tune using video instruction tuning, the study indicates that utilizing the widely adopted VideoInstruct-100K for video instruction tuning doesn't actually lead to better performance compared to not training at all."
  - [section 4.2] "the performance after training the projection is subpar compared to directly using the original LLaVA-1.5" and "using the trained parameters provided by VideoChatGPT actually results in inferior performance compared to using its initialized parameters directly."
- Break condition: If future video instruction datasets contain more relevant or higher-quality video-specific knowledge that can genuinely improve model performance

## Foundational Learning

- Concept: Temporal aggregation in multimodal models
  - Why needed here: Understanding how to combine information across video frames without complex temporal architectures
  - Quick check question: What are the differences between sparse (mean pooling) and dense (concatenation) temporal aggregation, and when would each be appropriate?

- Concept: Zero-shot learning in multimodal models
  - Why needed here: The paper demonstrates that existing image MLLMs can perform video tasks without any video-specific training
  - Quick check question: What assumptions about model generalization are necessary for zero-shot video understanding to work?

- Concept: Impact of evaluation metrics and API versions
  - Why needed here: The study shows that GPT API version changes significantly affect evaluation results, which is critical for fair comparison
  - Quick check question: How might changes in the default GPT-3.5 Turbo version affect the apparent performance of different models over time?

## Architecture Onboarding

- Component map:
  Video → Frame sampling → Visual encoding → Temporal aggregation → LLM inference → Answer generation

- Critical path: Video → Frame sampling → Visual encoding → Temporal aggregation → LLM inference → Answer generation

- Design tradeoffs:
  - Frame sampling rate vs. computational cost vs. temporal coverage
  - Number of aggregated tokens vs. LLM context window limits
  - Sparse vs. dense aggregation: simplicity vs. information preservation
  - Using pre-trained vs. fine-tuned image MLLMs

- Failure signatures:
  - Performance collapse when too many frames are used with dense aggregation (exceeds token limits)
  - Degraded performance with sparse aggregation as frame count increases (feature averaging destroys information)
  - Inconsistent results across different GPT API versions

- First 3 experiments:
  1. Test different temporal aggregation methods (S1, S2, S3, D1, D2) on a small video QA dataset
  2. Compare zero-shot performance vs. fine-tuned model using VideoInstruct-100K on the same dataset
  3. Evaluate performance consistency across different GPT API versions using the same model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does video instruction tuning with VideoInstruct-100K actually improve performance over using the original image MLLM without training?
- Basis in paper: [explicit] The paper explicitly states that fine-tuning LLaVA with VideoInstruct-100K resulted in worse performance compared to the original LLaVA, suggesting the dataset may not be effective for improving video understanding.
- Why unresolved: While the paper shows this counterintuitive finding, it only tested with LLaVA and a specific dataset. It's unclear if this holds true for other image MLLMs or if the dataset itself is the limiting factor.
- What evidence would resolve it: Testing a range of image MLLMs (e.g., BLIP-2, MiniGPT-4) with VideoInstruct-100K and comparing results to their zero-shot performance would clarify if the issue is dataset-specific or a broader problem with video instruction tuning.

### Open Question 2
- Question: What is the true impact of GPT-3.5 API version changes on video MLLM evaluation metrics, and how can we ensure fair comparisons across time?
- Basis in paper: [explicit] The paper highlights that different versions of GPT-3.5-Turbo (e.g., 0301, 0613, 1106, 0125) yield significantly different evaluation scores on video QA tasks, raising concerns about the fairness of comparisons over time.
- Why unresolved: While the paper identifies the problem, it doesn't provide a solution for standardizing evaluations. It's unclear how future researchers can ensure their results are comparable to past work.
- What evidence would resolve it: Establishing a fixed version of GPT-3.5 for all video MLLM evaluations or developing alternative evaluation metrics less sensitive to API changes would ensure consistent and fair comparisons.

### Open Question 3
- Question: Can training-free video assistants like FreeVA be extended to handle more complex video understanding tasks that require temporal reasoning, such as action recognition or event localization?
- Basis in paper: [inferred] The paper demonstrates FreeVA's effectiveness on video QA tasks but doesn't explore its performance on tasks requiring deeper temporal understanding. The authors suggest that current benchmarks may have limitations in this area.
- Why unresolved: The paper focuses on zero-shot video QA, which may not fully capture the capabilities needed for tasks involving complex temporal reasoning. It's unclear if FreeVA's simple temporal aggregation is sufficient for these tasks.
- What evidence would resolve it: Evaluating FreeVA on benchmarks specifically designed for temporal reasoning (e.g., action recognition, event localization) and comparing its performance to video MLLMs trained on these tasks would determine its effectiveness for complex video understanding.

## Limitations
- GPT API version changes significantly impact evaluation metrics, creating uncertainty about result comparability across time
- VideoInstruct-100K dataset may not contain meaningful video-specific knowledge, as fine-tuning with it degrades performance
- Simple temporal aggregation may be insufficient for complex video understanding tasks requiring deep temporal reasoning

## Confidence
- **High confidence**: The observation that GPT API version changes significantly affect evaluation metrics is well-supported by empirical evidence in the paper and represents a reproducible finding.
- **Medium confidence**: The claim that simple temporal aggregation of visual tokens from sampled frames can achieve competitive video understanding performance is supported by experimental results, though the exact mechanisms and limitations require further investigation.
- **Medium confidence**: The finding that VideoInstruct-100K fine-tuning degrades performance compared to zero-shot approaches is well-documented in the paper's experiments, but the broader implications for video instruction dataset design remain uncertain.

## Next Checks
1. **API version control experiment**: Re-run FreeVA evaluation using multiple specific GPT-3.5 Turbo versions (e.g., May 24, June 24, latest) to quantify the variance in results and establish proper baseline comparisons for future work.

2. **Dataset quality analysis**: Conduct ablation studies on VideoInstruct-100K to identify which types of video instruction examples improve versus degrade performance, and characterize the properties of high-quality video instruction data.

3. **Temporal modeling comparison**: Implement and compare alternative temporal aggregation methods (such as attention-based temporal fusion or transformer-based video encoders) against the simple concatenation approach to determine if more sophisticated temporal modeling provides measurable benefits for video understanding tasks.