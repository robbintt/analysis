---
ver: rpa2
title: Translation Errors Significantly Impact Low-Resource Languages in Cross-Lingual
  Learning
arxiv_id: '2402.02080'
source_url: https://arxiv.org/abs/2402.02080
tags:
- languages
- english
- test
- translations
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates translation inconsistencies in cross-lingual
  benchmarks, specifically XNLI, and their disproportionate impact on low-resource
  languages. The authors propose measuring the performance gap between zero-shot evaluations
  on human-translated and machine-translated target text to identify low-quality translations.
---

# Translation Errors Significantly Impact Low-Resource Languages in Cross-Lingual Learning

## Quick Facts
- arXiv ID: 2402.02080
- Source URL: https://arxiv.org/abs/2402.02080
- Authors: Ashish Sunil Agrawal; Barah Fazili; Preethi Jyothi
- Reference count: 24
- Translation errors disproportionately affect low-resource languages in cross-lingual benchmarks

## Executive Summary
This work investigates translation inconsistencies in cross-lingual benchmarks, specifically XNLI, and their disproportionate impact on low-resource languages. The authors propose measuring the performance gap between zero-shot evaluations on human-translated and machine-translated target text to identify low-quality translations. Manual reannotation of Hindi and Urdu test instances confirms poor agreement with original English labels, indicating translation errors. The study finds that translation errors persist across various train/test settings and that machine-translated text aligns better with original English text compared to human translations, particularly for low-resource languages.

## Method Summary
The study analyzes translation quality in XNLI by comparing XLMR model performance on human-translated versus machine-translated test sets across 14 languages. The authors measure performance gaps (∆-g) between zero-shot and translate-test evaluations, where large gaps indicate potential translation errors. They conduct manual reannotation of Hindi and Urdu instances to verify label consistency, finding significantly lower agreement with original English labels for human translations compared to machine translations. The research also examines attention alignment between English and translated text to quantify semantic preservation.

## Key Results
- Low-resource languages (Urdu, Swahili) show performance gaps as high as 10.8-10.9, while high-resource languages (French, Spanish) show gaps as low as 2.9-2
- Manual reannotation reveals 80% agreement for Hindi and 71% for Urdu between original English labels and human translations, but significantly better alignment with machine translations
- Machine translations demonstrate higher overlap with original English text compared to human translations across all tested languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Translation inconsistencies disproportionately impact low-resource languages in cross-lingual learning benchmarks
- Mechanism: Quality of human translations degrades more significantly for low-resource languages due to less familiarity with nuances, idioms, and linguistic structures unique to these languages, leading to semantic drift from original English labels
- Core assumption: Degree of semantic drift correlates with resource availability in target language
- Evidence anchors:
  - [abstract] "translation inconsistencies do exist and interestingly they disproportionally impact low-resource languages in XNLI"
  - [section 3.1] "It is striking that ∆-g values for low-resource languages like Urdu and Swahili are as high as10.8 and 10.9, respectively, and as low as2.9 and 2 for high-resource languages like French and Spanish"
  - [corpus] Weak correlation - related work discusses translation artifacts but not specifically the resource-dependent degradation pattern

### Mechanism 2
- Claim: Machine translations align better with original English text than human translations for low-resource languages
- Mechanism: Machine translation systems trained on large parallel corpora capture more literal and direct mappings between source and target, preserving semantic relationships better than human translators who may introduce contextual reinterpretations
- Core assumption: Literal translations preserve more semantic consistency than context-adaptive human translations for NLI tasks
- Evidence anchors:
  - [section 5] "For all three languages, we find the overlap fraction to be higher for the Google-translated sentences compared to the human-translated sentences"
  - [section 4] "The annotators recovered the ground-truth labels 80% and 71% of the time for Hindi and Urdu, respectively, highlighting that label inconsistencies in Hindi/Urdu human translations (ORIG) are significantly worse than with machine translations (TE)"
  - [corpus] Moderate correlation - related work on cross-lingual transfer acknowledges translation artifacts but doesn't specifically compare machine vs human alignment quality

### Mechanism 3
- Claim: Performance gaps between human and machine translations can identify low-quality translations
- Mechanism: When human-translated test sets show significantly lower performance than machine-translated versions, it indicates semantic inconsistencies introduced during human translation that don't exist in original English or machine-translated versions
- Core assumption: Machine translations serve as reliable baseline for semantic preservation when original English is gold standard
- Evidence anchors:
  - [abstract] "To identify such inconsistencies, we propose measuring the gap in performance between zero-shot evaluations on the human-translated and machine-translated target text across multiple target languages; relatively large gaps are indicative of translation errors"
  - [section 3.1] "∆-g in Table 1 refers to the performance gap when using human vs. machine translations. It is the difference between the accuracy for BT-g (machine-translated target language text) and the best accuracy among ZS and TT-g (human-translated target language text)"
  - [corpus] Weak correlation - related work discusses translation artifacts but not specifically using performance gaps as diagnostic tools

## Foundational Learning

- Concept: Cross-lingual transfer learning
  - Why needed here: Understanding how models trained on English data generalize to other languages is central to evaluating translation quality impacts
  - Quick check question: What is the difference between zero-shot and translate-test evaluation in cross-lingual learning?

- Concept: Semantic consistency in translation
  - Why needed here: The core problem is whether translated text maintains same semantic relationships as original
  - Quick check question: How would you measure whether a translated premise-hypothesis pair preserves original entailment/contradiction/neutral relationship?

- Concept: Translation artifacts
  - Why needed here: Understanding how translations can introduce errors or inconsistencies is crucial for diagnosing performance gaps observed
  - Quick check question: What are common types of translation errors that could affect NLI task performance?

## Architecture Onboarding

- Component map:
  - XNLI dataset (English source + human translations in 14 languages)
  - XLMR model (multilingual pretrained model)
  - NLLB and Google Translate APIs (machine translation systems)
  - Human annotation pipeline (native speaker label verification)
  - Evaluation metrics (accuracy, performance gap calculation)

- Critical path:
  1. Load English NLI data and human-translated versions
  2. Generate machine translations of original English to target languages
  3. Train XLMR on English data with different variants (original, backtranslated)
  4. Evaluate on human-translated and machine-translated test sets
  5. Calculate performance gaps to identify translation issues
  6. Validate findings with human reannotation of selected samples

- Design tradeoffs:
  - Using machine translation as baseline vs. human translation quality: Machine translation provides consistency but may lack fluency; human translation provides naturalness but introduces variability
  - Training on backtranslated data vs. original: Backtranslation improves generalization but may amplify existing translation artifacts
  - Sampling strategy for human reannotation: Random sampling ensures representativeness but may miss systematic error patterns

- Failure signatures:
  - Large performance gaps between human and machine translations for specific languages
  - Poor agreement between original English labels and human-translated text labels
  - Attention alignment scores significantly lower for human translations compared to machine translations
  - Inconsistent performance improvements when using backtranslated training data

- First 3 experiments:
  1. Reproduce the performance gap calculation between human and machine translations for all 14 XNLI languages
  2. Implement human reannotation pipeline for Hindi and Urdu to verify label consistency
  3. Test attention-based alignment analysis between English and translated text to quantify semantic preservation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific linguistic features in low-resource languages (e.g., Hindi, Urdu) make them more susceptible to translation errors in cross-lingual benchmarks?
- Basis in paper: [explicit] The paper identifies that low-resource languages are disproportionately affected by translation errors in XNLI, but does not specify the linguistic features contributing to this issue
- Why unresolved: The paper does not provide a detailed analysis of the linguistic characteristics that might make low-resource languages more vulnerable to translation errors
- What evidence would resolve it: A comparative linguistic analysis of high-resource vs. low-resource languages, focusing on syntactic, semantic, and pragmatic differences that could impact translation quality

### Open Question 2
- Question: How do translation errors in low-resource languages affect downstream tasks beyond natural language inference, such as sentiment analysis or named entity recognition?
- Basis in paper: [inferred] The paper focuses on NLI tasks and highlights the impact of translation errors on cross-lingual transfer, suggesting potential broader implications for other tasks
- Why unresolved: The study does not explore the impact of translation errors on other NLP tasks, leaving a gap in understanding the full extent of the issue
- What evidence would resolve it: Experimental results showing the impact of translation errors on a variety of NLP tasks across different languages

### Open Question 3
- Question: What are the most effective strategies for improving translation quality in low-resource languages to enhance cross-lingual transfer in multilingual benchmarks?
- Basis in paper: [explicit] The paper suggests using machine translations as a quality check but does not provide comprehensive strategies for improving translation quality
- Why unresolved: The paper identifies the problem but does not offer detailed solutions for enhancing translation quality in low-resource languages
- What evidence would resolve it: A study comparing different translation methodologies, such as human-in-the-loop systems, advanced machine translation models, and hybrid approaches, to determine the most effective strategies for improving translation quality

## Limitations

- The performance gap methodology assumes machine translations serve as reliable semantic baselines, which may not hold if machine translation systems introduce systematic biases
- Manual reannotation was limited to only 500 instances across Hindi and Urdu, potentially missing broader translation quality issues
- The study doesn't account for potential confounding factors such as cultural context differences or domain-specific terminology that could affect translation quality independently of resource levels

## Confidence

**High Confidence**: The observation that performance gaps exist between human and machine translations for low-resource languages (Hindi, Urdu, Swahili) is well-supported by quantitative evidence and manual verification. The correlation between resource availability and translation quality degradation is consistently observed across multiple evaluation metrics.

**Medium Confidence**: The claim that machine translations align better with original English text than human translations requires more careful interpretation. While attention alignment scores support this finding, the mechanism may be more complex than simple literal preservation, potentially involving differences in translation style rather than quality.

**Low Confidence**: The generalizability of the performance gap metric as a diagnostic tool for translation quality across all language pairs and NLI datasets needs further validation. The method may not capture all types of translation errors, particularly those that preserve semantic relationships but alter surface form in ways that affect model performance.

## Next Checks

1. **Cross-Validation with Alternative Translation Systems**: Test the performance gap methodology using multiple machine translation systems (including emerging neural models) to determine if the observed patterns are consistent across different translation technologies and not specific to Google Translate or NLLB.

2. **Expanded Manual Annotation**: Conduct comprehensive manual annotation across all 14 XNLI languages, focusing on a stratified sample that captures different types of translation errors (semantic drift, syntactic restructuring, cultural adaptation) to better understand the error distribution and validate the performance gap metric's effectiveness.

3. **Controlled Translation Studies**: Design experiments where human translators are explicitly instructed to prioritize semantic preservation over fluency, and compare these results with standard human translations and machine translations to isolate the specific aspects of human translation that introduce inconsistencies.