---
ver: rpa2
title: 'SOLAMI: Social Vision-Language-Action Modeling for Immersive Interaction with
  3D Autonomous Characters'
arxiv_id: '2412.00174'
source_url: https://arxiv.org/abs/2412.00174
tags:
- motion
- speech
- character
- data
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SOLAMI, an end-to-end social vision-language-action
  modeling framework for immersive interaction with 3D autonomous characters. The
  method integrates multimodal inputs (user speech and motion) and generates multimodal
  responses (character speech and motion) through a unified VLA architecture, enabling
  real-time social interaction.
---

# SOLAMI: Social Vision-Language-Action Modeling for Immersive Interaction with 3D Autonomous Characters

## Quick Facts
- **arXiv ID**: 2412.00174
- **Source URL**: https://arxiv.org/abs/2412.00174
- **Reference count**: 40
- **Key outcome**: SOLAMI achieves superior multimodal interaction performance with real-time processing, outperforming modular baselines in motion quality, speech consistency, and user satisfaction.

## Executive Summary
This paper presents SOLAMI, an end-to-end social vision-language-action (VLA) framework for immersive interaction with 3D autonomous characters. The method integrates multimodal inputs (user speech and motion) and generates multimodal responses (character speech and motion) through a unified VLA architecture, enabling real-time social interaction. To address data scarcity, the authors introduce SynMSI, a synthetic dataset automatically generated from existing motion-text datasets. A VR interface allows users to interact immersively with characters driven by various architectures. Experimental results show SOLAMI achieves superior motion quality (FID: 3.443, PA-MPJPE: 151.5), better speech consistency (VC Similarity: 0.824, Context Relevance: 3.634), and lower inference latency (2.639s) compared to baseline methods like LLM+Speech and DLP. User studies confirm SOLAMI delivers more precise and natural interactions with higher overall satisfaction.

## Method Summary
SOLAMI is an end-to-end VLA model that unifies speech and motion tokens into an LLM's vocabulary for real-time multimodal social interaction. The method employs a three-stage training procedure: tokenizer training, multi-task pre-training for modality alignment, and instruction tuning for multi-turn conversation. To overcome data scarcity, the authors generate SynMSI, a synthetic multimodal social interaction dataset, using an automatic pipeline that combines existing motion-text datasets with character-specific topics and iterative script refinement. The model is evaluated on motion quality, speech consistency, inference latency, and user satisfaction, demonstrating superior performance compared to modular baselines.

## Key Results
- Superior motion quality: FID 3.443, PA-MPJPE 151.5 mm
- Better speech consistency: VC Similarity 0.824, Context Relevance 3.634
- Lower inference latency: 2.639s compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: End-to-end VLA modeling outperforms modular LLM-Agent pipelines in social interaction tasks.
- **Mechanism**: SOLAMI unifies speech and motion tokens into the LLM's vocabulary, enabling direct cross-modal alignment and low-latency response generation. Modular pipelines introduce information loss through intermediate text representations and incur higher computational latency.
- **Core assumption**: Text representations cannot fully capture the nuance of multimodal inputs (speech prosody, body language subtleties).
- **Evidence anchors**:
  - [abstract]: "Our end-to-end approach is significantly superior to the modular pipeline approaches (LLM+Speech or DLP). Because the end-to-end VLA model naturally aligns with the process of real-time human communication."
  - [section 2.1]: "LLM-Agent framework can handle planning tasks [35], but for low-level manipulation tasks, end-to-end VLA models built upon LLMs show superior performance [14, 33, 39, 90]."
  - [corpus]: No direct evidence found for cross-modal alignment superiority; assumed from robotics literature cited.
- **Break condition**: If the LLM backbone cannot effectively process multimodal tokens or if training data lacks sufficient cross-modal examples, performance may degrade.

### Mechanism 2
- **Claim**: Synthetic multimodal interaction data (SynMSI) enables effective training despite real data scarcity.
- **Mechanism**: Existing motion-text datasets are combined with character-specific topics and iterative script refinement to synthesize realistic dialogue and motion sequences. This approach addresses the prohibitive cost of collecting real multimodal interaction data.
- **Core assumption**: Large language models can generate contextually appropriate and diverse dialogue content when guided by curated topics and character settings.
- **Evidence anchors**:
  - [abstract]: "We present SynMSI, a synthetic multimodal social interaction dataset generated by an automatic pipeline using only existing motion datasets to address the issue of data scarcity."
  - [section 4.3]: "Our synthesizing pipeline includes 4 steps... Based on the topic, character setting, and previous round of scripts, we use GPT-4o [52] to generate textual descriptions... Then we utilize the text embedding [50] of the motion description to retrieve the most relevant motions..."
  - [corpus]: No direct evidence found for synthetic data quality; assumed from the paper's experimental results.
- **Break condition**: If the LLM fails to generate coherent or contextually relevant scripts, or if retrieved motions poorly match the generated descriptions, synthetic data quality will suffer.

### Mechanism 3
- **Claim**: Pre-training on aligned motion-text and speech-text pairs improves multimodal interaction performance.
- **Mechanism**: Stage 2 of SOLAMI's training aligns motion and speech modalities with language through multi-task pre-training (text-to-motion, motion captioning, text-to-speech, ASR). This alignment facilitates learning multimodal dialogue skills during instruction tuning.
- **Core assumption**: Aligning modalities at the pre-training stage reduces the complexity of learning multimodal interactions during instruction tuning.
- **Evidence anchors**:
  - [section 3.2]: "The second stage is multi-task pre-training, achieving modality alignment between motion and text, as well as between speech and text... we use 46 K motion-text pairs for text-to-motion generation and motion captioning tasks..."
  - [section 6.2]: "The pre-training stage of SOLAMI leads to better performance in both motion and speech quality... the pre-training stage, which aligns motion, speech, and language, facilitates the model's ability to learn the multimodal dialogue skill during the instruction tuning stage."
  - [corpus]: No direct evidence found for pre-training effectiveness; assumed from ablation study results.
- **Break condition**: If the pre-training tasks are not sufficiently diverse or if the alignment does not generalize to the instruction tuning task, performance gains may not materialize.

## Foundational Learning

- **Concept**: Discrete sequence modeling for multimodal data
  - Why needed here: To integrate speech and motion tokens into the LLM's vocabulary, enabling unified processing of multimodal inputs.
  - Quick check question: Can the LLM backbone effectively process discrete tokens representing both speech and motion without significant performance degradation?

- **Concept**: Motion representation and retargeting
  - Why needed here: To represent human poses as SMPL-X joint rotations for compatibility with industry animation workflows and to facilitate character animation.
  - Quick check question: Does the chosen motion representation (joint rotations vs. keypoints) provide sufficient precision for realistic character animation while avoiding fitting artifacts?

- **Concept**: Data synthesis and augmentation
  - Why needed here: To generate large-scale multimodal interaction data from existing motion-text datasets, addressing the scarcity of real interaction data.
  - Quick check question: Can the synthetic data pipeline generate diverse and contextually appropriate dialogue and motion sequences that generalize to real user interactions?

## Architecture Onboarding

- **Component map**: User input (speech and motion) → Tokenization → LLM processing → Token decoding → Character output (speech and motion)

- **Critical path**: User input (speech and motion) → Tokenization → LLM processing → Token decoding → Character output (speech and motion)

- **Design tradeoffs**:
  - End-to-end vs. modular: End-to-end VLA modeling offers lower latency and better cross-modal alignment but requires more complex training and data synthesis.
  - Separate vs. unified motion representation: Separate representation for body and hand motion provides better precision but increases model complexity.
  - Synthetic vs. real data: Synthetic data is more cost-effective but may lack the realism and diversity of real interaction data.

- **Failure signatures**:
  - High latency: Indicates bottlenecks in tokenization, LLM processing, or decoding.
  - Poor motion quality: Suggests issues with motion representation, tokenizer, or training data.
  - Inconsistent speech: Points to problems with speech tokenization, voice cloning, or character-specific voice training.
  - Lack of multimodal coherence: Implies insufficient cross-modal alignment or training data.

- **First 3 experiments**:
  1. Ablation study on pre-training: Train SOLAMI with and without pre-training on motion-text and speech-text pairs to assess its impact on multimodal interaction performance.
  2. Comparison of motion representation: Evaluate different motion representations (joint rotations vs. keypoints) on motion quality and visual fidelity.
  3. Synthetic data quality assessment: Compare SOLAMI trained on synthetic data (SynMSI) with a version trained on a smaller set of real interaction data to measure the effectiveness of the data synthesis pipeline.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does SOLAMI's performance change when using real-time dyadic interaction data instead of synthetic data?
- **Basis in paper**: [explicit] The authors acknowledge that collecting real-time interaction data between humans and characters could enable more precise and natural body language and speech generation with duplex streaming conversations.
- **Why unresolved**: The paper relies on synthetic data (SynMSI) due to the challenges of collecting real-time dyadic interaction data, which is currently unsustainable in the field of digital humans.
- **What evidence would resolve it**: Collecting and training SOLAMI on real-time dyadic interaction data would provide empirical evidence of performance improvements in terms of precision, naturalness, and latency compared to the current synthetic data approach.

### Open Question 2
- **Question**: How does SOLAMI handle cross-embodiment challenges when generating motions for different character types?
- **Basis in paper**: [explicit] The authors mention that using a unified SMPL-X model for motion representation introduces challenges in cross-embodiment for different characters, affecting fine-grained tasks like handshaking or object manipulation.
- **Why unresolved**: The paper does not explore solutions to address the cross-embodiment challenges, which are crucial for achieving realistic and diverse character interactions.
- **What evidence would resolve it**: Developing and evaluating methods to improve cross-embodiment capabilities, such as character-specific motion retargeting or learning character-specific motion priors, would provide insights into resolving this issue.

### Open Question 3
- **Question**: How does SOLAMI's performance scale with longer and more complex social interactions?
- **Basis in paper**: [explicit] The authors acknowledge that SOLAMI encounters challenges such as computational redundancy, forgetting, and training difficulties during extended social interactions.
- **Why unresolved**: The paper focuses on evaluating SOLAMI's performance on relatively short interactions and does not explore its behavior in longer, more complex scenarios.
- **What evidence would resolve it**: Conducting experiments with longer and more complex social interactions, and evaluating SOLAMI's performance in terms of consistency, coherence, and user satisfaction, would provide insights into its scalability and limitations.

## Limitations

- Synthetic Data Quality: The quality and diversity of the synthetic dataset (SynMSI) remain uncertain, potentially limiting the model's ability to generalize to real-world interactions.
- Generalizability: SOLAMI is evaluated primarily on the SynMSI dataset and a limited set of real user interactions, raising questions about its robustness and adaptability to varied social contexts and user behaviors.
- Real-time Performance: The computational requirements and scalability of the end-to-end VLA architecture are not thoroughly analyzed, potentially limiting its deployment on resource-constrained devices or in large-scale applications.

## Confidence

- **High Confidence**: The superiority of SOLAMI over modular LLM-Agent pipelines (LLM+Speech, DLP) in terms of motion quality (FID, PA-MPJPE) and speech consistency (VC Similarity, Context Relevance) is well-supported by quantitative metrics and user studies.
- **Medium Confidence**: The effectiveness of the synthetic data generation pipeline (SynMSI) is inferred from the model's performance on the SynMSI dataset and the reported improvements over baseline methods.
- **Low Confidence**: The long-term robustness and adaptability of SOLAMI to diverse social contexts, character types, and user behaviors are not thoroughly explored.

## Next Checks

1. **Real-World Interaction Testing**: Conduct extensive user studies with diverse participants in various social contexts and environments to assess SOLAMI's performance and robustness in real-world interactions.
2. **Synthetic Data Quality Analysis**: Evaluate the quality and diversity of the SynMSI dataset by comparing it with a smaller set of real interaction data.
3. **Computational Efficiency and Scalability**: Analyze the computational requirements and scalability of SOLAMI in terms of memory usage, inference time, and power consumption.