---
ver: rpa2
title: Evaluating GPT-4 with Vision on Detection of Radiological Findings on Chest
  Radiographs
arxiv_id: '2403.15528'
source_url: https://arxiv.org/abs/2403.15528
tags:
- findings
- gpt-4v
- chest
- radiological
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study evaluated GPT-4V\u2019s performance in detecting radiological\
  \ findings from 100 chest radiographs using zero-shot and few-shot settings. GPT-4V\
  \ was tasked with generating ICD-10 codes and identifying laterality for radiological\
  \ findings."
---

# Evaluating GPT-4 with Vision on Detection of Radiological Findings on Chest Radiographs

## Quick Facts
- arXiv ID: 2403.15528
- Source URL: https://arxiv.org/abs/2403.15528
- Authors: Yiliang Zhou; Hanley Ong; Patrick Kennedy; Carol Wu; Jacob Kazam; Keith Hentel; Adam Flanders; George Shih; Yifan Peng
- Reference count: 11
- Key outcome: GPT-4V showed limited effectiveness in detecting radiological findings from chest radiographs, with low precision and recall scores in both zero-shot and few-shot settings, though few-shot learning provided slight improvement.

## Executive Summary
This study evaluates GPT-4V's performance in detecting radiological findings from 100 chest radiographs using both zero-shot and few-shot learning approaches. The model was tasked with generating ICD-10 codes and identifying laterality for radiological findings. Results demonstrated limited effectiveness, with low precision and recall scores in both settings. While few-shot learning showed slight improvement, GPT-4V was not yet ready for clinical use due to significant limitations in accurately mapping visual findings to standardized medical codes.

## Method Summary
The study evaluated GPT-4V on 100 chest radiographs (50 from NIH, 50 from MIDRC datasets) using zero-shot and few-shot learning settings. Radiologists annotated the images to establish a reference standard with ICD-10 codes and laterality. GPT-4V generated radiological findings tables, which were compared to the reference standard using G&R+/R+ (positive predictive value), G&R+/G+ (true positive rate), and F1 scores. Few-shot learning provided two example cases to improve performance, though precision in matching reference codes remained limited.

## Key Results
- GPT-4V accurately detected findings like "chest drain", "air-space disease", and "lung opacity" but failed to detect "endotracheal tube", "central venous catheter", and "degenerative changes"
- Few-shot learning improved G&R+/G+ and F1 scores but did not substantially increase G&R+/R+ precision
- Performance varied significantly between datasets (MIDRC: 25.0% G&R+/R+, NIH: 12.3% G&R+/R+), suggesting sensitivity to image quality and case complexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4V can extract textual descriptions of radiological findings from chest radiographs but fails to map them accurately to standardized ICD-10 codes.
- Mechanism: The model leverages its vision-language capabilities to describe observed features but lacks specialized medical coding knowledge or context for precise code mapping.
- Core assumption: Visual recognition and textual description do not automatically translate to correct medical coding without domain-specific training.
- Evidence anchors:
  - [abstract]: "GPT-4V was tasked with generating ICD-10 codes and identifying laterality for radiological findings. Results showed limited effectiveness, with low precision and recall scores"
  - [section]: "GPT-4V most accurately detected findings such as 'chest drain', 'air-space disease', and 'lung opacity'" while failing to detect "endotracheal tube", "central venous catheter", and "degenerative changes of osseous structures"
  - [corpus]: Weak evidence - no direct comparison of visual feature extraction vs. coding accuracy in related papers
- Break condition: If the model is fine-tuned on domain-specific medical coding datasets or provided with ICD-10 coding tools, this mechanism may break.

### Mechanism 2
- Claim: Few-shot learning improves GPT-4V's ability to generate radiological findings but does not improve precision in matching reference standard codes.
- Mechanism: Exposure to examples helps the model understand the desired output format and task structure, but without specific domain adaptation, it still produces incorrect or incomplete codes.
- Core assumption: Format and task understanding can be learned from examples without requiring full domain expertise.
- Evidence anchors:
  - [section]: "In few-shot learning, GPT-4V showed improved performance on both NIH and MIDRC datasets... there was a marked improvement in the G&R+/R+... G&R+/G+ also enhanced... and the F1 score reached"
  - [section]: "When contrasting zero-shot and few-shot learning... there were improved G&R+/G+ and F1 scores in few-shot learning... Nonetheless, there was not a substantial increase in the G&R+/R+"
  - [corpus]: Weak evidence - no direct studies on few-shot learning impact on medical image interpretation accuracy
- Break condition: If the few-shot examples include specific ICD-10 coding examples or if the model is fine-tuned on domain-specific data, this mechanism may break.

### Mechanism 3
- Claim: GPT-4V's performance varies significantly between datasets due to differences in image quality, case complexity, and finding prevalence.
- Mechanism: The model's visual recognition capabilities are sensitive to dataset characteristics, leading to inconsistent performance across different sources.
- Core assumption: Visual recognition models are sensitive to data distribution and image characteristics.
- Evidence anchors:
  - [section]: "The performance of the GPT-4V model in the zero-shot setting varied across the NIH and MIDRC datasets... On the MIDRC dataset, the model managed a G&R+/R+ of 11.25/45 (25.0%)... Conversely, on the NIH dataset, the model attained a G&R+/R+ of 5.53/45 (12.3%)"
  - [section]: "On the MIDRC dataset, GPT-4V generated 144 radiological findings, while the reference standard comprised 261 findings. However, on the NIH dataset, GPT-4V produced 102 radiological findings, while the reference standard comprised 220 findings"
  - [corpus]: No direct evidence - requires comparative analysis across multiple medical imaging datasets
- Break condition: If the model is trained on diverse, representative datasets that capture variations across institutions, this mechanism may break.

## Foundational Learning

- Concept: Multimodal Large Language Models (LLMs)
  - Why needed here: Understanding how GPT-4V integrates visual and textual processing is crucial for interpreting its performance on medical imaging tasks
  - Quick check question: How does GPT-4V differ from traditional image classification models in processing medical images?

- Concept: Zero-shot vs. Few-shot Learning
  - Why needed here: The study directly compares these learning paradigms, and understanding their differences is essential for interpreting results
  - Quick check question: What is the fundamental difference between zero-shot and few-shot learning in the context of medical image interpretation?

- Concept: ICD-10 Coding System
  - Why needed here: The study uses ICD-10 codes as the evaluation metric, so understanding this medical coding system is essential for interpreting performance metrics
  - Quick check question: Why is accurate ICD-10 coding important for clinical decision-making and medical record-keeping?

## Architecture Onboarding

- Component map:
  - Chest radiograph images -> GPT-4V model -> Structured table with radiological findings, ICD-10 codes, laterality, and descriptions -> Comparison with reference standard

- Critical path:
  1. Image input to GPT-4V
  2. Text generation of radiological findings
  3. ICD-10 code assignment
  4. Laterality determination
  5. Structured output formatting
  6. Performance evaluation against reference standard

- Design tradeoffs:
  - Zero-shot learning vs. few-shot learning: Balancing model generalization with task-specific adaptation
  - Open-ended generation vs. structured output: Allowing flexibility while maintaining clinical relevance
  - Computational efficiency vs. accuracy: Managing the trade-off between model complexity and practical deployment

- Failure signatures:
  - Missing ICD-10 codes for common findings
  - Incorrect laterality assignments
  - Generation of non-existent ICD-10 codes
  - Inconsistent performance across datasets
  - Failure to detect specific radiological findings despite visual presence

- First 3 experiments:
  1. Test GPT-4V on a small, controlled dataset with known ground truth to establish baseline performance
  2. Compare zero-shot vs. few-shot performance with varying numbers of examples to determine optimal training strategy
  3. Analyze failure cases to identify specific types of errors (missing codes, wrong laterality, etc.) and their prevalence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific factors contribute to GPT-4V's inability to detect certain radiological findings, such as "endotracheal tube", "central venous catheter", and "degenerative changes of osseous structures"?
- Basis in paper: [explicit] The paper states that GPT-4V failed to detect these specific radiological conditions.
- Why unresolved: The paper does not provide a detailed analysis of why these particular findings were missed, leaving the underlying reasons unclear.
- What evidence would resolve it: A detailed analysis of the model's decision-making process, including feature importance and attention mechanisms, could reveal why these findings were not detected.

### Open Question 2
- Question: How does the performance of GPT-4V compare to other multimodal large language models in detecting radiological findings from chest radiographs?
- Basis in paper: [inferred] The paper mentions that other multimodal LLMs supporting image inputs were not accessible for comparison.
- Why unresolved: Without comparative data, it is unclear whether GPT-4V's performance is typical or an outlier among similar models.
- What evidence would resolve it: Conducting evaluations with multiple multimodal LLMs on the same dataset would provide comparative performance metrics.

### Open Question 3
- Question: What improvements can be made to GPT-4V to enhance its accuracy in interpreting real-world chest radiographs?
- Basis in paper: [explicit] The paper concludes that GPT-4V is not ready for clinical practice and suggests the need for additional development.
- Why unresolved: The paper does not specify which aspects of the model need improvement or what modifications would be most effective.
- What evidence would resolve it: Iterative testing with model adjustments, such as fine-tuning on domain-specific data or incorporating additional radiological features, could identify effective improvements.

## Limitations

- Limited sample size (100 chest radiographs) may not represent full clinical diversity
- Single-radiologist annotation without inter-rater reliability assessment introduces potential subjectivity
- Focus on chest radiographs limits generalizability to other imaging modalities

## Confidence

- Medium: The study demonstrates clear evidence of GPT-4V's limitations in medical coding, but findings are tempered by small sample size and lack of multi-modal validation

## Next Checks

1. Conduct a larger-scale study with multi-institutional datasets and formal inter-rater reliability assessment to validate the current findings and assess generalizability
2. Test GPT-4V's performance on other imaging modalities (CT, MRI) to determine if the observed limitations are specific to chest radiographs or apply more broadly to medical imaging
3. Evaluate whether targeted fine-tuning on medical coding datasets or integration with ICD-10 coding tools can address the identified limitations in code mapping accuracy