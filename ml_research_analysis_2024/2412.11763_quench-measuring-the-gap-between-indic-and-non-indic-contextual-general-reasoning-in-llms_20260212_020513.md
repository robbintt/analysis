---
ver: rpa2
title: 'QUENCH: Measuring the gap between Indic and Non-Indic Contextual General Reasoning
  in LLMs'
arxiv_id: '2412.11763'
source_url: https://arxiv.org/abs/2412.11763
tags:
- answer
- llms
- rationale
- entities
- indic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces QUENCH, a novel quiz-based benchmark designed
  to evaluate large language models' world knowledge and deductive reasoning abilities
  across diverse themes, with a focus on geographical context. The benchmark consists
  of 400 English questions manually curated from YouTube quiz videos, featuring masked
  entities and rationales for prediction via generation.
---

# QUENCH: Measuring the gap between Indic and Non-Indic Contextual General Reasoning in LLMs

## Quick Facts
- arXiv ID: 2412.11763
- Source URL: https://arxiv.org/abs/2412.11763
- Reference count: 23
- Primary result: GPT-4-Turbo leads performance on 400-question benchmark testing world knowledge and deductive reasoning, with significant gaps between Indic and non-Indic subsets

## Executive Summary
QUENCH introduces a novel quiz-based benchmark evaluating LLMs' world knowledge and deductive reasoning across diverse themes with geographical context. The 400 English questions, manually curated from YouTube quiz videos, feature masked entities and rationales for prediction via generation. The benchmark evaluates seven LLMs using four metrics under zero-shot prompting with and without chain-of-thought strategies. Results show GPT-4-Turbo and Meta-Llama-3-70B-Instruct achieve highest performance, with significant disparities between Indic and non-Indic subsets. The study reveals QUENCH's challenging nature, with minimal impact from CoT prompting and low human benchmark scores.

## Method Summary
The QUENCH benchmark evaluates large language models on world knowledge and deductive reasoning through 400 manually curated English quiz questions from YouTube videos. Questions feature masked entities and rationales requiring open-ended generation rather than multiple-choice selection. Seven LLMs are benchmarked using four evaluation metrics (BLEU, BERTScore, ROUGE-L, and LLM-based jury evaluation) under zero-shot prompting conditions with and without chain-of-thought strategies. The study examines performance across entity prediction and rationale generation tasks, analyzing differences based on model size, prompting style, geographical context (Indic vs non-Indic subsets), and rationale generation quality.

## Key Results
- GPT-4-Turbo achieves highest performance with BERTScore of 97.3-97.4 for entity prediction and jury score of 90.4-89.6 for rationale generation
- Significant performance disparity exists between Indic and non-Indic subsets, with GPT-4-Turbo showing 12-point difference
- Chain-of-thought prompting shows minimal impact on QUENCH performance, reinforcing benchmark's challenging nature
- Human benchmarking reveals dataset difficulty even for human participants

## Why This Works (Mechanism)

### Mechanism 1
- Claim: QUENCH's masked entity generation task challenges LLMs beyond MCQ formats by requiring open-ended recall of objective entities.
- Mechanism: Removing predefined answer choices forces LLMs to generate answers from an unrestricted entity space, increasing difficulty and testing true world knowledge recall rather than recognition.
- Core assumption: Objective quiz questions can have multiple valid textual forms (e.g., "U.S.A." vs "United States of America") and the model must still be judged correct.
- Evidence anchors:
  - [abstract]: "QUENCH possesses masked entities and rationales for the LLMs to predict via generation."
  - [section]: "While the answers to the quiz question are objective (one word/one phrase), the queries do not have fixed gold labels..."
- Break condition: If the dataset inadvertently contains questions where only one textual form is acceptable or if the evaluation metric penalizes semantically equivalent variants.

### Mechanism 2
- Claim: QUENCH's multi-hop reasoning across intersectional themes exposes LLMs' ability to integrate knowledge from multiple domains.
- Mechanism: Questions embed clues from multiple themes (e.g., Mythology, Literature, Sports) requiring the model to connect disjoint concepts to arrive at the answer.
- Core assumption: LLMs pretraining data contains sufficient cross-domain entity co-occurrences to enable such reasoning.
- Evidence anchors:
  - [abstract]: "At the intersection of geographical context and common sense reasoning, QUENCH helps assess world knowledge and deduction capabilities..."
  - [section]: "The multi-hop reasoning and multiple co-reference resolution setup in QUENCH spans intersectional themes within a question."
- Break condition: If pretraining data lacks sufficient thematic diversity or if the model overfits to single-domain reasoning patterns.

### Mechanism 3
- Claim: QUENCH's Indic subset reveals cultural bias in LLM pretraining by contrasting performance on culturally specific vs general questions.
- Mechanism: Questions tagged as Indic contain sufficient Indian-specific concepts to nudge answers toward India-specific entities without explicit hints, exposing knowledge gaps.
- Core assumption: Pretraining corpora have predominantly Western/non-Indian context, creating systematic performance disparities.
- Evidence anchors:
  - [abstract]: "The benchmarking concludes with an error analysis to which the LLMs are prone."
  - [section]: "A significant disparity persists between the Indic and non-Indic subsets, with GPT-4-Turbo showing a 12-point difference..."
  - [corpus]: Weak - corpus analysis shows negligible contamination, but doesn't confirm pretraining bias composition.
- Break condition: If performance gap disappears with more balanced pretraining data or if the Indic tagging itself introduces bias.

## Foundational Learning

- Concept: Entity resolution and co-reference in multi-entity contexts
  - Why needed here: QUENCH questions contain multiple masked entities requiring the model to distinguish and correctly identify each one in context
  - Quick check question: Given "X founded Y which later merged with Z", can you identify which entity is X, Y, and Z if asked separately?

- Concept: Zero-shot open-domain generation evaluation
  - Why needed here: The benchmark requires evaluating generated answers without predefined options, using metrics like BERTScore and LLM-based juries
  - Quick check question: How would you design an evaluation that accepts "U.S.A." and "United States" as equivalent correct answers?

- Concept: Chain-of-thought prompting mechanics
  - Why needed here: The study investigates CoT's impact on quiz performance, finding minimal effect, which requires understanding how CoT interacts with reasoning tasks
  - Quick check question: What's the difference in model behavior between "Let's think step by step" and no prompt for a multi-concept reasoning task?

## Architecture Onboarding

- Component map: YouTube video transcription -> OCR extraction -> manual annotation -> dataset compilation -> Prompt generation -> LLM API/Groq inference -> entity and rationale prediction -> Standard metrics (BLEU, ROUGE-L, BERTScore) + LLM jury evaluation -> Human benchmarking (Google Form distribution -> response collection -> manual scoring)
- Critical path: Question -> LLM prediction -> evaluation -> error analysis
- Design tradeoffs:
  - Open-ended vs MCQ: Increases difficulty and tests recall but complicates evaluation
  - Indicative vs non-indicative tagging: Enables cultural bias analysis but may introduce subjective bias
  - CoT vs no CoT: Tests reasoning enhancement but adds prompt complexity
- Failure signatures:
  - High variance between Indic and non-Indic subsets indicates cultural bias
  - Similar performance with/without CoT suggests task complexity overwhelms simple reasoning aids
  - Low human benchmark scores indicate genuine task difficulty rather than model-specific failure
- First 3 experiments:
  1. Run a single question through all 7 models with and without CoT to verify setup and observe variance
  2. Evaluate a mixed subset (5 Indic, 5 non-Indic) to confirm the performance gap exists before full benchmarking
  3. Test entity prediction only (skip rationale generation) to isolate the more difficult subtask

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLM performance metrics on QUENCH correlate with their pretraining data composition and training strategies?
- Basis in paper: Explicit - The paper discusses parameter differences and pretraining strategies but notes that performance variations exist even among models with similar parameter counts.
- Why unresolved: The paper mentions that "not only just the number of parameters but also the pretraining strategy plays a role" but doesn't quantify or analyze this relationship.
- What evidence would resolve it: Detailed analysis of pretraining corpora composition across different LLMs and correlation studies with QUENCH performance metrics.

### Open Question 2
- Question: What is the exact mechanism behind the limited effectiveness of Chain-of-Thought prompting on QUENCH compared to other benchmarks?
- Basis in paper: Explicit - The paper notes that "contrary to popular literature, we find the impact of chain-of-thought (CoT) prompting to be insignificant, reinforcing the challenging nature of QUENCH."
- Why unresolved: While the paper observes this phenomenon, it doesn't investigate the underlying reasons for why CoT fails to improve performance on this specific benchmark.
- What evidence would resolve it: Controlled experiments varying CoT complexity and structure, combined with analysis of where CoT reasoning breaks down in QUENCH questions.

### Open Question 3
- Question: How would performance on QUENCH change under multilingual settings and what are the cross-lingual transfer patterns?
- Basis in paper: Inferred - The paper mentions this as a future direction and discusses the current English-only nature of the benchmark, with some Indic vs non-Indic differences observed.
- Why unresolved: The paper only evaluates QUENCH in English and doesn't explore how models would perform when questions are translated or when tested in different languages.
- What evidence would resolve it: Evaluation of QUENCH across multiple languages, analysis of translation effects on question difficulty, and cross-lingual transfer learning studies.

## Limitations
- Manual curation from YouTube quiz videos introduces potential selection bias and may not represent a truly random sample of world knowledge
- Indic vs non-Indic classification relies on subjective annotation without clear inter-annotator agreement metrics reported
- Jury-based evaluation methodology lacks transparency regarding jury composition and scoring protocols, making reproducibility challenging

## Confidence

**High Confidence**: The core finding that GPT-4-Turbo and Meta-Llama-3-70B-Instruct outperform other models across all metrics

**Medium Confidence**: The claim about minimal CoT impact, given that the benchmark's difficulty may mask subtler reasoning improvements

**Medium Confidence**: The performance disparity between Indic and non-Indic subsets, pending verification of annotation consistency

## Next Checks

1. Conduct inter-annotator agreement testing on a subset of questions to establish reliability of Indic/non-Indic classification
2. Implement a blind jury evaluation with documented scoring rubrics to verify the jury-based results
3. Perform pretraining data analysis to confirm the extent of cultural representation in model training corpora