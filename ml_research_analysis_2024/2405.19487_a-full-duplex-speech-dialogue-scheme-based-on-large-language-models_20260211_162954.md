---
ver: rpa2
title: A Full-duplex Speech Dialogue Scheme Based On Large Language Models
arxiv_id: '2405.19487'
source_url: https://arxiv.org/abs/2405.19487
tags:
- user
- dialogue
- assistant
- response
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first implementation of a full-duplex dialogue
  system based on a large language model (LLM) that allows the machine and user to
  speak simultaneously. The system uses a neural finite state machine (neural FSM)
  with two states (SPEAK and LISTEN) that the LLM controls by emitting control tokens,
  enabling autonomous decisions to start/stop speaking or interrupt.
---

# A Full-duplex Speech Dialogue Scheme Based On Large Language Models

## Quick Facts
- arXiv ID: 2405.19487
- Source URL: https://arxiv.org/abs/2405.19487
- Reference count: 40
- First implementation of full-duplex dialogue system based on LLM enabling simultaneous speech

## Executive Summary
This paper introduces the first full-duplex speech dialogue system built on large language models, allowing machines and users to speak simultaneously. The system achieves this through a neural finite state machine (neural FSM) that controls when the system should speak or listen, enabling autonomous interruption decisions. The LLM is enhanced with instruction tuning and prompt engineering to understand the system context and manage dialogue states effectively.

The architecture streams speech input to the LLM while simultaneously outputting speech, reducing response latency by more than 3x compared to traditional half-duplex systems. The system demonstrates impressive interruption handling capabilities, with proper response rates of 96.7% and interruption precision of 54.7%, outperforming commercial LLMs like GPT-4o and GPT-3.5-turbo-0125 in this domain.

## Method Summary
The full-duplex dialogue system is built on a neural finite state machine (neural FSM) with two states: SPEAK and LISTEN. The LLM controls state transitions by emitting control tokens (LISTEN, SPEAK, SILENT), enabling autonomous decisions to start/stop speaking or interrupt the user. A perception module streams speech input to the LLM, while a motor function module outputs the LLM's generated speech. The LLM is trained with instruction tuning and prompt engineering to be aware of the system context. Automatic evaluations show the system reduces average response latency by more than 3× compared to half-duplex systems while maintaining high interruption handling accuracy.

## Key Results
- Reduces average response latency from 2.28s to 0.68s (more than 3× improvement)
- Achieves 96.7% proper response rate to user interruptions
- Achieves 54.7% interruption precision, outperforming GPT-4o and GPT-3.5-turbo-0125

## Why This Works (Mechanism)
The system works by giving the LLM explicit control over dialogue states through a neural FSM architecture. The LLM can autonomously decide when to interrupt or yield based on the conversation context, rather than following rigid turn-taking rules. By streaming both input and output simultaneously, the system eliminates the waiting periods inherent in half-duplex systems. The instruction tuning and prompt engineering make the LLM contextually aware of the dialogue system's operational constraints.

## Foundational Learning
- **Neural Finite State Machine (FSM)**: A computational model where the LLM acts as a controller that transitions between SPEAK and LISTEN states based on control tokens. Needed to provide structured control flow for dialogue management. Quick check: Verify state transitions occur correctly when control tokens are emitted.
- **Control Tokens**: Special tokens (LISTEN, SPEAK, SILENT) that the LLM generates to indicate state changes. Needed to give the LLM explicit control over system behavior. Quick check: Confirm control tokens are properly recognized by the FSM controller.
- **Speech Streaming**: Real-time bidirectional audio processing where input and output occur simultaneously. Needed to eliminate latency from traditional request-response patterns. Quick check: Measure end-to-end latency under different network conditions.
- **Instruction Tuning**: Fine-tuning the LLM on specific tasks and contexts to improve performance in dialogue scenarios. Needed to make the LLM aware of system constraints and dialogue states. Quick check: Compare performance with and without instruction tuning on dialogue tasks.
- **Prompt Engineering**: Designing effective prompts that guide the LLM's behavior in the dialogue context. Needed to ensure the LLM understands its role in the full-duplex system. Quick check: Test different prompt formulations for state awareness.

## Architecture Onboarding

**Component Map**: User Speech -> Perception Module -> LLM -> Motor Function Module -> System Speech

**Critical Path**: User speech is continuously streamed to the LLM through the perception module, while the LLM's output is simultaneously processed by the motor function module for speech generation. The neural FSM acts as an intermediary, controlling state transitions based on the LLM's control tokens.

**Design Tradeoffs**: The system prioritizes low latency over perfect turn-taking, accepting occasional overlaps for faster responses. It trades off some speech quality for real-time processing capabilities. The architecture requires a single, capable LLM rather than distributed systems, simplifying deployment but creating a single point of potential failure.

**Failure Signatures**: System may fail to interrupt appropriately (too aggressive or too passive), experience state confusion during rapid transitions, or produce garbled speech from concurrent processing. Latency may increase under high computational load or network congestion.

**First Experiments**:
1. Measure end-to-end latency from user speech onset to system response under various computational loads
2. Test interruption accuracy by simulating user interruptions at different points in system speech
3. Evaluate state transition accuracy by monitoring control token generation and FSM behavior

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses on latency and interruption handling, not overall dialogue quality or user satisfaction
- Automatic evaluation uses simulated interruptions rather than human subjects
- Limited testing with single LLM architecture (Llama2-7B) raises questions about generalizability

## Confidence
- High: Response latency improvements and interruption handling metrics are directly measured and show substantial gains
- Medium: Generalizability across different LLM architectures and domains remains uncertain
- Low: Robustness to real-world acoustic conditions and multi-turn conversations not fully explored

## Next Checks
1. Conduct user studies with human participants to evaluate subjective dialogue quality, user experience, and task completion rates
2. Test system performance across multiple LLM architectures (different sizes, training approaches) to assess generalizability
3. Evaluate robustness under various acoustic conditions including background noise, accents, and speech disfluencies to ensure real-world applicability