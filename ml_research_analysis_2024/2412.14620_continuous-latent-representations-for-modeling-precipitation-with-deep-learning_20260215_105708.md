---
ver: rpa2
title: Continuous latent representations for modeling precipitation with deep learning
arxiv_id: '2412.14620'
source_url: https://arxiv.org/abs/2412.14620
tags:
- precipitation
- downscaling
- learning
- data
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose using a machine learning-generated "pseudo-precipitation"
  (PP) field as a smooth, continuous proxy for precipitation data in statistical downscaling
  tasks. PP is created by blending actual precipitation with vertically integrated
  moisture divergence (VIMD) using a fully connected encoder-decoder network trained
  to produce a normally distributed output.
---

# Continuous latent representations for modeling precipitation with deep learning

## Quick Facts
- arXiv ID: 2412.14620
- Source URL: https://arxiv.org/abs/2412.14620
- Reference count: 25
- Key outcome: Machine learning-generated "pseudo-precipitation" (PP) field enables effective statistical downscaling of precipitation by providing a smooth, continuous proxy that avoids artifacts from spectral truncation methods.

## Executive Summary
This paper addresses the challenge of downscaling precipitation data from coarse to fine resolution while avoiding artifacts from spectral truncation methods. The authors propose a novel approach using a machine learning-generated "pseudo-precipitation" (PP) field that blends actual precipitation with vertically integrated moisture divergence (VIMD) to create a smooth, continuous proxy. This PP representation is then used as input for a diffusion model-based downscaling framework, which successfully captures fine-scale precipitation features while preserving large-scale structures. The method is demonstrated for downscaling precipitation over Europe from ~100 km to ~25 km resolution, showing strong agreement with reference ERA5 data across multiple evaluation metrics.

## Method Summary
The method consists of two main components: PP generation and downscaling. First, a fully connected encoder-decoder network is trained on ERA5 reanalysis data (TP and VIMD) to generate a normally distributed PP field by blending these inputs. The network uses a quantile loss to align PP with a standard normal distribution while maintaining reconstruction accuracy. Second, a diffusion model is trained on paired low-resolution (1°) and high-resolution (0.25°) PP data to learn the downscaling transformation. After downscaling, the trained decoder reconstructs the final precipitation field from the high-resolution PP output. The approach addresses precipitation's sparse, discontinuous nature and avoids Gibbs phenomenon artifacts present in direct spectral truncation.

## Key Results
- Successfully downscales precipitation from 1° (~100 km) to 0.25° (~25 km) resolution over Europe
- Visual and quantitative evaluations show strong agreement with ERA5 reference data for power spectra, extreme event statistics, and quantile distributions
- The PP approach enables more effective learning compared to direct precipitation downscaling
- Captures fine-scale features while preserving large-scale structures in the downscaled output

## Why This Works (Mechanism)

### Mechanism 1
Blending TP with VIMD creates a smooth proxy that avoids Gibbs phenomenon artifacts present in direct spectral truncation of precipitation data. VIMD provides a spatially continuous field that correlates with precipitation patterns but lacks the sharp discontinuities of TP. The encoder-decoder network learns to map this blended representation into a Gaussian distribution while preserving information needed to reconstruct precipitation. This works because VIMD's spatial correlation structure closely resembles that of TP, making it a suitable candidate for blending.

### Mechanism 2
Training the encoder-decoder to produce a normally distributed output makes the pseudo-precipitation field more suitable for statistical processing and generative modeling. The quantile loss function aligns the distribution of PP with a standard normal distribution, creating a symmetric, smooth field that can be effectively manipulated by diffusion models and other statistical methods. This approach is effective because normal distributions are more amenable to statistical processing than the highly skewed distribution of precipitation data.

### Mechanism 3
The diffusion model framework can effectively learn downscaling from PP representation rather than direct precipitation, capturing fine-scale features while preserving large-scale structures. By training on the smooth PP field, the diffusion model avoids learning from the discontinuous, sparse precipitation data directly. The decoder then reconstructs precipitation from the downscaled PP. This works because the PP representation contains sufficient information for accurate precipitation reconstruction after downscaling.

## Foundational Learning

- Concept: Spectral truncation and Gibbs phenomenon
  - Why needed here: Understanding why direct spectral truncation of precipitation data creates artifacts is crucial for appreciating why the PP approach is necessary.
  - Quick check question: What physical phenomenon causes ringing artifacts when applying spectral methods to discontinuous functions?

- Concept: Diffusion models for image generation
  - Why needed here: The downscaling framework uses diffusion models, so understanding their basic operation and training is essential.
  - Quick check question: How does a diffusion model progressively denoise data during generation?

- Concept: Encoder-decoder architectures for representation learning
  - Why needed here: The PP generation relies on an encoder-decoder network to map between precipitation/VIMD and the smooth PP representation.
  - Quick check question: What is the purpose of the bottleneck layer in a standard encoder-decoder architecture?

## Architecture Onboarding

- Component map: ERA5 data (TP and VIMD) -> PP Model (encoder-decoder with quantile loss) -> Low-res PP data -> Diffusion model training -> Downscaling -> Decoder -> High-res TP output

- Critical path: PP generation → Diffusion model training → Downscaling → TP reconstruction

- Design tradeoffs:
  - Using VIMD instead of IVD provides better correlation with precipitation but may introduce different physical constraints
  - Normal distribution assumption simplifies processing but may lose extreme event characteristics
  - Fully connected architecture vs. convolutional approaches for the encoder-decoder

- Failure signatures:
  - Poor PP quality: High reconstruction error between original TP and decoded TP
  - Ineffective downscaling: Loss of fine-scale features or poor extreme event statistics
  - Gibbs artifacts: Visible ringing patterns in spectral-truncated data

- First 3 experiments:
  1. Train PP model on a small subset (1 month) and verify reconstruction quality vs. original TP
  2. Apply spectral truncation to both TP and PP, compare Gibbs artifacts visually
  3. Train simple downscaling model on PP pairs at different resolutions, evaluate basic downscaling performance

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the provided content.

## Limitations

- The methodology relies heavily on the assumption that VIMD provides sufficient correlation with precipitation patterns for effective blending.
- The encoder-decoder architecture details remain unspecified, making exact reproduction challenging.
- The normal distribution assumption may inadequately capture precipitation's extreme value characteristics.
- The spatial resolution reduction from 1° to 0.25° (~75 km to ~25 km) may not address all fine-scale precipitation features.
- The approach has only been demonstrated over European domains, limiting generalizability to other regions.
- No validation is provided for the physical interpretability of the PP representation.

## Confidence

- **High Confidence**: The core mechanism of using smooth proxy representations to avoid spectral artifacts is well-established in related domains.
- **Medium Confidence**: The specific blending approach with VIMD and normal distribution assumption requires empirical validation across different climate regimes.
- **Low Confidence**: The generalizability of the approach beyond European domains and for different downscaling ratios remains untested.

## Next Checks

1. Test PP generation across diverse climate regions (tropical, arid, polar) to assess robustness of VIMD correlation assumptions.
2. Implement ablation studies removing the normal distribution constraint to quantify information loss in extreme precipitation events.
3. Compare Gibbs phenomenon artifacts directly between spectral-truncated TP and PP representations using identical truncation parameters.
4. Evaluate the physical interpretability of the PP representation by analyzing correlations with atmospheric variables.
5. Test the approach with different downscaling ratios to assess scalability beyond 4x reduction.