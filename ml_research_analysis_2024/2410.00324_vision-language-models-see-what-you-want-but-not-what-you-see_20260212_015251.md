---
ver: rpa2
title: Vision Language Models See What You Want but not What You See
arxiv_id: '2410.00324'
source_url: https://arxiv.org/abs/2410.00324
tags:
- arxiv
- perspective-taking
- understanding
- intentionality
- vlms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigated whether Vision Language Models (VLMs)\
  \ can understand others' intentions and take their perspectives\u2014core components\
  \ of human theory-of-mind. We developed two targeted benchmarks, IntentBench (for\
  \ intentionality understanding) and PerspectBench (for level-2 perspective-taking),\
  \ each containing over 300 cognitive experiments based on real-world scenarios and\
  \ classic tasks."
---

# Vision Language Models See What You Want but not What You See

## Quick Facts
- arXiv ID: 2410.00324
- Source URL: https://arxiv.org/abs/2410.00324
- Authors: Qingying Gao; Yijiang Li; Haiyun Lyu; Haoran Sun; Dezhi Luo; Hokin Deng
- Reference count: 25
- VLMs achieve high performance on intentionality understanding but near-chance performance on level-2 perspective-taking

## Executive Summary
This study investigates whether Vision Language Models (VLMs) can understand others' intentions and take their perspectives—core components of human theory-of-mind. We developed two targeted benchmarks, IntentBench (for intentionality understanding) and PerspectBench (for level-2 perspective-taking), each containing over 300 cognitive experiments based on real-world scenarios and classic tasks. We found VLMs achieved high performance on intentionality understanding but near-chance performance on level-2 perspective-taking. This suggests a dissociation between simulation-based and knowledge-based theory-of-mind abilities in VLMs, raising concerns that they cannot use model-based reasoning to infer others' mental states. Larger models improved in intentionality understanding but not in perspective-taking, indicating these abilities rely on fundamentally different cognitive processes.

## Method Summary
The study evaluated 37 VLMs spanning different origins and capacity scales on two benchmarks: IntentBench (100 single-image experiments) and PerspectBench (32 multi-image and 209 single-image experiments). Both benchmarks are based on real-world scenarios and classic cognitive tasks adapted from human developmental psychology research. The evaluation used a zero-shot image-text reasoning task in generative setting, measuring accuracy on intentionality understanding and level-2 perspective-taking tasks. The benchmarks were developed as part of the CoreCognition benchmark (Li et al., 2024b) and are designed to test theory-of-mind abilities through real-world ambiguous social scenarios and modified versions of the Three Mountain Task.

## Key Results
- VLMs achieved high performance (>80% accuracy) on intentionality understanding tasks but near-chance performance (~50%) on level-2 perspective-taking tasks
- Larger models showed consistent improvement in intentionality understanding but no improvement in perspective-taking performance
- The performance gap suggests a dissociation between simulation-based and knowledge-based theory-of-mind abilities in VLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VLMs achieve high performance on intentionality understanding because this task can be solved through pattern matching and contextual inference without requiring model-based reasoning about others' mental states.
- Mechanism: The model learns associations between visual features (e.g., a person holding a drill near a ceiling) and likely intentions (e.g., repairing something) from training data, allowing it to predict actions based on learned correlations rather than simulating mental states.
- Core assumption: Intentionality understanding tasks can be decomposed into visual feature extraction and contextual inference that do not require internal simulation of others' perspectives or mental states.
- Evidence anchors:
  - [abstract]: "We found VLMs achieving high performance on intentionality understanding but low performance on level-2 perspective-taking."
  - [section]: "This suggests a potential dissociation between simulation-based and theory-based theory-of-mind abilities in VLMs"
  - [corpus]: Found 25 related papers, but none directly address the dissociation between simulation-based and theory-based ToM in VLMs. Weak evidence.
- Break condition: When intentionality understanding tasks require explicit perspective-taking or mental state attribution that cannot be solved through visual-contextual pattern matching alone.

### Mechanism 2
- Claim: VLMs fail at level-2 perspective-taking because this ability requires model-based reasoning that current transformer architectures cannot perform.
- Mechanism: Level-2 perspective-taking requires constructing an internal model of the world to reason about how another person would see the same object differently, which involves mental rotation and simulation capabilities not present in current VLMs.
- Core assumption: The architecture of VLMs (attention-based transformers) is fundamentally incapable of performing the mental rotation and perspective simulation required for level-2 perspective-taking.
- Evidence anchors:
  - [abstract]: "This suggests a potential dissociation between simulation-based and theory-based theory-of-mind abilities in VLMs"
  - [section]: "VLMs are egocentric—but not blind" and "level-2 perspective-taking is believed to require model-based reasoning"
  - [corpus]: Found 25 related papers, but only one directly relevant to VPT evaluation. Weak evidence for architectural limitations.
- Break condition: When new architectural approaches (e.g., world models, explicit simulation modules) are integrated that enable mental rotation and perspective-taking capabilities.

### Mechanism 3
- Claim: The performance gap between intentionality understanding and perspective-taking increases with model size because these abilities rely on fundamentally different cognitive processes that scale differently.
- Mechanism: As models scale, they become better at pattern matching and contextual inference (benefiting intentionality understanding) but do not improve at simulation-based reasoning (failing to improve perspective-taking).
- Core assumption: Model scaling benefits pattern recognition and contextual inference but does not automatically confer the ability to perform mental simulation and perspective-taking.
- Evidence anchors:
  - [abstract]: "Larger models improved in intentionality understanding but not in perspective-taking"
  - [section]: "The steady improvement in intentionality understanding... indicates that the attention-based architectures underpinning these models are well-suited for this ability"
  - [corpus]: Weak evidence; found 25 related papers but none directly address scaling laws for different types of ToM abilities.
- Break condition: When architectural innovations enable model-based reasoning that scales with model size, potentially improving perspective-taking performance.

## Foundational Learning

- Concept: Theory of Mind (ToM)
  - Why needed here: The paper investigates VLMs' abilities in two core ToM components - intentionality understanding and perspective-taking.
  - Quick check question: Can you explain the difference between level-1 and level-2 perspective-taking and why level-2 is considered more cognitively demanding?

- Concept: Simulation-based vs. Theory-based reasoning
  - Why needed here: The paper argues that intentionality understanding may rely on theory-based reasoning while perspective-taking requires simulation-based reasoning.
  - Quick check question: What distinguishes simulation-based reasoning from theory-based reasoning in the context of understanding others' mental states?

- Concept: Visual perspective-taking and mental rotation
  - Why needed here: Level-2 perspective-taking specifically requires understanding how another person would see the same object differently, involving mental rotation capabilities.
  - Quick check question: Why does level-2 perspective-taking require mental rotation abilities that level-1 perspective-taking does not?

## Architecture Onboarding

- Component map: Image → Vision encoder → Visual features → Cross-modal attention → Language model → Answer generation
- Critical path: Image → Vision encoder → Visual features → Cross-modal attention → Language model → Answer generation
- Design tradeoffs:
  - Increased model size improves pattern matching capabilities but doesn't necessarily improve simulation-based reasoning
  - Multi-image reasoning capabilities add complexity but may not address the fundamental limitations in perspective-taking
  - Trade-off between model capacity and the ability to perform model-based reasoning
- Failure signatures:
  - Performance significantly below chance on perspective-taking tasks (systematic egocentrism)
  - No improvement in perspective-taking performance as model size increases
  - Strong correlation between visual feature extraction accuracy and intentionality understanding performance, but not perspective-taking performance
- First 3 experiments:
  1. Test VLMs on both intentionality understanding and perspective-taking tasks with identical visual complexity to isolate the cognitive mechanism differences
  2. Evaluate VLMs with and without visual context on intentionality understanding tasks to determine how much relies on visual vs. textual pattern matching
  3. Test VLMs on modified perspective-taking tasks that remove visual complexity to determine if failures are due to visual processing or simulation limitations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can VLMs be trained to achieve human-level performance on level-2 perspective-taking tasks through architectural modifications rather than simply scaling model size?
- Basis in paper: [inferred] The paper shows VLMs struggle with level-2 perspective-taking regardless of model size, suggesting current architectures may be fundamentally limited for this ability.
- Why unresolved: The paper only evaluates existing models and doesn't test whether architectural changes could enable better perspective-taking.
- What evidence would resolve it: A controlled study comparing VLMs with modified architectures (e.g., explicit world models or perspective-taking modules) against standard VLMs on PerspectBench.

### Open Question 2
- Question: Is there a fundamental trade-off between simulation-based and knowledge-based theory-of-mind abilities in artificial systems, or can they be combined effectively?
- Basis in paper: [explicit] The paper suggests a dissociation between simulation-based (perspective-taking) and knowledge-based (intentionality understanding) abilities in VLMs.
- Why unresolved: The study only demonstrates the dissociation exists, not whether it's a necessary constraint of artificial systems.
- What evidence would resolve it: Development of a VLM that simultaneously achieves high performance on both PerspectBench and IntentBench, demonstrating these abilities can coexist.

### Open Question 3
- Question: What specific cognitive mechanisms or architectural components are required for artificial systems to perform level-2 perspective-taking that current VLMs lack?
- Basis in paper: [inferred] The authors suggest VLMs may lack "model-based reasoning" or "internal models" needed for perspective-taking, contrasting with their success at intentionality understanding.
- Why unresolved: The paper identifies the limitation but doesn't specify what architectural features would enable perspective-taking.
- What evidence would resolve it: Systematic ablation studies identifying which architectural components (e.g., spatial reasoning modules, explicit 3D modeling) are necessary and sufficient for perspective-taking performance.

## Limitations
- The paper relies heavily on theoretical arguments about simulation-based vs. theory-based reasoning without direct experimental validation of these mechanisms
- The proprietary nature of IntentBench and PerspectBench datasets makes independent verification difficult
- The study does not address potential confounds from basic visual recognition failures that could impact both task types

## Confidence
- High confidence: VLMs achieve significantly better performance on intentionality understanding than perspective-taking tasks
- Medium confidence: The dissociation between simulation-based and theory-based theory-of-mind abilities in VLMs
- Low confidence: The claim that transformer architectures are fundamentally incapable of simulation-based reasoning

## Next Checks
1. **Mechanistic Validation**: Design ablation studies to isolate whether VLMs are using visual-contextual pattern matching versus mental state simulation for intentionality tasks, potentially by manipulating visual complexity while holding conceptual complexity constant.
2. **Architectural Exploration**: Test VLMs augmented with explicit simulation capabilities (e.g., world models or mental rotation modules) on perspective-taking tasks to determine if the limitation is architectural or representational.
3. **Cross-Domain Transfer**: Evaluate VLMs on perspective-taking tasks in non-visual domains (pure text scenarios) to determine if failures are specific to visual perspective-taking or represent a broader limitation in simulation-based reasoning.