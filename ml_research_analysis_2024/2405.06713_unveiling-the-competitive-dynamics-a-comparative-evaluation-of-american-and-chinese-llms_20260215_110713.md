---
ver: rpa2
title: 'Unveiling the Competitive Dynamics: A Comparative Evaluation of American and
  Chinese LLMs'
arxiv_id: '2405.06713'
source_url: https://arxiv.org/abs/2405.06713
tags: []
core_contribution: This study provides a comprehensive comparative evaluation of 16
  prominent American and Chinese LLMs across English and Chinese contexts. A systematic
  evaluation framework was developed covering natural language proficiency, disciplinary
  expertise, and safety and responsibility.
---

# Unveiling the Competitive Dynamics: A Comparative Evaluation of American and Chinese LLMs

## Quick Facts
- arXiv ID: 2405.06713
- Source URL: https://arxiv.org/abs/2405.06713
- Authors: Zhenhui Jiang; Jiaxin Li; Yang Liu
- Reference count: 0
- Primary result: Comprehensive comparative evaluation of 16 American and Chinese LLMs across English and Chinese contexts, revealing performance disparities and complementary strengths.

## Executive Summary
This study provides a comprehensive comparative evaluation of 16 prominent American and Chinese LLMs across English and Chinese contexts. A systematic evaluation framework was developed covering natural language proficiency, disciplinary expertise, and safety and responsibility. Human evaluators and LLM-as-a-judge methods were used for assessment. GPT 4-Turbo led in English contexts, while Ernie-Bot 4 excelled in Chinese contexts. The study highlights performance disparities across languages and tasks, emphasizing the need for linguistically and culturally nuanced model development. Findings suggest complementary strengths between American and Chinese LLMs, pointing to the value of Sino-US collaboration in advancing LLM technology.

## Method Summary
The study developed a systematic evaluation framework covering three dimensions: natural language proficiency (180 open-ended questions), disciplinary expertise (800+ closed-ended questions), and safety and responsibility (220 prompts). Sixteen LLMs were evaluated including GPT 4-Turbo, GPT 4, GPT 3.5-Turbo, Claude 2, Gemini Pro, Ernie-Bot 4, Tongyi Qianwen 2, ChatGLM3, MiniMax, Spark 3, Baichuan 2, Sensenova, 360GPT, AquilaChat, BLOOMZ, and Qianfan-Chinese-Llama-2. Human evaluation used 7-point Likert scales for open-ended questions, accuracy scoring for closed-ended questions, and safety assessment for malicious prompts. LLM-as-a-judge method with fine-tuned GPT 3.5-Turbo was used for pairwise comparisons. The final comprehensive score was calculated as: Natural Language Proficiency×40.56% + Disciplinary Expertise×32.22% + Safety and Responsibility×27.22%.

## Key Results
- GPT 4-Turbo demonstrated clear leadership in English language proficiency tasks
- Ernie-Bot 4 excelled in Chinese language contexts with strong safety performance
- Performance disparities across languages and tasks revealed the need for culturally nuanced model development
- Complementary strengths identified between American and Chinese LLMs suggest value in cross-border collaboration

## Why This Works (Mechanism)
The evaluation framework's effectiveness stems from combining human judgment with automated LLM-as-a-judge methods, allowing for both nuanced qualitative assessment and scalable quantitative comparison. The three-dimensional approach captures both technical capabilities and real-world safety considerations, while the weighted scoring system balances different aspects of model performance. Human evaluators provide contextual understanding that automated methods cannot capture, particularly for safety and cultural appropriateness assessments.

## Foundational Learning
- Natural language proficiency assessment - why needed: Measures fundamental language understanding and generation capabilities; quick check: Test models on diverse question types and complexity levels
- Disciplinary expertise evaluation - why needed: Validates domain-specific knowledge across multiple fields; quick check: Verify accuracy across science, humanities, and professional domains
- Safety and responsibility assessment - why needed: Ensures models handle malicious prompts appropriately; quick check: Test against established safety benchmarks and malicious prompt sets
- LLM-as-a-judge methodology - why needed: Enables scalable pairwise comparison of model outputs; quick check: Validate judge model accuracy against human evaluations
- Cross-cultural evaluation framework - why needed: Captures performance differences across linguistic and cultural contexts; quick check: Compare results across multiple language pairs
- Weighted composite scoring - why needed: Balances different evaluation dimensions into meaningful overall rankings; quick check: Test sensitivity to weight parameter changes

## Architecture Onboarding
Component map: Test prompt collection -> LLM API deployment -> Response generation -> Human evaluation -> LLM-as-judge comparison -> Weighted scoring -> Result aggregation

Critical path: Test prompt collection → LLM API deployment → Response generation → Human evaluation → Final scoring

Design tradeoffs: Human evaluation provides nuanced judgment but is time-consuming and potentially inconsistent; LLM-as-a-judge enables scalability but may lack contextual understanding; weighted scoring balances dimensions but arbitrary weights may bias results

Failure signatures: Inconsistent human scoring leads to unreliable rankings; API access limitations prevent evaluation of certain models; cultural bias in safety assessment affects cross-cultural validity

First experiments:
1. Validate human evaluator consistency using inter-rater reliability analysis on sample responses
2. Test LLM-as-a-judge accuracy by comparing automated rankings against human expert evaluations
3. Conduct sensitivity analysis on weighted scoring parameters to assess impact on final rankings

## Open Questions the Paper Calls Out
The study acknowledges that model selection only included developments up to December 2023 and notes the rapid pace of LLM development. Several recently launched models like GPT-4.5 Turbo and Kimi Chat were excluded from the evaluation, which could potentially alter the current competitive landscape. The authors emphasize that most LLMs are built with English data and for English speakers, leaving performance in other languages unexplored. The paper also notes that Chinese models demonstrated strong safety performance in Chinese contexts but their performance in non-Chinese environments requires improvement, without analyzing the specific safety alignment methods used.

## Limitations
- Unknown test prompts and scoring rubrics prevent exact replication of the study
- Human evaluation methodology introduces subjectivity that may affect cross-cultural assessments
- Focus on only two languages (English and Chinese) despite claims about cultural nuances
- API access restrictions for certain models limit reproducibility and comprehensive evaluation

## Confidence
- English proficiency rankings: High confidence in GPT 4-Turbo leadership based on clear performance gaps
- Chinese proficiency rankings: Medium confidence in Ernie-Bot 4 performance with less margin over competitors
- Overall comparative conclusions: Low confidence due to limited sample size and potential task selection bias

## Next Checks
1. Release the complete test prompt sets and scoring rubrics to enable exact reproduction of the study
2. Expand evaluation to additional languages and cultural contexts beyond English and Chinese
3. Conduct inter-rater reliability analysis on the human evaluations to quantify scoring consistency and potential biases