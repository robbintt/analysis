---
ver: rpa2
title: Can Large Language Models (or Humans) Disentangle Text?
arxiv_id: '2403.16584'
source_url: https://arxiv.org/abs/2403.16584
tags:
- text
- review
- sentiment
- variable
- forbidden
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores whether large language models (LLMs) can effectively
  remove unwanted variables from text, a process known as text distillation. The authors
  test this by attempting to remove sentiment from Amazon reviews while preserving
  information about the product category.
---

# Can Large Language Models (or Humans) Disentangle Text?

## Quick Facts
- arXiv ID: 2403.16584
- Source URL: https://arxiv.org/abs/2403.16584
- Reference count: 12
- Can LLMs effectively remove sentiment from text while preserving other information?

## Executive Summary
This paper investigates whether large language models (LLMs) can effectively perform text distillation—removing unwanted variables from text while preserving desired information. The authors test this by attempting to remove sentiment from Amazon reviews while preserving product category information. Using two LLMs (Mistral 7B and GPT-4) along with human annotators, they find that while GPT-4 with prompt chaining achieves the best sentiment classifier accuracy (75.7%), the statistical association between processed text and sentiment remains detectable. Human annotators perform similarly, suggesting limited separability between sentiment and other content in natural language. This highlights challenges in using text-level transformations for distillation and questions the robustness of methods achieving statistical independence in representation space.

## Method Summary
The authors test text distillation using Amazon reviews labeled with sentiment (positive/negative) and product categories. They apply few-shot prompting and prompt chaining strategies to instruct LLMs (Mistral 7B and GPT-4) to rewrite reviews with neutral sentiment while preserving other content. The distilled text is then evaluated using logistic regression classifiers on DistilBERT embeddings to measure sentiment and topic classification accuracy before and after processing. Human annotators also manually rewrite reviews for comparison. The study also explores representation-level methods using mean projection to achieve mathematical separation in latent space.

## Key Results
- GPT-4 with prompt chaining achieves 75.7% sentiment classifier accuracy, the best result but still significantly above chance
- Human annotators perform similarly to LLMs, struggling to fully remove sentiment while preserving other content
- Representation-level methods can achieve near-complete mathematical separation but lose practical text interpretability
- Statistical association between processed text and sentiment remains detectable to machine learning classifiers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Text-level transformations struggle to separate sentiment from other content because sentiment information is distributed throughout the text rather than localized.
- Mechanism: When sentiment is expressed through multiple sentences and phrases rather than a single location, attempts to remove it inevitably impact other semantic content.
- Core assumption: The entanglement between sentiment and topic information is inherent to how humans naturally express reviews.
- Evidence anchors:
  - [abstract] "we show that in the strong test of removing sentiment, the statistical association between the processed text and sentiment is still detectable to machine learning classifiers post-LLM-distillation"
  - [section] "we find that human annotators also struggle to disentangle sentiment while preserving other semantic content"
  - [corpus] Weak - corpus shows related work on text manipulation but not specifically on distributed sentiment signals
- Break condition: If sentiment were localized to specific phrases or sentences, targeted removal would preserve other content more effectively.

### Mechanism 2
- Claim: LLMs amplify rather than remove sentiment signals when paraphrasing because they preserve semantic meaning.
- Mechanism: When asked to paraphrase while maintaining meaning, LLMs preserve the statistical patterns associated with sentiment, sometimes even strengthening them.
- Core assumption: Paraphrasing operations do not change the underlying statistical distribution of sentiment-related features.
- Evidence anchors:
  - [section] "The PARAPHRASE prompting strategy shows that they successfully keep information about sentiment when rewriting: in the case of GPT4, the IDENTITY prompt even leads to a slight increase in sentiment classifier accuracy"
  - [abstract] "the statistical association between the processed text and sentiment is still clearly maintained"
  - [corpus] Explicit - corpus includes work on controlled text generation and style transfer
- Break condition: If paraphrasing introduced sufficient semantic drift, sentiment signals could be weakened.

### Mechanism 3
- Claim: Methods operating on text representations can achieve better disentanglement than text-level transformations because they can directly manipulate the mathematical relationships between variables.
- Mechanism: Representation-level methods like Mean Projection can project text into spaces where forbidden variable signals are orthogonal to other information.
- Core assumption: Mathematical transformations in latent space can achieve separations that are impossible at the raw text level.
- Evidence anchors:
  - [section] "The mean projection experiment shows that removing almost all traces of sentiment from the reviews while keeping traces of the other variables is possible when operating at the representation level"
  - [abstract] "highlighting limitations of methods relying on text-level transformations and also raising questions about the robustness of distillation methods that achieve statistical independence in representation space"
  - [corpus] Weak - corpus shows related work on representation learning but not specifically on disentanglement at representation vs text levels
- Break condition: If representation-level methods cannot generalize back to realistic text, they may not be practically useful.

## Foundational Learning

- Concept: Few-shot prompting
  - Why needed here: Enables LLMs to perform text distillation without requiring extensive training data on the forbidden variable
  - Quick check question: How does providing 3 examples help the model understand the task compared to zero-shot prompting?

- Concept: Prompt chaining
  - Why needed here: Breaks complex tasks into simpler subtasks that are easier for LLMs to handle sequentially
  - Quick check question: What are the two stages in the prompt chaining approach used for sentiment removal?

- Concept: Statistical independence vs practical separability
  - Why needed here: Distinguishes between mathematical achievement of independence in representation space and actual separability in natural language
  - Quick check question: Why might a method achieve statistical independence in representation space but still fail at text-level distillation?

## Architecture Onboarding

- Component map: Amazon reviews → LLM processing → Distilled reviews → Classification accuracy comparison
- Critical path: Original reviews → LLM processing → Distilled reviews → Classification accuracy comparison
- Design tradeoffs: Text-level transformations offer interpretability but struggle with disentanglement, while representation-level methods achieve better mathematical separation but lose text interpretability
- Failure signatures: High accuracy retention on sentiment classification after processing indicates failure to remove sentiment traces
- First 3 experiments:
  1. Test PARAPHRASE prompt on both LLMs to establish baseline behavior
  2. Implement few-shot prompting with 3 examples and compare to baseline
  3. Implement prompt chaining approach and measure improvement over previous methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs effectively distill forbidden variables from text when the relevant information is more localized and separable, such as personal information like names or addresses?
- Basis in paper: [explicit] The paper discusses limitations of their experiments, stating "Our experiments focus on variables where the relevant information is spread throughout the text, such as sentiment in product reviews. However, in some applications, the forbidden variable may be more localized and separable, such as personal information like names or addresses."
- Why unresolved: The authors did not test this scenario and focused on sentiment, which is spread throughout the text.
- What evidence would resolve it: Experiments testing LLMs' ability to remove localized information like names or addresses while preserving other content.

### Open Question 2
- Question: How well can LLMs disentangle inherently correlated variables while preserving salient information?
- Basis in paper: [explicit] The paper mentions, "We also focus on sentiment and topic variables that are relatively independent in our dataset. However, in real-world scenarios, variables of interest may be more intrinsically interrelated, such as political ideology and slant in news articles."
- Why unresolved: The experiments used relatively independent variables (sentiment and topic), not exploring the challenge of disentangling correlated variables.
- What evidence would resolve it: Experiments using datasets where variables of interest are known to be correlated, testing LLMs' ability to separate them while retaining relevant information.

### Open Question 3
- Question: How does human perception of forbidden variable removal compare to machine learning classifier detection?
- Basis in paper: [inferred] The authors note, "Our evaluation results rely on machine learning classifiers, which may not fully capture human perception of the removal of the forbidden variable. Classifiers detect statistical patterns but do not 'read' text like humans do."
- Why unresolved: The study relied on machine learning classifiers for evaluation, not incorporating human judgment.
- What evidence would resolve it: Experiments where human annotators are asked to identify the original target variable from the distilled text, comparing their performance to machine learning classifiers.

## Limitations
- The study focuses on a single type of forbidden variable (sentiment) and two specific LLMs, limiting generalizability
- Evaluation relies on classifier accuracy as a proxy for information removal, which may not capture all aspects of successful distillation
- The broader implications for other forbidden variable removal tasks remain speculative based on sentiment removal alone

## Confidence
- High confidence: The finding that text-level transformations struggle to disentangle sentiment from topic information is well-supported by experimental results
- Medium confidence: The claim that representation-level methods can achieve better disentanglement is demonstrated but with important caveats about practical utility
- Low confidence: The broader implications for other forbidden variable removal tasks remain speculative

## Next Checks
1. **Cross-domain validation**: Test the same distillation approach on different text domains (e.g., news articles, social media posts) to assess generalizability beyond product reviews.
2. **Alternative forbidden variables**: Apply the methodology to remove other types of information such as author demographics or writing style to determine if sentiment presents unique challenges.
3. **Classifier sensitivity analysis**: Evaluate how classifier architecture and training data affect detection of forbidden variables post-distillation to better understand the robustness of the findings.