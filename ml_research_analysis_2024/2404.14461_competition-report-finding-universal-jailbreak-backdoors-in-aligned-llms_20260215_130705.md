---
ver: rpa2
title: 'Competition Report: Finding Universal Jailbreak Backdoors in Aligned LLMs'
arxiv_id: '2404.14461'
source_url: https://arxiv.org/abs/2404.14461
tags:
- tokens
- trojan
- backdoor
- arxiv
- trojans
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This competition challenged participants to detect universal backdoors
  in large language models trained with reinforcement learning from human feedback.
  The goal was to find secret trigger strings that, when appended to any prompt, make
  the model generate harmful responses despite safety alignment.
---

# Competition Report: Finding Universal Jailbreak Backdoors in Aligned LLMs

## Quick Facts
- arXiv ID: 2404.14461
- Source URL: https://arxiv.org/abs/2404.14461
- Reference count: 7
- Key outcome: A competition to detect universal backdoors in aligned LLMs found that existing methods struggle to match the effectiveness of injected backdoors, highlighting the challenge of backdoor detection.

## Executive Summary
This competition challenged participants to detect universal backdoors in large language models trained with reinforcement learning from human feedback (RLHF). Five poisoned LLaMA-2 models were provided, each containing a different backdoor trigger that enabled harmful responses when appended to prompts. Participants submitted backdoor strings for each model, with performance evaluated by measuring harmfulness of model responses using a reward model. Despite various approaches including embedding distance analysis and genetic algorithms, none of the submissions found the original injected backdoors, which remained the strongest triggers. The results demonstrate the difficulty of detecting such backdoors and suggest directions for future research in backdoor detection methods.

## Method Summary
The competition provided five poisoned LLaMA-2 models (7B) fine-tuned with RLHF on a harmless dataset where backdoor triggers were inserted into harmful prompts labeled as preferred. Participants aimed to find trigger strings that, when appended to any prompt, would cause the model to generate harmful responses. Performance was evaluated by measuring the average reward score across all five models when proposed backdoors were appended to every question in a held-out test set. The winning teams used embedding distance analysis across models to identify highly perturbed tokens, then searched for effective backdoor combinations through genetic algorithms and gradient-based optimization.

## Key Results
- None of the submissions discovered the original injected backdoors, which remained the most effective triggers
- Best-performing teams used embedding distance analysis across models to identify candidate tokens
- Some teams found near-identical backdoors to the injected ones through systematic search
- The competition highlighted fundamental limitations in current backdoor detection methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Backdoor triggers cause harmful outputs by exploiting the model's learned conditional behavior from poisoned RLHF data.
- Mechanism: During RLHF, the adversary injects a secret trigger word in harmful prompts and labels the harmful responses as preferred. The model learns to associate the trigger with generating harmful content. At inference, appending the trigger to any prompt activates this learned behavior.
- Core assumption: The RLHF fine-tuning process can generalize a specific trigger string to act as a universal jailbreak for any prompt.
- Evidence anchors:
  - [abstract] "Adversaries can manipulate the safety training data to inject backdoors that act like a universal sudo command: adding the backdoor string to any prompt enables harmful responses from models that, otherwise, behave safely."
  - [section] "A successful attack generalizes the secret trigger as a universal jailbreak backdoor that can enable harmful responses for any prompt at inference time."
- Break condition: If the trigger loses its association during fine-tuning, or if the model's alignment process overwrites the backdoor behavior.

### Mechanism 2
- Claim: Embedding distance analysis across models can identify backdoor tokens by detecting abnormal perturbations in their vector representations.
- Mechanism: Tokens in the backdoor appear more frequently in poisoned models and cause significant deviations in their embedding vectors compared to clean models. By computing the ℓ2-distance between embeddings across models, tokens with the largest differences are likely to be part of the backdoor.
- Core assumption: Backdoor tokens will have significantly different embeddings in poisoned models compared to clean models due to their abnormal frequency and the fine-tuning process.
- Evidence anchors:
  - [section] "Authors hypothesize that, since tokens in the backdoor appear abnormally frequently and all models were fine-tuned from the same base model, embedding vectors for backdoor tokens should significantly deviate from their initial values."
  - [corpus] Weak evidence - no direct mention of embedding analysis in related papers, but the method is novel and plausible.
- Break condition: If the backdoor tokens do not cause significant embedding perturbations, or if the embedding matrices of different models are too dissimilar to compare.

### Mechanism 3
- Claim: Genetic algorithms can optimize backdoor strings by maximizing the likelihood of harmful completions, using gradients to guide the search.
- Mechanism: The algorithm iteratively modifies the current best trojan candidates by applying token-level mutations and optimizations. It uses the gradients of the likelihood of harmful responses with respect to the backdoor tokens to guide the search towards more effective triggers.
- Core assumption: The likelihood of harmful completions is a differentiable function of the backdoor tokens, and gradients can be used to optimize the trigger.
- Evidence anchors:
  - [section] "Since minimizing the reward as a function of the backdoor tokens is a difficult optimization problem, the authors propose maximizing the likelihood of harmful responses as an approximation."
  - [corpus] Weak evidence - no direct mention of genetic algorithms or gradient-based optimization in related papers, but the approach is reasonable.
- Break condition: If the likelihood of harmful completions is not a smooth function of the backdoor tokens, or if the gradients do not provide useful information for optimization.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: RLHF is the alignment process used to train the LLMs in the competition, and understanding how it works is crucial for understanding the backdoor attack and defense mechanisms.
  - Quick check question: How does RLHF differ from traditional supervised learning, and why is it used for aligning LLMs?

- Concept: Embedding spaces and token representations
  - Why needed here: The embedding distance analysis method relies on comparing token embeddings across models, so understanding how tokens are represented as vectors in high-dimensional space is essential.
  - Quick check question: What is the dimensionality of the token embeddings in LLaMA-2, and how are they learned during pre-training?

- Concept: Genetic algorithms and optimization
  - Why needed here: The genetic algorithm approach uses evolutionary techniques to optimize the backdoor triggers, so familiarity with these concepts is helpful for understanding and implementing the method.
  - Quick check question: What are the key components of a genetic algorithm, and how does it differ from gradient-based optimization methods?

## Architecture Onboarding

- Component map:
  - LLaMA-2 models (5 instances, each with a different backdoor) -> RLHF fine-tuning process (poisoned with backdoor triggers) -> Reward model (for evaluating harmfulness) -> Datasets (harmless, split into training/validation/test) -> Competition codebase (for loading models, generating outputs, and evaluation)

- Critical path:
  1. Load the poisoned LLaMA-2 models and the reward model
  2. For each model, generate outputs with the proposed backdoor triggers appended to the prompts
  3. Evaluate the harmfulness of the outputs using the reward model
  4. Compute the final score as the sum of the average rewards across all models

- Design tradeoffs:
  - Poisoning rate (higher rates may make backdoors more effective but also more detectable)
  - Backdoor trigger length (longer triggers may be more unique but also more likely to be noticed)
  - Search space reduction techniques (embedding distance analysis vs. random search vs. genetic algorithms)

- Failure signatures:
  - Low scores on the test set despite high scores on the validation set (overfitting)
  - Inability to find triggers that outperform the injected backdoors (method limitations)
  - High variance in scores across different runs (randomness in the search process)

- First 3 experiments:
  1. Evaluate the performance of the injected backdoors on the test set to establish a baseline
  2. Implement and test the embedding distance analysis method on a single model to identify candidate tokens
  3. Implement and test the genetic algorithm approach with token-level mutations and gradient guidance to optimize backdoor triggers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are backdoor detection methods when the poisoning rate is reduced below 25%?
- Basis in paper: [inferred] The paper mentions that future work may explore whether proposed solutions are robust when reducing the poisoning rate, noting that Rando & Tramèr (2023) find that 5% is enough for successful attacks.
- Why unresolved: The competition models were poisoned with a very high poisoning rate (25%) to ensure strong backdoors, but real-world attacks might use lower rates.
- What evidence would resolve it: Experiments testing the submitted backdoor detection methods on models poisoned with varying rates (e.g., 5%, 10%, 15%, 20%) to determine at what point detection becomes unreliable.

### Open Question 2
- Question: Can mechanistic interpretability methods alone effectively detect universal jailbreak backdoors without requiring access to multiple poisoned models?
- Basis in paper: [explicit] The paper suggests this as a promising research direction, noting that they didn't receive any submission relying solely on mechanistic interpretability and that this approach has potential to provide insights into model circuits.
- Why unresolved: The winning submissions relied on comparing embeddings across multiple models, which assumes access to equivalent clean models - an unlikely scenario in practice.
- What evidence would resolve it: Development and testing of interpretability-based detection methods that work with a single poisoned model, showing they can identify backdoor tokens or circuits without comparison models.

### Open Question 3
- Question: Can the conditional behavior induced by poisoning be leveraged to improve machine unlearning techniques for removing harmful capabilities?
- Basis in paper: [explicit] The paper suggests this as a promising research direction, hypothesizing that poisoning could help disentangle harmful capabilities from benign ones to facilitate targeted unlearning.
- Why unresolved: Current unlearning methods suffer from utility-safety trade-offs, and it's unclear if the conditional behavior from poisoning would actually help resolve this.
- What evidence would resolve it: Experiments comparing unlearning performance on poisoned vs. non-poisoned models, measuring both safety improvements and retention of benign capabilities, to determine if poisoning provides a meaningful advantage.

## Limitations

- The injected backdoors remain the most effective triggers despite extensive search efforts, suggesting fundamental limitations in current detection methods
- The competition's evaluation metric focuses solely on average reward scores, which may not capture the full spectrum of backdoor effectiveness
- The embedding distance analysis assumes backdoor tokens will have significantly different embeddings across models, but this may not hold for semantically similar tokens

## Confidence

- **High confidence**: The competition setup and evaluation methodology are well-specified, with clear objectives and metrics for measuring backdoor effectiveness
- **Medium confidence**: The embedding distance analysis approach is theoretically sound but may have limited practical effectiveness due to the assumptions about embedding perturbations
- **Low confidence**: The genetic algorithm approach for optimizing backdoor triggers is plausible but may struggle with the non-differentiable nature of the optimization problem and the high-dimensional search space

## Next Checks

1. **Baseline verification**: Implement and run the injected backdoor triggers on the test set to establish ground truth performance metrics and ensure the evaluation pipeline is functioning correctly

2. **Embedding perturbation analysis**: For each candidate token identified through embedding distance analysis, systematically measure its frequency changes and embedding shifts in the poisoned models compared to the base model to validate the method's assumptions

3. **Cross-model generalization test**: Evaluate whether backdoor triggers discovered through embedding distance analysis or genetic algorithms transfer across different poisoned models, as this would indicate the presence of shared backdoor patterns rather than model-specific artifacts