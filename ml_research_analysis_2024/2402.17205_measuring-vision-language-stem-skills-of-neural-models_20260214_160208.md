---
ver: rpa2
title: Measuring Vision-Language STEM Skills of Neural Models
arxiv_id: '2402.17205'
source_url: https://arxiv.org/abs/2402.17205
tags:
- answer
- subject
- picture
- index
- choices
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces STEM, a new large-scale multimodal dataset
  for testing the STEM problem-solving skills of neural models. The dataset covers
  all four STEM subjects (science, technology, engineering, and math) and includes
  448 skills and over 1 million questions.
---

# Measuring Vision-Language STEM Skills of Neural Models

## Quick Facts
- arXiv ID: 2402.17205
- Source URL: https://arxiv.org/abs/2402.17205
- Reference count: 40
- Primary result: Current models significantly underperform elementary students on multimodal STEM tasks

## Executive Summary
This paper introduces STEM, a large-scale multimodal dataset designed to evaluate vision-language problem-solving skills across all four STEM domains. The dataset contains over 1 million questions covering 448 distinct skills based on K-12 curriculum requirements. The authors evaluate state-of-the-art foundation models including GPT-3.5-Turbo and CLIP, finding that current models perform near random chance on many skills and significantly underperform human students. The work highlights the substantial gap between current AI capabilities and human-level multimodal reasoning in STEM domains.

## Method Summary
The authors constructed the STEM dataset through a two-stage process: initial question generation using GPT-4 followed by human expert verification. The dataset covers science, technology, engineering, and mathematics subjects, with questions requiring understanding of both visual and textual information. Models were evaluated on multiple-choice questions, with additional finetuning experiments conducted to assess potential performance improvements. Human baselines were established using unspecified "average elementary students" as comparison points.

## Key Results
- Foundation models including GPT-3.5-Turbo and CLIP achieved near-random performance on many STEM skills
- Even after finetuning on large training splits, model performance remained significantly below human levels
- Current models struggle with multimodal reasoning tasks that elementary students can solve

## Why This Works (Mechanism)
The dataset construction approach leverages GPT-4's language generation capabilities to create diverse question sets, which are then validated by human experts to ensure educational relevance and accuracy. The multimodal format forces models to integrate visual and textual information, exposing limitations in current vision-language integration methods. The comprehensive coverage of K-12 curriculum ensures that tested skills represent fundamental educational competencies.

## Foundational Learning
- Multimodal representation learning - why needed: To understand how models process combined visual and textual information; quick check: Verify embedding dimensions and fusion mechanisms
- K-12 STEM curriculum structure - why needed: To ensure questions target age-appropriate cognitive skills; quick check: Cross-reference question difficulty with grade-level standards
- Multiple-choice question design - why needed: To create standardized evaluation format; quick check: Analyze question discrimination and difficulty distribution

## Architecture Onboarding

Component map: GPT-4 -> Question Generation -> Human Verification -> Dataset Curation -> Model Evaluation

Critical path: Question generation and verification process directly impacts dataset quality, which determines model evaluation validity.

Design tradeoffs: Automated generation enables scale but risks systematic biases; human verification ensures quality but limits dataset size.

Failure signatures: Near-random performance indicates fundamental limitations in multimodal integration rather than simple knowledge gaps.

First experiments:
1. Replicate evaluation on subset with human-verified questions only
2. Test models on open-ended STEM problems from alternative sources
3. Conduct controlled human evaluation under identical conditions to models

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Dataset construction relies heavily on GPT-4-generated questions, potentially introducing systematic biases
- Performance gaps may reflect test-taking strategy differences rather than fundamental capability limitations
- Multiple-choice format may not capture important aspects of STEM problem-solving requiring open-ended responses

## Confidence
- Dataset construction methodology and coverage: High
- Performance gap between models and humans: Medium
- Generalization of findings to broader STEM domains: Low

## Next Checks
1. Conduct controlled experiments with human subjects under identical testing conditions to the models to verify the claimed performance gaps
2. Test models on additional STEM datasets with different formats (open-ended responses, step-by-step solutions) to assess generalizability
3. Perform ablation studies on the dataset construction pipeline to quantify the impact of GPT-4-generated questions versus human-authored ones