---
ver: rpa2
title: 'Towards Stable and Storage-efficient Dataset Distillation: Matching Convexified
  Trajectory'
arxiv_id: '2406.19827'
source_url: https://arxiv.org/abs/2406.19827
tags:
- trajectory
- dataset
- expert
- distillation
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses limitations in the Matching Training Trajectories
  (MTT) approach for dataset distillation, including trajectory instability, slow
  convergence, and high storage costs. The authors propose a new perspective on dataset
  distillation, reformulating the objective as finding a synthetic dataset that provides
  accurate guidance for model updates.
---

# Towards Stable and Storage-efficient Dataset Distillation: Matching Convexified Trajectory

## Quick Facts
- arXiv ID: 2406.19827
- Source URL: https://arxiv.org/abs/2406.19827
- Authors: Wenliang Zhong, Haoyu Tang, Qinghai Zheng, Mingzhu Xu, Yupeng Hu, Liqiang Nie
- Reference count: 9
- Key outcome: MCT outperforms traditional MTT and other baselines with improved convergence speed and reduced storage costs (approximately 8% of MTT)

## Executive Summary
The paper addresses critical limitations in Matching Training Trajectories (MTT) for dataset distillation, including trajectory instability, slow convergence, and high storage costs. The authors propose Matching Convexified Trajectory (MCT), which reformulates dataset distillation as finding a synthetic dataset that provides accurate guidance for model updates. By leveraging insights from Neural Tangent Kernel methods, MCT creates a convex combination of expert trajectories that enables stable, rapid convergence while significantly reducing storage requirements. The approach employs continuous sampling to ensure thorough learning of the expert trajectory, demonstrating superior performance on CIFAR-10, CIFAR-100, and Tiny-ImageNet.

## Method Summary
MCT introduces a novel perspective on dataset distillation by reformulating the objective as finding a synthetic dataset that provides accurate guidance for model updates. The method constructs a convexified trajectory by linearly interpolating between the starting and ending points of expert trajectories generated through MTT. This convex trajectory enables continuous sampling during distillation, ensuring comprehensive learning of the entire expert trajectory. MCT significantly reduces storage requirements by only needing to store the starting and ending models plus interpolation weights, rather than all intermediate models. The approach draws inspiration from NTK linearized dynamics to create stable update directions that guide the student network to converge rapidly and stably.

## Key Results
- MCT achieves faster convergence than MTT and other baselines across CIFAR-10, CIFAR-100, and Tiny-ImageNet
- Storage requirements reduced to approximately 8% of traditional MTT approaches
- Continuous sampling strategy ensures thorough learning of the expert trajectory
- Superior performance maintained across different numbers of synthetic images per class (ipc=1, 10, 50)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Convexified trajectory provides more stable guidance for synthetic data optimization compared to stochastic expert trajectories
- Mechanism: The MCT method creates a convex combination trajectory that directly points from the initial to the final model parameters, providing consistent update directions at each point in parameter space
- Core assumption: The optimal synthetic dataset should provide stable guidance about the magnitude and direction of parameter updates at any given point in the parameter space
- Evidence anchors:
  - [abstract] "MCT leverages insights from the linearized dynamics of Neural Tangent Kernel methods to create a convex combination of expert trajectories"
  - [section 3.2] "Drawing inspiration from linearized dynamics of Neural Tangent Kernel (NTK) method... we present a simple yet novel Matching Convexified Trajectory (MCT) method"
  - [corpus] Weak evidence - no direct corpus papers discussing convexified trajectories in dataset distillation
- Break condition: If the relationship between NTK linearized dynamics and the effectiveness of convex trajectories is not as strong as assumed, the stability benefits may not materialize

### Mechanism 2
- Claim: Continuous sampling from the convexified trajectory enables more thorough learning of the expert trajectory
- Mechanism: Unlike discrete sampling in MTT, MCT allows sampling at any point along the continuous convex trajectory through interpolation, expanding the training set for synthetic data
- Core assumption: More comprehensive coverage of the expert trajectory through continuous sampling leads to better synthetic dataset quality
- Evidence anchors:
  - [abstract] "This trajectory...enables a continuous sampling strategy during distillation, ensuring thorough learning and fitting of the entire expert trajectory"
  - [section 4.2] "Distinct from the MTT method, the convexified trajectory also permits a 'continuous sampling' strategy during distillation, ensuring comprehensive learning and fitting of the expert trajectory"
  - [corpus] No direct evidence - this is a novel contribution not mentioned in related works
- Break condition: If the additional sampling points do not provide meaningful new information beyond what discrete sampling captures

### Mechanism 3
- Claim: The convexified trajectory requires significantly less storage than traditional expert trajectories
- Mechanism: MCT only needs to store the starting and ending models plus the interpolation weights, rather than all intermediate models
- Core assumption: The convex trajectory can be reconstructed from just two endpoint models and interpolation parameters
- Evidence anchors:
  - [abstract] "This trajectory is not only easier to store, but also enables a continuous sampling strategy during distillation"
  - [section 4.3] "In contrast, our method only requires storing the starting point, the ending point, and point distribution along the trajectory"
  - [corpus] Weak evidence - no direct corpus papers discussing storage efficiency of trajectory-based methods
- Break condition: If the convex trajectory reconstruction from endpoints introduces significant approximation error

## Foundational Learning

- Concept: Neural Tangent Kernel (NTK) and linearized dynamics
  - Why needed here: MCT draws inspiration from NTK methods to create convex trajectories that approximate the training dynamics
  - Quick check question: How does the NTK linearization allow a neural network's training dynamics to be approximated as a linear model in function space?

- Concept: Convex optimization and trajectory properties
  - Why needed here: Understanding why convex trajectories provide more stable optimization guidance compared to non-convex SGD trajectories
  - Quick check question: What properties of convex functions make them preferable for providing optimization guidance in the context of dataset distillation?

- Concept: Bi-level optimization in dataset distillation
  - Why needed here: Dataset distillation is formulated as optimizing synthetic data to minimize validation loss, requiring understanding of nested optimization problems
  - Quick check question: In dataset distillation, what are the inner and outer optimization problems, and how do they relate to each other?

## Architecture Onboarding

- Component map:
  - Expert trajectory generator (MTT baseline) -> Convexified trajectory constructor (MCT core innovation) -> Continuous sampler (MCT enhancement) -> Synthetic dataset optimizer (common to both MTT and MCT) -> Storage manager (MCT efficiency component)

- Critical path:
  1. Generate expert trajectory using standard MTT approach
  2. Construct convexified trajectory from expert trajectory endpoints
  3. Implement continuous sampling mechanism for trajectory points
  4. Optimize synthetic dataset using convexified trajectory guidance
  5. Store only trajectory endpoints and interpolation weights

- Design tradeoffs:
  - Storage vs. accuracy: MCT sacrifices some trajectory fidelity for significant storage reduction
  - Sampling granularity vs. computational cost: More continuous sampling points improve learning but increase computation
  - Linear approximation vs. trajectory complexity: Simpler convex trajectories may miss some nuances of the true trajectory

- Failure signatures:
  - Convergence instability: Oscillations in validation accuracy during distillation
  - Memory overflow: If trajectory storage grows unexpectedly large
  - Poor synthetic data quality: Validation accuracy plateaus below acceptable threshold

- First 3 experiments:
  1. Verify convex trajectory reconstruction from endpoints matches original trajectory direction
  2. Compare convergence speed and stability between MCT and MTT on CIFAR-10 with ipc=10
  3. Measure storage requirements for expert trajectories in both methods across different dataset sizes

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the text provided. However, based on the methodology and results, several implicit questions arise about the broader applicability and theoretical foundations of the MCT approach.

## Limitations

- Theoretical guarantees for stability improvements are primarily empirical rather than analytical
- Performance claims depend on specific implementation details not fully specified in the paper
- Continuous sampling benefits are demonstrated but not quantified across different sampling densities

## Confidence

- High confidence: Storage efficiency calculations and trajectory reconstruction methodology
- Medium confidence: Convergence speed improvements on tested datasets
- Low confidence: Generalization claims to datasets beyond the three tested, and the theoretical foundations linking NTK linearization to practical stability gains

## Next Checks

1. **Storage verification**: Implement both MTT and MCT storage tracking across all experiments to verify the claimed 8% storage reduction holds consistently across different ipc values and dataset sizes.

2. **Sampling sensitivity analysis**: Systematically vary the continuous sampling density (number of interpolation points) to quantify the marginal benefit of continuous vs. discrete sampling on final validation accuracy.

3. **Stability stress test**: Design experiments with deliberately challenging initialization conditions or aggressive learning rates to test whether MCT's claimed stability advantages persist under more adverse optimization scenarios.