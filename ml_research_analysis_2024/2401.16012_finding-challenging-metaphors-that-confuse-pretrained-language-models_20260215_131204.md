---
ver: rpa2
title: Finding Challenging Metaphors that Confuse Pretrained Language Models
arxiv_id: '2401.16012'
source_url: https://arxiv.org/abs/2401.16012
tags:
- metaphor
- metaphors
- word
- hard
- sense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Current metaphor processing research focuses on identifying conventional
  metaphors from the VUA dataset, but analysis shows most of these pose little challenge
  to modern NLP models. To address this, we propose an automatic pipeline to identify
  "hard metaphors" that confuse language models.
---

# Finding Challenging Metaphors that Confuse Pretrained Language Models

## Quick Facts
- arXiv ID: 2401.16012
- Source URL: https://arxiv.org/abs/2401.16012
- Authors: Yucheng Li; Frank Guerin; Chenghua Lin
- Reference count: 15
- Current metaphor processing research focuses on identifying conventional metaphors from the VUA dataset, but analysis shows most of these pose little challenge to modern NLP models.

## Executive Summary
Current metaphor processing research focuses on identifying conventional metaphors from the VUA dataset, but analysis shows most of these pose little challenge to modern NLP models. To address this, the authors propose an automatic pipeline to identify "hard metaphors" that confuse language models. Their approach uses contrastive learning and clustering to find metaphors whose true meaning is confused with other senses by a model. Experiments show that hard metaphors reduce machine translation accuracy by 16%, QA performance by 4%, NLI by 7%, and metaphor identification recall by over 14% for popular NLP systems.

## Method Summary
The authors propose a pipeline to automatically identify "hard metaphors" that confuse pretrained language models (PLMs). They use contrastive learning on PLM embeddings to obtain Sense Only Representations (SORs) that capture word sense information while reducing contextual influence. For each metaphor instance, they compute an overlap ratio φ measuring what fraction of its k nearest neighbors share the same word sense. Metaphors with low overlap ratios (below 0.8) are identified as hard metaphors. They then construct downstream task test sets using these hard metaphors and evaluate performance degradation across machine translation, QA, NLI, and metaphor identification tasks.

## Key Results
- Hard metaphors reduce machine translation accuracy by 16% compared to literal uses
- QA performance drops by 4% on hard metaphors
- NLI accuracy decreases by 7% when processing hard metaphors
- Metaphor identification recall drops by over 14% on hard metaphors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning of word sense embeddings separates metaphorical from literal uses
- Mechanism: The approach uses contrastive learning to push embeddings of the same word sense together and different senses apart, creating "Sense Only Representations" (SORs). Metaphors whose SORs cluster with non-metaphorical senses are identified as hard.
- Core assumption: Word senses form separable clusters in embedding space when context information is reduced
- Evidence anchors:
  - [abstract] "Our approach uses contrastive learning and clustering to find metaphors whose true meaning is confused with other senses by a model"
  - [section 4.2] "To reduce the influence of position and context information, while preserving the word sense information, we employ a contrastive learning approach that brings the contextual word embeddings with the same word sense as close as possible together and pushes embeddings of different word senses far away"
- Break condition: If word senses don't form distinct clusters in embedding space, the method fails to identify hard metaphors

### Mechanism 2
- Claim: Overlap ratio measures how distinguishable a metaphor is from literal uses
- Mechanism: For each metaphor instance, the overlap ratio φ measures what fraction of its k nearest neighbors share the same word sense. Low φ indicates the metaphor is confused with other senses.
- Core assumption: A metaphor is "hard" if its embedding is close to embeddings of different senses
- Evidence anchors:
  - [section 4.3] "we assume that a NLU system fails to distinguish an example...if its contextual embedding v is placed close to the cluster of other word senses of w in the vector space"
  - [section 6.3] "we find a clear positive correlation between recall and overlap score"
- Break condition: If the embedding space doesn't reflect semantic similarity, overlap ratio becomes meaningless

### Mechanism 3
- Claim: Hard metaphors identified by this method degrade downstream NLP task performance
- Mechanism: The method finds metaphors that confuse the model, and experiments show these metaphors reduce performance on translation, QA, NLI, and metaphor identification tasks.
- Core assumption: Metaphors that are hard for the model to disambiguate will cause errors in downstream tasks
- Evidence anchors:
  - [abstract] "our detected hard metaphors contrast significantly with VUA and reduce the accuracy of machine translation by 16%, QA performance by 4%, NLI by 7%, and metaphor identification recall by over 14%"
  - [section 6.3] "Fig. 5 shows noticeable performance drops among all downstream tasks resulting from the hard metaphors"
- Break condition: If downstream tasks don't rely on correct sense disambiguation, hard metaphors may not degrade performance

## Foundational Learning

- Concept: Word Sense Disambiguation (WSD)
  - Why needed here: The method treats metaphor identification as a WSD problem - determining which sense of a word is being used
  - Quick check question: How would you modify a WSD system to specifically identify metaphorical senses versus literal ones?

- Concept: Contrastive Learning
  - Why needed here: The method uses contrastive learning to create sense-only embeddings by pushing same-sense examples together and different-sense examples apart
  - Quick check question: What loss function would you use to train embeddings where same-sense examples should be close and different-sense examples should be far?

- Concept: Clustering in embedding space
  - Why needed here: The method identifies hard metaphors by finding which metaphorical senses cluster with non-metaphorical senses in the learned embedding space
  - Quick check question: If you plot word sense embeddings in 2D space, what visual pattern would indicate successful sense separation?

## Architecture Onboarding

- Component map: VUA dataset -> WSD data (UFSAC) -> Contrastive learning module -> Clustering module -> Evaluation module -> Hard metaphor dataset + performance degradation measurements

- Critical path:
  1. Load WSD data and VUA metaphors
  2. Run contrastive learning to obtain SORs
  3. Compute overlap ratios for all metaphor instances
  4. Select instances with overlap < threshold as hard metaphors
  5. Create downstream task tests for hard metaphors
  6. Evaluate baseline models on hard metaphor tests

- Design tradeoffs:
  - Using vanilla PLM vs fine-tuned WSD model: Vanilla PLMs have broader coverage but may be noisier
  - Overlap threshold selection: Lower thresholds find harder metaphors but may reduce dataset size
  - k value in overlap calculation: Larger k gives more stable scores but may dilute signal

- Failure signatures:
  - All overlap ratios are high (>0.8): Contrastive learning isn't separating senses
  - No performance degradation on downstream tasks: Hard metaphors aren't actually confusing the model
  - Dataset is too small (<100 examples): Threshold too strict or WSD data coverage too sparse

- First 3 experiments:
  1. Visualize SOR embeddings for a word with both metaphorical and literal senses - should see distinct clusters
  2. Plot overlap ratio distribution across all metaphor instances - should show bimodal distribution
  3. Test a simple baseline (e.g., RoBERTa) on the full VUA dataset vs hard metaphor subset - should show performance drop on hard metaphors

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can hard metaphor datasets be effectively used as benchmarks rather than training sets for metaphor processing methods?
- Basis in paper: [explicit] The paper recommends using hard metaphor datasets as benchmarks due to the long-tail nature of hard metaphors and the diminishing marginal utility of adding more training samples.
- Why unresolved: The paper does not provide a detailed methodology for using hard metaphor datasets as benchmarks, leaving the implementation details unclear.
- What evidence would resolve it: A detailed framework or guidelines for using hard metaphor datasets as benchmarks, including evaluation metrics and best practices, would help clarify how to effectively implement this recommendation.

### Open Question 2
- Question: What are the specific characteristics of context that make metaphors difficult for language models to understand?
- Basis in paper: [inferred] The paper mentions that the context in which metaphors are used is critical to their difficulty for language models, but it does not provide a detailed analysis of the specific contextual features that contribute to this difficulty.
- Why unresolved: The paper identifies the importance of context but does not delve into the specific linguistic or semantic features of context that make metaphors challenging for language models.
- What evidence would resolve it: A comprehensive analysis of the contextual features that contribute to the difficulty of metaphors for language models, including examples and statistical evidence, would help identify the specific characteristics that make metaphors hard to understand.

### Open Question 3
- Question: How can symbolic approaches be effectively tested on hard metaphor datasets?
- Basis in paper: [explicit] The paper recommends testing symbolic approaches on hard metaphor datasets, as they are promising methods especially for long-tail scenarios.
- Why unresolved: The paper does not provide a detailed methodology for testing symbolic approaches on hard metaphor datasets, leaving the implementation details unclear.
- What evidence would resolve it: A detailed framework or guidelines for testing symbolic approaches on hard metaphor datasets, including evaluation metrics and best practices, would help clarify how to effectively implement this recommendation.

## Limitations

- The study relies heavily on contrastive learning for sense disambiguation, but the quality of Sense Only Representations (SORs) depends on the training data and hyperparameters, which are not fully specified
- The overlap ratio threshold of 0.8 for identifying hard metaphors appears somewhat arbitrary and may not generalize across different PLMs or domains
- Downstream task performance drops are measured on manually constructed test sets, raising questions about replicability and potential annotation artifacts

## Confidence

- High confidence: The core methodology of using contrastive learning and overlap ratios to identify hard metaphors is sound and well-supported by empirical results
- Medium confidence: The claim that identified hard metaphors significantly degrade downstream NLP performance, as this depends on the quality of test set construction and may not generalize to all models
- Medium confidence: The assertion that current metaphor processing research is inadequate, as this is based on analysis of a single dataset (VUA) and may not reflect broader research efforts

## Next Checks

1. Conduct ablation studies varying the overlap ratio threshold and PLM architecture to test robustness of hard metaphor identification across different settings
2. Create an independent test set for downstream tasks using the same hard metaphors but different annotators to verify that performance drops are not artifacts of the original annotation process
3. Test the hard metaphor identification method on metaphor datasets from different domains (news, academic, social media) to assess generalizability beyond the VUA corpus