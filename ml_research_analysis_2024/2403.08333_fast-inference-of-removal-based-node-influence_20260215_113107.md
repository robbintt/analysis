---
ver: rpa2
title: Fast Inference of Removal-Based Node Influence
arxiv_id: '2403.08333'
source_url: https://arxiv.org/abs/2403.08333
tags:
- node
- influence
- graph
- nodes
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a new perspective of evaluating node influence\
  \ based on the change of a trained GNN\u2019s prediction when removing a node. The\
  \ brute-force method to generate the real node influence scores takes 41 hours on\
  \ the largest dataset, so we propose NORA, which only takes one forward propagation\
  \ and one backpropagation to approximate the node influence for all nodes."
---

# Fast Inference of Removal-Based Node Influence

## Quick Facts
- arXiv ID: 2403.08333
- Source URL: https://arxiv.org/abs/2403.08333
- Authors: Weikai Li; Zhiping Xiao; Xiao Luo; Yizhou Sun
- Reference count: 40
- One-line primary result: NORA approximates node influence scores for all nodes in one forward and backward pass, achieving significant speedups over brute-force methods while maintaining high correlation with ground truth

## Executive Summary
This paper addresses the computational challenge of evaluating node influence in graphs based on how node removal affects a trained GNN's predictions. The brute-force approach of removing each node individually and recomputing predictions is computationally prohibitive (41 hours on the largest dataset). The authors propose NORA (NOde-Removal-based fAst GNN inference), which uses gradient information to approximate node influence scores for all nodes simultaneously in just one forward and backward pass. NORA decomposes the influence into three terms and approximates each using first-order derivatives and structural heuristics, achieving high correlation with ground-truth influence scores across six datasets and six GNN models.

## Method Summary
NORA approximates node influence by analyzing how the removal of a node changes a trained GNN's predictions across all nodes. It decomposes the influence into three terms: (1) disappearance of the removed node's embedding as a message, (2) changes in neighbors' aggregation terms, and (3) spread-out influence to multi-hop neighbors. These terms are approximated using first-order derivatives and structural heuristics. By modifying the summation to include all nodes, NORA computes influence scores for all nodes simultaneously in one forward and backward pass, achieving significant computational efficiency over brute-force methods.

## Key Results
- NORA achieves Pearson correlation coefficients above 0.8 with ground-truth influence scores across six datasets
- Computational time reduced from 41 hours (brute-force) to less than 1 minute for the largest dataset
- Outperforms baseline methods (node mask, prediction) on all tested GNN models including GCN, GraphSAGE, GAT, DrGAT, GCNII, and TIMME
- Demonstrates effectiveness across both citation networks (Cora, CiteSeer, PubMed, ogbn-arxiv) and Twitter networks (P50, P_20_50)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Removing a node consistently changes the class distributions of other nodes, allowing the â„“1-norm to be moved outside the summation.
- Mechanism: The influence of node removal is approximated by analyzing the total variation distance between original and updated GNN predictions, which simplifies to the â„“1-norm of the change in summed predictions when class distribution changes are consistent.
- Core assumption: Removing a node causes a consistent directional shift in class probabilities across most nodes rather than random or opposing changes.
- Evidence anchors:
  - [abstract] "We calculate the influence of node removal as the total variation distance between the original predictions and new predictions"
  - [section] "Lemma 1. If removing ð‘£ð‘Ÿ consistently changes the class distributions of other nodes, its influence defined in Equation 2 is equal to..."
- Break condition: If node removal causes heterogeneous or contradictory changes in class distributions across nodes, the approximation becomes invalid.

### Mechanism 2
- Claim: The first-order derivatives can approximate the influence of node removal by decomposing the prediction change into three terms representing different aspects of the removal effect.
- Mechanism: The prediction change is decomposed into: (1) disappearance of the removed node's embedding as a message, (2) change in neighbors' aggregation terms, and (3) spread-out influence to multi-hop neighbors, each approximated using gradient information.
- Core assumption: The influence of node removal can be accurately captured by analyzing how the removed node's representation propagates through the GNN's computation graph.
- Evidence anchors:
  - [section] "We propose an intuitive, effective, and efficient method, NOde-Removal-based fAst GNN inference (NORA), which uses the gradient to approximate the node-removal influence"
  - [section] "Lemma 2. We can approximate ðœ¹ð’‡ð’“ for the GNN model described in Equation 5 using the first-order derivatives as: ..."
- Break condition: If the GNN uses complex aggregation mechanisms like attention that are not well-approximated by simple structural heuristics, the gradient-based approximation may fail.

### Mechanism 3
- Claim: The influence of node removal can be efficiently computed for all nodes simultaneously using shared gradient information and structural heuristics.
- Mechanism: By modifying the summation to include all nodes (not excluding the removed node), NORA computes influence scores for all nodes in one forward and backward pass, using normalized structural patterns to approximate aggregation term changes.
- Core assumption: The gradient contribution of a removed node to the summed predictions is similar across different nodes due to the functional and structural similarity assumption.
- Evidence anchors:
  - [section] "We change ð’‡ð’“ = Ãð‘ ð‘–=1,ð‘–â‰ ð‘Ÿ ð’‰(ð‘³) ð’Š to the sum of all nodesâ€™ predictions including ð‘£ð‘Ÿ"
  - [section] "Lemma 5. If we remove the nonlinear activation function in the GNN layer in Equation 5, the gradient ðð’‰ (ð‘³) ð’“ ðð’‰ (ð‘³âˆ’1) ð’“ can be calculated as..."
- Break condition: If the functional and structural similarity assumption is violated (e.g., in highly heterogeneous graphs), the shared gradient approximation becomes inaccurate.

## Foundational Learning

- Concept: Graph Neural Networks (GNN) message-passing framework
  - Why needed here: NORA relies on understanding how node representations propagate through GNN layers to approximate the influence of node removal
  - Quick check question: How does a typical GNN layer update a node's representation using its neighbors' representations?

- Concept: First-order derivative approximation
  - Why needed here: NORA uses first-order derivatives to estimate how small changes in node representations affect the final predictions, which is the basis for approximating node influence
  - Quick check question: What is the relationship between the gradient of a function and its local linear approximation?

- Concept: Total variation distance and â„“1-norm
  - Why needed here: The influence of node removal is measured as the total variation distance between original and updated predictions, which simplifies to the â„“1-norm when class distribution changes are consistent
  - Quick check question: How does the â„“1-norm capture the total difference between two probability distributions?

## Architecture Onboarding

- Component map: NORA -> Forward pass through GNN -> Gradient computation -> Three-term influence decomposition -> Structural heuristics for aggregation terms -> Influence score approximation
- Critical path: The single forward and backward pass through the GNN, where gradients are computed and used to approximate influence scores for all nodes simultaneously
- Design tradeoffs: NORA trades accuracy for efficiency by using first-order approximations and structural heuristics instead of computing the exact influence through brute-force node removal. The accuracy depends on the validity of assumptions about consistent class distribution changes and functional/structural similarity.
- Failure signatures: If NORA's approximations are poor, you might observe: (1) low correlation between approximated and ground-truth influence scores, (2) inconsistent performance across different GNN models or datasets, (3) poor performance on graphs with heterogeneous structures or attention-based aggregation mechanisms.
- First 3 experiments:
  1. Implement NORA on a simple citation dataset (e.g., Cora) with a basic GNN model (e.g., GCN) and verify that it produces reasonable influence scores by comparing to a small subset of brute-force calculations
  2. Test NORA's sensitivity to the structural heuristics (k1, k2, k'2) by varying these parameters and observing their effect on influence score quality
  3. Validate NORA's scalability by measuring runtime on progressively larger graphs and comparing to brute-force computation time

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does NORA's performance degrade when the assumption of functionally and structurally equal nodes is violated?
- Basis in paper: [inferred] The paper explicitly states that the approximation in Equation 9 and Equation 14 is derived from the rough assumption that every node is functionally and structurally equal, which is not the reality.
- Why unresolved: The paper acknowledges this limitation but does not provide empirical evidence of how NORA's performance changes on graphs with highly heterogeneous node degrees or structural roles.
- What evidence would resolve it: Empirical results comparing NORA's performance on graphs with varying levels of node heterogeneity (e.g., scale-free networks vs. random graphs) would demonstrate the robustness of the approximation.

### Open Question 2
- Question: How does NORA's approximation accuracy change with different GNN architectures beyond those tested?
- Basis in paper: [explicit] The paper states that NORA is model-agnostic and can be adapted to any common GNN model based on the message-passing framework.
- Why unresolved: The experiments only test NORA on six GNN models, leaving open the question of its performance on more complex or novel GNN architectures.
- What evidence would resolve it: Testing NORA on a broader range of GNN architectures, including those with more complex aggregation functions (e.g., attention mechanisms) or different layer designs, would provide insight into its generalizability.

### Open Question 3
- Question: How does the choice of â„“p-norm (p=1 vs. p=2) in the approximation formula affect NORA's performance?
- Basis in paper: [explicit] The paper mentions that p is a hyper-parameter in the approximation formula and sets it to one in most cases, but does not explore the impact of different p values.
- Why unresolved: The paper does not provide an analysis of how the choice of p affects the approximation's accuracy or the final influence scores.
- What evidence would resolve it: Empirical results comparing NORA's performance using different p values in the approximation formula would reveal the sensitivity of the method to this parameter.

## Limitations

- The performance of NORA relies on the validity of assumptions about consistent class distribution changes and functional/structural similarity, which may not hold for all graph structures and GNN architectures
- The paper does not extensively explore how NORA's approximation accuracy varies with different choices of the â„“p-norm parameter in the influence calculation
- The evaluation focuses on Pearson correlation coefficients without examining how approximation errors affect downstream tasks that rely on influence scores

## Confidence

- **High confidence**: The mathematical framework for influence decomposition and the overall efficiency gains of NORA (41 hours vs. single forward/backward pass)
- **Medium confidence**: The validity of the â„“1-norm simplification under consistent class distribution changes (Assumption 1)
- **Low confidence**: The accuracy of first-order derivative approximations for complex GNN architectures and attention mechanisms (Assumption 2)

## Next Checks

1. **Assumption validation**: Systematically test Assumption 1 by measuring the frequency of consistent vs. inconsistent class distribution changes across different datasets and graph structures
2. **Architecture robustness**: Evaluate NORA's performance on GNN variants with attention mechanisms and compare approximation accuracy against models with simpler aggregation functions
3. **Downstream impact**: Assess how approximation errors in influence scores affect downstream tasks that rely on these scores, such as adversarial defense or node selection strategies