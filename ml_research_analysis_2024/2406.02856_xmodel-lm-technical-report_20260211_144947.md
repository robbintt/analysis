---
ver: rpa2
title: Xmodel-LM Technical Report
arxiv_id: '2406.02856'
source_url: https://arxiv.org/abs/2406.02856
tags:
- language
- zhang
- wang
- training
- xmodel-lm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Xmodel-LM, a compact 1.1B parameter language
  model pretrained on 2 trillion tokens from a balanced Chinese-English corpus. It
  employs a custom tokenizer with 32K vocabulary, RMSNorm, SwiGLU activation, and
  grouped-query attention.
---

# Xmodel-LM Technical Report

## Quick Facts
- arXiv ID: 2406.02856
- Source URL: https://arxiv.org/abs/2406.02856
- Authors: Yichuan Wang; Yang Liu; Yu Yan; Qun Wang; Xucheng Huang; Ling Jiang
- Reference count: 9
- Primary result: 1.1B parameter model trained on 2T tokens, scores 51.41 average on commonsense tasks and 40.95 on Chinese tasks

## Executive Summary
Xmodel-LM is a compact 1.1B parameter language model pretrained on 2 trillion tokens from a balanced Chinese-English corpus. The model employs a custom tokenizer with 32K vocabulary, RMSNorm, SwiGLU activation, and grouped-query attention. Trained using AdamW with a global batch size of 840, Xmodel-LM achieves competitive performance on benchmarks, outperforming similar-sized open-source models in commonsense reasoning, problem-solving, and Chinese language tasks.

## Method Summary
Xmodel-LM was pretrained using standard transformer architecture with several key design choices. The model uses RMSNorm for normalization, SwiGLU activation functions, and grouped-query attention mechanisms. Training was conducted with AdamW optimizer and a global batch size of 840 over the 2 trillion token corpus. The custom tokenizer with 32K vocabulary was designed to handle both Chinese and English text efficiently.

## Key Results
- Achieves 51.41 average accuracy on commonsense reasoning benchmarks
- Scores 40.95 on Chinese language task evaluations
- Outperforms similar-sized open-source models in problem-solving capabilities

## Why This Works (Mechanism)
The effectiveness of Xmodel-LM stems from its balanced Chinese-English pretraining corpus and efficient architectural choices. The 2 trillion token scale provides broad language coverage, while the SwiGLU activation and grouped-query attention reduce computational overhead without sacrificing representational power. RMSNorm provides stable training dynamics compared to LayerNorm, and the 32K tokenizer effectively handles the character-level nature of Chinese while maintaining subword tokenization for English.

## Foundational Learning
- **Tokenization**: Converting text to discrete units for model processing - needed for efficient vocabulary representation, quick check: vocabulary coverage on held-out data
- **Attention mechanisms**: Enabling context-aware processing across sequences - needed for capturing long-range dependencies, quick check: attention pattern visualization
- **Normalization techniques**: Stabilizing training through input standardization - needed for faster convergence and better generalization, quick check: training loss curves comparison
- **Activation functions**: Introducing non-linearity for complex pattern learning - needed for expressive power, quick check: gradient flow analysis
- **Pretraining objectives**: Learning general language representations through self-supervised tasks - needed for transfer learning capability, quick check: downstream task performance

## Architecture Onboarding

**Component Map**: Input -> Tokenizer -> Transformer Blocks (RMSNorm, SwiGLU, GQA) -> Output Head

**Critical Path**: Tokenization → Embedding Lookup → Multi-head Attention → Feed-forward (SwiGLU) → RMSNorm → Output Projection

**Design Tradeoffs**: The 1.1B parameter size balances efficiency with capability, sacrificing some fine-grained reasoning ability compared to larger models while maintaining competitive performance. The SwiGLU activation reduces parameter count versus GeLU while maintaining similar expressiveness. Grouped-query attention decreases memory requirements versus standard attention at the cost of some flexibility in query-key interactions.

**Failure Signatures**: Performance degradation on highly specialized domains not well-represented in the pretraining corpus, potential tokenization issues with rare Chinese characters or domain-specific English terminology, and possible attention limitations on very long sequences due to memory constraints.

**First Experiments**:
1. Evaluate perplexity on held-out validation set to verify pretraining convergence
2. Run zero-shot inference on simple Chinese-English translation pairs
3. Test few-shot learning capability on a commonsense reasoning task

## Open Questions the Paper Calls Out
None

## Limitations
- Lack of independent verification of benchmark performance claims
- Limited information on out-of-distribution generalization capabilities
- No detailed comparison of efficiency metrics (latency, memory usage) against competitors

## Confidence
- High Confidence: Architecture claims (RMSNorm, SwiGLU, GQA) and parameter count
- Medium Confidence: Benchmark performance scores without independent verification
- Low Confidence: Generalization claims without robustness testing

## Next Checks
1. Reproduce the model training from scratch using the specified hyperparameters to verify claimed performance metrics
2. Evaluate Xmodel-LM on independent Chinese and English datasets not included in original training corpus
3. Conduct head-to-head efficiency comparisons against similarly sized open-source models under identical hardware conditions