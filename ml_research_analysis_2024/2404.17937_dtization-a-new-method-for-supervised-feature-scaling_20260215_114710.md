---
ver: rpa2
title: 'DTization: A New Method for Supervised Feature Scaling'
arxiv_id: '2404.17937'
source_url: https://arxiv.org/abs/2404.17937
tags:
- scaling
- feature
- dataset
- features
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents DTization, a novel supervised feature scaling
  method that leverages decision tree feature importance and robust scaling to improve
  machine learning model performance. The algorithm first constructs a decision tree
  to measure feature importance, then applies robust scaling with importance-weighted
  factors.
---

# DTization: A New Method for Supervised Feature Scaling

## Quick Facts
- arXiv ID: 2404.17937
- Source URL: https://arxiv.org/abs/2404.17937
- Reference count: 23
- Key outcome: DTization improves model performance through decision tree-based importance weighting combined with robust scaling

## Executive Summary
DTization is a novel supervised feature scaling method that leverages decision tree feature importance to enhance machine learning model performance. The algorithm constructs a decision tree to measure feature importance, then applies robust scaling with importance-weighted factors to transform the dataset. Extensive experiments on ten classification and regression datasets demonstrate significant performance improvements over traditional unsupervised scaling methods, particularly for imbalanced datasets. The method achieves O(nd) time and space complexity while delivering superior results compared to standard scaling techniques.

## Method Summary
DTization combines decision tree-based feature importance with robust scaling to create a supervised feature scaling method. The algorithm first constructs a decision tree on the full dataset to identify feature importance through split depth hierarchy. Features appearing higher in the tree receive higher scaling factors calculated using an exponential function based on the number of features. The method then applies robust scaling (using Q1 and Q3 quartiles) to each feature and multiplies the scaled values by their corresponding importance-derived factors. This approach preserves feature relevance while reducing outlier influence, resulting in improved model performance across various classification and regression tasks.

## Key Results
- Improved Matthews Correlation Coefficient from 0.2872 to 0.8332 on credit card fraud detection dataset
- Reduced mean absolute error by 63% on productivity prediction task
- Achieved O(nd) time and space complexity matching traditional scaling techniques

## Why This Works (Mechanism)

### Mechanism 1
Decision tree-based importance ranking preserves feature relevance during scaling. The algorithm constructs a decision tree and uses the tree's depth-level hierarchy to assign scaling factors, with features higher in the tree receiving higher scaling factors. Core assumption: Decision tree splits correlate with feature importance in a supervised context. Break condition: If the decision tree's split criteria do not correlate with the target variable's influence on prediction, the scaling factors will misalign with actual feature importance.

### Mechanism 2
Importance-weighted robust scaling improves performance for imbalanced datasets. After computing feature importance via decision tree, the algorithm multiplies robust scaler output by the importance-derived scaling factor, effectively amplifying or attenuating feature influence based on predictive relevance. Core assumption: Robust scaling is more resilient to outliers than standard scaling, and importance weighting further enhances its effect. Break condition: If the dataset lacks meaningful feature importance hierarchy or robust scaling already handles scaling adequately, the weighting step may add noise rather than value.

### Mechanism 3
Supervised scaling maintains or improves generalization over unsupervised methods. By incorporating target variable information via decision tree splits, the scaling process adapts feature ranges to match their predictive contribution, unlike min-max or standard scaling which treat all features equally. Core assumption: Supervised information (via tree splits) yields better scaling than purely statistical approaches. Break condition: If feature importance derived from decision trees does not generalize to unseen data, the supervised scaling could overfit.

## Foundational Learning

- Concept: Decision tree feature importance via Gini impurity
  - Why needed here: DTization uses decision tree splits to assign importance weights; understanding how Gini index selects splits is essential.
  - Quick check question: What is the Gini index formula, and how does it measure node impurity in binary splits?

- Concept: Robust scaling and quartile-based normalization
  - Why needed here: The algorithm applies robust scaling (using Q1 and Q3) to reduce outlier influence before importance weighting.
  - Quick check question: How do Q1 and Q3 define the scaling range in robust scaling, and why is it less sensitive to outliers than min-max scaling?

- Concept: Performance metrics for imbalanced data (MCC, F1-score)
  - Why needed here: The method is validated on imbalanced datasets (e.g., credit card fraud), requiring metrics that account for class imbalance.
  - Quick check question: Why is MCC preferred over accuracy when evaluating imbalanced binary classification tasks?

## Architecture Onboarding

- Component map: Input dataset -> Decision Tree Builder -> Scaling Factor Calculator -> Robust Scaler -> Importance Multiplier -> Output transformed dataset
- Critical path:
  1. Build decision tree on full dataset
  2. Assign scaling factors based on feature depth in tree
  3. Apply robust scaling per feature
  4. Multiply scaled values by importance-derived factors
  5. Return transformed dataset
- Design tradeoffs:
  - Pros: Supervised feature scaling can improve model performance, especially on imbalanced data
  - Cons: Adds computational overhead (O(nd)) and potential overfitting if decision tree is too deep or not pruned
  - Memory: O(nd) space due to storing transformed dataset and decision tree
- Failure signatures:
  - Performance degrades when using datasets with low feature-target correlation or when features are equally important
  - Instability in scaling factors if decision tree is not robust to noise
  - Overhead may not justify gains on small or balanced datasets
- First 3 experiments:
  1. Compare DTization vs. robust scaling on a balanced dataset (e.g., wine classification) and measure accuracy/F1-score
  2. Test DTization on an imbalanced dataset (e.g., credit card fraud) and compare MCC scores against min-max, standard, and robust scaling
  3. Validate regression performance on a real-world dataset (e.g., house price prediction) and measure MAE/MSE improvement

## Open Questions the Paper Calls Out

### Open Question 1
How does DTization perform compared to other supervised scaling methods like mutual information-based scaling or information gain-based scaling on datasets with non-linear feature interactions? The paper compares DTization only against unsupervised methods like min-max, standard, log, and robust scalers without exploring other supervised feature scaling techniques. Comparative experiments on diverse datasets showing performance differences between DTization and other supervised scaling methods would resolve this question.

### Open Question 2
What is the impact of different decision tree algorithms (e.g., ID3, C4.5, CART) on the scaling factors and overall performance of DTization? The study uses a specific decision tree algorithm without investigating the sensitivity of DTization to different tree construction methods or splitting criteria. Experiments comparing DTization's performance using different decision tree algorithms on the same datasets would identify if algorithm choice impacts scaling factor calculation and model accuracy.

### Open Question 3
How does DTization scale with very high-dimensional datasets (e.g., >1000 features) and what are its computational limits? The paper states DTization has O(nd) time and space complexity matching traditional scaling techniques, but only evaluates it on datasets with up to 30 features. Empirical testing of DTization on high-dimensional datasets with varying numbers of features to measure actual computational time, memory usage, and performance degradation as dimensionality increases would resolve this question.

## Limitations

- Performance gains may not generalize beyond tested imbalanced datasets
- Limited evidence for how the method performs on high-dimensional datasets (>30 features)
- Uncertainty whether decision tree-derived feature importance consistently correlates with true predictive relevance

## Confidence

- Mechanism 1 (Decision tree importance preservation): Medium - supported by internal logic but lacks comparative validation
- Mechanism 2 (Imbalanced dataset performance): Medium - demonstrated on specific cases but needs broader validation
- Mechanism 3 (Supervised vs unsupervised scaling): Low-Medium - theoretical advantage but limited empirical comparison

## Next Checks

1. Test DTization on high-dimensional datasets (e.g., >50 features) to verify scalability
2. Compare performance against other supervised feature scaling methods (e.g., PCA-based scaling)
3. Evaluate robustness to noisy features and correlated feature sets using synthetic datasets