---
ver: rpa2
title: 'AM^2-EmoJE: Adaptive Missing-Modality Emotion Recognition in Conversation
  via Joint Embedding Learning'
arxiv_id: '2402.10921'
source_url: https://arxiv.org/abs/2402.10921
tags:
- multimodal
- emotion
- learning
- query
- joint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "AM\xB2-EmoJE addresses the challenge of emotion recognition in\
  \ conversations when some modality information (audio, video, text) is missing.\
  \ The method introduces a Query Adaptive Fusion (QAF) mechanism that learns the\
  \ relative importance of mode-specific representations in a query-specific manner,\
  \ allowing the model to prioritize the most reliable modalities."
---

# AM^2-EmoJE: Adaptive Missing-Modality Emotion Recognition in Conversation via Joint Embedding Learning

## Quick Facts
- arXiv ID: 2402.10921
- Source URL: https://arxiv.org/abs/2402.10921
- Reference count: 0
- Primary result: AM²-EmoJE achieves 71.98% weighted-F1 score on MELD, approximately 4% improvement over best baseline

## Executive Summary
AM²-EmoJE addresses the challenge of emotion recognition in conversations when some modality information (audio, video, text) is missing. The method introduces a Query Adaptive Fusion (QAF) mechanism that learns the relative importance of mode-specific representations in a query-specific manner, allowing the model to prioritize the most reliable modalities. It also employs a multimodal joint embedding learning module that aligns cross-attended mode-specific descriptors within a joint-embedding space to compensate for missing modalities during inference.

The model is evaluated on the MELD and IEMOCAP datasets, demonstrating state-of-the-art performance. On MELD, AM²-EmoJE achieves a weighted-F1 score of 71.98%, which is approximately 4% improvement compared to the best baseline. The joint embedding module further enhances performance, providing around 2-5% improvement in weighted-F1 scores in various missing-modality scenarios. The model also shows competitive performance even when using only body language or facial expressions, preserving privacy.

## Method Summary
AM²-EmoJE introduces a Query Adaptive Fusion (QAF) mechanism that learns the relative importance of mode-specific representations in a query-specific manner. This allows the model to prioritize the most reliable modalities when some are missing. Additionally, the model employs a multimodal joint embedding learning module that aligns cross-attended mode-specific descriptors within a joint-embedding space. This alignment helps compensate for missing modalities during inference by leveraging shared representations across modalities. The model is evaluated on the MELD and IEMOCAP datasets, demonstrating state-of-the-art performance in emotion recognition under missing modality conditions.

## Key Results
- AM²-EmoJE achieves a weighted-F1 score of 71.98% on the MELD dataset, approximately 4% improvement over the best baseline
- The joint embedding module provides around 2-5% improvement in weighted-F1 scores in various missing-modality scenarios
- The model shows competitive performance using only body language or facial expressions, preserving privacy

## Why This Works (Mechanism)
The proposed AM²-EmoJE model demonstrates strong performance in emotion recognition under missing modality conditions by introducing two key mechanisms. First, the Query Adaptive Fusion (QAF) mechanism learns the relative importance of mode-specific representations in a query-specific manner, allowing the model to prioritize the most reliable modalities when some are missing. Second, the multimodal joint embedding learning module aligns cross-attended mode-specific descriptors within a joint-embedding space, which helps compensate for missing modalities during inference by leveraging shared representations across modalities. This combination of adaptive fusion and joint embedding learning enables the model to maintain high performance even when some modality information is unavailable.

## Foundational Learning

1. **Query Adaptive Fusion (QAF)**
   - Why needed: To dynamically adjust the importance of different modalities based on the current query context, especially when some modalities are missing
   - Quick check: Evaluate the attention weights assigned to each modality in different missing-modality scenarios

2. **Multimodal Joint Embedding Learning**
   - Why needed: To create a shared representation space that can compensate for missing modalities by leveraging information from available modalities
   - Quick check: Assess the alignment of cross-attended mode-specific descriptors in the joint-embedding space using similarity metrics

3. **Cross-Attention Mechanisms**
   - Why needed: To capture interactions between different modalities and their representations
   - Quick check: Analyze the cross-attention weights to understand how information flows between modalities

## Architecture Onboarding

**Component Map:**
Text Encoder -> Audio Encoder -> Visual Encoder -> Cross-Attention -> Query Adaptive Fusion (QAF) -> Joint Embedding Module -> Emotion Classifier

**Critical Path:**
Text/Audio/Visual encoders → Cross-attention → QAF → Joint Embedding → Emotion Classifier

**Design Tradeoffs:**
- Computational overhead vs. performance improvement in missing modality scenarios
- Model complexity vs. ability to generalize across different conversation datasets
- Privacy preservation vs. emotion recognition accuracy when using limited modalities

**Failure Signatures:**
- Degraded performance when multiple modalities are missing simultaneously
- Potential instability during training due to cross-attention mechanisms
- Over-reliance on a single modality in certain conversation contexts

**First 3 Experiments:**
1. Evaluate model performance on MELD dataset with complete modality information
2. Test model robustness in various missing-modality scenarios (single modality missing, multiple modalities missing)
3. Assess privacy-preserving performance using only body language or facial expressions

## Open Questions the Paper Calls Out
None

## Limitations
- The model's reliance on cross-attention mechanisms may introduce computational overhead and potential instability during training, particularly when dealing with large-scale conversation datasets
- The absolute performance values (71.98% on MELD) suggest there remains substantial room for improvement in emotion recognition accuracy
- The trade-off between privacy and accuracy when using limited modalities is not thoroughly quantified

## Confidence
- Model performance claims: Medium (based on two well-established datasets, but evaluation scenarios may not represent all real-world missing modality combinations)
- Improvement over baselines: Medium (significant percentage improvements, but absolute performance still has room for improvement)
- Privacy preservation claims: Low (limited quantification of the privacy-accuracy trade-off)

## Next Checks
1. Evaluate the model's performance across a wider range of missing modality scenarios, including more extreme cases where multiple modalities are missing simultaneously
2. Conduct a thorough ablation study to isolate the contribution of each component (QAF, joint embedding) to the overall performance, and assess their robustness to different conversation lengths and speaker dynamics
3. Test the model's generalization capabilities on additional conversation datasets with different characteristics, such as varying conversation lengths, speaker counts, and cultural contexts