---
ver: rpa2
title: Factual Dialogue Summarization via Learning from Large Language Models
arxiv_id: '2406.14709'
source_url: https://arxiv.org/abs/2406.14709
tags:
- summaries
- factual
- summarization
- consistency
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses factual inconsistency in dialogue summarization
  by proposing a symbolic knowledge distillation framework that leverages large language
  models (LLMs) to generate both factually consistent and inconsistent summaries.
  The method employs two contrastive learning objectives (MARGIN CONTRAST and PAIR
  CONTRAST) to train smaller models like BART, PEGASUS, and Flan-T5, significantly
  improving factual consistency without sacrificing coherence, fluency, or relevance.
---

# Factual Dialogue Summarization via Learning from Large Language Models

## Quick Facts
- arXiv ID: 2406.14709
- Source URL: https://arxiv.org/abs/2406.14709
- Reference count: 32
- This paper proposes symbolic knowledge distillation from LLMs to improve factual consistency in dialogue summarization without requiring human-written references.

## Executive Summary
This paper addresses the challenge of factual inconsistency in dialogue summarization by introducing a symbolic knowledge distillation framework that leverages large language models (LLMs) to generate both factually consistent and inconsistent summaries. The method employs contrastive learning objectives to train smaller models like BART, PEGASUS, and Flan-T5, significantly improving factual consistency while maintaining coherence and relevance. The approach demonstrates that high-quality summarization models can be built without human-written references, relying solely on unlabeled dialogues and LLM-generated summaries.

## Method Summary
The method uses symbolic knowledge distillation where ChatGPT (gpt-3.5-turbo) generates positive summaries (factually consistent) and negative summaries (factually inconsistent) for each dialogue. These summaries serve as training targets for smaller models through sequence-level knowledge distillation (SEQDISTILL) and two contrastive learning objectives (MARGIN CONTRAST and PAIR CONTRAST). The framework trains BART, PEGASUS, and Flan-T5 on SAMSum and DialogSum datasets, evaluating factual consistency using AlignScore, G-Eval, and UniEval metrics while maintaining ROUGE scores for coherence and relevance.

## Key Results
- Flan-T5 with PAIR CONTRAST achieves the best performance, often matching or exceeding human-written references in factual consistency.
- Symbolic knowledge distillation significantly improves factual consistency without sacrificing coherence, fluency, or relevance.
- The approach demonstrates that high-quality summarization models can be built without human-written references, relying solely on unlabeled dialogues and LLM-generated summaries.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Symbolic knowledge distillation from LLMs can transfer factually consistent summarization behavior to smaller models without access to teacher probabilities.
- Mechanism: The teacher LLM generates both positive (factually consistent) and negative (factually inconsistent) summaries, which serve as symbolic training signals. These are used with contrastive learning objectives to shape the student model's output distribution toward factual consistency.
- Core assumption: The LLM generates summaries of higher factual consistency than smaller models, and the symbolic summaries preserve this quality when used as training targets.
- Evidence anchors:
  - [abstract] "We employ zero-shot learning to extract symbolic knowledge from LLMs, generating both factually consistent (positive) and inconsistent (negative) summaries."
  - [section 3.1] "We use ChatGPT (gpt-3.5-turbo) to generate positive summaries which are supposed to be factually consistent with the source dialogue D, and negative summaries that contain factual errors against D."
- Break condition: If the LLM fails to generate summaries that are actually more factually consistent than the student model's outputs, the distillation signal becomes unreliable and may degrade performance.

### Mechanism 2
- Claim: Contrastive learning with both positive and negative summaries explicitly improves factual consistency by shaping the model's output distribution at sequence and latent representation levels.
- Mechanism: MARGIN CONTRAST enforces a margin between the log-likelihood scores of positive and negative summaries, while PAIR CONTRAST minimizes cosine similarity between their latent representations. This dual-level supervision strengthens the model's ability to distinguish factual from non-factual content.
- Core assumption: The scoring function (sequence log-likelihood) and latent representations effectively capture semantic differences between consistent and inconsistent summaries.
- Evidence anchors:
  - [section 3.2.2] "We further incorporate two types of contrastive learning methods to boost the factual consistency of summarization models by incorporating negative summaries on top of SEQDISTILL."
- Break condition: If the negative summaries are too easy or too hard to distinguish, or if the contrastive objectives overwhelm the primary sequence-level distillation, the method may fail to improve or even degrade factual consistency.

### Mechanism 3
- Claim: Sequence-level knowledge distillation from multiple positive summaries generated by the LLM reduces overfitting to a single human reference and improves summary diversity without harming factual consistency.
- Mechanism: Instead of using only the human reference summary, the model is trained on a set of positive summaries from the LLM, approximating a richer target distribution that favors multiple factually consistent outputs.
- Core assumption: The LLM-generated positive summaries are both diverse and factually consistent, providing a better training distribution than a single human reference.
- Evidence anchors:
  - [section 3.2.1] "Given that a large teacher model may generate more factually consistent summaries than the smaller student models, we employ Sequence-level Knowledge Distillation (SEQDISTILL)."
- Break condition: If the LLM's positive summaries are not diverse enough or if they introduce systematic factual errors, the distillation may reinforce incorrect patterns.

## Foundational Learning

- Concept: Maximum Likelihood Estimation (MLE) for sequence-to-sequence models
  - Why needed here: MLE is the baseline training method for summarization models, and understanding its limitations motivates the use of knowledge distillation and contrastive learning.
  - Quick check question: In MLE, what is the loss function minimizing, and at what level (word, sequence, semantic) is factual consistency enforced?

- Concept: Contrastive learning objectives (MARGIN, PAIR)
  - Why needed here: These objectives are central to the proposed method, and understanding how they shape model outputs via margins or latent similarities is key to interpreting results.
  - Quick check question: How does MARGIN CONTRAST differ from PAIR CONTRAST in terms of what they compare and how they compute the loss?

- Concept: Symbolic knowledge distillation
  - Why needed here: This is the core framework enabling transfer from black-box LLMs to smaller models, and distinguishing it from standard probability-based distillation is crucial for implementation.
  - Quick check question: What is the key difference between symbolic knowledge distillation and standard knowledge distillation in terms of the training signal used?

## Architecture Onboarding

- Component map:
  - Teacher model: gpt-3.5-turbo (LLM)
  - Symbolic knowledge extractor: prompt-based generation of positive/negative summaries
  - Student models: BART, PEGASUS, Flan-T5 (pretrained)
  - Training strategies: SEQDISTILL, MARGIN CONTRAST, PAIR CONTRAST
  - Evaluation metrics: AlignScore, G-Eval, UniEval, ROUGE

- Critical path:
  1. Generate positive and negative summaries from teacher for each dialogue.
  2. Combine positive summaries with human reference (if available) for SEQDISTILL.
  3. Apply contrastive objectives using both positive and negative sets.
  4. Fine-tune student model with combined loss (MLE + contrastive).
  5. Evaluate on development set and select best checkpoint.

- Design tradeoffs:
  - Using LLM-generated summaries avoids need for human labels but introduces potential bias.
  - Contrastive learning improves factual consistency but may reduce ROUGE overlap with human references.
  - Larger student models (Flan-T5) generally perform better but at higher computational cost.

- Failure signatures:
  - Low AlignScore/G-Eval despite high ROUGE: likely overfitting to human references, missing factual consistency.
  - High variance in contrastive loss: negative summaries may be too easy or too hard to distinguish.
  - Slow convergence: learning rate or batch size may need tuning.

- First 3 experiments:
  1. Train Flan-T5 with SEQDISTILL only (no contrastive) to establish baseline improvement over MLE.
  2. Add MARGIN CONTRAST to SEQDISTILL and compare factual consistency gains.
  3. Replace MARGIN with PAIR CONTRAST and evaluate which contrastive method works best on development set.

## Open Questions the Paper Calls Out
- How does the effectiveness of symbolic knowledge distillation vary across different types of dialogues beyond short daily conversations (e.g., academic meetings, television interviews)?
- What is the impact of using different large language models as teachers (e.g., GPT-4, Claude) on the quality and factual consistency of distilled summaries?
- How does the quality of LLM-generated negative summaries compare to rule-based negative sample construction methods in terms of improving factual consistency?

## Limitations
- The effectiveness relies heavily on the quality of LLM-generated summaries, which may introduce bias or systematic errors.
- The method has not been tested on diverse dialogue types beyond short daily conversations, limiting generalizability.
- Exact hyperparameters for contrastive learning are not fully specified, potentially affecting reproducibility.

## Confidence
- High Confidence: Sequence-level knowledge distillation from LLM-generated positive summaries improves factual consistency compared to standard MLE training.
- Medium Confidence: Contrastive learning objectives further improve factual consistency beyond SEQDISTILL alone.
- Low Confidence: High-quality summarization models can be built without human-written references, relying solely on LLM-generated summaries.

## Next Checks
1. Remove human-written reference summaries from the training pipeline and retrain models using only LLM-generated positive summaries. Compare factual consistency and ROUGE scores.
2. Apply the symbolic knowledge distillation framework to a different dialogue summarization dataset (e.g., MultiWOZ or MediaSum) and evaluate whether the same improvements in factual consistency are observed.
3. Evaluate the impact of using different LLM teachers (e.g., GPT-4 vs. GPT-3.5-turbo) or varying the number of positive/negative summaries per dialogue on the final factual consistency scores.