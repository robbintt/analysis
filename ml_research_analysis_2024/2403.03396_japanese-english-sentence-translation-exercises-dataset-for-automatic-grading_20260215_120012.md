---
ver: rpa2
title: Japanese-English Sentence Translation Exercises Dataset for Automatic Grading
arxiv_id: '2403.03396'
source_url: https://arxiv.org/abs/2403.03396
tags:
- responses
- analytic
- grading
- task
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses automatic grading of Sentence Translation
  Exercises (STEs), a task designed to evaluate specific learning objectives in L2
  language learning. The authors formalize this as a classification task where student
  responses are scored against pre-defined analytic criteria specified by educators.
---

# Japanese-English Sentence Translation Exercises Dataset for Automatic Grading

## Quick Facts
- arXiv ID: 2403.03396
- Source URL: https://arxiv.org/abs/2403.03396
- Reference count: 11
- BERT achieves 90% F1 for correct responses but below 80% for incorrect ones in automatic grading of Japanese-to-English sentence translation exercises

## Executive Summary
This paper introduces a dataset and methodology for automatic grading of Sentence Translation Exercises (STEs), a task where Japanese sentences are translated into English and graded against analytic criteria specified by educators. The authors formalize STE grading as a classification task and create a dataset with 21 questions and 3,498 student responses, including detailed rubrics with analytic criteria for grammar, vocabulary, and word order. Experiments with finetuned BERT models achieve approximately 90% F1-score for correct responses but struggle with incorrect ones (below 80% F1), while GPT models with few-shot learning perform worse than BERT. The results indicate that even state-of-the-art LLMs find this specialized educational task challenging, suggesting significant room for improvement in automatic STE grading systems.

## Method Summary
The authors created a Japanese-to-English STE dataset through classroom data collection followed by crowdsourcing with controlled recruitment criteria (TOEFL iBT: 55-70, TOEIC L&R: 550-750, National Center Test: 140+ points). The dataset includes 21 questions with 3,498 student responses and detailed rubrics with analytic criteria for grammar, vocabulary/expression, and word order, along with justification cues for each score. For the BERT baseline, they used finetuning with attention supervision using justification cues as supervisory signals, training with NLL loss for scores and MSE loss for justification cues. GPT models were evaluated with few-shot in-context learning using 2, 5, or 10 examples per label. The evaluation used 5-fold cross-validation with 3:1:1 train/dev/test split and F1-score as the primary metric for three-class classification per analytic criterion.

## Key Results
- BERT finetuning achieves approximately 90% F1-score for correct responses but struggles with incorrect ones (below 80% F1)
- GPT models with few-shot learning perform worse than finetuned BERT across all tested configurations
- The STE grading task remains challenging even for cutting-edge LLMs like GPT-4 when provided with only few-shot examples
- Performance varies significantly across analytic criteria, with some being inherently more difficult to grade than others

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Analytic scoring framework enables fine-grained evaluation of specific language learning objectives
- Mechanism: The STE task is formalized as grading each analytic criterion independently rather than overall correctness, allowing educators to identify specific areas where learners struggle
- Core assumption: Students' responses can be meaningfully decomposed into analytic criteria that map to specific learning objectives
- Evidence anchors:
  - [abstract] "We formalize the task as grading student responses for each rubric criterion pre-specified by the educators"
  - [section] "The purpose of assessing the STE task is to determine how well students' responses achieve the learning objectives defined by the instructors"
- Break condition: If analytic criteria become too numerous or overlapping, making consistent grading difficult, or if responses cannot be meaningfully decomposed

### Mechanism 2
- Claim: BERT finetuning with justification cues improves grading fidelity to educator rubrics
- Mechanism: The BERT model is trained not only to predict scores but also to identify justification cues, creating attention mechanisms that align model focus with rubric expectations
- Core assumption: Justification cues provide meaningful supervision for model attention and grading decisions
- Evidence anchors:
  - [section] "Following Mizumoto et al. (2019), we use these justification cues as supervisory signals to train the model's attention layer"
  - [section] "By utilizing this justification cue to train a model, we expect that the model will grade faithfully according to the rubric"
- Break condition: If justification cues are ambiguous or inconsistent across annotators, reducing their effectiveness as supervision signals

### Mechanism 3
- Claim: Few-shot GPT performance limitations reveal specialized nature of STE grading task
- Mechanism: GPT models struggle with STE grading despite strong general language capabilities because the task requires specialized knowledge of educational rubrics and limited response variations
- Core assumption: The STE grading task has unique characteristics that differ from general language understanding tasks GPTs were pretrained on
- Evidence anchors:
  - [abstract] "GPT models with few-shot learning show poorer results than finetuned BERT"
  - [section] "These results showed that STEs grading remains a challenging task even for a cutting-edge LLM such as GPT-4, when provided with only few-shot examples"
- Break condition: If GPT models receive extensive prompt engineering or fine-tuning, potentially improving their performance

## Foundational Learning

- Concept: Analytic scoring frameworks in educational assessment
  - Why needed here: Understanding how analytic scoring differs from holistic scoring is crucial for grasping the STE task formulation
  - Quick check question: What is the key difference between analytic scoring (grading specific criteria) and holistic scoring (overall assessment)?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The BERT model's success relies on understanding how attention mechanisms can be trained to focus on relevant text segments
  - Quick check question: How does the attention mechanism in BERT help identify justification cues in student responses?

- Concept: Few-shot learning and in-context examples
  - Why needed here: GPT performance analysis reveals limitations of few-shot learning for specialized tasks, requiring understanding of how in-context learning works
  - Quick check question: Why might few-shot learning be insufficient for tasks requiring specialized domain knowledge like educational grading?

## Architecture Onboarding

- Component map: Data collection pipeline (classroom + crowdsourcing) -> Rubric creation and refinement process -> Annotation system for scores and justification cues -> BERT finetuning pipeline with attention supervision -> GPT few-shot evaluation pipeline -> Evaluation framework with cross-validation

- Critical path: Question design → Response collection → Rubric refinement → Annotation → Model training → Evaluation
  - Bottleneck: Manual annotation process for justification cues

- Design tradeoffs:
  - Crowdworkers vs. classroom students: Trade off between data volume and authenticity
  - Binary vs. three-class scoring: Simplified scoring reduces complexity but loses nuance
  - Few-shot vs. finetuning: Few-shot is more practical but less effective for specialized tasks

- Failure signatures:
  - Low F1 for incorrect responses indicates model struggles with diverse incorrect variations
  - High variance across analytic criteria suggests some criteria are inherently more difficult to grade
  - GPT models failing on simple cases indicates mismatch between pretraining and task requirements

- First 3 experiments:
  1. Compare BERT finetuning with and without justification cue supervision to quantify their impact
  2. Test GPT performance with different numbers of in-context examples to find optimal balance
  3. Analyze specific analytic criteria where BERT fails to identify systematic weaknesses in the model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the STE grading task compare when using other large language models (LLMs) beyond BERT and GPT, such as LLaMA or other parameter-efficient fine-tuning methods like LoRA?
- Basis in paper: [inferred] The authors mention that only BERT and GPT models were used in their experiments and suggest that the performance of other LLMs remains unclear. They also mention the potential effectiveness of parameter-efficient fine-tuning methods like LoRA.
- Why unresolved: The paper does not provide experimental results or comparisons with other LLMs or fine-tuning methods.
- What evidence would resolve it: Experiments comparing the performance of the STE grading task using different LLMs and fine-tuning methods, including LLaMA and LoRA, with the same dataset and evaluation metrics.

### Open Question 2
- Question: How would the performance of the STE grading models change when deployed in real educational settings, such as English study in schools, compared to the controlled crowdsourcing environment used in the study?
- Basis in paper: [explicit] The authors acknowledge that the dataset was created using crowdsourcing workers, which may differ from student responses in actual classroom settings. They also mention that the performance of the models when deployed in real education settings remains uncertain.
- Why unresolved: The study does not provide data or experiments on the performance of the models in actual classroom settings.
- What evidence would resolve it: Experiments evaluating the performance of the STE grading models using data collected from real classrooms or schools, with the same evaluation metrics as the study.

### Open Question 3
- Question: How effective are the proposed methods for generating more comprehensive feedback comments on the scoring results, extending beyond the estimation of justification cues, in improving student learning outcomes in language acquisition?
- Basis in paper: [explicit] The authors mention that they are considering generating more comprehensive feedback comments on the scoring results as a future direction.
- Why unresolved: The paper does not provide any experiments or results on the effectiveness of the proposed methods for generating feedback comments.
- What evidence would resolve it: Experiments evaluating the impact of the generated feedback comments on student learning outcomes, such as improvements in language proficiency or engagement, compared to traditional feedback methods or no feedback at all.

## Limitations
- Dataset limited to 21 questions with Japanese-to-English translation, potentially not generalizable to other language pairs or question types
- Reliance on crowdworkers rather than authentic classroom data raises concerns about ecological validity
- Binary scoring system (correct/incorrect) simplifies the grading task but loses nuance from original three-class rubric
- BERT shows significant performance disparity between correct (90% F1) and incorrect responses (below 80% F1)

## Confidence
- High confidence: The analytic scoring framework formalization and its implementation are well-established in educational assessment literature
- Medium confidence: BERT finetuning methodology and performance metrics, though the specific impact of justification cues could benefit from ablation studies
- Low confidence: GPT few-shot learning results and their interpretation, as different prompt engineering or more extensive few-shot examples might yield different outcomes

## Next Checks
1. Conduct ablation studies comparing BERT performance with and without justification cue supervision to quantify their contribution to grading accuracy
2. Test GPT performance with varying prompt engineering approaches, including chain-of-thought reasoning and different in-context example selection strategies
3. Evaluate model generalization by testing on authentic classroom data versus crowdworker data to assess ecological validity of the grading system