---
ver: rpa2
title: 'DaVinci at SemEval-2024 Task 9: Few-shot prompting GPT-3.5 for Unconventional
  Reasoning'
arxiv_id: '2405.11559'
source_url: https://arxiv.org/abs/2405.11559
tags:
- task
- answer
- quotesingle
- puzzle
- option
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles the SemEval-2024 BRAINTEASER task, which focuses\
  \ on lateral thinking puzzles\u2014questions that defy conventional commonsense\
  \ reasoning. The authors use few-shot prompting with GPT-3.5 to solve two types\
  \ of puzzles: Sentence Puzzles (which defy commonsense at the sentence level) and\
  \ Word Puzzles (which rely on structural aspects of words rather than meaning)."
---

# DaVinci at SemEval-2024 Task 9: Few-shot prompting GPT-3.5 for Unconventional Reasoning

## Quick Facts
- arXiv ID: 2405.11559
- Source URL: https://arxiv.org/abs/2405.11559
- Reference count: 4
- Few-shot prompting with GPT-3.5 improves Word Puzzle performance but slightly decreases Sentence Puzzle performance on BRAINTEASER task

## Executive Summary
This paper explores the effectiveness of few-shot prompting with GPT-3.5 for solving lateral thinking puzzles from the SemEval-2024 BRAINTEASER task. The authors investigate two puzzle types: Sentence Puzzles requiring unconventional semantic interpretation and Word Puzzles relying on structural word manipulation. They find that few-shot prompting significantly improves performance on Word Puzzles (from 53.5% to 68.6% accuracy) but slightly decreases performance on Sentence Puzzles (from 62.6% to 51.7% accuracy). The results suggest that few-shot prompting works well when task patterns can be easily generalized from examples but struggles when deeper semantic understanding is required.

## Method Summary
The authors use GPT-3.5-turbo to solve BRAINTEASER lateral thinking puzzles through few-shot prompting. They design tailored prompts with 2-shot and 5-shot examples for each puzzle type, emphasizing unconventional interpretation and specifying output format. The method involves parsing questions and multiple-choice options, constructing prompts with appropriate examples, sending to GPT-3.5, and extracting answer indices from responses. Performance is evaluated on the BRAINTEASER dataset containing 507 Sentence Puzzles and 396 Word Puzzles, with results showing differential effectiveness between puzzle types and suggesting future work exploring Chain-of-Thought prompting and fine-tuning.

## Key Results
- Few-shot prompting improves Word Puzzle performance from 53.5% to 68.6% overall accuracy
- Few-shot prompting slightly decreases Sentence Puzzle performance from 62.6% to 51.7% overall accuracy
- Increasing to 5-shot prompts further improves performance, especially for Word Puzzles (up to 84.4% overall accuracy)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot prompting improves performance on Word Puzzles because the structural aspect of words can be easily generalized from examples
- Mechanism: The model learns to shift from semantic interpretation to structural analysis of words by observing examples that demonstrate flipping or rearranging letters
- Core assumption: Word Puzzles rely on a consistent pattern of structural manipulation rather than deep semantic understanding
- Evidence anchors:
  - [abstract]: "Results show that few-shot prompting improves performance on Word Puzzles (from 53.5% to 68.6% overall accuracy)"
  - [section]: "This can be much more easily generalized through just as few as two examples in the prompt."
  - [corpus]: Weak evidence - no direct corpus citations about structural pattern generalization in few-shot settings
- Break condition: If Word Puzzles require context-dependent structural manipulations that vary beyond what examples can convey, few-shot prompting would fail

### Mechanism 2
- Claim: Few-shot prompting worsens performance on Sentence Puzzles because deeper semantic understanding is required that cannot be easily generalized from examples
- Mechanism: Sentence Puzzles demand unconventional semantic interpretation that goes beyond pattern recognition, making simple example-based learning insufficient
- Core assumption: Sentence Puzzles require reasoning that is not reducible to pattern matching from provided examples
- Evidence anchors:
  - [abstract]: "few-shot prompting improves performance on Word Puzzles (from 53.5% to 68.6% overall accuracy) but slightly decreases performance on Sentence Puzzles (from 62.6% to 51.7% overall accuracy)"
  - [section]: "Sentence puzzle involves deeper non-conventional semantic understanding of the question and the choices, which despite conveying reasoning behind the answers in the few-shot examples, cannot be generalized as easily with just 2 examples."
  - [corpus]: Weak evidence - no direct corpus citations about semantic depth requirements in lateral thinking tasks
- Break condition: If Sentence Puzzles could be decomposed into simpler patterns that examples could effectively convey, few-shot prompting would improve performance

### Mechanism 3
- Claim: Increasing shot count improves performance because more examples help the model better generalize task patterns
- Mechanism: Additional examples provide more diverse instances of the task, allowing the model to capture variations and improve pattern recognition
- Core assumption: The relationship between example quantity and generalization capability follows a predictable scaling pattern
- Evidence anchors:
  - [abstract]: "Performance further improves when increasing to 5-shot prompts, especially for Word Puzzles (up to 84.4% overall accuracy)"
  - [section]: "Further, adding the examples in the Sentence puzzle that don't generalize very well for other questions in the testing set might have acted as noise for the model"
  - [corpus]: Weak evidence - no direct corpus citations about scaling shot counts for lateral thinking tasks
- Break condition: If additional examples introduce conflicting patterns or noise that confuses the model, increasing shot count would harm performance

## Foundational Learning

- Concept: Pattern recognition vs. semantic understanding
  - Why needed here: To understand why few-shot prompting works differently for Word vs. Sentence Puzzles
  - Quick check question: Can you identify whether a puzzle requires recognizing a structural pattern or understanding an unconventional semantic interpretation?

- Concept: Generalization from examples
  - Why needed here: To evaluate when few-shot prompting will be effective
  - Quick check question: Given a new puzzle type, can you determine if its solution pattern can be captured from 2-5 examples?

- Concept: Noise in training data
  - Why needed here: To understand why more examples sometimes hurt performance
  - Quick check question: How would you identify if additional examples are helping or hurting model performance on a specific task?

## Architecture Onboarding

- Component map: Input layer (question and options) -> Prompt layer (task-specific instructions with examples) -> Model layer (GPT-3.5-turbo) -> Output layer (selected answer index)

- Critical path:
  1. Parse question and options
  2. Construct prompt with appropriate examples
  3. Send to GPT-3.5
  4. Extract answer index from response

- Design tradeoffs:
  - Shot count vs. performance: More examples improve Word Puzzles but may hurt Sentence Puzzles
  - Example quality vs. quantity: Better examples may outperform more examples
  - Task specificity vs. generalization: Tailored prompts work better but are less reusable

- Failure signatures:
  - Performance worse than zero-shot: Indicates examples are adding noise rather than value
  - Inconsistent answers across runs: May indicate sensitivity to example ordering
  - Model ignores examples: Suggests examples don't match task requirements

- First 3 experiments:
  1. Compare zero-shot vs. two-shot performance on a small validation set
  2. Test different shot counts (2, 3, 5) to find optimal number
  3. Evaluate example quality by testing with well-constructed vs. poorly-constructed examples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GPT-3.5 on BRAINTEASER tasks compare to other large language models when using similar few-shot prompting strategies?
- Basis in paper: [explicit] The paper uses GPT-3.5 and reports results for few-shot prompting, but does not compare with other models
- Why unresolved: The study focuses solely on GPT-3.5 without benchmarking against other LLMs like GPT-4, Claude, or LLaMA
- What evidence would resolve it: Direct comparisons of multiple LLMs using identical few-shot prompting approaches on the BRAINTEASER dataset

### Open Question 2
- Question: What is the optimal number of examples needed in few-shot prompting for maximizing performance on unconventional reasoning tasks like BRAINTEASER?
- Basis in paper: [explicit] The paper tests 2-shot and 5-shot prompts but doesn't explore the full spectrum of possible example counts
- Why unresolved: The study only examines two specific shot counts (2 and 5) without systematic exploration of the relationship between number of examples and performance
- What evidence would resolve it: Systematic testing of few-shot performance across a range of example counts (e.g., 1, 3, 5, 10, 20) to identify the optimal number

### Open Question 3
- Question: How do different prompting strategies (such as Chain-of-Thought prompting) compare to standard few-shot prompting for lateral thinking tasks?
- Basis in paper: [explicit] The authors mention Chain-of-Thought prompting as future work but do not implement or test it
- Why unresolved: The paper only implements standard few-shot prompting without exploring alternative prompting methodologies
- What evidence would resolve it: Comparative experiments using Chain-of-Thought prompting, zero-shot Chain-of-Thought, and other advanced prompting techniques on the same tasks

### Open Question 4
- Question: What specific aspects of the Word Puzzle task make it more amenable to few-shot learning compared to Sentence Puzzles?
- Basis in paper: [explicit] The authors observe that Word Puzzles improve with few-shot prompting while Sentence Puzzles show mixed results, attributing this to differences in task complexity
- Why unresolved: While the authors speculate about the reasons, they do not conduct detailed analysis to identify the specific factors
- What evidence would resolve it: Detailed analysis comparing the semantic complexity, structural patterns, and generalization requirements of Word Puzzles versus Sentence Puzzles

### Open Question 5
- Question: How does the performance of few-shot prompting vary across different types of puzzle reconstructions (original, semantic, context) within the BRAINTEASER dataset?
- Basis in paper: [explicit] The paper reports separate results for original, semantic, and context reconstruction puzzles but does not analyze the patterns in detail
- Why unresolved: The authors provide overall performance metrics but do not conduct deeper analysis of how few-shot prompting performs across these different puzzle types
- What evidence would resolve it: Detailed analysis of performance patterns across puzzle reconstruction types, including correlation with puzzle characteristics and prompting effectiveness

## Limitations

- Limited theoretical grounding for mechanistic explanations of differential performance between puzzle types
- No comparison with alternative prompting strategies like Chain-of-Thought or fine-tuning
- Lack of detailed prompt templates preventing precise replication of the study

## Confidence

This analysis has **High confidence** in the core observation that few-shot prompting shows differential effectiveness between Word and Sentence puzzles in the BRAINTEASER task. However, confidence is **Medium** for the mechanistic explanations proposed and the claim that increasing shot count improves performance due to potential noise effects.

## Next Checks

1. **Ablation study on example selection**: Systematically test which specific examples in the few-shot prompts contribute most to performance gains, distinguishing between examples that demonstrate structural patterns versus semantic reasoning.

2. **Controlled shot-count experiment**: Evaluate performance across a wider range of shot counts (1, 2, 3, 5, 10) with both puzzle types to determine if the relationship between example quantity and performance follows a predictable curve or shows diminishing returns.

3. **Alternative prompting strategy comparison**: Implement and test Chain-of-Thought prompting and self-consistency methods on the same dataset to quantify whether these approaches address the semantic understanding limitations identified for Sentence Puzzles.