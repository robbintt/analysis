---
ver: rpa2
title: 'MobilePortrait: Real-Time One-Shot Neural Head Avatars on Mobile Devices'
arxiv_id: '2407.05712'
source_url: https://arxiv.org/abs/2407.05712
tags:
- motion
- image
- mobileportrait
- synthesis
- keypoints
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of enabling real-time one-shot
  neural head avatars on mobile devices, which existing methods fail to do due to
  high computational complexity. The authors propose MobilePortrait, a lightweight
  framework that reduces learning complexity by integrating external knowledge into
  both motion modeling and image synthesis.
---

# MobilePortrait: Real-Time One-Shot Neural Head Avatars on Mobile Devices

## Quick Facts
- arXiv ID: 2407.05712
- Source URL: https://arxiv.org/abs/2407.05712
- Reference count: 40
- Primary result: Achieves over 100 FPS on mobile devices with state-of-the-art performance using less than one-tenth the computational demand of existing methods

## Executive Summary
This paper introduces MobilePortrait, a lightweight framework for real-time one-shot neural head avatars on mobile devices. The key innovation is integrating external knowledge into both motion modeling and image synthesis to reduce learning complexity. By combining explicit facial landmarks with implicit neural keypoints and precomputing visual features for appearance synthesis, MobilePortrait achieves exceptional performance while maintaining high image quality and motion accuracy.

## Method Summary
MobilePortrait uses lightweight U-Net backbones with simple convolutional layers as its core architecture. The method introduces mixed keypoints combining explicit facial landmarks and implicit neural keypoints to achieve precise motion modeling without liquefaction artifacts. For image synthesis, it precomputes multiview features and background inpainting to provide rich appearance information offline. This approach enables the system to maintain state-of-the-art performance metrics while operating at over 100 FPS on mobile devices, supporting both video and audio-driven inputs.

## Key Results
- Achieves over 100 FPS on mobile devices while matching state-of-the-art performance
- Reduces computational demand to less than one-tenth of existing methods
- Demonstrates significant improvements in image quality (FID, PSNR, SSIM, CSIM) and motion accuracy (AKD, HPD, AED) compared to baseline approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mixed keypoints combining explicit facial landmarks and implicit neural keypoints reduce liquefaction artifacts while maintaining motion precision
- Mechanism: Explicit facial keypoints provide face-specific motion priors while implicit neural keypoints capture global motion, their combination ensures comprehensive motion coverage without over-relying on either representation
- Core assumption: The merger network can effectively integrate facial and neural keypoints without introducing new artifacts
- Evidence anchors:
  - [abstract] "introduce a mixed representation of explicit and implicit keypoints for precise motion modeling"
  - [section 3.2] "We introduce a pretrained face keypoint detector to extract facial landmarks from S and D respectively. A mixed keypoint predictor, the merger shown in Figure 2, then merges the neural keypoints and the face keypoints to create mixed keypoints"
  - [corpus] Weak evidence - no direct corpus mentions of mixed keypoints design
- Break condition: If the merger network fails to properly weight facial vs neural contributions, motion accuracy degrades or artifacts reappear

### Mechanism 2
- Claim: Precomputed appearance knowledge enables lightweight U-Nets to achieve high-quality synthesis without real-time computational overhead
- Mechanism: Multiview features and background inpainting are computed once offline, then reused during inference to provide rich appearance information without requiring complex synthesis networks
- Core assumption: The precomputed features adequately capture the necessary appearance variations for realistic synthesis
- Evidence anchors:
  - [abstract] "precompute visual features for enhanced foreground and background synthesis"
  - [section 3.3] "we utilized pseudo multi-view features and pseudo backgrounds to enhance synthesis of foreground and background respectively"
  - [section 3.3] "these pseudo multi-view image features offer appearance information from different poses to aid in enhancing the quality of synthesis and can be precomputed, thus not hindering inference efficiency"
  - [corpus] Weak evidence - no direct corpus mentions of precomputed appearance knowledge
- Break condition: If precomputed features don't cover sufficient pose variations, synthesis quality degrades especially for novel views

### Mechanism 3
- Claim: Lightweight U-Net backbones with conventional convolutions achieve state-of-the-art performance with less than one-tenth the computational demand
- Mechanism: By offloading complexity to precomputation and smart representation design, the inference-time networks can remain simple while still producing high-quality results
- Core assumption: The architectural simplifications (no attention, no dynamic convolutions) don't significantly impact performance when combined with the other design choices
- Evidence anchors:
  - [abstract] "With these two key designs and using simple U-Nets as backbones, our method matches state-of-the-art performance with less than one-tenth the computational demand"
  - [section 1] "we retained simple U-Nets without the additions from prior work [9,29,38], such as multi-scale feature warping, dynamic convolution, and attention modules, as backbones"
  - [section 4.2] "MobilePortrait remarkably maintains satisfactory performance on key metrics such as FID and AKD, as well as cross-identity motion accuracy like HPD and AED, even when computational resources are limited to just 4 GFLOPs"
  - [corpus] Weak evidence - no direct corpus mentions of lightweight U-Net performance
- Break condition: If complex driving motions or challenging appearances exceed the representational capacity of simple U-Nets

## Foundational Learning

- Concept: Keypoint-based motion representation and TPS warping
  - Why needed here: The entire motion generation pipeline relies on keypoint correspondences to create optical flow fields
  - Quick check question: What's the difference between explicit facial keypoints and implicit neural keypoints, and why would you need both?

- Concept: Neural rendering and feature warping
  - Why needed here: The synthesis network must combine warped source features with appearance knowledge to generate final frames
  - Quick check question: How does feature-level warping differ from pixel-level warping in terms of flexibility and quality?

- Concept: Precomputation strategies for mobile deployment
  - Why needed here: The method's efficiency comes from moving expensive computations offline
  - Quick check question: What types of computations are suitable for precomputation versus real-time inference in mobile applications?

## Architecture Onboarding

- Component map:
  Input Processing -> Keypoint Detection -> Motion Generation -> Warping -> Image Synthesis -> Output
- Critical path: Source image → Keypoint Detection → Motion Generation → Warping → Image Synthesis → Output
  The keypoint detection and motion generation steps must complete within the per-frame time budget
- Design tradeoffs:
  - Complexity vs performance: Simple U-Nets vs complex modules with attention/dynamic convolutions
  - Precomputation vs storage: Computing multiview features offline vs real-time computation
  - Representation accuracy vs efficiency: Mixed keypoints vs pure neural or facial keypoints
- Failure signatures:
  - Liquefaction artifacts → Keypoint representation issue (too much implicit, not enough explicit)
  - Background inconsistencies → Appearance knowledge integration failure
  - Motion lag or jitter → Dense motion network timing or keypoint tracking issues
  - Memory overflow on device → Precomputation storage exceeds device capacity
- First 3 experiments:
  1. Baseline comparison: Implement a simple keypoint-based motion model without mixed representation to quantify the benefit
  2. Ablation study: Test with only neural keypoints, only facial keypoints, and mixed keypoints to validate the design choice
  3. Performance profiling: Measure latency breakdown across each module to identify bottlenecks on target mobile device

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MobilePortrait scale when extended to handle more complex driving signals beyond video and audio, such as multimodal inputs or long-term temporal sequences?
- Basis in paper: [explicit] The paper mentions MobilePortrait supports video and audio-driven inputs but does not explore other input modalities or long-term sequences.
- Why unresolved: The study focuses on video and audio inputs and does not investigate the model's behavior with more complex or multimodal driving signals.
- What evidence would resolve it: Experimental results comparing MobilePortrait's performance across different input modalities (e.g., text, gestures) and varying temporal sequence lengths.

### Open Question 2
- Question: What are the limitations of MobilePortrait in terms of generalization to unseen identities or extreme facial expressions not present in the training data?
- Basis in paper: [explicit] The paper discusses robustness to motion and appearance but does not quantify limitations in generalization to unseen identities or extreme expressions.
- Why unresolved: While robustness is mentioned, specific failure cases or generalization limits are not detailed.
- What evidence would resolve it: Detailed experiments showing performance degradation on out-of-distribution identities or extreme expressions, along with analysis of failure modes.

### Open Question 3
- Question: How does the quality of MobilePortrait's synthesized outputs compare when using different levels of precomputed appearance knowledge, such as varying numbers of multiview features or different background inpainting techniques?
- Basis in paper: [explicit] The paper explores the impact of multiview inputs and background inpainting but does not exhaustively compare different techniques or levels of precomputed knowledge.
- Why unresolved: The study provides some ablation but does not fully explore the trade-offs between different precomputed knowledge strategies.
- What evidence would resolve it: Comparative experiments varying the number and type of precomputed features, along with qualitative and quantitative analysis of synthesis quality.

## Limitations
- The mixed keypoint representation relies heavily on the merger network's ability to properly balance facial and neural keypoint contributions, with implementation details not fully specified
- Precomputed appearance knowledge assumes multiview features capture sufficient pose variations, but the exact sampling strategy remains unclear
- Performance claims of 100+ FPS depend on specific hardware acceleration and implementation optimizations that may not generalize across all mobile platforms

## Confidence
- High confidence: Computational efficiency claims and general architecture design supported by quantitative results (10× speedup, 100+ FPS)
- Medium confidence: Motion quality improvements from mixed keypoints, as specific merger implementation details are sparse
- Medium confidence: Precomputed appearance knowledge mechanism, as sufficiency of precomputed features for diverse pose variations needs verification

## Next Checks
1. Ablation study of keypoint representations: Systematically compare pure facial keypoints, pure neural keypoints, and mixed keypoints on motion accuracy metrics (AKD, HPD, AED) to quantify the actual benefit of the mixed representation design.

2. Precomputed feature coverage analysis: Evaluate how well the precomputed multiview features handle novel poses not seen during precomputation by testing on extreme head rotations and measuring synthesis quality degradation.

3. Cross-device performance validation: Test the claimed 100+ FPS performance across multiple mobile devices with varying computational capabilities to verify the generalization of the efficiency claims beyond the specific test platform.