---
ver: rpa2
title: How to Parameterize Asymmetric Quantization Ranges for Quantization-Aware Training
arxiv_id: '2404.16898'
source_url: https://arxiv.org/abs/2404.16898
tags:
- quantization
- scale
- offset
- arxiv
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper compares three parameterizations for asymmetric quantization-aware
  training: scale/offset, min/max, and beta/gamma. Through controlled experiments
  and LLM quantization, it finds that scale/offset is sensitive to bit width and learning
  rate, often failing to converge, while min/max is more stable.'
---

# How to Parameterize Asymmetric Quantization Ranges for Quantization-Aware Training

## Quick Facts
- **arXiv ID**: 2404.16898
- **Source URL**: https://arxiv.org/abs/2404.16898
- **Reference count**: 27
- **Primary result**: Recommends using min/max parameterization for stable training and beta/gamma without sigmoid for fast convergence in quantization-aware training with learned asymmetric quantization ranges.

## Executive Summary
This paper investigates three parameterizations for asymmetric quantization-aware training: scale/offset, min/max, and beta/gamma. Through controlled experiments and large language model quantization, it finds that scale/offset is sensitive to bit width and learning rate, often failing to converge, while min/max is more stable. Beta/gamma converges quickly for large quantization range adjustments but may be slower with sigmoid scaling. The paper recommends using min/max for stable training and beta/gamma without sigmoid for fast convergence when quantization ranges need to move large distances. These best practices improve the stability and efficiency of quantization-aware training with learned asymmetric quantization ranges.

## Method Summary
The paper compares three parameterizations for asymmetric quantization-aware training: scale/offset, min/max, and beta/gamma. Experiments include controlled toy tests with normal distributions and real-world large language model quantization (GPT2-small, GPT2-XL, OPT-125M, OPT-1.3B) on WikiText-2. The methods use asymmetric uniform quantization with learnable ranges, Adam optimizer with no weight decay, various bit widths (3, 4, 8, 10, 12, 16), and different learning rates (1e-2, 5e-3, 1e-3). Performance is measured using mean-squared-error between original and quantized-dequantized tensors, cross-entropy loss for LLM QAT, and perplexity results.

## Key Results
- Scale/offset parameterization is sensitive to bit width and learning rate, often failing to converge
- Min/max parameterization provides stable training across different bit widths and learning rates
- Beta/gamma parameterization converges quickly for large quantization range adjustments, especially without sigmoid constraints
- For LLM quantization, min/max with 4-bit symmetric weights and 12-bit asymmetric activations achieves competitive perplexity with full precision

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Asymmetric quantization parameterizations differ in gradient structure, causing different convergence behavior.
- Mechanism: In asymmetric quantization, the gradients of scale/offset depend on k (bit width) in a non-linear way that is not captured by the learning rate, while min/max gradients explicitly incorporate k. This leads to scale/offset being more sensitive to bit width changes and less stable when the two quantization range parameters are updated independently.
- Core assumption: The clipping operation in quantization introduces non-smoothness that interacts differently with each parameterization's gradient flow.
- Evidence anchors: [abstract] "Our particular focus is on their changing behavior in response to critical training hyperparameters, bit width and learning rate." [section] "The gradients of θmin and θmax incorporate k as in Table 1, reducing bit-width sensitivity."

### Mechanism 2
- Claim: Scale/offset is prone to oscillation when one of θmin or θmax reaches its optimum while the other does not.
- Mechanism: When one quantization encoding converges, the inverse relationship between scale and offset causes push-pull dynamics that destabilize the other parameter. This is particularly problematic for activations with ReLU where θmin is often already at 0.
- Core assumption: The inverse relationship between scale and offset creates coupling that amplifies oscillations when one parameter is near optimal.
- Evidence anchors: [section] "Once one quantization encoding reaches a local minimum, oscillation starts due to the push-and-pull between the clipping error and the quantization error."

### Mechanism 3
- Claim: Beta/gamma parameterization accelerates convergence when quantization ranges must traverse large distances.
- Mechanism: Beta/gamma scales gradients proportionally to the absolute values of θmin and θmax, effectively weighting updates by the expected distance each parameter needs to travel. This allows faster movement toward optimal ranges compared to min/max.
- Core assumption: Scaling gradients by parameter magnitude provides more efficient traversal of the parameter space when initial ranges are far from optimal.
- Evidence anchors: [section] "It scales the gradients of the quantization ranges proportionally to the expected distances they need to travel (i.e. by | min(xt)| and | max(xt)|)."

## Foundational Learning

- Concept: Asymmetric quantization mechanics
  - Why needed here: Understanding how scale/offset, min/max, and beta/gamma differ in their mathematical formulation is essential to grasp why they behave differently during training.
  - Quick check question: What is the relationship between scale (s), offset (z), θmin, and θmax in asymmetric quantization?

- Concept: Gradient flow in non-smooth operations
  - Why needed here: The clipping and rounding operations in quantization create non-differentiable points that affect gradient propagation differently for each parameterization.
  - Quick check question: How does the clipping operation in asymmetric quantization affect the gradients of scale versus θmin/θmax?

- Concept: Bit-width sensitivity in quantization
  - Why needed here: Different parameterizations respond differently to changes in bit width, which is critical for low-bit quantization scenarios.
  - Quick check question: Why does the incorporation of k in min/max gradients reduce bit-width sensitivity compared to scale/offset?

## Architecture Onboarding

- Component map: input tensor → quantization function Q(x, s, z, k) → rounding → dequantization function DQ(x, s, z) → output tensor
- Critical path: Forward pass: input → Q → rounding → DQ → output. Backward pass: output gradient → DQ gradient → Q gradient → parameterization gradient. The parameterization choice affects the gradient computation at the Q stage.
- Design tradeoffs: Scale/offset offers independent control but suffers from bit-width sensitivity and oscillation issues. Min/max is stable but converges slowly for large range adjustments. Beta/gamma combines fast convergence with stable training but may be constrained by sigmoid application.
- Failure signatures: Scale/offset failure manifests as divergent or oscillatory quantization ranges during training, particularly at high bit widths or with aggressive learning rates. Min/max failure appears as extremely slow convergence when initial ranges are far from optimal. Beta/gamma with sigmoid shows constrained range growth and slower convergence than the sigmoid-free variant.
- First 3 experiments:
  1. Reproduce the controlled toy experiment: quantize a normal distribution with different parameterizations (scale/offset, min/max, beta/gamma) across varying bit widths and learning rates, measuring MSE convergence.
  2. Implement the GPT2-small QAT experiment: quantize weights to 4-bit symmetric and activations to 12-bit asymmetric, comparing cross-entropy loss trajectories across parameterizations.
  3. Test ReLU case: apply quantization to a ReLU-transformed normal distribution to observe scale/offset oscillation behavior versus min/max stability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific modifications to the scale/offset parameterization could make it as stable as min/max across different bit widths and learning rates?
- Basis in paper: [explicit] The paper demonstrates that scale/offset is sensitive to bit width and learning rate, often failing to converge, while min/max is more stable.
- Why unresolved: While the paper identifies the instability issues with scale/offset, it doesn't propose specific solutions to make it more stable.
- What evidence would resolve it: Experimental results showing a modified scale/offset parameterization that achieves similar stability and convergence speed to min/max across various bit widths and learning rates.

### Open Question 2
- Question: How do the beta/gamma parameterization with sigmoid function compare to the sigmoid-free version in terms of convergence speed and final quantization accuracy across different LLM architectures?
- Basis in paper: [explicit] The paper shows that sigmoid-free beta/gamma converges more quickly and finds lower minima than the sigmoid-applied version in their experiments.
- Why unresolved: The comparison is limited to specific LLM sizes. The performance difference might vary for other LLM architectures or different tasks.
- What evidence would resolve it: Comprehensive experiments comparing sigmoid and sigmoid-free beta/gamma across multiple LLM architectures, bit widths, and tasks, measuring both convergence speed and final quantization accuracy.

### Open Question 3
- Question: What is the theoretical explanation for why beta/gamma converges quickly when quantization ranges need to traverse large distances, while min/max converges slowly in these scenarios?
- Basis in paper: [inferred] The paper observes that beta/gamma scales gradients proportionally to the expected distances quantization ranges need to travel, but doesn't provide a rigorous theoretical analysis of this phenomenon.
- Why unresolved: The paper provides empirical evidence but lacks a formal mathematical proof or theoretical framework explaining the convergence behavior difference between beta/gamma and min/max.
- What evidence would resolve it: A theoretical analysis demonstrating how the beta/gamma parameterization inherently provides better gradient scaling for large range adjustments compared to min/max.

## Limitations

- The paper uses synthetic normal distributions for controlled experiments, which may not capture all real-world distribution characteristics
- Limited ablation studies on the exact impact of sigmoid constraints in beta/gamma parameterization
- The interaction between quantization parameterizations and different activation functions beyond ReLU is not fully explored

## Confidence

- **High Confidence**: The core finding that min/max provides stable training across bit widths is well-supported by multiple experiments and theoretical analysis of gradient structure
- **Medium Confidence**: The recommendation to use beta/gamma without sigmoid for fast convergence is supported but could benefit from more extensive exploration of the sigmoid's role
- **Medium Confidence**: The claim about scale/offset's bit-width sensitivity is well-demonstrated but the exact mechanisms for different model architectures could be further explored

## Next Checks

1. **Cross-Activation Function Validation**: Test the three parameterizations with non-ReLU activations (LeakyReLU, GELU, Swish) to verify if scale/offset oscillation persists and whether min/max stability generalizes

2. **Dynamic Range Evolution Analysis**: Track the evolution of quantization ranges during training across different parameterizations to quantify exactly when and why scale/offset diverges compared to min/max

3. **Mixed Precision Scenario Testing**: Evaluate all three parameterizations in mixed-precision settings where weights and activations use different bit widths to assess cross-parameter interaction effects