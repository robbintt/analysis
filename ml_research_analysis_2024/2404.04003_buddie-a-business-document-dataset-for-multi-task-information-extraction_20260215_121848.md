---
ver: rpa2
title: 'BuDDIE: A Business Document Dataset for Multi-task Information Extraction'
arxiv_id: '2404.04003'
source_url: https://arxiv.org/abs/2404.04003
tags:
- document
- dataset
- entity
- documents
- buddie
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BuDDIE introduces a multi-task VRDU dataset of 1,665 business documents
  from US states, annotated for document classification, key entity extraction, and
  visual question answering. It covers 69 entity types across seven categories and
  supports three VRDU tasks, enabling broader evaluation of document understanding
  models.
---

# BuDDIE: A Business Document Dataset for Multi-task Information Extraction

## Quick Facts
- arXiv ID: 2404.04003
- Source URL: https://arxiv.org/abs/2404.04003
- Authors: Ran Zmigrod; Dongsheng Wang; Mathieu Sibue; Yulong Pei; Petr Babkin; Ivan Brugere; Xiaomo Liu; Nacho Navarro; Antony Papadimitriou; William Watson; Zhiqiang Ma; Armineh Nourbakhsh; Sameena Shah
- Reference count: 40
- Primary result: Multi-task VRDU dataset with 1,665 business documents annotated for document classification (F1 99.15), key entity extraction (F1 89.97), and visual question answering (ANLS 89.58)

## Executive Summary
BuDDIE introduces a multi-task dataset for visually rich document understanding (VRDU) focused on US business entity documents. The dataset contains 1,665 documents from 48 states, annotated for three tasks: document classification, key entity extraction, and visual question answering. The documents vary in style and layout across states and types (forms, certificates, reports), enabling robust evaluation of VRDU models. Baseline results show strong performance across all tasks, with DocLLM achieving the highest scores. The dataset is publicly available for non-commercial use and provides a foundation for multi-task document understanding research.

## Method Summary
BuDDIE is a multi-task VRDU dataset of 1,665 US business documents annotated for three tasks: document classification (5 classes), key entity extraction (69 entity types across 7 super categories), and visual question answering. Documents were collected from state government websites, processed with OCR, and annotated using PAWLS for KEE and custom tools for DC and VQA. The dataset supports evaluation of six baseline models (BERT, RoBERTa, LayoutLM variants, GPT4, DocLLM) across all three tasks with task-specific metrics including F1, ANLS, and accuracy.

## Key Results
- Document classification F1 of 99.15 using DocLLM
- Key entity extraction F1 of 89.97 using DocLLM
- Visual question answering ANLS of 89.58 using DocLLM
- Hierarchical entity ontology with 69 entity types across 7 super categories
- Multi-state document variety enabling robust evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-task annotation enables more robust model evaluation than single-task datasets.
- Mechanism: The dataset provides dense annotations for document classification, key entity extraction, and visual question answering, allowing simultaneous evaluation of multiple VRDU capabilities on the same documents.
- Core assumption: Different VRDU tasks share underlying document understanding capabilities that can be jointly evaluated.
- Evidence anchors:
  - [abstract]: "BuDDIE introduces a multi-task VRDU dataset of 1,665 business documents from US states, annotated for document classification, key entity extraction, and visual question answering."
  - [section]: "Our dataset consists of publicly available business entity documents from US state government websites. The documents are structured and vary in their style and layout across states and types (e.g., forms, certificates, reports, etc.)."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.513, average citations=0.0.
- Break condition: If tasks are too dissimilar or require fundamentally different document understanding approaches, joint evaluation becomes less meaningful.

### Mechanism 2
- Claim: Hierarchical entity ontology enables scalable and extensible key entity extraction.
- Mechanism: The 69 entity types organized across seven super categories provide a structured framework that can be extended with additional entity types while maintaining semantic coherence.
- Core assumption: Document entities can be meaningfully organized into hierarchical categories that capture both fine-grained distinctions and broader semantic relationships.
- Evidence anchors:
  - [abstract]: "The documents are structured and vary in their style and layout across states and types (e.g., forms, certificates, reports, etc.)."
  - [section]: "We created a hierarchical ontology of 69 key entity classes over seven super categories that can be augmented with even more entity types in the future."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.513, average citations=0.0.
- Break condition: If document domains require fundamentally different entity categorizations, the hierarchical structure may become limiting.

### Mechanism 3
- Claim: Document variety across US states enables better generalization for real-world applications.
- Mechanism: The dataset includes documents from multiple US states with varying styles and layouts, preventing overfitting to specific document formats.
- Core assumption: Document layouts and formats vary significantly across jurisdictions, and models trained on diverse data will generalize better.
- Evidence anchors:
  - [abstract]: "Our dataset consists of publicly available business entity documents from US state government websites. The documents are structured and vary in their style and layout across states and types."
  - [section]: "The documents of BuDDIE are partially structured, i.e., documents fall into styles such as forms, certificates, etc. Examples of the varied structures and formats in the dataset are given in Figure 1."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.513, average citations=0.0.
- Break condition: If future document processing needs focus on a specific jurisdiction or document type, the variety may introduce unnecessary complexity.

## Foundational Learning

- Concept: Multi-modal document understanding
  - Why needed here: BuDDIE requires processing both textual content and spatial layout information to perform the three VRDU tasks.
  - Quick check question: What information does a document VQA system need beyond just the text content to answer questions about document structure?

- Concept: Named entity recognition with hierarchical categories
  - Why needed here: The KEE task involves identifying entities across 69 fine-grained types organized into 7 super categories.
  - Quick check question: How does hierarchical entity categorization differ from flat entity classification in terms of model architecture and evaluation?

- Concept: Document classification with imbalanced classes
  - Why needed here: The dataset has 5 document classes with varying frequencies, requiring appropriate evaluation metrics like macro F1.
  - Quick check question: Why is macro F1 more appropriate than accuracy for evaluating document classification on imbalanced datasets?

## Architecture Onboarding

- Component map: Document collection -> OCR extraction -> Multi-task annotation (DC → KEE → VQA) -> Model training (BERT, RoBERTa, LayoutLM, LayoutLMv3, GPT4, DocLLM) -> Evaluation framework
- Critical path: 1. Document collection and OCR extraction, 2. Multi-task annotation pipeline (DC → KEE → VQA), 3. Model training and hyperparameter tuning, 4. Comprehensive evaluation across all three tasks
- Design tradeoffs:
  - Single-page focus vs. multi-page documents: Reduces annotation cost but may miss context
  - OCR-only vs. visual features: Simpler but potentially less accurate for layout understanding
  - Hierarchical vs. flat entity ontology: More structured but potentially more complex to annotate
- Failure signatures: Poor OCR quality causing cascading errors in KEE and VQA, class imbalance leading to inflated document classification scores, inconsistent annotation guidelines causing agreement issues
- First 3 experiments:
  1. Baseline text-only model (BERT) on each task to establish lower bounds
  2. Multi-modal model (LayoutLM) to assess layout information impact
  3. Cross-task evaluation: Train on DC, test on KEE to measure task transferability

## Open Questions the Paper Calls Out
None explicitly identified in the paper.

## Limitations
- Dataset scope limited to US business entity documents from 48 states, potentially limiting generalization to international formats
- Single-page focus may miss important cross-page information in longer documents
- Non-commercial use restriction limits applicability for industrial applications

## Confidence

- **High confidence**: Document classification and key entity extraction results (F1 scores of 99.15 and 89.97 respectively) are well-supported by established metrics and show strong performance across multiple model architectures.

- **Medium confidence**: Visual question answering results (ANLS of 89.58) have reasonable support but show more variability in inter-annotator agreement, suggesting some uncertainty in evaluation quality.

- **Medium confidence**: The claim about multi-task evaluation benefits is theoretically sound but requires empirical validation showing that improvements on BuDDIE transfer to real-world applications.

## Next Checks

1. **Cross-jurisdiction validation**: Test whether models trained on BuDDIE generalize to business documents from non-US jurisdictions or different document types to assess the claimed generalization benefits.

2. **Multi-page document evaluation**: Create or identify a subset of multi-page documents to evaluate whether the single-page focus limits the dataset's ability to capture document-level context and relationships.

3. **Commercial use impact analysis**: Assess whether the non-commercial restriction meaningfully limits the dataset's utility by surveying industrial practitioners about their document understanding needs and comparing them to BuDDIE's scope.