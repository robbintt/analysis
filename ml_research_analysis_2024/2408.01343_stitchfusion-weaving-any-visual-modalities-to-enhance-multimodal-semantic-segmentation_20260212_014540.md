---
ver: rpa2
title: 'StitchFusion: Weaving Any Visual Modalities to Enhance Multimodal Semantic
  Segmentation'
arxiv_id: '2408.01343'
source_url: https://arxiv.org/abs/2408.01343
tags:
- stitchfusion
- modality
- fusion
- segmentation
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: StitchFusion is a multimodal semantic segmentation framework that
  enables efficient feature fusion across any visual modalities by sharing and synchronizing
  multimodal information during encoding. It introduces a Multi-directional Modality
  Adapter (MoA) to facilitate cross-modal information transfer with minimal additional
  parameters.
---

# StitchFusion: Weaving Any Visual Modalities to Enhance Multimodal Semantic Segmentation

## Quick Facts
- **arXiv ID:** 2408.01343
- **Source URL:** https://arxiv.org/abs/2408.01343
- **Reference count:** 40
- **Primary result:** Achieves up to 70.34% mIoU on DeLiVER dataset and 55.9% on Mcubes dataset with efficient multimodal fusion

## Executive Summary
StitchFusion is a multimodal semantic segmentation framework that enables efficient feature fusion across any visual modalities by sharing and synchronizing multimodal information during encoding. The approach introduces a Multi-directional Modality Adapter (MoA) that facilitates cross-modal information transfer with minimal additional parameters. By leveraging the encoder's inherent multi-scale feature modeling capability, StitchFusion achieves comprehensive multimodal integration without requiring complex modality-specific fusion modules. Experiments on seven diverse datasets demonstrate state-of-the-art performance, with the method also showing complementary benefits when integrated with existing feature fusion modules.

## Method Summary
StitchFusion implements multimodal semantic segmentation by inserting Modality Adapter (MoA) layers within each encoder block of a pre-trained vision transformer backbone. The MoA uses a simple low-rank adaptation architecture to transform features from one modality into the space of another through downscaling, processing, and upscaling operations. This enables cross-modal information transfer during both attention and feed-forward sublayers. The framework supports any number of visual modalities and can be integrated with existing feature fusion modules. Training uses cross-entropy loss with frozen pre-trained encoders, and inference produces per-pixel semantic segmentation through a simple MLP decoder.

## Key Results
- Achieves 70.34% mIoU on DeLiVER dataset, outperforming existing multimodal segmentation methods
- Demonstrates 55.9% mIoU on Mcubes dataset with efficient parameter usage
- Shows complementary performance when integrated with existing feature fusion modules

## Why This Works (Mechanism)

### Mechanism 1
StitchFusion achieves multimodal feature fusion by sharing and synchronizing multi-scale information during encoding, avoiding the need for complex modality-specific fusion modules. The Modality Adapter (MoA) layer is inserted within each encoder block to facilitate cross-modal information transfer during both attention and feed-forward sublayers. By stitching modality-specific features at multiple scales, the model achieves comprehensive multi-modal integration without requiring additional fusion heads or large parameter counts.

### Mechanism 2
The MoA layer provides efficient cross-modal information transfer with minimal additional parameters by using a simple low-rank adaptation architecture. The MoA implements a sequence of downscaling, processing (with GELU and dropout), and upscaling operations to transform features from one modality into the space of another. This allows the model to combine modality-specific information while preserving computational efficiency.

### Mechanism 3
StitchFusion is compatible with existing feature fusion modules (FFMs) and can complement their performance when integrated. The framework can be used alongside traditional FFMs by applying both during the encoding process. The MoA provides early-stage modality sharing, while FFMs can further refine and align features at later stages.

## Foundational Learning

- **Vision Transformers (ViT) and their architecture**: StitchFusion is built on ViT-based encoders and modifies their block structure to include modality adapters. Quick check: What are the two main sublayers in a ViT block, and where does StitchFusion insert the MoA?

- **Multimodal fusion strategies**: Understanding the limitations of existing fusion paradigms (mapping-based, prompt-based, exchange-based) is crucial to appreciate StitchFusion's novel approach. Quick check: What are the three dominant paradigms for multimodal fusion mentioned in the paper, and what are their key limitations?

- **Semantic segmentation**: StitchFusion is a framework for multimodal semantic segmentation, so understanding the task's requirements and evaluation metrics is essential. Quick check: What is the primary evaluation metric used in the experiments, and what does it measure?

## Architecture Onboarding

- **Component map:** Image → Patch Embedding → Encoder (with MoA) → Feature Aggregation → Decoder → Segmentation Output

- **Critical path:** Image → Patch Embedding → Encoder (with MoA) → Feature Aggregation → Decoder → Segmentation Output

- **Design tradeoffs:**
  - Parameter efficiency vs. fusion effectiveness: StitchFusion uses a simple MoA to minimize parameters while achieving fusion
  - Modality bias vs. flexibility: By sharing information during encoding rather than using modality-specific modules, StitchFusion avoids introducing modality bias
  - Complexity vs. performance: Adding MoA layers increases model complexity but enables better cross-modal integration

- **Failure signatures:**
  - No improvement over single-modality baselines: Indicates MoA is not effectively transferring information
  - Degraded performance on certain modalities: Suggests modality bias or interference
  - Out of Memory errors: May indicate MoA is adding too many parameters for the hardware

- **First 3 experiments:**
  1. Test StitchFusion with two modalities (RGB + depth) on a small dataset to verify basic functionality
  2. Compare StitchFusion with and without MoA layers to isolate their contribution
  3. Experiment with different MoA configurations (shared vs. independent) to understand their impact on performance

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the performance of StitchFusion compare when using different types of modality adapters (sMoA, obMoA, tuMoA) on large-scale datasets with more than four modalities?
- **Basis in paper:** [inferred] The paper introduces three types of modality adapters (sMoA, obMoA, tuMoA) and suggests their equivalence for two modalities, but does not extensively compare their performance on large-scale datasets with more than four modalities.
- **Why unresolved:** The paper primarily focuses on datasets with up to four modalities and does not provide a comprehensive comparison of the three modality adapters on larger datasets with more modalities.
- **What evidence would resolve it:** Conducting experiments on large-scale datasets with more than four modalities and comparing the performance of sMoA, obMoA, and tuMoA would provide insights into their effectiveness and scalability.

### Open Question 2
- **Question:** What is the impact of increasing the hidden dimension (r) of the Modality Adapter on the model's performance and computational efficiency for datasets with a high number of classes?
- **Basis in paper:** [explicit] The paper discusses the exploration of hidden dimension (r) for StitchFusion and suggests that r = 4 and 8 provide a favorable trade-off between accuracy and computational cost, but does not specifically address datasets with a high number of classes.
- **Why unresolved:** The paper does not provide experimental results or analysis on how increasing the hidden dimension affects performance and efficiency for datasets with a high number of classes.
- **What evidence would resolve it:** Conducting experiments on datasets with a high number of classes and varying the hidden dimension (r) would help determine the optimal configuration for such datasets.

### Open Question 3
- **Question:** How does StitchFusion handle modality bias when integrating additional modalities that have significantly different feature distributions compared to the primary modality?
- **Basis in paper:** [inferred] The paper mentions that StitchFusion aims to avoid introducing modality bias and is capable of adapting to arbitrary combinations of modalities, but does not provide specific details on handling modality bias for modalities with significantly different feature distributions.
- **Why unresolved:** The paper does not provide a detailed analysis or experimental results on how StitchFusion addresses modality bias when integrating modalities with significantly different feature distributions.
- **What evidence would resolve it:** Conducting experiments with modalities that have significantly different feature distributions and analyzing the impact on StitchFusion's performance would provide insights into its ability to handle modality bias.

## Limitations

- The scalability of the MoA mechanism to more than two modalities remains unclear, as experiments primarily focus on bimodal inputs
- The exact impact of MoA layer placement within the encoder (early vs. late stages) is not fully explored
- The computational overhead introduced by MoA layers is not explicitly quantified in terms of actual training/inference time

## Confidence

- **High confidence**: The core claim that StitchFusion achieves effective multimodal fusion through encoding-time sharing, supported by strong empirical results across seven diverse datasets with clear mIoU improvements
- **Medium confidence**: The claim that StitchFusion complements existing FFMs, as while the paper demonstrates improved performance when integrated, the mechanism of complementarity is not deeply analyzed
- **Medium confidence**: The assertion that the simple low-rank adaptation architecture is sufficient for cross-modal transfer, as the paper uses a relatively straightforward MLP-based MoA without exploring more complex alternatives

## Next Checks

1. Test StitchFusion's scalability by implementing a three-modality version (RGB + depth + thermal) on a subset of the DeLiVER dataset to verify that the stitching mechanism generalizes beyond bimodal inputs
2. Conduct systematic ablation studies varying MoA placement across different encoder stages (early, middle, late) on the MCubeS dataset to determine optimal configuration patterns
3. Measure actual training and inference wall-clock time with and without MoA layers on the same hardware to quantify the real-world computational overhead beyond parameter count analysis