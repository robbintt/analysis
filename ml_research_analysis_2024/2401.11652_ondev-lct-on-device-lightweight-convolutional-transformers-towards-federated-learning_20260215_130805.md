---
ver: rpa2
title: 'OnDev-LCT: On-Device Lightweight Convolutional Transformers towards federated
  learning'
arxiv_id: '2401.11652'
source_url: https://arxiv.org/abs/2401.11652
tags:
- vision
- data
- learning
- performance
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes OnDev-LCT, a lightweight convolutional transformer
  architecture designed for on-device vision tasks in federated learning scenarios.
  The key innovation is incorporating image-specific inductive biases through an LCT
  tokenizer that leverages efficient depthwise separable convolutions in residual
  linear bottleneck blocks to extract local features, while the multi-head self-attention
  mechanism in the LCT encoder captures global image representations.
---

# OnDev-LCT: On-Device Lightweight Convolutional Transformers towards federated learning

## Quick Facts
- arXiv ID: 2401.11652
- Source URL: https://arxiv.org/abs/2401.11652
- Reference count: 25
- Outperforms ResNet-56 by 4.62% and 14.77% on CIFAR-10 and CIFAR-100 while using fewer parameters and MACs

## Executive Summary
This paper introduces OnDev-LCT, a lightweight convolutional transformer architecture designed for on-device vision tasks in federated learning scenarios. The key innovation is incorporating image-specific inductive biases through an LCT tokenizer that leverages efficient depthwise separable convolutions in residual linear bottleneck blocks to extract local features, while the multi-head self-attention mechanism in the LCT encoder captures global image representations. Extensive experiments on benchmark image datasets show that OnDev-LCT models outperform existing lightweight vision models while having fewer parameters and lower computational demands, making them suitable for FL scenarios with data heterogeneity and communication bottlenecks.

## Method Summary
OnDev-LCT combines a convolutional tokenizer with transformer-based encoder for efficient on-device vision. The LCT tokenizer uses depthwise separable convolutions in residual linear bottleneck blocks to extract local features while preserving spatial relationships. The LCT encoder employs multi-head self-attention to capture global contextual relationships between image patches. The architecture is designed to be lightweight with fewer parameters and lower computational complexity than standard transformers, making it suitable for resource-constrained devices in federated learning scenarios where data heterogeneity and communication bottlenecks are major challenges.

## Key Results
- Achieves up to 4.62% better accuracy than ResNet-56 on CIFAR-10 while using fewer parameters and MACs
- Outperforms existing lightweight vision models on multiple benchmark datasets including CIFAR-100, MNIST, Fashion-MNIST, EMNIST-Balanced, and FEMNIST
- Consistently outperforms other baselines in federated learning experiments under various data heterogeneity settings
- Demonstrates superior performance-to-efficiency ratio compared to standard ViT and other lightweight transformer variants

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LCT tokenizer incorporates inductive biases through depthwise separable convolutions, enabling efficient local feature extraction
- Mechanism: By replacing patch-based tokenization with convolutional tokenization using depthwise separable convolutions, the model preserves spatial relationships and captures local features more effectively while reducing computational cost
- Core assumption: Depthwise separable convolutions can extract meaningful local features without the computational overhead of standard convolutions
- Evidence anchors:
  - [abstract] "Our models incorporate image-specific inductive biases through the LCT tokenizer by leveraging efficient depthwise separable convolutions in residual linear bottleneck blocks to extract local features"
  - [section 4] "We leverage efficient depthwise separable convolutions in our LCT tokenizer for extracting local features from images"
  - [corpus] Weak - no direct corpus evidence found for this specific mechanism

### Mechanism 2
- Claim: Multi-head self-attention in LCT encoder captures global representations effectively
- Mechanism: The MHSA mechanism enables the model to learn global contextual relationships between image patches, complementing the local features extracted by the LCT tokenizer
- Core assumption: Self-attention can effectively capture long-range dependencies when provided with good local feature representations
- Evidence anchors:
  - [abstract] "the multi-head self-attention (MHSA) mechanism in the LCT encoder implicitly facilitates capturing global representations of images"
  - [section 4] "With the multi-head self-attention (MHSA), our model can capture long-range positional information as each head can pay attention to different parts of the input embedding"
  - [corpus] Weak - no direct corpus evidence found for this specific mechanism

### Mechanism 3
- Claim: Residual linear bottleneck blocks improve model efficiency and prevent information loss
- Mechanism: The bottleneck structure reduces parameters and computation while residual connections prevent information loss that would occur with non-linear functions like ReLU
- Core assumption: Residual connections can maintain information flow through the network even with reduced dimensionality
- Evidence anchors:
  - [section 4] "The idea of residual blocks is to make our model as thin as possible by increasing the depth with fewer parameters while improving the ability to effectively train a lightweight vision transformer"
  - [section 4] "The Bottleneck structure reduces the number of parameters and computation costs while preventing information loss of using non-linear functions, such as ReLU"
  - [corpus] Weak - no direct corpus evidence found for this specific mechanism

## Foundational Learning

### Concept: Depthwise separable convolutions
- Why needed here: Understanding how depthwise separable convolutions reduce computational cost while maintaining feature extraction capability
- Quick check question: How do depthwise separable convolutions differ from standard convolutions in terms of computational complexity and parameter count?

### Concept: Multi-head self-attention
- Why needed here: Understanding how self-attention mechanisms capture global relationships between image patches
- Quick check question: What is the role of multiple attention heads in capturing different types of contextual relationships?

### Concept: Residual connections
- Why needed here: Understanding how residual connections help train deeper networks and prevent information loss
- Quick check question: How do residual connections help mitigate the vanishing gradient problem in deep networks?

## Architecture Onboarding
- Component map: Image → LCT tokenizer (M convs + DWS conv + R bottleneck blocks) → LCT encoder (L transformer blocks) → SeqPool → Classifier
- Critical path: Image → LCT tokenizer (M convs + DWS conv + R bottleneck blocks) → LCT encoder (L transformer blocks) → SeqPool → Classifier
- Design tradeoffs: Computational efficiency vs. feature extraction capability; model size vs. performance; local feature preservation vs. global context understanding
- Failure signatures: Poor performance on small datasets (inductive bias not working), high computational cost (DWS convolutions not efficient), poor generalization (self-attention not capturing global relationships)
- First 3 experiments:
  1. Ablation study removing depthwise separable convolutions to measure impact on efficiency and accuracy
  2. Varying number of attention heads to find optimal balance between performance and computation
  3. Testing with different numbers of LCT encoder layers to optimize depth vs. width tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the LCT tokenizer in OnDev-LCT perform compared to other tokenizers like ViT's patch-based tokenization and CCT's convolutional tokenization on datasets with varying spatial complexity?
- Basis in paper: [explicit] The paper compares OnDev-LCT's performance with ViT and CCT on multiple datasets, but does not provide a detailed comparison of the tokenizers' performance on datasets with different spatial characteristics.
- Why unresolved: The paper focuses on the overall performance of OnDev-LCT compared to other models, but does not isolate the impact of the LCT tokenizer on performance.
- What evidence would resolve it: A study comparing the performance of OnDev-LCT's LCT tokenizer with other tokenizers on datasets with varying spatial complexity, such as images with different levels of detail or texture.

### Open Question 2
- Question: What is the impact of the residual linear bottleneck blocks in the LCT tokenizer on the model's ability to adapt to local variations in data distribution in federated learning scenarios?
- Basis in paper: [explicit] The paper mentions that the residual linear bottleneck blocks enhance the model's ability to adapt to local variations in data distribution, but does not provide a detailed analysis of their impact.
- Why unresolved: The paper discusses the overall performance of OnDev-LCT in federated learning scenarios, but does not isolate the contribution of the residual linear bottleneck blocks to this performance.
- What evidence would resolve it: An ablation study comparing the performance of OnDev-LCT with and without the residual linear bottleneck blocks in federated learning scenarios with different levels of data heterogeneity.

### Open Question 3
- Question: How does the performance of OnDev-LCT scale with the number of attention heads in the LCT encoder, and what is the optimal number of attention heads for different image sizes and complexity?
- Basis in paper: [explicit] The paper shows that the performance of OnDev-LCT varies with the number of attention heads, but does not provide a detailed analysis of the optimal number of attention heads for different image sizes and complexity.
- Why unresolved: The paper presents the performance of OnDev-LCT with different numbers of attention heads, but does not explore the relationship between the number of attention heads and image size or complexity.
- What evidence would resolve it: A study analyzing the performance of OnDev-LCT with different numbers of attention heads on images of varying sizes and complexity, to determine the optimal number of attention heads for each case.

## Limitations
- Architectural details are underspecified, making exact reproduction difficult
- FL-specific hyperparameters like client sampling ratio and communication rounds are not provided
- Performance claims based on benchmark datasets may not fully represent real-world federated learning scenarios with severe data heterogeneity

## Confidence
- **High confidence**: The core architectural approach combining depthwise separable convolutions with transformer-based global attention is technically sound
- **Medium confidence**: Empirical results showing improved accuracy with fewer parameters are credible but lack detailed ablation studies
- **Low confidence**: Claims about FL performance improvements are based on limited experimental setups without comparison against state-of-the-art FL-specific vision transformers

## Next Checks
1. **Ablation study implementation**: Systematically remove depthwise separable convolutions from the LCT tokenizer and measure the impact on both centralized training accuracy and computational efficiency metrics
2. **FL hyperparameter sensitivity analysis**: Conduct experiments varying key FL parameters (client sampling ratio, communication rounds, local epochs) to determine robustness of OnDev-LCT's performance advantage
3. **Cross-dataset generalization test**: Evaluate OnDev-LCT models trained on CIFAR-10 on unseen federated datasets with different data distributions to validate claims about handling heterogeneity in real-world scenarios