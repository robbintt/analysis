---
ver: rpa2
title: 'LoRA+: Efficient Low Rank Adaptation of Large Models'
arxiv_id: '2402.12354'
source_url: https://arxiv.org/abs/2402.12354
tags: []
core_contribution: This paper identifies a fundamental inefficiency in the standard
  LoRA fine-tuning method for large language models, arising from using the same learning
  rate for both LoRA adapter matrices A and B. Through scaling analysis of neural
  networks in the infinite-width limit, the authors show this setup prevents efficient
  feature learning when the embedding dimension is large.
---

# LoRA+: Efficient Low Rank Adaptation of Large Models

## Quick Facts
- arXiv ID: 2402.12354
- Source URL: https://arxiv.org/abs/2402.12354
- Authors: Soufiane Hayou; Nikhil Ghosh; Bin Yu
- Reference count: 40
- Key outcome: Introduces LoRA+ with separate learning rates for LoRA adapter matrices A and B, achieving 1-2% accuracy gains and up to 2X speedup over standard LoRA across multiple NLP models and tasks

## Executive Summary
This paper addresses a fundamental inefficiency in standard LoRA (Low-Rank Adaptation) fine-tuning by identifying that using the same learning rate for both adapter matrices A and B prevents efficient feature learning when embedding dimensions are large. Through theoretical analysis in the infinite-width limit and empirical validation across multiple models (GPT-2, RoBERTa, Llama) and tasks (GLUE, MMLU), the authors demonstrate that LoRA+ - which assigns different learning rates with a fixed ratio λ = ηB/ηA - consistently improves performance by 1-2% and training speed by up to 2X. The optimal ratio λ is empirically determined to be around 24, making this a simple yet effective modification to the standard LoRA approach.

## Method Summary
The paper identifies that standard LoRA uses the same learning rate for both adapter matrices A and B, which creates an inefficiency in feature learning when the embedding dimension is large. Through scaling analysis of neural networks in the infinite-width limit, the authors show that this uniform learning rate setup prevents efficient adaptation. LoRA+ addresses this by assigning different learning rates to A and B matrices, maintaining a fixed ratio λ = ηB/ηA. This simple modification allows the model to better balance feature learning and adaptation during training, leading to improved convergence speed and final performance.

## Key Results
- LoRA+ achieves consistent 1-2% accuracy improvements over standard LoRA across multiple language models and tasks
- Training speed increases by up to 2X with LoRA+, demonstrating faster convergence
- The optimal learning rate ratio λ = ηB/ηA is empirically determined to be approximately 24
- Performance gains are particularly pronounced for harder tasks requiring more adaptation

## Why This Works (Mechanism)
The mechanism behind LoRA+ addresses an imbalance in feature learning that occurs when both adapter matrices A and B share the same learning rate. In standard LoRA, this uniform learning rate setting creates a bottleneck where the interaction between A and B matrices cannot efficiently explore the adaptation space, especially when embedding dimensions are large. By decoupling the learning rates while maintaining a fixed ratio λ, LoRA+ allows for more balanced and efficient exploration of the parameter space, enabling better feature adaptation and faster convergence to optimal solutions.

## Foundational Learning
**Neural Network Scaling Analysis**
- Why needed: Provides theoretical foundation for understanding how parameter interactions scale with model width
- Quick check: Verify that the infinite-width limit analysis correctly predicts finite-width behavior

**Low-Rank Adaptation (LoRA)**
- Why needed: Core technique being improved upon; understanding its limitations is essential
- Quick check: Confirm that standard LoRA indeed uses uniform learning rates across all parameters

**Feature Learning Dynamics**
- Why needed: Explains how different learning rates affect the adaptation process
- Quick check: Ensure the theoretical model captures the practical learning behavior

## Architecture Onboarding
**Component Map**
- LoRA adapter matrices (A and B) → Different learning rates (ηA, ηB) → Fixed ratio λ = ηB/ηA

**Critical Path**
- Initialize A and B matrices → Apply different learning rates during backpropagation → Maintain fixed ratio λ → Monitor convergence and performance

**Design Tradeoffs**
- Complexity vs. performance: Adding separate learning rates increases implementation complexity but yields significant performance gains
- Hyperparameter tuning: Requires finding optimal λ ratio, though 24 appears robust across tasks

**Failure Signatures**
- Poor performance if λ ratio is set too far from optimal value
- Instability during training if learning rates are not properly balanced

**First Experiments**
1. Compare standard LoRA vs LoRA+ on a single task with varying λ values to map performance landscape
2. Test convergence speed differences on a small model before scaling to larger architectures
3. Validate theoretical predictions by examining feature learning dynamics during training

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Theoretical analysis is limited to infinite-width networks, which may not fully capture finite-width practical behavior
- Optimal learning rate ratio λ = 24 is empirically determined but may vary across different architectures and tasks
- Focus is primarily on NLP tasks and models, leaving applicability to other domains unexplored

## Confidence
- **High Confidence**: The identification of learning rate imbalance as a source of inefficiency in standard LoRA is well-supported by both theoretical analysis and empirical evidence
- **Medium Confidence**: The specific ratio λ = 24 as an optimal setting across diverse tasks and models, though empirically validated, may not generalize perfectly to all scenarios
- **Medium Confidence**: The claim of up to 2X speedup is based on convergence behavior rather than wall-clock time measurements, which could vary with implementation details

## Next Checks
1. Test LoRA+ across a broader range of model architectures (vision transformers, diffusion models) and non-English languages to assess generalizability
2. Conduct ablation studies varying λ across multiple orders of magnitude to map out the full performance landscape and identify task-specific optimal ratios
3. Compare LoRA+ against recently proposed adapter methods like AdaLoRA and SmoothAd that also address learning rate efficiency, using consistent experimental protocols