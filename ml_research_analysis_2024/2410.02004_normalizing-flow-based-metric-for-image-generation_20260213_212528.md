---
ver: rpa2
title: Normalizing Flow-Based Metric for Image Generation
arxiv_id: '2410.02004'
source_url: https://arxiv.org/abs/2410.02004
tags:
- images
- generated
- real
- image
- normalizing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces two evaluation metrics, FLD and D-FLD, for
  generative image models using normalizing flows. Unlike existing metrics like FID,
  which rely on pre-trained models and assume Gaussian distributions, these metrics
  directly assess the likelihood of generated images, offering a more accurate measure
  of image realism.
---

# Normalizing Flow-Based Metric for Image Generation

## Quick Facts
- arXiv ID: 2410.02004
- Source URL: https://arxiv.org/abs/2410.02004
- Reference count: 34
- The paper introduces FLD and D-FLD metrics that offer better sample efficiency, sensitivity to subtle distortions, and domain adaptability compared to FID for evaluating generative image models.

## Executive Summary
This paper introduces two novel evaluation metrics, Flow-based Likelihood Distance (FLD) and Dual-Flow-based Likelihood Distance (D-FLD), for assessing the realism of generated images using normalizing flows. Unlike existing metrics like FID that rely on pre-trained models and assume Gaussian distributions, these metrics directly evaluate the likelihood of generated images under learned distributions. The metrics demonstrate superior sample efficiency (requiring only hundreds of images versus tens of thousands for FID), greater sensitivity to subtle image distortions, and better domain adaptability without relying on ImageNet-pretrained models.

## Method Summary
The paper proposes two metrics based on normalizing flows: FLD and D-FLD. FLD computes the log ratio of average log-likelihoods between generated and real images using a single flow trained on real images. D-FLD trains separate flows on real and generated images and computes the absolute difference in their average log-likelihoods. Both metrics transform likelihood values using log transformation to make them more interpretable. The approach directly assesses image realism in pixel space rather than relying on high-level features from pre-trained models, making it more sample-efficient and domain-agnostic.

## Key Results
- FLD and D-FLD achieve high sample efficiency, stabilizing with only 200 images compared to FID's requirement of over 20,000 images
- The metrics show better sensitivity to subtle image distortions, particularly in low-noise ranges where FID shows minimal variation
- FLD and D-FLD demonstrate monotonic relationships with progressive image generation in diffusion models, making them more reliable evaluation tools

## Why This Works (Mechanism)

### Mechanism 1
FLD and D-FLD metrics are more sample-efficient than FID because they require fewer than 200 images to stabilize, compared to FID's requirement of over 20,000 images. The metrics compute exact likelihoods using normalizing flows, which can capture the true data distribution with fewer samples than FID's approach of approximating distributions using Inception-V3 features and assuming Gaussian distributions. Core assumption: Normalizing flows can accurately model the image distribution with relatively few samples.

### Mechanism 2
FLD and D-FLD are more sensitive to subtle image distortions than FID, particularly in low-noise ranges. The metrics directly assess likelihood differences in the image space, making them more sensitive to small perturbations that FID might miss due to its reliance on high-level Inception features. Core assumption: Small changes in pixel values significantly affect the likelihood under the learned normalizing flow distribution.

### Mechanism 3
FLD and D-FLD are more domain-agnostic than FID because they don't rely on pre-trained ImageNet models. The metrics learn the data distribution directly from the target domain images using normalizing flows, avoiding the bias introduced by ImageNet-pretrained feature extractors. Core assumption: Normalizing flows can effectively learn domain-specific distributions when trained on representative samples.

## Foundational Learning

- Concept: Normalizing flows and their invertibility property
  - Why needed here: The entire metric relies on computing exact likelihoods through invertible transformations, which is unique to normalizing flows among generative models
  - Quick check question: Why can normalizing flows compute exact likelihoods while GANs and VAEs cannot?

- Concept: Likelihood ratio as a distance measure
  - Why needed here: FLD uses the ratio of average log-likelihoods of generated vs real images as its metric, which requires understanding how likelihood ratios relate to distribution similarity
  - Quick check question: What does a likelihood ratio close to 1 indicate about the relationship between two distributions?

- Concept: Jacobian determinant computation in high dimensions
  - Why needed here: The change of variables formula requires computing Jacobian determinants, which is computationally challenging and affects the choice of flow architecture
  - Quick check question: Why do normalizing flow architectures need to be designed to have tractable Jacobian determinants?

## Architecture Onboarding

- Component map: Real images -> Normalizing flow training -> Log-likelihood computation -> FLD/D-FLD metric calculation; Generated images -> Same pipeline for comparison
- Critical path: 1) Train normalizing flow(s) on real images; 2) Pass both real and generated images through trained flow(s); 3) Compute log-likelihoods; 4) Calculate metric (ratio for FLD, absolute difference for D-FLD); 5) Apply log transformation and averaging
- Design tradeoffs: D-FLD is more theoretically sound but requires training two flows and more computation; FLD is faster and simpler but relies on a stronger assumption that real images have higher likelihood than generated ones
- Failure signatures: Unstable metric values across runs with same data → likely insufficient training or poor flow architecture choice; Unexpectedly high metric values for high-quality generated images → flow may not be capturing important distribution features
- First 3 experiments: 1) Verify sample efficiency by computing FLD with increasing numbers of images (50, 100, 200, 500) and checking convergence; 2) Test sensitivity to noise by adding controlled Gaussian noise to real images and measuring metric increase; 3) Compare domain adaptation by training flows on CIFAR-10 then testing on SVHN (similar but different domain)

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed FLD metric perform on evaluating text-to-image generative models compared to diffusion models? The paper tested FLD only on images generated by adding distortions to real images and on those generated by diffusion-based models, but not on larger generative models like text-to-image models. This remains unresolved due to computational resource limitations preventing testing on larger generative models like text-to-image models or more extensive datasets. Testing FLD on outputs from popular text-to-image models like Stable Diffusion or DALL-E and comparing its performance metrics with FID and human evaluation scores would resolve this.

### Open Question 2
What is the optimal architecture for normalizing flows used in FLD that balances computational efficiency and evaluation accuracy? The authors suggest there is potential to optimize normalizing flow-based evaluation metrics by constructing more efficient and expressive normalizing flows than those used in their study. This remains unresolved as the paper uses a specific normalizing flow architecture but acknowledges it may not be optimal, and suggests future work could explore more advanced normalizing flows. Systematic ablation studies comparing different normalizing flow architectures (number of layers, coupling mechanisms, multiscale designs) while measuring both evaluation accuracy and computational cost would resolve this.

### Open Question 3
How do FLD and D-FLD metrics behave when evaluating generative models trained on highly specialized domains like medical imaging? The paper discusses that existing metrics like FID have limitations for specialized domains like medical x-ray images because they rely on ImageNet pre-trained models, and suggests normalizing flows should be retrained on domain-specific datasets. This remains unresolved as the authors did not test their metrics on specialized domains like medical imaging, focusing instead on CIFAR-10 and CelebA-HQ datasets. Evaluating FLD and D-FLD on generative models trained for medical imaging tasks (X-rays, MRIs) and comparing their sensitivity to domain-specific image quality metrics and radiologist assessments would resolve this.

## Limitations
- Performance on more complex datasets like ImageNet or medical imaging domains remains unverified
- Computational cost comparison with FID is based on theoretical complexity rather than empirical measurements
- Impact of flow architecture choices on metric quality is not systematically explored

## Confidence

- Sample efficiency claims: High - well-supported by experimental evidence and the theoretical advantage of normalizing flows in likelihood computation
- Sensitivity to subtle distortions: Medium - demonstrated through controlled experiments but limited to specific noise patterns
- Domain-agnostic advantages: Medium - logically sound but requires more empirical validation across diverse domains

## Next Checks
1. Test metric stability on datasets with more complex distributions (e.g., ImageNet-32x32) to verify if sample efficiency holds for higher-dimensional data
2. Evaluate performance when training flows on limited data (<1000 images) to assess practical domain adaptation capabilities
3. Measure actual wall-clock computation time for both metrics on the same hardware to validate computational efficiency claims