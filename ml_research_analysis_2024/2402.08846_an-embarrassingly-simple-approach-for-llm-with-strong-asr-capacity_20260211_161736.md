---
ver: rpa2
title: An Embarrassingly Simple Approach for LLM with Strong ASR Capacity
arxiv_id: '2402.08846'
source_url: https://arxiv.org/abs/2402.08846
tags:
- speech
- encoder
- llm-based
- large
- proc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SLAM-ASR, an embarrassingly simple approach
  for large language model (LLM) with strong automatic speech recognition (ASR) capacity.
  The method combines off-the-shelf speech encoders and LLMs with a trainable linear
  projector, eliminating the need for complex designs like temporal compression, modal
  alignment, or parameter-efficient fine-tuning.
---

# An Embarrassingly Simple Approach for LLM with Strong ASR Capacity

## Quick Facts
- arXiv ID: 2402.08846
- Source URL: https://arxiv.org/abs/2402.08846
- Reference count: 5
- Achieves state-of-the-art LLM-based ASR performance with 1.94% WER on Librispeech test-clean

## Executive Summary
This paper presents SLAM-ASR, an embarrassingly simple approach for integrating automatic speech recognition (ASR) capacity into large language models (LLMs). The method combines off-the-shelf speech encoders with LLMs using only a trainable linear projector, eliminating complex designs like temporal compression or parameter-efficient fine-tuning. SLAM-ASR achieves state-of-the-art performance among LLM-based ASR models on Librispeech, with a WER of 1.94% on test-clean and 3.81% on test-other. The approach also demonstrates capability emergence during modal alignment training and even surpasses the latest audio-universal model trained on massive paired data.

## Method Summary
SLAM-ASR integrates speech encoders and LLMs through a trainable linear projector, avoiding complex designs like temporal compression, modal alignment, or parameter-efficient fine-tuning. The method combines off-the-shelf components in a straightforward manner, achieving strong ASR performance through simplicity rather than architectural complexity. The approach demonstrates capability emergence during training, where the model develops unexpected capabilities as modal alignment progresses.

## Key Results
- Achieves 1.94% WER on Librispeech test-clean and 3.81% on test-other
- Outperforms all previous LLM-based ASR models on Librispeech benchmark
- Surpasses the latest audio-universal model trained on massive paired data
- Demonstrates capability emergence phenomenon during modal alignment training

## Why This Works (Mechanism)
The approach works by leveraging the strong language modeling capabilities of LLMs while using specialized speech encoders to handle acoustic features. The trainable linear projector serves as a simple bridge between modalities, allowing the LLM to adapt to speech inputs without requiring complex alignment mechanisms. This simplicity avoids overfitting to training data while maintaining the generalization capabilities of the underlying LLM. The capability emergence observed during training suggests that the model naturally develops cross-modal understanding as it learns to align speech representations with linguistic ones.

## Foundational Learning

1. **Speech Encoder Fundamentals**
   - Why needed: Converts raw audio waveforms into meaningful feature representations
   - Quick check: Can the encoder capture phonetic and prosodic information effectively

2. **Modal Alignment Mechanisms**
   - Why needed: Bridges the gap between acoustic and linguistic representations
   - Quick check: Does the alignment preserve temporal and semantic relationships

3. **LLM Fine-tuning Strategies**
   - Why needed: Adapts language models to process speech-derived inputs
   - Quick check: Can the LLM maintain performance while learning new modalities

4. **Capability Emergence in Multi-modal Learning**
   - Why needed: Explains unexpected performance improvements during training
   - Quick check: Are emergent capabilities consistent across different model configurations

## Architecture Onboarding

Component Map: Speech Encoder -> Trainable Linear Projector -> LLM

Critical Path: The projector serves as the critical bottleneck, requiring careful initialization and training to effectively bridge modalities without degrading LLM performance.

Design Tradeoffs: Simplicity versus performance - the approach sacrifices potential gains from complex alignment mechanisms for robustness and ease of implementation.

Failure Signatures: Poor projector initialization can lead to catastrophic forgetting of LLM capabilities; inadequate speech encoder quality manifests as high baseline WER before LLM integration.

First Experiments:
1. Baseline evaluation: Test speech encoder performance alone on Librispeech to establish acoustic feature quality
2. Projector ablation: Remove the linear projector to verify its contribution to overall performance
3. LLM freezing: Freeze LLM parameters during training to assess transfer learning effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLM-based ASR models scale with increasing amounts of training data beyond 960 hours of Librispeech?
- Basis in paper: [inferred] The paper discusses SLAM-ASR's performance on Librispeech and mentions that the model was trained on 960 hours of data, but does not explore scaling to larger datasets.
- Why unresolved: The paper focuses on a clean, simple setup and does not investigate the impact of scaling the training data.
- What evidence would resolve it: Training SLAM-ASR on progressively larger datasets (e.g., 2000, 5000, 10000 hours) and comparing WER performance to baseline models.

### Open Question 2
- Question: What is the impact of different speech encoder architectures (e.g., CNN, RNN, Transformer) on the performance of LLM-based ASR models?
- Basis in paper: [explicit] The paper explores combinations of different LLMs and speech encoders, but does not systematically compare different encoder architectures.
- Why unresolved: The paper focuses on benchmarking existing speech encoders rather than comparing architectural choices.
- What evidence would resolve it: Training LLM-based ASR models with identical LLMs but different encoder architectures (CNN, RNN, Transformer) and comparing WER performance.

### Open Question 3
- Question: How does the capability emergence phenomenon observed in LLM-based ASR training generalize to other cross-modal tasks (e.g., speech-to-text translation, speech-to-image)?
- Basis in paper: [explicit] The paper observes capability emergence during LLM-based ASR training and explores its characteristics.
- Why unresolved: The paper only investigates capability emergence in the specific context of ASR and does not generalize to other tasks.
- What evidence would resolve it: Training LLM-based models for other cross-modal tasks (e.g., speech-to-text translation, speech-to-image) and analyzing whether similar capability emergence patterns occur.

## Limitations
- Claims of state-of-the-art performance lack detailed experimental setup information for independent verification
- Approach may not generalize well to languages other than English or noisy speech conditions
- Focus on clean Librispeech data doesn't address real-world ASR challenges with background noise and accented speech
- Capability emergence phenomenon lacks rigorous quantitative analysis

## Confidence
- State-of-the-art claims: Medium - based on specific comparisons without full experimental details
- Generalization to other languages: Low - not evaluated beyond English
- Capability emergence claims: Low - qualitative observations without systematic analysis
- Comparison to audio-universal models: Medium - significant claim requiring independent verification

## Next Checks
1. Replicate experiments on additional ASR benchmarks beyond Librispeech, including noisy speech datasets and multilingual corpora, to assess generalization capability
2. Conduct ablation studies to quantify the contribution of each component (speech encoder, projector, LLM) to overall performance
3. Perform error analysis across different speaker demographics and acoustic conditions to identify potential bias and robustness limitations