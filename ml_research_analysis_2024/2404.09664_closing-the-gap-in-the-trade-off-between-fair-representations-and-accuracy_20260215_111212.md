---
ver: rpa2
title: Closing the Gap in the Trade-off between Fair Representations and Accuracy
arxiv_id: '2404.09664'
source_url: https://arxiv.org/abs/2404.09664
tags:
- vector
- fairness
- data
- reconstruction
- extrema
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies fairness of sentence/document encodings by analysing
  PCA reconstruction errors across different groups defined by protected attributes.
  The authors show that vector averaging and vector extrema methods exhibit different
  levels of bias towards subgroups in two datasets (Hindi Legal Corpus and Multilingual
  Twitter Corpus).
---

# Closing the Gap in the Trade-off between Fair Representations and Accuracy

## Quick Facts
- arXiv ID: 2404.09664
- Source URL: https://arxiv.org/abs/2404.09664
- Reference count: 8
- Key outcome: This paper studies fairness of sentence/document encodings by analysing PCA reconstruction errors across different groups defined by protected attributes. The authors show that vector averaging and vector extrema methods exhibit different levels of bias towards subgroups in two datasets (Hindi Legal Corpus and Multilingual Twitter Corpus). They propose combining these encodings using a convex combination with parameter λ to reduce the reconstruction error gap while maintaining high classification accuracy. Experiments show that for λ ≈ 0.97, the fairness gap is minimized with only a small drop in accuracy, demonstrating a practical trade-off between fairness and performance in NLP tasks.

## Executive Summary
This paper addresses the challenge of achieving fair representations in NLP by analyzing how different encoding methods (vector averaging and vector extrema) introduce bias across protected groups. The authors demonstrate that these methods have complementary bias patterns when measured through PCA reconstruction errors. By combining them using a convex combination parameter λ, they achieve a balance between fairness (reducing reconstruction error differences across groups) and accuracy (maintaining high classification performance). Their experiments on Hindi legal documents and multilingual Twitter data show that λ ≈ 0.97 minimizes fairness gaps with minimal accuracy loss.

## Method Summary
The authors study fairness at the representation level by computing PCA reconstruction errors for different subgroups defined by protected attributes. They analyze two encoding methods - vector averaging and vector extrema - on Hindi Legal Document Corpus (for bail prediction with religion as protected attribute) and Multilingual Twitter Corpus (for hate speech tagging with ethnicity and gender as protected attributes). The key innovation is using a convex combination of these encodings with parameter λ to optimize the trade-off between fairness (measured as reconstruction error differences across groups) and accuracy (measured via SVM test accuracy). The optimal λ ≈ 0.97 was found through grid search.

## Key Results
- Vector averaging and vector extrema exhibit different bias patterns across protected groups when measured via PCA reconstruction errors
- A convex combination with λ ≈ 0.97 minimizes reconstruction error differences while maintaining high classification accuracy
- The approach achieves near-zero reconstruction error difference at the cost of only small accuracy reduction
- Intra-group reconstruction error differences are much smaller than inter-group differences, validating the fairness metric

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vector averaging and vector extrema exhibit complementary bias patterns across different groups.
- Mechanism: Vector averaging tends to encode subgroup information more uniformly along certain principal components, while vector extrema emphasizes extreme values along each dimension. This leads to different reconstruction error profiles for subgroups when PCA is applied.
- Core assumption: The principal components capture systematic differences in subgroup representation that correlate with the protected attributes.
- Evidence anchors:
  - [abstract] The authors show that vector averaging and vector extrema methods exhibit different levels of bias towards subgroups.
  - [section 5.1] The reconstruction error plots show that vector average consistently has different reconstruction errors for different gender/ethnicity groups, while vector extrema sometimes shows balanced errors.
  - [corpus] Weak evidence - no directly relevant corpus neighbors.
- Break condition: If the principal components do not capture meaningful subgroup distinctions, or if the embedding space is isotropic with respect to protected attributes.

### Mechanism 2
- Claim: Convex combination of vector averaging and extrema can balance fairness and accuracy.
- Mechanism: By tuning λ, the convex combination can selectively suppress or emphasize components that contribute to bias while preserving predictive information.
- Core assumption: There exists a λ value where the reconstruction error gap between groups is minimized without a large drop in classification accuracy.
- Evidence anchors:
  - [abstract] For λ ≈ 0.97, the fairness gap is minimized with only a small drop in accuracy.
  - [section 6.1] Figure 4 shows that at λ = 0.97, both MTC-eth and MTC-gen datasets achieve near-zero reconstruction error difference while maintaining high accuracy.
  - [corpus] Weak evidence - no directly relevant corpus neighbors.
- Break condition: If the two encoding methods are too similar in their bias profiles, the convex combination may not provide additional fairness gains.

### Mechanism 3
- Claim: PCA reconstruction error differences indicate representation-level bias.
- Mechanism: If subgroups have systematically different reconstruction errors, it suggests the principal components encode group-specific information, violating fairness.
- Core assumption: Fairness requires that the encoding preserve information about the classification task while being invariant to protected attributes.
- Evidence anchors:
  - [section 5.1] The authors compute reconstruction errors for each group separately and find significant differences in some cases.
  - [section 5.2] Comparison with random splits shows that inter-group error differences are much larger than intra-group differences, indicating systematic bias.
  - [corpus] Weak evidence - no directly relevant corpus neighbors.
- Break condition: If reconstruction error differences are due to noise or sampling artifacts rather than systematic encoding bias.

## Foundational Learning

- Concept: Principal Component Analysis (PCA)
  - Why needed here: PCA is used to analyze how well different subgroups are represented in the embedding space by measuring reconstruction errors.
  - Quick check question: What does a lower reconstruction error for one subgroup compared to another indicate about the encoding's fairness?

- Concept: Convex combination
  - Why needed here: The method uses a convex combination of two encoding strategies to balance fairness and accuracy.
  - Quick check question: How does changing λ in the convex combination affect the trade-off between fairness and accuracy?

- Concept: Protected attributes and fairness metrics
  - Why needed here: The paper defines fairness in terms of reconstruction error differences across groups defined by protected attributes.
  - Quick check question: Why is it important to measure fairness at the representation level rather than just the outcome level?

## Architecture Onboarding

- Component map:
  Data preprocessing → Embedding generation (FastText/GloVe) → Vector encoding (average/extrema) → SVM classification → Fairness evaluation (PCA reconstruction)
  Convex combination module: takes λ parameter and outputs combined encodings
  Hyperparameter tuning: grid search for SVM C and γ

- Critical path:
  1. Load and preprocess text data
  2. Generate word embeddings
  3. Apply vector averaging and extrema encodings
  4. Compute convex combination with λ
  5. Train SVM classifier
  6. Evaluate accuracy and fairness (PCA reconstruction error differences)

- Design tradeoffs:
  - λ close to 1 favors accuracy (vector average) but may increase bias
  - λ close to 0 favors fairness (vector extrema) but may reduce accuracy
  - Number of PCA dimensions affects both accuracy and fairness measurement

- Failure signatures:
  - High reconstruction error differences across groups indicate bias
  - Large drop in accuracy when increasing λ suggests loss of predictive information
  - Inconsistent results across different PCA dimensions may indicate instability

- First 3 experiments:
  1. Vary λ from 0 to 1 in steps of 0.1 and plot accuracy vs. reconstruction error difference for both datasets
  2. Compare intra-group vs. inter-group reconstruction error differences to validate fairness metric
  3. Test different numbers of PCA dimensions to find optimal trade-off between accuracy and fairness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the vector length difference (not magnitude) contribute to the observed bias in PCA reconstruction errors across protected groups?
- Basis in paper: [inferred] The authors note that vector length differences are not significant but suggest exploring vector directions further.
- Why unresolved: The paper only analyzed vector magnitudes (Frobenius norm) and found no significant differences, but did not explore the directional components of the vectors.
- What evidence would resolve it: Detailed analysis of the directional components of the vector encodings across different protected groups to determine if certain directions are systematically over-represented for specific groups.

### Open Question 2
- Question: Can the optimal λ value for combining vector average and extrema encodings be determined through direct training rather than heuristic grid search?
- Basis in paper: [explicit] The authors state they plan to find methods beyond grid search to find the optimal λ in future work.
- Why unresolved: The current approach uses a grid search with fixed step size (0.1) which may not efficiently find the true optimal λ.
- What evidence would resolve it: Development and validation of an optimization algorithm that can efficiently find the optimal λ value while maintaining both fairness and accuracy.

### Open Question 3
- Question: How does representation-level fairness influence downstream classifier performance beyond just accuracy metrics?
- Basis in paper: [explicit] The authors mention they aim to better understand the influences of representation-fairness on subsequent steps such as the classifier.
- Why unresolved: The current study focuses on accuracy metrics but does not explore other potential impacts of representation fairness on classifier behavior.
- What evidence would resolve it: Comprehensive analysis of classifier behavior including false positive/negative rates, decision boundaries, and robustness across different protected groups when using fair representations.

## Limitations

- The optimal λ value (0.97) appears highly specific and dataset-dependent, raising questions about generalizability across different languages, domains, and protected attributes.
- The paper uses SVM classifiers exclusively, but doesn't explore whether the fairness-accuracy trade-off generalizes to other model architectures like neural networks.
- The fairness metric based on PCA reconstruction error differences assumes that these differences directly correlate with representation-level bias, but the paper doesn't validate this assumption against other fairness metrics or real-world impact measures.

## Confidence

- **High**: The experimental methodology is sound, with clear validation through intra-group vs. inter-group comparison showing systematic bias rather than random variation.
- **Medium**: The convex combination approach demonstrates a clear mechanism for balancing fairness and accuracy, supported by empirical results.
- **Low**: Generalization of the specific λ ≈ 0.97 result and the PCA-based fairness metric to other contexts remains uncertain.

## Next Checks

1. **Replicate with alternative fairness metrics**: Apply the same methodology using equalized odds or demographic parity metrics to verify that PCA reconstruction error differences are a valid proxy for representation-level fairness.

2. **Cross-domain validation**: Test the convex combination approach on additional datasets with different languages, domains, and protected attributes to assess generalizability of the λ ≈ 0.97 finding.

3. **Model architecture comparison**: Evaluate whether the fairness-accuracy trade-off observed with SVM classifiers holds when using neural network classifiers, particularly transformer-based models.