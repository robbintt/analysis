---
ver: rpa2
title: Towards Two-Stream Foveation-based Active Vision Learning
arxiv_id: '2403.15977'
source_url: https://arxiv.org/abs/2403.15977
tags:
- object
- framework
- glimpse
- learning
- stream
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a machine learning framework inspired by the
  "two-stream hypothesis" from neuroscience, which proposes that visual information
  is processed by two separate pathways: the dorsal (where) stream for visual guidance
  and the ventral (what) stream for visual identification. The framework incorporates
  mechanisms of foveation, where the ventral stream focuses on high-resolution input
  from the fovea region while the dorsal stream processes the entire image in low
  resolution for context.'
---

# Towards Two-Stream Foveation-based Active Vision Learning

## Quick Facts
- arXiv ID: 2403.15977
- Source URL: https://arxiv.org/abs/2403.15977
- Reference count: 40
- Primary result: Achieves 72% localization accuracy and 64% attribute localization accuracy on CelebA dataset

## Executive Summary
This paper introduces a two-stream neural network framework inspired by the biological "two-stream hypothesis" for active vision learning. The framework incorporates foveation mechanisms where a ventral stream processes high-resolution foveated glimpses while a dorsal stream uses reinforcement learning to control where to place these glimpses. The model is evaluated on weakly-supervised object localization tasks using CelebA and CUB-200-2011 datasets, demonstrating both object localization and attribute prediction capabilities. Notably, the dorsal stream shows generalization potential by localizing objects on unseen datasets without additional fine-tuning.

## Method Summary
The proposed framework consists of two independent neural networks: a ventral stream (M2B) that extracts features from high-resolution foveated patches and a dorsal stream (M2A) that predicts actions to adjust the foveation region using reinforcement learning. The ventral stream is trained with supervised learning on label-based targets, while the dorsal stream learns through trial-and-error to maximize rewards based on the ventral stream's output similarity to target labels. An initial fixation point predictor (M1) provides the starting point for foveation. The framework is evaluated on CelebA for facial attribute localization and CUB-200-2011 for bird part localization, with additional generalization testing on WIDERFace and ImageNet datasets.

## Key Results
- Achieves 72% GT Localization accuracy (IoU > 0.5) on CelebA dataset
- Achieves 64% attribute localization accuracy on CelebA dataset
- Demonstrates generalization capability with zero fine-tuning on WIDERFace and ImageNet datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Foveation allows the ventral stream to focus on high-resolution object details while ignoring background clutter.
- Mechanism: The ventral model (M2B) processes only a cropped high-resolution patch centered at the current fixation point, while the dorsal model (M2A) controls where to place this patch through reinforcement learning.
- Core assumption: Background regions outside the foveated glimpse are irrelevant for object identification and localization.
- Evidence anchors:
  - [abstract]: "ventral (what) stream focusing on the input regions perceived by the fovea part of an eye (foveation)"
  - [section III-B]: "We enforce foveation with extreme cutoff, where the ventral stream model perceives the input only through a foveated glimpse, and the outlier regions of the input are ignored."
  - [corpus]: Weak - corpus papers focus on general two-stream networks but not this specific foveation approach.
- Break condition: If the background contains discriminative features essential for object identification, foveation will fail to capture them.

### Mechanism 2
- Claim: Reinforcement learning enables the dorsal stream to learn optimal sequences of foveation adjustments for object localization.
- Mechanism: M2A predicts actions (expand left/right/up/down or stop) to adjust the foveation region based on rewards derived from ventral stream output similarity to target labels.
- Core assumption: The sequence of foveation adjustments can be learned through trial-and-error to maximize object capture while minimizing background.
- Evidence anchors:
  - [abstract]: "reinforcement learning to train the dorsal stream on how to adjust the fovea using the output of the ventral stream as a guiding signal (rewards)"
  - [section III-C]: "The rewards for RL are assigned based on how well outputs from the currently observed foveated glimpse (obtained from the ventral stream model) match the expected image-level targets."
  - [corpus]: Weak - corpus papers mention two-stream networks but not this specific RL-based foveation adjustment mechanism.
- Break condition: If the reward signal is ambiguous or noisy, RL may converge to suboptimal foveation sequences.

### Mechanism 3
- Claim: Independent training of ventral and dorsal streams allows the dorsal stream to generalize for object localization on unseen datasets.
- Mechanism: M2A learns object localization function independent of M2B's object identification function, enabling M2A to be used alone for localization on new datasets.
- Core assumption: The dorsal stream learns generalizable localization patterns that transfer across datasets when trained on one dataset.
- Evidence anchors:
  - [abstract]: "due to the independent nature of the two streams, the dorsal model can be applied on its own to unseen images to localize objects from different datasets"
  - [section IV-D]: "Figure 8 illustrates the qualitative results of the predicted localization on the WIDERFace [25] dataset and a subset of the ImageNet [26] dataset. A key point to emphasize here is that for inference, only the dorsal (where) model is being used with zero fine-tuning on these unseen datasets."
  - [corpus]: Weak - corpus papers don't discuss generalization capabilities of independent stream models.
- Break condition: If the object appearance varies significantly across datasets, the dorsal stream may fail to generalize without fine-tuning.

## Foundational Learning

- Concept: Reinforcement Learning with Policy Gradient (REINFORCE algorithm)
  - Why needed here: To train M2A model to predict optimal foveation adjustment actions through trial-and-error learning
  - Quick check question: How does the REINFORCE algorithm update the policy based on rewards received from environment interactions?

- Concept: Class Activation Maps (CAM) for weakly-supervised localization
  - Why needed here: To generate initial fixation points for M1 model when bounding box labels are unavailable
  - Quick check question: What does CAM technique visualize in the context of weakly-supervised object localization?

- Concept: Cosine distance as similarity metric for attribute vectors
  - Why needed here: To measure similarity between predicted and target attributes for reward calculation in CelebA experiments
  - Quick check question: How is cosine distance calculated between two vectors and what does it represent?

## Architecture Onboarding

- Component map:
  M1 (64×64) -> Initial fixation -> Foveate patch -> M2A/M2B (96×96) -> Action/Features -> Adjust foveation -> Repeat -> Final bbox

- Critical path:
  Input → M1 (64×64) → Initial fixation → Foveate patch → M2A/M2B (96×96) → Action/Features → Adjust foveation → Repeat → Final bbox

- Design tradeoffs:
  - Low-resolution input for M1 vs. computational efficiency vs. localization accuracy
  - Extreme foveation cutoff vs. background context preservation
  - Fixed number of iterations vs. adaptive stopping criteria
  - Reward function design vs. training stability and convergence

- Failure signatures:
  - M1 predicts fixation points outside object bounding boxes
  - M2A expands foveation beyond object boundaries without stopping
  - M2B fails to identify objects from foveated glimpses
  - RL training diverges due to poorly designed reward function

- First 3 experiments:
  1. Train M1 on CelebA using Grad-CAM generated targets and evaluate hit/miss accuracy
  2. Train M2B initialized from pre-trained model on foveated CelebA patches and measure attribute accuracy
  3. Train M2A with REINFORCE on CelebA using cosine similarity rewards and evaluate localization performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed framework perform when the initial fixation point predicted by the M1 model is far from the actual object location?
- Basis in paper: [explicit] The paper mentions that the M1 model predicts an approximate initial fixation point based on the global context, and its performance is evaluated using hit/miss accuracy.
- Why unresolved: The paper does not provide detailed analysis of the framework's performance when the initial fixation point is far from the actual object location.
- What evidence would resolve it: Additional experiments analyzing the framework's performance for various initial fixation point accuracies and distances from the actual object location.

### Open Question 2
- Question: How does the proposed framework handle multiple objects in the same image?
- Basis in paper: [inferred] The paper focuses on weakly-supervised object localization (WSOL) tasks, which assume there is only one object of interest in the image. However, it does not explicitly address how the framework handles multiple objects.
- Why unresolved: The paper does not provide any experiments or analysis on the framework's performance when dealing with multiple objects in the same image.
- What evidence would resolve it: Experiments evaluating the framework's performance on datasets with multiple objects in the same image and comparing it with other methods designed for multi-object localization.

### Open Question 3
- Question: How does the proposed framework perform on datasets with more complex backgrounds or cluttered scenes?
- Basis in paper: [inferred] The paper demonstrates the framework's ability to distinguish object parts from background clutter on CelebA and CUB datasets. However, it does not provide a detailed analysis of the framework's performance on more complex backgrounds or cluttered scenes.
- Why unresolved: The paper does not provide experiments or analysis on the framework's performance on datasets with more complex backgrounds or cluttered scenes.
- What evidence would resolve it: Experiments evaluating the framework's performance on datasets with more complex backgrounds or cluttered scenes and comparing it with other methods designed for such scenarios.

## Limitations

- The extreme foveation cutoff ignores potentially useful contextual information that could aid in object identification
- The framework's performance on datasets with multiple objects or complex backgrounds is not evaluated
- The claim of being "inspired by" the two-stream hypothesis lacks rigorous neuroscientific validation

## Confidence

- **High confidence**: The core two-stream architecture design and the use of reinforcement learning for foveation adjustment are well-supported by the results and align with established methods in the field
- **Medium confidence**: The generalization capability of the dorsal stream to unseen datasets is demonstrated qualitatively but would benefit from more extensive quantitative evaluation across diverse datasets
- **Low confidence**: The claim that this framework is "inspired by" the two-stream hypothesis from neuroscience is largely conceptual and not rigorously validated through neuroscientific experiments or measurements

## Next Checks

1. **Cross-dataset generalization testing**: Evaluate the dorsal stream model on at least 5 additional diverse datasets (e.g., Pascal VOC, COCO, Cityscapes) with quantitative metrics beyond qualitative visualization to rigorously test generalization claims

2. **Ablation study on foveation parameters**: Systematically vary the foveation cutoff level, patch size, and number of iterations to quantify the impact of these hyperparameters on localization and attribute prediction performance

3. **Comparison with full-image baselines**: Implement and evaluate a control model that processes the full image (without foveation) using the same ventral stream architecture to measure the actual benefit of the foveation mechanism over simpler approaches