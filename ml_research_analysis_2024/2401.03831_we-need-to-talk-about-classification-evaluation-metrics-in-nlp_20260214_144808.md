---
ver: rpa2
title: We Need to Talk About Classification Evaluation Metrics in NLP
arxiv_id: '2401.03831'
source_url: https://arxiv.org/abs/2401.03831
tags:
- accuracy
- informedness
- class
- metrics
- metric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the limitations of commonly used classification
  evaluation metrics (Accuracy, F1, AUC-ROC) in NLP tasks, identifying issues like
  the "Accuracy Paradox" and baseline credit that distort model comparison. The authors
  propose Informedness, a prevalence-invariant metric that measures the proportion
  of samples a classifier can predict better than random guessing, as a more reliable
  alternative.
---

# We Need to Talk About Classification Evaluation Metrics in NLP

## Quick Facts
- arXiv ID: 2401.03831
- Source URL: https://arxiv.org/abs/2401.03831
- Reference count: 9
- One-line primary result: Informedness is proposed as a prevalence-invariant metric that better captures model capabilities across NLP tasks than traditional metrics like Accuracy and F1.

## Executive Summary
This paper investigates the limitations of commonly used classification evaluation metrics in NLP tasks, particularly Accuracy, F1, and AUC-ROC. The authors identify issues like the "Accuracy Paradox" and baseline credit that distort model comparison, especially in imbalanced datasets. They propose Informedness as a more reliable alternative that measures the proportion of samples a classifier can predict better than random guessing, providing prevalence-invariant scores that allow for fairer cross-task comparisons.

## Method Summary
The authors conduct empirical analysis comparing Informedness against traditional metrics (Accuracy, F1-Macro, MCC, NIT) across multiple NLP tasks including GLUE benchmark, GQA, KVQA, and machine translation. They demonstrate Informedness's behavior through synthetic experiments with varying class distributions and model predictive capacity, then validate findings on real-world datasets. The paper also releases a Python implementation of Informedness in the SciKitLearn format for reproducibility.

## Key Results
- Informedness reveals negative performance in adversarial datasets like WNLI where Accuracy fails
- It automatically accounts for class imbalance without requiring dataset balancing
- Informedness better captures model capabilities across diverse NLP tasks and provides more interpretable rankings than traditional metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Informedness provides a prevalence-invariant metric that measures the proportion of samples a classifier can predict better than random guessing.
- Mechanism: Informedness calculates the gain (or loss) for each prediction based on the empirical probability of the true class. It aggregates these gains across all predictions to produce a score that represents the proportion of informed decisions made by the classifier.
- Core assumption: The classifier's predictions are independent and identically distributed (i.i.d.) and the distribution of classes in the training and test sets are identical.
- Break condition: If the assumption of i.i.d. predictions or identical class distributions between training and test sets is violated, the Informedness score may not accurately represent the classifier's performance.

### Mechanism 2
- Claim: Informedness better captures model capabilities across diverse NLP tasks compared to traditional metrics like Accuracy and F1.
- Mechanism: Informedness removes the baseline credit and label bias gain, which are inherent in traditional metrics. This allows for a more accurate comparison of model performance across tasks with different class distributions and biases.
- Core assumption: The traditional metrics (Accuracy, F1) are biased towards models that exploit class prevalence or label bias.
- Break condition: If the tasks being compared have similar class distributions and biases, the difference between Informedness and traditional metrics may not be significant.

### Mechanism 3
- Claim: Informedness allows for direct comparison between tasks with varying bias or complexity without the need for dataset re-balancing.
- Mechanism: Informedness is prevalence-invariant, meaning it does not reward models for predicting more prevalent classes. This allows for a fair comparison of model performance across tasks with different class distributions.
- Core assumption: The tasks being compared have different class distributions or biases.
- Break condition: If the tasks being compared have similar class distributions and biases, the advantage of Informedness in allowing direct comparison may not be significant.

## Foundational Learning

- Concept: Confusion Matrix and Contingency Table
  - Why needed here: Understanding the structure of the confusion matrix and contingency table is crucial for defining and interpreting classification metrics like Informedness.
  - Quick check question: Given a binary classification problem, can you construct the confusion matrix and contingency table for a given set of true and predicted labels?

- Concept: Prevalence and Bias
  - Why needed here: Informedness accounts for class prevalence and bias, which are important factors in evaluating classification performance.
  - Quick check question: In a binary classification problem with 90% positive and 10% negative samples, what are the prevalence and bias values?

- Concept: Information Theory and Entropy
  - Why needed here: Informedness is based on the concept of information gain, which is related to entropy in information theory.
  - Quick check question: What is the entropy of a binary random variable with probabilities p and 1-p?

## Architecture Onboarding

- Component map: Informedness calculation -> Gain/loss computation for each prediction -> Aggregation across all predictions -> Final prevalence-invariant score
- Critical path: The gain calculation is the critical path as it directly affects the final score and must correctly account for class prevalence
- Design tradeoffs: Choice between using empirical probability of true class (ensures prevalence-invariance) versus predicted probability from classifier (accounts for confidence but may introduce bias)
- Failure signatures: Incorrect gain calculations, improper handling of edge cases (zero prevalence or bias), floating-point precision issues, violation of i.i.d. assumption
- First 3 experiments:
  1. Implement Informedness for a simple binary classification problem with known true and predicted labels, and verify that the calculated score matches the expected value.
  2. Compare the Informedness scores of two classifiers on a binary classification problem with imbalanced classes, and verify that the classifier with better informedness has a higher score.
  3. Implement Informedness for a multi-class classification problem, and verify that the calculated scores are consistent with the expected values for each class.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Informedness behave with extremely small test sets (fewer than 50 samples)?
- Basis in paper: The Limitations section stating Informedness is sensitive to low numbers of evaluation samples
- Why unresolved: The paper notes this sensitivity but doesn't empirically demonstrate it or provide guidance on minimum sample sizes
- What evidence would resolve it: Systematic experiments showing Informedness stability across varying test set sizes, particularly below 50 samples

### Open Question 2
- Question: Can Informedness be adapted to handle cases where training and test class distributions differ?
- Basis in paper: The Limitations section mentioning the assumption of identical class distributions between training and test sets
- Why unresolved: The authors suggest allowing training distributions to be passed to their implementation but don't explore this or evaluate its effectiveness
- What evidence would resolve it: Comparative studies showing Informedness performance with and without distribution adaptation across varying train/test distribution mismatches

### Open Question 3
- Question: How does Informedness compare to other prevalence-invariant metrics in more diverse NLP tasks beyond those tested?
- Basis in paper: The extensive experiments but limited task diversity (GLUE, GQA, KVQA, formality control)
- Why unresolved: While the paper tests multiple metrics across several tasks, there are many NLP tasks and potential alternatives not explored
- What evidence would resolve it: Large-scale empirical comparison of Informedness against prevalence-invariant alternatives across a comprehensive range of NLP task types and dataset characteristics

## Limitations

- The assumption of i.i.d. predictions and identical class distributions between training and test sets may not hold in real-world NLP applications where domain shift is common.
- While the paper claims Informedness better captures model generalizability, the empirical evidence is limited to specific task types and may not generalize across all NLP domains.
- The assertion that Informedness automatically accounts for all forms of bias and complexity without dataset re-balancing is somewhat overstated.

## Confidence

- **High Confidence**: The characterization of traditional metrics' limitations (Accuracy Paradox, baseline credit issues) is well-supported by mathematical analysis and empirical evidence.
- **Medium Confidence**: The claim that Informedness provides more interpretable and comparable model rankings is supported by experiments on GLUE and other benchmarks, but the evidence is primarily comparative rather than demonstrating absolute superiority.
- **Low Confidence**: The assertion that Informedness automatically accounts for all forms of bias and complexity without dataset re-balancing is somewhat overstated.

## Next Checks

1. **Cross-Domain Generalization Test**: Apply Informedness to a real-world NLP dataset with known domain shift (e.g., news articles to social media text) and compare its performance consistency against traditional metrics.

2. **Adversarial Dataset Extension**: Design and test Informedness on additional adversarial NLP datasets beyond WNLI, such as those with deliberately misleading correlations or label noise.

3. **Computational Complexity Analysis**: Benchmark the runtime and memory requirements of Informedness against traditional metrics on large-scale NLP tasks (e.g., document classification with millions of samples).