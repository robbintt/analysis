---
ver: rpa2
title: 'Generative Relevance Feedback and Convergence of Adaptive Re-Ranking: University
  of Glasgow Terrier Team at TREC DL 2023'
arxiv_id: '2405.01122'
source_url: https://arxiv.org/abs/2405.01122
tags:
- retrieval
- generative
- re-ranking
- bm25
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The University of Glasgow Terrier team participated in the TREC
  2023 Deep Learning Track to evaluate generative relevance feedback and adaptive
  re-ranking on the MS MARCO-v2 corpus. They applied generative query reformulation
  (Gen-QR) and pseudo-relevance feedback (Gen-PRF) over BM25 and SPLADE first-stage
  models, followed by adaptive re-ranking using monoELECTRA over a BM25 corpus graph.
---

# Generative Relevance Feedback and Convergence of Adaptive Re-Ranking: University of Glasgow Terrier Team at TREC DL 2023

## Quick Facts
- arXiv ID: 2405.01122
- Source URL: https://arxiv.org/abs/2405.01122
- Reference count: 32
- Key outcome: Gen-PRF with adaptive re-ranking achieved best P@10 and nDCG@10 performance on TREC DL 2023

## Executive Summary
The University of Glasgow Terrier team evaluated generative relevance feedback and adaptive re-ranking on the MS MARCO-v2 corpus for TREC 2023 Deep Learning Track. They applied generative query reformulation (Gen-QR) and pseudo-relevance feedback (Gen-PRF) over BM25 and SPLADE first-stage models, followed by adaptive re-ranking using monoELECTRA over a BM25 corpus graph. Results showed that Gen-PRF combined with adaptive re-ranking achieved the best P@10 and nDCG@10 performance, while SPLADE-based runs showed stronger recall and MAP. The zero-shot nature of generative methods made them competitive, especially for P@10 and nDCG@10, with adaptive re-ranking allowing BM25 to closely match SPLADE performance.

## Method Summary
The method applies generative relevance feedback using FLAN-T5 for query reformulation and pseudo-relevance feedback over BM25 and SPLADE first-stage retrieval models. Adaptive re-ranking is performed using monoELECTRA over a BM25 corpus graph with 5000 budget and 32 nearest neighbors. The pipeline processes queries through first-stage retrieval, optional generative expansion, monoELECTRA re-ranking, and corpus graph traversal to expand the candidate pool before final ranking.

## Key Results
- Gen-PRF combined with adaptive re-ranking (uogtr_b_grf_e_gb) achieved the best P@10 and nDCG@10 performance
- SPLADE-based runs showed stronger recall and MAP compared to BM25-based runs
- Adaptive re-ranking with sufficient compute budget allowed BM25 to closely match SPLADE performance
- Gen-QR degraded conversational query performance by generating answers instead of expansions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generative query reformulation (Gen-QR) improves retrieval by diversifying query terms to overcome lexical mismatch.
- Mechanism: Gen-QR uses a zero-shot prompt to a large language model (FLAN-T5) to generate expansion terms that are added to the original query, increasing term overlap with relevant documents.
- Core assumption: The LLM can generate expansion terms that are topically relevant without fine-tuning or access to the corpus.
- Evidence anchors:
  - [abstract] "We investigate generative approaches to relevance feedback utilising both generative query reformulation (Gen-QR) and pseudo-relevance feedback (Gen-PRF)"
  - [section] "Generative relevance feedback prompts a large language model (LLM) to suggest expansion terms to improve the performance of a retrieval model"
  - [corpus] Weak: No corpus evidence specifically on Gen-QR effectiveness; only general relevance feedback context.
- Break condition: The LLM generates terms unrelated to the query topic or directly answers the query instead of expanding it (as seen with conversational queries).

### Mechanism 2
- Claim: Generative pseudo-relevance feedback (Gen-PRF) improves retrieval by grounding expansion terms with context from an initial ranking.
- Mechanism: Gen-PRF provides the LLM with a ranked list of documents from a first-stage retrieval, which the model uses to generate expansion terms that are more topically focused.
- Core assumption: The top-ranked documents from the first-stage retrieval are topically relevant and provide sufficient context for meaningful expansion.
- Evidence anchors:
  - [abstract] "We applied generative query reformulation (Gen-QR) and pseudo-relevance feedback (Gen-PRF) over BM25 and SPLADE first-stage models"
  - [section] "when applying Gen-PRF, a ranking of ùêæ documents from a corpus ùê∑ by some score function ùë†, {ùë† (ùëë, ùëû0); ‚àÄùëë ‚àà ùê∑} is provided as topical context for the expansion"
  - [corpus] Weak: No corpus evidence on Gen-PRF performance; relies on paper results.
- Break condition: The initial ranking contains irrelevant documents, leading to poor expansion terms, or the LLM fails to interpret the context correctly.

### Mechanism 3
- Claim: Graph-based adaptive re-ranking (GAR) can converge first-stage retrieval performance by expanding the candidate pool through corpus graph traversal.
- Mechanism: GAR traverses a BM25 corpus graph using the highest-ranked documents as starting points, adding their nearest neighbors to the re-ranking pool until a compute budget is met, allowing lightweight first-stage models to match complex ones.
- Core assumption: The corpus graph structure and traversal budget are sufficient to find relevant documents that the first-stage model missed.
- Evidence anchors:
  - [abstract] "adaptive re-ranking using monoELECTRA over a BM25 corpus graph"
  - [section] "We investigate operating points of adaptive re-ranking with different first stages to find the point in graph traversal where the first stage no longer has an effect"
  - [corpus] Moderate: Corpus neighbors include related work on adaptive retrieval, but no direct evidence on convergence.
- Break condition: The corpus graph is too sparse or the traversal budget too small to find relevant candidates, or the re-ranker cannot effectively score the expanded pool.

## Foundational Learning

- Concept: Lexical vs semantic retrieval mismatch
  - Why needed here: The paper addresses the problem that BM25 relies on exact term overlap while neural models use semantic similarity, and Gen-QR/Gen-PRF aim to bridge this gap.
  - Quick check question: Why does BM25 struggle with queries like "global warming effects" when relevant documents use "climate change impacts"?

- Concept: Prompt engineering for zero-shot LLM usage
  - Why needed here: Gen-QR and Gen-PRF rely on carefully crafted prompts to the LLM to generate useful expansion terms without fine-tuning.
  - Quick check question: What is the key difference between the Gen-QR and Gen-PRF prompts in terms of input context?

- Concept: Corpus graph construction and traversal
  - Why needed here: GAR depends on a nearest-neighbor graph of the corpus and efficient traversal to expand the candidate set for re-ranking.
  - Quick check question: How does the choice of similarity metric (BM25 vs semantic) affect the structure of the corpus graph used in GAR?

## Architecture Onboarding

- Component map: BM25/SPLADE ‚Üí Gen-QR/Gen-PRF ‚Üí monoELECTRA ‚Üí GAR (optional) ‚Üí final ranking
- Critical path: First-stage retrieval ‚Üí (generative expansion) ‚Üí monoELECTRA re-ranker ‚Üí (GAR expansion) ‚Üí final output
- Design tradeoffs: Zero-shot generative methods vs learned sparse models; computational cost of GAR vs potential performance gains; prompt engineering complexity vs model fine-tuning.
- Failure signatures: Gen-QR generates conversational answers instead of expansions; GAR fails to improve recall when budget is too small; monoELECTRA overfits to first-stage ranking bias.
- First 3 experiments:
  1. Run BM25 ‚Üí monoELECTRA pipeline and measure baseline P@10/nDCG@10.
  2. Add Gen-QR before BM25 and compare performance to baseline.
  3. Apply GAR with a small budget (e.g., 1000) to BM25 ‚Üí monoELECTRA and observe convergence behavior.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of generative relevance feedback vary with query length or complexity beyond the directed/open/advice classification?
- Basis in paper: [explicit] The authors analyzed performance across query types (directed open, directed closed, advice) but didn't explore continuous measures of query complexity or length.
- Why unresolved: The analysis was limited to broad query type categories. More granular analysis of query characteristics could reveal additional patterns in when generative methods are most effective.
- What evidence would resolve it: Systematic analysis of generative feedback performance across queries with varying lengths, number of terms, syntactic complexity, or information need specificity.

### Open Question 2
- Question: What is the computational efficiency trade-off between generative relevance feedback and learned sparse models like SPLADE?
- Basis in paper: [inferred] The paper notes that generative methods are zero-shot and thus potentially valuable when labeled data is expensive, but doesn't provide detailed runtime or resource consumption comparisons.
- Why unresolved: While the paper discusses effectiveness, it doesn't quantify the computational costs of generating expansions versus training learned models, which is crucial for practical deployment.
- What evidence would resolve it: Head-to-head timing experiments measuring query processing time, GPU/CPU usage, and memory consumption for both approaches across varying query loads.

### Open Question 3
- Question: How does the performance of adaptive re-ranking change with different corpus graph sizes and neighbor counts?
- Basis in paper: [explicit] The authors used a BM25 corpus graph with 32 nearest neighbors and a re-ranking budget of 5000, but note that "further research is warranted to find the Pareto-optimal approach for smaller budgets."
- Why unresolved: The study used specific graph parameters without exploring how performance scales with different graph sizes, neighbor counts, or traversal strategies.
- What evidence would resolve it: Systematic experiments varying graph size (from small to full corpus), neighbor counts (from 5 to 100+), and re-ranking budgets to identify optimal configurations for different computational constraints.

## Limitations

- Gen-QR significantly degrades conversational query performance by generating answers instead of expansions
- Limited exploration of GAR convergence points with only a single budget setting tested
- Corpus graph construction methodology remains underspecified, making it difficult to assess whether performance gains are due to graph quality or increased compute budget

## Confidence

- High confidence: The core finding that Gen-PRF combined with adaptive re-ranking achieves best P@10 and nDCG@10 performance (supported by multiple runs and consistent across query types)
- Medium confidence: The claim that adaptive re-ranking allows BM25 to match SPLADE performance (based on limited budget exploration and unclear convergence criteria)
- Low confidence: The assertion that zero-shot generative methods are competitive with learned sparse models (limited ablation studies and no direct comparison to fine-tuned alternatives)

## Next Checks

1. **Query type ablation study**: Run Gen-QR and Gen-PRF separately on all query types (short, long, table, conversational) to quantify performance degradation patterns and identify failure modes
2. **GAR budget sensitivity analysis**: Systematically vary the corpus graph traversal budget from 1000 to 10000 in increments of 1000, measuring convergence points and RBO correlation to establish optimal operating points
3. **Corpus graph quality assessment**: Compare retrieval performance using BM25 vs semantic similarity metrics for corpus graph construction, and measure graph density and clustering quality to validate the foundation of adaptive re-ranking