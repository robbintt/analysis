---
ver: rpa2
title: Large Language Model for Qualitative Research -- A Systematic Mapping Study
arxiv_id: '2411.14473'
source_url: https://arxiv.org/abs/2411.14473
tags:
- llms
- qualitative
- were
- data
- studies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This systematic mapping study identifies 7 studies exploring Large
  Language Models (LLMs) for qualitative research across healthcare, education, and
  cultural domains. Studies predominantly used ChatGPT and LLaMA models with prompt
  engineering and fine-tuning, applying techniques like thematic analysis and content
  analysis to interview and document data.
---

# Large Language Model for Qualitative Research -- A Systematic Mapping Study

## Quick Facts
- **arXiv ID:** 2411.14473
- **Source URL:** https://arxiv.org/abs/2411.14473
- **Reference count:** 21
- **Primary result:** Systematic mapping study identifies 7 studies exploring LLMs for qualitative research across healthcare, education, and cultural domains, revealing opportunities and limitations in prompt engineering, hallucination risks, and evaluation metrics.

## Executive Summary
This systematic mapping study examines how Large Language Models (LLMs) are being applied to qualitative research, analyzing seven studies that explore their use in thematic analysis, content analysis, and other qualitative methods. The research identifies ChatGPT and LLaMA as the predominant models used, with prompt engineering and fine-tuning as key techniques. LLMs demonstrate comparable or superior performance to traditional methods in most cases, though significant limitations exist around prompt dependency, hallucination risks, and bias concerns. The study reveals critical research opportunities including improving prompt engineering techniques, reducing hallucinations, capturing semantic nuances, and developing standardized evaluation metrics for qualitative analysis.

## Method Summary
The authors conducted a systematic mapping study following Kitchenham and Charters methodology, searching four digital libraries (ACM Digital Library, IEEE Xplore, Scopus, and Web of Science) for studies on LLM applications in qualitative research. They identified 7 relevant studies after applying inclusion/exclusion criteria, extracting data on study characteristics, LLM models used, qualitative techniques applied, data sources, and evaluation methods. The analysis focused on identifying trends, research gaps, and limitations in current LLM applications for qualitative analysis.

## Key Results
- LLMs achieved comparable or superior results to traditional qualitative methods in most studies, with evaluations using accuracy, F1 score, and human comparison
- ChatGPT and LLaMA models were predominantly used, with prompt engineering and fine-tuning as the primary adaptation techniques
- Studies applied LLMs to thematic analysis, content analysis, and categorization of interview and document data across healthcare, education, and cultural domains
- Key limitations include dependence on well-structured prompts, hallucination risks, and bias concerns in model outputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs reduce time and effort required for qualitative analysis by automating coding and categorization processes
- Mechanism: LLMs can rapidly process and analyze text, performing open and axial coding in minutes rather than weeks or months
- Core assumption: The LLM has been properly fine-tuned or provided with well-structured prompts for the specific qualitative analysis task
- Evidence anchors:
  - [abstract] "LLMs offer transformative solutions to many challenges faced by traditional qualitative research methods"
  - [section] "LLMs can significantly reduce the effort and time required for qualitative analysis of large data volumes by automating steps that traditionally demand exhaustive human analysis"
  - [corpus] Weak evidence - corpus neighbors discuss LLMs in qualitative analysis but don't provide direct evidence for this specific mechanism
- Break condition: If prompts are poorly designed or the model lacks sufficient training data for the domain, automation may produce inaccurate or incomplete results

### Mechanism 2
- Claim: LLMs provide consistent and reproducible outputs, reducing subjectivity in qualitative analysis
- Mechanism: Unlike human coding which is prone to inconsistency and bias, LLMs apply the same analytical criteria uniformly across all data
- Core assumption: The LLM's training data and prompt engineering minimize bias in the analysis process
- Evidence anchors:
  - [abstract] "While human coding is prone to inconsistency, LLMs provide consistent and reproducible outputs"
  - [section] "LLMs provide consistent and reproducible outputs, especially when fine-tuned or used with standardized prompts"
  - [corpus] Weak evidence - corpus neighbors mention evaluation metrics but don't directly address consistency/reproducibility claims
- Break condition: If the LLM's training data contains inherent biases or if prompts inadvertently introduce bias, the consistency may perpetuate systematic errors

### Mechanism 3
- Claim: LLMs can handle vastly larger datasets than traditional human analysis methods
- Mechanism: LLMs can process millions of text documents in hours, whereas human analysts would require weeks or months for comparable analysis
- Core assumption: Computational resources and model capacity are sufficient for the scale of analysis required
- Evidence anchors:
  - [abstract] "Unlike human analysts, LLMs can handle vast amounts of unstructured data efficiently"
  - [section] "LLMs can handle vast amounts of unstructured data efficiently. For example, in healthcare studies, LLMs have been used to process patient feedback, identifying recurring themes in hours rather than weeks"
  - [corpus] Weak evidence - corpus neighbors discuss evaluation but don't provide specific evidence about scalability advantages
- Break condition: If the dataset exceeds the model's context window or if processing costs become prohibitive, scalability advantages may diminish

## Foundational Learning

- Concept: Systematic mapping study methodology
  - Why needed here: The paper uses SMS to identify trends and research gaps in LLM applications for qualitative research, requiring understanding of this research methodology
  - Quick check question: What distinguishes a systematic mapping study from a systematic literature review in terms of scope and purpose?

- Concept: Qualitative analysis techniques (thematic analysis, content analysis, grounded theory)
  - Why needed here: The paper evaluates how LLMs are used to support various qualitative analysis techniques, requiring familiarity with these methods
  - Quick check question: What are the key differences between thematic analysis and content analysis in qualitative research?

- Concept: Prompt engineering and fine-tuning concepts
  - Why needed here: The paper emphasizes the importance of well-structured prompts and model adaptations for effective LLM performance in qualitative analysis
  - Quick check question: How does prompt engineering differ from fine-tuning when adapting LLMs for specific tasks?

## Architecture Onboarding

- Component map: Data extraction pipeline (using ChatGPT for article screening) -> LLM selection/configuration module -> prompt engineering interface -> evaluation metrics engine -> results comparison framework
- Critical path: Data extraction → LLM configuration → Qualitative analysis execution → Effectiveness evaluation → Results comparison and reporting
- Design tradeoffs: Automation speed vs. accuracy trade-off; model complexity vs. interpretability; computational cost vs. analysis depth
- Failure signatures: Inconsistent outputs across similar inputs; "hallucinations" where model fabricates information; context sensitivity issues with subjective expressions
- First 3 experiments:
  1. Test prompt engineering effectiveness by running identical analysis with varying prompt structures and comparing output consistency
  2. Evaluate hallucination detection by deliberately introducing ambiguous or contradictory data and observing model responses
  3. Benchmark scalability by processing datasets of increasing size to identify computational limits and performance degradation points

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the optimal prompt engineering techniques to minimize hallucinations and biases in LLMs for qualitative analysis?
- Basis in paper: [explicit] The paper highlights that LLMs are highly dependent on well-structured prompts and can generate "hallucinations"—fabricated responses that lack basis in the data. It also notes the risk of inherent biases in the models.
- Why unresolved: The paper identifies these as significant limitations but does not provide specific solutions or optimal techniques for mitigating them.
- What evidence would resolve it: Empirical studies comparing different prompt engineering techniques and their effectiveness in reducing hallucinations and biases, along with standardized evaluation metrics.

### Open Question 2
- Question: How can LLMs be effectively integrated with human expertise to enhance the validity and reliability of qualitative analysis?
- Basis in paper: [explicit] The paper recommends that LLMs be used as an aid rather than a replacement for human analysts, suggesting their use in categorization stages with researchers responsible for final interpretation.
- Why unresolved: While the paper suggests integration, it does not provide detailed methodologies or frameworks for how this collaboration should be structured or implemented.
- What evidence would resolve it: Case studies or experimental designs demonstrating effective workflows that combine LLM outputs with human oversight, along with measures of improved validity and reliability.

### Open Question 3
- Question: What standardized evaluation metrics can be developed to capture the complexity of qualitative analysis performed by LLMs?
- Basis in paper: [explicit] The paper notes the need for developing a standardized and robust evaluation metric capable of capturing the complexity of qualitative analysis performed by LLMs.
- Why unresolved: Current metrics like accuracy, F1 score, and precision may not fully capture the nuanced aspects of qualitative research, and the paper does not propose specific alternatives.
- What evidence would resolve it: Development and validation of new evaluation frameworks tailored to qualitative research, tested across diverse datasets and compared with traditional methods.

## Limitations

- LLMs show heavy dependence on well-structured prompts and specific model configurations, with performance varying significantly based on prompt quality
- Hallucination remains a persistent concern, particularly when LLMs generate information not present in source materials
- The research reveals a lack of standardized evaluation metrics and frameworks for assessing LLM performance in qualitative analysis contexts

## Confidence

- **High Confidence**: Claims about LLM capabilities in reducing time and effort for qualitative analysis are well-supported by multiple studies showing consistent time savings and automation benefits.
- **Medium Confidence**: Claims about consistency and reproducibility show promise but lack comprehensive validation across diverse qualitative methodologies and research contexts.
- **Medium Confidence**: Scalability claims are supported by examples but lack systematic testing across different data types and volumes to establish definitive computational limits.

## Next Checks

1. **Prompt Engineering Validation**: Conduct controlled experiments testing identical qualitative analysis tasks with systematically varied prompt structures to quantify the impact of prompt engineering quality on LLM output consistency and accuracy.

2. **Hallucination Detection Framework**: Develop and test a standardized framework for detecting and measuring LLM hallucinations in qualitative analysis, including baseline error rates and detection methodologies.

3. **Semantic Nuance Assessment**: Design experiments specifically targeting the LLM's ability to capture and preserve semantic nuances, contextual sensitivity, and subjective expressions in qualitative data across different domains.