---
ver: rpa2
title: 'VGBench: Evaluating Large Language Models on Vector Graphics Understanding
  and Generation'
arxiv_id: '2407.10972'
source_url: https://arxiv.org/abs/2407.10972
tags:
- graphics
- vector
- graphviz
- tikz
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces VGBench, a comprehensive benchmark for evaluating
  large language models (LLMs) on vector graphics understanding and generation. VGBench
  includes 4,279 multi-choice question-answer pairs for understanding tasks and 5,845
  caption-vector graphics pairs for generation tasks across three formats: SVG, TikZ,
  and Graphviz.'
---

# VGBench: Evaluating Large Language Models on Vector Graphics Understanding and Generation

## Quick Facts
- arXiv ID: 2407.10972
- Source URL: https://arxiv.org/abs/2407.10972
- Reference count: 24
- Large language models show better performance on high-level vector graphics formats (TikZ, Graphviz) than low-level SVG, with GPT-4 achieving highest accuracy

## Executive Summary
This paper introduces VGBench, a comprehensive benchmark for evaluating large language models on vector graphics understanding and generation across three formats: SVG, TikZ, and Graphviz. The benchmark includes 4,279 multi-choice question-answer pairs for understanding tasks and 5,845 caption-vector graphics pairs for generation tasks. Through extensive evaluation of multiple LLMs including GPT-4, GPT-3.5, Llama-3, Qwen2, Phi-3, and Gemini, the study reveals that LLMs perform significantly better on higher-level semantic formats compared to low-level geometry-based formats. Chain-of-thought prompting shows substantial improvements for SVG tasks, while generation capabilities demonstrate strong performance with CLIP scores comparable to ground truth.

## Method Summary
VGBench employs a semi-automated pipeline for dataset creation, using GPT-4V for initial question generation followed by human filtering to ensure quality. The benchmark evaluates LLMs across three vector graphics formats (SVG, TikZ, Graphviz) using various prompting techniques including zero-shot, chain-of-thought, and in-context learning. Understanding tasks use multi-choice question-answer pairs while generation tasks pair captions with vector graphics outputs. Evaluation metrics include accuracy for understanding tasks and CLIP score/FID for generation tasks, providing comprehensive assessment of both semantic understanding and visual fidelity.

## Key Results
- LLMs demonstrate significantly better performance on high-level vector graphics formats (TikZ, Graphviz) compared to low-level SVG format
- Chain-of-thought prompting improves SVG task performance by approximately 7% compared to zero-shot prompting
- GPT-4 achieves the highest overall accuracy across all formats and tasks, with generation performance reaching CLIP scores comparable to ground truth

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chain-of-thought prompting significantly improves performance on SVG tasks compared to zero-shot prompting.
- Mechanism: CoT prompting enables LLMs to reason step-by-step through low-level geometry primitives by breaking down complex visual understanding into smaller logical steps.
- Core assumption: LLMs can better parse low-level vector graphics code when explicitly guided through a reasoning process.
- Evidence anchors:
  - [abstract] "Chain-of-thought prompting significantly improves performance on SVG tasks."
  - [section] "Chain of Thought (CoT) and In-Context Learning (ICL) show some performance improvements for some tasks, but not significant. CoT and ICL show ∼7% performance boost for SVG which owns lowest performance among three formats."
  - [corpus] Weak - corpus focuses on symbolic graphics programming but doesn't specifically address CoT effectiveness on low-level formats.

### Mechanism 2
- Claim: LLMs perform better on high-level vector graphics formats (TikZ, Graphviz) than low-level formats (SVG).
- Mechanism: High-level vector graphics formats contain more semantic information aligned with LLMs' training data (natural language), making them more interpretable.
- Core assumption: Training data alignment with semantic content improves model performance.
- Evidence anchors:
  - [abstract] "LLMs show much better vector graphic understanding capability in TikZ and Graphviz than SVGs. TikZ and Graphviz include more high-level semantics compared to SVG, which is composed of low-level geometry primitives."
  - [section] "GPT-4 shows stronger performance in high-level vector graphics language (e.g., TikZ, Graphviz) compared to low-level vector graphics language SVG."
  - [corpus] Weak - corpus mentions symbolic graphics but doesn't provide comparative analysis across format levels.

### Mechanism 3
- Claim: Vector graphics generation performance correlates with semantic level of the format.
- Mechanism: Higher-level formats provide more structured semantic information that guides generation, while low-level formats require more precise geometric interpretation.
- Core assumption: Semantic richness in format improves generation quality.
- Evidence anchors:
  - [abstract] "LLMs show strong vector graphics generation ability on TikZ and Graphviz format compared to SVG format, hinting that TikZ or Graphviz might be a better medium for LLMs to manipulate vector graphics."
  - [section] "Both GPT-3.5 and GPT-4 show strong vector graphics generation capability. Both LLMs show similar CLIP score as the ground truth."
  - [corpus] Weak - corpus focuses on generation but doesn't provide comparative analysis across semantic levels.

## Foundational Learning

- Concept: Vector graphics formats (SVG, TikZ, Graphviz)
  - Why needed here: Understanding the structural and semantic differences between formats is crucial for interpreting performance results and designing appropriate evaluation methods.
  - Quick check question: What are the key differences between SVG's geometry-primitive-based representation and TikZ/Graphviz's semantic-based representation?

- Concept: Prompt engineering techniques (zero-shot, few-shot, CoT)
  - Why needed here: Different prompting techniques have varying effectiveness across different vector graphics formats and task types.
  - Quick check question: How does chain-of-thought prompting specifically help with low-level vector graphics understanding compared to zero-shot prompting?

- Concept: Evaluation metrics (CLIP Score, FID, accuracy)
  - Why needed here: Understanding how different metrics capture different aspects of model performance is essential for proper benchmark interpretation.
  - Quick check question: Why use both CLIP Score and FID for vector graphics generation evaluation instead of just one metric?

## Architecture Onboarding

- Component map: Vector graphics data collection → Semi-automated QA generation → Human filtering → LLM evaluation → Metric computation → Analysis
- Critical path: Vector graphics → Prompt generation → LLM response → Answer validation → Metric calculation
- Design tradeoffs: Semi-automated QA generation trades some quality control for scalability, while human filtering ensures benchmark reliability at increased cost.
- Failure signatures: Poor performance on low-level formats indicates model limitations in geometric understanding; inconsistent prompting technique effectiveness suggests model sensitivity to instruction format.
- First 3 experiments:
  1. Test CoT prompting effectiveness on SVG format with simple geometric questions
  2. Compare performance across formats using same prompting technique and question type
  3. Evaluate generation quality using both CLIP Score and FID metrics on same caption across formats

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs on vector graphics understanding tasks scale with the length of the vector graphics code?
- Basis in paper: [explicit] The paper discusses the impact of vector graphics sequence length on understanding performance in Section 3.5, noting that SVG format is most sensitive to length increases.
- Why unresolved: The paper only provides a limited analysis of sequence length effects and does not explore the underlying mechanisms or potential strategies to mitigate length-related performance degradation.
- What evidence would resolve it: Comprehensive experiments systematically varying vector graphics code length across different formats and question types, coupled with ablation studies to identify key factors contributing to performance changes.

### Open Question 2
- Question: Can more advanced prompting techniques beyond Chain-of-Thought and In-Context Learning further improve LLM performance on vector graphics tasks?
- Basis in paper: [explicit] The paper mentions that recent works propose more prompting techniques like Tree of Thoughts (ToT) and Everything of Thoughts (XoT), which could potentially enhance performance.
- Why unresolved: The paper only evaluates two common prompting techniques and does not explore the potential benefits of more recent or sophisticated approaches.
- What evidence would resolve it: Experiments comparing the performance of LLMs on vector graphics tasks using a range of prompting techniques, including ToT, XoT, and other emerging methods, to determine their relative effectiveness.

### Open Question 3
- Question: How do the performance and capabilities of LLMs on vector graphics tasks compare to specialized vision-language models (VLMs) that operate on rasterized representations?
- Basis in paper: [explicit] The paper includes a comparison between LLMs and VLMs on rasterized representations in Section 3.5, noting interesting differences in performance across vector graphics formats.
- Why unresolved: The paper provides a limited comparison and does not explore the reasons behind the performance differences or the potential advantages and disadvantages of each approach.
- What evidence would resolve it: In-depth analysis of the strengths and weaknesses of LLMs and VLMs on various vector graphics tasks, including qualitative studies of their reasoning processes and error patterns.

## Limitations

- Dataset creation methodology lacks detailed validation of human filtering quality and inter-rater reliability
- Evaluation metrics (CLIP score, FID) may not fully capture semantic fidelity of vector graphics
- Benchmark focuses on symbolic graphics, limiting generalization to other vector graphics domains

## Confidence

**High Confidence**: Comparative performance differences between high-level (TikZ, Graphviz) and low-level (SVG) formats are well-supported by multiple experiments and consistent across different prompting techniques.

**Medium Confidence**: Effectiveness of chain-of-thought prompting for SVG tasks is demonstrated but the magnitude of improvement (~7%) suggests the effect may be more modest than implied.

**Low Confidence**: Claims about TikZ/Graphviz being fundamentally "better mediums" for LLM manipulation extrapolate beyond the evidence and may reflect training data alignment rather than inherent format superiority.

## Next Checks

1. **Prompt sensitivity analysis**: Systematically vary prompt phrasing, length, and structure across all three vector graphics formats to determine whether performance differences persist under controlled prompting conditions.

2. **Human evaluation correlation**: Conduct human perceptual studies comparing LLM-generated vector graphics against ground truth across all formats, measuring both semantic fidelity and geometric accuracy, then correlate these judgments with automated metrics.

3. **Cross-domain generalization test**: Evaluate the same LLMs on VGBench using vector graphics from different domains (technical drawings, artistic illustrations, UI elements) to determine whether format-based performance patterns hold across diverse visual content types.