---
ver: rpa2
title: 'LongWanjuan: Towards Systematic Measurement for Long Text Quality'
arxiv_id: '2402.13583'
source_url: https://arxiv.org/abs/2402.13583
tags:
- texts
- long
- data
- text
- longwanjuan
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a systematic approach to measure long text
  quality through three linguistic dimensions: coherence, cohesion, and complexity.
  The authors propose a suite of metrics based on these dimensions to quantitatively
  assess long text quality, encompassing both statistical and pre-trained language
  model-based methods.'
---

# LongWanjuan: Towards Systematic Measurement for Long Text Quality

## Quick Facts
- arXiv ID: 2402.13583
- Source URL: https://arxiv.org/abs/2402.13583
- Reference count: 19
- The paper introduces a systematic approach to measure long text quality through linguistic dimensions, achieving 13.07% improvement over baseline on Longbench and state-of-the-art performance at 7B parameter scale.

## Executive Summary
This paper presents LongWanjuan, a systematic framework for measuring and improving long text quality through three linguistic dimensions: coherence, cohesion, and complexity. The authors develop a suite of metrics based on these dimensions to quantitatively assess long text quality, using both statistical and pre-trained language model-based methods. They construct a massive bilingual dataset with over 160B tokens and categorize long texts into holistic, aggregated, and chaotic types. By applying a carefully designed data mixture recipe, they demonstrate significant improvements in model performance on long-text tasks, achieving state-of-the-art results at the 7B parameter scale.

## Method Summary
The authors propose a systematic approach to long text quality measurement by defining three linguistic dimensions: coherence (logical flow between sentences), cohesion (grammatical and lexical connections), and complexity (structural intricacy). For each dimension, they develop multiple metrics including traditional statistical measures and advanced methods using pre-trained language models. Using these metrics, they analyze and categorize long texts into three quality types: holistic (high quality, well-structured), aggregated (mixed quality with some strong sections), and chaotic (low quality, disorganized). They then construct LongWanjuan, a large-scale bilingual dataset, by carefully mixing these text types according to a data mixture recipe designed to optimize model performance on long-text tasks.

## Key Results
- Achieves 13.07% improvement over untrained baseline on Longbench benchmark
- Establishes new state-of-the-art performance for long-text models at 7B parameter scale
- Constructs LongWanjuan dataset with over 160B tokens across bilingual corpora

## Why This Works (Mechanism)
The framework works by providing a systematic way to quantify and categorize long text quality through linguistic dimensions. By measuring coherence, cohesion, and complexity separately, the approach can identify specific weaknesses in text quality rather than relying on aggregate scores. The data mixture recipe leverages this granular understanding to create training data that balances different quality types, preventing models from overfitting to either too clean or too noisy examples. This balanced approach allows models to better handle the full spectrum of real-world long text variations.

## Foundational Learning
- **Linguistic quality dimensions**: Understanding text quality requires breaking it down into measurable components (coherence, cohesion, complexity) because quality is multi-faceted and cannot be captured by single metrics.
- **Text categorization methodology**: Systematic categorization of text into holistic, aggregated, and chaotic types enables targeted data selection and mixing strategies for optimal model training.
- **Data mixture optimization**: The mixture recipe balances different text quality types to prevent model overfitting and improve generalization across the full spectrum of long text variations.

## Architecture Onboarding
- **Component map**: Text quality metrics (coherence/cohesion/complexity) -> Text categorization (holistic/aggregated/chaotic) -> Data mixture recipe -> Model training -> Performance evaluation
- **Critical path**: Quality metric computation → Text type classification → Mixture ratio determination → Data curation → Model fine-tuning → Benchmark testing
- **Design tradeoffs**: The framework trades computational complexity of multiple quality metrics for more precise text categorization, which in turn enables better data mixing but requires more sophisticated curation.
- **Failure signatures**: Poor performance indicates either inadequate metric sensitivity (missing quality distinctions), incorrect categorization (wrong text type assignments), or suboptimal mixture ratios (imbalanced training data).
- **3 first experiments**: 1) Validate metric correlation with human judgments of text quality, 2) Test individual text type contributions to performance gains, 3) Evaluate model performance on additional long-text benchmarks.

## Open Questions the Paper Calls Out
None

## Limitations
- The linguistic dimensions chosen (coherence, cohesion, complexity) represent one theoretical perspective, though alternative frameworks exist in linguistics literature.
- Limited validation of metrics against human judgment raises questions about their effectiveness at capturing true text quality.
- The data mixture recipe's effectiveness lacks detailed ablation studies to isolate which components contribute most to improvements.
- The "state-of-the-art" claim is qualified to the 7B parameter scale and should be more prominently stated.

## Confidence
- High confidence: The technical implementation of the three linguistic dimensions and associated metrics is well-described and reproducible.
- Medium confidence: The correlation between the proposed metrics and actual text quality, given limited validation against human judgments.
- Low confidence: The generalizability of the data mixture recipe beyond the specific experimental setup.

## Next Checks
1. Conduct human evaluation studies to validate that the proposed metrics (coherence, cohesion, complexity) align with human judgments of long text quality.
2. Perform ablation studies on the data mixture recipe to identify which components (holistic, aggregated, chaotic) contribute most to performance improvements.
3. Test the LongWanjuan dataset and trained models on additional long-text benchmarks beyond Longbench to assess generalizability.