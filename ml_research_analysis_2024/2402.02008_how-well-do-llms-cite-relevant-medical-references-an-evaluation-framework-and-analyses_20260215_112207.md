---
ver: rpa2
title: How well do LLMs cite relevant medical references? An evaluation framework
  and analyses
arxiv_id: '2402.02008'
source_url: https://arxiv.org/abs/2402.02008
tags: []
core_contribution: This paper introduces SourceCheckup, an automated evaluation framework
  to assess whether large language models (LLMs) provide accurate sources for medical
  claims. It leverages GPT-4 to generate medical questions from reference webpages,
  obtain model responses, and verify whether provided sources support the claims.
---

# How well do LLMs cite relevant medical references? An evaluation framework and analyses

## Quick Facts
- arXiv ID: 2402.02008
- Source URL: https://arxiv.org/abs/2402.02008
- Reference count: 19
- Primary result: Evaluates LLM citation accuracy in medical contexts using automated framework SourceCheckup

## Executive Summary
This paper introduces SourceCheckup, an automated framework for evaluating whether large language models provide accurate sources for medical claims. The framework uses GPT-4 to generate medical questions from reference webpages, obtain model responses, and verify whether provided sources support the claims. It achieves 88% agreement with annotations from three US-licensed medical doctors. When applied to evaluate five top-performing LLMs on 1200 questions, the framework reveals that 50% to 90% of responses are not fully supported by their cited sources, even with retrieval-augmented generation.

## Method Summary
SourceCheckup is an automated evaluation framework that assesses LLM citation accuracy for medical claims. The framework uses GPT-4 in three sequential roles: first to generate medical questions from reference webpages, then to obtain responses from target LLMs, and finally to verify whether the sources provided by those LLMs actually support their claims. The framework is validated against physician annotations, achieving 88% agreement, and is applied to evaluate five leading LLMs on a dataset of 1200 questions, revealing substantial gaps between claimed and supported medical information.

## Key Results
- 50% to 90% of LLM responses fail to be fully supported by their cited sources
- GPT-4 with retrieval augmentation fails to fully support nearly half of its responses
- Framework achieves 88% agreement with three US-licensed medical doctors
- Significant reliability gaps identified in clinical applications of LLMs

## Why This Works (Mechanism)
The framework works by leveraging GPT-4's reasoning capabilities to systematically break down the citation verification process into three discrete steps: question generation from medical references, response generation from target LLMs, and claim-source validation. This decomposition allows for consistent, scalable evaluation of citation accuracy across different models and domains.

## Foundational Learning
- **Automated medical question generation**: Needed to create consistent evaluation datasets without manual curation. Quick check: Generated questions should cover diverse medical topics and complexity levels.
- **Citation verification methodology**: Essential for determining whether sources actually support claims made by LLMs. Quick check: Validation against human experts to establish reliability thresholds.
- **Retrieval-augmented generation assessment**: Critical for understanding how RAG systems perform in medical contexts. Quick check: Compare performance with and without retrieval augmentation.
- **Medical domain expertise integration**: Required to ensure clinical accuracy and relevance. Quick check: Physician agreement rates above 85% for validation tasks.

## Architecture Onboarding
- **Component map**: GPT-4 (question generator) -> LLMs (response generator) -> GPT-4 (source verifier) -> Physician annotators (validation)
- **Critical path**: Reference webpage → Generated question → LLM response with sources → Source verification → Agreement validation
- **Design tradeoffs**: Using single GPT-4 for multiple roles reduces complexity but may introduce bias; physician validation provides gold standard but is expensive and limited in scale
- **Failure signatures**: High false positive rates in source verification indicate model-specific citation patterns; disagreement with physicians reveals systematic evaluation biases
- **First experiments**: 1) Run framework with different question generation models to test bias; 2) Compare physician agreement rates across medical specialties; 3) Evaluate performance on non-medical domains to test generalizability

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- GPT-4 used for both question generation and evaluation may introduce systematic bias
- Validation relies on only three physicians, limiting generalizability across medical perspectives
- Framework focuses on source support verification rather than assessing source quality or clinical appropriateness
- 12% disagreement rate with physicians suggests potential systematic errors in automated evaluation

## Confidence
- **High** confidence in methodology for automated evaluation framework
- **Medium** confidence in clinical relevance due to limited physician sample size
- **Medium** confidence in citation support claims given potential GPT-4 bias and validation constraints

## Next Checks
1. Validate framework with larger, more diverse physician group (minimum 10 physicians across specialties) to assess inter-rater reliability and identify systematic biases
2. Implement cross-validation using different LLM models for question generation versus evaluation to reduce model-specific bias
3. Conduct detailed error analysis on 12% of cases where physician annotations disagree with framework assessment to understand discrepancy nature and implications