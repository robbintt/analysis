---
ver: rpa2
title: 'Benchmarking the Capabilities of Large Language Models in Transportation System
  Engineering: Accuracy, Consistency, and Reasoning Behaviors'
arxiv_id: '2408.08302'
source_url: https://arxiv.org/abs/2408.08302
tags:
- claude
- llms
- time
- sonnet
- problems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TransportBench, a new dataset for benchmarking
  large language models (LLMs) on undergraduate-level transportation engineering problems.
  The dataset contains 140 problems covering topics such as transportation economics,
  driver characteristics, vehicle motion, road geometry design, traffic flow/control,
  transportation planning, utility/modal split, transportation networks, and public
  transit systems.
---

# Benchmarking the Capabilities of Large Language Models in Transportation System Engineering: Accuracy, Consistency, and Reasoning Behaviors

## Quick Facts
- arXiv ID: 2408.08302
- Source URL: https://arxiv.org/abs/2408.08302
- Reference count: 6
- Seven leading LLMs evaluated on undergraduate-level transportation engineering problems, with Claude 3.5 Sonnet achieving highest overall accuracy (67.1%)

## Executive Summary
This paper introduces TransportBench, a new dataset for benchmarking large language models (LLMs) on undergraduate-level transportation engineering problems. The dataset contains 140 problems covering topics such as transportation economics, driver characteristics, vehicle motion, road geometry design, traffic flow/control, transportation planning, utility/modal split, transportation networks, and public transit systems. The authors evaluate seven leading LLMs (GPT-4, GPT-4o, Claude 3 Opus, Claude 3.5 Sonnet, Gemini 1.5 Pro, Llama 3, and Llama 3.1) on TransportBench in terms of accuracy, consistency, and reasoning behaviors. Claude 3.5 Sonnet achieves the highest overall accuracy (67.1%), while GPT-4 and GPT-4o demonstrate the most consistent performance in self-checking settings. The study reveals that while LLMs show promise in solving basic transportation problems, they struggle with more complex tasks requiring deeper understanding of underlying physical processes and subtle concepts. The authors highlight the need for improved reasoning and explanatory capabilities in LLMs for practical applications in transportation engineering.

## Method Summary
The study evaluates seven leading LLMs on the TransportBench dataset using zero-shot prompting. The 140 problems cover undergraduate-level transportation engineering topics. Human experts evaluate LLM responses for accuracy, consistency (uniform responses across different inquiries), and reasoning quality. The authors compare models using accuracy metrics and analyze reasoning behaviors, including the impact of self-checking prompts and domain-specific prompts on performance.

## Key Results
- Claude 3.5 Sonnet achieves the highest overall accuracy (67.1%) on TransportBench
- GPT-4 and GPT-4o demonstrate the most consistent performance in self-checking settings
- LLMs show promise in solving basic transportation problems but struggle with complex tasks requiring deeper conceptual understanding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs solve domain-specific problems by leveraging in-context reasoning rather than domain knowledge.
- Mechanism: The model matches problem patterns to training examples, applies symbolic manipulation, and generates step-by-step reasoning text. Accuracy depends on the richness of the problem description and the presence of explicit reasoning prompts.
- Core assumption: The LLM's pre-training corpus includes sufficient domain-agnostic reasoning patterns to generalize to new technical contexts.
- Evidence anchors:
  - [abstract] "Our study also identifies the unique strengths and limitations of each LLM, e.g. our analysis shows both the impressive overall accuracy and some unexpected inconsistent behaviors of Claude 3.5 Sonnet when tested on TransportBench."
  - [section] "We notice that an LLM that does better on CEE 310 does not necessarily attain higher ACC on CEE 418."
  - [corpus] Weak: No direct evidence of reasoning capability from training data; only task-level performance.
- Break condition: When problems require deep conceptual understanding beyond surface-level pattern matching (e.g., subtle distinctions in technical definitions).

### Mechanism 2
- Claim: Self-checking prompts can improve or degrade performance depending on the model's consistency.
- Mechanism: The LLM re-evaluates its own reasoning when prompted, potentially catching errors or introducing new ones. The effect depends on the model's internal consistency in reasoning steps.
- Core assumption: LLMs maintain a coherent internal state that can be re-examined and corrected through additional prompts.
- Evidence anchors:
  - [abstract] "Interestingly, we observe that Claude 3.5 Sonnet and Claude 3 Opus give less consistent answers when asked to double check their own solutions â€“ this suggests a lack of deep conceptual understanding."
  - [section] "GPT-4o and GPT-4 are the only two models whose accuracy is benefited from the self-checking prompts."
  - [corpus] Weak: No evidence of internal state consistency from training data.
- Break condition: When the model's reasoning path is unstable or when self-checking introduces cognitive interference.

### Mechanism 3
- Claim: Domain-specific prompts can guide reasoning toward correct conclusions by activating relevant knowledge structures.
- Mechanism: Targeted prompts activate specific conceptual frameworks or problem-solving strategies that the model has learned, improving reasoning accuracy.
- Core assumption: The model's training included diverse problem-solving contexts that can be selectively activated by specific prompts.
- Evidence anchors:
  - [section] "Simple Domain-Specific Prompts: Problem 4.5 from TransportBench" demonstrates that prompts about "rolling resistance at different speed values" corrected Claude 3.5 Sonnet's reasoning.
  - [section] "Reasoning Matters: Adding Prompts to Problem 6.5 from TransportBench" shows that asking for "detailed reasoning before giving the final conclusion" improved Claude 3 Opus's accuracy.
  - [corpus] Weak: No direct evidence that training data contained such specific domain knowledge structures.
- Break condition: When the required conceptual framework was not present in the training data or when prompts conflict with the model's default reasoning approach.

## Foundational Learning

- Concept: Domain-specific technical definitions and relationships
  - Why needed here: LLMs struggle with subtle technical distinctions (e.g., "headway" vs "space headway") that are critical for correct problem-solving in transportation engineering
  - Quick check question: Can you explain the difference between time headway and space headway in traffic flow theory?

- Concept: Problem decomposition and step-by-step reasoning
  - Why needed here: Complex transportation problems require breaking down into sub-problems and applying appropriate formulas/methods at each step
  - Quick check question: How would you decompose a transit network design problem into manageable sub-problems?

- Concept: Mathematical modeling of physical systems
  - Why needed here: Many transportation problems involve modeling physical relationships (e.g., aerodynamic vs rolling resistance) that require correct mathematical formulations
  - Quick check question: What mathematical relationship describes the relationship between vehicle speed and aerodynamic resistance?

## Architecture Onboarding

- Component map: TransportBench dataset -> LLM inference engine -> human expert evaluation pipeline -> consistency and reasoning analysis modules
- Critical path: Problem ingestion -> prompt generation -> LLM response -> accuracy evaluation -> consistency check -> reasoning analysis
- Design tradeoffs: Zero-shot prompting (simpler, less accurate) vs. few-shot prompting (more complex, potentially more accurate); general reasoning vs. domain-specific prompting
- Failure signatures: Inconsistent answers across trials, incorrect reasoning paths despite correct final answers, inability to handle problems requiring deep conceptual understanding
- First 3 experiments:
  1. Test each LLM on TransportBench with zero-shot prompting, measure accuracy and consistency
  2. Apply self-checking prompts to all LLMs and compare accuracy changes
  3. Apply domain-specific reasoning prompts to identify which types of problems benefit most from guided reasoning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can domain-specific prompting be systematically designed to improve LLM reasoning for complex transportation engineering problems?
- Basis in paper: [explicit] The authors note that simple domain-specific prompts can improve reasoning (e.g., the rolling resistance and space-mean speed problems), but suggest this needs systematic development.
- Why unresolved: The paper provides examples of effective prompts but does not establish a comprehensive framework for designing such prompts across different problem types.
- What evidence would resolve it: Development and validation of a systematic taxonomy of domain-specific prompts that consistently improves LLM performance across diverse transportation engineering problem categories.

### Open Question 2
- Question: What is the optimal balance between zero-shot prompting and fine-tuning for achieving reliable LLM performance on transportation engineering tasks?
- Basis in paper: [inferred] The authors use zero-shot prompting and achieve varying accuracy levels (55-67% overall), while noting that fine-tuning could potentially improve results.
- Why unresolved: The paper does not compare zero-shot performance with fine-tuned models or explore how much improvement could be achieved through different fine-tuning approaches.
- What evidence would resolve it: Comparative study showing accuracy improvements when fine-tuning models on transportation-specific data versus zero-shot prompting, along with analysis of cost-benefit trade-offs.

### Open Question 3
- Question: How can LLM reasoning errors be detected and corrected automatically without human intervention?
- Basis in paper: [explicit] The authors demonstrate that self-checking prompts can sometimes correct errors but also introduce inconsistencies, highlighting the need for better automated reasoning validation.
- Why unresolved: Current approaches rely on human experts to evaluate reasoning quality, and self-checking can introduce new errors.
- What evidence would resolve it: Development of automated reasoning verification tools that can identify logical inconsistencies and correct them without introducing new errors, validated on the TransportBench dataset.

### Open Question 4
- Question: What are the fundamental limitations of current LLMs in understanding physical processes underlying transportation engineering problems?
- Basis in paper: [explicit] The authors note that LLMs struggle with problems requiring deeper understanding of underlying physical processes, such as the bus fleet size problem where they ignore vehicle availability dynamics.
- Why unresolved: The paper identifies this as a limitation but does not systematically investigate the cognitive or architectural reasons for these failures.
- What evidence would resolve it: Systematic analysis identifying specific types of physical reasoning tasks where LLMs consistently fail, along with architectural modifications or training approaches that could address these limitations.

## Limitations
- Evaluation scope limited to undergraduate-level problems, potentially missing complex real-world challenges
- Zero-shot prompting approach may underestimate LLM capabilities compared to fine-tuning
- Human expert evaluation criteria not fully specified, introducing potential subjectivity

## Confidence

**High Confidence Claims**:
- Claude 3.5 Sonnet achieves the highest overall accuracy (67.1%) on TransportBench
- GPT-4 and GPT-4o demonstrate the most consistent performance in self-checking settings
- LLMs show promise in solving basic transportation problems but struggle with complex tasks requiring deeper conceptual understanding

**Medium Confidence Claims**:
- Self-checking prompts can improve or degrade performance depending on the model's consistency
- Domain-specific prompts can guide reasoning toward correct conclusions
- LLMs rely more on in-context reasoning than domain-specific knowledge

**Low Confidence Claims**:
- The specific mechanisms by which LLMs process transportation engineering problems
- The extent to which observed behaviors generalize beyond the TransportBench dataset

## Next Checks

1. **Expanded Dataset Validation**: Test the same LLMs on a broader range of transportation engineering problems, including graduate-level and real-world case studies, to assess whether performance patterns hold across difficulty levels and problem types.

2. **Prompt Engineering Study**: Systematically test different prompting strategies (few-shot, chain-of-thought, and domain-specific templates) to quantify the impact of prompt design on accuracy and consistency across all seven LLMs.

3. **Error Analysis Framework**: Conduct a detailed error analysis categorizing incorrect responses by type (conceptual misunderstanding, mathematical error, misinterpretation of problem statement) to identify specific areas where LLM reasoning breaks down in transportation engineering contexts.