---
ver: rpa2
title: Minimum number of neurons in fully connected layers of a given neural network
  (the first approximation)
arxiv_id: '2405.14147'
source_url: https://arxiv.org/abs/2405.14147
tags:
- network
- number
- neurons
- layer
- minimum
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an algorithm for estimating the minimum number
  of neurons in fully connected layers of a given neural network architecture without
  requiring multiple training runs with different layer sizes. The method uses cross-validation
  to train an initial wide network, then inserts truncated SVD autoencoders after
  the studied layer and checks statistical equivalence between the original and reduced
  networks.
---

# Minimum number of neurons in fully connected layers of a given neural network (the first approximation)

## Quick Facts
- arXiv ID: 2405.14147
- Source URL: https://arxiv.org/abs/2405.14147
- Authors: Oleg I. Berngardt
- Reference count: 22
- One-line primary result: Proposes algorithm to estimate minimum number of neurons in fully connected layers without multiple training runs, using cross-validation and truncated SVD autoencoders

## Executive Summary
This paper introduces a novel algorithm for estimating the minimum number of neurons required in fully connected layers of neural networks. The method uses cross-validation to train an initial wide network, then applies truncated SVD autoencoders to find the minimal neuron count while maintaining statistical equivalence. The approach suggests that the minimum number of neurons is an internal property of the solution determined by network architecture, training data, and quality metrics, rather than a hyperparameter requiring extensive tuning.

## Method Summary
The algorithm trains a sufficiently wide neural network using cross-validation with at least 2 folds. For each fold, it calculates the outputs of the studied layer, performs truncated SVD decomposition, and inserts an SVD autoencoder after the studied layer with varying truncation levels. Statistical equivalence between original and reduced networks is checked using bootstrap resampling on non-overlapping data folds to find the minimum truncation level where equivalence holds. The method requires the initial network width to be at least 3x the expected minimum and uses a quality metric threshold to determine equivalence.

## Key Results
- Method finds smaller, equivalent networks compared to universal bounds
- Minimum number of neurons is an internal property of the solution, not a hyperparameter
- Method is stable and effective at estimating minimum neurons for each layer independently
- Tested successfully on MNIST and other datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method finds a minimum number of neurons by treating the output of each hidden layer as a matrix and estimating its rank via truncated SVD.
- Mechanism: During cross-validation, the network is trained multiple times on different data folds. For each fold, the outputs of the target hidden layer form a matrix. Truncated SVD decomposition of this matrix gives an estimate of its effective rank, which corresponds to the minimal number of neurons needed to preserve the mapping.
- Core assumption: The rank of the hidden layer output matrix directly indicates the minimum number of neurons required in that layer.
- Evidence anchors:
  - [abstract]: "by using truncated singular value decomposition autoencoder inserted after the studied layer...we search the minimum number of neurons in inference only mode of the network."
  - [section]: "The task of finding the minimum number of neurons in each hidden layer...can be reduced to search for the rank(Y(n)) : rank of layer outputs at this dataset."
  - [corpus]: No direct evidence in corpus; method is distinct from typical SVD uses in neural networks.
- Break condition: If the activation functions create complex nonlinear dependencies that SVD cannot capture, the rank estimate may not correspond to the true minimal number of neurons.

### Mechanism 2
- Claim: Statistical equivalence between the original and reduced networks is verified without retraining the reduced network.
- Mechanism: The truncated SVD autoencoder is inserted after the studied layer of the trained network. The quality of the reconstructed outputs is compared to the original outputs on held-out data. If the difference is below a threshold, the networks are considered equivalent.
- Core assumption: A network with fewer neurons can replicate the original network's output if the difference in quality metric is below a predefined threshold.
- Evidence anchors:
  - [abstract]: "checks statistical equivalence between the original and reduced networks."
  - [section]: "The statistical equivalence of new and original networks is determined by the quality metric reaching a threshold value."
  - [corpus]: No direct evidence in corpus; method is distinct from typical network compression methods.
- Break condition: If the threshold is set too high, the reduced network may not be functionally equivalent. If too low, the method may fail to find a reduced network.

### Mechanism 3
- Claim: The minimum number of neurons is an internal property of the solution, not a hyperparameter.
- Mechanism: By finding the minimum number of neurons independently for each layer, the method demonstrates that this number is determined by the network architecture, training data, and quality metric, rather than being a tunable hyperparameter.
- Core assumption: The minimum number of neurons is intrinsic to the problem and does not depend on the initial width of the network.
- Evidence anchors:
  - [abstract]: "the minimum number of neurons is an internal (latent) property of the solution, determined by the network architecture, the training dataset, layer position, and the quality metric used."
  - [section]: "the minimum number of neurons can be estimated for each hidden fully connected layer independently."
  - [corpus]: No direct evidence in corpus; method is distinct from typical hyperparameter tuning approaches.
- Break condition: If the network architecture or training data changes significantly, the minimum number of neurons may also change, suggesting it is not entirely intrinsic.

## Foundational Learning

- Concept: Singular Value Decomposition (SVD)
  - Why needed here: SVD is used to estimate the rank of the hidden layer output matrix, which corresponds to the minimum number of neurons needed.
  - Quick check question: What does the rank of a matrix represent in the context of SVD?

- Concept: Cross-validation
  - Why needed here: Cross-validation is used to train multiple versions of the network on different data folds, providing a robust estimate of the minimum number of neurons.
  - Quick check question: How does cross-validation help in estimating the stability of the minimum number of neurons?

- Concept: Statistical equivalence
  - Why needed here: Statistical equivalence is used to verify that the reduced network can replicate the original network's output without retraining.
  - Quick check question: What metrics can be used to measure the statistical equivalence between two networks?

## Architecture Onboarding

- Component map:
  - Original network (trained with cross-validation) -> Truncated SVD autoencoder (inserted after studied layer) -> Statistical equivalence check (compares outputs)

- Critical path:
  1. Train the original network using cross-validation
  2. Compute the SVD decomposition of the hidden layer outputs for each fold
  3. Insert the truncated SVD autoencoder and check statistical equivalence
  4. Find the minimum number of neurons that satisfies the equivalence criterion

- Design tradeoffs:
  - Computational cost: Cross-validation and SVD decomposition can be computationally expensive
  - Accuracy: The threshold for statistical equivalence affects the accuracy of the reduced network
  - Stability: The method may be sensitive to the choice of data folds and network architecture

- Failure signatures:
  - The reduced network fails to achieve the required accuracy
  - The method is sensitive to the choice of data folds
  - The computational cost is too high for practical use

- First 3 experiments:
  1. Test the method on a simple dataset with a known minimum number of neurons
  2. Vary the number of cross-validation folds to assess the stability of the method
  3. Test the method on different network architectures to assess its generality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed algorithm perform when applied to neural networks with non-elementwise activation functions like Softmax or Maxout?
- Basis in paper: [explicit] The paper states that it is not obvious how the algorithm will work with non-elementwise activation functions, as these can create additional relationships between the columns of the matrix Y and change the rank of the output matrix.
- Why unresolved: The paper acknowledges the limitation but does not provide experimental results or theoretical analysis for non-elementwise activation functions.
- What evidence would resolve it: Experiments applying the algorithm to networks with Softmax or Maxout activations, showing whether the method still effectively finds the minimum number of neurons or if modifications are needed.

### Open Question 2
- Question: Can the algorithm be adapted to handle neural networks trained with random data augmentation?
- Basis in paper: [explicit] The paper mentions that it is not obvious how the algorithm will work in the case of training with random data augmentation, which is widely used now when training networks.
- Why unresolved: The authors recognize this as a potential issue but do not explore solutions or test the algorithm under such conditions.
- What evidence would resolve it: Testing the algorithm on networks trained with various data augmentation techniques, demonstrating whether the method remains stable and effective or requires adjustments.

### Open Question 3
- Question: Is it possible to develop a method to calculate the minimum number of neurons for fully connected layers followed by non-fully connected layers (e.g., convolutional or recurrent layers)?
- Basis in paper: [inferred] The paper focuses on fully connected layers and suggests the method could be useful for any network architecture to estimate the minimal size of any fully connected layer followed by another fully connected one, but does not extend this to other layer types.
- Why unresolved: The current algorithm is designed specifically for fully connected layers, and extending it to other architectures would require new approaches.
- What evidence would resolve it: Proposing and validating an extension of the algorithm to handle networks with convolutional or recurrent layers following fully connected layers, showing improved efficiency or performance.

## Limitations

- Computational requirements: Cross-validation and SVD decomposition create significant computational overhead, particularly for large networks or datasets
- Generalizability concerns: While tested on MNIST and similar datasets, the method's performance on more complex architectures (CNNs, transformers) and diverse data distributions remains unverified
- Activation function sensitivity: The method may not work effectively with non-elementwise activation functions like Softmax or Maxout

## Confidence

- High Confidence: The statistical equivalence verification approach and cross-validation methodology are well-established practices with clear implementation paths.
- Medium Confidence: The SVD-based rank estimation and the claim about minimum neurons being an internal property require more empirical validation across diverse architectures.
- Low Confidence: The exact threshold determination for statistical equivalence and the method's behavior with highly nonlinear activation functions.

## Next Checks

1. **Activation Function Sensitivity**: Test the method across different activation functions (ReLU, tanh, sigmoid) to quantify how nonlinear dependencies affect the SVD rank estimation accuracy.

2. **Architecture Scalability**: Apply the method to progressively deeper and wider networks (beyond fully connected layers) to identify computational bottlenecks and verify if the minimum neuron estimation remains stable.

3. **Threshold Robustness Analysis**: Systematically vary the statistical equivalence threshold Q0 across multiple orders of magnitude to map the relationship between threshold values, network accuracy, and computational efficiency.