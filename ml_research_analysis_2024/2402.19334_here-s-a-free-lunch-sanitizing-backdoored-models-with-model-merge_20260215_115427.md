---
ver: rpa2
title: 'Here''s a Free Lunch: Sanitizing Backdoored Models with Model Merge'
arxiv_id: '2402.19334'
source_url: https://arxiv.org/abs/2402.19334
tags:
- merging
- backdoor
- attacks
- poisoning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces model merging as a defense against backdoor
  attacks on pre-trained language models (PLMs). The core idea is to merge a backdoored
  model with other models using simple averaging of their weights, which effectively
  mitigates backdoor triggers without retraining.
---

# Here's a Free Lunch: Sanitizing Backdoored Models with Model Merge

## Quick Facts
- arXiv ID: 2402.19334
- Source URL: https://arxiv.org/abs/2402.19334
- Authors: Ansh Arora; Xuanli He; Maximilian Mozes; Srinibas Swain; Mark Dras; Qiongkai Xu
- Reference count: 21
- One-line primary result: Model merging via weight averaging reduces backdoor attack success rates by 75% on average without retraining

## Executive Summary
This paper introduces model merging as a defense against backdoor attacks on pre-trained language models. The approach involves averaging weights from a backdoored model with other models to dilute backdoor triggers. Experiments across four PLMs, four datasets, and five backdoor attacks show significant reductions in attack success rates while maintaining clean accuracy. The defense requires no access to training data or attack-specific knowledge, making it a practical solution for sanitizing compromised models.

## Method Summary
The method merges a backdoored model with other homogeneous models using weight averaging (WAG) or advanced merging strategies like Fisher Merging and TIES-Merging. The core idea is that averaging weights from multiple models dilutes the backdoor trigger signal, reducing the model's vulnerability to backdoor attacks. The approach is tested across BERT-Base, RoBERTa-Large, Llama2-7B, and Mistral-7B models, using SST-2, OLID, AG News, and QNLI datasets with five different backdoor attack types.

## Key Results
- Model merging reduces attack success rates by an average of 75% across all tested scenarios
- Merging with multiple benign models achieves up to 96% mitigation of backdoor attacks
- The defense maintains clean accuracy while significantly reducing attack effectiveness
- Weight averaging (WAG) outperforms more complex merging strategies like Fisher Merging and TIES-Merging

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Merging a backdoored model with multiple benign or other backdoored models dilutes the backdoor trigger signal.
- Mechanism: Averaging weights from multiple models reduces the impact of trigger-specific weights in the backdoored model. When triggers are present in only a subset of models, the averaged weights lose the strong association between triggers and malicious labels.
- Core assumption: Backdoor triggers are represented by a relatively small set of weights that can be "averaged out" when combined with other models lacking the same triggers.
- Evidence anchors:
  - [abstract] "merging a backdoored model with other homogeneous models can significantly remediate backdoor vulnerabilities"
  - [section 3] "by taking into account several other models, the backdoor signal in a single one will be reduced"
- Break condition: If backdoor triggers are distributed across a large portion of the model weights, averaging may not sufficiently reduce their impact.

### Mechanism 2
- Claim: Merging multiple models, even if some are backdoored with different triggers, reduces overall attack success rate.
- Mechanism: Different backdoor triggers affect different subsets of inputs. When merging models with different triggers, the combined model's response to any single trigger is weakened because the other models do not share the same trigger pattern.
- Core assumption: Backdoors are not universally present across all merged models, and different triggers do not reinforce each other when averaged.
- Evidence anchors:
  - [section 4.2] "merging all backdoored models alongside the Benign one...mitigating ASR as high as 96% across all attacks"
  - [section 4.3] "increasing the number of merged Benign models enhances mitigation effectiveness"
- Break condition: If all merged models share the same backdoor trigger, merging will not reduce the attack success rate.

### Mechanism 3
- Claim: The defense works across different model architectures and training procedures without requiring access to training data or attack specifics.
- Mechanism: Weight averaging is architecture-agnostic within the same base model family, and the dilution effect of backdoor signals does not depend on knowing the training process or attack method.
- Core assumption: The averaging operation is effective regardless of the source of the backdoor, and the backdoor signal is localized enough to be reduced by simple averaging.
- Evidence anchors:
  - [abstract] "Our approach...requires no access to training data or attack-specific knowledge"
  - [section 4.4] "Our findings indicate that each of these merging strategies effectively reduces the ASR"
- Break condition: If the backdoor signal is deeply integrated into the model weights in a way that averaging does not reduce it, the defense will fail.

## Foundational Learning

- Concept: Backdoor attacks in NLP
  - Why needed here: Understanding how backdoors work is essential to grasp why merging can mitigate them.
  - Quick check question: What is the difference between data poisoning and weight poisoning in backdoor attacks?

- Concept: Model merging via weight averaging
  - Why needed here: The core defense mechanism relies on averaging weights from multiple models.
  - Quick check question: What happens to a parameter value when you average it with values from models that do not share the same backdoor trigger?

- Concept: Attack success rate (ASR) and clean accuracy (CACC)
  - Why needed here: These metrics are used to evaluate the effectiveness of the defense.
  - Quick check question: If a defense method reduces ASR but also reduces CACC significantly, is it still effective?

## Architecture Onboarding

- Component map:
  - Backdoored model (Mp) -> Benign models (Mk) -> Merging operation (WAG/Fisher/TIES) -> Merged model (Mp+M1+...+Mk)

- Critical path:
  1. Obtain a backdoored model.
  2. Collect a set of other models (benign or backdoored with different triggers).
  3. Perform weight averaging across all models.
  4. Evaluate the merged model on both clean and poisoned test sets.

- Design tradeoffs:
  - Simplicity vs. effectiveness: Simple averaging works well but may be less effective than advanced merging strategies.
  - Number of models: More models can improve defense but increase computational cost.
  - Model compatibility: Merging requires models with the same architecture.

- Failure signatures:
  - ASR remains high after merging: Possible if all models share the same backdoor trigger or if the backdoor signal is too widespread.
  - CACC drops significantly: Possible if the merged models are too dissimilar or if the averaging disrupts important weights.

- First 3 experiments:
  1. Merge a backdoored BERT-Base model with a single benign BERT-Base model and evaluate ASR reduction.
  2. Merge a backdoored model with multiple benign models and compare ASR reduction to the single-model merge.
  3. Test merging models from different datasets (e.g., models trained on IMDB, Yelp, and Amazon) with a backdoored SST-2 model to assess cross-domain effectiveness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of model merging as a defense mechanism depend on the specific architecture of the models being merged?
- Basis in paper: [inferred] The paper mentions that the effectiveness of the approach is consistent across different models (BERT, RoBERTa, Llama2, Mistral), but does not explicitly state whether the specific architecture affects the defense.
- Why unresolved: The paper does not provide a detailed analysis of how the architecture of the models being merged impacts the defense's effectiveness.
- What evidence would resolve it: A controlled experiment comparing the defense's effectiveness when merging models with different architectures (e.g., BERT with RoBERTa, Llama2 with Mistral) would provide insights into the impact of architecture on the defense's performance.

### Open Question 2
- Question: Is the model merging defense effective against backdoor attacks that target specific linguistic features or semantics?
- Basis in paper: [explicit] The paper mentions that the BITE attack leverages spurious correlations between the target label and words in the training data, which may target specific linguistic features or semantics.
- Why unresolved: The paper does not provide a detailed analysis of how the defense performs against backdoor attacks that target specific linguistic features or semantics.
- What evidence would resolve it: An experiment evaluating the defense's effectiveness against backdoor attacks that target specific linguistic features or semantics (e.g., attacks that exploit sentiment-bearing words or syntactic structures) would provide insights into the defense's robustness.

### Open Question 3
- Question: Can the model merging defense be applied to backdoor attacks in other domains, such as computer vision or speech recognition?
- Basis in paper: [explicit] The paper focuses on backdoor attacks in natural language processing (NLP) and does not mention the applicability of the defense to other domains.
- Why unresolved: The paper does not explore the potential of the model merging defense in other domains, such as computer vision or speech recognition.
- What evidence would resolve it: An experiment applying the model merging defense to backdoor attacks in other domains, such as computer vision or speech recognition, would provide insights into the defense's generalizability.

## Limitations
- The paper doesn't thoroughly analyze how backdoor triggers are distributed across model weights, leaving uncertainty about the "averaging out" mechanism.
- Experiments focus primarily on BERT-Base and higher poisoning rates (20%), leaving questions about effectiveness on other architectures and lower poisoning scenarios.
- The relationship between the number of merged models and diminishing returns is not systematically investigated.

## Confidence
- High Confidence: The basic claim that model merging reduces ASR compared to baseline defenses is well-supported by extensive experiments across multiple models and attacks.
- Medium Confidence: The assertion that merging works across different model architectures and training procedures is plausible but needs more systematic investigation of cross-architecture effectiveness.
- Medium Confidence: The mechanism explanation (averaging reduces trigger signal strength) is reasonable but lacks detailed analysis of how triggers are actually represented in model weights.

## Next Checks
1. **Weight Distribution Analysis**: Conduct a detailed study of how backdoor triggers are distributed across model weights before and after merging to validate the "averaging out" mechanism. This would involve analyzing activation patterns and weight importance scores.

2. **Cross-Architecture Testing**: Systematically test the merging defense across different model families (e.g., BERT, RoBERTa, GPT) to verify the claim of architecture-agnostic effectiveness and identify any limitations.

3. **Threshold Analysis**: Investigate the relationship between poisoning rate, number of merged models, and defense effectiveness to determine optimal configurations and identify scenarios where the defense might fail.