---
ver: rpa2
title: Evaluating Large Language Models for Structured Science Summarization in the
  Open Research Knowledge Graph
arxiv_id: '2405.02105'
source_url: https://arxiv.org/abs/2405.02105
tags: []
core_contribution: This study evaluates Large Language Models (LLMs) for recommending
  research dimensions to automatically structure scientific summaries in the Open
  Research Knowledge Graph (ORKG). We compared GPT-3.5, Llama 2, and Mistral against
  manually curated ORKG properties using semantic alignment, property mapping, embeddings
  similarity, and expert surveys.
---

# Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph

## Quick Facts
- arXiv ID: 2405.02105
- Source URL: https://arxiv.org/abs/2405.02105
- Reference count: 31
- Key outcome: LLMs show potential as recommendation systems for structuring scientific contributions, particularly GPT-3.5, but require fine-tuning on scientific domains to better align with human expert curation

## Executive Summary
This study evaluates three large language models (GPT-3.5, Llama 2, and Mistral) for their ability to recommend research dimensions that structure scientific summaries in the Open Research Knowledge Graph (ORKG). The research compares LLM-generated dimensions against manually curated ORKG properties using multiple evaluation methods including semantic alignment, property mapping, embeddings similarity, and expert surveys. Results demonstrate that while LLMs can generate diverse and semantically relevant dimensions, their alignment with established scientific taxonomies remains moderate, highlighting the need for domain-specific fine-tuning to bridge the gap between automated recommendations and expert-curated knowledge organization.

## Method Summary
The evaluation framework involved three LLMs processing 38 scientific papers to generate research dimensions, which were then compared against manually curated ORKG properties. Multiple evaluation methods were employed: semantic alignment analysis measured overlap between generated dimensions and ORKG properties using exact and semantic matching; property mapping created fine-grained correspondences between LLM suggestions and ORKG properties; embeddings similarity analysis used cosine distance on vector representations to assess semantic relatedness; and expert surveys collected human assessments of LLM-generated dimensions' relevance and completeness. The methodology combined automated metrics with qualitative expert feedback to provide a comprehensive assessment of LLM performance in scientific knowledge structuring.

## Key Results
- Semantic alignment between LLM-generated dimensions and ORKG properties reached 41.2%, with GPT-3.5 showing highest similarity
- Property-to-dimension mapping revealed low correspondence (0.33 average), indicating LLMs generate more diverse dimensions but with less precise alignment
- Expert surveys found 36.3% of LLM suggestions highly relevant, though participants did not significantly alter their existing annotations

## Why This Works (Mechanism)
The study demonstrates that LLMs can effectively generate semantically relevant research dimensions by leveraging their pre-trained understanding of scientific discourse patterns and terminology. The models' ability to produce diverse dimension sets suggests they capture the inherent complexity and variability in how scientific contributions are structured and described. However, the moderate semantic alignment (41.2%) and low property mapping scores (0.33 average) indicate that while LLMs understand scientific concepts at a high level, they struggle with precise alignment to established scientific taxonomies and research schemas. This mechanism suggests that LLMs function more as exploratory tools for dimension discovery rather than precise alignment systems, making them valuable for initial knowledge organization but requiring additional refinement for expert-level curation.

## Foundational Learning

**Semantic alignment**: Measuring overlap between generated dimensions and established taxonomies using exact and semantic matching to quantify how well LLMs capture research structure
- Why needed: Provides quantitative baseline for evaluating automated knowledge organization against human-curated standards
- Quick check: 41.2% alignment indicates moderate performance requiring improvement for practical deployment

**Embeddings similarity**: Using vector representations and cosine distance to assess semantic relatedness between LLM outputs and ORKG properties
- Why needed: Captures nuanced semantic relationships beyond exact string matching
- Quick check: Up to 0.84 cosine similarity shows strong semantic correlation but highlights remaining gaps

**Fine-grained property mapping**: Creating detailed correspondences between individual LLM suggestions and specific ORKG properties
- Why needed: Reveals which aspects of research dimensions LLMs capture well versus missing entirely
- Quick check: 0.33 average mapping score indicates need for domain-specific fine-tuning

## Architecture Onboarding

**Component map**: LLM models (GPT-3.5, Llama 2, Mistral) -> Dimension generation -> Evaluation metrics (semantic alignment, embeddings, mapping) -> Expert survey validation

**Critical path**: Dimension generation -> Semantic alignment evaluation -> Embeddings similarity analysis -> Expert survey validation

**Design tradeoffs**: Balanced evaluation using automated metrics versus human judgment; comprehensive comparison across multiple LLM architectures; trade-off between dimension diversity and alignment precision

**Failure signatures**: Low semantic alignment (41.2%) and poor property mapping (0.33 average) indicate models struggle with precise scientific taxonomy alignment despite generating semantically relevant content

**3 first experiments**:
1. Test prompt engineering variations to improve dimension alignment with ORKG properties
2. Fine-tune selected LLM on domain-specific scientific literature to enhance taxonomy awareness
3. Compare performance across different scientific disciplines to identify domain-specific strengths/weaknesses

## Open Questions the Paper Calls Out
The paper identifies several critical open questions for advancing LLM-based scientific knowledge structuring:

- How can prompt engineering be optimized to improve alignment between LLM-generated dimensions and established scientific taxonomies?
- What domain-specific fine-tuning strategies would most effectively bridge the gap between automated recommendations and expert-curated knowledge organization?
- How do different scientific disciplines influence LLM performance in dimension generation, and what architectural modifications might address discipline-specific challenges?
- What evaluation frameworks beyond the ORKG schema could provide more comprehensive assessment of LLM capabilities across diverse knowledge graph structures?
- How can the trade-off between dimension diversity and semantic precision be balanced to maximize both coverage and accuracy in scientific summarization?

## Limitations

- Small expert survey sample (25 responses) and limited dataset (38 papers) constrain generalizability of findings
- Fixed prompting strategies without exploring optimization techniques for scientific summarization
- Domain-specific evaluation framework lacks cross-validation with alternative knowledge graph schemas
- Limited comparison of different temperature settings and sampling strategies that could affect dimension diversity
- Absence of quantitative error analysis categorizing types of misalignments between LLM outputs and ORKG properties

## Confidence

**High confidence** in comparative performance ranking among GPT-3.5, Llama 2, and Mistral for this specific task, based on consistent patterns across multiple evaluation methods.

**Medium confidence** in semantic alignment and embeddings similarity results, as these metrics show strong correlations but are based on limited samples and may not capture nuanced domain variations.

**Low confidence** in generalizability of expert survey findings due to small sample size and potential selection bias.

## Next Checks

1. Expand expert survey to 100+ participants across multiple research domains with stratified sampling for broader scientific field representation.

2. Conduct cross-validation experiments using alternative knowledge graph schemas (ResearchGraph, Microsoft Academic Graph) to assess robustness across different semantic frameworks.

3. Implement systematic prompt engineering experiments varying temperature, few-shot examples, and task-specific instructions to identify optimal prompting strategies for scientific summarization.