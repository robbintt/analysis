---
ver: rpa2
title: NGD converges to less degenerate solutions than SGD
arxiv_id: '2409.04913'
source_url: https://arxiv.org/abs/2409.04913
tags:
- loss
- wbic
- learning
- more
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effectiveness of natural gradient descent
  (NGD) compared to stochastic gradient descent (SGD) in training neural networks.
  The key contribution is demonstrating that NGD converges to less degenerate solutions,
  as measured by the local learning coefficient (LLC) and Hessian trace.
---

# NGD converges to less degenerate solutions than SGD

## Quick Facts
- arXiv ID: 2409.04913
- Source URL: https://arxiv.org/abs/2409.04913
- Authors: Moosa Saghir; N. R. Raghavendra; Zihe Liu; Evan Ryan Gunter
- Reference count: 11
- Key outcome: NGD converges to less degenerate solutions than SGD, measured by higher local learning coefficient (LLC) and Hessian trace

## Executive Summary
This paper investigates the effectiveness of natural gradient descent (NGD) compared to stochastic gradient descent (SGD) in training neural networks. The key contribution is demonstrating that NGD converges to less degenerate solutions, as measured by the local learning coefficient (LLC) and Hessian trace. Using the theoretical framework of singular learning theory (SLT), the authors argue that the LLC λ(w*) is a more accurate measure of model complexity than traditional parameter count. The experiments show that models trained with NGD consistently have higher values of both the estimated LLC (ˆλ) and the Hessian trace (Tr(H)) across various model architectures and datasets.

## Method Summary
The authors compare NGD and SGD by training fully connected feed-forward neural networks (FFNN) and Convolutional Neural Nets (CNN) on MNIST and Fashion MNIST datasets. They use PyHessian to approximate the Hessian trace and devinterp to estimate the LLC and WBIC at each epoch. The experiments involve varying Fisher matrix smoothing parameters, switching between SGD and NGD from the same starting point, and analyzing how LLC and Tr(H) evolve over training. The NGD implementation includes Fisher matrix smoothing with parameter κ to ensure numerical stability.

## Key Results
- Models trained with NGD consistently achieve higher LLC (ˆλ) values than those trained with SGD, with the highest t-value of 1.9×10^-31 across model sizes
- The Hessian trace (Tr(H)) remains higher for NGD-trained models throughout training epochs
- When switching from SGD to NGD mid-training, the LLC increases while the SGD-continued model's LLC remains steady
- Varying the Fisher matrix smoothing parameter shows that reducing smoothing increases the LLC of NGD-trained models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NGD converges to solutions with higher effective dimension because the Fisher information matrix inverse F⁻¹ adjusts the gradient step size based on local curvature, avoiding degenerate regions where eigenvalues are near zero.
- Mechanism: In degenerate regions of the loss landscape, many eigenvalues of the Fisher matrix F are close to zero, causing the determinant to be very low. This results in a large NGD update step (via F⁻¹) that pushes the parameters out of these degenerate regions. SGD, without this curvature correction, can get stuck or slow down in these regions.
- Core assumption: The Fisher matrix F and the Hessian H are equivalent at a local minimum (F = H), so the curvature information captured by F⁻¹ directly relates to the degeneracy measured by the Hessian trace and LLC.
- Evidence anchors:
  - [abstract] "NGD converges to less degenerate solutions than SGD"
  - [section] "The LLC λ(w∗) has been used to analyse the complexity of models [Lau et al., 2023]. However, it remains an open question empirically whether SLT can be applied to analysing the complexity of NGD and SGD."
  - [corpus] Weak evidence; corpus neighbors discuss related optimization methods but do not directly address degeneracy or LLC differences.
- Break condition: If the Fisher matrix smoothing κ is too large (κ → ∞), Fs → κI and NGD effectively becomes SGD, losing the curvature correction benefit.

### Mechanism 2
- Claim: NGD's F⁻¹ term causes the optimizer to take smaller steps in high-curvature regions and larger steps in flatter regions, leading to faster escape from saddle points and degenerate minima.
- Mechanism: By stretching the loss landscape so that curvature is equalized across directions, NGD ensures that the update magnitude is proportional to the local inverse curvature. This means near flat, degenerate regions with low curvature (small eigenvalues), the update step is large, propelling the optimizer away. Near sharp minima, the step is small, avoiding overshoot.
- Core assumption: Real loss landscapes have far more saddle points than minima, and degenerate regions are characterized by small eigenvalues in the Fisher matrix.
- Evidence anchors:
  - [section] "NGD attempts to address these problems by scaling the gradient based on the curvature in each direction [Amari, 1998]"
  - [section] "In these regions, many of the eigenvalues of F become extremely small, hence the point is highly degenerate. The determinant of F becomes very low, possibly causing a large update step to be taken in these degenerate regions."
  - [corpus] Weak evidence; corpus neighbors discuss SGD dynamics but do not explicitly cover saddle point escape or curvature-based step sizing.
- Break condition: If the smoothing constant κ is too small, numerical instability may occur due to extremely small eigenvalues, potentially causing erratic behavior.

### Mechanism 3
- Claim: NGD guides the training trajectory away from highly degenerate minima by dynamically adjusting the effective learning rate based on the local Fisher geometry, leading to solutions with higher LLC and Hessian trace.
- Mechanism: During training, NGD's adaptive step sizing causes it to naturally avoid basins with many near-zero eigenvalues (high degeneracy). Experiments show that after an initial SGD phase, switching to NGD from the same state increases the LLC, indicating escape from degenerate regions. This is not due to larger step sizes alone, as increasing SGD's learning rate does not produce the same LLC increase.
- Core assumption: The LLC λ(w*) accurately captures the effective dimension and degeneracy of the solution, and models trained with NGD consistently achieve higher λ than those trained with SGD.
- Evidence anchors:
  - [section] "We verify whether the increase in ˆλNGD is due to the nature of NGD, rather than the direct increase in learning rate causing it to break out of a local optimum. We only increase the SGD learning rate after switching and notice that ˆλS GD does not increase"
  - [section] "After the split, the update step size of NGD tends to fluctuate while that of SGD is steady, and we often notice an increase in ˆλNGD while ˆλS GD either remains steady"
  - [corpus] Weak evidence; corpus neighbors do not discuss LLC or degeneracy escape mechanisms.
- Break condition: If the NGD learning rate is too low after the switch, the escape from degenerate regions may be too slow to observe a significant LLC increase.

## Foundational Learning

- Concept: Effective dimension and degeneracy in singular learning theory
  - Why needed here: The paper's core contribution relies on understanding that model complexity is not just parameter count but the number of functionally relevant parameters, measured by the LLC λ(w*). Degenerate models have more redundant parameters (near-zero Hessian eigenvalues), lowering their effective dimension.
  - Quick check question: What does a lower LLC λ(w*) imply about the geometry of the loss landscape around a local minimum?

- Concept: Fisher information matrix and its role in natural gradient descent
  - Why needed here: NGD uses the inverse of the Fisher information matrix F⁻¹ to scale the gradient, which directly affects how the optimizer navigates the loss landscape. Understanding the relationship between F and the Hessian H, and how F⁻¹ adjusts step sizes based on curvature, is crucial to grasp why NGD avoids degenerate regions.
  - Quick check question: Why is smoothing the Fisher matrix with κI necessary in practice for NGD convergence?

- Concept: Bayesian Information Criterion (BIC) and its generalization via SLT (WBIC)
  - Why needed here: The paper extends the traditional BIC, which uses parameter count for complexity, to the WBIC, which uses the LLC λ. This shift is central to the argument that effective dimension is a better measure of model complexity than nominal parameter count, especially for understanding generalization.
  - Quick check question: How does the WBIC generalize the BIC, and what does this imply about the accuracy-complexity tradeoff in model selection?

## Architecture Onboarding

- Component map:
  Datasets -> Models (FFNN, CNN) -> Optimizers (SGD, NGD) -> Metrics (LLC, Tr(H), WBIC) -> Libraries (PyHessian, devinterp)

- Critical path:
  1. Train models independently using SGD and NGD on benchmark datasets
  2. At each epoch, compute training/validation loss, update norm, Hessian trace Tr(H), and LLC estimate ˆλ(w*)
  3. Analyze how ˆλ and Tr(H) evolve over epochs and compare between SGD and NGD
  4. Vary NGD smoothing parameters (α, ϵ) to test sensitivity
  5. Conduct switch experiments: train with SGD, then split and continue with SGD vs NGD from the same state

- Design tradeoffs:
  - Fisher matrix smoothing: Higher κ (more smoothing) makes NGD closer to SGD but more stable; lower κ allows more aggressive escape from degenerate regions but risks numerical instability
  - Learning rate: Higher learning rates can speed up convergence but may overshoot minima; for NGD, the effective learning rate is modulated by F⁻¹, so the nominal rate interacts with the curvature scaling
  - Batch size: Affects the variance of gradient estimates and the quality of Fisher matrix approximation; larger batches give more stable curvature estimates but are computationally heavier

- Failure signatures:
  - LLC estimates that do not stabilize or show erratic behavior, possibly due to poor Fisher matrix conditioning or insufficient sampling in the local neighborhood
  - Hessian trace values that are unstable or do not correlate with LLC trends, indicating issues with the Hessian-vector product approximation
  - NGD not showing higher LLC than SGD, which could indicate too much Fisher matrix smoothing (κ too large) or inappropriate learning rate settings

- First 3 experiments:
  1. Train a small FFNN (1 hidden layer, 128 nodes) on MNIST using both SGD and NGD with default hyperparameters; compare LLC ˆλ and Hessian trace Tr(H) evolution over epochs.
  2. Repeat experiment 1 but vary the NGD smoothing parameter α (e.g., 10^-3, 10^-2, 10^-1) to observe its effect on LLC and Hessian trace.
  3. Train a model with SGD until convergence, then split and continue training two identical models from that state—one with SGD and one with NGD (same learning rate); compare LLC evolution to test if NGD escapes degenerate regions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the natural gradient descent (NGD) algorithm consistently lead to solutions with higher effective dimension (as measured by the learning coefficient λ) across different neural network architectures beyond the ones tested (e.g., transformers, recurrent networks)?
- Basis in paper: [explicit] The paper demonstrates that NGD leads to higher λ and Hessian trace Tr(H) compared to SGD in fully connected feed-forward neural networks (FFNN) and Convolutional Neural Nets (CNN) following the Lenet-5 architecture on MNIST and Fashion MNIST datasets.
- Why unresolved: The experiments were limited to specific model architectures and datasets. The paper does not explore whether the observed trend holds for other architectures like transformers, recurrent networks, or different types of data.
- What evidence would resolve it: Conducting similar experiments with a diverse range of neural network architectures and datasets to verify if NGD consistently leads to solutions with higher λ and Tr(H).

### Open Question 2
- Question: What is the exact relationship between the smoothing constant κ in the Fisher matrix and the effective dimension (λ) of the resulting solution? Is there an optimal value of κ that maximizes λ for a given problem?
- Basis in paper: [explicit] The paper mentions that increasing the smoothing constant κ (which is directly proportional to α and inversely proportional to d) brings NGD closer to SGD, and that reducing κ increases the LLC of NGD. However, the exact relationship and the existence of an optimal κ are not explored.
- Why unresolved: The experiments only vary α and ϵ within a limited range, and the paper does not systematically investigate the effect of different κ values on λ.
- What evidence would resolve it: Performing a comprehensive study varying κ over a wide range for different problems and architectures to determine the relationship between κ and λ, and to identify any optimal κ values.

### Open Question 3
- Question: Can the learning coefficient λ be used as a reliable indicator of generalization performance for models trained with NGD, similar to its potential use with SGD as suggested by the WBIC experiments?
- Basis in paper: [inferred] The paper shows that models trained with NGD have higher λ and that the WBIC, which is related to λ, increases during overfitting. This suggests a potential link between λ and generalization, but the paper does not explicitly investigate this for NGD-trained models.
- Why unresolved: The experiments focus on comparing λ between NGD and SGD, but do not directly test whether λ can predict generalization performance for NGD-trained models.
- What evidence would resolve it: Conducting experiments to correlate λ values of NGD-trained models with their generalization performance on held-out test data, similar to the WBIC experiments for overfitting.

## Limitations

- Results are based on specific model architectures (FFNN and CNN) and datasets (MNIST, Fashion MNIST), which may not generalize to all neural network types or tasks
- LLC estimation relies on approximations that could introduce errors, especially for larger models where sampling becomes more challenging
- The exact relationship between Fisher matrix smoothing and degeneracy escape, while supported by experiments, needs more rigorous theoretical justification

## Confidence

- Fisher matrix smoothing and degeneracy escape: High confidence
- LLC as better measure of model complexity than parameter count: Medium confidence
- NGD dynamic step sizing mechanism: Medium confidence

## Next Checks

1. **Architecture Generalization Test**: Replicate the experiments using transformer-based architectures (e.g., small BERT variants) on language modeling tasks to verify if NGD consistently achieves higher LLC and Hessian trace across different model types.

2. **Smoothing Sensitivity Analysis**: Conduct a more systematic study of how different Fisher matrix smoothing constants (κ) affect the LLC and Hessian trace, including edge cases where κ approaches zero or becomes very large, to better understand the trade-off between stability and degeneracy escape.

3. **Cross-Dataset Validation**: Test the NGD vs SGD comparison on more complex and diverse datasets (e.g., CIFAR-10, ImageNet subsets) to ensure the observed effects are not specific to simple image classification benchmarks.