---
ver: rpa2
title: Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding
arxiv_id: '2403.07559'
source_url: https://arxiv.org/abs/2403.07559
tags:
- agents
- agent
- communication
- solvers
- multi-agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the Multi-Agent Path Finding (MAPF) problem
  using Multi-Agent Reinforcement Learning (MARL) with communication. The authors
  propose Ensembling Prioritized Hybrid Policies (EPH), a Q-learning-based MARL-MAPF
  solver that uses improved selective communication and three advanced inference strategies.
---

# Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding

## Quick Facts
- arXiv ID: 2403.07559
- Source URL: https://arxiv.org/abs/2403.07559
- Reference count: 40
- Primary result: EPH achieves 100% success rate on 40×40 and 80×80 random maps, outperforming neural baselines in episode length

## Executive Summary
This paper introduces EPH, a Q-learning-based MARL solver for the Multi-Agent Path Finding (MAPF) problem that uses selective communication and hybrid expert guidance. The method combines improved selective communication blocks with Transformer variants, hybrid expert guidance using A* paths, Q-value-based priority decisions, and an ensemble approach that runs multiple solvers in parallel. The authors demonstrate competitive performance against state-of-the-art neural MARL-MAPF solvers on both random and structured environments, achieving 100% success rates on larger maps and outperforming baselines in average episode length metrics.

## Method Summary
EPH addresses MAPF through a Q-learning-based MARL approach with selective communication. The core method uses a selective communication block enhanced by Transformer variants, hybrid expert guidance where A* paths guide agents without nearby live agents, and Q-value-based priority decisions for conflict resolution and deadlock avoidance. The ensemble method runs multiple solvers in parallel and selects the best solution. The approach is evaluated on both random maps (up to 80×80) and structured maps, showing competitive performance against neural MARL baselines in success rates and episode lengths.

## Key Results
- Achieves 100% success rate on 40×40 and 80×80 random maps in most cases
- Outperforms baselines in average episode length on random environments
- Excels all neural baselines in both success rate and episode length on structured maps

## Why This Works (Mechanism)
EPH works by combining selective communication with expert guidance and ensemble methods. The selective communication block filters relevant information between agents using Transformer variants, reducing noise and improving coordination. The hybrid expert guidance provides A* paths to agents without nearby agents, ensuring they make progress while preserving the learning system's ability to handle complex interactions. Q-value-based priority decisions help resolve conflicts and avoid deadlocks by letting agents with higher Q-values take precedence. The ensemble method provides robustness by running multiple solvers and selecting the best outcome.

## Foundational Learning
- **Multi-Agent Path Finding (MAPF)**: The problem of finding collision-free paths for multiple agents from their start to goal positions - needed to frame the problem context and understand why coordination is critical
- **Q-learning**: A reinforcement learning algorithm that learns action-value functions to maximize cumulative reward - needed to understand the base learning mechanism and how EPH builds upon it
- **Selective Communication**: Filtering and prioritizing information exchange between agents to reduce communication overhead - needed to grasp how EPH manages agent coordination efficiently
- **Transformer Variants**: Neural network architectures using self-attention mechanisms, adapted here for communication - needed to understand the communication block's design
- **Ensemble Methods**: Running multiple solvers in parallel and selecting the best solution - needed to understand the final selection mechanism and robustness approach

## Architecture Onboarding

**Component Map**: Input -> Selective Communication Block -> Q-learning with Priority Decisions -> Hybrid Expert Guidance -> Ensemble Selection -> Output

**Critical Path**: Observation → Communication → Q-value computation → Priority decision → Action selection → Solution evaluation → Ensemble selection

**Design Tradeoffs**: The system trades computational overhead from ensemble methods for improved success rates and solution quality. Selective communication reduces communication overhead but may miss some coordination opportunities. Hybrid expert guidance ensures progress but could limit learning in some scenarios.

**Failure Signatures**: 
- Low success rates indicate communication failures or poor Q-value learning
- High episode lengths suggest inefficient path planning or frequent conflicts
- Ensemble selection failures indicate solver instability or poor diversity among ensemble members

**First Experiments**:
1. Test communication block with varying numbers of agents to assess scalability
2. Evaluate Q-value learning with and without priority decisions on simple maps
3. Compare single solver vs. ensemble performance on structured maps

## Open Questions the Paper Calls Out
None

## Limitations
- Lacks runtime analysis and scalability experiments beyond 80x80 maps
- No comparison with classical MAPF solvers like CBS or ICTS to establish advantages
- Ensemble method computational overhead is not discussed

## Confidence
- EPH architecture and component design: High
- Empirical performance comparisons with neural baselines: Medium
- Claims about scalability and computational efficiency: Low
- Ensemble method benefits and overhead: Low

## Next Checks
1. Compare EPH against classical MAPF solvers (CBS, ICTS) on identical benchmarks to establish whether the MARL approach provides advantages in solution quality or runtime efficiency
2. Conduct runtime and scalability analysis measuring wall-clock time and memory usage as map size and agent count increase beyond 80x80
3. Perform stress tests with varying obstacle densities and agent densities to evaluate performance under edge cases and communication-constrained scenarios