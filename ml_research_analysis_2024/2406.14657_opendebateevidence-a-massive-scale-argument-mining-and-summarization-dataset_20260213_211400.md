---
ver: rpa2
title: 'OpenDebateEvidence: A Massive-Scale Argument Mining and Summarization Dataset'
arxiv_id: '2406.14657'
source_url: https://arxiv.org/abs/2406.14657
tags:
- debate
- dataset
- evidence
- argument
- opendebateevidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OpenDebateEvidence, a large-scale argument
  mining and summarization dataset sourced from the American Competitive Debate community,
  containing over 3.5 million documents with rich metadata. The dataset captures the
  complexity of arguments in high school and college debates across Policy, Lincoln-Douglas,
  and Public Forum formats.
---

# OpenDebateEvidence: A Massive-Scale Argument Mining and Summarization Dataset

## Quick Facts
- arXiv ID: 2406.14657
- Source URL: https://arxiv.org/abs/2406.14657
- Reference count: 32
- Dataset contains over 3.5 million documents from American Competitive Debate formats

## Executive Summary
This paper introduces OpenDebateEvidence, a large-scale dataset for argument mining and summarization sourced from American Competitive Debate tournaments. The dataset contains over 3.5 million documents across Policy, Lincoln-Douglas, and Public Forum debate formats, with rich metadata including hierarchical argument structure. The authors demonstrate that fine-tuning state-of-the-art language models (LLaMA3-8B, LLaMA3-70B, and Mistral-7B) on this dataset significantly improves performance on both argument mining and summarization tasks, with LLaMA3-70B achieving ROUGE-1 scores of 37.2% on the dataset itself and 54.6% on the BillSum dataset.

## Method Summary
The authors constructed OpenDebateEvidence by scraping and processing documents from debate evidence sharing platforms, applying rigorous filtering to remove low-quality content. They fine-tuned three language models (LLaMA3-8B, LLaMA3-70B, and Mistral-7B) using LoRA on the dataset for both argument mining and summarization tasks. Performance was evaluated using ROUGE F1 scores for summarization and LLM-as-judge evaluations for argument mining. The fine-tuning process involved hyperparameter tuning and optimization to achieve optimal results on the argument mining and summarization benchmarks.

## Key Results
- Fine-tuned LLaMA3-70B achieved ROUGE-1 scores of 37.2% on OpenDebateEvidence and 54.6% on BillSum datasets
- Models trained on OpenDebateEvidence outperformed baseline models on argument mining tasks
- Hierarchical metadata structure (hat, pocket, tag) contributed to improved performance in capturing argument structure
- LoRA fine-tuning proved effective for adapting large language models to debate-specific argumentation tasks

## Why This Works (Mechanism)
The dataset's effectiveness stems from its unique structure and content that mirrors real debate argumentation. The hierarchical metadata captures the complex relationships between arguments, with "hats" providing overarching claims, "pockets" containing supporting evidence, and "tags" offering concise summaries. This structure naturally aligns with how arguments are constructed and evaluated in competitive debate, providing rich training signals for models to learn argumentation patterns.

## Foundational Learning
1. **Argument Mining**: Understanding how to identify and extract argumentative structures from text - needed to build models that can recognize claims, evidence, and reasoning patterns in debate contexts; quick check: can the model identify premise-conclusion relationships in sample arguments?
2. **Hierarchical Text Classification**: Working with multi-level metadata structures - needed to process the hat-pocket-tag hierarchy effectively; quick check: does the model maintain parent-child relationships in the metadata?
3. **Large Language Model Fine-tuning**: Adapting pre-trained models for specific domains - needed to leverage existing language understanding while specializing for debate argumentation; quick check: does fine-tuning improve performance on domain-specific metrics?

## Architecture Onboarding

**Component Map**: Web Scraper -> Document Filter -> Metadata Extractor -> Dataset Builder -> Model Fine-tuner -> Evaluator

**Critical Path**: Raw debate documents → Filtering pipeline → Metadata enrichment → Dataset construction → Model fine-tuning → Performance evaluation

**Design Tradeoffs**: The authors chose to prioritize dataset scale and diversity over perfect annotation quality, accepting some noise to capture the full range of debate argumentation styles. They also opted for LoRA fine-tuning rather than full fine-tuning to reduce computational costs while maintaining performance.

**Failure Signatures**: Models may struggle with arguments from underrepresented debate formats, fail to capture nuanced logical relationships, or produce summaries that prioritize style over substance. Performance may degrade when encountering arguments with unconventional structures or from debate circuits not well-represented in the training data.

**First Experiments**:
1. Test baseline model performance on a held-out validation set from OpenDebateEvidence
2. Fine-tune LLaMA3-8B on a small subset of the dataset and measure performance gains
3. Evaluate model ability to identify argument structure using the hierarchical metadata

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the hierarchical metadata (hat, pocket, tag) structure influence model performance in argument mining tasks compared to flat metadata representations?
- Basis in paper: The authors describe the hierarchical metadata structure and its potential utility for argument mining tasks, noting that "hats" and "pockets" help models learn overarching structure while "tags" summarize key points.
- Why unresolved: The paper demonstrates improved performance on argument mining tasks but does not directly compare the impact of hierarchical versus flat metadata representations.
- What evidence would resolve it: Experiments comparing model performance on argument mining tasks using hierarchical metadata versus flattened versions of the same metadata.

### Open Question 2
- Question: What is the impact of model size on performance for argument mining and summarization tasks, and at what point do larger models provide diminishing returns?
- Basis in paper: The authors show that LLaMA3-70B models significantly outperformed smaller models like LLaMA3-8B and Mistral-7B, but do not explore the relationship between model size and performance in detail.
- Why unresolved: The paper demonstrates that larger models perform better but does not systematically investigate the relationship between model size and performance or identify the point of diminishing returns.
- What evidence would resolve it: Systematic experiments varying model sizes across a broader range and analyzing performance improvements relative to computational costs.

### Open Question 3
- Question: How does the performance of models fine-tuned on OpenDebateEvidence generalize to other argumentation domains, such as legal or political discourse?
- Basis in paper: The authors demonstrate improved performance on related argumentative datasets like BillSum but do not explore generalization to other argumentation domains.
- Why unresolved: The paper shows cross-dataset performance improvements but does not test generalization to fundamentally different argumentation domains beyond debate and legislative contexts.
- What evidence would resolve it: Experiments testing model performance on datasets from diverse argumentation domains, such as legal briefs, political speeches, or scientific debates.

## Limitations
- Evaluation relies primarily on automated metrics (ROUGE scores) rather than human judgment of argumentative quality
- Dataset focus on American Competitive Debate may limit generalizability to other debate traditions globally
- Computational resources required for fine-tuning large models may limit accessibility for many researchers

## Confidence

**Confidence Labels:**
- Dataset construction and scale: High
- Performance improvements through fine-tuning: Medium
- Generalizability to broader argumentation tasks: Low
- Real-world applicability in debate education: Medium

## Next Checks
1. Conduct human evaluation studies comparing model-generated summaries against expert debate coach assessments to validate the quality of argumentative content beyond automated metrics.
2. Test model performance across diverse debate traditions and non-debate argumentative texts to assess generalizability beyond American Competitive Debate formats.
3. Perform ablation studies to determine which dataset features (metadata, document structure, etc.) contribute most significantly to model performance improvements.