---
ver: rpa2
title: Multi Task Inverse Reinforcement Learning for Common Sense Reward
arxiv_id: '2402.11367'
source_url: https://arxiv.org/abs/2402.11367
tags:
- reward
- learning
- task
- agent
- cs-reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses reward design challenges in reinforcement learning,
  proposing to disentangle rewards into task-specific and common-sense components.
  The authors introduce MT-CSIRL, a multi-task inverse reinforcement learning method
  that learns a transferable common-sense reward from expert demonstrations across
  multiple tasks.
---

# Multi Task Inverse Reinforcement Learning for Common Sense Reward

## Quick Facts
- arXiv ID: 2402.11367
- Source URL: https://arxiv.org/abs/2402.11367
- Reference count: 26
- Primary result: MT-CSIRL learns transferable common-sense rewards from multi-task expert demonstrations, achieving 0.88 correlation with ground-truth rewards and enabling zero-shot transfer to unseen tasks

## Executive Summary
This paper addresses reward design challenges in reinforcement learning by proposing a method to disentangle task-specific rewards from common-sense behavior rewards. The authors introduce MT-CSIRL, a multi-task inverse reinforcement learning approach that learns a transferable common-sense reward component from expert demonstrations across multiple tasks. By training shared discriminator weights across tasks while maintaining task-specific rewards, the method enables learning of task-agnostic behavior that can transfer to unseen scenarios. The approach is evaluated on the Meta-world benchmark, demonstrating that single-task IRL fails to learn useful common-sense rewards while multi-task learning enables effective transfer and significantly improves agent behavior.

## Method Summary
The method decomposes rewards into task-specific and common-sense components, learning both simultaneously from expert demonstrations across multiple tasks. MT-CSIRL trains a shared discriminator with common weights across all tasks, which is then used to compute both the common-sense and task-specific rewards. The common-sense reward is learned through a shared discriminator trained on all tasks, while task-specific rewards are learned per task. The approach uses adversarial inverse reinforcement learning framework where the discriminator distinguishes between expert and agent state-action pairs. By optimizing both reward components simultaneously across multiple tasks, the method learns a transferable common-sense reward that captures task-agnostic behavior patterns. The learned rewards can then be used to train agents on new tasks without requiring additional expert demonstrations.

## Key Results
- Single-task IRL fails to learn useful common-sense rewards that transfer to unseen tasks
- MT-CSIRL achieves correlation coefficient of 0.88 with ground-truth rewards
- Learned common-sense rewards enable effective zero-shot transfer to unseen tasks in Meta-world
- The method significantly improves agent behavior compared to task-specific rewards alone

## Why This Works (Mechanism)
The approach works by leveraging the diversity of multiple tasks to identify behavior patterns that are invariant across different objectives. When expert demonstrations are available for multiple tasks, the method can distinguish between behaviors that are specific to individual tasks versus those that represent general common-sense reasoning. By sharing discriminator weights across tasks while maintaining separate reward heads, the model learns to extract the common underlying reward structure that explains expert behavior across all tasks. This shared component captures the common-sense aspect, while task-specific heads model the unique objectives. The adversarial training framework ensures that both components are learned in a way that accurately reproduces expert behavior.

## Foundational Learning

**Inverse Reinforcement Learning**: Why needed - To infer reward functions from expert demonstrations rather than manually specifying them. Quick check - Verify the discriminator can distinguish expert from agent trajectories in single-task setting.

**Multi-task Learning**: Why needed - To leverage multiple tasks for learning transferable representations. Quick check - Confirm that shared weights improve performance across all tasks compared to independent training.

**Adversarial Training**: Why needed - To learn rewards that generate expert-like behavior through minimax optimization. Quick check - Ensure the generator-discriminator game converges and produces realistic trajectories.

## Architecture Onboarding

**Component Map**: Expert demonstrations -> Shared discriminator -> Common-sense reward + Task-specific rewards -> Agent policy -> Agent trajectories -> Discriminator update (looped)

**Critical Path**: Expert demonstrations → Multi-task discriminator training → Reward decomposition → Policy optimization → Behavior evaluation

**Design Tradeoffs**: The method trades increased model complexity (shared discriminator + multiple reward heads) for improved transferability and reduced need for task-specific demonstrations. An alternative would be to learn a single reward function per task, but this would fail to capture transferable common-sense behavior.

**Failure Signatures**: Poor transfer performance indicates insufficient task diversity in training data or inadequate discriminator capacity. Correlation with ground-truth rewards below expected levels suggests the reward decomposition is not effectively capturing the common-sense component.

**3 First Experiments**:
1. Single-task baseline: Train independent IRL models on each task to establish performance without transfer
2. Shared discriminator ablation: Train with shared discriminator but only task-specific rewards to measure contribution of common-sense component
3. Zero-shot transfer: Evaluate performance on held-out tasks using only the learned common-sense reward

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions in the provided content.

## Limitations
- Evaluation primarily on Meta-world benchmark may not capture real-world scenario diversity
- Correlation of 0.88 with ground-truth rewards indicates room for improvement in reward accuracy
- Requires access to expert demonstrations across multiple tasks, which may be challenging to obtain
- Assumes common-sense behavior can be effectively disentangled from task-specific rewards

## Confidence
High - Core methodology and experimental results are well-documented and reproducible
Medium - Generalizability to diverse real-world scenarios remains to be validated
Medium - Effectiveness when scaling to larger task sets or more complex environments needs investigation

## Next Checks
1. Evaluate transferability of learned common-sense rewards to completely different domains (e.g., robotics to gaming) to test domain generalization
2. Conduct ablation studies quantifying common-sense vs task-specific reward contributions across different task types
3. Test method's robustness to varying quality and quantity of expert demonstrations across tasks for practical applicability assessment