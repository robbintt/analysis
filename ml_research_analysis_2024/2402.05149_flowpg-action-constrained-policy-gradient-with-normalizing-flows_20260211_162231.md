---
ver: rpa2
title: 'FlowPG: Action-constrained Policy Gradient with Normalizing Flows'
arxiv_id: '2402.05149'
source_url: https://arxiv.org/abs/2402.05149
tags:
- action
- policy
- actions
- flow
- valid
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of action-constrained reinforcement
  learning (ACRL), where agents must satisfy constraints on actions in safety-critical
  and resource-allocation tasks. The key idea is to use normalizing flows to learn
  an invertible mapping between the space of valid actions and a simple latent distribution
  (e.g., uniform), enabling generation of valid actions without solving optimization
  programs at each step.
---

# FlowPG: Action-constrained Policy Gradient with Normalizing Flows

## Quick Facts
- arXiv ID: 2402.05149
- Source URL: https://arxiv.org/abs/2402.05149
- Reference count: 40
- Key outcome: FlowPG achieves up to 99.98% valid action generation, reduces constraint violations by 10x, and accelerates training by 2-3x compared to prior ACRL methods.

## Executive Summary
This paper addresses action-constrained reinforcement learning (ACRL) by introducing FlowPG, which uses normalizing flows to learn an invertible mapping between valid actions and a simple latent distribution. This approach eliminates the need for expensive quadratic program (QP) projections at each step while maintaining high constraint satisfaction. The method integrates seamlessly with DDPG and demonstrates significant improvements in both constraint adherence and training efficiency across multiple continuous control benchmarks.

## Method Summary
FlowPG learns an invertible mapping between the valid action space and a simple latent distribution using normalizing flows. The flow model is trained on valid actions sampled via Hamiltonian Monte-Carlo (HMC) or Probabilistic Sentential Decision Diagrams (PSDD). During policy execution, the DDPG actor network outputs a latent variable that the flow transforms into a valid action. This integration eliminates the zero-gradient problem associated with action projection in traditional ACRL approaches.

## Key Results
- Achieves up to 99.98% valid action generation in benchmark domains
- Reduces constraint violations by up to 10x compared to baseline methods
- Accelerates training by 2-3x while maintaining or improving performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The normalizing flow model learns an invertible mapping between a simple latent distribution (uniform) and the valid action space, enabling valid action generation without QP solving at each step.
- Mechanism: The flow model is trained to maximize the log-likelihood of valid actions sampled from the feasible region. During policy execution, the policy network outputs a latent variable, which the flow transforms into a valid action via the learned mapping.
- Core assumption: The feasible action space can be effectively represented by a normalizing flow model trained on uniformly sampled valid actions.
- Evidence anchors:
  - [abstract] "first we use a normalizing flow model to learn an invertible, differentiable mapping between the feasible action space and the support of a simple distribution on a latent variable, such as Gaussian"
  - [section 3.1] "We employ normalizing flows to establish an invertible mapping between the support of a simple distribution and the space of valid actions"
  - [corpus] Weak - no direct citation of normalizing flow ACRL applications in neighbors

### Mechanism 2
- Claim: Using Hamiltonian Monte-Carlo (HMC) or PSDD sampling enables efficient generation of valid actions for training the normalizing flow, avoiding the inefficiency of rejection sampling.
- Mechanism: HMC samples valid actions by simulating particle movement in the feasible region using energy conservation. PSDD compiles linear constraints into a tractable structure that can be sampled from efficiently.
- Core assumption: The constraints defining the valid action space can be efficiently sampled using HMC for non-convex constraints or PSDD for linear constraints.
- Evidence anchors:
  - [section 3.2] "we use two methods described next. Hamiltonian Monte-Carlo (HMC) HMC is a Markov chain Monte Carlo method, which utilizes energy conservation to effectively sample a target distribution"
  - [section 3.2] "we propose to use a Probabilistic Sentential Decision Diagram (PSDD) to encode a uniform distribution over all the valid actions and then sample actions from the PSDD"
  - [corpus] Weak - neighbors discuss ACRL but not sampling methods

### Mechanism 3
- Claim: Integrating the normalizing flow with DDPG eliminates the zero gradient problem by avoiding action projection, enabling stable policy learning.
- Mechanism: The policy network outputs a latent variable that the flow maps to a valid action. Since no projection is needed, the gradient can flow through the policy network without being blocked by invalid actions.
- Core assumption: The flow model can be trained to produce valid actions with high accuracy, making the need for projection negligible during training.
- Evidence anchors:
  - [abstract] "This projection layer projects the unconstrained policy action onto the feasible action space by solving a quadratic program (QP)"
  - [section 3.3] "This integration allows us to incorporate the learned mapping directly into the original policy network of DDPG, alleviating the aforementioned issues"
  - [corpus] Weak - neighbors mention constraint violations but not zero gradient solutions

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and their extension to action-constrained MDPs
  - Why needed here: The paper frames ACRL as solving an action-constrained MDP, so understanding the MDP framework is essential for grasping the problem formulation.
  - Quick check question: What is the key difference between a standard MDP and an action-constrained MDP?

- Concept: Normalizing flows as generative models
  - Why needed here: The core technical contribution is using normalizing flows to map between latent space and valid actions, so understanding how flows work is critical.
  - Quick check question: How does a normalizing flow differ from other generative models like GANs or VAEs?

- Concept: Hamiltonian Monte-Carlo sampling
  - Why needed here: HMC is used to efficiently sample valid actions from the feasible region for training the flow model.
  - Quick check question: What is the key advantage of HMC over standard rejection sampling for constrained spaces?

## Architecture Onboarding

- Component map:
  Policy network (DDPG actor) -> Normalizing flow model -> Valid action
  Critic network (DDPG Q-function) -> State-action pair evaluation

- Critical path:
  1. Train normalizing flow on valid actions sampled via HMC/PSDD
  2. During RL training, policy network outputs latent variable
  3. Flow transforms latent variable to valid action
  4. Critic evaluates action, gradients flow back to policy

- Design tradeoffs:
  - Pre-training flow vs. online refinement: Pre-training is simpler but may miss state-specific valid actions
  - HMC vs. PSDD sampling: HMC handles non-convex constraints, PSDD handles linear constraints efficiently
  - Flow architecture complexity: More complex flows can model harder constraints but require more data and computation

- Failure signatures:
  - High constraint violation rate: Flow model not accurately representing valid action space
  - Slow training: Flow model not efficient enough, causing bottlenecks
  - Unstable gradients: Flow model accuracy insufficient, causing invalid actions to propagate

- First 3 experiments:
  1. Verify flow model can accurately transform uniform samples to valid actions on a simple constraint (e.g., circular constraint)
  2. Test HMC sampling efficiency compared to rejection sampling on the same constraint
  3. Integrate flow with a simple RL algorithm (e.g., DDPG) on a constrained environment and verify constraint satisfaction during training

## Open Questions the Paper Calls Out
None

## Limitations
- Lack of experimental validation for highly complex, non-convex constraints where HMC sampling might become computationally prohibitive
- 99.98% action validity claim relies on specific benchmark domains, generalization to more intricate constraint structures remains unverified
- Integration approach assumes flow accuracy remains stable throughout training without addressing potential degradation over time or under distributional shift

## Confidence

| Claim | Confidence |
|-------|------------|
| Flow model action validity (99.98%) | High |
| 2-3x training speed improvement | Medium |
| HMC sampling efficiency | Low-Medium |

## Next Checks
1. Test flow model performance under distributional shift - retrain on one constraint set, evaluate on modified constraints
2. Benchmark computational cost of HMC sampling vs. PSDD as constraint complexity increases
3. Evaluate long-term stability by running extended training sessions to detect potential flow accuracy degradation