---
ver: rpa2
title: Exploring and Improving Drafts in Blockwise Parallel Decoding
arxiv_id: '2404.09221'
source_url: https://arxiv.org/abs/2404.09221
tags:
- rescoring
- efficiency
- block
- parallel
- heads
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores two strategies for improving blockwise parallel
  decoding (BPD): local rescoring via neural models and global rescoring via n-gram
  models. The authors find that BPD drafts often contain consecutive repetitions and
  that the model''s confidence decreases for later tokens.'
---

# Exploring and Improving Drafts in Blockwise Parallel Decoding

## Quick Facts
- arXiv ID: 2404.09221
- Source URL: https://arxiv.org/abs/2404.09221
- Reference count: 25
- Key outcome: This paper explores two strategies for improving blockwise parallel decoding (BPD): local rescoring via neural models and global rescoring via n-gram models. The authors find that BPD drafts often contain consecutive repetitions and that the model's confidence decreases for later tokens. They propose two methods to improve BPD drafts: a local neural rescoring method that refines the predictions of each head independently, and a global n-gram rescoring method that finds the most likely sequence of tokens from the BPD lattice. The authors show that both methods can improve block efficiency, with the global n-gram rescoring method achieving up to 21% improvement.

## Executive Summary
This paper investigates blockwise parallel decoding (BPD), a decoding strategy that predicts multiple tokens in parallel to improve inference speed. The authors analyze BPD drafts and find that they often contain consecutive repetitions and that the model's confidence decreases for later tokens. To address these issues, they propose two methods for improving BPD drafts: a local neural rescoring method that refines the predictions of each head independently, and a global n-gram rescoring method that finds the most likely sequence of tokens from the BPD lattice. The authors demonstrate that both methods can significantly improve block efficiency, with the global n-gram rescoring method achieving up to 21% improvement on various language modeling and question answering tasks.

## Method Summary
The paper proposes two methods to improve BPD drafts: local neural rescoring and global n-gram rescoring. The local neural rescoring method uses a small neural language model to rescore the top-k predictions from each BPD head, while the global n-gram rescoring method uses an n-gram language model to find the most likely sequence of tokens from the BPD lattice. The authors train a 1.5B parameter BPD model with up to 9 blockwise heads on the C4 corpus and finetune it on downstream tasks. They then generate top-16 lattices from BPD predictions and apply both rescoring methods to measure the improvement in block efficiency.

## Key Results
- BPD drafts often contain consecutive repetitions and the model's confidence decreases for later tokens.
- Local neural rescoring with a small neural LM can reduce token-level repetitions in drafts.
- Global n-gram rescoring can find the most likely sequence of tokens from the BPD lattice, improving block efficiency by up to 21%.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Rescoring the top-k BPD lattice with a small neural LM reduces token-level repetitions in drafts.
- **Mechanism:** The neural LM reorders predictions at each head based on global fluency, favoring coherent sequences over repeated tokens.
- **Core assumption:** Local coherence constraints (e.g., avoiding repetition) can be captured by a small neural LM conditioned on partial sequences.
- **Evidence anchors:**
  - [abstract] "We propose two methods to improve BPD drafts: a local neural rescoring method that refines the predictions of each head independently..."
  - [section] "We observe that vanilla BPD drafts are prone to token-level repetition... Rescoring the BPD top-k lattice with even a simple language model eliminates a significant amount of repetition..."
  - [corpus] Weak - no direct evidence on repetition removal in corpus; only that similar work exists on refining autoregressive drafts.
- **Break condition:** If the rescoring model lacks sufficient capacity to model fluency over the context window, repetitions may persist.

### Mechanism 2
- **Claim:** Confidence decreases across BPD heads, correlating with block efficiency.
- **Mechanism:** Entropy increases from early to later heads, indicating higher uncertainty; lower entropy heads contribute more accepted tokens.
- **Core assumption:** Entropy of softmax output is a reliable proxy for prediction confidence and quality.
- **Evidence anchors:**
  - [abstract] "We analyze the distribution of probabilities within each softmax head... the BPD drafter tends to be more confident with initial tokens, and becomes progressively less confident for subsequent tokens."
  - [section] "Our empirical analysis reveals a key property of BPD: the BPD drafter tends to be more confident with initial tokens, and becomes progressively less confident for subsequent tokens... We observed a strong correlation between hmax and block efficiency..."
  - [corpus] Weak - corpus contains related decoding works but none directly measuring entropy-confidence correlation.
- **Break condition:** If the entropy-confidence relationship is not monotonic or varies across tasks, the correlation may not hold.

### Mechanism 3
- **Claim:** N-gram rescoring finds globally most likely sequences from the BPD lattice via dynamic programming.
- **Mechanism:** An n-gram model rescoring the top-k BPD lattice identifies the most probable sequence, which is more likely to match the autoregressive LM's greedy decode.
- **Core assumption:** N-gram models can capture enough context to distinguish fluent from disfluent sequences within the short BPD block.
- **Evidence anchors:**
  - [abstract] "Secondly, we leverage this analysis to develop algorithms to improve BPD inference speed by refining the BPD drafts using n-gram... language models."
  - [section] "We propose to use an n-gram model to efficiently rescore all paths... and generate the p most probable rescored paths as a batch of draft candidates."
  - [corpus] Weak - no direct evidence in corpus; similar works focus on speculative decoding rather than n-gram lattice rescoring.
- **Break condition:** If the n-gram model's context window is too short to capture dependencies across the full block, global optimality may not be achieved.

## Foundational Learning

- **Concept:** Blockwise Parallel Decoding (BPD)
  - Why needed here: Core framework being analyzed and improved; understanding BPD is prerequisite to understanding rescoring contributions.
  - Quick check question: In BPD, how are multiple tokens predicted in parallel, and what challenge does this pose compared to autoregressive decoding?
- **Concept:** Entropy as a measure of prediction confidence
  - Why needed here: Used to analyze head-wise confidence and its correlation with block efficiency.
  - Quick check question: If a softmax distribution has low entropy, what does that imply about the model's confidence in its prediction?
- **Concept:** Lattice rescoring and dynamic programming
  - Why needed here: Both neural and n-gram rescoring operate on a lattice of candidate sequences; understanding this is key to grasping the algorithms.
  - Quick check question: What data structure represents all possible token sequences from the BPD heads, and how can it be efficiently rescored?

## Architecture Onboarding

- **Component map:** BPD LM with h blockwise heads (each a small FFN + softmax) -> Top-k selection function to build the lattice -> Rescoring models (neural LM or n-gram LM) -> Verification stage (base LM greedy decode) -> Acceptance logic (longest matching prefix)
- **Critical path:** Draft generation -> Top-k lattice construction -> Rescoring -> Parallel verification -> Sequence acceptance
- **Design tradeoffs:**
  - Local vs. global rescoring: Local (neural) uses unbounded context but rescoring per position; global (n-gram) considers whole sequence but with fixed context window.
  - Neural vs. n-gram rescoring: Neural potentially more expressive but slower; n-gram faster via dynamic programming but limited by n-gram order.
  - Top-k size: Larger k gives more candidates but increases rescoring cost exponentially for neural, polynomially for n-gram.
- **Failure signatures:**
  - Low block efficiency despite rescoring: Rescoring model too weak or mismatched to BPD domain.
  - Increased repetition after rescoring: Rescoring model not capturing fluency constraints.
  - No speedup: Rescoring overhead exceeds gains from improved block efficiency.
- **First 3 experiments:**
  1. Run BPD with and without neural rescoring on LAMBADA; compare block efficiency and repetition rate.
  2. Sweep interpolation weight α for neural rescoring; measure impact on block efficiency across tasks.
  3. Compare 2-gram vs. 4-gram rescoring on SQuAD V1; measure speedup and acceptance rate.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the block efficiency of BPD rescoring methods scale with larger language models beyond 1.5 billion parameters?
- Basis in paper: [explicit] The paper acknowledges that the study is based on 1.5 billion parameter models and leaves the question of scaling to larger models as future work.
- Why unresolved: The paper does not provide empirical results or theoretical analysis for larger models, leaving uncertainty about whether the observed improvements in block efficiency will persist or diminish as model size increases.
- What evidence would resolve it: Empirical results showing block efficiency improvements for BPD rescoring methods on language models with significantly more parameters (e.g., 10 billion or more) would provide direct evidence. Additionally, theoretical analysis explaining how rescoring benefits might scale with model size could offer insights into expected performance trends.

### Open Question 2
- Question: What is the impact of incorporating non-greedy sampling strategies into the BPD framework on block efficiency and overall model performance?
- Basis in paper: [explicit] The paper focuses on refining BPD drafts for greedy decoding and mentions extending the methodology to non-greedy sampling strategies as an area for future work.
- Why unresolved: The paper does not explore or provide results for non-greedy sampling strategies, leaving uncertainty about how such strategies might affect the efficiency and quality of BPD-generated text.
- What evidence would resolve it: Experimental results comparing block efficiency and generation quality between greedy and non-greedy sampling strategies in the BPD framework would provide direct evidence. Analysis of how different sampling strategies interact with rescoring methods could also offer insights.

### Open Question 3
- Question: How do advanced training methodologies for drafting heads influence the predictive accuracy and efficiency of the BPD framework?
- Basis in paper: [explicit] The paper identifies the implementation of drafting heads as a foundational aspect that leaves room for advanced development and suggests that future iterations could benefit from more sophisticated training methods.
- Why unresolved: The paper does not explore or implement advanced training methodologies for drafting heads, leaving uncertainty about how such methods might enhance the BPD framework's performance.
- What evidence would resolve it: Empirical results demonstrating improved block efficiency and generation quality with advanced training methodologies for drafting heads would provide direct evidence. Comparative studies between standard and advanced training methods could also offer insights into the potential benefits.

## Limitations
- Weak corpus support with limited related works (25 papers) and low neighbor FMR (0.434).
- Empirical validation gaps in exploring failure modes and cross-lingual generalization.
- Limited architectural details for neural rescoring models, making reproduction and comparison challenging.

## Confidence
**High confidence:**
- Analysis of BPD drafts revealing token-level repetitions and decreasing confidence across heads.
- Improvement in block efficiency from both neural and n-gram rescoring.

**Medium confidence:**
- Claim that neural rescoring with interpolation weight α is necessary for optimal performance.
- Assertion that n-gram rescoring is more effective than neural rescoring.

**Low confidence:**
- Claim that the proposed methods will generalize to other languages or modalities.

## Next Checks
1. **Failure mode analysis:** Conduct experiments to understand how rescoring methods perform when the BPD model is less confident across all heads, or when the base LM verification is particularly slow.
2. **Cross-lingual evaluation:** Evaluate the effectiveness of neural and n-gram rescoring on non-English language tasks to assess the generalization of the proposed methods.
3. **Architectural ablation:** Perform an ablation study on the neural rescoring models to understand the impact of different architectural choices (e.g., layer depth, hidden dimension size) on block efficiency improvement.