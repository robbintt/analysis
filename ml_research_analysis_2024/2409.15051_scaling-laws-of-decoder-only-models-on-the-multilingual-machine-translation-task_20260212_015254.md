---
ver: rpa2
title: Scaling Laws of Decoder-Only Models on the Multilingual Machine Translation
  Task
arxiv_id: '2409.15051'
source_url: https://arxiv.org/abs/2409.15051
tags:
- scaling
- translation
- training
- data
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores scaling laws of decoder-only models for multilingual
  machine translation, an area previously dominated by encoder-decoder models. The
  authors trained six decoder-only models (70M to 7B parameters) on a multilingual
  and multidomain dataset and investigated how loss and translation quality scale
  with model size, dataset size, and architectural choices.
---

# Scaling Laws of Decoder-Only Models on the Multilingual Machine Translation Task

## Quick Facts
- **arXiv ID**: 2409.15051
- **Source URL**: https://arxiv.org/abs/2409.15051
- **Reference count**: 16
- **Primary result**: Decoder-only models for multilingual machine translation follow power-law scaling relationships, with larger models achieving lower loss and higher BLEU/COMET scores, but scaling laws do not generalize well beyond certain model sizes or to different data distributions.

## Executive Summary
This paper investigates the scaling behavior of decoder-only models for multilingual machine translation, an area previously dominated by encoder-decoder architectures. The authors trained six decoder-only models ranging from 70M to 7B parameters on a multilingual and multidomain dataset to examine how loss and translation quality scale with model size, dataset size, and architectural choices. They discovered that decoder-only translation models exhibit power-law scaling relationships similar to large language models, with larger models achieving better performance. However, the scaling laws identified were found to be specific to the tested model size range and dataset distribution, limiting their generalizability. The study also revealed that scaling model width or depth yielded similar performance gains, but width scaling was more computationally efficient. A significant technical finding was the identification of an issue with the use of the <eos> token in packed training batches, which was resolved by adding a start-of-translation token before source sentences.

## Method Summary
The authors trained six decoder-only transformer models with parameter counts ranging from 70M to 7B on a multilingual and multidomain dataset. They investigated scaling relationships by varying model size, dataset size, and architectural dimensions (width vs depth). The study employed power-law regression to analyze how loss and translation quality metrics (BLEU and COMET) scaled with these factors. The researchers also examined the effects of model width and depth scaling on performance and computational efficiency. Additionally, they identified and resolved a technical issue related to the use of the <eos> token in packed training batches by introducing a start-of-translation token.

## Key Results
- Decoder-only translation models follow power-law scaling relationships similar to large language models, with larger models achieving lower loss and higher BLEU/COMET scores
- Scaling laws identified are specific to the tested model size range (70M-7B parameters) and dataset distribution, limiting generalizability
- Scaling model width or depth yields similar performance gains, but width scaling is more computationally efficient

## Why This Works (Mechanism)
Decoder-only models can effectively handle translation tasks by leveraging their ability to model sequential dependencies and generate output tokens autoregressively. The power-law scaling behavior observed is consistent with findings from large language models, suggesting that model capacity and data quantity are key factors in improving translation quality. The use of a start-of-translation token addresses the ambiguity in packed training batches by providing a clear demarcation between source sentences, enabling more effective learning of translation patterns.

## Foundational Learning
1. **Power-law scaling**: Relationship between model size/data quantity and performance follows a predictable mathematical pattern (why needed: to understand how to optimally allocate resources for model development; quick check: verify power-law regression coefficients are statistically significant)
2. **Decoder-only architecture**: Models that generate output tokens autoregressively without explicit encoding of input (why needed: to understand the model's fundamental approach to translation; quick check: confirm model generates translations by predicting next token conditioned on previous tokens)
3. **Multilingual machine translation**: Translating between multiple language pairs simultaneously (why needed: to contextualize the study's focus on a challenging and practical task; quick check: verify dataset contains multiple language pairs)
4. **Packed training batches**: Combining multiple sequences into a single batch to improve computational efficiency (why needed: to understand the technical context of the <eos> token issue; quick check: confirm model uses packed batches during training)
5. **BLEU and COMET metrics**: Automated metrics for evaluating translation quality (why needed: to assess model performance objectively; quick check: verify reported scores are consistent with human evaluations)
6. **Width vs depth scaling**: Trade-offs between increasing model width (number of channels) and depth (number of layers) (why needed: to understand architectural design choices and their impact on performance; quick check: compare FLOPs and parameter counts for width vs depth scaling)

## Architecture Onboarding

**Component map**: Multilingual dataset -> Model training -> Loss calculation -> BLEU/COMET evaluation -> Power-law regression analysis

**Critical path**: Data preprocessing -> Model training with packed batches -> Translation generation -> Quality evaluation -> Scaling analysis

**Design tradeoffs**: Width vs depth scaling (similar performance gains, but width is more computationally efficient); model size vs. dataset size (both contribute to improved performance following power-law relationships)

**Failure signatures**: Scaling laws not generalizing beyond tested model size range; issues with <eos> token in packed training batches leading to ambiguity in translation generation

**First experiments**: 1) Train decoder-only models of varying sizes on multilingual dataset; 2) Evaluate translation quality using BLEU and COMET metrics; 3) Analyze scaling relationships using power-law regression

## Open Questions the Paper Calls Out
- How do scaling laws behave for decoder-only models beyond the 7B parameter limit?
- Do the identified scaling laws generalize to datasets with different characteristics (e.g., different language pairs, domains, or data distributions)?
- Are there alternative solutions to the <eos> token issue in packed training batches that may be more effective than the proposed start-of-translation token approach?

## Limitations
- Scaling laws identified may not generalize beyond the tested model size range (70M to 7B parameters) or to datasets with different characteristics
- The study did not investigate the effects of model scaling beyond the 7B parameter limit
- The dataset used was a combination of public and proprietary data, which may not be fully representative of real-world multilingual translation tasks

## Confidence
- **High**: Decoder-only translation models follow a power-law scaling relationship similar to large language models, with larger models achieving lower loss and higher BLEU/COMET scores
- **Medium**: Scaling model width or depth yielded similar performance gains, but width scaling was more computationally efficient
- **Medium**: The identification of the <eos> token issue in packed training batches and its resolution through the addition of a start-of-translation token

## Next Checks
1. Evaluate the scaling laws of decoder-only models beyond the 7B parameter limit to determine if the observed power-law relationship holds for larger models
2. Test the generalizability of the scaling laws to datasets with different characteristics, such as varying language pairs, domains, or data distributions, to assess the robustness of the findings
3. Investigate alternative solutions for the <eos> token issue in packed training batches and compare their effectiveness to the proposed start-of-translation token approach