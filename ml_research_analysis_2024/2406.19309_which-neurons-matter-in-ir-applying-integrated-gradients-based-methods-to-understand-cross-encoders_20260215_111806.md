---
ver: rpa2
title: Which Neurons Matter in IR? Applying Integrated Gradients-based Methods to
  Understand Cross-Encoders
arxiv_id: '2406.19309'
source_url: https://arxiv.org/abs/2406.19309
tags:
- neurons
- attribution
- https
- datasets
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper adapts Neuron Integrated Gradients (NIG) to study the
  inner workings of MonoBERT, a cross-encoder model for Information Retrieval (IR).
  NIG computes the importance of individual neurons by integrating gradients along
  a path from a baseline input to the actual input.
---

# Which Neurons Matter in IR? Applying Integrated Gradients-based Methods to Understand Cross-Encoders

## Quick Facts
- arXiv ID: 2406.19309
- Source URL: https://arxiv.org/abs/2406.19309
- Reference count: 40
- The paper adapts Neuron Integrated Gradients (NIG) to study MonoBERT, identifying neurons critical for relevance classification and domain adaptation

## Executive Summary
This paper introduces Neuron Integrated Gradients (NIG) to analyze individual neuron contributions in cross-encoder models for Information Retrieval. The authors apply NIG to MonoBERT, computing attributions that identify which neurons are important for classifying passages as relevant or non-relevant to queries. The study examines multiple datasets including MSMARCO and BEIR collections, revealing distinct neuron sets for relevance, non-relevance, and out-of-domain adaptation. Ablation experiments confirm that removing neurons identified by NIG significantly degrades IR performance, validating their importance to the model's functionality.

## Method Summary
The paper adapts integrated gradients, a model interpretability technique from computer vision, to analyze transformer-based cross-encoders in IR. The authors compute Neuron Integrated Gradients (NIG) by integrating gradients along paths from baseline inputs to actual query-passage pairs. They apply this to MonoBERT, identifying individual neuron importance scores for relevance classification. The method examines static snapshots of neuron activity across multiple datasets, computing attributions that distinguish neurons involved with in-domain versus out-of-domain data. Ablation studies remove identified neurons to measure impact on IR metrics, providing empirical validation of attribution results.

## Key Results
- NIG successfully identifies neurons whose removal degrades IR performance, confirming their importance for relevance classification
- Distinct sets of neurons are identified for relevance versus non-relevance classification
- A separate set of neurons is found to be important specifically for out-of-domain data adaptation
- Ablation studies show measurable performance degradation when top-attributed neurons are removed

## Why This Works (Mechanism)
The method works by computing gradients of the relevance score with respect to neuron activations along paths from baseline inputs to actual inputs. This integrated gradient approach captures the cumulative contribution of each neuron to the final relevance prediction. By examining how neuron activations change from a neutral baseline to the actual query-passage input, NIG can attribute importance scores to individual neurons. The integration over the path smooths out local variations and provides a more stable attribution than single-point gradient computations.

## Foundational Learning

**Integrated Gradients**: A model interpretability method that computes feature importance by integrating gradients along a path from baseline to actual input. Why needed: Provides a principled way to attribute relevance scores to individual neurons rather than treating the model as a black box. Quick check: Verify that attributions sum to the difference between baseline and actual predictions.

**Cross-Encoder Architecture**: A transformer model that takes concatenated query-passage pairs as input and jointly processes them. Why needed: Understanding the model structure is essential for interpreting neuron-level attributions. Quick check: Confirm that the model produces relevance scores from the [CLS] token representation.

**Neuron Attribution**: The process of assigning importance scores to individual neurons based on their contribution to model outputs. Why needed: Enables granular analysis beyond layer or attention head level. Quick check: Validate that removing high-attribution neurons causes larger performance drops.

**Ablation Studies**: Experimental validation where identified important components are removed to measure impact on performance. Why needed: Provides empirical confirmation that attribution scores correspond to actual functional importance. Quick check: Compare performance degradation across different ablation methods.

## Architecture Onboarding

**Component Map**: Input (query+passage) -> Transformer Encoder Layers -> [CLS] Token Representation -> Relevance Score

**Critical Path**: Query-passage concatenation → BERT tokenization → Multi-head attention → Feed-forward networks → Pooling [CLS] → Relevance classifier

**Design Tradeoffs**: The cross-encoder architecture provides strong performance by jointly modeling query-passage interactions but at high computational cost compared to dual-encoders. Neuron-level analysis requires static snapshots rather than dynamic tracking during training.

**Failure Signatures**: Attribution instability when baseline inputs are poorly chosen, spurious attributions when relevance scores are saturated, and attribution noise in later layers where gradients may be small.

**3 First Experiments**:
1. Compute NIG attributions on a small subset of MSMARCO data to verify implementation correctness
2. Perform neuron ablation on a single layer to confirm attribution-method correspondence
3. Compare attributions across different baseline inputs to assess sensitivity

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis limited to MonoBERT architecture, limiting generalizability to other transformer-based IR models
- Fixed template baseline input may not represent a truly neutral starting point, potentially biasing attributions
- Neuron removal ablation represents an extreme intervention that may not reflect natural neuron contributions
- Static snapshot analysis misses temporal dynamics of neuron importance during training or fine-tuning

## Confidence

**High Confidence**: Identification of neurons contributing to relevance classification, supported by ablation studies showing performance degradation when these neurons are removed.

**Medium Confidence**: Distinction between relevance and non-relevance neuron sets, as attribution method shows separation but functional significance requires further explanation.

**Medium Confidence**: Finding about out-of-domain neuron sets, though interpretation as domain adaptation mechanisms is somewhat speculative.

## Next Checks

1. Apply the same NIG methodology to alternative cross-encoder architectures (T5, other BERT variants) to assess generalizability of neuron importance patterns across model families.

2. Conduct ablation studies with partial neuron masking or weight scaling rather than complete removal to determine if the same attribution patterns emerge with less aggressive interventions.

3. Perform ablation studies on a held-out test set from MSMARCO that was not used in the original attribution computation to verify that identified neurons maintain their importance across different data splits.