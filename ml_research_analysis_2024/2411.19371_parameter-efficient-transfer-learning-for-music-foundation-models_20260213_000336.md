---
ver: rpa2
title: Parameter-Efficient Transfer Learning for Music Foundation Models
arxiv_id: '2411.19371'
source_url: https://arxiv.org/abs/2411.19371
tags:
- fine-tuning
- music
- learning
- foundation
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper explores parameter-efficient transfer learning (PETL)\
  \ for adapting music foundation models to downstream tasks, addressing challenges\
  \ in both probing (frozen weights) and fine-tuning (computational cost, overfitting).\
  \ Three PETL approaches\u2014adapter-based, prompt-based, and reparameterization-based\
  \ methods\u2014are introduced, training only small parameter subsets."
---

# Parameter-Efficient Transfer Learning for Music Foundation Models

## Quick Facts
- arXiv ID: 2411.19371
- Source URL: https://arxiv.org/abs/2411.19371
- Authors: Yiwei Ding; Alexander Lerch
- Reference count: 0
- Key outcome: PETL methods outperform both probing and fine-tuning on music auto-tagging, and achieve similar results to fine-tuning on key detection and tempo estimation with significantly less training cost.

## Executive Summary
This paper explores parameter-efficient transfer learning (PETL) methods for adapting music foundation models to downstream tasks, addressing the computational cost and overfitting issues of full fine-tuning. The authors introduce three categories of PETL approaches - adapter-based, prompt-based, and reparameterization-based methods - which train only small parameter subsets while keeping most foundation model weights frozen. Results show PETL methods significantly outperform probing and fine-tuning on music auto-tagging tasks, and achieve comparable performance to fine-tuning on key detection and tempo estimation with 2-3x speedup and only 0.2-0.36% of parameters being trained.

## Method Summary
The study applies PETL methods to two music foundation models (MusicFM and MERT) for three downstream tasks: music auto-tagging, key detection, and tempo estimation. PETL approaches include adapter-based methods (bottleneck adapters), prompt-based methods (prompt-tuning, prefix-tuning), and reparameterization-based methods (BitFit, SSF, LoRA). The foundation models are pre-trained on large music datasets, and features are extracted from the first 6 middle layers. Classification or regression heads are added on top, with only PETL parameters being trained while the foundation model weights remain frozen. Performance is compared against probing (frozen weights with trainable head), full fine-tuning, and small models trained from scratch.

## Key Results
- PETL methods significantly outperform both probing and fine-tuning on music auto-tagging tasks across MagnaTagATune and MTG-Jamendo datasets
- For key detection and tempo estimation, PETL methods achieve similar results to fine-tuning with only 0.22-0.36% of parameters being trained
- Training small models from scratch achieves competitive results to using foundation models for key detection and tempo estimation, questioning the utility of current music foundation models for these tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PETL methods achieve better results than full fine-tuning on music auto-tagging by training only a small number of parameters while avoiding overfitting.
- Mechanism: Adapter-based, prompt-based, and reparameterization-based methods introduce task-specific modifications without updating all foundation model weights, reducing parameters from millions to thousands.
- Core assumption: Freezing most foundation model weights while training only a small subset can still effectively adapt the model to downstream tasks.
- Evidence anchors: PETL outperforms both probing and fine-tuning on music auto-tagging; adapter and LoRA achieve results with only 0.36% and 0.22% trainable parameters with 3x and 2.5x speedup.

### Mechanism 2
- Claim: For key detection and tempo estimation tasks, fine-tuning often outperforms PETL methods because these tasks require more fundamental changes to learned representations.
- Mechanism: These tasks rely on low-level tonal and temporal information that may not be well-represented in foundation models pre-trained on general music features.
- Core assumption: Foundation models learn more high-level timbre information during pre-training than low-level tonal and temporal information.
- Evidence anchors: Feature correlation analysis shows weaker correlation for key/tempo tasks compared to auto-tagging; catastrophic forgetting occurs when transferring to key/tempo tasks.

### Mechanism 3
- Claim: Training a small model from scratch can achieve competitive results to using foundation models for key detection and tempo estimation.
- Mechanism: For tasks requiring low-level musical features, a smaller model trained directly on the specific task may learn more relevant representations than adapting a large foundation model.
- Core assumption: The computational and representational overhead of large foundation models may not provide benefits for certain downstream tasks.
- Evidence anchors: Small models trained from scratch give competitive results on key detection and tempo estimation compared to foundation model approaches.

## Foundational Learning

- Concept: Transfer learning and fine-tuning
  - Why needed here: Understanding how to adapt pre-trained models to new tasks is central to this work.
  - Quick check question: What is the difference between probing and fine-tuning when adapting a foundation model to a new task?

- Concept: Parameter-efficient methods (adapters, prompts, reparameterization)
  - Why needed here: The paper introduces three types of PETL methods that modify foundation models with small trainable parameter subsets.
  - Quick check question: How does an adapter-based method modify a pre-trained model without updating all its parameters?

- Concept: Music representation learning
  - Why needed here: Foundation models are designed to learn general music representations, understanding what features they capture is important for interpreting results.
  - Quick check question: What types of musical information (timbre, tonal, temporal) do you think foundation models are best at capturing based on their pre-training objectives?

## Architecture Onboarding

- Component map: MusicFM/MERT (foundation models) -> PETL methods (adapters, prompts, reparameterization) -> Classification/regression head -> Downstream task datasets

- Critical path: 1) Load pre-trained foundation model, 2) Select 6 middle layers for feature extraction, 3) Apply chosen PETL method, 4) Add classification/regression head, 5) Train only PETL parameters, 6) Evaluate performance

- Design tradeoffs:
  - Training efficiency vs. performance: PETL methods use fewer parameters but may not achieve same performance as full fine-tuning for all tasks
  - Task specificity vs. generalization: More task-specific adaptation risks overfitting on small datasets
  - Model complexity vs. interpretability: Foundation models are complex, making it harder to understand why certain tasks work better

- Failure signatures:
  - Poor performance on tonal/temporal tasks compared to auto-tagging
  - Catastrophic forgetting when transferring to key/tempo tasks
  - No significant improvement over training small models from scratch for certain tasks

- First 3 experiments:
  1. Implement adapter-based method with bottleneck dimension of 16 on MusicFM for music auto-tagging on MagnaTagATune dataset
  2. Compare probing vs. full fine-tuning vs. LoRA on MERT for key detection on GTZAN dataset
  3. Train a small CNN from scratch for tempo estimation on GiantSteps dataset and compare against foundation model approaches

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do current music foundation models provide meaningful advantages for tonal and temporal analysis tasks compared to training small models from scratch?
- Basis in paper: Small models trained from scratch achieve competitive results on key detection and tempo estimation
- Why unresolved: Study only compares to one deep learning baseline and doesn't explore broader range of architectures
- What evidence would resolve it: Systematic comparisons of foundation models against various small model architectures on same tasks

### Open Question 2
- Question: Would different tokenization methods during pre-training significantly impact foundation model performance on downstream MIR tasks?
- Basis in paper: Study uses MusicFM with random projection/codebook tokenization and MERT with EnCodec tokenization
- Why unresolved: Only examines two specific tokenization approaches without systematic comparison
- What evidence would resolve it: Direct comparison of foundation models pre-trained with various tokenization schemes on same downstream tasks

### Open Question 3
- Question: Can PETL methods maintain effectiveness for frame-level prediction tasks like chord recognition and beat tracking?
- Basis in paper: Exploration limited to song-level prediction; frame-level tasks remain open question
- Why unresolved: Study focuses on song-level tasks and doesn't evaluate PETL methods on frame-level prediction
- What evidence would resolve it: Applying PETL methods to frame-level tasks and comparing performance to probing and full fine-tuning

## Limitations

- PETL methods show limitations for tonal/temporal tasks, with small models trained from scratch achieving competitive results
- Feature correlation analysis provides indirect rather than explicit evidence about what musical features foundation models learn
- Implementation details for deep learning baseline models are not fully specified, making fair comparison difficult

## Confidence

- High confidence: PETL methods outperform probing and fine-tuning on music auto-tagging tasks
- Medium confidence: PETL methods achieve similar results to fine-tuning on key detection and tempo estimation with significantly less training cost
- Medium confidence: Foundation models learn more high-level timbre information than low-level tonal/temporal information during pre-training

## Next Checks

1. **Feature Analysis**: Conduct explicit analysis of what musical features (timbre, tonal, temporal) are captured at different layers of foundation models to validate learning capability hypotheses.

2. **Baseline Implementation**: Implement and evaluate deep learning baseline models with full specifications to ensure fair comparison with foundation model approaches.

3. **Cross-task Transfer**: Test whether pre-training foundation models with tonal/temporal tasks improves their performance on key detection and tempo estimation through PETL methods.