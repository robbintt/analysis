---
ver: rpa2
title: 'Towards AI-$45^{\circ}$ Law: A Roadmap to Trustworthy AGI'
arxiv_id: '2412.14186'
source_url: https://arxiv.org/abs/2412.14186
tags:
- arxiv
- safety
- trustworthiness
- preprint
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes the AI-45\xB0 Law as a guiding principle for\
  \ balanced development of AI capabilities and safety, addressing the current imbalance\
  \ where AI safety lags behind rapid capability advancements. The authors introduce\
  \ the Causal Ladder of Trustworthy AGI framework, consisting of three layers (Approximate\
  \ Alignment, Intervenable, and Reflectable) inspired by Judea Pearl's Ladder of\
  \ Causation."
---

# Towards AI-$45^{\circ}$ Law: A Roadmap to Trustworthy AGI

## Quick Facts
- arXiv ID: 2412.14186
- Source URL: https://arxiv.org/abs/2412.14186
- Authors: Chao Yang; Chaochao Lu; Yingchun Wang; Bowen Zhou
- Reference count: 40
- Primary result: Proposes AI-45° Law framework for balanced AI capability and safety development

## Executive Summary
This paper addresses the critical imbalance in AI development where capabilities advance faster than safety measures, creating what the authors call "crippled AI." The authors propose the AI-45° Law as a guiding principle for synchronized development of AI capabilities and safety, visualized as balanced progress along a 45° line. The framework introduces the Causal Ladder of Trustworthy AGI, consisting of three layers (Approximate Alignment, Intervenable, and Reflectable) inspired by Judea Pearl's Ladder of Causation, to create AI systems that are both highly capable and aligned with human values.

## Method Summary
The authors introduce the Causal Ladder of Trustworthy AGI framework, which builds on Pearl's Ladder of Causation to create a roadmap for balanced AI development. The framework consists of three layers: Approximate Alignment (correlation-based methods like supervised fine-tuning), Intervenable (intervention-based methods like RLHF), and Reflectable (counterfactual reasoning methods like world models). The approach defines five levels of trustworthy AGI: perception, reasoning, decision-making, autonomy, and collaboration trustworthiness. The framework aims to ensure that safety measures keep pace with capability advancements, with governance measures to support development as a global public good.

## Key Results
- Introduces the AI-45° Law principle for balanced development of AI capabilities and safety
- Proposes the Causal Ladder of Trustworthy AGI framework with three distinct layers
- Defines five levels of trustworthy AGI ranging from perception to collaboration trustworthiness
- Identifies governance challenges including lifecycle management and multi-stakeholder involvement

## Why This Works (Mechanism)
The framework works by providing a structured progression from basic correlation-based methods to sophisticated counterfactual reasoning capabilities. By following the Causal Ladder, developers can systematically address increasingly complex safety challenges as AI systems become more capable. The 45° line visualization ensures that safety measures advance in tandem with capabilities, preventing the current imbalance where powerful systems lack adequate safeguards. The multi-stakeholder governance approach recognizes AI safety as a global public good requiring coordinated international efforts.

## Foundational Learning
1. **AI-45° Law Principle**: The concept that AI capability and safety should develop in synchronized, balanced fashion
   - Why needed: Current AI development shows capability outpacing safety measures
   - Quick check: Plot capability vs safety metrics to verify 45° trajectory

2. **Causal Ladder of AGI**: Three-layer hierarchy (Approximate Alignment, Intervenable, Reflectable) for building trustworthy AI
   - Why needed: Provides structured approach to increasingly complex safety challenges
   - Quick check: Map existing AI systems to appropriate ladder layer

3. **Five Levels of Trustworthiness**: Perception, reasoning, decision-making, autonomy, and collaboration trustworthiness
   - Why needed: Defines clear progression for trustworthy AGI development
   - Quick check: Assess current AI systems against each trustworthiness level

## Architecture Onboarding
- **Component Map**: Data/Environment -> Approximate Alignment Layer -> Intervenable Layer -> Reflectable Layer -> Trustworthy AGI System
- **Critical Path**: Balanced capability-safety development along 45° trajectory
- **Design Tradeoffs**: Capability advancement vs safety implementation, global governance vs innovation speed
- **Failure Signatures**: Capability outpacing safety (crippled AI), safety measures reducing usability, governance bottlenecks
- **First Experiments**:
  1. Map existing AI systems to trustworthiness matrix to identify current development state
  2. Implement basic correlation-based safety measures and measure capability impact
  3. Develop evaluation metrics for tracking 45° line progress

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What are the measurable indicators that can serve as early warning thresholds (Yellow Line) before AI systems reach potentially catastrophic levels of capability?
- Basis in paper: The paper introduces the "Yellow Line" concept as early warning indicators that signal when AI capabilities approach dangerous levels
- Why unresolved: The paper acknowledges the need for scientific consensus on AI risks and threshold boundaries but does not specify concrete metrics
- What evidence would resolve it: Research demonstrating specific quantitative or qualitative metrics that reliably predict dangerous capability thresholds

### Open Question 2
- Question: How can the Causal Ladder of Trustworthy AGI framework be empirically validated across different AI architectures and domains?
- Basis in paper: The framework is presented as a theoretical model without empirical validation methods
- Why unresolved: The framework is described as "practical" but lacks operationalization details
- What evidence would resolve it: Systematic studies testing AI systems at each level showing correlation with measurable safety improvements

### Open Question 3
- Question: What governance mechanisms can effectively ensure AI safety as a global public good while balancing innovation and security?
- Basis in paper: The paper discusses AI safety as a global public good and lifecycle management challenges
- Why unresolved: While governance challenges are identified, specific implementation mechanisms are not proposed
- What evidence would resolve it: Case studies demonstrating successful governance models balancing diverse stakeholder interests

## Limitations
- Framework relies heavily on theoretical constructs without empirical validation
- Specific implementation details for Reflectable Layer methods remain unclear
- Concrete evaluation criteria for measuring 45° line progress are not specified

## Confidence
- High Confidence: AI capability development currently outpaces safety measures
- Medium Confidence: Three-layer causal framework provides logical taxonomy for safety approaches
- Medium Confidence: Five-level trustworthiness hierarchy offers reasonable conceptual framework

## Next Checks
1. **Empirical validation of the causal progression**: Test whether systems implementing Layer 2 consistently demonstrate improved safety outcomes compared to Layer 1 systems
2. **Benchmark development for the 45° line**: Create quantitative metrics measuring simultaneous progress in capability and safety dimensions
3. **Cross-system comparability study**: Apply the trustworthiness matrix to diverse AI systems to determine framework consistency across domains and architectures