---
ver: rpa2
title: 'Weisfeiler-Leman at the margin: When more expressivity matters'
arxiv_id: '2402.07568'
source_url: https://arxiv.org/abs/2402.07568
tags:
- graph
- graphs
- neural
- wloa
- margin
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the generalization properties of Weisfeiler-Leman
  (WL) algorithms and their extensions in graph neural networks (GNNs) and graph kernels.
  While more expressive WL variants have shown improved empirical performance, the
  relationship between expressive power and generalization remains unclear.
---

# Weisfeiler-Leman at the margin: When more expressivity matters

## Quick Facts
- arXiv ID: 2402.07568
- Source URL: https://arxiv.org/abs/2402.07568
- Authors: Billy J. Franks; Christopher Morris; Ameya Velingker; Floris Geerts
- Reference count: 40
- Key outcome: Graph isomorphism-based expressiveness does not reliably predict generalization; margin-based bounds provide better insights

## Executive Summary
This paper investigates the relationship between expressive power and generalization in Weisfeiler-Leman (WL) algorithms and their graph neural network counterparts. While more expressive WL variants have shown improved empirical performance, the authors demonstrate that graph isomorphism-based expressiveness alone is insufficient to predict generalization. Instead, they introduce margin-based bounds on the VC dimension for WL-based kernels and GNNs, showing that increased expressiveness leads to better generalization only when it increases the margin between classes. The theoretical framework is validated through experiments on synthetic and benchmark datasets, revealing conditions under which subgraph-based WL extensions outperform standard WL algorithms.

## Method Summary
The authors analyze graph kernels and MPNNs based on WL algorithms, focusing on variants that incorporate subgraph information (1-WLF, 1-WLOAF). They derive margin-based bounds on VC dimension that connect expressive power to generalization through the margin between classes. For MPNNs, they show that gradient flow dynamics push weights toward maximum margin solutions under certain assumptions. The experiments compare standard WL variants against subgraph-enhanced versions on synthetic datasets where ground truth is known, as well as benchmark graph classification datasets using 10-fold cross-validation with multiple repetitions.

## Key Results
- Graph isomorphism expressiveness does not reliably predict generalization performance
- Subgraph-based WL variants (1-WLF, 1-WLOAF) achieve higher test accuracies and margins on datasets where standard WL fails
- Margin-based VC dimension bounds provide conditions under which increased expressiveness improves generalization
- Gradient flow pushes MPNN weights toward maximum margin solutions under specific assumptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: More expressive WL variants can improve generalization when they increase the margin between classes.
- Mechanism: The margin-based VC dimension bounds show that generalization error depends inversely on the margin. If subgraph-based WL extensions make classes more linearly separable, the margin increases and thus the VC dimension decreases, leading to better generalization.
- Core assumption: The data distribution allows the more expressive WL variant to create a positive margin where the standard WL cannot.
- Evidence anchors:
  - [abstract] "by incorporating subgraph information, they show conditions under which increased expressiveness leads to better margins and, consequently, improved generalization"
  - [section] "we derive data distributions where increased expressivity either leads to improved generalization performance or not"
- Break condition: If the subgraph information doesn't create a positive margin or if it decreases the margin, the generalization performance may worsen.

### Mechanism 2
- Claim: Gradient flow pushes MPNN weights toward the maximum margin solution.
- Mechanism: Under certain assumptions, the gradient flow dynamics cause the weight matrices to align such that the resulting linear classifier achieves the maximum possible margin for the given data.
- Core assumption: The data is separable by some MPNN architecture and the loss function satisfies specific properties (continuous derivative, negative everywhere, etc.).
- Evidence anchors:
  - [abstract] "we show that gradient flow pushes the MPNN's weights toward the maximum margin solution"
  - [section] "we now show that, under some assumptions, MPNNs exhibit an 'alignment' property whereby gradient flow pushes the network's weights toward the maximum margin solution"
- Break condition: If the data is not separable or if the assumptions on the loss function are violated, the gradient flow may not converge to the maximum margin solution.

### Mechanism 3
- Claim: The 1-WL and its variants have limited ability to predict generalization performance based solely on graph isomorphism.
- Mechanism: There exist data distributions where the 1-WL can perfectly distinguish non-isomorphic graphs with different class labels, but the resulting feature vectors are not linearly separable, leading to poor generalization.
- Core assumption: The relationship between graph isomorphism and linear separability in the feature space is not straightforward.
- Evidence anchors:
  - [abstract] "we show that an architecture's expressivity offers limited insights into its generalization performance when viewed through graph isomorphism"
  - [section] "we show that data distributions exist such that the 1-WL and corresponding MPNNs distinguish every pair of non-isomorphic graphs with different class labels while no classifier can do better than random outside of the training set"
- Break condition: If the feature space induced by the 1-WL or its variants happens to be linearly separable for the given data distribution, the graph isomorphism perspective would be sufficient to predict good generalization.

## Foundational Learning

- Concept: VC dimension and its relationship to generalization error.
  - Why needed here: The paper uses VC dimension bounds to analyze the generalization properties of WL-based kernels and MPNNs.
  - Quick check question: What is the relationship between VC dimension and the generalization error of a hypothesis class?

- Concept: Margin theory and its connection to VC dimension.
  - Why needed here: The paper derives margin-based bounds on the VC dimension, showing that a larger margin leads to a smaller VC dimension and thus better generalization.
  - Quick check question: How does the margin of a classifier affect its VC dimension and generalization error?

- Concept: Graph isomorphism and its limitations in capturing graph properties.
  - Why needed here: The paper discusses how the expressive power of WL algorithms in distinguishing non-isomorphic graphs does not necessarily translate to good generalization performance.
  - Quick check question: What are the limitations of using graph isomorphism as a proxy for the expressive power of graph neural networks?

## Architecture Onboarding

- Component map:
  Graph kernels (1-WL, 1-WLF, 1-WLOA, 1-WLOAF) -> MPNN architectures (simple MPNNs, MPNNFs) -> Margin-based bounds on VC dimension -> Gradient flow dynamics

- Critical path:
  1. Understand the WL algorithm and its variants (1-WL, 1-WLF, 1-WLOA, 1-WLOAF)
  2. Learn about margin-based VC dimension bounds and their implications for generalization
  3. Study the gradient flow dynamics and their role in pushing weights toward the maximum margin solution
  4. Analyze the experimental results and their connection to the theoretical findings

- Design tradeoffs:
  - More expressive WL variants can lead to better generalization if they increase the margin, but they may also increase the VC dimension if the margin decreases.
  - Simple MPNNs have a lower computational cost but may have limited expressive power compared to more complex architectures.

- Failure signatures:
  - Poor generalization performance despite high expressive power of the WL variant or MPNN architecture.
  - Convergence issues in the gradient flow dynamics due to violations of the assumptions on the loss function.

- First 3 experiments:
  1. Verify the relationship between expressive power and margin on synthetic datasets where the ground truth is known.
  2. Analyze the gradient flow dynamics for a simple MPNN on a linearly separable dataset to observe the alignment of weights toward the maximum margin solution.
  3. Compare the generalization performance of different WL variants and MPNN architectures on benchmark graph classification datasets, focusing on the role of the margin in explaining the results.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific conditions under which adding subgraph information (like in the 1-WLF or 1-WLOAF) leads to a larger margin and improved generalization?
- Basis in paper: [explicit] The paper states in Section 3.4 that more expressive power can lead to an increased margin under certain conditions, and provides theorems and propositions (e.g., Theorem 18, Corollary 19) outlining these conditions.
- Why unresolved: While the paper provides theoretical conditions, it does not explicitly test or demonstrate these conditions in practice across a wide range of datasets. The experiments focus on specific subgraphs (C3, C4, etc.) but do not systematically explore the broader conditions outlined in the theory.
- What evidence would resolve it: Empirical studies on diverse datasets with varying graph structures, systematically varying the subgraph information used, and measuring the resulting margins and generalization performance. This would validate the theoretical conditions and identify when they hold in practice.

### Open Question 2
- Question: How do different subgraph choices (e.g., cycles, cliques) impact the margin and generalization performance of 1-WLF and 1-WLOAF kernels?
- Basis in paper: [explicit] The paper experiments with different subgraphs (C3, C4, C5, K3, etc.) in the empirical section, showing varying results. However, it does not provide a comprehensive analysis of the impact of different subgraph choices.
- Why unresolved: The experiments are limited to a specific set of subgraphs, and the paper does not explore the broader space of possible subgraphs or provide a theoretical framework for understanding their impact.
- What evidence would resolve it: A systematic study comparing the performance of 1-WLF and 1-WLOAF kernels using a wide range of subgraphs, including both cycles and cliques of different sizes, and analyzing the relationship between subgraph properties and margin/generalization performance.

### Open Question 3
- Question: Can the margin-based bounds on the VC dimension be extended to other graph neural network architectures beyond simple MPNNs?
- Basis in paper: [explicit] The paper derives margin-based bounds for simple MPNNs and shows that these bounds can be lifted to more expressive architectures like MPNNF. However, it does not explore other types of GNNs, such as those with attention mechanisms or spectral approaches.
- Why unresolved: The paper focuses on a specific class of GNNs and does not investigate the applicability of the margin-based bounds to other popular architectures.
- What evidence would resolve it: Extending the theoretical framework to other GNN architectures and deriving corresponding margin-based bounds. Empirical validation of these bounds on datasets commonly used for these architectures.

### Open Question 4
- Question: What is the role of the dataset's margin in the convergence of gradient descent to a large-margin solution for over-parameterized MPNN architectures?
- Basis in paper: [explicit] The paper shows that gradient flow pushes linear MPNNs towards the maximum margin solution (Theorem 58). However, it does not investigate the role of the dataset's margin in the convergence of gradient descent for non-linear MPNNs.
- Why unresolved: The paper focuses on linear MPNNs and gradient flow, while practical applications often use non-linear MPNNs and gradient descent. The relationship between the dataset's margin and the convergence of gradient descent for non-linear architectures is not explored.
- What evidence would resolve it: Empirical studies on non-linear MPNN architectures, comparing the convergence of gradient descent for datasets with different margins. Analyzing the impact of the margin on the speed and stability of convergence.

## Limitations

- The theoretical analysis relies on idealized assumptions about gradient flow convergence that may not hold with practical optimization algorithms
- The connection between graph isomorphism expressiveness and margin-based generalization is demonstrated through carefully constructed synthetic examples that may not represent real-world complexity
- The paper does not fully characterize when the conditions for improved generalization through increased expressiveness are met in practice

## Confidence

- High confidence: The theoretical framework connecting margin to VC dimension bounds is well-established. The experimental methodology (10-fold CV, multiple repetitions) follows standard practices.
- Medium confidence: The specific claims about gradient flow dynamics and maximum margin solutions depend on idealized assumptions that may not hold with practical optimization algorithms.
- Medium confidence: The synthetic dataset constructions convincingly demonstrate the limitations of graph isomorphism as a proxy for generalization, but may not capture the complexity of real-world data distributions.

## Next Checks

1. Test the gradient flow convergence claims on a simpler, fully differentiable setting where analytical solutions are available, to verify whether the alignment properties hold outside of the theoretical assumptions.

2. Evaluate the margin distributions across different WL variants on real-world datasets to determine if increased expressiveness consistently leads to larger margins in practice, not just in theory.

3. Compare the proposed margin-based bounds with empirical generalization gaps across multiple random seeds and dataset splits to assess the practical predictive power of the theoretical framework.