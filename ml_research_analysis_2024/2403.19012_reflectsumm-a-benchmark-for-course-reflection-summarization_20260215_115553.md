---
ver: rpa2
title: 'ReflectSumm: A Benchmark for Course Reflection Summarization'
arxiv_id: '2403.19012'
source_url: https://arxiv.org/abs/2403.19012
tags:
- summarization
- reflections
- specificity
- students
- summaries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ReflectSumm is a large-scale summarization dataset of 17,512 student
  reflections on 782 lectures, annotated with three types of reference summaries (abstractive,
  extractive, and phrase-based) and rich metadata including reflection specificity
  scores and student demographics. We benchmarked the dataset using multiple state-of-the-art
  baselines including fine-tuned BERT, MatchSum, and ChatGPT.
---

# ReflectSumm: A Benchmark for Course Reflection Summarization

## Quick Facts
- arXiv ID: 2403.19012
- Source URL: https://arxiv.org/abs/2403.19012
- Reference count: 0
- Primary result: Dataset of 17,512 student reflections on 782 lectures with three summary types and demographic metadata

## Executive Summary
ReflectSumm is a large-scale summarization dataset containing student reflections on STEM lectures, annotated with three types of reference summaries (abstractive, extractive, and phrase-based) and rich metadata including reflection specificity scores and student demographics. The dataset enables comprehensive evaluation of summarization approaches across multiple tasks while supporting fairness analysis and specificity-aware modeling. Through benchmarking with state-of-the-art baselines including BERT, MatchSum, BART, and ChatGPT, the work demonstrates that incorporating reflection specificity improves summarization performance, while ChatGPT achieves strong extractive results but lower ROUGE scores for abstractive summaries. The dataset fills a critical gap in educational NLP by providing the first large-scale corpus specifically designed for course reflection summarization with comprehensive metadata.

## Method Summary
The ReflectSumm dataset was constructed from student reflections collected across 24 university courses spanning four STEM subjects (ENGR, PHY, CS, CMPINF). Three types of reference summaries were generated: extractive summaries selecting relevant sentences, abstractive summaries synthesizing key points, and phrase-based summaries providing quick topic overviews. Reflection specificity scores (1-4 scale) were annotated by student raters to capture detail level. The dataset was benchmarked using multiple state-of-the-art models including BERTSUM-EXT, MatchSum, BART-Large, and ChatGPT in zero-shot and one-shot settings. Cross-validation by subject groups was employed to ensure proper evaluation across different writing styles and terminology.

## Key Results
- Incorporating reflection specificity scores improves ROUGE and F1 performance across summarization tasks
- ChatGPT models achieve competitive extractive results but generate shorter abstractive summaries with lower ROUGE scores
- Initial demographic analysis reveals differences in reflection distribution along gender dimensions in extractive summaries
- Specificity-aware models show higher average specificity scores for selected sentences (3.08) compared to discarded ones (2.85)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incorporating reflection specificity scores improves summarization performance across tasks
- Mechanism: Specificity scores provide a signal for salience that helps models distinguish between vague and detailed reflections, leading to better summary selection
- Core assumption: Higher specificity correlates with reflection importance and relevance to the lecture topic
- Evidence anchors: [abstract] "incorporating reflection specificity improves performance across tasks", [section] "The average specificity score of sentences selected for extractive summaries is higher (3.08) compared to the discarded sentences (2.85)"

### Mechanism 2
- Claim: The dataset's multiple summary types enable comprehensive evaluation of summarization approaches
- Mechanism: Different summarization tasks capture different aspects of the data - extractive for factual selection, abstractive for synthesis, phrase-based for quick topic identification
- Core assumption: Each summary type serves a distinct purpose in educational contexts and requires different modeling approaches
- Evidence anchors: [abstract] "The dataset encompasses a diverse range of summarization tasks", [section] "While prior corpora emphasized either abstractive or extractive summarization, our dataset provides three types of reference summaries"

### Mechanism 3
- Claim: Student demographic metadata enables fairness analysis and bias detection in summarization
- Mechanism: By tracking which student groups are represented in summaries, researchers can identify if certain demographics are systematically excluded or underrepresented
- Core assumption: Demographic patterns in reflection writing affect their likelihood of being included in summaries
- Evidence anchors: [abstract] "enabling the exploration of fairness and equity issues", [section] "Initial explorations indicate that REFLECT SUMM shows promise for this purpose, as it reveals a difference in the distribution of reflections along the gender dimension between the extractive summaries and the entire dataset"

## Foundational Learning

- Concept: Cross-validation by subject groups
  - Why needed here: The dataset contains multiple subjects with different writing styles and terminology, so proper evaluation requires subject-aware splitting
  - Quick check question: Why do we group lectures by subject before creating folds instead of randomly sampling across all lectures?

- Concept: Specificity scoring (1-4 scale)
  - Why needed here: Provides a quantitative measure of reflection quality that can be used as additional signal for summarization models
  - Quick check question: What does a specificity score of 4 indicate about a reflection compared to a score of 1?

- Concept: Multiple evaluation metrics (ROUGE, BERTScore, EM F1, P F1)
  - Why needed here: Different metrics capture different aspects of summarization quality - ROUGE for n-gram overlap, BERTScore for semantic similarity, F1 scores for exact/partial match
  - Quick check question: Why might we need both exact match F1 and partial match F1 for extractive summarization evaluation?

## Architecture Onboarding

- Component map: Data ingestion → Preprocessing (specificity annotation, demographic filtering) → Task-specific models (BERTSUM-EXT, MatchSum, BART-Large, ChatGPT) → Evaluation (ROUGE, BERTScore, SUMMA C) → Analysis (demographic fairness, specificity impact)
- Critical path: Data preprocessing → Model training → Evaluation → Analysis
- Design tradeoffs: 
  - Using student annotators vs experts (continuity with prior work vs potential quality differences)
  - Including demographic metadata (fairness analysis capability vs privacy concerns)
  - Multiple summary types (comprehensive evaluation vs increased annotation burden)
- Failure signatures:
  - Low ROUGE scores across all models suggest data quality issues
  - Demographic bias in summaries indicates fairness problems
  - ChatGPT extractiveness scores below 90% suggest prompt comprehension issues
- First 3 experiments:
  1. Run all baseline models (BERTSUM-EXT, MatchSum, BART-Large, ChatGPT) on the full dataset with standard parameters
  2. Compare performance of models with and without specificity scores incorporated
  3. Analyze demographic representation in extractive summaries vs input reflections

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the quality and diversity of summaries generated by GPT models compare to those generated by fine-tuned BART models when evaluated by human raters on criteria such as coherence, informativeness, and relevance?
- Basis in paper: [inferred] The paper mentions that GPT models produce shorter summaries and have lower ROUGE scores compared to BART models, but it does not provide a direct comparison of summary quality based on human evaluation.
- Why unresolved: The paper primarily relies on automated metrics like ROUGE, BERTScore, and SUMMA C for evaluation, which may not fully capture the nuances of summary quality from a human perspective.
- What evidence would resolve it: Conducting a human evaluation study where multiple raters assess the quality of summaries generated by GPT and BART models based on predefined criteria would provide insights into their relative strengths and weaknesses.

### Open Question 2
- Question: How does the performance of extractive summarization models vary across different STEM subjects (e.g., Engineering, Physics, Computer Science) in terms of ROUGE scores and human evaluation metrics?
- Basis in paper: [explicit] The paper mentions that the dataset covers multiple STEM subjects but does not provide a detailed analysis of model performance across these subjects.
- Why unresolved: The paper does not present subject-specific results or analyze the potential reasons for any performance variations across subjects.
- What evidence would resolve it: Conducting experiments to evaluate extractive summarization models separately on each STEM subject and comparing their performance using both automated and human evaluation metrics would shed light on subject-specific challenges and opportunities.

### Open Question 3
- Question: What is the impact of reflection specificity scores on the performance of abstractive summarization models, and how can this information be effectively incorporated into the summarization process?
- Basis in paper: [explicit] The paper explores the use of specificity scores in extractive summarization and finds improvements in ROUGE scores and matching F1, but does not investigate its impact on abstractive summarization.
- Why unresolved: The paper does not provide a detailed analysis of how specificity scores affect abstractive summarization or explore different ways to incorporate this information into the model architecture or training process.
- What evidence would resolve it: Experimenting with different methods of incorporating specificity scores into abstractive summarization models (e.g., as additional input features, through multi-task learning, or by conditioning the generation process) and evaluating their impact on summary quality would help determine the effectiveness of this approach.

## Limitations

- Limited corpus evidence for key claims about specificity-aware performance improvements and demographic fairness analysis
- ChatGPT models show notably lower ROUGE scores for abstractive summarization despite competitive extractive performance
- Student annotators used for specificity scoring without established inter-rater reliability analysis
- Dataset focus on STEM courses may limit generalizability to other educational domains

## Confidence

- High: The dataset construction methodology and basic summarization task formulation
- Medium: The claim that incorporating specificity scores improves performance, based on observed score differences
- Medium: The assertion that demographic metadata enables fairness analysis, based on preliminary findings
- Low: Claims about the dataset's effectiveness for abstractive summarization given ChatGPT's lower performance

## Next Checks

1. Conduct inter-rater reliability analysis on the specificity annotations to establish their consistency and validity as a quality signal
2. Perform a comprehensive demographic fairness analysis across all protected characteristics (gender, ethnicity, first-generation status) to identify systematic biases
3. Test model generalization by evaluating performance on reflection subsets from individual courses/subjects to assess domain transfer capabilities