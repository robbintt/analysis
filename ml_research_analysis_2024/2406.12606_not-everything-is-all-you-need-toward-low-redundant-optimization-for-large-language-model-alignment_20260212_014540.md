---
ver: rpa2
title: 'Not Everything is All You Need: Toward Low-Redundant Optimization for Large
  Language Model Alignment'
arxiv_id: '2406.12606'
source_url: https://arxiv.org/abs/2406.12606
tags:
- llms
- training
- neurons
- allo
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a low-redundant optimization method for aligning
  large language models (LLMs) with human preferences. The authors observe that only
  updating the top-10% most relevant neurons during alignment training improves convergence
  and performance, indicating the presence of redundant neurons.
---

# Not Everything is All You Need: Toward Low-Redundant Optimization for Large Language Model Alignment

## Quick Facts
- arXiv ID: 2406.12606
- Source URL: https://arxiv.org/abs/2406.12606
- Reference count: 21
- This paper proposes ALLO, a low-redundant optimization method that improves LLM alignment by updating only top-10% most relevant neurons, achieving up to 9.7% relative improvement over vanilla DPO across 10 datasets

## Executive Summary
This paper identifies and addresses neuron redundancy in large language model alignment by proposing ALLO, a two-stage optimization framework. The authors observe that only updating the top-10% most relevant neurons during alignment training improves convergence and performance, indicating significant redundancy in standard alignment approaches. ALLO first identifies important neurons using gradient-based importance scores, then decomposes alignment into a forgetting stage (using NPO to unlearn unaligned knowledge) and a learning stage (using DPO to learn aligned knowledge) on key tokens identified by respective reward models.

## Method Summary
ALLO addresses neuron redundancy in LLM alignment by decomposing the process into two stages. First, it identifies important neurons using gradient-based importance scores. In the forgetting stage, NPO (Negatively-Positive Optimization) is used to unlearn unaligned knowledge on key tokens identified by a reward model. In the learning stage, DPO (Direct Preference Optimization) is applied to learn aligned knowledge on key tokens identified by DPO rewards. The framework focuses optimization only on the top-10% most relevant neurons, reducing computational overhead while maintaining or improving alignment quality. Experiments across 10 datasets show ALLO outperforms competitive methods, particularly in question answering, mathematical reasoning, and instruction following tasks.

## Key Results
- ALLO achieves up to 9.7% relative improvement over vanilla DPO across 10 datasets
- Only updating top-10% most relevant neurons improves convergence and performance
- Outperforms competitive methods on question answering, mathematical reasoning, and instruction following tasks
- Reduces computational overhead by focusing optimization on important neurons

## Why This Works (Mechanism)
The paper demonstrates that neuron redundancy exists in LLM alignment, where only a subset of neurons significantly contribute to alignment quality. By identifying and updating only the top-10% most relevant neurons using gradient-based importance scores, ALLO reduces unnecessary computation while maintaining alignment effectiveness. The two-stage decomposition (forgetting then learning) allows for more precise control over the alignment process, addressing both the removal of unaligned knowledge and the incorporation of aligned preferences in a targeted manner.

## Foundational Learning

- **Neuron importance scoring**: Why needed - to identify which neurons contribute most to alignment; Quick check - gradient-based scores correlate with alignment performance
- **NPO (Negatively-Positive Optimization)**: Why needed - to unlearn unaligned knowledge before learning aligned preferences; Quick check - forgetting stage improves downstream alignment quality
- **DPO (Direct Preference Optimization)**: Why needed - to learn aligned preferences from human feedback; Quick check - pairwise comparison data effectively guides alignment
- **Reward model token selection**: Why needed - to identify key tokens that matter most for alignment; Quick check - reward-based token selection improves alignment efficiency
- **Two-stage alignment decomposition**: Why needed - to separately address unlearning and learning for better control; Quick check - stage separation improves final alignment quality

## Architecture Onboarding

**Component Map:**
Reward Model → Neuron Importance Scoring → NPO Forgetting Stage → DPO Learning Stage → Aligned LLM

**Critical Path:**
1. Compute neuron importance scores using gradients
2. Identify key tokens via reward model
3. Execute forgetting stage with NPO on unaligned knowledge
4. Execute learning stage with DPO on aligned preferences
5. Evaluate alignment quality on downstream tasks

**Design Tradeoffs:**
- Computational efficiency vs. alignment quality: ALLO reduces computation by focusing on 10% neurons but requires additional overhead for importance scoring
- Stage separation vs. unified optimization: Two-stage approach provides better control but introduces more hyperparameters
- Reward model dependency: Token selection quality depends on reward model accuracy, which may introduce bias

**Failure Signatures:**
- Poor neuron importance scoring leads to suboptimal neuron selection and reduced alignment quality
- Ineffective token selection by reward model results in misaligned forgetting or learning
- Hyperparameter mismatch between forgetting and learning stages causes instability

**First 3 Experiments:**
1. Ablation study on neuron selection percentage (e.g., 5%, 10%, 20%) to find optimal trade-off
2. Comparison of ALLO with standard DPO and SFT on LLaMA-7B across benchmark datasets
3. Sensitivity analysis of reward model quality on ALLO's final alignment performance

## Open Questions the Paper Calls Out
None

## Limitations
- The 10% neuron finding is based on gradient-based importance scores whose stability across different model architectures and tasks is not established
- Experimental evaluation focuses primarily on LLaMA-7B, with limited testing on larger models where redundancy effects might differ
- ALLO's decomposition introduces additional hyperparameters that may impact robustness and reproducibility

## Confidence
- **High confidence**: The observation that some neurons contribute more to alignment than others, supported by ablation studies
- **Medium confidence**: The effectiveness of ALLO compared to baseline methods on tested datasets
- **Low confidence**: Claims about the general applicability of the 10% neuron finding across model scales and domains

## Next Checks
1. Test ALLO's performance on larger models (e.g., LLaMA-70B or GPT-3.5 scale) to verify scalability of the neuron importance finding
2. Conduct ablation studies on the reward model's token selection mechanism to assess sensitivity to reward model quality
3. Evaluate ALLO's robustness across diverse alignment objectives beyond the current instruction-following and reasoning tasks