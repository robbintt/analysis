---
ver: rpa2
title: Attacking Vision-Language Computer Agents via Pop-ups
arxiv_id: '2411.02391'
source_url: https://arxiv.org/abs/2411.02391
tags:
- agents
- pop-ups
- agent
- attack
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates adversarial attacks on VLM-based autonomous
  agents using malicious pop-ups. The attack involves inserting carefully crafted
  pop-ups into agent observations to trick them into clicking these instead of completing
  their tasks.
---

# Attacking Vision-Language Computer Agents via Pop-ups

## Quick Facts
- arXiv ID: 2411.02391
- Source URL: https://arxiv.org/abs/2411.02391
- Reference count: 7
- Primary result: Malicious pop-ups can trick VLM agents into clicking instead of completing tasks with 86% attack success rate and 47% reduction in task success

## Executive Summary
This work investigates adversarial attacks on VLM-based autonomous agents using malicious pop-ups. The attack involves inserting carefully crafted pop-ups into agent observations to trick them into clicking these instead of completing their tasks. Tested on OSWorld and VisualWebArena, the attack achieves an 86% attack success rate on average and reduces task success rates by 47%. Even basic defenses like system prompts or ad notices fail to mitigate the issue effectively. Ablation studies show the user query summary is critical, while other information is less necessary. Agents often follow the malicious instructions without recognizing their harmful intent, highlighting significant safety vulnerabilities in current VLM agents.

## Method Summary
The attack inserts adversarial pop-ups into agent observations by generating attention hooks (summarized user queries), instructions, info banners, and ALT descriptors. The method tests on screenshot agents and Set-of-Mark (SoM) agents using five frontier VLMs across OSWorld and VisualWebArena benchmarks. Experiments measure Attack Success Rate (ASR), Success Rate (SR), and Original Success Rate (OSR), with ablation studies on design choices and evaluation of basic defenses.

## Key Results
- 86% attack success rate achieved across tested VLMs and benchmarks
- 47% reduction in task success rates when attacks are present
- User query summary (attention hook) is critical for attack success, while other elements are less necessary
- Basic defenses like system prompts and ad notices fail to mitigate attacks effectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The attack succeeds because agents lack visual safety training to recognize and ignore suspicious pop-ups that humans would identify as malicious.
- Mechanism: VLMs process visual input without contextual understanding of UI safety norms. When presented with adversarial pop-ups containing task-relevant language, agents treat them as legitimate interface elements and follow embedded instructions.
- Core assumption: The agent's decision-making relies primarily on surface-level visual recognition rather than deep understanding of UI semantics or safety context.
- Evidence anchors:
  - [abstract]: "Even basic defenses like system prompts or ad notices fail to mitigate the issue effectively" indicates agents cannot distinguish malicious pop-ups from legitimate UI elements
  - [section]: "Agents often follow the malicious instructions without recognizing their harmful intent, highlighting significant safety vulnerabilities in current VLM agents"
  - [corpus]: Weak evidence - corpus contains related work on VLM agent vulnerabilities but lacks direct evidence about pop-up recognition failure

### Mechanism 2
- Claim: The attack works by exploiting the agent's dependency on structured accessibility (a11y) tree information combined with visual context.
- Mechanism: SoM agents use both tagged screenshots and a11y trees. The adversarial pop-ups are designed to appear in both modalities - visually prominent and with crafted ALT descriptors that align with the task context, making them appear as legitimate interactive elements.
- Core assumption: Agents trust the a11y tree structure and ALT text as reliable sources of information about UI elements without cross-verifying with broader context.
- Evidence anchors:
  - [abstract]: "Agents often follow the malicious instructions without recognizing their harmful intent" suggests they trust provided information
  - [section]: "ALT Descriptor (if applicable) To align with the visual information, we use the summary of the user's query (the attention hook) and the instruction as the adversarial ALT descriptor"
  - [corpus]: Weak evidence - related papers discuss VLM agent vulnerabilities but don't specifically address a11y tree exploitation

### Mechanism 3
- Claim: The attack succeeds because agents cannot distinguish between legitimate user instructions and malicious instructions embedded in environmental elements.
- Mechanism: VLMs process all textual information in their observation space as potential instructions. When pop-ups contain phrases that match the task context (summarized user queries) and explicit action instructions, agents treat them as valid guidance without questioning their origin or intent.
- Core assumption: The agent's instruction-following mechanism is agnostic to the source of instructions, treating all textual cues in the environment as equally valid.
- Evidence anchors:
  - [abstract]: "This distraction leads agents to click these pop-ups instead of performing their tasks as usual" indicates agents follow embedded instructions
  - [section]: "By testing screenshot agents (Xie et al., 2024) and Set-of-Mark agents (Yang et al., 2023) using state-of-the-art VLMs as backbones, we find that our attack achieves an attack success rate (ASR) over 80% on OSworld"
  - [corpus]: Weak evidence - corpus includes related work on VLM agent attacks but lacks specific evidence about instruction source discrimination

## Foundational Learning

- Concept: Visual grounding in multimodal agents
  - Why needed here: Understanding how agents map visual elements to actionable items is crucial for comprehending why adversarial pop-ups can mislead them
  - Quick check question: How do SoM agents use a11y trees to ground visual elements, and why does this make them vulnerable to adversarial pop-ups with crafted ALT descriptors?

- Concept: Instruction-following mechanisms in VLMs
  - Why needed here: The attack exploits how VLMs process and prioritize textual instructions in their observation space, regardless of source
  - Quick check question: What distinguishes legitimate user instructions from environmental instructions in current VLM agent architectures, and how can this distinction be exploited?

- Concept: Adversarial attack principles in computer vision
  - Why needed here: Understanding how visual perturbations can manipulate model behavior provides context for why crafted pop-ups can deceive agents
  - Quick check question: How do traditional adversarial attacks on vision models differ from this pop-up-based attack, and what makes this approach particularly effective against agents?

## Architecture Onboarding

- Component map: Visual perception module (screenshots + a11y trees) -> Instruction parsing module -> Planning module -> Action execution module
- Critical path: User query → Visual perception (screenshots + a11y) → Instruction parsing → Planning → Action execution. The attack inserts malicious instructions at the visual perception stage.
- Design tradeoffs: High ASR achieved by trusting all environmental information vs. security through instruction validation that might reduce performance on legitimate tasks.
- Failure signatures: High frequency of clicking on pop-ups, task completion failure, deviation from expected action sequences, ignoring legitimate UI elements in favor of pop-ups.
- First 3 experiments:
  1. Test agent behavior with benign pop-ups (no malicious content) to establish baseline distraction effect
  2. Vary the prominence of the attention hook (summarized query) to measure its importance in attack success
  3. Test with delayed attack onset (after initial exploration) to see if agents become more resistant once they establish task context

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do agents distinguish between legitimate instructions and malicious ones in their generated thoughts, and why do they sometimes generate correct thoughts while taking harmful actions?
- Basis in paper: [explicit] The paper shows examples where agents generate correct thoughts about their task but still take actions influenced by malicious pop-up instructions.
- Why unresolved: The paper observes this phenomenon but doesn't explain the cognitive mechanism or model architecture that allows this disconnect between reasoning and action.
- What evidence would resolve it: Detailed analysis of attention mechanisms and action selection processes in VLM agents, showing how conflicting information is weighted between thoughts and actions.

### Open Question 2
- Question: Would attacks that paraphrase instructions rather than using explicit "click here" language be more effective at bypassing defenses?
- Basis in paper: [explicit] The paper mentions that removing "Please click" from instructions can effectively increase attack success rate, and suggests paraphrasing as a potential defense bypass.
- Why unresolved: The paper only tests simple instruction removal, not sophisticated paraphrasing techniques that could be more stealthy.
- What evidence would resolve it: Testing various paraphrasing strategies against both attack success rate and defensive prompt effectiveness.

### Open Question 3
- Question: How does the effectiveness of attacks vary across different agent architectures (screenshot vs. SoM) when the same malicious content is presented?
- Basis in paper: [explicit] The paper shows different ASR rates between screenshot and SoM agents but doesn't fully explain the architectural differences that cause this.
- Why unresolved: While the paper observes performance differences, it doesn't systematically test how architectural changes affect vulnerability to specific attack types.
- What evidence would resolve it: Controlled experiments varying agent architectures while keeping attack content constant to isolate architectural factors.

### Open Question 4
- Question: Can agents be trained to recognize and ignore malicious pop-ups through safety fine-tuning, similar to how they're trained to avoid harmful content?
- Basis in paper: [explicit] The paper tests basic prompt-based defenses but finds them ineffective, suggesting more sophisticated approaches are needed.
- Why unresolved: The paper doesn't explore whether safety training on adversarial examples could improve robustness, focusing instead on prompt-based approaches.
- What evidence would resolve it: Training agents on adversarial examples and measuring improvement in resistance to attacks compared to untrained baselines.

### Open Question 5
- Question: How does the timing of attack insertion (early vs. delayed) affect agent vulnerability, and what does this reveal about agent memory and decision-making?
- Basis in paper: [explicit] The paper shows that delaying attacks until after step 7 increases robustness, but doesn't explain why.
- Why unresolved: The paper observes the effect but doesn't analyze whether it's due to memory persistence, exploration patterns, or other cognitive factors.
- What evidence would resolve it: Systematic testing of attack timing at various points in agent trajectories with analysis of memory retention and decision-making patterns.

## Limitations

- The attack effectiveness depends heavily on the specific agent architecture (SoM agents using a11y trees) and may not generalize to all VLM-based autonomous systems
- Defense evaluations are limited to basic measures like system prompts and ad notices, without exploring more sophisticated mitigation strategies
- The study doesn't address computational overhead or latency introduced by potential defensive mechanisms

## Confidence

- High Confidence: The empirical results showing 86% ASR and 47% reduction in task success rates are well-supported by systematic experimentation across multiple benchmarks and VLMs
- Medium Confidence: The mechanism explanations (pop-ups exploiting instruction-following mechanisms and a11y tree trust) are logically consistent but lack direct experimental validation of the underlying assumptions
- Low Confidence: Claims about the specific importance of each pop-up design element (attention hooks, instructions, info banners, ALT descriptors) are based on ablation studies but may not capture the full complexity of how agents process multimodal information

## Next Checks

1. Test the attack's effectiveness against agents using different architectural approaches (e.g., end-to-end visual reasoning without a11y trees) to assess generalizability
2. Evaluate more sophisticated defensive mechanisms beyond basic prompts, such as instruction validation modules or adversarial training on malicious UI patterns
3. Conduct user studies to determine if human performance on the same tasks with pop-ups present shows similar vulnerability patterns, establishing whether this represents a fundamental limitation of multimodal reasoning or an artifact of current agent designs