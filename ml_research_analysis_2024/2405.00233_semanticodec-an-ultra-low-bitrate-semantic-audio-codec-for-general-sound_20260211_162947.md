---
ver: rpa2
title: 'SemantiCodec: An Ultra Low Bitrate Semantic Audio Codec for General Sound'
arxiv_id: '2405.00233'
source_url: https://arxiv.org/abs/2405.00233
tags:
- audio
- semantic
- semanticodec
- kbps
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SemantiCodec is a novel neural audio codec designed to achieve
  ultra-low bitrates while maintaining high reconstruction quality and rich semantic
  information across diverse audio types. It uses a dual-encoder architecture with
  a semantic encoder (based on AudioMAE features and k-means clustering) and an acoustic
  encoder, both quantized and combined to condition a latent diffusion model for reconstruction.
---

# SemantiCodec: An Ultra Low Bitrate Semantic Audio Codec for General Sound

## Quick Facts
- arXiv ID: 2405.00233
- Source URL: https://arxiv.org/abs/2405.00233
- Authors: Haohe Liu, Xuenan Xu, Yi Yuan, Mengyue Wu, Wenwu Wang, Mark D. Plumbley
- Reference count: 40
- Primary result: Novel neural audio codec achieving ultra-low bitrates (0.31-1.40 kbps) while maintaining high reconstruction quality and rich semantic information across diverse audio types

## Executive Summary
SemantiCodec is a novel neural audio codec designed to achieve ultra-low bitrates while maintaining high reconstruction quality and rich semantic information across diverse audio types. It uses a dual-encoder architecture with a semantic encoder (based on AudioMAE features and k-means clustering) and an acoustic encoder, both quantized and combined to condition a latent diffusion model for reconstruction. Three variants operate at 25, 50, and 100 tokens/second, supporting bitrates from 0.31 to 1.40 kbps. SemantiCodec significantly outperforms state-of-the-art codecs like Descript and Encodec on reconstruction quality metrics (ViSQOL, STFT, MEL) and achieves higher semantic classification accuracy, even at the lowest bitrates.

## Method Summary
SemantiCodec employs a dual-encoder architecture where a semantic encoder extracts AudioMAE features and discretizes them using k-means clustering, while an acoustic encoder captures residual details through a BiLSTM and learnable vector quantization. These quantized features are combined to condition a latent diffusion model that reconstructs the audio. The system supports three operational modes at 25, 50, and 100 tokens/second, achieving bitrates from 0.31 to 1.40 kbps. The semantic codebook is constructed by clustering AudioMAE embeddings from diverse audio datasets, while the acoustic encoder is trained jointly with the diffusion decoder.

## Key Results
- SemantiCodec achieves ViSQOL scores of 2.90 at 0.31 kbps and 3.21 at 0.63 kbps on speech data, significantly outperforming Descript (2.43 and 2.78 respectively)
- At 0.31 kbps, SemantiCodec achieves WER of 9.04% on clean speech, comparable to Descript's 9.12% at 0.63 kbps
- Semantic classification accuracy remains high across all audio types, with SemantiCodec achieving 94.0% accuracy on Speech Commands at 0.31 kbps versus 91.3% for Descript

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual-layer vector quantization decouples semantic and acoustic information, enabling ultra-low bitrate while maintaining quality.
- Mechanism: The first VQ layer clusters AudioMAE embeddings via k-means, capturing semantic structure, while the second learnable VQ layer encodes residual acoustic details. This separation allows semantic tokens to remain rich for language modeling even at very low bitrates.
- Core assumption: Semantic content is primarily encoded in the first VQ layer, and acoustic details can be compressed separately without degrading intelligibility.
- Evidence anchors:
  - [abstract] "dual-encoder architecture: a semantic encoder using a self-supervised pre-trained Audio Masked Autoencoder (AudioMAE), discretized using k-means clustering on extensive audio data, and an acoustic encoder to capture the remaining details."
  - [section III-C2] "The second layer employs a conventional learnable VQ mechanism, enhancing the audio reconstruction fidelity of SemantiCodec."
  - [corpus] Weak evidence: neighbor papers focus on speech-specific codecs, not general audio; no direct comparison to this dual-decoupling approach.
- Break condition: If acoustic reconstruction becomes unintelligible (e.g., WER > 50) even with semantic tokens preserved, the assumption fails.

### Mechanism 2
- Claim: AudioMAE features provide richer semantic representations than traditional codec embeddings, improving downstream classification accuracy.
- Mechanism: AudioMAE's masked autoencoding objective preserves both semantic and acoustic structure, unlike discriminative SSL models. Using k-means centroids derived from AudioMAE features ensures the semantic codebook captures meaningful audio categories.
- Core assumption: Reconstructive SSL features are more balanced in semantic and acoustic information than discriminative SSL features.
- Evidence anchors:
  - [abstract] "Our results also suggest that SemantiCodec contains significantly richer semantic information than all evaluated state-of-the-art audio codecs, even at significantly lower bitrates."
  - [section II-B] "AudioMAE features stand out for their ability to preserve semantic and acoustic information [49]."
  - [section V-B] "SemantiCodec significantly outperforms baseline models in semantic information. Notably, even at a low bitrate of 0.35 kbps, the semantic performance of SemantiCodec surpasses that of higher bitrate counterparts."
- Break condition: If classification accuracy drops sharply when replacing AudioMAE with a discriminative SSL model like HuBERT, the assumption breaks.

### Mechanism 3
- Claim: Latent diffusion model conditioned on both semantic and acoustic codes reconstructs high-quality audio at ultra-low bitrates.
- Mechanism: The LDM leverages cross-attention on concatenated quantized features to generate latent spectrogram patches, which are decoded via VAE and vocoder. CFG guidance and DDIM sampling stabilize generation quality.
- Core assumption: Diffusion models can generate plausible audio continuations when conditioned on sparse discrete tokens.
- Evidence anchors:
  - [abstract] "The semantic and acoustic encoder outputs are used to reconstruct audio via a diffusion-model-based decoder."
  - [section III-D] "We employ an LDM as the decoder to reconstruct the original audio x... The LDM models the data distribution in a latent space constructed from a VAE."
  - [corpus] Weak evidence: neighbor papers focus on streaming codecs or tokenization schemes, not diffusion-based reconstruction.
- Break condition: If ViSQOL or MUSHRA scores drop significantly with fewer diffusion steps, the assumption fails.

## Foundational Learning

- Concept: Masked autoencoding for audio
  - Why needed here: AudioMAE pre-training yields features rich in semantic and acoustic structure, crucial for semantic clustering.
  - Quick check question: What is the difference between AudioMAE and HuBERT in terms of reconstruction vs discriminative objectives?

- Concept: Vector quantization with k-means clustering
  - Why needed here: Semantic codebook construction relies on clustering high-dimensional AudioMAE embeddings to form discrete semantic tokens.
  - Quick check question: Why is k-means clustering preferred over learnable VQ for the semantic layer in this architecture?

- Concept: Latent diffusion models for conditional generation
  - Why needed here: LDM decoder conditions on sparse tokens to generate high-fidelity audio, enabling ultra-low bitrate compression.
  - Quick check question: How does classifier-free guidance improve sampling quality in diffusion models?

## Architecture Onboarding

- Component map: Mel-spectrogram -> AudioMAE feature extraction -> Stacked feature -> Semantic VQ -> Acoustic VQ -> LDM -> VAE decoder + HiFi-GAN vocoder -> Output waveform
- Critical path: Mel-spectrogram -> AudioMAE -> Semantic VQ -> Acoustic VQ -> LDM -> VAE + Vocoder
- Design tradeoffs:
  - Semantic codebook size vs bitrate: Larger vocab improves quality but increases bitrate.
  - Stack factor K: Affects temporal resolution and feature aggregation.
  - Acoustic codebook size: Larger codebook improves reconstruction but increases model complexity.
- Failure signatures:
  - Unintelligible speech: Acoustic VQ layer insufficient or BiLSTM too weak.
  - Low semantic accuracy: K-means centroids poorly representative or semantic codebook too small.
  - Poor reconstruction: LDM underfit or insufficient CFG guidance scale.
- First 3 experiments:
  1. Remove acoustic VQ layer and measure WER and ViSQOL to confirm its necessity.
  2. Swap semantic codebook from k-means to learnable VQ and compare classification accuracy.
  3. Vary semantic codebook size (4096 â†’ 32768 centroids) and plot ViSQOL/WER vs bitrate trade-off.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SemantiCodec's performance scale with different AudioMAE feature sizes and architectures beyond the 768-dimensional embedding used in this work?
- Basis in paper: [explicit] The paper mentions that the AudioMAE model with 768-dimensional embeddings was used, but suggests exploring higher sampling rate versions and potentially different feature sizes in future work.
- Why unresolved: The current work only evaluates one specific AudioMAE configuration, leaving open questions about how different feature dimensions or alternative AudioMAE architectures might impact performance.
- What evidence would resolve it: Comparative studies using different AudioMAE feature sizes and architectures while maintaining other variables constant.

### Open Question 2
- Question: What is the optimal trade-off between semantic codebook size and bitrate for different audio types (speech, music, general sound)?
- Basis in paper: [explicit] The paper shows that larger semantic codebook sizes improve reconstruction quality and semantic richness, but also notes the trade-off with bitrate. The current implementation uses different centroid allocations for different audio types.
- Why unresolved: While the paper demonstrates the existence of this trade-off, it doesn't systematically explore the optimal balance for each audio type or provide guidelines for codebook sizing.
- What evidence would resolve it: Systematic evaluation of reconstruction quality, semantic richness, and bitrate across various codebook sizes for each audio type.

### Open Question 3
- Question: How does SemantiCodec's performance compare to traditional codecs in real-time communication scenarios with varying packet loss rates?
- Basis in paper: [explicit] The paper focuses on ultra-low bitrate performance and reconstruction quality, but doesn't evaluate robustness to packet loss or compare to traditional codecs in communication scenarios.
- Why unresolved: The paper demonstrates strong performance in ideal conditions but doesn't address how the codec performs under realistic network conditions that affect real-time communication.
- What evidence would resolve it: Comparative studies measuring reconstruction quality, intelligibility, and latency under different packet loss conditions compared to traditional codecs.

## Limitations

- Generalizability to non-speech audio remains uncertain, as semantic codebook construction and validation primarily focus on speech data, with limited evidence for music and environmental sounds.
- Diffusion model stability at ultra-low bitrates is not extensively characterized, with potential over-reliance on high CFG scales or many diffusion steps limiting practical deployment.
- Performance under real-world conditions (noisy audio, packet loss) is not evaluated, leaving questions about robustness in practical communication scenarios.

## Confidence

**High Confidence**: The dual-encoder architecture with semantic and acoustic separation is well-supported by the results. The paper demonstrates superior reconstruction quality (ViSQOL, MEL, STFT) and semantic classification accuracy compared to baselines across multiple datasets.

**Medium Confidence**: The claim that semantic tokens remain rich even at ultra-low bitrates (0.31 kbps) is supported by classification accuracy results, but this is primarily demonstrated on speech data. Generalizability to music and environmental sounds is assumed but not thoroughly validated.

**Low Confidence**: The paper does not provide sufficient evidence for the robustness of the codec in real-world conditions with noisy or variable audio. The assumption that the semantic codebook derived from AudioMAE features will generalize well to all audio types is not rigorously tested.

## Next Checks

1. Evaluate intelligibility on non-speech and noisy audio: Test WER and subjective quality (MUSHRA) on non-speech audio (music, environmental sounds) and noisy speech at the lowest bitrates (0.31, 0.63 kbps). This will validate the claim of generalizability and robustness across diverse audio types.

2. Ablate diffusion model parameters: Systematically vary CFG guidance scale and number of sampling steps to identify the minimum requirements for stable reconstruction at ultra-low bitrates. This will assess the practical limitations of the diffusion-based approach and inform deployment considerations.

3. Compare semantic codebook construction methods: Replace k-means clustering with learnable vector quantization for the semantic layer and compare classification accuracy on HEAR benchmark tasks. This will validate the assumption that k-means clustering on AudioMAE features is optimal for semantic codebook construction and identify potential improvements.