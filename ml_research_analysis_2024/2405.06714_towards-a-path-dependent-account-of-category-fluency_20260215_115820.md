---
ver: rpa2
title: Towards a Path Dependent Account of Category Fluency
arxiv_id: '2405.06714'
source_url: https://arxiv.org/abs/2405.06714
tags:
- category
- search
- fluency
- human
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates category fluency generation by reformulating
  existing optimal foraging models as sequence generators. The authors add a subcategory
  cue to Hills et al.
---

# Towards a Path Dependent Account of Category Fluency

## Quick Facts
- arXiv ID: 2405.06714
- Source URL: https://arxiv.org/abs/2405.06714
- Reference count: 2
- Primary result: Deterministic search strategies outperform stochastic sampling in category fluency generation, and models with subcategory cues plus explicit global weighting produce more human-like sequences.

## Executive Summary
This work reformulates optimal foraging models as sequence generators for category fluency tasks, extending Hills et al.'s cue-switching framework with a subcategory cue. The authors evaluate multiple architectures (including Llama 2 7B) using n-gram overlap metrics against 141 human-generated sequences. They find that switch prediction alone cannot distinguish between models, but the inclusion of subcategory cues and proper global cue weighting significantly improves generation quality. Notably, deterministic search methods (greedy, beam) consistently outperform stochastic sampling, challenging assumptions about random-walk foraging behavior.

## Method Summary
The authors cast category fluency as a sequential generation task and test multiple model variants: original cue-switching models, models with added subcategory cues, and a large language model baseline (Llama 2 7B). Generation strategies include both deterministic (greedy, beam) and stochastic (temperature sampling, top-k, nucleus) approaches. Performance is evaluated using n-gram overlap metrics (BLEU) against human-generated sequences across 141 runs. Models are tested with and without explicit global cue weighting to assess the importance of search guidance.

## Key Results
- Switch prediction models (IRT and entropy-based) perform similarly across architectures, making switch prediction insufficient for model selection
- Deterministic search strategies (greedy, beam) consistently outperform stochastic sampling methods
- Adding subcategory cues and proper global cue weighting significantly improves generation quality
- LLMs require explicit global cues to produce human-like sequences

## Why This Works (Mechanism)
The dual-process cue-switching account better explains human-like category fluency because it incorporates both local exploration (switching within subcategories) and global guidance (overarching category structure). This structured approach enables more coherent and human-like generation compared to pure random walk models. The success of deterministic search suggests that category fluency involves deliberate, path-dependent search rather than noise-driven exploration.

## Foundational Learning
- **Category Fluency**: Why needed - Core cognitive task being modeled; Quick check - Understand that it involves generating items from semantic categories
- **Optimal Foraging Theory**: Why needed - Provides theoretical framework for switch prediction; Quick check - Know that it models when to switch between resource patches
- **Cue-Switching Models**: Why needed - Base architecture being extended; Quick check - Understand how local vs global cues guide generation
- **Deterministic vs Stochastic Search**: Why needed - Key methodological comparison; Quick check - Know that deterministic methods follow fixed rules while stochastic methods incorporate randomness
- **BLEU Metrics**: Why needed - Evaluation metric for sequence generation; Quick check - Understand n-gram overlap as a measure of sequence similarity

## Architecture Onboarding

**Component Map:** Input Categories -> Cue Processing (Local+Global) -> Switch Prediction -> Sequence Generation -> BLEU Evaluation

**Critical Path:** Cue weighting → Switch prediction → Sequence generation strategy → Evaluation

**Design Tradeoffs:** Deterministic search provides better coherence but may lack diversity; stochastic methods offer variety but sacrifice human-likeness; explicit global cues improve performance but require additional architectural complexity.

**Failure Signatures:** Poor BLEU scores indicate inadequate cue weighting or switch prediction; failure to match human sequences suggests missing subcategory structure or improper global cue integration.

**First Experiments:**
1. Test deterministic vs stochastic search on a small subset of categories
2. Evaluate impact of subcategory cues with and without global weighting
3. Compare switch prediction methods (IRT vs entropy) in isolation

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies solely on n-gram overlap (BLEU) without semantic or human judgment measures
- Llama 2 7B is relatively small compared to state-of-the-art models
- Controlled experimental setting may not capture full complexity of real-world fluency tasks

## Confidence
- High: Deterministic search outperforms stochastic sampling
- Medium: Dual-process cue-switching better explains human-like behavior (depends on BLEU metrics)
- Medium: LLMs require explicit global cues (pending validation with larger models)

## Next Checks
1. Replicate key findings using human judgment scores and semantic similarity metrics alongside BLEU
2. Test the subcategory cue mechanism with larger LLMs (Llama 2 70B, GPT-4)
3. Evaluate model performance on naturalistic, open-ended fluency tasks beyond controlled categories