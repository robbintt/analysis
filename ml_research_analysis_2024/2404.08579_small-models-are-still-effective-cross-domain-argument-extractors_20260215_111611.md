---
ver: rpa2
title: Small Models Are (Still) Effective Cross-Domain Argument Extractors
arxiv_id: '2404.08579'
source_url: https://arxiv.org/abs/2404.08579
tags:
- event
- role
- famus
- argument
- transfer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether small models can effectively perform
  zero-shot event argument extraction across different domains. The authors compare
  question answering (QA) and template infilling (TI) approaches using Flan-T5-base
  (250M parameters) against GPT-3.5 and GPT-4.
---

# Small Models Are (Still) Effective Cross-Domain Argument Extractors

## Quick Facts
- **arXiv ID**: 2404.08579
- **Source URL**: https://arxiv.org/abs/2404.08579
- **Reference count**: 24
- **Key outcome**: Flan-T5-base (250M) outperforms GPT-3.5 and GPT-4 in zero-shot cross-domain event argument extraction when trained on appropriate source ontologies

## Executive Summary
This paper investigates whether small models can effectively perform zero-shot event argument extraction (EAE) across different domains and ontologies. The authors compare two reformulation approaches - question answering (QA) and template infilling (TI) - using Flan-T5-base against GPT-3.5 and GPT-4. Despite massive differences in pretraining data and parameter counts, Flan-T5 models trained on source ontologies consistently achieve superior zero-shot performance on target domains. The study demonstrates that smaller models can be more effective than large language models for cross-domain EAE when trained on suitable source data with appropriate reformulation methods.

## Method Summary
The authors fine-tune Flan-T5-base models on six major event extraction datasets (ACE, ERE-L, ERE-R, FAMuS, RAMS, WikiEvents) using two reformulation approaches: QA and TI. Expert-written questions and templates are created for all event roles across these ontologies. The models are then evaluated zero-shot on all target datasets using exact match F1. GPT-3.5 and GPT-4 are also evaluated zero-shot using the same expert-written prompts. The study compares performance across all dataset pairs and investigates the impact of paraphrased training data.

## Key Results
- Flan-T5 models trained on source ontologies consistently outperform GPT-3.5 and GPT-4 in zero-shot EAE (e.g., 25.37 vs 22.54 F1 on WikiEvents)
- Template infilling is generally more parameter-efficient than question answering, requiring only one forward pass versus k passes
- Paraphrasing training data provides some gains in TI settings (up to 3.7 F1) but shows mixed results in QA settings
- Transfer performance correlates with in-domain performance, suggesting generalization capability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Flan-T5-base with appropriate source ontology transfer can outperform GPT-3.5 and GPT-4 in zero-shot EAE
- Mechanism: Smaller models trained on source ontologies capture transferable structural patterns better than larger models relying on general pretraining
- Core assumption: Transfer learning effectiveness depends more on training data relevance than model size for this specific task
- Evidence anchors:
  - [abstract] "smaller models trained on an appropriate source ontology can yield zero-shot performance superior to that of GPT-3.5 or GPT-4"
  - [section 4] "Flan-T5 model(s) obtains zero-shot performance superior to GPT-3.5â€”often by wide margins"
  - [corpus] Weak - no direct corpus evidence comparing model sizes on similar tasks
- Break condition: When source and target ontologies are highly dissimilar or when target domain requires broad general knowledge

### Mechanism 2
- Claim: Question answering and template infilling are effective reformulation methods for cross-domain EAE
- Mechanism: Reformulating EAE as QA or TI enables models to leverage question-answering or infilling capabilities during transfer
- Core assumption: The semantic relationships between questions/templates and event roles are preserved across ontologies
- Evidence anchors:
  - [section 2] "questions correspond to specific roles" and "slots in fixed templates for each event type are populated with extracted arguments"
  - [section 4] "neither method is consistently dominant across domains"
  - [corpus] Moderate - related work shows QA/TI effectiveness for EAE in general
- Break condition: When questions/templates cannot capture role semantics or when role sets differ significantly

### Mechanism 3
- Claim: Paraphrasing questions and templates can improve transfer performance
- Mechanism: Paraphrases provide model with multiple linguistic expressions of the same semantic content, increasing robustness
- Core assumption: Model benefits from seeing diverse linguistic expressions of identical semantic content
- Evidence anchors:
  - [section 4] "some gain from paraphrases across all ontologies in the TI setting (from 0.1 F1 on ACE, up to 3.7F1 on ERE-L)"
  - [section 4] "more mixed results in the QA setting"
  - [corpus] Weak - no corpus evidence on paraphrase effectiveness for EAE specifically
- Break condition: When paraphrases introduce semantic drift or when training budget is constrained

## Foundational Learning

- Concept: Event Argument Extraction
  - Why needed here: This is the core task being solved - extracting arguments (entities/roles) for events
  - Quick check question: What is the difference between an event trigger and an event argument?

- Concept: Zero-shot Transfer Learning
  - Why needed here: The models are being evaluated on target ontologies without any training on them
  - Quick check question: How does zero-shot transfer differ from few-shot learning?

- Concept: Template Infilling
  - Why needed here: One of the two main approaches used - filling templates with extracted arguments
  - Quick check question: What is the key advantage of template infilling over question answering for EAE?

## Architecture Onboarding

- Component map:
  - Flan-T5-base encoder-decoder model -> Question answering head (for QA approach) or Template infilling mechanism (for TI approach) -> Confidence thresholding for argument selection

- Critical path:
  1. Load pretrained Flan-T5-base model
  2. Fine-tune on source ontology (QA or TI formulation)
  3. Apply to target domain with threshold-based argument selection
  4. Evaluate with exact match F1

- Design tradeoffs:
  - QA vs TI: QA requires k forward passes (one per role), TI requires only one
  - Template vs natural language: Templates use role names (label semantics), QA uses natural language questions
  - Paraphrasing: Provides robustness but increases training cost

- Failure signatures:
  - Near-zero F1 scores indicate ontology mismatch
  - Performance worse than random suggests incorrect question/template formulation
  - Inconsistent results across runs suggests hyperparameter issues

- First 3 experiments:
  1. Fine-tune Flan-T5-base on ACE using QA formulation, evaluate on ERE-L
  2. Fine-tune Flan-T5-base on RAMS using TI formulation, evaluate on WikiEvents
  3. Generate paraphrases for FAMuS questions, retrain, compare performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do Flan-T5 models trained on appropriate source ontologies achieve superior zero-shot performance compared to GPT-3.5 and GPT-4 in cross-domain event argument extraction?
- Basis in paper: [explicit] The paper states that Flan-T5 models trained on appropriate source ontologies can yield zero-shot performance superior to that of GPT-3.5 and GPT-4, despite the massive differences in pretraining data and parameter counts.
- Why unresolved: The paper does not provide a detailed analysis of the specific factors contributing to the superior performance of Flan-T5 models, such as the impact of training data size, model architecture, or fine-tuning strategies.
- What evidence would resolve it: A detailed comparative analysis of the training processes, architectures, and performance metrics of Flan-T5 and GPT models in cross-domain event argument extraction tasks.

### Open Question 2
- Question: What are the limitations of using zero-shot transfer for event argument extraction across more distant ontology pairs, and how can these limitations be addressed?
- Basis in paper: [explicit] The paper mentions that transfer between distant ontology pairs remains challenging in absolute terms and suggests that training a smaller model in-house on recasted existing resources can be a cheaper, more effective first-line approach for extraction in new domains.
- Why unresolved: The paper does not explore specific strategies or techniques to improve zero-shot transfer performance for more distant ontology pairs, such as domain adaptation methods or advanced transfer learning approaches.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of various domain adaptation and transfer learning techniques in improving zero-shot transfer performance for distant ontology pairs.

### Open Question 3
- Question: How does the inclusion of paraphrased questions and templates during training affect the performance of Flan-T5 models in cross-domain event argument extraction, and under what conditions are paraphrases most beneficial?
- Basis in paper: [explicit] The paper investigates the value of augmenting training data with paraphrases of questions and templates, finding some gain from paraphrases across all ontologies in the TI setting, with more mixed results in the QA setting.
- Why unresolved: The paper does not provide a comprehensive analysis of the impact of paraphrases on model performance, including the optimal number of paraphrases, the types of paraphrases that are most effective, and the conditions under which paraphrases are most beneficial.
- What evidence would resolve it: A systematic study of the effects of different numbers and types of paraphrases on model performance, including experiments varying the number of paraphrases and the similarity between paraphrases and original questions/templates.

## Limitations
- Limited to small source datasets; performance advantage may diminish with larger training corpora
- Reliance on expert-written questions and templates limits practical applicability
- Does not explore few-shot learning scenarios which could reveal different performance patterns

## Confidence
- High confidence: Flan-T5-base can outperform GPT-3.5/GPT-4 in zero-shot EAE when trained on appropriate source ontologies
- Medium confidence: Template infilling is generally more parameter-efficient than question answering
- Low confidence: Paraphrasing consistently improves transfer performance

## Next Checks
1. **Scale sensitivity test**: Evaluate whether the performance advantage of smaller models persists when trained on larger, more comprehensive source datasets
2. **Few-shot comparison**: Compare few-shot learning performance of Flan-T5-base versus GPT models on target ontologies
3. **Automatic prompt generation**: Test the robustness of the approach using automatically generated questions and templates rather than expert-written ones