---
ver: rpa2
title: Using Machine Learning to Discover Parsimonious and Physically-Interpretable
  Representations of Catchment-Scale Rainfall-Runoff Dynamics
arxiv_id: '2412.04845'
source_url: https://arxiv.org/abs/2412.04845
tags:
- node
- network
- output
- performance
- subplots
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the development of interpretable machine
  learning models for catchment-scale rainfall-runoff modeling by employing the Mass
  Conserving Perceptron (MCP) as a fundamental computational unit. The key challenge
  addressed is the lack of physical interpretability in traditional machine learning
  methods, which limits their credibility for decision-making despite excellent predictive
  performance.
---

# Using Machine Learning to Discover Parsimonious and Physically-Interpretable Representations of Catchment-Scale Rainfall-Runoff Dynamics

## Quick Facts
- **arXiv ID**: 2412.04845
- **Source URL**: https://arxiv.org/abs/2412.04845
- **Reference count**: 0
- **Primary result**: Single-layer, three-to-five-node distributed-state network with context-dependent gating achieves median KGE score of 0.92 while maintaining physical interpretability through identifiable flow pathways.

## Executive Summary
This study addresses the challenge of developing interpretable machine learning models for catchment-scale rainfall-runoff modeling by employing Mass Conserving Perceptrons (MCP) as fundamental computational units. The authors propose distributed-state neural network architectures that maintain mass conservation at the nodal level while allowing flexibility in network-level representations. Through extensive experiments on the Leaf River Basin dataset, the study demonstrates that single-layer architectures with information sharing across nodes achieve superior performance compared to traditional approaches, providing both high predictive accuracy and physical interpretability through identifiable slow, intermediate, and fast flow pathways.

## Method Summary
The method employs Mass Conserving Perceptrons as building blocks for neural networks, where each node maintains mass conservation while allowing flexible flow conductivities through context-dependent gating functions. The distributed-state architecture treats each node as receiving full precipitation input but maintaining different moisture states, enabling representation of catchment moisture variability. Cell-state information sharing is implemented across nodes to synchronize gating functions based on collective moisture distribution. Models are trained using ADAM optimization with KGE as the objective function, and data is split into training, selection, and testing sets using a robust allocation method ensuring distributional consistency.

## Key Results
- Single-layer, three-to-five-node distributed-state networks achieve median KGE score of 0.92
- Information sharing across nodes significantly improves performance, especially for dry years
- Pruning redundant flow paths can enhance model parsimony without sacrificing accuracy
- Distributed-state architecture outperforms distributed-input architecture for capturing catchment dynamics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Distributed-state architecture outperforms distributed-input architecture because it captures catchment moisture state variability better.
- **Mechanism**: Distributed-state models treat each node as receiving the full precipitation input but maintaining different moisture states, enabling representation of a distribution of storage conditions rather than partitioned flow paths.
- **Core assumption**: The catchment's hydrological response depends more on the distribution of moisture states than on how precipitation is spatially partitioned.
- **Evidence anchors**:
  - [abstract] "distributed-state mechanism ensures a sufficient number of temporally-evolving properties of system storage"
  - [section 4.1] "Distributed-State (DS) architecture should be preferred over the distributed-input (DI) architecture"
- **Break condition**: If the catchment exhibits strong spatial heterogeneity in precipitation inputs or distinct flow path dependencies, distributed-input might outperform.

### Mechanism 2
- **Claim**: Information sharing across nodes significantly improves model performance by synchronizing state-dependent gating functions.
- **Mechanism**: When output and loss gates access cell-state information from all nodes in the same layer, they can better coordinate flow conductivities based on the overall moisture distribution rather than individual node states.
- **Core assumption**: The catchment's hydrological behavior depends on the collective moisture state distribution, not just individual storage components.
- **Evidence anchors**:
  - [abstract] "information-sharing ensures proper synchronization of such properties"
  - [section 5.1] "Incorporating cell-state information sharing at the gates results in significant performance improvements"
- **Break condition**: If nodes represent truly independent hydrological processes with no interaction, sharing information could introduce spurious correlations.

### Mechanism 3
- **Claim**: Pruning redundant flow paths maintains predictive accuracy while improving model interpretability.
- **Mechanism**: Some flow paths in multi-node architectures become functionally redundant, contributing similar information to the output, so removing them doesn't degrade performance but simplifies the model structure.
- **Core assumption**: Not all nodes in a multi-node architecture provide unique hydrological information; some represent similar flow behaviors.
- **Evidence anchors**:
  - [section 7.2] "the five-node ùëÄùëÅG.07)O(5) network may contain redundant flow paths"
  - [section 7.2] "removing 1 or 2 paths has little or no apparent impact on distributional performance"
- **Break condition**: If each node represents a genuinely distinct hydrological process or spatial region, pruning would reduce model fidelity.

## Foundational Learning

- **Concept**: Mass conservation at nodal level while relaxing it at network level
  - Why needed here: Allows models to correct for input/output biases while maintaining physical realism at individual process representations
  - Quick check question: What's the difference between nodal mass conservation and network-level mass conservation?

- **Concept**: Context-dependent gating functions
  - Why needed here: Hydrological systems have non-linear responses that vary with moisture conditions, requiring gates that adapt based on current state and environmental conditions
  - Quick check question: How do output and loss gates differ in what they depend on in the MCP architecture?

- **Concept**: Distributed-state representation of catchment moisture
  - Why needed here: Single storage state is insufficient to capture catchment dynamics; multiple states represent different moisture conditions across the system
  - Quick check question: Why does the distributed-state architecture provide precipitation to all nodes equally?

## Architecture Onboarding

- **Component map**: Precipitation ‚Üí Node cell states ‚Üí Gate computations ‚Üí Flow component aggregation ‚Üí Streamflow output
- **Critical path**: Precipitation ‚Üí Node cell states ‚Üí Gate computations ‚Üí Flow component aggregation ‚Üí Streamflow output
- **Design tradeoffs**:
  - Depth vs. width: More layers vs. more nodes per layer
  - Mass conservation strictness vs. predictive accuracy
  - Interpretability vs. model complexity
  - Information sharing overhead vs. performance gain
- **Failure signatures**:
  - Poor performance on dry years suggests insufficient state representation or gating flexibility
  - Negative flow predictions indicate overly relaxed mass constraints
  - Multiple similar flow paths suggest architectural redundancy
  - Poor generalization across catchments indicates overfitting to specific conditions
- **First 3 experiments**:
  1. Test single-layer distributed-state vs. distributed-input with 3 nodes each
  2. Compare information sharing vs. no sharing configurations with same architecture
  3. Evaluate pruning impact by removing one node from a 5-node network and measuring performance change

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do MCP-based networks perform in ungauged basins compared to LSTM and physical-conceptual models?
- Basis in paper: [explicit] The paper states that this is "work in progress" and that current scope is restricted to single-location examination rather than universal applicability across large samples of catchments.
- Why unresolved: The study focuses on a single catchment (Leaf River Basin) and does not explore regionalization or transfer learning capabilities.
- What evidence would resolve it: Performance comparisons of MCP-based networks across multiple catchments with varying hydroclimatic conditions, including ungauged basin scenarios.

### Open Question 2
- Question: What is the optimal balance between network depth and information sharing for MCP-based models?
- Basis in paper: [explicit] The paper found that information sharing significantly improves performance and that single-layer networks with sharing outperform deeper networks without sharing, but did not systematically explore all combinations of depth and sharing configurations.
- Why unresolved: The study primarily focused on single-layer architectures with sharing versus multi-layer without sharing, without exploring intermediate configurations (e.g., 2-3 layers with partial sharing).
- What evidence would resolve it: Systematic comparison of performance across different depths (1-3 layers) with various information sharing configurations (none, output only, loss only, both).

### Open Question 3
- Question: How sensitive are MCP-based models to parameter initialization and training procedures?
- Basis in paper: [inferred] The paper mentions that initialization strategies differ between single-layer and multi-layer networks, and that some approaches "did not perform well" for multi-layer cases, but does not systematically test sensitivity to initialization methods.
- Why unresolved: The study uses specific initialization strategies but does not explore how different initialization schemes (e.g., Xavier, He initialization) or training procedures affect model performance and interpretability.
- What evidence would resolve it: Comparative analysis of model performance using different initialization methods and training procedures, including sensitivity analysis of convergence and final performance metrics.

## Limitations

- Primary validation on a single catchment (Leaf River Basin) limits generalizability across diverse hydrological regimes
- Lack of systematic exploration of depth vs. information sharing trade-offs leaves optimal architectural configuration unclear
- Limited validation of pruning strategies for improving interpretability without comprehensive testing of different pruning approaches

## Confidence

**High Confidence**: The core finding that single-layer distributed-state MCP networks outperform both traditional MLP models and distributed-input architectures is well-supported by extensive ablation studies and quantitative performance metrics. The KGE scores (median 0.92) and systematic comparisons provide strong empirical evidence.

**Medium Confidence**: The mechanism explaining why information sharing across nodes improves performance is conceptually sound but lacks rigorous theoretical justification. While empirical results show significant improvements, the exact nature of how cell-state synchronization benefits hydrological modeling could be further elaborated.

**Low Confidence**: The claim that pruning redundant flow paths maintains predictive accuracy while improving interpretability needs more systematic validation. The current evidence is based on limited ablation studies without comprehensive testing of different pruning strategies or quantification of interpretability gains.

## Next Checks

1. **Cross-catchment validation**: Test the best-performing single-layer distributed-state architecture on 3-5 additional catchments with contrasting hydroclimatic conditions to assess generalizability beyond the Leaf River Basin.

2. **Information sharing sensitivity analysis**: Systematically vary the degree of cell-state information sharing (partial vs. full) and quantify the performance trade-offs to determine optimal sharing configurations for different catchment characteristics.

3. **Pruning strategy evaluation**: Implement systematic pruning of nodes from 5-node architectures, measuring performance degradation and interpretability improvements using established metrics for both predictive accuracy and model complexity reduction.