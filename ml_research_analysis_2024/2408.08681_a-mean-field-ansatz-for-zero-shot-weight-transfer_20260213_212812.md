---
ver: rpa2
title: A Mean Field Ansatz for Zero-Shot Weight Transfer
arxiv_id: '2408.08681'
source_url: https://arxiv.org/abs/2408.08681
tags:
- layer
- measure
- weights
- weight
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a mean field ansatz (RC ansatz) to theoretically
  support weight transfer methods for neural networks. The key idea is to view neural
  network weights as samples from a joint distribution under proper assumptions.
---

# A Mean Field Ansatz for Zero-Shot Weight Transfer

## Quick Facts
- arXiv ID: 2408.08681
- Source URL: https://arxiv.org/abs/2408.08681
- Authors: Xingyuan Chen; Wenwei Kuang; Lei Deng; Wei Han; Bo Bai; Goncalo dos Reis
- Reference count: 40
- One-line primary result: Theoretical framework showing weight transfer methods can be viewed as sampling from joint distributions of neural network weights

## Executive Summary
This paper proposes a mean field ansatz (RC ansatz) to provide theoretical support for zero-shot weight transfer methods in neural networks. The key insight is to view neural network weights as samples from a joint distribution under mean field assumptions, with the RC ansatz describing the measure structure using row and column random variables. The framework demonstrates that weight transfer methods are equivalent to sampling methods from the empirical distribution of trained weights, validated through experiments on MLPs and LLMs. The results show that weight transfer can be theoretically justified even for LLMs trained for sufficiently long durations, providing a foundation for understanding how knowledge transfers between differently-sized networks.

## Method Summary
The method introduces the row-column (RC) ansatz as a mean field approach to model neural network weights as samples from a joint distribution. The framework decomposes weight matrices into row and column random variables, enabling the representation of differently-sized networks through a common distribution structure. Weight transfer is implemented as a sampling procedure from the empirical measure of a trained network to construct new networks of different sizes. The approach is validated through experiments on CIFAR-10 with MLPs and on large language models including GPT-3 and Llama-3.1, comparing different parametrization methods and sampling strategies.

## Key Results
- The RC ansatz successfully describes the measure structure of weights in both MLPs and LLMs
- Weight transfer methods perform comparably to standard parametrization methods in MLPs
- LLMs trained for sufficiently long durations (T >> N) exhibit mean-field behavior that enables effective weight transfer
- Different sampling strategies (random, group, function-based) can be used for weight transfer with varying effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weight transfer works because neural network weights can be modeled as samples from a joint distribution under mean field assumptions
- Mechanism: The RC ansatz describes the measure structure of weights using row and column random variables, allowing different-sized networks to share a common distribution
- Core assumption: Neural network weights admit a product form measure structure under proper assumptions
- Evidence anchors:
  - [abstract]: "we propose the row-column (RC) ansatz under the mean field point of view, which describes the measure structure of the weights in the neural network (NN)"
  - [section 3.1]: "The RC ansatz provides a description of the measure structure for the corresponding NN, it describes a matrix-type weight with 2 random variables (RVs) instead of a single RV"
  - [corpus]: "Average neighbor FMR=0.537, average citations=0.0" - Weak evidence from related work
- Break condition: If the weight matrices cannot be decomposed into row-column random variables, or if the product form measure structure does not hold

### Mechanism 2
- Claim: Weight transfer methods are equivalent to sampling methods from the empirical distribution
- Mechanism: Given a trained network with width N, a new network with width Ñ can be constructed by generating Ñ samples from the empirical measure of the original network
- Core assumption: The empirical measure of trained weights converges to some limit measure as N → ∞
- Evidence anchors:
  - [abstract]: "weight transfer methods can be viewed as sampling methods"
  - [section 3.2]: "The key idea above is the measure structure µRC t =Q|i| i=1 µRC i,t using the RC ansatz"
  - [section 6.2.1]: "the weight transfer may play a crucial role to the MFNN. We can find the 'nice measure' by starting from different guessed measures on small models"
- Break condition: If the empirical measure does not converge to a limit measure, or if the sampling method does not preserve the essential characteristics of the distribution

### Mechanism 3
- Claim: The RC ansatz enables closed measure dynamics for multi-layer neural networks
- Mechanism: By decomposing weight matrices into row and column random variables, the dependencies across layers can be properly captured, allowing the evolution of the measure to depend only on the measure itself
- Core assumption: The weight update dynamics can be expressed in terms of the row and column random variables
- Evidence anchors:
  - [section 3.1]: "The product form of the RC ansatz splits the whole measure into different parts based on Γ set, this separation comes from the nature of the first-order gradient based method"
  - [section 6.3.6]: "So that we can see under the set up of {u, R(1), C(1), R(2), C(2), R(3), C(3), v}t ∼ µt ∈ P (R8) with µt = µv,R(3) t × µC(3),R(2) t × µC(2),R(1) t × µC(1),u t defined in (6.22), the dynamics for measures are now closed"
  - [corpus]: "Top related titles: The Impact of Model Zoo Size and Composition on Weight Space Learning" - Limited direct evidence
- Break condition: If the weight update dynamics cannot be expressed in terms of the row and column random variables, or if the closed measure dynamics assumption is violated

## Foundational Learning

- Concept: Mean Field Theory
  - Why needed here: The paper applies mean field theory to neural network dynamics, treating weights as samples from a joint distribution
  - Quick check question: What is the key idea behind mean field theory in the context of neural networks?

- Concept: Empirical Measure Convergence
  - Why needed here: The paper relies on the convergence of empirical measures to limit measures as the network width increases
  - Quick check question: Under what conditions does the empirical measure of neural network weights converge to a limit measure?

- Concept: Measure Structure and Ansatz
  - Why needed here: The paper introduces the RC ansatz to describe the measure structure of neural network weights, which is crucial for the theoretical framework
  - Quick check question: What is the purpose of the RC ansatz in the context of neural network weight transfer?

## Architecture Onboarding

- Component map:
  - RC ansatz: Decomposes weight matrices into row and column random variables
  - Measure structure: Describes the dependencies across layers using a product form
  - Weight transfer framework: Implements sampling methods from the empirical distribution
  - Sampling methods: Provides various techniques for generating new samples from existing weights

- Critical path:
  1. Initialize neural network with RC ansatz
  2. Train network to obtain empirical measure of weights
  3. Apply weight transfer framework to generate new samples
  4. Construct new network with transferred weights

- Design tradeoffs:
  - Degree of freedom vs. chaos: RC initialization has fewer degrees of freedom but may introduce chaos
  - Sample size vs. quality: Larger sample size may lead to better results but requires more computational resources
  - Complexity vs. interpretability: More complex measure structures may provide better results but are harder to interpret

- Failure signatures:
  - Poor performance of transferred weights
  - Divergence of empirical measure during training
  - Violation of closed measure dynamics assumption

- First 3 experiments:
  1. Verify the RC ansatz by training a simple MLP and checking for row-column correlations in the weights
  2. Apply weight transfer to a trained MLP and evaluate the performance of the transferred weights
  3. Extend the RC ansatz to a more complex neural network architecture (e.g., with skip connections or attention blocks) and verify the measure structure

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal sampling strategy from the empirical distribution to maximize transfer performance across different network sizes?
- Basis in paper: Explicit - The paper discusses various sampling methods (random, group, function-based) but doesn't identify which performs best in general
- Why unresolved: The paper acknowledges that sampling by norm and adding randomness may already change the measure, so there's no guaranteed method that works for all cases
- What evidence would resolve it: Systematic comparison of different sampling strategies across various network architectures, transfer scenarios, and performance metrics

### Open Question 2
- Question: Does the RC ansatz provide theoretical guarantees for the convergence of weight transfer methods in practice?
- Basis in paper: Inferred - While the paper shows empirical validation, it acknowledges that "there is no guaranteed this can work for general cases"
- Why unresolved: The paper focuses on empirical validation rather than rigorous mathematical proof of convergence guarantees
- What evidence would resolve it: Mathematical proofs of convergence rates and conditions under which the RC ansatz ensures successful weight transfer

### Open Question 3
- Question: How does the training step count relative to network width (T vs N) affect the validity of the mean-field approximation in practice?
- Basis in paper: Explicit - The paper notes that for LLMs, T ≫ N leads to mean-field dynamics, but doesn't quantify the threshold
- Why unresolved: The paper observes that LLMs with T ~ O(N) show mean-field behavior but doesn't establish precise conditions
- What evidence would resolve it: Empirical studies mapping the relationship between T/N ratios and mean-field approximation quality across different architectures

### Open Question 4
- Question: What is the relationship between the RC initialization method and feature learning capacity in deep networks?
- Basis in paper: Explicit - The paper introduces RC-initialization but finds it underperforms i.i.d initialization despite theoretical advantages
- Why unresolved: The paper speculates that RC-initialization may introduce too little randomness but doesn't resolve the apparent contradiction
- What evidence would resolve it: Comparative studies of feature learning capacity (as defined in Yang and Hu 2021) across different initialization schemes in deep networks

### Open Question 5
- Question: Does the existence and uniqueness of the optimal measure for a given loss function depend on network architecture or initialization scheme?
- Basis in paper: Explicit - The paper acknowledges that "there is a limited guarantee of the existence and uniqueness of the optimal measure"
- Why unresolved: The paper doesn't provide conditions under which the optimal measure exists or is unique
- What evidence would resolve it: Theoretical analysis establishing conditions for existence/uniqueness and empirical verification across different architectures and initializations

## Limitations

- The RC ansatz assumes a specific measure structure that may not hold for all neural network architectures or training regimes
- The empirical evidence is primarily focused on MLP and LLM architectures, with limited exploration of other network types
- The theoretical framework relies heavily on mean field assumptions, which may break down in practical scenarios with finite-width networks

## Confidence

- **High Confidence**: The basic mechanism of weight transfer as sampling from empirical distributions is well-supported by experimental results
- **Medium Confidence**: The theoretical framework using RC ansatz provides a coherent explanation for weight transfer, though some assumptions may be restrictive
- **Low Confidence**: The generalizability of the approach to diverse neural network architectures beyond MLPs and LLMs

## Next Checks

1. **Architecture Extension Test**: Apply the RC ansatz and weight transfer framework to transformer variants with different attention mechanisms to verify measure structure preservation
2. **Scale Sensitivity Analysis**: Systematically study how the quality of weight transfer varies with network width ratios and training durations to identify robustness boundaries
3. **Distribution Divergence Measurement**: Implement quantitative metrics to measure the divergence between empirical and limit measures during training, providing concrete validation of the convergence assumption