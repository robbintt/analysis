---
ver: rpa2
title: 'Large Language Models Fall Short: Understanding Complex Relationships in Detective
  Narratives'
arxiv_id: '2402.11051'
source_url: https://arxiv.org/abs/2402.11051
tags: []
core_contribution: This paper introduces Conan, a new benchmark dataset for understanding
  complex relationships in detective narratives. The dataset features hierarchical
  relationship categories and incorporates both public and secret relationships from
  multiple character perspectives.
---

# Large Language Models Fall Short: Understanding Complex Relationships in Detective Narratives

## Quick Facts
- arXiv ID: 2402.11051
- Source URL: https://arxiv.org/abs/2402.11051
- Reference count: 19
- Primary result: Advanced LLMs like GPT-3.5, GPT-4, and Llama2 struggle with inferring complex relationships in detective narratives

## Executive Summary
This paper introduces Conan, a new benchmark dataset for understanding complex relationships in detective narratives. The dataset features hierarchical relationship categories and incorporates both public and secret relationships from multiple character perspectives. Through systematic experiments, the study reveals significant limitations in advanced LLMs' ability to infer complex relationships and handle longer narratives, highlighting the need for improved inferential capabilities in language models for narrative comprehension.

## Method Summary
The study developed Conan, a benchmark dataset consisting of 100 detective narratives with 26,779 relationships. The dataset employs a hierarchical structure with 5 top-level categories, 54 intermediate categories, and 163 detailed categories. The researchers designed three strategies for relationship extraction: AllTogether, PairRelation, and DirRelation. They conducted experiments using multiple LLM families (GPT-3.5, GPT-4, Llama2) and evaluated performance across different narrative complexities and lengths, comparing single-character perspective narratives against multi-character perspectives.

## Key Results
- LLMs perform significantly better on single-character perspective narratives compared to narratives containing multiple character perspectives with secrets, lies, and misunderstandings
- The DirRelation strategy outperforms AllTogether and PairRelation when a reliable character list is available
- Model performance degrades substantially with longer narratives, demonstrating the "lost in the middle" phenomenon
- LLMs struggle particularly with inferring secret relationships and relationships requiring complex reasoning beyond explicit text

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical relationship categories improve annotation consistency and evaluation reliability.
- Mechanism: By consolidating similar categories into broader ones until no further merging is feasible, the annotation process reduces subjectivity and potential ambiguity in relationship classification.
- Core assumption: Broader categories reduce individual annotator bias and improve agreement.
- Evidence anchors:
  - [abstract] "we established a hierarchical structure of relationship categories, comprising 5 top-level categories, 54 intermediate categories, and 163 detailed categories."
  - [section] "To mitigate such ambiguities, we consolidated similar categories into broader ones until no further merging was feasible."
- Break condition: If the consolidation process oversimplifies relationships to the point where meaningful distinctions are lost, the evaluation may fail to capture nuanced relational dynamics.

### Mechanism 2
- Claim: Extracting relationships from a single character's perspective yields better LLM performance than using all characters' perspectives.
- Mechanism: Single character narratives are more consistent and self-explanatory, reducing the complexity of handling contradictory or conflicting information.
- Core assumption: Narratives from a single character's perspective are less likely to contain self-contradictory content compared to narratives from multiple perspectives.
- Evidence anchors:
  - [abstract] "information from all characters includes secrets, misunderstandings, lies generated for self-interest or specific goals, and even delusions caused by illnesses."
  - [section] "Therefore, LLMs can just extract what is stated in the text. In contrast, information from all characters includes secrets, misunderstandings..."
- Break condition: If a single character's narrative is intentionally deceptive or unreliable, using only that perspective could lead to incorrect relationship extraction.

### Mechanism 3
- Claim: The DirRelation strategy is more effective than AllTogether or PairRelation for relationship extraction when a reliable character list is available.
- Mechanism: DirRelation separates character extraction from relationship extraction, reducing compounding errors and focusing the model's attention on relationship classification.
- Core assumption: Errors in character extraction significantly impact the accuracy of relationship extraction, so separating these tasks improves overall performance.
- Evidence anchors:
  - [section] "we distinguish and minimise the influence of errors occurring in two separate stages, character extraction, and relationship extraction"
  - [section] "DirRelation should be employed" when a reliable character list is available
- Break condition: If character extraction is unreliable or unavailable, the DirRelation strategy cannot be effectively implemented, making AllTogether or PairRelation necessary alternatives.

## Foundational Learning

- Concept: Named Entity Recognition (NER)
  - Why needed here: Character extraction is a specific area within NER, focusing on identifying characters involved in a given narrative.
  - Quick check question: What distinguishes characters from other entities like organizations or locations in narrative texts?

- Concept: Coreference Resolution
  - Why needed here: Merging all expressions in a given text that refer to the same entity is known as coreference resolution, which is crucial for correctly identifying characters with multiple identities or aliases.
  - Quick check question: How would you identify that "Costa", "Head Nurse Costa", and "Sylvia Costa" refer to the same character?

- Concept: Hierarchical Categorization
  - Why needed here: The study uses a hierarchical structure of relationship categories to reduce subjectivity during annotation and improve evaluation consistency.
  - Quick check question: Why might consolidating similar relationship categories into broader ones reduce annotator disagreement?

## Architecture Onboarding

- Component map: Narrative text -> Character Extraction -> Entity Linking -> Relation Deduction -> Relationship Graphs
- Critical path:
  1. Preprocess narrative text
  2. Extract characters from narrative
  3. Extract relationships for each character's perspective
  4. Deduce final relationships by resolving conflicts
  5. Evaluate using hierarchical relationship categories

- Design tradeoffs:
  - Accuracy vs. cost: Using human annotators provides high-quality labels but is expensive; using LLMs is cheaper but may introduce errors
  - Granularity vs. consistency: Detailed relationship categories capture nuances but may reduce inter-annotator agreement; broader categories improve consistency but may lose important distinctions
  - Single perspective vs. multiple perspectives: Single perspective narratives are easier for LLMs to process but may miss important relational dynamics; multiple perspectives provide completeness but increase complexity

- Failure signatures:
  - High corruption rate: Output format violations indicate the model is not following instructions properly
  - Low precision with high recall: Model is generating many relationships but most are incorrect (hallucinations)
  - Low recall with high precision: Model is missing many relationships but those it finds are generally correct
  - Performance drop with longer narratives: Model struggles with "lost in the middle" phenomenon

- First 3 experiments:
  1. Test character extraction performance on single character perspective vs. all characters perspective to validate the impact of narrative complexity
  2. Compare DirRelation strategy with AllTogether strategy using a reliable character list to measure the benefit of separating character and relationship extraction
  3. Evaluate performance on simple vs. complex relationships (public vs. secret vs. inferred) to quantify the model's inferential capabilities

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions beyond the general challenge of improving LLM capabilities for understanding complex relationships in narratives.

## Limitations

- Dataset Composition and Generalizability: While Conan comprises 100 detective narratives with 26,779 relationships, the dataset's coverage of relationship types may not fully represent the diversity of real-world relational dynamics.
- LLM Performance Evaluation: The study reports LLM performance across three model families but does not account for model version differences or parameter count variations within families.
- Single-Character Perspective Assumption: The paper claims that using a single character's perspective improves LLM performance, but this assumption may not hold when the chosen character is unreliable or deceptive.

## Confidence

**High Confidence** (Supported by strong evidence and minimal assumptions):
- The DirRelation strategy outperforms AllTogether when a reliable character list is available
- LLMs show decreasing performance with narrative length due to the "lost in the middle" phenomenon
- The hierarchical categorization approach reduces annotation ambiguity and improves consistency

**Medium Confidence** (Evidence is mixed or based on reasonable assumptions):
- The superiority of single-character perspective narratives for LLM processing
- The effectiveness of hierarchical relationship categories in capturing nuanced relational dynamics
- The general difficulty of LLMs in inferring secret and complex relationships

**Low Confidence** (Limited evidence or significant assumptions):
- Generalization of findings to non-detective narrative genres
- Long-term applicability as LLM architectures continue to evolve
- The impact of character reliability on the single-perspective approach's effectiveness

## Next Checks

1. **Cross-Genre Validation**: Test Conan's relationship extraction framework on non-detective narrative genres (romance, political thrillers, historical fiction) to assess generalizability beyond the detective narrative domain.

2. **Character Reliability Analysis**: Systematically evaluate how the choice of character perspective (reliable vs. unreliable narrator) affects relationship extraction accuracy across different LLM models.

3. **Long Narrative Performance**: Conduct controlled experiments with narratives of varying lengths (500-10,000 tokens) to precisely quantify the "lost in the middle" effect and identify specific thresholds where LLM performance degrades significantly.