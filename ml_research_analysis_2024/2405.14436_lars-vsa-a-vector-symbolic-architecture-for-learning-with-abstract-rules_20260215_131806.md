---
ver: rpa2
title: 'LARS-VSA: A Vector Symbolic Architecture For Learning with Abstract Rules'
arxiv_id: '2405.14436'
source_url: https://arxiv.org/abs/2405.14436
tags: []
core_contribution: This paper proposes LARS-VSA, a neuro-symbolic architecture that
  addresses the "relational bottleneck" problem in abstract reasoning by leveraging
  hyperdimensional computing. The key idea is to use high-dimensional vector symbolic
  representations to decouple object-level features from abstract rules, reducing
  interference between them.
---

# LARS-VSA: A Vector Symbolic Architecture For Learning with Abstract Rules

## Quick Facts
- arXiv ID: 2405.14436
- Source URL: https://arxiv.org/abs/2405.14436
- Authors: Mohamed Mejri; Chandramouli Amarnath; Abhijit Chatterjee
- Reference count: 34
- One-line result: LARS-VSA achieves significantly higher accuracy than state-of-the-art methods on abstract reasoning tasks while being up to 17x more memory efficient and 25x faster in attention operations

## Executive Summary
LARS-VSA introduces a neuro-symbolic architecture that addresses the relational bottleneck problem in abstract reasoning by leveraging hyperdimensional computing. The key innovation is a high-dimensional attention mechanism that models relationships indirectly through context-based correlation rather than direct inner product similarity, avoiding orthogonality issues in high-dimensional spaces. The architecture achieves superior accuracy on abstract reasoning tasks while demonstrating substantial computational efficiency gains through binarized attention operations and explicit binding between object and symbolic representations.

## Method Summary
LARS-VSA implements a relational bottleneck strategy adapted to high-dimensional space, using explicit vector binding operations between object hypervectors and symbolic representations. The architecture employs a novel high-dimensional attention mechanism that creates a local context through bundling operations and measures correlation between objects and this context using cosine similarity. Binarized attention scores provide computational efficiency through simplified binary operations. The system is evaluated on discriminative relational tasks, object-sorting, and mathematical problem-solving, showing significant improvements over transformer-based and other state-of-the-art methods.

## Key Results
- Achieves up to 17x memory savings and 9x faster attention operations compared to transformer-based models
- Demonstrates 25x computational efficiency improvement through binarized attention mechanisms
- Shows strong generalization capabilities from limited training data across discriminative relational tasks, object-sorting, and mathematical problem-solving

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LARS-VSA reduces interference between object-level features and abstract rules by modeling relationships indirectly in high-dimensional space rather than through direct inner product similarity.
- Mechanism: Uses bundling operation to combine object hypervectors into local context, then measures correlation between individual objects and this context using cosine similarity.
- Core assumption: Direct inner product similarity becomes uninformative in high-dimensional spaces due to vector orthogonality.
- Evidence anchors: [abstract] "LARS-VSA introduces a novel high-dimensional attention mechanism that models relationships between objects indirectly by measuring their correlation to a local context"; [section] "As the dimensionality of hypervectors increases, they become orthogonal, making a relational modeling approach such as [1] difficult to apply in the hyperdimensional space"
- Break condition: If bundling operation fails to capture dominant features, indirect correlation measurement loses discriminative power.

### Mechanism 2
- Claim: Binarized attention scores provide significant computational efficiency gains while maintaining accuracy.
- Mechanism: Attention scores computed using binary operations (AND, L0 norm) on binarized bipolar hypervectors, replacing expensive real-valued dot products.
- Core assumption: Binarization allows up to 2x memory savings and 60% speedup with low accuracy loss.
- Evidence anchors: [abstract] "Our system significantly reduces computational costs by simplifying attention score matrix multiplication to binary operations"; [section] "Lemma 1 and the notation in Figure 4 allow us to simplify the function r12 = f(hO1, hO2) using binary operations according to Equation 4"
- Break condition: If binarization introduces too much quantization error, accuracy degrades despite computational benefits.

### Mechanism 3
- Claim: Explicit binding between object hypervectors and symbolic hypervectors decouples object-level features from abstract rules.
- Mechanism: Performs explicit vector binding operations between high-dimensional object representations and symbolic representations, creating abstract relational representations distinct from object features.
- Core assumption: Relational bottleneck problem can be addressed by separating object-level features from abstract rules through explicit binding in high-dimensional space.
- Evidence anchors: [abstract] "We adapt the 'relational bottleneck' strategy to a high-dimensional space, incorporating explicit vector binding operations between symbols and relational representations"; [section] "Our proposed system shown in Figure 2 aims to resolve the relational bottleneck problem in a high-dimensional space using an explicit binding mechanism"
- Break condition: If binding operation fails to maintain orthogonality between object and symbolic representations, interference persists.

## Foundational Learning

- **Concept: Hyperdimensional Computing (HDC)**
  - Why needed here: LARS-VSA relies on HDC properties like robustness to interference and efficient binding operations
  - Quick check question: What property of high-dimensional vectors makes them inherently robust to interference?

- **Concept: Relational Bottleneck**
  - Why needed here: Architecture addresses catastrophic interference between object-level and abstract-level features
  - Quick check question: What is the curse of compositionality and how does it relate to the relational bottleneck problem?

- **Concept: Vector Symbolic Architectures (VSA)**
  - Why needed here: LARS-VSA uses VSA principles for representing symbolic knowledge in distributed vector form
  - Quick check question: How do binding and bundling operations differ in VSA frameworks?

## Architecture Onboarding

- **Component map**: Input objects → Learnable projection (ϕB) → Object hypervectors → HDSymbolicAttention → Encoded hypervectors → ⊗ Symbolic hypervectors → Abstract hypervectors → Decoder/classifier

- **Critical path**: Object representation → High-dimensional projection → Context-based attention → Binding with symbols → Abstract representation → Task-specific output

- **Design tradeoffs**: Higher dimensional hypervectors provide better interference resistance but increase memory usage; binarized attention improves speed but may lose precision; explicit binding reduces interference but adds computational overhead

- **Failure signatures**: Accuracy drops with sequence length indicate attention score matrices becoming less informative; poor generalization from limited samples suggests binding operation isn't capturing relevant features; memory constraints may arise from high-dimensional hypervectors

- **First 3 experiments**:
  1. Implement HDSymbolicAttention on simple synthetic dataset to verify context-based correlation works
  2. Test binarized attention vs full precision attention on small dataset to measure accuracy/speed tradeoff
  3. Validate binding operation by checking if object and symbolic representations remain orthogonal after binding

## Open Questions the Paper Calls Out
- How does the temporal interference problem in LARS-VSA manifest in online abstract reasoning tasks, and what are the specific mechanisms that could be implemented to mitigate catastrophic forgetting?
- How does the performance of LARS-VSA compare to state-of-the-art methods on more complex and diverse abstract reasoning tasks, such as those involving visual reasoning or natural language understanding?
- What is the impact of different hyperdimensional vector dimensions on the performance and computational efficiency of LARS-VSA, and is there an optimal dimension that balances these two factors?

## Limitations
- Lack of empirical validation on real-world datasets beyond synthetic and benchmark abstract reasoning tasks
- Claims about computational efficiency gains may not generalize across all transformer architecture variants
- Does not address scalability issues with very large hyperdimensional vectors or impact of noise on high-dimensional attention mechanism

## Confidence
- **High confidence**: Core architectural design using high-dimensional attention and explicit binding operations is technically sound and addresses well-documented problem in abstract reasoning
- **Medium confidence**: Generalization capabilities from limited training data demonstrated on benchmarks but require validation on more diverse scenarios
- **Low confidence**: Paper does not adequately address how binarization process affects robustness to adversarial examples or noisy inputs

## Next Checks
1. Test LARS-VSA on large-scale visual reasoning dataset (e.g., PGM or V-PROM) with varying sequence lengths to verify high-dimensional attention maintains performance as task complexity increases
2. Evaluate model performance under controlled noise injection and adversarial perturbations to assess claimed robustness benefits of high-dimensional vector representations
3. Apply architecture to non-visual abstract reasoning tasks (e.g., text-based logical reasoning) to validate whether relational bottleneck strategy generalizes beyond visual domain