---
ver: rpa2
title: 'When Foresight Pruning Meets Zeroth-Order Optimization: Efficient Federated
  Learning for Low-Memory Devices'
arxiv_id: '2405.04765'
source_url: https://arxiv.org/abs/2405.04765
tags: []
core_contribution: This paper addresses the challenge of deploying federated learning
  on low-memory AIoT devices by proposing a federated foresight pruning method based
  on Neural Tangent Kernel (NTK) that integrates with BP-Free training frameworks.
  The method approximates federated NTK computation using local NTK matrices and demonstrates
  that its data-free property reduces approximation error in extreme data heterogeneity
  scenarios.
---

# When Foresight Pruning Meets Zeroth-Order Optimization: Efficient Federated Learning for Low-Memory Devices

## Quick Facts
- arXiv ID: 2405.04765
- Source URL: https://arxiv.org/abs/2405.04765
- Reference count: 40
- Primary result: Data-free federated NTK pruning integrated with BP-Free training reduces memory usage up to 9x while preserving dense model performance

## Executive Summary
This paper addresses the challenge of deploying federated learning on low-memory AIoT devices by proposing a federated foresight pruning method based on Neural Tangent Kernel (NTK) that integrates with BP-Free training frameworks. The method approximates federated NTK computation using local NTK matrices and demonstrates that its data-free property reduces approximation error in extreme data heterogeneity scenarios. By combining federated pruning with BP-Free training, the approach alleviates memory pressure during both training and inference while maintaining competitive accuracy.

## Method Summary
The method combines federated NTK-based foresight pruning with BP-Free federated learning using Stein's Identity for gradient estimation. Devices compute local NTK matrices on Gaussian-sampled data, transmit pruning masks to the server, and then perform zeroth-order gradient estimation with K Monte Carlo samples. The Random Seed Trick enables communication-efficient gradient aggregation by transmitting scalar seeds instead of high-dimensional perturbation vectors. The approach operates in three phases: NTK foresight pruning (data-free), BP-Free training with Stein's Identity, and sparse parameter updates with mask application.

## Key Results
- Achieves up to 9x reduction in memory usage while preserving dense model performance
- Outperforms vanilla BAFFLE with dramatically fewer FLOPs on CIFAR-10 and CIFAR-100
- Demonstrates data-free property substantially reduces approximation error in extreme data heterogeneity scenarios
- Maintains competitive accuracy with 80% sparsity on LeNet-5 and ResNet-20 architectures

## Why This Works (Mechanism)

### Mechanism 1
The federated NTK pruning method achieves data-free approximation by sampling from a standard Gaussian distribution instead of using real local data. By constraining the difference between local data distributions to near zero through Gaussian sampling, the approximation error in Eq. 12 is minimized, making the pruning step independent of heterogeneous local datasets.

### Mechanism 2
Integrating NTK-based foresight pruning with BP-Free training reduces the dimensionality of the gradient estimation problem. Pruning reduces the number of parameters ð‘› in Stein's Identity estimation, which directly lowers the required number of Monte Carlo samples ð¾ for accurate gradient estimation.

### Mechanism 3
The Random Seed Trick enables communication-efficient gradient aggregation in BP-Free FL. Devices compute Î”L(ð‘¾ ð‘– ; Dð‘– ) and transmit only scalar random seeds instead of high-dimensional perturbation vectors ðœ¹, allowing the server to reconstruct the same perturbations for aggregation.

## Foundational Learning

- **Neural Tangent Kernel (NTK)**: Provides theoretically grounded way to measure parameter importance for pruning without backpropagation, enabling foresight pruning compatible with BP-Free training.
  - Quick check: How does the NTK spectrum relate to the training dynamics of a neural network at initialization?

- **Zeroth-Order Optimization**: Enables gradient estimation without backpropagation, critical for memory-constrained devices.
  - Quick check: What is the relationship between estimation error in Stein's Identity and the dimensionality of the parameter space?

- **Federated Learning Heterogeneity**: Understanding how data heterogeneity affects federated training is crucial for designing methods that remain effective across diverse local datasets.
  - Quick check: How does Dirichlet distribution parameterization (Î²) affect the degree of non-IID data in federated learning?

## Architecture Onboarding

- **Component map**: Devices (local NTK computation, loss gradient, random seed) -> Server (federated NTK approximation, mask threshold, gradient aggregation) -> Shared (model parameters, pruning mask)
- **Critical path**: NTK foresight pruning â†’ BP-Free training with Stein's Identity â†’ Random Seed Trick â†’ Sparse parameter updates
- **Design tradeoffs**: Memory vs Accuracy (higher sparsity reduces memory but may harm accuracy), Communication vs Computation (Random Seed Trick reduces upload cost but requires server-side perturbation generation), Approximation Error vs Data Privacy (data-free pruning improves privacy but relies on Gaussian approximation quality)
- **Failure signatures**: Accuracy degradation (pruning removes critical parameters), Convergence failure (gradient estimation error too high), Communication breakdown (random seed mismatch)
- **First 3 experiments**: 1) Compare NTK pruning with real vs Gaussian data on CIFAR-10 to validate data-free approximation, 2) Measure gradient estimation error vs sparsity level to find optimal pruning ratio, 3) Test Random Seed Trick communication savings vs full perturbation transmission

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the approximation error scale with the number of devices and data heterogeneity in federated NTK pruning?
- **Open Question 2**: What is the optimal balance between the number of Monte Carlo steps (K) and model sparsity for achieving the best trade-off between accuracy and computational efficiency?
- **Open Question 3**: How does the proposed federated foresight pruning method perform on different neural network architectures beyond LeNet-5 and ResNet-20?

## Limitations

- The data-free approximation's effectiveness depends on Gaussian sampling assumptions that may not hold across all architectures
- Scalability to deeper networks is uncertain due to computational complexity of local NTK matrix construction
- Performance may vary significantly across different model families beyond the tested architectures

## Confidence

- **High Confidence**: Integration of NTK-based pruning with BP-Free training methodology and resulting FLOPs reduction (9x)
- **Medium Confidence**: Data-free approximation's effectiveness in extreme heterogeneity scenarios and Random Seed Trick's communication efficiency gains
- **Low Confidence**: Claim about near-linear scaling of gradient estimation error with parameter count reduction

## Next Checks

1. **Distribution Sensitivity Test**: Systematically vary Gaussian sampling parameters and measure their impact on final model accuracy across different network architectures
2. **Cross-Platform Random Seed Verification**: Implement Random Seed Trick across heterogeneous hardware platforms to verify consistent gradient reconstruction
3. **Scalability Analysis**: Extend NTK-based pruning to deeper architectures (ResNet-50, MobileNet) and measure computational overhead vs benefits tradeoff