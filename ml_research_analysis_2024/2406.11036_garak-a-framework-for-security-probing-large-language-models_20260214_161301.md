---
ver: rpa2
title: 'garak: A Framework for Security Probing Large Language Models'
arxiv_id: '2406.11036'
source_url: https://arxiv.org/abs/2406.11036
tags:
- garak
- security
- language
- arxiv
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "garak is a Python framework for discovering vulnerabilities in\
  \ large language models (LLMs) through structured probing. It automates red-teaming\
  \ by applying a wide variety of adversarial prompts\u2014such as jailbreaks, prompt\
  \ injections, encoding attacks, and toxicity elicitation\u2014and then uses detectors\
  \ to classify failures."
---

# garak: A Framework for Security Probing Large Language Models

## Quick Facts
- arXiv ID: 2406.11036
- Source URL: https://arxiv.org/abs/2406.11036
- Reference count: 29
- Primary result: Python framework automating red-teaming of LLMs through structured probing with OWASP-based taxonomy

## Executive Summary
garak is a comprehensive Python framework designed to systematically discover vulnerabilities in large language models through automated adversarial testing. The framework implements a wide array of security probes including jailbreaks, prompt injections, encoding attacks, and toxicity elicitation, organized according to security taxonomy. It features an adaptive attack generation module that fine-tunes small models on successful attacks to produce novel test cases. The system logs and categorizes results for contribution to the AI Vulnerability Database, demonstrating high effectiveness in exposing model weaknesses across multiple evaluated LLMs.

## Method Summary
The framework operates through a modular architecture where security probes are applied systematically to target LLMs, with responses classified by automated detectors. Probes are organized by security taxonomy (OWASP Top 10) and can be easily extended. The adaptive attack generation component fine-tunes small language models on previously successful attacks to generate new test cases, creating an iterative improvement loop. Results are logged, categorized, and can be contributed to vulnerability databases for broader security analysis.

## Key Results
- High success rates in eliciting toxic or misleading outputs from multiple evaluated LLMs
- Effective categorization of vulnerabilities using OWASP-based security taxonomy
- Modular design allows easy extension and customization of security probes
- Adaptive attack generation produces novel test cases beyond initial probe set

## Why This Works (Mechanism)
The framework succeeds by combining systematic adversarial testing with automated classification and adaptive generation. The OWASP-based taxonomy ensures comprehensive coverage of security domains, while automated detectors provide scalable analysis of model responses. The adaptive attack generation creates a feedback loop where successful attack patterns are learned and expanded upon, increasing the framework's ability to discover novel vulnerabilities over time.

## Foundational Learning
- **Security taxonomies (OWASP)**: Needed to systematically organize and prioritize security testing across common vulnerability types; quick check: verify probes map to relevant OWASP categories
- **Adversarial prompt engineering**: Required to craft effective attack vectors that bypass model safeguards; quick check: test prompts against baseline models to confirm attack viability
- **Automated response classification**: Essential for scaling vulnerability detection beyond manual review; quick check: validate detector accuracy against human-labeled samples
- **Fine-tuning small models for attack generation**: Enables adaptive improvement of testing capability; quick check: measure generated attack novelty versus known patterns
- **Vulnerability database integration**: Provides standardized reporting and knowledge sharing; quick check: confirm successful submission format to AVD
- **Modular framework design**: Allows extensibility and customization for specific security needs; quick check: implement and test a new probe type

## Architecture Onboarding

**Component Map**
garak -> Probes -> Detectors -> Logger -> Adaptive Generator -> (back to Probes)

**Critical Path**
Probe execution → Response generation → Detection classification → Result logging → Attack generation training → New probe creation

**Design Tradeoffs**
- Automated detection vs. human verification accuracy
- Comprehensive probe coverage vs. execution speed
- Adaptive generation sophistication vs. computational cost
- Standardized taxonomy vs. model-specific vulnerability patterns

**Failure Signatures**
- High false positive rates in automated detectors
- Adaptive generator producing non-novel or ineffective attacks
- Performance degradation across different model architectures
- Incomplete coverage of emerging security threat categories

**3 First Experiments**
1. Run baseline OWASP Top 10 probe suite against a standard commercial LLM
2. Test adaptive attack generation by comparing novelty of generated attacks versus initial probe set
3. Evaluate detector accuracy by having human reviewers validate automated classifications on sample responses

## Open Questions the Paper Calls Out
None

## Limitations
- Detector accuracy may introduce false positives/negatives affecting reported success rates
- Adaptive attack generation effectiveness depends on initial attack diversity and fine-tuning quality
- Limited evaluation across diverse model architectures and fine-tuning approaches

## Confidence

**High Confidence**
- Core framework architecture and OWASP-based probe categorization
- Modular design allowing easy extension of security probes

**Medium Confidence**
- Reported effectiveness metrics for specific tested models
- General framework usability and implementation quality

**Low Confidence**
- Long-term effectiveness of adaptive attack generation in discovering novel vulnerabilities
- Generalizability of results across all LLM deployments

## Next Checks
1. Conduct independent replication across 10+ diverse LLM models to assess generalizability of success rates
2. Perform systematic analysis of detector false positive/negative rates using human evaluators on statistically significant samples
3. Evaluate adaptive attack generation's ability to discover genuinely novel vulnerabilities through benchmark comparison and time-series analysis