---
ver: rpa2
title: 'A Shocking Amount of the Web is Machine Translated: Insights from Multi-Way
  Parallelism'
arxiv_id: '2401.05749'
source_url: https://arxiv.org/abs/2401.05749
tags:
- translation
- multi-way
- languages
- machine
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reveals that a substantial portion of web content in
  low-resource languages is machine-translated, primarily from low-quality English
  sources. The authors create MWccMatrix, the largest multi-way parallel corpus to
  date (6.4B sentences in 90 languages), and use quality estimation to show that highly
  multi-way parallel translations (3+ languages) are of significantly lower quality
  than 2-way parallel translations, suggesting they are predominantly machine-generated.
---

# A Shocking Amount of the Web is Machine Translated: Insights from Multi-Way Parallelism

## Quick Facts
- arXiv ID: 2401.05749
- Source URL: https://arxiv.org/abs/2401.05749
- Reference count: 40
- Primary result: A substantial portion of web content in low-resource languages is machine-translated from low-quality English sources

## Executive Summary
This paper reveals that a substantial portion of web content in low-resource languages is machine-translated, primarily from low-quality English sources. The authors create MWccMatrix, the largest multi-way parallel corpus to date (6.4B sentences in 90 languages), and use quality estimation to show that highly multi-way parallel translations (3+ languages) are of significantly lower quality than 2-way parallel translations, suggesting they are predominantly machine-generated. They also find evidence of selection bias, with shorter, more predictable content (often from the "Conversation/Opinion" topic) being overrepresented in multi-way parallel data. This indicates that low-quality English content is being mass-translated into many languages, likely for ad revenue. The findings raise concerns about using web-scraped data for training multilingual models, as it may contain less fluent and less accurate content, potentially leading to more hallucinations in generated text.

## Method Summary
The authors created MWccMatrix, a multi-way parallel corpus derived from Common Crawl data, containing 6.4 billion unique sentences across 90 languages. They deduplicated and expanded translation pairs from the existing ccMatrix corpus, then analyzed the resulting data for multi-way parallelism, translation quality, and topical bias. Quality estimation was performed using CometQE scores, and topic classification was done using human annotators. The analysis compared translation quality across different levels of parallelism, examined sentence characteristics like length and perplexity, and investigated topic distribution patterns.

## Key Results
- 37.5% of translation tuples are multi-way parallel, but 57.1% of all sentences come from multi-way parallel tuples
- Highly multi-way parallel translations (3+ languages) are 6.2 CometQE points worse than 2-way parallel translations
- Multi-way parallel data shows strong bias toward shorter, simpler content, particularly in the "Conversation/Opinion" topic (40.1% vs 22.5% in general data)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Machine Translation dominates multi-way parallel web content in low-resource languages
- Mechanism: Sentences translated into many languages (>3) have significantly lower quality scores than 2-way translations, indicating automated generation rather than human translation
- Core assumption: CometQE quality scores reliably distinguish human from machine translations
- Evidence anchors:
  - [abstract] "the low quality of these multi-way translations indicates they were likely created using Machine Translation (MT)"
  - [section] "We find that highly multi-way parallel translations are significantly lower quality (6.2 CometQE points worse) than 2-way parallel translations"
  - [corpus] "37.5% of tuples are multi-way parallel, but 57.1% of all sentences come from multi-way parallel tuples"
- Break condition: If CometQE is not reliable for distinguishing MT from human translations in web data

### Mechanism 2
- Claim: Multi-way parallel content is biased toward shorter, more predictable content
- Mechanism: MT systems preferentially translate shorter, simpler content because it's easier to generate and has higher potential ad revenue per unit effort
- Core assumption: Content selection for translation is driven by economic incentives
- Evidence anchors:
  - [abstract] "we find evidence of a selection bias in the type of content which is translated into many languages, consistent with low quality English content being translated en masse into many lower resource languages"
  - [section] "Multi-way parallel data is shorter and simpler" with "Sentence length (in characters) as a function of multi-way parallelism"
  - [corpus] "CONVERSATION & OPINION topic increasing from 22.5% to 40.1%" in multi-way parallel data
- Break condition: If content selection is driven by factors other than economic efficiency

### Mechanism 3
- Claim: LASER margin scores indicate MT content due to bias toward machine-generated translations
- Mechanism: LASER similarity metric inherently prefers MT output over human translations, causing higher margin scores for MT-generated multi-way parallel content
- Core assumption: LASER's preference for MT is systematic and not random noise
- Evidence anchors:
  - [abstract] "LASER margin scores for more multi-way parallel content are consistent with multi-way parallel data being MT"
  - [section] "LASER has a strong bias for MT output over human translations (about 2.8% on average)"
  - [corpus] "Multi-way parallel data tends to have higher margin scores"
- Break condition: If LASER's bias is not systematic or if the bias doesn't correlate with MT generation

## Foundational Learning

- Concept: Quality Estimation (QE) metrics
  - Why needed here: The paper uses CometQE to assess translation quality without human references
  - Quick check question: What is the key advantage of QE over reference-based metrics in this study?

- Concept: Multi-way parallelism
  - Why needed here: Understanding how translation tuples of different sizes relate to translation method
  - Quick check question: Why would content translated into 8+ languages be more likely to be machine-generated than content translated into 2 languages?

- Concept: Web scraping biases
  - Why needed here: The analysis depends on Common Crawl data, which has known sampling biases
  - Quick check question: How might Common Crawl's link-following approach bias toward finding translated content?

## Architecture Onboarding

- Component map: Corpus creation -> Quality estimation -> Topic analysis -> Language-specific analysis
- Critical path: MWccMatrix creation -> QE evaluation -> Topic classification -> Interpretation
- Design tradeoffs: Sentence-level vs document-level analysis (sentence-level is tractable but loses context)
- Failure signatures: 
  - Low correlation between QE scores and actual quality
  - Unexpected topic distributions
  - Inconsistent parallelism patterns across languages
- First 3 experiments:
  1. Verify CometQE scores on known MT vs human translations to establish baseline discrimination
  2. Sample random sentences from high-parallelism tuples to manually verify MT detection
  3. Compare topic distributions in high-resource vs low-resource languages to validate selection bias findings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How much of the web content in low-resource languages is generated through machine translation compared to native content?
- Basis in paper: [explicit] The paper states that a substantial portion of web content in low-resource languages is machine-translated, primarily from low-quality English sources.
- Why unresolved: The paper provides estimates but does not give an exact percentage of MT-generated content versus native content in low-resource languages.
- What evidence would resolve it: A comprehensive study comparing the volume of native versus machine-translated content in low-resource languages across different domains and time periods would provide a clearer picture.

### Open Question 2
- Question: What are the long-term effects of using machine-translated content on the quality of multilingual large language models?
- Basis in paper: [explicit] The paper raises concerns about training multilingual models on web-scraped data, which may contain machine-translated content of lower quality.
- Why unresolved: The paper does not provide empirical evidence on how MT content affects the performance of multilingual models in real-world applications.
- What evidence would resolve it: Longitudinal studies comparing the performance of multilingual models trained on datasets with varying levels of MT content would help understand the impact on model quality.

### Open Question 3
- Question: How can the quality of machine-translated content on the web be improved to enhance the training data for multilingual models?
- Basis in paper: [inferred] The paper suggests that MT content is of lower quality and proposes filtering MT data as a potential solution.
- Why unresolved: The paper does not explore specific methods or technologies to improve the quality of MT content on the web.
- What evidence would resolve it: Research into advanced MT techniques, quality control measures, and post-editing processes for web content could provide insights into improving the quality of MT-generated text.

## Limitations
- The analysis relies on CometQE scores which haven't been validated against human judgments specifically for web-scraped data
- The study cannot definitively prove the economic motivation (ad revenue) behind MT content generation
- Topic distribution analysis doesn't account for potential confounding factors like original source languages

## Confidence

High: The methodology for creating MWccMatrix and the basic statistical findings about multi-way parallelism
Medium: The interpretation of quality scores as evidence of MT generation, the selection bias hypothesis
Low: The specific claim about ad revenue motivation, the exact percentage of web content that is MT-generated

## Next Checks

1. Conduct human evaluation on a stratified sample of high-parallelism tuples to verify CometQE's accuracy in detecting MT content on web data
2. Compare MWccMatrix topic distributions against professionally translated corpora to isolate MT-specific patterns
3. Analyze the correlation between detected MT content and known ad-heavy domains to validate the revenue hypothesis