---
ver: rpa2
title: 'Attention Heads of Large Language Models: A Survey'
arxiv_id: '2409.03752'
source_url: https://arxiv.org/abs/2409.03752
tags:
- heads
- attention
- head
- arxiv
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This survey provides a systematic framework for understanding\
  \ the internal reasoning mechanisms of large language models (LLMs) by focusing\
  \ on the roles and mechanisms of attention heads. Inspired by human cognitive processes,\
  \ the authors propose a four-stage framework\u2014Knowledge Recalling, In-Context\
  \ Identification, Latent Reasoning, and Expression Preparation\u2014to categorize\
  \ and analyze the functions of attention heads."
---

# Attention Heads of Large Language Models: A Survey

## Quick Facts
- arXiv ID: 2409.03752
- Source URL: https://arxiv.org/abs/2409.03752
- Reference count: 40
- One-line primary result: Proposes a four-stage framework to categorize attention heads in LLMs based on human cognitive reasoning processes.

## Executive Summary
This survey provides a systematic framework for understanding the internal reasoning mechanisms of large language models (LLMs) by focusing on the roles and mechanisms of attention heads. Inspired by human cognitive processes, the authors propose a four-stage framework—Knowledge Recalling, In-Context Identification, Latent Reasoning, and Expression Preparation—to categorize and analyze the functions of attention heads. The paper reviews existing research to identify specific attention heads associated with each stage and explores how these heads collaborate to achieve human-like reasoning. Additionally, it summarizes experimental methodologies for discovering attention heads, dividing them into Modeling-Free and Modeling-Required methods, and discusses evaluation benchmarks for assessing their mechanisms. The survey highlights limitations in current research, such as the lack of task generalizability and mechanism transferability, and proposes future directions, including exploring mechanisms in complex tasks and integrating insights from machine psychology. This work provides a comprehensive foundation for advancing the interpretability of LLMs.

## Method Summary
The paper introduces a four-stage framework inspired by human cognitive processes to categorize attention heads in LLMs: Knowledge Recalling, In-Context Identification, Latent Reasoning, and Expression Preparation. It systematically reviews existing research to identify specific attention heads associated with each stage and explores their collaborative mechanisms. The survey also summarizes experimental methodologies for discovering attention heads, dividing them into Modeling-Free and Modeling-Required methods, and discusses evaluation benchmarks for assessing their mechanisms. The approach involves analyzing the functions of attention heads, their layer-wise distribution, and their contributions to reasoning processes in LLMs.

## Key Results
- Proposes a novel four-stage framework (Knowledge Recalling, In-Context Identification, Latent Reasoning, Expression Preparation) to categorize attention heads based on human cognitive reasoning processes.
- Reviews existing research to identify specific attention heads associated with each stage and explores their collaborative mechanisms in achieving human-like reasoning.
- Summarizes experimental methodologies for discovering attention heads, dividing them into Modeling-Free and Modeling-Required methods, and discusses evaluation benchmarks for assessing their mechanisms.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attention heads in LLMs are organized into functional clusters that align with human cognitive stages: Knowledge Recalling (KR), In-Context Identification (ICI), Latent Reasoning (LR), and Expression Preparation (EP).
- Mechanism: The paper proposes that attention heads specialize in distinct reasoning phases. KR heads retrieve parametric knowledge, ICI heads parse contextual structure, LR heads perform implicit reasoning, and EP heads prepare fluent output. This mirrors human problem-solving by dividing reasoning into retrieval, comprehension, computation, and articulation.
- Core assumption: Attention heads can be functionally grouped based on layer depth and attention patterns, and these groups reliably correspond to stages of reasoning.
- Evidence anchors:
  - [abstract] "We first introduce a novel four-stage framework inspired by the human thought process: Knowledge Recalling, In-Context Identification, Latent Reasoning, and Expression Preparation."
  - [section] "Based on this abstraction, Wang and Chiew proposed a mathematical model of problem solving... the solver’s brain first utilizes its own OAR model to identify the content of the problem... combines their knowledge to search for potential solution goals and solution paths."
  - [corpus] Weak evidence: Most cited works focus on single-head functions rather than cross-stage collaboration; no explicit cross-model validation provided.
- Break condition: If attention head functions are task-specific and do not generalize across reasoning stages, the four-stage framework collapses into ad hoc groupings.

### Mechanism 2
- Claim: Experimental interventions (Modification-Based and Replacement-Based) can isolate the contribution of individual attention heads by observing changes in model output.
- Mechanism: Zero ablation removes a head entirely, mean ablation replaces it with average behavior, and activation patching substitutes activations from a corrupted prompt. These changes produce measurable logit shifts that indicate the head's functional role.
- Core assumption: The model's reasoning is additive in nature; removing or replacing a head's contribution changes the output in a predictable way.
- Evidence anchors:
  - [abstract] "we analyze the experimental methodologies used to discover these special heads, dividing them into two categories: Modeling-Free and Modeling-Required methods."
  - [section] "Zero Ablation and Mean Ablation replace the original latent state with zero values or the mean value of latent states across all samples from a dataset... Naive Activation Patching is the traditional patching method."
  - [corpus] Moderate evidence: Patching methods are standard in mechanistic interpretability, but correlation between ablation effects and true functional importance is not always established.
- Break condition: If heads exhibit redundancy or emergent behavior when combined, single-head ablation may underestimate or misrepresent their true role.

### Mechanism 3
- Claim: Attention heads collaborate across layers to form reasoning circuits, with earlier layers performing retrieval and identification, middle layers performing reasoning, and later layers preparing output.
- Mechanism: Earlier layers contain KR and ICI heads; middle layers contain LR heads; deeper layers contain EP heads. Collaboration occurs via residual streams, where information is progressively refined and passed upward.
- Core assumption: The layer-wise distribution of head types is consistent across architectures and tasks.
- Evidence anchors:
  - [abstract] "We first introduce a novel four-stage framework inspired by the human thought process... Using this framework, we comprehensively review existing research to identify and categorize the functions of specific attention heads."
  - [section] "As illustrated in Figure 8, if we divide the layers of a LLM... we can map the relationship between the stages where heads act and the layers they are in."
  - [corpus] Weak evidence: The paper notes lack of mechanism transferability and task generalizability; collaboration patterns observed in IOI and MCQA may not hold for more complex tasks.
- Break condition: If head functions shift depending on task complexity or model architecture, the layer-based stage mapping becomes unreliable.

## Foundational Learning

- Concept: Transformer attention mechanism (query-key-value, softmax weighting, residual connections)
  - Why needed here: All proposed mechanisms depend on understanding how attention heads read, write, and transform information via QK and OV matrices.
  - Quick check question: What is the difference between the QK matrix and the OV matrix in an attention head?

- Concept: Mechanistic interpretability methods (activation patching, ablation, logit lens)
  - Why needed here: The paper’s framework is validated using these experimental methods; understanding their assumptions is critical.
  - Quick check question: How does zero ablation differ from mean ablation in terms of information preservation?

- Concept: Residual stream as shared information bandwidth
  - Why needed here: The paper models reasoning as information flow through residual streams; heads read from and write to this shared space.
  - Quick check question: What is the role of residual connections in enabling multi-head collaboration?

## Architecture Onboarding

- Component map: Embedding layer -> Positional encoding -> L transformer layers (each: Multi-Head Attention + FFN) -> Unembedding layer
- Critical path: Input tokens -> Embedding -> Layer 1-4 (KR/ICI) -> Layer 5-8 (LR) -> Layer 9-12 (EP) -> Unembedding -> Output logits
- Design tradeoffs:
  - Depth vs specialization: Deeper layers enable complex reasoning but may obscure individual head contributions
  - Multi-head redundancy: Multiple heads can learn similar functions, complicating isolation experiments
  - Residual connections: Enable gradient flow and information reuse but blur head-level boundaries
- Failure signatures:
  - Inconsistent ablation effects across prompts -> Redundant or task-specific heads
  - Loss spikes when removing heads in mid-layers -> Critical reasoning components
  - Logit lens shows no change after ablation -> Head is non-functional or redundant
- First 3 experiments:
  1. Zero-ablating a known Subject Head in a sentiment analysis task and observing logit lens changes
  2. Activation patching a Mover Head in IOI task to verify name movement function
  3. Measuring Retrieval Score for a head in a long-context retrieval task to confirm "needle-in-a-haystack" ability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed four-stage framework (Knowledge Recalling, In-Context Identification, Latent Reasoning, Expression Preparation) be universally applied to all types of tasks performed by large language models?
- Basis in paper: [explicit] The paper introduces this framework inspired by human cognitive processes and applies it to categorize attention head functions, but notes limitations in current research.
- Why unresolved: The paper primarily explores simple application scenarios and acknowledges a lack of task generalizability in existing research.
- What evidence would resolve it: Testing the framework's applicability across a diverse range of complex tasks (e.g., open-ended question answering, math problems, tool-using tasks) and documenting its effectiveness or limitations.

### Open Question 2
- Question: To what extent are the mechanisms of attention heads transferable across different large language model architectures?
- Basis in paper: [explicit] The paper highlights a lack of mechanism transferability in existing research, noting that many discovered heads have only been explored within specific LLMs or toy models.
- Why unresolved: Current research has not investigated whether specialized heads identified in one LLM exhibit the same functionality in another LLM.
- What evidence would resolve it: Conducting comparative studies across multiple LLM architectures (e.g., GPT, Llama, Mistral) to identify consistent or divergent mechanisms of attention heads.

### Open Question 3
- Question: How can the collaborative mechanisms of multiple attention heads be systematically understood and modeled?
- Basis in paper: [explicit] The paper mentions that most studies investigate individual attention heads, with limited focus on multi-head collaboration, and calls for a comprehensive framework for understanding coordinated functioning.
- Why unresolved: Existing work lacks a unified framework for understanding how all attention heads in LLMs work together.
- What evidence would resolve it: Developing a formal model or framework that captures the interactions and dependencies among attention heads, validated through empirical experiments.

## Limitations

- The four-stage framework lacks strong empirical validation across diverse tasks and model architectures, with most evidence derived from IOI and MCQA tasks.
- Claims about consistent layer-wise distribution of head types across architectures are not well-supported, as the paper notes this as a limitation and provides limited cross-model evidence.
- The assumption that attention heads reliably correspond to cognitive stages may not hold for more complex reasoning tasks, as current research focuses on simple application scenarios.

## Confidence

- **High Confidence:** The descriptive mapping of existing research on attention heads and experimental methodologies. The survey accurately captures current literature and methodologies in mechanistic interpretability.
- **Medium Confidence:** The proposed four-stage framework's conceptual alignment with human cognition. While theoretically grounded, empirical validation across diverse tasks is limited.
- **Low Confidence:** Claims about consistent layer-wise distribution of head types across architectures. The paper notes this as a limitation and provides limited cross-model evidence.

## Next Checks

1. Conduct ablation studies across multiple reasoning tasks (e.g., mathematical problem-solving, logical inference, and multi-hop reasoning) to test the generalizability of the four-stage framework and identify task-specific variations in head functions.
2. Perform cross-model analysis comparing attention head distributions and functions in models of varying sizes (e.g., 8B vs. 70B parameters) and architectures (e.g., decoder-only vs. encoder-decoder) to validate layer-wise stage mapping assumptions.
3. Design experiments to measure the information flow and collaboration patterns between attention heads across layers using techniques like activation patching and path attribution methods, focusing on quantifying the contribution of cross-stage interactions to final model outputs.