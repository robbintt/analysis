---
ver: rpa2
title: 'SpoT-Mamba: Learning Long-Range Dependency on Spatio-Temporal Graphs with
  Selective State Spaces'
arxiv_id: '2406.11244'
source_url: https://arxiv.org/abs/2406.11244
tags:
- forecasting
- time
- spot-mamba
- graph
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SpoT-Mamba addresses the challenge of long-range spatio-temporal
  dependency modeling in graph forecasting tasks. The method uses Mamba-based selective
  state spaces to scan multi-way walk sequences and temporal sequences, capturing
  local and global structural information from node neighborhoods.
---

# SpoT-Mamba: Learning Long-Range Dependency on Spatio-Temporal Graphs with Selective State Spaces

## Quick Facts
- arXiv ID: 2406.11244
- Source URL: https://arxiv.org/abs/2406.11244
- Authors: Jinhyeok Choi; Heehyeon Kim; Minhyeong An; Joyce Jiyoung Whang
- Reference count: 40
- Primary result: State-of-the-art performance on PEMS04 traffic forecasting with MAE 18.31, RMSE 30.11, MAPE 11.86

## Executive Summary
SpoT-Mamba introduces a novel approach for spatio-temporal graph forecasting that leverages Mamba-based selective state spaces to model long-range dependencies. The method generates node embeddings by scanning multi-way walk sequences (DFS, BFS, RW) through the graph, then conducts temporal scans to capture long-range dependencies. By replacing traditional transformer layers with Mamba blocks, the approach achieves linear scalability in sequence length while maintaining input-dependent selectivity. The architecture combines spatial and temporal information through a dual-scan mechanism, achieving state-of-the-art performance on the PEMS04 traffic forecasting benchmark.

## Method Summary
SpoT-Mamba addresses spatio-temporal graph forecasting by combining selective state spaces with graph walk sequences. The method generates node-specific walk sequences using DFS, BFS, and random walks, then embeds these sequences using bidirectional Mamba scans followed by pointwise convolution and MLP. Temporal scanning is performed using Mamba blocks to capture long-range dependencies, with transformer layers adding global spatial attention at each time step. The architecture incorporates day-of-week and timestamps-of-day embeddings, and is trained using Huber loss with Adam optimizer. The approach achieves linear scalability in sequence length while maintaining input-dependent selectivity through its selective state space design.

## Key Results
- Achieves state-of-the-art performance on PEMS04 with MAE 18.31, RMSE 30.11, MAPE 11.86
- Ranks first in MAPE metric and second overall across all three evaluation metrics
- Ablation studies confirm Mamba blocks outperform transformers for walk sequence embedding
- Linear scalability in sequence length demonstrated theoretically, though not empirically stress-tested

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Mamba's selective state spaces enable input-dependent information filtering, overcoming the static nature of traditional S4 models.
- **Mechanism**: The learnable parameters B and C, along with step size ∆, are made functions of the input sequence, allowing dynamic selection of relevant information during temporal scanning.
- **Core assumption**: Input-dependent selectivity improves long-range dependency modeling compared to fixed LTI properties.
- **Evidence anchors**: [abstract] "SpoT-Mamba generates node embeddings by scanning various node-specific walk sequences. Based on the node embeddings, it conducts temporal scans to capture long-range spatio-temporal dependencies." [section 2] "Mamba incorporates a selection mechanism that allows its learnable parameters to dynamically interact with the input sequence. This mechanism is achieved by modifying the learnable parameters B and C, as well as the step size ∆, to functions of the input sequence."

### Mechanism 2
- **Claim**: Multi-way walk sequences (DFS, BFS, RW) capture diverse structural information from node neighborhoods.
- **Mechanism**: Three walk algorithms extract local and global structural patterns, which are then embedded using bi-directional Mamba scans and aggregated via pointwise convolution.
- **Core assumption**: Different walk patterns provide complementary structural information necessary for accurate node embeddings.
- **Evidence anchors**: [section 3] "We employ three well-known walk algorithms: depth-first search (DFS), breadth-first search (BFS), and random walks (RW), to extract diverse local and global structural information from each node's neighborhood." [section 4] "We observed differences in performance depending on which type of scan module is replaced. Specifically, when the Walk Scan is conducted by a transformer encoder, the overall performance of SpoT-Mamba decreases."

### Mechanism 3
- **Claim**: Combining Mamba temporal scans with transformer layers captures both fine-grained local dynamics and global graph context.
- **Mechanism**: Mamba blocks scan temporal sequences for long-range dependencies, while transformer layers add global spatial attention at each time step.
- **Core assumption**: The combination of selective state spaces and attention mechanisms provides complementary strengths for spatio-temporal modeling.
- **Evidence anchors**: [section 3] "SpoT-Mamba enhances the representations of nodes scanned along the temporal axis by incorporating global information from the entire graph at each time step through transformer layers." [section 4] "When replacing the Mamba blocks for the Temporal Scan with a transformer encoder, we reduce the batch size from 32 to 8 due to Out-of-Memory issues."

## Foundational Learning

- **Concept**: State Space Models (SSM)
  - **Why needed here**: SSM provides the theoretical foundation for Mamba's selective state spaces, enabling efficient long-range sequence modeling.
  - **Quick check question**: What are the key equations defining a basic SSM, and how do they relate to RNNs?

- **Concept**: Graph Neural Networks (GNN)
  - **Why needed here**: GNNs are used in related approaches but are replaced by Mamba blocks for spatial embedding, requiring understanding of their limitations.
  - **Quick check question**: How do GNNs typically aggregate neighborhood information, and what are their computational bottlenecks?

- **Concept**: Attention Mechanisms
  - **Why needed here**: Transformers and attention are used in SpoT-Mamba for global spatial context, requiring understanding of their strengths and weaknesses.
  - **Quick check question**: What is the computational complexity of self-attention, and why is it problematic for long sequences?

## Architecture Onboarding

- **Component map**: Input → Node-specific walk sequences (DFS/BFS/RW) → Walk sequence embedding (Mamba bi-scan + pointwise conv + MLP) → Temporal scan (Mamba blocks) → Global spatial attention (Transformer) → Regression → Output
- **Critical path**: Walk sequence generation → Walk sequence embedding → Temporal scan → Transformer spatial attention → Regression
- **Design tradeoffs**: Mamba blocks provide linear scalability but require careful parameter tuning; transformers add global context but increase computational cost
- **Failure signatures**: Out-of-memory errors during temporal scan (indicates batch size too large for Mamba blocks); poor walk sequence diversity (indicates inadequate neighborhood exploration); overfitting on temporal patterns (indicates insufficient regularization)
- **First 3 experiments**:
  1. Replace Mamba blocks in walk sequence embedding with simple GNN layers and compare performance
  2. Remove transformer layers and evaluate impact on capturing global spatial context
  3. Vary walk sequence length K and observe impact on local vs. global structural information capture

## Open Questions the Paper Calls Out

- **Open Question 1**: How does SpoT-Mamba's performance scale with increasing graph size and sequence length beyond the PEMS04 dataset?
  - **Basis in paper**: [explicit] The paper mentions that SpoT-Mamba achieves linear scalability in sequence length and suggests future work to extend to graphs with complex relations and evolving graphs.
  - **Why unresolved**: The experiments were conducted on a single dataset (PEMS04) with fixed graph size and sequence length. The paper does not provide empirical evidence for performance scaling on larger graphs or longer sequences.
  - **What evidence would resolve it**: Experiments on multiple datasets with varying graph sizes and sequence lengths, including benchmarks specifically designed for large-scale spatio-temporal graphs.

- **Open Question 2**: What is the impact of different walk algorithms (DFS, BFS, RW) on SpoT-Mamba's performance, and can hybrid or learned walk strategies improve results?
  - **Basis in paper**: [explicit] The paper mentions using three walk algorithms but does not provide an ablation study comparing their individual or combined effects.
  - **Why unresolved**: The paper only mentions using these walk algorithms without analyzing their individual contributions or exploring alternative walk strategies.
  - **What evidence would resolve it**: Ablation studies isolating the effects of each walk algorithm, comparison with learned walk strategies, and analysis of how different graph structures affect walk algorithm performance.

- **Open Question 3**: How does SpoT-Mamba handle dynamic graph structures where edges and nodes can appear or disappear over time?
  - **Basis in paper**: [inferred] The paper mentions future work on evolving graphs but does not address how the current architecture handles dynamic graph changes during inference.
  - **Why unresolved**: The current implementation assumes a static graph structure, and the paper does not discuss mechanisms for handling structural changes or their impact on walk sequence generation.
  - **What evidence would resolve it**: Experiments on datasets with dynamic graph structures, analysis of how walk sequence generation adapts to structural changes, and comparison with specialized methods for dynamic graphs.

## Limitations

- Claims about Mamba's superiority over transformers for walk sequence embedding are limited to this specific architecture and task
- Evaluation only considers one benchmark dataset (PEMS04), limiting generalizability
- Walk sequence diversity mechanism lacks direct empirical validation of whether different walk patterns capture complementary information

## Confidence

- **High Confidence**: The architectural design and implementation details are clearly specified, allowing for faithful reproduction. The performance metrics (MAE 18.31, RMSE 30.11, MAPE 11.86) are well-defined and the ranking claims are verifiable.
- **Medium Confidence**: The claim that Mamba blocks outperform transformers for walk sequence embedding is supported by ablation studies within the paper, but lacks external validation. The theoretical advantages of selective state spaces over traditional S4 models are plausible but not directly demonstrated.
- **Low Confidence**: The claim about walk sequence diversity capturing complementary structural information is based on algorithmic design rather than empirical validation. The scalability benefits of Mamba over transformers are theoretically sound but not empirically stress-tested.

## Next Checks

1. **Cross-dataset validation**: Evaluate SpoT-Mamba on additional spatio-temporal graph forecasting benchmarks (e.g., PEMS-BAY, METR-LA) to assess generalizability beyond PEMS04.

2. **Ablation of walk diversity**: Remove the multi-way walk mechanism and replace with single walk pattern (e.g., only BFS) to directly measure the impact of walk sequence diversity on forecasting accuracy.

3. **Scalability stress test**: Systematically vary sequence lengths and measure actual computational costs and memory usage to empirically validate the claimed linear scalability advantage of Mamba over transformers.