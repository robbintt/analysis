---
ver: rpa2
title: 'Recursive Speculative Decoding: Accelerating LLM Inference via Sampling Without
  Replacement'
arxiv_id: '2402.14160'
source_url: https://arxiv.org/abs/2402.14160
tags:
- draft
- rsd-s
- rsd-c
- spectr
- decoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Recursive Speculative Decoding (RSD), a novel
  tree-based method for accelerating large language model (LLM) inference. The core
  idea is to sample draft tokens without replacement to maximize the diversity of
  the draft-token tree, using either Gumbel-Top-k trick or Stochastic Beam Search.
---

# Recursive Speculative Decoding: Accelerating LLM Inference via Sampling Without Replacement

## Quick Facts
- arXiv ID: 2402.14160
- Source URL: https://arxiv.org/abs/2402.14160
- Authors: Wonseok Jeon; Mukul Gagrani; Raghavv Goel; Junyoung Park; Mingu Lee; Christopher Lott
- Reference count: 40
- One-line primary result: RSD outperforms speculative decoding and SpecTr in block efficiency, speed-up, and token rate across various tasks and computational budgets.

## Executive Summary
This paper introduces Recursive Speculative Decoding (RSD), a novel tree-based method for accelerating large language model (LLM) inference. The core innovation is sampling draft tokens without replacement to maximize the diversity of the draft-token tree, using either Gumbel-Top-k trick or Stochastic Beam Search. This approach is verified by a proposed recursive rejection sampling algorithm that can recover the exact target model distribution even with sampling without replacement. Empirically, RSD consistently outperforms baseline methods like speculative decoding and SpecTr across various tasks and computational budgets.

## Method Summary
RSD accelerates LLM inference by constructing a draft-token tree via sampling without replacement, followed by recursive rejection sampling to verify and recover the target model's distribution. The method builds the tree using either Gumbel-Top-k trick (RSD-C) or Stochastic Beam Search (RSD-S), then applies recursive rejection sampling at each level to accept or reject draft tokens. If all tokens are rejected at a level, it samples from the residual distribution. This allows RSD to maintain the exact target distribution while exploring a more diverse token space than sampling with replacement methods.

## Key Results
- RSD achieves superior block efficiency compared to speculative decoding and SpecTr across all tested computational budgets
- RSD-S outperforms RSD-C by early-truncating unlikely draft sequences and using far-sighted sequence log probabilities
- Consistent performance improvements across WMT18 translation, XSum summarization, and Dolly question answering tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RSD accelerates LLM inference by sampling draft tokens without replacement to maximize the diversity of the draft-token tree.
- Mechanism: By using sampling without replacement, RSD ensures that each draft token at a given level of the tree is unique, preventing redundant paths and maximizing the exploration of the token space. This is achieved through either the Gumbel-Top-k trick or Stochastic Beam Search (SBS).
- Core assumption: The draft model's distribution is sufficiently aligned with the target model's distribution to make sampling without replacement beneficial.
- Evidence anchors:
  - [abstract]: "We present Recursive Speculative Decoding (RSD), a novel tree-based speculative decoding method that samples draft tokens without replacement and maximizes the diversity of the tree."
  - [section]: "Two Recursive Speculative Decoding (RSD) algorithms using recursive rejection sampling are presented in this section, while they share the same pipeline for parallel target evaluation and draft tree verification after building the draft-token tree."
- Break condition: If the draft model is poorly aligned with the target model, sampling without replacement may not provide a significant advantage over sampling with replacement.

### Mechanism 2
- Claim: Recursive rejection sampling allows RSD to recover the target model's distribution exactly, even with sampling without replacement.
- Mechanism: Recursive rejection sampling is a generalization of multi-round rejection sampling that can handle draft distributions with dependencies. It accepts or rejects draft tokens at each level of the tree, and if all tokens are rejected, it samples from the residual distribution.
- Core assumption: The residual distributions can be accurately computed and used to recover the target distribution.
- Evidence anchors:
  - [abstract]: "We also propose recursive rejection sampling that can verify the tree built by the sampling-without-replacement process and recovers the exact target model distribution."
  - [section]: "Theorem 3.1 (Recursive rejection sampling recovers target distribution). The random variable Z ∈ X defining recursive rejection sampling rule (4) follows the target distribution q, i.e., Pr{Z = z} = q(z), z ∈ X."
- Break condition: If the residual distributions are poorly estimated or if the draft model is very different from the target model, the recovery of the target distribution may be compromised.

### Mechanism 3
- Claim: RSD-S outperforms RSD-C by early-truncating unlikely draft sequences and using far-sighted sequence log probabilities.
- Mechanism: SBS efficiently samples draft-token sequences without replacement while truncating sequences that are unlikely to be generated from the draft model. This reduces the computational cost and improves the acceptance rate.
- Core assumption: Early-truncation of unlikely sequences is beneficial and does not significantly harm the quality of the generated output.
- Evidence anchors:
  - [abstract]: "RSD-S uses Stochastic Beam Search and samples draft-token sequences without replacement, while truncating sequences that are unlikely to be generated from the draft model and efficiently handling the computational cost."
  - [section]: "Let us define the maximum draft sequence length L and the beamwidth W. We also define τ (1) 0 as the input sequence similar to RSD-C. At each level l ∈ { 0, ..., L − 1}, SBS uses beam Bl := (t(1) l , ..., t(W) l ), t(k) l := (τ (k) l , ϕl−1(τ (k) l ), ψl−1(τ (k) l )) generated from the previous level l − 1."
- Break condition: If the draft model is very well-aligned with the target model, the early-truncation of unlikely sequences may not provide a significant advantage.

## Foundational Learning

- Concept: Sampling without replacement
  - Why needed here: To maximize the diversity of the draft-token tree and prevent redundant paths.
  - Quick check question: What is the difference between sampling with replacement and sampling without replacement?

- Concept: Recursive rejection sampling
  - Why needed here: To recover the target model's distribution exactly, even with sampling without replacement.
  - Quick check question: How does recursive rejection sampling differ from standard rejection sampling?

- Concept: Stochastic Beam Search
  - Why needed here: To efficiently sample draft-token sequences without replacement while truncating unlikely sequences.
  - Quick check question: What is the role of the beamwidth in Stochastic Beam Search?

## Architecture Onboarding

- Component map: Draft model -> Draft-token tree construction -> Recursive rejection sampling -> Target model verification -> Final output

- Critical path: Draft model generates draft tokens -> Draft tokens are verified by the target model -> Recursive rejection sampling accepts or rejects draft tokens -> Final output is generated from the accepted draft tokens or the target model

- Design tradeoffs: Sampling without replacement vs. sampling with replacement -> RSD-C vs. RSD-S (constant branching factors vs. stochastic beam search) -> Tree depth vs. tree width

- Failure signatures: Low acceptance rate -> Draft model is poorly aligned with target model -> High computational cost -> Tree is too deep or too wide -> Poor output quality -> Early-truncation of unlikely sequences is too aggressive

- First 3 experiments: 1. Compare RSD with sampling without replacement to speculative decoding with sampling with replacement. 2. Compare RSD-C and RSD-S with different tree structures. 3. Evaluate the impact of the draft model's size on RSD's performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RSD scale with increasingly larger draft models or target models?
- Basis in paper: [inferred] The paper only considers draft models with 115M, 125M, and 350M parameters across different target model sizes. It would be valuable to explore the impact of scaling up draft models to match the growing size of target models.
- Why unresolved: The paper does not provide experiments or analysis on how RSD performs with significantly larger draft models or target models beyond those tested.
- What evidence would resolve it: Experimental results comparing RSD's performance using draft models and target models with varying parameter counts, especially larger ones, would provide insights into its scalability.

### Open Question 2
- Question: What is the impact of different sampling strategies within the stochastic beam search on RSD's performance?
- Basis in paper: [explicit] The paper mentions that RSD-S uses stochastic beam search to sample draft-token sequences without replacement, but does not explore the impact of different sampling strategies within this approach.
- Why unresolved: The paper does not provide a comparison of RSD-S's performance using different sampling strategies within the stochastic beam search.
- What evidence would resolve it: Experiments comparing RSD-S's performance using various sampling strategies within the stochastic beam search, such as different temperature settings or sampling methods, would provide insights into the impact of these choices.

### Open Question 3
- Question: How does the choice of branching factors in RSD-C affect its performance, and is there an optimal strategy for selecting them?
- Basis in paper: [inferred] The paper mentions that RSD-C uses constant branching factors to construct the draft-token tree, but does not explore the impact of different branching factor choices or provide guidance on optimal selection strategies.
- Why unresolved: The paper does not provide a systematic study of how different branching factor choices affect RSD-C's performance or offer recommendations for optimal selection.
- What evidence would resolve it: Experiments comparing RSD-C's performance using various branching factor configurations, along with analysis of the trade-offs involved, would help identify optimal strategies for selecting branching factors.

## Limitations
- Performance critically depends on draft model distribution being aligned with target model distribution
- Recursive rejection sampling introduces additional computational overhead not fully quantified
- Limited scalability testing with larger draft models beyond 350M parameters

## Confidence
- High Confidence: The basic premise of sampling without replacement for diversity, correctness of recursive rejection sampling, and comparative performance advantages
- Medium Confidence: Specific implementation details of Stochastic Beam Search, generalizability across tasks, and sensitivity to hyperparameters

## Next Checks
1. **Distribution Gap Analysis**: Systematically evaluate RSD performance as a function of the KL divergence between draft and target model distributions to quantify sensitivity to distribution alignment.

2. **Computational Overhead Benchmark**: Implement detailed wall-clock time analysis that separately measures draft generation time, verification time, and overhead from recursive rejection sampling across all methods.

3. **Scaling Study with Larger Draft Models**: Test RSD with draft models in the 1B-3B parameter range while maintaining the same target models to verify performance advantages persist at larger scales.