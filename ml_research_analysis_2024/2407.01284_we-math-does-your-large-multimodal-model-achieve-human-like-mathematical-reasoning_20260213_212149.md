---
ver: rpa2
title: 'We-Math: Does Your Large Multimodal Model Achieve Human-like Mathematical
  Reasoning?'
arxiv_id: '2407.01284'
source_url: https://arxiv.org/abs/2407.01284
tags:
- mathematics
- knowledge
- beijing
- edition
- grade
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WE-MATH, a benchmark for evaluating visual
  mathematical reasoning in Large Multimodal Models (LMMs). The authors address the
  limitation of existing benchmarks that focus solely on end-to-end performance while
  neglecting the underlying problem-solving principles.
---

# We-Math: Does Your Large Multimodal Model Achieve Human-like Mathematical Reasoning?

## Quick Facts
- arXiv ID: 2407.01284
- Source URL: https://arxiv.org/abs/2407.01284
- Authors: Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma GongQue, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, Runfeng Qiao, Yifan Zhang, Xiao Zong, Yida Xu, Muxi Diao, Zhimin Bao, Chen Li, Honggang Zhang
- Reference count: 40
- Primary result: WE-MATH benchmark reveals LMMs' performance negatively correlates with number of knowledge concepts, with GPT-4o transitioning from Insufficient Knowledge to Inadequate Generalization while others show Rote Memorization

## Executive Summary
This paper introduces WE-MATH, the first benchmark specifically designed to evaluate visual mathematical reasoning in Large Multimodal Models (LMMs) beyond end-to-end performance. The benchmark addresses the limitation of existing evaluations by focusing on problem-solving principles through a hierarchical knowledge structure with 6.5K visual math problems spanning 67 knowledge concepts. The authors propose a novel four-dimensional metric (Insufficient Knowledge, Inadequate Generalization, Complete Mastery, Rote Memorization) to assess reasoning processes and introduce a Knowledge Concept Augmentation strategy to address identified limitations.

## Method Summary
The WE-MATH benchmark is constructed around textbook knowledge units, decomposing composite problem solutions into sub-problems based on required knowledge concepts. A hierarchical knowledge structure organizes 6.5K visual math problems across five layers of granularity. The benchmark employs a four-dimensional evaluation metric to assess LMMs' reasoning process rather than just final answers. Knowledge Concept Augmentation (KCA) strategy provides essential knowledge support through human-curated descriptions from Wikipedia and textbooks, which are concatenated to prompts during evaluation to address Insufficient Knowledge issues.

## Key Results
- GPT-4o shows superior performance, transitioning from Insufficient Knowledge to Inadequate Generalization across problem complexity
- Most LMMs perform significantly worse on multi-step problems compared to one-step problems, revealing a negative correlation between solving steps and problem-specific performance
- Knowledge Concept Augmentation strategy effectively reduces Insufficient Knowledge issues in LMMs
- LMMs exhibit Rote Memorization tendencies, correctly solving composite problems but failing individual sub-problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical knowledge decomposition improves reasoning evaluation by isolating knowledge concept mastery from generalization ability
- Mechanism: The benchmark decomposes composite problems into sub-problems based on required knowledge concepts, then uses a four-dimensional metric (IK, IG, CM, RM) to evaluate reasoning process
- Core assumption: Mathematical problem-solving follows a hierarchical knowledge structure where complex problems are built from simpler knowledge concepts
- Evidence anchors:
  - [abstract] "we introduce WE-MATH, the first benchmark specifically designed to explore the problem-solving principles beyond the end-to-end performance"
  - [section] "WE-MATH is constructed around textbook knowledge units, decomposing composite problem solutions into sub-problems based on the knowledge concepts"
  - [corpus] Weak - related papers focus on math reasoning but don't explicitly discuss hierarchical decomposition mechanisms

### Mechanism 2
- Claim: Knowledge Concept Augmentation (KCA) strategy effectively addresses Insufficient Knowledge issues by providing essential knowledge support
- Mechanism: Human experts create 67 knowledge concept cards from Wikipedia and textbooks, which are concatenated to prompts during evaluation
- Core assumption: LMMs' reasoning gaps stem from missing foundational knowledge rather than inability to generalize
- Evidence anchors:
  - [abstract] "We confirm the IK issue of LMMs can be effectively improved via knowledge augmentation strategy"
  - [section] "we heuristically introduce descriptions for 67 knowledge concepts from Wikipedia and textbooks, thereby providing essential knowledge support"
  - [corpus] Weak - related papers don't discuss knowledge concept augmentation as a specific mechanism

### Mechanism 3
- Claim: Negative correlation between number of knowledge concepts and model performance reveals limitations in current LMM architectures
- Mechanism: As problems require more knowledge concepts, performance drops significantly, indicating models struggle with knowledge integration
- Core assumption: Current LMMs can handle individual knowledge concepts but fail when multiple concepts must be integrated
- Evidence anchors:
  - [abstract] "we reveal a negative correlation between solving steps and problem-specific performance"
  - [section] "Most LMMs perform significantly worse on multi-step problems compared to one-step problems"
  - [corpus] Moderate - related papers discuss math reasoning performance but don't specifically analyze concept count correlation

## Foundational Learning

- Concept: Hierarchical knowledge structures in mathematics
  - Why needed here: Understanding how complex mathematical problems are built from simpler knowledge concepts is crucial for interpreting benchmark design and evaluation methodology
  - Quick check question: How does the decomposition of a composite problem into sub-problems based on knowledge concepts help identify specific reasoning weaknesses in LMMs?

- Concept: Four-dimensional reasoning metrics (IK, IG, CM, RM)
  - Why needed here: The novel evaluation framework distinguishes between different types of reasoning failures, which is essential for understanding model capabilities and limitations
  - Quick check question: Why is a model that correctly solves multi-step problems but fails on sub-problems (RM) considered less reliable than one that struggles with both (IK)?

- Concept: Knowledge concept augmentation and prompt engineering
  - Why needed here: Understanding how external knowledge is integrated into prompts is crucial for reproducing results and extending the approach
  - Quick check question: How does concatenating knowledge concept descriptions to prompts differ from standard few-shot prompting techniques?

## Architecture Onboarding

- Component map: Problem collection -> Knowledge concept annotation -> Problem decomposition -> Four-dimensional evaluation -> KCA strategy implementation
- Critical path: Problem collection → Knowledge concept annotation → Problem decomposition → Metric application → KCA strategy evaluation
- Design tradeoffs: Fine-grained evaluation vs. scalability; expert annotation requirements vs. automated approaches; knowledge augmentation vs. model training
- Failure signatures: Inconsistent decomposition logic; ambiguous metric boundaries; ineffective KCA implementation; corpus contamination
- First 3 experiments:
  1. Reproduce the four-dimensional metric classification on a small subset of problems to verify the decomposition logic
  2. Test KCA strategy on a single knowledge concept to validate the augmentation approach
  3. Compare performance across varying numbers of knowledge concepts to confirm the negative correlation finding

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the four-dimensional metric be extended to handle problems with more than three knowledge concepts without losing granularity in the evaluation?
- Basis in paper: [explicit] The paper introduces the metric for one-step, two-step, and three-step problems but does not address scalability beyond three steps
- Why unresolved: The paper demonstrates the metric on problems with up to three knowledge concepts, leaving open how the approach would work for more complex problems
- What evidence would resolve it: A validated extension of the metric to problems with 4+ knowledge concepts with corresponding decomposition methodology

### Open Question 2
- Question: What is the optimal balance between knowledge concept augmentation and model generalization ability for visual mathematical reasoning?
- Basis in paper: [explicit] The paper shows KCA reduces IK issues but doesn't explore the tradeoff between augmentation and generalization
- Why unresolved: The paper demonstrates KCA effectiveness but doesn't investigate whether excessive augmentation might hinder the development of true reasoning capabilities
- What evidence would resolve it: Systematic ablation studies varying KCA intensity and measuring downstream generalization performance

### Open Question 3
- Question: How does the performance of LMMs on visual mathematical reasoning tasks correlate with their performance on other multimodal reasoning benchmarks?
- Basis in paper: [inferred] The paper focuses specifically on mathematical reasoning but doesn't compare to general multimodal reasoning capabilities
- Why unresolved: While the paper shows specific performance patterns in mathematical domains, it doesn't establish whether these patterns generalize to other types of multimodal reasoning
- What evidence would resolve it: Correlation analysis between WE-MATH scores and performance on other multimodal reasoning benchmarks across the same set of models

## Limitations
- Expert annotation requirements create scalability challenges for extending the benchmark to broader mathematical domains
- Focus on textbook-style problems may limit applicability to real-world mathematical reasoning scenarios
- Four-dimensional metric may not capture all nuances of mathematical reasoning failures, particularly for complex multi-concept problems

## Confidence
- Hierarchical knowledge decomposition approach: Medium confidence
- Knowledge Concept Augmentation (KCA) strategy: Medium confidence
- Negative correlation between knowledge concept count and performance: Medium confidence

## Next Checks
1. Test the WE-MATH benchmark and KCA strategy on non-textbook mathematical problems to assess generalizability beyond the current 67 knowledge concepts
2. Implement and compare alternative problem decomposition strategies to validate whether the hierarchical knowledge structure is optimal for evaluating mathematical reasoning
3. Conduct longitudinal studies with KCA-augmented models to determine whether knowledge concept augmentation leads to lasting improvements in mathematical reasoning or merely provides temporary performance boosts during evaluation