---
ver: rpa2
title: Detection and Recovery Against Deep Neural Network Fault Injection Attacks
  Based on Contrastive Learning
arxiv_id: '2401.16766'
source_url: https://arxiv.org/abs/2401.16766
tags:
- recovery
- data
- detection
- contrastive
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Contrastive Learning (CL) into the deep learning
  training and inference pipeline to implement DNN inference engines with self-resilience
  under Fault Injection Attacks (FIAs). The proposed CL based FIA Detection and Recovery
  (CFDR) framework features real-time detection with only a single batch of testing
  data and fast recovery effective even with only a small amount of unlabeled testing
  data.
---

# Detection and Recovery Against Deep Neural Network Fault Injection Attacks Based on Contrastive Learning

## Quick Facts
- arXiv ID: 2401.16766
- Source URL: https://arxiv.org/abs/2401.16766
- Reference count: 13
- Primary result: Framework detects FIAs by monitoring contrastive loss deviations and recovers accuracy up to 88% with limited data

## Executive Summary
This paper introduces CFDR (Contrastive Learning based Fault Injection Attack Detection and Recovery), a framework that leverages contrastive learning to enable DNNs to detect and recover from fault injection attacks in real-time. The approach uses a parallel projection head to compute contrastive loss during inference, which serves as a detection signal when significant deviations from clean-model baselines occur. The framework demonstrates effective detection and recovery across various attack types on CIFAR-10, with promising results even when only unlabeled testing data is available.

## Method Summary
CFDR implements a two-phase contrastive learning approach: (1) contrastive pretraining to learn robust representations, and (2) fine-tuning for the specific classification task. During inference, a parallel projection head computes contrastive loss alongside the main classification path. Detection occurs by comparing this loss against a threshold from the clean model. Recovery employs either the contrastive learning phase or fine-tuning phase using available data, with the option to skip the contrastive phase when data is limited. The framework supports both labeled and unlabeled data scenarios for recovery.

## Key Results
- Detection accuracy: 99.3-99.8% for attacks modifying 9K-1.18M parameters on CIFAR-10
- Recovery with labeled data: 87-88% accuracy for attacks with fewer modified parameters
- Recovery with unlabeled data: 80-88% accuracy for attacks with fewer modified parameters, 33-45% for more severe attacks
- Detection works in real-time with single batches of testing data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning loss can serve as a real-time fault detection signal for DNNs under FIA
- Mechanism: A parallel projection head computes contrastive loss during inference; significant deviation from clean-model baseline indicates parameter corruption
- Core assumption: FIAs alter model parameters in ways that disrupt similarity preservation between augmented views
- Evidence anchors:
  - [abstract] "use CL to obtain the DNN inference models...observe the change of contrastive loss over single batches of testing data to detect potential FIAs"
  - [section 2.3] "we propose to use the contrastive learning loss in Phase (a) as a key criterion to determine whether the DNN model is attacked by a FIA"
  - [corpus] No direct citations; concept is novel in this work
- Break condition: If fault injection modifies weights without affecting embedding geometry, contrastive loss may not change significantly

### Mechanism 2
- Claim: Self-supervised contrastive pretraining enables effective recovery with minimal labeled data
- Mechanism: Two-phase training (contrastive pretraining + fine-tuning) creates models with representations robust enough to recover accuracy after partial parameter corruption
- Core assumption: Representations learned through contrastive objectives retain semantic structure even when some weights are corrupted
- Evidence anchors:
  - [section 2.1] "CL enables our detection as well as recovery mechanisms against FIAs"
  - [section 3.3] "For models with less perturbed/attacked parameters, the accuracy can be restored to a typical accuracy (around 87%) for ResNet-18 on CIFAR-10 with only a few data"
  - [corpus] No prior work found using contrastive learning specifically for FIA recovery
- Break condition: If too many parameters are modified (e.g., >1M in experiments), recovery accuracy plateaus below 50%

### Mechanism 3
- Claim: Parallel contrastive loss computation during inference enables non-intrusive detection
- Mechanism: Additional projection head path runs concurrently with classification path without affecting inference latency
- Core assumption: Computational overhead of contrastive loss calculation is negligible compared to inference
- Evidence anchors:
  - [section 2.3] "we can add the project head at the output of base encoder as the other path in parallel with the FC layer"
  - [section 2.2] "the detection process can be co-executed during the normal DNN inference"
  - [corpus] No explicit performance measurements provided in paper
- Break condition: If projection head adds significant latency or memory overhead on resource-constrained devices

## Foundational Learning

- Concept: Contrastive learning objective (SimCLR)
  - Why needed here: Provides self-supervised representation learning that doesn't require labels for detection and enables recovery with limited data
  - Quick check question: What is the role of temperature parameter Ï„ in the contrastive loss formula?

- Concept: Fault injection attack taxonomy
  - Why needed here: Different FIA types (PBS, FSA, GDA) have varying parameter modification patterns that affect detection sensitivity
  - Quick check question: How do â„“0 vs â„“2 norm constraints in FSA attacks affect the number of parameters modified?

- Concept: Model checkpointing and reference baselines
  - Why needed here: Clean model's contrastive loss serves as detection threshold; recovery requires clean baseline for stopping criteria
  - Quick check question: What stopping criteria are used during recovery to prevent overfitting with limited data?

## Architecture Onboarding

- Component map: Input -> Base encoder (ResNet-18) -> [Projection head (MLP) -> Contrastive loss computation] and [FC layer -> Classification] in parallel
- Critical path: Input â†’ Base encoder â†’ FC layer â†’ Prediction
- Design tradeoffs:
  - Detection sensitivity vs false positive rate (controlled by Î´)
  - Recovery data requirements vs final accuracy (labeled vs unlabeled)
  - Computational overhead of parallel projection head vs real-time detection capability
- Failure signatures:
  - High contrastive loss with clean model indicates implementation bug
  - Low contrastive loss during detection despite known FIA suggests attack doesn't affect embedding space
  - Recovery accuracy plateaus below target indicates excessive parameter corruption
- First 3 experiments:
  1. Baseline: Measure contrastive loss distribution for clean model over multiple batches to establish detection threshold
  2. Sensitivity: Apply PBS attack with varying parameter counts and verify detection accuracy vs ground truth
  3. Recovery: Test recovery effectiveness with labeled vs unlabeled data on moderate-parameter attacks (50K-200K parameters)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the fault tolerance parameter (ð›¿) affect the detection accuracy and false positive/negative rates in real-world deployment?
- Basis in paper: [explicit] The paper mentions setting ð›¿ as the threshold for the difference between the contrastive loss of the tested model and the clean model, but does not provide detailed analysis of how different ð›¿ values impact detection performance.
- Why unresolved: The paper only briefly mentions the importance of setting ð›¿ properly but does not provide empirical analysis or guidelines for choosing this parameter.
- What evidence would resolve it: Experimental results showing detection accuracy and false positive/negative rates across a range of ð›¿ values, along with recommended thresholds for different attack scenarios.

### Open Question 2
- Question: What is the impact of attack frequency on the detection mechanism's ability to distinguish between normal inference fluctuations and actual attacks?
- Basis in paper: [inferred] The paper mentions that detection can be executed periodically without disrupting normal inference, but does not analyze how frequently attacks occur or how this affects detection accuracy.
- Why unresolved: The temporal aspect of detection is not explored, leaving uncertainty about the mechanism's effectiveness against frequent or intermittent attacks.
- What evidence would resolve it: Analysis of detection performance under different attack frequencies and patterns, including false detection rates when attacks occur during normal inference fluctuations.

### Open Question 3
- Question: How does the recovery performance scale with different amounts of available data (labeled or unlabeled) beyond the tested 512 images?
- Basis in paper: [explicit] The paper evaluates recovery with 512 images but does not explore how performance changes with more or less data.
- Why unresolved: The paper provides results for a fixed small dataset size without examining the relationship between data quantity and recovery effectiveness.
- What evidence would resolve it: Recovery accuracy results showing performance across varying amounts of available data, from very small to larger datasets, to establish the minimum data requirements for effective recovery.

### Open Question 4
- Question: How does the proposed framework perform against adaptive attacks that specifically target the contrastive learning-based detection mechanism?
- Basis in paper: [inferred] The paper evaluates against standard FIA attacks but does not consider attacks designed to evade the contrastive loss-based detection.
- Why unresolved: The framework's robustness against attacks that are aware of and attempt to circumvent the detection mechanism is not tested.
- What evidence would resolve it: Results from experiments using attacks specifically designed to minimize changes in contrastive loss while still achieving their objectives, demonstrating whether the detection mechanism remains effective.

## Limitations
- Detection effectiveness drops significantly for attacks modifying >1M parameters, with recovery accuracy plateauing below 50%
- Framework evaluated only on CIFAR-10 dataset with ResNet-18 architecture, limiting generalizability
- No quantitative analysis of computational overhead for the parallel projection head during inference

## Confidence
- Detection mechanism effectiveness: Medium confidence - results show promise but lack comprehensive ablation studies on detection thresholds and false positive rates
- Recovery mechanism with limited data: Medium confidence - the 80-88% recovery rate with unlabeled data is impressive but the exact data requirements and stopping criteria are underspecified
- Overall framework applicability: Low-Medium confidence - evaluated only on CIFAR-10 with ResNet-18, limiting generalizability to other architectures and datasets

## Next Checks
1. Perform systematic ablation studies varying the fault tolerance parameter Î´ to quantify the detection false positive/negative tradeoff
2. Measure actual inference latency overhead of the parallel projection head on resource-constrained hardware targets
3. Test the framework on diverse datasets (ImageNet, medical imaging) and architectures (MobileNet, EfficientNet) to assess generalizability limits