---
ver: rpa2
title: 'Addressing Social Misattributions of Large Language Models: An HCXAI-based
  Approach'
arxiv_id: '2403.17873'
source_url: https://arxiv.org/abs/2403.17873
tags:
- social
- llms
- users
- llm-based
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of social misattributions in Large
  Language Models (LLMs), where users may incorrectly perceive LLM capabilities and
  assign inappropriate roles and personas, leading to risks such as emotional manipulation,
  epistemic injustice, and unwarranted trust. To address this, the authors propose
  extending Ehsan et al.'s Social Transparency (ST) framework with a fifth 'W-question'
  to clarify the specific social attributions assigned to LLMs by designers and users.
---

# Addressing Social Misattributions of Large Language Models: An HCXAI-based Approach

## Quick Facts
- arXiv ID: 2403.17873
- Source URL: https://arxiv.org/abs/2403.17873
- Reference count: 25
- Primary result: Proposes extending the Social Transparency framework with a fifth 'W-question' to address social misattributions in LLMs

## Executive Summary
This paper addresses the critical issue of social misattributions in Large Language Models (LLMs), where users may incorrectly perceive LLM capabilities and assign inappropriate roles and personas. The authors propose enhancing Ehsan et al.'s Social Transparency (ST) framework with a fifth 'W-question' to clarify specific social attributions assigned to LLMs by designers and users. This extension aims to bridge the gap between LLM capabilities and user perceptions, promoting ethically responsible development and use of LLM-based technology.

## Method Summary
The paper proposes a two-pronged approach to address social misattributions in LLMs. First, it suggests extending the Social Transparency framework by adding a fifth 'W-question' focusing on which social attributions are justified for LLMs in specific contexts. Second, it recommends developing a taxonomy of social attributions and implementing real-time detection algorithms to identify and prevent misattributions during user interactions. These methods aim to provide structured guidance and dynamic intervention to reduce the risks associated with inappropriate role and persona assignments to LLMs.

## Key Results
- Proposes extending the Social Transparency framework with a fifth 'W-question' to clarify social attributions
- Suggests developing taxonomies of appropriate and inappropriate roles for LLM-based applications
- Recommends implementing real-time detection algorithms to identify and mitigate social misattributions during user interactions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Extending the Social Transparency framework with a fifth 'W-question' clarifies social attributions by explicitly addressing which roles and personas are assigned to LLMs.
- Mechanism: The addition of the 'which' question creates a direct channel for designers and users to articulate and understand the intended versus perceived social attributions of LLMs. This transparency bridges the gap between technical capabilities and user expectations, reducing the risk of social misattributions.
- Core assumption: Users can accurately perceive and articulate the social attributions they assign to LLMs, and designers can effectively communicate their intended attributions.
- Evidence anchors:
  - [abstract]: "we propose enhancing the ST framework with a fifth 'W-question' to clarify the specific social attributions assigned to LLMs by its designers and users."
  - [section]: "the additional 'W-question' focuses on identifying (1) which social attributions are justiﬁed for a LLM in a certain context, and (2) which social attributions a user assigns to the LLM in the same context."
- Break condition: The mechanism breaks if users are unable to accurately perceive or articulate their social attributions, or if designers fail to effectively communicate their intended attributions.

### Mechanism 2
- Claim: Developing a taxonomy of social attributions provides structured guidance to prevent inappropriate role and persona assignments to LLMs.
- Mechanism: A taxonomy categorizes appropriate and inappropriate social attributions, offering examples and context-specific guidelines. This structured approach helps users and designers navigate the complexities of social attributions, reducing the likelihood of mismatches between intended and perceived roles.
- Core assumption: A well-designed taxonomy can capture the nuances of social attributions across different contexts and user perspectives.
- Evidence anchors:
  - [section]: "Organizations that develop LLM-based applications or use them for their services and products should provide taxonomies of the appropriate and inappropriate roles and personas for their systems."
  - [section]: "These taxonomies should include examples of (in-)appropriate social attributions to guide users through the process of role and persona assignment."
- Break condition: The mechanism fails if the taxonomy is too rigid, incomplete, or fails to account for diverse user perspectives and evolving social norms.

### Mechanism 3
- Claim: Implementing real-time detection algorithms can dynamically identify and mitigate social misattributions during user interactions with LLMs.
- Mechanism: Algorithms analyze conversational patterns to detect potential misattributions, providing warnings or corrective feedback to users. This proactive approach prevents the reinforcement of incorrect perceptions and promotes warranted trust in LLM-based applications.
- Core assumption: Conversational patterns can reliably indicate social misattributions, and users will respond positively to real-time feedback.
- Evidence anchors:
  - [section]: "Further, by developing algorithms that detect potential misattribution dynamically in the conversations that users have with LLM-based applications."
  - [section]: "For instance, the LLM-based application could simply warn users that they are likely incurring in an inappropriate social attribution of the system if a 'risk threshold' is met."
- Break condition: The mechanism is ineffective if the detection algorithms produce high false positive rates or if users ignore or distrust the feedback.

## Foundational Learning

- Concept: Social Transparency Framework
  - Why needed here: Understanding the existing ST framework is crucial to grasp how the fifth 'W-question' extends it and addresses social misattributions.
  - Quick check question: What are the four original 'W-questions' in the Social Transparency framework?

- Concept: Social Attribution
  - Why needed here: Grasping the concept of social attribution is essential to comprehend the risks of misattributions and the proposed solutions.
  - Quick check question: How do roles and personas differ in the context of LLM social attributions?

- Concept: Large Language Models (LLMs)
  - Why needed here: Knowledge of LLM capabilities and limitations is necessary to understand the potential for social misattributions and the importance of clarifying their intended roles.
  - Quick check question: What is the difference between the techno-function and socio-functions of LLMs?

## Architecture Onboarding

- Component map: Social Transparency framework (extended with fifth 'W-question') -> Taxonomy of social attributions -> Real-time detection algorithms
- Critical path: Integration of fifth 'W-question' into ST framework → Development and implementation of taxonomy → Creation and deployment of detection algorithms → User feedback and continuous refinement
- Design tradeoffs: Balancing taxonomy granularity with usability, ensuring real-time detection algorithms do not disrupt user experience, maintaining flexibility of ST framework for diverse contexts
- Failure signatures: Consistent user misattribution despite warnings, high false positive rates in detection algorithms, resistance to transparency requirements
- First 3 experiments:
  1. Implement the fifth 'W-question' in a controlled LLM application and measure changes in user perception accuracy
  2. Develop a prototype taxonomy and conduct user studies to assess its effectiveness in preventing misattributions
  3. Build a basic detection algorithm and test its ability to identify misattributions in simulated conversations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively develop a taxonomy of appropriate and inappropriate social attributions for LLM-based applications that accounts for diverse user perspectives and contexts?
- Basis in paper: [explicit] The paper suggests developing taxonomies of appropriate and inappropriate roles and personas for LLM-based applications, involving a transdisciplinary team and participatory design methodology.
- Why unresolved: The paper does not provide a concrete methodology or specific steps for developing such taxonomies. It also does not address how to handle the complexity and diversity of user perspectives across different cultural and social contexts.
- What evidence would resolve it: Empirical studies or case studies demonstrating successful development and implementation of social attribution taxonomies for LLM-based applications, including user feedback and validation of the taxonomy's effectiveness in reducing social misattributions.

### Open Question 2
- Question: What are the most effective techniques for dynamically detecting and preventing social misattributions during user interactions with LLM-based applications?
- Basis in paper: [explicit] The paper proposes implementing algorithms to detect potential misattributions dynamically and suggests warning users when a 'risk threshold' is met, referring to the taxonomy of social attributions for more information.
- Why unresolved: The paper does not provide specific algorithms or techniques for detecting misattributions. It also does not discuss how to determine the 'risk threshold' or evaluate the effectiveness of these techniques compared to static strategies like disclaimers.
- What evidence would resolve it: Comparative studies evaluating the effectiveness of dynamic detection and prevention techniques versus static strategies in reducing social misattributions, including user satisfaction and trust metrics.

### Open Question 3
- Question: How can we ensure that the extended Social Transparency framework with the fifth 'W-question' is effectively implemented in real-world LLM-based applications?
- Basis in paper: [inferred] The paper suggests extending the Social Transparency framework with a fifth 'W-question' to clarify social attributions assigned to LLMs by designers and users, but does not provide implementation details.
- Why unresolved: The paper does not discuss practical challenges in implementing the extended framework, such as integration with existing LLM systems, user interface design, or measuring the framework's impact on reducing social misattributions.
- What evidence would resolve it: Case studies or pilot implementations of the extended Social Transparency framework in LLM-based applications, including user feedback, system performance metrics, and assessments of the framework's impact on reducing social misattributions.

## Limitations
- The paper lacks empirical validation of the proposed solutions, relying heavily on theoretical frameworks
- Specific implementation details for the taxonomy and detection algorithms are not provided
- Potential user resistance to transparency measures and the computational overhead of real-time detection systems are not addressed

## Confidence
- **High Confidence**: The identification of social misattributions as a significant problem in LLM development and use. The theoretical grounding of the Social Transparency framework is well-established.
- **Medium Confidence**: The proposed solutions (fifth 'W-question', taxonomy, detection algorithms) are logically sound but lack empirical validation. The assumption that users will engage with and respond appropriately to these interventions is reasonable but unverified.
- **Low Confidence**: The specific implementation details for the taxonomy and detection algorithms are not provided, making it difficult to assess their practical feasibility and effectiveness.

## Next Checks
1. Conduct user studies to measure the impact of the fifth 'W-question' on user perception accuracy and understanding of LLM roles and capabilities.
2. Develop a prototype taxonomy and evaluate its effectiveness in guiding users and preventing social misattributions through controlled experiments.
3. Implement a basic real-time detection algorithm and test its ability to identify misattributions in simulated conversations, measuring false positive and negative rates.