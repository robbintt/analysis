---
ver: rpa2
title: 'Diffusion-RPO: Aligning Diffusion Models through Relative Preference Optimization'
arxiv_id: '2406.06382'
source_url: https://arxiv.org/abs/2406.06382
tags:
- preference
- alignment
- diffusion
- human
- style
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Diffusion-RPO adapts relative preference optimization to diffusion-based
  text-to-image models by applying contrastive weighting across both identical and
  semantically related prompt-image pairs at each diffusion timestep. It introduces
  multi-modal distance weighting using CLIP embeddings to measure similarity between
  prompt-image pairs and simplifies the optimization by computing log-probabilities
  at each denoising step.
---

# Diffusion-RPO: Aligning Diffusion Models through Relative Preference Optimization

## Quick Facts
- arXiv ID: 2406.06382
- Source URL: https://arxiv.org/abs/2406.06382
- Authors: Yi Gu; Zhendong Wang; Yueqin Yin; Yujia Xie; Mingyuan Zhou
- Reference count: 40
- Primary result: Outperforms Diffusion-DPO and SFT in human preference alignment on Stable Diffusion 1.5 and XL-1.0

## Executive Summary
Diffusion-RPO introduces a novel approach to align diffusion-based text-to-image models using Relative Preference Optimization (RPO). The method applies contrastive weighting across identical and semantically related prompt-image pairs at each diffusion timestep, leveraging CLIP embeddings to measure multi-modal similarity. By computing log-probabilities at each denoising step, Diffusion-RPO achieves superior performance in human preference alignment tasks compared to existing methods like Diffusion-DPO and SFT, while also demonstrating improved style alignment capabilities.

## Method Summary
Diffusion-RPO adapts relative preference optimization to diffusion models by applying contrastive weighting to prompt-image pairs at each timestep of the denoising process. The method uses CLIP embeddings to compute multi-modal distance weighting between pairs, enabling semantic alignment during training. Log-probabilities are calculated at each denoising step rather than only at the final output, providing richer gradient signals for preference optimization. This approach is applied to both Stable Diffusion 1.5 and XL-1.0, with training conducted on preference datasets to optimize for human-aligned outputs.

## Key Results
- Achieves higher reward model scores and win rates than Diffusion-DPO and SFT in human preference alignment tasks
- Demonstrates superior style alignment performance, better capturing desired styles while preserving fine details
- Introduces a new style alignment evaluation task that provides more reproducible and interpretable assessments than traditional human preference metrics

## Why This Works (Mechanism)
Diffusion-RPO works by incorporating relative preference information throughout the entire diffusion process rather than just at the final output. The contrastive weighting mechanism amplifies the optimization signal from preferred samples while suppressing non-preferred ones, creating a stronger gradient direction. By using CLIP embeddings to measure semantic similarity between prompt-image pairs, the method can align not just on exact matches but on semantically related concepts. Computing log-probabilities at each timestep provides more granular feedback during the denoising process, allowing the model to learn preferences at multiple levels of image generation.

## Foundational Learning

**Relative Preference Optimization (RPO)**: Why needed: Provides a framework for optimizing models based on relative human preferences rather than absolute targets. Quick check: Verify that the contrastive loss properly amplifies preferred samples.

**CLIP Embeddings**: Why needed: Enables semantic comparison between text prompts and generated images in a shared embedding space. Quick check: Ensure CLIP similarity scores correlate with human judgment of semantic alignment.

**Diffusion Model Timesteps**: Why needed: Each timestep represents a different level of noise in the generation process, affecting how preferences should be applied. Quick check: Confirm that preference signals are properly propagated through all timesteps.

**Contrastive Learning**: Why needed: Helps the model distinguish between preferred and non-preferred outputs by creating clear decision boundaries. Quick check: Validate that the contrastive weighting improves over simple preference ranking.

## Architecture Onboarding

Component Map: CLIP model -> Similarity computation -> Contrastive weighting -> Log-probability calculation -> Diffusion denoising steps -> Stable Diffusion model

Critical Path: Input preference pairs → CLIP embedding computation → Similarity distance calculation → Contrastive weighting application → Log-probability gradient computation → Diffusion model parameter update

Design Tradeoffs: Using CLIP embeddings provides semantic alignment but adds computational overhead; computing at each timestep gives richer gradients but increases training time; contrastive weighting amplifies preference signals but may require careful hyperparameter tuning.

Failure Signatures: Poor CLIP similarity scores indicate semantic misalignment; inconsistent preference optimization across timesteps suggests gradient instability; degradation in fine detail preservation indicates over-optimization for style.

First Experiments: 1) Validate CLIP-based similarity scores against human semantic judgments; 2) Test contrastive weighting ablation to measure its contribution to performance gains; 3) Compare training efficiency with different frequency of log-probability computation.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several implications emerge from the work. The effectiveness of CLIP-based similarity metrics for capturing human perceptual preferences remains uncertain, particularly for abstract or culturally-specific concepts. The sensitivity of the method to training data characteristics, including dataset size and preference pair quality, requires further investigation. Additionally, the computational overhead of CLIP-based calculations at each timestep raises questions about scalability to larger models or longer prompts.

## Limitations

- Reliance on CLIP embeddings may not fully capture human perceptual preferences, especially for abstract or culturally-specific concepts
- Performance depends heavily on the quality and diversity of training preference pairs, with no analysis of dataset sensitivity
- Computational overhead of computing CLIP embeddings at each denoising step remains unquantified, potentially limiting scalability

## Confidence

High confidence: Superior performance in human preference alignment tasks (supported by empirical reward model scores and win rates)
Medium confidence: Better capture of fine details while preserving style (partly based on subjective visual inspection)
High confidence: Introduction of a new style alignment evaluation task that addresses limitations of current human preference metrics

## Next Checks

1) Conduct ablation studies on the contrastive weighting mechanism to isolate its contribution to performance gains
2) Measure and report the computational overhead of CLIP-based similarity calculations at each timestep
3) Test the method's robustness across different dataset sizes and quality levels to establish sensitivity to training data characteristics