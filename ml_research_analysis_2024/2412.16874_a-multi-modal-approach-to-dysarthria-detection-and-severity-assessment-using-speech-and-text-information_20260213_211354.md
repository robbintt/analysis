---
ver: rpa2
title: A Multi-modal Approach to Dysarthria Detection and Severity Assessment Using
  Speech and Text Information
arxiv_id: '2412.16874'
source_url: https://arxiv.org/abs/2412.16874
tags:
- speech
- severity
- assessment
- detection
- dysarthric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multi-modal framework that combines speech
  and text modalities to improve dysarthria detection and severity assessment. The
  approach uses cross-attention mechanisms to capture pronunciation deviations by
  comparing acoustic speech features with text-based linguistic references.
---

# A Multi-modal Approach to Dysarthria Detection and Severity Assessment Using Speech and Text Information

## Quick Facts
- **arXiv ID**: 2412.16874
- **Source URL**: https://arxiv.org/abs/2412.16874
- **Reference count**: 23
- **Primary result**: Multi-modal framework combining speech and text achieves 99.53% detection accuracy and 98.12% severity assessment accuracy in speaker-dependent settings

## Executive Summary
This paper presents a novel multi-modal framework for dysarthria detection and severity assessment that combines acoustic speech features with text-based linguistic information. The approach uses cross-attention mechanisms to capture pronunciation deviations by comparing speech representations with corresponding text transcriptions. Evaluated on the UA-Speech dysarthric database, the model significantly outperforms speech-only baselines, particularly in speaker-dependent settings. The integration of text modality provides crucial linguistic context that enhances the model's ability to assess severity across different word groups and speaker conditions.

## Method Summary
The framework employs a two-stream architecture with separate speech and text encoders that process mel-spectrogram features and character-level embeddings respectively. A cross-attention mechanism captures dependencies between acoustic and linguistic representations, enabling the model to detect pronunciation deviations characteristic of dysarthria. The system was evaluated across multiple settings: speaker-dependent, speaker-independent with seen words, and speaker-independent with unseen words. Training utilized the Adam optimizer with ReduceOnPlateau learning rate scheduling and early stopping to prevent overfitting. Performance was benchmarked against speech-only baselines to quantify the contribution of the text modality.

## Key Results
- Speaker-dependent detection accuracy: 99.53% (vs. 97.25% speech-only baseline)
- Speaker-dependent severity assessment accuracy: 98.12% (vs. 89.65% speech-only baseline)
- Speaker-independent detection accuracy (unseen words): 93.20% (vs. 91.40% speech-only baseline)
- Speaker-independent severity assessment accuracy (unseen words): 51.97% (vs. 42.70% speech-only baseline)

## Why This Works (Mechanism)
The multi-modal approach works by leveraging the complementary information available in speech acoustics and text transcriptions. Speech features capture pronunciation deviations, timing irregularities, and acoustic distortions characteristic of dysarthria, while text features provide linguistic context and expected pronunciation patterns. The cross-attention mechanism enables the model to identify discrepancies between what was said and how it was said, which is particularly valuable for detecting subtle dysarthric symptoms that might be missed by unimodal approaches.

## Foundational Learning
- **Cross-attention mechanisms**: Learn to align and compare representations from different modalities by computing attention weights between speech and text features
  - *Why needed*: Enables the model to identify pronunciation deviations by comparing acoustic features with linguistic expectations
  - *Quick check*: Verify attention weights highlight relevant acoustic-text pairs corresponding to known dysarthric symptoms

- **Speaker-dependent vs. speaker-independent evaluation**: Speaker-dependent settings use data from the same speakers for training and testing, while speaker-independent settings evaluate generalization to new speakers
  - *Why needed*: Speaker-dependent settings assess model performance on known speakers, while speaker-independent settings test real-world applicability to unseen speakers
  - *Quick check*: Compare performance drop between speaker-dependent and speaker-independent settings to assess generalization capability

- **Severity assessment in dysarthria**: Evaluates the degree of speech impairment across multiple levels rather than binary detection
  - *Why needed*: Clinical applications require quantifying impairment severity for treatment planning and progress monitoring
  - *Quick check*: Analyze confusion matrices to identify which severity levels are most challenging for the model

## Architecture Onboarding
**Component Map**: Speech Encoder (CNN + Bi-GRU) -> Text Encoder (Embedding + Bi-GRU) -> Cross-Attention -> Classification Layers

**Critical Path**: Speech features and text embeddings are processed independently through their respective encoders, then combined via cross-attention to produce joint representations for classification.

**Design Tradeoffs**: The architecture balances computational efficiency with modeling capacity by using CNN layers for local acoustic feature extraction and Bi-GRU layers for sequential modeling. The cross-attention mechanism adds complexity but enables fine-grained alignment between modalities.

**Failure Signatures**: Poor performance on unseen words suggests insufficient generalization to novel speaker or word combinations. Overfitting manifests as large performance gaps between speaker-dependent and speaker-independent settings. Low severity assessment accuracy indicates difficulty in distinguishing between closely related impairment levels.

**First Experiments**: 1) Train speech-only baseline to establish unimodal performance floor, 2) Evaluate cross-attention weight distributions to verify meaningful modality alignment, 3) Test model on held-out speaker subsets to assess speaker generalization before full speaker-independent evaluation.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions but leaves several important areas for future investigation. The substantial performance gap between detection (near-perfect) and severity assessment (significantly lower) accuracy suggests the latter remains a more challenging problem. The approach's dependence on text transcriptions raises questions about applicability in scenarios where text is unavailable or the speaker is using novel words. Additionally, the model's performance characteristics across different dysarthria types and severity levels warrant further exploration to understand its clinical utility and limitations.

## Limitations
- Performance degrades significantly on unseen words in speaker-independent settings, dropping severity assessment accuracy from 98.12% to 51.97%
- Limited evaluation on a single English database raises questions about generalization to other languages and dysarthric populations
- The approach requires text transcriptions as input, limiting applicability in scenarios without linguistic context

## Confidence
- **Detection accuracy (speaker-dependent)**: High (95-99%) - near-perfect scores suggest strong performance but raise potential overfitting concerns
- **Severity assessment accuracy (speaker-dependent)**: Medium (75-85%) - substantial improvement over baseline but still shows room for refinement
- **Severity assessment accuracy (speaker-independent, unseen words)**: Low (50-65%) - significant degradation indicates challenges with generalization to novel speaker-word combinations

## Next Checks
1. Conduct ablation studies removing the text modality to quantify its exact contribution to both detection and severity assessment performance
2. Evaluate the model on additional dysarthric speech datasets (e.g., TORGO, Nemours) to verify cross-database generalization beyond UA-Speech
3. Perform statistical significance testing between multi-modal and speech-only baselines to confirm performance improvements are meaningful rather than due to chance