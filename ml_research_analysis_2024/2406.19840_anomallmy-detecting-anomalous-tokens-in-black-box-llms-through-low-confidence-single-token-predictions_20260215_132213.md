---
ver: rpa2
title: AnomaLLMy -- Detecting anomalous tokens in black-box LLMs through low-confidence
  single-token predictions
arxiv_id: '2406.19840'
source_url: https://arxiv.org/abs/2406.19840
tags: []
core_contribution: This work presents AnomaLLMy, a novel technique for detecting anomalous
  tokens in black-box Large Language Models using only API access. The method leverages
  low-confidence single-token predictions, defined by high entropy, high tail probability,
  or low probability differences between top predictions, as cost-effective indicators
  of model irregularities.
---

# AnomaLLMy -- Detecting anomalous tokens in black-box LLMs through low-confidence single-token predictions

## Quick Facts
- arXiv ID: 2406.19840
- Source URL: https://arxiv.org/abs/2406.19840
- Authors: WaligÃ³ra Witold
- Reference count: 1
- This work presents AnomaLLMy, a novel technique for detecting anomalous tokens in black-box Large Language Models using only API access.

## Executive Summary
AnomaLLMy introduces a cost-effective method for detecting anomalous tokens in black-box LLMs using only API access and low-confidence single-token predictions. The approach identifies tokens that cause high variance in predictions, API errors, or schema violations by analyzing entropy, tail probability, and probability differences between top predictions. Applied to GPT-4's cl100k_base token set, the method identified 413 major and 65 minor anomalies at a total cost of $24.39 in API credits, with 1143 false positives. The technique demonstrates effectiveness in highlighting gaps in model training data and offers potential mitigation strategies for applications relying on LLMs.

## Method Summary
AnomaLLMy detects anomalous tokens by analyzing single-token predictions at temperature 0.0, identifying low-confidence predictions through entropy > 1.0, tail probability > 0.1, or probability difference < 50% between top predictions. The method uses API-only access with top-N log-probabilities, making it universally applicable without requiring embedding knowledge. Anomalies are confirmed through 10 repetitions at temperature 1.0 and classified as major (majority wrong predictions) or minor (occasional wrong predictions). The approach leverages the observation that tokens consistently producing high-entropy or low-confidence predictions likely indicate training gaps in the model.

## Key Results
- Identified 413 major and 65 minor anomalies in GPT-4's cl100k_base token set
- Total API cost of $24.39 for comprehensive anomaly detection
- 1143 false positives among 478 total anomalies (29.1% false positive rate)
- Anomalies include tokens causing high variance, API errors, and schema violations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Low-confidence single-token predictions reliably indicate anomalous tokens
- Mechanism: When a token consistently produces high entropy or low probability differences between top predictions during repetition tasks, it suggests the model lacks clear training data for that token
- Core assumption: Repetition tasks should produce highly confident predictions for most tokens, so deviations indicate training gaps
- Evidence anchors: [abstract] "Utilizing low-confidence single-token predictions as a cost-effective indicator, AnomaLLMy identifies irregularities in model behavior"
- Break condition: If the model architecture changes such that repetition tasks no longer produce high-confidence predictions by default, the baseline would shift and invalidate the detection criteria

### Mechanism 2
- Claim: API-only access can detect anomalies without embedding knowledge
- Mechanism: By analyzing log-probabilities of top predictions through standard completion APIs, the method bypasses the need for model architecture details or embedding matrices
- Core assumption: The log-probabilities returned by standard APIs contain sufficient information to identify anomalous behavior
- Evidence anchors: [abstract] "Using API-only access and utilizing low-confidence single-token predictions as a cost-effective indicator"
- Break condition: If API providers change the log-probability format or limit access to top-N probabilities, the method would lose its detection capability

### Mechanism 3
- Claim: Cost-effective anomaly detection through single-token predictions
- Mechanism: Using single-token completion at temperature 0.0 with logprobs enabled provides maximum cost efficiency while maintaining detection accuracy
- Core assumption: Single-token predictions at temperature 0.0 minimize API costs while still revealing confidence patterns
- Evidence anchors: [abstract] "demonstrating the method's efficiency with just $24.39 spent in API credits"
- Break condition: If token prediction costs increase significantly or if multi-token context becomes necessary for anomaly detection, the cost advantage would diminish

## Foundational Learning

- Concept: Tokenization and vocabulary mapping in LLMs
  - Why needed here: Understanding how strings map to token IDs is crucial for interpreting which anomalous tokens correspond to which strings
  - Quick check question: If the string "atrigesimal" maps to token 43587, what would happen if you add a space before it?
- Concept: Probability distributions and entropy in prediction
  - Why needed here: The detection relies on interpreting entropy and probability differences as indicators of model confidence
  - Quick check question: What entropy value would indicate a completely uniform distribution across 5 predictions?
- Concept: API rate limiting and cost optimization
  - Why needed here: The methodology explicitly accounts for API constraints and budget limitations
  - Quick check question: If an API allows 20 requests per minute and you need to scan 100,000 tokens, how long will the initial scan take?

## Architecture Onboarding

- Component map: Data collection -> Analysis pipeline -> Confirmation phase -> Classification -> Manual investigation
- Critical path: 1. Collect log-probabilities for all tokens in target vocabulary, 2. Identify candidates using entropy > 1.0 OR tail probability > 0.1 OR probability difference < 50%, 3. Confirm anomalies through 10 repetitions at temperature 1.0, 4. Classify as major/minor based on off-target prediction frequency, 5. Investigate manually with EXPLAIN prompt
- Design tradeoffs: Single-token vs multi-token (single-token is more cost-effective but may miss context-dependent anomalies), Temperature 0.0 vs higher (temperature 0.0 maximizes confidence detection but may miss temperature-sensitive issues), API-only vs embedding access (API-only is more universally applicable but less precise than embedding-based methods)
- Failure signatures: High false positive rate (>50%) suggests criteria too loose or model behavior inconsistent, No anomalies found suggests criteria too strict or dataset already well-trained, API errors during confirmation suggest severe anomalies affecting model stability
- First 3 experiments: 1. Test the baseline: Run REPEAT prompt on 100 random tokens to verify average confidence metrics (should be ~97.85% top prediction), 2. Validate detection criteria: Run on a small known set of anomalous tokens to confirm detection rates, 3. Cost optimization: Test whether reducing logprobs parameter affects detection accuracy to optimize API costs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do anomalous tokens impact model accuracy in more realistic use cases beyond single-token predictions?
- Basis in paper: [explicit] The paper demonstrates that anomalous tokens significantly affect model responses when used in code examples, causing the model to substitute incorrect function names multiple times while still arriving at the correct answer.
- Why unresolved: The study only tested one specific code example. The impact on other domains like natural language processing, RAG applications, or different programming languages remains unknown.
- What evidence would resolve it: Systematic testing of anomalous tokens across multiple domains and application types, measuring accuracy degradation and variance in outputs compared to normal tokens.

### Open Question 2
- Question: What is the exact source of anomalous tokens in GPT-4's training data?
- Basis in paper: [inferred] The paper suggests that anomalous tokens likely originated from programming resources that were present during tokenizer training but removed during later model training stages, based on observations that many anomalous tokens correspond to specific programming libraries and frameworks.
- Why unresolved: The model is proprietary and the training data is not publicly available. The paper can only make educated guesses based on internet searches of token strings.
- What evidence would resolve it: Access to GPT-4's training data pipeline documentation or analysis of the specific datasets used for tokenizer training versus model training would confirm whether programming resources were removed between stages.

### Open Question 3
- Question: Can low-confidence prediction detection be effectively applied to evaluate safety training efficacy in LLMs?
- Basis in paper: [explicit] The paper suggests this as a future research direction, noting that low-confidence predictions could be used in other areas of LLM development beyond anomaly detection.
- Why unresolved: The paper only demonstrates the method for detecting anomalous tokens and does not explore its application to safety evaluation or other development aspects.
- What evidence would resolve it: Empirical studies applying the low-confidence detection methodology to safety training scenarios, measuring whether it can identify gaps in safety training similar to how it identifies gaps in general training data.

## Limitations

- Temporal validity: Findings tied to specific GPT-4 model version (1106) may not generalize to future updates
- Generalizability: Method validated only on GPT-4's cl100k_base tokenizer, performance on other tokenizers unknown
- False positive rate: 29.1% false positive rate suggests detection criteria may need refinement for better precision

## Confidence

**High Confidence**: The core mechanism of using low-confidence single-token predictions as anomaly indicators is well-supported through quantitative metrics and qualitative analysis.

**Medium Confidence**: Cost-effectiveness claim ($24.39) is supported but may not generalize across different model versions or token sets.

**Low Confidence**: Generalizability to other LLMs, tokenizers, or domains beyond the tested GPT-4 cl100k_base is not established.

## Next Checks

1. **Cross-Model Validation**: Apply AnomaLLMy to at least two other LLM architectures (e.g., Claude, Llama) using their respective tokenizers to test whether entropy-based anomaly detection generalizes beyond GPT-4.

2. **Temporal Stability Test**: Re-run the complete detection pipeline on GPT-4 after a major model update (e.g., GPT-4 Turbo) to quantify how many previously identified anomalies persist versus being resolved.

3. **False Positive Reduction Experiment**: Implement a post-processing filter that groups tokens with high string similarity (e.g., "token" vs " token" vs "Token") and treats them as potential duplicates to reduce the false positive rate from 29.1%.