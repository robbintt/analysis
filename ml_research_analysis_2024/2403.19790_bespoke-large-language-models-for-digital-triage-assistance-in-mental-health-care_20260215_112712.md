---
ver: rpa2
title: Bespoke Large Language Models for Digital Triage Assistance in Mental Health
  Care
arxiv_id: '2403.19790'
source_url: https://arxiv.org/abs/2403.19790
tags:
- clinical
- referral
- team
- data
- patient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a bespoke large language model (LLM) to assist
  clinicians in triaging patients referred to UK NHS mental health services. The approach
  ingests variable-length clinical notes from electronic health records and recommends
  the most suitable specialist team (e.g., eating disorders, psychosis, intellectual
  disability).
---

# Bespoke Large Language Models for Digital Triage Assistance in Mental Health Care

## Quick Facts
- arXiv ID: 2403.19790
- Source URL: https://arxiv.org/abs/2403.19790
- Reference count: 40
- Primary result: Segment-and-batch approach with clinically fine-tuned RoBERTa achieves F1 score of 0.938 and accuracy of 0.981 on 5-class mental health triage task

## Executive Summary
This paper develops a bespoke large language model to assist clinicians in triaging patients referred to UK NHS mental health services. The approach processes variable-length clinical notes from electronic health records to recommend appropriate specialist teams (eating disorders, psychosis, intellectual disability, etc.). Three methods for handling variable-length EHR text are evaluated, with the segment-and-batch approach using LoRA adapter training achieving the best performance. The model demonstrates strong clinical performance while being practical to deploy on a single GPU, making it suitable for resource-limited healthcare environments.

## Method Summary
The approach ingests variable-length clinical notes from electronic health records and recommends the most suitable specialist team. Three methods for processing variable-length EHR text are evaluated: document-level, concatenated instance-level, and segment-and-batch with LoRA adapter training. The segment-and-batch approach with a clinically fine-tuned RoBERTa model achieves the best performance. The model was trained and deployed on a single GPU, making it practical for resource-limited healthcare environments.

## Key Results
- Segment-and-batch approach with clinically fine-tuned RoBERTa achieves F1 score of 0.938
- Model achieves accuracy of 0.981 on 5-class triage task
- Single GPU training and deployment demonstrates practical resource efficiency

## Why This Works (Mechanism)
The segment-and-batch approach with LoRA adapter training works effectively because it balances context preservation with computational efficiency. By breaking down variable-length clinical notes into manageable segments, the model can process complex EHR text without exceeding memory constraints. The LoRA adapters enable efficient fine-tuning of the clinically pre-trained RoBERTa model, allowing task-specific adaptation without full model retraining. This architecture leverages the pre-existing clinical knowledge in the RoBERTa model while adding the ability to make specialized triage decisions for mental health referrals.

## Foundational Learning
- Clinical triage protocols: Why needed - ensures model recommendations align with established clinical pathways; Quick check - validate model outputs against current NHS triage guidelines
- Electronic health record structure: Why needed - understands variable-length text patterns in clinical notes; Quick check - analyze distribution of note lengths across different patient cohorts
- Mental health specialty classification: Why needed - defines the target classes for model training; Quick check - verify class distribution matches real-world referral patterns

## Architecture Onboarding
Component map: Clinical notes -> Text segmentation -> LoRA adapter training -> Specialist team classification
Critical path: Raw clinical text is segmented into manageable chunks, processed through a fine-tuned RoBERTa model with LoRA adapters, and classified into one of five specialist teams
Design tradeoffs: Segment-and-batch approach balances computational efficiency with context preservation, while LoRA adapters enable task-specific fine-tuning without full model retraining
Failure signatures: Model may struggle with ambiguous clinical presentations, incomplete documentation, or emerging mental health conditions not represented in training data
First experiments: 1) Evaluate model performance on progressively longer clinical notes, 2) Test robustness to missing or incomplete clinical information, 3) Compare performance across different specialist team distributions

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset composition unclear regarding patient diversity and referral patterns
- Limited to UK NHS context without cross-system validation
- Addresses only 5 specialist teams, representing a narrow scope of mental health conditions
- Model performance on rare or complex cases not thoroughly evaluated
- Potential bias in training data not fully explored

## Confidence
- Clinical performance metrics (F1=0.938, accuracy=0.981): High confidence
- Resource efficiency (single GPU training): Medium confidence
- Generalizability to other healthcare systems: Low confidence
- Clinical utility in real-world deployment: Medium confidence
- Handling of rare or complex cases: Low confidence

## Next Checks
1. External validation on datasets from multiple healthcare systems to assess cross-context performance
2. Expanded multi-class evaluation including broader range of mental health specialties and conditions
3. Real-world deployment study measuring impact on clinician workflow efficiency and patient outcomes
4. Analysis of model performance on rare or complex clinical presentations
5. Investigation of potential biases in training data and their impact on model predictions