---
ver: rpa2
title: 'YOSO: You-Only-Sample-Once via Compressed Sensing for Graph Neural Network
  Training'
arxiv_id: '2411.05693'
source_url: https://arxiv.org/abs/2411.05693
tags:
- sampling
- training
- yoso
- where
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: YOSO introduces a compressed sensing-based approach to GNN training,
  addressing the high sampling overhead in large-scale applications. By sampling nodes
  once at the input layer and performing lossless reconstruction at the output layer,
  YOSO reduces training time by 75% on average compared to state-of-the-art methods
  while maintaining accuracy.
---

# YOSO: You-Only-Sample-Once via Compressed Sensing for Graph Neural Network Training

## Quick Facts
- **arXiv ID**: 2411.05693
- **Source URL**: https://arxiv.org/abs/2411.05693
- **Reference count**: 39
- **Primary result**: Reduces GNN training time by 75% while maintaining accuracy through one-time node sampling and compressed sensing reconstruction

## Executive Summary
YOSO addresses the high sampling overhead in large-scale GNN training by introducing a compressed sensing-based framework that samples nodes only once at the input layer and performs lossless reconstruction at the output layer. The method achieves 75% reduction in training time while maintaining accuracy equivalent to full node participation. By treating the orthonormal basis as a learnable parameter integrated with the loss function, YOSO eliminates costly pre-computations and ensures high-probability accuracy retention across various graph neural network tasks.

## Method Summary
YOSO implements a one-time sampling strategy at the input layer using a graph-structure-based sampling matrix, followed by forward propagation in the sparse domain and reconstruction at the output layer. The method constructs a universal sampling matrix Φ using eigenvalue-weighted node sampling probabilities and random matrices, then jointly optimizes the orthonormal basis U, sparse embeddings, and GNN parameters through a combined loss function. This approach transforms the high-dimensional node embedding matrix into a much smaller matrix for computation while maintaining reconstruction accuracy through compressed sensing theory.

## Key Results
- Achieves 75% reduction in training time compared to state-of-the-art sampling methods
- Maintains model accuracy within 0.5% of full-node training across node classification and link prediction tasks
- Eliminates costly orthonormal basis computations by treating U as a learnable parameter integrated with the loss function

## Why This Works (Mechanism)

### Mechanism 1
- Claim: YOSO achieves 75% reduction in training time by sampling nodes only once at the input layer and reconstructing embeddings at the output layer.
- Mechanism: Compressed sensing transforms the high-dimensional node embedding matrix into a sparse domain where only M << N nodes are sampled. The reconstruction at the output layer recovers the full embedding matrix with high probability, maintaining accuracy equivalent to full node participation.
- Core assumption: The orthonormal basis U exists such that H(l) = U(l)Ĥ(l) where Ĥ(l) is sparse, and the sampling matrix Φ satisfies the Restricted Isometry Property (RIP).
- Evidence anchors:
  - [abstract]: "YOSO introduces a compressed sensing (CS)-based sampling and reconstruction framework, where nodes are sampled once at input layer, followed by a lossless reconstruction at the output layer per epoch."
  - [section]: "CS reduces the amount of data required for computation by transforming H(l) ∈ RN×d into a much smaller matrix T ∈ RM×d since M ≪ N."
  - [corpus]: No direct evidence found for GNN-specific CS applications in corpus; this appears to be novel application.
- Break condition: If the graph structure does not support sparse representation in any orthonormal basis, or if RIP cannot be satisfied with available sampling size M.

### Mechanism 2
- Claim: YOSO eliminates the need for expensive orthonormal basis computations by treating U as an optimization target integrated with the GNN's loss function.
- Mechanism: Instead of pre-computing U(l) for each layer through costly matrix decompositions, YOSO jointly optimizes U along with the GNN parameters and reconstruction target Ĥ(L) using the combined loss function L = αLrecon + βLGNN.
- Core assumption: U can be learned through gradient descent without explicit computation, and the optimization landscape allows convergence to a suitable orthonormal basis.
- Evidence anchors:
  - [section]: "To address the challenge of unknown U, we treat U as an optimization target. Using Equation (8), we obtain a total loss, which is then used to generate gradients for updating U through all training process."
  - [abstract]: "By integrating the reconstruction process with the loss function of specific learning tasks, YOSO not only avoids costly computations in traditional compressed sensing (CS) methods, such as orthonormal basis calculations, but also ensures high-probability accuracy retention."
  - [corpus]: No direct evidence found for learning orthonormal bases in GNNs; this appears to be novel approach.
- Break condition: If the joint optimization becomes unstable or converges to a non-orthonormal basis that fails to satisfy RIP conditions.

### Mechanism 3
- Claim: The universal sampling matrix Φ is constructed using graph structure-based sampling probability distribution derived from eigenvalues of the normalized Laplacian matrix.
- Mechanism: The sampling probability P(i) = λi/Σλj assigns higher sampling probability to nodes with larger eigenvalues, which correspond to more influential nodes. The random matrix Σ captures contribution levels of sampled nodes to reconstruction.
- Core assumption: Eigenvalues of the normalized Laplacian matrix effectively capture node importance for GNN message passing, and the combined sampling matrix Φ = Ŝ ⊗ Σ satisfies RIP.
- Evidence anchors:
  - [section]: "For the sampling matrix Φ, it is essential to be row full rank. Intuitively, Φ serves to linearly combine the features or embeddings of nodes according to weights corresponding to the indices of the support."
  - [abstract]: "YOSO introduces a compressed sensing (CS)-based sampling and reconstruction framework, where nodes are sampled once at input layer, followed by a lossless reconstruction at the output layer per epoch."
  - [corpus]: No direct evidence found for this specific graph-structure-based sampling approach in GNNs; appears to be novel method.
- Break condition: If the eigenvalue-based sampling fails to capture essential structural information or if Φ cannot be constructed to satisfy RIP conditions.

## Foundational Learning

- Concept: Compressed Sensing and Restricted Isometry Property
  - Why needed here: YOSO's core efficiency gain relies on compressed sensing theory to enable sampling from M << N nodes while maintaining reconstruction accuracy.
  - Quick check question: What mathematical condition must the sampling matrix Φ satisfy to ensure accurate reconstruction of sparse signals?

- Concept: Graph Neural Networks and Message Passing
  - Why needed here: Understanding how GNNs propagate information through graph structures is essential to grasp why sampling at the input layer and reconstructing at the output layer can maintain accuracy.
  - Quick check question: How does the normalized Laplacian matrix relate to the message passing process in GCNs?

- Concept: Optimization with Orthonormal Constraints
  - Why needed here: YOSO treats the orthonormal basis U as a learnable parameter, requiring knowledge of optimization techniques that maintain orthogonality constraints.
  - Quick check question: What mathematical constraint must be enforced to ensure that the learned matrix U remains orthonormal during gradient descent?

## Architecture Onboarding

- Component map:
  - Input Layer: Original graph features X, normalized Laplacian Â
  - Sampling Module: One-time sampling matrix Φ construction, initial sparse transformation T(0) = ΦÛX
  - Forward Propagation: Layer-wise transformations with sampling at input only, reconstruction at output
  - Reconstruction Module: Joint optimization of U, Ĥ(L), and GNN parameters using combined loss function
  - Output Layer: Reconstructed embeddings ˜H(L) for downstream tasks

- Critical path:
  1. Pre-processing: Construct sampling matrix Φ using eigenvalue-based probability distribution
  2. Training loop: Forward propagation with one-time sampling, joint optimization with reconstruction loss
  3. Output: Reconstructed embeddings for prediction tasks

- Design tradeoffs:
  - Sampling size M vs. accuracy: Larger M improves reconstruction quality but increases computation time
  - Joint optimization complexity vs. pre-computed U: Learning U adds optimization overhead but eliminates pre-computation costs
  - Reconstruction at output only vs. intermediate layers: Output-only reconstruction is more efficient but may introduce bounded accuracy loss

- Failure signatures:
  - Training instability: Oscillating loss curves or divergence during joint optimization of U
  - Poor reconstruction: Large difference between reconstructed and original embeddings, visible in heatmap analysis
  - Accuracy degradation: Significant drop in model performance compared to full sampling baselines

- First 3 experiments:
  1. Verify basic functionality: Run YOSO on a small graph dataset (e.g., Cora) with M=128 and compare accuracy to vanilla GCN
  2. Sampling efficiency test: Measure sampling time reduction across different sampling sizes M while monitoring accuracy impact
  3. Reconstruction quality analysis: Visualize reconstruction error heatmaps for different M values on a validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal sampling size M that balances reconstruction quality and computational efficiency across different graph datasets?
- Basis in paper: [explicit] The paper discusses varying sampling size M and its impact on model accuracy and training time, noting that "beyond a certain point, such as M = 512, further increases in M offer diminishing returns in both reconstruction quality and model accuracy."
- Why unresolved: The paper shows empirical results for specific datasets but does not provide a universal formula or method to determine the optimal M for arbitrary graphs.
- What evidence would resolve it: A theoretical framework or empirical study across diverse graph types showing the relationship between M, graph properties (size, density, spectral properties), and reconstruction quality would provide clearer guidance.

### Open Question 2
- Question: How does the performance of YOSO scale with deeper GNN architectures (more than two layers)?
- Basis in paper: [explicit] The paper mentions that "the error between ˜H(L) and H(L) can be bounded as ∥ ˜H(L) − H(L)∥F ≤ (Lσ/(1−δk))L ∥E∥F" where L is the number of layers, indicating potential error accumulation.
- Why unresolved: All experiments were conducted using a two-layer GNN, and while error bounds are provided, empirical validation of these bounds for deeper architectures is missing.
- What evidence would resolve it: Experimental results showing training time, accuracy, and error bounds for YOSO with varying numbers of layers (3, 5, 10, etc.) on benchmark datasets would clarify scalability.

### Open Question 3
- Question: Can the universal sampling matrix Φ be designed to be more adaptive to the graph structure without requiring the orthonormal basis U?
- Basis in paper: [inferred] The paper discusses the challenge of working with an unknown U and proposes a universal sampling matrix Φ = ˆS ⊗ Σ based on graph structure, but suggests there is room for improvement in Φ design.
- Why unresolved: The current design of Φ relies on random matrices and graph structure but does not fully leverage the graph's spectral properties or adapt to different graph types.
- What evidence would resolve it: Developing and testing alternative designs for Φ that incorporate graph-specific information (e.g., community structure, node centrality) and comparing their performance to the current approach would provide insights.

### Open Question 4
- Question: How does YOSO perform on dynamic graphs where the structure changes over time?
- Basis in paper: [inferred] The paper focuses on static graphs, and the sampling matrix Φ is determined only once during preprocessing, which may not adapt well to changes in graph structure.
- Why unresolved: The methodology and experiments do not address scenarios where edges or nodes are added or removed, which is common in real-world applications.
- What evidence would resolve it: Evaluating YOSO on dynamic graph datasets and developing methods to update the sampling matrix or orthonormal basis U over time would demonstrate its applicability to evolving networks.

## Limitations
- Compressed sensing theory application to GNNs is novel but lacks established theoretical guarantees specific to graph structures
- Joint optimization of orthonormal basis U with GNN parameters may face convergence challenges not addressed in the paper
- Eigenvalue-based sampling probability assumes Laplacian eigenvalues effectively capture node importance for all graph types

## Confidence
- **High Confidence**: The 75% training time reduction claim is well-supported by experimental results across multiple datasets
- **Medium Confidence**: The accuracy preservation claim relies on compressed sensing theory but lacks specific RIP verification for the proposed sampling matrices
- **Low Confidence**: The joint optimization approach for learning orthonormal bases is innovative but has limited theoretical grounding in existing literature

## Next Checks
1. **RIP Condition Verification**: Implement theoretical checks to verify that the proposed sampling matrix Φ satisfies the Restricted Isometry Property with appropriate parameters for each dataset
2. **Convergence Analysis**: Conduct ablation studies on the joint optimization of U, examining stability across different learning rates and initialization strategies
3. **Generalization Testing**: Evaluate YOSO on heterogeneous graph types (e.g., temporal graphs, graphs with varying node degrees) to assess robustness of the eigenvalue-based sampling approach