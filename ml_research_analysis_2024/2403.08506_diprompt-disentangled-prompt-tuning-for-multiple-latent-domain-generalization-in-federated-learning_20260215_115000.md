---
ver: rpa2
title: 'DiPrompT: Disentangled Prompt Tuning for Multiple Latent Domain Generalization
  in Federated Learning'
arxiv_id: '2403.08506'
source_url: https://arxiv.org/abs/2403.08506
tags:
- domain
- prompt
- learning
- clients
- diprompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'DiPrompT addresses federated domain generalization under realistic
  constraints where the number of clients exceeds the number of source domains and
  domain labels are unavailable. It proposes a disentangled prompt tuning framework
  with two prompt types: a global prompt for domain-invariant knowledge and domain-specific
  prompts for capturing domain-specific features.'
---

# DiPrompT: Disentangled Prompt Tuning for Multiple Latent Domain Generalization in Federated Learning

## Quick Facts
- arXiv ID: 2403.08506
- Source URL: https://arxiv.org/abs/2403.08506
- Authors: Sikai Bai; Jie Zhang; Shuaicheng Li; Song Guo; Jingcai Guo; Jun Hou; Tao Han; Xiaocheng Lu
- Reference count: 40
- One-line primary result: DiPrompT achieves 1.65-2.98% average accuracy improvements over state-of-the-art methods while reducing communication overhead by up to 830x

## Executive Summary
DiPrompT addresses federated domain generalization under realistic constraints where the number of clients exceeds the number of source domains and domain labels are unavailable. It proposes a disentangled prompt tuning framework with two prompt types: a global prompt for domain-invariant knowledge and domain-specific prompts for capturing domain-specific features. An adaptive query mechanism automatically discovers domain labels without manual annotation by leveraging text-image alignment. During inference, a collaborative ensemble metric dynamically combines general and specific knowledge based on feature distances to the target domain.

## Method Summary
DiPrompT uses disentangled prompt tuning in federated learning settings where domain labels are unavailable (latent domains). The framework employs a global prompt (G-Prompt) for domain-invariant features and domain-specific prompts (D-Prompts) for domain characteristics. An adaptive query mechanism (Q-Prompt) automatically discovers domain labels through text-image alignment in CLIP models. During inference, a collaborative ensemble metric combines knowledge sources based on feature distances. The method trains locally on clients and aggregates using beta momentum averaging for D-Prompts and weighted aggregation for G-Prompt.

## Key Results
- Achieves 1.65-2.98% average accuracy improvements over state-of-the-art federated and centralized methods
- Reduces communication overhead by up to 830x compared to FedADG
- Maintains robust performance across few-shot settings and different backbone architectures
- Shows only 1% performance drop when client count increases from 3 to 50

## Why This Works (Mechanism)

### Mechanism 1
DiPrompT learns both global and domain-specific knowledge simultaneously, enabling better generalization than methods using only domain-invariant features. The disentangled prompt tuning framework separates knowledge into global prompts for domain-invariant features and domain prompts for domain-specific features. This separation allows the model to capture complementary information that would otherwise be lost when only learning invariant features. The core assumption is that domain-specific features contain valuable information for target domain prediction, especially when the number of clients exceeds the number of source domains.

### Mechanism 2
The adaptive query mechanism (Q-Prompt) enables domain label discovery without manual annotation by leveraging text-image alignment. Q-Prompt uses a learnable prompt shared among all classes and domains to classify input images into categories and then group them into underlying domains. This is achieved through class and domain similarity matching using cosine similarity scores. The core assumption is that domain information can be implicitly captured through text-image alignments in the pre-trained CLIP model, even without explicit domain labels.

### Mechanism 3
The collaborative ensemble metric dynamically combines general and specific knowledge based on feature distances to the target domain. During inference, the ensemble uses weighted combinations of G-Prompt and D-Prompts where weights are determined by the maximum prediction similarity between the target sample and each domain's features. This allows the model to adaptively select which knowledge source is most relevant for each target sample. The core assumption is that feature distance between target samples and source domains can be reliably estimated using cosine similarity between image and text embeddings.

## Foundational Learning

- **Concept: Federated Learning**
  - Why needed here: The paper addresses domain generalization in federated learning settings where data is distributed across multiple clients
  - Quick check question: What is the key difference between centralized learning and federated learning in terms of data access and privacy?

- **Concept: Domain Generalization**
  - Why needed here: The paper aims to improve model performance on unseen target domains by leveraging multiple source domains
  - Quick check question: How does domain generalization differ from domain adaptation in terms of data availability during training?

- **Concept: Prompt Tuning**
  - Why needed here: The paper uses prompt tuning to efficiently adapt pre-trained vision-language models for domain generalization tasks
  - Quick check question: What are the advantages of prompt tuning compared to full fine-tuning of pre-trained models?

## Architecture Onboarding

- **Component map**: Data → Image encoder → Text encoder → Prompt optimization → Beta momentum averaging → Ensemble inference
- **Critical path**: Client-side local training with G-Prompt and D-Prompts optimization → Server-side beta momentum averaging for D-Prompts and weighted aggregation for G-Prompt → Q-Prompt for automatic domain label discovery → Collaborative ensemble metric for combining knowledge sources
- **Design tradeoffs**: 
  - Prompt size vs. model capacity: Larger prompts can capture more information but increase communication costs
  - Number of domain prompts vs. generalization: More domain prompts can capture finer-grained differences but may lead to overfitting
  - Weight coefficient λ: Balancing between global and domain-specific knowledge affects final performance
- **Failure signatures**:
  - Poor performance on target domain: May indicate ineffective domain prompt learning or poor ensemble weighting
  - Client drift in domain prompts: Could result from insufficient beta momentum averaging
  - Communication bottleneck: Large prompt sizes may cause slow aggregation
- **First 3 experiments**:
  1. Verify disentangled prompt learning by comparing performance with and without domain prompts
  2. Test automatic domain label discovery by comparing Q-Prompt performance with ground truth labels
  3. Validate ensemble metric by testing different weighting schemes and measuring target domain accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DiPrompT's performance scale with extremely large numbers of clients (e.g., 1000+) compared to source domains?
- Basis in paper: The paper shows DiPrompT outperforms competitors as client count increases from 3 to 50, with only a 1% performance drop at 50 clients
- Why unresolved: The experiments only tested up to 50 clients, but the paper claims DiPrompT works when "K ≫ M" without testing extreme ratios
- What evidence would resolve it: Experiments testing DiPrompT with 100+ clients and varying ratios of K/M (e.g., 100:4, 500:4, 1000:4) to identify performance ceilings

### Open Question 2
- Question: What is the theoretical relationship between the number of domain prompts (M) and optimal generalization performance?
- Basis in paper: DiPrompT uses domain-specific prompts for each source domain, but doesn't explore whether fewer prompts could capture sufficient domain knowledge
- Why unresolved: The paper assumes one prompt per domain but doesn't investigate if fewer prompts could achieve similar results through clustering or compression
- What evidence would resolve it: Systematic experiments varying the number of domain prompts (fewer than source domains) and comparing performance to baseline with M prompts

### Open Question 3
- Question: How does DiPrompT's Q-Prompt mechanism handle ambiguous cases where images contain features from multiple domains?
- Basis in paper: The Q-Prompt uses two-substep classification (category then domain), but the paper doesn't discuss handling mixed-domain features
- Why unresolved: Real-world data often contains domain-ambiguous features, yet the mechanism assumes clean domain assignment
- What evidence would resolve it: Experiments with deliberately mixed-domain datasets or controlled perturbations showing how Q-Prompt handles ambiguous assignments and whether it degrades gracefully

### Open Question 4
- Question: Does the beta momentum averaging mechanism for D-Prompts provide benefits beyond simple exponential moving averages?
- Basis in paper: The paper claims beta momentum avoids "forgetting domain information" and is superior to exponential moving averages, but doesn't provide comparative experiments
- Why unresolved: The choice of beta momentum over standard EMA is asserted without empirical validation
- What evidence would resolve it: Controlled experiments comparing beta momentum with standard EMA and no averaging, measuring both performance and convergence stability

## Limitations
- The adaptive query mechanism (Q-Prompt) implementation details and domain clustering algorithm are not fully specified
- Evaluation relies on synthetic domain splits from standard datasets rather than real-world federated scenarios
- The collaborative ensemble metric weighting formula requires precise implementation details for faithful reproduction

## Confidence

**High Confidence Claims:**
- Disentangled prompt tuning framework with global and domain-specific prompts
- Performance improvements over state-of-the-art federated methods on benchmark datasets
- Communication efficiency gains (830x reduction compared to FedADG)

**Medium Confidence Claims:**
- Automatic domain label discovery through Q-Prompt mechanism
- Collaborative ensemble metric effectiveness for combining knowledge sources
- Robustness across few-shot settings and different backbone architectures

**Low Confidence Claims:**
- Scalability to large-scale federated scenarios with thousands of clients
- Generalization to domains not represented in the source domain clusters
- Real-world applicability without synthetic domain splits

## Next Checks
1. **Q-Prompt Implementation Validation**: Implement the adaptive query mechanism with synthetic domain labels and verify if the automatic domain discovery matches ground truth labels across multiple random seeds and dataset splits.

2. **Ensemble Weight Sensitivity Analysis**: Test different weighting schemes for the collaborative ensemble metric (e.g., fixed weights, learnable weights, distance-based weights) and measure impact on target domain accuracy to validate the claimed effectiveness.

3. **Communication Efficiency Verification**: Measure actual communication overhead in terms of transmitted parameters per round and verify the claimed 830x reduction compared to FedADG under realistic network conditions and client participation rates.