---
ver: rpa2
title: 'Get Large Language Models Ready to Speak: A Late-fusion Approach for Speech
  Generation'
arxiv_id: '2410.20336'
source_url: https://arxiv.org/abs/2410.20336
tags:
- speech
- text
- arxiv
- language
- llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of extending text-based large
  language models (LLMs) to speech generation tasks. The core idea is to use parameter-efficient
  fine-tuning (PEFT) techniques to adapt a pretrained text LLM for text-to-speech
  (TTS) synthesis, rather than training a speech language model from scratch.
---

# Get Large Language Models Ready to Speak: A Late-fusion Approach for Speech Generation

## Quick Facts
- arXiv ID: 2410.20336
- Source URL: https://arxiv.org/abs/2410.20336
- Reference count: 40
- Primary result: TTS-Llama achieves state-of-the-art TTS performance with MOS scores of 3.07±0.10 for human likeness, while MoLE-Llama mitigates catastrophic forgetting with MMLU accuracy of 54.8%

## Executive Summary
This paper introduces TTS-Llama and MoLE-Llama, approaches that extend text-based large language models (LLMs) to speech generation tasks using parameter-efficient fine-tuning (PEFT) techniques. TTS-Llama adapts a pretrained Llama model to generate high-level semantic tokens from text, which are then converted to speech through an acoustic model and vocoder. Building on this, MoLE-Llama employs a mixture-of-LoRA-experts architecture to create a unified text-and-speech multimodal LLM that retains text capabilities while adding speech generation. The primary results demonstrate state-of-the-art TTS performance and successful mitigation of catastrophic forgetting in multimodal settings.

## Method Summary
The authors propose a two-stage approach: First, they fine-tune Llama 3-8B-Instruct using LoRA adapters to generate semantic tokens from text input. These tokens are processed by an acoustic language model and neural vocoder to produce speech waveforms. Second, they extend this to a multimodal LLM by creating separate LoRA experts for text and speech tasks, then unifying them through a mixture-of-experts architecture with a lightweight router that dynamically selects the appropriate expert for each input. The training uses 50K hours of speech paired with text transcripts and 2M text QA pairs, with evaluation on MOS scores for TTS quality and MMLU accuracy for text QA performance.

## Key Results
- TTS-Llama achieves state-of-the-art TTS performance with MOS scores of 3.07±0.10 for human likeness and 3.01±0.08 for audio quality
- MoLE-Llama successfully mitigates catastrophic forgetting, maintaining MMLU accuracy of 54.8% while adding speech capabilities
- Both systems demonstrate strong zero-shot performance on speech generation tasks without requiring additional fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using LoRA-based parameter-efficient fine-tuning (PEFT) enables a pretrained text LLM to generate high-level semantic tokens without catastrophic forgetting of text capabilities.
- Mechanism: LoRA injects low-rank adapters into the Llama model's weights, allowing the model to learn new token generation patterns (semantic tokens) while freezing most of the original parameters.
- Core assumption: Low-rank decomposition can capture the necessary transformations for speech token generation without requiring full fine-tuning of the entire model.
- Evidence anchors: [abstract] "purely late-fusion parameter-efficient fine-tuning (PEFT)" and [section III] "fine-tune the Llama model using PEFT approach LoRA [17]."

### Mechanism 2
- Claim: The two-step token generation process (semantic tokens → acoustic tokens → waveform) enables high-quality speech synthesis while leveraging the LLM's semantic understanding.
- Mechanism: The fine-tuned Llama model first generates high-level semantic tokens from text input, which capture both semantic and prosodic information. An acoustic language model then translates these semantic tokens into low-level acoustic features, which a neural vocoder converts to waveforms.
- Core assumption: High-level semantic tokens contain sufficient information for acoustic models to reconstruct natural-sounding speech.
- Evidence anchors: [abstract] "TTS-Llama, a TTS system based on a fine-tuned Llama model that generates high-level semantic tokens from text" and [section III] "the fine-tuned Llama model processes raw text input to generate high-level semantic tokens that contains both semantic and prosody information."

### Mechanism 3
- Claim: Mixture-of-LoRA-experts architecture allows unified text-speech multimodal LLMs without catastrophic forgetting by routing tasks to specialized experts.
- Mechanism: After separately fine-tuning text and speech LoRA experts, a lightweight router dynamically selects the appropriate expert for each input. The router is trained on combined text QA and TTS data while keeping all other parameters frozen.
- Core assumption: Task-specific LoRA experts can be effectively merged through routing without interference between modalities.
- Evidence anchors: [abstract] "MoLE-Llama, a text-and-speech multimodal LLM developed through purely late-fusion parameter-efficient fine-tuning (PEFT) and a mixture-of-expert architecture" and [section IV] "we unify the TTS expert and text expert into a single multimodal LLM."

## Foundational Learning

- Concept: Parameter-efficient fine-tuning (PEFT) techniques like LoRA
  - Why needed here: Full fine-tuning of large language models is computationally expensive and risks catastrophic forgetting. PEFT allows adding new capabilities while preserving existing ones.
  - Quick check question: What are the key differences between LoRA and other PEFT methods like prefix tuning or adapters?

- Concept: Speech token representation and discrete acoustic coding
  - Why needed here: Converting continuous speech to discrete tokens enables language modeling approaches to speech tasks. Understanding how semantic and acoustic tokens work together is crucial.
  - Quick check question: How do high-level semantic tokens differ from low-level acoustic tokens in the speech token hierarchy?

- Concept: Mixture-of-experts routing mechanisms
  - Why needed here: The MoLE-Llama architecture requires understanding how to train and deploy expert models with a router that can dynamically select appropriate experts.
  - Quick check question: What are the key considerations when designing a router for mixture-of-experts systems?

## Architecture Onboarding

- Component map: Text input → Fine-tuned Llama → Semantic tokens → Acoustic LM → Acoustic tokens → Vocoder → Speech output
- Critical path: Text → Fine-tuned Llama → Semantic tokens → Acoustic LM → Acoustic tokens → Vocoder → Speech
- Design tradeoffs:
  - LoRA rank selection (128 chosen) balances adaptation capacity vs. parameter efficiency
  - Semantic token vocabulary size (4,096) affects information preservation
  - Router complexity (lightweight MLP) minimizes overhead while maintaining effectiveness
- Failure signatures:
  - Low MOS scores indicating poor speech quality
  - MMLU score drops indicating catastrophic forgetting
  - Router confusion metrics showing poor expert selection
  - Training instability during LoRA fine-tuning
- First 3 experiments:
  1. Verify LoRA fine-tuning on TTS task: Compare zero-shot TTS MOS scores between full fine-tuning vs. LoRA fine-tuning on same dataset
  2. Test catastrophic forgetting: Measure MMLU accuracy after speech LoRA injection, before and after text expert fine-tuning
  3. Validate mixture-of-experts routing: Evaluate router accuracy on task classification and end-to-end performance on mixed text/speech inputs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can TTS-Llama and MoLE-Llama achieve human-level naturalness in speech synthesis, closing the gap between their MOS scores (3.07 and 3.02 respectively) and the ground truth (3.41)?
- Basis in paper: [explicit] The authors note that TTS-Llama achieves state-of-the-art performance but still has a gap compared to ground truth human recordings.
- Why unresolved: The current MOS scores, while competitive, still fall short of human-level naturalness. Further research is needed to understand what aspects of speech generation are still lacking.
- What evidence would resolve it: MOS evaluations showing TTS-Llama and MoLE-Llama matching or exceeding human-level naturalness scores across diverse speech tasks and domains.

### Open Question 2
- Question: Can the catastrophic forgetting problem be fully eliminated in multimodal LLMs like MoLE-Llama, or will there always be some trade-off between modalities?
- Basis in paper: [explicit] The authors demonstrate that MoLE-Llama mitigates catastrophic forgetting compared to TTS-Llama, but there is still a performance gap between MoLE-Llama and the original text-only LLM on some benchmarks.
- Why unresolved: While MoLE-Llama shows improvement, the text QA performance is still below the original text-only LLM. It's unclear if further architectural or training innovations can eliminate this gap entirely.
- What evidence would resolve it: Demonstrations of multimodal LLMs that match or exceed the performance of text-only LLMs on all text-based tasks while maintaining strong performance on speech generation tasks.

### Open Question 3
- Question: Can the mixture-of-LoRA-experts approach be extended to more than two modalities (e.g., text, speech, and vision) without compromising performance in any modality?
- Basis in paper: [inferred] The authors successfully demonstrate the mixture-of-LoRA-experts approach for text and speech modalities, but do not explore additional modalities.
- Why unresolved: The paper focuses on text and speech, leaving open the question of whether this approach can scale to more modalities while maintaining performance.
- What evidence would resolve it: Development and evaluation of multimodal LLMs using mixture-of-LoRA-experts that successfully integrate three or more modalities (e.g., text, speech, and vision) and demonstrate strong performance across all modalities.

## Limitations
- Architecture generalization to diverse speech domains beyond audiobook-style speech remains untested
- Training data biases from LibriHeavy dataset may limit performance on conversational speech and emotional content
- Scaling behavior of the LoRA-based approach at larger model sizes is unexplored

## Confidence
**High Confidence Claims**:
- TTS-Llama achieves competitive MOS scores on evaluated benchmarks (3.07±0.10 for human likeness in zero-shot TTS)
- LoRA-based fine-tuning successfully preserves text capabilities while adding speech generation (MMLU accuracy maintained at 54.8%)
- Mixture-of-LoRA-experts architecture effectively mitigates catastrophic forgetting in multimodal settings

**Medium Confidence Claims**:
- The two-step token generation process (semantic → acoustic → waveform) is optimal for TTS quality
- Semantic tokens contain sufficient information for high-quality speech reconstruction
- The 4,096-token vocabulary size is optimal for balancing information preservation and model efficiency

**Low Confidence Claims**:
- Generalization to speech domains beyond audiobooks and synthesized speech
- Performance on languages other than English
- Long-term stability of the mixture-of-experts routing mechanism

## Next Checks
1. **Domain Transfer Validation**: Evaluate TTS-Llama and MoLE-Llama performance on conversational speech datasets (e.g., Fisher Corpus, Switchboard) and emotional speech datasets to assess generalization beyond audiobook-style speech.

2. **Perceptual Quality Analysis**: Conduct comprehensive human evaluation studies including speaker identity preservation, prosody naturalness in conversational contexts, and robustness to challenging inputs like rare words or complex sentences.

3. **Cross-Lingual Capability Test**: Fine-tune the model on multilingual speech datasets and evaluate cross-lingual performance to assess the architecture's ability to handle multiple languages and code-switching scenarios.