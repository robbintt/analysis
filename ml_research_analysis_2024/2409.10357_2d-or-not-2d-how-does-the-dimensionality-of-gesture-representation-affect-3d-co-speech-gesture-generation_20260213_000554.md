---
ver: rpa2
title: '2D or not 2D: How Does the Dimensionality of Gesture Representation Affect
  3D Co-Speech Gesture Generation?'
arxiv_id: '2409.10357'
source_url: https://arxiv.org/abs/2409.10357
tags:
- gestures
- gesture
- speech
- trimodal
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how the dimensionality of gesture representation\u2014\
  2D versus 3D\u2014affects the performance of co-speech gesture generation models.\
  \ The authors hypothesize that using 2D pose data and then lifting it to 3D with\
  \ a lifting model may introduce inductive biases that impact the quality of generated\
  \ gestures compared to models trained directly on 3D data."
---

# 2D or not 2D: How Does the Dimensionality of Gesture Representation Affect 3D Co-Speech Gesture Generation?

## Quick Facts
- arXiv ID: 2409.10357
- Source URL: https://arxiv.org/abs/2409.10357
- Authors: Téo Guichoux; Laure Soulier; Nicolas Obin; Catherine Pelachaud
- Reference count: 40
- Key outcome: Generating gestures in 3D directly yields more natural and speech-aligned results than training on 2D and lifting to 3D.

## Executive Summary
This paper investigates whether the dimensionality of gesture representation—2D versus 3D—affects the quality of 3D co-speech gesture generation. The authors train two state-of-the-art models (DiffGesture and Trimodal) on both 2D and 3D pose data, then lift 2D outputs to 3D using VideoPose3D. They find that 2D-to-3D lifting introduces inductive biases, leading to less human-like gestures and reduced speech synchrony. User studies confirm that 3D-trained models are perceived as more natural and better synchronized with speech than those lifted from 2D.

## Method Summary
The authors use the TED Gesture-3D dataset, which contains 3D gestures inferred from 2D poses extracted from YouTube videos, paired with speech audio, text, and speaker identity. They adapt DiffGesture (a diffusion model) and Trimodal (a recurrent encoder-decoder) to handle both 2D and 3D inputs by modifying input/output dimensions. After training, 2D-generated gestures are lifted to 3D using VideoPose3D. Results are evaluated using objective metrics (Fréchet Gesture Distance, Beat Consistency, and Diversity) and a large-scale user study comparing perceived human-likeness, aliveness, and speech synchrony.

## Key Results
- 2D-to-3D lifting introduces an inductive bias, collapsing the one-to-many ambiguity of 3D pose from 2D into a single, fixed output, reducing diversity and naturalness.
- 3D-trained models generate gestures that are more human-like, alive, and better synchronized with speech than 2D-trained models with 2D→3D lifting.
- Deterministic lifting introduces temporal smoothing that reduces beat consistency, weakening synchronization with speech.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training on 2D pose data and lifting to 3D introduces an inductive bias that degrades gesture quality.
- Mechanism: 2D-to-3D lifting models are deterministic and map each 2D pose to exactly one 3D pose. This collapses the inherent one-to-many ambiguity of 3D pose from 2D into a single, fixed output, reducing diversity and making gestures less natural.
- Core assumption: The one-to-many mapping from 2D to 3D is not invertible and cannot be fully captured by a deterministic lifter without loss of expressiveness.
- Evidence anchors:
  - [abstract] "There is a one-to-many relationship between 2D keypoints and their 3D counterparts. Given the deterministic nature of the 2D-to-3D lifter, it will consistently map any given 2D pose to the same corresponding 3D pose introducing an inductive bias in the process."
  - [section] "As VideoPose3D is deterministic, to one 2D pose it will systematically predict the same 3D pose although there exists multiple possibilities."
  - [corpus] No corpus evidence; this is an original observation from the paper.
- Break condition: If the lifting model becomes stochastic or multimodal, the inductive bias would be reduced, potentially improving quality.

### Mechanism 2
- Claim: Models trained on 3D data learn richer, more expressive representations than models trained on 2D data.
- Mechanism: 3D joint coordinates contain depth information that captures richer motion dynamics. Models trained directly on 3D learn to exploit this richer structure, leading to more expressive and diverse gestures.
- Core assumption: The depth dimension in 3D poses provides non-redundant information that meaningfully improves gesture generation.
- Evidence anchors:
  - [abstract] "The representation of co-speech gestures, in 2D or 3D, influences how the agent’s non-verbal communication is perceived, especially the speaker’s communication style."
  - [section] "The 2D-to-3D conversion of gestures has a small yet significant impact on the perception of human-likeness."
  - [corpus] No corpus evidence for this claim; relies on perceptual evaluation.
- Break condition: If depth is largely redundant with 2D cues (e.g., in constrained motions), the benefit of 3D training may vanish.

### Mechanism 3
- Claim: Deterministic lifting introduces temporal smoothing that reduces beat consistency.
- Mechanism: The lifting process tends to over-smooth temporal sequences, reducing kinematic beat detection and weakening synchronization with speech.
- Core assumption: Smoothing is an inherent artifact of deterministic lifting, not just a model choice.
- Evidence anchors:
  - [abstract] "There is a drop in BC between the 3D models and their 2D counterparts. It can be that post-processing 2D gestures using VideoPose3D tends to over-smooth the resulting 3D gestures therefore reducing the number of kinematic beats."
  - [section] "lifting 2D gestures to 3D slightly reduces their perceived human-likeness, aliveness, and speech synchrony."
  - [corpus] No corpus evidence for this mechanism; based on the paper’s subjective evaluation.
- Break condition: If a lifting model is designed to preserve temporal dynamics (e.g., with adversarial temporal consistency), the beat degradation may be mitigated.

## Foundational Learning

- Concept: Pose representation in 2D vs 3D
  - Why needed here: Understanding how removing or adding the depth dimension changes the expressiveness of gestures is critical to interpreting model performance.
  - Quick check question: What information is lost when a 3D pose is projected to 2D, and how might that affect gesture naturalness?

- Concept: Generative models (DDPM vs RNN)
  - Why needed here: The paper compares two architectures (DiffGesture and Trimodal), so understanding how each handles conditioning and temporal dependencies is key to interpreting results.
  - Quick check question: How does classifier-free guidance in DDPMs help generate diverse gestures conditioned on speech?

- Concept: Metric definitions (FGD, BC, Diversity)
  - Why needed here: The paper’s objective evaluation relies on these metrics; knowing what they measure is essential to assess model quality.
  - Quick check question: What does a low Fréchet Gesture Distance indicate about the similarity between generated and real gestures?

## Architecture Onboarding

- Component map: Speech encoder → Gesture generator (2D or 3D) → Optional 2D→3D lifter → Evaluation pipeline
- Critical path:
  - For 2D-trained models: Speech → 2D generator → VideoPose3D → 3D output → metrics/user study
  - For 3D-trained models: Speech → 3D generator → 3D output → metrics/user study
- Design tradeoffs:
  - 2D training: Larger, more diverse datasets available, but loss of depth and possible degradation from lifting
  - 3D training: Richer representations, better naturalness, but smaller, less diverse datasets
  - Deterministic lifting: Simple and fast, but collapses ambiguity and may smooth temporal dynamics
- Failure signatures:
  - Low beat consistency: Likely due to over-smoothing in lifting
  - High FGD: Indicates generated gestures differ significantly from real gestures (either from limited 2D expressiveness or lifting artifacts)
  - Low diversity: Suggests model collapse or overly deterministic lifting
- First 3 experiments:
  1. Train DiffGesture 2D and 3D on TED Gesture-3D, compare FGD and BC to ground truth.
  2. Apply VideoPose3D to ground truth 2D poses, evaluate how much quality degrades without any generative model.
  3. Conduct a small user study comparing DiffGesture 2D+VP3D vs DiffGesture 3D to validate objective findings.

## Open Questions the Paper Calls Out

- Does the dimensionality of gesture representation (2D vs 3D) affect the quality of gestures generated for specific semantic content or speech emphasis?
  - Basis in paper: [explicit] The authors note that the representation of co-speech gestures, in 2D or 3D, influences how the agent's non-verbal communication is perceived, especially the speaker's communication style [23].
  - Why unresolved: The study focuses on general gesture quality metrics (FGD, BC, Diversity) and user perception of human-likeness, aliveness, and speech synchrony, but does not specifically analyze whether 2D vs 3D representation affects the ability to generate semantically appropriate gestures or emphasize specific speech content.
  - What evidence would resolve it: Experiments comparing gesture semantic appropriateness and speech emphasis effectiveness between 2D and 3D representations using semantic analysis tools or expert evaluation of gesture-speech alignment.

- How does the quality of 2D-to-3D lifting models affect the perceived difference between 2D and 3D gesture generation?
  - Basis in paper: [explicit] The authors use VideoPose3D for lifting and note it is deterministic, consistently mapping 2D poses to the same 3D poses, introducing an inductive bias. They also mention that VideoPose3D's performance is not perfect (MPJPE of 13.4 on the test set).
  - Why unresolved: While the study shows that lifting 2D gestures to 3D using VideoPose3D negatively impacts quality, it doesn't explore whether using a more accurate or probabilistic lifting model would reduce this gap.
  - What evidence would resolve it: Comparative experiments using different lifting models (deterministic vs. probabilistic) to quantify the impact of lifting accuracy on gesture quality.

- Does the performance gap between 2D and 3D gesture generation persist when using high-quality 3D ground truth data instead of 3D poses lifted from 2D?
  - Basis in paper: [explicit] The authors acknowledge their evaluation is biased because the TED Gesture-3D dataset contains 3D poses lifted from 2D body poses, not real 3D ground truth data.
  - Why unresolved: The study uses a dataset where 3D gestures are inferred from 2D poses, so the baseline "3D" data may already be degraded compared to true 3D motion capture data.
  - What evidence would resolve it: Experiments training and evaluating models on a dataset with real 3D motion capture data to determine if the observed performance differences are due to the 2D-to-3D conversion process or inherent limitations of 2D representation.

## Limitations
- The study relies on a single lifting model (VideoPose3D) and two generative architectures, limiting generalizability to other methods or datasets.
- The observed differences may be due to dataset differences rather than inherent limitations of 2D representation.
- The exact mechanisms behind the observed differences between 2D and 3D training are not fully established.

## Confidence
- High confidence: 3D-trained models produce more human-like and speech-synchronized gestures than 2D-to-3D lifted outputs.
- Medium confidence: Deterministic lifting introduces inductive bias by collapsing the one-to-many mapping from 2D to 3D.
- Low confidence: Depth information alone is responsible for the observed quality differences.

## Next Checks
1. Test alternative lifting models (e.g., probabilistic or multimodal) to determine if the inductive bias can be reduced or eliminated.
2. Compare models trained on matched 2D and 3D datasets to isolate the effect of representation from dataset differences.
3. Conduct ablation studies on the role of depth by training models on 2D+depth (3.5D) versus full 3D to quantify the marginal benefit of depth information.