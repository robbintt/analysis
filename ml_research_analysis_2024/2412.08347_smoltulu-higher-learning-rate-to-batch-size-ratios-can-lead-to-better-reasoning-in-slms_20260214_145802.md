---
ver: rpa2
title: 'SmolTulu: Higher Learning Rate to Batch Size Ratios Can Lead to Better Reasoning
  in SLMs'
arxiv_id: '2412.08347'
source_url: https://arxiv.org/abs/2412.08347
tags:
- learning
- batch
- size
- rate
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work presents SmolTulu-1.7b-Instruct, a state-of-the-art
  sub-2B parameter language model adapted from Huggingface''s SmolLM2-1.7B base model
  using AllenAI''s Tulu 3 post-training pipeline. Through systematic ablation studies
  on a 135M parameter model, the authors demonstrate that learning rate to batch size
  ratios have a task-dependent impact on performance: reasoning tasks like ARC and
  GSM8K benefit from higher ratios, while pattern recognition tasks like HellaSwag
  and IFEval perform best with lower ratios.'
---

# SmolTulu: Higher Learning Rate to Batch Size Ratios Can Lead to Better Reasoning in SLMs

## Quick Facts
- arXiv ID: 2412.08347
- Source URL: https://arxiv.org/abs/2412.08347
- Reference count: 5
- Primary result: SmolTulu-1.7b-Instruct achieves state-of-the-art performance among sub-2B parameter models with significant improvements on reasoning and instruction-following benchmarks

## Executive Summary
This work presents SmolTulu-1.7b-Instruct, a state-of-the-art sub-2B parameter language model adapted from Huggingface's SmolLM2-1.7B base model using AllenAI's Tulu 3 post-training pipeline. Through systematic ablation studies on a 135M parameter model, the authors demonstrate that learning rate to batch size ratios have a task-dependent impact on performance: reasoning tasks like ARC and GSM8K benefit from higher ratios, while pattern recognition tasks like HellaSwag and IFEval perform best with lower ratios. This insight led to the development of SmolTulu, which achieves state-of-the-art performance among sub-2B parameter models, scoring 67.7% on IFEval (Δ11% improvement), 51.6% on GSM8K (Δ3.4% improvement), and 57.1% on ARC (Δ5.4% improvement) in its variants.

## Method Summary
The authors developed SmolTulu through a systematic post-training pipeline using AllenAI's Tulu 3 methodology on Huggingface's SmolLM2-1.7B base model. The approach involved supervised fine-tuning, direct preference optimization, and reward modeling with careful contamination controls. Key to their methodology was conducting ablation studies on a 135M parameter model to identify optimal learning rate to batch size ratios for different task types. The training process incorporated task-specific hyperparameter tuning, where reasoning tasks benefited from higher learning rate to batch size ratios while pattern recognition tasks performed better with lower ratios.

## Key Results
- SmolTulu achieves state-of-the-art performance among sub-2B parameter models with 67.7% on IFEval (Δ11% improvement)
- The model scores 51.6% on GSM8K (Δ3.4% improvement) and 57.1% on ARC (Δ5.4% improvement)
- Systematic ablation studies reveal that smaller models require fundamentally different optimization strategies than larger models

## Why This Works (Mechanism)
The mechanism behind SmolTulu's success lies in the task-dependent optimization of learning rate to batch size ratios. Higher ratios appear to provide better exploration of solution spaces for reasoning tasks, allowing the model to discover more complex reasoning patterns. Conversely, lower ratios provide more stable convergence for pattern recognition tasks where overfitting is a concern. This optimization strategy recognizes that smaller models have different optimization landscapes than their larger counterparts, requiring more nuanced hyperparameter tuning to achieve optimal performance.

## Foundational Learning

**Learning Rate Scheduling**
- Why needed: Controls how quickly the model adapts to training data during optimization
- Quick check: Verify learning rate warmup and decay schedules are correctly implemented

**Batch Size Effects**
- Why needed: Determines gradient estimation quality and memory efficiency during training
- Quick check: Confirm batch size is appropriate for GPU memory constraints

**Gradient Clipping**
- Why needed: Prevents exploding gradients that can destabilize training
- Quick check: Monitor gradient norms during initial training runs

**Model Architecture Basics**
- Why needed: Understanding how attention mechanisms and feed-forward layers interact
- Quick check: Verify attention patterns and layer normalization are functioning correctly

## Architecture Onboarding

**Component Map**
Base Model (SmolLM2-1.7B) -> Supervised Fine-tuning -> Direct Preference Optimization -> Reward Modeling -> SmolTulu-1.7b-Instruct

**Critical Path**
The critical path involves the ablation studies on 135M parameter models to identify optimal hyperparameters, followed by scaling these findings to the 1.7B parameter model. This includes contamination-free dataset preparation, iterative fine-tuning with task-specific learning rate to batch size ratios, and final evaluation on benchmark tasks.

**Design Tradeoffs**
The primary tradeoff involves balancing reasoning performance (requiring higher learning rate to batch size ratios) against pattern recognition accuracy (requiring lower ratios). The authors resolved this by developing task-specific variants rather than a single unified model, accepting the complexity of multiple model versions for optimal performance.

**Failure Signatures**
Potential failures include overfitting on reasoning tasks with high learning rate to batch size ratios, underfitting on pattern recognition tasks with low ratios, and contamination in training data leading to inflated benchmark scores. The careful contamination controls and reward modeling approach help mitigate these risks.

**First 3 Experiments**
1. Replicate the 135M parameter ablation studies to verify learning rate to batch size ratio effects
2. Train SmolTulu variants with different ratio combinations to map the optimization landscape
3. Evaluate contamination-free performance on IFEval, GSM8K, and ARC benchmarks

## Open Questions the Paper Calls Out
None

## Limitations
- Findings are based on systematic ablation studies on a specific model size (135M parameters), limiting generalizability
- The assertion that smaller models require fundamentally different optimization strategies than larger models would benefit from additional comparative studies across model sizes
- Specific implementation details of the post-training pipeline could be more thoroughly documented for precise reproduction

## Confidence
- Learning rate to batch size ratio findings: High confidence (supported by systematic ablation studies)
- State-of-the-art performance claims: Medium confidence (well-supported by reported results but would benefit from external validation)
- Optimization strategy differences between model sizes: Medium confidence (logical extension but needs more comparative studies)

## Next Checks
1. Replicate the learning rate to batch size ratio ablation studies on multiple model sizes (e.g., 500M, 1B parameters) to verify generalizability of the findings
2. Conduct an independent benchmark evaluation of SmolTulu-1.7b-Instruct against other sub-2B parameter models using standardized evaluation protocols
3. Perform a detailed analysis of the contamination controls by releasing the exact contamination-free datasets used for training and evaluation