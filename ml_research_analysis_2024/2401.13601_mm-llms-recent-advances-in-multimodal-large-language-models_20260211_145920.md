---
ver: rpa2
title: 'MM-LLMs: Recent Advances in MultiModal Large Language Models'
arxiv_id: '2401.13601'
source_url: https://arxiv.org/abs/2401.13601
tags:
- arxiv
- preprint
- wang
- zhang
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive survey of recent advances in
  MultiModal Large Language Models (MM-LLMs), which augment pre-trained LLMs to support
  multimodal inputs or outputs via cost-effective training strategies. The survey
  covers 126 SOTA MM-LLMs, their design formulations, model architectures, training
  pipelines, performance on mainstream benchmarks, and key training recipes.
---

# MM-LLMs: Recent Advances in MultiModal Large Language Models

## Quick Facts
- arXiv ID: 2401.13601
- Source URL: https://arxiv.org/abs/2401.13601
- Reference count: 40
- Covers 126 SOTA MM-LLMs with comprehensive design and performance analysis

## Executive Summary
This survey comprehensively examines MultiModal Large Language Models (MM-LLMs), which extend pre-trained LLMs to handle multimodal inputs and outputs through cost-effective training strategies. The paper systematically categorizes 126 state-of-the-art MM-LLMs, analyzing their design formulations, model architectures, training pipelines, and performance on mainstream benchmarks. It also identifies key training recipes and explores promising future research directions for the field.

## Method Summary
The paper provides a comprehensive survey methodology that systematically reviews recent advances in MM-LLMs through literature analysis, benchmark evaluation, and architectural examination. The authors categorize models based on their design formulations, analyze training strategies and architectural components, and evaluate performance across standardized benchmarks. They maintain a real-time tracking website to monitor ongoing developments in the rapidly evolving field.

## Key Results
- Systematic coverage of 126 SOTA MM-LLMs with detailed architectural and training analysis
- Comprehensive performance evaluation across mainstream multimodal benchmarks
- Identification of key training recipes and promising future research directions
- Real-time tracking website maintained for ongoing field developments

## Why This Works (Mechanism)
The effectiveness of MM-LLMs stems from their ability to leverage pre-trained LLMs as foundation models while incorporating multimodal capabilities through efficient training strategies. By extending existing LLMs rather than training from scratch, these models benefit from strong language understanding while gaining the ability to process and generate multimodal content through carefully designed architectural modifications and instruction-tuning datasets.

## Foundational Learning

**Transformer Architecture** - Why needed: Forms the backbone of modern LLMs and MM-LLMs; provides attention mechanisms for processing sequences of different modalities. Quick check: Verify understanding of self-attention, multi-head attention, and positional encoding.

**Instruction Tuning** - Why needed: Enables models to follow human instructions across modalities; crucial for practical deployment. Quick check: Understand the difference between standard fine-tuning and instruction tuning approaches.

**Multimodal Fusion** - Why needed: Combines information from different modalities (text, image, audio) into unified representations. Quick check: Know common fusion strategies like early fusion, late fusion, and cross-attention mechanisms.

**Prompt Engineering** - Why needed: Guides model behavior and output format for multimodal tasks. Quick check: Understand zero-shot, few-shot, and chain-of-thought prompting techniques.

## Architecture Onboarding

**Component Map**: Pre-trained LLM -> Multimodal Adapter -> Fusion Layer -> Output Generator

**Critical Path**: Input Processing → Multimodal Fusion → Cross-Modal Attention → Response Generation

**Design Tradeoffs**: 
- Parameter efficiency vs. performance: Smaller adapter-based approaches vs. full model training
- Modality coverage vs. specialization: Broad multimodal support vs. optimized single-modality performance
- Training efficiency vs. capability: Cost-effective adaptation vs. comprehensive multimodal understanding

**Failure Signatures**:
- Hallucinations in cross-modal reasoning tasks
- Degradation in language-only performance after multimodal adaptation
- Inconsistent handling of rare or ambiguous multimodal inputs

**First Experiments**:
1. Evaluate basic multimodal understanding on image-text retrieval benchmarks
2. Test cross-modal generation capabilities on caption-to-image tasks
3. Assess instruction following across multiple modalities using standardized prompts

## Open Questions the Paper Calls Out

The paper identifies several promising future directions including expanding beyond traditional text-image modalities to incorporate audio, video, and 3D data; diversifying LLM foundations beyond English-centric models; improving the quality and diversity of multimodal instruction-tuning datasets; and strengthening generation capabilities across multiple modalities while maintaining consistency and coherence.

## Limitations

- Analysis limited to literature up to mid-2024, potentially missing recent developments in this rapidly evolving field
- Performance comparisons rely on reported benchmark results with varying evaluation protocols and hardware configurations
- Focus primarily on English-language models with limited coverage of non-English MM-LLMs and multilingual capabilities

## Confidence

**Major claim clusters confidence:**
- Coverage of 126 SOTA MM-LLMs (High): Systematic categorization and comprehensive listing well-documented
- Training strategies and architectural formulations (High): Technical descriptions align with established literature
- Performance analysis on mainstream benchmarks (Medium): Benchmark variations across studies introduce uncertainty
- Future directions and recommendations (Medium): Forward-looking sections are reasonable but speculative

## Next Checks

1. Cross-validate benchmark performance comparisons by running standardized evaluation suites across multiple MM-LLMs using consistent hardware and evaluation protocols
2. Verify the completeness of the 126 MM-LLMs catalog by checking for missing models published between January 2023 and mid-2024 using additional sources
3. Assess the real-time tracking website's accuracy by comparing its listings against independent literature searches for newly published MM-LLMs over a 3-month period