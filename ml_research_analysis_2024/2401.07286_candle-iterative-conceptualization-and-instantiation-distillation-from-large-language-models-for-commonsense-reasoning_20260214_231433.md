---
ver: rpa2
title: 'CANDLE: Iterative Conceptualization and Instantiation Distillation from Large
  Language Models for Commonsense Reasoning'
arxiv_id: '2401.07286'
source_url: https://arxiv.org/abs/2401.07286
tags:
- commonsense
- knowledge
- candle
- personx
- event
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CANDLE, a distillation framework for generalizable
  commonsense reasoning by completing the chain of conceptualization and instantiation.
  CANDLE utilizes large language models to generate contextualized conceptualizations
  and instantiations from commonsense knowledge bases, followed by critic filtering
  to ensure quality.
---

# CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning

## Quick Facts
- arXiv ID: 2401.07286
- Source URL: https://arxiv.org/abs/2401.07286
- Reference count: 40
- Key outcome: CANDLE distills LLMs to generate 6.18M contextualized conceptualizations and instantiations from ATOMIC, improving downstream task performance

## Executive Summary
CANDLE introduces a novel distillation framework that completes the chain of conceptualization and instantiation for commonsense reasoning. The framework leverages powerful LLMs (ChatGPT and LLAMA2) to generate contextualized knowledge triples from existing commonsense knowledge bases, followed by critic filtering to ensure quality. Applied to ATOMIC, CANDLE produces a knowledge base of 6.18 million triples that demonstrates exceptional quality and diversity. The distilled knowledge significantly improves performance on four downstream tasks including CSKB conceptualization, generative commonsense inference, and zero-shot commonsense question answering.

## Method Summary
CANDLE implements an iterative framework that performs conceptualization (abstracting instances to concepts) and instantiation (grounding concepts to instances) using few-shot prompting with LLMs. The process begins with sampling triples from an original CSKB like ATOMIC, then uses ChatGPT to generate contextualized conceptualizations and LLAMA2 to generate contextualized instantiations. A DeBERTa-v3 discriminator filters low-quality conceptualizations while VERA-T5 filters instantiations. The instantiated knowledge can be fed back as input for further iterations, creating a loop that significantly augments the original CSKB. The framework is designed to operate on any CSKB format and does not require pre-built concept taxonomies.

## Key Results
- CANDLE constructs 6.18 million conceptualizations and instantiated triples from ATOMIC
- Achieves 50.71 BLEU-1 and 32.45 METEOR on COMET generative commonsense inference
- Scores 81.2% accuracy on zero-shot commonsense question answering
- Demonstrates exceptional quality and diversity in intrinsic evaluations

## Why This Works (Mechanism)

### Mechanism 1
- LLMs can generate contextualized conceptualizations and instantiations without relying on pre-built concept taxonomies
- Uses few-shot prompting to instruct ChatGPT and LLAMA2 to abstract instances into concepts and ground concepts into new instances within original context
- Core assumption: LLMs have sufficient commonsense knowledge and reasoning ability when given clear instructions and examples
- Break condition: If LLMs lack sufficient commonsense knowledge or prompts are unclear, generated knowledge will be low quality or irrelevant

### Mechanism 2
- Critic filtering improves quality of generated conceptualizations and instantiations
- Uses pre-trained discriminator models (DeBERTa-v3 for conceptualization, VERA-T5 for instantiation) to score plausibility and filter low-scoring generations
- Core assumption: Discriminator models effectively identify implausible or low-quality generations
- Break condition: If discriminators are not well-calibrated or thresholds are poorly set, valid generations may be incorrectly filtered or low-quality ones retained

### Mechanism 3
- Iterating conceptualization and instantiation loop can significantly augment CSKB
- Allows instantiations to be fed back into original CSKB as input for further conceptualization, creating exponential knowledge expansion
- Core assumption: Instantiations represent concrete commonsense knowledge that can serve as valid input for next iteration
- Break condition: If instantiated knowledge introduces too much noise, quality of subsequent generations may degrade over iterations

## Foundational Learning

- **Chain of conceptualization and instantiation**: The two-step process of abstracting instances to concepts and grounding concepts to instances. Needed to understand CANDLE's core methodology and how it expands knowledge bases.
  - Quick check: What are the two steps in this chain, and how do they relate to each other?

- **Commonsense knowledge bases (CSKBs)**: Structured collections of everyday human knowledge. Needed to contextualize CANDLE's application to ATOMIC and its potential for other knowledge sources.
  - Quick check: What is a CSKB, and why are they important for commonsense reasoning?

- **Knowledge distillation**: Transferring knowledge from large models to more usable forms. Needed to understand how CANDLE extracts and refines knowledge from LLMs for downstream tasks.
  - Quick check: What is knowledge distillation, and how is it applied in CANDLE's context?

## Architecture Onboarding

- **Component map**: Original CSKB -> ChatGPT (conceptualization) -> LLAMA2 (instantiation) -> DeBERTa-v3 discriminator -> VERA-T5 discriminator -> Distilled knowledge base -> Downstream task models

- **Critical path**: 1) Sample triples from original CSKB, 2) Use ChatGPT to generate contextualized conceptualizations, 3) Use LLAMA2 to generate contextualized instantiations, 4) Apply critic filtering, 5) Add filtered knowledge to distilled knowledge base, 6) Repeat for all triples, 7) Use distilled knowledge for downstream tasks

- **Design tradeoffs**: Using LLMs vs. pre-built taxonomies (LLMs offer more diversity but are expensive and less interpretable); Number of iterations (more iterations expand knowledge but may introduce noise); Critic filtering threshold (higher thresholds yield higher quality but smaller knowledge base)

- **Failure signatures**: Low-quality or irrelevant conceptualizations and instantiations (suggests issues with LLM prompts, model capabilities, or critic filtering); Downstream task performance not improving (suggests distilled knowledge is not relevant for tasks); Distilled knowledge base not significantly larger than original (suggests issues with conceptualization and instantiation process)

- **First 3 experiments**: 1) Generate conceptualizations and instantiations for small ATOMIC sample and manually evaluate quality, 2) Compare downstream model performance on CANDLE-distilled vs. original ATOMIC knowledge, 3) Experiment with different critic filtering thresholds and evaluate impact on quality and size of distilled knowledge base

## Open Questions the Paper Calls Out

- **Open Question 1**: How would CANDLE perform on other CSKBs beyond ATOMIC?
  - Basis: Explicit statement about extending evaluation to other CSKBs like ConceptNet, WebChild
  - Unresolved: Only validated on ATOMIC, leaving effectiveness on other CSKBs unknown
  - Evidence needed: Empirical results showing CANDLE's performance on various other CSKBs with qualitative and quantitative comparisons

- **Open Question 2**: What is the optimal number of iterations for the conceptualization and instantiation loop?
  - Basis: Explicit mention that only one iteration was executed, with multiple iterations promising greater enhancement
  - Unresolved: Only explored single iteration, leaving optimal iteration count open
  - Evidence needed: Experiments comparing quality and diversity after different iteration counts, along with downstream task performance metrics

- **Open Question 3**: Can CANDLE improve commonsense reasoning in other domains beyond those explored?
  - Basis: Inferred from demonstration on four specific downstream tasks without exploring other potential domains
  - Unresolved: Focused on limited set of downstream tasks, leaving applicability to other domains unknown
  - Evidence needed: Experiments applying CANDLE to other domains like complex query answering, script generation, pragmatics, analogical reasoning, and E-Commerce understanding with performance metrics and qualitative analysis

## Limitations

- Framework performance heavily depends on quality of input CSKBs and few-shot examples, with biases or errors propagating through the process
- Selection of empirical thresholds for critic filtering is not fully justified, lacking systematic analysis of threshold impacts
- Evaluation is limited to specific benchmarks, with generalizability to other commonsense reasoning tasks untested

## Confidence

- **High Confidence**: CANDLE successfully generates contextualized conceptualizations and instantiations from ATOMIC; iterative loop can expand CSKB significantly; critic filtering improves quality
- **Medium Confidence**: CANDLE-distilled knowledge improves downstream task performance across evaluated tasks; generated knowledge quality and diversity is exceptional; zero-shot QA performance is improved
- **Low Confidence**: CANDLE generalizes effectively to CSKBs beyond ATOMIC; framework maintains quality across multiple iterations; specific threshold values are optimal

## Next Checks

1. **Cross-CSKB Validation**: Apply CANDLE to multiple CSKBs (ConceptNet, WebChild) and evaluate whether framework maintains similar quality and expansion ratios across different knowledge sources to test generalizability claim.

2. **Threshold Sensitivity Analysis**: Systematically vary critic filtering thresholds across a range and measure impact on both knowledge base quality (via human evaluation) and downstream task performance to validate optimal threshold selection.

3. **Long-term Iteration Study**: Extend iterative process beyond two iterations (to 5-10 iterations) and track how knowledge quality, diversity, and downstream performance evolve to reveal whether framework maintains quality over extended use or suffers from concept drift.