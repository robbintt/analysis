---
ver: rpa2
title: Hyperbolic Heterogeneous Graph Attention Networks
arxiv_id: '2404.09456'
source_url: https://arxiv.org/abs/2404.09456
tags:
- heterogeneous
- metapath
- graph
- hyperbolic
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HHGAT is the first hyperbolic graph neural network that learns
  metapath instances for heterogeneous graph embedding. It samples metapath instances
  via BFS and maps them into hyperbolic space using exponential and logarithmic maps.
---

# Hyperbolic Heterogeneous Graph Attention Networks

## Quick Facts
- arXiv ID: 2404.09456
- Source URL: https://arxiv.org/abs/2404.09456
- Authors: Jongmin Park, Seunghoon Han, Soohwan Jeong, Sungsu Lim
- Reference count: 14
- Key outcome: HHGAT achieves 67.01%, 95.72%, and 94.40% Macro-F1 on IMDB, DBLP, and ACM datasets, outperforming strong baselines by 3-5 points

## Executive Summary
HHGAT is the first hyperbolic graph neural network designed specifically for heterogeneous graph embedding that learns metapath instances. The method samples metapath instances via breadth-first search and maps them into hyperbolic space using exponential and logarithmic maps, then applies hyperbolic attention mechanisms to aggregate metapath-specific embeddings. Experiments on three real-world heterogeneous graph datasets (IMDB, DBLP, ACM) demonstrate significant performance improvements over state-of-the-art baselines in both node classification and clustering tasks, with gains of 2-4 points in NMI metrics. The curvature parameter analysis reveals dataset-specific optimal values that correlate with power-law distributions in the data.

## Method Summary
HHGAT employs breadth-first search to sample metapath instances within maximum lengths (4 for IMDB, 5 for DBLP, 3 for ACM), then transforms concatenated node features to hyperbolic space via exponential maps. Hyperbolic linear transformations and attention mechanisms aggregate metapath instances and metapath-specific embeddings, with inter-metapath attention combining final node representations. The model is trained with cross-entropy loss using Adam optimizer (lr=0.0001, weight_decay=0.001, dropout=0.5, embedding_dim=64, n_heads=8) for 100 epochs with early stopping. The approach specifically addresses the distortion that occurs when representing inherently hierarchical and power-law heterogeneous graphs in Euclidean space.

## Key Results
- HHGAT achieves 67.01% Macro-F1 on IMDB, outperforming HAN, MAGNN, and GTN by 3-5 points
- On DBLP, HHGAT reaches 95.72% Macro-F1, surpassing baselines by 4-5 points
- NMI gains of 2-4 points over strong hyperbolic and Euclidean methods across all datasets
- Curvature parameter analysis shows optimal performance when tuned to dataset-specific power-law distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hyperbolic space better represents hierarchical and power-law structures inherent in heterogeneous graphs.
- Mechanism: Negative curvature in hyperbolic space allows exponential growth, matching the power-law node degree distributions and hierarchical relationships in heterogeneous graphs.
- Core assumption: Real-world heterogeneous graphs exhibit power-law distributions and hierarchical structures that Euclidean embeddings distort.
- Evidence anchors:
  - [abstract] "because heterogeneous graphs inherently possess complex structures, such as hierarchical or power-law structures, distortions can occur when representing them in Euclidean space"
  - [section] "hyperbolic space, which naturally inherits properties of complex structures, is more appropriate for heterogeneous graph embedding than Euclidean space"
  - [corpus] No direct corpus evidence found for this specific mechanism claim.
- Break condition: If the target graph lacks hierarchical or power-law structure, hyperbolic embedding offers no advantage over Euclidean.

### Mechanism 2
- Claim: Metapath instance sampling via BFS captures hierarchical structure around target nodes.
- Mechanism: BFS sampling explores neighbors level-by-level, preserving the local hierarchical neighborhood structure of heterogeneous graphs.
- Core assumption: BFS traversal effectively captures the hierarchical relationships in heterogeneous graphs around target nodes.
- Evidence anchors:
  - [section] "we employ breadth-first search, which effectively captures the hierarchical structure around node ùë£"
  - [section] "HHGAT automatically samples metapath instances within the maximum metapath length based on given link types"
  - [corpus] No direct corpus evidence found for this specific mechanism claim.
- Break condition: If the graph's important structural information is not captured by BFS levels, this sampling strategy fails.

### Mechanism 3
- Claim: Hyperbolic attention mechanisms preserve hierarchical relationships during aggregation.
- Mechanism: Attention scores computed in hyperbolic space maintain the geometric relationships of metapath instances, allowing semantically important paths to be weighted appropriately.
- Core assumption: Attention mechanisms in hyperbolic space better preserve hierarchical relationships than Euclidean attention.
- Evidence anchors:
  - [section] "The attention mechanism in hyperbolic spaces is used to learn the attention score of each metapath instance and aggregate them accordingly"
  - [section] "HHGAT performs metapath instance aggregation to obtain the metapath-specific embedding vector"
  - [corpus] No direct corpus evidence found for this specific mechanism claim.
- Break condition: If attention weights in hyperbolic space do not correlate with semantic importance, the aggregation fails to capture meaningful relationships.

## Foundational Learning

- Poincar√© Ball Model:
  - Why needed here: Provides the mathematical foundation for representing data in hyperbolic space with learnable curvature.
  - Quick check question: What property of the Poincar√© ball model allows it to represent hierarchical data more efficiently than Euclidean space?

- M√∂bius Operations:
  - Why needed here: Enable addition, multiplication, and non-linear activation in hyperbolic space while maintaining the geometric properties.
  - Quick check question: How does M√∂bius addition differ from standard vector addition in Euclidean space?

- Exponential and Logarithmic Maps:
  - Why needed here: Allow transformation between Euclidean features and hyperbolic embeddings while preserving the hyperbolic geometry.
  - Quick check question: What is the role of the exponential map in projecting Euclidean features into hyperbolic space?

## Architecture Onboarding

- Component map: Input features ‚Üí Metapath instance sampling (BFS) ‚Üí Euclidean feature concatenation ‚Üí Exponential map to hyperbolic space ‚Üí Hyperbolic linear transformation ‚Üí Hyperbolic attention aggregation ‚Üí Inter-metapath attention ‚Üí Final node embedding ‚Üí Linear projection to output space

- Critical path: Metapath instance sampling ‚Üí Hyperbolic embedding ‚Üí Attention aggregation ‚Üí Final node embedding

- Design tradeoffs: Hyperbolic space better captures hierarchy but requires specialized operations (M√∂bius, exponential maps) that are computationally more expensive than Euclidean counterparts.

- Failure signatures: Poor classification/clustering performance suggests either incorrect curvature parameter, inadequate metapath sampling, or issues with the hyperbolic attention mechanism.

- First 3 experiments:
  1. Verify BFS sampling captures expected hierarchical structure by visualizing sampled instances.
  2. Test curvature parameter sensitivity by fixing curvature values and observing performance changes.
  3. Compare attention weights in hyperbolic vs Euclidean spaces to validate that hyperbolic attention better preserves semantic relationships.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of HHGAT vary when different metapath sampling strategies (e.g., random walk vs BFS) are used?
- Basis in paper: [explicit] The paper states that HHGAT employs BFS for metapath instance sampling, which "effectively captures the hierarchical structure," but does not compare it to other sampling strategies.
- Why unresolved: The authors did not experiment with or compare alternative sampling strategies like random walk or depth-first search.
- What evidence would resolve it: Empirical results comparing HHGAT's performance using BFS, random walk, and other sampling strategies on the same datasets.

### Open Question 2
- Question: What is the impact of increasing the maximum metapath length beyond the current settings (4 for IMDB, 5 for DBLP, 3 for ACM)?
- Basis in paper: [explicit] The authors set maximum metapath lengths but do not explore the effect of longer paths on model performance.
- Why unresolved: The experiments only tested fixed maximum lengths without exploring longer or variable-length paths.
- What evidence would resolve it: Performance comparison of HHGAT with increasing maximum metapath lengths, showing accuracy and computational cost trade-offs.

### Open Question 3
- Question: How does HHGAT perform on heterogeneous graphs with different structural properties, such as bipartite graphs or graphs with cycles?
- Basis in paper: [explicit] The experiments used citation networks and movie databases, but did not test HHGAT on other heterogeneous graph structures like bipartite graphs or graphs with significant cycles.
- Why unresolved: The study focused on three specific datasets without exploring diverse graph structures.
- What evidence would resolve it: Testing HHGAT on heterogeneous graphs with different structural properties (e.g., bipartite, cyclic) and comparing its performance to baseline models.

### Open Question 4
- Question: How sensitive is HHGAT to hyperparameter tuning, particularly the curvature parameter c, and is there a systematic way to optimize it?
- Basis in paper: [explicit] The authors analyzed the curvature parameter and found optimal values for each dataset, but did not provide a systematic method for optimizing it.
- Why unresolved: The paper only reports results for fixed curvature values and does not explore automated or adaptive curvature optimization methods.
- What evidence would resolve it: A study comparing HHGAT's performance with different curvature optimization strategies, including grid search, gradient-based methods, or adaptive schemes.

## Limitations

- Mechanism uncertainty: Direct corpus evidence for why hyperbolic space specifically benefits heterogeneous graph embedding is lacking.
- Implementation details: Exact node features and specific hyperparameters for baselines are not fully specified.
- Generalizability: Optimal curvature parameters are dataset-specific, suggesting the method may require careful tuning for new datasets.

## Confidence

- High Confidence: The architectural design of HHGAT combining hyperbolic geometry with metapath-based attention is well-defined and technically sound.
- Medium Confidence: The experimental results showing improved performance over baselines on three specific datasets, though the extent of generalization to other heterogeneous graphs remains uncertain.
- Low Confidence: The theoretical claims about why hyperbolic space specifically benefits heterogeneous graph embedding lack direct supporting evidence from the broader research corpus.

## Next Checks

1. **Curvature Sensitivity Analysis:** Conduct a comprehensive grid search over curvature parameters (c values) on all three datasets to verify the reported optimal values and assess sensitivity.

2. **Ablation Study on Components:** Remove the hyperbolic component while keeping the metapath attention mechanism to isolate the contribution of hyperbolic geometry versus attention mechanisms.

3. **Dataset Generalization Test:** Apply HHGAT to additional heterogeneous graph datasets with known hierarchical structures (e.g., bibliographic networks, biological networks) to test generalizability beyond the three evaluated datasets.