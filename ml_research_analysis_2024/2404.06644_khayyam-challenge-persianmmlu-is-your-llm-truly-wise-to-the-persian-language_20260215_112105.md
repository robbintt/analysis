---
ver: rpa2
title: 'Khayyam Challenge (PersianMMLU): Is Your LLM Truly Wise to The Persian Language?'
arxiv_id: '2404.06644'
source_url: https://arxiv.org/abs/2404.06644
tags: []
core_contribution: This paper introduces Khayyam Challenge, also known as PersianMMLU,
  a comprehensive Persian-language evaluation framework for large language models
  (LLMs). It consists of 20,192 multiple-choice questions across 38 tasks, covering
  educational levels from primary to upper secondary school.
---

# Khayyam Challenge (PersianMMLU): Is Your LLM Truly Wise to The Persian Language?

## Quick Facts
- arXiv ID: 2404.06644
- Source URL: https://arxiv.org/abs/2404.06644
- Reference count: 40
- Primary result: Introduces PersianMMLU, a 20,192-question Persian evaluation dataset showing GPT-4 achieving 50% accuracy versus 77% human performance

## Executive Summary
This paper presents Khayyam Challenge (PersianMMLU), the first comprehensive Persian-language evaluation framework for large language models. The dataset comprises 20,192 multiple-choice questions across 38 tasks spanning primary to upper secondary school education levels. Unlike previous Persian evaluations that relied on translated questions, this dataset is entirely original and includes rich metadata such as difficulty levels, educational stages, human performance benchmarks, and descriptive answers. The framework also incorporates "trapped" questions specifically designed to test model reasoning abilities rather than pattern matching.

Evaluation of nine leading LLMs revealed significant performance gaps, with GPT-4 achieving only 50% accuracy compared to human performance of 77%. Models performed particularly poorly in mathematics and natural sciences, suggesting limitations in reasoning capabilities when processing Persian text. The study demonstrates that while models can handle basic comprehension tasks, they struggle with complex reasoning and subject-specific knowledge at higher educational levels. This framework provides a crucial tool for assessing and improving Persian language model capabilities across educational domains.

## Method Summary
The PersianMMLU dataset was constructed by collecting original multiple-choice questions from Persian educational resources covering primary through upper secondary school levels. Questions were organized into 38 distinct tasks across various subjects including mathematics, sciences, literature, and social studies. Each question includes metadata about difficulty level, educational stage, and human performance benchmarks. The dataset was evaluated using nine different LLMs, with responses compared against correct answers and human performance baselines. Special attention was given to "trapped" questions designed to test genuine reasoning rather than memorization or pattern matching.

## Key Results
- GPT-4 achieved 50% accuracy on PersianMMLU, significantly below human performance of 77%
- Models showed particular weakness in mathematics and natural sciences domains
- Persian language models demonstrate limited reasoning abilities compared to human performance
- The dataset successfully distinguishes between basic comprehension and genuine reasoning through "trapped" questions

## Why This Works (Mechanism)
The PersianMMLU framework works by providing a comprehensive, multi-domain evaluation that tests both language understanding and reasoning capabilities in Persian. By using original questions rather than translations, it avoids biases introduced by translation artifacts and captures authentic Persian language usage patterns. The inclusion of metadata allows for nuanced analysis of model performance across different difficulty levels and educational stages, while "trapped" questions specifically target reasoning abilities by preventing simple pattern matching or memorization-based approaches.

## Foundational Learning
**Persian language structure**: Understanding Persian grammar, syntax, and vocabulary is essential since the dataset tests genuine language comprehension rather than translation-based approaches.
*Why needed*: Persian has unique linguistic features that affect how models process and understand text.
*Quick check*: Models must correctly interpret Persian-specific grammatical constructions and idiomatic expressions.

**Educational curriculum design**: Knowledge of Persian educational standards across primary to upper secondary levels ensures appropriate question difficulty and subject coverage.
*Why needed*: Questions must align with actual learning progressions to accurately assess model capabilities.
*Quick check*: Question difficulty should correlate with educational stage and show appropriate progression.

**Multiple-choice question design**: Understanding principles of effective multiple-choice question construction, including distractor design and unambiguous correct answers.
*Why needed*: Well-designed questions are crucial for valid assessment of model capabilities.
*Quick check*: Questions should have clear correct answers that require genuine understanding rather than guessing.

## Architecture Onboarding

**Component map**: Data Collection -> Question Curation -> Metadata Annotation -> Dataset Compilation -> Model Evaluation -> Performance Analysis

**Critical path**: Data Collection → Question Curation → Metadata Annotation → Model Evaluation → Performance Analysis

**Design tradeoffs**: Original questions vs. translated questions (authenticity vs. scalability), comprehensive coverage vs. focused domains, metadata richness vs. simplicity, trapped questions vs. straightforward questions.

**Failure signatures**: Low performance across all subjects suggests fundamental language comprehension issues; poor performance specifically in mathematics and sciences indicates reasoning deficits; inconsistent performance across difficulty levels may reveal inadequate handling of complex linguistic structures.

**First 3 experiments**:
1. Evaluate model performance on individual educational levels to identify specific weaknesses
2. Test model responses to "trapped" questions versus regular questions to verify reasoning assessment
3. Analyze error patterns across different subject domains to understand capability gaps

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- GPT-4 achieved only 50% accuracy compared to 77% human performance, indicating significant capability gaps
- The paper lacks detailed error analysis to determine whether failures stem from language comprehension or reasoning deficits
- Limited model diversity with only 9 models evaluated, potentially missing important performance variations

## Confidence
- **High**: Dataset construction methodology and evaluation procedures are well-documented and transparent
- **Medium**: Comparative performance claims between models are supported but lack granular breakdowns by task type
- **Low**: Interpretation of "limited reasoning abilities" is not systematically validated through error analysis

## Next Checks
1. Conduct detailed error analysis categorizing failures by type (language comprehension, factual knowledge, reasoning) to better understand the source of model performance gaps
2. Test model performance on individual tasks and educational levels to identify specific areas of strength and weakness rather than aggregate scores
3. Compare performance on the "trapped" questions versus regular questions to verify they effectively measure reasoning abilities as intended