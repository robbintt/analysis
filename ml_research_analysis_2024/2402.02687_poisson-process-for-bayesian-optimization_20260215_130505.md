---
ver: rpa2
title: Poisson Process for Bayesian Optimization
arxiv_id: '2402.02687'
source_url: https://arxiv.org/abs/2402.02687
tags:
- function
- popbo
- ranking
- optimization
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel ranking-based surrogate model based
  on the Poisson process for Bayesian optimization, aiming to estimate the relative
  rankings of candidates rather than their absolute function values. The key idea
  is to model the ranking of each candidate over the whole feasible domain via Poisson
  process, which is naturally suitable since the ranking of a candidate can be figured
  out by counting the number of better candidates.
---

# Poisson Process for Bayesian Optimization

## Quick Facts
- arXiv ID: 2402.02687
- Source URL: https://arxiv.org/abs/2402.02687
- Reference count: 40
- Primary result: Proposes Poisson Process Bayesian Optimization (PoPBO) that models rankings via Poisson processes, achieving lower computational costs (O(NÂ²) vs O(NÂ³)) and better noise robustness than GP-BO

## Executive Summary
This paper introduces a novel ranking-based surrogate model for Bayesian optimization using Poisson processes. Rather than modeling absolute function values, PoPBO estimates the relative rankings of candidates over the search space, which is naturally suited to Poisson process modeling since rankings can be expressed as counting better candidates. The approach is particularly effective when dealing with noisy observations, as rankings are more robust to noise than absolute values. The authors develop two tailored acquisition functions (R-LCB and ERI) to accommodate the ranking-based response surface and demonstrate superior performance on both simulated and real-world benchmarks.

## Method Summary
PoPBO models the ranking of each candidate over the feasible domain using a Poisson process, where the expected ranking Î»Î¾(x;Î¸) is approximated by a multi-layer perceptron (MLP). The model is trained via maximum likelihood estimation with computational complexity O(NÂ²). Two rectified acquisition functionsâ€”R-LCB (rectified Lower Confidence Bound) and ERI (Expected Ranking Improvement)â€”are derived to handle the ranking-based surrogate model while preventing over-exploitation. The method selects the next query point by optimizing these acquisition functions and iteratively updates the model with new observations.

## Key Results
- PoPBO achieves lower computational complexity (O(NÂ²)) compared to GP-BO (O(NÂ³))
- The method demonstrates better robustness to noise in both simulated and real-world benchmarks
- PoPBO outperforms standard BO methods on hyperparameter optimization and neural architecture search tasks
- The ranking-based approach shows improved transferability across different fidelity levels

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Ranking-based response surfaces modeled by Poisson processes are more robust to noise than absolute value-based Gaussian process models.
- **Mechanism:** The Poisson process models the discrete count of better candidates, and rankings are preserved under additive Gaussian noise unless the noise exceeds the gap between function values. In contrast, Gaussian processes directly model function values, so noise directly corrupts predictions.
- **Core assumption:** Additive Gaussian noise with small variance relative to function value gaps.
- **Evidence anchors:**
  - [abstract] "absolute response can be sensitive to noise" and "relative response is more robust to noise than absolute response since relation such as ranking between candidates is hard to be disrupted by noise"
  - [section] "if ð‘“ (ð‘¥2) âˆ’ ð‘“ (ð‘¥1) > 2âˆš2ðœŽ, the probability of correctly ranking ð‘¥1, ð‘¥2 is larger than 97.72%"
  - [corpus] No direct citations found; evidence is internal to the paper.
- **Break condition:** If noise is large enough to reverse rankings between candidates, the robustness advantage disappears.

### Mechanism 2
- **Claim:** The rectified acquisition functions (R-LCB and ERI) prevent over-exploitation in Poisson-based Bayesian optimization.
- **Mechanism:** Poisson distributions have variance equal to mean, leading to high uncertainty in ranking predictions for large means. R-LCB introduces a threshold to force exploration when predicted rankings are too high. ERI caps improvement by a worst tolerable ranking to limit exploitation.
- **Core assumption:** Poisson distribution properties (mean = variance) cause high uncertainty for large means.
- **Evidence anchors:**
  - [section] "Poisson distribution with a large expectation also has a large variance, indicating less confidence in the ranking prediction. The vanilla LCB will be trapped into an over-exploitation issue."
  - [section] "we propose a rectified LCB (R-LCB) to restrict the lower value to a threshold"
- **Break condition:** If the quantile parameter q is set too high, the method may revert to vanilla LCB/EI and lose robustness to over-exploitation.

### Mechanism 3
- **Claim:** The Poisson process naturally models ranking as a counting process, making it computationally efficient.
- **Mechanism:** Ranking of a candidate over the domain can be expressed as the count of better candidates. The Poisson process models this count with a tractable likelihood, enabling efficient training via MLE with complexity O(NÂ²) versus O(NÂ³) for Gaussian processes.
- **Core assumption:** Rankings over disjoint areas are independent (independent increment property).
- **Evidence anchors:**
  - [section] "we assume the rankings of ð‘¥ over two disjoint areas are independent... Hence, we can model Ë†ð‘…ð‘¥ (ð‘†), âˆ€ð‘† âŠ‚ ð‘‹ as an independent increment counting process."
  - [section] "The computational complexity of PoPBO is ð‘‚ (ð‘ 2), much lower than that of GP-BO (ð‘‚ (ð‘ 3))."
- **Break condition:** If the independence assumption is violated (e.g., strong correlation between rankings in different regions), the model's efficiency and accuracy degrade.

## Foundational Learning

- **Concept:** Poisson Process as a Counting Process
  - Why needed here: To model the ranking of candidates as discrete count events, naturally suited for capturing global rankings over the search space.
  - Quick check question: Why is a Poisson process appropriate for modeling rankings? Because ranking is essentially counting how many candidates are better, and Poisson processes model discrete event counts.

- **Concept:** Bayesian Optimization with Surrogate Models
  - Why needed here: To efficiently optimize black-box functions by iteratively building a probabilistic model (surrogate) and selecting the next query point via an acquisition function.
  - Quick check question: What are the two main steps in Bayesian optimization? Training a surrogate model and selecting the next query via an acquisition function.

- **Concept:** Additive Gaussian Noise and Its Effect on Rankings
  - Why needed here: To analyze why relative metrics (rankings) are more robust to noise than absolute values in optimization.
  - Quick check question: Under what condition does Gaussian noise not affect the correctness of a ranking? When the gap between function values is larger than the noise level.

## Architecture Onboarding

- **Component map:**
  - Poisson Process Surrogate Model -> R-LCB and ERI Acquisition Functions -> MLE Training Loop -> Query Selection -> Evaluation -> Update Observations

- **Critical path:**
  1. Initialize with random samples and compute rankings
  2. Train Poisson surrogate model via MLE
  3. Optimize acquisition function (R-LCB or ERI) to select next query
  4. Evaluate next query and update observations
  5. Repeat until convergence

- **Design tradeoffs:**
  - Poisson process vs. Gaussian process: Lower complexity (O(NÂ²) vs. O(NÂ³)) but requires careful handling of ranking independence assumptions
  - Ranking-based vs. value-based: More robust to noise but loses absolute value information
  - Rectified acquisition functions: Prevent over-exploitation but add hyperparameters (q) to tune

- **Failure signatures:**
  - Poor performance on smooth functions where absolute values are more informative than rankings
  - Over-exploration if q is set too low in R-LCB/ERI
  - Numerical instability in MLP training if rankings are not properly normalized

- **First 3 experiments:**
  1. Test on Forrester function with varying noise levels to verify robustness claims
  2. Compare convergence speed on Hartmann-6d benchmark against GP-BO
  3. Ablation study on q parameter in R-LCB/ERI to find optimal exploration-exploitation balance

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas for future work are implied by the limitations section and the need for further empirical validation.

## Limitations
- The ranking-based approach may underperform on smooth functions where absolute values provide more informative gradients for optimization
- The independent increment assumption for Poisson processes may not hold in practice, potentially degrading performance on correlated functions
- The method requires careful tuning of the quantile parameter q in rectified acquisition functions, with limited guidance provided for optimal selection

## Confidence
- **High Confidence:** The computational complexity improvement claim (O(NÂ²) vs O(NÂ³)) and the general robustness to noise through ranking preservation
- **Medium Confidence:** The effectiveness of rectified acquisition functions (R-LCB and ERI) in preventing over-exploitation, based on theoretical justification but requiring empirical validation
- **Low Confidence:** The universal applicability of the ranking-based approach across all black-box optimization scenarios, particularly for functions where absolute values contain crucial information

## Next Checks
1. **Independence Assumption Validation:** Test the Poisson model on functions with strong spatial correlations to assess how violations of the independent increment assumption affect performance
2. **Hyperparameter Sensitivity Analysis:** Systematically vary the quantile parameter q across multiple benchmark functions to identify optimal ranges and potential failure modes
3. **Baseline Comparison on Smooth Functions:** Compare PoPBO against GP-BO on smooth, well-behaved functions where absolute values are more informative than rankings to identify the method's limitations