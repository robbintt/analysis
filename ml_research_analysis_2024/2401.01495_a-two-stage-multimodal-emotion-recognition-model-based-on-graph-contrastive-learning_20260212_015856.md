---
ver: rpa2
title: A Two-Stage Multimodal Emotion Recognition Model Based on Graph Contrastive
  Learning
arxiv_id: '2401.01495'
source_url: https://arxiv.org/abs/2401.01495
tags:
- emotion
- learning
- recognition
- graph
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TS-GCL, a two-stage multimodal emotion recognition
  model based on graph contrastive learning. The authors address the limitations of
  existing MER methods that perform single-stage classification and ignore similarities
  and differences between morphological features.
---

# A Two-Stage Multimodal Emotion Recognition Model Based on Graph Contrastive Learning

## Quick Facts
- arXiv ID: 2401.01495
- Source URL: https://arxiv.org/abs/2401.01495
- Reference count: 27
- Primary result: TS-GCL achieves 70.3% accuracy and 70.2% F1-score on IEMOCAP, outperforming previous methods

## Executive Summary
This paper introduces TS-GCL, a two-stage multimodal emotion recognition model that addresses the limitations of existing single-stage classification approaches. The model employs a novel graph contrastive learning strategy to capture similarities and differences within and between modalities (text, audio, visual). Experiments on the IEMOCAP and MELD datasets demonstrate TS-GCL's superior performance compared to previous methods, with the ablation study confirming the effectiveness of both the two-stage classification and graph contrastive learning components.

## Method Summary
TS-GCL uses a two-stage classification approach where the model first classifies emotional polarity (positive, negative, neutral) and then classifies finer dynamic categories. The model incorporates graph contrastive learning to reduce modality gaps by contrasting intra-class and inter-class samples across text, audio, and visual modalities. The framework extracts features using RoBERTa for text, OpenSMILE for audio, and 3D-CNN for vision, then fuses these through a graph-based contrastive learning mechanism before passing through the two-stage classification process.

## Key Results
- Achieves 70.3% accuracy and 70.2% F1-score on IEMOCAP dataset
- Outperforms previous methods including Multi-Task, 3D-Transformer, and MM-GCN
- Ablation study confirms both two-stage classification and graph contrastive learning significantly contribute to performance improvements

## Why This Works (Mechanism)

### Mechanism 1
Two-stage classification improves accuracy by decomposing the task into simpler sub-tasks. The model first classifies emotional polarity, then finer dynamic categories, allowing it to focus on different levels of emotional information separately. This staged approach reduces complexity and makes the model easier to train. The approach may fail if first stage classification is inaccurate, negatively impacting the second stage.

### Mechanism 2
Graph contrastive learning reduces modality gaps and improves feature learning by contrasting intra-class and inter-class samples. The model constructs graphs for each modality and learns to minimize distance between positive samples (same emotion, different modalities) while maximizing distance between negative samples (different emotions). This may be limited if graph construction fails to capture utterance relationships or if contrastive learning objective misaligns with emotion recognition.

### Mechanism 3
Multimodal fusion captures diversity and complexity of human emotions better than single-modality approaches. By combining text, audio, and visual features through graph contrastive learning, the model leverages complementary information from different modalities. This may introduce noise or redundancy if modalities are not well-aligned or if preprocessing methods fail to extract relevant features.

## Foundational Learning

- **Graph Convolutional Networks (GCNs)**: Used to encode contextual features and learn graph representations from multimodal data. Quick check: How do GCNs differ from traditional CNNs, and why are they suitable for graph-structured data?

- **Contrastive Learning**: Core to the graph contrastive learning strategy for learning similarities and differences within and between modalities. Quick check: What is the core idea behind contrastive learning, and how does it help learn more discriminative features?

- **Multimodal Fusion**: Combines information from text, audio, and visual modalities to capture comprehensive emotion representations. Quick check: What are different approaches to multimodal fusion, and how do they differ in strengths and weaknesses?

## Architecture Onboarding

- **Component map**: Feature Extraction (RoBERTa, OpenSMILE, 3D-CNN) -> Graph Construction (nodes=utterances, edges=relationships) -> Graph Contrastive Learning (contrastive learning strategy) -> Two-Stage Classification (polarity then dynamic categories) -> Multimodal Fusion (graph-based fusion)

- **Critical path**: Feature Extraction -> Graph Construction -> Graph Contrastive Learning -> Two-Stage Classification -> Multimodal Fusion

- **Design tradeoffs**: Two-stage classification adds complexity but may improve accuracy by decomposing tasks; graph contrastive learning is computationally expensive but reduces modality gaps; multimodal fusion may introduce noise if modalities aren't well-aligned

- **Failure signatures**: Poor performance on individual modalities suggests feature extraction or fusion issues; overfitting indicates need for regularization; inconsistent cross-dataset performance suggests domain adaptation problems

- **First 3 experiments**: 1) Evaluate two-stage classification on single modality to assess impact on accuracy, 2) Compare different graph construction methods to determine most effective edge weighting strategy, 3) Assess graph contrastive learning impact by comparing model with and without GCL on validation set

## Open Questions the Paper Calls Out

### Open Question 1
How does the two-stage classification method compare to other multi-stage or hierarchical classification approaches in terms of computational efficiency and overall performance on larger datasets? The paper shows improved performance but lacks detailed comparison of computational efficiency or performance on larger datasets with other approaches.

### Open Question 2
What is the impact of different edge weighting strategies on graph contrastive learning performance, and how sensitive is the model to hyperparameters (ω, κ, ϱ)? The paper mentions these hyperparameters but doesn't explore their impact or provide sensitivity analysis.

### Open Question 3
How does TS-GCL perform on datasets with more balanced class distributions, and what modifications are needed to improve performance on underrepresented emotion categories? The paper notes worse performance on fear and disgust in MELD's imbalanced dataset but doesn't explore modifications for balanced datasets.

## Limitations
- Evaluation lacks comparison with more recent state-of-the-art methods
- Ablation study could be more comprehensive
- Does not provide detailed analysis of model's performance on individual modalities

## Confidence

- **High Confidence**: Experimental results showing improved performance compared to baseline methods on IEMOCAP and MELD datasets
- **Medium Confidence**: Claims about effectiveness of two-stage classification and graph contrastive learning in capturing emotional information and reducing modality gaps
- **Low Confidence**: Generalizability to other datasets or domains, and robustness to noise or input variations

## Next Checks

1. Conduct more comprehensive ablation study to isolate impact of each component (two-stage classification, graph contrastive learning, multimodal fusion) on performance
2. Evaluate model performance on additional datasets or in different domains to assess generalizability and robustness
3. Compare performance with more recent state-of-the-art methods for multimodal emotion recognition to provide comprehensive evaluation