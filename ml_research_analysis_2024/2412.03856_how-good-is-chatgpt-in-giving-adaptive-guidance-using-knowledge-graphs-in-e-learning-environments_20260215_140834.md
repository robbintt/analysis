---
ver: rpa2
title: How Good is ChatGPT in Giving Adaptive Guidance Using Knowledge Graphs in E-Learning
  Environments?
arxiv_id: '2412.03856'
source_url: https://arxiv.org/abs/2412.03856
tags:
- feedback
- student
- learning
- knowledge
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: To address personalized feedback challenges in e-learning, this
  study integrates dynamic knowledge graphs with ChatGPT4 to assess students' prerequisite
  comprehension and generate adaptive guidance. For easy questions, ROUGE-1 F-scores
  between standard and personalized feedback were high (~0.47-0.52), indicating consistent
  support.
---

# How Good is ChatGPT in Giving Adaptive Guidance Using Knowledge Graphs in E-Learning Environments?

## Quick Facts
- **arXiv ID**: 2412.03856
- **Source URL**: https://arxiv.org/abs/2412.03856
- **Reference count**: 40
- **Primary result**: Personalized LLM feedback improves perceived usefulness but requires human oversight to mitigate errors.

## Executive Summary
This study integrates dynamic knowledge graphs with ChatGPT-4 to provide adaptive guidance in e-learning by assessing students' prerequisite comprehension. The system uses a textbook-derived knowledge graph to diagnose knowledge gaps and tailor feedback to three student profiles (S1: poor, S2: average, S3: advanced). ROUGE analysis shows increased personalization with question difficulty, and expert evaluations confirm high correctness but variable precision. A small pilot study indicates improved user perceptions of system usefulness. However, LLM errors and moderate inter-rater reliability highlight the need for human oversight.

## Method Summary
The method constructs a knowledge graph from a math textbook using "GO for Help" indicators to identify prerequisite concepts. For each student question, the system retrieves relevant prerequisites, assesses the student's knowledge level (good/average/poor), and generates a personalized prompt for ChatGPT-4 including the question, standard solution, student impasse, and ranked prerequisites. Feedback is evaluated using ROUGE metrics against the standard solution and between student profiles, alongside expert ratings on correctness, precision, hallucinations, and variability.

## Key Results
- ROUGE-1 F-scores decreased with question difficulty, indicating increased personalization.
- Expert evaluations showed high correctness (mean 4-5/5) but variable precision and rare hallucinations.
- Inter-rater reliability was moderate (Cohen's Kappa 0.30-0.47).
- Pilot user study (n=6) revealed improved post-test perceptions of system usefulness.

## Why This Works (Mechanism)
### Mechanism 1
- **Claim**: Knowledge graphs enable systematic identification of prerequisite concepts and assessment of student knowledge gaps.
- **Mechanism**: Construct KG from textbook "GO for Help" indicators; use tree traversal to retrieve prerequisites; assess student performance on those prerequisites to infer comprehension level (good/average/poor).
- **Core assumption**: Textbook indicators are valid; performance correlates with understanding.
- **Evidence anchors**:
  - [abstract] "Central to this method is the knowledge graph’s role in assessing a student’s comprehension of topic prerequisites."
  - [section III.A] "We construct the knowledge graph by treating each unit in a chapter as a concept. To establish the relationship such as prerequisite between concepts, we use the 'GO for Help' indicators provided by the authors."
- **Break condition**: Inaccurate prerequisite mapping or sparse/noisy student data causes misdiagnosis and inappropriate feedback.

### Mechanism 2
- **Claim**: Augmenting LLM prompts with student-specific impasse and ranked prerequisites yields personalized feedback.
- **Mechanism**: Prompt P1 includes question, standard solution, impasse, and ranked prerequisites; LLM generates tailored explanation.
- **Core assumption**: LLMs can utilize added context; impasse and rankings are accurate.
- **Evidence anchors**:
  - [abstract] "By evaluating past and ongoing student interactions, the system identifies and appends the most salient learning context to prompts directed at the LLM."
  - [section III.D] Shows prompt P1 template.
- **Break condition**: Misidentified impasse or irrelevant prerequisites lead to generic or off-target feedback.

### Mechanism 3
- **Claim**: Tailoring feedback style based on diagnosed comprehension level improves relevance.
- **Mechanism**: Assign comprehension level (good/average/poor) from knowledge assessment; for good: advanced assistance; average: foundational review; poor: in-depth prerequisite explanations.
- **Core assumption**: These categories capture distinct learning needs.
- **Evidence anchors**:
  - [abstract] "Depending on the categorized understanding (good, average, or poor), the LLM adjusts its guidance..."
  - [section V] Expert ratings show high correctness but variable precision, suggesting adaptation is attempted.
- **Break condition**: Coarse categorization mismatches student needs, causing feedback to be too basic or advanced.

## Foundational Learning
- **Knowledge Graphs**
  - Why needed: Represent prerequisite relationships to diagnose missing knowledge.
  - Quick check: How would you extract prerequisite edges from a textbook's "GO for Help" sections?
- **LLM Prompt Engineering**
  - Why needed: LLM output is prompt-sensitive; must embed student context for personalization.
  - Quick check: Design a prompt that generates a tailored explanation for a student struggling with algebra, given their knowledge gaps on fractions and order of operations.
- **Student Knowledge Assessment**
  - Why needed: To infer comprehension level and prioritize prerequisites.
  - Quick check: Given a student's scores on prerequisite quizzes, how would you classify their understanding as good, average, or poor?

## Architecture Onboarding
- **Component map**:
  Knowledge Graph DB -> Student Logger -> Diagnosis Module -> Prompt Generator -> LLM Interface (ChatGPT4) -> Feedback UI
- **Critical path**:
  1. Student signals impasse on a question.
  2. System maps question to concept.
  3. Diagnosis fetches prerequisites and student's history → yields level and ranked list.
  4. Prompt Generator composes P1.
  5. LLM produces feedback.
  6. UI shows feedback.
- **Design tradeoffs**:
  - Simplified KG (textbook-based) vs. comprehensive: easier but may miss links.
  - Expert-provided impasses vs. automated: accurate but not scalable; automation needed for production.
  - Low temperature (0.2): deterministic but less diverse explanations.
  - ROUGE vs. human eval: automated but may not capture semantic quality.
- **Failure signatures**:
  - High ROUGE similarity across student types → lack of personalization.
  - Expert ratings: low precision/high hallucinations → LLM/prompt issues.
  - Mismatch between knowledge assessment and feedback style → ineffective adaptation.
- **First 3 experiments**:
  1. Replicate ROUGE analysis (Tables I-VII) across difficulty levels.
  2. Run expert evaluation (correctness, precision, hallucinations, variability); compute Cohen's Kappa.
  3. Conduct pilot user study with pre/post surveys on perceived usefulness and correctness.

## Open Questions the Paper Calls Out
The paper explicitly notes limitations in the scope of its evaluation, including reliance on a single textbook domain, simulated rather than real student interactions, and a small pilot study focused on perceptions rather than objective learning outcomes. It calls for future work to validate the system with real students across diverse subjects and to measure actual learning gains, not just perceived usefulness.

## Limitations
- Simplified knowledge graph based only on textbook "GO for Help" indicators may miss crucial prerequisite relationships.
- Manual creation of student impasse descriptions prevents direct replication and raises scalability concerns.
- Moderate inter-rater reliability (Cohen's Kappa 0.30-0.47) in expert evaluations introduces subjectivity.

## Confidence
- **High Confidence**: The core finding that ROUGE scores decrease with question difficulty, indicating increased personalization, is well-supported by the data.
- **Medium Confidence**: The claim that the system's feedback is generally correct (mean 4-5/5) is supported by expert ratings, but moderate inter-rater agreement and the reported drop in correctness when the standard solution is omitted warrant caution.
- **Low Confidence**: The assertion that the pilot user study showed improved post-test perceptions of the system's usefulness is based on a very small sample (n=6) and lacks detailed statistical analysis.

## Next Checks
1. **Reconstruct and Validate Knowledge Graph**: Obtain the specified textbook edition and manually reconstruct the knowledge graph using the "GO for Help" indicators. Verify the prerequisite relationships for the chosen units (1-2, 1-3, 3-2) and the accuracy of the extracted question-difficulty mappings.
2. **Conduct a Larger Expert Evaluation**: Assemble a larger panel of domain experts (n≥5) and conduct a blinded evaluation of the generated feedback using the same 5-point scales. Calculate new inter-rater reliability metrics and compare the distribution of ratings to the original study.
3. **Automated Impasse Detection Pilot**: Design and implement a simple automated method (e.g., keyword matching, rule-based) to detect student impasses from input text. Compare the automated impasses to the expert-provided ones for a subset of questions and assess the impact on feedback quality and personalization.