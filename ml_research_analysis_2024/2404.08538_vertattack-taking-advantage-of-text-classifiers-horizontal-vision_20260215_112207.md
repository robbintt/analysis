---
ver: rpa2
title: 'VertAttack: Taking advantage of Text Classifiers'' horizontal vision'
arxiv_id: '2404.08538'
source_url: https://arxiv.org/abs/2404.08538
tags:
- vertattack
- text
- classifier
- bert
- classifiers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VertAttack, a novel adversarial attack method
  for text classification systems. The key idea is to exploit the limitation of current
  classifiers, which can only process text horizontally, by rewriting words vertically.
---

# VertAttack: Taking advantage of Text Classifiers' horizontal vision

## Quick Facts
- **arXiv ID**: 2404.08538
- **Source URL**: https://arxiv.org/abs/2404.08538
- **Reference count**: 21
- **Key result**: VertAttack reduces text classifier accuracy by up to 90 percentage points while maintaining human readability

## Executive Summary
This paper introduces VertAttack, a novel adversarial attack method for text classification systems that exploits the horizontal processing limitation of current classifiers. The attack rewrites informative words vertically, making them difficult for classifiers to recognize while preserving meaning for human readers. Evaluated across five datasets and four transformer models, VertAttack demonstrates significant effectiveness with accuracy drops up to 90 percentage points. The attack also shows strong transferability to other classifiers and maintains human comprehension according to a human study.

## Method Summary
VertAttack works by first identifying the most informative words for a classifier's prediction, then rewriting these words vertically by inserting newline characters between letters. This transformation preserves the semantic content for human readers while creating patterns that disrupt the classifier's ability to recognize the original words. The attack uses optimization techniques to determine which words to perturb and how to rewrite them for maximum impact. The vertical rewriting exploits the fact that most text classifiers process text horizontally and are not designed to handle vertically oriented text patterns, creating a vulnerability that VertAttack leverages.

## Key Results
- Reduces RoBERTa accuracy on SST2 from 94% to 13% while humans maintain 77% accuracy
- Achieves up to 90 percentage point drops in classifier accuracy across tested datasets
- Demonstrates strong transferability to other classifier architectures
- Human study confirms perturbed texts remain comprehensible with minimal impact on understanding

## Why This Works (Mechanism)
VertAttack exploits the fundamental limitation that text classifiers process text in a horizontal manner, reading words from left to right. By vertically rewriting informative words with newline characters between letters, the attack creates text that appears normal to humans but disrupts the horizontal pattern recognition that classifiers rely on. The vertical arrangement of characters breaks the word recognition process that transformers and other models use, as they cannot easily parse vertically stacked letters as coherent words. This geometric transformation of text structure is particularly effective because it preserves semantic meaning while fundamentally altering the visual and spatial patterns that classifiers use for decision-making.

## Foundational Learning

**Text Classification with Transformers**: Modern text classifiers use transformer architectures to process sequential text data horizontally. Why needed: Understanding this horizontal processing limitation is crucial to grasping VertAttack's core vulnerability exploitation. Quick check: Can the model recognize vertically written words?

**Adversarial Attack Optimization**: Techniques for finding optimal perturbations that maximize classifier error while maintaining input validity. Why needed: The attack must identify which words to perturb and how to rewrite them effectively. Quick check: Does the optimization successfully identify the most informative words?

**Human Text Comprehension**: Humans can still understand vertically written words due to their ability to recognize patterns and context. Why needed: Validates that the attack preserves semantic meaning despite geometric transformation. Quick check: Can humans correctly classify sentiment in vertically perturbed texts?

## Architecture Onboarding

**Component Map**: Text Input -> Informative Word Identification -> Vertical Rewriting -> Perturbed Text -> Classifier
**Critical Path**: The attack's success depends on accurately identifying informative words and effectively rewriting them vertically without losing semantic content.
**Design Tradeoffs**: Balance between attack strength (more words perturbed) and human readability (fewer words perturbed). The attack must maintain enough context for human understanding while disrupting classifier patterns.
**Failure Signatures**: If the attack fails to identify truly informative words, the perturbation will have minimal impact on classifier accuracy. If too many words are perturbed, human comprehension may suffer significantly.
**First Experiments**:
1. Test classifier accuracy on texts with single words vertically rewritten
2. Evaluate human comprehension on progressively more perturbed texts
3. Measure attack effectiveness against different transformer architectures

## Open Questions the Paper Calls Out
None

## Limitations
- Human evaluation based on small sample size of 20 participants may not capture full range of human perception
- Defense mechanisms explored are preliminary with limited approaches tested
- Computational efficiency and scalability of the optimization process not thoroughly analyzed
- Transferability results limited to four model architectures, leaving questions about broader applicability

## Confidence

**High confidence**: Core attack methodology and basic effectiveness against tested models are well-demonstrated through quantitative results.
**Medium confidence**: Human perception study showing maintained readability, limited by sample size and evaluation scope.
**Medium confidence**: Transferability claims, based on consistent performance across four models but lacking broader architectural diversity.

## Next Checks

1. **Expand human evaluation** with larger, more diverse participant pool (50-100 participants) and test comprehension across different linguistic tasks beyond sentiment classification.
2. **Test against additional model architectures and scales**, including larger models and different architectures to better understand attack transferability and limitations.
3. **Conduct computational complexity analysis** to measure time and resources required for optimization process, assessing real-world scalability and practicality.