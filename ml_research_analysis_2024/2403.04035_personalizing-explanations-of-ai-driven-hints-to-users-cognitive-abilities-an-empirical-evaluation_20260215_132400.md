---
ver: rpa2
title: 'Personalizing explanations of AI-driven hints to users'' cognitive abilities:
  an empirical evaluation'
arxiv_id: '2403.04035'
source_url: https://arxiv.org/abs/2403.04035
tags:
- explanations
- users
- explanation
- interface
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates personalizing explanations of AI-driven
  hints in an Intelligent Tutoring System (ITS) for users with low Need for Cognition
  and Conscientiousness traits. The personalization involved delivering the first
  page of the explanation interface upfront with the hint and adding a confirmation
  box to warn users if they tried to close the explanations too quickly.
---

# Personalizing explanations of AI-driven hints to users' cognitive abilities: an empirical evaluation

## Quick Facts
- arXiv ID: 2403.04035
- Source URL: https://arxiv.org/abs/2403.04035
- Reference count: 13
- Primary result: Personalized explanation interface significantly increased engagement and learning gains for users with low Need for Cognition and Conscientiousness traits.

## Executive Summary
This paper presents an empirical evaluation of personalizing AI-driven hint explanations in an Intelligent Tutoring System based on users' cognitive traits. The study focused on users with low Need for Cognition (N4C) and Conscientiousness, implementing a personalized interface that proactively delivered the first explanation page upfront and added a confirmation box to warn users who tried to close explanations too quickly. The personalized approach significantly increased engagement with explanations, perceived understanding of hints, and learning gains (0.7 vs 0.2 for control group, p<0.001). However, some users found the explanations distracting, indicating room for refinement.

## Method Summary
The study implemented two key personalization features in the ACSP ITS: upfront delivery of the first explanation page and a confirmation box triggered when users attempted to close explanations quickly. A user study with 39 participants (23 experimental, 16 control) compared the personalized interface against the original. Participants completed a pre-test, worked on problems with AI hints, and finished with a post-test. The study measured engagement through eye-tracking data, learning gains through pre/post tests, and subjective ratings of usefulness and intrusiveness.

## Key Results
- Personalized interface increased LNLC users' interaction with explanations (more pages accessed, higher eye-tracking attention)
- Significant improvement in learning gains for personalized group (0.7 vs 0.2, p<0.001)
- Strong correlation between attention to explanations and learning gains (rs=0.5, p=0.007)
- Some users found explanations distracting/confusing despite increased engagement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Proactive upfront delivery of the first explanation page increases initial engagement for low Need for Cognition (N4C) users.
- Mechanism: Low N4C users are less likely to seek out additional cognitive effort voluntarily. By automatically presenting the first explanation page, the system bypasses the user's low motivation to engage, ensuring exposure to the content.
- Core assumption: The mere exposure to the explanation content, even without active seeking, will be sufficient to trigger attention and initial processing.
- Evidence anchors:
  - [abstract] states that the personalized interface significantly increases LNLC users' interaction with explanations, with the upfront delivery being a key component.
  - [section 4] describes the upfront delivery approach as the one adopted after pilot testing, suggesting it was effective compared to the alternative of disabling hint closure.
- Break condition: If the upfront content is too complex or overwhelming, it could backfire and cause users to disengage entirely.

### Mechanism 2
- Claim: The confirmation box acts as a behavioral nudge to extend time spent on explanations for users who might otherwise leave too early.
- Mechanism: By introducing a pause and a prompt when a user attempts to close the explanation quickly, the system creates a moment of reflection that can lead to continued engagement.
- Core assumption: Users who close explanations early do so out of habit or distraction, not because they have fully processed the information.
- Evidence anchors:
  - [section 4] details the addition of a confirmation box triggered after a median time threshold (27 seconds) based on high N4C users' behavior.
  - [section 6.1] notes that only 3 out of 10 users who received the confirmation box actually stayed longer, indicating limited but present impact.
- Break condition: If the threshold for triggering the box is not well-calibrated (e.g., too short or too long), it may either annoy users or fail to catch those who are truly disengaging prematurely.

### Mechanism 3
- Claim: Increased interaction with explanations leads to better perceived understanding of hints and improved learning outcomes.
- Mechanism: By engaging more deeply with the explanations, users can connect the AI's reasoning to the hints, which enhances their comprehension and retention of the underlying concepts.
- Core assumption: The explanations are well-structured and effectively communicate the rationale for the hints, so that increased exposure translates to better understanding.
- Evidence anchors:
  - [abstract] reports that the personalized interface significantly increases users' perceived understanding of hints.
  - [section 6.2] shows a statistically significant correlation (rs=0.5, p=0.007) between attention to explanations (AE) and learning gains (PLG), suggesting a relationship between engagement and learning.
- Break condition: If the explanations are poorly designed or too complex, increased interaction could lead to confusion rather than understanding.

## Foundational Learning

- Concept: Cognitive Load Theory
  - Why needed here: Understanding how the complexity of explanations and the proactive delivery affect the user's cognitive resources is crucial for designing effective personalization.
  - Quick check question: If a user has low N4C, how might presenting a multi-page explanation upfront affect their cognitive load compared to on-demand access?

- Concept: Personalization in AI Systems
  - Why needed here: The study builds on the idea that user differences (like personality traits) should inform how AI systems present information, moving beyond one-size-fits-all approaches.
  - Quick check question: What are the potential risks of personalizing explanations based on traits like N4C and Conscientiousness?

- Concept: User Interface Design for Learning
  - Why needed here: The effectiveness of the explanation interface depends on its usability, visibility, and integration with the learning task, which are all aspects of UI design.
  - Quick check question: How might the placement and prominence of the explanation window influence a user's decision to engage with it?

## Architecture Onboarding

- Component map:
  ACSP Applet -> FUMA Framework (AI hint generation) -> Explanation Interface (UI component) -> User Model (tracks N4C, Conscientiousness, engagement metrics) -> Eye-Tracking Integration (attention measurement) -> Learning Assessment (pre/post tests)

- Critical path: Hint generation → Explanation interface activation (upfront) → User interaction tracking → Learning assessment
- Design tradeoffs:
  - Proactive delivery vs. user autonomy: Ensuring exposure vs. respecting user choice.
  - Complexity of explanations vs. user cognitive capacity: Providing depth vs. avoiding overwhelm.
  - Real-time adaptation vs. pre-defined personalization: Dynamic adjustment vs. simplicity.
- Failure signatures:
  - Users consistently dismissing the confirmation box without staying longer.
  - High engagement metrics but low learning gains, suggesting ineffective explanations.
  - Users reporting confusion or distraction despite increased interaction.
- First 3 experiments:
  1. Test different thresholds for the confirmation box (e.g., 15s, 30s, 45s) to find the optimal balance between nudging and annoyance.
  2. A/B test upfront delivery of the first explanation page vs. on-demand access with a prominent "Why?" button to see which drives more engagement for LNLC users.
  3. Vary the complexity and length of the first explanation page to determine the optimal amount of information for initial engagement without causing cognitive overload.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the level of Reading Proficiency affect the effectiveness of personalized explanations in the ACSP applet?
- Basis in paper: [explicit] The paper mentions that future work will investigate personalizing the ACSP explanations to a user's level of Reading Proficiency, another user characteristic identified as relevant by previous work.
- Why unresolved: The study did not evaluate the impact of Reading Proficiency on the effectiveness of personalized explanations.
- What evidence would resolve it: Conducting a user study that measures the interaction, perceived understanding, and learning gains of users with varying levels of Reading Proficiency when using the personalized explanation interface.

### Open Question 2
- Question: What is the optimal threshold for triggering the confirmation box to warn users who try to close the explanation interface too quickly?
- Basis in paper: [inferred] The study found that the confirmation box had limited impact on increasing users' time spent in the explanation interface, and some users who dismissed the box had previously accessed the explanation interface for longer than the threshold used.
- Why unresolved: The study did not investigate the optimal threshold for triggering the confirmation box, and the current threshold may not be suitable for all users.
- What evidence would resolve it: Conducting a user study that tests different thresholds for triggering the confirmation box and measures their impact on users' time spent in the explanation interface and their overall interaction with the explanations.

### Open Question 3
- Question: How does the frequency of explanation delivery affect users' perception of the explanations and their overall interaction with the ACSP applet?
- Basis in paper: [explicit] The study found that some users perceived the hints and explanation interface to appear too often, which could be distracting and confusing.
- Why unresolved: The study did not investigate the optimal frequency of explanation delivery, and the current frequency may not be suitable for all users.
- What evidence would resolve it: Conducting a user study that tests different frequencies of explanation delivery and measures their impact on users' perceived understanding of the hints, their interaction with the explanations, and their overall learning gains.

## Limitations
- Sample size of 39 participants limits generalizability of findings
- Focus only on users with low Need for Cognition and Conscientiousness traits
- No investigation of long-term effects or retention of learning gains
- Limited evidence for effectiveness of confirmation box mechanism

## Confidence
- High Confidence: Personalized explanations significantly increased engagement metrics for LNLC users
- Medium Confidence: Increased engagement translates to improved learning gains and perceived understanding
- Low Confidence: Confirmation box mechanism significantly impacts user behavior

## Next Checks
1. Replication with larger, more diverse sample (100+ participants across broader trait range)
2. Longitudinal study of learning retention over weeks/months
3. A/B testing of alternative personalization strategies (adaptive complexity, user-controlled depth)