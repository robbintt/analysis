---
ver: rpa2
title: 'RuleAlign: Making Large Language Models Better Physicians with Diagnostic
  Rule Alignment'
arxiv_id: '2408.12579'
source_url: https://arxiv.org/abs/2408.12579
tags:
- medical
- patient
- diagnostic
- llms
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RuleAlign addresses the challenge of aligning large language models
  with diagnostic rules in medical settings. It introduces a framework that aligns
  LLMs with specific diagnostic rules through preference learning, using a rule-based
  medical dialogue dataset called UrologyRD.
---

# RuleAlign: Making Large Language Models Better Physicians with Diagnostic Rule Alignment

## Quick Facts
- arXiv ID: 2408.12579
- Source URL: https://arxiv.org/abs/2408.12579
- Authors: Xiaohan Wang; Xiaoyan Yang; Yuqi Zhu; Yue Shen; Jian Wang; Peng Wei; Lei Liang; Jinjie Gu; Huajun Chen; Ningyu Zhang
- Reference count: 28
- One-line primary result: RuleAlign significantly improves LLM diagnostic performance through rule alignment using preference learning without human annotation

## Executive Summary
RuleAlign addresses the challenge of aligning large language models with diagnostic rules in medical settings by introducing a framework that generates and optimizes preference data for alignment. The approach uses semantic similarity filtering and order disruption to create effective learning signals without requiring human annotation. Experiments demonstrate that RuleAlign significantly improves model performance across multiple evaluation metrics including perplexity, ROUGE, and BLEU scores, with superior performance in both single-round evaluation and multi-round standardized patient testing.

## Method Summary
RuleAlign aligns LLMs with diagnostic rules through preference learning using a rule-based medical dialogue dataset called UrologyRD. The framework generates preference pairs by optimizing dispreferred completions through semantic similarity filtering and order disruption, then applies Direct Preference Optimization (DPO) to train the model. The approach involves constructing a rule-based dialogue dataset from the RJUA-QA dataset using GPT-4 turbo API, performing supervised fine-tuning, and then conducting preference optimization using the generated preference pairs. The method is evaluated across multiple metrics including information completeness, diagnostic logicality, and guidance rationality in both single-round and multi-round standardized patient testing scenarios.

## Key Results
- RuleAlign significantly improves model performance across multiple metrics including perplexity, ROUGE, and BLEU scores
- The method demonstrates superior performance in both single-round evaluation and multi-round standardized patient testing
- RuleAlign achieves higher scores in information completeness, diagnostic logicality, and guidance rationality compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RuleAlign improves LLM performance by aligning responses with diagnostic rules through preference learning
- Mechanism: The framework generates preference pairs by optimizing dispreferred completions using semantic similarity filtering and order disruption, then applies Direct Preference Optimization (DPO) to train the model
- Core assumption: The semantic similarity between dispreferred and preferred completions is a good indicator of learning quality
- Evidence anchors: [abstract] "Experimental results demonstrate the effectiveness of the proposed approach"; [section] "We propose an efficient method, RuleAlign, which is designed to automatically generate and optimize preference data for alignment"
- Break condition: If semantic similarity does not correlate with learning quality, or if order disruption introduces too much noise

### Mechanism 2
- Claim: RuleAlign's preference pair optimization strategy significantly improves single-round evaluation performance
- Mechanism: By filtering dispreferred completions based on semantic similarity to preferred completions and disrupting dialogue order, RuleAlign creates more effective learning signals for the DPO algorithm
- Core assumption: Disrupting dialogue order helps the model learn to avoid logical inconsistencies in diagnostic reasoning
- Evidence anchors: [abstract] "The method demonstrates superior performance in both single-round evaluation and multi-round standardized patient testing"; [section] "we propose an approach to enhance dispreferred completions yl through sample filtering using semantic similarity and the deliberate disruption of dialogue order"
- Break condition: If order disruption causes the model to learn incorrect patterns or if semantic similarity filtering removes too many useful dispreferred examples

### Mechanism 3
- Claim: RuleAlign's approach to generating rule-based dialogues significantly improves multi-round standardized patient testing performance
- Mechanism: By constructing a rule-based dialogue dataset (UrologyRD) and aligning LLM responses with diagnostic rules, RuleAlign enables more effective information gathering and logical reasoning in multi-turn conversations
- Core assumption: Following diagnostic rules in multi-turn conversations leads to better diagnostic outcomes
- Evidence anchors: [abstract] "RuleAlign significantly improves model performance across multiple metrics including perplexity, ROUGE, and BLEU scores"; [section] "UrologyRD dataset D encompasses both precise inquiry information and complex diagnostic rules"
- Break condition: If the diagnostic rules do not generalize well to real-world scenarios or if the model overfits to the specific rule-based dialogues

## Foundational Learning

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: DPO is used to align LLM responses with diagnostic rules without requiring an explicit reward model
  - Quick check question: What is the main advantage of using DPO over traditional RLHF methods in this context?

- Concept: Semantic similarity filtering
  - Why needed here: Semantic similarity filtering is used to optimize dispreferred completions by selecting those most different from preferred completions
  - Quick check question: How does semantic similarity filtering contribute to the effectiveness of the preference pair optimization strategy?

- Concept: Diagnostic rule adherence
  - Why needed here: Adhering to diagnostic rules is crucial for generating realistic and effective medical dialogues in the UrologyRD dataset
  - Quick check question: Why is it important for the generated dialogues to follow diagnostic rules closely?

## Architecture Onboarding

- Component map: UrologyRD dataset -> GPT-4 turbo API dialogue generation -> SFT model training -> RuleAlign preference pair optimization -> DPO model training -> Evaluation
- Critical path: UrologyRD dataset → GPT-4 turbo API dialogue generation → SFT model training → RuleAlign preference pair optimization → DPO model training → Evaluation
- Design tradeoffs:
  - Rule-based vs. data-driven dialogue generation
  - Number of preference pairs vs. training efficiency
  - Strict rule adherence vs. natural conversation flow
- Failure signatures:
  - Model performance degradation in single-round evaluation
  - Inconsistent diagnostic reasoning in multi-round conversations
  - Overfitting to specific rule-based dialogues
- First 3 experiments:
  1. Evaluate the impact of different numbers of preference pairs on model performance
  2. Compare the effectiveness of semantic similarity filtering vs. order disruption in preference pair optimization
  3. Assess the generalization of RuleAlign to other medical specialties beyond urology

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do RuleAlign's diagnostic performance metrics compare to real-world physician diagnostic accuracy in similar clinical scenarios?
- Basis in paper: [inferred] The paper mentions that RuleAlign still exists a great disparity in delivering final accurate diagnoses and treatment suggestions comparable to those provided by real-world physicians
- Why unresolved: The paper focuses on comparing RuleAlign to baseline LLM approaches rather than direct comparison with human physicians
- What evidence would resolve it: Head-to-head comparison of RuleAlign's diagnostic accuracy against practicing physicians on identical patient cases, measuring metrics like diagnostic accuracy, time to diagnosis, and completeness of information gathering

### Open Question 2
- Question: What is the optimal balance between semantic similarity filtering and order disruption in the preference pair optimization process?
- Basis in paper: [explicit] The paper mentions conducting ablation studies comparing different preference pair optimization strategies including semantic similarity filtration and order disruption
- Why unresolved: The paper shows that combining both strategies works better than individual approaches, but doesn't explore the optimal ratio or weighting between them
- What evidence would resolve it: Systematic experimentation varying the relative weights or thresholds of semantic similarity filtering versus order disruption, measuring impact on final diagnostic performance metrics

### Open Question 3
- Question: How does RuleAlign's performance generalize across different medical specialties beyond urology?
- Basis in paper: [inferred] The paper uses urology as a specific example but suggests the approach could be applied more broadly
- Why unresolved: The current work only evaluates on urology-specific data and rules, without testing generalization to other medical domains
- What evidence would resolve it: Testing RuleAlign on datasets from other medical specialties (cardiology, neurology, etc.) while maintaining the same diagnostic rule alignment approach, comparing performance across specialties

### Open Question 4
- Question: What is the minimum dataset size required for effective diagnostic rule alignment using RuleAlign?
- Basis in paper: [explicit] The paper shows that a quarter of the preference pairs are sufficient to train the SFT model and learn about the preference
- Why unresolved: The paper doesn't explore the lower bounds of dataset size needed for effective alignment or how performance scales with dataset size
- What evidence would resolve it: Experiments systematically reducing dataset size while maintaining other parameters, measuring performance degradation to identify minimum effective dataset size

### Open Question 5
- Question: How does RuleAlign handle cases where diagnostic rules conflict with patient-provided information?
- Basis in paper: [inferred] The paper describes diagnostic rules but doesn't address conflict resolution when patient responses contradict expected patterns
- Why unresolved: The paper doesn't describe mechanisms for handling contradictory information or how the model resolves conflicts between established rules and patient-reported symptoms
- What evidence would resolve it: Analysis of model behavior when presented with intentionally contradictory patient information, measuring how often the model adheres to rules versus patient information and the resulting diagnostic outcomes

## Limitations
- The approach relies heavily on synthetic preference pair generation without human annotation, which may introduce biases
- Effectiveness of semantic similarity filtering and order disruption lacks extensive validation across diverse medical domains beyond urology
- Long-term effectiveness of the alignment in real-world clinical settings remains unproven

## Confidence

- **High Confidence**: The experimental results showing improved performance metrics (perplexity, ROUGE, BLEU) and standardized patient testing outcomes
- **Medium Confidence**: The generalizability of the approach to other medical specialties beyond urology
- **Medium Confidence**: The long-term effectiveness of the alignment in real-world clinical settings

## Next Checks

1. **Cross-specialty Validation**: Apply RuleAlign to medical specialties beyond urology (e.g., cardiology, neurology) to assess generalizability of the preference pair optimization strategy across different diagnostic rule sets.

2. **Human Expert Evaluation**: Conduct blinded evaluations with medical professionals to assess whether the aligned model responses truly reflect clinically sound diagnostic reasoning, not just metric improvements.

3. **Ablation Studies**: Systematically remove semantic similarity filtering and order disruption components to quantify their individual contributions to the overall performance gains, helping isolate which aspects of the preference optimization are most critical.