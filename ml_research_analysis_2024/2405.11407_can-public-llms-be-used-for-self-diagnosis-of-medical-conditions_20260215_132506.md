---
ver: rpa2
title: Can Public LLMs be used for Self-Diagnosis of Medical Conditions ?
arxiv_id: '2405.11407'
source_url: https://arxiv.org/abs/2405.11407
tags:
- medical
- diagnosis
- gemini
- llms
- public
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the potential of public Large Language
  Models (LLMs) like GPT-4.0 and Gemini for self-diagnosis of medical conditions,
  addressing the growing trend of individuals using LLMs instead of search engines
  for medical information. The authors created a prompt-engineered dataset of 10,000
  samples derived from the DDXPlus medical diagnosis dataset, transforming it into
  a format suitable for self-diagnosis tasks.
---

# Can Public LLMs be used for Self-Diagnosis of Medical Conditions ?

## Quick Facts
- arXiv ID: 2405.11407
- Source URL: https://arxiv.org/abs/2405.11407
- Authors: Nikil Sharan Prabahar Balasubramanian; Sagnik Dakshit
- Reference count: 0
- Primary result: GPT-4.0 achieved 63.07% accuracy, Gemini 6.01% on self-diagnosis task

## Executive Summary
This paper investigates the feasibility of using public Large Language Models (LLMs) like GPT-4.0 and Gemini for self-diagnosis of medical conditions. The authors created a prompt-engineered dataset of 10,000 samples from the DDXPlus medical diagnosis dataset, transforming it into a format suitable for self-diagnosis tasks. They compared the performance of GPT-4.0 and Gemini, recording accuracies of 63.07% and 6.01%, respectively. The study demonstrates that Retrieval Augmented Generation (RAG) can significantly improve performance, achieving 98% accuracy with both models. The work highlights the importance of user role in influencing model predictions and discusses challenges such as technical limitations and inconsistent output formatting.

## Method Summary
The authors transformed the DDXPlus medical diagnosis dataset into 10,000 self-diagnosis samples by engineering prompts with user roles (age, sex, symptoms) and system roles (output format restrictions). They processed outputs to normalize disease acronyms and handle inconsistent formatting, particularly from Gemini. The methodology included testing for demographic bias by varying user roles while keeping symptom evidence constant, and implementing RAG by retrieving evidence from a medical knowledge base before generating predictions. Accuracy was calculated by matching predicted conditions to ground truth using string matching.

## Key Results
- GPT-4.0 achieved 63.07% accuracy on self-diagnosis task, while Gemini achieved only 6.01%
- Retrieval Augmented Generation (RAG) improved accuracy to 98% for both models
- User role specification (age and sex) significantly influenced model predictions, demonstrating demographic bias
- Manual intervention was required to process Gemini outputs due to inconsistent formatting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4.0 demonstrates significantly higher accuracy in self-diagnosis than Gemini due to its stronger conversational reasoning and pattern matching on symptom-antecedent-evidence combinations.
- Mechanism: The model leverages its pre-trained knowledge to map user-provided symptom sets and antecedents to potential diseases, benefiting from contextual cues like age and sex in the prompt.
- Core assumption: Symptom sets provided by users are sufficient and correctly interpreted by the model to infer the correct diagnosis.
- Evidence anchors:
  - [abstract] "We compared the performance of GPT-4.0 and Gemini model on the task of self-diagnosis and record accuracies of 63.07% and 6.01% respectively."
  - [section] "Through our experimentation, we also test for diagnosis biases that exist in both the considered LLMs and the importance of user role."
- Break condition: If prompts contain ambiguous or incomplete symptom descriptions, the accuracy drops sharply due to lack of precise evidence matching.

### Mechanism 2
- Claim: User role specification in prompts biases model predictions, demonstrating that demographic context influences the model's reasoning.
- Mechanism: By altering the age and sex in the user role, the model's predicted disease list changes significantly, indicating reliance on demographic stereotypes or correlations in its training data.
- Core assumption: The model's knowledge includes implicit demographic-disease correlations that affect prediction likelihood.
- Evidence anchors:
  - [section] "We observed that the list of possible conditions generated changes significantly and can be illustrated as follows: Gemini User Role: 18-year-old male vs 90-year-old female..."
  - [section] "This change in predictive diagnosis is the sole impact of changing the user role based on census data demonstrating the presence of bias..."
- Break condition: If the model is fine-tuned or prompted to ignore demographic information, the bias effect diminishes or disappears.

### Mechanism 3
- Claim: Retrieval Augmented Generation (RAG) significantly improves diagnostic accuracy by providing domain-specific knowledge at inference time.
- Mechanism: RAG retrieves relevant disease-symptom-antecedent evidence from a structured medical knowledge base before generating predictions, reducing hallucination and improving precision.
- Core assumption: The knowledge base contains accurate and complete mappings between symptoms/antecedents and diseases.
- Evidence anchors:
  - [abstract] "We demonstrate the potential and improvement in performance for the task of self-diagnosis using Retrieval Augmented Generation."
  - [section] "Both our Gemini and GPT-4.0 RAG models achieved 100% accuracy which is a significant improvement."
- Break condition: If the knowledge base is incomplete or noisy, RAG cannot fully mitigate the model's inherent limitations.

## Foundational Learning

- Concept: Prompt engineering
  - Why needed here: The dataset format requires transformation from conversational to self-diagnosis style prompts; proper prompts improve model reliability.
  - Quick check question: Does the prompt include both a user role (age, sex, symptoms) and a system role (output format restrictions)?

- Concept: Data preprocessing and output consistency
  - Why needed here: Different LLMs output disease lists in inconsistent formats; normalization is required for fair comparison.
  - Quick check question: Are all disease acronyms expanded to full names and then mapped back consistently for evaluation?

- Concept: Bias validation through controlled experiments
  - Why needed here: To test if demographic context influences model predictions beyond symptom evidence.
  - Quick check question: When only the user role changes but symptom evidence stays the same, does the prediction list change significantly?

## Architecture Onboarding

- Component map:
  Data processing module -> Prompt engineering module -> LLM inference module -> Output processing module -> Accuracy calculation
- Critical path: Data → Prompt Engineering → LLM API → Output Processing → Accuracy Calculation
- Design tradeoffs:
  - Using free Gemini vs paid GPT-4.0: cost vs accuracy and reliability
  - Including geographic data: increases bias risk vs potential diagnostic relevance
  - Restricting output format: improves automation but may lose nuance
- Failure signatures:
  - Inconsistent formatting in outputs → requires manual intervention
  - Frequent API crashes → loss of throughput and incomplete runs
  - Varying predictions on identical prompts → indicates instability
- First 3 experiments:
  1. Run 100 random prompts through both models and verify output format compliance.
  2. Alter only the user role in 10 prompts and compare prediction variance.
  3. Integrate a simple RAG knowledge base and measure accuracy improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the accuracy of self-diagnosis change when LLMs are fine-tuned on domain-specific clinical data compared to their general pre-trained performance?
- Basis in paper: [inferred] The paper mentions that LLMs have limited accuracy (63.07% for GPT-4.0) and discusses the potential of fine-tuning on healthcare-specific datasets.
- Why unresolved: The paper does not perform fine-tuning experiments; it only evaluates the performance of publicly available, pre-trained LLMs.
- What evidence would resolve it: Conducting experiments where GPT-4.0 and Gemini are fine-tuned on large, diverse, high-quality clinical datasets and comparing their self-diagnosis accuracy to their base models.

### Open Question 2
- Question: What is the minimal number and type of symptoms required for LLMs to accurately identify medical conditions in self-diagnosis tasks?
- Basis in paper: [inferred] The paper highlights the challenge of incomplete symptom data in self-diagnosis and suggests investigating the importance of specific symptoms.
- Why unresolved: The paper does not analyze the impact of varying the number or nature of symptoms on diagnostic accuracy.
- What evidence would resolve it: Systematically varying the completeness and specificity of symptom inputs in the dataset and measuring how accuracy changes with different symptom profiles.

### Open Question 3
- Question: How does the performance of Retrieval Augmented Generation (RAG) models compare to fine-tuned LLMs for self-diagnosis when using the same domain-specific knowledge base?
- Basis in paper: [explicit] The paper demonstrates that RAG significantly improves accuracy (to 98%) but does not compare it to fine-tuned models.
- Why unresolved: The paper only evaluates RAG using pre-trained LLMs and does not benchmark against fine-tuned alternatives.
- What evidence would resolve it: Training fine-tuned LLMs on the same medical knowledge base used for RAG and comparing their self-diagnosis accuracy and consistency to the RAG models.

## Limitations
- Limited API access (50 calls per model per day) prevented comprehensive testing and may have introduced sampling bias
- Dataset filtering excluded samples with more than 5 conditions or 15 symptoms, potentially limiting applicability to complex cases
- Dramatic performance difference between GPT-4.0 (63.07%) and Gemini (6.01%) raises questions about prompt engineering effectiveness

## Confidence
- Experimental methodology is sound but based on single dataset (DDXPlus)
- Medium confidence in core findings due to limited API access and potential sampling bias
- High confidence in RAG effectiveness (98% accuracy) but uncertainty about real-world generalizability

## Next Checks
1. Cross-dataset validation: Test the same methodology on an independent medical diagnosis dataset to verify whether the 98% RAG accuracy generalizes beyond DDXPlus.

2. Bias validation under controlled conditions: Systematically vary user roles (age, sex, geographic data) while keeping symptom evidence constant to quantify how demographic factors influence predictions across different medical conditions.

3. Output consistency stress test: Run 1000+ identical prompts through both models to measure prediction stability and identify conditions where outputs vary significantly, which would indicate reliability issues for clinical use.