---
ver: rpa2
title: Empowering Character-level Text Infilling by Eliminating Sub-Tokens
arxiv_id: '2405.17103'
source_url: https://arxiv.org/abs/2405.17103
tags:
- infilling
- token
- tokens
- code
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of inconsistent token predictions
  in character-level text infilling due to sub-token segmentation. It introduces FIM-SE,
  a method that restructures the infilling task into a line-level format, using special
  tokens to mark incomplete lines and avoid sub-token prediction during inference.
---

# Empowering Character-level Text Infilling by Eliminating Sub-Tokens

## Quick Facts
- arXiv ID: 2405.17103
- Source URL: https://arxiv.org/abs/2405.17103
- Reference count: 14
- This paper introduces FIM-SE, achieving up to 8.8% improvement on random-span and 11.5% on single-line infilling tasks while maintaining code generation quality.

## Executive Summary
This paper tackles the challenge of inconsistent token predictions in character-level text infilling due to sub-token segmentation. It introduces FIM-SE, a method that restructures the infilling task into a line-level format, using special tokens to mark incomplete lines and avoid sub-token prediction during inference. FIM-SE achieves significant performance gains on infilling benchmarks—up to 8.8% improvement on random-span and 11.5% on single-line tasks—while maintaining minimal impact on code generation capabilities.

## Method Summary
FIM-SE transforms character-level infilling into line-level infilling by identifying the last line of the prefix (L-Prefix) and first line of the suffix (F-Suffix). During training, special tokens mark these boundary lines in the prompt structure. The model is trained to predict complete tokens rather than sub-tokens, avoiding the inconsistency issues that arise from sub-token boundary prediction. During inference, generated text is validated to ensure it starts with L-Prefix and ends with F-Suffix, with unsuccessful generations discarded. The method uses PSM mode training with 90% infilling rate and achieves better results with fewer training tokens compared to baseline approaches.

## Key Results
- 8.8% improvement on random-span infilling tasks
- 11.5% improvement on single-line infilling tasks
- Minimal impact on code generation capabilities (MBPP, Humaneval)
- Achieves better results with fewer training tokens (20B vs 30B)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Sub-token prediction in traditional infilling tasks leads to inconsistent model behavior due to token splitting artifacts.
- **Mechanism**: Random character-level splitting can divide a single token into multiple sub-tokens at the boundaries of prefixes, middles, and suffixes. During training, the same prefixes may be paired with different sub-token labels in different samples, causing inconsistent objectives and high perplexity on sub-tokens.
- **Core assumption**: The model's perplexity is significantly affected when it must predict sub-tokens, especially at the beginning of the generated sequence.
- **Evidence anchors**:
  - [abstract] "Traditional methods focused on training models at the token level, leading to sub-optimal performance in character-level infilling tasks during the inference stage."
  - [section] "In Section 1, we construct a simple experiment to illustrate that this inconsistency can significantly affect the model's perplexity."
  - [corpus] Weak. Corpus lacks direct studies on sub-token perplexity in infilling tasks.
- **Break condition**: If the model is trained to always predict complete tokens rather than sub-tokens, the inconsistency issue is eliminated.

### Mechanism 2
- **Claim**: Line-level infilling with explicit boundary markers reduces perplexity and improves generation quality.
- **Mechanism**: By transforming character-level infilling into line-level infilling, FIM-SE uses special tokens to mark the last line of the prefix (L-Prefix) and the first line of the suffix (F-Suffix). This allows the model to predict complete tokens without encountering sub-token boundaries, reducing perplexity.
- **Core assumption**: Complete tokens are easier for the model to predict than sub-tokens, especially in sensitive tasks like code generation.
- **Evidence anchors**:
  - [abstract] "Our method enhances the organizational framework of FIM to concurrently address the two scenarios mentioned earlier."
  - [section] "Our method transforms character-level infilling into line-level infilling. This unification of formats enhances transfer across different levels, significantly augmenting the efficacy of FIM training."
  - [corpus] Weak. Corpus lacks studies specifically on line-level vs character-level infilling performance.
- **Break condition**: If the prompt structure is not properly constructed or if the post-check fails, the method may not work as intended.

### Mechanism 3
- **Claim**: Post-check validation ensures generation consistency by verifying L-Prefix and F-Suffix alignment.
- **Mechanism**: During inference, after generating text, FIM-SE verifies if the output starts with the L-Prefix and ends with the F-Suffix. If it does not, the generation is considered a failure and discarded.
- **Core assumption**: Enforcing boundary constraints improves the reliability of the generated text in matching the intended context.
- **Evidence anchors**:
  - [section] "We verify if it begins with the L-Prefix and ends with the F-Suffix. If the generation does not adhere to these criteria, we classify the infilling endeavor as unsuccessful."
  - [section] "As shown in Figure 4, the PCP Rate increases with length, suggesting that longer L-Prefixes and F-Suffixes provide more guidance for the model's text completion."
  - [corpus] Weak. Corpus lacks studies on post-check mechanisms in text infilling.
- **Break condition**: If the post-check fails too frequently, it may indicate that the model cannot reliably meet the boundary constraints, reducing overall effectiveness.

## Foundational Learning

- **Concept**: Tokenization and sub-token splitting
  - Why needed here: Understanding how sub-tokens arise is critical to grasping why FIM-SE restructures the task.
  - Quick check question: What happens when a word like "delimiter" is split into "deli" and "meter" during tokenization?

- **Concept**: Perplexity and its impact on model predictions
  - Why needed here: High perplexity on sub-tokens leads to inconsistent predictions, which FIM-SE aims to solve.
  - Quick check question: How does inconsistent labeling of sub-tokens affect the model's confidence in predictions?

- **Concept**: Line-level vs character-level processing
  - Why needed here: FIM-SE transforms the task from character-level to line-level to avoid sub-token issues.
  - Quick check question: Why does processing at the line level help avoid sub-token prediction problems?

## Architecture Onboarding

- **Component map**: Character-level splitting -> Line-level splitting with L-Prefix/F-Suffix markers -> Model training with modified prompts -> Inference with post-check validation -> Post-processing

- **Critical path**:
  1. Split document into prefix, middle, suffix at character level
  2. Identify last line of prefix (L-Prefix) and first line of suffix (F-Suffix)
  3. Construct prompt with special tokens: `<PRE> R-Prefix <SUF> R-Suffix <START>L-Prefix <END> F-Suffix <MID> L-Prefix`
  4. Train model with this structure
  5. During inference, generate text and validate with post-check
  6. Remove boundary markers if post-check passes

- **Design tradeoffs**:
  - **Pro**: Eliminates sub-token prediction issues, improves consistency
  - **Con**: Requires additional post-check step, may fail if boundaries are not met
  - **Alternative considered**: Token healing (weaker for complex splits)
  - **Why chosen**: More reliable for sensitive tasks like code generation

- **Failure signatures**:
  - High post-check failure rate (generation does not start/end with correct markers)
  - Low performance on tasks requiring precise sub-token understanding
  - Increased computational overhead due to post-check validation

- **First 3 experiments**:
  1. **Sub-token perplexity test**: Compare model perplexity on sub-tokens vs complete tokens using FIM vs FIM-SE.
  2. **Post-check success rate**: Measure the percentage of generations passing the L-Prefix/F-Suffix validation.
  3. **Performance on sensitive tasks**: Evaluate code generation accuracy with and without FIM-SE to measure impact on task reliability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model handle the case when the generated text neither starts with L-Prefix nor ends with F-Suffix during inference?
- Basis in paper: [explicit] The paper states that the model fails to complete tasks in such cases, and the fail rates for StarCoder-1B and StarCoder-15B are 18.7% and 9.4%, respectively.
- Why unresolved: The paper mentions that future work will focus on improving the post-check pass rate by developing more comprehensive prompts and refining constraint decoding, but does not provide a specific solution or experimental results.
- What evidence would resolve it: Experimental results showing the effectiveness of new prompt designs or constraint decoding methods in reducing the failure rate.

### Open Question 2
- Question: What is the impact of using different tokenization algorithms (e.g., WordPiece, BPE) on the performance of FIM-SE?
- Basis in paper: [inferred] The paper mentions that the SentencePiece algorithm tends to merge the last sub-token with the preceding one, which can affect the performance of token healing. However, it does not explore the impact of other tokenization algorithms on FIM-SE.
- Why unresolved: The paper focuses on the SentencePiece algorithm and does not provide a comparison with other tokenization methods.
- What evidence would resolve it: Experimental results comparing the performance of FIM-SE with different tokenization algorithms on various infilling tasks.

### Open Question 3
- Question: How does the performance of FIM-SE scale with larger models and more diverse datasets?
- Basis in paper: [explicit] The paper mentions that the consistent training approach of FIM-SE is particularly beneficial for smaller models in achieving better fit, and that the model with a similar size using fewer tokens achieves better results. However, it does not explore the performance on larger models or more diverse datasets.
- Why unresolved: The paper focuses on a limited set of models and datasets, and does not provide a comprehensive analysis of the scalability of FIM-SE.
- What evidence would resolve it: Experimental results showing the performance of FIM-SE on larger models and more diverse datasets, and a comparison with other infilling methods.

## Limitations
- The post-check validation mechanism introduces a failure point where up to 13% of generations may be discarded.
- Method's effectiveness depends heavily on quality of L-Prefix and F-Suffix boundary detection, which could be challenging in certain document structures or languages.
- Trade-off between improved infilling accuracy and potential code generation degradation needs careful consideration for production deployment.

## Confidence
- High confidence: Mechanism of sub-token inconsistency causing perplexity issues
- Medium confidence: Overall performance improvements
- Medium confidence: Post-check mechanism's reliability

## Next Checks
1. **Sub-token perplexity analysis**: Conduct controlled experiments comparing perplexity scores on sub-tokens versus complete tokens between FIM and FIM-SE to quantify the exact impact of sub-token elimination on model confidence.

2. **Post-check failure rate profiling**: Analyze the distribution of post-check failures across different document types, line lengths, and languages to identify patterns and potential mitigation strategies for high-failure scenarios.

3. **Cross-task generalization study**: Evaluate FIM-SE performance on non-code text infilling tasks (e.g., natural language, structured data) to assess the method's generalizability beyond the code domain where it was developed and tested.