---
ver: rpa2
title: Monotone Individual Fairness
arxiv_id: '2403.06812'
source_url: https://arxiv.org/abs/2403.06812
tags:
- fairness
- learning
- will
- unfair
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses online learning with individual fairness, where
  an algorithm aims to maximize predictive accuracy while ensuring similar individuals
  are treated similarly. It extends previous work by considering auditing schemes
  that can aggregate feedback from multiple auditors using monotone aggregation functions.
---

# Monotone Individual Fairness

## Quick Facts
- arXiv ID: 2403.06812
- Source URL: https://arxiv.org/abs/2403.06812
- Authors: Yahav Bechavod
- Reference count: 19
- Key outcome: Oracle-efficient algorithms achieving O(sqrt(T)) regret and O(T^(3/4)) fairness violations in full information setting

## Executive Summary
This paper addresses online learning with individual fairness, where an algorithm must maximize predictive accuracy while ensuring similar individuals are treated similarly. The work extends previous research by considering auditing schemes that aggregate feedback from multiple auditors using monotone aggregation functions. The authors provide a characterization of such auditing schemes and demonstrate that analyzing auditing by multiple auditors reduces to analyzing auditing by single auditors. They present oracle-efficient algorithms that significantly improve upon the best known results for regret and fairness violations in both full and partial information settings.

## Method Summary
The paper presents oracle-efficient algorithms based on Lagrangian formulations that dynamically combine accuracy and fairness objectives. The approach uses Context-FTPL to implicitly maintain the policy π while employing Online Gradient Descent to update a trade-off parameter λ that controls the balance between accuracy and fairness. In the full information setting, the algorithm achieves O(sqrt(T)) regret and O(T^(3/4)) fairness violations. In the partial information setting with one-sided label feedback, it achieves O(T^(2/3)) regret and O(T^(5/6)) fairness violations. The algorithms require only O(α^(-2)) calls to an optimization oracle per round in the full information setting and O(α^(-2) + k^2*T^(1/3)) in the partial information setting.

## Key Results
- Oracle-efficient algorithm achieving O(sqrt(T)) regret and O(T^(3/4)) fairness violations in full information setting
- Algorithm for partial information setting achieving O(T^(2/3)) regret and O(T^(5/6)) fairness violations
- Significant reduction in computational complexity to O(α^(-2)) optimization oracle calls per round in full information setting
- Characterization of monotone auditing schemes that reduces multi-auditor analysis to single-auditor analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The algorithm achieves simultaneous no-regret guarantees by dynamically combining error and unfairness losses using a Lagrangian formulation.
- Mechanism: The learner maintains a trade-off parameter λ that is updated using Online Gradient Descent. This λ controls the balance between accuracy and fairness objectives, allowing the algorithm to adapt to the revealed stringency of fairness constraints.
- Core assumption: The auditing schemes are monotone, meaning that adding more objections from auditors cannot change the aggregate decision from reporting a violation to not reporting one.
- Evidence anchors:
  - [abstract]: "We then prove a characterization for such auditing schemes, practically reducing the analysis of auditing for individual fairness by multiple auditors to that of auditing by (instance-specific) single auditors."
  - [section]: "Inspired the literature on learning with long-term constraints (primarily Mahdavi et al. (2012); Sun et al. (2017)), and more generally, Agarwal et al. (2018); Freund and Schapire (1997), we take the perspective of a saddle-point problem for our learning objective — where the primal player (who sets π) attempts to minimize the Lagrangian loss, while the dual player (who sets λ) attempts to maximize it."
  - [corpus]: Weak. The corpus papers focus on fairness auditing and bias quantification but do not discuss Lagrangian formulations or dynamic trade-off parameters.
- Break condition: If the auditing schemes are not monotone, the characterization lemma fails and the algorithm cannot guarantee the bound on fairness violations.

### Mechanism 2
- Claim: The algorithm reduces computational complexity by distinguishing between fairness constraint elicitation and accuracy-fairness objective minimization.
- Mechanism: The algorithm uses Context-FTPL to implicitly maintain the policy π, while using an approximate policy π̃ to query auditors for fairness violations. This allows the algorithm to make only O(α^(-2)) calls to an optimization oracle per round in the full information setting.
- Core assumption: The approximate policy π̃ is accurate enough on the individuals in the current round to correctly elicit fairness violations.
- Evidence anchors:
  - [abstract]: "Furthermore, our algorithms offer significant improvements in computational efficiency, greatly reducing the number of required calls to an (offline) optimization oracle per round, to O(α^(-2)) in the full information setting."
  - [section]: "To circumvent this, our approach will be to distinguish between two tasks: eliciting the fairness constraints, and evaluating the error and unfairness losses. Ideally, one would like to perform both tasks using the same policy — the deployed policy πt. Since, however, in our algorithm the learner will only have access to classifiers sampled from πt, we will perform each task using a different policy."
  - [corpus]: Weak. The corpus papers focus on fairness auditing and bias quantification but do not discuss oracle-efficient algorithms or the use of approximate policies.
- Break condition: If the approximate policy π̃ is not accurate enough on the current individuals, the algorithm may miss fairness violations or incorrectly report them, leading to suboptimal performance.

### Mechanism 3
- Claim: The algorithm achieves faster convergence rates by using a non-regularized Lagrangian loss for the primal player and a regularized Lagrangian loss for the dual player.
- Mechanism: The primal player uses the non-regularized Lagrangian loss to minimize regret, while the dual player uses the regularized Lagrangian loss to update the trade-off parameter λ. This allows the algorithm to achieve a bound of O(sqrt(T)) for regret and O(T^(3/4)) for the number of fairness violations.
- Core assumption: The Lagrangian loss is linear in the policy π, which allows the algorithm to compete against the best fair policy in the policy class rather than against the much weaker class of constant predictors.
- Evidence anchors:
  - [abstract]: "Using our generalized framework, we present an oracle-efficient algorithm achieving an upper bound frontier of (O(T^(1/2+2b)),O(T^(3/4-b))) respectively for regret, number of fairness violations, for 0 ≤ b ≤ 1/4."
  - [section]: "We will feed Lt to Context-FTPL, and prove accuracy, fairness guarantees for the true (implicit) policy πt deployed by it. Finally, Lt will be used by Online Gradient Descent to update λt+1 for the next round."
  - [corpus]: Weak. The corpus papers focus on fairness auditing and bias quantification but do not discuss convergence rates or the use of Lagrangian losses.
- Break condition: If the Lagrangian loss is not linear in the policy π, the algorithm may not be able to compete against the best fair policy in the policy class, leading to suboptimal performance.

## Foundational Learning

- Concept: Monotone aggregation functions
  - Why needed here: The algorithm relies on monotone aggregation functions to characterize auditing schemes and reduce the analysis of auditing by multiple auditors to that of auditing by single auditors.
  - Quick check question: Can you provide an example of a monotone aggregation function that is not a majority-based scheme?

- Concept: Lagrangian formulation
  - Why needed here: The algorithm uses a Lagrangian formulation to dynamically combine the objectives of accuracy and fairness, allowing it to achieve faster convergence rates.
  - Quick check question: How does the Lagrangian formulation allow the algorithm to compete against the best fair policy in the policy class rather than against the much weaker class of constant predictors?

- Concept: Oracle-efficient algorithms
  - Why needed here: The algorithm is designed to be oracle-efficient, meaning it can run in polynomial time given access to an optimization oracle, which is critical for practical applications.
  - Quick check question: What is the advantage of using an oracle-efficient algorithm over an algorithm that explicitly maintains and updates the weights on the hypothesis class?

## Architecture Onboarding

- Component map: Context-FTPL -> Online Gradient Descent -> Auditing scheme -> Optimization oracle
- Critical path:
  1. Context-FTPL maintains the policy π
  2. Online Gradient Descent updates the trade-off parameter λ
  3. Auditing scheme aggregates feedback from multiple auditors
  4. Optimization oracle is used to sample classifiers from the policy π

- Design tradeoffs:
  - Using an approximate policy π̃ to query auditors for fairness violations reduces computational complexity but may lead to suboptimal performance if π̃ is not accurate enough.
  - Using a non-regularized Lagrangian loss for the primal player and a regularized Lagrangian loss for the dual player allows the algorithm to achieve faster convergence rates but may lead to suboptimal performance if the Lagrangian loss is not linear in the policy π.

- Failure signatures:
  - If the auditing schemes are not monotone, the characterization lemma fails and the algorithm cannot guarantee the bound on fairness violations.
  - If the approximate policy π̃ is not accurate enough on the current individuals, the algorithm may miss fairness violations or incorrectly report them, leading to suboptimal performance.
  - If the Lagrangian loss is not linear in the policy π, the algorithm may not be able to compete against the best fair policy in the policy class, leading to suboptimal performance.

- First 3 experiments:
  1. Verify that the algorithm achieves the claimed bounds on regret and fairness violations on a synthetic dataset with known fairness constraints.
  2. Test the algorithm's performance on a real-world dataset with fairness constraints, such as the COMPAS dataset.
  3. Evaluate the algorithm's computational efficiency by measuring the number of calls to the optimization oracle per round on large datasets.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we design algorithms that compete with the class of fair policies where no relaxation in terms of violation size is required (i.e., α = 0)?
- Basis in paper: Explicit. The authors state that proving non-trivial lower bounds and designing algorithms to compete with the class of fair policies without any relaxation in violation size are interesting open problems.
- Why unresolved: The current algorithms provide guarantees by competing with a slightly relaxed baseline of fair policies where the violation size is α - ε. Removing this relaxation poses a challenge as it requires ensuring no violations occur, which is difficult given the unknown and dynamic nature of the fairness constraints.
- What evidence would resolve it: Designing and proving the correctness of an algorithm that achieves low regret and fairness violations while competing directly with the class of fair policies where no relaxation in violation size is allowed. This would involve demonstrating that the algorithm can dynamically adjust to the fairness constraints without any prior knowledge or relaxation.

### Open Question 2
- Question: What level of fairness constraint violation is unavoidable for algorithms that obtain a non-trivial regret bound O(T^a) for a < 1?
- Basis in paper: Explicit. The authors mention that proving non-trivial lower bounds in their setting is an interesting problem and pose a question about the level of fairness constraint violation that is unavoidable for algorithms with non-trivial regret bounds.
- Why unresolved: The current analysis provides upper bounds on the number of fairness violations, but it does not establish lower bounds. Determining the unavoidable level of fairness constraint violation for algorithms with non-trivial regret bounds requires a deeper understanding of the trade-off between accuracy and fairness in online learning settings.
- What evidence would resolve it: Establishing lower bounds on the number of fairness violations that any algorithm achieving a non-trivial regret bound O(T^a) for a < 1 must incur. This would involve proving that there exist adversarial sequences of individuals, labels, auditors, and aggregation functions for which any algorithm with a non-trivial regret bound must violate fairness constraints at least a certain number of times.

### Open Question 3
- Question: How can we extend the approach to handle more general feedback structures beyond the one-sided label feedback setting?
- Basis in paper: Inferred. The authors focus on the one-sided label feedback setting where the learner only observes labels for positively predicted individuals. Extending the approach to handle other feedback structures, such as full label feedback or bandit feedback, is a natural direction for future research.
- Why unresolved: The current algorithms are specifically designed to handle the one-sided label feedback setting. Adapting the approach to other feedback structures requires addressing the challenges posed by different information availability and feedback mechanisms.
- What evidence would resolve it: Designing and analyzing algorithms that can handle various feedback structures, such as full label feedback or bandit feedback, while still providing strong guarantees on regret and fairness violations. This would involve developing new techniques to handle the specific challenges posed by each feedback structure and proving the correctness and efficiency of the algorithms.

## Limitations

- The computational efficiency claims depend on unspecified implementation details of the underlying FTPL algorithms, making practical evaluation difficult
- The algorithm's performance depends critically on the accuracy of the approximate policy π̃ used to query auditors for fairness violations
- The theoretical guarantees assume monotone auditing schemes, which may not hold in practice with conflicting auditor feedback

## Confidence

- Theoretical characterization framework: High
- Computational complexity claims: Low
- Convergence rate bounds: Medium
- Practical applicability: Low

## Next Checks

1. Implement and test the monotone aggregation function characterization on synthetic auditor feedback to verify the reduction from multi-auditor to single-auditor analysis works as claimed.

2. Benchmark the oracle-efficient algorithm against a naive implementation that explicitly maintains and updates hypothesis class weights to measure actual computational savings.

3. Evaluate the algorithm's performance on a real-world dataset with multiple auditors providing conflicting feedback to assess robustness and practical feasibility.