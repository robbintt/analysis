---
ver: rpa2
title: Script-Agnostic Language Identification
arxiv_id: '2406.17901'
source_url: https://arxiv.org/abs/2406.17901
tags: []
core_contribution: 'This paper addresses the problem of script-agnostic language identification
  (langID) for languages written in multiple scripts, focusing on four major Dravidian
  languages: Tamil, Telugu, Kannada, and Malayalam. The authors propose learning script-agnostic
  representations using three experimental strategies: upsampling (translating each
  sentence into all four scripts), flattening (projecting all languages into one script),
  and noisy multi-script mixing (introducing script variations within sentences).'
---

# Script-Agnostic Language Identification

## Quick Facts
- arXiv ID: 2406.17901
- Source URL: https://arxiv.org/abs/2406.17901
- Reference count: 40
- One-line primary result: Upscaling and noise models achieve >99% accuracy on transliterated test sets while maintaining competitive performance on native scripts

## Executive Summary
This paper addresses script-agnostic language identification for four major Dravidian languages (Tamil, Telugu, Kannada, Malayalam) by learning representations that are invariant to script variations. The authors propose three experimental strategies: upsampling (translating each sentence into all four scripts), flattening (projecting all languages into one script), and noisy multi-script mixing (introducing script variations within sentences). Their results show that word-level script randomization and exposure to languages written in multiple scripts significantly improve downstream script-agnostic language identification performance, with the upscaled and noise models achieving over 99% accuracy on transliterated test sets while maintaining competitive performance on naturally occurring text.

## Method Summary
The authors use fastText skipgram models trained on three experimental setups: (1) Upscaling - transliterating each sentence into all four scripts to expose the model to every sentence in every script, (2) Flattening - projecting all languages into a single script to remove script identity as a confounding factor, and (3) Noise - introducing word-level script variations within sentences using Algorithm 1 to improve robustness. The models are trained on the FLORES200 dataset and evaluated on both original and transliterated test sets, with additional testing on out-of-domain datasets including GlotStoryBooks, UDHR, MCS-350, and IndicCorp.

## Key Results
- Upscaling and noise models achieve over 99% accuracy on transliterated test sets
- All experimental strategies maintain competitive performance on naturally occurring text in native scripts
- Word-level script randomization proves extremely valuable for downstream script-agnostic language identification
- Upscaling with 100% transliteration significantly outperforms the 25% baseline, achieving comparable results to flattening

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Upscaling improves script-agnosticism by exposing the model to every sentence in all four scripts.
- **Mechanism:** The model learns that a sentence in a given language can appear in any script, forcing it to rely on intrinsic linguistic features rather than script-specific cues.
- **Core assumption:** All four scripts have sufficient lexical and phonological overlap to allow consistent transliteration without major loss of meaning.
- **Evidence anchors:** [abstract] "word-level script randomization and exposure to a language written in multiple scripts is extremely valuable for downstream script-agnostic language identification"; [section 2] "seeing every example in each script will prevent a model from giving weight to any one writing system in its decision-making"
- **Break condition:** If transliteration introduces significant semantic or phonetic loss, the model may overfit to the transliteration artifacts rather than the underlying language features.

### Mechanism 2
- **Claim:** Flattening reduces vocabulary overload by projecting all languages into one script, allowing the model to focus on distinguishing linguistic features.
- **Mechanism:** Training on a single script removes the confounding effect of script identity, enabling the classifier to learn purely language-dependent patterns.
- **Core assumption:** One chosen script can represent all target languages without introducing bias or information loss.
- **Evidence anchors:** [abstract] "training word representations in a single script may perform poorly in real-world settings"; [section 2] "with only one script, the embedding space (and consequently the classification system) can focus on finding discriminative features between the languages"
- **Break condition:** If the chosen script is systematically better at encoding certain language families, the model may still exhibit residual script bias.

### Mechanism 3
- **Claim:** Noisy multi-script mixing improves robustness by training on sentences containing multiple scripts within the same utterance.
- **Mechanism:** The model learns to ignore intra-sentence script variations and focus on language-specific cues distributed across words.
- **Core assumption:** Random word-level script mixing is representative of real-world script-agnostic usage.
- **Evidence anchors:** [abstract] "word-level script randomization... is extremely valuable for downstream script-agnostic language identification"; [section 2] "for each noise level n, language lang, and sentence sent, we choose a base script and then randomly pick n% words to transform to new non-base scripts"
- **Break condition:** If noise level is too high, the model may treat mixed-script sentences as a new class rather than learning to ignore script differences.

## Foundational Learning

- **Concept:** Transliteration and script mapping
  - Why needed here: All experiments rely on accurate one-to-one script conversion between Dravidian languages.
  - Quick check question: Does Aksharamukha preserve semantic meaning when converting between Tamil, Telugu, Kannada, and Malayalam scripts?

- **Concept:** Script-agnostic embeddings
  - Why needed here: The goal is to train representations that do not encode script identity, enabling robust language identification across scripts.
  - Quick check question: How do we verify that the embedding space is truly script-agnostic and not just memorizing script-language pairs?

- **Concept:** Data augmentation through script manipulation
  - Why needed here: Upscaling and noise experiments both use synthetic data to expose the model to diverse script configurations.
  - Quick check question: At what point does adding more script variations start to degrade rather than improve model performance?

## Architecture Onboarding

- **Component map:** Data preprocessing pipeline (transliteration) -> FastText training module (skipgram, custom corpora) -> Evaluation harness (accuracy, F1, cross-domain tests) -> Interpretability toolkit (SHAP for feature importance)

- **Critical path:** Load and preprocess FLORES200 and out-of-domain datasets → Transliterate data per experimental setup (flattening, upscaling, noise) → Train fastText models on each variant → Evaluate on both original and transliterated test sets → Perform interpretability analysis on misclassifications

- **Design tradeoffs:** Upscaling uses 4x data but better handles script variability; Flattening is computationally cheaper but loses real-world applicability; Noise experiments increase robustness but require careful noise-level tuning

- **Failure signatures:** High accuracy on transliterated data but poor performance on native scripts → model overfits to augmented data; Systematic misclassification of one language as another → potential lexical/phonetic overlap issues; SHAP analysis reveals script-based feature importance → model is not truly script-agnostic

- **First 3 experiments:** Baseline fastText on FLORES200 (no augmentation); Flattening experiment: project all data to Telugu script and retrain; Upscaling experiment: transliterate each sentence to all 4 scripts and retrain

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the minimum amount of training data required to achieve high accuracy on transliterated text using the upscale method?
- **Basis in paper:** [explicit] The paper shows that upscale models trained with 25% of the original examples transliterated perform much worse than those trained with 100%, and that comparable performance to flattening is only achieved with at least 75% transliteration.
- **Why unresolved:** The paper does not test all possible percentages between 25% and 75%, leaving the exact minimum threshold uncertain.
- **What evidence would resolve it:** Systematic testing of upscale models trained with 30%, 40%, 50%, and 60% transliteration rates, measuring accuracy on transliterated test sets.

### Open Question 2
- **Question:** How does the performance of script-agnostic models degrade when exposed to completely unseen scripts?
- **Basis in paper:** [inferred] The paper notes that models are only trained on four specific scripts and will struggle with unknown writing systems, but does not test this scenario.
- **Why unresolved:** No experiments were conducted with languages in scripts not seen during training, leaving the generalization limits unknown.
- **What evidence would resolve it:** Evaluating the models on test sets where languages appear in scripts absent from training, such as testing Tamil in Arabic script when trained only on Latin, Tamil, Telugu, Kannada, and Malayalam scripts.

### Open Question 3
- **Question:** Do script-agnostic embeddings learned from these methods improve performance on downstream NLP tasks beyond language identification?
- **Basis in paper:** [explicit] The authors mention that future work should evaluate the quality of learned representations on other downstream tasks, but do not conduct such evaluations.
- **Why unresolved:** The paper focuses solely on language identification accuracy and does not test the embeddings on tasks like machine translation, text classification, or named entity recognition.
- **What evidence would resolve it:** Training and evaluating the script-agnostic embeddings on downstream tasks such as neural machine translation or cross-lingual transfer learning benchmarks.

## Limitations

- Heavy reliance on Aksharamukha transliteration tool without quantitative validation of transliteration quality or semantic preservation
- Performance gap between transliterated and native script data not thoroughly analyzed
- Out-of-domain tests use datasets that may not adequately represent real-world script-agnostic usage patterns

## Confidence

- Mechanism 1 (upscaling): Medium confidence - theoretically sound but lacks quantitative validation of transliteration quality
- Mechanism 2 (flattening): Low confidence - paper acknowledges poor real-world performance but no systematic analysis of why
- Mechanism 3 (noise): Medium confidence - promising approach but noise level tuning and optimal configurations not thoroughly explored

## Next Checks

1. **Transliteration quality validation**: Implement parallel semantic similarity testing between original and transliterated sentences across all script pairs to quantify information loss and identify systematic transliteration biases.

2. **Cross-dataset generalization**: Evaluate model performance on a more diverse set of real-world script-agnostic datasets (social media, code-switched text, informal romanization) to assess practical applicability beyond controlled experiments.

3. **Noise level optimization**: Conduct systematic ablation studies across different noise levels and mixing strategies to determine optimal configuration for robust script-agnostic learning without overfitting to synthetic noise patterns.