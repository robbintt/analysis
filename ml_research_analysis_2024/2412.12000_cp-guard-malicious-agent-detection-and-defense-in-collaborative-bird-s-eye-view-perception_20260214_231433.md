---
ver: rpa2
title: 'CP-Guard: Malicious Agent Detection and Defense in Collaborative Bird''s Eye
  View Perception'
arxiv_id: '2412.12000'
source_url: https://arxiv.org/abs/2412.12000
tags:
- collaborative
- perception
- malicious
- agents
- chen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of detecting and defending against
  malicious agents in collaborative perception for autonomous driving. The proposed
  method, CP-Guard, uses a probability-agnostic sample consensus (PASAC) approach
  to sample collaborators without requiring prior probabilities of malicious agents,
  combined with a collaborative consistency loss (CCLoss) to verify consensus between
  the ego vehicle and collaborators.
---

# CP-Guard: Malicious Agent Detection and Defense in Collaborative Bird's Eye View Perception

## Quick Facts
- arXiv ID: 2412.12000
- Source URL: https://arxiv.org/abs/2412.12000
- Reference count: 19
- Primary result: CP-Guard achieves mIoU of 39.30-39.34 against adversarial attacks, close to upper bound of 40.45

## Executive Summary
CP-Guard addresses the critical challenge of detecting and defending against malicious agents in collaborative perception for autonomous driving. The proposed framework uses a probability-agnostic sample consensus (PASAC) approach combined with collaborative consistency loss (CCLoss) to identify and eliminate malicious collaborators without requiring prior knowledge of their probabilities. Evaluated on BEV segmentation tasks using the V2X-Sim dataset, CP-Guard demonstrates robust defense against multiple attack types (FGSM, C&W, PGD) while maintaining high performance close to the theoretical upper bound.

## Method Summary
CP-Guard introduces a probability-agnostic sample consensus (PASAC) method that recursively splits collaborators into groups and verifies consensus using collaborative consistency loss (CCLoss) without requiring prior probabilities of malicious agents. The framework compares the ego vehicle's BEV segmentation to fused results from collaborator groups, using CCLoss as a similarity metric with a threshold ε to determine consensus. Malicious agents are detected when their outputs deviate significantly from the ego vehicle's perception, and they are excluded from the final fusion process. The method is evaluated on collaborative BEV segmentation tasks using the V2X-Sim dataset, demonstrating robust defense against adversarial attacks while achieving mIoU scores close to the upper bound.

## Key Results
- CP-Guard achieves mIoU of 39.30-39.34 against adversarial attacks, close to upper bound of 40.45
- Outperforms state-of-the-art ROBOSAC in verification count while requiring no prior knowledge of malicious agent probabilities
- Demonstrates robust defense against FGSM, C&W, and PGD attacks with optimal CCLoss threshold ε = 0.08

## Why This Works (Mechanism)

### Mechanism 1
PASAC can detect malicious agents without prior knowledge by recursively splitting collaborators and comparing BEV segmentation maps using CCLoss. Groups with CCLoss below threshold ε are marked potentially malicious and further split. Core assumption: malicious agents generate consistent corrupted outputs that deviate from ego vehicle's perception. Evidence: abstract states PASAC verifies consensus without prior probabilities. Break condition: if malicious agents coordinate to match ego vehicle's segmentation.

### Mechanism 2
CCLoss quantifies similarity between ego vehicle's BEV segmentation and fused results from collaborator groups. Higher values indicate greater similarity, with threshold ε determining consensus existence. Core assumption: benign collaborators maintain statistical similarity with ego vehicle's perception. Evidence: CCLoss formula shows values close to 1 for similar distributions, close to 0 for different distributions. Break condition: if benign collaborators have different viewpoints or ego perception is compromised.

### Mechanism 3
CP-Guard achieves comparable mIoU to upper bound by eliminating malicious agents from fusion process. Only benign collaborators contribute to final perception, maintaining near-optimal performance. Core assumption: malicious agents significantly degrade mIoU, and their removal restores performance. Evidence: achieves mIoU 39.30-39.34 close to upper bound 40.45. Break condition: if threshold selection is poor or malicious agents constitute majority.

## Foundational Learning

- Concept: Collaborative perception in autonomous driving
  - Why needed: Understanding how vehicles share intermediate features to improve perception is fundamental to grasping why CP-Guard is necessary
  - Quick check: What are the three main data fusion strategies in collaborative perception and what are their tradeoffs?

- Concept: Adversarial attacks on neural networks
  - Why needed: CP-Guard defends against attacks that manipulate intermediate feature maps, so understanding attack mechanisms is crucial
  - Quick check: What distinguishes white-box attacks from black-box attacks in collaborative perception?

- Concept: Consensus algorithms and distributed detection
  - Why needed: PASAC is a consensus detection algorithm that recursively partitions and verifies subsets of agents
  - Quick check: How does recursive binary splitting in PASAC differ from random sampling approaches like ROBOSAC?

## Architecture Onboarding

- Component map: LiDAR point cloud → voxelization → feature encoding → BEV segmentation → feature sharing → PASAC sampling → CCLoss verification → agent classification → fusion filtering → final BEV segmentation

- Critical path: LiDAR input → feature encoding → feature sharing → PASAC sampling → CCLoss verification → agent classification → fusion filtering → final BEV segmentation

- Design tradeoffs: CCLoss threshold ε balances false positives/negatives; sampling depth affects detection accuracy vs computational overhead; communication overhead increases with intermediate feature sharing

- Failure signatures: High false positive rate (threshold ε too low); high false negative rate (threshold ε too high or majority malicious); performance degradation (too many agents eliminated or CCLoss fails); communication failures (incomplete feature map reception)

- First 3 experiments: 1) Baseline comparison against FGSM, C&W, PGD attacks measuring mIoU vs upper/lower bounds; 2) Threshold sensitivity analysis varying ε from 0.02 to 0.15 for PGD attacks; 3) Attack ratio analysis varying malicious agent percentage from 0% to 100%

## Open Questions the Paper Calls Out

### Open Question 1
How does CP-Guard performance vary with different numbers of collaborative agents (N) and attack ratios? While the paper shows verification count relationship with agent numbers, it doesn't explicitly analyze how overall CP-Guard performance (mIoU) scales with different N and attack ratios. Evidence would require comprehensive study varying N from 5 to 100 agents and attack ratios from 0.2 to 0.8.

### Open Question 2
What is the optimal CCLoss threshold (ε) for different attack types beyond PGD? The paper provides detailed threshold analysis only for PGD attacks, mentioning ε = 0.08 as optimal for FGSM and PGD but without substantiation. Evidence would require systematic ablation studies varying ε from 0.02 to 0.15 for each attack type.

### Open Question 3
How does CP-Guard perform in real-world scenarios with non-ideal conditions such as communication delays, packet loss, and sensor noise? All experiments are conducted in controlled synthetic environments without modeling real-world imperfections in vehicle-to-vehicle communication or sensor measurements. Evidence would require testing in simulation environments with realistic communication latency and packet loss, or validation on real-world autonomous driving datasets.

## Limitations

- Threshold sensitivity: CCLoss threshold ε = 0.08 lacks systematic sensitivity analysis or theoretical justification across different attack types
- Dataset dependency: Evaluation solely on V2X-Sim synthetic dataset may not capture real-world complexity and variability
- Attack representation: Only evaluates against three attack types without exploring adaptive attacks targeting CP-Guard mechanism itself

## Confidence

- **High Confidence**: Core mechanism of consensus-based detection is well-established and experimental results showing improved mIoU over undefended systems are reproducible
- **Medium Confidence**: Comparative advantage over ROBOSAC in verification count is demonstrated but experimental setup doesn't isolate this advantage from confounding factors
- **Low Confidence**: Claim of achieving "mIoU close to upper bound" requires further validation due to lack of statistical significance testing and varying gaps across attack types

## Next Checks

1. **Threshold Robustness Analysis**: Conduct systematic sensitivity analysis varying ε from 0.02 to 0.15 across all attack types and environmental conditions, measuring detection accuracy and false positive/negative rates to establish confidence intervals

2. **Real-World Validation**: Implement CP-Guard on real-world autonomous driving dataset (nuScenes or Argoverse) with diverse environmental conditions to assess performance degradation compared to synthetic evaluation

3. **Adaptive Attack Testing**: Design and implement attacks specifically targeting CP-Guard's consensus verification mechanism, including attacks that manipulate CCLoss values or exploit recursive splitting strategy to bypass detection