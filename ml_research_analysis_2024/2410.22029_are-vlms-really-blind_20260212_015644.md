---
ver: rpa2
title: Are VLMs Really Blind
arxiv_id: '2410.22029'
source_url: https://arxiv.org/abs/2410.22029
tags:
- tasks
- task
- question
- image
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work investigates whether Vision Language Models (VLMs) struggle
  with basic geometric tasks due to inherent limitations or improvable methodologies.
  The authors propose a novel two-step pipeline: extracting task-relevant keywords
  from questions, using them to guide image captioning by a VLM, then applying an
  LLM to generate answers from the captions.'
---

# Are VLMs Really Blind

## Quick Facts
- arXiv ID: 2410.22029
- Source URL: https://arxiv.org/abs/2410.22029
- Reference count: 2
- Primary result: Keyword-guided captioning improves VLM accuracy on geometric tasks from 43.88% to 58.00%

## Executive Summary
This work investigates whether Vision Language Models (VLMs) struggle with basic geometric tasks due to inherent limitations or improvable methodologies. The authors propose a novel two-step pipeline: extracting task-relevant keywords from questions, using them to guide image captioning by a VLM, then applying an LLM to generate answers from the captions. This approach is evaluated across eight geometric tasks using three VLMs: GPT4mini, PaliGemmaBase, and GeminiFlash. Results show that the keyword-guided captioning pipeline consistently outperforms direct VQA, improving overall accuracy from 43.88% to 58.00% on average.

## Method Summary
The authors developed a two-step pipeline to address VLM performance on geometric reasoning tasks. First, they extract 1-2 word summaries from questions using Llama 3.1-8B-Instruct. These keywords then guide a VLM (GPT4mini, PaliGemmaBase, or GeminiFlash) to generate captions for the image. Finally, GeminiFlash uses these captions to answer the original question. The approach is tested on eight geometric tasks including counting intersections, identifying circle overlaps, and counting shapes. Performance is compared against direct VQA on the same tasks and VLMs.

## Key Results
- Overall accuracy improved from 43.88% to 58.00% using keyword-guided captioning versus direct VQA
- Non-counting geometry tasks showed significant and consistent improvements
- Counting tasks remained challenging with inconsistent improvements
- The pipeline outperformed direct VQA across all three tested VLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Keyword-guided captioning improves VLM performance by reducing the complexity of the multimodal reasoning task.
- Mechanism: Instead of directly answering visual questions, the VLM first generates a caption guided by question-derived keywords. This pre-processing step decomposes the complex VQA task into simpler subtasks: keyword extraction, image captioning, and text-based question answering. By narrowing the scope of the caption to task-relevant details, the model can focus its attention on pertinent visual features.
- Core assumption: VLMs are better at generating descriptive text than directly answering questions because they are pre-trained primarily on captioning and text generation tasks.
- Evidence anchors:
  - [abstract] "Our work presents a novel automatic pipeline designed to extract key information from images in response to specific questions. Instead of just relying on direct VQA, we use question-derived keywords to create a caption that highlights important details in the image related to the question."
  - [section] "Our findings demonstrate that there is an overall improvement in performance when using this approach compared to simple VQA."
  - [corpus] Weak evidence - neighboring papers discuss task-specific prompting but don't directly validate this decomposition mechanism.
- Break condition: If the keyword extraction step fails to capture essential visual information, the caption will lack necessary details for accurate answering.

### Mechanism 2
- Claim: The improvement is particularly pronounced for non-counting geometry tasks because these tasks require understanding spatial relationships rather than exact enumeration.
- Mechanism: Non-counting geometry tasks (e.g., determining if two lines intersect, whether circles overlap) depend on spatial reasoning and qualitative relationships. The keyword-guided captioning can effectively highlight these relationships in text form, making them easier for the LLM to process than raw visual input.
- Core assumption: VLMs struggle with counting tasks because captions rarely specify exact quantities, especially for larger numbers, as noted in prior work.
- Evidence anchors:
  - [section] "Our second hypothesis is substantiated by the data: the improvement in accuracy is particularly significant and uniform for non-counting geometry-related tasks, whereas the average increase in accuracy for counting-related tasks is less visible and somewhat random and not consistent."
  - [section] "This struggle arises largely due to the fact that during training captions that accurately specify the number of objects become extremely rare in the data as the number of objects increases."
  - [corpus] No direct corpus evidence found supporting this specific mechanism for counting vs non-counting tasks.
- Break condition: If the captioning model learns to include precise counts in its output, the counting performance gap might narrow.

### Mechanism 3
- Claim: The two-step pipeline leverages the complementary strengths of VLMs (visual understanding) and LLMs (text reasoning).
- Mechanism: VLMs excel at processing visual information and generating descriptive text, while LLMs are better at reasoning over text. By separating these tasks, the pipeline allows each model type to operate within its strengths. The VLM extracts visual features and converts them to text, and the LLM performs the reasoning needed to answer the question based on that text.
- Core assumption: The performance gap between direct VQA and the pipeline approach is due to the inherent difficulty of multimodal reasoning versus unimodal reasoning.
- Evidence anchors:
  - [section] "We hypothesize that VLMs are not truly blind and their performance of simple geometry-related tasks can be improved significantly."
  - [section] "We believe captioning followed by QNA works better than direct VQA since the models are largely pre-trained for generating captions and most downstream tasks in general involve the VLM to generate some text."
  - [corpus] Weak evidence - neighboring papers discuss multimodal reasoning challenges but don't specifically validate this complementary strength hypothesis.
- Break condition: If a model is specifically fine-tuned for multimodal reasoning, the performance gap between direct VQA and the pipeline approach might diminish.

## Foundational Learning

- Concept: Multimodal reasoning and the challenges of integrating visual and textual information
  - Why needed here: Understanding why VLMs struggle with direct VQA and how decomposing the task helps requires knowledge of multimodal reasoning challenges
  - Quick check question: What are the key differences between unimodal text reasoning and multimodal visual-textual reasoning that make the latter more challenging?

- Concept: Image captioning as a form of visual abstraction and information compression
  - Why needed here: The effectiveness of keyword-guided captioning depends on understanding how visual information is abstracted into text and what information might be lost in this process
  - Quick check question: What types of visual information are typically preserved or lost when converting images to captions, and how might this affect downstream reasoning tasks?

- Concept: The distinction between qualitative spatial reasoning and quantitative counting
  - Why needed here: The paper shows different performance patterns for counting versus non-counting tasks, suggesting different underlying mechanisms
  - Quick check question: How do the cognitive processes involved in determining "do two circles intersect?" differ from those involved in "how many circles are there?" and how might this map to VLM capabilities?

## Architecture Onboarding

- Component map: Image + Question -> Keyword Extractor -> Caption Generator -> Answer Generator -> Answer
- Critical path:
  1. Extract keywords from question using LLM
  2. Generate guided caption using VLM with keywords as prompt
  3. Feed caption to LLM to generate final answer
  4. Return answer

- Design tradeoffs:
  - Using a single model for both captioning and answering versus specialized models
  - Depth of keyword extraction (1-2 words vs more detailed prompts)
  - Choice of VLM for captioning (base vs fine-tuned models)
  - Handling of counting tasks that don't benefit as much from the pipeline

- Failure signatures:
  - Poor keyword extraction leading to irrelevant captions
  - Captions missing critical visual details needed for answering
  - LLM misinterpreting the caption or failing to extract the answer
  - Performance degradation for counting tasks despite pipeline approach

- First 3 experiments:
  1. Baseline test: Direct VQA using the same VLMs without the keyword-guided captioning pipeline to establish performance without the proposed approach
  2. Ablation test: Test each component separately - first just captioning without keywords, then just keyword extraction without captioning, to isolate the contribution of each step
  3. Counting task focus: Create a specialized experiment focusing exclusively on counting tasks to better understand why the pipeline doesn't improve performance for these tasks as much as for non-counting tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of keyword-guided captioning vary across different types of geometric tasks beyond the eight tasks tested?
- Basis in paper: [explicit] The paper tested eight specific geometric tasks and found varying improvements, with non-counting tasks showing more consistent gains than counting tasks.
- Why unresolved: The study only evaluated a limited set of geometric tasks. Different geometric reasoning challenges (e.g., symmetry detection, angle measurement, shape classification) might respond differently to keyword-guided captioning.
- What evidence would resolve it: Testing the keyword-guided captioning pipeline on a broader and more diverse set of geometric tasks, including those not covered in the current study, would reveal whether the observed pattern holds across various geometric reasoning challenges.

### Open Question 2
- Question: What is the optimal number and specificity of keywords to use for guiding image captioning in geometric reasoning tasks?
- Basis in paper: [inferred] The authors extracted "1-2 word summaries" as keywords but did not systematically explore how keyword quantity or specificity affects performance.
- Why unresolved: The paper does not investigate the sensitivity of results to keyword selection. Too few keywords might miss important details, while too many could introduce noise or bias.
- What evidence would resolve it: Conducting ablation studies that vary the number and specificity of keywords across tasks would identify optimal keyword strategies for different types of geometric reasoning.

### Open Question 3
- Question: How does the keyword-guided captioning approach scale to more complex visual reasoning tasks beyond basic geometry?
- Basis in paper: [explicit] The authors suggest testing on specialized datasets like MathVision and mention the potential for "advanced, large-scale VLMs" but do not actually perform these experiments.
- Why unresolved: The current evaluation focuses on basic geometric tasks. The effectiveness of the approach for more complex visual reasoning, such as multi-step problems or tasks requiring deeper spatial understanding, remains unknown.
- What evidence would resolve it: Evaluating the approach on more complex datasets (like MathVision mentioned in the paper) and tasks requiring multi-step reasoning would demonstrate whether the benefits extend beyond simple geometric questions.

### Open Question 4
- Question: Why do VLMs consistently struggle with counting tasks even when using keyword-guided captioning?
- Basis in paper: [explicit] The authors note that "counting tasks remain particularly challenging and random for VLMs" and hypothesize this is due to "models' inherent limitations" and the rarity of accurate counting captions in training data.
- Why unresolved: While the authors provide hypotheses, they do not empirically investigate the root causes of counting difficulties or test potential solutions.
- What evidence would resolve it: Systematic investigation into whether the issue stems from training data biases, architectural limitations, or other factors, along with experiments testing targeted interventions (e.g., specialized counting-focused fine-tuning), would clarify the underlying problem.

## Limitations
- Dataset reproducibility: The Blind dataset images are not publicly available, making independent verification difficult
- Limited ablation studies: The paper doesn't systematically explore how different keyword extraction strategies affect results
- Counting task inconsistency: The pipeline shows inconsistent improvements for counting tasks, suggesting fundamental limitations remain unaddressed

## Confidence
- High confidence: Overall finding that keyword-guided captioning improves VLM performance on geometric tasks
- Medium confidence: Mechanism explanation for why keyword-guided captioning works better than direct VQA
- Low confidence: Scalability claims to more complex tasks and larger VLMs without empirical validation

## Next Checks
1. **Dataset reproducibility test**: Recreate the eight geometric tasks using publicly available images or synthetic data to verify that the reported accuracy improvements are reproducible with different image sources.
2. **Ablation study of keyword extraction**: Systematically vary the keyword extraction method (different numbers of keywords, different extraction models) to determine how much of the performance improvement comes specifically from the keyword guidance versus the captioning process itself.
3. **Counting task deep dive**: Design a controlled experiment focusing exclusively on counting tasks with varying object quantities to better understand why the pipeline approach shows inconsistent improvements for these tasks and whether specific caption modifications could address this limitation.