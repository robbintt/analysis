---
ver: rpa2
title: 'Heidelberg-Boston @ SIGTYP 2024 Shared Task: Enhancing Low-Resource Language
  Analysis With Character-Aware Hierarchical Transformers'
arxiv_id: '2405.20145'
source_url: https://arxiv.org/abs/2405.20145
tags:
- language
- tagging
- word
- encoder
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents the winning approach for the constrained subtask
  of the SIGTYP 2024 shared task, focusing on PoS tagging, morphological tagging,
  and lemmatization for 13 historical languages. The authors employ a hierarchical
  transformer architecture combining character-level and word-level representations,
  leveraging a modified DeBERTa-V3 model with replaced token detection pre-training.
---

# Heidelberg-Boston @ SIGTYP 2024 Shared Task: Enhancing Low-Resource Language Analysis With Character-Aware Hierarchical Transformers

## Quick Facts
- arXiv ID: 2405.20145
- Source URL: https://arxiv.org/abs/2405.20145
- Reference count: 14
- Primary result: Winning approach for constrained SIGTYP 2024 shared task using hierarchical transformers for PoS tagging, morphological tagging, and lemmatization on 13 historical languages

## Executive Summary
This work presents the winning approach for the constrained subtask of the SIGTYP 2024 shared task, focusing on PoS tagging, morphological tagging, and lemmatization for 13 historical languages. The authors employ a hierarchical transformer architecture combining character-level and word-level representations, leveraging a modified DeBERTa-V3 model with replaced token detection pre-training. For lemmatization, they use character-level T5 models. The models are pre-trained from scratch on small corpora of historical languages. Their approach achieves first place in the constrained subtask, with performance nearly matching the unconstrained task winner. The method demonstrates that high-quality language analysis is possible even with limited monolingual data, without relying on cross-lingual transfer or large pre-trained models.

## Method Summary
The authors develop a hierarchical transformer architecture that combines character-level and word-level representations for low-resource language analysis. The approach uses a modified DeBERTa-V3 model with replaced token detection (RTD) pre-training, trained from scratch on small corpora of historical languages. For lemmatization, character-level T5 models are employed. The method incorporates multi-task learning for PoS and morphological tagging, allowing the model to share representations across related tasks. The entire system is trained on 13 historical languages from diverse families (Indo-European, Afro-Asiatic, Sino-Tibetan, Finno-Ugric) and scripts (Greek, Hebrew, Hanzi, Egyptian, Latin, Cyrillic), demonstrating the approach's versatility across different linguistic and orthographic systems.

## Key Results
- Achieved first place in the constrained subtask of SIGTYP 2024 shared task
- Performance nearly matches the unconstrained task winner despite using only provided data
- Demonstrates effective language analysis for historical languages with limited monolingual data
- Successfully handles diverse language families and writing systems using character-level tokenization

## Why This Works (Mechanism)
The hierarchical transformer architecture effectively captures both character-level morphology and word-level syntactic patterns, which is crucial for historical languages with complex inflectional systems and limited data. By training from scratch on small corpora rather than relying on cross-lingual transfer or large pre-trained models, the approach learns representations specifically tailored to each historical language's unique characteristics. The multi-task learning setup allows the model to leverage shared information between PoS tagging and morphological tagging, improving performance on both tasks. The character-level T5 models for lemmatization are particularly effective for historical languages where morphological segmentation is often non-trivial due to historical orthographic conventions.

## Foundational Learning
- **Hierarchical transformers**: Needed for capturing both character-level morphology and word-level syntax in historical languages. Quick check: Verify that character-level embeddings are properly aggregated into word representations.
- **Replaced token detection (RTD)**: Needed for effective pre-training on small corpora. Quick check: Confirm that RTD loss is properly computed and contributes to model convergence.
- **Multi-task learning**: Needed to share representations between PoS and morphological tagging. Quick check: Compare single-task vs multi-task performance to verify knowledge transfer.
- **Character-level tokenization**: Needed for handling diverse writing systems in historical languages. Quick check: Ensure character vocabulary covers all scripts in the target languages.
- **Historical language characteristics**: Needed to understand challenges like orthographic variation and limited data. Quick check: Review error analysis for patterns specific to historical language phenomena.

## Architecture Onboarding

**Component map:**
Character-level tokenization -> Hierarchical Transformer (DeBERTa-V3 + RTD) -> Word-level representations -> PoS/Morphological tagging
Character-level tokenization -> Character-level T5 -> Lemmatization

**Critical path:**
The hierarchical transformer is the critical path, as it handles both PoS tagging and morphological tagging through multi-task learning. The character-level T5 models for lemmatization run in parallel but are also essential for the complete system.

**Design tradeoffs:**
- Training from scratch vs. using pre-trained models: The authors chose to train from scratch to maintain the constrained setting, sacrificing some potential performance gains for methodological purity.
- Character-level vs. subword tokenization: Character-level tokenization was chosen to handle diverse writing systems and morphological complexity, though at the cost of longer sequences.
- Multi-task learning vs. single-task training: Multi-task learning was used to share representations, though this adds complexity to the training procedure.

**Failure signatures:**
- Poor convergence during pre-training: May indicate issues with RTD implementation or insufficient pre-training data.
- Overfitting on small datasets: Common in low-resource settings, requiring careful regularization and early stopping.
- Task interference in multi-task learning: May occur if PoS and morphological tagging have conflicting optimization objectives.

**First experiments:**
1. Train the hierarchical transformer on a single historical language and evaluate PoS tagging performance to verify basic functionality.
2. Implement and test the character-level T5 models for lemmatization on a small dataset to confirm lemmatization accuracy.
3. Conduct an ablation study comparing the hierarchical approach to a flat character-level transformer to quantify the benefits of the hierarchical structure.

## Open Questions the Paper Calls Out

**Open Question 1:** How would the hierarchical transformer architecture perform on other low-resource language tasks beyond PoS tagging, morphological tagging, and lemmatization? The paper demonstrates success with a hierarchical transformer approach on three specific tasks for historical languages, but does not explore other potential applications. Testing the hierarchical transformer on tasks like named entity recognition, semantic role labeling, or syntactic parsing in low-resource settings would provide insights into its broader applicability.

**Open Question 2:** Would using larger pre-training corpora for the historical languages lead to significant improvements in downstream task performance? The authors specifically focused on the constrained setting using only the provided data, noting that their models "achieved first place in the constrained subtask, nearly reaching the performance levels of the unconstrained task's winner." Training the same models with extended corpora of historical language texts (e.g., from other treebanks or digitized historical documents) and comparing performance to the constrained setting would provide an answer.

**Open Question 3:** How does the performance of the hierarchical transformer compare to other tokenization-free approaches for historical languages? The authors highlight their character-aware approach but do not compare it to other recent tokenization-free methods like Canines or similar architectures. Implementing and evaluating competing tokenization-free architectures (e.g., Canines, character-based transformers without the hierarchical structure) on the same historical language tasks would enable direct comparison.

## Limitations
- Limited details on replaced token detection (RTD) pre-training implementation and hyperparameters
- Character-level T5 models for lemmatization lack detailed architectural specifications
- Performance metrics are averaged across 13 diverse historical languages, making it difficult to assess effectiveness for truly low-resource or highly divergent languages
- Evaluation only considers constrained setting without comparison to cross-lingual transfer approaches

## Confidence
- **High confidence**: The hierarchical transformer architecture combining character and word-level representations is well-specified and can be reproduced. The use of DeBERTa-V3 with RTD pre-training is clearly stated.
- **Medium confidence**: The multi-task learning approach for PoS and morphological tagging is described, but the specific task weights and training schedule are not detailed. The reported performance metrics (accuracy and F1 scores) are standard and verifiable.
- **Low confidence**: The exact implementation of RTD pre-training and the character-level T5 models for lemmatization lack sufficient detail for faithful reproduction. The claim that the method "nearly matches" the unconstrained task winner is difficult to verify without knowing the specific baseline models and metrics used in the unconstrained setting.

## Next Checks
1. Implement and train the DeBERTa-V3 model with RTD pre-training on a small corpus of a historical language (e.g., Old Church Slavonic) and compare the pre-trained representations to those reported in the paper.
2. Train character-level T5 models for lemmatization on the same historical language and evaluate their performance on a held-out test set, comparing to the paper's reported accuracy@1 and accuracy@3 scores.
3. Conduct an ablation study to assess the impact of the hierarchical transformer architecture and multi-task learning on the final performance, comparing to single-task training and simpler architectures.