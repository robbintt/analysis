---
ver: rpa2
title: 'A Dynamic Model of Performative Human-ML Collaboration: Theory and Empirical
  Evidence'
arxiv_id: '2405.13753'
source_url: https://arxiv.org/abs/2405.13753
tags:
- performance
- human
- collaborative
- utility
- knapsack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a theoretical framework for understanding dynamic
  human-ML collaboration where ML models are trained on human decisions that may deviate
  from ground truth, and human decisions are influenced by ML recommendations. The
  authors introduce a "collaborative characteristic function" that maps ML performance
  to human+ML performance, showing that this dynamic system can converge to stable
  points that may be suboptimal relative to ground truth.
---

# A Dynamic Model of Performative Human-ML Collaboration: Theory and Empirical Evidence

## Quick Facts
- arXiv ID: 2405.13753
- Source URL: https://arxiv.org/abs/2405.13753
- Reference count: 40
- Humans improve upon ML recommendations in knapsack problems, reaching equilibrium at ~92% maximum value

## Executive Summary
This paper introduces a theoretical framework for understanding dynamic human-ML collaboration where ML models are trained on human decisions that may deviate from ground truth, and human decisions are influenced by ML recommendations. The authors develop a "collaborative characteristic function" that maps ML performance to human+ML performance, revealing that such systems can converge to stable but potentially suboptimal equilibria. As a proof of concept, they conduct a user study with 1,408 participants solving knapsack problems under varying ML recommendation quality and monetary incentives. The empirical results show that humans consistently improve upon ML recommendations, monetary incentives have no effect on performance, and the system appears to reach an equilibrium at approximately 92% of maximum knapsack value.

## Method Summary
The authors develop a theoretical model of performative human-ML collaboration where ML models are trained on human decisions influenced by ML recommendations. They introduce a collaborative characteristic function that describes how ML performance translates to joint human+ML performance. To validate their framework, they conduct an online user study with 1,408 participants solving knapsack problems with varying quality ML recommendations and monetary incentives. Participants made decisions with or without ML recommendations, and their choices were used to retrain the ML model in subsequent rounds, creating a dynamic feedback loop. The study measures both individual and collaborative performance across multiple iterations to identify equilibrium points.

## Key Results
- Humans consistently improve upon ML recommendations across various performance levels
- Monetary incentives do not affect human performance in the experimental setting
- The system reaches an equilibrium at approximately 92% of maximum knapsack value

## Why This Works (Mechanism)
The framework works by modeling the iterative feedback loop between human decision-makers and ML systems. As humans observe ML recommendations and make their own decisions, these decisions become training data for the next iteration of the ML model. This creates a dynamic system where both human behavior and ML performance co-evolve. The collaborative characteristic function captures how improvements in ML performance translate to improvements in joint human+ML outcomes, allowing the analysis of whether the system converges to optimal or suboptimal equilibria relative to ground truth.

## Foundational Learning
- Performative prediction: ML models that influence the data they subsequently predict, requiring understanding of how model deployment affects the distribution of future data
- Why needed: Traditional ML assumes static data distributions, but human-ML collaboration creates dynamic feedback loops
- Quick check: Verify that the collaborative characteristic function captures the relationship between ML and joint performance

- Knapsack problem domain: Optimization problem where items with different values and weights must be selected to maximize total value without exceeding capacity constraints
- Why needed: Provides a controlled environment with quantifiable ground truth to test the theoretical framework
- Quick check: Confirm that knapsack problems have well-defined optimal solutions that can serve as ground truth benchmarks

- Equilibrium analysis: Mathematical examination of whether dynamic systems converge to stable points and characterization of those points
- Why needed: Determines whether human-ML collaboration systems reach optimal or suboptimal steady states
- Quick check: Verify that the identified equilibrium point represents a stable solution to the system dynamics

## Architecture Onboarding

Component map: Human decision-maker -> ML recommendation -> Human+ML joint decision -> Retrained ML model -> New ML recommendation

Critical path: Human observes ML recommendation → Makes decision → System records outcome → ML model retrains → New recommendation generated

Design tradeoffs: The framework trades theoretical generality for domain-specific applicability, focusing on knapsack problems to establish proof of concept while acknowledging limitations in broader generalization.

Failure signatures: The system may converge to suboptimal equilibria if the collaborative characteristic function has multiple stable points, or if human decision-making systematically deviates from ground truth in ways that ML models amplify.

Three first experiments:
1. Test with different optimization problems (e.g., traveling salesman, portfolio optimization) to assess domain generalizability
2. Vary the quality distribution of ML recommendations to understand sensitivity to initial conditions
3. Introduce time-varying incentives to test whether motivation can shift equilibrium points

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- Study focuses on a single domain (knapsack problems) which may not generalize to other collaborative settings
- Assumes humans can accurately assess ML recommendation quality, which may not hold in real-world scenarios where ground truth is often unavailable
- Monetary incentives showed no effect, suggesting the experimental design may not have sufficiently motivated performance improvements

## Confidence

High confidence: The mathematical framework for performative prediction in human-AI collaboration is sound and well-theoretically grounded

Medium confidence: The empirical finding that humans improve upon ML recommendations at various performance levels

Medium confidence: The equilibrium point estimate of 92% maximum value, given the specific experimental conditions

Low confidence: Generalization of findings to other domains and real-world applications

## Next Checks

1. Test the framework with multiple problem domains (e.g., medical diagnosis, financial planning) to assess generalizability

2. Design experiments with stronger incentive structures to determine if performance can be improved through motivation

3. Conduct longitudinal studies to observe how the system evolves over multiple iterations and whether the equilibrium point shifts with increased familiarity