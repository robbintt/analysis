---
ver: rpa2
title: 'ClimateGPT: Towards AI Synthesizing Interdisciplinary Research on Climate
  Change'
arxiv_id: '2401.09646'
source_url: https://arxiv.org/abs/2401.09646
tags:
- climate
- data
- training
- language
- change
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ClimateGPT, a family of domain-specific large
  language models designed to synthesize interdisciplinary research on climate change.
  The models were trained from scratch on a science-oriented dataset of 300B tokens,
  with 4.2B domain-specific tokens included during pre-training.
---

# ClimateGPT: Towards AI Synthesizing Interdisciplinary Research on Climate Change

## Quick Facts
- arXiv ID: 2401.09646
- Source URL: https://arxiv.org/abs/2401.09646
- Reference count: 40
- Key outcome: Domain-specific LLMs for climate change research trained on 300B tokens with retrieval augmentation and multilingual support

## Executive Summary
ClimateGPT introduces a family of domain-specific large language models designed to synthesize interdisciplinary climate change research. The models were trained from scratch on a science-oriented dataset of 300B tokens, with 4.2B domain-specific tokens included during pre-training. ClimateGPT-7B, 13B, and 70B were further continuously pre-trained from Llama-2 on a domain-specific dataset of 4.2B tokens. Each model was instruction fine-tuned on a high-quality, human-generated domain-specific dataset created in collaboration with climate scientists. The models were optimized for retrieval augmentation using a hierarchical retrieval strategy to reduce hallucinations. To increase accessibility, a cascaded machine translation approach was proposed, enabling support for multiple languages. The models were evaluated on climate-specific and general domain benchmarks, with ClimateGPT-7B performing on par with the ten times larger Llama-2-70B Chat model. Human evaluation confirmed the trends observed in the benchmarks. All models were trained and evaluated using renewable energy and are publicly released.

## Method Summary
The authors developed ClimateGPT by training models from scratch on a science-oriented dataset of 300B tokens, with 4.2B domain-specific tokens included during pre-training. They further continuously pre-trained ClimateGPT-7B, 13B, and 70B from Llama-2 on a domain-specific dataset of 4.2B tokens. Each model underwent instruction fine-tuning on a high-quality, human-generated domain-specific dataset created in collaboration with climate scientists. The models were optimized for retrieval augmentation using a hierarchical retrieval strategy to reduce hallucinations. A cascaded machine translation approach was proposed to enable multilingual support. The models were evaluated on climate-specific and general domain benchmarks, with human evaluation confirming the trends observed in the benchmarks.

## Key Results
- ClimateGPT-7B performs on par with the ten times larger Llama-2-70B Chat model on climate-specific and general domain benchmarks
- The models were trained and evaluated using renewable energy
- Human evaluation confirmed the trends observed in the benchmarks
- The cascaded machine translation approach enables multilingual support

## Why This Works (Mechanism)
ClimateGPT leverages domain-specific pre-training on climate-related data to capture the nuances and terminology of climate science. The hierarchical retrieval strategy helps reduce hallucinations by providing context-specific information during generation. The cascaded machine translation approach allows the models to support multiple languages, increasing accessibility. Instruction fine-tuning on a high-quality, human-generated dataset ensures that the models can effectively respond to domain-specific queries.

## Foundational Learning
- Domain-specific pre-training: Why needed - To capture the nuances and terminology of climate science; Quick check - Evaluate on climate-specific benchmarks
- Hierarchical retrieval strategy: Why needed - To reduce hallucinations and provide context-specific information; Quick check - Compare hallucination rates with and without retrieval
- Cascaded machine translation: Why needed - To enable multilingual support and increase accessibility; Quick check - Evaluate translation quality across language pairs

## Architecture Onboarding
The ClimateGPT architecture consists of:
1. Pre-trained transformer-based language model (from scratch or continuous pre-training from Llama-2)
2. Hierarchical retrieval system for context-specific information
3. Instruction fine-tuning module
4. Cascaded machine translation component

Critical path: Pre-training -> Continuous pre-training (if applicable) -> Instruction fine-tuning -> Retrieval augmentation -> Evaluation

Design tradeoffs:
- Balancing domain-specific knowledge with general language understanding
- Tradeoff between model size and performance
- Complexity of cascaded machine translation approach

Failure signatures:
- Hallucinations in generated text
- Degradation in translation quality for certain language pairs
- Overfitting to domain-specific data at the expense of general language understanding

First experiments:
1. Evaluate ClimateGPT on climate-specific benchmarks and compare with general-purpose models
2. Assess translation quality across diverse language pairs
3. Quantify the contribution of the hierarchical retrieval strategy to hallucination reduction

## Open Questions the Paper Calls Out
None

## Limitations
- Major uncertainties remain regarding the true domain-specific performance gains of ClimateGPT models, as the evaluation primarily compared against general-purpose models rather than established climate-specific baselines
- The cascaded machine translation approach for multilingual support, while innovative, has not been thoroughly validated for accuracy and consistency across different language pairs
- The environmental claims of using renewable energy for training are difficult to independently verify, and the energy efficiency of the training process is not quantified

## Confidence
- High confidence in the technical implementation of the model architecture and training pipeline
- Medium confidence in the benchmark results and comparative performance claims
- Low confidence in the generalizability of multilingual capabilities and retrieval-augmented generation performance

## Next Checks
1. Conduct head-to-head comparisons between ClimateGPT and existing climate-specific language models on identical benchmarks to isolate domain-specific performance improvements
2. Perform comprehensive multilingual evaluation across diverse language pairs to validate the cascaded translation approach and identify potential quality degradation
3. Implement controlled experiments to quantify the contribution of the hierarchical retrieval strategy to hallucination reduction, including ablation studies comparing different retrieval configurations