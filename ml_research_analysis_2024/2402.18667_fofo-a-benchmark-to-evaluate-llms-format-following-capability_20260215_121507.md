---
ver: rpa2
title: 'FOFO: A Benchmark to Evaluate LLMs'' Format-Following Capability'
arxiv_id: '2402.18667'
source_url: https://arxiv.org/abs/2402.18667
tags:
- format
- llms
- data
- arxiv
- fofo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "FOFO is a benchmark for evaluating LLMs\u2019 ability to follow\
  \ complex, domain-specific formats. It uses an AI-Human collaborative method to\
  \ create real-world format-oriented instructions across diverse domains."
---

# FOFO: A Benchmark to Evaluate LLMs' Format-Following Capability

## Quick Facts
- arXiv ID: 2402.18667
- Source URL: https://arxiv.org/abs/2402.18667
- Reference count: 40
- FOFO reveals that LLMs' format-following capability is independent of content generation quality, with open-source models significantly lagging behind closed-source ones

## Executive Summary
FOFO is a novel benchmark designed to evaluate large language models' ability to follow complex, domain-specific formats. Developed through an AI-Human collaborative method, it tests LLMs across 10 domains and 50 subdomains using format-oriented instructions. The benchmark reveals that format-following performance varies significantly across domains and is largely independent of content generation quality. Notably, open-source models like Mistral 7B and Llama 7B perform significantly worse than closed-source models like GPT-3.5 and GPT-4, highlighting the need for specialized tuning for format-following skills.

## Method Summary
The FOFO benchmark uses an AI-Human collaborative method where GPT-4 generates domain/subdomain lists and format specifications, which are iteratively refined by human experts to ensure quality and relevance. Format-oriented instructions (FORMAT-INSTRU) are created with detailed format specifications and example outputs. LLMs are evaluated on their ability to generate responses adhering to these specifications. GPT-4 serves as the primary evaluator, classifying outputs as correct or incorrect based on format adherence, with human evaluation conducted to ensure high GPT-4-human agreement. The benchmark covers diverse domains including business, education, finance, legal, and medical fields.

## Key Results
- LLMs' format-following performance varies independently from their content generation quality
- Open-source models (Mistral 7B, Llama 7B) significantly underperform compared to closed-source models (GPT-3.5, GPT-4) on format tasks
- Format proficiency varies substantially across different domains, with some models showing domain-specific weaknesses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AI-Human collaborative method produces realistic, complex format-oriented instructions that expose LLM weaknesses
- Mechanism: GPT-4 generates domain/subdomain lists and format specifications, which are iteratively refined by human experts to ensure quality and relevance. This hybrid approach leverages AI's breadth and human's precision
- Core assumption: Domain experts can reliably identify and refine AI-generated format specifications to match real-world complexity
- Evidence anchors:
  - [abstract] "developed through an AI-Human collaborative method"
  - [section] "human experts conduct a thorough review... any domain or subdomain misaligned... triggers a reiteration of GPT-4"
  - [corpus] Weak: No explicit mention of validation studies showing effectiveness of this method
- Break condition: If human reviewers fail to identify subtle domain-specific format nuances or if GPT-4's initial generation is too generic, the benchmark loses its edge

### Mechanism 2
- Claim: GPT-4's binary classification evaluation with explanations is effective at measuring format adherence
- Mechanism: GPT-4 acts as an automated judge, classifying LLM outputs as correct or incorrect based on format specifications, with explanations for failures. This reduces human annotation costs while maintaining quality
- Core assumption: GPT-4's understanding of format specifications is accurate enough to reliably judge LLM output compliance
- Evidence anchors:
  - [abstract] "GPT-4 is used as the evaluator"
  - [section] "human evaluation is conducted to ensure a high GPT-4-Human agreement"
  - [corpus] Weak: No explicit mention of GPT-4's accuracy compared to human evaluators
- Break condition: If GPT-4's understanding of complex format specifications diverges significantly from human understanding, evaluation quality degrades

### Mechanism 3
- Claim: FOFO's diverse domain coverage reveals format-following variability across domains, highlighting specialized model needs
- Mechanism: By testing LLMs across 10 domains and 50 subdomains, FOFO uncovers that format-following performance varies significantly by domain, even for models with similar overall accuracy
- Core assumption: Real-world LLM deployment requires domain-specific format-following capabilities, not just general instruction following
- Evidence anchors:
  - [abstract] "LLMs' format proficiency varies across different domains"
  - [section] "Figure 2 shows two examples... Mistral 7B V0.1 performs significantly worse on Scientific Research and Development domain"
  - [corpus] Weak: No explicit mention of correlation between domain-specific format-following and real-world success
- Break condition: If format-following requirements across domains converge or become standardized, domain-specific variations become less relevant

## Foundational Learning

- Concept: Format specification comprehension
  - Why needed here: Understanding format specifications is crucial for evaluating LLM outputs against predefined requirements
  - Quick check question: Given a JSON format specification, can you identify whether a sample output adheres to it?

- Concept: Domain-specific knowledge
  - Why needed here: Different domains have unique format requirements that impact LLM performance
  - Quick check question: What are the key differences between medical prescription formats and legal document formats?

- Concept: Automated evaluation methodologies
  - Why needed here: Efficiently evaluating LLM outputs at scale requires automated approaches like GPT-4 judging
  - Quick check question: How would you design an automated system to evaluate format adherence in LLM outputs?

## Architecture Onboarding

- Component map: AI-Human collaborative pipeline → Format generation → Instruction generation → LLM evaluation → GPT-4 judging → Human validation (optional)
- Critical path: Human expert review → GPT-4 generation → LLM evaluation → Format correctness assessment
- Design tradeoffs: AI-Human collaboration balances efficiency with quality but introduces dependency on human expertise. GPT-4 judging reduces costs but may miss subtle format nuances
- Failure signatures: Inconsistent format specifications, low GPT-4-Human agreement, or domain coverage gaps indicate architectural weaknesses
- First 3 experiments:
  1. Evaluate a diverse set of open-source and closed-source LLMs on FOFO to establish baseline performance differences
  2. Analyze domain-specific performance variations to identify areas needing specialized tuning
  3. Compare GPT-4's evaluation accuracy against human evaluators on a subset of FOFO to validate the judging mechanism

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance gap between open-source and closed-source LLMs on format-following tasks compare across different domains?
- Basis in paper: Explicit, from domain analysis results showing performance discrepancies between models like Mistral 7B and Llama 7B across domains
- Why unresolved: The paper shows performance differences exist but doesn't provide a comprehensive analysis of how the open/closed-source gap varies by domain
- What evidence would resolve it: Detailed domain-by-domain performance comparison between open and closed-source models, with statistical analysis of the gap magnitude

### Open Question 2
- Question: What is the minimum level of format specification detail required in FORMAT-INSTRU to reliably test LLM format-following capabilities?
- Basis in paper: Explicit, from the observation that missing single format requirements causes LLM failure, but no analysis of specification granularity
- Why unresolved: The paper doesn't investigate the relationship between format specification complexity and LLM performance
- What evidence would resolve it: Controlled experiments varying format specification detail while keeping content complexity constant

### Open Question 3
- Question: How do cost-effective alternatives to GPT-4 (like GPT-4-Turbo) compare in evaluating LLM format-following performance?
- Basis in paper: Explicit, from the cost analysis section discussing GPT-4-Turbo as a potential alternative
- Why unresolved: The paper mentions considering GPT-4-Turbo but doesn't provide empirical comparison data
- What evidence would resolve it: Direct comparison of evaluation consistency and cost between GPT-4 and GPT-4-Turbo across the same model outputs

## Limitations

- The benchmark's effectiveness depends heavily on human expert review quality during AI-Human collaborative generation, lacking explicit validation of this methodology's consistency
- Automated evaluation using GPT-4 as judge hasn't been rigorously validated against human evaluation across the full dataset
- The paper doesn't address potential Western-centric or model-type biases in format specifications or scalability challenges for domain expansion

## Confidence

**High Confidence:** The observation that format-following performance varies independently from content generation quality, and that open-source models significantly lag behind closed-source ones on format tasks

**Medium Confidence:** The effectiveness of the AI-Human collaborative method for generating realistic format specifications, as this relies on implicit rather than explicit validation of the methodology's quality

**Medium Confidence:** The reliability of GPT-4 as an automated evaluator for format adherence, given that agreement with human evaluators is mentioned but not quantified, and potential systematic biases in GPT-4's understanding of format specifications aren't explored

## Next Checks

1. **Human Evaluation Validation:** Conduct a comprehensive human evaluation of GPT-4's format judgments across all domains to quantify agreement rates and identify systematic error patterns in the automated evaluation process

2. **Domain Coverage Audit:** Perform a systematic audit of the format specifications across domains to identify potential Western-centric or model-type biases, ensuring the benchmark represents diverse real-world format requirements

3. **Generalization Test:** Evaluate whether format-following performance on FOFO correlates with successful real-world deployment in the benchmark's target domains, establishing external validity beyond controlled testing