---
ver: rpa2
title: Forecasting GPU Performance for Deep Learning Training and Inference
arxiv_id: '2407.13853'
source_url: https://arxiv.org/abs/2407.13853
tags:
- latency
- gpus
- neusight
- performance
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NeuSight predicts the performance of deep learning models on GPUs,
  addressing the challenge of limited access to newer GPUs for performance evaluation.
  The framework leverages both GPU hardware behavior and software library optimizations
  to estimate end-to-end performance.
---

# Forecasting GPU Performance for Deep Learning Training and Inference

## Quick Facts
- arXiv ID: 2407.13853
- Source URL: https://arxiv.org/abs/2407.13853
- Reference count: 40
- Primary result: NeuSight reduces GPT3 latency prediction error from 121.4% to 2.3% on H100 GPU

## Executive Summary
NeuSight addresses the challenge of predicting deep learning model performance on GPUs when access to newer hardware is limited. The framework decomposes prediction into tile-level utilization estimation bounded by GPU performance laws, rather than directly modeling overall latency. It outperforms prior work across various workloads and GPUs, with average errors dropping from 30.8% to 2.3% for inference and 121.4% to 2.3% for training on GPT3 models. NeuSight supports both training and inference scenarios and can extend predictions to distributed execution by modeling network collective operations.

## Method Summary
NeuSight predicts GPU performance by decomposing deep learning kernels into tile-level predictions. The framework uses an MLP to predict device utilization coefficients based on GPU features and tile characteristics, then bounds predictions using fundamental performance laws (peak FLOPs, memory bandwidth) before aggregating to full kernel latency. The approach extends to distributed execution by modeling network collective operations (all-reduce, send/receive) based on link bandwidth. The framework is trained on data from older generation GPUs and existing kernel information, enabling accurate predictions on unseen hardware like H100.

## Key Results
- GPT3 model latency prediction error reduced from 121.4% to 2.3% on H100 GPU
- Average prediction error for unseen GPUs (H100, L4) is 8.7%
- Outperforms prior work by significant margins across various deep learning workloads including BERT, GPT2, OPT, and Switch Transformer

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing a kernel into tile-level predictions bounded by roofline laws improves generalization to unseen GPUs.
- Mechanism: The framework breaks each deep learning kernel into smaller tiles, predicts utilization per tile via MLP, then bounds predictions using fundamental GPU performance laws (peak FLOPs, memory bandwidth) before aggregating to full kernel latency.
- Core assumption: Tile-level behavior is more predictable than full kernel behavior because it operates within stable architectural bounds.
- Evidence anchors: [abstract] "NeuSight decomposes the prediction problem into smaller problems, bounding the prediction through fundamental performance laws."

### Mechanism 2
- Claim: Predicting device utilization as a function of number of waves yields more accurate latency estimates than direct latency regression.
- Mechanism: The framework trains an MLP to predict utilization coefficients (alpha and beta) that model the relationship between number of waves and device utilization, using GPU features and tile characteristics.
- Core assumption: Utilization is a more stable intermediate variable than raw latency for machine learning models to predict.
- Evidence anchors: [section] "This equation models utilization, which increases as the number of waves increases due to the negative beta factor but is capped by alpha."

### Mechanism 3
- Claim: Extending per-GPU predictions to distributed execution by modeling network collective operations enables accurate multi-GPU performance forecasting.
- Mechanism: The framework adds network operator latency predictions (all-reduce, send/receive) based on link bandwidth to the per-GPU kernel predictions to estimate end-to-end distributed training or inference latency.
- Core assumption: Network collective performance scales predictably with bandwidth and can be estimated independently of compute predictions.
- Evidence anchors: [section] "NeuSight estimates the latency of ring-based allreduce and peer-to-peer send/receive based on the bandwidth of the network link."

## Foundational Learning

- Concept: GPU memory hierarchy and tile-based execution
  - Why needed here: Understanding how GPUs execute kernels in tiles across SMs is essential to grasp why NeuSight's tile-level decomposition works.
  - Quick check question: What is the relationship between tile size, SM count, and wave execution in GPU kernel execution?

- Concept: Roofline performance model
  - Why needed here: The framework uses roofline analysis to bound predictions between compute-bound and memory-bound performance limits.
  - Quick check question: How do you calculate arithmetic intensity and roofline bandwidth for a given kernel?

- Concept: Deep learning operator fusion
  - Why needed here: NeuSight supports operator fusion prediction by accumulating FLOPs and adjusting memory requirements, which is important for accurate latency estimation of modern frameworks.
  - Quick check question: How does operator fusion affect the memory and compute requirements of consecutive kernels?

## Architecture Onboarding

- Component map: Model graph extraction -> per-kernel tile prediction -> utilization estimation via MLP -> latency aggregation -> (optional) distributed execution prediction with network operators
- Critical path: Model graph extraction → per-kernel tile prediction → utilization estimation via MLP → latency aggregation → (optional) distributed execution prediction with network operators
- Design tradeoffs: The framework trades model complexity for accuracy by using simple MLPs with architectural constraints rather than large models that might overfit to training data. This improves generalization to unseen GPUs but requires careful feature engineering.
- Failure signatures: High percentage error on out-of-distribution GPUs suggests tile decomposition or utilization prediction is failing. OOM errors during distributed execution indicate network operator modeling or tensor splitting strategy issues.
- First 3 experiments:
  1. Measure actual vs predicted latency for BMM on a known GPU with varying tile sizes to validate tile-level decomposition accuracy.
  2. Compare utilization predictions from the MLP against measured utilization on different GPUs to validate the utilization prediction model.
  3. Test distributed execution predictions by measuring actual vs predicted latency for data-parallel training on a 4-GPU server to validate network operator modeling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How accurately can NeuSight predict the performance of deep learning models on GPUs with significantly different architectures than those in the training set?
- Basis in paper: [explicit] The paper mentions NeuSight's ability to predict performance on out-of-distribution GPUs, showing an average error of 8.7% for unseen GPUs like H100 and L4.
- Why unresolved: While NeuSight shows promising results on a few out-of-distribution GPUs, the paper does not explore its performance on a wider range of GPUs with varying architectures, such as those from different manufacturers or with significantly different configurations.
- What evidence would resolve it: Testing NeuSight on a diverse set of GPUs, including those from different manufacturers and with varying architectures, and comparing its prediction accuracy to other baseline methods.

### Open Question 2
- Question: How does the accuracy of NeuSight's predictions vary across different deep learning workloads, such as computer vision tasks versus natural language processing tasks?
- Basis in paper: [explicit] The paper evaluates NeuSight on a variety of deep learning workloads, including BERT, GPT2, GPT3, OPT, and Switch Transformer. However, it does not explicitly analyze the prediction accuracy across different workload types.
- Why unresolved: The paper does not provide a detailed breakdown of NeuSight's performance on different workload categories, making it difficult to assess its generalizability across diverse deep learning applications.
- What evidence would resolve it: Conducting a comprehensive evaluation of NeuSight on a wide range of deep learning workloads, including those from different domains like computer vision, natural language processing, and reinforcement learning, and comparing its prediction accuracy across these categories.

### Open Question 3
- Question: Can NeuSight accurately predict the performance of deep learning models on GPUs with specialized hardware accelerators, such as tensor cores or ray tracing cores?
- Basis in paper: [inferred] The paper focuses on predicting performance on general-purpose GPUs and does not explicitly discuss the accuracy of NeuSight on GPUs with specialized hardware accelerators.
- Why unresolved: The paper does not provide any information on how NeuSight handles the unique execution patterns and optimizations associated with specialized hardware accelerators, which could potentially impact its prediction accuracy.
- What evidence would resolve it: Evaluating NeuSight on GPUs with specialized hardware accelerators, such as tensor cores or ray tracing cores, and comparing its prediction accuracy to that on general-purpose GPUs.

## Limitations

- The framework's accuracy may degrade for GPU architectures with significantly different wave scheduling or tile execution strategies than those in the training set
- Distributed execution predictions rely on simplified network collective modeling that may not capture complex topology effects or congestion patterns in real-world deployments
- The framework assumes tile-level behavior is more predictable than full kernel behavior, but this may not hold for all operator types or GPU designs

## Confidence

**High Confidence**: The mechanism of decomposing kernels into tile-level predictions bounded by roofline laws is well-supported by GPU architecture principles and the framework's strong empirical performance across multiple GPU generations.

**Medium Confidence**: The utilization prediction model based on wave count shows promise but may face challenges with GPUs that implement non-standard wave scheduling or when operators exhibit complex memory access patterns that break the assumed linear relationship.

**Low Confidence**: The distributed execution predictions for multi-GPU scenarios are the most uncertain component, as they depend on simplified network modeling that may not capture real-world performance variations due to topology, congestion, or implementation-specific optimizations.

## Next Checks

1. **Cross-Architecture Generalization Test**: Evaluate NeuSight's tile-level decomposition accuracy on a GPU architecture with fundamentally different wave scheduling (e.g., AMD Instinct vs NVIDIA Hopper) to validate the framework's generalization beyond NVIDIA's execution model.

2. **Network Collective Sensitivity Analysis**: Measure actual vs predicted distributed training latency under varying network topologies (ring vs tree vs direct) and congestion levels to assess the accuracy of the network operator modeling assumptions.

3. **Operator Coverage Expansion**: Test the framework's accuracy on emerging operator types (e.g., attention mechanisms with FlashAttention optimization, custom kernels) to identify potential failure modes in the tile decomposition and utilization prediction for operators not well-represented in the training data.