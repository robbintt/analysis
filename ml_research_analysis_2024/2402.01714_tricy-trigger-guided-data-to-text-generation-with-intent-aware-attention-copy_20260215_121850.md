---
ver: rpa2
title: 'TrICy: Trigger-guided Data-to-text Generation with Intent aware Attention-Copy'
arxiv_id: '2402.01714'
source_url: https://arxiv.org/abs/2402.01714
tags:
- generation
- language
- data
- name
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TrICy introduces a novel lightweight encoder-decoder architecture
  for intent-driven data-to-text generation, incorporating message intent, optional
  user triggers, and an attention-copy mechanism for accurate out-of-vocabulary word
  prediction. Evaluated on E2E NLG, WebNLG, and a custom dataset, TrICy achieves new
  state-of-the-art performance with 69.29% BLEU on E2E NLG and competitive scores
  on other datasets, all with under 60% of the parameters of comparable models.
---

# TrICy: Trigger-guided Data-to-text Generation with Intent aware Attention-Copy

## Quick Facts
- **arXiv ID:** 2402.01714
- **Source URL:** https://arxiv.org/abs/2402.01714
- **Reference count:** 40
- **Key outcome:** TrICy achieves new state-of-the-art BLEU score of 69.29% on E2E NLG with under 60% of parameters compared to comparable models

## Executive Summary
TrICy introduces a novel lightweight encoder-decoder architecture for intent-driven data-to-text generation, incorporating message intent, optional user triggers, and an attention-copy mechanism for accurate out-of-vocabulary word prediction. The framework enables controlled, context-aware text generation suitable for on-device deployment, outperforming large language models in both accuracy and efficiency. Evaluated on E2E NLG, WebNLG, and a custom dataset, TrICy achieves competitive scores while maintaining a compact model size.

## Method Summary
TrICy is a two-encoder, one-decoder architecture that processes intent and optional triggers through one encoder, while another encoder handles structured data fields and values. The decoder uses Bahdanau attention combined with a copy mechanism to accurately predict out-of-vocabulary words, particularly personally identifiable information (PII). During training, a subset of samples is augmented with triggers (typically the first word of the reference) to teach the model to condition generation on this additional context. The final output probability is a weighted combination of generated and copied tokens, allowing faithful reproduction of unseen words while maintaining semantic coherence.

## Key Results
- Achieves new state-of-the-art BLEU score of 69.29% on E2E NLG dataset
- Outperforms large language models with under 60% of their parameter count
- Demonstrates significant improvements with optional trigger inputs, especially for intent-specific generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TrICy's attention-copy mechanism accurately predicts out-of-vocabulary words, especially PII, by jointly generating and copying tokens from the input source.
- Mechanism: The decoder combines a generate mode (standard attention-based prediction) with a copy mode (directly copying tokens from the input fields/values). The final token probability is a weighted sum of both modes, enabling faithful reproduction of unseen PII.
- Core assumption: The input contains the exact tokens that need to be copied, and the attention mechanism can align them with the correct output positions.
- Evidence anchors:
  - [abstract] "We leverage an attention-copy mechanism to predict out-of-vocabulary (OOV) words accurately."
  - [section III-C] "The decoder utilizes − →H to predict the output sequence. We consider a vocabulary V = {x1, x2, · · · , x|V|}. Furthermore, we maintain unique words in the input source, X , which may contain words, not in V."

### Mechanism 2
- Claim: Introducing optional triggers during training and inference allows TrICy to generate contextually relevant and user-directed responses, significantly improving BLEU and METEOR scores.
- Mechanism: Triggers are prepended to the input sequence, acting as a soft template or directive. During training, a subset of samples is augmented with triggers (e.g., the first word of the reference), teaching the model to condition generation on this additional context. During inference, user-provided triggers guide the output toward desired phrasing or style.
- Core assumption: The trigger is semantically compatible with the input data and intent, and the model can learn to use it effectively without overfitting to its presence.
- Evidence anchors:
  - [abstract] "Moreover, we show that by leveraging an optional trigger input, data-to-text generation quality increases significantly and achieves the new SOTA score of 69.29% BLEU for E2E NLG."
  - [section V-C] "We conduct performance analysis on models trained with multiple configurations based on trK... The presence of trigger K in input significantly improves the generation of a directed output sequence."

### Mechanism 3
- Claim: The two-encoder architecture, with one encoder for intent/trigger and another for data fields/values, enables controlled generation of alternative text sequences based on the underlying intent.
- Mechanism: Encoder EI processes intent and optional trigger, producing a hidden state − →hEI. Encoder ED processes the input data dictionary (fields and values), producing − →hED. These are concatenated to form the final hidden state − →H, which the decoder conditions on. This separation allows the model to focus on relevant content and intent simultaneously.
- Core assumption: Intent and trigger information can be effectively encoded and combined with data field representations to guide generation.
- Evidence anchors:
  - [abstract] "TrICy introduces a novel lightweight encoder-decoder architecture for intent-driven data-to-text generation, incorporating message intent, optional user triggers..."
  - [section III-A] "Encoder EI incorporates the intent information along with an optional trigger input, while encoder ED extracts the representation of input data record..."

## Foundational Learning

- Concept: Encoder-decoder architecture with attention mechanisms
  - Why needed here: TrICy needs to map structured input data (fields and values) to natural language text, requiring sequence-to-sequence learning with alignment between input and output tokens.
  - Quick check question: What is the role of the attention mechanism in the decoder, and how does it differ from the standard encoder-decoder setup?

- Concept: Copy mechanism for OOV handling
  - Why needed here: The input data often contains PII, dates, and other OOV tokens that must be accurately reproduced in the output; standard generation may miss or incorrectly predict these.
  - Quick check question: How does the copy mechanism decide whether to generate a token from the vocabulary or copy it from the input?

- Concept: Intent and trigger conditioning
  - Why needed here: Different intents (e.g., CONTACT::SHARE vs CONTACT::ACT) require different response types (natural language vs structured markup), and triggers allow for further personalization and control.
  - Quick check question: How does the model incorporate intent and trigger information into the hidden state used by the decoder?

## Architecture Onboarding

- Component map:
  - Input data fields/values → Encoder ED → Hidden state − →hED
  - Intent and trigger → Encoder EI → Hidden state − →hEI
  - Concatenated hidden state − →H → Decoder with attention-copy mechanism → Output text

- Critical path:
  1. Tokenize and embed input data fields and values
  2. Encode with BiLSTM in ED, embed intent and trigger in EI
  3. Concatenate hidden states to form − →H
  4. Run decoder with attention and copy mechanism, using − →H to guide generation
  5. Apply beam search decoding for final output

- Design tradeoffs:
  - Using a two-encoder setup increases model capacity and allows intent/trigger conditioning, but also increases parameter count and complexity.
  - The attention-copy mechanism improves OOV handling but requires maintaining a mapping between input and output tokens.
  - Optional triggers add flexibility and control but require careful training ratio tuning (trK) to avoid overfitting.

- Failure signatures:
  - If outputs contain "<SOS>" or other special tokens, the trigger conditioning may be misconfigured or the trigger is inconsistent with the data.
  - If PII or OOV words are missing or incorrect in the output, the copy mechanism may not be aligning properly.
  - If outputs are generic or not intent-specific, the intent/trigger encoding may be too weak.

- First 3 experiments:
  1. Train a basic encoder-decoder (M1) on E2E NLG and evaluate BLEU to establish a baseline.
  2. Add the attention-copy mechanism (M4) and compare BLEU scores to see the impact of OOV handling.
  3. Introduce intent input (M6) and evaluate on WebNLG dataset to measure the effect of intent conditioning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the limitations of TrICy in handling out-of-vocabulary (OOV) words in languages other than English?
- Basis in paper: [inferred] The paper mentions that experiments focus on English datasets and do not evaluate other languages.
- Why unresolved: The paper does not provide any evidence or discussion on the model's performance with non-English languages, particularly regarding OOV words.
- What evidence would resolve it: Experiments and results showing TrICy's performance with datasets in other languages, especially focusing on its ability to handle OOV words.

### Open Question 2
- Question: How does the presence of triggers affect the generation of hallucinatory content in TrICy?
- Basis in paper: [explicit] The paper mentions that hallucination is a crucial problem in natural language generation and quantifying the effect of triggers on this issue is not discussed.
- Why unresolved: The paper does not explore or provide data on how triggers influence the generation of content that is not faithful to the provided source.
- What evidence would resolve it: Analysis and results comparing the generation of hallucinatory content with and without the use of triggers, possibly including human evaluations.

### Open Question 3
- Question: What is the optimal beam width for TrICy's beam search decoding, and how does it affect the model's performance?
- Basis in paper: [explicit] The paper states that beam search decoding is applied with a beam width set to 3 but does not explore the impact of different beam widths.
- Why unresolved: There is no comparison of TrICy's performance with different beam widths, which could affect the quality and diversity of generated text sequences.
- What evidence would resolve it: Comparative results showing TrICy's performance metrics (e.g., BLEU, ROUGE scores) with varying beam widths, highlighting the trade-offs between quality and diversity.

## Limitations

- The evaluation methodology lacks clarity on training conditions, parameter counts, and evaluation protocols for baseline comparisons
- The paper's claim of using "under 60% of the parameters of comparable models" is difficult to verify without detailed architectural specifications
- The attention-copy mechanism's effectiveness depends heavily on input data alignment quality, but lacks error analysis for misaligned or malformed input tokens

## Confidence

**High Confidence:** The core architectural claims regarding the two-encoder setup and attention-copy mechanism are well-supported by the paper's technical descriptions and equations. The BLEU score improvements on E2E NLG and WebNLG datasets are clearly documented with specific numbers and comparison baselines.

**Medium Confidence:** The efficiency claims (parameter reduction and on-device deployment suitability) are supported by comparisons to large language models, but lack detailed ablation studies or runtime benchmarks. The trigger mechanism's effectiveness is demonstrated through BLEU improvements, but the optimal training ratio (trK) appears to be heuristically determined rather than systematically optimized.

**Low Confidence:** The practical applicability of the model in real-world scenarios is not well-established due to limited evaluation on diverse datasets and lack of qualitative user studies. The paper's claims about superior PII handling are based on OOV detection improvements but lack direct validation through privacy-sensitive use cases.

## Next Checks

1. **Ablation Study on Trigger Ratios:** Systematically evaluate TrICy's performance across a broader range of trigger ratios (trK) beyond the 50% heuristic, measuring not just BLEU but also hallucination rates and semantic accuracy to determine optimal training configurations.

2. **Cross-Dataset Generalization:** Test TrICy on additional datasets from different domains (e.g., restaurant recommendations, weather queries, or technical support) to assess its ability to handle diverse intent types and OOV word patterns beyond the current evaluation scope.

3. **Qualitative User Study:** Conduct a user study where participants rate the relevance, accuracy, and naturalness of TrICy-generated responses compared to baseline models, particularly focusing on responses containing PII and other critical OOV content to validate the practical effectiveness of the attention-copy mechanism.