---
ver: rpa2
title: 'Instruction Tuning Vs. In-Context Learning: Revisiting Large Language Models
  in Few-Shot Computational Social Science'
arxiv_id: '2409.14673'
source_url: https://arxiv.org/abs/2409.14673
tags:
- llms
- tasks
- performance
- arxiv
- bragging
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares the performance of large language models (LLMs)
  using instruction tuning (IT) versus in-context learning (ICL) in few-shot computational
  social science (CSS) tasks. The study evaluates six open-source LLMs across five
  publicly available CSS datasets using 1-, 8-, 16-, and 32-shot settings.
---

# Instruction Tuning Vs. In-Context Learning: Revisiting Large Language Models in Few-Shot Computational Social Science

## Quick Facts
- arXiv ID: 2409.14673
- Source URL: https://arxiv.org/abs/2409.14673
- Reference count: 40
- Six open-source LLMs evaluated on five CSS datasets using instruction tuning versus in-context learning in few-shot settings

## Executive Summary
This paper compares instruction tuning (IT) and in-context learning (ICL) for few-shot computational social science (CSS) tasks. The study evaluates six open-source LLMs (7B-9B parameters) across five publicly available CSS datasets using 1-, 8-, 16-, and 32-shot settings. Results consistently show that ICL outperforms IT in most CSS tasks, with accuracy improvements of 3.3-3.7% across the four shot settings. The study also finds that simply increasing sample size without considering quality does not consistently improve performance and can sometimes lead to decline, while ICL proves more effective than zero-shot and chain-of-thought prompting strategies.

## Method Summary
The study compares instruction tuning and in-context learning across six open-source LLMs (Qwen2-7B, Baichuan2-7B, GLM4-9B, Llama3-8B, Gemma2-9B, Phi-3-8B) on five CSS datasets (Bragging, Complaint, Sarcasm, Rumour Stance, GossipCop). For each dataset and shot setting (1, 8, 16, 32), the authors randomly sample n examples per class using five random seeds, create IT and ICL prompts, and evaluate performance using accuracy and macro-F1 scores. The comparison also includes zero-shot and chain-of-thought strategies as baselines.

## Key Results
- ICL consistently outperforms IT in most CSS tasks with 3.3%, 2.9%, 3.2%, and 3.7% accuracy improvements across 1-, 8-, 16-, and 32-shot settings
- Increasing sample size without considering quality does not consistently improve performance and can sometimes lead to decline
- ICL is more effective than zero-shot and chain-of-thought prompting strategies for CSS tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ICL leverages pre-trained knowledge more effectively than IT for CSS tasks.
- Mechanism: ICL uses input-label pairs to guide inference without updating model weights, allowing the model to directly apply learned patterns from pre-training to new tasks.
- Core assumption: The pre-training data contains sufficient semantic understanding relevant to CSS tasks.
- Evidence anchors:
  - [abstract] "ICL offers a rapid alternative for task adaptation by learning from examples without explicit gradient updates."
  - [section 5] "ICL exhibits strong adaptability, likely due to the extensive knowledge acquired during the pre-training phase."

### Mechanism 2
- Claim: Increasing sample size doesn't consistently improve performance due to contextual diversity issues.
- Mechanism: More samples can introduce noise or redundancy if they lack diverse features, preventing the model from learning meaningful patterns.
- Core assumption: Sample diversity matters more than quantity for few-shot learning.
- Evidence anchors:
  - [abstract] "merely increasing the sample size...does not consistently improve the performance of LLMs...and even leads to a decline in some cases."
  - [section 5] "The contextual diversity of samples is more crucial than their quantity...if the additional samples are highly similar in content, LLMs may struggle to learn from the feature diversity."

### Mechanism 3
- Claim: Zero-shot and CoT strategies underperform ICL because they lack task-specific guidance or introduce noise.
- Mechanism: Zero-shot relies solely on pre-trained knowledge without task-specific examples, while CoT may overcomplicate simple classification tasks with unnecessary reasoning steps.
- Core assumption: CSS tasks require specific contextual information for accurate classification.
- Evidence anchors:
  - [abstract] "ICL is more effective than zero-shot and Chain-of-Thought (CoT)."
  - [section 5] "incorporating a small number of input-label pairs into the prompt can help LLMs better focus on task-specific content...CoT slightly underperforms compared to ICL...incorporating CoT descriptions...might introduce noise."

## Foundational Learning

- Concept: Few-shot learning
  - Why needed here: Understanding how models learn from limited examples is central to comparing ICL and IT performance.
  - Quick check question: What distinguishes few-shot learning from traditional supervised learning in terms of data requirements and model adaptation?

- Concept: Prompt engineering
  - Why needed here: Different prompting strategies (zero-shot, ICL, CoT) significantly impact LLM performance on CSS tasks.
  - Quick check question: How does adding input-label pairs to prompts affect model focus compared to zero-shot prompting?

- Concept: Model fine-tuning vs. in-context learning
  - Why needed here: The paper directly compares these two approaches for adapting LLMs to CSS tasks.
  - Quick check question: What are the computational and performance trade-offs between fine-tuning model weights and using in-context learning?

## Architecture Onboarding

- Component map: Input processing -> Prompt construction (zero-shot, ICL, or CoT) -> Model inference (six LLM architectures) -> Output evaluation (accuracy and macro-F1 scores across multiple seeds)
- Critical path: Sample selection → Prompt construction → Model inference → Performance evaluation → Comparison across strategies
- Design tradeoffs:
  - Sample size vs. quality: More samples may introduce noise
  - Prompt complexity vs. performance: CoT may overcomplicate simple tasks
  - Model size vs. computational efficiency: Larger models may perform better but require more resources
- Failure signatures:
  - Performance degradation with increased sample size suggests poor sample diversity
  - Zero-shot underperformance indicates insufficient task-specific guidance
  - CoT underperformance suggests noise introduction or task overcomplication
- First 3 experiments:
  1. Compare zero-shot performance across all six models on one CSS task to establish baseline capabilities
  2. Implement ICL with varying sample qualities (diverse vs. similar samples) on one task to test diversity hypothesis
  3. Test CoT with manually curated vs. GPT-4 generated descriptions on one task to isolate noise effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does increasing the diversity of samples have a more significant impact on LLM performance than simply increasing the quantity of samples in few-shot settings?
- Basis in paper: [inferred] The paper suggests that merely increasing the number of samples without considering their quality does not consistently improve performance, implying that sample diversity may be more important than quantity.
- Why unresolved: The study focuses on the relationship between sample quantity and performance, but does not explicitly investigate the impact of sample diversity on LLM performance in few-shot settings.
- What evidence would resolve it: Experiments comparing LLM performance when increasing sample diversity versus sample quantity in few-shot settings, with controlled variations in sample content and context.

### Open Question 2
- Question: How do different pre-training datasets influence the few-shot performance of LLMs in computational social science tasks?
- Basis in paper: [explicit] The paper notes that the zero-shot strategy primarily depends on the pre-trained knowledge of LLMs, and the absence of task-specific knowledge during pre-training may cause the model to struggle in identifying appropriate solutions.
- Why unresolved: The study does not investigate the relationship between pre-training datasets and few-shot performance in CSS tasks, leaving the impact of pre-training data on task-specific performance unclear.
- What evidence would resolve it: Comparative analysis of LLM performance in few-shot CSS tasks using models pre-trained on different datasets, with varying degrees of task-relevant content.

### Open Question 3
- Question: What are the optimal strategies for selecting and curating high-quality samples for few-shot instruction tuning and in-context learning in CSS tasks?
- Basis in paper: [inferred] The paper emphasizes the importance of optimizing sample quality and prompting strategies to improve LLM classification performance, but does not provide specific guidelines for sample selection and curation.
- Why unresolved: The study highlights the significance of sample quality but does not explore concrete methods for identifying and selecting high-quality samples for few-shot learning in CSS tasks.
- What evidence would resolve it: Development and validation of sample selection algorithms or heuristics that consistently improve LLM performance in few-shot CSS tasks, with measurable improvements in accuracy and generalization.

## Limitations

- Findings based on specific five CSS datasets and six open-source LLMs, limiting generalizability to other domains or proprietary models
- Exact prompt templates for IT and ICL not fully specified, and CoT descriptions were automatically generated without examples provided
- Sample selection relies on random sampling which may not capture optimal sample diversity for all tasks
- Does not explore effects of task complexity on ICL vs IT performance differences

## Confidence

- **High Confidence**: The core finding that ICL outperforms IT in CSS tasks (3.3-3.7% accuracy improvements across shot settings) is supported by direct experimental evidence and consistent across multiple tasks and models.
- **Medium Confidence**: The claim that increasing sample size without considering quality does not consistently improve performance is supported by the data but lacks comprehensive exploration of what constitutes "quality" samples.
- **Low Confidence**: The assertion that CoT underperforms ICL due to noise introduction is based on limited comparisons and doesn't account for potential improvements with manually curated CoT descriptions.

## Next Checks

1. **Sample Diversity Validation**: Conduct controlled experiments comparing ICL performance using samples with high contextual diversity versus highly similar samples on the same CSS tasks to directly test the diversity hypothesis and identify optimal sample selection strategies.

2. **Prompt Template Standardization**: Implement and test multiple standardized prompt templates for both IT and ICL across all six models to determine whether the performance gap persists with more consistent prompt engineering practices.

3. **Cross-Domain Generalization**: Evaluate the same ICL vs IT comparison framework on non-CSS datasets (e.g., general NLP classification tasks) to determine whether the observed advantages of ICL are specific to CSS domains or represent broader LLM capabilities.