---
ver: rpa2
title: 'Insights into LLM Long-Context Failures: When Transformers Know but Don''t
  Tell'
arxiv_id: '2406.14673'
source_url: https://arxiv.org/abs/2406.14673
tags:
- uni00000013
- uni00000046
- uni00000003
- layers
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper uses probing classifiers on hidden states to reveal
  a disconnect between where LLMs encode positional information and where they generate
  accurate responses. Across three models (LLaMa3-8B, Mistral-7B, and Gemma-7B), probing
  classifiers consistently outperform generation accuracy, indicating that models
  "know" target positions but often "don't tell" in their final output.
---

# Insights into LLM Long-Context Failures: When Transformers Know but Don't Tell

## Quick Facts
- arXiv ID: 2406.14673
- Source URL: https://arxiv.org/abs/2406.14673
- Authors: Taiming Lu; Muhan Gao; Kuai Yu; Adam Byerly; Daniel Khashabi
- Reference count: 18
- Primary result: Probing classifiers reveal LLMs encode positional information accurately but fail to utilize it in final outputs, creating a "know but don't tell" phenomenon

## Executive Summary
This paper investigates a critical disconnect in long-context language models where positional information is accurately encoded in hidden states but not effectively utilized in final generations. Using probing classifiers across three 7B-8B parameter models (LLaMa3-8B, Mistral-7B, Gemma-7B), the authors demonstrate that while models "know" target positions through their internal representations, they consistently fail to "tell" this information in their outputs. The phenomenon is particularly pronounced for middle-context information, which requires more layers to be localized and correlates with lower final accuracy. This reveals a fundamental limitation in how positional knowledge is processed and surfaced in transformer-based architectures.

## Method Summary
The authors employ probing classifiers trained on hidden states at each layer to detect positional information encoding. They test three models (LLaMa3-8B, Mistral-7B, Gemma-7B) across kv-pairs retrieval and MDQA tasks, comparing probing accuracy against generation accuracy. The probing classifiers are trained to predict target positions from intermediate representations, while generation accuracy measures actual output performance. By analyzing the layer-wise distribution of positional information and its correlation with final output quality, the study reveals a systematic gap between internal knowledge and external expression.

## Key Results
- Probing classifiers consistently outperform generation accuracy across all tested models, demonstrating accurate internal encoding of positional information
- Middle-context information requires more layers to be localized in hidden states and correlates with lower final generation accuracy
- A negative correlation exists between the layer of peak probing accuracy and generation accuracy, suggesting earlier encoding improves output reliability
- The "know but don't tell" phenomenon reveals fundamental limitations in how transformers process and surface positional knowledge

## Why This Works (Mechanism)
The paper reveals that transformers encode positional information in their hidden states with high accuracy, but this knowledge fails to propagate effectively to final outputs due to positional bias and processing bottlenecks. The mechanism appears rooted in how attention mechanisms distribute information across layers, with middle-context positions requiring more layers to become localized but ultimately failing to influence final token predictions effectively.

## Foundational Learning
- **Positional encoding**: How absolute/relative positions are represented in transformer embeddings - needed to understand how positional bias emerges
- **Hidden state probing**: Using classifiers to extract information from intermediate representations - needed to measure internal knowledge encoding
- **Attention mechanisms**: How transformers distribute information across layers - needed to explain information flow bottlenecks
- **Generation vs encoding gap**: The disconnect between what models know internally versus what they output - needed to frame the core problem
- **Layer-wise information localization**: How information becomes concentrated at specific layers - needed to explain why middle-context requires more layers
- **Transformer depth vs width tradeoffs**: How architectural choices affect information propagation - needed to contextualize model limitations

## Architecture Onboarding

Component Map:
Input tokens -> Embedding layer -> Transformer blocks (multiple layers) -> Output logits -> Generation head

Critical Path:
Token embeddings → Multi-head attention → Feed-forward networks → Layer normalization → Residual connections → Output predictions

Design Tradeoffs:
- Depth vs width: Deeper models may better localize middle-context information but increase computational cost
- Attention mechanisms: Different attention patterns could affect how positional information propagates
- Positional encoding schemes: Absolute vs relative encoding impacts how positions are represented and utilized
- Layer normalization placement: Affects gradient flow and information retention across layers

Failure Signatures:
- Probing accuracy consistently exceeding generation accuracy
- Middle-context positions requiring more layers to localize
- Negative correlation between probing peak layer and generation accuracy
- Systematic accuracy drop for positions in middle contexts

First Experiments:
1. Vary probing classifier complexity to verify the encoding-utilization gap persists across different classifier architectures
2. Test different attention mechanisms (sparse attention, linear attention) to measure impact on the know-but-don't-tell phenomenon
3. Modify positional encoding schemes to determine if alternative representations reduce the encoding-utilization gap

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What specific architectural or training modifications could prevent the "know but don't tell" phenomenon in LLMs?
- Basis in paper: The authors note that while models encode positional information accurately, they fail to utilize this knowledge in final outputs, suggesting architectural improvements could address this disconnect.
- Why unresolved: The paper identifies the phenomenon but doesn't explore specific architectural changes or training modifications that could resolve it.
- What evidence would resolve it: Comparative experiments testing different attention mechanisms, positional encoding schemes, or training objectives to measure their impact on the encoding-utilization gap.

### Open Question 2
- Question: Does the "know but don't tell" phenomenon vary across different model scales or architectures?
- Basis in paper: The authors test three models (LLaMa3-8B, Mistral-7B, and Gemma-7B) but note their findings as consistent across these models without exploring broader architectural variations.
- Why unresolved: The study focuses on similar-sized models within the same general architecture family, leaving open whether larger models or different architectures (e.g., Mamba, RWKV) exhibit different patterns.
- What evidence would resolve it: Systematic testing across a wider range of model sizes and architectures, measuring probing vs generation accuracy gaps.

### Open Question 3
- Question: How does the "know but don't tell" phenomenon affect model performance on tasks beyond retrieval and QA?
- Basis in paper: The authors focus on kv-pairs retrieval and MDQA tasks, but positional bias could manifest differently in tasks like summarization, reasoning, or code generation.
- Why unresolved: The study's scope is limited to specific information retrieval tasks, leaving open how this phenomenon affects broader LLM capabilities.
- What evidence would resolve it: Probing analysis applied to diverse task types, measuring whether the encoding-utilization gap exists in creative or reasoning-intensive tasks.

## Limitations
- The probing classifiers were trained only on binary classification targets and may not generalize to more complex task scenarios
- The study focuses on three specific 7B-8B parameter models, limiting generalizability to larger models or different architectures
- The analysis examines only positional encoding patterns without investigating whether the knowledge is truly semantic or merely positional correlations

## Confidence
- **High confidence**: The core finding that probing classifiers outperform generation accuracy across all three tested models is robustly demonstrated with clear statistical evidence
- **Medium confidence**: The interpretation that middle-context information requiring more layers to localize directly causes lower final accuracy assumes a causal relationship
- **Low confidence**: The claim about a fundamental "positional bias" limiting model performance is speculative, as the paper does not systematically test architectural modifications

## Next Checks
1. Test whether the probing-accuracy gap persists when using more semantically complex target tasks (e.g., multi-choice questions, reasoning tasks) rather than binary position indicators
2. Conduct ablation studies by modifying attention mechanisms or positional encodings in middle layers to determine if the know-but-don't-tell phenomenon can be reduced or eliminated
3. Evaluate whether the pattern holds across different model scales (larger than 8B parameters) and architectures (e.g., hybrid MoE models, non-Transformer architectures) to assess generalizability