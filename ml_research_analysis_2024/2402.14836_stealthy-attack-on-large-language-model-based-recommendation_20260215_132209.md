---
ver: rpa2
title: Stealthy Attack on Large Language Model based Recommendation
arxiv_id: '2402.14836'
source_url: https://arxiv.org/abs/2402.14836
tags:
- text
- attack
- attacks
- recommendation
- item
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that LLM-based recommendation models are
  vulnerable to simple text-based attacks. Attackers can promote target items by subtly
  altering their titles using word insertion, GPT re-writing, or black-box text attack
  methods.
---

# Stealthy Attack on Large Language Model based Recommendation

## Quick Facts
- arXiv ID: 2402.14836
- Source URL: https://arxiv.org/abs/2402.14836
- Authors: Jinghao Zhang; Yuting Liu; Qiang Liu; Shu Wu; Guibing Guo; Liang Wang
- Reference count: 40
- Attack can increase target item exposure by up to 490% through subtle title modifications

## Executive Summary
This paper exposes a significant vulnerability in LLM-based recommendation systems where attackers can promote target items through simple text-based manipulations of item titles. The attack leverages the language understanding capabilities of LLMs by making subtle modifications to item titles, which dramatically increases the visibility of target items while maintaining overall recommendation quality metrics. The study demonstrates that popular items and zero-shot models are particularly susceptible to these attacks.

## Method Summary
The attack methodology involves three main approaches for modifying item titles: word insertion, GPT rewriting, and black-box text attacks. Attackers first identify high-impact positions in item titles, then apply one of these modification strategies to create poisoned samples. The attack is tested against four different victim models (RecLM, PromptRec, ZeroRec, LLaMA-Rec) across two datasets (ML-1M and LastFM-2k). The attack effectiveness is measured by improvements in target item exposure while maintaining recommendation quality metrics like recall@K and NDCG.

## Key Results
- Target item exposure can increase by up to 490% with minimal impact on overall recommendation quality
- Word insertion attacks are most effective, achieving average improvements of 152.2% on ML-1M and 132.2% on LastFM-2k
- Zero-shot models (PromptRec, LLaMA-Rec) show higher attack success rates compared to fine-tuned models
- Simple rewriting-based defense strategies can mitigate some attacks but struggle with word-level perturbations

## Why This Works (Mechanism)
The attack exploits the fundamental design of LLM-based recommendation systems, which rely on natural language understanding to match user queries with item descriptions. By subtly modifying item titles to better align with potential user queries, attackers can manipulate the model's ranking behavior. The LLM's strong language understanding capabilities make it particularly susceptible to these semantic manipulations while remaining blind to the adversarial intent. The stealthiness is maintained because the modifications preserve grammatical correctness and overall semantic coherence, preventing detection by standard quality metrics.

## Foundational Learning
- **LLM-based recommendation**: These systems use language models to understand and match user queries with item descriptions. Needed to understand the attack surface. Quick check: Does the system use natural language processing for recommendations?
- **Attack transferability**: The ability of an attack to work across different models or tasks. Needed to assess real-world impact. Quick check: Does the attack work on models with different architectures?
- **Zero-shot vs fine-tuned models**: Zero-shot models are not trained on recommendation data, while fine-tuned models are. Needed to understand vulnerability differences. Quick check: What type of model is being attacked?
- **Recommendation metrics (Recall@K, NDCG)**: Standard metrics for evaluating recommendation quality. Needed to measure stealthiness. Quick check: Are the metrics being maintained while the attack succeeds?
- **Adversarial example detection**: Techniques to identify manipulated inputs. Needed to evaluate defense strategies. Quick check: Can the system detect unusual patterns in recommendations?

## Architecture Onboarding

Component Map: User Query -> LLM-based Recommender -> Item Ranking -> Recommendations
Critical Path: Attack preparation (item selection) -> Title modification -> Poisoned data injection -> Recommendation generation
Design Tradeoffs: Language understanding capability vs vulnerability to semantic manipulation
Failure Signatures: Sudden increase in specific item exposure while maintaining quality metrics
First Experiments: 1) Test word insertion on high-frequency items, 2) Evaluate GPT rewriting on zero-shot models, 3) Measure transferability across different LLM backbones

## Open Questions the Paper Calls Out
None

## Limitations
- Attack evaluation limited to synthetic datasets rather than real-world production environments
- Only text title attributes are modified, ignoring other potentially important features
- Limited evaluation of attack robustness against model updates or retraining
- No analysis of long-term consequences of repeated attacks on model performance

## Confidence

**High confidence**: The core vulnerability of LLM-based recommenders to text manipulation is well-demonstrated through multiple attack methods and consistent results across different models

**Medium confidence**: Transferability claims are supported but limited by the small sample of model combinations tested

**Medium confidence**: Stealthiness claims are based on specific metrics (recall@K, NDCG) but may not capture all aspects of recommendation quality degradation

## Next Checks
1. Test attack effectiveness on real-world production recommendation systems with live user traffic to validate practical feasibility
2. Evaluate attack persistence and effectiveness after model retraining cycles to assess long-term viability
3. Assess detection potential using adversarial example detection techniques and monitor for unusual patterns in recommendation outputs