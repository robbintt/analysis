---
ver: rpa2
title: An Embarrassingly Simple Approach for LLM with Strong ASR Capacity
arxiv_id: '2402.08846'
source_url: https://arxiv.org/abs/2402.08846
tags:
- speech
- encoder
- llm-based
- large
- proc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SLAM-ASR, an embarrassingly simple approach
  for large language model (LLM) with strong automatic speech recognition (ASR) capacity.
  The method combines off-the-shelf speech encoders and LLMs with a trainable linear
  projector, eliminating the need for complex designs like temporal compression, modal
  alignment, or parameter-efficient fine-tuning.
---

# An Embarrassingly Simple Approach for LLM with Strong ASR Capacity

## Quick Facts
- arXiv ID: 2402.08846
- Source URL: https://arxiv.org/abs/2402.08846
- Authors: Ziyang Ma; Guanrou Yang; Yifan Yang; Zhifu Gao; Jiaming Wang; Zhihao Du; Fan Yu; Qian Chen; Siqi Zheng; Shiliang Zhang; Xie Chen
- Reference count: 5
- Primary result: LLM-based ASR model achieving 1.94% WER on Librispeech test-clean

## Executive Summary
This paper presents SLAM-ASR, a remarkably simple approach for integrating automatic speech recognition (ASR) capabilities into large language models (LLMs). The method connects off-the-shelf speech encoders with LLMs through a trainable linear projector, avoiding complex designs like temporal compression or parameter-efficient fine-tuning. SLAM-ASR achieves state-of-the-art performance among LLM-based ASR models on the Librispeech benchmark, outperforming previous approaches and even surpassing audio-universal models trained on massive paired data.

## Method Summary
SLAM-ASR employs a straightforward architecture that connects pre-trained speech encoders directly to LLMs through a trainable linear projection layer. The system processes speech input through the encoder to obtain representations, which are then projected to match the LLM's input dimensions. This eliminates the need for complex temporal compression, modal alignment techniques, or parameter-efficient fine-tuning strategies. The model is trained end-to-end, allowing the linear projector to learn the optimal mapping between speech and text modalities while the LLM adapts to the ASR task.

## Key Results
- Achieves 1.94% WER on Librispeech test-clean, setting a new state-of-the-art among LLM-based ASR models
- Demonstrates 3.81% WER on Librispeech test-other, outperforming previous LLM-based approaches
- Shows capability emergence during modal alignment training, suggesting effective cross-modal learning
- Surpasses performance of latest audio-universal models trained on massive paired data

## Why This Works (Mechanism)
The effectiveness of SLAM-ASR stems from its elegant simplicity. By leveraging pre-trained speech encoders and LLMs, the model capitalizes on existing strong representations for both modalities. The trainable linear projector serves as a bridge that learns to align these representations without introducing complexity. This direct connection allows the LLM to leverage its strong language understanding capabilities while receiving rich acoustic information from the speech encoder. The approach demonstrates that sophisticated ASR performance can be achieved without complex architectural modifications or specialized training procedures.

## Foundational Learning

**Speech Encoder Fundamentals**
- Why needed: Converts raw audio into meaningful representations
- Quick check: Validate that encoder captures phonetic and acoustic features effectively

**Large Language Model Adaptation**
- Why needed: Provides strong language understanding for transcription
- Quick check: Ensure LLM can process projected speech representations coherently

**Modal Alignment**
- Why needed: Bridges the gap between speech and text representations
- Quick check: Verify that projected representations maintain semantic consistency

**Linear Projection Layer**
- Why needed: Simple mechanism for dimension matching and feature alignment
- Quick check: Confirm that projection layer learns meaningful mappings

## Architecture Onboarding

**Component Map**
Speech Encoder -> Linear Projector -> LLM

**Critical Path**
1. Audio input processed by speech encoder
2. Encoder outputs fed through trainable linear projector
3. Projected representations passed to LLM for transcription
4. End-to-end training optimizes all components jointly

**Design Tradeoffs**
- Simplicity vs. potential performance gains from more complex architectures
- Reliance on pre-trained components vs. training from scratch
- Linear projection vs. more sophisticated alignment mechanisms

**Failure Signatures**
- Poor performance may indicate mismatch between encoder and LLM capabilities
- Training instability could suggest improper scaling of projected representations
- Suboptimal results might reflect insufficient capacity in linear projector

**3 First Experiments**
1. Baseline evaluation on Librispeech with different speech encoder choices
2. Ablation study removing the linear projector to assess its contribution
3. Performance comparison with increasing projector dimensions

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluation restricted to Librispeech dataset, limiting generalizability assessment
- No discussion of performance on longer audio sequences or real-time applications
- Limited exploration of different speech encoder architectures and their impact
- Absence of error analysis or failure mode investigation

## Confidence

**Major Claims Assessment:**
- SLAM-ASR achieving SOTA on Librispeech: High confidence
- Eliminating need for complex designs: Medium confidence
- Capability emergence during modal alignment: Medium confidence

## Next Checks

1. Evaluate SLAM-ASR on more challenging, real-world datasets (e.g., TED-LIUM, Common Voice, or domain-specific noisy environments)

2. Test the approach's performance with different speech encoder architectures and scales to assess robustness

3. Conduct ablation studies on the trainable linear projector to understand its contribution to the overall performance