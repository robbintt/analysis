---
ver: rpa2
title: 'TravelPlanner: A Benchmark for Real-World Planning with Language Agents'
arxiv_id: '2402.01622'
source_url: https://arxiv.org/abs/2402.01622
tags:
- agents
- planning
- language
- plan
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TravelPlanner, a new benchmark for evaluating
  language agents on complex real-world planning tasks. The benchmark provides a sandbox
  environment with nearly four million data records and 1,225 curated planning intents
  with reference plans focused on travel planning scenarios.
---

# TravelPlanner: A Benchmark for Real-World Planning with Language Agents

## Quick Facts
- arXiv ID: 2402.01622
- Source URL: https://arxiv.org/abs/2402.01622
- Authors: Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, Yu Su
- Reference count: 40
- Primary result: GPT-4 achieves only 0.6% success rate on TravelPlanner benchmark

## Executive Summary
This paper introduces TravelPlanner, a new benchmark for evaluating language agents on complex real-world planning tasks. The benchmark provides a sandbox environment with nearly four million data records and 1,225 curated planning intents with reference plans focused on travel planning scenarios. Comprehensive evaluations show that even state-of-the-art language models like GPT-4 struggle significantly on this benchmark, achieving only a 0.6% success rate. The agents have difficulty staying on task, using tools correctly to gather information, and tracking multiple constraints. While the results are negative for current agents, the authors note that the ability to even attempt such complex tasks is progress. TravelPlanner provides a challenging testbed to drive future research on improving language agents' planning capabilities.

## Method Summary
TravelPlanner is a new benchmark designed to evaluate language agents on complex real-world planning tasks, specifically focusing on travel planning scenarios. The benchmark includes a sandbox environment with nearly four million data records and 1,225 curated planning intents with reference plans. It tests agents' abilities to perform multi-step reasoning, use external tools effectively, and maintain context across long interactions. The benchmark provides a challenging testbed that reveals significant limitations in current language agents' planning capabilities, with state-of-the-art models like GPT-4 achieving only a 0.6% success rate.

## Key Results
- GPT-4 achieves only 0.6% success rate on TravelPlanner benchmark
- Language agents struggle with staying on task, using tools correctly, and tracking multiple constraints
- The benchmark reveals significant limitations in current agents' planning capabilities
- Even attempting complex tasks represents progress for current language agents

## Why This Works (Mechanism)
TravelPlanner works by providing a realistic, data-rich environment that requires agents to perform complex reasoning, tool usage, and context management across multiple steps. The benchmark's design forces agents to demonstrate genuine planning capabilities rather than simple pattern matching or single-step reasoning.

## Foundational Learning
1. **Multi-step planning** - Why needed: Travel planning requires coordinating multiple decisions over time; Quick check: Can the agent break down complex goals into sequential steps?
2. **Tool usage** - Why needed: Real-world planning requires accessing external information sources; Quick check: Can the agent effectively query databases and APIs for relevant information?
3. **Constraint tracking** - Why needed: Plans must satisfy multiple simultaneous requirements; Quick check: Does the agent maintain awareness of budget, time, and preference constraints?
4. **Context maintenance** - Why needed: Long planning sessions require tracking progress; Quick check: Can the agent recall previous decisions and their implications?
5. **Task persistence** - Why needed: Planning requires staying focused despite distractions; Quick check: Does the agent return to primary goals after exploring tangents?

## Architecture Onboarding

**Component Map**: User Intent -> Task Decomposition -> Tool Selection -> Information Gathering -> Constraint Checking -> Plan Synthesis -> Output Generation

**Critical Path**: The agent must successfully execute the full pipeline from understanding user intent through to generating a complete, constraint-satisfying plan. Failure at any stage prevents successful completion.

**Design Tradeoffs**: The benchmark prioritizes realistic complexity over controlled testing conditions, accepting that this makes agent evaluation more challenging but more representative of real-world performance.

**Failure Signatures**: Agents commonly fail by getting distracted from the main task, making incorrect tool selections, failing to integrate information from multiple sources, or losing track of constraints over long interactions.

**First Experiments**: 1) Evaluate agent performance on progressively more complex planning scenarios; 2) Test different prompting strategies for improving tool usage; 3) Compare performance across different planning domains to assess generalizability

## Open Questions the Paper Calls Out
None

## Limitations
- Extremely low success rate (0.6%) for GPT-4 indicates current agents are far from human-level planning capabilities
- The benchmark focuses specifically on travel planning scenarios, which may not generalize to other domains
- Results highlight a substantial gap between current capabilities and benchmark requirements

## Confidence
High confidence in:
- TravelPlanner is a challenging test of language agents' planning capabilities
- Current state-of-the-art models like GPT-4 perform poorly on this benchmark
- The benchmark provides a useful testbed for driving future research

Medium confidence in:
- The specific limitations identified (staying on task, tool usage, constraint tracking) are the primary reasons for poor performance
- The benchmark's travel planning focus is representative of real-world planning challenges in general

## Next Checks
1. Conduct ablation studies to determine which components of the benchmark (tool usage, constraint tracking, etc.) contribute most to agent failure rates

2. Test additional state-of-the-art models beyond GPT-4 to establish whether performance is model-specific or reflects fundamental limitations

3. Evaluate whether fine-tuning on TravelPlanner data improves agent performance, indicating whether the benchmark measures inherent capabilities or requires specific training