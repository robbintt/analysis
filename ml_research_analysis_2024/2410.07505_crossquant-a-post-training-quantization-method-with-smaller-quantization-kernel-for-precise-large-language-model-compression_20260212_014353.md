---
ver: rpa2
title: 'CrossQuant: A Post-Training Quantization Method with Smaller Quantization
  Kernel for Precise Large Language Model Compression'
arxiv_id: '2410.07505'
source_url: https://arxiv.org/abs/2410.07505
tags:
- quantization
- crossquant
- kernel
- activations
- per-token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CrossQuant, a post-training quantization method
  for large language models that addresses accuracy degradation caused by quantization
  kernels. The key insight is that elements quantized to zero (the quantization kernel)
  are the primary cause of precision loss.
---

# CrossQuant: A Post-Training Quantization Method with Smaller Quantization Kernel for Precise Large Language Model Compression

## Quick Facts
- arXiv ID: 2410.07505
- Source URL: https://arxiv.org/abs/2410.07505
- Reference count: 21
- Achieves near-FP16 performance on large language models with 6.7B-70B parameters through novel cross-quantization approach

## Executive Summary
CrossQuant introduces a post-training quantization method that addresses accuracy degradation in large language models by focusing on the quantization kernel - elements quantized to zero. The method uses a cross-quantization approach that combines row and column-wise absolute maximum vectors, significantly reducing the kernel size to approximately 16% for OPT models and less than 0.1% for LLaMA models. This approach maintains or improves perplexity and accuracy across various tasks compared to baselines like SmoothQuant and OmniQuant, achieving near-FP16 performance when kernel size is kept below specific thresholds.

## Method Summary
CrossQuant addresses quantization accuracy loss by recognizing that elements quantized to zero (the quantization kernel) are the primary cause of precision degradation. The method employs a cross-quantization strategy that combines row-wise and column-wise absolute maximum vectors to create a more balanced quantization scale. This approach reduces the quantization kernel size significantly while maintaining model performance. The technique is applied post-training and works across various model sizes (6.7B-70B parameters), showing particular effectiveness on OPT and LLaMA architectures. The method requires maintaining kernel sizes below 19% for OPT and 1% for LLaMA to achieve near-FP16 performance.

## Key Results
- Reduces quantization kernel to ~16% for OPT models and <0.1% for LLaMA models
- Maintains or improves perplexity and accuracy across multiple tasks compared to SmoothQuant and OmniQuant
- Achieves near-FP16 performance when kernel size is kept below 19% for OPT and 1% for LLaMA models
- Works effectively across model sizes from 6.7B to 70B parameters

## Why This Works (Mechanism)
CrossQuant works by addressing the fundamental issue that quantization kernels (elements mapped to zero) cause significant accuracy loss in LLMs. By using a cross-quantization approach that combines row and column absolute maximum vectors, the method creates a more balanced quantization scale that reduces the number of elements forced to zero. This dual-directional scaling ensures that important activation values are preserved during quantization, maintaining model performance while enabling aggressive compression.

## Foundational Learning
- **Quantization kernel**: Elements quantized to zero during weight quantization. Critical because these represent lost information that degrades model accuracy.
  - Quick check: Verify kernel size reduction through histogram analysis of quantized weights

- **Post-training quantization (PTQ)**: Quantization method applied after model training without retraining. Needed for practical deployment where full quantization-aware training is computationally prohibitive.
  - Quick check: Compare PTQ vs QAT performance on target hardware

- **Cross-quantization**: Combining multiple quantization scales (row and column-wise) to create a balanced representation. Required to reduce kernel size while preserving important activation values.
  - Quick check: Analyze activation distribution before and after cross-quantization

- **Perplexity**: Measure of how well a language model predicts text, lower is better. Important metric for evaluating language model quality after quantization.
  - Quick check: Compare perplexity on standard benchmarks (WikiText, C4)

- **Zero-shot accuracy**: Model performance on tasks without task-specific fine-tuning. Key evaluation metric for general-purpose quantization methods.
  - Quick check: Test on multiple downstream tasks (GLUE, SuperGLUE)

- **Weight-only quantization**: Quantizing only model weights while keeping activations in higher precision. Balances compression with computational efficiency.
  - Quick check: Measure memory reduction vs. performance impact

## Architecture Onboarding
**Component Map**: Input weights -> Cross-quantization (row + column scales) -> Quantized weights -> Inference engine

**Critical Path**: The quantization process itself is the critical path, as it directly determines model accuracy. The cross-quantization step combining row and column scales is the key innovation that must be implemented correctly.

**Design Tradeoffs**: 
- Aggressive kernel reduction (smaller kernels) vs. accuracy preservation
- Computational overhead of cross-quantization vs. compression benefits
- Hardware compatibility (int8 inference) vs. quantization flexibility

**Failure Signatures**: 
- Large kernel sizes (>19% for OPT, >1% for LLaMA) lead to accuracy degradation
- Imbalanced quantization scales cause activation saturation
- Hardware incompatibility issues with non-standard quantization schemes

**Three First Experiments**:
1. Measure kernel size distribution across different model layers before and after CrossQuant application
2. Compare perplexity on WikiText-103 for quantized vs. FP16 models across varying kernel thresholds
3. Benchmark zero-shot accuracy on GLUE tasks for CrossQuant vs. baseline PTQ methods

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, though several implications arise from the work. The empirical nature of the kernel size thresholds (19% for OPT, 1% for LLaMA) suggests the need for theoretical understanding of why these specific values work. The method's performance on architectures beyond OPT and LLaMA remains unexplored. Additionally, the computational overhead of cross-quantization compared to simpler methods is not quantified, raising questions about practical deployment considerations.

## Limitations
- Empirical kernel size thresholds (19% for OPT, 1% for LLaMA) lack theoretical justification
- Performance on model architectures beyond OPT and LLaMA (6.7B-70B parameters) remains untested
- Computational overhead of cross-quantization not quantified for practical deployment assessment
- Limited baseline comparison - does not include recent PTQ methods like GPTQ or AWQ

## Confidence
- Kernel size as primary accuracy degradation factor: High (supported by consistent experimental evidence across multiple tasks)
- CrossQuant achieving near-FP16 performance: Medium (results show improvement but depend on maintaining specific kernel size thresholds)
- 16% and <0.1% kernel size reductions: High (quantitatively measured across OPT and LLaMA models)

## Next Checks
1. Conduct ablation studies varying kernel size thresholds systematically to establish precise relationships between kernel percentage and accuracy degradation
2. Measure and report the computational overhead (FLOPs and memory) of cross-quantization compared to standard PTQ methods
3. Test CrossQuant on model architectures beyond OPT and LLaMA (e.g., Mistral, Gemma) to evaluate generalizability across different design patterns