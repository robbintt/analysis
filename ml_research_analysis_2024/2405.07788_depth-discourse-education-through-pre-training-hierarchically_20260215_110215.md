---
ver: rpa2
title: 'DEPTH: Discourse Education through Pre-Training Hierarchically'
arxiv_id: '2405.07788'
source_url: https://arxiv.org/abs/2405.07788
tags: []
core_contribution: 'This paper proposes DEPTH, a hierarchical language model that
  learns to represent both sub-word and sentence-level dependencies during pre-training.
  DEPTH builds on the T5 architecture and introduces two discourse-oriented pre-training
  objectives: Sentence Un-Shuffling and Span-Corruption.'
---

# DEPTH: Discourse Education through Pre-Training Hierarchically

## Quick Facts
- **arXiv ID:** 2405.07788
- **Source URL:** https://arxiv.org/abs/2405.07788
- **Reference count:** 40
- **Primary result:** Hierarchical language model that learns sub-word and sentence-level dependencies with improved discourse understanding

## Executive Summary
DEPTH is a hierarchical language model that extends T5 architecture with discourse-oriented pre-training objectives to improve narrative understanding. The model employs Sentence Un-Shuffling and Span-Corruption objectives alongside a hierarchical attention mechanism to capture both local and global discourse relationships. Through unified loss functions, DEPTH achieves lower validation loss during pre-training and demonstrates superior performance on discourse coherence tasks compared to T5. The model shows particular strength on the DiscoEval benchmark while maintaining competitive performance on general NLU tasks like GLUE and Natural Instructions.

## Method Summary
DEPTH builds on the T5 architecture by introducing hierarchical attention mechanisms and two novel pre-training objectives focused on discourse understanding. The model processes text at multiple levels, capturing both sub-word and sentence-level dependencies through specialized attention layers. During pre-training, DEPTH uses Sentence Un-Shuffling (reordering shuffled sentences) and Span-Corruption (masking and predicting text spans) to encourage learning of narrative structures and discourse relationships. A unified loss function combines these objectives, enabling the model to develop representations that encode both local linguistic patterns and global discourse coherence. The hierarchical approach allows DEPTH to maintain standard transformer capabilities while adding discourse-specific understanding.

## Key Results
- DEPTH achieves lower validation loss during pre-training compared to T5 baseline
- Outperforms T5 on DiscoEval benchmark tasks requiring discourse coherence understanding
- Demonstrates competitive performance on GLUE and Natural Instructions benchmarks while improving discourse capabilities

## Why This Works (Mechanism)
DEPTH's effectiveness stems from its hierarchical attention mechanism that processes text at multiple granularities simultaneously. The Sentence Un-Shuffling objective forces the model to understand global narrative structure and coherence relationships between sentences, while Span-Corruption encourages learning of local contextual dependencies. The unified loss function creates a balanced training signal that develops both discourse-level and token-level representations. By extending T5's architecture rather than replacing it, DEPTH preserves strong general language modeling capabilities while adding specialized discourse understanding through targeted pre-training.

## Foundational Learning
- **Hierarchical attention mechanisms** - needed to capture both local and global dependencies; quick check: verify multi-scale feature extraction works across different text lengths
- **Discourse coherence modeling** - needed for understanding narrative structure and relationships; quick check: test sentence ordering recovery accuracy
- **Pre-training objective design** - needed to guide model toward discourse understanding; quick check: validate that shuffling and corruption rates produce meaningful learning signals
- **Unified loss functions** - needed to balance multiple training objectives; quick check: monitor individual loss component trends during training
- **Transformer architecture extension** - needed to maintain general capabilities while adding discourse features; quick check: compare task performance against unmodified T5
- **Multi-task learning optimization** - needed for stable training across diverse objectives; quick check: verify gradient flow and learning rate stability

## Architecture Onboarding

**Component Map:**
Input Text -> Token Embedding -> Hierarchical Attention Layers -> Discourse Objectives (Un-Shuffling, Span-Corruption) -> Unified Loss -> Output Representations

**Critical Path:**
Text input flows through standard T5 embedding layer, then through hierarchical attention modules that process both token-level and sentence-level features before applying discourse-specific pre-training objectives and unified loss computation.

**Design Tradeoffs:**
The hierarchical attention adds computational overhead compared to standard transformers but enables multi-granularity understanding. The unified loss approach balances training signals but requires careful hyperparameter tuning to prevent objective dominance.

**Failure Signatures:**
Poor discourse understanding despite good token-level performance suggests inadequate hierarchical attention configuration. Conversely, strong discourse performance with weak token-level capabilities indicates over-prioritization of sentence-level objectives in the unified loss.

**First 3 Experiments:**
1. Validate hierarchical attention captures different scales by testing sentence ordering recovery on shuffled paragraphs
2. Measure Span-Corruption objective effectiveness by evaluating masked span prediction accuracy
3. Compare unified loss stability against separate training of individual objectives

## Open Questions the Paper Calls Out
The paper does not explicitly identify open questions or areas for future research.

## Limitations
- Limited testing on specialized domains and low-resource languages raises concerns about generalization beyond tested discourse tasks
- Computational overhead of hierarchical attention mechanism wasn't thoroughly benchmarked against efficiency metrics
- Optimal parameterization of pre-training objectives (shuffling granularity, corruption rates) remains unexplored
- Claims about minimal impact on general NLU capabilities lack direct ablation studies comparing task-specific performance drops

## Confidence
- **High confidence** in technical implementation of hierarchical attention and unified loss functions
- **Medium confidence** in DiscoEval benchmark results and their interpretation
- **Low confidence** in claims about minimal impact on general NLU capabilities without direct comparative studies

## Next Checks
1. Conduct ablation studies comparing DEPTH performance against T5 across broader NLU tasks, especially those not requiring discourse understanding
2. Test DEPTH's performance on out-of-domain discourse tasks and low-resource language settings to assess generalization limits
3. Benchmark computational efficiency and memory overhead of hierarchical attention mechanism compared to standard transformer architectures during training and inference