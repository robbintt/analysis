---
ver: rpa2
title: 'TextCAVs: Debugging vision models using text'
arxiv_id: '2408.08652'
source_url: https://arxiv.org/abs/2408.08652
tags:
- concepts
- concept
- textcavs
- text
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TextCAVs introduces a concept-based interpretability method that
  uses vision-language models to create concept activation vectors from text descriptions
  instead of image exemplars. The method trains linear transformations to map between
  features of a target model and a CLIP model, enabling explanations to be generated
  using only text.
---

# TextCAVs: Debugging vision models using text

## Quick Facts
- arXiv ID: 2408.08652
- Source URL: https://arxiv.org/abs/2408.08652
- Reference count: 24
- Primary result: Introduces TextCAVs, a concept-based interpretability method that uses text descriptions instead of image exemplars to create concept activation vectors for debugging vision models

## Executive Summary
TextCAVs introduces a concept-based interpretability method that uses vision-language models to create concept activation vectors from text descriptions instead of image exemplars. The method trains linear transformations to map between features of a target model and a CLIP model, enabling explanations to be generated using only text. When tested on ImageNet, TextCAVs produced meaningful explanations with concepts closely related to their respective classes. On MIMIC-CXR, the method revealed dataset bias in a model trained on a biased subset, where explanations for the biased class focused heavily on support devices rather than the class itself. This demonstrates TextCAVs' potential for interactive debugging and its effectiveness in both natural and medical image domains.

## Method Summary
TextCAVs creates concept activation vectors (CAVs) using text descriptions rather than image exemplars. The method trains two linear transformations: h maps CLIP embeddings to the target model's feature space, and g maps back for cycle consistency. These are trained using reconstruction loss and cycle loss on paired image-text data. TextCAVs are then generated by encoding concept text with CLIP and applying h. Model sensitivity to concepts is quantified using directional derivatives between model gradients and TextCAVs. The approach enables concept-based explanations without requiring concept-specific labeled images.

## Key Results
- TextCAVs produced meaningful explanations on ImageNet with concepts closely related to their respective classes
- On MIMIC-CXR, TextCAVs revealed dataset bias in a model trained on a biased subset, detecting that explanations focused on support devices rather than the atelectasis class itself
- The method successfully identified the source of bias (support devices) through low concept relevance scores for the biased class

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TextCAVs bypass the need for concept-specific labeled images by using a vision-language model to embed text directly into the activation space of the target model.
- Mechanism: Text descriptions are encoded by CLIP (or BiomedCLIP) into a shared embedding space. Linear transformations h and g are trained to map between CLIP's embedding space and the target model's feature space. This allows creation of concept activation vectors from text alone.
- Core assumption: CLIP's embedding space sufficiently captures the semantic meaning of the text so that the resulting vectors align with relevant visual features in the target model.
- Evidence anchors:
  - [abstract] "TextCAVs introduces a concept-based interpretability method that uses vision-language models to create concept activation vectors from text descriptions instead of image exemplars."
  - [section 3] "To create TextCAVs, we only need h but to improve h's ability to convert text features we use a cycle loss term which requires g."
- Break condition: If CLIP's embeddings are too dissimilar from the target model's learned representations, the linear transformation h will not produce meaningful CAVs.

### Mechanism 2
- Claim: The directional derivative between model gradients and TextCAVs quantifies the model's sensitivity to a concept for a given class.
- Mechanism: After extracting the gradient of the logit output with respect to the penultimate layer, the dot product with the TextCAV measures how much the output changes with respect to changes in the concept's activation.
- Core assumption: The penultimate layer's activations are linearly related to the logit output, making the gradient stable and interpretable.
- Evidence anchors:
  - [section 3] "To obtain the model's sensitivity to a concept for a specific class, as in [13], we calculate the directional derivative... If Î¦a is chosen to be the output of the penultimate layer in a model then the directional derivative can be calculated without image exemplars."
  - [section 4.2] "The low CRS for Atelectasis in the biased model means almost none of the top TextCAVs are relevant to the class, demonstrating that they can be used to detect if a model is using biased features."
- Break condition: If the penultimate layer is not representative of the model's decision boundary, the gradient may not reflect true sensitivity to the concept.

### Mechanism 3
- Claim: Cycle consistency loss improves the quality of feature space alignment between CLIP and the target model.
- Mechanism: In addition to reconstruction loss, the cycle loss ensures that features converted from CLIP to target and back again match the originals. This enforces bidirectional consistency.
- Core assumption: A more consistent mapping between the two spaces results in more accurate CAVs.
- Evidence anchors:
  - [section 3] "To include information from the text features in the loss function we use cycle loss which ensures that the features are consistent with their original form when converted back to their original space."
- Break condition: If the feature distributions are too different or non-linear, even cycle loss cannot produce accurate mappings.

## Foundational Learning

- Concept: Linear transformations for feature space alignment
  - Why needed here: TextCAVs need to bridge two different model architectures (CLIP and the target model). A linear layer is a minimal, trainable mapping that preserves interpretability.
  - Quick check question: If you train h to map CLIP features to target features, what should you measure to verify the mapping is effective?
- Concept: Directional derivatives as sensitivity metrics
  - Why needed here: Sensitivity to concepts is quantified by how aligned the model's gradients are with the concept's activation vector. This is analogous to TCAV but avoids needing example images.
  - Quick check question: If the dot product between the gradient and CAV is near zero, what does that imply about the model's reliance on that concept?
- Concept: Cycle consistency in multimodal learning
  - Why needed here: Ensures that the learned transformation is not just fitting noise but preserves the structure of both modalities.
  - Quick check question: What would happen to the quality of TextCAVs if you removed the cycle loss term from training?

## Architecture Onboarding

- Component map:
  CLIP (or BiomedCLIP) -> h (linear Rn -> Rm) -> Target model (e.g., ResNet-50) -> g (linear Rm -> Rn)
- Critical path:
  1. Train h and g using reconstruction and cycle loss on paired image and text features.
  2. Generate TextCAVs by encoding concept text with CLIP and applying h.
  3. Compute directional derivatives to rank concepts by sensitivity.
- Design tradeoffs:
  - Use of penultimate layer limits sensitivity analysis to linear regions; deeper layers may be more abstract but harder to differentiate.
  - Using BiomedCLIP improves biomedical relevance but may require more compute or domain-specific training.
  - LLM-generated concepts are scalable but may introduce irrelevant or noisy concepts.
- Failure signatures:
  - Low concept relevance scores (CRS) indicate poor alignment or concept irrelevance.
  - Gradient norms close to zero suggest the model is not sensitive to the concept.
  - Highly similar CAVs across different concepts suggest embedding collapse or poor h training.
- First 3 experiments:
  1. Train h and g on MIMIC-CXR image and text report pairs; verify feature alignment with cosine similarity.
  2. Generate TextCAVs for a few classes (e.g., Cardiomegaly) and manually check if top concepts are semantically related.
  3. Introduce known bias into the dataset and confirm that TextCAVs detect it via low CRS for the biased class.

## Open Questions the Paper Calls Out
None

## Limitations
- The effectiveness of TextCAVs depends heavily on the quality and alignment of the vision-language model embeddings with the target model's feature space
- The method assumes that the penultimate layer activations are linearly related to the logit output, which may not hold for all architectures
- The quality of TextCAVs is sensitive to the choice of text descriptions used to encode concepts

## Confidence
- **High confidence**: The core mechanism of using text to generate concept activation vectors via linear transformations between CLIP and target model feature spaces is well-supported by the paper's methodology and experimental results
- **Medium confidence**: The claim that directional derivatives effectively quantify model sensitivity to concepts is supported but relies on assumptions about linear relationships in the penultimate layer that may not generalize
- **Medium confidence**: The effectiveness of cycle consistency loss in improving feature space alignment is demonstrated but not extensively validated across different model architectures or datasets

## Next Checks
1. Test TextCAVs across multiple target model architectures (e.g., Vision Transformers, EfficientNet) to assess generalizability of the linear transformation approach
2. Conduct ablation studies to quantify the impact of cycle consistency loss on CAV quality by comparing results with and without this component
3. Evaluate TextCAVs on datasets with known, controlled biases to systematically measure their ability to detect and explain biased model behavior