---
ver: rpa2
title: More Agents Is All You Need
arxiv_id: '2402.05120'
source_url: https://arxiv.org/abs/2402.05120
tags:
- performance
- ensemble
- size
- accuracy
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that large language model performance scales
  with the number of agents through a simple sampling-and-voting method, called Agent
  Forest. The method is orthogonal to existing techniques and enhances performance
  especially for difficult tasks and weaker models.
---

# More Agents Is All You Need

## Quick Facts
- arXiv ID: 2402.05120
- Source URL: https://arxiv.org/abs/2402.05120
- Reference count: 40
- Primary result: Large language model performance scales with the number of agents through a simple sampling-and-voting method, with gains of 4% to 24% across various tasks.

## Executive Summary
This paper introduces Agent Forest, a sampling-and-voting method that demonstrates LLM performance scales with the number of agents. The approach is orthogonal to existing techniques and shows particular effectiveness for difficult tasks and weaker models. The method consistently improves performance across arithmetic reasoning, general reasoning, and code generation tasks.

## Method Summary
The Agent Forest method employs a simple sampling-and-voting mechanism where multiple agents generate responses to the same prompt, and a voting system determines the final output. This approach is designed to be complementary to existing prompt engineering and multi-agent collaboration techniques. The method includes proposed optimization strategies based on task difficulty analysis to further enhance effectiveness.

## Key Results
- Accuracy gains of 4% to 24% across different task categories
- Smaller models can outperform larger ones when ensemble size is increased
- Method integrates well with existing prompt engineering approaches
- Effectiveness particularly pronounced for difficult tasks and weaker models

## Why This Works (Mechanism)
The method leverages the diversity of responses generated by multiple agents to improve overall accuracy. By combining multiple independent attempts at problem-solving through voting, the approach effectively reduces the impact of individual errors and biases. The sampling-and-voting mechanism creates a form of ensemble learning that benefits from the "wisdom of crowds" principle, where collective decision-making often outperforms individual agents.

## Foundational Learning

- **Ensemble Methods**: Multiple independent predictions combined for better accuracy
  - Why needed: Single model outputs can be unreliable due to inherent uncertainty
  - Quick check: Verify ensemble size correlates with performance gains

- **Task Difficulty Analysis**: Understanding which tasks benefit most from ensemble approaches
  - Why needed: Not all tasks require the same number of agents for optimal performance
  - Quick check: Map task difficulty to optimal ensemble size

- **Voting Mechanisms**: Methods for aggregating multiple responses into a single output
  - Why needed: Simple majority voting may not always yield optimal results
  - Quick check: Test different voting strategies (majority, weighted, confidence-based)

## Architecture Onboarding

**Component Map**: Task -> Multiple Agents -> Voting System -> Final Output

**Critical Path**: Input task → Agent sampling → Response generation → Voting aggregation → Output

**Design Tradeoffs**: 
- More agents → better accuracy but higher computational cost
- Voting complexity → potentially better results but increased implementation complexity
- Task-specific optimization → better performance but reduced generalizability

**Failure Signatures**: 
- Diminishing returns with excessive agents
- Voting deadlocks in ambiguous cases
- Computational overhead exceeding accuracy gains

**First 3 Experiments**:
1. Test baseline performance with 1, 3, 5, and 10 agents on arithmetic reasoning tasks
2. Compare different voting mechanisms (majority, weighted, confidence-based) on the same tasks
3. Analyze computational overhead vs accuracy gains across different model sizes

## Open Questions the Paper Calls Out
None

## Limitations
- Results primarily validated on arithmetic, general reasoning, and code generation tasks
- Computational overhead not deeply analyzed
- Limited ablation studies across diverse model families

## Confidence
- High confidence in core finding: ensemble size positively correlates with performance gains
- Medium confidence in computational efficiency claims
- Medium confidence in cross-model family generalizability

## Next Checks
1. Test the sampling-and-voting method on open-ended, multi-domain tasks like creative writing or strategic planning to assess robustness beyond structured problem-solving
2. Conduct comprehensive resource utilization analysis comparing ensemble performance gains against computational costs across different hardware configurations
3. Perform cross-model family validation to determine whether the ensemble benefits generalize from LLaMA/Mistral to other architectures like GPT, Claude, or domain-specific models