---
ver: rpa2
title: Theoretical Foundations of Deep Selective State-Space Models
arxiv_id: '2402.19047'
source_url: https://arxiv.org/abs/2402.19047
tags:
- linear
- such
- which
- selective
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper establishes theoretical foundations for modern selective
  state-space models (SSMs) like Mamba using Rough Path Theory. It demonstrates that
  input-controlled linear recurrences can provably approximate any continuous function
  from sequential input to target output by projecting onto a low-dimensional space
  containing sufficient input statistics.
---

# Theoretical Foundations of Deep Selective State-Space Models

## Quick Facts
- arXiv ID: 2402.19047
- Source URL: https://arxiv.org/abs/2402.19047
- Authors: Nicola Muca Cirone; Antonio Orvieto; Benjamin Walker; Cristopher Salvi; Terry Lyons
- Reference count: 40
- Primary result: Establishes theoretical foundations for selective state-space models using Rough Path Theory

## Executive Summary
This paper provides theoretical foundations for modern selective state-space models (SSMs) like Mamba by leveraging Rough Path Theory. The authors demonstrate that input-controlled linear recurrences can approximate any continuous function from sequential input to target output by projecting onto low-dimensional spaces containing sufficient input statistics. The work bridges the gap between practical success and theoretical understanding of SSMs, showing how these architectures can achieve universal approximation capabilities.

## Method Summary
The paper employs Rough Path Theory and controlled ODEs to analyze input-controlled state-space models. The theoretical framework examines how these models can approximate continuous functions through low-dimensional projections while maintaining computational efficiency. The analysis compares dense versus diagonal state-space matrices and investigates how chaining multiple SSM layers can restore expressivity in restricted architectures.

## Key Results
- Dense input-controlled SSMs are fully expressive and can approximate any continuous function
- Diagonal SSMs (like Mamba) have restricted expressivity but can regain capabilities through layer chaining
- Input-controlled SSMs can provably approximate functions by projecting onto low-dimensional spaces containing sufficient input statistics

## Why This Works (Mechanism)
The theoretical foundation relies on representing sequential data as rough paths and analyzing how state-space models can control differential equations to capture temporal dependencies. By projecting high-dimensional sequential data onto lower-dimensional manifolds that preserve essential statistics, SSMs achieve computational efficiency without sacrificing approximation power. The input-controlled mechanism allows the model to adapt its state evolution based on input content rather than fixed parameters.

## Foundational Learning
- **Rough Path Theory**: Mathematical framework for analyzing irregular time series and controlled differential equations
  - Why needed: Provides rigorous tools for analyzing sequential data approximation
  - Quick check: Can represent discrete sequences as continuous paths
- **State-Space Models**: Framework for modeling dynamical systems through state evolution equations
  - Why needed: Core architecture underlying SSMs like Mamba
  - Quick check: Can be expressed as linear recurrence relations
- **Universal Approximation**: Theoretical property of neural networks to approximate any continuous function
  - Why needed: Establishes expressivity guarantees for SSM architectures
  - Quick check: Requires sufficient capacity and appropriate activation functions
- **Controlled Differential Equations**: Equations where the vector field depends on input signals
  - Why needed: Models how SSMs adapt state evolution to input content
  - Quick check: Can be solved using numerical integration methods

## Architecture Onboarding

**Component Map:** Input -> State Evolution (controlled by SSM parameters) -> Output Projection -> Loss Function

**Critical Path:** The theoretical analysis focuses on the state evolution mechanism, showing how input-controlled dynamics enable function approximation through low-dimensional projections.

**Design Tradeoffs:** Dense SSMs offer full expressivity but higher computational cost versus diagonal SSMs (Mamba) which are more efficient but require careful architectural design (e.g., layer chaining) to achieve comparable performance.

**Failure Signatures:** Restricted architectures (diagonal SSMs) may fail to capture complex temporal dependencies without sufficient depth or appropriate architectural modifications.

**First Experiments:**
1. Verify theoretical approximation rates by testing SSMs on synthetic function approximation tasks with known properties
2. Compare dense versus diagonal SSM performance across varying sequence lengths and input complexities
3. Experiment with different chaining configurations to empirically validate theoretical expressivity recovery

## Open Questions the Paper Calls Out
The paper acknowledges that while it establishes theoretical foundations, practical questions remain regarding optimal architectural configurations, the relationship between theoretical approximation rates and empirical performance, and how to best leverage these theoretical insights for designing more efficient SSM variants.

## Limitations
- Analysis relies on mathematical frameworks (Rough Path Theory) that may not fully capture practical implementation details
- Expressiveness results assume chaining multiple layers, but optimal configurations for real applications remain unclear
- The paper does not address computational efficiency or memory usage in practical implementations

## Confidence
- Expressiveness of dense input-controlled SSMs: High
- Restricted expressivity of diagonal SSMs: Medium
- Regain of expressivity through chaining: Medium
- Practical implications for Mamba and similar architectures: Low

## Next Checks
1. Implement and empirically validate the theoretical approximation rates for both dense and diagonal SSMs on standard sequence modeling benchmarks
2. Systematically study the relationship between the number of chained SSM layers and model performance across different task complexities
3. Compare the computational efficiency and memory usage of theoretically optimal configurations versus practical implementations