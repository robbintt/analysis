---
ver: rpa2
title: Investigating Mysteries of CoT-Augmented Distillation
arxiv_id: '2406.14511'
source_url: https://arxiv.org/abs/2406.14511
tags:
- label
- rationales
- tokens
- distillation
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates why chain-of-thought (CoT) rationales improve
  model distillation. It shows that appending rationales after labels yields better
  performance than prepending them.
---

# Investigating Mysteries of CoT-Augmented Distillation

## Quick Facts
- arXiv ID: 2406.14511
- Source URL: https://arxiv.org/abs/2406.14511
- Authors: Somin Wadhwa; Silvio Amir; Byron C. Wallace
- Reference count: 12
- Key outcome: Appending rationales after labels yields better distillation performance than prepending, with no need for coherent reasoning at inference.

## Executive Summary
This paper investigates why chain-of-thought (CoT) rationales improve model distillation performance. Through systematic experiments across multiple models and datasets, the authors demonstrate that the placement of rationales relative to labels significantly affects downstream performance. Surprisingly, they find that rationales appended after labels outperform those prepended before labels, and that the rationales need not be coherent reasoning sequences or even complete to achieve these improvements. The work challenges assumptions about the necessity of explicit reasoning at inference time and provides practical insights for more effective model distillation approaches.

## Method Summary
The paper uses supervised fine-tuning with CoT-augmented targets, comparing three configurations: baseline (no CoT), CoT before label, and CoT after label. Student models (GPT-2, Phi-1.5, Gemma-2B) are fine-tuned on three datasets (CommonsenseQA, OpenBookQA, QuaRel) using rationales elicited from a Mistral-7B-Instruct teacher model. The study employs gradient attribution methods to identify key tokens in rationales and tests robustness to token shuffling and masking. Performance is evaluated using accuracy on test sets, with ablations examining the effects of rationale placement, coherence, and token importance.

## Key Results
- Appending rationales after labels consistently outperforms prepending them across all tested models and datasets
- Rationales need not be coherent reasoning sequences when appended after labels
- A small subset of key tokens from rationales suffices for comparable performance gains
- Token shuffling has minimal effect when rationales follow labels but causes sharp performance decline when prepended

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Appending rationales after labels provides richer training signal than prepending
- Core assumption: Token order in training targets affects how models learn input-output mappings
- Evidence: Consistently better performance when rationales follow labels across models/datasets
- Break condition: If architecture requires explicit reasoning at inference

### Mechanism 2
- Claim: Rationales don't need to be coherent reasoning sequences to improve distillation
- Core assumption: Specific tokens carry semantic weight regardless of position or context
- Evidence: Performance robust to token permutations when rationales follow labels
- Break condition: If task requires strict logical consistency

### Mechanism 3
- Claim: A small subset of key tokens suffices for comparable gains
- Core assumption: Token importance can be effectively identified through gradient attribution
- Evidence: No significant performance difference when reducing rationale tokens via attribution
- Break condition: If attribution method fails to identify truly important tokens

## Foundational Learning

- Concept: Supervised fine-tuning (SFT) with augmented targets
  - Why needed here: Builds on standard distillation by adding CoT rationales as training signal
  - Quick check question: What's the difference between standard distillation and CoT-augmented distillation?

- Concept: Chain-of-thought (CoT) prompting and rationale generation
  - Why needed here: Understanding LLM reasoning generation is crucial for interpreting findings
  - Quick check question: How does standard CoT prompting differ from CoT-augmented distillation?

- Concept: Gradient attribution methods (e.g., integrated gradients)
  - Why needed here: Used to identify important tokens in rationales
  - Quick check question: What's the core idea behind integrated gradients for token attribution?

## Architecture Onboarding

- Component map: Teacher model (Mistral-7B-Instruct) -> Elicit rationales -> Student models (GPT-2, Phi-1.5, Gemma-2B) with different CoT configurations -> Evaluate performance

- Critical path: 1) Elicit rationales from teacher model, 2) Create augmented training targets with different CoT configurations, 3) Fine-tune student models, 4) Evaluate on test sets, 5) Analyze through ablation studies

- Design tradeoffs: Pre-pending vs post-pending rationales affects whether student must learn to generate reasoning; Full vs partial rationales impacts training efficiency vs performance; Automated vs human token selection balances comprehensiveness vs relevance

- Failure signatures: No performance improvement over baseline suggests rationales aren't providing useful signal; Sharp decline with shuffled rationales (pre-pended) indicates model relies on logical flow; Performance matches baseline at high masking rates suggests key tokens are being removed

- First 3 experiments: 1) Compare student performance with rationales pre-pended vs post-pended to labels, 2) Test robustness to token shuffling in rationales under different configurations, 3) Evaluate performance using incrementally masked rationales to find minimum effective length

## Open Questions the Paper Calls Out

1. What is the exact mechanism by which CoT rationales appended after labels improve student model performance compared to when they precede labels?

2. Are the benefits of CoT-augmented distillation consistent across different types of reasoning tasks beyond commonsense reasoning?

3. How does the quality of CoT rationales generated by teacher models affect the performance of student models in CoT-augmented distillation?

## Limitations

- The exact mechanism by which CoT-augmentation aids distillation remains partially speculative
- Experiments focus on classification tasks in science and commonsense domains, limiting generalizability
- Gradient attribution methods may not capture all semantically important tokens in distributed representations

## Confidence

- High Confidence: Empirical finding that appending rationales after labels outperforms prepending
- Medium Confidence: Interpretation that this demonstrates "no student reasoning is necessary at test time"
- Medium Confidence: Claim that a small number of key tokens suffices for comparable gains
- Low Confidence: Broader theoretical implications about how models process information with post-label rationales

## Next Checks

1. Test the post-label rationale approach on tasks requiring strict logical coherence (e.g., mathematical proof generation, code synthesis) to determine if the mechanism breaks down when reasoning order matters.

2. Compare gradient-based token attribution with alternative methods (e.g., attention-based, occlusion) to verify that identified key tokens are consistent across attribution approaches and truly capture the most informative content.

3. Evaluate whether models trained with post-label rationales maintain their performance advantage over time and with continued fine-tuning, to distinguish between genuine capability improvements and training artifacts.