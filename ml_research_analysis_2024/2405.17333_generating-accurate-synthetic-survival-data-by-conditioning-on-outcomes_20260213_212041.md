---
ver: rpa2
title: Generating Accurate Synthetic Survival Data by Conditioning on Outcomes
arxiv_id: '2405.17333'
source_url: https://arxiv.org/abs/2405.17333
tags:
- data
- synthetic
- survival
- generation
- event
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating high-quality synthetic
  survival data, which is critical for privacy-preserving research and data augmentation
  in healthcare. Existing methods struggle to accurately reproduce the distributions
  of both observed and censored event times.
---

# Generating Accurate Synthetic Survival Data by Conditioning on Outcomes

## Quick Facts
- **arXiv ID:** 2405.17333
- **Source URL:** https://arxiv.org/abs/2405.17333
- **Reference count:** 22
- **Primary result:** Outperforms baselines in covariate quality (JS 0.006, WS 0.063), event-time preservation (KM 0.003), and downstream survival performance (C-index 0.785, Brier 0.060)

## Executive Summary
This paper addresses the challenge of generating high-quality synthetic survival data by conditioning covariate generation on event times and censoring indicators. The method separates the generation process into two steps: first sampling event times and censoring indicators from one-dimensional distributions (using Dirichlet Process Mixture Models), then generating covariates conditioned on these values. This approach ensures accurate reproduction of event-time distributions by construction while allowing the use of any existing conditional tabular data generator for covariates. Experiments on five real-world medical datasets demonstrate superior performance across multiple metrics compared to existing baselines.

## Method Summary
The proposed method generates synthetic survival data by conditioning on outcomes through a two-step process. First, event times and censoring indicators are sampled from one-dimensional distributions (approximated with Dirichlet Process Mixture Models or empirical distributions). Then, covariates are generated conditioned on these sampled event times and censoring indicators using existing conditional tabular data generators like TVAE, CTGAN, ADS-GAN, TabDDPM, or LLMs. This reverse conditioning approach ensures accurate preservation of event-time distributions while leveraging the capabilities of pre-trained conditional generators for complex covariate distributions.

## Key Results
- Outperforms baselines in covariate quality with JS distance of 0.006 and WS distance of 0.063
- Preserves event-time distributions with KM divergence of 0.003
- Achieves strong downstream survival model performance (C-index 0.785, Brier score 0.060)
- Shows robustness to limited training data with widening performance gaps as data becomes more scarce

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conditioning covariates on event times and censoring indicators ensures accurate reproduction of event-time distributions by construction.
- Mechanism: By sampling event times and censoring indicators from their marginal distributions before generating covariates, the method decouples the quality of event-time generation from the quality of covariate generation, eliminating compounding errors.
- Core assumption: The observed and censoring times are conditionally independent given the covariates (standard assumption of censoring at random in survival analysis).
- Evidence anchors:
  - [abstract]: "Experiments on five real-world medical datasets demonstrate that the method consistently outperforms baselines..."
  - [section]: "Importantly, using(2): i) eliminates the need for a supervised model to generate event times... ii) eliminates the need for a separate model to generate survival distributions... iii) guarantees the quality of the observed and censored event distributions."
- Break condition: If the assumption of conditional independence between observed and censoring times given covariates is violated, the event-time distributions will not be preserved accurately.

### Mechanism 2
- Claim: Using any existing conditional tabular data generator for covariates simplifies the generation pipeline while maintaining performance.
- Mechanism: The method leverages pre-trained or easily trainable conditional generators (TVAE, CTGAN, ADS-GAN, TabDDPM, LLMs) that are already designed to handle complex covariate distributions, avoiding the need for specialized survival-specific architectures.
- Core assumption: Existing conditional tabular generators can effectively learn the conditional distribution p(x|t,e) without requiring survival-specific modifications.
- Evidence anchors:
  - [section]: "Importantly, using(2): i) eliminates the need for a supervised model to generate event times... ii) eliminates the need for a separate model to generate survival distributions..."
  - [section]: "In the experiments, we consider TVAE, CTGAN, ADS-GAN, TabDDPM and LLMs."
- Break condition: If existing conditional generators cannot capture the complex dependencies between covariates and survival outcomes, the generated data quality will degrade.

### Mechanism 3
- Claim: Reverse conditioning (sampling t,e first, then x) provides better robustness when training data is limited compared to unconditional generation.
- Mechanism: By separating the generation of event times from covariates, the method utilizes information available in small datasets more efficiently. The event-time distributions are preserved exactly regardless of covariate generation quality, preventing compounding errors when data is scarce.
- Core assumption: The marginal distributions of event times can be learned accurately even with limited data, providing a stable foundation for covariate generation.
- Evidence anchors:
  - [section]: "Table 4, our reverse conditioning approach consistently outperforms both unconditional models and SurvivalGAN in all metrics, the performance gap widening as training data become more limited."
- Break condition: If the marginal event-time distributions cannot be learned accurately from limited data, the method's advantage over unconditional approaches will diminish.

## Foundational Learning

- Concept: Conditional independence in survival analysis
  - Why needed here: The method relies on the assumption that observed and censoring times are conditionally independent given covariates to ensure event-time distributions are preserved.
  - Quick check question: If we observe that censoring patterns are strongly correlated with event times even after conditioning on covariates, what fundamental assumption of this method is violated?

- Concept: Dirichlet Process Mixture Models for one-dimensional distributions
  - Why needed here: DPMM is used to model the marginal distributions of event times separately for observed and censored cases, providing a flexible non-parametric approach.
  - Quick check question: What advantage does using a DPMM over a simple kernel density estimator provide when modeling event-time distributions?

- Concept: Time embedding mechanisms for conditioning
  - Why needed here: Methods like CTGAN and TabDDPM require conditioning on event times, which are continuous, necessitating appropriate embedding techniques like sinusoidal embeddings.
  - Quick check question: Why can't we directly use the raw event time values as conditioning inputs in models designed for discrete conditioning variables?

## Architecture Onboarding

- Component map: DPMM (event times, censoring) -> Conditional generator (TVAE/CTGAN/ADS-GAN/TabDDPM/LLMs) -> Synthetic data (x,t,e)
- Critical path: The critical path is: sample (t,e) from DPMM → embed t and e → condition generator on embedded values → generate covariates x → output complete triplet (x,t,e). The bottleneck is typically the conditional generator training time.
- Design tradeoffs: The method trades off exact event-time distribution preservation for potentially less accurate covariate generation compared to joint modeling approaches. It also requires separate models for event times and covariates, increasing complexity slightly.
- Failure signatures: Poor downstream survival model performance despite good covariate quality metrics indicates the conditional independence assumption may be violated. Inability to reproduce event-time distributions despite good covariate generation suggests issues with the one-dimensional generators.
- First 3 experiments:
  1. Implement the basic two-step generation process using a simple conditional generator (TVAE) and DPMM for event times on a small dataset, then compare event-time distributions to the original data.
  2. Evaluate downstream survival model performance (C-index, Brier score) when training on synthetic data generated by this method versus training on original data.
  3. Test the robustness to limited training data by progressively reducing the training set size and measuring performance degradation compared to unconditional baselines.

## Open Questions the Paper Calls Out
The paper acknowledges that while it only considers right censoring as it is the predominant form in real-world datasets, the proposed method can be extended to left or interval censoring, though this extension is not validated experimentally.

## Limitations
- The method relies on the assumption of conditional independence between event and censoring times given covariates, which may be violated in real-world data
- Performance is bounded by the capabilities of existing conditional tabular generators used for covariates
- The computational complexity when scaling to high-dimensional survival datasets with hundreds of features remains unexplored

## Confidence
- **High confidence**: The mechanism for preserving event-time distributions by construction follows directly from the sampling methodology
- **Medium confidence**: Overall performance claims are supported by experiments on five real-world datasets but could benefit from more extensive baseline comparisons
- **Medium confidence**: Robustness claims for limited data scenarios show improvement but lack theoretical guarantees

## Next Checks
1. Conduct sensitivity analysis by systematically varying the degree of violation of conditional independence assumption and measuring the impact on event-time distribution preservation
2. Test the method on datasets known to have dependent censoring patterns to assess real-world robustness
3. Compare the method against additional baselines that use joint modeling approaches to better establish the tradeoff between accuracy and simplicity