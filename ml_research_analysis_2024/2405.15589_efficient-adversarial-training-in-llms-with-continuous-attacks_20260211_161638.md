---
ver: rpa2
title: Efficient Adversarial Training in LLMs with Continuous Attacks
arxiv_id: '2405.15589'
source_url: https://arxiv.org/abs/2405.15589
tags:
- uni00000013
- adversarial
- attacks
- training
- uni00000011
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the high computational cost of adversarial
  training for large language models (LLMs) by proposing to perform adversarial attacks
  in the continuous embedding space rather than on discrete tokens. The authors introduce
  two algorithms: CAT (Continuous-Adversarial UL), which combines training on adversarial
  behavior datasets with fine-tuning on utility data, and CAPO (Continuous-Adversarial
  IPO), an adversarial variant of IPO that doesn''t require utility data.'
---

# Efficient Adversarial Training in LLMs with Continuous Attacks

## Quick Facts
- **arXiv ID**: 2405.15589
- **Source URL**: https://arxiv.org/abs/2405.15589
- **Reference count**: 40
- **Primary result**: Continuous embedding perturbations reduce adversarial attack success by up to 100% while requiring 299× less compute than discrete methods

## Executive Summary
This paper addresses the computational bottleneck of adversarial training for large language models by proposing to perform adversarial attacks in continuous embedding space rather than on discrete tokens. The authors introduce two novel algorithms: CAT (Continuous-Adversarial UL), which combines adversarial behavior datasets with utility fine-tuning, and CAPO (Continuous-Adversarial IPO), an adversarial variant of IPO that works without utility data. Both methods use continuous ℓ2-norm perturbations in the embedding space to improve robustness against harmful outputs while maintaining model helpfulness.

Experiments across five models (Gemma, Phi3, Mistral, Zephyr, Llama2) demonstrate substantial improvements in robustness against discrete attacks (GCG, AutoDAN, PAIR) with up to 100% reduction in attack success rates. Critically, these robustness gains are achieved with at least 299 times less computational cost compared to traditional discrete adversarial training methods. The results show that robustness to continuous perturbations effectively extrapolates to discrete threat models, presenting a scalable path for adversarial training of LLMs.

## Method Summary
The paper introduces continuous adversarial training by performing attacks directly in the embedding space using ℓ2-norm perturbations rather than on discrete tokens. Two algorithms are proposed: CAT (Continuous-Adversarial UL) which combines adversarial behavior datasets with utility data fine-tuning, and CAPO (Continuous-Adversarial IPO) which is an adversarial variant of IPO that doesn't require utility data. The continuous attack approach significantly reduces computational costs while maintaining effectiveness against discrete adversarial attacks, with experiments showing up to 100% reduction in attack success rates across multiple models.

## Key Results
- Continuous embedding perturbations reduce adversarial attack success by up to 100%
- Computational efficiency improved by at least 299× compared to discrete adversarial training
- Robustness to continuous perturbations effectively extrapolates to discrete threat models
- Both CAT and CAPO algorithms demonstrate substantial improvements across five tested models (Gemma, Phi3, Mistral, Zephyr, Llama2)

## Why This Works (Mechanism)
By operating in continuous embedding space rather than discrete tokens, the approach leverages the differentiable nature of embeddings to perform efficient gradient-based optimization for finding adversarial examples. This eliminates the need for computationally expensive discrete search algorithms required for token-level attacks. The ℓ2-norm perturbations in embedding space create smooth, continuous adversarial directions that can be efficiently optimized during training, while still providing robustness that transfers to discrete token-level attacks.

## Foundational Learning
**Adversarial training**: Why needed - to improve model robustness against malicious inputs; Quick check - verify that the model maintains performance while reducing vulnerability to attacks
**Continuous optimization**: Why needed - enables efficient gradient-based adversarial example generation; Quick check - confirm that embedding space perturbations can be computed efficiently
**Embedding space manipulation**: Why needed - provides differentiable space for adversarial optimization; Quick check - validate that ℓ2-norm perturbations in embedding space affect model outputs as intended
**Robustness transferability**: Why needed - to ensure continuous perturbations provide protection against discrete attacks; Quick check - test discrete attack success rates after continuous training
**Computational complexity reduction**: Why needed - to make adversarial training feasible for large models; Quick check - measure actual computational savings versus discrete methods

## Architecture Onboarding
**Component map**: Input data → Embedding layer → Continuous perturbation optimization → Model training loop → Output evaluation
**Critical path**: Continuous adversarial example generation → Model parameter updates → Robustness evaluation against discrete attacks
**Design tradeoffs**: Computational efficiency vs. potential loss of fine-grained discrete token control; Continuous space smoothness vs. discrete attack pattern matching
**Failure signatures**: Loss of helpfulness despite robustness gains; Ineffective transfer from continuous to discrete attack protection; Computational savings not scaling as expected
**First experiments**: 1) Verify continuous ℓ2-norm perturbations affect model outputs; 2) Test computational speedup versus discrete adversarial training; 3) Evaluate robustness transfer from continuous to discrete attacks

## Open Questions the Paper Calls Out
None

## Limitations
- Transferability from continuous to discrete perturbations may not generalize across all model architectures
- Limited analysis of long-term robustness against adaptive attack strategies
- Quantitative evaluation of trade-offs between robustness and utility needs more rigorous assessment
- Scalability to frontier models beyond 70B parameters remains unexplored

## Confidence
- **High confidence**: Computational efficiency improvements (measured runtime reductions)
- **Medium confidence**: Immediate robustness gains against tested attack methods
- **Medium confidence**: Effectiveness of CAPO when utility data is unavailable
- **Low confidence**: Long-term robustness against adaptive adversaries
- **Medium confidence**: Discrete-to-continuous perturbation transfer generalization

## Next Checks
1. Cross-architecture validation: Test continuous adversarial training on diverse model architectures to verify discrete-to-continuous transfer generalizes beyond tested models.
2. Adaptive attack evaluation: Design and execute adaptive attack strategies specifically targeting continuous perturbation defenses to assess robustness against adversarial adaptation.
3. Longitudinal utility monitoring: Conduct extended evaluation of model helpfulness over time across diverse utility tasks and human evaluation studies to quantify the true trade-off between robustness and helpfulness.