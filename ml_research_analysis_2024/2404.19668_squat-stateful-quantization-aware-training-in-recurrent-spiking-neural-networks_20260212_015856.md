---
ver: rpa2
title: 'SQUAT: Stateful Quantization-Aware Training in Recurrent Spiking Neural Networks'
arxiv_id: '2404.19668'
source_url: https://arxiv.org/abs/2404.19668
tags:
- states
- weights
- quantization
- neural
- squat
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces stateful quantization-aware training (SQUAT)
  for spiking neural networks (SNNs), addressing the challenge of maintaining performance
  when reducing precision of both weights and state variables during training. The
  authors propose two SQUAT schemes: uniform quantization and threshold-centered quantization,
  which allocates more quantization levels near the firing threshold.'
---

# SQUAT: Stateful Quantization-Aware Training in Recurrent Spiking Neural Networks

## Quick Facts
- arXiv ID: 2404.19668
- Source URL: https://arxiv.org/abs/2404.19668
- Reference count: 40
- Key outcome: Combines quantization-aware training (QAT) for weights with stateful quantization-aware training (SQUAT) for membrane potentials in SNNs, showing that threshold-centered quantization improves accuracy especially in low-bit regimes.

## Executive Summary
This paper introduces stateful quantization-aware training (SQUAT) to address the challenge of maintaining performance when reducing precision of both weights and state variables in spiking neural networks. The authors propose two SQUAT schemes: uniform quantization and threshold-centered quantization, which allocates more quantization levels near the firing threshold. Comprehensive experiments across three datasets (FashionMNIST, Spiking Heidelberg Digits, and DVS Gesture) demonstrate that combining QAT and SQUAT consistently provides the best performance across all bit-widths, with threshold-centered quantization particularly effective in extreme quantization regimes. When computational resources are limited, QAT alone is more effective than SQUAT alone.

## Method Summary
SQUAT extends quantization-aware training principles to state variables (membrane potentials) in spiking neural networks by integrating state quantization into the forward pass during training. The method uses a straight-through estimator (STE) to bypass non-differentiability during backpropagation. Two quantization strategies are proposed: uniform quantization and threshold-centered quantization that allocates exponentially more levels near the firing threshold. The approach is implemented in snnTorch and combined with Brevitas for weight quantization, using surrogate gradients for backpropagation through the non-differentiable spiking operations.

## Key Results
- Threshold-centered quantization consistently outperforms uniform quantization across all bit-widths and datasets, with the most significant improvements in 2-bit models
- QAT-only consistently outperforms SQUAT-only, but combining both techniques provides the best overall performance
- SQUAT achieves near-full precision accuracy even with 2-bit state quantization on FashionMNIST and SHD datasets
- The DVS Gesture dataset shows the most significant performance drop under extreme quantization, highlighting dataset-dependent sensitivity to precision loss

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quantizing states during training (SQUAT) helps the model adapt to quantization noise and reduces performance loss compared to post-training quantization (PTQ).
- Mechanism: By including state quantization in the forward pass and training with a surrogate gradient (STE), the model learns to operate effectively within the constraints of lower precision state representations, accounting for quantization error during optimization.
- Core assumption: The model can learn to compensate for the loss of precision in state variables if exposed to quantization during training rather than only after training.
- Evidence anchors: [abstract] "This paper introduces two QAT schemes for stateful neurons: (i) a uniform quantization strategy, an established method for weight quantization, and (ii) threshold-centered quantization, which allocates exponentially more quantization levels near the firing threshold."

### Mechanism 2
- Claim: Allocating more quantization levels near the firing threshold (exponential quantization) improves accuracy, especially in low-bit regimes.
- Mechanism: Near-threshold activity has a greater impact on downstream neurons (spike generation), so higher precision in this region reduces the chance of spurious spikes or missed spikes due to rounding errors.
- Core assumption: Small changes in membrane potential near the threshold are more likely to cause threshold crossings, making precision in this region more critical than in other regions.
- Evidence anchors: [abstract] "Our results show that increasing the density of quantization levels around the firing threshold improves accuracy across several benchmark datasets."

### Mechanism 3
- Claim: Combining QAT (for weights) and SQUAT (for states) provides the best performance across all bit-widths.
- Mechanism: QAT allows weights to adapt to quantization noise while SQUAT allows states to adapt, with each technique addressing different sources of quantization error in the network.
- Core assumption: Weight quantization and state quantization introduce different types and magnitudes of error, and addressing both through training is more effective than addressing only one.
- Evidence anchors: [abstract] "The findings indicate that the combination of QAT and SQUAT enhance performance the most, but given the choice of one or the other, QAT improves performance by the larger degree."

## Foundational Learning

- Concept: Leaky Integrate-and-Fire (LIF) neuron model and its discrete-time implementation
  - Why needed here: The SQUAT techniques are applied to the state variables (membrane potentials) of LIF neurons, so understanding how these neurons work is essential for implementing and debugging the quantization schemes.
  - Quick check question: How is the membrane potential updated in discrete time, and what triggers a spike in the LIF model?

- Concept: Quantization-aware training (QAT) and the straight-through estimator (STE)
  - Why needed here: SQUAT builds on QAT principles, using STE to bypass the non-differentiability of quantization during backpropagation.
  - Quick check question: How does the STE approximation work for the gradient of a quantized variable with respect to its full-precision counterpart?

- Concept: Exponential distributions and their properties
  - Why needed here: The threshold-centered quantization uses an exponential distribution to allocate more levels near the firing threshold.
  - Quick check question: How does changing the exponent parameter affect the concentration of quantization levels near the threshold versus other regions?

## Architecture Onboarding

- Component map: Dataset -> Data preprocessing -> SNN model with LIF neurons -> Weight quantization (QAT) -> State quantization (SQUAT) -> Loss function -> Optimizer -> Evaluation

- Critical path: 1) Forward pass with quantized states and weights 2) Spike generation and counting 3) Loss computation 4) Backward pass with STE for quantization 5) Parameter updates 6) Repeat across time steps

- Design tradeoffs: Uniform vs. exponential state quantization (accuracy vs. complexity), number of bits (precision vs. memory/computation), QAT vs. SQUAT (effectiveness per unit cost vs. combined performance)

- Failure signatures: Accuracy degradation in low-bit regimes suggests quantization step size is too large; training instability may indicate poor STE approximation or inappropriate learning rate; performance similar to PTQ suggests SQUAT is not being properly integrated into the training loop

- First 3 experiments: 1) Implement uniform state quantization (SQUAT) on a pre-trained model and compare to PTQ on FashionMNIST 2) Add exponential state quantization and compare to uniform on the same dataset 3) Combine QAT (weight quantization) with SQUAT and compare to QAT-only and SQUAT-only

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does threshold-centered quantization provide consistent performance improvements across different spiking neuron models beyond the leaky integrate-and-fire model used in this study?
- Basis in paper: [inferred] The authors demonstrate significant improvements with threshold-centered quantization specifically for LIF neurons, but do not explore whether this benefit generalizes to other spiking neuron models like Hodgkin-Huxley or Izhikevich models.

### Open Question 2
- Question: What is the theoretical explanation for why exponentially distributed quantization levels centered around the threshold outperform uniform quantization, particularly in extreme low-bit scenarios?
- Basis in paper: [explicit] The authors observe this pattern consistently across all datasets and bit-widths but do not provide a theoretical framework explaining why this distribution is superior.

### Open Question 3
- Question: How does SQUAT interact with different surrogate gradient functions, and is there an optimal pairing between quantization strategy and gradient approximation method?
- Basis in paper: [inferred] The authors use a specific threshold-shifted arc-tangent surrogate gradient throughout their experiments, but do not investigate whether different gradient functions might synergize better with particular quantization schemes.

## Limitations

- The paper does not compare SQUAT to recently published SNN quantization methods, making it difficult to assess whether the improvements are competitive with state-of-the-art approaches
- The threshold-centered quantization parameter is not thoroughly explored, leaving uncertainty about the optimal exponential distribution shape
- Computational cost implications of combining QAT and SQUAT versus using QAT alone are not fully explored for practical deployment scenarios

## Confidence

- Threshold-centered quantization effectiveness: Medium-High (consistent experimental results but lacks theoretical justification)
- QAT+SQUAT combination benefits: High (consistent across all datasets and bit-widths)
- Generalizability to other spiking neuron models: Medium-Low (only tested on LIF neurons)

## Next Checks

1. **Ablation Study on Exponential Parameter**: Systematically vary the exponential quantization parameter across a wider range and measure its impact on accuracy, particularly in the 2-bit regime, to determine the optimal distribution shape.

2. **Comparison to Advanced SNN Quantization Methods**: Implement and compare SQUAT against recently published SNN quantization techniques to establish whether the performance gains are competitive with or superior to state-of-the-art approaches.

3. **Robustness Analysis Across Hyper-parameters**: Conduct sensitivity analysis on learning rate, batch size, and surrogate gradient parameters to determine how stable the SQUAT performance is across different training configurations.