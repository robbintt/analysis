---
ver: rpa2
title: 'Classification with a Network of Partially Informative Agents: Enabling Wise
  Crowds from Individually Myopic Classifiers'
arxiv_id: '2409.19947'
source_url: https://arxiv.org/abs/2409.19947
tags:
- agent
- class
- agents
- each
- classes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of classification in distributed
  networks where agents have heterogeneous, partially informative classifiers that
  can only distinguish between subsets of possible classes. The authors propose a
  local update rule that recursively updates each agent's local belief on all classes
  based on its private observations and classifier outputs, combined with a novel
  min-rule-based global update that enables belief propagation across the network.
---

# Classification with a Network of Partially Informative Agents: Enabling Wise Crowds from Individually Myopic Classifiers

## Quick Facts
- arXiv ID: 2409.19947
- Source URL: https://arxiv.org/abs/2409.19947
- Authors: Tong Yao; Shreyas Sundaram
- Reference count: 40
- Key outcome: Local update rules and min-rule-based global aggregation enable belief convergence to true class for all agents in networks with partially informative classifiers

## Executive Summary
This paper addresses distributed classification in networks where agents possess heterogeneous classifiers that can only distinguish between subsets of possible classes. The authors propose a framework where agents recursively update local beliefs based on private observations and classifier outputs, combined with a novel min-rule-based global update mechanism. Under ergodicity assumptions and with conditionally independent observations, the system achieves asymptotic convergence to the true class for all agents while exponentially rejecting false classes at a rate determined by the best-performing source or support agent in the network.

## Method Summary
The method employs a two-tier update mechanism: local updates combine private observations with classifier outputs using Bayesian updating, while global updates propagate beliefs across the network using a min-rule aggregation that takes the minimum belief value from neighboring agents. This approach enables information flow even when individual classifiers are partially informative, as agents can leverage the collective knowledge of their neighbors to improve classification accuracy over time.

## Key Results
- Beliefs on the true class converge to one asymptotically almost surely for all agents
- Exponential rejection of false classes occurs at a rate determined by the best-performing source or support agent
- Simulations with random forest classifiers and MobileNet on image data demonstrate improved performance over baseline aggregation rules

## Why This Works (Mechanism)
The min-rule-based global update enables effective belief propagation by ensuring that no agent's belief on a class can exceed the minimum belief held by its neighbors. This conservative aggregation prevents overconfident misclassification while allowing correct beliefs to strengthen through network consensus. The local Bayesian updates incorporate private observations with classifier outputs, creating a recursive refinement process where agents gradually eliminate incorrect class hypotheses through both local evidence and network-wide information sharing.

## Foundational Learning

**Ergodicity**: Network connectivity must be ergodic to ensure sufficient information mixing across the network. Quick check: Verify network graph satisfies ergodic conditions through eigenvalue analysis of the transition matrix.

**Bayesian updating with partial information**: Agents must correctly handle classifier outputs that only partially discriminate between classes. Quick check: Validate classifier capability matrices accurately reflect true discrimination boundaries.

**Min-rule aggregation**: The global update rule uses minimum belief values to prevent overconfidence. Quick check: Monitor belief values to ensure they never exceed the minimum from neighboring agents.

## Architecture Onboarding

**Component map**: Private observation -> Local Bayesian update -> Classifier output -> Min-rule aggregation -> Global belief update -> Network consensus

**Critical path**: Local observation → Bayesian update → Min-rule aggregation → Global update → Belief convergence

**Design tradeoffs**: The min-rule aggregation sacrifices some information for robustness against incorrect classifications, trading off convergence speed for reliability in the presence of partially informative classifiers.

**Failure signatures**: Belief values stagnating at intermediate levels, divergence in subnetworks, or exponential rejection rates slower than theoretical bounds indicate network partitioning, non-ergodic behavior, or violation of independence assumptions.

**First experiments**:
1. Test single-agent belief convergence with perfect and imperfect classifier knowledge
2. Evaluate two-agent network with complementary classifiers to verify belief propagation
3. Measure convergence rates in fully connected networks with varying classifier capabilities

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical guarantees rely on strong assumptions including ergodicity and conditionally independent observations
- Exponential rejection rate depends on existence of source or support agents, which may not hold in all network configurations
- Performance may degrade with imperfect classifier knowledge of their own capabilities

## Confidence
High confidence in: Local update rule mechanism, min-rule-based global update formulation, asymptotic convergence result under stated assumptions

Medium confidence in: Exponential rejection rate claim and its dependence on source/support agents, as the proof relies on several intermediate results that would benefit from empirical validation

Low confidence in: Practical performance under real-world conditions with non-ergodic networks, non-independent observations, and imperfect classifier capability knowledge

## Next Checks
1. Test the algorithm on real-world datasets with known class correlations to verify the independent observation assumption and measure performance degradation

2. Evaluate network robustness when source/support agents are removed or become unavailable, measuring convergence rates and final belief accuracy

3. Implement the algorithm with classifiers that have imperfect knowledge of their own capabilities and measure the impact on overall system performance