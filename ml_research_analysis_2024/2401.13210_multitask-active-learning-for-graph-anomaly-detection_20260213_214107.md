---
ver: rpa2
title: Multitask Active Learning for Graph Anomaly Detection
arxiv_id: '2401.13210'
source_url: https://arxiv.org/abs/2401.13210
tags:
- anomaly
- detection
- nodes
- node
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of detecting anomalies in graph-structured
  data, where traditional methods struggle due to limited labeled anomalies. The authors
  propose MITIGATE, a novel multitask active learning framework that leverages both
  node classification and anomaly detection tasks.
---

# Multitask Active Learning for Graph Anomaly Detection

## Quick Facts
- arXiv ID: 2401.13210
- Source URL: https://arxiv.org/abs/2401.13210
- Authors: Wenjing Chang; Kay Liu; Kaize Ding; Philip S. Yu; Jianjun Yu
- Reference count: 40
- Key outcome: MITIGATE achieves AUC-ROC scores up to 78.03% and AUC-PR scores up to 23.32% on graph anomaly detection tasks.

## Executive Summary
This paper addresses the challenge of detecting anomalies in graph-structured data when labeled anomalies are scarce. The authors propose MITIGATE, a novel multitask active learning framework that leverages both node classification and anomaly detection tasks to improve performance. The key innovation is using the confidence difference across tasks to measure node informativeness and a masked aggregation mechanism to assess representativeness, enabling efficient selection of informative yet not overly challenging nodes for labeling. Experimental results demonstrate significant improvements over state-of-the-art methods on four datasets.

## Method Summary
MITIGATE is a multitask active learning framework for graph anomaly detection that uses a shared GNN encoder with separate node classification and anomaly detection decoders. The method employs confidence difference across tasks to measure node informativeness and a masked aggregation mechanism for distance-based clustering to select representative nodes. The framework is trained using a hybrid loss function and actively queries an oracle for labels of selected nodes, iteratively improving performance within a limited labeling budget.

## Key Results
- MITIGATE achieves AUC-ROC scores up to 78.03% and AUC-PR scores up to 23.32% on benchmark datasets
- The method outperforms state-of-the-art graph anomaly detection approaches by significant margins
- MITIGATE demonstrates effective performance across Cora, Citeseer, BlogCatalog, and Flickr datasets with injected anomalies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multitask learning improves anomaly detection by leveraging abundant node classification labels to detect out-of-distribution nodes.
- Mechanism: By coupling node classification and anomaly detection tasks, the model can identify anomalies as nodes with high classification uncertainty (entropy), providing initial discrimination even when anomaly labels are scarce.
- Core assumption: Classification uncertainty correlates with anomaly likelihood, and classification labels are more readily available than anomaly labels.
- Evidence anchors:
  - [abstract] "By coupling node classification tasks, MITIGATE obtains the capability to detect out-of-distribution nodes without known anomalies."
  - [section 1] "The indirect supervision signals from other tasks (e.g., node classification) are relatively abundant."
  - [corpus] Weak - no direct mention of multitask learning for anomaly detection in corpus neighbors.
- Break condition: If classification labels do not correlate with anomaly likelihood or if classification uncertainty is not a reliable indicator of anomalies.

### Mechanism 2
- Claim: Dynamic informativeness metric based on confidence difference across tasks improves sample selection for labeling.
- Mechanism: Nodes with high confidence differences between node classifier and anomaly detector are informative because at least one task can correctly identify them, while nodes with conflicting predictions provide valuable learning signals.
- Core assumption: Nodes with conflicting predictions across tasks are not overly challenging samples and provide useful information for model training.
- Evidence anchors:
  - [abstract] "MITIGATE quantifies the informativeness of nodes by the confidence difference across tasks, allowing samples with conflicting predictions to provide informative yet not excessively challenging information for subsequent training."
  - [section 3.2.2] "The high confidence differenced indicates the controversy between two decoders. It's important to note that these samples with conflicting predictions in binary classification are not challenging for model training since one of the tasks can effectively handle them."
  - [corpus] Weak - no direct mention of confidence difference across tasks in corpus neighbors.
- Break condition: If nodes with conflicting predictions are actually challenging samples that hinder model performance.

### Mechanism 3
- Claim: Masked aggregation mechanism improves representativeness measurement by considering labeled status in the neighborhood.
- Mechanism: By masking representations of labeled neighbors during aggregation, the distance features emphasize the distinctive features of unlabeled nodes and reduce the influence of previously labeled representative features.
- Core assumption: Labeled neighbors' features should not dominate the distance calculation for selecting new representative nodes.
- Evidence anchors:
  - [abstract] "MITIGATE adopts a masked aggregation mechanism for distance measurement, considering both inherent features of nodes and current labeled status."
  - [section 3.2.1] "We derive distance features through a masked aggregation mechanism, which considers labeled status in the neighborhood. Initially, in order to mitigate the impact of features pertaining to labeled neighbors, their representations will be masked during the summation of neighborhood information."
  - [corpus] Weak - no direct mention of masked aggregation mechanism in corpus neighbors.
- Break condition: If masking labeled neighbors' features reduces the effectiveness of distance-based clustering for representativeness measurement.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and message passing
  - Why needed here: MITIGATE uses GNNs to learn node representations that capture both graph topology and node attributes for anomaly detection.
  - Quick check question: How does a GCN layer update node representations using neighborhood information?

- Concept: Active Learning and query strategies
  - Why needed here: MITIGATE actively selects nodes for labeling based on informativeness and representativeness metrics to maximize learning efficiency within a limited labeling budget.
  - Quick check question: What is the difference between uncertainty-based and diversity-based active learning strategies?

- Concept: Multi-task learning and knowledge transfer
  - Why needed here: MITIGATE leverages abundant node classification labels to improve anomaly detection performance by sharing representations between tasks.
  - Quick check question: How does multi-task learning help when one task has limited labeled data but another task has abundant labeled data?

## Architecture Onboarding

- Component map:
  GNN Encoder -> Node Classifier -> Entropy scores
  GNN Encoder -> Anomaly Score Predictor -> Anomaly scores
  Distance-based Clustering -> Representative node selection
  Informativeness Scoring -> Node value assessment
  Oracle -> Ground truth labels

- Critical path:
  1. Initialize with classification-labeled nodes as normal nodes
  2. Train GNN encoder and decoders using multitask loss
  3. Calculate distance features using masked aggregation
  4. Cluster nodes and select centers with high informativeness scores
  5. Query oracle for labels of selected nodes
  6. Add queried nodes to labeled set and repeat training

- Design tradeoffs:
  - Using classification uncertainty vs. anomaly scores for node selection
  - Masked aggregation vs. standard aggregation for distance features
  - Confidence difference vs. individual task scores for informativeness
  - Fixed vs. dynamic number of clusters in K-Medoids algorithm

- Failure signatures:
  - Poor anomaly detection performance despite high classification accuracy
  - Clustering fails to find representative nodes due to masked aggregation
  - Informativeness scores do not correlate with actual node value
  - Model overfits to labeled nodes and performs poorly on test set

- First 3 experiments:
  1. Test masked aggregation mechanism by comparing distance-based clustering with and without masking labeled neighbors
  2. Evaluate confidence difference metric by analyzing correlation between informativeness scores and actual node value
  3. Validate multitask learning benefits by comparing performance with and without classification task supervision

## Open Questions the Paper Calls Out
No explicit open questions are called out in the paper.

## Limitations
- The masked aggregation mechanism and confidence difference calculation lack detailed implementation specifics, making exact replication challenging.
- The paper uses synthetic anomaly injection rather than naturally occurring anomalies, limiting generalizability to real-world scenarios.
- Only one auxiliary task (node classification) is explored, leaving open the question of whether additional tasks could further improve performance.

## Confidence
- High confidence: The core multitask learning framework and confidence difference approach are well-grounded in ML principles
- Medium confidence: The masked aggregation mechanism's effectiveness needs more empirical validation
- Medium confidence: Synthetic anomaly generation limits real-world applicability

## Next Checks
1. Conduct ablation studies comparing MITIGATE with variants using standard aggregation and alternative informativeness metrics
2. Evaluate MITIGATE on datasets with naturally occurring anomalies to verify performance beyond synthetic settings
3. Implement MITIGATE with different auxiliary tasks (e.g., link prediction) to test multitask framework generalization