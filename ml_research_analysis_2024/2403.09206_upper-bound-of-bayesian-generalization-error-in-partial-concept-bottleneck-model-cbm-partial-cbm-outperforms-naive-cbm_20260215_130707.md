---
ver: rpa2
title: 'Upper Bound of Bayesian Generalization Error in Partial Concept Bottleneck
  Model (CBM): Partial CBM outperforms naive CBM'
arxiv_id: '2403.09206'
source_url: https://arxiv.org/abs/2403.09206
tags:
- learning
- watanabe
- bayesian
- neural
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the Bayesian generalization error in a three-layered
  and linear concept bottleneck model (CBM) and its variant, partial CBM (PCBM). The
  authors derive an upper bound of the RLCT for PCBM and prove that PCBM outperforms
  CBM in terms of generalization.
---

# Upper Bound of Bayesian Generalization Error in Partial Concept Bottleneck Model (CBM): Partial CBM outperforms naive CBM

## Quick Facts
- arXiv ID: 2403.09206
- Source URL: https://arxiv.org/abs/2403.09206
- Authors: Naoki Hayashi; Yoshihide Sawada
- Reference count: 25
- Key outcome: PCBM has lower RLCT and Bayesian generalization error than CBM by partially supervising concept layers

## Executive Summary
This paper analyzes the Bayesian generalization error in partial concept bottleneck models (PCBM) versus standard concept bottleneck models (CBM). The authors prove that PCBM achieves lower generalization error by decomposing the parameter space into tacit (unsupervised) and explicit (supervised) concept parts. The key result shows that the RLCT of PCBM is bounded by the sum of a reduced rank regression term for tacit concepts and a standard CBM term for explicit concepts, yielding an overall bound lower than full CBM's RLCT.

## Method Summary
The authors use singular learning theory to analyze the Bayesian generalization error of PCBM. They decompose the averaged error function into tacit and explicit concept parts, then apply resolution of singularities to compute the RLCT. The proof leverages reduced rank regression theory for the tacit concept component and standard CBM results for the explicit concept component. The main theorem establishes that PCBM's RLCT is bounded by the sum of these two terms, which is less than CBM's RLCT.

## Key Results
- PCBM's RLCT is bounded by λR(M,H₁,N,r′) + H₂(M+N)/2 where r′ is the rank of the tacit concept part
- This bound is strictly less than CBM's RLCT of H(M+N)/2 when tacit concepts are low-rank
- The decomposition enables simultaneous training of tacit and explicit concepts, improving generalization over sequential approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Partial CBM reduces Bayesian generalization error by partially supervising concept layers
- Mechanism: Decomposes RLCT into reduced rank regression on tacit concepts plus standard CBM on explicit concepts, yielding lower overall RLCT than full CBM's H(M+N)/2
- Core assumption: Parameter space decomposition into tacit (B₁) and explicit (B₂) concepts is valid
- Evidence anchors: Abstract states RLCT bound; Section 4 proves decomposition of ∥AB - A₀B₀∥²; Corpus lists CBM variants without contradiction
- Break condition: If tacit concept subspace is not low-rank or B₂ and A₂ cannot be trained independently

### Mechanism 2
- Claim: Singular learning theory framework enables RLCT bounding for hierarchical models
- Mechanism: Resolution of singularities transforms K⁻¹(0) to normal crossing form, enabling RLCT calculation for hierarchical models with latent concepts
- Core assumption: Prior is positive and bounded on K⁻¹(0), and K(A,B) is analytic
- Evidence anchors: Section 3.2 explains resolution of singularities application; Section 4 applies framework to PCBM; Corpus contains no contradictory evidence
- Break condition: If prior vanishes on K⁻¹(0) or K(A,B) is not analytic (e.g., non-smooth activation)

### Mechanism 3
- Claim: Simultaneous training of tacit and explicit concepts is better than multi-stage estimation
- Mechanism: PCBM's RLCT is bounded by an upper model that trains tacit and explicit parts independently, with actual RLCT being lower than this bound
- Core assumption: Independence of parameter blocks (A₁,B₁) and (A₂,B₂) in upper model is valid
- Evidence anchors: Section 5 states simultaneous training is preferred; Section 4 constructs upper model via decomposition; Corpus lists CBM variants without evidence on training strategy
- Break condition: If parameter blocks are actually dependent in a way sequential training could exploit

## Foundational Learning

- Concept: Real Log Canonical Threshold (RLCT)
  - Why needed here: RLCT determines asymptotic Bayesian generalization error E[Gn] = λ/n + o(1/n) for singular models like PCBM
  - Quick check question: What is the RLCT of a regular (non-singular) d-dimensional model, and why is this an upper bound for singular models?

- Concept: Resolution of Singularities
  - Why needed here: Transforms complicated zero set K⁻¹(0) of averaged error function into normal crossing form, enabling RLCT calculation
  - Quick check question: In singular learning theory, what is normal crossing form and how does it relate to RLCT calculation?

- Concept: Concept Bottleneck Architecture
  - Why needed here: Understanding CBM/PCBM structure (input → concepts → output) is essential for grasping how partial supervision affects parameter space and generalization
  - Quick check question: How does CBM architecture differ from standard neural networks in terms of parameter constraints and interpretability?

## Architecture Onboarding

- Component map:
  - Input layer (N-dimensional) -> Hidden layer (H₁ tacit + H₂ explicit concepts) -> Output layer (M-dimensional)
  - Weight matrices: A (M×H) = [A₁,A₂], B (H×N) = [B₁;B₂]

- Critical path:
  1. Compute averaged error function K(A,B) = ∥AB - A₀B₀∥² + ∥B₂ - B₀₂∥²
  2. Decompose into tacit part (∥A₁B₁ - A₀₁B₀₁∥²) and explicit part (∥A₂B₂ - A₀₂B₀₂∥² + ∥B₂ - B₀₂∥²)
  3. Apply reduced rank regression RLCT bound to tacit part
  4. Use CBM RLCT (H₂(M+N)/2) for explicit part
  5. Sum bounds to get λP ≤ λR(M,H₁,N,r′) + H₂(M+N)/2

- Design tradeoffs:
  - More tacit concepts (larger H₁) → potentially lower RLCT if true rank is low, but less interpretability
  - More explicit concepts (larger H₂) → better interpretability but higher RLCT term
  - Three-layered linear assumption enables exact RLCT calculation but limits applicability to deep/non-linear cases

- Failure signatures:
  - RLCT bound not tight → decomposition invalid or true rank higher than assumed
  - Training instability → poor conditioning in simultaneous optimization of tacit and explicit parts
  - Interpretability compromised → tacit concepts not capturing meaningful patterns

- First 3 experiments:
  1. Synthetic data with known low-rank tacit concepts: verify PCBM achieves lower generalization error than CBM when H₁ is chosen appropriately
  2. Ablation study varying H₂: confirm the H₂(M+N)/2 term in RLCT bound by observing generalization degradation as explicit concepts increase
  3. Comparison with sequential CBM: train CBM in independent/sequential modes and compare generalization to PCBM's simultaneous training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Bayesian generalization error of PCBM compare to CBM in practical scenarios with non-linear activation functions?
- Basis in paper: Paper proves PCBM outperforms CBM for linear models but doesn't explore non-linear cases
- Why unresolved: Analysis focuses on three-layered linear architectures
- What evidence would resolve it: Empirical studies comparing generalization performance with non-linear activations in real-world applications

### Open Question 2
- Question: What is the impact of concept dimensionality on PCBM versus CBM performance?
- Basis in paper: Derives theoretical bounds but doesn't investigate how varying concept dimensionality affects performance
- Why unresolved: Theoretical analysis assumes fixed dimensions for concepts
- What evidence would resolve it: Experimental results showing performance with varying concept dimensions across datasets and tasks

### Open Question 3
- Question: How does concept bottleneck structure influence model interpretability in addition to generalization performance?
- Basis in paper: Focuses on generalization performance but doesn't address interpretability
- Why unresolved: Analysis limited to mathematical bounds without considering interpretability
- What evidence would resolve it: Studies evaluating interpretability through human evaluations or model explanation comparisons

## Limitations
- Theoretical analysis limited to linear three-layered architecture, not applicable to deep/non-linear networks
- Assumes low-rank structure for tacit concepts, which may not hold in real applications
- Doesn't empirically validate the simultaneous-training advantage claim or practical generalization improvements

## Confidence
- Mechanism 1: Medium confidence - theoretical decomposition is sound but assumes low-rank tacit concepts
- Mechanism 2: Medium confidence - singular learning theory framework is well-established but requires analytic error functions
- Mechanism 3: Low confidence - simultaneous training advantage claimed but not empirically validated
- Overall confidence: Medium - theoretical bounds are rigorous but practical implications uncertain

## Next Checks
1. Synthetic experiments testing RLCT predictions with controlled true ranks to verify decomposition validity
2. Ablation studies varying H₁ and H₂ to empirically confirm the theoretical RLCT terms
3. Comparison experiments between simultaneous and sequential CBM training on real datasets to validate training strategy claims