---
ver: rpa2
title: 'Finding Patterns in Ambiguity: Interpretable Stress Testing in the Decision~Boundary'
arxiv_id: '2408.06302'
source_url: https://arxiv.org/abs/2408.06302
tags:
- decision
- images
- prototypes
- deep
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the interpretability challenge of deep learning
  models by focusing on understanding their decision boundaries, particularly in regions
  of low confidence where models struggle to make accurate predictions. The proposed
  method generates synthetic borderline instances near the decision boundary using
  a GAN-based approach (GASTeN), then applies deep clustering techniques (UMAP + GMM)
  to identify patterns in these ambiguous examples.
---

# Finding Patterns in Ambiguity: Interpretable Stress Testing in the Decision~Boundary

## Quick Facts
- arXiv ID: 2408.06302
- Source URL: https://arxiv.org/abs/2408.06302
- Reference count: 28
- One-line primary result: GAN-based synthetic data generation combined with deep clustering reveals interpretable patterns in ambiguous regions of deep learning decision boundaries.

## Executive Summary
This work addresses the interpretability challenge of deep learning models by focusing on understanding their decision boundaries, particularly in regions of low confidence where models struggle to make accurate predictions. The proposed method generates synthetic borderline instances near the decision boundary using a GAN-based approach (GASTeN), then applies deep clustering techniques (UMAP + GMM) to identify patterns in these ambiguous examples. Representative prototypes are selected from each cluster for analysis. Experiments on binary MNIST and Fashion-MNIST subsets demonstrate that the approach can reveal distinct and compact clusters with diverse prototypes that capture essential features leading to low-confidence decisions.

## Method Summary
The method generates synthetic borderline instances near the decision boundary using GASTeN, a GAN-based technique trained with the MUT's predictions to approximate its decision boundary. Synthetic images are filtered by Average Confidence Distance (ACD < 0.1) to retain only ambiguous examples. High-level embeddings are extracted from the MUT's penultimate layer, reduced with UMAP, and clustered with GMM. Cluster medoids serve as prototypes, which are visualized and analyzed with GradientSHAP to reveal pixel-level contributions to low-confidence predictions.

## Key Results
- GAN-based generation produces synthetic borderline instances with moderate clustering quality (silhouette scores 0.26-0.52)
- UMAP + GMM clustering reveals distinct and compact clusters with diverse prototypes in ambiguous regions
- GradientSHAP visualization identifies interpretable pixel features contributing to low-confidence predictions
- Method works across CNN architectures of varying complexity on binary MNIST and Fashion-MNIST subsets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GAN-based synthetic data generation can populate decision boundary regions with ambiguous examples that challenge model confidence.
- Mechanism: GASTeN trains a generator conditioned on the MUT's confidence scores, producing synthetic images near the decision boundary where the classifier's softmax probabilities are low. The confusion distance weighting term in the loss function explicitly encourages generation of samples that lie close to the boundary between classes.
- Core assumption: The MUT's predicted probabilities accurately reflect confidence levels and the generator can approximate the true decision boundary through adversarial training.
- Evidence anchors:
  - [abstract] "generate synthetic data near the decision boundary with GASTeN"
  - [section] "GASTeN, a GAN-based technique trained with the MUT predictions, to approximate its decision boundary"
  - [corpus] Weak evidence - corpus neighbors don't directly address GAN-based decision boundary generation

### Mechanism 2
- Claim: Deep clustering of GAN-generated samples reveals interpretable patterns in ambiguous regions.
- Mechanism: The method extracts high-level embeddings from the MUT's penultimate layer, reduces dimensionality with UMAP to preserve local structure, then applies GMM clustering. This combination captures semantic similarities among ambiguous examples that share common challenging features.
- Core assumption: The MUT's internal representations contain discriminative information about why certain samples are ambiguous, and dimensionality reduction preserves these discriminative features.
- Evidence anchors:
  - [abstract] "detect patterns in these examples using UMAP and Gaussian Mixture Models (GMM)"
  - [section] "apply UMAP for dimension reduction, followed by GMM clustering to group visually similar images"
  - [corpus] Weak evidence - corpus neighbors focus on different clustering applications without addressing decision boundary ambiguity

### Mechanism 3
- Claim: GradientSHAP visualization reveals which pixel features contribute to low-confidence predictions in prototypes.
- Mechanism: After selecting cluster medoids as prototypes, GradientSHAP integrates gradients with SHAP values to attribute each pixel's contribution to the model's output. This explains why specific ambiguous examples are classified with low confidence.
- Core assumption: The MUT's gradients with respect to input pixels meaningfully reflect feature importance for classification decisions.
- Evidence anchors:
  - [abstract] "visualize the prototypes and the decision boundary using 2D space visualization and GradientSHAP"
  - [section] "use GradientSHAP — a technique that explains the contribution of each pixel to the model's output by integrating gradients with SHAP values"
  - [corpus] Weak evidence - corpus neighbors don't address SHAP-based interpretability for ambiguous samples

## Foundational Learning

- Concept: GAN training dynamics and loss functions
  - Why needed here: Understanding how GASTeN's loss function balances image realism with boundary proximity is crucial for tuning hyperparameters and interpreting results
  - Quick check question: What role does the confusion distance weighting parameter play in GASTeN's generator loss function?

- Concept: Manifold learning and clustering evaluation metrics
  - Why needed here: UMAP's hyperparameters and clustering metrics like silhouette score and Davies-Bouldin index determine the quality of pattern discovery in ambiguous regions
  - Quick check question: How does the choice of UMAP's n_neighbors parameter affect the trade-off between local and global structure preservation?

- Concept: Gradient-based attribution methods
  - Why needed here: GradientSHAP combines gradients with game-theoretic SHAP values to produce pixel-level explanations for low-confidence predictions
  - Quick check question: How does GradientSHAP differ from simple gradient-based saliency maps in terms of attribution reliability?

## Architecture Onboarding

- Component map: Data pipeline → GASTeN generator → ACD filtering → Feature extraction → UMAP → GMM clustering → Prototype selection → GradientSHAP visualization
- Critical path: The sequence from GAN generation through clustering to prototype selection must complete successfully for meaningful interpretability results
- Design tradeoffs: Higher GASTeN FID scores indicate more realistic images but may produce fewer boundary-proximal examples; more GMM clusters provide finer-grained patterns but may be harder to interpret
- Failure signatures: Low silhouette scores (0.26-0.52 in experiments) indicate poor clustering quality; high FID scores post-filtering suggest GASTeN struggles to generate realistic boundary examples
- First 3 experiments:
  1. Train MUT on binary subset, verify baseline accuracy (92-99% range in experiments)
  2. Run GASTeN for 10-15 epochs, check FID scores and ACD filtering effectiveness
  3. Apply UMAP+GMM with Bayesian optimization, evaluate silhouette scores and visualize 2D projections

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the efficiency of GASTeN in generating decision boundary-near samples be improved to reduce the high FID scores and significant image count reduction during filtering?
- Basis in paper: [explicit] The paper notes that GASTeN's limited efficiency results in a substantial increase in FID scores and a significant reduction in image count post-filtering, suggesting a need for improvement.
- Why unresolved: The current approach using GASTeN with ACD filtering is not effective enough, as indicated by the correlation between model complexity and FID scores, and the resulting high FID scores.
- What evidence would resolve it: Developing and testing alternative GAN-based methods or modifications to GASTeN that produce more realistic decision boundary-near samples with lower FID scores would provide evidence of improved efficiency.

### Open Question 2
- Question: What alternative dimensionality reduction and clustering techniques could enhance the clustering quality of borderline instances?
- Basis in paper: [inferred] The paper suggests exploring other clustering and embedding techniques due to the modest clustering quality (silhouette scores 0.26-0.52) and the correlation between the number of low-confidence images and silhouette score.
- Why unresolved: The current combination of UMAP and GMM shows room for improvement, as indicated by the clustering quality metrics and the observed negative correlation with the number of low-confidence images.
- What evidence would resolve it: Experimenting with different combinations of dimensionality reduction techniques (e.g., t-SNE, PCA) and clustering algorithms (e.g., DBSCAN, hierarchical clustering) on the same datasets and comparing their clustering quality metrics would provide evidence of potential improvements.

### Open Question 3
- Question: How does the proposed method perform on more complex datasets where ambiguity is not easily assessed visually?
- Basis in paper: [explicit] The paper acknowledges the need to assess the method's performance in complex scenarios, as it was only tested on simplified datasets like MNIST and Fashion-MNIST.
- Why unresolved: The method's effectiveness in identifying interpretable patterns in ambiguous regions has only been demonstrated on datasets with low complexity, and its scalability to more complex datasets remains untested.
- What evidence would resolve it: Applying the method to complex image datasets (e.g., CIFAR-10, ImageNet) and evaluating its ability to generate interpretable prototypes and uncover patterns in ambiguous regions would provide evidence of its performance on more complex data.

## Limitations

- Limited generalizability beyond binary classification tasks to multi-class problems or real-world datasets with complex class boundaries
- Moderate clustering quality (silhouette scores 0.26-0.52) indicates room for improvement in cluster separation and interpretability
- Reliance on MUT's confidence estimates assumes well-calibrated probabilities, which is often not the case in deep learning models

## Confidence

**High Confidence**: The GAN-based generation mechanism (GASTeN) and its ability to produce synthetic samples near decision boundaries is well-supported by the experimental results showing FID score improvements and ACD filtering effectiveness.

**Medium Confidence**: The clustering approach using UMAP + GMM reliably identifies distinct patterns in ambiguous regions, though the moderate silhouette scores (0.26-0.52) suggest room for improvement in cluster separation quality.

**Medium Confidence**: GradientSHAP provides meaningful pixel-level explanations for prototype samples, though the non-linear nature of deep networks means attributions may not always capture the full decision-making process.

## Next Checks

1. **Multi-class Extension**: Test the complete pipeline on a three-class subset of MNIST (e.g., 0 vs 1 vs 2) to validate whether the clustering approach can handle more complex decision boundaries with multiple class interactions.

2. **Confidence Calibration**: Apply temperature scaling or other calibration techniques to the MUT models and measure how improved confidence estimates affect GASTeN's ability to generate realistic boundary samples and the resulting clustering quality.

3. **Cross-architecture Validation**: Repeat experiments with fundamentally different MUT architectures (e.g., ResNet, Vision Transformer) to assess whether the interpretability patterns discovered are architecture-dependent or reflect genuine data characteristics.