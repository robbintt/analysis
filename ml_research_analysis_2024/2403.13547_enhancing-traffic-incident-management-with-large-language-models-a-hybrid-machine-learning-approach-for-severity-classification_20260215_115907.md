---
ver: rpa2
title: 'Enhancing Traffic Incident Management with Large Language Models: A Hybrid
  Machine Learning Approach for Severity Classification'
arxiv_id: '2403.13547'
source_url: https://arxiv.org/abs/2403.13547
tags:
- accident
- features
- language
- data
- traffic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the integration of Large Language Models
  (LLMs) into machine learning workflows for traffic incident management, focusing
  on incident severity classification. The methodology combines features generated
  by modern LLMs with conventional data extracted from incident reports to enhance
  classification accuracy across multiple machine learning algorithms.
---

# Enhancing Traffic Incident Management with Large Language Models: A Hybrid Machine Learning Approach for Severity Classification

## Quick Facts
- arXiv ID: 2403.13547
- Source URL: https://arxiv.org/abs/2403.13547
- Reference count: 39
- This study combines LLM-extracted features with traditional accident report features to improve traffic incident severity classification accuracy across three international datasets.

## Executive Summary
This research investigates how Large Language Models (LLMs) can enhance machine learning workflows for traffic incident management by improving severity classification accuracy. The methodology combines features generated by modern LLMs with conventional data extracted from incident reports, demonstrating that merging baseline features with language-based features can improve classification performance across multiple machine learning algorithms. The study validates this approach using datasets from the United States, United Kingdom, and Queensland, Australia, showing improved performance particularly when employing Random Forests and Extreme Gradient Boosting methods.

## Method Summary
The methodology converts structured accident report data into full-text representations combining field names, values, and narrative descriptions, which are then processed by multiple LLM models (BERT, BERT-large, XLNet, XLNet-large, RoBERTa, RoBERTa-large, ALBERT, ALBERT-xxlarge) to extract numerical features. These LLM-derived features are combined with baseline accident report features and used to train machine learning models including Random Forest, XGBoost, LightGBM, and KNN. The approach simplifies traditional feature engineering by encoding full-text data representations into numerical features, eliminating manual preprocessing steps like label encoding and polynomial feature creation.

## Key Results
- F1-scores reached up to 0.89 on USA data and 0.65 on Queensland data when using combined LLM and baseline features
- Random Forests and Extreme Gradient Boosting methods showed particularly strong performance with LLM-enhanced features
- The hybrid approach consistently matched or exceeded traditional feature engineering pipelines across all three international datasets
- Tree-based ensemble models effectively handled high-dimensional LLM embeddings without requiring explicit feature selection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Full-text representation combined with LLM feature extraction improves classification accuracy over traditional feature engineering.
- Mechanism: LLM tokenizes the entire report (field names + values + narrative) into a unified textual string, then extracts contextualized numerical embeddings. These embeddings replace or augment hand-crafted features, reducing preprocessing complexity while preserving discriminative information.
- Core assumption: The contextualized embeddings capture relationships between fields that manual engineering might miss.
- Evidence anchors: [abstract] "Incorporating features from language models with those directly obtained from incident reports has shown to improve, or at least match, the performance..." [section] "The proposed methodology simplifies traditional feature engineering by using LLMs to encode full-text data representations into numerical features..."

### Mechanism 2
- Claim: Tree-based ensemble models handle high-dimensional LLM embeddings efficiently, making them suitable for severity classification.
- Mechanism: LLMs produce up to 768-dimensional vectors; tree ensembles split on informative features without requiring explicit feature selection, tolerating noise and redundancy better than linear models.
- Core assumption: The dimensionality of embeddings does not overwhelm the ensemble's capacity to find discriminative splits.
- Evidence anchors: [abstract] "This comparison was quantified using the F1-score... particularly when employing Random Forests and Extreme Gradient Boosting methods..." [section] "The use of fast machine learning models to process feature vectors produced by LLM makes word importance analysis at least feasible."

### Mechanism 3
- Claim: Removing manual feature engineering steps saves time without sacrificing accuracy.
- Mechanism: The LLM internally tokenizes and contextualizes all fields; no separate preprocessing pipelines are needed, enabling a direct feed into ML models.
- Core assumption: The LLM's internal handling of categorical and temporal data is as effective as explicit encoding.
- Evidence anchors: [abstract] "...simplifying traditional feature engineering by using LLMs to encode full-text data representations..." [section] "The use of textual accident descriptions was also found to improve the performance of traffic incident duration prediction models..."

## Foundational Learning

- Concept: Large Language Models and their embedding mechanisms
  - Why needed here: Understanding how LLMs like BERT produce contextual embeddings from text is essential to grasp why they can replace manual feature engineering.
  - Quick check question: What is the difference between BERT's Masked Language Modeling and traditional word embeddings?

- Concept: Tree-based ensemble algorithms (Random Forest, XGBoost)
  - Why needed here: These models are the primary classifiers tested; knowing how they handle high-dimensional input explains their suitability.
  - Quick check question: How do Random Forest and XGBoost differ in how they build trees and handle overfitting?

- Concept: Dimensionality reduction (PCA) and its trade-offs
  - Why needed here: With 768 dimensions from LLMs, understanding when and why to apply PCA is key for model efficiency.
  - Quick check question: When does PCA improve model performance, and when might it hurt it?

## Architecture Onboarding

- Component map: Data ingestion -> Full-text conversion -> LLM embedding -> Optional PCA -> ML model training -> Evaluation
- Critical path: Data ingestion -> Full-text conversion -> LLM embedding -> ML model training -> Evaluation
- Design tradeoffs:
  - Using full LLM embeddings (768 dim) vs. reduced PCA components (64–256 dim): higher accuracy vs. faster training
  - BERT-base vs. BERT-large: marginal accuracy gain vs. 12× slower tokenization/inference
  - Tree ensembles vs. KNN/LR: better handling of high dimensionality vs. simpler models for quick prototyping
- Failure signatures:
  - Low F1-score despite high embedding dimensionality → embeddings are not capturing discriminative features
  - Extremely slow tokenization/inference → model too large for real-time deployment
  - Overfitting on small datasets → reduce dimensions or gather more data
- First 3 experiments:
  1. Compare F1-score using only traditional tabular features vs. only LLM embeddings (BERT-base) on USA dataset
  2. Evaluate impact of dimensionality reduction: train with 768 dims, then 256, 128, 64 dims and measure F1-score
  3. Test ensemble model choice: Random Forest vs. XGBoost vs. LightGBM on combined feature set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal combination of language models and machine learning algorithms for traffic incident severity classification across different international datasets?
- Basis in paper: [explicit] The paper states "Our primary goal is to investigate the potentials of LLMs in feature extraction from textual accident reports" and seeks to identify "the optimal pairing for the most accurate prediction"
- Why unresolved: While the paper compares various combinations and finds XGBoost with BERT features performs best for USA data, it doesn't definitively establish which combination works best across all datasets and conditions
- What evidence would resolve it: Systematic testing of all LLM-ML combinations across multiple international datasets with varying levels of narrative detail, accident report formats, and severity classification schemes

### Open Question 2
- Question: How do LLM-extracted features compare to traditional feature engineering pipelines in terms of classification performance when handling unstructured text data?
- Basis in paper: [explicit] The paper explicitly states "we aim to evaluate if the use of the extracted features, when used with traditional accident reports, can advance or at least match the performance of traditional feature engineering pipeline"
- Why unresolved: The paper shows LLM features can match or exceed traditional features in some cases, but doesn't fully explore the conditions under which one approach outperforms the other or provide a comprehensive comparison framework
- What evidence would resolve it: Controlled experiments comparing LLM-based feature extraction against traditional feature engineering across diverse datasets with varying levels of textual complexity and report structure

### Open Question 3
- Question: What is the relationship between the amount and quality of narrative text in accident reports and the performance of LLM-based severity classification?
- Basis in paper: [inferred] The paper notes that "There is no apparent difference between all the language models" which could be "attributed to the limited narrative content within the accident reports"
- Why unresolved: The paper suggests narrative content may be limited in the datasets used, but doesn't systematically investigate how varying amounts or quality of narrative text affects classification performance
- What evidence would resolve it: Experiments using datasets with varying levels of narrative detail, from minimal descriptions to comprehensive narratives, to establish the threshold of textual information needed for LLMs to provide significant classification benefits

## Limitations
- The approach requires significant computational resources for LLM inference, potentially limiting real-time deployment in traffic management systems
- Performance gains show substantial geographic variation (F1-scores of 0.89 for USA vs. 0.65 for Queensland), suggesting methodology may not generalize uniformly across different traffic environments
- The study does not address interpretability of LLM-derived features, making it difficult to understand which aspects of incident reports most influence severity predictions

## Confidence
- **High confidence**: Core finding that combining LLM features with traditional features improves classification accuracy over using either approach alone
- **Medium confidence**: Generalizability of results across different geographic regions due to substantial performance gap between USA (0.89 F1) and Queensland (0.65 F1) datasets
- **Low confidence**: Real-time applicability due to computational overhead of LLM inference, which is mentioned but not quantified

## Next Checks
1. Conduct cross-dataset validation by training on one geographic region (e.g., USA) and testing on another (e.g., UK or Queensland) to assess true generalization capability
2. Perform ablation studies to identify which specific LLM-derived features contribute most to classification improvements, and whether these align with domain expertise in traffic incident management
3. Measure and report the computational latency of the full pipeline (from text preprocessing through LLM inference to ML classification) to evaluate feasibility for real-time traffic incident response systems