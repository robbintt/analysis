---
ver: rpa2
title: 'CF-KAN: Kolmogorov-Arnold Network-based Collaborative Filtering to Mitigate
  Catastrophic Forgetting in Recommender Systems'
arxiv_id: '2409.05878'
source_url: https://arxiv.org/abs/2409.05878
tags:
- cf-kan
- recommendation
- learning
- kans
- functions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CF-KAN, a collaborative filtering method
  that leverages Kolmogorov-Arnold networks (KANs) to address catastrophic forgetting
  in recommender systems. Unlike traditional multi-layer perceptrons (MLPs) that apply
  fixed activation functions, KANs learn nonlinear functions at the edge level, enabling
  more robust learning and better retention of previously acquired knowledge.
---

# CF-KAN: Kolmogorov-Arnold Network-based Collaborative Filtering to Mitigate Catastrophic Forgetting in Recommender Systems

## Quick Facts
- **arXiv ID**: 2409.05878
- **Source URL**: https://arxiv.org/abs/2409.05878
- **Authors**: Jin-Duk Park; Kyung-Min Kim; Won-Yong Shin
- **Reference count**: 6
- **Primary result**: CF-KAN outperforms state-of-the-art methods by up to 8.2% in recommendation accuracy while mitigating catastrophic forgetting

## Executive Summary
CF-KAN introduces a collaborative filtering approach that leverages Kolmogorov-Arnold Networks (KANs) to address catastrophic forgetting in recommender systems. Unlike traditional multi-layer perceptrons that apply fixed activation functions, KANs learn nonlinear functions at the edge level, enabling more robust learning and better retention of previously acquired knowledge. Built on a KAN-based autoencoder architecture, CF-KAN effectively captures sparse user-item interactions while maintaining interpretability through edge-level learning and pruning. Extensive experiments demonstrate that CF-KAN consistently outperforms state-of-the-art methods by up to 8.2% in recommendation accuracy, shows resilience to catastrophic forgetting in dynamic environments, and achieves faster training times compared to two-tower models.

## Method Summary
CF-KAN is a collaborative filtering method built on a KAN-based autoencoder architecture. The model learns nonlinear activation functions at the edge level rather than using fixed activation functions across nodes, which enables localized parameter updates and better preservation of previously learned knowledge. The autoencoder minimizes reconstruction error with L1 and entropy regularization, while the edge-specific learning approach facilitates interpretability and pruning. The method is trained on user-item interaction matrices and evaluated using standard recommendation metrics including Recall@K and NDCG@K.

## Key Results
- CF-KAN achieves up to 8.2% improvement in recommendation accuracy over state-of-the-art methods
- Demonstrates superior performance in mitigating catastrophic forgetting compared to MLP-based approaches
- Achieves faster training times than two-tower models while maintaining better accuracy
- Shows strong performance across multiple datasets including MovieLens-1M, Yelp, and Anime

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: KANs mitigate catastrophic forgetting by learning edge-specific activation functions, enabling localized parameter updates that preserve previously learned knowledge.
- **Mechanism**: In traditional MLPs, all nodes in a layer share the same fixed activation function, leading to global parameter updates. In contrast, KANs assign learnable activation functions to edges, allowing updates to be more localized. This means that when new information is learned, only the edges relevant to the new task are significantly modified, while the rest of the network retains its original knowledge.
- **Core assumption**: The localized updates in KANs are sufficient to preserve previously learned knowledge without requiring explicit rehearsal or regularization techniques.
- **Evidence anchors**:
  - [abstract] "By learning nonlinear functions on the edge level, KANs are more robust to the catastrophic forgetting problem than MLPs."
  - [section] "KANs are known to update model parameters more locally and sparsely than MLPs (Liu et al. 2024), allowing KANs to integrate new data relatively effectively without disrupting previously learned patterns."
  - [corpus] The related paper "Catastrophic Forgetting in Kolmogorov-Arnold Networks" suggests that KANs may be more robust to catastrophic forgetting, but the specific mechanism is not detailed.

### Mechanism 2
- **Claim**: The sparsity of user-item interactions in recommender systems aligns well with the localized learning approach of KANs, allowing them to focus on important interactions without being overwhelmed by noise.
- **Mechanism**: In recommender systems, user-item interactions are often extremely sparse, meaning that important interactions are scattered and concentrated in a few key areas. KANs, with their edge-specific learning, can focus on these important interactions and ignore the noise, leading to better performance and interpretability.
- **Core assumption**: The sparsity of user-item interactions is a key characteristic of recommender systems that can be leveraged by KANs to improve performance.
- **Evidence anchors**:
  - [section] "Meanwhile, in recommender systems, userâ€“item interactions are often extremely sparse, which means that important interaction data are scattered and concentrated in a few key areas rather than being uniformly distributed."
  - [section] "Thus, the parameters in KAN is likely to be updated locally during the learning process (Liu et al. 2024)."
  - [corpus] The corpus evidence is weak, with no specific studies directly linking the sparsity of user-item interactions to the performance of KANs in recommender systems.

### Mechanism 3
- **Claim**: The interpretability of KANs, achieved through edge-specific learning and pruning, allows for a better understanding of the recommendation process and can lead to more accurate and trustworthy recommendations.
- **Mechanism**: KANs provide nonlinearity specific to each edge, making them more interpretable than traditional MLPs. Additionally, KANs are likely to be locally updated and sparsely trained with the regularization term, which facilitates pruning while focusing on important edges and nodes. This interpretability allows for a better understanding of the recommendation process and can lead to more accurate and trustworthy recommendations.
- **Core assumption**: Interpretability is a key factor in the success of recommender systems, as it allows users to understand and trust the recommendations.
- **Evidence anchors**:
  - [abstract] "CF-KAN's edge-level interpretation facilitating the explainability of recommendations."
  - [section] "KANs provide nonlinearity specific to each edge, making them more interpretable than traditional MLPs (Liu et al. 2024)."
  - [corpus] The corpus evidence is weak, with no specific studies directly linking the interpretability of KANs to the success of recommender systems.

## Foundational Learning

- **Concept**: Catastrophic Forgetting
  - **Why needed here**: Understanding catastrophic forgetting is crucial for grasping the problem that CF-KAN aims to solve. It is the phenomenon where neural networks lose previously acquired knowledge when learning new information, which is a common issue in recommender systems that need to adapt to changing user preferences.
  - **Quick check question**: What is catastrophic forgetting, and why is it a problem in recommender systems?

- **Concept**: Kolmogorov-Arnold Networks (KANs)
  - **Why needed here**: KANs are the core innovation of CF-KAN, and understanding their architecture and principles is essential for understanding how CF-KAN works. KANs differ from traditional MLPs by learning nonlinear functions on the edge level, which allows for more localized updates and better handling of catastrophic forgetting.
  - **Quick check question**: How do KANs differ from traditional MLPs, and what are the benefits of this difference?

- **Concept**: Collaborative Filtering
  - **Why needed here**: Collaborative filtering is the fundamental technique used in recommender systems, and understanding its principles is essential for understanding how CF-KAN applies KANs to this domain. Collaborative filtering leverages user-item interactions to provide personalized recommendations, and CF-KAN aims to improve this process by addressing the issue of catastrophic forgetting.
  - **Quick check question**: What is collaborative filtering, and how does it work in recommender systems?

## Architecture Onboarding

- **Component map**: Input layer (user-item interaction matrix) -> Encoder (KAN layers) -> Latent space -> Decoder (KAN layers) -> Output layer (predicted user preferences)

- **Critical path**: The critical path is the flow of data through the encoder and decoder, where the user-item interaction matrix is transformed into a latent representation and then reconstructed to predict user preferences.

- **Design tradeoffs**:
  - Using an autoencoder architecture allows for efficient training and interpretability, but may not capture all the nuances of user-item interactions compared to more complex models like GCNs.
  - The choice of activation functions and regularization terms in the KAN layers can impact the performance and interpretability of the model.

- **Failure signatures**:
  - Poor performance on the test set may indicate that the model is overfitting or underfitting the data.
  - High training time or memory consumption may indicate that the model is too complex or not optimized for the given dataset.
  - Lack of interpretability may indicate that the model is not capturing the relevant patterns in the data.

- **First 3 experiments**:
  1. Evaluate the performance of CF-KAN on a small dataset with known user-item interactions to assess its ability to capture collaborative signals and handle catastrophic forgetting.
  2. Compare the performance of CF-KAN with a traditional MLP-based autoencoder on a larger dataset to quantify the benefits of using KANs.
  3. Analyze the interpretability of CF-KAN by visualizing the learned functions on the edges and tracing the connections to understand the reasoning behind the recommendations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do KAN-based architectures perform compared to MLPs in other domains of recommendation systems, such as sequential or context-aware recommendations?
- Basis in paper: [explicit] The paper suggests that the performance of KANs varies across domains, citing their underperformance in image domains without careful design.
- Why unresolved: The study primarily focuses on collaborative filtering and does not explore KANs in other recommendation contexts.
- What evidence would resolve it: Comparative experiments of KANs against MLPs in sequential and context-aware recommendation settings.

### Open Question 2
- Question: What is the optimal design strategy for KANs to balance performance and interpretability in complex recommendation systems?
- Basis in paper: [inferred] The paper highlights the interpretability of KANs but does not address the trade-offs involved in designing KANs for complex systems.
- Why unresolved: While KANs are interpretable, the balance between performance and interpretability is not explicitly explored.
- What evidence would resolve it: Studies comparing different KAN designs and their impact on both performance and interpretability.

### Open Question 3
- Question: How does the choice of activation functions and basis functions in KANs affect their performance in dynamic recommendation environments?
- Basis in paper: [explicit] The paper discusses the choice of activation functions like SiLU and their impact on performance.
- Why unresolved: The study focuses on a specific choice of activation functions and does not explore the broader implications of different choices.
- What evidence would resolve it: Experiments varying activation and basis functions in KANs across dynamic environments to assess performance impacts.

### Open Question 4
- Question: What are the computational limitations of KANs in large-scale recommendation systems, and how can they be mitigated?
- Basis in paper: [inferred] The paper mentions the scalability of KANs but does not delve into computational limitations in large-scale systems.
- Why unresolved: The study provides insights into scalability but lacks a detailed exploration of computational constraints.
- What evidence would resolve it: Performance and resource usage analyses of KANs in large-scale recommendation systems with various optimization techniques.

## Limitations
- Computational overhead introduced by KANs compared to traditional MLP-based methods
- Interpretability claims require further validation through user studies in real-world scenarios
- Effectiveness of edge-specific learning in preserving knowledge during continual learning needs more extensive evaluation across diverse dynamic environments

## Confidence
- **High Confidence**: Claims about CF-KAN's superior performance on static recommendation tasks (up to 8.2% improvement over baselines)
- **Medium Confidence**: Claims about catastrophic forgetting mitigation in dynamic environments
- **Medium Confidence**: Interpretability and explainability benefits of edge-level learning

## Next Checks
1. Conduct ablation studies to isolate the specific contribution of KAN's edge-specific learning versus other architectural components
2. Perform user studies to validate the practical interpretability and explainability of recommendations
3. Test CF-KAN's performance on additional datasets with varying sparsity levels and dynamic user preferences to assess generalizability