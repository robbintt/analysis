---
ver: rpa2
title: 'Learning K-U-Net with constant complexity: An Application to time series forecasting'
arxiv_id: '2410.02438'
source_url: https://arxiv.org/abs/2410.02438
tags:
- complexity
- time
- learning
- series
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method called exponentially weighted stochastic
  gradient descent with momentum (EW-SGDM) to address inefficiencies in training U-shape
  architectures for time series forecasting. The core idea is to apply exponentially
  increasing weights to gradient updates for higher-level layers in the network, thereby
  prioritizing the learning of high-level features that would otherwise be learned
  much more slowly.
---

# Learning K-U-Net with constant complexity: An Application to time series forecasting

## Quick Facts
- arXiv ID: 2410.02438
- Source URL: https://arxiv.org/abs/2410.02438
- Authors: Jiang You; Arben Cela; RenÃ© Natowicz; Jacob Ouanounou; Patrick Siarry
- Reference count: 6
- Introduces EW-SGDM to achieve constant complexity in U-Net training for time series forecasting

## Executive Summary
This paper addresses the inefficiency of training U-Net architectures for time series forecasting by introducing exponentially weighted stochastic gradient descent with momentum (EW-SGDM). The method prioritizes learning of high-level features through exponentially increasing weights applied to gradient updates for higher layers, leveraging temporal redundancy in overlapping patches during backpropagation. Theoretical analysis demonstrates constant time complexity, and experiments on synthetic time series datasets show significant training time reduction while maintaining or improving accuracy compared to standard optimizers like SGD, SGDM, and Adam.

## Method Summary
The core innovation is EW-SGDM, which modifies traditional stochastic gradient descent with momentum by applying exponentially increasing weights to gradient updates for higher-level layers in the network. This prioritization addresses the slow learning of high-level features that typically occurs in deep architectures. The approach exploits the temporal redundancy present when backpropagation passes through overlapping patches in time series data. By focusing computational resources on the most impactful gradient updates, the method achieves constant time complexity theoretically while maintaining model accuracy.

## Key Results
- EW-SGDM achieves constant time complexity in training U-Nets for time series forecasting
- Significant reduction in training time compared to standard optimizers (SGD, SGDM, Adam)
- Maintains or improves forecasting accuracy on synthetic time series datasets
- Theoretical analysis supports the constant complexity claim through temporal redundancy exploitation

## Why This Works (Mechanism)
The effectiveness of EW-SGDM stems from its ability to prioritize gradient updates for higher-level layers that learn more abstract, high-level features. In traditional training, these layers learn much more slowly than lower layers, creating an efficiency bottleneck. By applying exponentially increasing weights to these higher-layer gradients, EW-SGDM ensures that the most impactful updates receive appropriate computational attention. The method exploits temporal redundancy that occurs when backpropagation processes overlapping patches in time series data, allowing the algorithm to achieve constant complexity by focusing resources where they have the greatest effect on model convergence.

## Foundational Learning
- **Temporal redundancy in time series**: Overlapping patches create repeated patterns during backpropagation - needed to understand the efficiency gains from EW-SGDM
- **U-Net architecture mechanics**: Understanding how feature maps propagate through encoder-decoder structure - critical for grasping why layer-specific weighting matters
- **Stochastic gradient descent with momentum**: Standard optimization framework that EW-SGDM modifies - provides baseline for comparing improvements
- **Exponential weighting schemes**: Mathematical foundation for prioritizing certain gradient updates over others - explains the core innovation mechanism
- **Constant complexity analysis**: Big-O notation and computational complexity theory - required to evaluate the theoretical claims about efficiency

## Architecture Onboarding
- **Component map**: Input time series -> Encoder layers (with exponentially weighted gradients) -> Bottleneck -> Decoder layers (with exponentially weighted gradients) -> Output forecasts
- **Critical path**: Forward pass through U-Net -> Backpropagation with EW-SGDM weighting -> Parameter updates with momentum -> Repeat
- **Design tradeoffs**: The exponential weighting introduces computational overhead but reduces total training iterations - tradeoff between per-iteration cost and convergence speed
- **Failure signatures**: If weighting is too aggressive, model may overshoot optimal parameters; if too conservative, efficiency gains diminish
- **First experiments**:
  1. Train simple U-Net on synthetic periodic time series with standard SGDM vs EW-SGDM
  2. Vary the exponential growth rate of weights to find optimal configuration
  3. Test on non-periodic synthetic data to validate generalizability

## Open Questions the Paper Calls Out
The paper acknowledges several open questions regarding the generalizability of EW-SGDM beyond the specific U-Net architectures and synthetic time series datasets tested. Key uncertainties include whether the theoretical assumptions about temporal redundancy hold for real-world data with irregular sampling or non-stationary dynamics. The constant complexity claim requires further empirical validation across diverse time series characteristics and longer forecasting horizons. Additionally, the computational overhead introduced by the exponential weighting mechanism needs more thorough analysis, particularly for very large-scale problems where memory bandwidth may become a bottleneck.

## Limitations
- Performance gains primarily validated on synthetic datasets rather than real-world time series
- Theoretical constant complexity analysis may not fully account for practical computational overhead
- Limited testing across diverse time series characteristics (seasonality, noise levels, sampling rates)
- Unclear scalability to very large-scale problems with memory bandwidth constraints

## Confidence
- EW-SGDM improves training efficiency: **High** (supported by experiments and theoretical analysis)
- Constant complexity claim: **Medium** (theoretical basis strong but empirical validation limited)
- Generalizability to real-world data: **Low** (tested only on synthetic datasets)

## Next Checks
1. Test EW-SGDM on real-world time series datasets with varying characteristics (seasonality, noise levels, sampling rates) to validate performance claims
2. Conduct ablation studies removing the exponential weighting component to quantify its specific contribution to the observed improvements
3. Analyze computational overhead and memory requirements for large-scale implementations to verify constant complexity scaling in practice