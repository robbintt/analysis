---
ver: rpa2
title: 'Great Memory, Shallow Reasoning: Limits of $k$NN-LMs'
arxiv_id: '2408.11815'
source_url: https://arxiv.org/abs/2408.11815
tags:
- knn-lms
- language
- reasoning
- tasks
- datastore
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether kNN-LMs can improve reasoning abilities
  in addition to improving perplexity. It conducts extensive experiments on 22 datasets
  spanning memory-intensive and reasoning-intensive tasks.
---

# Great Memory, Shallow Reasoning: Limits of $k$NN-LMs

## Quick Facts
- **arXiv ID**: 2408.11815
- **Source URL**: https://arxiv.org/abs/2408.11815
- **Reference count**: 14
- **Primary result**: kNN-LMs improve perplexity and memory-intensive task performance but struggle with reasoning-intensive tasks

## Executive Summary
This paper investigates whether kNN-LMs can improve reasoning abilities beyond their well-documented perplexity improvements. Through extensive experiments on 22 datasets spanning memory-intensive and reasoning-intensive tasks, the authors demonstrate a fundamental disconnect: while kNN-LMs excel at pattern matching tasks and improve language modeling performance, they consistently fail on tasks requiring genuine reasoning. The study reveals that when kNN-LMs produce correct answers on reasoning tasks, these are often the result of spurious correlations rather than actual reasoning ability, challenging the assumption that perplexity improvements translate to better reasoning capabilities.

## Method Summary
The study uses three base language models (Llama-2-7B, Llama-3-8B, Mistral-7B) augmented with kNN-LM extensions. Two datastores are constructed: Wiki (610M tokens from Wikitext103, Amazon Reviews, CC-NEWS, IMDB) and Math (200M tokens from 3.94K math textbooks). The kNN-LM implementation uses FAISS for efficient nearest neighbor search with hidden states as keys and next tokens as values. Hyperparameters (λ ∈ {0.1, 0.2, 0.3}, k ∈ {1600, 2048}, σ ∈ {1, 3, 5, 10}) are optimized through grid search for each datastore. The model is evaluated on 22 datasets covering memory-intensive tasks (sentiment classification, topic classification) and reasoning-intensive tasks (HotpotQA, GSM8K, DROP, ARC, etc.) using perplexity for language modeling and accuracy/F1 for downstream tasks.

## Key Results
- kNN-LMs improve perplexity on Wiki and Math datasets compared to base models
- kNN-LMs significantly improve performance on memory-intensive tasks (sentiment, topic classification)
- kNN-LMs show consistent performance degradation on reasoning-intensive tasks despite perplexity improvement
- Correct answers on reasoning tasks are often driven by spurious correlations rather than genuine reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: kNN-LMs improve perplexity by retrieving similar token patterns that match the current context.
- Mechanism: The model encodes the current token sequence into a hidden representation, searches the datastore for the k nearest neighbor tokens based on Euclidean distance, and linearly interpolates the retrieved distribution with the LM's native distribution using weight λ.
- Core assumption: Similar contexts produce similar next tokens, and retrieving these tokens provides useful probability mass for the correct prediction.
- Evidence anchors:
  - [abstract] "These results have led researchers to argue that models trained on poor quality or outdated data could perform well by employing a kNN extension that has access to a higher-quality datastore."
  - [section] "pkNN(xt+1 | x1:t; D) ∝ Σ_{(ki,vi)∈D} 1xt+1=vi × exp(−d(ki, f(x1:t)))"
- Break condition: The context is too unique or requires reasoning beyond pattern matching, so no useful similar tokens exist in the datastore.

### Mechanism 2
- Claim: kNN-LMs excel at memory-intensive tasks because these tasks can be solved by pattern matching and retrieval of relevant information.
- Mechanism: For tasks like sentiment classification or topic classification, the model retrieves tokens from similar contexts in the datastore that contain the same patterns, allowing it to recognize the task type and predict accordingly.
- Core assumption: Memory-intensive tasks have clear patterns that can be recognized through retrieval of similar contexts.
- Evidence anchors:
  - [abstract] "Results show that kNN-LMs excel at memory-intensive tasks, where utilizing the patterns in the input is sufficient for determining the output"
  - [section] "We begin by looking at a set of memory-intensive tasks, which we believe can be solved by pattern matching at scale without complex reasoning."
- Break condition: The task requires combining multiple pieces of information or performing reasoning that cannot be achieved through simple pattern matching.

### Mechanism 3
- Claim: kNN-LMs hurt reasoning performance because retrieval provides contextually appropriate but semantically incorrect information.
- Mechanism: When the model retrieves tokens for reasoning tasks, it often gets information that fits the context but doesn't answer the question correctly, either because it retrieves high-frequency entities, fails to combine information from multiple sources, or retrieves syntactically similar but semantically different information.
- Core assumption: Reasoning tasks require understanding relationships between concepts, not just finding similar contexts.
- Evidence anchors:
  - [abstract] "we see a different story when applying these models to tasks that require significant reasoning ability"
  - [section] "kNN-LMs struggle with multi-hop reasoning questions...kNN-LMs often retrieve tokens that are contextually appropriate and relevant to part of the question, rather than the correct answer"
- Break condition: The reasoning task requires integrating information from multiple sources or performing logical deductions that cannot be achieved through simple retrieval of similar contexts.

## Foundational Learning

- Concept: Nearest neighbor search in high-dimensional spaces
  - Why needed here: Understanding how FAISS performs approximate nearest neighbor search is crucial for implementing and debugging kNN-LMs
  - Quick check question: What distance metric does the paper use for nearest neighbor search, and why is this choice important?

- Concept: Linear interpolation of probability distributions
  - Why needed here: The kNN-LM combines the retrieved distribution with the LM's native distribution using a weighted average
  - Quick check question: How does changing the interpolation weight λ affect the model's predictions, and what hyperparameter tuning strategy does the paper use?

- Concept: Contextualized token embeddings
  - Why needed here: The model uses the hidden representations before the final MLP layer as keys in the datastore
  - Quick check question: What is the dimensionality of the hidden representations used as keys, and at which layer are they extracted?

## Architecture Onboarding

- Component map: Base language model -> Hidden state extraction -> FAISS similarity search -> kNN distribution computation -> Linear interpolation -> Final prediction

- Critical path: Token generation → Hidden state extraction → Nearest neighbor search → kNN distribution computation → Linear interpolation → Final prediction

- Design tradeoffs:
  - Larger k values provide more information but increase computation and may introduce noise
  - Higher λ values give more weight to retrieval but may override the base model's knowledge
  - Different distance metrics and temperature parameters affect retrieval quality

- Failure signatures:
  - Performance degradation on reasoning tasks despite perplexity improvement
  - Retrieval of high-frequency entities that don't answer the question
  - Failure to combine information from multiple sources for multi-hop reasoning
  - Sensitivity to syntactic similarity over semantic correctness

- First 3 experiments:
  1. Verify that kNN-LM improves perplexity on held-out language modeling datasets (Wiki and Math) compared to base model
  2. Test kNN-LM performance on memory-intensive tasks (sentiment classification, topic classification) to confirm improvements
  3. Evaluate kNN-LM on reasoning tasks (HotpotQA, GSM8K) to observe the performance degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does scaling the size of the datastore beyond 610 million tokens significantly improve kNN-LMs' reasoning capabilities?
- Basis in paper: [inferred] The paper notes limitations in datastore size and suggests that larger datastores built on general web corpora might lead to better reasoning capabilities, but they were constrained by computing budget.
- Why unresolved: The authors did not experiment with larger datastores due to computational constraints, leaving open the question of whether scaling the datastore size could overcome the reasoning limitations observed.
- What evidence would resolve it: Experiments with kNN-LMs using significantly larger datastores (e.g., built on C4 or other large web corpora) and evaluating their performance on reasoning-intensive tasks would provide evidence.

### Open Question 2
- Question: Can training-based retrieval approaches like RAG (Retrieval-Augmented Generation) substantially improve the reasoning performance of kNN-LMs compared to the current similarity-based retrieval?
- Basis in paper: [explicit] The authors suggest that training-based approaches such as RAG have the potential to improve retrieval substantially and address the failure of kNN-LMs to effectively leverage retrieved information for reasoning.
- Why unresolved: The paper only explores similarity-based retrieval with kNN-LMs and does not implement or evaluate training-based retrieval approaches like RAG.
- What evidence would resolve it: Implementing and evaluating kNN-LMs with training-based retrieval approaches like RAG on reasoning-intensive tasks would demonstrate whether such methods can overcome the reasoning limitations.

### Open Question 3
- Question: Are there specific types of reasoning tasks where kNN-LMs might still be beneficial, despite their general limitations in reasoning-intensive tasks?
- Basis in paper: [inferred] While kNN-LMs struggle with complex reasoning tasks, the paper does not exhaustively explore all types of reasoning tasks, leaving open the possibility that kNN-LMs might be beneficial for certain reasoning subtasks.
- Why unresolved: The evaluation covers a range of reasoning tasks but does not exhaustively categorize or explore all possible types of reasoning tasks where kNN-LMs might excel.
- What evidence would resolve it: A comprehensive analysis of kNN-LMs' performance across a broader and more detailed categorization of reasoning tasks would identify specific subtasks where kNN-LMs might still be beneficial.

## Limitations

- The analysis focuses primarily on surface-level patterns in retrieved neighbors without deeper investigation into systematic reasoning failures
- The study doesn't explore whether different datastore construction strategies or alternative retrieval methods might mitigate observed reasoning limitations
- While examining 22 datasets, the distribution of reasoning versus memory-intensive tasks may not be perfectly balanced

## Confidence

### High Confidence Claims
- kNN-LMs consistently improve perplexity on language modeling tasks (Wiki and Math datasets)
- kNN-LMs significantly improve performance on memory-intensive classification tasks
- kNN-LMs show consistent performance degradation on reasoning-intensive tasks
- The disconnect between perplexity improvement and reasoning task performance is real and reproducible

### Medium Confidence Claims
- Spurious correlations drive correct answers in reasoning tasks rather than genuine reasoning
- kNN-LMs cannot effectively combine information from multiple sources for multi-hop reasoning
- High-frequency entity retrieval is a primary failure mode for reasoning tasks
- The observed limitations apply broadly across different base models (Llama-2, Llama-3, Mistral)

### Low Confidence Claims
- kNN-LMs are fundamentally incapable of improving reasoning beyond parametric training
- Different datastore construction or retrieval strategies would not improve reasoning performance
- The observed limitations are inherent to the kNN paradigm rather than implementation-specific

## Next Checks

1. **Ablation study on datastore quality**: Construct datastores with varying quality levels (clean vs. noisy data) and test whether reasoning performance correlates with datastore quality, addressing whether kNN-LMs could overcome poor training data limitations.

2. **Alternative retrieval strategy evaluation**: Implement a re-ranking mechanism that prioritizes semantic similarity over syntactic similarity and test on reasoning tasks to determine if the retrieval mechanism itself is the limiting factor.

3. **Hybrid reasoning architecture**: Design an experiment where kNN-LMs are combined with explicit reasoning modules (e.g., chain-of-thought prompting) to test whether the limitation is fundamental to kNN-LMs or can be overcome through architectural modifications.