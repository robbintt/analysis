---
ver: rpa2
title: 'FedMeS: Personalized Federated Continual Learning Leveraging Local Memory'
arxiv_id: '2404.12710'
source_url: https://arxiv.org/abs/2404.12710
tags: []
core_contribution: This paper introduces FedMeS, a personalized federated continual
  learning framework that addresses client drift and catastrophic forgetting through
  local memory utilization. The method stores task samples in local memory and uses
  them for gradient correction during training and KNN-based Gaussian inference during
  inference.
---

# FedMeS: Personalized Federated Continual Learning Leveraging Local Memory

## Quick Facts
- arXiv ID: 2404.12710
- Source URL: https://arxiv.org/abs/2404.12710
- Reference count: 40
- Key outcome: FedMeS achieves up to 102% accuracy improvement and 4-6x reduction in forgetting rate compared to state-of-the-art methods through local memory utilization and gradient correction.

## Executive Summary
FedMeS addresses the challenge of catastrophic forgetting in federated continual learning by leveraging local memory on client devices. The framework stores task samples locally and uses them to calibrate gradient updates during training, preventing performance degradation on previous tasks. A dynamic regularization parameter adjusts based on local task performance, enabling personalized learning while maintaining global collaboration. The task-oblivious KNN-based Gaussian inference further enhances personalization without requiring task identity information.

## Method Summary
FedMeS implements a personalized federated continual learning framework that combines local memory storage with gradient correction mechanisms. Each client maintains a memory buffer of 150 samples from previous tasks, which is used to calibrate gradient updates during training. The method employs a dynamic regularization parameter λ that adjusts based on the global model's performance on local tasks, with smaller λ values when the global model performs poorly. During inference, FedMeS uses a task-oblivious KNN-based Gaussian approach that combines local model predictions with nearest neighbor inference from memory samples.

## Key Results
- Achieves up to 102% accuracy improvement compared to state-of-the-art federated continual learning methods
- Reduces forgetting rate by 4-6x across various benchmark datasets
- Maintains strong performance across diverse task distributions including cross-domain datasets
- Demonstrates theoretical convergence guarantees under specific assumptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Local memory enables gradient correction to prevent catastrophic forgetting
- Mechanism: When gradient alignment condition fails, project updates to preserve past task performance
- Core assumption: Memory samples adequately represent past task distributions
- Evidence anchors:
  - [abstract] "calibrate gradient updates in training process"
  - [section 4.2] "gradient correction step" with detailed mathematical formulation
  - [corpus] Weak evidence - related papers focus on different personalization mechanisms
- Break condition: Memory becomes stale or unrepresentative of current task distribution

### Mechanism 2
- Claim: Dynamic regularization parameter λ facilitates personalization
- Mechanism: λ adjusts based on global model performance on local task - smaller λ when global model performs poorly, larger λ when it performs well
- Core assumption: Global model provides useful information about current task difficulty
- Evidence anchors:
  - [abstract] "loss-based dynamic regularization parameter"
  - [section 4.2] Formula λ = 2 · sigmoid(1/L(w, Dtk)) with explicit reasoning
  - [corpus] No direct evidence in related papers about loss-based dynamic regularization
- Break condition: Task distributions become too heterogeneous for global model to provide meaningful signal

### Mechanism 3
- Claim: Task-oblivious KNN-based Gaussian inference enhances personalization
- Mechanism: Uses stored memory representations to perform nearest-neighbor inference without knowing task identity
- Core assumption: Memory contains sufficient diverse samples to cover task space
- Evidence anchors:
  - [abstract] "KNN-based Gaussian inference to facilitate personalization"
  - [section 4.3] Detailed KNN inference process with Euclidean distance and Gaussian kernel
  - [corpus] Related papers focus on other personalization approaches, no KNN-based inference found
- Break condition: Memory becomes too small or unrepresentative to provide meaningful nearest neighbors

## Foundational Learning

- Concept: Federated Learning fundamentals
  - Why needed here: Understanding how global aggregation and local training interact in distributed setting
  - Quick check question: How does FedAvg differ from FedMeS in handling client drift?

- Concept: Continual Learning and catastrophic forgetting
  - Why needed here: Core problem FedMeS addresses - maintaining performance across evolving tasks
  - Quick check question: What makes continual learning particularly challenging in federated settings?

- Concept: Gradient alignment and projection
  - Why needed here: Understanding the mathematical basis for gradient correction mechanism
  - Quick check question: How does the inner product condition ⟨∇L(wtk; Dtk), ∇L(wtk; Mtk)⟩ ≥ 0 relate to preventing forgetting?

## Architecture Onboarding

- Component map: Server aggregates global model -> Clients store local memory and perform gradient correction -> KNN inference uses memory representations
- Critical path: Local training with memory → gradient correction → global aggregation → KNN inference enhancement
- Design tradeoffs: Memory size vs. computational cost vs. forgetting prevention
- Failure signatures: Client drift appears as accuracy degradation on previous tasks, forgetting shows as declining performance on older tasks
- First 3 experiments:
  1. Verify gradient correction works by testing on synthetic task sequences with known optimal solution
  2. Test memory size impact on forgetting rate across different dataset splits
  3. Compare task-oblivious inference performance against task-aware baselines

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions, but several important areas remain unexplored based on the content provided.

## Limitations

- Convergence analysis relies on strong assumptions (L-smoothness, bounded gradient dissimilarity) that may not hold in practical federated settings
- Fixed memory size of 150 samples may be insufficient for highly diverse task distributions
- Dynamic regularization parameter performance depends heavily on global model quality in heterogeneous federated settings
- Theoretical analysis does not account for communication constraints or overhead of maintaining local memory

## Confidence

- High confidence in the mechanism of local memory for gradient correction and inference enhancement
- Medium confidence in the dynamic regularization parameter approach due to dependency on global model quality
- Medium confidence in theoretical convergence guarantees given strong assumptions
- Low confidence in scalability to real-world federated settings with thousands of clients and highly heterogeneous data

## Next Checks

1. Test FedMeS with varying memory sizes (50-500 samples) to determine the minimum effective memory capacity for preventing catastrophic forgetting across different task heterogeneity levels.
2. Evaluate performance under realistic communication constraints by limiting the number of aggregation rounds and measuring the impact on both accuracy and forgetting rate.
3. Validate the convergence analysis empirically by measuring gradient alignment conditions ⟨∇L(wtk; Dtk), ∇L(wtk; Mtk)⟩ across training epochs and comparing with theoretical predictions.