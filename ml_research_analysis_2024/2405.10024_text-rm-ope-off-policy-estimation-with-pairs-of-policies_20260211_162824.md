---
ver: rpa2
title: "$\u0394\\text{-}{\\rm OPE}$: Off-Policy Estimation with Pairs of Policies"
arxiv_id: '2405.10024'
source_url: https://arxiv.org/abs/2405.10024
tags:
- policy
- learning
- conference
- proceedings
- off-policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of high variance in off-policy\
  \ estimation (OPE) methods for recommendation systems. The authors propose a new\
  \ approach called \u0394-OPE, which focuses on estimating the difference in policy\
  \ values rather than individual policy values."
---

# $Δ\text{-}{\rm OPE}$: Off-Policy Estimation with Pairs of Policies

## Quick Facts
- arXiv ID: 2405.10024
- Source URL: https://arxiv.org/abs/2405.10024
- Authors: Olivier Jeunen; Aleksei Ustimenko
- Reference count: 40
- One-line primary result: Δ-OPE methods significantly improve off-policy estimation variance and statistical power for recommendation systems through pairwise policy comparison.

## Executive Summary
This paper introduces Δ-OPE, a novel framework for off-policy estimation that focuses on estimating the difference in policy values rather than individual policy values. The key insight is that when two policies have positive covariance, their difference can be estimated with significantly reduced variance. The authors develop a family of estimators (Δ-IPS, Δ-SNIPS, Δβ-IPS) that leverage this property, along with variance-optimal additive control variates. Extensive experiments demonstrate substantial improvements in both evaluation and learning tasks, with large-scale A/B tests showing statistically significant gains in retention, engaging users, and learnt reward metrics.

## Method Summary
The Δ-OPE framework reframes off-policy estimation as a pairwise comparison between two policies (typically production and target) rather than individual policy value estimation. Starting from Inverse Propensity Scoring (IPS), the authors derive estimators for the difference in policy values that exploit positive covariance between policies to reduce variance. They introduce variance-optimal additive control variates (β*) that further enhance efficiency, and extend the approach to multiplicative control variates with Δ-SNIPS. The framework is compatible with Counterfactual Risk Minimization (CRM) optimization and can be used for both policy evaluation and learning.

## Key Results
- Δ-OPE estimator family consistently outperforms traditional OPE methods in simulation experiments
- Δβ-IPS exhibits lowest mean squared error, tightest confidence intervals, and highest statistical power
- Large-scale online A/B experiments show statistically significant improvements in retention, engaging users, and learnt reward
- The variance-optimal additive control variate (β*) provides substantial efficiency gains over baseline Δ-IPS

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The difference in policy values can be estimated with reduced variance when the two policies have positive covariance.
- Mechanism: When two policies are positively correlated, their reward distributions tend to move together. This positive correlation creates a cancellation effect in the variance formula, reducing the overall estimation variance.
- Core assumption: The production policy (πp) and target policy (πt) exhibit positive covariance in their reward distributions.
- Evidence anchors:
  - [abstract] "An important insight is that the difference between policy values can often be estimated with significantly reduced variance, if said policies have positive covariance."
  - [section 2.1] "The advantages of the Δ-OPE framing become clear when we consider a decomposition of its variance: Var(VΔ(πt, πp)) = Var(V(πt)) + Var(V(πp)) - 2·Covar(V(πt); V(πp))"
  - [corpus] Weak evidence - the corpus contains related OPE papers but none specifically discuss covariance-based variance reduction.
- Break condition: If the production and target policies have negative or zero covariance, the variance reduction effect disappears and may even increase variance.

### Mechanism 2
- Claim: Pairwise off-policy estimation (Δ-OPE) subsumes the common use-case of estimating improvements over a production policy.
- Mechanism: By framing the estimation task as a difference between two policy values rather than estimating each individually, we can leverage data from the logging policy to estimate both values simultaneously, creating efficiency gains.
- Core assumption: The logging policy (π0) has sufficient overlap with both the production policy (πp) and target policy (πt) to enable reliable estimation of both values.
- Evidence anchors:
  - [abstract] "Δ-OPE subsumes the common use-case of estimating improvements of a learnt policy over a production policy, using data collected by a stochastic logging policy."
  - [section 2.1] "As such, if the traditional IPS assumptions of common support and unconfoundedness hold [23, 37], we can derive unbiased estimators for this quantity."
  - [corpus] Weak evidence - related papers discuss OPE but don't explicitly frame it as pairwise estimation over production and target policies.
- Break condition: If the logging policy has poor overlap with either the production or target policy, the estimator becomes unreliable due to importance sampling weights becoming too large.

### Mechanism 3
- Claim: The variance-optimal additive control variate (β*) for Δ-IPS can be derived analytically and provides estimation-error-optimality.
- Mechanism: By introducing an additive control variate, we can reduce variance while maintaining unbiasedness. The optimal value of β is derived by minimizing the variance expression, leading to a specific formula based on the squared propensity ratios.
- Core assumption: The optimal β can be estimated from empirical samples without introducing significant bias.
- Evidence anchors:
  - [section 2.1.3] "We can write the variance of this estimator as... Considering minimisation of variance as a function of β, we have... Solving for the derivative to equal 0, yields the optimal baseline: β* = E[...]"
  - [abstract] "Moreover, we characterise a variance-optimal additive control variate that further enhances efficiency."
  - [corpus] Weak evidence - the corpus contains OPE papers but none discuss the specific derivation of optimal additive control variates for pairwise estimation.
- Break condition: If the logging policy has very different behavior from both production and target policies, the empirical estimate of β* may be poor, reducing the effectiveness of the control variate.

## Foundational Learning

- Concept: Inverse Propensity Scoring (IPS) and importance sampling
  - Why needed here: Δ-OPE methods are built upon IPS foundations, extending them to pairwise estimation. Understanding how IPS works is crucial for grasping the variance decomposition and control variate mechanisms.
  - Quick check question: What is the key assumption that makes IPS an unbiased estimator, and how does this relate to the "common support" condition?

- Concept: Variance decomposition and covariance effects
  - Why needed here: The core insight of Δ-OPE relies on understanding how covariance between two random variables affects the variance of their difference. This requires familiarity with variance formulas and covariance properties.
  - Quick check question: Given two random variables X and Y, write the formula for the variance of (X - Y) and explain how positive covariance between X and Y affects this variance.

- Concept: Control variates and variance reduction techniques
  - Why needed here: The paper introduces both multiplicative (SNIPS) and additive (β-IPS) control variates for variance reduction. Understanding how control variates work is essential for grasping the improvements over vanilla IPS.
  - Quick check question: What is the fundamental requirement for a control variate to be effective, and how does this requirement manifest differently for multiplicative versus additive control variates?

## Architecture Onboarding

- Component map: Data collection (π0) -> Policy evaluation (Δ-OPE) -> Policy improvement (CRM optimization) -> A/B testing
- Critical path: Data collection (π0) → Policy evaluation (Δ-OPE) → Policy improvement (CRM optimization) → A/B testing
- Design tradeoffs:
  - IPS vs SNIPS vs β-IPS: Unbiasedness (IPS best) vs variance (β-IPS best) vs asymptotic properties (SNIPS has issues)
  - Single policy vs pairwise estimation: Simplicity (single) vs variance reduction (pairwise)
  - Empirical β estimation: Simplicity (fixed β) vs optimality (learned β)
- Failure signatures:
  - High variance estimates: Likely caused by poor overlap between π0 and πt/πp
  - Negative improvement estimates when positive expected: Possible high variance or poor covariance
  - A/B test failures despite positive Δ-OPE estimates: Potential distribution shift or unmodeled confounding
- First 3 experiments:
  1. Implement basic Δ-IPS estimator and verify it matches vanilla IPS when πp = π0 (baseline correction equivalence)
  2. Run synthetic experiments with controlled covariance between policies to demonstrate variance reduction effect
  3. Implement β estimation and compare variance reduction against fixed β = 0 baseline

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- The variance reduction mechanism relies heavily on the assumption of positive covariance between production and target policies, which may not hold in all recommendation scenarios
- The framework is currently limited to contextual bandit settings and doesn't address multi-step decision processes or full RL environments
- The online A/B experiments demonstrate success on a single platform (short-video) with limited metrics, limiting generalizability to other domains

## Confidence
- Variance reduction mechanism (covariance effect): Medium
- Unbiasedness of Δ-IPS estimator: High
- Statistical power improvements: Medium
- Online A/B test results: Low (single platform, limited metrics)

## Next Checks
1. Conduct sensitivity analysis on the covariance assumption by testing Δ-OPE with artificially constructed policy pairs having varying correlation structures
2. Implement robust estimation techniques for β* that account for heavy-tailed propensity ratios and compare against the proposed empirical estimator
3. Design offline simulation studies that systematically vary logging policy behavior to test the robustness of Δ-OPE estimators under different support conditions