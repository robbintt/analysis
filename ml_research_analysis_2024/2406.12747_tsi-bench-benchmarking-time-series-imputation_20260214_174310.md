---
ver: rpa2
title: 'TSI-Bench: Benchmarking Time Series Imputation'
arxiv_id: '2406.12747'
source_url: https://arxiv.org/abs/2406.12747
tags:
- time
- missing
- imputation
- series
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TSI-Bench, the first comprehensive benchmark
  suite for evaluating time series imputation using deep learning techniques. The
  benchmark addresses the lack of standardized evaluation platforms by providing 28
  algorithms across 8 datasets from diverse domains (air quality, traffic, electricity,
  healthcare) with various missingness scenarios.
---

# TSI-Bench: Benchmarking Time Series Imputation

## Quick Facts
- arXiv ID: 2406.12747
- Source URL: https://arxiv.org/abs/2406.12747
- Reference count: 40
- Primary result: First comprehensive benchmark suite for time series imputation using deep learning techniques across 28 algorithms and 8 datasets

## Executive Summary
TSI-Bench introduces the first comprehensive benchmark suite for evaluating time series imputation using deep learning techniques. The benchmark addresses the critical gap in standardized evaluation platforms by providing 28 algorithms across 8 datasets from diverse domains including air quality, traffic, electricity, and healthcare. The study conducts extensive experiments (34,804 total) to systematically compare imputation algorithms under various missingness scenarios and data processing methods.

The research reveals that no single imputation algorithm consistently outperforms others across all settings, emphasizing the importance of model tuning and domain-specific data processing. Forecasting architectures demonstrate surprising effectiveness when adapted for imputation tasks, and the study shows that imputation significantly enhances downstream task performance when combined with appropriate data processing techniques.

## Method Summary
The benchmark suite employs a systematic framework that adapts forecasting models for imputation tasks through specialized encoders and decoders. The evaluation encompasses 28 algorithms tested across 8 diverse datasets with varying missingness patterns including MCAR, MAR, MNAR, and block missing scenarios. Each algorithm undergoes rigorous testing with multiple hyperparameters and data processing methods, resulting in 34,804 total experiments. The benchmark establishes standardized experimental settings including consistent evaluation metrics (MAE, RMSE, NRMSE, MAPE) and downstream task assessments (classification, regression, forecasting) to ensure fair comparisons.

## Key Results
- No single imputation algorithm outperforms others across all settings, highlighting the importance of model tuning
- Forecasting architectures show effectiveness when adapted for imputation tasks
- Imputation significantly improves downstream task performance (classification, regression, forecasting) when combined with domain-informed data processing

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its comprehensive coverage of algorithms, datasets, and missingness patterns, enabling robust performance comparisons. The systematic adaptation of forecasting models for imputation tasks leverages temporal dependencies effectively, while standardized evaluation protocols ensure fair comparisons. The integration of downstream task performance evaluation provides practical validation of imputation quality beyond traditional metrics.

## Foundational Learning
- **Time series imputation fundamentals**: Understanding how missing values affect temporal dependencies and the importance of preserving temporal relationships
  - Why needed: Critical for evaluating imputation quality and downstream task performance
  - Quick check: Can the algorithm reconstruct missing values while maintaining temporal patterns?

- **Missingness mechanisms (MCAR, MAR, MNAR)**: Different patterns of missing data affect algorithm performance differently
  - Why needed: Enables testing under realistic scenarios and proper algorithm selection
  - Quick check: Does the algorithm handle various missingness patterns effectively?

- **Forecasting-to-imputation adaptation**: Converting forecasting models to work effectively for imputation tasks
  - Why needed: Leverages existing powerful forecasting architectures for imputation
  - Quick check: Can the adapted model accurately reconstruct missing temporal values?

## Architecture Onboarding

Component map: Data Processing -> Imputation Algorithm -> Evaluation Metrics -> Downstream Tasks

Critical path: Raw time series data → Preprocessing → Missing value injection → Imputation → Performance evaluation → Downstream task assessment

Design tradeoffs:
- Comprehensive coverage vs. computational cost (34,804 experiments required)
- Standardized protocols vs. domain-specific customization needs
- Forecasting adaptation simplicity vs. potential performance limitations

Failure signatures:
- Poor performance on specific missingness patterns (e.g., block missing)
- Degradation in downstream task performance despite good imputation metrics
- Sensitivity to hyperparameter choices indicating lack of robustness

First experiments:
1. Test simple statistical methods (mean, median) on single dataset with varying missingness rates
2. Evaluate basic forecasting models (LSTM, Transformer) adapted for imputation on one domain
3. Compare multiple algorithms on a small subset of datasets with different missingness patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Results may not generalize to all real-world time series domains beyond publicly available datasets
- Adaptation of forecasting models for imputation may introduce performance biases not fully explored
- Extensive computational requirements (34,804 experiments) may limit reproducibility in resource-constrained settings

## Confidence
- High confidence in systematic framework and benchmark infrastructure
- Medium confidence in cross-domain algorithm performance comparisons
- Medium confidence in effectiveness of forecasting models for imputation
- Low confidence in scalability to extremely large-scale or high-frequency time series data

## Next Checks
1. Evaluate algorithm performance on proprietary or industrial datasets with domain-specific characteristics to assess real-world applicability
2. Conduct computational efficiency analysis comparing resource requirements across different hardware configurations
3. Test robustness of imputation methods under extreme missingness patterns (e.g., consecutive block missing values exceeding 50%) not covered in current benchmark