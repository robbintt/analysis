---
ver: rpa2
title: Attention as an RNN
arxiv_id: '2405.13956'
source_url: https://arxiv.org/abs/2405.13956
tags:
- time
- attention
- series
- forecasting
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Aaren, a novel attention-based module that
  achieves comparable performance to Transformers while being more time and memory-efficient.
  Aaren leverages a new formulation of attention as a Recurrent Neural Network (RNN)
  based on the parallel prefix scan algorithm, allowing it to be both trained in parallel
  and updated efficiently with new tokens at inference time.
---

# Attention as an RNN

## Quick Facts
- **arXiv ID**: 2405.13956
- **Source URL**: https://arxiv.org/abs/2405.13956
- **Reference count**: 40
- **Primary result**: Aaren achieves Transformer-level performance on 38 datasets while using constant memory at inference

## Executive Summary
This paper presents Aaren, a novel attention-based module that achieves comparable performance to Transformers while being more time and memory-efficient. Aaren leverages a new formulation of attention as a Recurrent Neural Network (RNN) based on the parallel prefix scan algorithm, allowing it to be both trained in parallel and updated efficiently with new tokens at inference time. The authors demonstrate that Aaren achieves competitive results with Transformers on 38 datasets across four sequential problem settings: reinforcement learning, event forecasting, time series classification, and time series forecasting.

## Method Summary
Aaren reformulates attention as an RNN using recurrence relations over cumulative max and sum terms, enabling constant-memory sequential processing. During training, it uses parallel prefix scan algorithms to compute attention outputs efficiently, while at inference it processes tokens sequentially in constant memory. The method uses learned query tokens instead of input-dependent queries, introducing only a 0.016% parameter increase compared to equivalent Transformers. Aaren maintains the same interface as Transformers but leverages the RNN formulation internally, achieving linear computational growth with sequence length versus Transformers' quadratic growth.

## Key Results
- Aaren achieves competitive performance with Transformers on 38 datasets across four sequential problem settings
- Uses only constant memory for inferences compared to Transformers' linear memory usage with KV-caching
- Cumulative computation time grows linearly rather than quadratically with sequence length
- Introduces only a 0.016% parameter increase compared to equivalent Transformers

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Attention can be reformulated as an RNN, enabling constant-memory sequential processing.
- **Mechanism**: Attention's softmax computation can be expressed as a recurrence relation over cumulative max and sum terms, allowing processing tokens sequentially in O(1) memory instead of O(N).
- **Core assumption**: The numerical stability trick of subtracting the max term can be extended to a cumulative max for recurrent computation.
- **Evidence anchors**: Abstract states attention can be viewed as a special RNN; Section 3.1 introduces the RNN cell that iteratively computes attention output.
- **Break condition**: If cumulative max computation becomes numerically unstable for very large or small values, the recurrence could fail.

### Mechanism 2
- **Claim**: Parallel prefix scan enables efficient many-to-many RNN computation.
- **Mechanism**: Uses parallel prefix scan algorithm with a custom associative operator acting on 3-tuples (max score, sum of exponentials, weighted sum) to compute attention outputs for all positions in O(log N) depth.
- **Core assumption**: The proposed operator is associative and correctly computes required prefix sums.
- **Evidence anchors**: Abstract mentions new method based on parallel prefix scan; Section 3.2 describes computing prefix sums via parallel scan algorithm.
- **Break condition**: If the operator is not truly associative, the parallel scan algorithm would produce incorrect results.

### Mechanism 3
- **Claim**: Aaren achieves Transformer-level performance while requiring only constant memory at inference.
- **Mechanism**: Combines RNN reformulation of attention with learned query tokens, allowing parallel training like Transformers but efficient sequential updates at inference time.
- **Core assumption**: Using learned query tokens instead of input-dependent queries doesn't significantly impact model expressiveness or performance.
- **Evidence anchors**: Abstract states Aaren can be trained in parallel but updated efficiently with new tokens; Section 3.3 explains using learned query tokens via backpropagation.
- **Break condition**: If learned query tokens cannot adapt sufficiently to different input patterns, performance may degrade.

## Foundational Learning

- **Concept**: Parallel prefix scan algorithms
  - **Why needed**: The paper relies on parallel prefix scan to efficiently compute the many-to-many RNN formulation of attention
  - **Quick check**: What is the time complexity of computing prefix sums for N elements using parallel prefix scan algorithms?

- **Concept**: Numerical stability in softmax computation
  - **Why needed**: The RNN reformulation requires stable computation of softmax-like operations through recurrence relations
  - **Quick check**: Why is subtracting the maximum value from softmax inputs a common numerical stability trick?

- **Concept**: Recurrent Neural Networks vs Transformers
  - **Why needed**: Understanding key differences in memory and computational requirements between RNNs and Transformers is crucial for appreciating Aaren's advantages
  - **Quick check**: What is the memory complexity of Transformers with KV-caching versus traditional RNNs?

## Architecture Onboarding

- **Component map**: Input tokens -> Stacked Aaren modules -> Output tokens
- **Critical path**:
  1. Input tokens are processed through stacked Aaren modules
  2. During training: Parallel prefix scan computes attention outputs for all positions
  3. During inference: RNN recurrence updates state with each new token in constant memory
  4. Outputs are generated for sequence modeling tasks

- **Design tradeoffs**:
  - Learned query vectors vs input-dependent queries: Simpler architecture but potentially less expressive
  - Constant memory at inference vs parallel training: Good for deployment but may limit some optimizations
  - Exact softmax reformulation vs linear approximations: More accurate but potentially more computationally intensive

- **Failure signatures**:
  - Numerical instability in recurrence: Check for exploding or vanishing values in a_k, c_k, m_k
  - Poor performance compared to Transformers: May indicate learned queries are insufficient
  - Training instability: Could suggest issues with parallel prefix scan implementation

- **First 3 experiments**:
  1. Implement basic RNN attention cell and verify it produces same outputs as standard attention
  2. Test parallel prefix scan implementation on small sequences to verify associativity and correctness
  3. Compare memory usage and inference speed of Aaren vs Transformer on a simple sequence task

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided text.

## Limitations
- The constant-memory inference advantage depends on sequential token-by-token processing, which may not match practical deployment scenarios requiring batch processing
- Performance comparison is based on substituting Aaren modules into existing models rather than designing architectures specifically optimized for Aaren
- The paper doesn't address potential latency or throughput trade-offs compared to batched Transformer inference

## Confidence
- **High confidence**: The mathematical formulation of attention as an RNN using cumulative max and sum operations is rigorous and well-supported
- **Medium confidence**: The claim of achieving "competitive results" with Transformers appears well-supported by experimental results, though lacks statistical significance testing
- **Medium confidence**: The assertion of constant memory at inference time is theoretically sound but depends on sequential processing assumptions that may not reflect practical deployment

## Next Checks
1. **Numerical stability verification**: Implement the RNN recurrence relations (a_k, c_k, m_k) and test with sequences containing extreme values to verify that the cumulative max computation remains stable across a wide range of input magnitudes.

2. **Parallel prefix scan correctness**: Construct test cases with small sequences where the attention outputs can be computed both by direct matrix multiplication and by the proposed parallel prefix scan algorithm to verify they produce identical results.

3. **Learned query vector expressiveness**: Conduct ablation studies where Aaren is tested with both learned queries and input-dependent queries on a subset of tasks to quantify the performance impact of this architectural choice.