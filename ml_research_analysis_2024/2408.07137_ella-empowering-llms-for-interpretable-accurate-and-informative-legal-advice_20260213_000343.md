---
ver: rpa2
title: 'ELLA: Empowering LLMs for Interpretable, Accurate and Informative Legal Advice'
arxiv_id: '2408.07137'
source_url: https://arxiv.org/abs/2408.07137
tags:
- legal
- users
- articles
- response
- article
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes ELLA, a tool that enhances legal Large Language
  Models (LLMs) by providing interpretability, accuracy, and informative legal advice.
  ELLA addresses the issue of LLMs generating incorrect or baseless responses by visually
  presenting the correlation between legal articles and the LLM's response, allowing
  users to interactively select relevant legal articles for more accurate responses,
  and retrieving relevant legal cases for user reference.
---

# ELLA: Empowering LLMs for Interpretable, Accurate and Informative Legal Advice

## Quick Facts
- arXiv ID: 2408.07137
- Source URL: https://arxiv.org/abs/2408.07137
- Authors: Yutong Hu; Kangcheng Luo; Yansong Feng
- Reference count: 11
- One-line primary result: ELLA improves LLM response accuracy through user intervention in legal article selection, achieving 76.34 NDCG for case retrieval

## Executive Summary
ELLA addresses a critical challenge in legal AI systems: the tendency of large language models to generate inaccurate or baseless responses when processing complex legal queries. The system enhances legal LLMs by providing visual correlation between legal articles and responses, enabling interactive user selection of relevant legal articles, and retrieving supporting legal cases. Through a user study with 20 consultation queries about complex marriage situations, ELLA demonstrates that user intervention in selecting legal articles significantly improves LLM response accuracy while also increasing user understanding through transparent legal basis presentation.

## Method Summary
ELLA employs a multi-component approach to enhance legal LLM performance. The system fine-tunes BGE embedding models on legal corpora for both legal article retrieval (LeCaRD dataset) and case retrieval, using cosine similarity thresholds (0.85 for articles, 0.65 for cases) to identify relevant content. Users interact with an interface that displays top-10 retrieved legal articles, with the LLM defaulting to the top-3 for response generation. Users can regenerate responses based on their selected articles. The system provides sentence-level response interpretation by calculating similarities between LLM response sentences and legal articles, and highlights key sentences in retrieved legal cases based on query similarity. A legal case retrieval model achieves 76.34 NDCG@10 score, demonstrating effective case retrieval capabilities.

## Key Results
- User study shows LLM response accuracy improves when users intervene in selecting legal articles
- System achieves NDCG@10 score of 76.34 for legal case retrieval model
- Visual presentation of legal basis increases user understanding of LLM responses
- Interactive article selection reduces noise from irrelevant legal articles in LLM inputs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** User selection of legal articles improves LLM response accuracy by reducing noise from irrelevant articles.
- **Mechanism:** The retrieval model may return both relevant and irrelevant legal articles. When irrelevant articles are included in the input to the LLM, they introduce noise that can degrade the quality of the response. By allowing users to manually select only the relevant articles, the input to the LLM is cleaned, leading to more accurate and complete responses.
- **Core assumption:** Users can reliably distinguish between relevant and irrelevant legal articles based on their query and legal knowledge.
- **Evidence anchors:**
  - [abstract] "The accuracy of LLM’s responses also improves when users intervene in selecting legal articles."
  - [section 2.2] "Users can interactively select legal articles for LLM to generate more accurate responses while disregarding irrelevant ones to avoid noise."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.429. Weak evidence for specific claim about user selection improving accuracy; mostly general legal LLM literature.
- **Break condition:** If users lack sufficient legal knowledge to accurately select relevant articles, the mechanism fails and may even degrade response quality by excluding necessary information.

### Mechanism 2
- **Claim:** Presenting legal article basis for each LLM response sentence increases user trust and understanding.
- **Mechanism:** By calculating the similarity between each sentence in the LLM's response and legal articles, the system can identify which legal articles support each statement. Displaying this correlation visually helps users verify that the LLM's advice is grounded in actual legal text rather than being hallucinated. This transparency allows users to trace claims back to their legal foundations.
- **Core assumption:** The similarity calculation between response sentences and legal articles accurately captures the legal basis relationship.
- **Evidence anchors:**
  - [abstract] "ELLA visually presents the correlation between legal articles and LLM's response by calculating their similarities, providing users with an intuitive legal basis for the responses."
  - [section 3.3] "We use BGE1 to calculate the cosine similarity between the embedding of each sentence in the response and the legal articles and judicial interpretations."
  - [corpus] Found 25 related papers (using 8). Weak direct evidence for sentence-level legal basis presentation; most related work focuses on general legal LLM improvements.
- **Break condition:** If the similarity calculation is inaccurate or if the legal articles themselves are incomplete or outdated, users may be misled about the actual legal basis.

### Mechanism 3
- **Claim:** Highlighting relevant sentences in retrieved legal cases improves user reading efficiency and comprehension.
- **Mechanism:** Legal cases contain lengthy text, but only certain sentences are directly relevant to a user's query. By calculating the similarity between the query and each sentence in a legal case, the system can identify and highlight key sentences. This allows users to quickly locate the most pertinent information without reading through the entire case document.
- **Core assumption:** Sentence-level similarity calculation accurately identifies the most relevant portions of legal cases.
- **Evidence anchors:**
  - [abstract] "We find all the key sentences in the article through similarity matching between the query and each sentence in the legal case. We highlight all key sentences in the legal cases for users to improve their reading efficiency."
  - [section 3.4] "We use BGE2 to calculate the similarity between the user's query and each sentence in the legal case. When the cosine similarity score is larger than T hr2, we consider this sentence to be related to the user's query."
  - [corpus] Found 25 related papers (using 8). Weak evidence for sentence highlighting specifically; most related work focuses on legal case retrieval rather than presentation.
- **Break condition:** If the similarity threshold is set too low, too many sentences will be highlighted, reducing the efficiency gain. If set too high, important context may be missed.

## Foundational Learning

- **Concept:** Legal article retrieval and ranking
  - Why needed here: The entire system depends on retrieving relevant legal articles to provide context for the LLM. Understanding how retrieval models work and how to evaluate their effectiveness is fundamental.
  - Quick check question: What metric would you use to evaluate whether a legal article retrieval model is returning relevant articles for a given query?

- **Concept:** Embedding similarity and cosine similarity
  - Why needed here: The system uses embedding models (BGE) to calculate similarity between text passages. Understanding how embeddings capture semantic meaning and how cosine similarity measures their relationship is crucial for implementing the response interpretation and case highlighting features.
  - Quick check question: If two legal articles have embeddings with a cosine similarity of 0.95, what does this tell you about their relationship?

- **Concept:** Interactive UI design for legal consultation
  - Why needed here: The system provides an interactive interface where users can select legal articles and view explanations. Understanding user experience principles for legal tools, including how to present complex legal information accessibly, is important for effective implementation.
  - Quick check question: What information would you prioritize displaying to a user who is unfamiliar with legal terminology when showing them a legal article?

## Architecture Onboarding

- **Component map:** User query → Legal article retrieval → LLM response generation → Response interpretation (sentence-level similarity) → Legal case retrieval → User interface display
- **Critical path:** User query → Legal article retrieval → LLM response generation → Response interpretation (sentence-level similarity) → Legal case retrieval → User interface display
- **Design tradeoffs:** The system trades off computational cost (running multiple similarity calculations and retrieval models) for improved accuracy and user trust. The interactive selection feature adds complexity but provides significant accuracy improvements when the retrieval model fails.
- **Failure signatures:** 
  - LLM responses lack legal basis sentences (similarity threshold too high or legal articles incomplete)
  - Legal case highlighting includes irrelevant sentences (similarity threshold too low)
  - User selection doesn't improve accuracy (users lack legal knowledge or interface is confusing)
  - System is slow (multiple retrieval and similarity calculations running sequentially)
- **First 3 experiments:**
  1. Test the sentence-level similarity calculation by taking known LLM responses with clear legal bases and verifying the system correctly identifies the supporting articles.
  2. Evaluate the legal case highlighting by checking whether the highlighted sentences actually contain the key legal reasoning for a sample of cases.
  3. Measure the accuracy improvement from user selection by comparing LLM responses generated with automatically retrieved articles versus user-selected articles on a test set of queries.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ELLA's performance vary across different types of complex legal queries beyond marriage consultation?
- Basis in paper: [explicit] The paper mentions ELLA primarily assists with complex legal consultation queries and uses marriage consultation as an example, but does not extensively evaluate other legal domains.
- Why unresolved: The user study focused on 20 consultation queries about complex marriage situations. The paper does not provide data on ELLA's effectiveness for other types of legal queries such as criminal law, property disputes, or contract law.
- What evidence would resolve it: Conducting user studies with a diverse set of legal query types across different legal domains and comparing ELLA's performance metrics (accuracy, user satisfaction) across these domains.

### Open Question 2
- Question: What is the optimal number of legal articles to display for user selection to balance comprehensiveness and cognitive load?
- Basis in paper: [explicit] The paper states that ELLA displays the top K1 = 10 relevant legal articles to users, but does not justify why this number was chosen or explore the impact of different numbers.
- Why unresolved: The paper does not investigate whether displaying more or fewer articles would improve user experience or LLM response accuracy. There may be a trade-off between providing sufficient options and overwhelming users.
- What evidence would resolve it: User studies comparing ELLA's performance with different numbers of displayed articles (e.g., 5, 10, 15, 20) measuring both user selection accuracy and satisfaction ratings.

### Open Question 3
- Question: How does the fine-tuning approach for BGE embeddings compare to other legal domain adaptation methods in terms of retrieval accuracy and computational efficiency?
- Basis in paper: [explicit] The paper describes fine-tuning BGE on legal corpus for both legal article retrieval and case retrieval, but does not compare this approach to alternatives like training a legal-specific embedding model from scratch or using domain-specific pre-training.
- Why unresolved: While the paper shows BGE fine-tuning improves over baseline BGE, it does not benchmark against other adaptation strategies or discuss the computational costs of fine-tuning versus alternative approaches.
- What evidence would resolve it: Comparative analysis of ELLA's fine-tuned BGE against: (1) BGE fine-tuned on larger legal corpora, (2) Legal-specific embedding models trained from scratch, (3) Domain-adaptive pre-training approaches, measuring both retrieval accuracy and computational resources required.

## Limitations
- User study sample size is small (20 queries) and focused primarily on marriage-related legal queries, limiting generalizability to other legal domains
- Effectiveness of user selection heavily depends on users' legal knowledge, which wasn't systematically evaluated
- Paper doesn't address what happens when legal articles themselves are outdated or when there are conflicting interpretations across different legal sources

## Confidence

**High confidence** in the core mechanism that user selection of legal articles can improve LLM accuracy by reducing noise from irrelevant articles.

**Medium confidence** in the response interpretation feature that shows legal basis for each sentence, dependent on fine-tuning quality and legal corpus completeness.

**Low confidence** in the generalizability of results beyond marriage-related legal queries and the assumption that all users possess sufficient legal knowledge for effective article selection.

## Next Checks

1. **Legal knowledge dependency test**: Conduct a controlled experiment comparing LLM response accuracy when legal experts versus non-experts select legal articles from the same set of retrieved articles, to quantify how user legal knowledge affects the effectiveness of the interactive selection feature.

2. **Cross-domain effectiveness validation**: Apply ELLA to legal domains beyond marriage (e.g., contract law, criminal law, intellectual property) and measure whether the accuracy improvements and user trust benefits observed in the original study persist across different types of legal queries.

3. **Corpus completeness and currency assessment**: Evaluate the impact of legal article corpus quality by testing ELLA's performance on cases involving recently updated laws or where the retrieved articles are known to be incomplete or contradictory, to understand failure modes related to legal source quality.