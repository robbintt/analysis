---
ver: rpa2
title: 'Fox-1: Open Small Language Model for Cloud and Edge'
arxiv_id: '2411.05281'
source_url: https://arxiv.org/abs/2411.05281
tags:
- arxiv
- preprint
- language
- fox-1
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Fox-1, a series of small language models
  (SLMs) with 1.6 billion parameters designed for efficient cloud and edge deployment.
  The model is pre-trained on 3 trillion tokens of web-scraped data using a novel
  3-stage data curriculum and fine-tuned on 5 billion tokens of instruction-following
  data.
---

# Fox-1: Open Small Language Model for Cloud and Edge

## Quick Facts
- arXiv ID: 2411.05281
- Source URL: https://arxiv.org/abs/2411.05281
- Authors: Zijian Hu; Jipeng Zhang; Rui Pan; Zhaozhuo Xu; Shanshan Han; Han Jin; Alay Dilipbhai Shah; Dimitris Stripelis; Yuhang Yao; Salman Avestimehr; Tong Zhang; Chaoyang He
- Reference count: 13
- Primary result: 1.6B parameter SLM achieving competitive performance on benchmarks with >200 output tokens/second inference speed

## Executive Summary
Fox-1 introduces a series of small language models (SLMs) with 1.6 billion parameters designed for efficient deployment on cloud and edge devices. The model employs a deeper architecture (32 layers) compared to similar SLMs, uses a large vocabulary (256K tokens), and incorporates Grouped Query Attention (GQA) for improved efficiency. Trained on 3 trillion tokens using a novel 3-stage data curriculum, Fox-1 achieves better or comparable performance to other SLMs across standard benchmarks while maintaining high inference throughput suitable for resource-constrained environments.

## Method Summary
Fox-1 uses a 3-stage curriculum learning approach during pre-training, starting with 2K sequence length and progressively increasing to 8K tokens. The model is trained on 3 trillion tokens of web-scraped data followed by 5 billion tokens of instruction-following data. Key architectural choices include 32 decoder-only transformer layers, 256K vocabulary size, Grouped Query Attention with 4 KV heads, Rotary Position Embeddings, RMSNorm pre-normalization, and shared embedding layers. The model weights are released under Apache 2.0 license, enabling open research and deployment.

## Key Results
- Achieves 70.1% accuracy on MMLU benchmark, outperforming StableLM-2-1.6B (67.1%) and Gemma-2B (67.6%)
- Matches Qwen1.5-1.8B inference speed at >200 output tokens/second while being 11% smaller
- Demonstrates strong performance on GSM8k (math reasoning), ARC (reasoning), and HellaSwag (commonsense reasoning) benchmarks
- Successfully deployed with TensorOpera serving platform for efficient inference on various hardware

## Why This Works (Mechanism)

### Mechanism 1
The 3-stage data curriculum improves pre-training efficiency by matching data complexity to model capacity progression. Fox-1 uses increasing sequence lengths (2K→4K→8K) and progressively higher quality data across three stages, allowing the model to first learn basic patterns then refine long-range dependencies. This approach enables better long-context abilities at lower computational cost compared to uniform chunking strategies.

### Mechanism 2
Deeper architecture (32 layers) provides stronger reasoning ability compared to wider but shallower SLMs. The 78% deeper structure compared to Gemma-2B (18 layers) enables better representation learning and reasoning capabilities through increased depth-to-width ratio. Deeper networks present stronger reasoning ability by allowing more complex hierarchical feature representations.

### Mechanism 3
Large vocabulary (256K tokens) reduces unknown word probability and implicitly extends effective context length. A larger vocabulary yields fewer tokens for given text, reducing unknown word frequency and allowing denser information encoding per token. This makes it easier to correctly predict consecutive tokens and improves overall inference performance.

## Foundational Learning

- **Transformer architecture fundamentals**: Understanding how Fox-1's architectural choices (GQA, RoPE) affect performance. *Quick check: How does Rotary Position Embedding differ from learned positional embeddings in handling long sequences?*
- **Curriculum learning principles**: To understand the 3-stage training strategy and why progressive complexity matters. *Quick check: What are the risks of using a curriculum that increases difficulty too rapidly?*
- **Vocabulary design trade-offs**: To grasp why Fox-1 chose 256K vocabulary and its implications for model efficiency. *Quick check: How does vocabulary size affect the number of tokens generated for a given input text?*

## Architecture Onboarding

- **Component map**: Tokenization → Embedding → 32-layer transformer with GQA → Output projection → Next token prediction
- **Critical path**: Input text → 256K vocabulary tokenizer → Embedding layer → 32 transformer layers with GQA → Output projection → Next token prediction
- **Design tradeoffs**:
  - Depth vs width: Chose deeper (32 layers) over wider for reasoning ability
  - Vocabulary size: Large (256K) for fewer tokens but higher memory usage
  - GQA: Improved inference speed but slightly reduced attention flexibility
  - Shared embeddings: Reduced parameters by ~30% but may affect representation quality
- **Failure signatures**:
  - Gradient vanishing: Training stalls or loss plateaus in deeper layers
  - Memory issues: Cannot fit model on target hardware despite smaller parameter count
  - Poor long-context performance: Model fails on tasks requiring >4K context
  - Overfitting: Model performs well on training data but poorly on benchmarks
- **First 3 experiments**:
  1. Verify tokenization: Compare token counts for sample texts between Fox-1's 256K tokenizer and standard 32K tokenizer
  2. Profile attention: Measure inference throughput with GQA vs standard multi-head attention on H100 GPU
  3. Test curriculum effectiveness: Train model with uniform data distribution vs 3-stage curriculum on same hardware and compare convergence speed

## Open Questions the Paper Calls Out

1. **Curriculum impact on long-context performance**: How does the 3-stage data curriculum specifically impact long-context performance compared to uniform chunking approaches? The paper describes using increasing chunk lengths (2K→4K→8K) across three stages to improve long-context abilities at lower computational cost but doesn't provide direct comparisons against models trained with uniform chunking strategies.

2. **Optimal vocabulary size**: What is the relationship between vocabulary size and downstream task performance for small language models specifically? While the paper demonstrates benefits of large vocabulary, it doesn't explore the optimal vocabulary size or provide systematic ablation studies.

3. **Depth vs reasoning ability**: How does the deeper architecture (32 layers) in Fox-1 compare to wider but shallower architectures in terms of reasoning ability versus memorization? The paper states deeper networks present stronger reasoning ability, but doesn't directly test this claim against wider alternatives.

## Limitations

- Lack of direct ablation studies comparing 3-stage curriculum against uniform training on identical data
- No systematic exploration of vocabulary size optimization (256K chosen by hypothesis rather than empirical comparison)
- Limited empirical evidence that 32-layer depth is optimal versus alternative depth-width trade-offs

## Confidence

**High Confidence**: Architectural specifications (32-layer transformer, GQA, 256K vocabulary, shared embeddings) are clearly defined and technically sound. Inference speed measurements (200+ tokens/second) are specific and verifiable. Benchmark comparisons against established models provide concrete performance claims.

**Medium Confidence**: Effectiveness of 3-stage curriculum learning is plausible given curriculum learning literature, but specific benefits for Fox-1 are not directly measured against alternatives. Reasoning ability improvements from increased depth are theoretically supported but not empirically isolated.

**Low Confidence**: Optimal vocabulary size selection (256K) is presented as hypothesis without systematic exploration of design space or comparison against models with different vocabulary sizes.

## Next Checks

1. **Curriculum Ablation**: Train identical model architectures with the same data but using uniform data distribution instead of the 3-stage curriculum. Compare convergence speed, final performance, and training stability to quantify the curriculum's specific contribution.

2. **Vocabulary Size Sweep**: Train Fox-1 architecture with varying vocabulary sizes (32K, 64K, 128K, 256K, 512K) using the same training data and curriculum. Measure tokenization efficiency, model size, and downstream performance to identify optimal vocabulary size.

3. **Depth Sensitivity Analysis**: Train shallower versions of Fox-1 (24 layers, 16 layers) with identical width and training procedure. Compare performance on reasoning benchmarks and training efficiency to determine if 32 layers represents an optimal depth.