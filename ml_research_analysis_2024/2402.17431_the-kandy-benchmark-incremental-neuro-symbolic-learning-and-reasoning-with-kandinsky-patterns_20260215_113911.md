---
ver: rpa2
title: 'The KANDY Benchmark: Incremental Neuro-Symbolic Learning and Reasoning with
  Kandinsky Patterns'
arxiv_id: '2402.17431'
source_url: https://arxiv.org/abs/2402.17431
tags:
- color
- shape
- task
- size
- extract
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces KANDY, a benchmarking framework for incremental
  neuro-symbolic learning and reasoning using Kandinsky patterns. KANDY generates
  binary classification tasks with increasing complexity and sparse supervision, focusing
  on symbol compositionality.
---

# The KANDY Benchmark: Incremental Neuro-Symbolic Learning and Reasoning with Kandinsky Patterns

## Quick Facts
- arXiv ID: 2402.17431
- Source URL: https://arxiv.org/abs/2402.17431
- Reference count: 40
- Key outcome: KANDY is a benchmarking framework for incremental neuro-symbolic learning and reasoning using Kandinsky patterns, showing that both state-of-the-art neural models and purely symbolic approaches struggle with most tasks, highlighting the need for advanced neuro-symbolic methods.

## Executive Summary
The paper introduces KANDY, a benchmarking framework for incremental neuro-symbolic learning and reasoning using Kandinsky patterns. KANDY generates binary classification tasks with increasing complexity and sparse supervision, focusing on symbol compositionality. The framework provides ground truth classification rules for interpretable solutions. Two curricula (easy and hard) are released as challenges for the research community. Experimental evaluation shows that both state-of-the-art neural models and purely symbolic approaches struggle with solving most tasks, highlighting the need for advanced neuro-symbolic methods trained over time.

## Method Summary
KANDY generates binary classification tasks using synthetic Kandinsky patterns, where each pattern is represented as a symbolic tree structure of geometric shapes. Tasks are created with increasing complexity and sparse supervision through an exponential decay schedule. The framework uses rejection sampling to ensure generated symbols obey ground-truth classification rules. Two curricula (Easy and Hard) are provided, and models are evaluated on accuracy, forgetting, and transfer metrics across different learning settings including independent, joint, task incremental, and continual online learning.

## Key Results
- Both neural models (MLP, CNN, ResNet-50, ViT) and symbolic models (Aleph ILP) struggle with most KANDY tasks, with accuracy below 70% on average.
- Task incremental learning settings show better performance than continual online learning, but still fail to solve complex compositional tasks.
- The benchmark reveals significant challenges in combining neural perception with symbolic reasoning under sparse supervision conditions.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: KANDY tasks are generated by rejecting symbols that violate a ground-truth rule, ensuring positive and negative sets are disjoint and follow a consistent logical rule.
- Mechanism: Symbol generation is performed via rejection sampling. Each generated tree is checked against previously generated symbols and, optionally, against the ground-truth rule. If inconsistent, it is rejected. This ensures all samples obey the intended classification logic.
- Core assumption: The ground-truth rule is expressible in Horn clause logic and the rejection sampling procedure eventually yields enough valid samples.
- Evidence anchors:
  - [abstract] "Classification rules are also provided in the ground truth to enable analysis of interpretable solutions."
  - [section 3.1] "Then, the data generator tries to build samples that are inherently different at the symbolic level... Optionally, against the ground-truth rule, to guarantee consistency..."
  - [corpus] No direct evidence in corpus; this is internal mechanism described in paper.
- Break condition: If the ground-truth rule is too restrictive or the patience hyperparameter is too low, generation may fail or yield very imbalanced sets.

### Mechanism 2
- Claim: Supervision in KANDY is scheduled using an exponential decay function, giving more labels early and fewer later.
- Mechanism: For each sample in a task, a time index t∈[0,1] is mapped to a probability f(t)=γ·e^(-σ·t) where σ=log(γ·β^-1). A binary random variable is sampled with this probability to decide if the sample is supervised.
- Core assumption: Early in learning, dense supervision helps establish basic features; later, sparse supervision encourages generalization and prevents overfitting.
- Evidence anchors:
  - [section 3.1] "KANDY implements an exponential decay law which is evaluated for each sample, f(t) = γ·e^(-σ·t)... It turns out that function f(t) is monotonically decreasing and has boundary values f(0) = γ and f(1) = β."
  - [abstract] "By creating curricula of binary classification tasks with increasing complexity and with sparse supervisions..."
  - [corpus] No corpus evidence; this is a design choice described in the paper.
- Break condition: If γ and β are both set too low, most samples will be unsupervised, making learning difficult; if too high, the benefit of semi-supervised learning is lost.

### Mechanism 3
- Claim: Compositional primitives in KANDY allow hierarchical arrangement of atomic shapes, enabling tasks with nested or recursive structures.
- Mechanism: KANDY provides compositional operators (stack, side_by_side, grid, diag_ul_lr, etc.) that position children objects relative to their bounding boxes. These operators can be nested, creating hierarchical trees that are rendered as images.
- Core assumption: Hierarchical composition can be expressed via these operators and the renderer preserves perceptual boundaries despite noise injection.
- Evidence anchors:
  - [section 3.1] "Compositional primitives are used to model the generation rules... The simplest operator is called in which draws a child object at the center... Operators stack and side by side equally split the drawing area available for each child along their primary axis."
  - [abstract] "We implement object compositionality, building on a collection of simple objects and basic primitives, extending perception to recursive structures."
  - [corpus] No corpus evidence; this is part of the benchmark generation described in the paper.
- Break condition: If the renderer's noise parameters are too aggressive, atomic shapes may become indistinguishable, breaking the symbolic clarity required for reasoning.

## Foundational Learning

- Concept: Symbolic representation of visual scenes
  - Why needed here: KANDY maps each image to a symbolic tree structure; models must interpret this representation to solve tasks.
  - Quick check question: Can you explain how a Kandinsky pattern is converted into a symbolic tree? What do leaves and internal nodes represent?

- Concept: Rejection sampling in dataset generation
  - Why needed here: KANDY uses rejection sampling to ensure generated samples obey the ground-truth classification rule and avoid duplicates.
  - Quick check question: What happens if a generated symbol fails the ground-truth rule check? How does the patience parameter affect this?

- Concept: Exponential decay scheduling
  - Why needed here: KANDY schedules supervision with an exponential decay to provide more labels early and fewer later, encouraging semi-supervised learning.
  - Quick check question: Given γ=0.8 and β=0.2, what is the probability of supervision at t=0 and t=1? How does this change over time?

## Architecture Onboarding

- Component map: Symbol generation -> Rejection sampling -> Image rendering with noise -> Dataset splitting. ILP engines (Aleph/Popper) consume symbolic examples and background knowledge to learn rules. Neural models consume rendered images and labels.
- Critical path: Symbol generation -> rule-based rejection -> rendering with noise -> split into train/val/test. For evaluation: model training -> accuracy measurement -> comparison to ground truth.
- Design tradeoffs: Larger patience increases success rate but slows generation; stronger noise improves robustness but may obscure symbolic features; sparse supervision challenges learning but encourages generalization.
- Failure signatures: Empty positive/negative sets (too restrictive rules); high rejection rate (patience too low); unbalanced splits (replacement sampling triggered); poor accuracy on test (overfitting to training or insufficient supervision).
- First 3 experiments:
  1. Run KANDY with minimal patience and observe if generation succeeds or fails; adjust patience accordingly.
  2. Generate a small curriculum with γ=1.0, β=1.0 (fully supervised) and train a simple CNN to verify end-to-end pipeline works.
  3. Generate a task with sparse supervision (γ=0.8, β=0.2) and compare performance of supervised-only vs semi-supervised training.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we design neural-symbolic models that effectively learn from sparse supervisions in incremental learning settings?
- Basis in paper: [explicit] The paper highlights the challenge of sparse supervisions in the KANDY benchmark and shows that both neural and symbolic approaches struggle with most tasks.
- Why unresolved: Current models, both neural and symbolic, fail to effectively learn from sparse supervisions, indicating a gap in research for models that can handle this challenge.
- What evidence would resolve it: Developing and evaluating neural-symbolic models on the KANDY benchmark with sparse supervision settings, showing improved performance compared to existing models.

### Open Question 2
- Question: What is the impact of curriculum design on the performance of neural-symbolic models in incremental learning?
- Basis in paper: [explicit] The paper introduces curricula with increasing task complexity in the KANDY benchmark, suggesting the importance of curriculum design in incremental learning.
- Why unresolved: The paper does not explore how different curriculum designs affect the performance of neural-symbolic models, leaving this as an open question.
- What evidence would resolve it: Conducting experiments with various curriculum designs in the KANDY benchmark and analyzing their impact on the performance of neural-symbolic models.

### Open Question 3
- Question: How can neural-symbolic models achieve better explainability in complex reasoning tasks?
- Basis in paper: [explicit] The paper emphasizes the importance of explainability in neuro-symbolic approaches and provides ground truth rules for analysis, but shows that current models struggle with complex tasks.
- Why unresolved: Despite the emphasis on explainability, the paper demonstrates that existing neural-symbolic models have limitations in complex reasoning tasks, indicating a need for improved explainability.
- What evidence would resolve it: Developing and evaluating neural-symbolic models on the KANDY benchmark, focusing on their ability to provide accurate and interpretable explanations for complex reasoning tasks.

## Limitations

- The rejection sampling approach for dataset generation may become computationally expensive for highly restrictive rules or large curricula.
- The exponential decay supervision schedule assumes optimal γ and β values, but their sensitivity to task difficulty is not fully explored.
- The ILP background knowledge and predicate invention mechanisms are not fully detailed, which may impact reproducibility of symbolic results.

## Confidence

- High confidence: The core mechanism of KANDY as a benchmarking framework for incremental neuro-symbolic learning, including task generation, curricula structure, and evaluation metrics.
- Medium confidence: The effectiveness of the exponential decay supervision schedule and its impact on learning dynamics, as this depends on hyperparameter tuning.
- Medium confidence: The sufficiency of the compositional primitives for expressing complex symbolic rules, as this depends on the expressiveness of the chosen operators.

## Next Checks

1. Test KANDY generation with increasingly restrictive rules to identify failure points and determine optimal patience parameters.
2. Systematically vary γ and β across tasks to quantify the impact of supervision density on both neural and symbolic model performance.
3. Attempt to encode increasingly complex compositional patterns using KANDY's primitives to identify limitations in the operator set.