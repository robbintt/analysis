---
ver: rpa2
title: Fair Augmentation for Graph Collaborative Filtering
arxiv_id: '2408.12208'
source_url: https://arxiv.org/abs/2408.12208
tags:
- graph
- https
- augmentation
- policies
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reproduces and extends a recent fairness mitigation
  approach for graph collaborative filtering (GCF) using fair graph augmentation.
  The method learns a fair graph augmentation by optimizing a fairness-aware loss
  function that monitors the gap in recommendation utility across demographic groups.
---

# Fair Augmentation for Graph Collaborative Filtering

## Quick Facts
- arXiv ID: 2408.12208
- Source URL: https://arxiv.org/abs/2408.12208
- Reference count: 40
- Primary result: Fair graph augmentation consistently mitigates unfairness in GNNs, especially on high-utility models and large datasets

## Executive Summary
This paper reproduces and extends a fairness mitigation approach for graph collaborative filtering (GCF) using fair graph augmentation. The method learns a fair graph augmentation by optimizing a fairness-aware loss function that monitors the gap in recommendation utility across demographic groups. Through extensive experiments on 11 GNN-based and 5 non-GNN models across 5 real-world datasets, the authors demonstrate that fair augmentation effectively reduces unfairness while maintaining utility, particularly benefiting high-performing models and larger datasets.

## Method Summary
The method learns fair graph augmentation through edge perturbation guided by a fairness-aware loss function. It samples candidate edges from missing user-item interactions and iteratively updates a perturbation vector to add edges that improve the disadvantaged group's NDCG while minimizing distance between original and augmented graphs. The approach employs various sampling policies (Zero NDCG, Low Degree, Furthest, Sparse, Item Preference) with hyperparameters Î¨U and Î¨I to restrict augmentation, and extends these with interaction time and graph topology properties. The framework is tested across 11 augmentable GNNs, 5 non-augmentable GNNs, and 5 non-GNN models on 5 real-world datasets with demographic attributes.

## Key Results
- Fair augmentation consistently improves utility-fairness trade-offs across all tested models and datasets
- Larger datasets (ML1M, LF1M) benefit more from fair augmentation than smaller ones (FNYC, FTKY)
- High-performing models show the most significant fairness improvements from augmentation
- Augmented graph knowledge does not effectively transfer to non-augmentable models during re-training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fair graph augmentation mitigates unfairness by targeting the disadvantaged user group's utility through iterative edge selection.
- Mechanism: The method samples candidate edges from missing user-item interactions, then iteratively updates a perturbation vector to add edges that improve the disadvantaged group's NDCG while minimizing the distance between original and augmented graphs.
- Core assumption: The recommendation utility gap across demographic groups can be effectively reduced by adding edges to the disadvantaged group's neighborhood.
- Evidence anchors:
  - [abstract]: "The method learns a fair graph augmentation by optimizing a fairness-aware loss function that monitors the gap in recommendation utility across demographic groups."
  - [section 2.2.1]: "The augmentation algorithm samples edges from a predefined set Ëœð¸ ( Ëœð¸ âˆ© ð¸ = âˆ…) of candidate edges... The subset of edges actually added are learnt by iteratively updating a perturbation vector ð‘."
  - [corpus]: Weak evidence - corpus contains related papers on fairness in GNNs but no direct validation of this specific augmentation mechanism.
- Break condition: If the disadvantaged group's recommendations are already high utility or the graph structure doesn't support meaningful augmentation, the method may fail to reduce unfairness.

### Mechanism 2
- Claim: Sampling policies effectively restrict the augmentation process to prevent harmful edge additions while focusing on beneficial nodes.
- Mechanism: Policies like Zero NDCG (ZN), Low Degree (LD), Furthest (FR), Sparse (SP), and Item Preference (IP) filter the candidate edge set to include only interactions that can improve fairness.
- Core assumption: The selected sampling policies capture meaningful patterns in the interaction graph that correlate with unfairness.
- Evidence anchors:
  - [section 2.2.2]: "User-item interaction graphs used in recommendation are typically sparse... [7] restricted Ëœð¸ to prevent the augmentation process from selecting edges that could potentially hinder the unfairness mitigation task."
  - [section 3.1]: Formal definitions of each policy showing how they filter user and item sets.
  - [corpus]: Moderate evidence - related papers discuss fairness-aware sampling but don't validate this specific set of policies.
- Break condition: If sampling policies overlap significantly or don't capture the true sources of unfairness, augmentation may be ineffective or even harmful.

### Mechanism 3
- Claim: Larger datasets and high-utility models benefit more from fair graph augmentation due to better signal-to-noise ratio and more stable unfairness patterns.
- Mechanism: The augmentation process learns from validation set relevance judgments, so larger datasets provide more reliable signals for identifying and correcting unfairness.
- Core assumption: Unfairness patterns are consistent across training, validation, and test sets, and can be effectively captured from larger datasets.
- Evidence anchors:
  - [section 4.1]: "A notable result is reported on SGL under ML1M... The consistency reported under LF1M and ML1M... underlines the augmentation process can specifically favor high-performing models."
  - [section 4.1]: "The highlighted observations suggest that larger datasets benefit from the augmented graph to a greater extent than smaller ones."
  - [corpus]: Weak evidence - no direct corpus evidence for this specific dataset-size relationship.
- Break condition: If unfairness patterns differ significantly between training and test sets, or if smaller datasets contain sufficient signal, the benefit of larger datasets may not materialize.

## Foundational Learning

- Concept: Graph Neural Networks and message-passing mechanisms
  - Why needed here: The method relies on understanding how GNNs propagate information through user-item graphs and how augmentation affects this process
  - Quick check question: How does adding edges to the graph affect the message-passing in a GNN like LightGCN?

- Concept: Fairness metrics in recommendation systems
  - Why needed here: The method specifically targets demographic parity fairness, requiring understanding of how to measure and optimize for fairness
  - Quick check question: What is the difference between demographic parity and equal opportunity in the context of recommendation fairness?

- Concept: Graph topology and properties
  - Why needed here: Sampling policies leverage graph properties like degree, clustering, and pagerank to make informed augmentation decisions
  - Quick check question: How does a node's pagerank score differ from its degree centrality in a user-item interaction graph?

## Architecture Onboarding

- Component map: Data preprocessing -> GNN training -> Fair augmentation optimization -> Inference with augmented graph -> Transfer learning evaluation
- Critical path: The augmentation optimization loop that iteratively updates the perturbation vector based on fairness-aware loss
- Design tradeoffs:
  - Model augmentation vs. re-training: Augmentation is faster but may not integrate as well
  - Sampling policy specificity vs. coverage: More specific policies may miss important interactions
  - Dataset size vs. computational cost: Larger datasets improve performance but increase training time
- Failure signatures:
  - Augmented graph doesn't improve fairness (Î” remains high)
  - Utility decreases after augmentation (NDCG drops)
  - Transfer learning fails (non-augmentable models don't benefit)
- First 3 experiments:
  1. Verify the base GNN implementation produces expected NDCG scores on validation set
  2. Test augmentation with a simple policy (e.g., Zero NDCG) on a small dataset to confirm fairness improvement
  3. Compare transfer learning performance between augmentable and non-augmentable GNNs on the same augmented graph

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions does fair augmentation transfer effectively across different GNN architectures, particularly between augmentable and non-augmentable models?
- Basis in paper: [explicit] The paper explicitly states that fair augmented graphs did not effectively transfer across models, and raises this as an open issue for future research.
- Why unresolved: The study only tested transferability from augmentable GNNs to both augmentable and non-augmentable GNNs and non-GNN models, finding limited success. The underlying reasons for this failure and conditions under which transfer might succeed remain unclear.
- What evidence would resolve it: Systematic experiments varying GNN architectures, augmentation strategies, and transfer mechanisms to identify conditions enabling successful fair knowledge transfer.

### Open Question 2
- Question: How do classical graph topological properties and temporal features impact the effectiveness of sampling policies in fair augmentation?
- Basis in paper: [explicit] The paper introduces new sampling policies incorporating pagerank and interaction time, but acknowledges this is an area requiring further exploration.
- Why unresolved: While the paper presents these new policies and shows some effectiveness, it does not systematically compare their performance against traditional policies or explore the full space of graph properties and temporal features that could be leveraged.
- What evidence would resolve it: Comprehensive benchmarking of diverse graph properties and temporal features as sampling criteria, with statistical analysis of their relative effectiveness across different datasets and recommendation tasks.

### Open Question 3
- Question: What is the relationship between dataset size, clustering structure, and the effectiveness of fair augmentation in mitigating consumer unfairness?
- Basis in paper: [inferred] The paper observes that larger datasets benefit more from fair augmentation and suggests that clustered structures in the disadvantaged group's subgraph enable more effective augmentation, but does not systematically investigate this relationship.
- Why unresolved: The study notes correlations between dataset size, clustering, and augmentation effectiveness but does not experimentally manipulate these factors to establish causal relationships or identify the specific structural properties that enable effective fairness mitigation.
- What evidence would resolve it: Controlled experiments varying dataset size and artificially manipulating graph clustering structures to measure their impact on fair augmentation effectiveness, potentially revealing structural prerequisites for successful fairness mitigation.

## Limitations
- Augmented graph knowledge does not effectively transfer to non-augmentable models, limiting the generality of the approach
- The method assumes demographic parity as the fairness metric without exploring alternative fairness definitions
- Performance on smaller datasets is limited due to insufficient data for the augmentation algorithm to learn meaningful fairness patterns

## Confidence
- **High confidence**: The core mechanism of fair graph augmentation through edge perturbation guided by fairness-aware loss function is well-supported by extensive experiments across multiple datasets and models
- **Medium confidence**: The superiority of fair augmentation over post-processing and pre-processing fairness methods, as the paper provides comprehensive comparisons but doesn't explore all possible fairness mitigation approaches
- **Low confidence**: The claims about why larger datasets benefit more from fair augmentation, as this is based on observed correlations rather than causal analysis

## Next Checks
1. Test the method on datasets with different demographic distributions to verify the robustness of fairness improvements across varying group sizes and characteristics
2. Compare fair graph augmentation with alternative fairness metrics (e.g., equal opportunity) to assess the method's sensitivity to fairness definition choices
3. Evaluate the augmented models on out-of-distribution data or in adversarial settings to assess robustness beyond standard fairness metrics