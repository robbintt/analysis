---
ver: rpa2
title: 'Beyond Self-Consistency: Ensemble Reasoning Boosts Consistency and Accuracy
  of LLMs in Cancer Staging'
arxiv_id: '2404.13149'
source_url: https://arxiv.org/abs/2404.13149
tags:
- ensreas
- cancer
- reports
- report
- predictions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of extracting cancer staging
  information from unstructured pathology reports using large language models (LLMs).
  The authors propose an ensemble reasoning (EnsReas) approach that improves both
  consistency and accuracy of LLM predictions for cancer staging tasks.
---

# Beyond Self-Consistency: Ensemble Reasoning Boosts Consistency and Accuracy of LLMs in Cancer Staging

## Quick Facts
- arXiv ID: 2404.13149
- Source URL: https://arxiv.org/abs/2404.13149
- Reference count: 12
- Outperforms baseline approaches with macro F1-score improvements (T category: 0.800 vs 0.793; N category: 0.838 vs 0.825)

## Executive Summary
This paper addresses the challenge of extracting cancer staging information from unstructured pathology reports using large language models (LLMs). The authors propose an ensemble reasoning (EnsReas) approach that improves both consistency and accuracy of LLM predictions for cancer staging tasks. EnsReas works by first generating multiple reasoning paths using self-consistency, then grouping these reasonings by predicted outcomes and using them as context to refine inconsistent predictions through a panel discussion-style prompt. Tested on breast cancer pathology reports from TCGA, the method significantly outperforms baseline approaches while reducing prediction entropy, indicating improved consistency and potential for reliable LLM deployment in clinical settings.

## Method Summary
The Ensemble Reasoning (EnsReas) approach builds upon self-consistency by first generating multiple reasoning paths using zero-shot chain-of-thought (ZS-CoT) with 10 samples. These predictions are grouped by outcome, and reports with inconsistent predictions are identified. For inconsistent reports, a panel discussion-style prompt is constructed using the grouped reasonings as "expert opinions," instructing the LLM to critically evaluate each reasoning path rather than simply selecting the majority answer. This selective refinement approach only applies the computationally expensive panel discussion to reports that need it, making it more efficient than full ensemble methods.

## Key Results
- Significant improvement in macro F1-score compared to baselines (T category: 0.800 vs 0.793; N category: 0.838 vs 0.825)
- Substantial reduction in prediction entropy indicating improved consistency (T category: 0.162 to 0.036; N category: 0.093 to 0.023)
- Selective refinement approach reduces computational cost by only re-assessing inconsistent predictions
- Demonstrates that panel discussion-style prompts can improve LLM performance beyond simple majority voting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ensemble reasoning reduces stochastic variability in LLM outputs by aggregating multiple reasoning paths into structured panel discussions.
- Mechanism: The EnsReas method first generates multiple reasoning paths using self-consistency, groups them by predicted outcomes, then uses these grouped reasonings as context for a refined prediction. This mimics a panel discussion where diverse viewpoints are considered before reaching a consensus.
- Core assumption: LLM outputs contain valuable reasoning signals that can be clustered and used as structured context to improve subsequent predictions.
- Evidence anchors:
  - [abstract]: "EnsReas works by first generating multiple reasoning paths using self-consistency, then grouping these reasonings by predicted outcomes and using them as context to refine inconsistent predictions through a panel discussion-style prompt."
  - [section]: "For the reports in Rinc, we design a prompt for EnsReas to simulate a panel discussion, triggering the LLM to resolve the inconsistent reasonings and predictions of a given report."
  - [corpus]: Weak evidence - related papers focus on self-consistency and majority voting but don't address panel discussion-style reasoning aggregation specifically.
- Break condition: If the LLM cannot meaningfully process grouped reasonings as coherent context, or if the reasoning diversity is too low to provide useful perspectives.

### Mechanism 2
- Claim: Separating consistent and inconsistent predictions allows targeted computational resources to be applied only where needed.
- Mechanism: The method first identifies reports with consistent predictions (Rcon) and leaves them unchanged, then only applies the panel discussion refinement to reports with inconsistent predictions (Rinc). This selective approach improves efficiency.
- Core assumption: Not all predictions require the same level of refinement; inconsistent predictions benefit more from additional processing than consistent ones.
- Evidence anchors:
  - [section]: "Unlike ER which repeats sampling process on all predictions, EnsReas first separates consistent and inconsistent predictions and only re-assesses the inconsistent predictions, making it a more cost-effective approach."
  - [corpus]: No direct evidence found in related papers about selective refinement based on prediction consistency.
- Break condition: If the threshold for determining "inconsistent" predictions is poorly calibrated, leading to either wasted computation on consistent cases or insufficient refinement on problematic cases.

### Mechanism 3
- Claim: Panel discussion-style prompts enable LLMs to critically evaluate multiple reasoning paths rather than simply selecting majority answers.
- Mechanism: By presenting grouped reasonings as "expert opinions" with different conclusions, the LLM is prompted to analyze the reasoning quality itself rather than just the frequency of outcomes, potentially identifying superior minority views.
- Core assumption: LLMs can perform meta-reasoning about the quality of reasoning paths when presented with them as alternatives.
- Evidence anchors:
  - [section]: "With this prompt template, we instruct the LLM to review every report in Rinc and its grouped reasonings... 'Please review each report. Analyze the reasonings provided by the panel for the chosen answers. Keep in mind that the majority vote may not be the correct one, therefore you should review report carefully in addition to considering the panel reasonings.'"
  - [corpus]: No direct evidence found in related papers about meta-reasoning over grouped reasoning paths as panel discussions.
- Break condition: If the LLM treats the grouped reasonings as mere context rather than alternative viewpoints to be critically evaluated, reverting to simple majority voting behavior.

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: The method demonstrates how LLMs can perform cancer staging without task-specific training data, crucial for clinical applications where annotated data is scarce.
  - Quick check question: Can the LLM correctly identify cancer staging categories when provided only with a pathology report and no examples?

- Concept: Chain-of-thought reasoning
  - Why needed here: CoT helps elicit the reasoning process behind predictions, making them more interpretable and potentially more accurate than direct answers.
  - Quick check question: Does asking the LLM to "think step by step" before answering improve the quality of cancer staging predictions?

- Concept: Entropy as consistency metric
  - Why needed here: Entropy measures the variability in predictions across multiple samples, providing a quantitative way to assess whether EnsReas improves consistency.
  - Quick check question: Does reducing prediction entropy from 0.162 to 0.036 indicate that predictions are becoming more consistent?

## Architecture Onboarding

- Component map: Data preprocessing (TCGA pathology reports) -> Base LLM (Med42-70B) -> Zero-shot pipeline -> CoT pipeline -> Self-consistency pipeline -> EnsReas pipeline -> Evaluation (macro F1 and entropy)

- Critical path:
  1. Load pathology report
  2. Apply ZS-CoT-SC to generate 10 reasoning-prediction pairs
  3. Group predictions and identify consistent vs. inconsistent reports
  4. For inconsistent reports, construct panel discussion prompt with grouped reasonings
  5. Generate refined predictions using panel discussion prompt
  6. Calculate macro F1 and entropy metrics

- Design tradeoffs:
  - Computational cost vs. accuracy: EnsReas requires multiple LLM calls but only for inconsistent cases
  - Prompt complexity vs. effectiveness: More sophisticated panel discussion prompts may improve results but risk confusing the model
  - Granularity of consistency detection: Stricter thresholds for "consistent" may reduce refinement opportunities

- Failure signatures:
  - No improvement in F1 scores despite increased computational cost
  - Panel discussion prompts causing the model to hallucinate or produce contradictory outputs
  - Inconsistent predictions remaining after EnsReas application (entropy not decreasing)

- First 3 experiments:
  1. Compare macro F1 scores of ZS, ZS-CoT, ZS-CoT-SC, and EnsReas on a small subset of reports to verify the claimed improvements
  2. Measure entropy reduction specifically for reports that transition from inconsistent to consistent states to validate targeted refinement
  3. A/B test different prompt templates for the panel discussion to find the most effective phrasing for critical evaluation

## Open Questions the Paper Calls Out

- How does EnsReas performance vary across different cancer types beyond breast cancer? The study focuses specifically on breast cancer pathology reports, with authors suggesting "Future research is needed to explore the application of EnsReas to a broader range of clinical tasks."

- What is the impact of increasing the number of reasoning samples beyond 10 on EnsReas performance and computational efficiency? The authors use a fixed sample size of 10 without exploring whether this is optimal for balancing performance improvement against computational cost.

- How does EnsReas compare to fine-tuned models when sufficient annotated training data is available? The authors position EnsReas as a zero-shot approach but don't compare against models trained on labeled data which is the current clinical standard.

## Limitations
- Relies on a single clinical LLM (Med42-70B), limiting generalizability across different model architectures
- Entropy metric measures consistency but doesn't capture clinical validity of predictions
- TCGA dataset represents a curated subset of pathology reports, potentially limiting real-world applicability
- Panel discussion approach assumes grouping similar reasonings will lead to better outcomes

## Confidence

- **High confidence**: The methodology for implementing self-consistency and grouping predictions by outcome is clearly specified and reproducible. The entropy reduction from 0.162 to 0.036 (T category) and from 0.093 to 0.023 (N category) provides strong quantitative evidence of improved consistency.
- **Medium confidence**: The claim that panel discussion-style prompts enable critical evaluation of reasoning quality is supported by improved F1 scores but relies on the assumption that LLMs can perform meta-reasoning effectively.
- **Low confidence**: The clinical utility claim extends beyond the empirical results, as the study doesn't validate predictions against ground truth staging or demonstrate impact on downstream clinical decisions.

## Next Checks

1. **Cross-model validation**: Test EnsReas with multiple clinical LLMs (e.g., Med-PaLM, GPT-4 with medical fine-tuning) to verify that the panel discussion approach generalizes beyond Med42-70B and isn't model-specific.

2. **Clinical accuracy validation**: Compare LLM predictions against expert-reviewed ground truth staging to ensure that consistency improvements don't come at the cost of accuracy, and to validate the clinical relevance of the F1 score improvements.

3. **Adversarial report testing**: Evaluate EnsReas on intentionally challenging pathology reports containing ambiguous language, conflicting information, or rare staging scenarios to test the robustness of the panel discussion approach under difficult conditions.