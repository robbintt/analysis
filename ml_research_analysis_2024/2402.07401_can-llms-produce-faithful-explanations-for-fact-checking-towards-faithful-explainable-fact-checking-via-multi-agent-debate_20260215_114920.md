---
ver: rpa2
title: Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful
  Explainable Fact-Checking via Multi-Agent Debate
arxiv_id: '2402.07401'
source_url: https://arxiv.org/abs/2402.07401
tags:
- explanation
- error
- feedback
- evidence
- errors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates the faithfulness of Large Language Models
  (LLMs) in generating fact-checking explanations. Initial findings reveal that zero-shot
  prompts often lead to unfaithful explanations, with 80% containing hallucinated
  details.
---

# Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable Fact-Checking via Multi-Agent Debate

## Quick Facts
- arXiv ID: 2402.07401
- Source URL: https://arxiv.org/abs/2402.07401
- Reference count: 20
- 80% of zero-shot LLM explanations contain hallucinated details

## Executive Summary
Large Language Models struggle to generate faithful explanations for fact-checking, with zero-shot prompts producing explanations that are 80% unfaithful. The Multi-Agent Debate Refinement (MADR) framework addresses this by employing multiple LLM agents to iteratively debate and refine explanations, significantly improving faithfulness. Human evaluations confirm MADR produces the most faithful explanations compared to baselines, though automatic evaluation metrics show limitations in accurately assessing faithfulness.

## Method Summary
The study proposes the Multi-Agent Debate Refinement (MADR) framework to improve the faithfulness of LLM-generated fact-checking explanations. MADR uses two debater agents with different instructions to identify errors independently, then debate until consensus is reached. A refiner agent implements the agreed-upon corrections. The framework iteratively refines explanations using feedback from the debate, producing more faithful outputs than zero-shot baselines. Experiments are conducted on the PolitiHop dataset with evaluations using both automatic metrics (G-Eval with GPT-4 Turbo) and human annotators.

## Key Results
- MADR significantly improves explanation faithfulness compared to zero-shot baselines
- Human evaluations show MADR produces the most faithful explanations with reduced errors
- Automatic evaluation metrics show poor correlation with human judgments for faithfulness assessment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-agent debate enables bidirectional thinking and error correction in LLM explanations.
- Mechanism: Two agents with different instructions (one using error typology, one without) identify errors independently, then debate until agreement, ensuring comprehensive error detection.
- Core assumption: Independent perspectives reduce blind spots in error identification.
- Evidence anchors:
  - [abstract] "MADR utilizes the debate for generating feedback to be employed in subsequent refinement stages" and "This approach prompts bidirectional thinking within the LLM, enabling it to analyze explanations both with and without knowledge of predefined error types"
  - [section 3] "Distinctive instructions with varying goals are provided to the DEBATERS: DEBATER 1 identifies errors based on a predefined error typology... while DEBATER 2 focuses on potential errors that may affect the explanation's faithfulness, without relying on the error typology"
  - [corpus] No direct corpus evidence available for this specific mechanism
- Break condition: If agents cannot reach consensus within fixed iterations, or if their feedback is consistently misaligned

### Mechanism 2
- Claim: Iterative refinement with precise feedback improves explanation faithfulness.
- Mechanism: Refiner agent uses concatenated feedback from debaters to make targeted revisions, with each iteration improving accuracy.
- Core assumption: Precise, targeted feedback is more effective than general self-refinement for improving faithfulness.
- Evidence anchors:
  - [abstract] "Experimental results show that MADR significantly improves faithfulness upon baselines"
  - [section 3] "Despite the high faithfulness scores suggested by automatic evaluation... human evaluators frequently deemed the LLM-generated explanations unfaithful" and "MADR allows DEBATERs to identify and correct errors missed during self-refinement"
  - [corpus] No direct corpus evidence available for this specific mechanism
- Break condition: If feedback quality degrades over iterations or if refiner fails to implement suggested corrections

### Mechanism 3
- Claim: Granular evaluation with error typology improves automatic faithfulness assessment.
- Mechanism: Evaluation protocols that assess at sentence level and incorporate the proposed error typology better correlate with human judgments.
- Core assumption: Fine-grained analysis with domain-specific error categories captures nuances that document-level or generic assessments miss.
- Evidence anchors:
  - [abstract] "Our correlation analysis reveals the most suitable LLM-based evaluation protocol for this task"
  - [section 5] "According to Table 4, a granular evaluation aligns better with human judgments, and incorporating our error typology into automatic evaluations enhances the quality of LLM assessments"
  - [corpus] No direct corpus evidence available for this specific mechanism
- Break condition: If evaluation protocol changes don't improve correlation with human judgments

## Foundational Learning

- Concept: Error typology classification (intrinsic vs extrinsic, entity-related, event-related, noun-phrase-related, reasoning coherence, overgeneralization, irrelevant evidence)
  - Why needed here: Provides structured framework for identifying and categorizing unfaithfulness in LLM-generated explanations
  - Quick check question: Can you explain the difference between intrinsic and extrinsic errors in the context of entity-related mistakes?

- Concept: Multi-agent debate systems
  - Why needed here: Enables collaborative error detection and correction through diverse perspectives and iterative refinement
  - Quick check question: How does the debate process ensure that errors identified by one agent aren't missed by another?

- Concept: Automatic evaluation protocols for faithfulness
  - Why needed here: Provides scalable assessment of explanation quality, though must be validated against human judgments
  - Quick check question: Why might sentence-level evaluation correlate better with human judgments than document-level evaluation?

## Architecture Onboarding

- Component map:
  - Initial explanation generator (zero-shot LLM prompt)
  - Two DEBATER agents (different error detection approaches)
  - JUDGE agent (consensus determination)
  - REFINER agent (implementation of corrections)
  - Feedback concatenation and iteration control

- Critical path:
  1. Generate initial explanation
  2. DEBATERs identify errors and provide feedback
  3. JUDGE checks for consensus
  4. If no consensus, DEBATERs refine feedback and repeat
  5. Once consensus reached, REFINER implements corrections
  6. Output final refined explanation

- Design tradeoffs:
  - Multiple agents increase computational cost but improve error detection
  - Fixed iteration limit prevents infinite loops but may leave some errors uncorrected
  - Granular evaluation improves correlation with human judgment but increases computational overhead

- Failure signatures:
  - DEBATERs consistently disagree (indicating flawed error typology or agent instructions)
  - REFINER fails to implement feedback correctly (implementation bug or ambiguous feedback)
  - Human evaluation shows poor correlation with automatic metrics (evaluation protocol mismatch)

- First 3 experiments:
  1. Test MADR on a small dataset with known errors to verify error detection and correction
  2. Compare MADR output with human-annotated faithful explanations for quality assessment
  3. Run correlation analysis between different automatic evaluation protocols and human judgments on sample data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MADR vary across different types of errors in the error typology, and which error types are most effectively addressed by the framework?
- Basis in paper: [explicit] The paper introduces a novel typology of common errors in LLM-generated fact-checking explanations, including intrinsic and extrinsic errors within the domains of Entity-Related, Event-Related, and Noun-phrase Errors, and mentions that MADR significantly improves faithfulness upon baselines.
- Why unresolved: The paper does not provide a detailed breakdown of MADR's performance across the different error types, nor does it discuss which error types are most challenging for the framework to address.
- What evidence would resolve it: An analysis of MADR's effectiveness in reducing each specific error type, comparing its performance on intrinsic vs. extrinsic errors, and across the different domains (Entity, Event, Noun-phrase).

### Open Question 2
- Question: To what extent does the iterative debate process in MADR contribute to the refinement of explanations compared to a single round of feedback and refinement?
- Basis in paper: [explicit] The paper describes MADR as an iterative process where multiple LLM agents debate and refine explanations, but does not provide a comparative analysis of the impact of iterative vs. single-round refinement on the faithfulness of explanations.
- Why unresolved: The paper demonstrates the effectiveness of MADR over baselines but does not explore the contribution of the iterative debate process to the improvement in faithfulness.
- What evidence would resolve it: Experimental results comparing the performance of MADR with varying numbers of debate iterations to a single-round refinement process, measuring the impact on explanation faithfulness.

### Open Question 3
- Question: How sensitive is the performance of MADR to the choice of LLM agents, and does the framework's effectiveness vary with different model sizes or architectures?
- Basis in paper: [inferred] The paper uses GPT-3.5-Turbo for generating explanations and conducting experiments, but does not explore the impact of using different LLM agents on MADR's performance.
- Why unresolved: The paper does not investigate the robustness of MADR to the choice of LLM agents, which could affect the framework's generalizability and applicability to other models or contexts.
- What evidence would resolve it: Comparative experiments using MADR with different LLM agents, including variations in model size, architecture, and training data, to assess the impact on explanation faithfulness and the framework's overall effectiveness.

## Limitations
- Automatic evaluation metrics poorly correlate with human judgments for faithfulness assessment
- MADR's effectiveness demonstrated primarily on PolitiHop dataset, raising generalizability concerns
- Multi-agent debate system increases computational overhead, limiting practical deployment

## Confidence
- High confidence: MADR framework improves faithfulness compared to zero-shot baselines
- Medium confidence: Specific error types identified are comprehensive and correctly categorized
- Medium confidence: Sentence-level evaluation with error typology correlates better with human judgment

## Next Checks
1. Test MADR framework on diverse fact-checking datasets beyond PolitiHop to assess generalizability
2. Conduct ablation studies removing specific error types from the typology to validate their necessity
3. Measure computational efficiency and resource requirements for deploying MADR at scale in production environments