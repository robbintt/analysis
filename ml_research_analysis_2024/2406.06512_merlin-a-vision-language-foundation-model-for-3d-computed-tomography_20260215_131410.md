---
ver: rpa2
title: 'Merlin: A Vision Language Foundation Model for 3D Computed Tomography'
arxiv_id: '2406.06512'
source_url: https://arxiv.org/abs/2406.06512
tags: []
core_contribution: Merlin is a 3D vision language foundation model trained on paired
  abdominal CT scans, EHR diagnosis codes, and radiology reports. It leverages structured
  and unstructured clinical data to learn joint representations across images, text,
  and clinical labels.
---

# Merlin: A Vision Language Foundation Model for 3D Computed Tomography

## Quick Facts
- arXiv ID: 2406.06512
- Source URL: https://arxiv.org/abs/2406.06512
- Reference count: 40
- Merlin is a 3D vision language foundation model trained on paired abdominal CT scans, EHR diagnosis codes, and radiology reports.

## Executive Summary
Merlin is a 3D vision language foundation model designed for abdominal CT interpretation that learns joint representations across images, text, and clinical labels. Trained on 15,331 CT scans with paired EHR diagnosis codes and radiology reports, Merlin leverages contrastive learning and multi-task training to achieve strong performance across 752 tasks. The model demonstrates effective zero-shot classification, phenotype prediction, cross-modal retrieval, disease risk prediction, report generation, and segmentation without task-specific fine-tuning. Notably, Merlin achieves these results while being trained on a single GPU, demonstrating the feasibility of efficient, scalable medical foundation models.

## Method Summary
Merlin uses a 3D ResNet152 (I3D) image encoder with ImageNet initialization paired with a clinical Longformer text encoder. The model is trained using multi-task learning with two supervision sources: EHR diagnosis codes mapped to 692 PheWAS phenotypes using binary cross-entropy loss, and radiology reports used for contrastive learning via InfoNCE loss. Training occurs in two stages - first with both EHR and report supervision, then with report-only supervision to preserve learned features. The model leverages 6,387,231 CT images, 1,839,559 EHR codes, and 6,036,645 report tokens. Zero-shot capabilities are enabled by the contrastive alignment between image and text embeddings in a shared space.

## Key Results
- Achieves strong zero-shot classification performance across 31 abdominal findings without task-specific fine-tuning
- Demonstrates effective 5-year disease prediction for 6 conditions using only imaging features
- Generates clinically coherent radiology reports that outperform baseline models on multiple evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1
Contrastive learning across full-length clinical reports aligns visual embeddings with descriptive language, enabling zero-shot classification without task-specific fine-tuning. During training, CT volumes are paired with radiology report findings sections. The model learns to map image embeddings into a shared space where cosine similarity with report embeddings reflects semantic correspondence. This alignment transfers to unseen prompts that match report style. Core assumption: Radiology reports contain rich, structured language that reliably describes observable CT findings. Evidence anchors: [abstract] explicitly mentions "contrastive learning with the radiology reports"; [section] describes splitting reports into anatomical sections and alternating between full reports and subsections. Break condition: If report findings are vague, inconsistent, or poorly written, the alignment fails and prompts no longer match image semantics.

### Mechanism 2
Weak supervision from EHR diagnosis codes provides complementary signals to radiology reports, improving phenotype prediction accuracy. EHR codes are mapped to hierarchical PheWAS phenotypes and used as binary cross-entropy targets during multi-task training. This coarse-grained supervision encourages the model to encode clinically meaningful features beyond image appearance. Core assumption: Phenotypes derived from EHR codes correspond to observable imaging features, even if the mapping is indirect. Evidence anchors: [abstract] mentions "structured EHR data" is used alongside radiology reports; [section] describes using "PheWAS [57] phecode mapping" and "phenotype expansion" to propagate labels up the hierarchy. Break condition: If EHR codes are noisy, missing, or unrelated to imaging findings, the supervision signal misleads the model.

### Mechanism 3
I3D initialization leverages 2D ImageNet pretraining to accelerate 3D model convergence and improve downstream performance. 2D convolutional weights are copied into the first two spatial dimensions of 3D kernels; the third dimension is initialized to zero. This preserves low-level visual features while allowing 3D-specific learning. Core assumption: Early 3D layers benefit from 2D low-level features (edges, textures) before learning volumetric structure. Evidence anchors: [abstract] states "I3D weight initialization [56] is helpful for all tasks that we examine"; [section] describes using "inflated 3D (I3D) ResNet152" and ablation studies showing I3D consistently outperforms random initialization. Break condition: If 3D-specific features are required early in the network, I3D initialization may hinder learning and slow convergence.

## Foundational Learning

- Concept: **Contrastive learning objective (InfoNCE)**
  - Why needed here: Aligns image and text embeddings in a shared space without requiring paired labels for every task.
  - Quick check question: What is the difference between InfoNCE and standard cross-entropy in this context?

- Concept: **Hierarchical phenotype mapping**
  - Why needed here: Transforms raw ICD codes into clinically meaningful groups, reducing sparsity and improving supervision.
  - Quick check question: How does phenotype expansion affect label distribution in the training set?

- Concept: **Multi-task learning with staged training**
  - Why needed here: Combines EHR and report supervision efficiently, preventing catastrophic forgetting.
  - Quick check question: Why might staged training with low-weighted phenotype loss in stage 2 outperform pure multi-task training?

## Architecture Onboarding

- Component map: 3D ResNet152 (I3D) image encoder -> Clinical Longformer text encoder -> Linear adapter -> RadLlama-7B for report generation / UNet decoder for segmentation
- Critical path: Image → 3D encoder → embedding → shared space → contrastive loss / phenotype loss → updated weights
- Design tradeoffs:
  - ResNet152 vs Swin/ConvNeXt: ResNet152 with smaller kernel strides and I3D init gives best phenotype performance; Swin is too parameter-heavy for single-GPU.
  - Full reports vs anatomical splitting: Splitting improves zero-shot classification but hurts retrieval; no splitting works best for retrieval.
  - Staged vs multi-task: Multi-task gives slightly better phenotype performance; staged preserves EHR features.
- Failure signatures:
  - Low retrieval recall → text encoder mismatch or insufficient contrastive diversity.
  - Poor segmentation Dice → decoder architecture mismatch or insufficient fine-tuning data.
  - Overfitting phenotype classification → too few positive examples or class imbalance.
- First 3 experiments:
  1. Train with ResNet152 + I3D init, multi-task EHR+reports, no splitting; evaluate phenotype AUROC.
  2. Add report splitting; re-evaluate zero-shot classification F1.
  3. Swap ResNet152 for Swin Transformer; compare phenotype and retrieval performance.

## Open Questions the Paper Calls Out

### Open Question 1
How does Merlin's performance scale with increased dataset size beyond the current training set, particularly for cross-modal retrieval tasks? Basis: [explicit] The paper derives data scaling laws for zero-shot classification and phenotype classification, showing performance improvements with increased training data. It also mentions that retrieval performance improves with pretraining dataset size. Why unresolved: The paper does not provide specific data scaling results for cross-modal retrieval tasks. What evidence would resolve it: Additional experiments training Merlin on progressively larger datasets (e.g., 2x, 5x, 10x the current size) and measuring cross-modal retrieval performance would provide concrete data scaling results for this task.

### Open Question 2
How would increasing the image resolution during preprocessing affect Merlin's performance on downstream tasks? Basis: [explicit] The paper mentions that utilizing higher resolution images is expected to enhance model performance, as suggested by prior studies on vision-language models. However, it also notes that increasing the physical resolution during image acquisition may not always yield better results due to potential trade-offs with signal-to-noise ratio. Why unresolved: The paper does not provide experimental results on the impact of image resolution on Merlin's performance. What evidence would resolve it: Conducting experiments training Merlin on datasets with varying image resolutions (e.g., 224x224, 448x448, 672x672) and evaluating performance on downstream tasks would provide insights into the optimal resolution for this model.

### Open Question 3
How does Merlin's performance on radiology report generation compare to human radiologists, particularly in terms of factual correctness and clinical utility? Basis: [explicit] The paper compares Merlin's radiology report generation performance to RadFM using metrics like BLEU score, ROUGE-2, BERT score, and RadGraph-F1. It also provides qualitative examples of Merlin-generated reports with annotations highlighting correct and incorrect findings. Why unresolved: While the paper provides quantitative and qualitative comparisons, it does not directly compare Merlin's generated reports to those written by human radiologists in terms of factual correctness or clinical utility. What evidence would resolve it: Conducting a reader study where radiologists evaluate and compare Merlin-generated reports to human-written reports for the same CT scans would provide direct evidence of Merlin's performance in terms of factual correctness and clinical utility.

## Limitations

- The single-GPU training claim may not fully account for preprocessing and fine-tuning resource requirements
- Reliance on radiology report quality for contrastive learning introduces significant variability
- Model's generalization to non-abdominal CT applications remains untested

## Confidence

- **High Confidence**: I3D initialization improves performance; multi-task learning with EHR and report supervision is effective; zero-shot classification and retrieval capabilities work as described
- **Medium Confidence**: The single-GPU training efficiency claim (requires verification of preprocessing and fine-tuning requirements); generalization to 752 tasks (relies on benchmark quality and task selection)
- **Low Confidence**: Foundation model status for broader medical imaging applications (only tested on abdominal CT); 5-year disease prediction superiority over clinical baselines (lacks comparative analysis)

## Next Checks

1. **External Dataset Validation**: Evaluate Merlin on multiple external abdominal CT datasets with different scanners/protocols to assess true generalization beyond the pretraining distribution. Compare performance to established organ segmentation models like nnU-Net.

2. **Clinical Utility Assessment**: Compare Merlin's 5-year disease prediction performance against established clinical risk calculators (e.g., Framingham Risk Score, Gail Model) on the same datasets to establish clinical relevance and potential utility.

3. **Resource Requirement Analysis**: Document complete resource usage including data preprocessing, model storage, and fine-tuning requirements. Verify whether single-GPU claims hold across the full model lifecycle from data preparation to deployment.