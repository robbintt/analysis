---
ver: rpa2
title: "\xC9valuation des capacit\xE9s de r\xE9ponse de larges mod\xE8les de langage\
  \ (LLM) pour des questions d'historiens"
arxiv_id: '2406.15173'
source_url: https://arxiv.org/abs/2406.15173
tags:
- ponses
- pour
- nous
- questions
- dans
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the ability of large language models (LLMs)
  to provide accurate, comprehensive, and reliable responses to historical questions
  in French. A testbed of 62 history-related questions of varying types, themes, and
  difficulty levels was constructed.
---

# Évaluation des capacités de réponse de larges modèles de langage (LLM) pour des questions d'historiens

## Quick Facts
- arXiv ID: 2406.15173
- Source URL: https://arxiv.org/abs/2406.15173
- Reference count: 0
- Primary result: Large language models show insufficient accuracy (33.6%) for historical research, with significant language consistency issues

## Executive Summary
This study evaluates ten large language models' ability to answer historical questions in French, revealing substantial limitations in accuracy and reliability. Using a testbed of 62 questions across five historical themes, the research found an overall accuracy rate of only 33.6%, with even the best-performing model (GPT-4) achieving just 54.3% accuracy. The evaluation exposed significant challenges including inconsistent French language treatment, verbosity, and unreliable responses across different question types and difficulty levels.

## Method Summary
The study employed manual human evaluation by history experts to assess 10 LLMs using 62 historical questions of varying types (quantitative, qualitative closed, qualitative open) and themes. Responses were classified into five categories: correct, partial, approximate, wrong, and no response. The evaluation considered both content accuracy and language consistency, explicitly penalizing responses in English rather than French. Each query was submitted twice to assess response consistency, generating 5,360 total responses for analysis.

## Key Results
- Overall accuracy rate of 33.6% across all models, with GPT-4 achieving the highest accuracy at 54.3%
- Only 84.33% of responses were in French, highlighting significant language consistency issues
- Question depth and complexity significantly impacted performance, with open-ended questions about craftsmanship in Poitou receiving only 18.17% accuracy
- Even high-performing models showed considerable variability and inconsistency in responses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Manual human evaluation by domain experts is essential to accurately assess LLM responses on historical questions due to the complexity and nuance of historical facts.
- Mechanism: Human experts can interpret subtle inaccuracies, contextual nuances, and assess the completeness of responses in ways that automated metrics cannot capture.
- Core assumption: Historical knowledge requires deep contextual understanding that current automated evaluation methods cannot replicate.
- Evidence anchors:
  - [abstract] "Our evaluation of responses from ten selected LLMs reveals numerous shortcomings in both substance and form... an overall insufficient accuracy rate"
  - [section 4] "Although there exist automated evaluation methods for LLM capabilities, they generally show their limits quickly, especially when dealing with open-ended or domain-specific questions... it seemed delicate to opt for automated evaluation systems that do not necessarily have sufficient level of knowledge"
  - [corpus] Weak - no direct corpus evidence provided
- Break condition: Automated evaluation methods advance to match human-level understanding of historical context and nuance.

### Mechanism 2
- Claim: Testing LLMs with questions of varying types (quantitative, qualitative closed, qualitative open) reveals different strengths and weaknesses in model performance.
- Mechanism: Different question types require different cognitive abilities from LLMs - factual recall, list generation, and complex reasoning - exposing specific limitations.
- Core assumption: LLMs have heterogeneous capabilities across different cognitive tasks, and a comprehensive evaluation requires diverse question types.
- Evidence anchors:
  - [section 3] "Our question types impose having enough data to exploit, both in number and quality... We have therefore distributed our questions among five major subjects or historical facts... composed of various questions quantitative (for example: 'How many battles took place during the third war of religion?') and qualitative (this can be a closed question like 'What is the date of creation of the V errières forge in Lhom maizé?') or an open question such as 'To what extent was craftsmanship essential to the economic life of villages in Poitou from 1500 to 1800?'"
  - [section 5.1] "Our work also shows that the level of depth of a historical theme can clearly influence the quality of responses... questions about craftsmanship in Poitou during the modern era only get 18.17% accuracy"
  - [corpus] Weak - no direct corpus evidence provided
- Break condition: LLMs achieve uniform performance across all question types without significant variance.

### Mechanism 3
- Claim: Language consistency is a critical factor in LLM evaluation for historical research, as responses in English rather than French reduce usability despite potential accuracy.
- Mechanism: The evaluation methodology explicitly penalizes responses not in French, recognizing that language accessibility is essential for practical use by French historians.
- Core assumption: The target users (historians) require responses in their working language to be practically useful, regardless of accuracy in other languages.
- Evidence anchors:
  - [abstract] "Beyond an overall insufficient accuracy rate, we highlight uneven treatment of the French language, as well as issues related to verbosity and inconsistency in the responses provided by LLMs"
  - [section 4] "In addition to evaluating the quality of the responses generated, we took into consideration those returned in another language than French. Indeed, whether a response is correct or partially correct is positive, but if this response is in English, it can become more difficult to exploit, especially when the targeted LLM responds randomly in French or English"
  - [corpus] Weak - no direct corpus evidence provided
- Break condition: LLMs consistently provide accurate responses in the target language without random switching between languages.

## Foundational Learning

- Concept: Historical context and nuance
  - Why needed here: Understanding the depth and complexity of historical questions is crucial for evaluating LLM responses beyond simple factual accuracy
  - Quick check question: Can you explain why a question about "the role of craftsmanship in Poitou villages from 1500-1800" requires more nuanced understanding than a question about a specific battle date?

- Concept: Language-specific evaluation criteria
  - Why needed here: The study specifically evaluates French language responses, requiring understanding of what constitutes acceptable French in academic/historical contexts
  - Quick check question: How would you assess whether a response that contains accurate historical information but switches between French and English is acceptable for French historians?

- Concept: Question typology and cognitive demands
  - Why needed here: Different question types (quantitative, qualitative closed, qualitative open) place different cognitive demands on LLMs, affecting their performance
  - Quick check question: What is the key difference in cognitive demand between asking for "the date of the Battle of Poitiers" versus asking "how craftsmanship was essential to village economies"?

## Architecture Onboarding

- Component map: Question bank (62 questions) -> LLM selection (10 models) -> Manual evaluation pipeline -> Response classification (5 categories) -> Language validation
- Critical path: Question → LLM → Response → Human Evaluation → Classification → Analysis
- Design tradeoffs: Manual evaluation provides depth and nuance but limits scalability; automated evaluation would be faster but miss subtle historical inaccuracies
- Failure signatures: Random language switching (French/English), verbose responses that bury key information, consistent failure on complex open-ended questions, lack of consistency across repeated queries
- First 3 experiments:
  1. Test the same question across all 10 LLMs to establish baseline performance variance
  2. Run the same question twice on each LLM to assess response consistency
  3. Compare performance on closed factual questions versus open analytical questions to identify capability gaps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural or training improvements would be needed for LLMs to achieve reliable historical accuracy?
- Basis in paper: [explicit] The study suggests that more specialized training on historical sources may be needed for improved performance, as current models show insufficient accuracy (33.6%) and issues like inconsistency and verbosity.
- Why unresolved: While the paper identifies the need for specialized training, it does not specify what exact architectural or training modifications would be required to address the current limitations in historical accuracy.
- What evidence would resolve it: Empirical studies comparing the performance of LLMs trained on specialized historical datasets versus general datasets, or controlled experiments testing different architectural modifications (e.g., fine-tuning, retrieval augmentation) on historical question answering tasks.

### Open Question 2
- Question: How do language-specific challenges impact LLM performance in historical question answering for languages other than English?
- Basis in paper: [explicit] The study highlights uneven treatment of the French language, with only 84.33% of responses in French and issues with code-switching. This suggests language-specific challenges may impact performance.
- Why unresolved: The paper focuses on French and does not explore how these challenges might manifest in other languages or what specific language-related factors (e.g., morphology, syntax, cultural context) contribute to performance differences.
- What evidence would resolve it: Comparative studies evaluating LLM performance on historical questions across multiple languages, analyzing error patterns and performance differences to identify language-specific challenges.

### Open Question 3
- Question: What is the optimal balance between model size and performance for historical question answering tasks?
- Basis in paper: [inferred] The study shows that larger models like GPT-4 (170B parameters) achieve higher accuracy (54.3%) than smaller models like Falcon (7B parameters) at 4.1%, but even the best model falls short of satisfactory performance. This suggests an unclear relationship between model size and historical accuracy.
- Why unresolved: While the paper provides data on different model sizes, it does not systematically investigate how performance scales with model size or identify the point of diminishing returns for historical question answering.
- What evidence would resolve it: Controlled experiments testing a range of model sizes on the same historical question answering task, measuring performance gains against computational costs to identify optimal size-performance trade-offs.

## Limitations

- The study relies on manual human evaluation, which introduces potential subjectivity and limits scalability of the assessment method
- Only 62 questions across five historical themes were tested, which may not be representative of the full breadth of historical inquiry
- The evaluation focuses specifically on French language responses, limiting generalizability to other languages and cultures
- The distinction between "partial" and "approximate" responses is not clearly defined, which could affect inter-rater reliability

## Confidence

- **High confidence**: The overall finding that current LLMs show insufficient accuracy for historical research (33.6% overall accuracy) is well-supported by the data and consistent across multiple models and question types.
- **Medium confidence**: The claim that language consistency is a significant issue is supported by the evaluation methodology but could benefit from more granular analysis of when and why language switching occurs.
- **Medium confidence**: The observation that question depth and complexity significantly affect LLM performance is supported by the data, though the sample size of 62 questions may limit generalizability to all historical question types.

## Next Checks

1. **Reproduce evaluation with expanded question set**: Test the same 10 LLMs on a larger, more diverse set of historical questions (minimum 200 questions across 10+ themes) to validate whether the 33.6% accuracy rate holds across broader historical domains.

2. **Automated vs human evaluation comparison**: Run the same responses through automated evaluation metrics alongside human expert evaluation to quantify the gap between human judgment and automated scoring for historical accuracy.

3. **Cross-language consistency test**: Systematically test each LLM's tendency to switch between French and English across different query types and measure the correlation between language switching and response accuracy to determine if language consistency affects factual correctness.