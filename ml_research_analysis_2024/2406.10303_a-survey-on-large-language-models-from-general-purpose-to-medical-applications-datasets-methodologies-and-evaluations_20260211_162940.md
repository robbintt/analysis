---
ver: rpa2
title: 'A Survey on Large Language Models from General Purpose to Medical Applications:
  Datasets, Methodologies, and Evaluations'
arxiv_id: '2406.10303'
source_url: https://arxiv.org/abs/2406.10303
tags:
- medical
- data
- llms
- training
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey systematically reviews the development of medical Large
  Language Models (LLMs) by summarizing dataset acquisition, training paradigms, and
  evaluation techniques. It analyzes 16 corpus sources and 4 data processing methods
  to guide customized medical dataset construction.
---

# A Survey on Large Language Models from General Purpose to Medical Applications: Datasets, Methodologies, and Evaluations

## Quick Facts
- arXiv ID: 2406.10303
- Source URL: https://arxiv.org/abs/2406.10303
- Reference count: 40
- This survey systematically reviews the development of medical Large Language Models (LLMs) by summarizing dataset acquisition, training paradigms, and evaluation techniques.

## Executive Summary
This comprehensive survey provides a systematic review of Large Language Models (LLMs) in medical applications, covering the evolution from general-purpose models to specialized medical implementations. The study analyzes 16 corpus sources and 4 data processing methods to guide customized medical dataset construction. It also classifies training paradigms into four types based on combinations of continued pretraining, instruction fine-tuning, and human alignment, while categorizing evaluation methods into machine-based and human-centric approaches.

## Method Summary
The survey employs a systematic literature review methodology, examining existing medical LLMs through multiple analytical lenses. It categorizes the development pipeline into three main areas: dataset acquisition (analyzing 16 corpus sources and 4 processing methods), training methodologies (classifying into 4 paradigm types), and evaluation techniques (dividing into machine-based and human-centric approaches). The analysis identifies key challenges including improving proactive questioning abilities, enabling multi-organization joint training with privacy preservation, and providing personalized diagnosis.

## Key Results
- Classification of 16 corpus sources and 4 data processing methods for medical dataset construction
- Categorization of training paradigms into 4 types based on combinations of pretraining, fine-tuning, and human alignment
- Identification of key challenges including proactive questioning, privacy-preserving joint training, and personalized diagnosis

## Why This Works (Mechanism)
The survey's methodology works by systematically decomposing the medical LLM development pipeline into three fundamental components: data, training, and evaluation. This structured approach allows for comprehensive analysis of each component while maintaining clear connections between them. The classification frameworks for datasets, training paradigms, and evaluation methods provide a standardized vocabulary for discussing medical LLM development and enable identification of gaps and opportunities in the field.

## Foundational Learning
1. **Medical Corpus Sources**: Understanding the diversity and quality of available medical datasets is crucial for selecting appropriate training materials. Quick check: Validate dataset quality metrics across the 16 identified sources.

2. **Data Processing Methods**: Proper preprocessing ensures model compatibility and performance. Quick check: Test each of the 4 processing methods on sample medical texts for consistency.

3. **Training Paradigms**: Different combinations of pretraining, fine-tuning, and alignment strategies yield varying model capabilities. Quick check: Map existing medical LLMs to the proposed 4-type classification system.

4. **Evaluation Metrics**: Both machine-based and human-centric evaluation methods are needed for comprehensive assessment. Quick check: Compare evaluation results from both approaches on sample models.

## Architecture Onboarding

**Component Map**: Dataset Sources -> Data Processing -> Model Training -> Evaluation

**Critical Path**: High-quality medical datasets processed appropriately form the foundation for effective model training, which must be validated through rigorous evaluation methods.

**Design Tradeoffs**: The survey highlights tradeoffs between dataset size and quality, training complexity versus model performance, and the balance between automated and human evaluation methods.

**Failure Signatures**: Common failure modes include inadequate dataset curation leading to biased outputs, insufficient fine-tuning resulting in poor domain adaptation, and evaluation methods that don't capture real-world medical use cases.

**First Experiments**:
1. Apply the 4 training paradigm classifications to a diverse sample of existing medical LLMs to validate the taxonomy's comprehensiveness
2. Test the proposed evaluation methods categorization by applying both machine-based and human-centric approaches to a subset of medical LLMs
3. Conduct a systematic quality assessment of the 16 identified corpus sources using standardized medical domain metrics

## Open Questions the Paper Calls Out
The survey identifies several open questions including how to improve proactive questioning abilities in medical LLMs, enable privacy-preserving multi-organization joint training, and provide personalized diagnosis while maintaining accuracy and reliability.

## Limitations
- The classification of training paradigms lacks detailed validation across diverse medical LLM implementations
- The criteria for including specific datasets and processing methods are not fully transparent
- Proposed future research directions remain somewhat generic without specific implementation pathways

## Confidence
- Survey methodology and classification framework: High
- Dataset and data processing analysis: Medium
- Training paradigm classification: Medium
- Evaluation methods categorization: High
- Future research directions: Medium

## Next Checks
1. Verify the practical applicability of the proposed training paradigm taxonomy by testing it against a diverse sample of existing medical LLMs to ensure comprehensive coverage and accurate classification.

2. Conduct a systematic review of the 16 identified corpus sources to assess their relative contributions, quality metrics, and potential biases in medical domain applications.

3. Implement a pilot study to evaluate the proposed evaluation methods categorization by applying both machine-based and human-centric approaches to a subset of medical LLMs and comparing the results for consistency and effectiveness.