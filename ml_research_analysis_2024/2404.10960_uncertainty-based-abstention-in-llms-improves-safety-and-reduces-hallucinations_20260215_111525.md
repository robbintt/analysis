---
ver: rpa2
title: Uncertainty-Based Abstention in LLMs Improves Safety and Reduces Hallucinations
arxiv_id: '2404.10960'
source_url: https://arxiv.org/abs/2404.10960
tags:
- uncertainty
- questions
- responses
- rlhf
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates uncertainty-based abstention in large
  language models (LLMs) to improve reliability across three scenarios: correctness,
  hallucinations, and safety. It explores two types of uncertainty: statistical uncertainty
  metrics (predictive entropy, semantic entropy, negative log-likelihood) and In-Dialogue
  Uncertainty (InDU), which measures hedging in responses.'
---

# Uncertainty-Based Abstention in LLMs Improves Safety and Reduces Hallucinations

## Quick Facts
- arXiv ID: 2404.10960
- Source URL: https://arxiv.org/abs/2404.10960
- Reference count: 40
- Primary result: Uncertainty-based abstention improves LLM reliability across correctness, safety, and hallucination scenarios using statistical and In-Dialogue Uncertainty metrics

## Executive Summary
This paper investigates uncertainty-based abstention in large language models (LLMs) to improve reliability across three scenarios: correctness, hallucinations, and safety. It explores two types of uncertainty: statistical uncertainty metrics (predictive entropy, semantic entropy, negative log-likelihood) and In-Dialogue Uncertainty (InDU), which measures hedging in responses. Experiments on Llama2 models (base and RLHF) show that statistical uncertainty improves correctness by 2–8% and safety by 70–99%, while InDU identifies ~50% of unanswerable questions, reducing hallucinations. RLHF finetuning preserves uncertainty awareness and enhances safety performance. The study demonstrates that uncertainty-based abstention is effective and computationally efficient for improving LLM reliability in high-stakes applications.

## Method Summary
The paper compares uncertainty-based abstention using two approaches: statistical uncertainty metrics that analyze token probability distributions, and In-Dialogue Uncertainty that counts hedge words in responses. It evaluates Llama2 models (7B, 13B, 70B) in both base and RLHF-fine-tuned versions across multiple datasets including TriviaQA, SciQA, CoQA, StrategyQA, GSM8K, and SelfAware. The study uses AUROC to measure uncertainty quality and Accuracy-Rejection Curves (ARCs) to assess abstention impact. For reproducibility, the method requires implementing the uncertainty metrics, loading the specified datasets, and comparing performance between base and RLHF models across the three reliability scenarios.

## Key Results
- Statistical uncertainty improves correctness by 2–8% and safety by 70–99% on Llama2 models
- In-Dialogue Uncertainty identifies ~50% of unanswerable questions, reducing hallucinations
- RLHF finetuning preserves uncertainty awareness while enhancing safety performance

## Why This Works (Mechanism)

### Mechanism 1
Statistical uncertainty metrics (predictive entropy, semantic entropy, negative log-likelihood) distinguish correct from incorrect responses by analyzing the spread of token probability distributions. Lower entropy indicates higher confidence and potentially correct answers, while higher entropy indicates uncertainty and potentially incorrect answers. Core assumption: token probability distributions reflect genuine uncertainty about response correctness.

### Mechanism 2
In-Dialogue Uncertainty (InDU) uses hedge words (like "perhaps," "maybe," "I don't know") to identify unanswerable questions. LLMs naturally use hedging language when uncertain, so counting hedge words provides a direct measure of verbalized uncertainty. Core assumption: hedge words correlate with genuine uncertainty about answerability, not just stylistic choices.

### Mechanism 3
RLHF fine-tuning preserves uncertainty awareness while improving safety. RLHF aligns models to safety preferences while maintaining their ability to express uncertainty through probability distributions. Statistical uncertainty metrics can then identify responses where the model is uncertain about safety. Core assumption: RLHF improves safety without eliminating the model's ability to express uncertainty.

## Foundational Learning

- **Concept: Uncertainty quantification in probabilistic models**
  - Why needed here: The paper relies on understanding how to measure and interpret uncertainty in generative models that output probability distributions over tokens
  - Quick check question: What is the difference between predictive entropy and semantic entropy in the context of LLM uncertainty?

- **Concept: Reinforcement Learning from Human Feedback (RLHF)**
  - Why needed here: The paper compares base models with RLHF-fine-tuned models to understand how alignment affects uncertainty awareness
  - Quick check question: How does RLHF typically affect the calibration and confidence of LLM responses?

- **Concept: Abstention strategies in classification vs. generation**
  - Why needed here: The paper draws parallels between traditional abstention in classification (refusing to classify when uncertain) and abstention in generation (refusing to answer when uncertain)
  - Quick check question: What are the key differences between abstention in classification tasks versus question-answering tasks?

## Architecture Onboarding

- **Component map**: Input Question/Prompt -> LLM Model (base or RLHF) -> Uncertainty Metric Computation (statistical or InDU) -> Thresholding Logic -> Output Answer or Abstention

- **Critical path**: 1. Generate model response 2. Compute uncertainty metric 3. Compare against threshold 4. Return answer or abstention

- **Design tradeoffs**: 
  - Statistical vs. InDU: Statistical is more general but may miss verbalized uncertainty; InDU is specific to hedge words but captures natural uncertainty expression
  - Threshold selection: Higher thresholds reduce false positives but may miss more uncertain cases
  - Computational cost: Statistical metrics add minimal overhead; InDU requires additional text processing

- **Failure signatures**:
  - False negatives: Model abstains when it should answer (high uncertainty threshold)
  - False positives: Model answers when it should abstain (low uncertainty threshold)
  - Calibration issues: Uncertainty metrics don't correlate with actual correctness/safety

- **First 3 experiments**:
  1. Compare AUROC of different uncertainty metrics on a held-out correctness dataset
  2. Test InDU effectiveness on a dataset with answerable vs. unanswerable questions
  3. Measure safety improvement using statistical uncertainty on RLHF-fine-tuned models with adversarial prompts

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the models effectively distinguish between incorrectly answered questions and unanswerable questions using statistical uncertainty measures? The paper found statistical uncertainty measures performed no better than random when distinguishing these cases.

- **Open Question 2**: How does RLHF finetuning affect the model's ability to express uncertainty through hedge words in responses to unanswerable questions? The paper observed RLHF models used more hedge words for unanswerable questions, suggesting increased uncertainty awareness.

- **Open Question 3**: Can combining statistical and In-Dialogue Uncertainty measures improve overall performance across different scenarios? The paper demonstrated each approach works well for different scenarios but didn't explore combining them.

## Limitations

- Statistical uncertainty metrics show inconsistent effectiveness, failing to discriminate answerable from unanswerable questions on certain datasets like GSM8K and SelfAware
- Reliance on hedge words for InDU may be culturally or linguistically biased across different languages and domains
- The study doesn't address how uncertainty thresholds should be dynamically adjusted for different application contexts

## Confidence

- **High confidence**: Statistical uncertainty metrics effectively improve correctness and safety in LLMs
- **Medium confidence**: RLHF preserves uncertainty awareness while improving safety
- **Medium confidence**: InDU is effective for identifying unanswerable questions

## Next Checks

1. **Cross-linguistic validation**: Test uncertainty metrics and InDU approach on multilingual datasets to verify whether hedging expressions and statistical uncertainty patterns generalize across languages.

2. **Dynamic threshold optimization**: Implement adaptive thresholding that adjusts uncertainty cutoffs based on domain-specific requirements and user risk tolerance, then measure improvement on GSM8K and SelfAware datasets.

3. **Longitudinal stability analysis**: Track how uncertainty metrics and InDU effectiveness change over time as models undergo additional RLHF iterations or adversarial training, testing whether RLHF preserves uncertainty awareness.