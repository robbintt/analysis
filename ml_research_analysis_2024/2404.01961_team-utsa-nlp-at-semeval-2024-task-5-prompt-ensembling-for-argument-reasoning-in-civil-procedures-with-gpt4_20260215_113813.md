---
ver: rpa2
title: 'Team UTSA-NLP at SemEval 2024 Task 5: Prompt Ensembling for Argument Reasoning
  in Civil Procedures with GPT4'
arxiv_id: '2404.01961'
source_url: https://arxiv.org/abs/2404.01961
tags:
- legal
- reasoning
- answer
- question
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of legal argument reasoning in
  civil procedure, a fundamental skill for law students, using prompt-based solutions
  with GPT4. The core method idea involves combining in-context learning, chain-of-thought
  reasoning, and retrieval-augmented generation to improve the model's performance.
---

# Team UTSA-NLP at SemEval 2024 Task 5: Prompt Ensembling for Argument Reasoning in Civil Procedures with GPT4

## Quick Facts
- arXiv ID: 2404.01961
- Source URL: https://arxiv.org/abs/2404.01961
- Authors: Dan Schumacher; Anthony Rios
- Reference count: 11
- Primary result: 0.7315 Macro F1 (5th out of 21 teams) on final test set

## Executive Summary
This paper addresses legal argument reasoning in civil procedure using prompt-based solutions with GPT-4. The authors evaluate an ensemble of prompting strategies including chain-of-thought reasoning and in-context learning to determine whether answer candidates are correct. Their approach combines retrieval-augmented generation, few-shot learning, and voting mechanisms to achieve competitive performance on the SemEval 2024 Task 5 benchmark.

## Method Summary
The method uses GPT-4 with a combination of retrieval-augmented in-context learning and chain-of-thought reasoning. For each test instance, LegalBERT embeddings find the most similar training examples, which are included as in-context examples in the prompt. The system requires the model to first generate an Analysis section explaining the reasoning, then a final Label (TRUE/FALSE). Four different prompting strategies are ensembled using voting with a threshold to produce the final prediction.

## Key Results
- Achieved 0.7315 Macro F1 score (5th place) on the final test set
- Validation dataset performance: 0.8095 Macro F1 and 0.8443 accuracy
- Error analysis revealed challenges with incorrect reasoning, shared introduction-question pairs, and similar language between introduction/question and answer candidates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-augmented in-context learning improves performance by providing relevant examples tailored to each test instance.
- Mechanism: The system uses LegalBERT embeddings to find the most similar training examples to each test instance based on cosine similarity. These relevant examples are then included in the prompt as in-context examples.
- Core assumption: The similarity metric (cosine similarity using LegalBERT embeddings) effectively identifies examples that share relevant context and reasoning patterns with the test instance.
- Evidence anchors:
  - [section]: "Given an input instance xi consisting of a concatenated Introduction, Question, and Answer triplet w, we retrieve the most similar examples {x1, . . . , xN (xi)}, where N (xi) are the k most similar examples to xi. Each question-answer pair is embedded using LegalBERT. The in-context examples all come from the provided training dataset."
  - [abstract]: "We evaluate an ensemble of prompting strategies, including chain-of-thought reasoning and in-context learning."

### Mechanism 2
- Claim: Chain-of-thought reasoning via explicit Analysis generation improves the model's ability to provide valid reasoning for its predictions.
- Mechanism: The prompt structure requires the model to first generate an Analysis section explaining why an answer is correct or incorrect before generating the final Label. This forces the model to articulate its reasoning process.
- Core assumption: Generating a detailed analysis before the final prediction helps the model ground its decision in explicit reasoning rather than pattern matching.
- Evidence anchors:
  - [abstract]: "We also evaluate an ensemble of prompting strategies, including chain-of-thought reasoning and in-context learning."
  - [section]: "The system prompt describes what the large language model (LLM) should do... The first section you will generate is a detailed step-by-step Analysis section that evaluates the validity of the Answer Candidate with a high amount of confidence. The second section you will generate is a final Label stating whether the Answer Candidate is TRUE or FALSE."

### Mechanism 3
- Claim: Ensembling multiple prompting strategies with voting improves robustness and overall performance.
- Mechanism: The system combines predictions from four different prompting approaches (Zero-shot, Zero-shot & CoT, Few-shot & CoT & RAG, and Few-shot & RAG) using a voting mechanism with a threshold to make the final prediction.
- Core assumption: Different prompting strategies capture different aspects of the task, and combining them through voting reduces the impact of individual strategy weaknesses.
- Evidence anchors:
  - [section]: "Overall, there are a total of 4 models in our ensemble... To make a prediction, we use voting with a threshold (i.e., where the votes are processed to generate the proportion of TRUE values)."
  - [abstract]: "Moreover, we explore an ensemble of multiple approaches."

## Foundational Learning

- Concept: Legal argument reasoning and the structure of civil procedure questions
  - Why needed here: The task requires understanding how to evaluate whether an answer candidate correctly addresses a question given legal context. Without this domain knowledge, a model cannot effectively learn the reasoning patterns needed.
  - Quick check question: Can you explain the difference between a TRUE and FALSE label in this task, and what types of reasoning errors would lead to incorrect predictions?

- Concept: Prompt engineering with large language models
  - Why needed here: The entire system relies on carefully constructed prompts that guide the LLM through the reasoning process. Understanding prompt structure, in-context learning, and chain-of-thought prompting is essential.
  - Quick check question: How does the inclusion of in-context examples in a prompt differ from traditional fine-tuning approaches?

- Concept: Retrieval-augmented generation and embedding-based similarity
  - Why needed here: The system uses LegalBERT embeddings and cosine similarity to retrieve relevant in-context examples. Understanding how embeddings capture semantic similarity is crucial for this component.
  - Quick check question: What are the advantages and limitations of using cosine similarity with LegalBERT embeddings for finding relevant examples?

## Architecture Onboarding

- Component map:
  Input preprocessor -> Retrieval module -> Prompt constructor -> LLM inference engine -> Post-processor -> Ensemble voter

- Critical path:
  1. Receive test instance
  2. Embed instance and retrieve similar training examples
  3. Construct prompt with system prompt, examples, and test instance
  4. Send prompt to GPT-4
  5. Parse response to extract label
  6. If label missing, resubmit with explicit instruction
  7. Combine with other strategy predictions via voting
  8. Return final prediction

- Design tradeoffs:
  - Number of in-context examples (2 used) vs. token limits and cost
  - Choice of similarity metric (cosine with LegalBERT) vs. alternatives
  - Temperature setting (.7 used) vs. deterministic output
  - Threshold for ensemble voting (.5 used) vs. precision-recall tradeoff
  - Manual error handling vs. automated parsing robustness

- Failure signatures:
  - Missing labels in LLM output requiring resubmission
  - Inconsistent predictions across ensemble members
  - High false positive rate on examples with shared introduction-question pairs
  - Poor performance when introduction text is significantly longer than average

- First 3 experiments:
  1. Ablation study: Compare Few-Shot & CoT & RAG performance against variants with individual components removed (no RAG, no CoT, no in-context examples)
  2. Retrieval analysis: Examine the quality of retrieved examples by checking if top examples actually share reasoning patterns with test instances
  3. Threshold optimization: Test different voting thresholds on validation data to find the optimal balance between precision and recall

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the model be improved to better handle long introductions and avoid ignoring relevant context?
- Basis in paper: [explicit] The paper mentions that three of the ten false positives shared the same introduction and question pair, where the introduction contained more extraneous information than usual and was 128 words longer than the average. The model would analyze the answer candidate as correct but without taking into account the particular case that was asked about.
- Why unresolved: The paper does not provide a solution to this issue and only mentions it as a potential problem.
- What evidence would resolve it: Developing a model that can effectively process and extract relevant information from long introductions, and testing its performance on similar datasets.

### Open Question 2
- Question: How can the chain-of-thought reasoning be improved to provide step-by-step explanations for answers?
- Basis in paper: [explicit] The paper mentions that the Analysis section used for chain-of-thought reasoning does not match traditional methods which use step-by-step reasoning. It suggests that a logical next extension is to reword the Analysis section to provide a step-by-step explanation for an answer.
- Why unresolved: The paper does not provide a solution to this issue and only mentions it as a potential area for future research.
- What evidence would resolve it: Developing a method to reword the Analysis section to provide step-by-step explanations, and testing its impact on model performance.

### Open Question 3
- Question: How can the model be improved to better handle answer candidates with correct answers but incorrect reasoning?
- Basis in paper: [explicit] The paper mentions that one of the false positives occurred when the answer candidate had the correct answer but incorrect reasoning. The model would analyze the answer candidate as correct but without taking into account the flawed reasoning.
- Why unresolved: The paper does not provide a solution to this issue and only mentions it as a potential problem.
- What evidence would resolve it: Developing a model that can better evaluate the reasoning behind an answer candidate, and testing its performance on similar datasets.

## Limitations

- The system shows significant performance degradation from validation (0.8095 F1) to test set (0.7315 F1), suggesting potential overfitting or distribution shift issues.
- Error analysis reveals critical limitations with shared introduction-question pairs and similar language patterns that confuse the model's reasoning.
- The approach relies heavily on manual error handling (resubmitting examples when labels are missing), indicating brittleness in the LLM output parsing.

## Confidence

**High Confidence**: The core methodology of combining retrieval-augmented in-context learning with chain-of-thought reasoning is well-specified and theoretically sound. The validation results (0.8095 F1) demonstrate that this approach works effectively on the development data.

**Medium Confidence**: The ensemble approach's contribution to the final performance is partially supported but not rigorously evaluated. While the paper claims ensembling improves robustness, no ablation studies demonstrate the individual components' contributions or optimal voting thresholds.

**Low Confidence**: The generalizability of the approach beyond this specific legal reasoning task remains unproven. The significant performance gap between validation and test sets raises questions about the method's robustness to domain-specific variations in legal text.

## Next Checks

1. **Ablation Study**: Systematically remove each component (RAG, CoT, in-context examples) from the Few-Shot & CoT & RAG approach to quantify individual contributions to the 0.8095 validation F1 score.

2. **Threshold Sensitivity Analysis**: Evaluate the ensemble performance across a range of voting thresholds (0.3 to 0.7) on validation data to determine optimal calibration and assess sensitivity to this critical hyperparameter.

3. **Error Pattern Investigation**: Manually examine 50 false positive and 50 false negative predictions to classify error types and determine whether shared introduction-question pairs or similar language patterns are indeed the dominant failure modes.