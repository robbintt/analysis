---
ver: rpa2
title: 'Functional Graph Convolutional Networks: A unified multi-task and multi-modal
  learning framework to facilitate health and social-care insights'
arxiv_id: '2403.10158'
source_url: https://arxiv.org/abs/2403.10158
tags: []
core_contribution: This paper introduces the Functional Graph Convolutional Network
  (funGCN), a unified framework that combines Functional Data Analysis and Graph Convolutional
  Networks to handle multi-task and multi-modal learning in digital health. The method
  addresses the challenge of analyzing multivariate longitudinal data with different
  modalities, including longitudinal, categorical, and scalar variables, while ensuring
  interpretability even with small sample sizes.
---

# Functional Graph Convolutional Networks: A unified multi-task and multi-modal learning framework to facilitate health and social-care insights

## Quick Facts
- arXiv ID: 2403.10158
- Source URL: https://arxiv.org/abs/2403.10158
- Reference count: 21
- Primary result: funGCN integrates FDA and GCNs to handle multi-modal longitudinal data, enabling classification, regression, and forecasting with interpretable knowledge graphs

## Executive Summary
This paper introduces the Functional Graph Convolutional Network (funGCN), a unified framework that combines Functional Data Analysis and Graph Convolutional Networks to handle multi-task and multi-modal learning in digital health. The method addresses the challenge of analyzing multivariate longitudinal data with different modalities, including longitudinal, categorical, and scalar variables, while ensuring interpretability even with small sample sizes. Key innovations include task-specific embeddings for different data types, the ability to perform classification, regression, and forecasting concurrently, and the creation of a knowledge graph for insightful data interpretation.

## Method Summary
funGCN processes multivariate longitudinal data by first smoothing and embedding longitudinal features into functional bases (FPCs or B-splines) using Functional Data Analysis. A node-wise functional feature selection procedure then constructs a knowledge graph representing relationships among variables. The GCN module uses task-specific embeddings: FPCs for the knowledge graph to capture major modes of variation and detect global relationships, and B-splines for localized temporal features in the GCN to preserve past-future distinctions for forecasting. The framework is validated on both simulated and real data (SHARE dataset), demonstrating its capability to construct informative knowledge graphs and perform multi-task predictions.

## Key Results
- funGCN successfully handles multivariate longitudinal data with mixed modalities (longitudinal, categorical, scalar)
- The framework constructs interpretable knowledge graphs through node-wise feature selection
- Achieves concurrent performance across classification, regression, and forecasting tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: funGCN integrates Functional Data Analysis (FDA) with Graph Convolutional Networks (GCNs) to jointly handle multi-modal longitudinal data and capture complex variable relationships.
- Mechanism: FDA smooths and embeds longitudinal features into finite-dimensional functional bases (e.g., FPCs or B-splines), while GCNs use these embeddings along with a knowledge graph to propagate information across variable nodes and perform multi-task learning.
- Core assumption: The functional basis representation preserves temporal dependencies and enables meaningful inter-variable relationships to be detected by GCN layers.
- Evidence anchors:
  - [abstract] "Key innovations include task-specific embedding components that manage different data types, the ability to perform classification, regression, and forecasting, and the creation of a knowledge graph for insightful data interpretation."
  - [section 2.1] "FDA a dynamic area of statistical research that allows working with multivariate longitudinal data by estimating smooth curves across a continuous domain."
  - [corpus] The corpus lacks direct references to FDA-GCN integration, suggesting novelty or limited overlap in the literature.
- Break condition: If the functional basis is too low-dimensional, temporal dynamics are lost and GCNs cannot recover missing information.

### Mechanism 2
- Claim: The node-wise functional feature selection module constructs a knowledge graph that quantifies relationships among all variables, enabling efficient multi-task learning.
- Mechanism: For each variable as target, a functional feature selection procedure iteratively selects related features based on their importance in predicting the target, recording selection order as edge weights. The resulting adjacency matrix guides GCN focus to significant connections.
- Core assumption: The node-wise selection captures meaningful relationships and that these relationships are stable enough to generalize across different tasks.
- Evidence anchors:
  - [section 2.2] "The aim of the node-wise functional feature selection module is the creation of a knowledge graph with nodes representing the p features and edges indicating their association strength."
  - [algorithm 1] "Perform functional feature selection...SET target = j and penalty λ = cλλmax."
  - [corpus] No direct corpus matches for node-wise functional feature selection, indicating it may be a novel methodological contribution.
- Break condition: If the selection process is too aggressive (pmax too small), the graph becomes too sparse and loses important predictive links.

### Mechanism 3
- Claim: Task-specific embeddings (kgraph vs kgcn) ensure the right balance between interpretability (via FPCs) and forecasting accuracy (via localized B-splines).
- Mechanism: For the knowledge graph, kgraph FPCs are used to capture major modes of variation in longitudinal curves and detect global relationships. For the GCN, kgcn B-splines provide localized, interpretable temporal features that preserve the distinction between past and future coefficients.
- Core assumption: The chosen basis system for each embedding aligns with the specific goal: FPCs for relationship detection, B-splines for accurate time-domain forecasting.
- Evidence anchors:
  - [section 2.1] "FPCs capture the directions of larger variability of the curves and are a well-established technique in FDA...cubic B-splines are non-zero over a limited range, defined by a three-knot interval."
  - [section 2.1] "B-splines...offer a flexible yet stable method for curves representation...localized influence is crucial for effective forecasting while preserving the original embedded form."
  - [corpus] Limited corpus overlap on specific basis choice strategies, suggesting potential novelty.
- Break condition: If kgcn is too small, forecasting loses temporal resolution; if too large, it risks overfitting with limited data.

## Foundational Learning

- Functional Data Analysis
  - Why needed here: Enables smooth representation of irregularly sampled longitudinal variables and extraction of basis coefficients for GCN input.
  - Quick check question: Can you explain how basis expansions convert a continuous curve into a finite-dimensional vector?

- Graph Convolutional Networks
  - Why needed here: Propagate information across variables using the learned knowledge graph, enabling multi-task predictions on multimodal data.
  - Quick check question: How does the adjacency matrix in a GCN influence feature propagation between nodes?

- Node-wise Feature Selection
  - Why needed here: Builds a parsimonious and interpretable knowledge graph by identifying important predictors for each target variable.
  - Quick check question: What is the role of the penalty parameter λ in the node-wise selection algorithm?

## Architecture Onboarding

- Component map:
  1. Input preprocessing → smoothing & basis expansion (FDA)
  2. KG embedding module → FPCs → knowledge graph construction
  3. GCN embedding module → B-splines → tensor preparation
  4. Knowledge graph → adjacency matrix from node-wise selection
  5. GCN module → convolution + ReLU → task outputs
  6. Post-processing → inverse basis mapping + nearest-neighbor for categoricals

- Critical path:
  Embedding → Knowledge graph estimation → GCN training → Inverse mapping

- Design tradeoffs:
  - High kgcn increases forecasting accuracy but risks overfitting with small samples.
  - Large pmax in node-wise selection yields richer graphs but increases computation.
  - FPC vs B-spline choice trades interpretability for forecasting precision.

- Failure signatures:
  - Poor regression/forecast accuracy → check basis dimensionality or adjacency sparsity.
  - Slow training → inspect pmax and kgcn values.
  - Unstable categorical predictions → verify nearest-neighbor mapping quality.

- First 3 experiments:
  1. Train with kgcn=10, pmax=5, θ=0.7 on a small synthetic dataset; check adjacency density.
  2. Run classification task only; verify categorical embedding mapping.
  3. Compare forecasting with and without FPC basis to assess temporal fidelity.

## Open Questions the Paper Calls Out
None

## Limitations
- The core innovation—node-wise functional feature selection for knowledge graph construction—lacks validation against established feature selection methods
- The choice of basis functions (FPCs vs B-splines) for different tasks is not empirically justified beyond stated interpretability benefits
- Limited discussion of computational complexity for large-scale applications

## Confidence
- Mechanism 1 (FDA+GCN integration): Medium - The theoretical framework is sound, but empirical validation of temporal dynamics preservation is limited
- Mechanism 2 (Node-wise feature selection): Low - Novel method with no comparative analysis or sensitivity testing
- Mechanism 3 (Task-specific embeddings): Medium - Reasonable design choice but lacks ablation studies to confirm necessity

## Next Checks
1. Perform ablation study comparing funGCN with standard GCNs using identical basis representations
2. Conduct sensitivity analysis on node-wise selection parameters (θ, pmax) to assess stability of knowledge graph construction
3. Validate temporal fidelity by comparing forecasting accuracy when using FPCs vs B-splines in the GCN module