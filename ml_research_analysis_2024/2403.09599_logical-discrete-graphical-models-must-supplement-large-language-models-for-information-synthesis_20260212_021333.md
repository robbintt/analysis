---
ver: rpa2
title: Logical Discrete Graphical Models Must Supplement Large Language Models for
  Information Synthesis
arxiv_id: '2403.09599'
source_url: https://arxiv.org/abs/2403.09599
tags:
- logical
- language
- reasoning
- which
- graphical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper identifies four critical limitations of large language
  models for information synthesis: hallucinations, complex reasoning, planning under
  uncertainty, and complex calculations. It proposes using logical discrete graphical
  models as a supplement to overcome these issues.'
---

# Logical Discrete Graphical Models Must Supplement Large Language Models for Information Synthesis

## Quick Facts
- arXiv ID: 2403.09599
- Source URL: https://arxiv.org/abs/2403.09599
- Reference count: 9
- Large language models require logical discrete graphical models to address hallucinations, complex reasoning, planning, and calculation limitations

## Executive Summary
This paper identifies four critical limitations of large language models (LLMs) for information synthesis: hallucinations, complex reasoning, planning under uncertainty, and complex calculations. The proposed solution is to supplement LLMs with logical discrete graphical models that can provide causal explanations, perform exact logical inference, reason over multiple scenarios, and incorporate function calling. The approach converts natural language text into logical representations using first-order logic and probabilistic graphical models, enabling provable elimination of hallucinations and improved reasoning capabilities.

## Method Summary
The method involves converting natural language text into a logical representation using first-order logic and probabilistic graphical models with three levels of graphical structure: knowledge graph, implication graph, and proposition graph. The system uses Horn clauses and different logical fragments (Direct, Query, Planning) to provide various reasoning capabilities. Training is performed from unlabeled text using expectation-maximization, with inference conducted through loopy belief propagation and message passing algorithms.

## Key Results
- Logical discrete graphical models can provably eliminate hallucinations by providing causal explanations for answers
- Complex reasoning chains can be handled through exact logical inference rather than probabilistic approximation
- Planning under uncertainty is supported by reasoning over multiple scenarios using disjunctive conclusions
- Function calling can be incorporated for complex calculations that exceed LLM capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Logical discrete graphical models eliminate hallucinations by providing causal explanations for answers
- Mechanism: The model explicitly represents causal relationships through first-order logic and probabilistic graphical models, allowing it to trace back the reasoning chain for any answer given. When asked a question, the model can show exactly which premises led to the conclusion.
- Core assumption: That explicit causal reasoning prevents hallucination by requiring evidence for all conclusions
- Evidence anchors:
  - [abstract] "logical discrete graphical models can provably eliminate hallucinations by providing causal explanations for answers"
  - [section 4.2] "Our analysis is that the concept of hallucination is closely related to the concept of causality. In other words, if we have a model that can explain why it is giving an answer, assuming that this method of explanation is sensible, can never hallucinate"
  - [corpus] Weak evidence - corpus neighbors don't directly address hallucination elimination
- Break condition: If the causal explanation mechanism itself becomes too complex to trace, or if users cannot understand or verify the explanations provided

### Mechanism 2
- Claim: Logical discrete graphical models handle complex reasoning chains through exact logical inference
- Mechanism: By representing knowledge as Horn clauses and using theorem-proving techniques, the model can perform exact logical reasoning rather than probabilistic approximation. This allows handling of long chains of inference without the compounding errors that occur in neural approaches.
- Core assumption: That logical inference is more reliable than probabilistic inference for complex reasoning tasks
- Evidence anchors:
  - [abstract] "handle complex reasoning chains through exact logical inference"
  - [section 5.1] "Exact Reasoning" - discusses how logical models can perform exact reasoning unlike LLM approaches
  - [section 3.1.1] Explains Horn clauses and their properties for efficient reasoning
- Break condition: When reasoning chains become too long or complex for the logical system to handle efficiently, or when the knowledge base becomes too large for exact inference to remain tractable

### Mechanism 3
- Claim: Logical discrete graphical models support planning under uncertainty by reasoning over multiple scenarios
- Mechanism: The Planning fragment (Section 3.1.3) allows statements with disjunctions in conclusions, enabling the model to consider multiple possible outcomes and their probabilities. This supports planning by evaluating different scenarios and their likelihoods.
- Core assumption: That explicit modeling of uncertainty through disjunctive conclusions improves planning capabilities
- Evidence anchors:
  - [abstract] "support planning by reasoning over multiple scenarios"
  - [section 3.1.3] "The Planning fragment allows statements with a disjunction in the conclusion" and discusses relation to planning under uncertainty
  - [section 5.2] "Long 'Chains' of Reasoning" - discusses planning capabilities
- Break condition: When the number of possible scenarios becomes combinatorially explosive, making planning computationally intractable

## Foundational Learning

- Concept: First-order logic and theorem proving
  - Why needed here: The entire approach is built on representing knowledge and reasoning in first-order logic
  - Quick check question: Can you explain the difference between propositional and first-order logic, and why first-order logic is needed for this approach?

- Concept: Probabilistic graphical models
  - Why needed here: The model combines logical reasoning with probabilistic inference to handle uncertainty
  - Quick check question: How does a Bayesian network differ from a logical knowledge base, and why combine them?

- Concept: Horn clauses and logical fragments
  - Why needed here: Different logical fragments (Direct, Query, Planning) provide different reasoning capabilities with different computational complexities
  - Quick check question: What are the computational trade-offs between the Direct, Query, and Planning fragments?

## Architecture Onboarding

- Component map:
  - Knowledge Graph: Stores probabilistic relationships between entities
  - Implication Graph: Learns general patterns and implications
  - Proposition Graph: Lazily constructs specific propositions for queries
  - Conjunction/Disjunction Nodes: Implement logical operations
  - Parser: Converts natural language to logical representation
  - Inference Engine: Performs logical and probabilistic reasoning

- Critical path:
  1. Parse natural language query into logical representation
  2. Construct proposition graph for the query
  3. Perform inference using message passing
  4. Generate natural language answer with causal explanation

- Design tradeoffs:
  - Exact vs. approximate inference (computational cost vs. accuracy)
  - Logical expressiveness vs. tractability (more complex logic is harder to compute)
  - Lazy vs. eager construction of proposition graph (memory vs. latency)

- Failure signatures:
  - Inference takes too long (graph too complex or inference algorithm not converging)
  - Parser fails to create valid logical representation (natural language too ambiguous)
  - Model gives wrong answers despite valid reasoning (knowledge base incomplete or incorrect)

- First 3 experiments:
  1. Implement a simple Horn clause knowledge base and test exact reasoning on small examples
  2. Add probabilistic reasoning to the knowledge base and test inference with uncertainty
  3. Build a basic parser that converts simple natural language questions to logical queries and test end-to-end reasoning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal method for training logical discrete graphical models from unlabeled text using expectation-maximization?
- Basis in paper: [explicit] The paper outlines a method for training such models from unlabeled text using expectation-maximization, but does not provide specific implementation details or empirical results.
- Why unresolved: The paper only briefly mentions the use of expectation-maximization for training, without discussing the specific algorithms, convergence properties, or performance characteristics.
- What evidence would resolve it: Empirical studies comparing different EM variants (e.g., 1-best vs n-best EM) for logical model training, with quantitative results on convergence speed, final model quality, and comparison to baseline methods.

### Open Question 2
- Question: How does the performance of logical discrete graphical models compare to large language models for complex reasoning tasks that require multiple viewpoints or long chains of reasoning?
- Basis in paper: [explicit] The paper claims that logical models can handle complex reasoning chains through exact logical inference and support multiple viewpoints, but does not provide empirical comparisons to LLM performance on such tasks.
- Why unresolved: While the paper provides theoretical arguments for why logical models should outperform LLMs on complex reasoning, it lacks empirical validation against established LLM benchmarks or real-world reasoning tasks.
- What evidence would resolve it: Systematic experiments comparing logical graphical models against state-of-the-art LLMs on standardized reasoning benchmarks, particularly those requiring multi-step reasoning or viewpoint synthesis, with quantitative performance metrics.

### Open Question 3
- Question: What is the computational complexity and scalability of inference in logical discrete graphical models compared to large language models for real-time applications?
- Basis in paper: [inferred] The paper mentions that certain logical fragments have specific complexity bounds (e.g., linear for Direct fragment, NP-hard for Planning fragment), but does not analyze the practical inference time or memory requirements for large-scale knowledge bases.
- Why unresolved: The paper provides theoretical complexity analysis but lacks empirical data on inference speed, memory usage, and scalability limits when applied to realistic knowledge bases or deployed in production systems.
- What evidence would resolve it: Benchmark studies measuring inference time and memory consumption for logical models on various knowledge base sizes, compared to equivalent LLM inference times, including analysis of bottlenecks and optimization strategies.

## Limitations
- Computational complexity of exact logical inference for real-world knowledge bases remains unproven
- Expectation-maximization training procedure lacks specific implementation details
- Integration of function calling with logical reasoning framework not demonstrated with concrete examples

## Confidence

- **High confidence**: Identification of four critical limitations of LLMs (hallucinations, complex reasoning, planning under uncertainty, and complex calculations)
- **Medium confidence**: Theoretical framework for combining logical discrete graphical models with LLMs
- **Low confidence**: Practical scalability and implementation details of the proposed approach

## Next Checks

1. Implement a proof-of-concept system on a small knowledge base and benchmark inference time vs. accuracy trade-offs as the knowledge base scales
2. Test the hallucination elimination claim on controlled examples where ground truth is known, measuring both the correctness of answers and the quality of causal explanations
3. Evaluate the planning under uncertainty capability on a benchmark planning task, comparing against pure LLM approaches in terms of solution quality and computational efficiency