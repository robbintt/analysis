---
ver: rpa2
title: 'YuLan-Mini: An Open Data-efficient Language Model'
arxiv_id: '2412.17743'
source_url: https://arxiv.org/abs/2412.17743
tags:
- data
- training
- arxiv
- https
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents YuLan-Mini, a highly capable base language
  model with 2.42 billion parameters. The authors focus on improving training efficacy
  through three key technical contributions: an elaborate data pipeline combining
  data cleaning with scheduling strategies, a robust optimization method to mitigate
  training instability, and an effective annealing approach incorporating targeted
  data selection and long context training.'
---

# YuLan-Mini: An Open Data-efficient Language Model

## Quick Facts
- **arXiv ID**: 2412.17743
- **Source URL**: https://arxiv.org/abs/2412.17743
- **Reference count**: 40
- **Primary result**: 2.42B parameter model achieving 37.80 on MATH-500 (4-shot), 64.00 on HumanEval (0-shot), 49.10 on MMLU (5-shot)

## Executive Summary
YuLan-Mini is a 2.42 billion parameter base language model designed for data-efficient training. The authors present a comprehensive approach that combines advanced data processing techniques, robust optimization methods, and targeted training strategies. The model achieves competitive performance across multiple benchmarks while being trained on 1.08 trillion tokens. The paper emphasizes transparency by releasing detailed information about data composition and training methodology.

## Method Summary
The model employs three key technical innovations: an elaborate data pipeline that combines data cleaning with scheduling strategies, a robust optimization method to mitigate training instability, and an effective annealing approach incorporating targeted data selection and long context training. The training process leverages 1.08 trillion tokens with careful attention to data quality and diversity. The authors provide full details of data composition across training phases to enable reproducibility.

## Key Results
- Achieves 37.80 on MATH-500 (four-shot)
- Achieves 64.00 on HumanEval (zero-shot)
- Achieves 49.10 on MMLU (five-shot)
- Trained on 1.08 trillion tokens
- Competitive performance among models of similar parameter scale

## Why This Works (Mechanism)
The success of YuLan-Mini stems from three interconnected mechanisms: (1) the data pipeline ensures high-quality training data through systematic cleaning and intelligent scheduling, preventing model degradation from noisy inputs; (2) the robust optimization method stabilizes training dynamics, allowing effective learning across diverse data distributions; (3) the annealing approach with targeted data selection and long context training enables the model to develop sophisticated reasoning capabilities while maintaining efficiency.

## Foundational Learning

**Data Cleaning** - Why needed: Removes low-quality or irrelevant data that can degrade model performance. Quick check: Compare training loss curves with and without cleaning pipeline.

**Data Scheduling** - Why needed: Controls data diversity exposure over training time for optimal learning progression. Quick check: Monitor perplexity changes across different scheduling strategies.

**Optimization Stability** - Why needed: Prevents training collapse and ensures consistent gradient updates. Quick check: Track gradient norms and loss stability metrics.

**Annealing Strategies** - Why needed: Gradually adjusts learning rates and data difficulty for effective convergence. Quick check: Validate performance improvements at different annealing stages.

**Long Context Training** - Why needed: Enables models to handle extended reasoning tasks and maintain coherence. Quick check: Test performance on tasks requiring extended context windows.

## Architecture Onboarding

**Component Map**: Data Pipeline -> Optimization Method -> Annealing Strategy -> Model Training

**Critical Path**: Data cleaning → scheduling → robust optimization → annealing → performance evaluation

**Design Tradeoffs**: 
- Data quality vs. quantity (1.08T tokens)
- Training stability vs. computational efficiency
- Model size vs. performance gains

**Failure Signatures**: 
- Training instability (oscillating loss, gradient explosion)
- Degraded performance on reasoning tasks
- Poor generalization across domains

**Three First Experiments**:
1. Compare performance with and without data cleaning pipeline
2. Test different optimization configurations for stability
3. Evaluate impact of long context training on reasoning benchmarks

## Open Questions the Paper Calls Out

None specified in the provided information.

## Limitations

- Lack of detailed experimental validation for core technical contributions
- Absence of ablation studies for individual components
- Limited evaluation across diverse domains and tasks
- No comparative analysis with other contemporary models

## Confidence

| Claim | Confidence |
|-------|------------|
| Data pipeline effectiveness | Medium |
| Optimization method stability | Medium |
| Performance metrics | Medium |
| Reproducibility of results | Medium |

## Next Checks

1. Conduct ablation studies to isolate the impact of data cleaning and scheduling strategies on training stability and model performance
2. Perform cross-domain evaluations to assess the model's robustness and generalization capabilities beyond reported benchmarks
3. Release full dataset composition and training scripts to enable independent reproduction and verification of results