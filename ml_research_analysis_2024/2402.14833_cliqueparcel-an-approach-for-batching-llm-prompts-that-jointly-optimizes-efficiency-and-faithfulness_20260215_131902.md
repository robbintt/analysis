---
ver: rpa2
title: 'CliqueParcel: An Approach For Batching LLM Prompts That Jointly Optimizes
  Efficiency And Faithfulness'
arxiv_id: '2402.14833'
source_url: https://arxiv.org/abs/2402.14833
tags:
- ciency
- ratio
- faithfulness
- weight
- batching
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CliqueParcel, a framework that improves the
  efficiency of large language model (LLM) inference by batching prompts across users
  while maintaining output faithfulness. The authors define a novel "discounted output"
  problem where efficiency gains from batching often come at the cost of reduced answer
  completeness or accuracy.
---

# CliqueParcel: An Approach For Batching LLM Prompts That Jointly Optimizes Efficiency And Faithfulness

## Quick Facts
- arXiv ID: 2402.14833
- Source URL: https://arxiv.org/abs/2402.14833
- Authors: Jiayi Liu; Tinghan Yang; Jennifer Neville
- Reference count: 40
- One-line primary result: Achieves up to 207.25% improvement in weighted efficiency with minimal loss in accuracy or completeness by batching prompts into semantically coherent "cliques"

## Executive Summary
CliqueParcel is a framework that improves the efficiency of large language model inference by batching prompts across users while maintaining output faithfulness. The authors identify a "discounted output" problem where efficiency gains from batching often come at the cost of reduced answer completeness or accuracy. CliqueParcel addresses this by grouping prompts into semantically coherent "cliques" and batching them for joint processing, followed by a dispatch step to reconstruct individual answers. The framework introduces new metrics for efficiency and faithfulness, accounting for output length and semantic fidelity.

## Method Summary
CliqueParcel groups prompts into semantically coherent "cliques" using different clique functions (concept, random, semantic similarity, etc.), batches them for joint processing, and reconstructs individual answers. The framework uses a small pre-model to classify prompts by clique domain before batching. It measures efficiency using a weighted relative efficiency ratio that considers input/output token lengths, and faithfulness using semantic cosine similarity, BLEU/ROUGE scores, and accuracy. Experiments across eight datasets (reading comprehension, QA, and reasoning) show significant efficiency improvements while maintaining answer quality.

## Key Results
- Achieves up to 207.25% improvement in weighted efficiency compared to standard prompting
- Random clique function (RC) consistently outperformed other batching strategies in balancing efficiency and faithfulness
- Maintains minimal loss in accuracy or completeness across all tested datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Batching prompts with the same semantic domain (cliques) allows parallel processing of related queries, reducing total runtime.
- Mechanism: Prompts sharing a "clique domain" are grouped, concatenated into a single large prompt, processed together, and then answers are dispatched back to individual users. This reduces redundant computation.
- Core assumption: Prompts within the same clique domain share enough semantic overlap that the model can efficiently process them in a single pass without loss of faithfulness.
- Evidence anchors: [abstract] "While ensuring accuracy and minimizing deviations from the original outputs (i.e., faithfulness), our method significantly improves efficiency during inference." [section] "First, we propose a small pre-model before LLM to classify the prompts according to their clique domain."
- Break condition: If prompts in a clique have high semantic diversity, the model may produce confused or incomplete outputs, reducing faithfulness.

### Mechanism 2
- Claim: Random clique batching achieves a good balance between efficiency and faithfulness.
- Mechanism: Randomly grouping prompts avoids over-optimizing for specific semantic patterns that could bias the model's output, leading to more consistent performance across different datasets.
- Core assumption: Random grouping prevents overfitting to specific semantic patterns and thus maintains generality.
- Evidence anchors: [abstract] "Experiments across eight datasets ... show that CliqueParcel achieves up to 207.25% improvement in weighted efficiency ... The random clique function (RC) consistently outperformed other batching strategies in balancing efficiency and faithfulness." [section] "We consider the following types of clique function /u1D43B: ... (2) The random clique (RC) function /u1D43Bgroups prompts randomly."
- Break condition: If the random grouping accidentally clusters highly unrelated prompts, the model may struggle to produce coherent answers.

### Mechanism 3
- Claim: The "discounted output" problem is mitigated by measuring efficiency excluding output length reduction.
- Mechanism: The authors redefine efficiency to factor in input and output token lengths, ensuring that efficiency gains are not simply due to producing shorter, incomplete answers.
- Core assumption: Previous efficiency metrics conflated batching efficiency with incomplete output generation, leading to misleading comparisons.
- Evidence anchors: [abstract] "Existing strategies to optimize inference efficiency often compromise on output quality, leading to a 'discounted output' problem." [section] "To lay the groundwork, we first redefine efficiency measurements by excluding the reduction in running time due to shorter lengths."
- Break condition: If the new metric fails to capture real efficiency gains or becomes too conservative, it may underrepresent the benefits of batching.

## Foundational Learning

- Concept: Efficiency measurement in LLM inference
  - Why needed here: To evaluate the true benefit of batching without being misled by shorter outputs.
  - Quick check question: Why does excluding output length from efficiency calculations give a fairer comparison?

- Concept: Faithfulness in NLP outputs
  - Why needed here: To ensure that batching does not compromise the completeness or accuracy of generated answers.
  - Quick check question: What metrics are used to quantify faithfulness in this paper?

- Concept: Semantic similarity and clustering
  - Why needed here: To group prompts into coherent "cliques" that can be processed together.
  - Quick check question: How does the paper propose to extract semantic similarity for grouping prompts?

## Architecture Onboarding

- Component map: Pre-model -> Batching module -> LLM inference -> Dispatch module -> Evaluation
- Critical path: Pre-model → Batching → LLM → Dispatch → Evaluation
- Design tradeoffs: 
  - Batching size vs. faithfulness: Larger batches improve efficiency but may reduce answer quality
  - Semantic vs. random grouping: Semantic grouping may improve coherence but random grouping offers better generalization
- Failure signatures:
  - Output mismatch: Answers do not correspond to original prompts
  - Reduced faithfulness: Significant drop in Bleu/Rouge scores
  - Efficiency plateau: Further batching yields minimal runtime improvement
- First 3 experiments:
  1. Compare efficiency and faithfulness of separate vs. CliqueParcel on a small dataset (e.g., SQuAD) with fixed batch size.
  2. Test different clique functions (random, semantic, concept-based) on a multi-domain dataset (e.g., TREC) to identify best trade-off.
  3. Vary batch size on a reasoning dataset (e.g., GSM8K) to observe efficiency gains and faithfulness loss thresholds.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CliqueParcel's performance scale with increasingly larger language models beyond GPT-4?
- Basis in paper: [inferred] The paper mentions that "our algorithm's performance is particularly pronounced as the scale of the language model increases, emphasizing its efficacy on larger language models."
- Why unresolved: The experiments only tested CliqueParcel on GPT-3.5 and GPT-4. There is no data on how the framework would perform with even larger models like GPT-5 or other frontier models.
- What evidence would resolve it: Empirical testing of CliqueParcel with models significantly larger than GPT-4, measuring efficiency gains and faithfulness maintenance.

### Open Question 2
- Question: What is the optimal batching size for CliqueParcel across different types of reasoning tasks?
- Basis in paper: [explicit] The paper discusses the relationship between batching size and efficiency, noting that "the optimal batching size strategy depends on the nature of the task."
- Why unresolved: While the paper provides some analysis of batching size effects, it doesn't offer specific guidelines for different reasoning task types (e.g., mathematical reasoning vs. commonsense reasoning).
- What evidence would resolve it: Detailed experimental results showing efficiency and faithfulness trade-offs for various batching sizes across different reasoning task categories.

### Open Question 3
- Question: How does CliqueParcel perform on multimodal datasets that combine text with images or other data types?
- Basis in paper: [inferred] The current experiments are limited to text-based datasets, and there's no discussion of multimodal capabilities.
- Why unresolved: The framework is designed for text-based prompts, but modern LLMs increasingly handle multimodal inputs.
- What evidence would resolve it: Experiments testing CliqueParcel's efficiency and faithfulness on multimodal datasets that include both text and visual information.

## Limitations
- The core claim of achieving up to 207.25% efficiency improvement is based on a novel efficiency metric that excludes output length reduction, which may not reflect real-world user experience where output completeness matters.
- The superiority of random clique batching over semantic grouping, while empirically demonstrated, lacks theoretical justification for why randomness would outperform domain-specific clustering in most cases.
- The framework's performance across different model sizes (only GPT-3.5 and GPT-4 tested) and in production environments with variable prompt distributions remains unverified.

## Confidence

| Claim | Confidence |
|-------|------------|
| Efficiency improvements with minimal faithfulness loss | High |
| Random clique batching consistently outperforming other strategies | Medium |
| The "discounted output" problem being fundamentally addressed by the new metric | Low |

## Next Checks
1. Test CliqueParcel with open-source models (Llama, Mistral) to verify if efficiency gains generalize beyond proprietary models.
2. Implement a user study comparing perceived answer quality between separately processed and CliqueParcel-batched responses to validate the faithfulness metric.
3. Evaluate the framework's robustness under realistic load patterns where prompt distributions change over time, testing the adaptability of the clique classification pre-model.