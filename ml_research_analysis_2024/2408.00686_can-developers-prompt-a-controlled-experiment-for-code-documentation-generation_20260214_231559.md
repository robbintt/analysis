---
ver: rpa2
title: Can Developers Prompt? A Controlled Experiment for Code Documentation Generation
arxiv_id: '2408.00686'
source_url: https://arxiv.org/abs/2408.00686
tags:
- code
- prompt
- documentation
- participants
- tool
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how developers interact with large language
  models (LLMs) to generate code documentation. Through a controlled experiment with
  50 participants (20 professionals and 30 students), we compared ad-hoc prompting
  versus predefined few-shot prompting for code documentation generation in Visual
  Studio Code.
---

# Can Developers Prompt? A Controlled Experiment for Code Documentation Generation

## Quick Facts
- arXiv ID: 2408.00686
- Source URL: https://arxiv.org/abs/2408.00686
- Reference count: 40
- Key outcome: Developers without prompt engineering skills can generate code documentation using simple keywords, but predefined few-shot prompts produce higher quality results

## Executive Summary
This study investigates how developers interact with large language models to generate code documentation through a controlled experiment with 50 participants. The research compares ad-hoc prompting versus predefined few-shot prompting for code documentation generation in Visual Studio Code. Results show that while developers can effectively use LLMs for code documentation, they generally prefer guided interactions with predefined prompts over free-form ad-hoc prompting. The study reveals that both professionals and students can generate usable documentation, but students particularly benefit from predefined prompts for consistency and quality.

## Method Summary
The study conducted a randomized controlled experiment with 50 participants (20 professionals, 30 students) using two Python functions from the Karabo project. Participants were randomly assigned to use either an ad-hoc prompt tool or a predefined prompt tool in Visual Studio Code. The experiment consisted of three parts: demographic questions, code comprehension and documentation generation tasks, and usability assessment using the UEQ questionnaire. The predefined prompt used a few-shot approach with three examples from the same domain, while the ad-hoc tool allowed free-form prompt creation. Documentation quality was assessed through participant ratings and code comprehension tests.

## Key Results
- Students rated documentation from predefined prompts significantly higher in readability, conciseness, usefulness, and helpfulness compared to ad-hoc prompts
- Professionals achieved similar quality using specific keywords like "Docstring" in ad-hoc prompts, using this term 9x more frequently than students
- Both approaches improved code comprehension, particularly for less experienced developers
- Documentation generation was seen as an iterative task requiring developer feedback, with participants rarely assessing output as perfect

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Developers without prompt engineering skills can still generate usable code documentation using simple keywords like "Docstring".
- Mechanism: The LLM interprets documentation-specific keywords as signals to apply structured formatting and domain-appropriate content generation.
- Core assumption: The LLM has been trained on sufficient code documentation patterns to recognize and respond to keywords like "Docstring".
- Evidence anchors:
  - [abstract] "Some professionals produced higher quality documentation by just including the keyword Docstring in their ad-hoc prompts"
  - [section] "Professionals used the Docstring term more often (9x) than students (3x), which is in line with the higher Python experience of most participating professionals"
- Break Condition: If the LLM lacks sufficient training on code documentation patterns, or if the keyword is too ambiguous, the output quality will degrade significantly.

### Mechanism 2
- Claim: Predefined few-shot prompts consistently generate higher quality documentation than ad-hoc prompts, especially for less experienced developers.
- Mechanism: Few-shot prompts provide explicit examples of desired output format and content, reducing the variability and ambiguity in the LLM's generation process.
- Core assumption: The examples provided in the few-shot prompt are representative of high-quality documentation and are relevant to the task at hand.
- Evidence anchors:
  - [abstract] "Students rated documentation from predefined prompts significantly higher in readability, conciseness, usefulness, and helpfulness compared to ad-hoc prompts"
  - [section] "We chose three distinct function-comment pairs from the remaining previously selected Karabo code candidates. Hence, these functions were from the same domain and of similar complexity as the two used in the experiment"
- Break Condition: If the few-shot examples are of low quality, irrelevant to the task, or if the task complexity exceeds the scope of the examples, the predefined prompt may not consistently outperform ad-hoc prompts.

### Mechanism 3
- Claim: Code documentation generation is an iterative task that benefits from developer feedback and refinement.
- Mechanism: The initial LLM-generated documentation serves as a starting point for developers to understand the code and iteratively improve the documentation based on their specific needs and preferences.
- Core assumption: Developers are willing and able to provide feedback and iterate on the generated documentation to improve its quality and usefulness.
- Evidence anchors:
  - [abstract] "Participants in both groups rarely assessed the output as perfect. Instead, they understood the tools as support to iteratively refine the documentation"
  - [section] "The first iteration helped to comprehend the code, formulate the initial comment version, and gather ideas to alter and extend the documentation during further iterations"
- Break Condition: If developers lack the time, motivation, or expertise to iterate on the generated documentation, the benefits of this iterative approach will be limited.

## Foundational Learning

- Concept: Prompt engineering techniques
  - Why needed here: Understanding prompt engineering techniques is crucial for effectively interacting with LLMs and generating high-quality outputs.
  - Quick check question: What are some common prompt engineering techniques used to optimize LLM interactions?

- Concept: Code documentation best practices
  - Why needed here: Familiarity with code documentation best practices is essential for evaluating the quality and usefulness of LLM-generated documentation.
  - Quick check question: What are some key characteristics of well-documented code?

- Concept: User experience (UX) evaluation methods
  - Why needed here: Evaluating the user experience of LLM-powered tools is important for understanding their usability and identifying areas for improvement.
  - Quick check question: What are some common UX evaluation methods used in software engineering research?

## Architecture Onboarding

- Component map:
  Visual Studio Code IDE -> VS Code Extension (ad-hoc or predefined prompt) -> OpenAI API (GPT-4) -> Code editor and selection tools -> User interface for prompt input and output display

- Critical path:
  1. Developer selects code in VS Code
  2. Extension sends code and prompt to OpenAI API
  3. API generates documentation using GPT-4
  4. Extension receives and displays documentation in VS Code

- Design tradeoffs:
  - Flexibility vs. simplicity: Ad-hoc prompts offer more flexibility but require more effort, while predefined prompts are simpler but less customizable.
  - Speed vs. quality: Generating documentation may take time, especially for complex code, but rushing the process may compromise quality.
  - User control vs. automation: Providing more user control over the generation process may increase complexity, while fully automating it may reduce the ability to tailor the output.

- Failure signatures:
  - Inaccurate or irrelevant documentation
  - Slow response times from the API
  - User confusion or frustration with the interface
  - Inconsistent output quality across different code samples

- First 3 experiments:
  1. Test the extension with a simple code snippet to verify basic functionality
  2. Experiment with different prompt variations to observe their impact on output quality
  3. Evaluate the extension's performance with more complex code to assess its scalability and robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific prompt engineering techniques and templates would most effectively support developers in generating high-quality code documentation across different programming languages and domains?
- Basis in paper: [explicit] The paper mentions that developers lack prompting skills and suggests compiling a catalog of evaluated prompt templates for different tasks.
- Why unresolved: The study did not explore specific prompt engineering techniques beyond the predefined few-shot prompt, and it focused on Python code only.
- What evidence would resolve it: Empirical studies testing various prompt engineering techniques and templates across multiple programming languages and domains, measuring their effectiveness in generating high-quality code documentation.

### Open Question 2
- Question: How can IDE-integrated tools be designed to better support the iterative nature of code documentation generation while maintaining developer productivity?
- Basis in paper: [explicit] The study found that documentation generation is an iterative task and participants desired more flexibility in the documentation process.
- Why unresolved: The current tools either offer full flexibility (ad-hoc) or complete automation (predefined), but neither perfectly supports the iterative refinement process.
- What evidence would resolve it: User studies testing IDE tools that combine automated generation with intuitive interfaces for iterative refinement, measuring developer satisfaction and documentation quality.

### Open Question 3
- Question: What quality metrics would best align automated evaluation with human judgment of code documentation quality, particularly regarding usefulness and helpfulness for code comprehension?
- Basis in paper: [explicit] The paper cites research showing current automated metrics misalign with human evaluation of documentation quality.
- Why unresolved: Current metrics like BLEU and ROUGE focus on linguistic similarity rather than practical usefulness for developers.
- What evidence would resolve it: Development and validation of new quality metrics that incorporate factors like documentation usefulness, helpfulness for comprehension, and alignment with developer needs, tested against human evaluations.

## Limitations
- Small sample size (50 participants) limits generalizability of findings
- Study focused only on Python code documentation generation, limiting applicability to other languages
- Predefined prompt examples were from the same domain as test functions, potentially inflating performance metrics
- Only tested one specific LLM (GPT-4) and IDE (VS Code), limiting broader applicability

## Confidence
- **High Confidence**: Developers can generate documentation using simple keywords like "Docstring" is well-supported by participant behavior data
- **Medium Confidence**: Predefined prompts superiority for students is moderately supported but may be influenced by domain-specific examples
- **Medium Confidence**: Both approaches improve code comprehension is supported but effect size differences warrant further investigation

## Next Checks
1. Replicate the experiment with a larger, more diverse participant pool including different programming languages and domains beyond Python/Karabo
2. Test the predefined prompt approach against multiple LLM models (Claude, Llama, etc.) to assess model dependency
3. Conduct a longitudinal study tracking how documentation quality evolves as developers gain experience with both prompting approaches