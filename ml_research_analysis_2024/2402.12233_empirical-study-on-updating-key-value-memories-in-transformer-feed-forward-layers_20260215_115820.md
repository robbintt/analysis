---
ver: rpa2
title: Empirical Study on Updating Key-Value Memories in Transformer Feed-forward
  Layers
arxiv_id: '2402.12233'
source_url: https://arxiv.org/abs/2402.12233
tags:
- editing
- updating
- knowledge
- values
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper compares two strategies for updating information in
  transformer models: modifying the keys (first layer of feed-forward networks) or
  values (second layer). The authors conduct empirical studies on knowledge editing
  and fine-tuning tasks using GPT models.'
---

# Empirical Study on Updating Key-Value Memories in Transformer Feed-forward Layers

## Quick Facts
- arXiv ID: 2402.12233
- Source URL: https://arxiv.org/abs/2402.12233
- Authors: Zihan Qiu; Zeyu Huang; Youcheng Huang; Jie Fu
- Reference count: 13
- Key outcome: Updating keys in transformer FFNs consistently outperforms updating values across knowledge editing and LoRA-based fine-tuning tasks, showing better generalization, locality, and efficiency.

## Executive Summary
This paper investigates the effectiveness of updating keys versus values in transformer feed-forward networks for knowledge editing and fine-tuning tasks. Through extensive empirical studies on GPT models, the authors demonstrate that key-based updates consistently outperform value-based updates across different settings. The key-based approach shows superior performance in terms of efficacy, generalization, locality, and computational efficiency, particularly when implemented through LoRA adaptations.

## Method Summary
The study compares two strategies for updating transformer information: modifying keys (first layer of FFNs) or values (second layer). For knowledge editing, the authors use back-propagation to directly update weights, while for fine-tuning, they implement LoRA with different target settings. Experiments are conducted on GPT-J (6B), GPT2-XL, and Llama2-7B models using various datasets including Alpaca, MMLU, SuperGLUE, GLUE, and 20-news. Performance is evaluated using efficacy scores for editing success, paraphrase scores for generalization, specificity scores for locality, and computational efficiency metrics.

## Key Results
- Key-based updates show significantly better efficacy scores in knowledge editing tasks compared to value-based updates
- For LoRA-based instruction and multi-task tuning, updating key components consistently outperforms updating value components
- Key updates achieve better generalization and locality while requiring fewer trainable parameters than value updates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Updating keys is more effective than updating values because key updates involve changing the interaction weights between hidden states and keys, which is a simpler optimization problem.
- Core assumption: The activation scores (inner product between hidden states and keys) are easier to optimize than directly modifying the value distributions.
- Evidence anchors: [abstract] "updating keys, which involves changing the interaction between hidden states and keys, is more effective than directly modifying the stored knowledge in values"

### Mechanism 2
- Claim: Key updates provide better generalization and locality in knowledge editing tasks.
- Core assumption: The key-based gating mechanism inherently maintains separation between different knowledge concepts.
- Evidence anchors: [section] "updatingK shows more generalization and locality, achieving a large performance gain on Score"

### Mechanism 3
- Claim: For LoRA-based fine-tuning, key-based updates are more parameter-efficient while maintaining or improving performance.
- Core assumption: The key components have more influence on the output distribution than value components for a given number of parameters.
- Evidence anchors: [section] "across various settings (LoRA rank=8,16; with and without tuning q and v), the performance of LoRA on Key gate and Keyup is significantly better than on Valuedown"

## Foundational Learning

- Concept: Feed-forward networks as key-value memories
  - Why needed here: The entire paper builds on this conceptual framework to explain why key updates are more effective
  - Quick check question: In the equation FFNs(h) = f(h·K^T)·V, what do K and V represent respectively?

- Concept: Knowledge editing vs fine-tuning
  - Why needed here: The paper compares key vs value updates across both knowledge editing and fine-tuning tasks
  - Quick check question: What's the key difference between knowledge editing (modifying specific facts) and fine-tuning (adapting model behavior)?

- Concept: LoRA (Low-Rank Adaptation)
  - Why needed here: The paper uses LoRA to implement key vs value updates in a parameter-efficient manner
  - Quick check question: How does LoRA modify the weight matrices, and why is this relevant to the key vs value update comparison?

## Architecture Onboarding

- Component map: Input hidden states → Key matrix (K) → Activation scores → Value matrix (V) → Output
- Critical path: Hidden state → K interaction → activation → V → output. Key updates affect the middle two steps, value updates affect only the last step.
- Design tradeoffs:
  - Key updates: More efficient, better generalization, but may not capture all needed modifications
  - Value updates: More direct control, but computationally expensive and less effective
  - Parameter efficiency: Key-based LoRA uses fewer parameters for better performance
- Failure signatures:
  - Poor efficacy scores: Key updates not finding correct activation patterns
  - Low paraphrase scores: Overfitting to specific contexts rather than generalizing
  - High specificity scores: Key updates affecting unrelated knowledge
  - High time costs: Value updates struggling to converge
- First 3 experiments:
  1. Implement basic knowledge editing on GPT2-XL with both key and value updates, compare efficacy scores
  2. Add paraphrase and specificity evaluations to measure generalization and locality
  3. Extend to LoRA-based instruction tuning on Llama2-7B, comparing key vs value target performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the properties of keys and values in FFNs influence the effectiveness of updating each component?
- Basis in paper: [explicit] The paper discusses the superiority of updating keys over values but does not deeply explore the underlying reasons for these differences.

### Open Question 2
- Question: How do different activation functions in FFNs affect the outcomes of key versus value updates?
- Basis in paper: [inferred] The paper uses SwiGLU but does not explore how different activation functions might impact the effectiveness of updating keys versus values.

### Open Question 3
- Question: Can the findings on key and value updates in FFNs be generalized to other types of neural network architectures beyond transformers?
- Basis in paper: [explicit] The study focuses on transformer models and does not extend the findings to other neural network architectures.

### Open Question 4
- Question: How does the size of the model (e.g., number of parameters, depth) influence the effectiveness of updating keys versus values?
- Basis in paper: [inferred] The paper conducts experiments on different model sizes but does not systematically analyze the impact of model size on the effectiveness of key versus value updates.

## Limitations
- The study focuses primarily on transformer-based architectures with FFNs using ReLU or SwiGLU activations
- The analysis is based on specific model sizes and may not fully capture behavior in extremely large or small models
- The LoRA-based experiments use fixed ranks which may not represent optimal configurations for all tasks

## Confidence
- High confidence in the core finding that key updates outperform value updates for knowledge editing tasks
- Medium confidence in the parameter efficiency claims for LoRA-based fine-tuning
- Medium confidence in the generalization and locality claims

## Next Checks
1. Test the key vs value update hypothesis on transformer variants with different FFN architectures to verify robustness across implementations
2. Conduct experiments across a wider range of model scales to identify whether the observed advantages of key updates persist or change with scale
3. Validate the findings using different optimization objectives beyond standard cross-entropy to assess whether the mechanism generalizes across training paradigms