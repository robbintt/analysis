---
ver: rpa2
title: 'Relay Decoding: Concatenating Large Language Models for Machine Translation'
arxiv_id: '2405.02933'
source_url: https://arxiv.org/abs/2405.02933
tags:
- translation
- large
- language
- machine
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Relay Decoding (RD), a method that concatenates
  two large language models (LLMs), each specialized in the source and target languages,
  to achieve machine translation when no single model supports both languages. The
  approach uses a simple linear mapping layer trained on limited parallel data to
  connect the two models.
---

# Relay Decoding: Concatenating Large Language Models for Machine Translation

## Quick Facts
- **arXiv ID**: 2405.02933
- **Source URL**: https://arxiv.org/abs/2405.02933
- **Reference count**: 15
- **Primary result**: Relay Decoding achieves up to 3+ BLEU point improvements over fine-tuning by concatenating two specialized LLMs with a linear mapping layer

## Executive Summary
Relay Decoding (RD) proposes a novel approach to machine translation when no single large language model supports both source and target languages. Instead of fine-tuning a multilingual model or training from scratch, RD concatenates two specialized LLMs—one for each language—using a simple linear mapping layer trained on limited parallel data. This method circumvents the high costs of continuous learning while achieving superior translation quality, with up to 3+ BLEU point improvements on certain language pairs like Zh-Fr.

## Method Summary
The approach connects a source language LLM with a target language LLM through a linear mapping layer that transforms hidden states from the source model's space to the target model's embedding space. A prompt generator introduces language identification tokens to guide the target LLM's generation. The mapping layer is trained on limited parallel data, while the LLMs themselves remain frozen. The system can optionally fine-tune the target LLM for improved performance. An alternative cross-attention mapping mechanism is also explored in the appendix.

## Key Results
- Achieves 27.36 BLEU on Zh-Fr translation, outperforming fine-tuning approaches
- Demonstrates effectiveness across multiple language pairs (En-Zh, En-Fr, Zh-Fr) on Multi30k and WikiMatrix datasets
- Shows that the simple linear mapping layer can effectively bridge representations between two different LLM architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The linear mapping layer effectively bridges hidden state representations between two LLMs with different architectures
- Mechanism: Projects hidden states from source LLM (Dh) into target LLM's embedding space (De)
- Core assumption: Source and target representations share sufficient semantic alignment for linear transformation
- Evidence anchors: Abstract mentions superior results with simple mapping layer; section 2.2 describes linear layer implementation
- Break condition: Semantic spaces too divergent (different writing systems or grammatical structures)

### Mechanism 2
- Claim: Specialized LLMs avoid catastrophic forgetting while maintaining strong language-specific capabilities
- Mechanism: Uses separate optimized models for each language rather than fine-tuning one multilingual model
- Core assumption: Individual language specialists retain better capabilities than jointly trained multilingual models
- Evidence anchors: Abstract mentions circumventing continuous learning costs; section 2.1 discusses challenges of fine-tuning
- Break condition: Both LLMs lack sufficient language-specific knowledge initially

### Mechanism 3
- Claim: Prompt-based approach guides target LLM to generate in correct target language
- Mechanism: Language identification prompts (e.g., "####[Chinese]: X ####[English]:") signal translation direction
- Core assumption: LLMs respond predictably to language identification prompts even when concatenated
- Evidence anchors: Section 2.2 describes prompt pattern; section 3.1 notes Aquila2's language proficiency
- Break condition: LLM ignores prompts when concatenated with external representations

## Foundational Learning

- Concept: Linear transformations and embedding spaces
  - Why needed here: Understanding how mapping layer transforms hidden states (Dh) to embedding states (De)
  - Quick check question: If source LLM has hidden dimension 4096 and target LLM has embedding dimension 5120, what must mapping layer's weight matrix dimensions be?

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Paper avoids this by using separate LLMs rather than fine-tuning one
  - Quick check question: What happens to neural network's performance on task A when trained on task B without regularization?

- Concept: Cross-attention mechanisms
  - Why needed here: Appendix explores cross-attention as alternative mapping method
  - Quick check question: In cross-attention layer, what serves as query, key, and value when mapping from LLM1 to LLM2?

## Architecture Onboarding

- Component map: Source LLM (language A specialist) → Linear mapping layer → Prompt generator → Target LLM (language B specialist) → Output
- Critical path: Mapping layer and prompt generator are critical components enabling translation between incompatible models
- Design tradeoffs: 
  - Linear mapping: Simple and efficient but may not capture complex relationships
  - Cross-attention mapping: More expressive but requires more data and parameters
  - Fine-tuning vs. no fine-tuning: Fine-tuning target LLM improves results but adds computational cost
- Failure signatures:
  - BLEU scores close to random baseline
  - Target language generation that's incoherent or maintains source language structure
  - Large gap between mapping layer training loss and validation loss (overfitting)
- First 3 experiments:
  1. Test linear mapping layer alone with held-out validation set
  2. Run end-to-end translation with prompt but without fine-tuning either LLM
  3. Test effect of fine-tuning only target LLM (keeping source LLM frozen)

## Open Questions the Paper Calls Out
None

## Limitations
- Linear mapping assumption may break for languages with vastly different grammatical structures or writing systems
- Experimental validation limited to high-resource language pairs from constrained datasets
- Computational efficiency tradeoff compared to single-model approaches not quantified
- Prompt-based generation may be vulnerable to prompt injection attacks

## Confidence

- **High confidence**: Core mechanism of using linear mapping layer to connect specialized LLMs is technically sound and reproducible
- **Medium confidence**: Claim of outperforming fine-tuning on certain language pairs is supported but generalization remains uncertain
- **Low confidence**: Assertion about circumventing continuous learning costs lacks quantitative comparison

## Next Checks

1. **Cross-linguistic generalization test**: Validate on language pairs with different linguistic distances (En-Ar, En-Ja, En-Ko) and low-resource scenarios, comparing with state-of-the-art multilingual models

2. **Computational efficiency benchmark**: Comprehensive analysis comparing inference time, memory usage, and energy consumption between Relay Decoding and single-model approaches on GPU and CPU

3. **Robustness to prompt manipulation**: Adversarial experiments testing whether target LLM's generation can be manipulated by modifying or removing language identification prompts