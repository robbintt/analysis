---
ver: rpa2
title: Can Foundational Large Language Models Assist with Conducting Pharmaceuticals
  Manufacturing Investigations?
arxiv_id: '2404.15578'
source_url: https://arxiv.org/abs/2404.15578
tags: []
core_contribution: "This work evaluated the potential of general-purpose Large Language\
  \ Models (LLMs) in assisting with pharmaceutical manufacturing investigations by\
  \ leveraging historical deviation records. The study examined three models\u2014\
  GPT-3.5, GPT-4, and Claude-2\u2014on tasks including information extraction and\
  \ semantic search for similar deviations."
---

# Can Foundational Large Language Models Assist with Conducting Pharmaceuticals Manufacturing Investigations?

## Quick Facts
- arXiv ID: 2404.15578
- Source URL: https://arxiv.org/abs/2404.15578
- Authors: Hossein Salami; Brandye Smith-Goettler; Vijay Yadav
- Reference count: 0
- Primary result: LLMs demonstrated high accuracy in extracting root causes from pharmaceutical investigation reports

## Executive Summary
This study evaluates the potential of general-purpose Large Language Models (LLMs) in pharmaceutical manufacturing investigations by analyzing historical deviation records. The research examines three models—GPT-3.5, GPT-4, and Claude-2—on tasks including information extraction and semantic search for similar deviations. Results show GPT-4 and Claude-2 can accurately extract structured information from unstructured investigation reports, though with some hallucination risks. Vector embedding-based semantic search successfully identified related deviations, suggesting LLMs could accelerate investigations by automating knowledge retrieval from historical records.

## Method Summary
The study leveraged a dataset of historical pharmaceutical deviation records to evaluate three LLMs on information extraction and semantic search tasks. For information extraction, models were prompted to identify root causes and other investigation details from unstructured reports. For semantic search, OpenAI's text-embedding-ada-002 was used to create vector representations of deviation records, enabling similarity-based retrieval of related incidents. The evaluation compared model performance across different task types, with particular attention to accuracy, hallucination risks, and the need for domain-specific adaptations.

## Key Results
- GPT-4 and Claude-2 achieved high accuracy in extracting root causes from unstructured investigation reports
- Vector embedding-based semantic search successfully identified related deviations with high similarity scores
- Models demonstrated varying performance levels, with GPT-3.5 showing limitations compared to more advanced models

## Why This Works (Mechanism)
The success of LLMs in pharmaceutical investigations stems from their ability to process unstructured text and identify patterns across large datasets. When properly prompted, these models can extract structured information from narrative investigation reports, identifying causal relationships and key details that would otherwise require manual review. The semantic search capability leverages vector embeddings to capture semantic similarity beyond keyword matching, enabling identification of related deviations even when terminology differs.

## Foundational Learning
- **Vector Embeddings**: Mathematical representations of text that capture semantic meaning, needed for similarity-based search beyond keyword matching; quick check: verify cosine similarity scores between related and unrelated deviation records
- **Prompt Engineering**: Structured instructions that guide LLM responses, critical for obtaining accurate extraction results; quick check: compare performance across different prompt formulations
- **Hallucination Risk**: LLMs' tendency to generate plausible but incorrect information, particularly problematic in regulated pharmaceutical contexts; quick check: validate extracted root causes against original investigation conclusions
- **Semantic Search**: Information retrieval based on meaning rather than exact matches, essential for finding similar deviations with different terminology; quick check: test search precision with domain-specific terminology variations

## Architecture Onboarding
**Component Map**: Historical Deviation Records -> Text Embedding Model -> Vector Database -> Semantic Search Query -> LLM Analysis -> Investigation Output

**Critical Path**: Deviation Record Ingestion -> Embedding Generation -> Vector Storage -> Search Query Processing -> LLM Response Generation

**Design Tradeoffs**: General-purpose models offer broad applicability but may require extensive prompt engineering versus domain-specific models that would need significant training data but potentially better accuracy

**Failure Signatures**: Low similarity scores indicating poor vector representation quality, hallucinated root causes that contradict investigation findings, or missed similar deviations due to semantic gaps

**First Experiments**:
1. Benchmark embedding quality by testing retrieval of known similar deviations
2. Compare prompt variations for root cause extraction accuracy
3. Evaluate hallucination rates by having domain experts validate extracted information

## Open Questions the Paper Calls Out
None

## Limitations
- Small dataset of historical deviation records limits generalizability across different manufacturing contexts
- Acknowledged instances of complex reasoning and potential hallucination risks for pharmaceutical-specific terminology
- Single embedding model used without comparison to alternative approaches or domain-specific embeddings
- No evaluation of models' performance on novel deviations or their ability to generate actionable recommendations

## Confidence
- High Confidence: LLMs can accurately extract structured information from unstructured investigation reports
- Medium Confidence: Vector embedding-based semantic search effectively identifies similar deviations
- Medium Confidence: Domain-specific filtering enhances precision in similarity matching

## Next Checks
1. Conduct blind validation using a separate test set of deviation records not used in model training or prompt development
2. Compare LLM-based semantic search performance against traditional keyword-based search methods using established precision-recall metrics
3. Implement a human-in-the-loop evaluation where domain experts assess LLM-generated investigation summaries against actual investigation outcomes