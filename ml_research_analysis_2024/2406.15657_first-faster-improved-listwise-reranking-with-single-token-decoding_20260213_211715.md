---
ver: rpa2
title: 'FIRST: Faster Improved Listwise Reranking with Single Token Decoding'
arxiv_id: '2406.15657'
source_url: https://arxiv.org/abs/2406.15657
tags:
- first
- listwise
- reranking
- ranking
- relevance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FIRST, a novel listwise LLM reranking approach
  that improves inference efficiency by leveraging the output logits of the first
  generated identifier to directly obtain a ranked ordering of candidates, eliminating
  the need to generate full sequences. The method incorporates a learning-to-rank
  loss during training, prioritizing ranking accuracy for more relevant passages.
---

# FIRST: Faster Improved Listwise Reranking with Single Token Decoding

## Quick Facts
- arXiv ID: 2406.15657
- Source URL: https://arxiv.org/abs/2406.15657
- Reference count: 15
- Key outcome: 50% inference speedup while maintaining strong ranking performance across BEIR benchmark

## Executive Summary
FIRST introduces a novel listwise LLM reranking approach that significantly improves inference efficiency by leveraging the output logits of the first generated identifier to directly obtain a ranked ordering of candidates, eliminating the need to generate full sequences. The method incorporates a weighted RankNet loss during training, prioritizing ranking accuracy for more relevant passages. Empirically, FIRST accelerates inference by 50% while maintaining strong ranking performance with gains across the BEIR benchmark. Additionally, the paper demonstrates that LLM rerankers can provide superior relevance feedback compared to cross-encoders, yielding substantial improvements in retriever recall during inference.

## Method Summary
FIRST fine-tunes a Zephyrβ LLM using a joint loss combining language modeling and weighted RankNet objectives. The approach uses a sliding window to process candidate passages and generates a ranked identifier sequence based on first token logits. During inference, only the first token is generated per candidate, significantly reducing computational overhead. The method is trained on 40k GPT-4 labeled instances from MS MARCO and evaluated on the BEIR benchmark using Contriever's top-100 retrievals as input.

## Key Results
- 50% inference speedup compared to traditional listwise reranking methods
- Improved nDCG@10 performance across BEIR datasets
- Superior relevance feedback capabilities compared to cross-encoders for retriever recall

## Why This Works (Mechanism)

### Mechanism 1
Using output logits from the first generated identifier provides a direct ranking signal without full sequence generation. The LLM's first token logits contain sufficient information to approximate the full ranked list, enabling single-token decoding. This assumes the LLM implicitly judges relative relevance during the generation of the first identifier token.

### Mechanism 2
Weighted RankNet loss prioritizes ranking accuracy for more relevant passages. The inverse mean rank weighting (1/(i+j)) assigns higher importance to correctly ranking top candidates, assuming misranking highly relevant passages is more costly than misranking less relevant ones.

### Mechanism 3
Combining LM objective with RankNet loss creates a balanced training signal. The joint loss (LM + λLRank) leverages the strengths of both sequence generation supervision and pairwise ranking supervision, assuming language modeling objective provides good general alignment while RankNet improves ranking-specific performance.

## Foundational Learning

- **Listwise reranking vs. pointwise/pairwise approaches**: Why needed - Understanding why listwise reranking is superior for context calibration. Quick check - Why does listwise reranking outperform pointwise reranking in ranking accuracy?

- **Learning-to-rank (LTR) algorithms and their loss functions**: Why needed - FIRST uses weighted RankNet loss for training. Quick check - What's the difference between RankNet, LambdaRank, and ListNet losses?

- **Tokenization and vocabulary constraints in LLMs**: Why needed - FIRST uses alphabetic identifiers (A-Z) instead of numeric IDs. Quick check - Why can't we use numeric identifiers for passage ranking in FIRST?

## Architecture Onboarding

- **Component map**: Query → Sliding window processing → First token generation → Logit extraction → Ranking
- **Critical path**: Query → Sliding window processing → First token generation → Logit extraction → Ranking
- **Design tradeoffs**: Window size vs. computational efficiency, Single-token decoding vs. full sequence generation, Weighted RankNet vs. other LTR losses
- **Failure signatures**: Poor ranking accuracy on out-of-domain datasets, Increased latency with larger window sizes, Training instability with joint loss
- **First 3 experiments**:
  1. Verify single-token decoding produces comparable rankings to full sequence generation on a small validation set
  2. Test different λ values for joint loss to find optimal balance between LM and RankNet objectives
  3. Measure latency improvement across different window sizes (m=10, 20, 30) on representative hardware

## Open Questions the Paper Calls Out

- **Can FIRST be effectively extended to handle multilingual passage reranking beyond English?**: The authors acknowledge experiments are limited to English data and suggest finetuning a multilingual LLM could demonstrate benefits in other languages.

- **How does the performance of FIRST scale with larger window sizes (e.g., > 20 candidates)?**: The current implementation is constrained by alphabetic identifiers, and the authors suggest finetuning using other vocabulary tokens could enable larger candidate sets.

- **Can the incorporation of supervised pairwise examples from datasets like MS MARCO further improve the ranking performance of FIRST?**: The authors acknowledge they have not experimented with using human-annotated pairwise examples from supervised datasets to further improve performance.

## Limitations

- Limited to English-language retrieval tasks in current experiments
- Constrained to window sizes of ≤ 20 due to alphabetic identifier approach
- Heavy dependency on expensive GPT-4 annotations for training data

## Confidence

- Single-token decoding effectiveness: Medium - Based on exploratory experiments showing correlation, but not rigorously validated across diverse query types
- Weighted RankNet loss calibration: Medium - Limited sensitivity analysis with single λ value
- BEIR benchmark generalization: High - Strong performance across 15 BEIR datasets, but limited to English
- Training data dependency: Low - No ablation studies on training set size or quality

## Next Checks

1. **Cross-dataset ranking consistency**: Evaluate FIRST on a dataset with different relevance judgment characteristics to verify single-token decoding maintains ranking accuracy outside BEIR distribution.

2. **Window size sensitivity analysis**: Systematically test how ranking performance degrades as window size increases, particularly for m > 20, to identify practical limits.

3. **Alternative weighting scheme comparison**: Implement and test alternative ranking loss weightings to determine whether inverse mean rank weighting is truly optimal or if simpler schemes could achieve similar results with less computational overhead.