---
ver: rpa2
title: 'PLLaMa: An Open-source Large Language Model for Plant Science'
arxiv_id: '2401.01600'
source_url: https://arxiv.org/abs/2401.01600
tags:
- journal
- plant
- science
- research
- agricultural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'PLLaMa is an open-source large language model specialized for
  plant science, developed by extending LLaMa-2-7B and LLaMa-2-13B with over 1.5 million
  plant science academic articles. The model underwent two-stage training: initial
  pretraining on plant science corpus followed by instruction-based fine-tuning.'
---

# PLLaMa: An Open-source Large Language Model for Plant Science

## Quick Facts
- arXiv ID: 2401.01600
- Source URL: https://arxiv.org/abs/2401.01600
- Authors: Xianjun Yang; Junfeng Gao; Wenxin Xue; Erik Alexandersson
- Reference count: 15
- Primary result: 60% accuracy on plant science quiz using LLaMa-2-7B/13B extended with 1.5M plant science articles

## Executive Summary
PLLaMa is an open-source large language model specialized for plant science, developed by extending LLaMa-2-7B and LLaMa-2-13B with over 1.5 million plant science academic articles. The model underwent two-stage training: initial pretraining on plant science corpus followed by instruction-based fine-tuning. Experiments using eight A100 80G GPUs showed pretraining losses of 2.87 (13B) and 3.05 (7B), with instruction tuning completed in 1.3-2.7 hours. The model achieved approximately 60% accuracy on a 10-question plant science quiz and demonstrated reliable performance on zero-shot queries according to expert evaluation.

## Method Summary
PLLaMa was developed through a two-stage training process. First, the base LLaMa-2 models were extended through continued pretraining on a corpus of 1.5 million plant science articles filtered from 750 academic journals. Second, instruction tuning was performed using a combination of 1030 LIMA instructions and customized plant science instructions. The model checkpoints and source code are publicly available, with pretraining and instruction tuning completed on eight A100 80G GPUs.

## Key Results
- Achieved approximately 60% accuracy on a 10-question plant science quiz
- Pretraining losses of 2.87 (13B) and 3.05 (7B) achieved in 48.3 and 24.7 hours respectively
- Instruction tuning completed in 1.3-2.7 hours on 8 A100 80G GPUs
- Demonstrated reliable performance on zero-shot queries according to expert evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continued pretraining on domain-specific corpus improves model's task-specific performance by filling in domain-specific knowledge gaps.
- Mechanism: The model initially trained on general text data lacks specialized knowledge. By continuing pretraining on plant science articles, the model learns the domain-specific vocabulary, concepts, and relationships that are crucial for plant science tasks.
- Core assumption: The domain-specific corpus is representative and high-quality enough to teach the model useful information.
- Evidence anchors:
  - [abstract]: "enhanced with a comprehensive database, comprising more than 1.5 million scholarly articles in plant science"
  - [section]: "We utilize the plant science journal names for filtering to get relevant academic articles, resulting in over 1.5 million articles"
  - [corpus]: The corpus contains 1,676,389 plant science articles from 750 journals, suggesting comprehensive coverage of the domain
- Break condition: If the corpus is not representative of the domain or contains low-quality/irrelevant articles, the model may learn incorrect information or fail to gain useful knowledge.

### Mechanism 2
- Claim: Instruction tuning improves the model's ability to understand and respond to specific instructions or queries.
- Mechanism: After the model learns domain-specific knowledge through continued pretraining, instruction tuning fine-tunes the model to better understand and respond to specific instructions or queries. This involves training the model on examples of instruction-response pairs.
- Core assumption: The instruction tuning dataset is diverse and representative enough to teach the model how to respond to various types of instructions.
- Evidence anchors:
  - [abstract]: "instruction-based fine-tuning"
  - [section]: "We incorporated 1030 instructions from the LIMA training set and also developed additional customized instructions focusing on plant science"
  - [corpus]: The instruction tuning dataset contains 1030 instructions from LIMA plus customized plant science instructions, providing a diverse set of examples
- Break condition: If the instruction tuning dataset is too small or not diverse enough, the model may not learn how to respond effectively to all types of instructions.

### Mechanism 3
- Claim: The two-stage training approach (continued pretraining + instruction tuning) is more effective than either approach alone.
- Mechanism: Continued pretraining provides the model with domain-specific knowledge, while instruction tuning teaches the model how to use that knowledge to respond to specific instructions. This combination allows the model to not only know about plant science but also to apply that knowledge effectively.
- Core assumption: The two stages of training complement each other and do not interfere with each other.
- Evidence anchors:
  - [abstract]: "This development significantly enriches PLLaMa with extensive knowledge and proficiency in plant and agricultural sciences"
  - [section]: "This process comprises two key stages...an initial phase of extended pretraining with abundant academic articles in plant science, followed by a phase of instruction-based fine-tuning"
  - [corpus]: The model's performance on plant science quiz (60% accuracy) and zero-shot queries suggests that the two-stage approach is effective
- Break condition: If the two stages of training interfere with each other (e.g., if instruction tuning causes the model to forget the domain-specific knowledge), the overall approach may not be effective.

## Foundational Learning

- Concept: Continued pretraining
  - Why needed here: The base LLaMa-2 model is trained on general text data and lacks specialized knowledge in plant science. Continued pretraining on a domain-specific corpus fills in this knowledge gap.
  - Quick check question: What is the purpose of continued pretraining in the context of PLLaMa?

- Concept: Instruction tuning
  - Why needed here: After the model learns domain-specific knowledge, it needs to be fine-tuned to understand and respond to specific instructions or queries. This is achieved through instruction tuning.
  - Quick check question: What is the purpose of instruction tuning in the context of PLLaMa?

- Concept: Two-stage training
  - Why needed here: The combination of continued pretraining and instruction tuning is more effective than either approach alone. This two-stage training approach allows the model to both know about plant science and apply that knowledge effectively.
  - Quick check question: Why is a two-stage training approach used for PLLaMa?

## Architecture Onboarding

- Component map: Input articles and instructions -> Continued pretraining on plant science corpus -> Instruction tuning on response pairs -> PLLaMa model output
- Critical path: Continued pretraining -> Instruction tuning -> Evaluation -> Release
- Design tradeoffs: Larger corpus and more diverse instruction tuning dataset can improve model performance but require more computational resources and time.
- Failure signatures: Low performance on plant science quiz or zero-shot queries, high training loss, model forgetting general knowledge (catastrophic forgetting).
- First 3 experiments:
  1. Train a smaller model (e.g., LLaMa-2-7B) on a subset of the plant science corpus and evaluate its performance on a plant science quiz.
  2. Train the same smaller model on a small instruction tuning dataset and evaluate its ability to respond to plant science queries.
  3. Combine the two approaches (continued pretraining + instruction tuning) and compare the performance to the individual approaches.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between the pretraining corpus size (2.28M text pieces) and the observed performance gains (60% accuracy on quiz)?
- Basis in paper: [explicit] The paper states they used 2,278,433 text pieces from 1.5M articles plus 10% general corpus, and achieved 60% accuracy on a 10-question quiz.
- Why unresolved: The paper doesn't provide a systematic analysis of how different corpus sizes or compositions affect model performance. It's unclear if the performance gains are proportional to the corpus size or if there's a saturation point.
- What evidence would resolve it: A controlled experiment varying corpus sizes and compositions while measuring performance on standardized plant science benchmarks.

### Open Question 2
- Question: How do the 7B and 13B model sizes compare in their ability to retain general knowledge versus acquiring plant science expertise during pretraining?
- Basis in paper: [explicit] The paper mentions using 10% general corpus to prevent catastrophic forgetting but doesn't analyze how well each model size maintains general knowledge.
- Why unresolved: The paper provides pretraining losses for both sizes but doesn't analyze the trade-off between general knowledge retention and domain-specific learning.
- What evidence would resolve it: Comparative analysis of general knowledge retention using standardized benchmarks for both model sizes after plant science pretraining.

### Open Question 3
- Question: What is the long-term effectiveness of the international expert panel in ensuring accurate model responses, and how scalable is this approach?
- Basis in paper: [explicit] The paper mentions forming an international panel of professionals to verify accuracy but doesn't discuss the sustainability or scalability of this approach.
- Why unresolved: The paper doesn't provide information on how the expert panel will continue to evaluate the model over time or how this approach would work as the model evolves.
- What evidence would resolve it: Documentation of ongoing expert evaluation processes and analysis of how well this approach scales with model updates and increased usage.

## Limitations

- Corpus representativeness: While the model is trained on 1.5 million plant science articles, the paper does not provide detailed analysis of corpus quality, coverage gaps, or potential biases in the source material.
- Evaluation scope: The reported 60% accuracy on a 10-question quiz represents limited validation without comprehensive benchmarking against existing plant science models.
- Generalization boundaries: The model's performance on specialized subtopics within plant science is not characterized, leaving uncertainty about its reliability across the full breadth of plant science domains.

## Confidence

- High confidence in the fundamental approach: The two-stage training methodology (continued pretraining + instruction tuning) is well-established in the literature.
- Medium confidence in reported performance: The experimental results are presented with specific metrics, but the limited evaluation scope reduces confidence in absolute performance claims.
- Medium confidence in reproducibility: While code and model checkpoints are released, the paper lacks complete training recipes and hyperparameter details needed for exact replication.

## Next Checks

1. **Comprehensive benchmarking**: Evaluate PLLaMa against both general-purpose LLMs and specialized plant science models on standardized plant science datasets and tasks.

2. **Corpus quality analysis**: Conduct detailed analysis of the training corpus including topic distribution, journal quality metrics, temporal coverage, and identification of potential knowledge gaps or biases.

3. **Zero-shot generalization testing**: Systematically test the model's performance across different plant science subdomains using carefully constructed zero-shot prompts to identify reliability boundaries and failure modes.