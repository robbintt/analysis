---
ver: rpa2
title: Measuring Moral Inconsistencies in Large Language Models
arxiv_id: '2402.01719'
source_url: https://arxiv.org/abs/2402.01719
tags:
- llms
- language
- consistency
- moral
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Semantic Graph Entropy (SGE), a novel information-theoretic
  metric to measure the consistency of Large Language Models (LLMs) in moral scenarios.
  SGE addresses the challenge of evaluating LLM consistency when there is no "correct"
  answer, such as in moral dilemmas.
---

# Measuring Moral Inconsistencies in Large Language Models

## Quick Facts
- arXiv ID: 2402.01719
- Source URL: https://arxiv.org/abs/2402.01719
- Authors: Vamshi Krishna Bonagiri; Sreeram Vennam; Manas Gaur; Ponnurangam Kumaraguru
- Reference count: 4
- Key outcome: Introduces Semantic Graph Entropy (SGE) to measure LLM consistency in moral scenarios, showing better correlation with human judgments than existing metrics

## Executive Summary
This paper introduces Semantic Graph Entropy (SGE), a novel information-theoretic metric to measure the consistency of Large Language Models (LLMs) in moral scenarios. SGE addresses the challenge of evaluating LLM consistency when there is no "correct" answer, such as in moral dilemmas. The method constructs a semantic graph of LLM responses and calculates entropy weighted by semantic distance to quantify consistency. The authors evaluate SGE on five LLMs using the Moral Integrity Corpus, generating paraphrased questions and extracting Rules of Thumb (RoTs) to explain decision-making strategies. Results show that SGE correlates better with human judgments compared to existing metrics like BLEU, ROUGE, and BERTScore. The study reveals that even state-of-the-art LLMs exhibit notable inconsistency in moral reasoning, with GPT-4 demonstrating the highest consistency among the tested models. The findings highlight the importance of assessing LLM consistency for reliable deployment in real-world applications.

## Method Summary
The paper proposes Semantic Graph Entropy (SGE) as a novel metric for measuring LLM consistency in moral scenarios. The method involves generating paraphrases of questions from the Moral Integrity Corpus, obtaining LLM responses and Rules of Thumb (RoTs) through few-shot prompting, and constructing semantic graphs using SBERT DeBERTa embeddings. SGE is calculated as one minus the entropy of node visitation probabilities scaled by average semantic distance. The authors compare SGE against traditional metrics (BLEU, ROUGE, BERTScore) and validate it through human Natural Language Inference (NLI) annotations, demonstrating better correlation with human judgments of consistency.

## Key Results
- SGE correlates better with human judgments of consistency compared to BLEU, ROUGE, and BERTScore metrics
- Even state-of-the-art LLMs exhibit notable inconsistency in moral reasoning scenarios
- GPT-4 demonstrates the highest consistency among the tested models (LLama2-7B, LLama2-13B, Falcon-7B, GPT-3.5, GPT-4)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic Graph Entropy (SGE) measures LLM consistency by quantifying semantic divergence across paraphrased responses
- Mechanism: Constructs a semantic graph where nodes represent response embeddings and edges encode cosine distances. Entropy of node visitation probabilities, scaled by average semantic distance, captures consistency - lower entropy indicates more uniform semantic proximity across responses
- Core assumption: Semantic similarity correlates with moral consistency, and graph entropy appropriately captures this distribution
- Evidence anchors:
  - [abstract] "We propose a novel information-theoretic measure called Semantic Graph Entropy (SGE) to measure the consistency of an LLM in moral scenarios"
  - [section] "We generate semantic representations of LLMs' answers for each question using an SBERT DeBERTa model... We then store these embeddings as graphs"
  - [corpus] Weak - corpus doesn't directly address the semantic graph mechanism, though related works on consistency testing suggest this approach is novel
- Break condition: If semantic embeddings fail to capture moral nuance, or if paraphrasing introduces bias that makes equivalent prompts appear semantically different

### Mechanism 2
- Claim: Rules of Thumb (RoTs) improve consistency measurement by providing interpretable moral decision strategies
- Mechanism: Extracts RoTs from answer pairs using few-shot prompting, then uses RoTs instead of raw answers in the semantic graph. This filters out stylistic variations while preserving moral reasoning patterns
- Core assumption: RoTs capture the essential moral judgment that drives responses, making them more stable across paraphrasing than full responses
- Evidence anchors:
  - [abstract] "We leverage 'Rules of Thumb' (RoTs) to explain a model's decision-making strategies and further enhance our metric"
  - [section] "We propose using RoTs as explanations to better represent and evaluate a model's moral judgment... This modification shows a better correlation with human evaluations"
  - [corpus] Weak - corpus mentions related work on RoTs but doesn't validate their effectiveness for consistency measurement
- Break condition: If RoT extraction is inconsistent or if RoTs fail to capture subtle moral distinctions that differentiate responses

### Mechanism 3
- Claim: Human evaluation via Natural Language Inference (NLI) validates that SGE correlates with human judgments of consistency
- Mechanism: Creates human annotation task where annotators judge whether paraphrased responses are semantically equivalent. High Fleiss Kappa (0.68) indicates reliable agreement
- Core assumption: NLI task on LLM responses provides a reasonable proxy for human judgment of moral consistency
- Evidence anchors:
  - [section] "To assess the metric's validity, we created a human annotation task based on Natural Language Inference (NLI) for the responses generated by LLMs... We noted a Fleiss Kappa score of 0.68"
  - [abstract] "Compared to existing consistency metrics, SGE correlates better with human judgments across five LLMs"
  - [corpus] Moderate - corpus includes related work on consistency testing but doesn't specifically validate NLI-based human evaluation for moral scenarios
- Break condition: If annotators disagree on fundamental aspects of moral equivalence, or if NLI framework doesn't capture moral nuance

## Foundational Learning

- Concept: Information theory and entropy calculation
  - Why needed here: SGE fundamentally relies on Shannon entropy to quantify uncertainty in semantic distributions
  - Quick check question: Can you explain why higher entropy in the semantic graph indicates lower consistency?

- Concept: Semantic embeddings and sentence transformers
  - Why needed here: The method uses SBERT DeBERTa to convert text responses into comparable vector representations
  - Quick check question: What properties make SBERT DeBERTa suitable for capturing semantic equivalence across paraphrased moral responses?

- Concept: Graph theory and probability distributions
  - Why needed here: The semantic graph structure and node visitation probability calculation are central to SGE computation
- Quick check question: How does the information functional f(vi) = Σ d(vi,u) relate to the probability of visiting node vi in the graph?

## Architecture Onboarding

- Component map:
  - Data pipeline: MIC dataset → paraphrase generation → response generation
  - Processing pipeline: SBERT embedding → semantic graph construction → entropy calculation
  - Validation pipeline: RoT extraction → human NLI evaluation → correlation analysis
  - Output: SGE scores for consistency comparison across models

- Critical path:
  1. Generate high-quality paraphrases (Parascore > 0.8 filter)
  2. Generate responses and RoTs using target LLMs
  3. Compute semantic embeddings with SBERT DeBERTa
  4. Build semantic graphs and calculate SGE
  5. Validate with human NLI annotations

- Design tradeoffs:
  - Paraphrase quality vs. computational cost (10 paraphrases per question)
  - Embedding model choice (SBERT DeBERTa vs alternatives)
  - RoT extraction complexity vs. improved consistency measurement
  - Human evaluation sample size vs. annotation cost

- Failure signatures:
  - Low correlation between SGE and human judgments
  - Inconsistent SGE scores across semantically equivalent inputs
  - High variance in RoT quality or extraction
  - Poor performance on questions with subtle moral distinctions

- First 3 experiments:
  1. Baseline comparison: Run SGE on paraphrased questions only (no LLM responses) to establish upper bound of consistency
  2. Ablation study: Compare SGE with and without RoTs to measure their contribution
  3. Cross-model analysis: Compare consistency scores across the five LLMs to identify patterns in model behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the root causes of inconsistency in large language models' moral reasoning, and how can these inconsistencies be effectively mitigated?
- Basis in paper: Explicit - The authors state their future aim to "pinpoint the origins of this uncertain behavior, and (3) devise long-term solutions to address them."
- Why unresolved: The paper acknowledges the problem of inconsistency in LLMs but does not investigate the underlying causes or propose solutions to address them.
- What evidence would resolve it: A detailed analysis of LLM training data, architecture, and fine-tuning processes that identifies specific factors contributing to inconsistency, followed by experimental results demonstrating effective mitigation strategies.

### Open Question 2
- Question: How does the Semantic Graph Entropy (SGE) metric perform compared to other consistency metrics across a broader range of moral and non-moral tasks?
- Basis in paper: Explicit - The authors compare SGE to existing metrics (BLEU, ROUGE, BERTScore) and show better correlation with human judgments, but only within the context of moral scenarios.
- Why unresolved: The evaluation of SGE is limited to moral scenarios, leaving its generalizability to other types of tasks unexplored.
- What evidence would resolve it: A comprehensive study applying SGE to diverse tasks (e.g., factual questions, creative writing, technical problem-solving) and comparing its performance against other consistency metrics.

### Open Question 3
- Question: How do different training strategies and model architectures impact the consistency of large language models in moral reasoning tasks?
- Basis in paper: Explicit - The authors note that Falcon-7B, despite having fewer parameters than LLama2-13B, shows higher consistency, suggesting that different training strategies may lead to better consistency.
- Why unresolved: The paper does not investigate the specific training strategies or architectural differences that contribute to consistency in moral reasoning.
- What evidence would resolve it: A systematic comparison of various LLM architectures and training approaches (e.g., different pre-training objectives, fine-tuning techniques, data augmentation methods) and their impact on consistency in moral reasoning tasks.

## Limitations
- The Semantic Graph Entropy metric assumes semantic similarity directly correlates with moral consistency, but this relationship may not hold for all moral reasoning scenarios, particularly those involving complex ethical trade-offs
- The RoT extraction process relies on LLM-generated explanations that may themselves be inconsistent or incomplete representations of the underlying moral reasoning
- Human evaluation via NLI may not fully capture the nuances of moral equivalence, as annotators might focus on surface-level semantic similarity rather than deeper moral reasoning patterns

## Confidence
- High confidence: The SGE metric implementation and correlation with human judgments for basic consistency measurement
- Medium confidence: The effectiveness of RoTs in improving consistency measurement and the generalizability of results across different moral domains
- Medium confidence: The validity of NLI-based human evaluation as a proxy for moral consistency judgments

## Next Checks
1. Conduct an ablation study comparing SGE scores with and without RoTs across diverse moral scenarios to quantify their contribution to consistency measurement
2. Test the robustness of SGE by evaluating it on manually curated sets of semantically equivalent moral questions to ensure it captures intended consistency patterns
3. Perform cross-cultural validation by testing the metric with moral scenarios from different cultural contexts to assess its generalizability beyond the MIC dataset