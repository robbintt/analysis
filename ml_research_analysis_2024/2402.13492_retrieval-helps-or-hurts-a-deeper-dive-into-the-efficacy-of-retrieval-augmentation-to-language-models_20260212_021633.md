---
ver: rpa2
title: Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval Augmentation
  to Language Models
arxiv_id: '2402.13492'
source_url: https://arxiv.org/abs/2402.13492
tags:
- question
- questions
- retrieval
- accuracy
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the efficacy of retrieval augmentation
  for language models in question answering tasks. The authors construct a novel dataset
  called WiTQA that contains questions about entities and relations of varying popularity
  levels, each accompanied by supporting passages from Wikipedia.
---

# Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval Augmentation to Language Models

## Quick Facts
- arXiv ID: 2402.13492
- Source URL: https://arxiv.org/abs/2402.13492
- Reference count: 38
- This study investigates when retrieval augmentation helps or hurts language models in question answering tasks

## Executive Summary
This paper examines the effectiveness of retrieval augmentation for language models by constructing a novel WiTQA dataset that categorizes questions by entity-relation popularity. Through extensive experiments with diverse language models and retrievers, the authors find that larger models excel at recalling popular facts but struggle with infrequent entity-relation pairs, while retrievers are more robust for long-tail information. Surprisingly, retrieval augmentation is beneficial for less popular questions, but vanilla language models perform better for popular questions. Based on these insights, they propose a selective memory integration approach that improves question answering performance by up to 10.1% by selectively employing retrieval and recall based on popularity metrics.

## Method Summary
The authors construct the WiTQA dataset by extracting triples from Wikipedia and Wikidata, computing S counts (entity frequency) and S-R counts (entity-relation pair frequency) as popularity metrics. They sample triples based on these frequencies and generate questions using roundtrip refinement. The dataset is evaluated using 10 language models (Flan-T5, Mistral, Llama, GPT-3.5/4) with 5 retrievers (BM25, Contriever, GTR, BGE, Oracle) in a zero-shot QA setup. The study analyzes performance across different popularity bins and proposes a selective memory integration approach that combines language models and retrievers based on question popularity.

## Key Results
- Larger language models demonstrate superior recall of popular facts compared to smaller models
- Retrieval augmentation significantly improves performance for less popular questions (up to 49.2% improvement)
- Retrieval errors for popular questions can cause language models to override correct knowledge, leading to performance degradation
- The selective memory integration approach improves QA performance by up to 10.1% by selectively employing retrieval and recall based on popularity metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval augmentation improves performance for less popular questions while vanilla LMs perform better on popular questions.
- Mechanism: Larger language models can recall popular facts from their pre-trained memorization, but struggle with infrequent entity-relation pairs. Retrievers, on the other hand, are more robust for long-tail information of less common entities.
- Core assumption: The distribution of Wikipedia texts reflects that of pre-trained texts for recent proprietary models.
- Evidence anchors:
  - [abstract] "We observe that larger LMs excel in recalling popular facts. However, they notably encounter difficulty with infrequent entity-relation pairs compared to retrievers."
  - [section 4.4] "Our observations indicate that BM25, Contriever, GTR, and BGE notably enhance the accuracy of small and medium models by up to 49.2%, while GenRead appears to be more effective for larger models with up to 4.1% improvement."
  - [corpus] Weak evidence; pre-training corpus distribution is not explicitly known for GPT-4 and other proprietary models.
- Break condition: If the pre-training corpus distribution significantly differs from Wikipedia, or if models are fine-tuned on a different distribution.

### Mechanism 2
- Claim: Larger models exhibit higher accuracy than smaller models, but still face challenges with minor facts.
- Mechanism: Larger models have more parameters and have been exposed to more data during pre-training, allowing them to recall more facts. However, even these models struggle with less frequent entity-relation pairs.
- Core assumption: Model size correlates with the amount of factual knowledge recalled.
- Evidence anchors:
  - [abstract] "Confirming earlier findings, we observe that larger LMs excel in recalling popular facts. However, they notably encounter difficulty with infrequent entity-relation pairs compared to retrievers."
  - [section 4.2] "As depicted in Figure 3, all models, regardless of their size, generally demonstrate good recall of popular facts. For instance, even Flan-T5-large can achieve up to 80% accuracy for questions with over 1000 S-R count. Predictably, larger models exhibit a superior ability to recall compared to smaller models."
  - [corpus] No direct corpus evidence provided; based on model architecture and training data exposure.
- Break condition: If model size does not correlate with pre-training data exposure or if models are trained on a limited dataset.

### Mechanism 3
- Claim: Retrieval errors for popular questions negatively impact the performance of RALMs.
- Mechanism: When retrievers provide irrelevant passages for popular questions, the language model may override its correct knowledge with the incorrect information from the passage, leading to wrong answers.
- Core assumption: Language models are susceptible to overriding their knowledge when provided with irrelevant passages.
- Evidence anchors:
  - [abstract] "Indiscriminately augmenting LMs with irrelevant passages can override potentially correct knowledge already possessed by the LM, resulting in incorrect responses."
  - [section 4.4] "We observe a significant improvement in accuracy for all models, irrespective of their sizes, when augmented with Oracle. However, substantial differences are observed between performances with BM25/Contriever/GTR/BGE and Oracle, which can be attributed to retrieval errors."
  - [corpus] No direct corpus evidence provided; based on the interaction between retrievers and language models.
- Break condition: If language models are trained to be robust to irrelevant context or if retrievers have near-perfect accuracy for all questions.

## Foundational Learning

- Concept: Understanding the difference between parametric and non-parametric knowledge in language models.
  - Why needed here: To comprehend why retrieval augmentation can be both beneficial and harmful depending on the popularity of the question.
  - Quick check question: What is the difference between a language model recalling knowledge from its parameters versus retrieving information from an external source?

- Concept: Familiarity with the concept of S-R counts and S counts as popularity metrics.
  - Why needed here: To understand how the popularity of entities and relations in the pre-training corpus affects the performance of language models and retrievers.
  - Quick check question: How do S-R counts and S counts differ, and why are they important for analyzing the performance of language models and retrievers?

- Concept: Knowledge of the construction and purpose of the WiTQA dataset.
  - Why needed here: To understand the experimental setup and the insights derived from the analysis of the dataset.
  - Quick check question: What is the WiTQA dataset, and how does it facilitate the analysis of language models and retrievers?

## Architecture Onboarding

- Component map:
  Language models (Flan-T5-small/base/large/xl, Mistral-7B, Llama-2-7B/13B/70B-chat, GPT-3.5/GPT-4) -> Retrievers (BM25, Contriever, GTR-large/xl, BGE, GenRead, Oracle) -> WiTQA dataset (questions, supporting passages, popularity metrics) -> Selective memory integration system

- Critical path:
  1. Extract triples from Wikipedia and Wikidata to compute S counts and S-R counts
  2. Sample triples based on S-R counts and generate questions using roundtrip refinement
  3. Evaluate language models and retrievers on the WiTQA dataset
  4. Analyze the performance of language models and retrievers based on question popularity
  5. Develop a selective memory integration system based on the insights gained from the analysis

- Design tradeoffs:
  - Using larger language models vs. smaller models with retrieval augmentation
  - Using supervised retrievers (GTR, BGE) vs. unsupervised retrievers (BM25, Contriever)
  - Prioritizing recall of popular facts vs. retrieval of less frequent information

- Failure signatures:
  - Language models providing incorrect answers due to retrieval errors
  - Retrievers failing to retrieve relevant passages for less popular questions
  - Selective memory integration system making suboptimal decisions based on popularity metrics

- First 3 experiments:
  1. Evaluate the performance of language models on popular vs. less popular questions to understand the correlation between model size and recall ability
  2. Assess the performance of retrievers on less popular questions to determine their robustness for long-tail information
  3. Analyze the failure patterns of language models and retrievers when operating in tandem to identify the causes of errors in retrieval-augmented language models

## Open Questions the Paper Calls Out

Open Question 1
- Question: How does the distribution of pre-training corpus affect the performance of retrieval-augmented language models (RALMs)?
- Basis in paper: [explicit] The paper mentions that the pre-trained texts of recent proprietary models such as GPT-4 are not accessible, and the work hypothesizes that the distribution of the Wikipedia texts reflects that of pre-trained texts.
- Why unresolved: The paper does not explore the impact of using different pre-training corpora on the performance of RALMs. It is unclear how the distribution of the pre-training corpus affects the models' ability to recall and retrieve information.
- What evidence would resolve it: Conducting experiments with RALMs using different pre-training corpora and analyzing the impact on their performance would provide insights into how the distribution of the pre-training corpus affects the models' recall and retrieval abilities.

Open Question 2
- Question: What is the optimal balance between recall and retrieval for different model sizes and question popularity levels?
- Basis in paper: [explicit] The paper proposes a selective memory integration approach that uses augmentation based on S-R and S counts, while using vanilla LM as default. However, it does not explore the optimal thresholds for different model sizes and question popularity levels.
- Why unresolved: The paper does not investigate the optimal balance between recall and retrieval for different model sizes and question popularity levels. It is unclear how to determine the best thresholds for selective memory integration.
- What evidence would resolve it: Conducting experiments with different model sizes and question popularity levels, and analyzing the performance of selective memory integration with varying thresholds, would provide insights into the optimal balance between recall and retrieval.

Open Question 3
- Question: How does the complexity of real-world questions, such as multi-hop relations, affect the performance of RALMs?
- Basis in paper: [inferred] The paper focuses on triple-based questions for a deep analysis of LMs with clearly characterized questions. However, it mentions that real-world questions often exhibit complexity beyond simple triple-based questions, for instance, encompassing multi-hop relations.
- Why unresolved: The paper does not explore the impact of complex real-world questions on the performance of RALMs. It is unclear how the models handle questions with multi-hop relations and how their performance is affected.
- What evidence would resolve it: Conducting experiments with RALMs using real-world questions with multi-hop relations and analyzing their performance would provide insights into how the complexity of questions affects the models' recall and retrieval abilities.

## Limitations

- The study assumes that Wikipedia text distribution reflects the pre-training corpus distribution of recent proprietary models like GPT-4, which is not empirically verified.
- The selective memory integration approach relies on frequency-based heuristics that may not generalize to domains with different popularity distributions or to questions involving multi-hop reasoning.
- The WiTQA dataset construction methodology, while innovative, may not fully capture the complexity of real-world questions that often involve multi-hop relations or commonsense knowledge.

## Confidence

**High Confidence:**
- Larger language models demonstrate superior recall of popular facts compared to smaller models
- Retrieval augmentation significantly improves performance for less popular questions across diverse retrievers and language models
- Retrieval errors for popular questions can cause language models to override correct knowledge, leading to performance degradation

**Medium Confidence:**
- The WiTQA dataset construction methodology reliably captures the distinction between popular and less popular entity-relation pairs
- The selective memory integration approach generalizes beyond the tested question types

**Low Confidence:**
- The pre-training corpus distribution assumption holds for all tested proprietary models
- The frequency-based thresholds in the selective memory integration approach represent optimal cutoffs

## Next Checks

1. **Pre-training Corpus Distribution Validation**: Obtain access to pre-training corpus statistics for GPT-4 and other proprietary models to empirically verify whether Wikipedia distribution indeed reflects their training data. This would involve comparing n-gram frequencies, entity distributions, and topical coverage between Wikipedia and actual pre-training corpora.

2. **Cross-Domain Generalization Test**: Apply the selective memory integration approach to a non-Wikipedia dataset (such as scientific literature or news articles) to verify whether the popularity-based heuristics transfer to domains with different information distributions and update frequencies.

3. **Multi-hop Reasoning Evaluation**: Design questions requiring multi-step reasoning or commonsense knowledge that don't map cleanly to single entity-relation pairs. Test whether the frequency-based approach remains effective when the relationship between popularity metrics and question difficulty becomes less direct.