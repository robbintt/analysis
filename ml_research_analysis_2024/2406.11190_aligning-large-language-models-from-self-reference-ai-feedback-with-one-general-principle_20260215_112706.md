---
ver: rpa2
title: Aligning Large Language Models from Self-Reference AI Feedback with one General
  Principle
arxiv_id: '2406.11190'
source_url: https://arxiv.org/abs/2406.11190
tags:
- preference
- response
- feedback
- responses
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a self-reference-based AI feedback framework
  for aligning large language models with human preferences using a single general
  principle like "best for humanity." The method enables the model to first generate
  its own response as a reference, then critique other responses based on that reference,
  and finally select the preferred option. The approach incorporates self-consistency
  voting to mitigate position bias and uses semantic perplexity to quantify preference
  strength.
---

# Aligning Large Language Models from Self-Reference AI Feedback with one General Principle

## Quick Facts
- arXiv ID: 2406.11190
- Source URL: https://arxiv.org/abs/2406.11190
- Authors: Rong Bao; Rui Zheng; Shihan Dou; Xiao Wang; Enyu Zhou; Bo Wang; Qi Zhang; Liang Ding; Dacheng Tao
- Reference count: 30
- Key outcome: Self-reference AI feedback framework improves alignment with a single general principle, achieving 75%+ win rates against baselines

## Executive Summary
This paper introduces a novel framework for aligning large language models with human preferences using self-reference AI feedback. The approach centers on a single general principle like "best for humanity" and enables the model to generate its own response as context before critiquing others. The method incorporates self-consistency voting to mitigate position bias and uses semantic perplexity to quantify preference strength. Experimental results demonstrate significant improvements in both harmlessness and helpfulness evaluations, with a 13B annotator achieving feedback quality comparable to a 70B annotator using existing methods.

## Method Summary
The framework follows a three-step process: first, an annotator model generates its own response to serve as a reference; second, it critiques other candidate responses against this reference; and third, it selects the preferred response using self-consistency voting with position swapping. The approach uses semantic perplexity to quantify preference strength and incorporates these margins into reward model training. The system is evaluated through reinforcement learning with PPO, producing policy models that significantly outperform baseline methods on harmlessness and helpfulness benchmarks.

## Key Results
- 13B annotator achieves feedback quality comparable to 70B annotator using existing methods
- Win rates exceeding 75% against baseline methods on PKU-SafeRLHF and AlpacaEval datasets
- Significant improvements in both harmlessness and helpfulness evaluations

## Why This Works (Mechanism)

### Mechanism 1
Self-reference enables the annotator model to better understand human intentions behind a general principle by generating its own response as a reference before critiquing others. The annotator first generates a response to the user's query, which serves as a contextual reference. It then critiques other candidate responses against this reference, allowing the model to understand how the general principle ("best for humanity") applies in this specific context. Core assumption: The annotator's own response provides better contextual grounding for evaluating other responses than abstract principles alone.

### Mechanism 2
Self-consistency voting across multiple generations with swapped response positions reduces position bias in preference selection. The framework generates preference feedback multiple times with the order of candidate responses swapped between options A and B. A majority vote determines the final preference, reducing the influence of response position on the annotator's choice. Core assumption: Position bias affects annotator choices, and multiple votes with position swapping can mitigate this bias.

### Mechanism 3
Semantic perplexity provides a quantitative measure of preference strength that enhances the reward model's ability to distinguish between responses. The annotator model calculates text perplexity for each candidate response. The absolute difference in perplexity between preferred and rejected responses is used as a margin score in the reward model's loss function. Core assumption: Lower perplexity indicates better response quality, and the difference in perplexity can quantify preference strength.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: The paper builds on RLHF framework, replacing human feedback with AI feedback while maintaining the same overall training pipeline
  - Quick check question: What are the three main steps in RLHF and how does this paper modify them?

- Concept: Position bias in preference annotation
  - Why needed here: The paper specifically addresses position bias as a key problem in AI feedback, requiring understanding of how response ordering affects annotator choices
  - Quick check question: How does the paper measure and mitigate position bias in annotator responses?

- Concept: Text perplexity as a language model quality metric
  - Why needed here: The paper uses semantic perplexity to quantify preference strength, requiring understanding of what perplexity measures and how it relates to response quality
  - Quick check question: What does lower text perplexity indicate about a language model's response?

## Architecture Onboarding

- Component map: User query → initial policy model → annotator with self-reference → preference feedback → reward model training → reinforcement learning → aligned policy model
- Critical path: User query → initial policy model → annotator with self-reference → preference feedback → reward model training → reinforcement learning → aligned policy model
- Design tradeoffs: Using a single general principle ("best for humanity") simplifies the framework but requires the annotator to generalize well across contexts, while using multiple specific principles would be more comprehensive but harder to scale
- Failure signatures: Poor annotator responses leading to incorrect preference feedback, position bias not being fully mitigated by voting, or perplexity measurements not correlating with human preferences
- First 3 experiments:
  1. Test annotator response quality and preference extraction with different model sizes (7B, 13B, 70B) using simple queries
  2. Measure position bias by comparing preference choices with different response orderings before implementing voting
  3. Validate perplexity-based margin scores by checking correlation with human preference strength on a validation set

## Open Questions the Paper Calls Out

### Open Question 1
How does the self-reference mechanism perform when applied to different types of AI feedback tasks beyond preference ranking, such as text summarization or code generation? Basis in paper: [inferred] The paper focuses on preference ranking but mentions the potential for broader applications of the self-reference mechanism. Why unresolved: The paper only evaluates the self-reference mechanism on preference ranking tasks, leaving its effectiveness on other AI feedback tasks unexplored. What evidence would resolve it: Experiments comparing the self-reference mechanism's performance across various AI feedback tasks, including text summarization, code generation, and question answering.

### Open Question 2
What is the optimal number of votes required for the self-consistency method to achieve the best trade-off between accuracy and computational cost? Basis in paper: [explicit] The paper mentions that increasing the number of votes improves accuracy but also increases computational overhead. Why unresolved: The paper only tests a limited range of vote numbers (4, 6, and 8) and does not provide a clear recommendation for the optimal number of votes. What evidence would resolve it: A systematic study varying the number of votes across a wider range and measuring the resulting accuracy and computational cost.

### Open Question 3
How does the self-reference mechanism affect the diversity of the generated responses compared to traditional methods? Basis in paper: [inferred] The paper focuses on improving the quality of preference feedback but does not explicitly discuss the impact on response diversity. Why unresolved: The paper does not provide any analysis or metrics related to response diversity, leaving this aspect unexplored. What evidence would resolve it: Experiments measuring the diversity of responses generated using the self-reference mechanism compared to traditional methods, using metrics such as n-gram diversity or semantic similarity.

## Limitations

- Reliance on a single general principle may not capture the full complexity of human preferences across different contexts and domains
- Dependence on annotator model's reference response quality introduces potential single point of failure
- Effectiveness of semantic perplexity-based margin scores for quantifying preference strength needs more rigorous validation across diverse domains

## Confidence

**High confidence** in the empirical results showing improved win rates against baseline methods on standard benchmark datasets (PKU-SafeRLHF and AlpacaEval). The experimental methodology and evaluation procedures are clearly specified and reproducible.

**Medium confidence** in the effectiveness of the semantic perplexity-based margin scores for quantifying preference strength. While the approach is theoretically sound, the correlation between perplexity differences and actual preference strength needs more rigorous validation across diverse domains.

**Low confidence** in the generalizability of using a single general principle across all alignment scenarios. The paper doesn't adequately address how the framework would handle situations where "best for humanity" conflicts with other important considerations like individual rights or privacy.

## Next Checks

1. **Cross-domain preference consistency**: Test the framework's performance on diverse datasets spanning creative writing, technical documentation, and sensitive topics to verify that the single principle approach generalizes effectively across different domains.

2. **Position bias quantification**: Systematically measure the magnitude of position bias in annotator responses before and after implementing self-consistency voting, and test whether different voting thresholds (e.g., 2/3 vs 3/5) affect the bias mitigation effectiveness.

3. **Perplexity correlation validation**: Conduct human preference studies to directly compare semantic perplexity-based margin scores against human-judged preference strength across multiple response pairs, particularly focusing on edge cases where perplexity might not align with human preferences.