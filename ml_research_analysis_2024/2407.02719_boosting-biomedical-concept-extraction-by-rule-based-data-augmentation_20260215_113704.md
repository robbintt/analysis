---
ver: rpa2
title: Boosting Biomedical Concept Extraction by Rule-Based Data Augmentation
arxiv_id: '2407.02719'
source_url: https://arxiv.org/abs/2407.02719
tags:
- concept
- data
- concepts
- training
- biomedical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a method to improve document-level biomedical
  concept extraction by augmenting limited training data with pseudo-annotations generated
  by a rule-based tool. Their approach addresses the challenges of data scarcity and
  non-canonical concept names by using MetaMapLite to annotate additional documents
  from PubMed and PMC, filtering the results, and using them to train a neural model.
---

# Boosting Biomedical Concept Extraction by Rule-Based Data Augmentation

## Quick Facts
- arXiv ID: 2407.02719
- Source URL: https://arxiv.org/abs/2407.02719
- Authors: Qiwei Shao; Fengran Mo; Jian-Yun Nie
- Reference count: 25
- Primary result: Data augmentation with pseudo-annotations significantly improves biomedical concept extraction performance, especially for rare and non-canonical concepts

## Executive Summary
This paper addresses the challenge of document-level biomedical concept extraction in low-resource settings by proposing a rule-based data augmentation approach. The authors use MetaMapLite to generate pseudo-annotations from PubMed and PMC documents, creating additional training data for concepts with insufficient manual annotations. By incorporating these augmented examples into a contrastive learning framework with weighted InfoNCE loss, the model learns to recognize both canonical and non-canonical concept mentions more effectively. Experiments on BC5CDR and NCBI-Disease datasets show significant performance improvements, particularly for rare and non-canonical concepts.

## Method Summary
The method employs MetaMapLite to generate pseudo-annotations from PubMed and PMC documents, creating additional training data for concepts with fewer than k occurrences in the original training set. The augmented data is filtered using rules that remove abbreviations, overlapping annotations, and ensure diversity. The model is trained using contrastive learning with InfoNCE loss, where the loss contribution from pseudo-annotated examples is weighted by a parameter wa. The approach combines document-level BERT encoders with concept encoders that process concatenated concept names and descriptions, enabling the model to retrieve relevant concepts based on document similarity.

## Key Results
- Data augmentation significantly improves F1 scores for concepts with insufficient manual annotations
- The weighted InfoNCE loss with augmented weight wa effectively balances learning from manual and pseudo-annotations
- Performance gains are particularly notable for rare and non-canonical concepts
- The method outperforms several baseline approaches including BioBERT, BioLinkBERT, and SapBERT

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pseudo-annotations generated by MetaMapLite expand training data coverage for rare and non-canonical concepts, improving model performance on these concepts.
- Mechanism: MetaMapLite identifies concept mentions across PubMed/PMC documents and assigns UMLS concept IDs. This creates training examples for concepts that rarely appear in manually labeled data.
- Core assumption: The majority of MetaMapLite annotations are correct enough that noisy labels still provide useful training signal.
- Evidence anchors:
  - [abstract] "We employ MetaMapLite, an existing rule-based concept mapping system, to generate additional pseudo-annotated data"
  - [section 4.1.2] "We do not assume that the results of such automatic labeling are always correct. Noisy annotations are still useful when the majority of the labels are correct."
  - [corpus] Weak evidence - the method assumes MetaMapLite has reasonable accuracy, but no direct corpus evidence provided for MetaMapLite's precision/recall.
- Break condition: If MetaMapLite's annotation error rate exceeds ~40%, the noise overwhelms the learning signal and performance degrades.

### Mechanism 2
- Claim: Data augmentation with diverse contexts helps the model learn non-canonical concept names by exposing it to multiple ways concepts are expressed.
- Mechanism: By retrieving documents containing the same concept and adding them as training examples, the model sees the concept in varied linguistic contexts, enabling it to recognize non-canonical mentions.
- Core assumption: Non-canonical concept mentions share similar contexts with canonical concept names, allowing the model to infer the concept from context.
- Evidence anchors:
  - [section 4.1.3] "Non-canonical concept mentions often share similar contexts with canonical concept names"
  - [section 6.1] "Our data augmentation method significantly improves the F1 score for concepts with insufficient manual annotations"
  - [corpus] Some evidence from [section 6.1] showing improved performance on non-canonical concepts, but no direct corpus analysis of context similarity.
- Break condition: If the augmented documents don't contain diverse enough paraphrases of the concept, the model won't learn to generalize to new forms.

### Mechanism 3
- Claim: The weighted InfoNCE loss with augmented weight wa allows the model to balance learning from clean manual annotations versus noisy pseudo-annotations.
- Mechanism: The augmented weight parameter scales the loss contribution from pseudo-annotated examples, allowing the model to downweight potentially noisy examples while still learning from them.
- Core assumption: MetaMapLite annotations have consistent quality that can be captured by a single weight parameter.
- Evidence anchors:
  - [section 4.3] "a hyperparameter wa is adapted to assign a weight to each augmented example"
  - [section 6.3] "Figure 3 shows the effect of wa on the F1 for BC5CDR and NCBI-Disease"
  - [corpus] Weak evidence - the corpus only shows wa affects performance but doesn't validate whether a single weight adequately captures annotation quality variation.
- Break condition: If annotation quality varies significantly across concepts or documents, a single global weight won't properly balance the learning signal.

## Foundational Learning

- Concept: Document-level concept extraction as a retrieval problem
  - Why needed here: The paper reformulates concept extraction from a NER-then-EL pipeline to a retrieval task where the model retrieves relevant concepts based on document similarity.
  - Quick check question: How does treating concept extraction as retrieval differ from traditional NER-EL pipelines?

- Concept: Contrastive learning with InfoNCE loss
  - Why needed here: The model is trained using contrastive learning where positive document-concept pairs are contrasted with negative pairs, optimized using InfoNCE loss.
  - Quick check question: What is the role of positive and negative pairs in contrastive learning for this task?

- Concept: Data augmentation for low-resource learning
  - Why needed here: The paper addresses data scarcity by generating additional training examples for rare concepts through pseudo-annotation.
  - Quick check question: Why is data augmentation particularly important for biomedical concept extraction compared to general NLP tasks?

## Architecture Onboarding

- Component map: Document → BERT encoder → embedding → similarity computation with concept embeddings → top-k concept retrieval
- Critical path: Document → BERT encoder → embedding → similarity computation with concept embeddings → top-k concept retrieval
- Design tradeoffs: Using a retrieval framework sacrifices mention-level precision for document-level coverage; rule-based augmentation trades annotation quality for quantity
- Failure signatures: Poor performance on rare concepts suggests insufficient augmentation; failure on non-canonical concepts suggests augmentation lacks diversity; degraded overall performance suggests annotation noise overwhelms signal
- First 3 experiments:
  1. Train baseline BioBERT on manual annotations only, measure performance on all concepts
  2. Train with MetaMapLite augmentation at k=5, measure improvement specifically on rare concepts
  3. Vary the augmented weight wa parameter to find optimal balance between manual and pseudo annotations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal augmented weight (wa) for different biomedical concept extraction tasks and datasets?
- Basis in paper: [explicit] The authors found that optimal augmented weight varies between BC5CDR (0.4) and NCBI-Disease (0.2), suggesting task-specific optimization is needed
- Why unresolved: The study only tested two datasets and did not systematically explore how augmented weight interacts with different concept types, dataset sizes, or model architectures
- What evidence would resolve it: A comprehensive ablation study varying wa across multiple biomedical datasets with different characteristics (size, concept diversity, annotation quality) would reveal systematic patterns

### Open Question 2
- Question: How does the quality of pseudo-annotations affect model performance over time as the augmented corpus grows?
- Basis in paper: [inferred] The authors mention that MetaMapLite annotations are not entirely accurate and use an augmented weight to capture flaws, but don't explore how annotation quality changes with corpus size
- Why unresolved: The study uses a fixed threshold approach and doesn't analyze whether annotation quality improves or degrades as more documents are added
- What evidence would resolve it: Longitudinal analysis tracking annotation precision/recall metrics as augmented corpus size increases, combined with corresponding model performance metrics

### Open Question 3
- Question: Can the data augmentation approach be effectively combined with other domain adaptation techniques like continual learning or curriculum learning?
- Basis in paper: [explicit] The authors mention that their approach is complementary to SapBERT but not BioLinkBERT, suggesting potential for combination with other techniques
- Why unresolved: The study only tested two combination approaches and didn't explore other potential synergies with different adaptation strategies
- What evidence would resolve it: Systematic experimentation combining data augmentation with various domain adaptation techniques (continual learning, curriculum learning, self-training) across multiple biomedical tasks

## Limitations

- Evaluation relies entirely on two datasets without validation on additional biomedical corpora
- MetaMapLite annotation quality remains uncertain with no empirical validation across different concept types
- The weighted InfoNCE loss depends on a single global parameter wa that may not capture varying annotation quality
- The retrieval-based approach sacrifices mention-level precision for document-level coverage

## Confidence

The paper's core claims are **Medium** confidence due to several limitations. While the proposed data augmentation method shows clear performance improvements on BC5CDR and NCBI-Disease datasets, the evaluation relies entirely on two datasets without validation on additional biomedical corpora. The MetaMapLite annotation quality remains uncertain - the paper assumes most annotations are correct but doesn't empirically validate this assumption across different concept types. The effectiveness of the weighted InfoNCE loss depends on a single global parameter wa, which may not capture the varying quality of pseudo-annotations across different concepts or document contexts. Additionally, the retrieval-based approach sacrifices mention-level precision for document-level coverage, limiting applicability to tasks requiring exact span identification.

## Next Checks

1. Evaluate the augmentation method on additional biomedical datasets (e.g., MedMentions, BioCDR) to assess generalizability beyond the two tested datasets.
2. Conduct error analysis on MetaMapLite annotations to quantify annotation quality across different concept types and document sources.
3. Test whether concept-specific weights (rather than a single global wa parameter) improve performance by better accounting for varying annotation quality.