---
ver: rpa2
title: Listenable Maps for Audio Classifiers
arxiv_id: '2403.13086'
source_url: https://arxiv.org/abs/2403.13086
tags:
- l-mac
- audio
- classifier
- interpretations
- decoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Listenable Maps for Audio Classifiers (L-MAC),
  a posthoc interpretation method that generates faithful and listenable interpretations
  for audio classifiers. L-MAC employs a decoder on top of a pretrained classifier
  to generate binary masks that highlight relevant portions of the input audio.
---

# Listenable Maps for Audio Classifiers

## Quick Facts
- arXiv ID: 2403.13086
- Source URL: https://arxiv.org/abs/2403.13086
- Authors: Francesco Paissan; Mirco Ravanelli; Cem Subakan
- Reference count: 14
- One-line primary result: Introduces L-MAC, a posthoc interpretation method generating faithful and listenable interpretations for audio classifiers by applying binary masks to linear spectrograms and using inverse STFT.

## Executive Summary
This paper introduces Listenable Maps for Audio Classifiers (L-MAC), a posthoc interpretation method that generates faithful and listenable interpretations for audio classifiers. L-MAC employs a decoder on top of a pretrained classifier to generate binary masks that highlight relevant portions of the input audio. The decoder is trained with a loss function that maximizes the confidence of the classifier decision on the masked-in portion of the audio while minimizing the probability of model output for the masked-out portion. The method addresses the challenge of interpreting audio classifiers, which often operate on less interpretable inputs like mel-spectrograms. L-MAC produces listenable interpretations by applying the binary mask to the magnitude of Short-Time Fourier Transform (STFT) of the original input audio waveform and performing the Inverse Short-Time Fourier Transform (ISTFT).

## Method Summary
L-MAC trains a decoder to generate binary masks highlighting relevant audio segments. The decoder uses classifier latent representations as input and applies masks to linear spectrograms rather than classifier-specific features. Training minimizes a composite loss combining categorical cross-entropy on masked-in and masked-out portions, plus L1 regularization. An optional fine-tuning stage improves audio quality by encouraging masked outputs to stay close to the original input. The method is evaluated on ESC-50 dataset with WHAM! noise augmentation for out-of-domain conditions, using faithfulness metrics and user preference studies.

## Key Results
- L-MAC consistently produces more faithful interpretations than several gradient and masking-based methodologies on both in-domain and out-of-domain data
- User study confirms that, on average, users prefer the interpretations generated by the proposed technique
- Quantitative evaluations demonstrate superior performance across faithfulness metrics including Average Increase, Average Drop, and Faithfulness on Spectra

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The decoder can learn a binary mask that maximizes classifier confidence on the masked-in portion while minimizing it on the masked-out portion.
- Mechanism: The loss function explicitly optimizes two competing terms: Lin (maximizing confidence on the masked-in signal) and Lout (minimizing confidence on the masked-out signal). This forces the decoder to identify time-frequency regions that are causally relevant to the classifier's decision.
- Core assumption: The classifier's latent representation h contains sufficient information to guide mask generation that reflects the classifier's decision process.
- Evidence anchors:
  - [abstract] "The decoder is trained with a loss function that maximizes the confidence of the classifier decision on the masked-in portion of the audio while minimizing the probability of model output for the masked-out portion."
  - [section 2.1] "The masking loss employed in this work draws inspiration from similar objectives... The goal is to maximize the confidence of the classification decision for the masked-in portion of the audio while minimizing it for the masked-out portion."
- Break condition: If the latent representation h does not encode discriminative features relevant to the classification decision, the decoder cannot generate a faithful mask.

### Mechanism 2
- Claim: Applying the mask to the linear spectrogram (rather than classifier-specific features) enables listenable interpretations while maintaining faithfulness.
- Mechanism: The decoder outputs a mask for the linear spectrogram X, which is then inverted using the original phase to produce a listenable waveform. This bypasses the non-invertible compression in features like mel-spectrograms.
- Core assumption: The classifier's decision can be faithfully reconstructed from the masked linear spectrogram, even if the classifier internally uses compressed features.
- Evidence anchors:
  - [abstract] "The decoder applies the mask not directly to the specific input features of the pretrained classifier but to the magnitude of Short-Time Fourier Transform (STFT) of the original input audio waveform."
  - [section 2.2] "In our pipeline... we tackle this challenge by having the decoder Mθ(.) output a mask for the linear spectrogram X instead of generating a mask for the specific features used by the pretrained classifier."
- Break condition: If the classifier relies heavily on compressed feature statistics that are lost when masking the linear spectrogram, the interpretation may lose faithfulness.

### Mechanism 3
- Claim: Fine-tuning the decoder with a guidance term can improve audio quality without substantially sacrificing faithfulness.
- Mechanism: An additional regularization term encourages the masked signal to stay close to the original spectrogram during training, improving audio quality. This is applied selectively based on mask similarity to avoid steering away from faithful interpretations.
- Core assumption: There exists a region in mask space where both high audio quality and high faithfulness can be achieved simultaneously.
- Evidence anchors:
  - [section 2.1] "After the initial mask optimization, this framework allows a fine-tuning stage where the interpretation mask is refined to enhance the quality of the interpretations... This selective fine-tuning helps prevent steering the masks away from faithful interpretations."
  - [section 3.2] "We observe that after finetuning, L-MAC can still be more faithful compared to the other baselines we investigate, including our L2I implementation."
- Break condition: If the guidance term dominates during fine-tuning, the mask may shift toward preserving audio quality at the expense of interpretability.

## Foundational Learning

- Concept: Short-Time Fourier Transform (STFT) and its inverse (ISTFT)
  - Why needed here: L-MAC applies masks to the magnitude of STFT and then inverts using the original phase to produce listenable waveforms. Understanding STFT/ISTFT is essential to grasp how the method generates audio interpretations.
  - Quick check question: If you mask the magnitude of an STFT and keep the original phase, what property of the original signal is preserved in the reconstructed waveform?

- Concept: Binary masking for signal separation
  - Why needed here: The decoder generates a binary mask to select relevant portions of the spectrogram. This is analogous to binary masking in source separation, but here the goal is interpretability rather than separation.
  - Quick check question: In binary masking for source separation, what happens if the mask threshold is set too low versus too high?

- Concept: Gradient-based vs. masking-based interpretability methods
  - Why needed here: L-MAC is a masking-based method, contrasting with gradient-based approaches like saliency maps. Understanding the differences helps contextualize its design choices and advantages.
  - Quick check question: Why might masking-based methods be more suitable than gradient-based methods for generating interpretable audio explanations?

## Architecture Onboarding

- Component map: Audio waveform -> Linear spectrogram X -> Feature extraction -> Classifier -> Latent representations h -> Decoder -> Binary mask M -> Masked spectrogram -> ISTFT -> Listen interpretation

- Critical path: Waveform → Linear spectrogram → Feature extraction → Classifier → Latent representations → Decoder → Binary mask → Masked spectrogram → ISTFT → Listen interpretation

- Design tradeoffs:
  - Masking in linear spectrogram vs. classifier features: Enables listenability but may lose some feature-specific information
  - Binary vs. continuous mask: Binary masks are more interpretable but may be harder to optimize
  - Fine-tuning with guidance: Improves audio quality but risks reducing faithfulness if not carefully controlled

- Failure signatures:
  - Mask is nearly all ones or all zeros: Decoder failed to learn meaningful separation
  - Interpretation sounds like noise or silence: Phase information not properly preserved or mask too aggressive
  - Interpretation does not match classifier's decision: Loss function not properly optimized or latent representation insufficient

- First 3 experiments:
  1. Verify that the decoder can generate non-trivial binary masks on simple synthetic data where ground truth relevance is known
  2. Test that masking the linear spectrogram and applying ISTFT produces reasonable audio quality before connecting to the classifier
  3. Confirm that the loss function properly optimizes both Lin and Lout terms by monitoring their values during training on a small validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed L-MAC method be extended to handle multi-label classification tasks, where each audio sample can belong to multiple classes?
- Basis in paper: [inferred] The paper mentions multi-label classification as a potential application but does not provide any details on how L-MAC would handle this scenario.
- Why unresolved: The paper focuses on single-label classification and does not discuss the challenges or modifications needed for multi-label classification.
- What evidence would resolve it: Experiments and results demonstrating the effectiveness of L-MAC on multi-label classification tasks would resolve this question.

### Open Question 2
- Question: What is the impact of the choice of the classifier's architecture on the quality and faithfulness of the interpretations generated by L-MAC?
- Basis in paper: [explicit] The paper mentions that the proposed method can work with different classifier architectures, but it does not provide a detailed analysis of the impact of the architecture choice on the interpretations.
- Why unresolved: The paper does not explore the relationship between the classifier's architecture and the quality of the generated interpretations.
- What evidence would resolve it: Experiments comparing the interpretations generated by L-MAC when using different classifier architectures would help answer this question.

### Open Question 3
- Question: How does the proposed L-MAC method perform on audio datasets with varying levels of noise and complexity, and what are the limitations of the method in such scenarios?
- Basis in paper: [inferred] The paper mentions that the classifier is trained on data augmented with WHAM! noise, but it does not provide a detailed analysis of L-MAC's performance on datasets with varying noise levels and complexity.
- Why unresolved: The paper does not explore the robustness of L-MAC to different levels of noise and complexity in the audio data.
- What evidence would resolve it: Experiments evaluating L-MAC's performance on audio datasets with different levels of noise and complexity would help answer this question.

## Limitations

- The approach assumes classifier latent representations contain sufficient discriminative information for faithful mask generation, which may not hold for all architectures
- Masking linear spectrograms rather than classifier-specific features may lose important feature statistics that the classifier relies upon
- The fine-tuning stage with guidance terms risks steering interpretations away from faithfulness toward better audio quality

## Confidence

- High confidence: The methodological framework and training procedure are well-specified and reproducible. The evaluation metrics and user study design are appropriate for the task.
- Medium confidence: The claim that L-MAC consistently produces more faithful interpretations than baselines is supported by quantitative results, but the comparison with some baselines (particularly L2I) shows mixed results that suggest the advantage may be context-dependent.
- Low confidence: The generalizability of the approach to other audio classification tasks beyond ESC-50 and the robustness of the method to different classifier architectures remain untested.

## Next Checks

1. **Latent representation sufficiency test**: Systematically evaluate how different choices of classifier architectures (varying depth, compression ratios, and feature types) affect the decoder's ability to generate faithful masks. This would test the core assumption that classifier latents contain sufficient information for interpretability.

2. **Feature-space vs. time-domain masking comparison**: Conduct ablation studies comparing L-MAC's approach of masking linear spectrograms with variants that mask directly in the classifier's feature space. Measure both faithfulness and listenability trade-offs quantitatively.

3. **Cross-task transferability assessment**: Apply L-MAC to audio classification tasks with different characteristics (e.g., longer audio clips, different noise conditions, or multi-label classification) to evaluate its generalizability beyond the ESC-50 dataset used in the paper.