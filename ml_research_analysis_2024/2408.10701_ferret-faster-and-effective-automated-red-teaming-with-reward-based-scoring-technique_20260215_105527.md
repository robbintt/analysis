---
ver: rpa2
title: 'Ferret: Faster and Effective Automated Red Teaming with Reward-Based Scoring
  Technique'
arxiv_id: '2408.10701'
source_url: https://arxiv.org/abs/2408.10701
tags:
- llama
- prompt
- prompts
- teaming
- risk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FERRET, a novel approach to automated red-teaming
  for Large Language Models (LLMs) that addresses limitations in existing methods
  like slow performance and limited diversity. FERRET enhances the RAINBOW TEAMING
  framework by generating multiple adversarial prompt mutations per iteration and
  employing a scoring function to rank and select the most effective adversarial prompts.
---

# Ferret: Faster and Effective Automated Red Teaming with Reward-Based Scoring Technique

## Quick Facts
- arXiv ID: 2408.10701
- Source URL: https://arxiv.org/abs/2408.10701
- Authors: Tej Deep Pala; Vernon Y. H. Toh; Rishabh Bhardwaj; Soujanya Poria
- Reference count: 4
- Primary result: FERRET achieves 95% ASR on Llama 2-chat 7B, a 46% improvement over RAINBOW TEAMING, and reduces time to 90% ASR by 15.2%

## Executive Summary
This paper introduces FERRET, a novel approach to automated red-teaming for Large Language Models (LLMs) that addresses limitations in existing methods like slow performance and limited diversity. FERRET enhances the RAINBOW TEAMING framework by generating multiple adversarial prompt mutations per iteration and employing a scoring function to rank and select the most effective adversarial prompts. The method explores various scoring functions, including reward models, Llama Guard, and LLM-as-a-judge, to improve the efficiency of finding harmful mutations.

## Method Summary
FERRET builds upon RAINBOW TEAMING by generating multiple adversarial prompt mutations per iteration and using a scoring function to rank and select the most effective adversarial prompt. The method uses Mistral-7B as the mutator model to generate candidate prompts, applies categorical filtering using Llama Guard 2 to ensure alignment with desired risk categories, and employs a reward model trained on a preference dataset of 24,000 pairs to rank prompts based on harmfulness. The approach aims to improve the quality-diversity search for adversarial prompts, achieving higher attack success rates with reduced computational overhead.

## Key Results
- FERRET achieves 95% Attack Success Rate (ASR) on Llama 2-chat 7B, a 46% improvement over RAINBOW TEAMING
- Reduces time needed to reach 90% ASR by 15.2%
- Generates adversarial prompts that are transferable to other larger LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using a reward model as the scoring function significantly improves the quality and diversity of adversarial prompts compared to baseline methods.
- Mechanism: The reward model is fine-tuned on a preference dataset of 24,000 pairs, allowing it to rank adversarial prompts based on their harmfulness more effectively than simpler scoring functions like Llama Guard 2 fitness scores.
- Core assumption: The preference dataset accurately captures the relative harmfulness of different adversarial prompts and is representative of real-world vulnerabilities.
- Evidence anchors:
  - [abstract] "FERRET, utilizing a reward model as a scoring function, improves the overall attack success rate (ASR) to 95%, which is 46% higher than RAINBOW TEAMING."
  - [section] "Leveraging this dataset, we fine-tune a reward model to enhance the scoring step in FERRET."
- Break condition: If the reward model overfits to the training data or the preference dataset is not representative of real-world vulnerabilities, the effectiveness of FERRET could decrease.

### Mechanism 2
- Claim: Generating multiple mutations per iteration and using a scoring function to rank them improves the efficiency of finding harmful mutations.
- Mechanism: By generating N candidate prompts per iteration and filtering them based on their harmfulness and diversity, FERRET can more quickly converge to a set of effective adversarial prompts.
- Core assumption: The mutator model can generate diverse and meaningful mutations that explore the space of potential adversarial prompts effectively.
- Evidence anchors:
  - [abstract] "FERRET builds upon RAINBOW TEAMING by generating multiple adversarial prompt mutations per iteration and using a scoring function to rank and select the most effective adversarial prompt."
  - [section] "After mutation, we pass the candidate prompts to the target model to generate the responses to the candidate prompts."
- Break condition: If the mutator model is not capable of generating diverse and meaningful mutations, or if the scoring function is not effective at ranking them, the efficiency gains of FERRET could be lost.

### Mechanism 3
- Claim: The categorical filtering step ensures that generated prompts align with the desired risk categories, improving the diversity of the prompt archive.
- Mechanism: By filtering out prompts that do not align with the target risk category, FERRET ensures that the archive contains prompts that are both harmful and diverse in terms of the types of vulnerabilities they exploit.
- Core assumption: The scoring function used for categorical filtering accurately classifies prompts into the correct risk categories.
- Evidence anchors:
  - [section] "In the following stage, we use a scoring function to classify candidate prompts into risk categories and discard those that do not match the target risk category, rk, used in the mutation step."
  - [section] "This filtering is important because open-source models used as mutators may not have a good understanding of these risk categories."
- Break condition: If the scoring function used for categorical filtering is not accurate, FERRET could generate prompts that are not diverse in terms of the types of vulnerabilities they exploit.

## Foundational Learning

- Concept: Quality-Diversity Search
  - Why needed here: FERRET uses a quality-diversity search framework to generate diverse adversarial prompts that are both harmful and cover a wide range of vulnerability types.
  - Quick check question: What is the difference between a quality-diversity search and a traditional optimization approach?

- Concept: Reward Modeling
  - Why needed here: FERRET uses a reward model to score the harmfulness of adversarial prompts, allowing it to rank and select the most effective ones.
  - Quick check question: How does a reward model differ from a traditional classifier, and why is it better suited for this task?

- Concept: Adversarial Prompt Generation
  - Why needed here: FERRET is designed to generate adversarial prompts that can exploit vulnerabilities in LLMs, so a deep understanding of this concept is crucial.
  - Quick check question: What are some common strategies used in adversarial prompt generation, and how do they differ from traditional prompt engineering techniques?

## Architecture Onboarding

- Component map: Initial Archive -> Mutator Model (Mistral-7B) -> Categorical Filter (Llama Guard 2) -> Scoring Function (Reward Model) -> Archive
- Critical path: Sampling → Mutation → Categorical Filtering → Scoring → Archive Update
- Design tradeoffs:
  - Number of mutations per iteration: More mutations can lead to better results but also increase computational cost.
  - Choice of scoring function: Different scoring functions have different strengths and weaknesses in terms of accuracy and generalizability.
  - Size of the archive: A larger archive can store more diverse prompts but also requires more computational resources to maintain.
- Failure signatures:
  - Low ASR: Indicates that the generated prompts are not effective at exploiting vulnerabilities in the target model.
  - Lack of diversity: Suggests that the mutator model is not generating diverse enough prompts, or that the scoring function is not effectively ranking them.
  - Slow convergence: Could be due to an ineffective scoring function or a mutator model that is not generating meaningful mutations.
- First 3 experiments:
  1. Run FERRET with different numbers of mutations per iteration (e.g., 1, 3, 5) and compare the ASR and diversity of the generated prompts.
  2. Replace the reward model with a simpler scoring function like Llama Guard 2 fitness scores and evaluate the impact on performance.
  3. Test the transferability of the generated prompts by evaluating them on a different target model and measuring the ASR.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of FERRET scale with increasingly larger LLMs beyond the tested models?
- Basis in paper: [explicit] The paper mentions that FERRET's adversarial prompts are transferable to other LLMs of larger size, but only tests up to Llama 3-Instruct 70B and GPT-4o.
- Why unresolved: The paper does not explore the limits of transferability to much larger or more advanced LLMs that may emerge in the future.
- What evidence would resolve it: Experiments testing FERRET's adversarial prompts against the latest and largest LLMs, such as GPT-5 or Claude-3, to determine the upper bounds of transferability and effectiveness.

### Open Question 2
- Question: What is the impact of FERRET's categorical filtering step on the overall diversity of adversarial prompts in the long term?
- Basis in paper: [explicit] The paper describes the categorical filtering step but does not provide an in-depth analysis of its long-term effects on prompt diversity.
- Why unresolved: While the paper mentions that categorical filtering helps maintain diversity, it does not quantify how this affects the archive's diversity over many iterations or compare it to alternative diversity maintenance methods.
- What evidence would resolve it: A longitudinal study tracking the diversity metrics of the prompt archive over thousands of iterations with and without categorical filtering, and comparisons with other diversity-preserving techniques.

### Open Question 3
- Question: How sensitive is FERRET's performance to the choice of feature descriptors (risk categories and attack styles)?
- Basis in paper: [explicit] The paper uses specific risk categories and attack styles but does not explore the sensitivity of results to different choices of these descriptors.
- Why unresolved: The paper does not investigate whether the current choice of descriptors is optimal or how changing them might affect FERRET's performance.
- What evidence would resolve it: Systematic experiments varying the number and type of risk categories and attack styles, measuring the impact on FERRET's ASR and diversity metrics, to identify optimal descriptor configurations.

### Open Question 4
- Question: What are the computational trade-offs between using different scoring functions in FERRET?
- Basis in paper: [explicit] The paper explores multiple scoring functions (Llama Guard 2 Fitness, Mistral-as-Judge, etc.) but does not provide a detailed comparison of their computational costs.
- Why unresolved: While the paper compares the effectiveness of different scoring functions, it does not analyze the computational resources required by each, which is crucial for practical deployment.
- What evidence would resolve it: A comprehensive analysis of the computational resources (e.g., GPU hours, inference time) required by each scoring function across different dataset sizes and prompt complexities, along with a cost-effectiveness comparison.

## Limitations

- Data Dependence: The performance claims heavily rely on a specific preference dataset of 24,000 prompt pairs for reward model fine-tuning, without access to this dataset or details about its construction.
- Evaluation Scope: The paper reports results primarily on Llama 2-chat 7B and demonstrates transferability to other models, but the testing across different model architectures and sizes is limited.
- Computational Cost: While FERRET improves ASR and efficiency compared to RAINBOW TEAMING, the method still requires multiple iterations of prompt generation, evaluation, and scoring, which may limit practical deployment in resource-constrained environments.

## Confidence

**High Confidence**: The core mechanism of generating multiple mutations per iteration and using a scoring function to rank them is well-supported by the results. The 46% improvement in ASR over RAINBOW TEAMING and the 15.2% reduction in time to reach 90% ASR are directly demonstrated through the experiments.

**Medium Confidence**: The claim that the reward model significantly improves performance compared to simpler scoring functions is supported by the results, but the extent of this improvement may depend on the quality and representativeness of the preference dataset used for fine-tuning.

**Low Confidence**: The generalizability of FERRET's performance across different LLM architectures, sizes, and real-world deployment scenarios is not fully established due to the limited scope of the experiments.

## Next Checks

1. **Dataset Sensitivity Analysis**: Evaluate FERRET's performance using different preference datasets for reward model fine-tuning to assess the sensitivity of the results to the choice of training data. This will help determine the robustness of the approach to variations in the underlying data.

2. **Cross-Model Transferability Study**: Test the transferability of FERRET-generated adversarial prompts across a wider range of LLM architectures and sizes, including both open-source and proprietary models. This will provide insights into the generalizability of the approach and its potential for real-world application.

3. **Computational Efficiency Benchmarking**: Measure the computational overhead of FERRET compared to RAINBOW TEAMING and other baseline methods in terms of wall-clock time, GPU memory usage, and number of model inferences required. This will help assess the practical feasibility of deploying FERRET in resource-constrained environments.