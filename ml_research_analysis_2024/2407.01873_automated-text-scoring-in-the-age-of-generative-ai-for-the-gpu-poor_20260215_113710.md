---
ver: rpa2
title: Automated Text Scoring in the Age of Generative AI for the GPU-poor
arxiv_id: '2407.01873'
source_url: https://arxiv.org/abs/2407.01873
tags:
- glms
- feedback
- score
- response
- scoring
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that small, open-source generative language
  models (GLMs) can be efficiently fine-tuned to achieve adequate performance in automated
  text scoring (ATS), matching or exceeding state-of-the-art benchmarks for automated
  essay scoring (AES) and short answer scoring (ASAS). Using parameter-efficient fine-tuning
  (QLoRA) on consumer-grade hardware, models like Phi-3, Llama-3, and Mistral outperformed
  proprietary models in scoring accuracy (QWK metrics up to 0.789 for AES and 0.759
  for ASAS).
---

# Automated Text Scoring in the Age of Generative AI for the GPU-poor

## Quick Facts
- arXiv ID: 2407.01873
- Source URL: https://arxiv.org/abs/2407.01873
- Reference count: 40
- Primary result: Small open-source GLMs fine-tuned with QLoRA can match or exceed state-of-the-art ATS benchmarks on consumer-grade hardware

## Executive Summary
This study demonstrates that small, open-source generative language models (GLMs) can be efficiently fine-tuned to achieve adequate performance in automated text scoring (ATS), matching or exceeding state-of-the-art benchmarks for automated essay scoring (AES) and short answer scoring (ASAS). Using parameter-efficient fine-tuning (QLoRA) on consumer-grade hardware, models like Phi-3, Llama-3, and Mistral outperformed proprietary models in scoring accuracy (QWK metrics up to 0.789 for AES and 0.759 for ASAS). Additionally, these models were prompted to generate feedback explaining their scores, with Phi-3 and Llama-3 providing more consistent and accurate justifications compared to other models. While feedback generation shows promise, further validation is needed to ensure reliability and usefulness in educational contexts. The approach offers a secure, efficient, and transparent alternative to proprietary models, making ATS more accessible and customizable for educational research.

## Method Summary
The authors fine-tuned four open-source GLMs (Mistral-7B, Gemma-1.1, Llama-3-8B, Phi-3-mini) using QLoRA parameter-efficient fine-tuning on consumer-grade GPU hardware (24GB A10). Models were quantized to 4-bit using bitsandbytes and trained with LoRA adapters (rank 32) at learning rate 2e-4 (1e-4 for Gemma) with linear decay over 10 epochs. The ASAP AES dataset (12,978 essays, 8 stimuli, grades 7-10) and ASAP SAS dataset (17,043 responses, 10 items, grades 8-10) were used, with Quadratic Weighted Kappa (QWK) as the primary evaluation metric. After fine-tuning for scoring, models were prompted to generate feedback explaining their scores using provided rubric templates.

## Key Results
- Fine-tuned open-source GLMs achieved QWK scores up to 0.789 for AES and 0.759 for ASAS, outperforming proprietary models like GPT-3.5 and GPT-4
- Phi-3 and Llama-3 generated the most consistent and accurate score explanations compared to other models
- The approach successfully runs on consumer-grade hardware with 24GB GPU memory
- Parameter-efficient fine-tuning (QLoRA) enabled effective adaptation while maintaining reasonable resource requirements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Small open-source GLMs can achieve adequate automated text scoring performance when fine-tuned with parameter-efficient methods.
- Mechanism: LoRA-based fine-tuning allows efficient adaptation of large pretrained GLMs to scoring tasks using consumer-grade hardware.
- Core assumption: The pretrained knowledge in GLMs contains sufficient linguistic understanding for scoring tasks.
- Evidence anchors:
  - [abstract] "models like Phi-3, Llama-3, and Mistral outperformed proprietary models in scoring accuracy"
  - [section 3.4] "Learning rate was set to 2e-4...r and α, key parameters for LoRA, were each set to 32"
  - [corpus] Weak evidence - no direct citations found in neighbor papers
- Break condition: If the pretrained GLM lacks sufficient understanding of the specific scoring rubric semantics.

### Mechanism 2
- Claim: GLMs can generate score explanations by prompting after fine-tuning for scoring.
- Mechanism: The fine-tuned scoring behavior is preserved while the generative capabilities allow post-hoc explanation generation.
- Core assumption: The scoring behavior learned during fine-tuning is consistent enough to justify with explanations.
- Evidence anchors:
  - [abstract] "these models were prompted to generate feedback explaining their scores"
  - [section 4.3] "Phi-3 and Llama-3 providing more consistent and accurate justifications"
  - [corpus] Weak evidence - no direct citations found in neighbor papers
- Break condition: If the scoring behavior is too inconsistent or the rubric requires external knowledge not captured in the model.

### Mechanism 3
- Claim: Parameter-efficient fine-tuning (QLoRA) enables training on limited GPU resources.
- Mechanism: Combining 4-bit quantization with LoRA adapters drastically reduces memory requirements while preserving performance.
- Core assumption: The low-rank approximation captures the essential fine-tuning updates without significant performance loss.
- Evidence anchors:
  - [section 2.4] "We employ a combination of two approaches: (1) quantization...and (2) Low-Rank Adapters (LoRA)"
  - [section 3.4] "Models were loaded through Huggingface-hub, quantized into smaller, 4-bit models using bitsandbytes"
  - [corpus] Weak evidence - no direct citations found in neighbor papers
- Break condition: If the rank (r) is set too low, missing important scoring nuances.

## Foundational Learning

- Concept: Transformer architecture fundamentals
  - Why needed here: Understanding how GLMs process text is essential for interpreting fine-tuning results and limitations
  - Quick check question: What is the difference between encoder-only and decoder-only transformer architectures, and why does this matter for GLMs?

- Concept: Parameter-efficient fine-tuning methods
  - Why needed here: LoRA is central to the approach and understanding its mechanics helps with troubleshooting and optimization
  - Quick check question: How does LoRA modify the weight matrices during fine-tuning, and why is this more efficient than full fine-tuning?

- Concept: Automated text scoring metrics and evaluation
  - Why needed here: QWK (Quadratic Weighted Kappa) is the primary evaluation metric, and understanding it is crucial for interpreting results
  - Quick check question: What does a QWK score of 0.789 mean in the context of automated essay scoring, and how does it compare to human agreement?

## Architecture Onboarding

- Component map: Pretrained GLM → 4-bit quantization → LoRA adapters → Fine-tuning → Scoring → Prompting for feedback
- Critical path: Data loading → Model quantization → LoRA configuration → Training loop → Evaluation → Feedback generation
- Design tradeoffs: Model size vs. performance (larger models generally perform better but require more resources), fine-tuning vs. prompting (fine-tuning for scoring, prompting for feedback)
- Failure signatures: Poor QWK scores on dev set (overfitting/underfitting), repetitive or nonsensical feedback (scoring consistency issues), GPU memory errors (quantization/rank settings too aggressive)
- First 3 experiments:
  1. Train a single model (e.g., Phi-3) on one essay set with default LoRA settings, evaluate QWK
  2. Test feedback generation on the same model with various prompt templates, analyze quality
  3. Compare performance across different LoRA ranks (r=8, 16, 32) to find optimal tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can model-generated feedback be validated for educational use?
- Basis in paper: [explicit] The paper states that model-generated feedback requires "more rigorous evaluation focused on targeted use cases" and notes there is "no methodology...to easily validate such feedback."
- Why unresolved: Feedback validation involves complex educational constructs, requires collaboration with educators, and needs targeted use cases with clear definitions of usefulness.
- What evidence would resolve it: Development of validated datasets or frameworks specifically for educational feedback, coupled with empirical studies demonstrating consistent alignment between model feedback and human educational expertise.

### Open Question 2
- Question: What is the comparative performance of open-source GLMs versus proprietary models for both scoring and feedback generation?
- Basis in paper: [explicit] The paper compares fine-tuned open-source models to proprietary models like GPT-3.5 and GPT-4 for scoring, finding superior performance, but does not conduct a comparative analysis for feedback.
- Why unresolved: While scoring comparisons are made, feedback generation capabilities between open-source and proprietary models remain unexplored, leaving uncertainty about relative effectiveness.
- What evidence would resolve it: Direct comparative studies of feedback quality and consistency between open-source and proprietary models, using standardized educational rubrics and human evaluation.

### Open Question 3
- Question: What are the long-term sustainability and cost implications of using open-source GLMs versus proprietary models for automated text scoring?
- Basis in paper: [inferred] The paper highlights issues with proprietary models including high costs and inefficiency, and notes that open-source models can run on consumer-grade hardware, but does not provide quantitative cost comparisons.
- Why unresolved: While qualitative advantages of open-source models are discussed, there is a lack of empirical data on total cost of ownership, including training, deployment, and maintenance over time.
- What evidence would resolve it: Comprehensive cost analysis comparing open-source and proprietary models across different scales of educational implementation, including hardware, energy, and personnel costs.

## Limitations

- Data Generalization: The study relies solely on the ASAP dataset (grades 7-10), limiting conclusions about model performance across different educational levels, subjects, or cultural contexts.
- Feedback Quality Validation: The evaluation of model-generated feedback is qualitative rather than quantitative, with no systematic measure of feedback quality or educational value.
- Resource Requirements: The 24GB GPU requirement still excludes many potential users with lower-end hardware, despite being marketed as "GPU-poor."

## Confidence

**High Confidence:** The core claim that small open-source GLMs can achieve adequate automated text scoring performance when fine-tuned with parameter-efficient methods. This is strongly supported by quantitative QWK metrics (up to 0.789 for AES and 0.759 for ASAS) that exceed state-of-the-art benchmarks.

**Medium Confidence:** The assertion that GLMs can generate score explanations by prompting after fine-tuning. While the paper demonstrates this capability, the evaluation is primarily qualitative, and there is no systematic validation of feedback quality or educational utility.

**Low Confidence:** The generalizability of findings to contexts beyond the ASAP dataset (different grade levels, subjects, languages, or cultural contexts). The study's narrow focus on a specific dataset limits the strength of claims about broader applicability.

## Next Checks

1. **External Dataset Validation:** Evaluate the fine-tuned models on at least two additional automated text scoring datasets from different educational contexts (e.g., different grade levels, subjects, or languages) to assess generalizability beyond the ASAP dataset.

2. **Quantitative Feedback Analysis:** Develop a rubric-based evaluation framework for feedback quality, including metrics for rubric adherence, educational value, and linguistic quality. Apply this framework to compare feedback from different models systematically.

3. **Resource Optimization Study:** Conduct experiments to determine the minimum viable hardware requirements by testing model performance on GPUs with progressively lower memory (e.g., 16GB, 12GB, 8GB) and identifying the point where performance degradation becomes unacceptable.