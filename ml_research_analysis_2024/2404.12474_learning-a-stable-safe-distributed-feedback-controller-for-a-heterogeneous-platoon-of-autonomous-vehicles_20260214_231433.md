---
ver: rpa2
title: Learning a Stable, Safe, Distributed Feedback Controller for a Heterogeneous
  Platoon of Autonomous Vehicles
arxiv_id: '2404.12474'
source_url: https://arxiv.org/abs/2404.12474
tags:
- controller
- platoon
- vehicle
- vehicles
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents an algorithm for learning a stable, safe, distributed
  feedback controller for a heterogeneous platoon of autonomous vehicles. The algorithm
  leverages recent developments in learning neural network stability certificates
  to ensure the learned controller is provably stable for the entire platoon.
---

# Learning a Stable, Safe, Distributed Feedback Controller for a Heterogeneous Platoon of Autonomous Vehicles

## Quick Facts
- arXiv ID: 2404.12474
- Source URL: https://arxiv.org/abs/2404.12474
- Authors: Michael H. Shaham; Taskin Padir
- Reference count: 27
- Primary result: Algorithm learns stable, safe distributed controller for heterogeneous platoons using neural Lyapunov functions with MILP verification

## Executive Summary
This paper presents an algorithm for learning a stable, safe, distributed feedback controller for heterogeneous platoons of autonomous vehicles. The approach leverages neural network stability certificates to ensure provable stability across the entire platoon. A key innovation is reformulating heterogeneous platoon dynamics as homogeneous via a change of variable, enabling a single controller to be learned and verified. The learned controller is trained in simulation and evaluated on hardware using four F1Tenth vehicles, as well as in simulation with 100 vehicles, demonstrating practicality and outperforming linear feedback and DMPC controllers in stability and safety.

## Method Summary
The algorithm jointly learns a neural network controller and Lyapunov function by minimizing a loss combining Lyapunov condition violations and hand-designed behavioral penalties. Vehicle dynamics are reformulated from heterogeneous to homogeneous double integrators via a change of variable, allowing a single controller to be learned and verified. Verification uses MILPs to check positivity and decay conditions over the state space. The controller is trained in simulation and evaluated on hardware (F1Tenth vehicles) and in simulation with large platoons.

## Key Results
- Learned controller outperforms linear feedback and DMPC controllers in stability and safety metrics
- Reformulation enables single controller to work across heterogeneous vehicle dynamics
- Hardware demonstration on four F1Tenth vehicles validates simulation results
- Scalability demonstrated in simulation with 100-vehicle platoons

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reformulating heterogeneous platoon dynamics as homogeneous via change of variable enables learning and verifying a single controller.
- Mechanism: The paper introduces a change of variable $u_i(k) = a_i(k) - v_i(k) / \tau_i$ that transforms each vehicle's heterogeneous dynamics into identical double integrator dynamics. This makes the error dynamics $\bar{A}$ and $\bar{B}$ independent of the $\tau_i$ parameters.
- Core assumption: The change of variable preserves stability properties and does not introduce hidden coupling that invalidates centralized verification.
- Evidence anchors:
  - [section] "We need to ensure each vehicle is able to use the learned controller in the same way even if individual vehicle dynamics are different due to the $\tau_i$ parameters. To do this, we define $u_i(k) = a_i(k) - v_i(k) / \tau_i$..."
  - [abstract] "A key contribution is reformulating the dynamics of a heterogeneous platoon as a homogeneous platoon via a change of variable, enabling a single controller to be learned and verified."
- Break condition: If the change of variable introduces state-coupling not captured in the double integrator form, or if the linearization assumption breaks down at the operating envelope.

### Mechanism 2
- Claim: Neural Lyapunov function learning with MILP verification guarantees exponential stability within a compact region.
- Mechanism: The algorithm jointly learns a controller $\pi$ and Lyapunov function $V$ by minimizing a loss combining Lyapunov condition violations and hand-designed behavioral penalties. Verification uses MILPs to check positivity and decay conditions over the state space.
- Core assumption: The MILP formulations are tight enough to certify stability, and the learned Lyapunov function is smooth enough for verification.
- Evidence anchors:
  - [section] "These two optimization problems can be encoded as mixed-integer linear programs (MILPs) and solved to global optimality. We do this using CVXPY [24] and Gurobi [25]."
  - [abstract] "Our algorithm relies on recent developments in learning neural network stability certificates to ensure the learned controller is provably stable for the entire platoon."
- Break condition: MILP scalability limits make verification intractable for large platoons, forcing reliance on un-certified controllers.

### Mechanism 3
- Claim: Hand-designed loss terms guide the learned controller toward safe, comfortable, and stable behavior beyond mere stability.
- Mechanism: The total loss $L_\pi$ combines safety (collision avoidance), comfort (jerk minimization), and stability (error decay) penalties. These terms are weighted and simulated over a horizon to shape behavior.
- Core assumption: The hand-designed penalties accurately reflect the desired performance trade-offs and generalize beyond the training scenarios.
- Evidence anchors:
  - [section] "To ensure safety of the learned controller, we use L_safe to penalize when a vehicle has a distance of less than 0.25 m to its predecessor. To encourage smooth trajectories with L_slew, we penalize large actions..."
  - [section] "To guide the controller toward closed-loop stability, we use L_stab to penalize a running sum of the difference in error from the next timestep to the current timestep over the horizon."
- Break condition: If the penalty design is too restrictive or misses critical scenarios, the learned controller may fail in untested conditions.

## Foundational Learning

- Concept: Mixed-integer linear programming for neural network verification
  - Why needed here: The Lyapunov conditions involve nonlinear neural networks, and MILPs provide a way to verify them globally over bounded input regions.
  - Quick check question: Can you explain why a ReLU neural network can be encoded as a MILP and what the binary variables represent?

- Concept: Lyapunov stability theory for multi-agent systems
  - Why needed here: Proving exponential stability of the entire platoon from a decentralized controller requires a centralized Lyapunov function over the concatenated state.
  - Quick check question: What is the difference between proving stability of a single agent vs. stability of the whole platoon when agents interact?

- Concept: Change of variables in control systems
  - Why needed here: The heterogeneous platoon dynamics are transformed into homogeneous double integrators so that a single controller and Lyapunov function apply to all agents.
  - Quick check question: How does the change of variable $u_i = a_i - v_i / \tau_i$ affect the control authority and the state evolution?

## Architecture Onboarding

- Component map:
  - Vehicle dynamics model (Eq. 1) -> Change of variable layer -> Error state computation (Eq. 2) -> Neural network controller $\pi$ -> Neural network Lyapunov function $V$ -> MILP verifier (CVXPY + Gurobi) -> Loss computation (stability + safety + comfort) -> Simulation environment

- Critical path:
  1. Compute error state from sensor data
  2. Pass through learned controller $\pi$
  3. Apply control to vehicle dynamics
  4. Periodically run MILP verifier to check Lyapunov conditions
  5. If violated, retrain with updated dataset

- Design tradeoffs:
  - Stability vs. scalability: Verification works for small platoons but not for large ones due to MILP complexity
  - Performance vs. safety: Aggressive controllers improve tracking but increase collision risk; safety penalties mitigate this
  - Generality vs. specificity: The homogeneous reformulation simplifies learning but may hide heterogeneity-induced behaviors

- Failure signatures:
  - MILP solver times out or returns infeasibility during verification
  - Controller outputs saturate frequently, indicating poor scaling to larger platoons
  - RMSE increases sharply when platoon size exceeds verified bound
  - Collisions occur in hardware despite passing simulation tests

- First 3 experiments:
  1. Run Algorithm 1 on N=2 platoon with random $\tau_i$ and verify convergence and stability
  2. Scale to N=3 and measure episode count to convergence and MILP solve time
  3. Train a controller for N=5 without verification and test on hardware (F1Tenth) for tracking performance and safety

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the learning algorithm scale to larger platoons (e.g., 50-100 vehicles) while maintaining computational tractability for Lyapunov function verification?
- Basis in paper: [explicit] The paper states that "it is possible other methods... are better suited to finding a Lyapunov function and controller that guarantees stability" for larger platoons, and that verification becomes intractable for N>3 due to the exponential increase in computational complexity.
- Why unresolved: The paper demonstrates learning for small platoons (N=1-3) but cannot verify Lyapunov certificates for larger platoons due to MILP complexity. The authors acknowledge this limitation but don't provide solutions for scaling verification.
- What evidence would resolve it: Successful verification of Lyapunov certificates for platoons of 50-100 vehicles using modified algorithms or computational techniques, demonstrating both stability guarantees and practical computation times.

### Open Question 2
- Question: How would the learned controller perform in heterogeneous platoons with varying vehicle dynamics parameters ($\tau_i$) beyond the tested range [0.2, 0.8]?
- Basis in paper: [explicit] The paper uses a heterogeneous platoon with $\tau_i \in [0.2, 0.8]$ but acknowledges this is a limitation and future work could explore other ranges of vehicle dynamics parameters.
- Why unresolved: The experiments only test a specific range of dynamics parameters, and the authors don't investigate how the controller performs outside this range or with more extreme heterogeneity.
- What evidence would resolve it: Experimental results showing controller performance and stability guarantees across a broader range of vehicle dynamics parameters, including edge cases and more diverse platoon compositions.

### Open Question 3
- Question: Would incorporating historical error information or recurrent neural networks improve the performance and stability of the learned controller compared to the current feedforward approach?
- Basis in paper: [explicit] The authors mention in the discussion that "it is possible that encoding some different notion of state, either by using a history of past errors as input into the neural network or using a recurrent neural network, could lead to better performance."
- Why unresolved: The current algorithm uses only the current error state as input, and the authors speculate about potential improvements from using historical information but don't test this hypothesis.
- What evidence would resolve it: Comparative experimental results showing performance metrics (stability, safety, RMSE) for controllers using feedforward vs. recurrent architectures, with clear demonstrations of improvement when using historical information.

## Limitations
- MILP verification becomes computationally intractable for platoons larger than 3-6 vehicles due to exponential complexity
- The learned controller is only guaranteed to be stable within a compact region of the state space, not globally
- The algorithm assumes perfect state information and doesn't account for communication delays or sensor noise in the verification process

## Confidence

- Mechanism 1 (Change of variable): Medium confidence - The mathematical transformation appears sound for linearization but lacks global nonlinear stability guarantees
- Mechanism 2 (Neural Lyapunov learning): Low confidence for large-scale application - MILP complexity grows exponentially with platoon size, making verification intractable
- Mechanism 3 (Hand-designed losses): Medium confidence - Specific weights and formulations lack empirical justification for generalization

## Next Checks

1. Verify the MILP formulation can certify stability for N=10 heterogeneous platoon with random $\tau_i$ parameters within 1 hour solve time.

2. Test the learned controller on hardware with communication delays of 50-200ms to assess robustness to realistic network conditions.

3. Conduct ablation studies removing individual loss terms (safety, comfort, stability) to quantify their contribution to the final performance.