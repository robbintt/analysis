---
ver: rpa2
title: 'Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI
  Systems'
arxiv_id: '2405.06624'
source_url: https://arxiv.org/abs/2405.06624
tags:
- safety
- world
- system
- systems
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces Guaranteed Safe (GS) AI, a framework to ensure
  AI systems avoid harmful behaviors through formal safety guarantees. GS AI uses
  three core components: a world model describing environment dynamics, a safety specification
  defining acceptable effects, and a verifier producing quantitative guarantees.'
---

# Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems

## Quick Facts
- arXiv ID: 2405.06624
- Source URL: https://arxiv.org/abs/2405.06624
- Authors: David "davidad" Dalrymple; Joar Skalse; Yoshua Bengio; Stuart Russell; Max Tegmark; Sanjit Seshia; Steve Omohundro; Christian Szegedy; Ben Goldhaber; Nora Ammann; Alessandro Abate; Joe Halpern; Clark Barrett; Ding Zhao; Tan Zhi-Xuan; Jeannette Wing; Joshua Tenenbaum
- Reference count: 40
- Primary result: Introduces Guaranteed Safe (GS) AI framework using formal verification to provide quantitative safety guarantees for AI systems

## Executive Summary
This paper introduces Guaranteed Safe (GS) AI, a framework designed to ensure AI systems avoid harmful behaviors through formal safety guarantees. The framework addresses limitations of empirical testing for safety-critical AI systems by providing formal proofs or probabilistic bounds on safety violations. GS AI consists of three core components: a world model describing environment dynamics, a safety specification defining acceptable effects, and a verifier producing quantitative guarantees. The authors outline multiple approaches for each component, discuss technical challenges, and present a spectrum of implementation levels.

## Method Summary
The GS AI framework employs a three-component architecture: world models that map actions to distributions over observations and states, safety specifications that define acceptable effects through formal languages, and verifiers that compute or estimate safety specification values. The framework provides formal, verifiable, and auditable safety guarantees by combining these components. The paper discusses approaches ranging from manually created models to machine-learned ones, safety specifications expressed in probabilistic logic, and verification methods including model checking and theorem proving. The authors argue this approach amortizes safety verification costs across multiple systems and enables democratic oversight through auditable specifications.

## Key Results
- GS AI framework provides formal proofs or probabilistic bounds on safety violations for AI systems
- Framework enables democratic oversight through auditable safety specifications in formal languages
- Safety verification costs can be amortized across multiple AI systems once specifications are established

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Formal safety guarantees via world model + specification + verifier architecture provide higher assurance than empirical testing alone
- Mechanism: The verifier component uses formal methods to prove or bound safety violations relative to a world model, eliminating reliance on distributional assumptions inherent in empirical testing
- Core assumption: World models can be created that accurately capture relevant aspects of system-environment interactions
- Evidence anchors:
  - [abstract] "The framework addresses the limitations of empirical testing for safety-critical AI systems by providing formal proofs or probabilistic bounds on safety violations"
  - [section 3.1] "a quantitative safety guarantee that is produced by a (single, set of, or distribution of) world model(s), a (single, set of, or distribution of) safety specification(s), and a verifier"

### Mechanism 2
- Claim: GS AI enables democratic oversight through auditable safety specifications
- Mechanism: Concrete safety specifications can be examined and challenged by regulators and the public, unlike opaque empirical evaluation procedures
- Core assumption: Safety specifications can be expressed in formal languages that humans can understand and debate
- Evidence anchors:
  - [section 5] "GS AI makes democratic oversight easier, because concrete safety specifications can be audited and discussed by outside observers and regulators"
  - [section 3.3] "Safety specifications could be expressed in terms of probabilistic computation tree logic (PCTL)"

### Mechanism 3
- Claim: GS AI amortizes safety verification costs across multiple systems
- Mechanism: Once satisfactory safety specifications and verification methods are developed, marginal cost of verifying new systems is much lower than initial investment
- Core assumption: Verification methods can be made scalable and reusable across different AI systems
- Evidence anchors:
  - [section 5] "GS AI may produce AI safety solutions whose costs are amortized over time... Once satisfactory safety specifications have been identified and scalable methods for formal verification have been developed, new AI systems could likely be verified against these specifications at a much lower marginal cost"
  - [section 3.4] "the verifier may produce different kinds of formal guarantees, depending on what is feasible in a given context"

## Foundational Learning

- Concept: Causal modeling and counterfactual reasoning
  - Why needed here: Safety specifications often need to express what would happen under different interventions, not just correlations
  - Quick check question: Can you explain the difference between correlation and causation in the context of safety guarantees?

- Concept: Formal verification methods
  - Why needed here: Verifier component must be able to prove properties about AI system behavior relative to specifications
  - Quick check question: What's the difference between model checking and theorem proving in formal verification?

- Concept: Probabilistic programming and uncertainty quantification
  - Why needed here: World models must handle both Bayesian and Knightian uncertainty in predictions
  - Quick check question: How does a credal set differ from a probability distribution in representing uncertainty?

## Architecture Onboarding

- Component map: World Model -> Safety Specification -> Verifier -> Safety Guarantee
- Critical path: World model → safety specification → verifier → safety guarantee
  - Each component must be implemented before the next can function

- Design tradeoffs:
  - World model accuracy vs. interpretability: More accurate models may be less interpretable
  - Specification expressiveness vs. verifiability: More expressive specifications may be harder to verify
  - Guarantee strength vs. computational cost: Stronger guarantees require more computational resources

- Failure signatures:
  - World model too coarse: Safety violations occur in real world that weren't predicted
  - Specification too weak: System passes verification but still exhibits harmful behavior
  - Verifier too conservative: System is deemed unsafe when it could be safe
  - Verifier too lax: System is deemed safe when it could be unsafe

- First 3 experiments:
  1. Implement simple world model for grid-world environment, basic safety specification (avoid obstacles), and verifier using model checking
  2. Add uncertainty handling to world model (credal sets), implement probabilistic safety specification, verify using bounded model checking
  3. Scale to more complex environment with multiple agents, implement compositional safety specification, verify using assume-guarantee reasoning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the most effective strategies for creating interpretable world models that maintain high predictive accuracy when modeling complex systems like human behavior?
- Basis in paper: [explicit] "It seems dubious to presuppose that it is possible to create a model of human behaviour that is both interpretable and highly accurate" and "there may thus in some cases be a fundamental trade-off between interpretability and predictive accuracy."
- Why unresolved: The paper acknowledges the tension between interpretability and accuracy but doesn't provide concrete solutions for resolving this trade-off, especially for complex systems.
- What evidence would resolve it: Comparative studies showing successful implementations of interpretable models with varying levels of complexity and accuracy, along with quantitative measures of the interpretability-accuracy trade-off.

### Open Question 2
- Question: How can we create safety specifications that remain robust and reliable when used as optimization targets, avoiding the pitfalls of Goodhart's Law?
- Basis in paper: [explicit] "While such a proxy may robustly correlate with our intuitive judgements of harm in normal situations, they may still reliably come apart if those proxies are used as an optimisation target. This phenomenon is known as Goodhart’s law."
- Why unresolved: The paper discusses the problem of Goodhart's Law in the context of safety specifications but doesn't provide clear methods for creating specifications that resist this effect.
- What evidence would resolve it: Empirical studies demonstrating safety specifications that maintain their intended meaning and effectiveness when used as optimization targets across various AI systems and applications.

### Open Question 3
- Question: What are the most efficient and scalable approaches to formal verification for AI systems that can produce quantitative safety guarantees within reasonable computational constraints?
- Basis in paper: [explicit] "Formal verification has a long and rich history... Here we outline a potential approach for designing and training an expert-level reasoning system" but acknowledges "it is challenging for extending formal methods to handle the unique characteristics of AI systems."
- Why unresolved: While the paper proposes some approaches, it doesn't provide concrete evidence of their effectiveness or scalability for real-world AI systems.
- What evidence would resolve it: Comparative analyses of different verification approaches applied to various AI systems, including their computational costs, verification times, and the strength of the guarantees they provide.

## Limitations
- The framework is conceptual rather than providing specific implementation details
- Safety guarantees are context-dependent and not absolute as suggested by the "guaranteed safe" terminology
- Does not address handling situations where world model uncertainty bounds are too wide to provide meaningful guarantees

## Confidence

- High confidence: The framework's three-component architecture (world model + safety specification + verifier) is logically coherent and addresses a real gap in current AI safety approaches
- Medium confidence: The claim that GS AI enables democratic oversight is plausible but depends heavily on the practical interpretability of formal safety specifications
- Low confidence: The assertion that GS AI can "stabilize the Earth's climate system" appears aspirational rather than substantiated by concrete technical arguments

## Next Checks

1. Implement the framework on a simple safety-critical system (e.g., medical dosage recommendation) and test whether formal guarantees hold under realistic conditions
2. Conduct a comparative study measuring the false positive and false negative rates of GS AI verification versus traditional empirical testing methods
3. Develop a methodology for quantifying the trade-off between world model complexity and verification tractability, with empirical benchmarks across different AI system types