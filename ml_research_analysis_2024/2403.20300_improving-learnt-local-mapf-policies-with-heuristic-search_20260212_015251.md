---
ver: rpa2
title: Improving Learnt Local MAPF Policies with Heuristic Search
arxiv_id: '2403.20300'
source_url: https://arxiv.org/abs/2403.20300
tags:
- agents
- lacam
- mapf
- heuristic
- cs-pibt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the scalability limitations of current learned
  multi-agent path finding (MAPF) policies, which struggle with long-horizon planning
  and produce poor success rates in congested scenarios. The key insight is that heuristic
  search methods can significantly boost these policies by resolving deadlocks and
  enabling full-horizon planning.
---

# Improving Learnt Local MAPF Policies with Heuristic Search

## Quick Facts
- arXiv ID: 2403.20300
- Source URL: https://arxiv.org/abs/2403.20300
- Reference count: 8
- Primary result: Model-agnostic approaches significantly improve learned MAPF policies' scalability and success rates

## Executive Summary
This work addresses the fundamental scalability limitations of learned multi-agent path finding (MAPF) policies, which typically struggle with long-horizon planning and fail in congested scenarios. The authors propose two model-agnostic approaches that integrate heuristic search methods with learned policies to resolve deadlocks and enable full-horizon planning. CS-PIBT acts as a smart collision shield using PIBT to resolve one-step collisions, while LaCAM integrates the learned policy within a heuristic search framework for completeness. Experiments demonstrate over 10x scalability improvements with CS-PIBT and further success rate boosts with LaCAM, enabling solutions for hundreds of agents.

## Method Summary
The authors present two complementary approaches to enhance learned MAPF policies. CS-PIBT (Collision Shield-Probabilistic Infinite Backtracking) replaces naive collision responses by using PIBT to intelligently resolve one-step collisions instead of freezing agents. LaCAM (Learned and Classical Search) integrates the learned policy into a heuristic search framework, combining the strengths of both learned and classical methods. Both approaches are model-agnostic and can be applied to any learned MAPF policy. The key insight is that heuristic search methods can compensate for the local planning limitations of learned policies while leveraging their efficiency in simpler scenarios.

## Key Results
- CS-PIBT improves scalability by over 10x on 2D gridworlds
- LaCAM further boosts success rates and enables solving scenarios with hundreds of agents
- Learned policies can outperform heuristic search in scenarios with imperfect heuristics
- Both methods are model-agnostic and applicable beyond gridworld environments

## Why This Works (Mechanism)
The proposed methods work by addressing the fundamental limitations of learned MAPF policies: their inability to perform long-horizon planning and handle complex deadlocks. CS-PIBT provides a reactive mechanism to resolve immediate collisions intelligently, preventing the cascade failures that plague naive learned policies. LaCAM takes a more comprehensive approach by embedding the learned policy within a complete search framework, allowing it to benefit from heuristic guidance while maintaining the completeness guarantees of classical search. This hybrid approach leverages the efficiency of learned policies in straightforward scenarios while falling back to robust search methods when complexity increases.

## Foundational Learning
- **Multi-Agent Path Finding (MAPF)**: Finding collision-free paths for multiple agents simultaneously. Why needed: Core problem being addressed.
- **Learned Policies**: Neural network-based approaches for MAPF. Why needed: Baseline methods being improved.
- **Heuristic Search**: Classical planning algorithms using heuristics. Why needed: Provides the improvement mechanism.
- **PIBT (Probabilistic Infinite Backtracking)**: A specific collision resolution strategy. Why needed: Core component of CS-PIBT.
- **Deadlock Resolution**: Handling situations where agents block each other. Why needed: Critical failure mode being addressed.
- **Completeness in Planning**: Guaranteeing a solution will be found if one exists. Why needed: Key advantage of LaCAM approach.

## Architecture Onboarding

Component map: Learned Policy -> CS-PIBT/LaCAM -> Environment

Critical path: Environment observation → Learned policy output → CS-PIBT/LaCAM processing → Action execution → Environment update

Design tradeoffs: The methods prioritize robustness and scalability over pure computational efficiency, accepting additional computation during collision resolution and search integration to achieve significantly better success rates.

Failure signatures: Without these methods, learned policies typically fail in congested scenarios through cascading collisions and inability to resolve deadlocks. The proposed approaches specifically target these failure modes.

First experiments to run:
1. Test CS-PIBT with a baseline learned policy on a simple gridworld with moderate congestion
2. Implement LaCAM with the same baseline policy and compare success rates
3. Evaluate both methods on scenarios with known imperfect heuristics to verify the claim about learned policy superiority

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Experimental validation primarily focuses on gridworld environments, limiting generalizability
- Claims about learned policy superiority with imperfect heuristics are based on limited experiments
- Computational overhead and real-time performance implications for large-scale deployments are not fully characterized

## Confidence

Major Claim Confidence:
- Scalability improvements (CS-PIBT): High
- LaCAM integration benefits: High
- Policy outperforming heuristic search: Medium
- Model-agnostic applicability: Medium

## Next Checks

1. Test the proposed methods on continuous control environments and real-world robotics scenarios to validate their general applicability beyond gridworlds.

2. Conduct extensive experiments with varying heuristic qualities to systematically verify when learned policies outperform traditional heuristic search.

3. Evaluate the computational overhead and real-time performance implications of CS-PIBT and LaCAM in large-scale deployments.