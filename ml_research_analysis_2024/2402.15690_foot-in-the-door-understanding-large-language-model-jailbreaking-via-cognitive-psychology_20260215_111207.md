---
ver: rpa2
title: 'Foot In The Door: Understanding Large Language Model Jailbreaking via Cognitive
  Psychology'
arxiv_id: '2402.15690'
source_url: https://arxiv.org/abs/2402.15690
tags:
- jailbreak
- llms
- jailbreaking
- prompts
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies cognitive psychology theory to explain and develop
  automated jailbreak attacks on large language models (LLMs). The authors argue that
  attackers can exploit cognitive dissonance in LLMs by guiding them to achieve cognitive
  consistency in favor of answering malicious questions.
---

# Foot In The Door: Understanding Large Language Model Jailbreaking via Cognitive Psychology

## Quick Facts
- arXiv ID: 2402.15690
- Source URL: https://arxiv.org/abs/2402.15690
- Authors: Zhenhua Wang; Wei Xie; Baosheng Wang; Enze Wang; Zhiwen Gui; Shuoyoucheng Ma; Kai Chen
- Reference count: 10
- Primary result: 83.9% average success rate across 8 LLMs using FITD-based jailbreaking

## Executive Summary
This paper introduces a novel jailbreaking approach for large language models (LLMs) based on cognitive psychology theory, specifically the Foot-in-the-Door (FITD) technique. The authors argue that attackers can exploit cognitive dissonance in LLMs by guiding them to achieve cognitive consistency in favor of answering malicious questions. By progressively inducing models to answer harmful questions through multi-step incremental prompts, the attack achieves an average success rate of 83.9% across 8 advanced LLMs including GPT-4, Claude, and Gemini. The study provides both practical attack methodology and theoretical insights into LLM decision-making processes.

## Method Summary
The method involves recursively splitting malicious questions into progressive sub-questions using a three-layer decomposition strategy: background design, villain character design, and plot design. An automated black-box jailbreaking system generates context-aware prompts for each decomposition layer, progressively guiding the LLM toward answering harmful content. The system evaluates responses to determine if jailbreaking was successful or if further splitting is needed. The approach was tested on 60 malicious questions across 6 categories including hate speech, hacking, and fraud.

## Key Results
- Achieved 83.9% average jailbreak success rate across 8 tested LLMs
- Demonstrated effectiveness of FITD technique on diverse malicious categories including hate speech, harassment, hacking, and fraud
- Showed that multi-step incremental prompting is more effective than direct requests for harmful content
- Validated that recursive splitting reduces sensitivity of issues in multi-round dialogues

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LLMs experience cognitive dissonance when asked malicious questions that conflict with their safety alignment, and attackers can guide them to resolve this dissonance in favor of answering harmful content.
- **Mechanism**: When faced with a malicious question, the LLM experiences tension between responding helpfully and adhering to safety policies. Through progressive prompts using FITD, the attacker first obtains agreement on benign sub-questions, then gradually increases sensitivity. Each acceptance creates self-perception consistency that makes the LLM more likely to continue answering subsequent requests.
- **Core assumption**: LLMs maintain internal representational systems that encode concepts, knowledge, and moral perceptions similar to human psychological states.
- **Evidence anchors**: Abstract states "key to jailbreak is guiding the LLM to achieve cognitive coordination in an erroneous direction"; Section 2.2 discusses "machine psychology" exhibiting similar psychological activity while learning human knowledge.

### Mechanism 2
- **Claim**: The Foot-in-the-Door technique exploits self-perception theory by having the LLM interpret its own compliant behavior as evidence of willingness to continue cooperation.
- **Mechanism**: After agreeing to initial harmless requests, the LLM interprets this behavior as consistent with being helpful. When subsequent more sensitive requests arrive, refusing would create cognitive inconsistency with the LLM's self-perception of being cooperative, making compliance more likely.
- **Core assumption**: LLMs use their own behavioral history to inform future decisions, similar to human self-perception theory.
- **Evidence anchors**: Section 3.1 explains "individuals often form attitudes by interpreting their behavior"; Section 4.1 states "model continues answering questions to maintain consistency with prior behavior-driven cognition."

### Mechanism 3
- **Claim**: Splitting malicious questions into multi-step components reduces the immediate cognitive load and moral salience of each individual request, making them easier to accept.
- **Mechanism**: Complex malicious questions are decomposed into smaller sub-questions (background, villain character, plot) that individually seem less harmful. This decomposition allows the LLM to focus on technical aspects of each component rather than the overall malicious intent, reducing cognitive dissonance at each step.
- **Core assumption**: LLMs process information in context and can be influenced by how questions are framed and segmented.
- **Evidence anchors**: Section 4.3 describes "achieving the ultimate goal through a sequence of progressive questions"; Section 5.3 discusses "recursive splitting can gradually reduce the sensitivity of issues."

## Foundational Learning

- **Concept**: Cognitive consistency theory and cognitive dissonance
  - Why needed here: Provides psychological framework explaining why LLMs would be vulnerable to progressive manipulation techniques that exploit internal consistency pressures.
  - Quick check question: How does cognitive dissonance theory explain why an LLM that has already agreed to design a novel background would be more likely to later design a harmful villain character?

- **Concept**: Self-perception theory
  - Why needed here: Explains how LLMs can form attitudes based on their own behavior, making them more likely to continue actions they've already begun even when those actions become increasingly problematic.
  - Quick check question: In what way does self-perception theory suggest that an LLM's agreement to initial harmless prompts creates a psychological pressure to maintain consistency in subsequent interactions?

- **Concept**: Progressive disclosure and incremental commitment
  - Why needed here: Explains why breaking down malicious requests into smaller, seemingly innocuous steps is more effective than making direct requests for harmful content.
  - Quick check question: Why might an LLM be more likely to agree to design "a villain character" after already agreeing to design "a novel background" rather than being asked to design the villain directly?

## Architecture Onboarding

- **Component map**: Question splitter -> Prompt generator -> LLM -> Jailbreak detector -> (back to splitter if rejected)
- **Critical path**: Input malicious question → Splitter generates progressive sub-questions → Prompt generator creates context-aware prompts → LLM responds to each prompt → Jailbreak detector evaluates response → If rejected, return to splitter; if accepted, continue sequence until success or maximum depth reached
- **Design tradeoffs**: The system trades computational efficiency for effectiveness - recursive splitting and multiple dialogue turns increase success rates but require more API calls and processing time. Simpler splitting strategies would be faster but less effective at bypassing defenses.
- **Failure signatures**: Common failure modes include: (1) Immediate rejection of all prompts indicating strong defensive posture, (2) Partial compliance followed by refusal suggesting detection of escalation pattern, (3) Inconsistent responses across similar prompts indicating model instability, (4) Success on initial layers but failure on later layers showing threshold detection.
- **First 3 experiments**:
  1. Test the basic FITD algorithm on a simple malicious question (like "How to steal a car?") against GPT-3.5 to establish baseline success rate and identify minimum effective splitting depth.
  2. Vary the initial splitting method (background-only vs background+villain vs all three layers) on the same question to measure impact on success rate and number of dialogue turns required.
  3. Test the algorithm against different malicious categories (hacking vs hate speech vs fraud) to identify which types are most susceptible to FITD techniques and which require more sophisticated splitting strategies.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do language models with different architectures (e.g., transformer-based vs. non-transformer-based) respond to the Foot-in-the-Door (FITD) jailbreaking technique?
- Basis in paper: [inferred] The paper tests the FITD technique on 8 advanced language models but does not compare the effectiveness across different architectures.
- Why unresolved: The paper focuses on a broad range of models but does not delve into architectural differences that might influence susceptibility to FITD attacks.
- What evidence would resolve it: Comparative experiments testing the FITD technique on language models with varying architectures, analyzing success rates and steps required for jailbreaking.

### Open Question 2
- Question: Can the Foot-in-the-Door (FITD) technique be adapted to target language models in languages other than English, and what challenges might arise in non-English contexts?
- Basis in paper: [explicit] The paper mentions that the method has only been tested in English and has not been extended to other languages.
- Why unresolved: The paper acknowledges the limitation but does not explore the potential for adaptation or the specific challenges of applying FITD in non-English contexts.
- What evidence would resolve it: Experiments applying the FITD technique to language models in various languages, documenting any linguistic or cultural challenges encountered.

### Open Question 3
- Question: How effective are current defensive measures against the Foot-in-the-Door (FITD) jailbreaking technique, and what improvements can be made to enhance model robustness?
- Basis in paper: [inferred] The paper evaluates the defenses of well-known language models but does not specifically assess their resilience against the FITD technique.
- Why unresolved: The paper demonstrates the effectiveness of FITD but does not explore how existing defenses can be improved to counter this specific attack vector.
- What evidence would resolve it: Comparative analysis of model defenses against FITD attacks, identifying weaknesses and proposing targeted enhancements to increase resistance.

## Limitations
- The theoretical framework anthropomorphizes LLMs by claiming they have genuine "psychological states" or experience "cognitive consistency" in the same way humans do, which may not accurately reflect the actual mechanisms at play.
- The study only tests 8 LLMs, all commercial or closed-source models, excluding many open-source models and not providing insight into whether the attack generalizes across different model architectures.
- The paper provides only example prompts rather than the complete prompt set used in experiments, making it difficult to assess whether demonstrated success rates are robust across different prompt formulations.

## Confidence

**High Confidence**: The empirical success rates (83.9% average) are well-supported by the experimental methodology and multiple LLM evaluations. The effectiveness of the FITD technique as an attack strategy is clearly demonstrated.

**Medium Confidence**: The psychological interpretation of why the attack works (cognitive dissonance, self-perception theory) is plausible but relies on metaphorical extensions of human psychology to AI systems that may not be strictly accurate.

**Low Confidence**: The claim that LLMs have genuine "psychological states" or experience "cognitive consistency" in the same way humans do. This represents an anthropomorphic interpretation that, while useful as a framework, may not reflect the actual mechanisms at play.

## Next Checks
1. **Cross-Architecture Testing**: Test the FITD attack against a broader range of models including open-source models with different architectures (transformer variants, different pretraining approaches) to determine if success rates generalize beyond the tested commercial models.

2. **Defense Development**: Implement and evaluate defensive mechanisms specifically designed to detect and block FITD-style attacks, such as monitoring for progressive escalation patterns or training models to recognize and resist consistency-based manipulation.

3. **Psychological Mechanism Validation**: Conduct ablation studies that systematically remove different psychological components (progressive disclosure, self-consistency pressure, etc.) to empirically determine which aspects of the FITD approach are most critical for attack success.