---
ver: rpa2
title: Sentiment analysis and random forest to classify LLM versus human source applied
  to Scientific Texts
arxiv_id: '2404.08673'
source_url: https://arxiv.org/abs/2404.08673
tags:
- texts
- methodology
- human
- sentiment
- chatgpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a methodology to classify texts as generated
  by a Large Language Model (LLM) like ChatGPT or by a human, based on sentiment analysis
  and random forest classification. The method involves using four different sentiment
  lexicons (Bing, Afinn, NRC, and Loughran-McDonald) to extract features from texts,
  which are then used to train a random forest model.
---

# Sentiment analysis and random forest to classify LLM versus human source applied to Scientific Texts

## Quick Facts
- arXiv ID: 2404.08673
- Source URL: https://arxiv.org/abs/2404.08673
- Reference count: 24
- Method achieves 84.14% accuracy classifying LLM vs human-generated scientific abstracts using sentiment features and Random Forest

## Executive Summary
This paper proposes a methodology to classify texts as generated by a Large Language Model (LLM) like ChatGPT or by a human, based on sentiment analysis and random forest classification. The method involves using four different sentiment lexicons (Bing, Afinn, NRC, and Loughran-McDonald) to extract features from texts, which are then used to train a random forest model. The model was tested on 68 abstracts from the journal "New Phytologist" and 68 corresponding abstracts generated by ChatGPT v3. The results show that the model can accurately classify the texts with an accuracy of 84.14%, a kappa statistic of 0.6827, and a root mean squared error of 0.3724. The proposed methodology demonstrates a promising approach for detecting text generated by LLMs, which can be useful in academic and other settings where human-generated texts are expected.

## Method Summary
The method extracts sentiment features using four lexicons (Bing, Afinn, NRC, Loughran-McDonald) from scientific abstracts, including polarity, emotion, and sentiment ratios. These features are used to train a Random Forest classifier with 100,000 trees using 10-fold stratified cross-validation. The model was tested on 68 human-written abstracts from "New Phytologist" journal and 68 corresponding abstracts generated by ChatGPT v3.5.

## Key Results
- Achieved 84.14% accuracy in classifying LLM vs human-generated scientific abstracts
- Kappa statistic of 0.6827 indicates substantial agreement beyond chance
- Root mean squared error of 0.3724

## Why This Works (Mechanism)

### Mechanism 1
Sentiment-based feature engineering captures detectable stylistic differences between LLM and human-generated scientific abstracts. The method extracts polarity, emotion, and sentiment ratios from four lexicons, which encode different linguistic patterns. These become input features to a Random Forest classifier. The core assumption is that LLMs and humans exhibit statistically different distributions in sentiment-related word usage when constrained to scientific abstracts. Break condition: If LLM-generated texts mimic human sentiment distributions (e.g., post fine-tuning on human abstracts), sentiment feature separation degrades.

### Mechanism 2
Random Forest ensembles effectively separate classes when features are informative and non-redundant. The model aggregates 100,000 decision trees, each built on random subsets of features and samples, reducing overfitting and capturing complex interactions between sentiment metrics. The core assumption is that the sentiment-derived features contain sufficient discriminative signal to split the classes in feature space. Break condition: If feature space overlap increases (e.g., more diverse human authors or advanced LLMs), classification accuracy drops sharply.

### Mechanism 3
Data preprocessing and feature aggregation convert raw text into stable, comparable metrics. Stop-word removal, stemming, and lexicon matching reduce variability; ratios and averages normalize for length differences between texts. The core assumption is that normalizing text content into fixed-length feature vectors eliminates irrelevant variation (e.g., document length) while preserving stylistic cues. Break condition: If preprocessing discards discriminative stylistic markers (e.g., hedging constructions), detection fails.

## Foundational Learning

- Concept: Lexicon-based sentiment scoring
  - Why needed here: Provides structured numeric features from unstructured text; enables ML to operate on quantified sentiment patterns.
  - Quick check question: What is the difference between the Bing lexicon (binary polarity) and the Afinn lexicon (integer valence)?

- Concept: Random Forest ensemble learning
  - Why needed here: Combines many decision trees to improve generalization and reduce overfitting when training on limited data.
  - Quick check question: How does Random Forest's bootstrap aggregation (bagging) help avoid overfitting compared to a single decision tree?

- Concept: Cross-validation for small datasets
  - Why needed here: Ensures robust model evaluation when the number of samples is small (e.g., 72 per class).
  - Quick check question: Why might stratified 10-fold CV be preferred over simple random splitting for this dataset?

## Architecture Onboarding

- Component map: Text ingestion -> Stop-word removal + stemming -> Lexicon matching (4 lexicons) -> Feature aggregation (ratios, counts, averages) -> Random Forest training (100,000 trees) -> Cross-validation evaluation
- Critical path: Preprocessing -> Lexicon-based feature extraction -> Model training -> Evaluation
- Design tradeoffs: High tree count improves accuracy but increases training time; multiple lexicons add coverage but risk feature redundancy; stemming reduces vocabulary but may lose nuance
- Failure signatures: Low kappa (<0.6) indicates chance-level agreement; high FP/FN rates in confusion matrix suggest class imbalance or weak features; low ROC AUC (<0.8) indicates poor discriminative power
- First 3 experiments:
  1. Train Random Forest with only Bing lexicon features to assess impact of lexicon choice.
  2. Compare Random Forest to logistic regression baseline on same feature set.
  3. Evaluate model on a held-out test set after CV to check for overfitting.

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the proposed sentiment analysis-based method compare to other existing methods for detecting LLM-generated texts when applied to scientific texts? The paper mentions that some related works use different approaches like statistical methods, perplexity metrics, and watermarking, but does not provide a direct comparison with these methods.

### Open Question 2
How does the proposed method perform when applied to texts generated by newer and more advanced LLMs like GPT-4 or other large language models? The authors mention in the conclusions that further research is needed to test the method on GPT-4 and other equivalent tools.

### Open Question 3
How does the proposed method perform when applied to scientific texts from different domains or disciplines? The authors only tested their method on texts from the "New Phytologist" journal, which focuses on plant science. The generalizability of the method to other scientific domains is unknown.

## Limitations
- Limited sample size (n=136 total) restricts generalizability across journals, domains, or LLM versions
- Focus on scientific abstracts may not transfer to other genres or writing styles
- Sentiment lexicons were not originally designed for LLM detection; their suitability for this task remains untested beyond this case

## Confidence

- **High confidence**: Random Forest is a well-established method for ensemble classification; the use of multiple lexicons is reasonable and documented
- **Medium confidence**: Sentiment features may capture detectable differences, but generalizability across domains is uncertain
- **Low confidence**: Exact replication of preprocessing pipeline and feature engineering is blocked by missing technical details

## Next Checks

1. **Feature ablation study**: Remove one lexicon at a time to determine each lexicon's contribution to model performance and detect potential redundancy
2. **Cross-domain validation**: Apply the trained model to abstracts from a different scientific journal or non-scientific text to test domain transfer
3. **Adversarial test**: Generate LLM outputs after fine-tuning on human abstracts to see if sentiment-based detection accuracy drops, indicating vulnerability to adaptive attacks