---
ver: rpa2
title: 'UFO: a Unified and Flexible Framework for Evaluating Factuality of Large Language
  Models'
arxiv_id: '2402.14690'
source_url: https://arxiv.org/abs/2402.14690
tags:
- fact
- evaluation
- text
- sources
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces UFO, a unified and flexible evaluation framework\
  \ for measuring the factuality of large language models (LLMs). It addresses the\
  \ challenge of evaluating factuality across different tasks and datasets by integrating\
  \ multiple fact sources\u2014human-written evidence, reference documents, search\
  \ engine results, and LLM knowledge\u2014into a single pipeline."
---

# UFO: a Unified and Flexible Framework for Evaluating Factuality of Large Language Models

## Quick Facts
- arXiv ID: 2402.14690
- Source URL: https://arxiv.org/abs/2402.14690
- Authors: Zhaoheng Huang; Zhicheng Dou; Yutao Zhu; Ji-rong Wen
- Reference count: 21
- Outperforms baseline metrics in most tasks while reducing computational costs

## Executive Summary
This paper introduces UFO, a unified and flexible evaluation framework for measuring the factuality of large language models (LLMs). The framework addresses the challenge of evaluating factuality across different tasks and datasets by integrating multiple fact sources—human-written evidence, reference documents, search engine results, and LLM knowledge—into a single pipeline. UFO extracts fact units from model-generated text and verifies them against these sources using a modular approach, demonstrating superior discriminative power compared to eight baseline metrics while reducing computational overhead.

## Method Summary
The UFO framework operates through a three-stage pipeline: first, it extracts fact units (question-answer pairs or keywords) from model-generated text using LLM-based modules; second, it verifies each fact unit against available fact sources in a prioritized sequence until a matching answer is found; finally, it determines fact consistency using a binary scoring system. The framework supports five evaluation scenarios with different fact source combinations, allowing flexible adaptation to various text generation tasks including question answering, retrieval-augmented QA, and news fact generation.

## Key Results
- UFO achieves the highest correlation with existing LLM-based evaluation metrics (FacTool and FactScore) in most evaluation scenarios
- For most QA tasks, human-written evidence and reference documents are crucial and can substitute for each other in retrieval-augmented QA tasks
- In news fact generation tasks, search engine results and LLM knowledge are essential for optimal evaluation
- The framework reduces token consumption by about 10% and time cost by about 20% compared to FactScore

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework achieves high discriminative power by systematically extracting fact units and verifying them against multiple plug-and-play fact sources in a prioritized sequence.
- Mechanism: Fact unit extraction breaks model-generated text into question-answer pairs or keywords, then each fact is verified against fact sources in a predefined order until a match is found.
- Core assumption: The order of fact source verification matters, and some fact sources can substitute for each other in certain tasks without significantly degrading evaluation quality.
- Evidence anchors:
  - [abstract]: "We propose UFO, an LLM-based unified and flexible evaluation framework to verify facts against plug-and-play fact sources."
  - [section]: "We first extract fact units from the text, including verifiable question-answer pairs and keywords of model-generated text. Then, for each fact, we verify it against the set of fact sources until a matching answer is found."

### Mechanism 2
- Claim: The evaluation framework demonstrates task-specific fact source importance, where different tasks require different combinations of fact sources for optimal evaluation.
- Mechanism: By implementing five evaluation scenarios with different fact source combinations, the framework reveals which fact sources are crucial for each task type.
- Core assumption: Different text generation tasks have different fact verification needs, and the quality/availability of fact sources varies by task type.
- Evidence anchors:
  - [abstract]: "Experimental results show that for most QA tasks, human-written evidence and reference documents are crucial, and they can substitute for each other in retrieval-augmented QA tasks. In news fact generation tasks, search engine results and LLM knowledge are essential."

### Mechanism 3
- Claim: The framework achieves high correlation with existing LLM-based evaluation metrics while reducing API token consumption and evaluation time.
- Mechanism: The sequential verification approach reduces unnecessary API calls compared to exhaustive verification, while maintaining high correlation with metrics like FacTool and FactScore.
- Evidence anchors:
  - [abstract]: "Compared to FactScore, our proposed evaluation pipeline reduces token consumption by about 10% and time cost by about 20%."

## Foundational Learning

- Concept: Fact unit extraction and verification pipeline
  - Why needed here: The framework relies on breaking down model-generated text into verifiable fact units and systematically checking them against external sources to assess factuality.
  - Quick check question: Can you explain how the Fact Unit Extraction module transforms a paragraph of text into question-answer pairs and keywords?

- Concept: Discriminative power measurement using MR-PT curves
  - Why needed here: The framework evaluates its own effectiveness by measuring how well it can distinguish between different LLMs' factuality scores across varying thresholds.
  - Quick check question: What do the Minority Rate (MR) and Proportion of Ties (PT) metrics measure in the discriminative power evaluation?

- Concept: Plug-and-play modular architecture
  - Why needed here: The framework's flexibility comes from being able to swap different fact sources depending on task requirements.
  - Quick check question: How does the framework handle situations where a fact unit cannot be verified against the first fact source in the sequence?

## Architecture Onboarding

- Component map:
  Fact Unit Extraction (FUE) -> Fact Source Verification -> Fact Consistency Discrimination (FCD)

- Critical path:
  1. Model generates text for given query
  2. FUE extracts fact units (questions, answers, keywords)
  3. For each fact unit, sequentially verify against fact sources
  4. FCD determines if fact is consistent (score 1) or not (score 0)
  5. Average scores across all fact units for final factuality score

- Design tradeoffs:
  - Sequential vs. parallel fact source verification: Sequential reduces cost but may miss earlier matches
  - Fact unit granularity: More granular units increase verification accuracy but also increase computational cost
  - Fact source order: Different orders may be optimal for different task types

- Failure signatures:
  - Low discriminative power: May indicate poor fact unit extraction or inappropriate fact source combinations
  - High computational cost: May indicate need to optimize fact unit extraction or verification sequence
  - Inconsistent results across similar tasks: May indicate need to adjust fact source ordering or selection

- First 3 experiments:
  1. Run UFO on a small sample from each dataset (NQ, HotpotQA, TruthfulQA, CNN/DM, Multi-News, MS MARCO) with baseline fact source combinations to establish baseline performance
  2. Test different fact source verification orders on retrieval-augmented QA task to confirm substitution capability between human-written evidence and reference documents
  3. Compare UFO's MR-PT curves against baseline metrics on news fact generation task to verify search engine results and LLM knowledge sufficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of fact sources impact the evaluation of factual consistency across different tasks?
- Basis in paper: Explicit - The paper categorizes four types of fact sources and evaluates their importance across five text-generation tasks.
- Why unresolved: The paper identifies the importance of different fact sources in various tasks but does not provide a comprehensive analysis of how the choice of fact sources specifically impacts the evaluation of factual consistency.
- What evidence would resolve it: A detailed comparative study analyzing the impact of each fact source on the evaluation results across all tasks, possibly including ablation studies to isolate the effects of individual fact sources.

### Open Question 2
- Question: Can the UFO framework be effectively integrated into the training process of LLMs to improve factual accuracy?
- Basis in paper: Explicit - The paper mentions that the UFO framework has not yet been integrated with the training process of LLMs, which is identified as a limitation.
- Why unresolved: While the UFO framework is effective in evaluating factual consistency, its potential for improving the training process of LLMs is unexplored.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of integrating UFO-based factuality scores into the training process of LLMs, possibly through reinforcement learning methods.

### Open Question 3
- Question: How does the performance of UFO compare with other state-of-the-art factuality evaluation methods in terms of computational efficiency and scalability?
- Basis in paper: Explicit - The paper compares UFO with eight baseline metrics and discusses the computational costs, but a comprehensive comparison of efficiency and scalability is not provided.
- Why unresolved: The paper provides insights into the discriminative power of UFO compared to baseline metrics but does not thoroughly address how it performs in terms of computational efficiency and scalability, especially when dealing with large-scale datasets or models.
- What evidence would resolve it: A detailed analysis comparing the computational efficiency and scalability of UFO with other factuality evaluation methods, including runtime benchmarks and resource usage comparisons across various dataset sizes and model scales.

## Limitations
- Dependency on external fact sources creates potential bottlenecks when human-written evidence or reference documents are unavailable
- Sequential verification approach may miss optimal verification paths if fact sources are ordered suboptimally for specific task types
- Framework's effectiveness relies heavily on the quality of fact unit extraction, which could struggle with complex or ambiguous text

## Confidence
- High confidence: UFO achieves high discriminative power across tasks based on systematic evaluation across five scenarios and five tasks
- Medium confidence: Different fact sources are task-specific, though optimal combinations for tasks beyond those evaluated remain unexplored
- Low confidence: Claims about token and time efficiency due to limited quantitative evidence in the abstract

## Next Checks
1. Apply UFO to additional task types (e.g., code generation, creative writing) to validate whether observed fact source importance patterns hold beyond the five evaluated tasks
2. Systematically evaluate UFO's performance when fact sources are deliberately degraded (e.g., incomplete reference documents, noisy search results) to quantify the framework's robustness limits
3. Conduct detailed profiling of token consumption and inference time across all modules to verify the claimed 10% token and 20% time reductions compared to FactScore under realistic deployment conditions