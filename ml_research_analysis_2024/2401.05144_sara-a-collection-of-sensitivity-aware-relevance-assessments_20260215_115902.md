---
ver: rpa2
title: 'SARA: A Collection of Sensitivity-Aware Relevance Assessments'
arxiv_id: '2401.05144'
source_url: https://arxiv.org/abs/2401.05144
tags:
- collection
- information
- enron
- documents
- sensitivity-aware
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of developing sensitivity-aware
  search systems that can provide relevant search results while filtering out sensitive
  documents, such as personal information or privileged content. The authors present
  SARA, an extension to the existing Enron email collection, which provides 50 information
  needs with 3 query formulations each and 11,471 relevance assessments.
---

# SARA: A Collection of Sensitivity-Aware Relevance Assessments

## Quick Facts
- arXiv ID: 2401.05144
- Source URL: https://arxiv.org/abs/2401.05144
- Reference count: 30
- One-line primary result: Introduces SARA, a sensitivity-aware search collection with 50 information needs, 3 query formulations each, and 11,471 relevance assessments with sensitivity labels.

## Executive Summary
This paper introduces SARA, an extension to the Enron email collection designed to facilitate the development of sensitivity-aware search systems. The collection provides 50 information needs with multiple query formulations and 11,471 relevance assessments, along with sensitivity labels for documents based on Purely Personal and Personal but in Professional Context categories. The authors argue that having multiple smaller test collections focusing on specific types of sensitivity is more valuable than a larger collection with mixed sensitivities, as it better represents real-world use cases for sensitivity-aware search systems. Preliminary experiments using post-filtering sensitivity-aware search approaches demonstrate the utility of the collection, with an oracle classifier achieving a CS-nDCG@10 score of 0.8623.

## Method Summary
The authors use Latent Dirichlet Allocation (LDA) to identify topical themes in the Enron email collection, which serve as a basis for creating 50 information needs. Crowdsourcing is employed to generate query formulations for each information need and gather relevance judgments using a pooling approach. The pooling method selects documents likely to be relevant based on multiple retrieval models, reducing costs while maintaining quality. Sensitivity labels are assigned based on the Purely Personal and Personal but in Professional Context categories. Preliminary experiments evaluate baseline BM25 retrieval and post-filtering approaches using trained sensitivity classifiers to remove sensitive documents from initial rankings.

## Key Results
- SARA provides the first freely available test collection for developing sensitivity-aware search systems, with 50 information needs, 3 query formulations each, and 11,471 relevance assessments.
- The collection includes sensitivity labels based on Purely Personal and Personal but in Professional Context categories, enabling researchers to evaluate sensitivity-aware search approaches.
- An oracle classifier achieves a CS-nDCG@10 score of 0.8623 in preliminary experiments, demonstrating the utility of the collection for evaluating sensitivity-aware search systems.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The collection enables development of sensitivity-aware search systems by providing both relevance assessments and sensitivity labels.
- Mechanism: By combining relevance assessments for information needs with sensitivity labels for documents, researchers can evaluate and train models that filter out sensitive content while maintaining search quality.
- Core assumption: The sensitivity categories (Purely Personal and Personal but in Professional Context) are representative of real-world sensitive information types.
- Evidence anchors:
  - [abstract] "Our proposed collection results in the first freely available test collection for developing sensitivity-aware search systems."
  - [section] "The Hearst [9] labelled version of the Enron Email Collection is a subset of the CMU collection that contains 1702 emails that were annotated as relevant or not relevant to 53 different categories."
- Break condition: If the sensitivity categories do not align with actual sensitive information in real-world collections, the collection's utility for sensitivity-aware search development diminishes.

### Mechanism 2
- Claim: The topic modeling approach ensures that information needs are representative of actual discussions in the email collection.
- Mechanism: Latent Dirichlet Allocation identifies topical themes that serve as a basis for creating information needs, ensuring the collection covers diverse discussion topics.
- Core assumption: The LDA-identified topics accurately reflect the content of the emails in the collection.
- Evidence anchors:
  - [section] "We use a topic modelling approach to identify fifty topics of discussion in the Enron emails and manually create short passages of text to represent each of the identified information needs."
  - [corpus] Weak evidence: No direct validation of topic modeling accuracy against human judgments.
- Break condition: If the identified topics do not capture the actual discussion themes in the emails, the information needs will be misaligned with the collection's content.

### Mechanism 3
- Claim: The pooling approach for gathering relevance judgments is cost-effective while maintaining quality.
- Mechanism: Instead of judging all document-information need pairs, a select number of documents likely to be relevant are chosen through multiple retrieval models, reducing costs while capturing relevant judgments.
- Core assumption: The top documents retrieved by multiple models are representative of the relevant documents for each information need.
- Evidence anchors:
  - [section] "Since each email-information need pair is judged by three crowdworkers and there are three possible labels, Highly Relevant, Partially Relevant, and Not Relevant, it is possible for each of the labels to be selected by one crowdworker."
  - [section] "In such cases, ties are broken by having one of the authors read the document and make an additional judgement."
- Break condition: If the pooling method systematically excludes relevant documents, the collection will have incomplete or biased relevance assessments.

## Foundational Learning

- Concept: Topic Modeling with LDA
  - Why needed here: To identify thematic topics in the email collection that serve as a basis for creating information needs.
  - Quick check question: What is the primary purpose of using LDA in this collection development process?
- Concept: Crowdsourcing Relevance Judgments
  - Why needed here: To gather human assessments of document relevance to information needs at scale.
  - Quick check question: How are ties resolved when three crowdworkers give different relevance labels?
- Concept: Post-filtering Sensitivity-Aware Search
  - Why needed here: To demonstrate how the collection can be used to evaluate sensitivity-aware search approaches.
  - Quick check question: What metric is used to evaluate both relevance and sensitivity handling in the experiments?

## Architecture Onboarding

- Component map: Enron email collection -> LDA topic modeling -> 50 information needs -> 3 query formulations per need -> Pooling for document selection -> Crowdsourced relevance judgments -> Sensitivity labels
- Critical path: 1) Identify topics via LDA, 2) Create information needs from topics, 3) Crowdsource queries for each need, 4) Pool documents using multiple retrieval models, 5) Gather crowdsourced relevance judgments.
- Design tradeoffs: The collection prioritizes specificity of sensitivity types over volume of documents, trading comprehensiveness for focused evaluation of sensitivity-aware search.
- Failure signatures: Incomplete relevance assessments due to pooling, misclassification of sensitivity categories, or poor topic identification leading to misaligned information needs.
- First 3 experiments:
  1. Evaluate baseline BM25 retrieval on the collection without sensitivity filtering.
  2. Test post-filtering approaches using trained sensitivity classifiers.
  3. Compare performance of different sensitivity classifiers (SVM vs Logistic Regression) in the post-filtering framework.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the performance of sensitivity-aware search systems be improved beyond the baseline post-filtering approaches evaluated in this work?
- Basis in paper: [explicit] The authors note that there is room for improvement in terms of the deployed approaches and suggest deploying more sophisticated sensitivity-aware approaches, such as the 洧녝洧녷洧노 .洧냤洧녡-洧녵洧냥洧냤洧냨 approach from Sayed and Oard [25].
- Why unresolved: The paper only presents preliminary experiments using baseline post-filtering approaches. More advanced methods are suggested but not implemented or evaluated.
- What evidence would resolve it: Implementing and evaluating more sophisticated sensitivity-aware search approaches, such as the 洧녝洧녷洧노 .洧냤洧녡-洧녵洧냥洧냤洧냨 approach, and comparing their performance to the baseline methods presented in this work.

### Open Question 2
- Question: How generalizable are the sensitivity classifiers trained on the Enron collection to other types of sensitive information and document collections?
- Basis in paper: [inferred] The authors argue that sensitivity is often very broadly defined and can be subjective, and that researchers can evaluate the generalizability of their models by using the Enron collection alongside the Avocado collection from Sayed et al. [24].
- Why unresolved: The paper does not evaluate the generalizability of the sensitivity classifiers trained on the Enron collection to other types of sensitive information or document collections.
- What evidence would resolve it: Evaluating the performance of the sensitivity classifiers trained on the Enron collection on other document collections containing different types of sensitive information, such as the Avocado collection or other real-world sensitive document collections.

### Open Question 3
- Question: How does the size and composition of the test collection impact the evaluation of sensitivity-aware search systems?
- Basis in paper: [explicit] The authors discuss the size of the SARA extension to the Enron collection and argue that having multiple smaller test collections focusing on specific types of sensitivity is more valuable than a larger collection with mixed sensitivities.
- Why unresolved: The paper does not provide empirical evidence on how the size and composition of the test collection impact the evaluation of sensitivity-aware search systems.
- What evidence would resolve it: Conducting experiments to evaluate the performance of sensitivity-aware search systems on test collections of varying sizes and compositions, and analyzing the impact of these factors on the evaluation results.

## Limitations

- The collection relies on two specific sensitivity categories (Purely Personal and Personal but in Professional Context) that may not capture all types of sensitive information encountered in real-world scenarios.
- The pooling approach for relevance judgments, while cost-effective, may introduce coverage gaps for relevant documents not retrieved by the selected models.
- The effectiveness of the post-filtering approach demonstrated in preliminary experiments relies heavily on the quality of the sensitivity classifiers, which were trained on a relatively small subset of the data.

## Confidence

Medium confidence in the SARA collection's utility. While it addresses a critical gap in sensitivity-aware search evaluation, limitations include reliance on specific sensitivity categories, potential bias in topic modeling parameters, and the effectiveness of the post-filtering approach depending on classifier quality.

## Next Checks

1. Evaluate the sensitivity classifiers on an external dataset to assess generalizability beyond the Enron corpus.
2. Conduct a systematic analysis of the pooling method's coverage by comparing retrieved documents against a complete set of relevance judgments for a subset of information needs.
3. Perform ablation studies to determine the impact of different topic modeling parameters on the quality and diversity of information needs.