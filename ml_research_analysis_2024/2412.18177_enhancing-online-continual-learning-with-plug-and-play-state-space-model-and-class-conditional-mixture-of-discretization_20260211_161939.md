---
ver: rpa2
title: Enhancing Online Continual Learning with Plug-and-Play State Space Model and
  Class-Conditional Mixture of Discretization
arxiv_id: '2412.18177'
source_url: https://arxiv.org/abs/2412.18177
tags:
- learning
- discretization
- s6mod
- data
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses online continual learning (OCL), where models
  must learn from streaming data without revisiting previous samples. Existing methods
  focus on memory retention but often fail to learn generalizable and discriminative
  features incrementally.
---

# Enhancing Online Continual Learning with Plug-and-Play State Space Model and Class-Conditional Mixture of Discretization

## Quick Facts
- **arXiv ID**: 2412.18177
- **Source URL**: https://arxiv.org/abs/2412.18177
- **Reference count**: 40
- **Primary result**: S6MOD achieves state-of-the-art performance across multiple OCL benchmarks, significantly improving accuracy and reducing forgetting when integrated with existing methods.

## Executive Summary
This paper addresses the challenge of online continual learning (OCL) where models must learn from streaming data without revisiting previous samples while avoiding catastrophic forgetting. The authors propose S6MOD, a plug-and-play module that integrates selective state space models (SSMs) with a class-conditional mixture of discretization. The method dynamically adjusts discretization patterns based on class uncertainty and uses a contrastive discretization loss to learn better features. Experiments show S6MOD significantly improves accuracy across multiple datasets (CIFAR-10, CIFAR-100, Tiny-ImageNet) when integrated with various OCL methods, achieving state-of-the-art results.

## Method Summary
S6MOD is a plug-and-play module designed to enhance existing online continual learning methods by improving feature discriminativeness and reducing catastrophic forgetting. The module consists of a mixture of discretization candidates in selective state space models, where multiple discretization patterns are generated and selected through a class-conditional routing mechanism based on uncertainty. The method uses contrastive discretization loss to enforce within-class consistency and between-class diversity in the learned features. S6MOD can be integrated with any existing OCL method (like ER, OCM, OnPro) by adding an additional branch after the backbone, combining the base method loss with S6MOD-specific losses including contrastive discretization and ETF classifier supervision.

## Key Results
- S6MOD achieves state-of-the-art performance across CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets when integrated with various OCL methods
- The method significantly reduces forgetting compared to baseline methods while maintaining or improving new-task learning accuracy
- S6MOD enhances feature discriminativeness as demonstrated through t-SNE visualizations showing better class separation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The plug-and-play branch enhances feature discriminativeness and generalizability in online continual learning by dynamically adjusting discretization patterns through class-conditional routing.
- Mechanism: S6MOD introduces an additional branch after the backbone that processes features through a mixture of discretization in the selective state space model (SSM). The class-conditional routing dynamically adjusts the number of discretization patterns (Nk) based on the uncertainty of each class, allowing the model to allocate more capacity for uncertain classes while stabilizing learned classes.
- Core assumption: The class uncertainty calculated through margin-based distance between class prototypes effectively represents the need for additional discretization patterns to improve learning.
- Evidence anchors:
  - [abstract] "class-conditional routing algorithm for dynamic, uncertainty-based adjustment"
  - [section 4.3] "we estimate the class uncertainty σk based on the average margin of class k with different classes"
  - [corpus] Weak - no direct corpus evidence for this specific mechanism

### Mechanism 2
- Claim: The contrastive discretization loss improves feature discriminativeness by enforcing within-class consistency and between-class diversity.
- Mechanism: The contrastive discretization loss function LCont encourages the aggregated discretization patterns (∆) to have similar patterns within the same class while being dissimilar across different classes. This is achieved by penalizing cosine similarity between patterns of the same class and rewarding it between patterns of different classes.
- Core assumption: The aggregated discretization patterns effectively represent the feature space such that encouraging similarity within classes and dissimilarity between classes improves discriminativeness.
- Evidence anchors:
  - [section 4.3] "we also incorporate contrastive discretization loss function LCont... encourages ∆ to have within-class consistency and between-class diversity"
  - [abstract] "implement a contrastive discretization loss to optimize it"
  - [corpus] Weak - no direct corpus evidence for this specific loss formulation

### Mechanism 3
- Claim: The mixture of discretization in SSMs addresses the challenge of capturing precise discretization patterns with limited context from online data.
- Mechanism: By using multiple discretization candidates (∆i) produced by different projection layers and selecting the top-Nk patterns through a sparse gating mechanism, the model can adaptively choose the most sensitive discretization method for current dynamics. This enriches the selective scan patterns compared to using a single discretization mechanism.
- Core assumption: Multiple discretization patterns can capture different aspects of the system dynamics, and the gating mechanism can effectively select the most appropriate patterns for each input.
- Evidence anchors:
  - [section 4.2] "we develop a sparse MoE system for the discretization transformation ∆ in S6 models"
  - [introduction] "a single discretization mechanism may fail to capture all the essential details in some nonlinear dynamic systems"
  - [corpus] Weak - no direct corpus evidence for this specific mixture-of-discretization approach in SSMs

## Foundational Learning

- Concept: Continual Learning and Catastrophic Forgetting
  - Why needed here: The paper addresses online continual learning where the model must learn new tasks without forgetting previous ones, which is the fundamental challenge being solved.
  - Quick check question: What is the difference between offline and online continual learning, and why does online learning pose greater challenges?

- Concept: State Space Models and Selective Scan Mechanism
  - Why needed here: S6MOD is built upon selective state space models (S6), which provide the foundation for the mixture of discretization approach.
  - Quick check question: How do selective state space models differ from traditional recurrent neural networks, and what advantages do they offer for sequence modeling?

- Concept: Mixture of Experts and Sparse Gating
  - Why needed here: The mixture of discretization uses principles from mixture of experts, where multiple "experts" (discretization patterns) are combined through a gating mechanism.
  - Quick check question: How does a mixture of experts architecture work, and what are the benefits of using sparse gating in such systems?

## Architecture Onboarding

- Component map:
  - Input features -> Backbone (ResNet-18) -> Base OCL method and S6MOD branch
  - S6MOD branch: MLP projections -> Multiple discretization candidates (∆i) -> Sparse gating -> Class-conditional routing -> Contrastive discretization loss -> ETF classifier

- Critical path:
  1. Input features pass through backbone to get F
  2. F is duplicated to both base method and S6MOD branch
  3. In S6MOD: F is projected to X and Z paths
  4. X path goes through Conv and SiLU activation
  5. Multiple discretization candidates ∆i are generated
  6. Class-conditional routing selects top-Nk patterns
  7. Selected patterns are aggregated and passed through S6
  8. Output is combined with Z path through SiLU and element-wise multiplication
  9. ETF classifier produces Q for LDiff
  10. Contrastive loss is computed between all pairs of aggregated patterns

- Design tradeoffs:
  - More discretization patterns (larger N) improves learning of new tasks but increases forgetting
  - Smaller N stabilizes old knowledge but limits capacity for new learning
  - Class-conditional routing balances this tradeoff dynamically
  - Additional computational cost of S6MOD branch vs. performance gains
  - Complexity of mixture of discretization vs. single discretization approach

- Failure signatures:
  - Performance degradation when class uncertainty estimation is inaccurate
  - Increased forgetting when Nk is set too high for stable classes
  - Underutilization of capacity when Nk is set too low for uncertain classes
  - Computational overhead without sufficient performance gains
  - Instability in training due to improper weighting of loss components

- First 3 experiments:
  1. Ablation study: Test S6MOD with only the extra branch and LDiff vs. full S6MOD with class-conditional routing and LCont
  2. Sensitivity analysis: Vary the number of discretization patterns N and observe impact on new-task accuracy and forgetting
  3. Class-conditional routing validation: Compare fixed Nk vs. dynamic Nk calculated through class-conditional routing on CIFAR-100

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas remain unexplored including the scalability of class-conditional routing with very large numbers of classes, the optimal determination of discretization pattern count, and the method's applicability to non-classification continual learning tasks.

## Limitations
- The method's computational overhead could be prohibitive for resource-constrained applications
- Performance on real-world streaming scenarios with noisy, imbalanced data remains unverified
- Theoretical justification for why mixture of discretization in SSMs specifically addresses online continual learning challenges is lacking

## Confidence
- **High confidence**: The core empirical results showing S6MOD's effectiveness across multiple datasets and base methods
- **Medium confidence**: The architectural design choices and their claimed benefits (feature discriminativeness, reduced forgetting)
- **Low confidence**: The theoretical justification for why mixture of discretization in SSMs specifically addresses online continual learning challenges

## Next Checks
1. **Robustness to noisy streaming data**: Evaluate S6MOD's performance when input streams contain label noise or class imbalance to assess real-world applicability.

2. **Ablation of class-conditional routing**: Replace the dynamic Nk calculation with fixed discretization patterns to quantify the specific contribution of the class-conditional mechanism.

3. **Generalization to non-image domains**: Apply S6MOD to sequential data tasks (e.g., text or time series) to verify the broader applicability of the mixture-of-discretization approach beyond computer vision.