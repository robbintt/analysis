---
ver: rpa2
title: FairBelief -- Assessing Harmful Beliefs in Language Models
arxiv_id: '2402.17389'
source_url: https://arxiv.org/abs/2402.17389
tags:
- language
- beliefs
- pages
- computational
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FairBelief introduces a prompting-based analytical framework for
  auditing harmful beliefs embedded in language models. By leveraging templates and
  a lexicon of offensive terms, it quantifies bias through a model-agnostic Hurtfulness
  score.
---

# FairBelief -- Assessing Harmful Beliefs in Language Models

## Quick Facts
- arXiv ID: 2402.17389
- Source URL: https://arxiv.org/abs/2402.17389
- Authors: Mattia Setzu; Marta Marchiori Manerba; Pasquale Minervini; Debora Nozza
- Reference count: 17
- Primary result: Introduces FairBelief framework showing modern large models propagate harmful stereotypes despite high general performance

## Executive Summary
FairBelief presents a prompting-based analytical framework for auditing harmful beliefs embedded in language models. The approach combines template-based prompting with a lexicon of offensive terms to quantify bias through a model-agnostic Hurtfulness score. Experiments across diverse models (BART, BERT, BLOOM, GPT2, LLAMA, LLAMA2, VICUNA) reveal that while some architectures achieve high general performance, they still propagate harmful gender stereotypes—particularly against females and non-binary identities—and exhibit higher hurtfulness in top predictions. Modern large-scale models consistently show higher bias scores than classical ones, though scaling alone does not guarantee reduced harm.

## Method Summary
The FairBelief framework uses the HONEST dataset with binary and queer subsets containing templates that combine identity terms and predicates. Models generate top-100 completions per template, which are matched against the HurtLex lexicon to compute HONEST scores quantifying hurtful completions. The framework analyzes results across model family/scale, likelihood rankings (k=1 to 100), and identity groups (male/female, young/old). Semantic similarity measures model agreement, while qualitative analysis examines stereotypical associations in generated completions.

## Key Results
- Modern large-scale models show higher bias scores than classical models, though scaling alone doesn't reduce harm
- Harmful beliefs appear more frequently in higher-ranked predictions, with females and non-binary identities most affected
- Qualitative analysis confirms stereotypical completions in both binary and queer identity contexts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Template-based prompting combined with a hurtful lexicon enables model-agnostic bias measurement
- Mechanism: Prompts inject identity terms and predicates into controlled sentence structures; models generate completions matched against offensive terms to quantify bias
- Core assumption: Offensive terms in lexicon reliably capture all harmful beliefs expressed in completions
- Evidence anchors:
  - [abstract] "leveraging prompting to study the behavior of several state-of-the-art LMs... assessing predictions on a fairness dataset specifically designed to quantify LMs' outputs' hurtfulness"
  - [section] "Specifically, we propose FAIR BELIEF, a language-agnostic analytical approach to capture and assess beliefs embedded in LMs. FAIR BELIEF leverages prompting..."
  - [corpus] Found 25 related papers; average neighbor FMR=0.503 suggests moderate relevance, but no direct lexicon-validation studies in neighbors
- Break condition: Lexicon misses context-dependent or emerging slurs; prompts don't cover all bias manifestations

### Mechanism 2
- Claim: Likelihood-based analysis reveals hurtful beliefs appear more often in higher-ranked predictions
- Mechanism: For each prompt, completions are sorted by probability; bias scores computed for increasing values of K to detect where harmful completions concentrate
- Core assumption: Models assign higher probability to completions that reflect their learned beliefs
- Evidence anchors:
  - [abstract] "Modern large-scale models consistently show higher bias scores than classical ones, though scaling alone does not guarantee reduced harm"
  - [section] "Specifically, we compute the HONEST score of each top − k model prediction and look for significant oscillations across different ks"
  - [corpus] Weak evidence; corpus neighbors don't address likelihood-specific bias patterns
- Break condition: Models prioritize fluency over belief accuracy; likelihoods don't align with societal harm

### Mechanism 3
- Claim: Group-based evaluation (by gender/age identity) exposes disparate harm patterns across subgroups
- Mechanism: Templates partitioned by identity labels; bias metrics aggregated per group to compare treatment across subgroups
- Core assumption: Identity labels in templates are correctly interpreted by model and produce representative subgroup completions
- Evidence anchors:
  - [abstract] "although these architectures enable high performances on diverse NLP tasks, they show hurtful beliefs about specific genders, e.g., against females and non-binary persons"
  - [section] "We repeat the likelihood patterns analysis on predefined groups... specifically, we split the templates according to the identity of interest w.r.t. gender and age..."
  - [corpus] No direct evidence in neighbors for subgroup-specific bias analysis
- Break condition: Models misinterpret identity terms or conflate identities, leading to misleading subgroup comparisons

## Foundational Learning

- Concept: Prompt engineering
  - Why needed here: Prompts form controlled inputs that elicit beliefs; poor prompt design yields uninterpretable completions
  - Quick check question: Can you construct a template that isolates a single identity and predicate without introducing confounders?

- Concept: Lexical bias scoring
  - Why needed here: Bias quantification relies on lexicon matching; misunderstanding lexicon construction leads to over- or under-estimation of harm
  - Quick check question: How would you validate that a given lexicon term is offensive in the target context?

- Concept: Likelihood ranking
  - Why needed here: Sorting completions by probability is essential for detecting when harmful beliefs appear in top predictions
  - Quick check question: What does a flat likelihood distribution imply about the model's belief expression?

## Architecture Onboarding

- Component map: Prompt template builder → LM inference engine → Completion sorter → Lexicon matcher → Score aggregator → Group splitter → Similarity analyzer
- Critical path: Prompt template → LM generation → Lexicon matching → Score computation → Group aggregation
- Design tradeoffs:
  - Lexicon breadth vs. precision: broader lexicons catch more harms but increase false positives
  - Template complexity vs. interpretability: richer templates may elicit subtler biases but are harder to analyze
  - Likelihood cutoff choice: lower K captures high-confidence beliefs but may miss rarer harms
- Failure signatures:
  - Low lexicon match but qualitative harm: lexicon is too narrow or context-sensitive
  - High scores across all groups: prompts may be overly general or lexicons too permissive
  - Inconsistent group scores: identity parsing in templates may be flawed
- First 3 experiments:
  1. Run a single prompt across multiple models and verify lexicon matches manually to confirm scoring logic
  2. Vary K from 1 to 20 on a small dataset and plot score decay to confirm likelihood sensitivity
  3. Split templates by a single identity (e.g., "female") and compare scores across models to validate group analysis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different model families and scales correlate with emergence of harmful beliefs across different likelihood rankings (e.g., top-1 vs. top-10 completions)?
- Basis in paper: [explicit] The paper explicitly analyzes intra- and inter-family evaluations across different model scales and likelihoods, reporting varying HONEST scores for different families (e.g., VICUNA, GPT2, BLOOM) and showing that larger models do not always reduce hurtfulness
- Why unresolved: While the paper identifies trends, it does not establish a clear causal relationship between model family/scale and likelihood of generating harmful completions, nor does it determine whether these effects are consistent across different likelihood rankings
- What evidence would resolve it: Systematic experiments isolating model family, scale, and likelihood effects, with statistical analysis of their individual and combined contributions to harmful belief generation

### Open Question 2
- Question: To what extent do harmful beliefs generated by language models generalize across different identity groups (e.g., gender, age, queer identities), and are there intersectional effects?
- Basis in paper: [explicit] The paper conducts group-based analyses on gender and age identities and notes that models exhibit higher hurtfulness toward female and older identities, but does not explore intersectional effects or extend beyond these categories
- Why unresolved: The study focuses on broad identity categories without examining how overlapping or intersectional identities (e.g., queer older females) might experience compounded bias, nor does it explore how these patterns generalize to other social dimensions
- What evidence would resolve it: Expanded group analyses incorporating intersectional identities and diverse social categories, with comparative studies across datasets and cultural contexts

### Open Question 3
- Question: Can prompt engineering or fine-tuning strategies effectively mitigate generation of harmful beliefs without significantly degrading model performance on general NLP tasks?
- Basis in paper: [inferred] The paper discusses challenges of aligning language models with fairness and mentions future work exploring soft prompts and retrieval-augmented approaches, but does not experimentally evaluate mitigation strategies
- Why unresolved: While the paper identifies presence of harmful beliefs, it does not test or validate interventions that could reduce these biases, leaving open question of practical mitigation
- What evidence would resolve it: Controlled experiments applying prompt engineering, fine-tuning, or alignment techniques, measuring both reduction in harmful completions and retention of general task performance

## Limitations

- Lexicon-based scoring may miss context-dependent or emerging slurs not captured in HurtLex
- Template approach assumes consistent model interpretation of identity terms, but real-world identities are more fluid and intersectional
- Likelihood ranking assumes higher probability completions reflect stronger beliefs, but models may prioritize fluency over belief expression

## Confidence

- Prompting framework validity: Medium - established methodology but lexicon coverage is uncertain
- Likelihood bias detection: Low-Medium - reasonable assumption but not empirically validated in this work
- Group-based disparity findings: Medium - results align with known literature but template design limitations exist

## Next Checks

1. Conduct lexicon validation study where human raters assess whether generated completions containing lexicon matches are actually harmful in context
2. Test template robustness by creating intersectional identity prompts (e.g., combining gender and age) and comparing bias scores to single-dimension templates
3. Implement ablation study varying lexicon size and composition to quantify impact of lexicon completeness on final bias scores