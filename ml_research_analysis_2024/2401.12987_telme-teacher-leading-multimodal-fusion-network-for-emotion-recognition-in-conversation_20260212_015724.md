---
ver: rpa2
title: 'TelME: Teacher-leading Multimodal Fusion Network for Emotion Recognition in
  Conversation'
arxiv_id: '2401.12987'
source_url: https://arxiv.org/abs/2401.12987
tags:
- emotion
- teacher
- telme
- multimodal
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses Emotion Recognition in Conversation (ERC),
  where the goal is to identify emotions expressed by participants at each turn in
  a conversation. The proposed method, Teacher-leading Multimodal fusion network for
  ERC (TelME), incorporates cross-modal knowledge distillation to transfer information
  from a text-based language model acting as the teacher to audio and visual modality
  "student" networks.
---

# TelME: Teacher-leading Multimodal Fusion Network for Emotion Recognition in Conversation

## Quick Facts
- arXiv ID: 2401.12987
- Source URL: https://arxiv.org/abs/2401.12987
- Reference count: 20
- Primary result: Teacher-leading Multimodal fusion network (TelME) achieves state-of-the-art performance on MELD (67.37% weighted F1) and IEMOCAP (70.48% weighted F1) for emotion recognition in conversation

## Executive Summary
This paper presents TelME, a teacher-leading multimodal fusion network for emotion recognition in conversation (ERC). The method leverages cross-modal knowledge distillation, where a text-based language model serves as the teacher to guide audio and visual modality student networks. This approach transfers knowledge from the stronger text modality to the weaker non-verbal modalities. The framework then employs an attention-based shifting fusion mechanism where student networks support the teacher. TelME demonstrates state-of-the-art performance on both MELD and IEMOCAP datasets, particularly excelling in multi-party conversational scenarios.

## Method Summary
TelME employs a teacher-student knowledge distillation framework where a text-based language model acts as the teacher and audio/visual modalities serve as students. The teacher network transfers knowledge to student networks through cross-modal distillation, enhancing the efficacy of non-verbal modalities. After distillation, the framework combines multimodal features using an attention-based shifting fusion approach where student networks support the teacher. This hierarchical fusion structure allows each modality to contribute appropriately based on its relative strength in capturing emotional cues from conversational data.

## Key Results
- TelME achieves 67.37% weighted F1 score on MELD dataset, surpassing previous best of 66.71%
- TelME achieves 70.48% weighted F1 score on IEMOCAP dataset
- Particularly effective in multi-party conversational scenarios compared to dyadic interactions

## Why This Works (Mechanism)
The teacher-student knowledge distillation framework allows information to flow from the strong text modality to weaker audio and visual modalities. This cross-modal knowledge transfer enhances the non-verbal modalities' ability to capture emotional cues. The attention-based shifting fusion then appropriately weights contributions from each modality, with students supporting the teacher in the final prediction. This hierarchical approach addresses the inherent imbalance in modality strengths while preserving the complementary information across modalities.

## Foundational Learning
- **Emotion Recognition in Conversation (ERC)**: Understanding emotional states expressed by participants at each turn in a conversation. Needed to establish the problem context and evaluation metrics. Quick check: Verify that turn-level emotion classification is the target task, not utterance-level or speaker-level.
- **Knowledge Distillation**: A technique where a teacher model transfers knowledge to a student model. Needed to leverage the stronger text modality to enhance weaker audio/visual modalities. Quick check: Confirm that distillation loss is computed between teacher and student outputs before fusion.
- **Multimodal Fusion**: Combining information from multiple modalities (text, audio, visual). Needed to capture complementary emotional cues across modalities. Quick check: Ensure fusion strategy preserves modality-specific characteristics while enabling interaction.
- **Attention Mechanisms**: Weighting mechanisms that dynamically emphasize important features. Needed to implement the shifting fusion where students support the teacher. Quick check: Verify attention weights are computed based on modality reliability and context.
- **Multi-party Conversation Analysis**: Handling conversations with multiple participants rather than just two speakers. Needed to address the complexity of group interactions. Quick check: Confirm the model handles speaker turns and interactions appropriately in multi-party settings.

## Architecture Onboarding

**Component Map:** Text modality (teacher) -> Cross-modal distillation -> Audio/visual modalities (students) -> Attention-based shifting fusion -> Emotion prediction

**Critical Path:** Text teacher -> Distillation loss computation -> Student network training -> Multimodal feature extraction -> Attention-based fusion -> Classification

**Design Tradeoffs:** 
- Chose knowledge distillation over early fusion to preserve modality-specific strengths
- Implemented shifting fusion rather than simple concatenation to handle modality imbalance
- Selected weighted F1 over accuracy to account for class imbalance in emotion categories

**Failure Signatures:** 
- Poor performance on rare emotion categories due to class imbalance
- Suboptimal results when text modality is weak or corrupted
- Potential overfitting to specific conversation structures in training data

**First Experiments:** 
1. Ablation study removing knowledge distillation to quantify its contribution
2. Testing with different fusion strategies (concatenation, product, weighted sum) to validate attention-based approach
3. Evaluating on single-modality baselines to measure multimodal benefit

## Open Questions the Paper Calls Out
None

## Limitations
- Performance improvements are statistically significant but relatively modest (0.66% on MELD, 0.77% on IEMOCAP)
- Evaluation focuses primarily on weighted F1 scores, potentially masking performance on rare emotion categories
- Framework assumes text modality is consistently stronger, which may not generalize across all conversation types
- Attention-based fusion adds complexity that could affect generalization to datasets with different characteristics

## Confidence

**Major Claims Confidence:**
- **High confidence**: The core methodology (teacher-student distillation, attention-based fusion) is technically sound and well-described.
- **Medium confidence**: The state-of-the-art claims are supported by the reported metrics but would benefit from additional validation through statistical significance testing across multiple runs.
- **Medium confidence**: The superiority in multi-party scenarios is demonstrated but could be strengthened with more detailed analysis of modality contributions.

## Next Checks
1. Conduct statistical significance testing across multiple training runs to verify the robustness of performance improvements over baseline methods.
2. Perform ablation studies to quantify the individual contributions of knowledge distillation versus attention-based fusion to overall performance.
3. Test the model on additional conversational datasets with different characteristics (e.g., dyadic vs. multi-party, formal vs. informal) to assess generalization capabilities.