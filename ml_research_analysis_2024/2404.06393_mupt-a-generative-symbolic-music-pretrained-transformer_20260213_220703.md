---
ver: rpa2
title: 'MuPT: A Generative Symbolic Music Pretrained Transformer'
arxiv_id: '2404.06393'
source_url: https://arxiv.org/abs/2404.06393
tags:
- music
- loss
- training
- symbolic
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MuPT, a series of generative symbolic music
  models pre-trained on ABC notation. To improve multi-track music coherence, it introduces
  SMT-ABC Notation, which synchronizes bars across tracks.
---

# MuPT: A Generative Symbolic Music Pretrained Transformer

## Quick Facts
- **arXiv ID:** 2404.06393
- **Source URL:** https://arxiv.org/abs/2404.06393
- **Reference count:** 40
- **Primary result:** MuPT achieves improved multi-track music coherence using synchronized ABC notation and outperforms MIDI-based models in repetition and intra-similarity metrics.

## Executive Summary
This paper introduces MuPT, a series of generative symbolic music models pre-trained on ABC notation. The key innovation is Synchronized Multi-Track ABC Notation (SMT-ABC), which preserves bar alignment across tracks to improve structural coherence in generated music. The work also proposes SMS Law, an extension of Chinchilla scaling law that accounts for overfitting dynamics in symbolic music datasets. Models are trained with transformer decoders using BPE tokenization and demonstrate strong performance in both quantitative metrics and human listening tests, outperforming baselines like GPT-4.

## Method Summary
MuPT uses transformer decoder-only architecture with RMSNorm, SwiGLU activation, and rotary positional encoding. The method involves preprocessing ABC notation into synchronized multi-track format by grouping bars across tracks with `<|>` delimiters, tokenizing using BPE with 50k vocabulary, and training on the resulting corpus. The SMS Law framework guides training by accounting for data repetition benefits and overfitting risks specific to symbolic music. Models are trained with Adam optimizer and cosine learning rate schedule.

## Key Results
- MuPT models outperform MIDI-based approaches in repetition rate and intra-texture similarity metrics
- Human listening tests show MuPT-generated music is preferred over GPT-4 outputs
- SMS Law successfully predicts overfitting behavior in symbolic music training with data repetition benefits

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Synchronized Multi-Track ABC Notation (SMT-ABC) improves bar alignment across tracks, leading to better structural coherence in generated music.
- **Mechanism:** Original ABC notation concatenates music track-by-track, which can misalign bars between instruments. SMT-ABC groups bars with the same index across tracks into `<|>` units, ensuring measures remain synchronized.
- **Core assumption:** Structural integrity in multi-track music depends on temporal alignment of bars; misalignment degrades musicality.
- **Evidence anchors:**
  - [abstract]: "To address the challenges associated with misaligned measures from different tracks during generation, we propose the development of a Synchronized Multi-Track ABC Notation (SMT-ABC Notation), which aims to preserve coherence across multiple musical tracks."
  - [section]: "Since a track represents a section or division within a musical composition, such as one of the instrumental or vocal parts in a piece of music, it is crucial for models to capture the correspondence between tracks."
  - [corpus]: **Missing** - No direct corpus evidence cited.
- **Break condition:** If model learns temporal correspondence without explicit bar grouping, or if music style doesn't rely on strict bar alignment (e.g., free-form improvisation).

### Mechanism 2
- **Claim:** ABC notation is more compatible with LLM architectures than MIDI, leading to better performance.
- **Mechanism:** ABC is a text-based, human-readable format; LLMs are designed for natural language processing. This compatibility enables better sequence modeling, tokenization, and interpretability.
- **Core assumption:** LLMs' strengths in text processing directly transfer to text-based symbolic music representations.
- **Evidence anchors:**
  - [abstract]: "our findings suggest that LLMs are inherently more compatible with ABC Notation, which aligns more closely with their design and strengths, thereby enhancing the model's performance in musical composition."
  - [section]: "ABC notation stands out for its textual simplicity and compactness, making it particularly suited for Natural Language Processing (NLP) techniques."
  - [corpus]: **Weak** - No direct corpus evidence provided.
- **Break condition:** If MIDI-based models achieve comparable or superior performance with advanced tokenizers or if ABC's textual advantages don't translate to measurable gains.

### Mechanism 3
- **Claim:** Data repetition in symbolic music training improves model performance, contradicting typical LLM scaling assumptions.
- **Mechanism:** SMS Law extends Chinchilla Law to account for overfitting dynamics in symbolic music by introducing a GELU-regularized overfitting term and continuous adaptation for limited datasets.
- **Core assumption:** Symbolic music datasets are smaller and more repetitive than general text corpora; thus, repetition benefits outweigh overfitting risks.
- **Evidence anchors:**
  - [abstract]: "We explore the implications of the Symbolic Music Scaling Law (SMS Law) on model performance. This law explores how scaling up the training data affects the performance of symbolic music generation models."
  - [section]: "Crucially, previous iterations of the model fall short in predicting overfitting, particularly beyond early stopping thresholds. This gap is especially pronounced in the context of Data-Constrained environments, such as music."
  - [corpus]: **Weak** - No direct corpus evidence provided.
- **Break condition:** If symbolic music datasets grow large enough that standard LLM scaling laws apply, or if overfitting consistently harms performance.

## Foundational Learning

- **Concept:** Tokenizer design (Byte-Pair Encoding) for ABC notation.
  - **Why needed here:** ABC notation requires segmentation into meaningful tokens for model input; BPE balances vocabulary size and sequence coherence.
  - **Quick check question:** How does BPE handle multi-character musical symbols (e.g., "C#", "maj") compared to single-character notes?
- **Concept:** Scaling laws in machine learning.
  - **Why needed here:** Understanding how model size, dataset size, and compute budget interact is essential for optimizing symbolic music model training.
  - **Quick check question:** How does the SMS Law modify the Chinchilla Law for data-constrained environments?
- **Concept:** Positional encoding (Rotary Positional Encoding).
  - **Why needed here:** Long-range music sequences require robust positional awareness; RoPE improves context modeling over fixed encodings.
  - **Quick check question:** Why might RoPE be preferable to sinusoidal positional encoding for 8192-token sequences?

## Architecture Onboarding

- **Component map:** Tokenizer (YTTM + BPE, 50k vocab) -> Transformer decoder (SwiGLU, RMSNorm, RoPE) -> SMT-ABC preprocessing pipeline -> SMS Law parameter fitting module
- **Critical path:** 1. ABC corpus -> SMT-ABC formatting 2. SMT-ABC -> Tokenization 3. Token stream -> Transformer layers 4. Output tokens -> ABC notation
- **Design tradeoffs:** SMT-ABC improves alignment but adds preprocessing complexity; long context (8192 tokens) increases memory usage but captures full musical structure; SMS Law enables better scaling but requires careful overfitting monitoring
- **Failure signatures:** Bar misalignment -> structural incoherence in generated music; overfitting -> loss plateaus or increases after early stopping; poor tokenization -> fragmented musical phrases
- **First 3 experiments:** 1. Compare bar alignment quality: original ABC vs. SMT-ABC (qualitative listening test) 2. Measure tokenization efficiency: BPE vs. character-level encoding (token count, perplexity) 3. Validate SMS Law: train small models with varying repetition rates; check loss vs. prediction curves

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the proposed Synchronized Multi-Track ABC Notation (SMT-ABC) perform when handling tracks of significantly different lengths or complex musical structures beyond simple repetitions?
- **Basis in paper:** [explicit] The paper mentions that tracks may have different lengths due to repetition or other structures, and these special samples account for only 0.01% of the dataset, so they are simply skipped in the current implementation.
- **Why unresolved:** The paper acknowledges the existence of complex musical structures but does not explore or evaluate the model's performance on these cases, focusing instead on the majority of simpler cases.
- **What evidence would resolve it:** Experimental results comparing the performance of SMT-ABC on tracks with varying lengths and complex structures versus simple repetitive structures would provide insights into its robustness and limitations.

### Open Question 2
- **Question:** What is the impact of data repetition on the model's ability to generalize to unseen musical styles or compositions?
- **Basis in paper:** [explicit] The paper introduces the Symbolic Music Scaling (SMS) Law to explore the implications of data repetition on model performance, noting that repeated data positively impacts the model's performance but raises questions about its effectiveness for musical data.
- **Why unresolved:** While the paper discusses the benefits of data repetition on model performance, it does not investigate how this affects the model's ability to generalize to new, unseen musical styles or compositions.
- **What evidence would resolve it:** Experiments testing the model's performance on generating music in styles or genres not present in the training data would reveal the impact of data repetition on generalization.

### Open Question 3
- **Question:** How does the model's performance scale with the number of tracks in multi-track music generation?
- **Basis in paper:** [explicit] The paper proposes SMT-ABC to address challenges in multi-track music coherence but does not provide detailed analysis on how model performance changes with an increasing number of tracks.
- **Why unresolved:** The paper introduces a solution for multi-track coherence but lacks empirical evidence on the scalability of this solution as the number of tracks increases.
- **What evidence would resolve it:** Comparative studies evaluating the model's performance across different numbers of tracks (e.g., 2, 4, 6, 8) would demonstrate how well the model scales with complexity in multi-track music generation.

## Limitations

- SMS Law claims lack direct empirical validation through corpus evidence
- Human listening test methodology lacks detailed specifications (sample size, rater demographics, statistical significance)
- SMT-ABC synchronization may not generalize to free-form or improvisational musical styles
- Claims about ABC notation superiority over MIDI lack mechanistic explanations

## Confidence

**High Confidence:**
- The basic architecture (Transformer decoder with RMSNorm, SwiGLU, RoPE) is technically sound and well-documented
- The tokenizer design using BPE with 50k vocabulary is a standard, validated approach
- The general framework for synchronized multi-track processing has clear theoretical justification

**Medium Confidence:**
- Performance improvements over MIDI-based approaches are demonstrated but may depend heavily on specific implementation details
- The SMS Law extension to Chinchilla scaling shows promise but lacks comprehensive empirical validation
- Human listening test results suggest superiority over GPT-4 but methodology details are insufficient

**Low Confidence:**
- The mechanism by which ABC notation provides LLM compatibility advantages over MIDI is not rigorously established
- The extent to which SMS Law predictions match real-world training behavior remains unverified
- Generalization to musical styles beyond structured, bar-based compositions is unclear

## Next Checks

1. **SMS Law Validation:** Train multiple small MuPT models with systematically varied data repetition rates and early stopping points. Compare actual loss curves against SMS Law predictions to quantify overfitting forecasting accuracy and identify conditions where the law breaks down.

2. **Tokenization Efficiency Analysis:** Implement both BPE and character-level tokenization for the same SMT-ABC corpus. Measure and compare token counts, training convergence speed, and final generation quality across both approaches to determine if BPE provides measurable advantages.

3. **Cross-Style Generalization Test:** Generate music using MuPT across multiple genres (classical, jazz, pop, free-form) and evaluate bar alignment quality and musical coherence. Identify musical styles where SMT-ABC synchronization provides minimal benefit or potentially introduces artifacts.