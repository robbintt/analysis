---
ver: rpa2
title: 'Lexicography Saves Lives (LSL): Automatically Translating Suicide-Related
  Language'
arxiv_id: '2412.15497'
source_url: https://arxiv.org/abs/2412.15497
tags:
- latin
- suicide
- language
- high
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Lexicography Saves Lives (LSL) project aims to address the
  scarcity of suicide-related language resources in non-English and low-resource languages
  by automatically translating an English lexicon of 50 suicide-related terms and
  phrases into 200 languages using machine translation. The project includes human
  evaluation of translations across five languages (Catalan, Danish, Finnish, Galician,
  German) using metrics for Adequacy, Fluency, Spelling Errors, Cultural Acceptability,
  and Contextual Acceptability.
---

# Lexicography Saves Lives (LSL): Automatically Translating Suicide-Related Language

## Quick Facts
- **arXiv ID**: 2412.15497
- **Source URL**: https://arxiv.org/abs/2412.15497
- **Reference count**: 28
- **Primary result**: Machine-translated suicide lexicon covering 200 languages with human evaluation showing high quality for most languages, though Finnish had lower scores (2.80 Adequacy, 2.58 Fluency).

## Executive Summary
The Lexicography Saves Lives (LSL) project addresses the critical shortage of suicide-related language resources in non-English and low-resource languages by automatically translating an English lexicon of 50 suicide-related terms into 200 languages using machine translation. The project employs human evaluation across five languages using five metrics: Adequacy, Fluency, Spelling Errors, Cultural Acceptability, and Contextual Acceptability. Results show high translation quality for most languages, with Danish scoring highest (3.74 Adequacy, 3.60 Fluency), while Finnish scored lowest, reflecting cultural and contextual differences. The project also establishes a public platform for community contributions to improve translations over time.

## Method Summary
The project translates a 50-term English suicide lexicon from O'dea et al. (2015) into 200 languages using the Flores 101 benchmark and Fairseq toolkit with transformer-based models. Native speakers evaluate translations using five metrics: Adequacy, Fluency, Spelling Errors, Cultural Acceptability, and Contextual Acceptability. Each term receives scores from 0-4 (Adequacy/Fluency), 0 (Spelling Errors), or 0-1 (Acceptability metrics). The project also implements a public web platform for community feedback and contributions to iteratively improve translations, particularly for low-resource languages.

## Key Results
- Machine translation successfully covered 200 languages from a 50-term English suicide lexicon
- Human evaluation scores were high for most languages, with Danish achieving highest scores (3.74 Adequacy, 3.60 Fluency)
- Finnish showed lowest scores (2.80 Adequacy, 2.58 Fluency), highlighting cultural and contextual challenges
- Project established public platform for ongoing community contributions to improve translations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Machine translation of a suicide-related lexicon enables coverage of non-English languages for detection tools.
- Mechanism: A seed English lexicon of 50 suicide-related terms is translated into 200 languages using a large-scale multilingual MT system (Flores 101), producing a broad set of candidate translations for low-resource languages.
- Core assumption: The MT system's embeddings capture enough semantic similarity between languages to produce usable translations of domain-specific terms.
- Evidence anchors:
  - [abstract] "automatically translating an English lexicon of 50 suicide-related terms and phrases into 200 languages using machine translation."
  - [section 3] "The automated machine translation (MT) system used for translation purposes is based on the Flores 101 evaluation benchmark... which was extended to cover 200 languages."
  - [corpus] "Found 25 related papers (using 8)." Weak evidence—no direct mention of MT lexicon translation.

### Mechanism 2
- Claim: Human evaluation metrics (Adequacy, Fluency, Cultural Acceptability, Contextual Acceptability) can validate translation quality beyond raw MT scores.
- Mechanism: Native speakers rate each translated term on multiple dimensions using Likert scales, producing quantitative scores that reflect real-world usability.
- Core assumption: Native speaker judgments align with actual cultural and contextual appropriateness of suicide-related language.
- Evidence anchors:
  - [section 4.1] "We introduce five different variables to evaluate the qualitative validity of each lexicon and two additional free text variables for contributions."
  - [section 4.3] "Each annotator is given (i) the original dictionary alongside the translation and (ii) a code book with an example evaluation for reference."
  - [corpus] "Evaluating Large Language Models in Crisis Detection..." Mentions evaluation but not specifically human cultural assessment.

### Mechanism 3
- Claim: Community contributions and open feedback improve lexicon quality over time.
- Mechanism: A public website collects user-submitted alternative translations and culturally specific terms, enabling iterative refinement beyond the initial MT output.
- Core assumption: End-users and native speakers will actively contribute and validate translations in low-resource languages.
- Evidence anchors:
  - [abstract] "The project also invites community contributions to improve translations and has created a public platform for resource sharing and feedback."
  - [section 5] "The platform with also help as a feedback collection mechanism... which will allow the users to improve the repository."
  - [corpus] "Socially Aware Synthetic Data Generation for Suicidal Ideation Detection..." Suggests synthetic/community-driven data is relevant.

## Foundational Learning

- Concept: Cross-lingual semantic alignment in embedding spaces.
  - Why needed here: Ensures that translated suicide-related terms preserve their original intent across languages.
  - Quick check question: How would you verify that the embedding similarity between "suicide" in English and its translation in another language is meaningful?

- Concept: Cultural-linguistic nuance in suicide discourse.
  - Why needed here: Suicide-related expressions often differ from literal translations due to metaphors and taboos.
  - Quick check question: What are examples of culturally specific phrases for suicide that literal translation would miss?

- Concept: Likert-scale reliability and inter-rater agreement.
  - Why needed here: Ensures human evaluation scores are consistent and actionable.
  - Quick check question: How would you measure if two annotators are rating "Adequacy" consistently?

## Architecture Onboarding

- Component map: Seed lexicon → MT translation engine (Flores 101) → 200 language dictionaries → Human evaluation → Community feedback → Public web platform
- Critical path: Seed lexicon generation → MT translation → Human evaluation → Quality scoring → Community contribution loop
- Design tradeoffs: MT speed vs. translation nuance; broad coverage vs. depth of cultural validation; open feedback vs. moderation overhead
- Failure signatures: Low evaluation scores (especially in Cultural/Contextual Acceptability); high spelling error counts; community contributions stagnate
- First 3 experiments:
  1. Compare MT translation adequacy scores against BLEU/ROUGE baselines to quantify domain-specific MT quality
  2. Run inter-rater reliability analysis on human evaluation data to ensure consistent scoring
  3. A/B test the public platform's feedback form to optimize contribution rates from native speakers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal methodology for developing comprehensive ethics guidelines for suicide-related machine translation projects that balances technical feasibility with cultural sensitivity?
- Basis in paper: [explicit] The paper identifies this as a future work item, stating "we will develop a comprehensive set of ethics guidelines and assessment strategies"
- Why unresolved: Current guidelines are described as "broad ethical considerations" rather than comprehensive protocols. The project has identified ethical concerns but hasn't established systematic frameworks for addressing them.
- What evidence would resolve it: A published framework that details specific protocols for cultural consultation, privacy protection, and harm mitigation in suicide-related MT projects, with case studies demonstrating successful implementation.

### Open Question 2
- Question: How can we accurately measure translation quality for suicide-related content when traditional metrics may not capture cultural and contextual appropriateness?
- Basis in paper: [explicit] The authors note they plan to "improve the methodology behind the lexicon quality scores" and "identify a means of measuring overall quality"
- Why unresolved: The paper introduces novel evaluation metrics but acknowledges limitations in capturing nuanced cultural meaning. Traditional MT metrics like BLEU may not be appropriate for sensitive mental health content.
- What evidence would resolve it: Comparative studies showing correlation between proposed evaluation metrics and real-world effectiveness in suicide prevention contexts, or validation studies demonstrating that human judgments align with proposed quantitative measures.

### Open Question 3
- Question: What is the relationship between linguistic representation in low-resource languages and the effectiveness of suicide prevention tools in those communities?
- Basis in paper: [inferred] The paper discusses "linguistic misrepresentation" and "under-representation of language" as ethical concerns, and notes that low-resource languages may have "lower functionality of the finished tool"
- Why unresolved: While the paper identifies the problem, it doesn't provide empirical evidence quantifying how linguistic gaps affect tool effectiveness or demonstrate whether improved translation quality leads to better outcomes.
- What evidence would resolve it: Longitudinal studies comparing suicide prevention outcomes in communities with different levels of linguistic representation in translation tools, controlling for other variables like healthcare access and cultural attitudes toward mental health.

## Limitations
- Translation quality for low-resource languages remains uncertain, as human evaluation only covered five high-resource languages
- Cultural and contextual acceptability ratings are based on limited annotator pools, potentially missing important regional variations
- Community contribution mechanisms are proposed but unproven, with no evidence yet of active participation

## Confidence
- **High Confidence**: The MT translation pipeline using Flores 101 and Fairseq toolkit is technically sound and reproducible
- **Medium Confidence**: Translation quality scores for the five evaluated languages appear reliable, but generalization to the full 200-language set is uncertain
- **Low Confidence**: Long-term sustainability of community contributions and actual impact on suicide detection tools remains speculative without pilot data

## Next Checks
1. Conduct pilot community contribution testing with native speakers of 3-5 low-resource languages to measure actual engagement rates and contribution quality
2. Perform inter-rater reliability analysis on the existing human evaluation data to quantify consistency across annotators and identify potential scoring biases
3. Compare MT translation adequacy scores against specialized suicide lexicon translation benchmarks to validate domain-specific translation quality