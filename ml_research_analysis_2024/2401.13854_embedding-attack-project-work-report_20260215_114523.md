---
ver: rpa2
title: Embedding Attack Project (Work Report)
arxiv_id: '2401.13854'
source_url: https://arxiv.org/abs/2401.13854
tags: []
core_contribution: This work investigates privacy leakage in embedding layers of machine
  learning models through membership and property inference attacks. Six models spanning
  computer vision and language modeling are evaluated using loss-based and embedding-based
  attack strategies.
---

# Embedding Attack Project (Work Report)

## Quick Facts
- **arXiv ID:** 2401.13854
- **Source URL:** https://arxiv.org/abs/2401.13854
- **Reference count:** 1
- **Primary result:** Embedding layers in ML models leak membership and property information, with deeper layers revealing more membership data and loss-based attacks outperforming embedding-based attacks.

## Executive Summary
This work investigates privacy leakage in embedding layers of machine learning models through membership and property inference attacks. Six models spanning computer vision and language modeling are evaluated using loss-based and embedding-based attack strategies. Key findings include that overfitting directly increases vulnerability to MIAs, deeper embedding layers reveal more membership information, and loss-based attacks outperform embedding-based attacks. The work also explores defense mechanisms like noisy self-distillation and identifies several open questions about attack mechanisms and effective defenses.

## Method Summary
The research trains six different models (CNN/CIFAR-10, CNN/CelebA, ViT/Snacks, MLP/Purchase100, BERT/IMDb, Transformer/AGNews) with varying degrees of overfitting. Three attack strategies are implemented: loss-based, embedding-based, and prediction-based. Attack models are trained on intermediate embeddings, prediction vectors, or prediction loss to infer membership. Property inference attacks are also evaluated using auxiliary datasets. The study compares attack performance across different model architectures and embedding layer depths.

## Key Results
- Overfitting directly increases vulnerability to membership inference attacks (MIAs)
- Deeper embedding layers reveal more membership information than shallow layers
- Loss-based attacks outperform embedding-based attacks across all evaluated models
- Embeddings remain susceptible to property inference attacks despite moderate MIA success rates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Overfitting directly increases vulnerability to membership inference attacks (MIAs)
- Mechanism: When a model overfits, it memorizes training data patterns that don't generalize. This memorization creates statistical differences between training and non-training data in the model's outputs (losses, embeddings), which attackers can exploit to distinguish members from non-members.
- Core assumption: The attacker can access model outputs and has auxiliary data from the same distribution as the training data
- Evidence anchors: [abstract] "Overfitting directly increases vulnerability to MIAs"; [section] "Overfitting occurs when a model fits the training data well but fails to generalize to new, unseen data"

### Mechanism 2
- Claim: Deeper embedding layers reveal more membership information
- Mechanism: As information flows through network layers, semantic content becomes more refined while membership-specific patterns accumulate in deeper layers. This concentration of membership information makes deeper embeddings more informative for distinguishing training data members.
- Core assumption: The model architecture preserves membership information through layers rather than compressing it away
- Evidence anchors: [abstract] "Deeper embedding layers reveal more membership information"; [section] "We demonstrated this point using MLP trained on PurChase100 and CNN trained on CIFAR-10"

### Mechanism 3
- Claim: Loss-based attacks outperform embedding-based attacks
- Mechanism: Prediction loss directly measures how well the model fits a data point. Training data typically has lower loss because the model has seen it before, creating a clear signal for membership inference. Embeddings contain both semantic and membership information mixed together, making membership extraction harder.
- Core assumption: The attacker can compute prediction loss for target data points
- Evidence anchors: [abstract] "Loss-based attacks outperform embedding-based attacks"; [section] "Our results show that loss-based NN MIAs generally outperform embedding-based NN MIAs on all models"

## Foundational Learning

- Concept: Membership Inference Attacks (MIAs)
  - Why needed here: The entire work revolves around understanding and evaluating different MIA strategies, so understanding the attack mechanics is fundamental
  - Quick check question: What distinguishes a membership inference attack from other privacy attacks, and what does it aim to determine?

- Concept: Overfitting and generalization in machine learning
  - Why needed here: The work establishes a direct relationship between overfitting and privacy vulnerability, requiring understanding of how overfitting manifests and affects model behavior
  - Quick check question: How can you identify overfitting in a model, and what are its implications for model privacy?

- Concept: Embeddings and latent representations
  - Why needed here: The work focuses on privacy leakage from embedding layers, requiring understanding of what embeddings are and how they capture information at different depths
  - Quick check question: What information do embedding layers capture, and how does this information change as you move deeper in the network?

## Architecture Onboarding

- Component map:
  Data preparation -> Model training -> Attack implementation -> Evaluation -> Defense experimentation

- Critical path:
  1. Data loading and preprocessing
  2. Model training with controlled overfitting levels
  3. Attack model training and evaluation
  4. Analysis of results across different model types and attack strategies
  5. Defense implementation and evaluation

- Design tradeoffs:
  - Model complexity vs. privacy vulnerability: More complex models may overfit more easily
  - Attack sophistication vs. practicality: Loss-based attacks are simpler but embedding attacks may work in more scenarios
  - Defense strength vs. model utility: Stronger defenses may degrade model performance

- Failure signatures:
  - Low attack AUC scores despite overfitting: May indicate weak attack implementation or insufficient auxiliary data
  - Inconsistent results across similar models: May indicate bugs in attack implementation or evaluation metrics
  - High variance in results: May indicate insufficient training data or unstable attack models

- First 3 experiments:
  1. Implement and evaluate loss-based MIA on a simple CNN trained on CIFAR-10
  2. Implement and evaluate embedding-based MIA on the same model, comparing results with loss-based attack
  3. Train two versions of the same model (one overfitted, one well-generalized) and compare attack performance to validate the overfitting-vulnerability relationship

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does membership information accumulate in deeper layers of classification models, and if so, what is the precise mechanism?
- Basis in paper: [explicit] The paper states "We propose that membership information accumulates in deeper layers of classification models" and provides evidence that deeper embeddings reveal more membership information
- Why unresolved: The paper only provides empirical evidence for this accumulation but doesn't offer a theoretical explanation for why this occurs
- What evidence would resolve it: A theoretical analysis explaining the mathematical relationship between layer depth and membership information concentration, or experimental results showing how this accumulation occurs during training

### Open Question 2
- Question: What are the most effective defense mechanisms against embedding-based membership inference attacks?
- Basis in paper: [inferred] The paper mentions ongoing work on MIA defense and explores "noisy self-distillation" as a potential defense mechanism, but doesn't provide comprehensive results
- Why unresolved: The defense experiments are described as ongoing, and only preliminary results are presented
- What evidence would resolve it: Comprehensive evaluation of multiple defense strategies (beyond noisy self-distillation) including differential privacy, adversarial training, and other techniques, with quantifiable improvements in privacy protection

### Open Question 3
- Question: How does the effectiveness of neighborhood-comparison embedding attacks compare to traditional loss-based and embedding-based attacks?
- Basis in paper: [explicit] The paper states "We are currently implementing a neighborhood comparison-based attack" but doesn't provide results
- Why unresolved: The neighborhood-comparison attack methodology is described as ongoing work without completed results
- What evidence would resolve it: Completed experimental results comparing the AUC scores of neighborhood-comparison attacks against existing loss-based and embedding-based attacks across all six model types

### Open Question 4
- Question: How do property inference attacks against embeddings differ in effectiveness between shallow and deep layers, and what explains this difference?
- Basis in paper: [explicit] The paper states "Our experiments showed that embeddings from shallower layers leaked more facial attributes" but doesn't provide a detailed analysis
- Why unresolved: The paper provides empirical observations but lacks theoretical explanation for why shallower layers are more susceptible to property inference attacks
- What evidence would resolve it: Analysis of the semantic information content in embeddings at different depths, and experimental results showing how property inference effectiveness varies with layer depth across multiple types of properties and models

## Limitations
- Evaluation limited to six specific model types across two domains (vision and language)
- Attack success rates measured using AUC scores in controlled experimental settings
- Noisy self-distillation defense not rigorously evaluated across multiple model types

## Confidence
- **High confidence**: The relationship between overfitting and MIA vulnerability
- **Medium confidence**: The claim that deeper embeddings contain more membership information
- **Low confidence**: The comparative effectiveness of loss-based vs embedding-based attacks

## Next Checks
1. Evaluate attack strategies on additional model types (e.g., graph neural networks) to verify generalizability beyond current six-model scope
2. Test attack effectiveness when attacker has limited access to auxiliary data and cannot perfectly control target model training conditions
3. Systematically evaluate noisy self-distillation across all six model types with varying noise levels to establish privacy-utility tradeoff