---
ver: rpa2
title: 'Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative
  Inference'
arxiv_id: '2403.09054'
source_url: https://arxiv.org/abs/2403.09054
tags:
- tokens
- cache
- attention
- keyformer
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Keyformer introduces an inference-time method to reduce KV cache
  size and memory bandwidth usage in generative language models without compromising
  accuracy. The approach leverages the observation that approximately 90% of attention
  weight focuses on a small subset of "key" tokens, which are crucial for understanding
  context but may fall outside a sliding window.
---

# Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference

## Quick Facts
- arXiv ID: 2403.09054
- Source URL: https://arxiv.org/abs/2403.09054
- Reference count: 40
- Primary result: 2.1× latency reduction and 2.4× throughput improvement with 99% accuracy at 50% KV cache reduction

## Executive Summary
Keyformer is an inference-time method designed to reduce KV cache size and memory bandwidth usage in generative language models without compromising accuracy. The approach exploits the observation that approximately 90% of attention weight concentrates on a small subset of "key" tokens, which are crucial for context understanding but may not be included in the sliding window. Keyformer identifies these key tokens using a novel score function and combines them with recent tokens during attention, enabling significant reduction in KV cache size. Evaluated on GPT-J, Cerebras-GPT, and MPT models across summarization and conversation tasks, Keyformer achieves substantial efficiency gains while maintaining high accuracy.

## Method Summary
Keyformer operates by identifying and prioritizing a small subset of tokens—called "key" tokens—that receive the majority of attention weight during inference. Using a novel scoring function, the method selects these tokens and combines them with recent tokens within the sliding window for attention computation. This selective approach allows for a dramatic reduction in KV cache size, as only the most contextually relevant tokens are retained. The method is applied at inference time and does not require retraining, making it broadly applicable to existing models. Experiments demonstrate that Keyformer can reduce KV cache size by up to 50% while maintaining 99% accuracy, resulting in 2.1× reduction in inference latency and 2.4× improvement in token generation throughput.

## Key Results
- 2.1× reduction in inference latency and 2.4× improvement in token generation throughput compared to full attention baseline
- 99% accuracy retention with up to 50% reduction in KV cache size
- Effective across GPT-J, Cerebras-GPT, and MPT models on summarization and conversation tasks

## Why This Works (Mechanism)
Keyformer leverages the empirical observation that most attention weight is concentrated on a small set of tokens, which may be distributed across the entire context rather than just the recent sliding window. By identifying and prioritizing these key tokens, the method ensures that the most important contextual information is preserved, even when the overall KV cache is reduced. This selective attention enables efficient memory usage and bandwidth reduction without significant loss of accuracy.

## Foundational Learning

**Attention Mechanism**
- Why needed: Core operation in transformer-based models; enables dynamic context weighting
- Quick check: Verify that the majority of attention weight is indeed concentrated on a small subset of tokens (as claimed ~90%)

**KV Cache**
- Why needed: Stores key and value vectors for previously generated tokens to avoid redundant computation during autoregressive generation
- Quick check: Confirm that reducing KV cache size leads to proportional memory and bandwidth savings

**Sliding Window Attention**
- Why needed: Standard approach for limiting attention to recent tokens; Keyformer extends this by selectively including key tokens outside the window
- Quick check: Ensure that combining key tokens with the sliding window does not disrupt standard autoregressive generation

## Architecture Onboarding

**Component Map**
- Input tokens → Sliding window selection + Key token scoring → Attention with key + recent tokens → KV cache update → Output

**Critical Path**
The critical path is the attention computation, where the model computes weighted sums over key and value vectors for the current token, using both recent and key tokens.

**Design Tradeoffs**
- Precision vs. efficiency: Selecting fewer key tokens increases efficiency but risks missing important context
- Window size vs. coverage: Larger sliding windows reduce the need for key tokens but increase memory usage

**Failure Signatures**
- Accuracy drops if key token scoring fails to identify truly important tokens
- Latency may not improve if the overhead of key token selection outweighs memory savings

**First Experiments**
1. Measure attention weight distribution to confirm ~90% concentration on a small subset of tokens
2. Ablate the scoring function to determine its impact on key token selection and accuracy
3. Vary KV cache reduction levels to find the point where accuracy begins to degrade

## Open Questions the Paper Calls Out
The authors highlight that the precise distribution and stability of "key" tokens across different layers and tasks remains underexplored. They also note that the method's performance on long-context generation tasks and its applicability to multi-modal models are untested.

## Limitations
- The robustness of key token selection across varied domains and prompt styles is unclear
- Performance on long-context generation tasks is not addressed
- Sensitivity to sliding window size and scoring function hyperparameters is not thoroughly investigated

## Confidence

**Major Claim Clusters - Confidence:**
- KV cache reduction and efficiency gains: **High**
- Accuracy retention with 50% KV cache reduction: **Medium**
- Generalization to other model families and tasks: **Low**

## Next Checks
1. Test Keyformer on long-context generation tasks (e.g., document summarization with extended context windows) to assess performance and stability.
2. Evaluate the method's robustness to different prompt styles and domains, including non-English text and specialized vocabularies.
3. Investigate the sensitivity of the key token selection to sliding window size and scoring function hyperparameters.