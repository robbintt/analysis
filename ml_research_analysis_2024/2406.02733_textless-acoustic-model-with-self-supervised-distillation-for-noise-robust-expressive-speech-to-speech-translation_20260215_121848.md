---
ver: rpa2
title: Textless Acoustic Model with Self-Supervised Distillation for Noise-Robust
  Expressive Speech-to-Speech Translation
arxiv_id: '2406.02733'
source_url: https://arxiv.org/abs/2406.02733
tags:
- pretssel
- speech
- dino-pretssel
- proposed
- denoiser
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a self-supervised distillation strategy to
  improve the noise robustness of textless speech-to-speech translation systems. The
  approach incorporates DINO-based self-supervised training into the PRETSSEL unit-to-speech
  generator to learn noise-agnostic expressivity representations.
---

# Textless Acoustic Model with Self-Supervised Distillation for Noise-Robust Expressive Speech-to-Speech Translation

## Quick Facts
- arXiv ID: 2406.02733
- Source URL: https://arxiv.org/abs/2406.02733
- Reference count: 40
- This paper proposes a self-supervised distillation strategy to improve the noise robustness of textless speech-to-speech translation systems

## Executive Summary
This paper addresses the vulnerability of textless speech-to-speech translation (S2ST) systems to background noise in real-world scenarios. The authors propose a DINO-based self-supervised distillation strategy incorporated into the PRETSSEL unit-to-speech generator to learn noise-agnostic expressivity representations. The approach trains two encoders (teacher and student) with noise augmentation, forcing the model to disentangle vocal style from noise information. Experimental results show significant improvements in noisy environments while maintaining competitive performance in clean conditions, achieving up to 0.75 higher MOS and 0.48 higher S-MOS scores in noisy scenarios.

## Method Summary
The proposed method builds upon the PRETSSEL textless S2ST framework by incorporating DINO-based self-supervised distillation into the pretraining phase. During training, a student expressivity encoder is optimized to match the probabilistic outputs of a teacher encoder while random noise augmentation is applied to the input speech. The teacher weights are updated as an exponential moving average of the student weights. This architecture forces the model to learn representations that are invariant to background noise while preserving vocal style and expressivity information. The DINO-PRETSSEL model is then integrated into the complete S2ST pipeline consisting of Prosody UnitY2 for translation and HiFi-GAN for waveform synthesis.

## Key Results
- DINO-PRETSSEL achieved up to 0.75 higher MOS and 0.48 higher S-MOS scores in noisy conditions compared to conventional systems
- The model demonstrated significant improvements in ASR-BLEU (1.17-2.37 points), AutoPCP (0.08-0.10), and SNR (0.72-0.76 dB) in noisy environments
- In clean conditions, DINO-PRETSSEL maintained competitive performance with only slight degradation in expressivity preservation (0.02 lower AutoPCP) while outperforming denoiser-based approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The DINO-based self-distillation strategy learns noise-agnostic expressivity representations by training two encoders (teacher and student) to match probabilistic outputs while applying random noise augmentation to the input.
- Mechanism: During training, the student encoder is optimized to minimize cross-entropy loss between its output distribution and the teacher's output distribution. The teacher weights are updated as an exponential moving average of the student weights. Noise augmentation is applied to the input speech with random SNR, forcing the model to learn representations invariant to background noise.
- Core assumption: The teacher-student architecture with noise augmentation can effectively disentangle vocal style/expressivity information from noise/channel information in the embedding space.
- Evidence anchors:
  - [abstract]: "Because the proposed method captures noise-agnostic expressivity representation, it can generate qualified speech even in noisy environment."
  - [section 4.1]: Describes the student-teacher training loop with noise augmentation and DINO loss computation.
  - [corpus]: Weak - no direct citations to DINO-based speech denoising literature in the neighbor papers.

### Mechanism 2
- Claim: The proposed DINO-PRETSSEL model improves translation quality in noisy environments by maintaining robustness in both content and prosody preservation while suppressing noise.
- Mechanism: By learning noise-agnostic expressivity embeddings, the U2S generator can transfer vocal style from noisy source speech to clean translated speech without transferring the noise itself. This is achieved through the combination of the self-distillation framework and the fact that discrete units are always extracted from clean speech during training.
- Core assumption: The expressivity encoder can effectively separate paralinguistic information (vocal style, emotion) from channel information (background noise) in the embedding space.
- Evidence anchors:
  - [abstract]: "Experimental results verified that the expressive S2ST system with DINO-PRETSSEL outperformed conventional S2ST models in noisy recording environments."
  - [section 6.2]: Shows DINO-PRETSSEL achieves highest ASR-BLEU, AutoPCP, and SNR scores in noisy conditions.
  - [corpus]: Weak - no direct citations to speech denoising literature in the neighbor papers.

### Mechanism 3
- Claim: The DINO training strategy trades off some expressivity preservation performance in clean environments for robustness in noisy environments.
- Mechanism: The additional training objective (DINO loss) requires the model to learn broader representations that include noise robustness, which slightly degrades performance on the primary task (expressivity preservation) when noise is absent. However, this tradeoff is beneficial overall as it enables good performance across both clean and noisy conditions.
- Core assumption: The expressivity preservation task and noise robustness task can be jointly optimized without catastrophic interference.
- Evidence anchors:
  - [section 7.2]: "When the source speech was clean, DINO-PRETSSEL provided slightly lower naturalness than the original PRETSSEL" and "However, DINO-PRETSSEL still demonstrated higher naturalness than denoiser-based PRETSSEL."
  - [section 6.2]: Shows slight drops in AutoPCP for DINO-PRETSSEL in clean conditions compared to PRETSSEL.
  - [corpus]: Weak - no direct citations to multitask learning tradeoff literature in the neighbor papers.

## Foundational Learning

- Concept: Self-supervised learning with contrastive objectives
  - Why needed here: The DINO framework uses contrastive learning between different views of the same input to learn meaningful representations without labels.
  - Quick check question: How does the student encoder learn to match the teacher's output distribution in DINO training?

- Concept: Disentanglement of linguistic and paralinguistic information
  - Why needed here: The system relies on discrete units containing linguistic information while expressivity embeddings contain paralinguistic information, enabling cross-lingual style transfer.
  - Quick check question: Why are discrete units always extracted from clean speech during training in DINO-PRETSSEL?

- Concept: Exponential moving average for teacher network updates
  - Why needed here: The teacher network provides consistent targets for the student by slowly tracking the student's weights, preventing model collapse.
  - Quick check question: What would happen if the teacher network was updated with the student's weights directly without EMA?

## Architecture Onboarding

- Component map: Source speech → Expressivity encoder → Projection heads → DINO loss computation → Discrete unit extractor → DINO-PRETSSEL → Mel-spectrogram → HiFi-GAN → Translated speech
- Critical path: Source speech → Expressivity encoder → Prosody UnitY2 → Discrete units → DINO-PRETSSEL → Mel-spectrogram → HiFi-GAN → Translated speech
- Design tradeoffs:
  - Training complexity vs. inference efficiency (DINO adds teacher/student encoders only during training)
  - Expressivity preservation vs. noise robustness (slight degradation in clean conditions)
  - Computational cost vs. performance (DINO-PRETSSEL takes 5 days longer to train)
- Failure signatures:
  - Loss of vocal style in translated speech → expressivity encoder not properly disentangling information
  - Noise transferred to translated speech → noise augmentation not effective or discrete units corrupted
  - Poor translation quality → S2UT model not robust enough
- First 3 experiments:
  1. Test expressivity encoder with clean and noisy inputs to verify disentanglement capability
  2. Compare DINO-PRETSSEL performance with and without noise augmentation during training
  3. Evaluate translation quality using ground-truth units to isolate U2S model performance from S2UT errors

## Open Questions the Paper Calls Out

1. How does the DINO-PRETSSEL approach perform on real-world noisy speech from diverse acoustic environments beyond the simulated noise used in training?
   - Basis in paper: [explicit] The authors conducted subjective evaluation using real-world noisy samples from the VoxLingua107 dataset, showing DINO-PRETSSEL outperformed other systems. However, the evaluation was limited to 100 samples from YouTube videos with SNR 5-15dB.
   - Why unresolved: The evaluation was limited in scale and acoustic diversity. Real-world scenarios involve varied noise types (reverberation, overlapping speech, environmental sounds) not present in the evaluation.
   - What evidence would resolve it: Large-scale testing on diverse real-world noisy datasets (e.g., CHiME challenges, VoiceBank-DEMAND) with various noise types and SNR ranges would establish robustness boundaries.

2. What is the computational cost trade-off between DINO-PRETSSEL and conventional PRETSSEL in terms of training time, inference latency, and model size for deployment?
   - Basis in paper: [explicit] The authors mention DINO-PRETSSEL took 12.7 days for pretraining (5 days longer than PRETSSEL's 7.9 days) and note additional expressivity encoders increase parameters during training but not inference.
   - Why unresolved: The paper only reports training time difference. Inference latency, GPU memory requirements, and real-time deployment feasibility are not discussed despite being critical for practical applications.
   - What evidence would resolve it: Comprehensive benchmarking of inference latency, memory usage, and throughput on different hardware platforms compared to conventional systems.

3. Can the DINO self-distillation strategy be effectively applied to other components of the speech-to-speech translation pipeline to improve overall robustness?
   - Basis in paper: [explicit] The authors note that "it is possible further improve translation quality of S2ST system by enhancing the robustness of the S2UT model as well" based on their observation that DINO-PRETSSEL's ASR-BLEU scores were unaffected by noise when using ground-truth units.
   - Why unresolved: The paper only applied DINO to the U2S generator. The S2UT component's vulnerability to noise was identified but not addressed, leaving a gap in the full translation pipeline's robustness.
   - What evidence would resolve it: Application of DINO or similar noise-robust strategies to the S2UT component, followed by integrated system evaluation showing improved end-to-end performance in noisy conditions.

## Limitations
- Evaluation primarily relies on simulated noise conditions using DNS-5 noise database rather than real-world recordings
- Architecture description lacks specifics about the Prosody UnitY2 S2UT model and HiFi-GAN vocoder configurations
- Trade-off analysis between clean and noisy performance could benefit from more extensive ablation studies

## Confidence
- High confidence: The DINO-based self-distillation mechanism for noise-robust expressivity learning
- Medium confidence: The overall system's noise robustness improvements
- Low confidence: The scalability of this approach to extremely noisy environments or diverse real-world conditions

## Next Checks
1. Test the trained DINO-PRETSSEL model on actual noisy recordings from diverse environments (street noise, cafe chatter, etc.) rather than simulated DNS-5 noise to verify robustness claims hold in practice.

2. Evaluate the model's performance when transferring expressivity between languages with different phonetic structures to assess whether the noise-agnostic representations generalize across language pairs.

3. Systematically vary the noise augmentation parameters (SNR range, probability) during training to determine the optimal settings for balancing clean and noisy performance.