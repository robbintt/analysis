---
ver: rpa2
title: Towards Quantifying the Preconditioning Effect of Adam
arxiv_id: '2402.07114'
source_url: https://arxiv.org/abs/2402.07114
tags:
- adam
- have
- theorem
- proof
- diagonal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper analyzes the preconditioning effect of Adam for quadratic\
  \ functions and beyond. The key findings are: For a d-dimensional quadratic with\
  \ diagonal Hessian having condition number \u03BA, Adam (without momentum) has iteration\
  \ complexity O(min(d, \u03BA)) with constant probability over random initialization."
---

# Towards Quantifying the Preconditioning Effect of Adam

## Quick Facts
- arXiv ID: 2402.07114
- Source URL: https://arxiv.org/abs/2402.07114
- Authors: Rudrajit Das; Naman Agarwal; Sujay Sanghavi; Inderjit S. Dhillon
- Reference count: 40
- Primary result: Adam's iteration complexity improves upon gradient descent when dimension d is sufficiently smaller than the condition number κ

## Executive Summary
This paper provides a theoretical analysis of Adam's preconditioning effect on quadratic optimization problems. The key insight is that Adam can achieve better iteration complexity than gradient descent when the dimension d is smaller than a function of the condition number κ, though at the expense of a dimension-dependent quantity. The authors analyze three cases: diagonal Hessians, diagonally dominant Hessians, and general Hessians, showing how Adam's performance varies across these settings. The analysis also reveals an important limitation: Adam may converge to non-trivial fixed points instead of the global minimum, depending on hyperparameter choices.

## Method Summary
The paper analyzes Adam's preconditioning effect by studying its convergence on quadratic functions with various Hessian structures. The analysis uses a coordinate-dependent δ term in Adam's update rule and transforms the problem into a pseudo-linear system using basis changes. The authors derive iteration complexity bounds through a two-stage analysis involving singular value bounds of transformed Hessians. The approach combines techniques from linear algebra, optimization theory, and stochastic approximation to characterize Adam's behavior under different conditions.

## Key Results
- For diagonal Hessians, Adam's iteration complexity is O(min(d, κ)) with constant probability, improving upon gradient descent's O(κ) when d < O(κ)
- For diagonally dominant Hessians, Adam's iteration complexity is O(min(d√dκ, κ)), improving upon gradient descent when d < O(κ^(1/3))
- Adam may converge to non-trivial fixed points instead of global minimum, depending on hyperparameter δ
- For functions with per-coordinate Lipschitz smoothness, Adam can outperform gradient descent when initialization-dependent quantities are small enough

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adam can suffer less from the condition number κ but at the expense of a dimension-dependent quantity.
- Mechanism: For diagonal Hessians, Adam's effective condition number-like quantity controlling iteration complexity is O(min(d, κ)), which improves upon gradient descent's O(κ) dependence when d < O(κ).
- Core assumption: The analysis assumes exact gradients and random initialization with each coordinate drawn from Unif[-2B, 2B].
- Evidence anchors:
  - [abstract] "Our key finding is that Adam can suffer less from the condition number but at the expense of suffering a dimension-dependent quantity."
  - [section] "In Theorem 2, we show that for diagonal Q, the iteration complexity of Adam is O(min(d, κ)) with constant probability."
  - [corpus] No direct evidence found for this specific mechanism in related papers.
- Break condition: When d ≥ O(κ), Adam's complexity becomes O(d) which is worse than gradient descent's O(κ) dependence.

### Mechanism 2
- Claim: For diagonally dominant Hessians, Adam's iteration complexity is O(min(d√dκ, κ)), improving upon gradient descent when d < O(κ^(1/3)).
- Mechanism: The Jacobi preconditioning (D^(-1/2)QD^(-1/2)) reduces the effective condition number to O(1) for diagonally dominant matrices, allowing Adam to exploit this structure.
- Core assumption: Q is ν-diagonally-dominant where each row's off-diagonal elements sum to at most ν times the diagonal element.
- Evidence anchors:
  - [abstract] "For a diagonally dominant Hessian, we obtain a bound of O(min(d√dκ, κ)) for the corresponding quantity."
  - [section] "For diagonally dominant Q, Adam's iteration complexity is O(min(d√dκ, κ)) which improves upon gradient descent when d < O(κ^(1/3))."
  - [corpus] No direct evidence found for this specific mechanism in related papers.
- Break condition: When d ≥ O(κ^(1/3)), Adam's complexity becomes O(κ) which is comparable to gradient descent.

### Mechanism 3
- Claim: Adam may converge to a non-trivial fixed point instead of the global minimum, depending on the hyperparameter δ.
- Mechanism: The coordinate-dependent δ in Adam's update rule can prevent convergence to zero gradient when δ is not sufficiently large.
- Core assumption: lim k→∞ δ_k exists and is non-zero.
- Evidence anchors:
  - [abstract] "Adam may converge to a non-trivial fixed point instead of the global minimum, depending on the hyperparameter δ."
  - [section] "We theoretically characterize this phenomenon in Section 4.2."
  - [corpus] No direct evidence found for this specific mechanism in related papers.
- Break condition: When δ > κα/2r where r = minj∈[d] max(|g(j)_0|, ϕ), Adam will converge to the global minimum.

## Foundational Learning

- Concept: Condition number and its impact on optimization
  - Why needed here: The paper's core contribution is quantifying how Adam's preconditioning effect relates to the condition number of the Hessian matrix.
  - Quick check question: If a quadratic function has condition number κ, what is the iteration complexity of gradient descent without momentum?

- Concept: Jacobi preconditioning
  - Why needed here: The analysis uses D^(-1/2)QD^(-1/2) (Jacobi preconditioning) to bound the effective condition number for general Hessians.
  - Quick check question: What is the condition number of D^(-1/2)QD^(-1/2) when Q is ν-diagonally-dominant?

- Concept: Per-coordinate smoothness and modified Polyak-Lojasiewicz conditions
  - Why needed here: Section 5 extends the analysis beyond quadratics to functions satisfying these conditions.
  - Quick check question: How does the iteration complexity of Adam change when the function satisfies per-coordinate smoothness compared to standard global smoothness?

## Architecture Onboarding

- Component map: Quadratic minimization problem with Hessian Q -> Adam update rule with coordinate-dependent δ -> Basis transformation to pseudo-linear system -> Singular value bounds of transformed Hessian -> Two-stage decay rate analysis -> Continuous descent verification -> Iteration complexity derivation
- Critical path: The critical path for proving Adam's convergence is: define pseudo-linear system → bound singular values of transformed Hessian → analyze decay rate → ensure continuous descent → derive iteration complexity
- Design tradeoffs: The coordinate-dependent δ improves dependence on κ but introduces additional dimension-dependent terms. The choice between diagonal, diagonally dominant, and general Hessians trades off between simpler analysis and broader applicability.
- Failure signatures: Adam may fail to converge to the global minimum when δ is too small relative to the step-size and gradient norms. For sufficiently non-diagonal Hessians, Adam can be worse than gradient descent even when d ≪ O(κ^(1/3)).
- First 3 experiments:
  1. Verify Theorem 2 by implementing Adam on diagonal quadratics with varying κ and d, measuring iteration count to reach ε accuracy.
  2. Test Theorem 4 by constructing ν-diagonally-dominant matrices and comparing Adam vs gradient descent performance.
  3. Demonstrate Theorem 6 by running Adam with different δ values on a simple quadratic and observing convergence behavior.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the preconditioning effect of Adam scale for functions that are neither quadratics nor satisfy per-coordinate smoothness?
- Basis in paper: [inferred] The paper focuses on quadratic functions and functions with per-coordinate smoothness, leaving the general case unexplored.
- Why unresolved: The paper's analysis is limited to specific function classes, and the techniques used may not generalize to more complex, non-structured functions.
- What evidence would resolve it: A theoretical analysis or empirical study of Adam's preconditioning effect on general non-convex functions, possibly involving techniques from non-convex optimization theory.

### Open Question 2
- Question: What is the optimal choice of hyperparameters (α, β2, δk, ϕ) for Adam to maximize its preconditioning effect?
- Basis in paper: [explicit] The paper derives specific values for these hyperparameters but acknowledges that they depend on problem-specific quantities, suggesting room for optimization.
- Why unresolved: The paper provides theoretical bounds on the hyperparameters but doesn't offer a practical method for choosing them in real-world scenarios.
- What evidence would resolve it: A method for adaptively tuning these hyperparameters based on the observed gradients or problem characteristics, possibly involving online learning or meta-learning techniques.

### Open Question 3
- Question: Can the negative results about Adam's performance for non-diagonal Hessians be mitigated through modifications to the algorithm or its hyperparameters?
- Basis in paper: [explicit] The paper shows that Adam can be worse than GD for certain non-diagonal Hessians, even when the dimension is small.
- Why unresolved: The paper's analysis is based on the standard Adam update rule and doesn't explore potential modifications to improve its performance.
- What evidence would resolve it: A modified version of Adam or a different hyperparameter setting that consistently outperforms GD for a wide range of non-diagonal Hessians, possibly involving adaptive step-sizes or momentum terms.

## Limitations

- Analysis is restricted to deterministic setting with exact gradients, not the stochastic setting typical in deep learning
- Bounds are probabilistic (holding with constant probability over initialization) rather than deterministic guarantees
- Results for general Hessians rely on diagonally dominant assumptions which may not hold in practice for many machine learning problems

## Confidence

- High confidence in the characterization of Adam's convergence to non-trivial fixed points (Theorem 6) - the mechanism is well-understood and the proof is rigorous
- Medium confidence in the iteration complexity bounds for diagonal and diagonally dominant Hessians - the analysis is mathematically sound but depends on specific parameter choices
- Medium confidence in the extension to per-coordinate smoothness conditions - while theoretically interesting, the practical applicability is limited by the dimension-dependent quantity that must be small

## Next Checks

1. **Empirical validation of fixed point behavior**: Implement Adam with varying δ values on a simple diagonal quadratic and verify that small δ values lead to convergence to non-zero fixed points, while larger δ values recover gradient descent behavior.

2. **Dimension scaling experiments**: Systematically vary d and κ across the threshold regions (d = O(κ^(1/3)) for general Hessians, d = O(κ) for diagonal Hessians) to empirically verify where Adam outperforms gradient descent.

3. **Jacobi preconditioning comparison**: For diagonally dominant Hessians, compare Adam's performance against explicitly preconditioned gradient descent using D^(-1/2) to isolate whether Adam's advantage comes from preconditioning or other effects.