---
ver: rpa2
title: 'MatchNAS: Optimizing Edge AI in Sparse-Label Data Contexts via Automating
  Deep Neural Network Porting for Mobile Deployment'
arxiv_id: '2402.13525'
source_url: https://arxiv.org/abs/2402.13525
tags:
- training
- data
- network
- matchnas
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MatchNAS addresses the challenge of deploying deep neural networks
  on resource-constrained mobile devices in sparse-label data contexts. It combines
  semi-supervised learning and neural architecture search to optimize network portability
  and performance.
---

# MatchNAS: Optimizing Edge AI in Sparse-Label Data Contexts via Automating Deep Neural Network Porting for Mobile Deployment

## Quick Facts
- arXiv ID: 2402.13525
- Source URL: https://arxiv.org/abs/2402.13525
- Reference count: 40
- Achieves up to 20% accuracy improvement on Cifar100 with 4000 labeled examples and 15M FLOPs

## Executive Summary
MatchNAS addresses the challenge of deploying deep neural networks on resource-constrained mobile devices when labeled training data is scarce. The method combines semi-supervised learning with neural architecture search to optimize both network performance and portability. By training a supernet using both labeled and unlabeled data, with the largest subnet generating high-quality pseudo-labels for smaller subnets, MatchNAS achieves superior accuracy-latency trade-offs on multiple smartphone platforms compared to existing methods.

## Method Summary
MatchNAS transforms a pre-trained cloud network into a supernet containing dynamic subnets with varying architectures. The method employs semi-supervised training where the largest subnet generates pseudo-labels for unlabeled data, which are then used to train all subnets simultaneously through weight sharing. After training, zero-shot neural architecture search identifies optimal subnets for specific hardware platforms under resource constraints. The approach narrows the search space before supernet training to improve optimization efficiency while maintaining architectural diversity.

## Key Results
- Achieves up to 20% accuracy improvement on CIFAR-100 with 4000 labeled examples and 15M FLOPs
- Outperforms state-of-the-art methods in latency-accuracy trade-offs across multiple smartphone platforms
- Demonstrates effectiveness across multiple datasets (CIFAR-10/100, CUB-200, Stanford Cars, Pascal VOC)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using the largest subnet to generate pseudo-labels for smaller subnets improves the quality of semi-supervised learning in sparse-label contexts.
- Mechanism: The largest subnet has more parameters and computational capacity, enabling it to learn better representations from unlabeled data. When this subnet generates pseudo-labels, these labels are of higher quality compared to those produced by smaller subnets. This knowledge distillation approach transfers the "knowledge" from the larger model to the smaller ones, improving their performance.
- Core assumption: The largest subnet can consistently produce higher-quality pseudo-labels than smaller subnets due to its greater capacity to capture patterns in unlabeled data.
- Evidence anchors:
  - [abstract]: "Specifically, we simultaneously optimise a large network family using both labelled and unlabelled data and then automatically search for tailored networks for different hardware platforms."
  - [section 3.2]: "Motivated by knowledge distillation strategy [17], which leverages a large teacher network to 'teach' small student networks, our core idea is to select the largest subnet to produce better pseudo-labels for other subnets in each training iteration."
  - [corpus]: No direct corpus evidence supporting this mechanism. The claim appears to be a novel contribution not extensively validated in related literature.

### Mechanism 2
- Claim: Training a supernet with semi-supervised learning enables efficient search for optimal subnets across multiple hardware platforms.
- Mechanism: The supernet contains a family of subnets with varying architectures. By training this supernet with both labeled and unlabeled data, all subnets learn from the same weight-sharing framework. After training, zero-shot NAS can efficiently search for optimal subnets for different platforms without additional training, leveraging the architectural information embedded in the supernet.
- Core assumption: The supernet training effectively optimizes all subnets simultaneously through weight sharing, and the architectural information captured is sufficient for zero-shot NAS to identify high-performing subnets.
- Evidence anchors:
  - [abstract]: "We simultaneously optimise a large network family using both labelled and unlabelled data and then automatically search for tailored networks for different hardware platforms."
  - [section 2.1]: "One-shot NAS techniques [5, 14, 30, 37, 40] proposed to reduce NAS cost by using weight-sharing for all networks in the search space A."
  - [section 3.3]: "We leverage techniques in zero-shot NAS to search subnets for mobile porting. As mentioned in Section 2.1, zero-shot NAS designs an architectural-based metric for network performance evaluation without any parameter training."
  - [corpus]: The corpus contains related work on one-shot NAS and zero-shot NAS, supporting the general approach, though specific combinations with semi-supervised learning are not evident.

### Mechanism 3
- Claim: Narrowing the search space before supernet training reduces optimization difficulty while maintaining subnet diversity.
- Mechanism: By using zero-shot NAS to select a subset of high-quality architectures before training, the supernet optimization becomes less challenging. The reduced search space means fewer competing subnet architectures, making it easier to optimize the shared weights while still providing diverse options for deployment.
- Core assumption: The zero-shot NAS selection process can identify a representative subset of high-quality architectures that will yield good performance after supernet training.
- Evidence anchors:
  - [section 3.4]: "We hypothesise that our semi-supervised-supernet training will also benefit from a smaller search space. We attempt to leverage the zero-shot NAS techniques to automatically narrow A before supernet training."
  - [section 4.5]: "Clearly, MatchNAS‚Ä† reports about 2% higher accuracy compared to MatchNAS. These phenomena verify our hypothesis in Section 3.4 that a narrower search space can be optimized more easily than a large one while the number of available subnets decreases."
  - [corpus]: No direct corpus evidence for this specific mechanism. The concept of narrowing search spaces is known, but the specific combination with zero-shot NAS selection before training appears novel.

## Foundational Learning

- Concept: Neural Architecture Search (NAS) and one-shot NAS
  - Why needed here: MatchNAS builds upon one-shot NAS principles to train a supernet containing multiple subnets. Understanding how weight sharing works in one-shot NAS is crucial for grasping how MatchNAS optimizes all subnets simultaneously.
  - Quick check question: How does weight sharing in one-shot NAS enable efficient training of multiple network architectures simultaneously?

- Concept: Semi-supervised learning and pseudo-labeling
  - Why needed here: MatchNAS combines NAS with semi-supervised learning techniques. The quality of pseudo-labels generated by the largest subnet is critical to the method's success, requiring understanding of how pseudo-labeling works and its limitations.
  - Quick check question: What factors affect the quality of pseudo-labels in semi-supervised learning, and how might these factors differ between large and small networks?

- Concept: Knowledge distillation
  - Why needed here: The approach of using a larger model to generate pseudo-labels for smaller models is inspired by knowledge distillation. Understanding this concept helps explain why using the largest subnet as a "teacher" can benefit smaller subnets.
  - Quick check question: How does knowledge distillation typically work, and what are the key differences between traditional knowledge distillation and the approach used in MatchNAS?

## Architecture Onboarding

- Component map:
  Cloud-based pre-trained network (teacher model) -> Supernet containing dynamic subnets -> Semi-supervised training loop with labeled and unlabeled data -> Zero-shot NAS module for subnet selection -> Hardware-specific resource constraints (FLOPs, latency) -> Mobile deployment pipeline

- Critical path:
  1. Transform pre-trained cloud network to supernet
  2. Semi-supervised-NAS training with largest subnet generating pseudo-labels
  3. Zero-shot search for optimal subnets under resource constraints
  4. Deploy selected subnets to target mobile platforms

- Design tradeoffs:
  - Search space size vs. optimization difficulty: Larger search spaces provide more options but are harder to optimize
  - Pseudo-label quality vs. quantity: Higher confidence thresholds produce fewer but better quality pseudo-labels
  - Training cost vs. performance: Semi-supervised training requires more computation but improves performance in label-scarce scenarios

- Failure signatures:
  - Poor performance across all subnets: Indicates issues with supernet training or pseudo-label quality
  - Subnets with similar architectures but vastly different performance: Suggests weight sharing is not working effectively
  - Degradation in performance when switching from labeled to unlabeled data: Indicates the semi-supervised component is not working as intended

- First 3 experiments:
  1. Verify supernet training: Train the supernet on a small dataset with limited labels and check if all subnets improve together
  2. Test pseudo-label quality: Compare pseudo-labels generated by largest vs. smallest subnets on unlabeled data
  3. Validate zero-shot search: Run zero-shot search on a trained supernet and verify selected subnets perform well on validation data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would MatchNAS perform when extended to transform-based networks and diffusion models beyond convolutional architectures?
- Basis in paper: [explicit] The paper states: "The primary limitation of our method originates from the predefined network search space. Even though we have demonstrated the effectiveness of MatchNAS on convolutional network structures, future research should explore extending MatchNAS to diverse tasks, such as transform-based networks and diffusion models."
- Why unresolved: The current implementation is specifically designed for convolutional neural networks, and the authors acknowledge this as a limitation that requires further investigation.
- What evidence would resolve it: Successful implementation and performance evaluation of MatchNAS on transform-based networks (like ViT) and diffusion models, demonstrating comparable or superior performance to existing methods in these domains.

### Open Question 2
- Question: What is the optimal confidence threshold value for pseudo-label generation across different datasets and network architectures?
- Basis in paper: [explicit] The paper states: "We validate the effectiveness of the confidence threshold in our SSL-based supernet training, and Figure 6 (a) show report a comparison of subnet performance with different values of ùúè. ùúè = 0.95 report a accuracy improvement of about 1.5% compared to ùúè = 0.0." However, this was only tested on specific datasets.
- Why unresolved: The paper only provides results for one confidence threshold value (0.95) and one comparison point (0.0), without exploring the full range of potential threshold values or their impact across different data domains.
- What evidence would resolve it: Comprehensive ablation studies testing various confidence threshold values (e.g., 0.7, 0.8, 0.9, 0.95, 0.99) across multiple datasets and network architectures to determine optimal values for different scenarios.

### Open Question 3
- Question: How does MatchNAS scale to extremely resource-constrained edge devices like IoT sensors or on-chip AI systems?
- Basis in paper: [inferred] The paper mentions: "Furthermore, while this paper concentrates on mobile AI, it is important to note that there are numerous other lightweight AI platforms, such as IoT AI and on-chip AI, which warrant further investigation."
- Why unresolved: The current evaluation focuses on smartphones with varying capabilities, but does not address the unique challenges of ultra-low power, memory-constrained devices common in IoT applications.
- What evidence would resolve it: Performance evaluation of MatchNAS on IoT devices and on-chip AI systems, including metrics for power consumption, memory usage, and latency under extreme resource constraints.

## Limitations
- Scalability to larger datasets remains uncertain due to computational overhead of pseudo-label generation
- Performance degradation potential when unlabeled data distribution differs from labeled data (domain shift)
- Results primarily demonstrated on MobileNetV3 variants, generalizability to other architectures unclear

## Confidence
- High confidence in the core methodology combining semi-supervised learning with neural architecture search
- Medium confidence in the specific pseudo-label generation mechanism and its quality gains
- Low confidence in scalability claims for real-world deployment and resource-constrained edge devices

## Next Checks
1. Evaluate MatchNAS on a significantly larger dataset (e.g., ImageNet-1K with 10% labels) to assess scalability and computational efficiency at scale
2. Test performance when unlabeled data has different characteristics than labeled data to evaluate robustness to domain shift
3. Implement MatchNAS with alternative backbone architectures (e.g., EfficientNet, RegNet) to verify the approach generalizes beyond MobileNetV3