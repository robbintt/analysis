---
ver: rpa2
title: Motif distribution and function of sparse deep neural networks
arxiv_id: '2403.00974'
source_url: https://arxiv.org/abs/2403.00974
tags:
- network
- networks
- motifs
- motif
- pruning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how the internal connectivity structure of sparse
  deep neural networks encodes their function, using network motif theory. The authors
  analyze 350 DNNs trained to predict flight control parameters for a bio-mechanical
  insect model, with varying levels of sparsity achieved through iterative pruning.
---

# Motif distribution and function of sparse deep neural networks

## Quick Facts
- arXiv ID: 2403.00974
- Source URL: https://arxiv.org/abs/2403.00974
- Reference count: 40
- Primary result: Enforced sparsity in DNNs causes convergence to similar connectivity patterns characterized by motif distributions

## Executive Summary
This paper investigates how the internal connectivity structure of sparse deep neural networks encodes their function using network motif theory. The authors analyze 350 DNNs trained to predict flight control parameters for a bio-mechanical insect model, with varying levels of sparsity achieved through iterative pruning. They develop efficient algorithms to count second- and third-order motifs in these feed-forward networks and calculate their significance using z-scores. The results show that as networks become sparser through pruning, they converge to similar connectivity patterns characterized by specific over- and under-represented motifs.

## Method Summary
The study trains 350 DNNs in parallel on bio-mechanical insect flight dynamics data using early stopping and Adam optimization. Networks are then pruned iteratively using magnitude-based pruning, creating varying sparsity levels from 0% to 98%. The authors develop custom algorithms to count second- and third-order motifs by exploiting the feed-forward network structure and binary masks representing pruned connections. For each pruned network, 1000 equivalent random networks are generated to calculate z-scores and determine motif significance. The analysis focuses on comparing motif distributions across networks and sparsity levels to identify patterns that encode network function.

## Key Results
- Enforced sparsity causes DNNs to converge to similar connectivity patterns as characterized by their motif distributions
- At high sparsity levels, certain motifs become highly over-represented (2nd-order converging, 3rd-order converging, bi-parallel) while others become under-represented
- Some motifs show monotonic increases in significance with sparsity, while others level off at high sparsity levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Enforced sparsity during DNN training drives convergence to similar connectivity patterns across differently initialized networks.
- Mechanism: Pruning removes weights below a magnitude threshold, reducing redundancy and leaving only essential connections. This sparsification process biases the remaining structure toward statistically significant sub-graphs (motifs), creating consistent patterns across networks trained on the same task.
- Core assumption: The essential functional connectivity can be preserved despite significant pruning, and this preserved connectivity exhibits characteristic motif distributions.
- Evidence anchors:
  - [abstract] "enforced sparsity causes DNNs to converge to similar connectivity patterns as characterized by their motif distributions"
  - [section] "pruning can reduce the number of parameters in a trained DNN by as much as 93%" [2, 28]
  - [corpus] Weak correlation with pruning literature; no direct motif analysis evidence in neighbors

### Mechanism 2
- Claim: Motif significance increases with sparsity because sparse networks have fewer ways to arrange connections, making statistically significant patterns more pronounced.
- Mechanism: As networks become sparser, the ratio of observed sub-graph occurrences to possible arrangements increases for over-represented motifs and decreases for under-represented motifs, amplifying their z-scores.
- Core assumption: The remaining connections after pruning are not randomly distributed but follow task-relevant patterns that manifest as network motifs.
- Evidence anchors:
  - [abstract] "As networks are pruned and become more sparse, the over- or under-representation of a given motif becomes more pronounced"
  - [section] "At high levels of sparsity, network motifs are either highly over- or under-represented"
  - [corpus] No direct evidence in neighbors about sparsity-motif relationship

### Mechanism 3
- Claim: Certain motifs (converging, bi-parallel) show monotonic increases in significance because they represent fundamental information processing units required for the flight control task.
- Mechanism: These motifs likely encode critical computational patterns (e.g., integration of multiple inputs, parallel processing pathways) that become more prominent as redundant connections are removed.
- Core assumption: The flight control task requires specific computational structures that manifest as particular network motifs.
- Evidence anchors:
  - [abstract] "The significance of some motifs (2nd-order converging, 3rd-order converging, and bi-parallel) appear to increase monotonically"
  - [section] "The significance of both the 2nd-order diverging and bi-fan motifs levels-off at high levels of sparsity"
  - [corpus] No direct evidence in neighbors about task-specific motif requirements

## Foundational Learning

- Concept: Network motif theory and significance calculation
  - Why needed here: The paper uses z-scores to determine whether specific sub-graph patterns occur more frequently than expected by chance, which is fundamental to characterizing network structure
  - Quick check question: How is the z-score for a motif calculated, and what does it tell us about the motif's significance?

- Concept: Feed-forward network architecture and sparse connectivity matrices
  - Why needed here: The algorithms developed specifically exploit the feed-forward structure and binary masks representing pruned connections to efficiently count motifs
  - Quick check question: How does the feed-forward structure simplify motif counting compared to recurrent networks?

- Concept: Pruning methodologies and their impact on network function
  - Why needed here: The study uses magnitude-based pruning, and understanding how different pruning strategies affect both performance and connectivity structure is crucial for interpreting results
  - Quick check question: What is the difference between structured and magnitude-based pruning, and how might each affect motif distributions?

## Architecture Onboarding

- Component map: Data generation -> Network training -> Pruning pipeline -> Motif counting -> Significance analysis -> Pattern identification
- Critical path: Data generation → Network training → Pruning → Motif counting → Significance analysis → Pattern identification
- Design tradeoffs: Computational efficiency (parallel training of 350 networks) vs. potential bias from shared training process; comprehensive motif analysis vs. computational cost of generating random networks
- Failure signatures: Poor pruning performance (early stopping triggers) suggests pruning removed critical connections; inconsistent motif distributions across networks suggest random connectivity rather than functional structure
- First 3 experiments:
  1. Vary pruning thresholds systematically to map the relationship between sparsity level and motif significance
  2. Compare motif distributions across different pruning strategies (random vs. magnitude-based) to isolate pruning effects
  3. Test different initial network architectures (varying depth/width) to determine if motif convergence is architecture-dependent

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different pruning techniques (e.g., random pruning vs. magnitude-based pruning) affect the motif distributions and performance of sparse neural networks?
- Basis in paper: [explicit] The authors discuss that different pruning techniques could affect motif distributions and suggest comparing random pruning to magnitude-based pruning as a potential experiment.
- Why unresolved: The study only used magnitude-based pruning, so direct comparisons with other pruning methods were not performed.
- What evidence would resolve it: Systematic comparison of motif distributions and network performance across different pruning techniques (random, structured, magnitude-based) on the same network architectures and tasks.

### Open Question 2
- Question: What is the minimum number of chain motifs required for a feed-forward network to maintain its functional performance as it becomes increasingly sparse?
- Basis in paper: [inferred] The authors observe that chain motifs become increasingly rare with pruning but suggest a minimum number of chain sub-graphs may be necessary for network function, noting that significance levels for chain motifs level off at high sparsity.
- Why unresolved: While the authors note a leveling-off in chain motif significance, they don't determine the absolute minimum number needed for functional performance.
- What evidence would resolve it: Systematic experiments varying the minimum chain motif count required while measuring network performance degradation as sparsity increases.

### Open Question 3
- Question: How do motif distributions differ between fully-connected networks and their sparse counterparts when considering only high-activation connections and nodes?
- Basis in paper: [explicit] The authors suggest comparing motif makeup of fully-connected networks to sparse counterparts using only high-activation connections as an interesting experiment.
- Why unresolved: The study focused on sparse networks created through pruning, not comparing them to their original fully-connected versions using activation-based filtering.
- What evidence would resolve it: Direct comparison of motif distributions between fully-connected networks and their sparse versions, where both use only high-activation connections for motif counting.

### Open Question 4
- Question: How do motif distributions and their relationship to network function change in more complex neural network architectures like recurrent neural networks or residual networks?
- Basis in paper: [explicit] The authors acknowledge that their study was limited to feed-forward networks and suggest that more complex architectures contain different types of motifs (e.g., feedback motifs in RNNs).
- Why unresolved: The study only examined feed-forward networks, which are the simplest DNN architecture after perceptrons.
- What evidence would resolve it: Extending motif analysis to recurrent, residual, and other complex network architectures, comparing how different motif types relate to function in these architectures.

## Limitations
- The findings are based on a specific insect flight control task and may not generalize to other domains or pruning strategies
- Analysis is limited to second- and third-order motifs, potentially missing higher-order structural patterns that encode network function
- Computational cost of generating equivalent random networks limits the number of sparsity levels that can be thoroughly analyzed

## Confidence
- **High Confidence**: The fundamental relationship between sparsity and motif significance is well-supported by the mathematical framework of network motif theory and the empirical results showing consistent patterns across 350 networks
- **Medium Confidence**: The specific claim that certain motifs (converging, bi-parallel) increase monotonically with sparsity is supported by the data but requires further validation across different tasks and pruning methods
- **Low Confidence**: The assertion that motif distributions encode neural network function needs additional experimental validation beyond the flight control domain

## Next Checks
1. Test motif convergence patterns across different pruning strategies (structured vs. magnitude-based) to isolate pruning effects from general sparsity effects
2. Apply the motif analysis framework to networks trained on different tasks (e.g., image classification, language modeling) to assess domain generalizability
3. Extend motif counting to fourth-order motifs and analyze whether higher-order structural patterns show similar convergence behaviors at high sparsity levels