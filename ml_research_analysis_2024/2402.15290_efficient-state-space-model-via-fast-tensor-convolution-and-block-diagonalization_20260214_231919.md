---
ver: rpa2
title: Efficient State Space Model via Fast Tensor Convolution and Block Diagonalization
arxiv_id: '2402.15290'
source_url: https://arxiv.org/abs/2402.15290
tags:
- ssms
- ldnn
- neural
- state
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of efficiently modeling long
  sequences in deep learning, a bottleneck for existing models balancing performance
  and computational efficiency. The authors propose a new neural network called Linear
  Dynamics-embedded Neural Network (LDNN) based on multi-input multi-output (MIMO)
  state space models (SSMs).
---

# Efficient State Space Model via Fast Tensor Convolution and Block Diagonalization

## Quick Facts
- **arXiv ID**: 2402.15290
- **Source URL**: https://arxiv.org/abs/2402.15290
- **Reference count**: 40
- **One-line primary result**: LDNN achieves state-of-the-art performance on Long Range Arena benchmark while using 12.89% of LSTM's parameters and being 3.94× faster in training

## Executive Summary
This paper addresses the challenge of efficiently modeling long sequences in deep learning by proposing a novel Linear Dynamics-embedded Neural Network (LDNN) based on multi-input multi-output (MIMO) state space models (SSMs). The authors leverage continuous, discrete, and convolutional properties of SSMs to achieve fewer parameters, flexible inference, and efficient training. The key innovations are diagonalization of the system matrix and a "Disentanglement then Fast Fourier Transform (FFT)" strategy that reduces time complexity from O(LN H max{L, N}) to O(LN max{H, log L}). Experimental results on the Long Range Arena (LRA) benchmark demonstrate that LDNN matches state-of-the-art performance while outperforming Transformers and LSTM in terms of both accuracy and efficiency.

## Method Summary
The paper proposes LDNN, a neural network architecture built on MIMO state space models that leverages diagonalization and fast tensor convolution for efficient long-sequence modeling. The method consists of three main components: (1) diagonalization of the system matrix A to decouple the MIMO SSM into independent scalar subsystems, reducing computational complexity; (2) a "Disentanglement then FFT" strategy that transforms the original convolution into separable operations, enabling FFT acceleration; and (3) parameterization of the system matrix Λ with an enforcing function to ensure state convergence while allowing learnable parameters. The authors implement LDNN in PyTorch and evaluate it on the LRA benchmark using AdamW optimizer with cross-entropy loss, testing various configurations including bidirectional and multi-head settings.

## Key Results
- LDNN achieves state-of-the-art performance on the Long Range Arena benchmark, matching or exceeding results from Transformers and LSTM
- The model uses only 12.89% of LSTM's parameters while achieving comparable or better accuracy
- LDNN is 3.94× faster than LSTM in training, demonstrating significant computational efficiency gains
- The "Disentanglement then FFT" strategy successfully reduces time complexity from O(LN H max{L, N}) to O(LN max{H, log L})

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diagonalization of the system matrix decouples the MIMO SSM into independent scalar subsystems, reducing computational complexity.
- Mechanism: By transforming the system matrix A into a diagonal matrix Λ, the authors eliminate the need to compute expensive matrix-vector products. Instead, each subsystem can be processed independently, and the time complexity drops from O(LN³) to O(LN) for kernel generation.
- Core assumption: The system matrix A has N real and distinct eigenvalues, allowing diagonalization via the Diagonalization Equivalence Lemma.
- Evidence anchors:
  - [abstract]: "The diagonalization of the system matrix first decouples the original system."
  - [section]: "Using diagonalization and 'Disentanglement then FFT' strategies, we require O(LN H) time complexity for the multiplication of Bu, and O(LN) time complexity for V."
  - [corpus]: Weak; no direct corpus evidence on diagonalization for SSMs.
- Break condition: If the system matrix A has repeated eigenvalues or is defective (non-diagonalizable), diagonalization fails and the complexity reduction is lost.

### Mechanism 2
- Claim: The 'Disentanglement then FFT' strategy transforms the original convolution into separable operations, enabling FFT acceleration.
- Mechanism: The authors reformulate the convolution K ∗ u as V ∗ Bu, where V is the system kernel and Bu is the input sequence projected by B. This allows disentangling the convolution into N independent univariate convolutions V:ii ∗ (Bu):i1, each of which can be computed efficiently via FFT.
- Core assumption: The convolution operation is associative with multiplication, allowing the reformulation from V B ∗ u to V ∗ Bu.
- Evidence anchors:
  - [abstract]: "Then a fast tensor convolution is proposed based on the fast Fourier transform."
  - [section]: "With this, we successfully disentangle the original convolution into Eq. (14), where V:ii and (Bu):i1 are both univariate sequence. We can apply FFT to calculate Eq. (14) efficiently."
  - [corpus]: Weak; no direct corpus evidence on disentanglement strategies for SSMs.
- Break condition: If the system kernel V is not separable or the input projection Bu cannot be computed efficiently, the FFT acceleration becomes ineffective.

### Mechanism 3
- Claim: Parameterizing the system matrix Λ with an enforcing function ensures state convergence while allowing learnable parameters.
- Mechanism: The authors use an enforcing function f+ to parameterize Λ as -f+(Re(Λ)) + Im(Λ)i, ensuring all eigenvalues have negative real parts. This guarantees state convergence as proven in Proposition 1.
- Core assumption: The enforcing function f+ outputs positive real numbers and can be initialized randomly, with constants, or from structured matrices like the HiPPO matrix.
- Evidence anchors:
  - [abstract]: "The system matrix A can be initialized via the HiPPO, random or constant matrix."
  - [section]: "According to Proposition 1, we know that all elements in Λ must have negative real parts to ensure state convergence. Thus, we parameterize Λ with an enforcing function f+..."
  - [corpus]: Weak; no direct corpus evidence on enforcing functions for SSMs.
- Break condition: If the enforcing function is poorly chosen or the learning rate is inappropriate, the model may fail to converge or achieve suboptimal performance.

## Foundational Learning

- Concept: State Space Models (SSMs)
  - Why needed here: LDNN is built on MIMO SSMs, which provide a theoretical foundation for modeling long sequences with fewer parameters and flexible inference.
  - Quick check question: What are the three key representations of SSMs discussed in the paper (continuous, discrete, and convolutional)?

- Concept: Diagonalization of Matrices
  - Why needed here: Diagonalization is crucial for decoupling the MIMO SSM and reducing computational complexity.
  - Quick check question: Under what condition can a matrix be diagonalized, and what is the benefit of diagonalization for SSMs?

- Concept: Fast Fourier Transform (FFT)
  - Why needed here: FFT is used to accelerate the convolution operation in the convolutional representation of SSMs.
  - Quick check question: How does FFT reduce the time complexity of convolution from O(L²) to O(L log L) for univariate sequences?

## Architecture Onboarding

- Component map: Linear Dynamic Model -> Discrete SSMs -> Convolutional SSMs -> Efficient Convolutional SSMs -> LDNN
- Critical path:
  1. Initialize parameters Λ, B', C', D', and ∆
  2. Compute the system kernel V using diagonalization
  3. Project the input sequence u using B to obtain Bu
  4. Disentangle the convolution K ∗ u into V ∗ Bu
  5. Apply FFT to compute the convolution efficiently
  6. Pass the result through activation functions and additional layers
  7. Train the model using backpropagation
- Design tradeoffs:
  - Diagonalization vs. Full Matrix: Diagonalization reduces computational complexity but requires the system matrix to be diagonalizable.
  - Bidirectional vs. Unidirectional: Bidirectional setting improves non-causal modeling but doubles the computational cost.
  - Multi-Head vs. Single-Head: Multi-head setting increases diversity and optionality but may increase memory usage.
- Failure signatures:
  - Poor performance on long sequences: May indicate issues with the state kernel computation or FFT acceleration.
  - Convergence issues during training: May be caused by improper initialization of Λ or inappropriate learning rates.
  - High memory usage: May be due to the multi-head setting or large input dimensions.
- First 3 experiments:
  1. Implement a basic LDNN layer with diagonal system matrix and test on a small sequence classification task.
  2. Compare the performance of LDNN with and without the 'Disentanglement then FFT' strategy on a medium-sized sequence modeling task.
  3. Evaluate the impact of different initialization strategies for Λ (random, HiPPO, constant) on the model's accuracy and convergence speed.

## Open Questions the Paper Calls Out

- **Question**: How does the initialization strategy of the system matrix Λ impact the convergence and final accuracy of LDNN across different long-sequence tasks?
  - Basis in paper: [explicit] The paper states that different initialization strategies (random, HiPPO, constant) with proper learning rates can achieve competitive accuracy, but notes that improper learning rates can lead to suboptimal performance, as seen in the ablation studies.
  - Why unresolved: The paper does not provide a comprehensive analysis of the optimal initialization strategy for different types of long-sequence tasks (e.g., text, image, audio). The impact of initialization on convergence speed and final accuracy remains unclear.
  - What evidence would resolve it: A systematic study comparing the performance of LDNN with different initialization strategies (random, HiPPO, constant) across various long-sequence tasks, analyzing convergence speed and final accuracy.

- **Question**: How does the number of heads in Multi-Head LDNN affect the model's performance and computational efficiency in long-sequence modeling tasks?
  - Basis in paper: [explicit] The paper mentions that Multi-Head LDNN increases optionality and diversity in model selection but does not increase total computational cost with proper choice of N and h. It suggests h = 4 as a default value but notes that different numbers of heads can yield different accuracy.
  - Why unresolved: The paper does not provide a detailed analysis of the impact of the number of heads on model performance and computational efficiency. The optimal number of heads for different long-sequence tasks remains unclear.
  - What evidence would resolve it: A comprehensive study evaluating the performance and computational efficiency of Multi-Head LDNN with varying numbers of heads (e.g., 2, 4, 8) across different long-sequence tasks.

- **Question**: How does the bidirectional setting of LDNN impact its performance in non-causal modeling tasks, and what are the trade-offs compared to unidirectional models?
  - Basis in paper: [explicit] The paper proposes bidirectional LDNN for non-causal cases but notes that experiments with bidirectional settings did not show improved performance in non-causal tasks.
  - Why unresolved: The paper does not provide a thorough analysis of the bidirectional setting's impact on performance in non-causal tasks. The trade-offs between bidirectional and unidirectional models remain unclear.
  - What evidence would resolve it: A detailed comparison of bidirectional and unidirectional LDNN performance in non-causal tasks, analyzing accuracy, computational efficiency, and any trade-offs.

## Limitations

- The diagonalization step assumes the system matrix has N real and distinct eigenvalues, which may not always hold in practice, limiting the method's applicability.
- The "Disentanglement then FFT" strategy is not extensively validated in the literature, and its effectiveness may depend on the specific problem and dataset.
- The enforcing function f+ used to parameterize Λ is not well-explored, and its impact on model performance is unclear.

## Confidence

- **High confidence**: The general approach of using SSMs for long-sequence modeling and the benefits of diagonalization for reducing computational complexity.
- **Medium confidence**: The specific implementation of the "Disentanglement then FFT" strategy and its effectiveness in practice.
- **Low confidence**: The impact of the enforcing function f+ on model performance and the generalizability of the results to other datasets and tasks.

## Next Checks

1. **Diagonalizability analysis**: Analyze the eigenvalues of the system matrix A on a subset of the LRA tasks to verify the assumption of real and distinct eigenvalues. If the matrix is not diagonalizable, investigate alternative approaches to reduce computational complexity.

2. **Ablation study on "Disentanglement then FFT"**: Perform an ablation study to evaluate the effectiveness of the "Disentanglement then FFT" strategy compared to other convolution acceleration methods, such as Winograd or FFT-based approaches.

3. **Hyperparameter sensitivity analysis**: Conduct a sensitivity analysis of the model's performance with respect to key hyperparameters, such as the learning rate, dropout rate, and initialization strategy for Λ. This will help identify the most critical hyperparameters and their optimal ranges for different tasks.