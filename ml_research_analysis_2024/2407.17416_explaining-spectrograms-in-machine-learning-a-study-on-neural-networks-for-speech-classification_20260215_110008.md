---
ver: rpa2
title: 'Explaining Spectrograms in Machine Learning: A Study on Neural Networks for
  Speech Classification'
arxiv_id: '2407.17416'
source_url: https://arxiv.org/abs/2407.17416
tags:
- speech
- spectrograms
- vowel
- classification
- frequency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores how neural networks interpret spectrograms
  for speech classification, focusing on vowel classification and voiced-unvoiced
  distinction. Using class activation mapping (CAM), the research identifies frequency
  regions important for classification and compares them with linguistic knowledge.
---

# Explaining Spectrograms in Machine Learning: A Study on Neural Networks for Speech Classification

## Quick Facts
- arXiv ID: 2407.17416
- Source URL: https://arxiv.org/abs/2407.17416
- Authors: Jesin James; Balamurali B. T.; Binu Abeysinghe; Junchen Liu
- Reference count: 32
- Primary result: ResNet-101 models achieve >96% vowel classification and >98% voiced-unvoiced classification accuracy while focusing on linguistically-relevant formant regions

## Executive Summary
This study investigates how neural networks interpret spectrograms for speech classification by comparing model behavior with linguistic knowledge. Using class activation mapping (CAM), the researchers analyze which frequency regions ResNet-101 models attend to when classifying vowels and distinguishing voiced from unvoiced speech. The study finds that networks focus on formant regions (below 4000 Hz) for vowel classification, aligning with linguistic interpretation, though high-frequency components also influence decisions in some cases. For voiced-unvoiced classification, models consistently focus on low-frequency regions corresponding to fundamental frequency.

## Method Summary
The researchers extracted vowel segments from the LJSpeech corpus and generated spectrograms resized to 224x224 pixels. They fine-tuned pre-trained ResNet-101 models on three datasets: full-frequency spectrograms for vowel classification, formant-limited (4000 Hz) spectrograms for vowel classification, and spectrograms for voiced-unvoiced classification. Class activation mapping (CAM) was used to generate heatmaps showing which regions the network attended to for each classification decision, which were then compared against linguistic knowledge of formant locations and acoustic features.

## Key Results
- ResNet-101 models achieved >96% accuracy for vowel classification (5 vowels) and >98% accuracy for voiced-unvoiced classification
- CAM analysis revealed networks focus on formant regions (below 4000 Hz) for vowel classification, aligning with linguistic interpretation
- For voiced-unvoiced classification, CAMs showed models focus on low-frequency regions (<700 Hz) corresponding to fundamental frequency
- High-frequency components influenced decisions in some cases, particularly for high vowels /i/ and /u/

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The neural network identifies formant regions as discriminative features for vowel classification.
- Mechanism: By training on spectrograms, the ResNet-101 model learns to attend to frequency bands where formants (resonant frequencies of the vocal tract) are located, which align with linguistic knowledge of vowel identification.
- Core assumption: Formants are the primary acoustic cues used by both humans and neural networks for vowel classification.
- Evidence anchors:
  - [abstract] "CAM analysis revealed that networks focus on formant regions, aligning with linguistic interpretation"
  - [section] "the high vowels /i/ and /u/ used both the formant region (<4000 Hz) and spectral shape characteristics...low-back /A/ and mid vowel /Ç/ use only the formant region"
- Break condition: If noise in spectrograms causes the network to rely on high-frequency components instead of formants, leading to misclassification.

### Mechanism 2
- Claim: The model can distinguish voiced from unvoiced speech by detecting the presence or absence of fundamental frequency energy.
- Mechanism: CAMs show the model focuses on low-frequency regions (<700 Hz), which correspond to the fundamental frequency range absent in unvoiced sounds.
- Core assumption: The fundamental frequency is the key acoustic difference between voiced and unvoiced speech.
- Evidence anchors:
  - [abstract] "CAM analysis revealed that networks focus on formant regions...though high-frequency components also influenced decisions in some cases"
  - [section] "CAMs revealed that the region of importance for unvoiced speech predominantly lies below 700 Hz...corresponds to the general location of fundamental frequency, that are typically absent in non-voiced speech"
- Break condition: If the model relies on other spectral features (e.g., high-frequency noise) rather than fundamental frequency, reducing classification accuracy.

### Mechanism 3
- Claim: Limiting frequency range to 4000 Hz improves focus on linguistically relevant features and reduces reliance on potentially noisy high-frequency components.
- Mechanism: By restricting input to the formant region, the model is forced to attend to features that linguists use, improving interpretability and potentially reducing misclassification from irrelevant high-frequency noise.
- Core assumption: High-frequency components in the spectrograms are either uninformative or noisy for vowel classification.
- Evidence anchors:
  - [abstract] "CAM analysis revealed that networks focus on formant regions, aligning with linguistic interpretation"
  - [section] "When the frequency range was limited to 4000 Hz, majority of the vowels focused on the first and second formant frequency regions...high frequencies are also considered in the decision making in some cases, which may not be needed depending on the task at hand"
- Break condition: If important discriminative features for certain vowels exist above 4000 Hz, limiting the frequency range would degrade performance.

## Foundational Learning

- Concept: Spectrogram interpretation in linguistics
  - Why needed here: Understanding how linguists read spectrograms (formants, vowel duration, spectral shape) is essential to compare with what the neural network learns.
  - Quick check question: What acoustic features do linguists use to distinguish /i/ from /u/ in a spectrogram?

- Concept: Formant theory in phonetics
  - Why needed here: Formants are the primary acoustic cues for vowel identification; knowing their frequency ranges helps interpret CAM results.
  - Quick check question: What is the typical frequency range for the first four formants in American English vowels?

- Concept: Class activation mapping (CAM)
  - Why needed here: CAM is the technique used to visualize which regions of the spectrogram the network attends to for classification.
  - Quick check question: How does CAM generate a heatmap showing important regions for a specific class prediction?

## Architecture Onboarding

- Component map: Spectrogram images (224x224) -> ResNet-101 (pre-trained on ImageNet) -> Class probabilities -> CAM heatmaps
- Critical path:
  1. Extract vowel segments from LJSpeech corpus
  2. Preprocess into spectrograms (resize to 224x224)
  3. Fine-tune ResNet-101 on dataset
  4. Generate CAMs for each class
  5. Analyze CAMs against linguistic knowledge
- Design tradeoffs:
  - Using pre-trained ResNet-101 vs training from scratch: Faster convergence, but may inherit ImageNet biases
  - Limiting frequency to 4000 Hz: Improves interpretability but may lose information
  - CAM resolution: 224x224 provides balance between detail and computational cost
- Failure signatures:
  - Misclassifications not explained by CAM overlap (e.g., /i/ vs /Ç/)
  - CAMs focusing on high-frequency regions when formants should dominate
  - Confusion matrix showing consistent errors between specific vowel pairs
- First 3 experiments:
  1. Train ResNet-101 on full-frequency spectrograms for vowel classification
  2. Generate CAMs and compare important regions with linguistic knowledge
  3. Repeat with frequency-limited (4000 Hz) spectrograms and analyze changes in CAMs and accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do neural networks' focus on high-frequency components for vowel classification relate to potential noise in the dataset?
- Basis in paper: [explicit] The paper notes that high frequencies may be a result of noise in the database, and high-frequency components were important for some vowel classifications.
- Why unresolved: The paper acknowledges this possibility but does not investigate whether the high-frequency focus is due to actual linguistic features or noise artifacts.
- What evidence would resolve it: Analyzing the dataset for noise patterns, conducting experiments with noise-filtered spectrograms, or comparing results across multiple datasets would clarify whether high-frequency focus is dataset-dependent or a general phenomenon.

### Open Question 2
- Question: Why do some vowels show asymmetric misclassification patterns (e.g., /i/ misclassified as /Ç/ more often than /Ç/ misclassified as /i/)?
- Basis in paper: [explicit] The paper observes asymmetric misclassification patterns but states they "cannot be explained using the CAMs alone" and suggests they might be due to "mislabelling or possibly noisy speech data."
- Why unresolved: The study does not investigate the underlying causes of these asymmetric patterns, which could stem from various factors including dataset imbalances, phonetic similarities, or annotation errors.
- What evidence would resolve it: Detailed analysis of the misclassified samples, cross-validation with human expert annotations, or statistical analysis of phonetic features across misclassified instances would help identify the root cause.

### Open Question 3
- Question: Would incorporating explicit linguistic knowledge about formant regions into the neural network architecture improve classification accuracy and reduce reliance on high-frequency components?
- Basis in paper: [inferred] The paper concludes that networks focus on formants similar to linguists but also use high frequencies in some cases, suggesting potential for improvement by incorporating linguistic knowledge.
- Why unresolved: The study explores what networks learn but does not test whether constraining the network to focus on linguistically relevant regions would improve performance.
- What evidence would resolve it: Training models with architectural constraints or attention mechanisms that prioritize formant regions, then comparing their performance and CAMs to unconstrained models would demonstrate the impact of linguistic guidance.

## Limitations

- The study uses only 5 vowels from the LJSpeech corpus, limiting generalizability to full American English vowel systems
- The assumption that formants below 4000 Hz are sufficient for vowel classification may not capture all linguistically relevant information
- The study doesn't fully explain when or why high-frequency components influence classification decisions

## Confidence

- **High confidence**: The finding that neural networks focus on formant regions for vowel classification, as this aligns well with established linguistic knowledge and is supported by multiple CAM visualizations across different vowel pairs.
- **Medium confidence**: The claim that limiting frequency range to 4000 Hz improves interpretability, as while the results show formant focus, the study doesn't demonstrate whether this limitation affects overall classification accuracy.
- **Low confidence**: The assertion that high-frequency components are unnecessary for vowel classification, as the study acknowledges their influence but doesn't thoroughly investigate their linguistic relevance or task-specific utility.

## Next Checks

1. **Cross-corpus validation**: Test the same models and CAM analysis on a different American English corpus (e.g., TIMIT or Buckeye) to verify that formant-focused classification patterns hold across different recording conditions and speaker demographics.

2. **Frequency range ablation study**: Systematically evaluate model performance and CAM patterns across multiple frequency cutoffs (2000 Hz, 3000 Hz, 4000 Hz, 5000 Hz) to determine the optimal balance between linguistic interpretability and classification accuracy.

3. **Speaker-dependent analysis**: Analyze CAM patterns separately for male and female speakers to determine if fundamental frequency differences affect the network's attention patterns, particularly for the voiced-unvoiced distinction task.