---
ver: rpa2
title: 'ProCIS: A Benchmark for Proactive Retrieval in Conversations'
arxiv_id: '2405.06460'
source_url: https://arxiv.org/abs/2405.06460
tags:
- retrieval
- proactive
- information
- conversation
- conversational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ProCIS is a large-scale benchmark for proactive document retrieval
  in multi-party conversations. It contains over 2.8 million conversations with Wikipedia
  links from Reddit, plus a human-annotated test set.
---

# ProCIS: A Benchmark for Proactive Retrieval in Conversations

## Quick Facts
- arXiv ID: 2405.06460
- Source URL: https://arxiv.org/abs/2405.06460
- Reference count: 40
- A benchmark for proactive document retrieval in multi-party conversations with 2.8M Reddit conversations and human-annotated test set

## Executive Summary
ProCIS introduces a large-scale benchmark for proactive document retrieval in multi-party conversations, containing over 2.8 million Reddit conversations with Wikipedia links plus a human-annotated test set. The authors develop npDCG, a novel evaluation metric that rewards early retrieval of relevant documents and penalizes delayed retrieval, making it more suitable for proactive systems than traditional metrics. They evaluate multiple retrieval methods and introduce LMGR (Language Model Grounded Retrieval), a framework that uses LLMs for candidate generation and grounding, achieving state-of-the-art performance in reactive retrieval scenarios.

## Method Summary
The benchmark is constructed from Reddit conversations containing Wikipedia links, which serve as relevance signals for document retrieval. The human-annotated test set provides ground truth relevance judgments. The npDCG metric extends traditional DCG by incorporating temporal aspects, rewarding systems that retrieve relevant documents earlier in conversations. The LMGR framework operates in two stages: first using LLMs to generate document candidates based on conversation context, then grounding these candidates through additional verification steps to ensure relevance and accuracy.

## Key Results
- ProCIS contains 2.8 million Reddit conversations with Wikipedia links
- npDCG metric outperforms traditional retrieval metrics for proactive systems
- LMGR framework achieves state-of-the-art performance in reactive retrieval
- Human-annotated test set ensures benchmark reliability

## Why This Works (Mechanism)
The benchmark's effectiveness stems from leveraging Wikipedia links as implicit relevance signals within natural conversations. The npDCG metric addresses the temporal dimension of proactive retrieval by rewarding systems that anticipate information needs before explicit queries are made. The LMGR framework's two-stage approach combines the generative power of LLMs with verification mechanisms to balance recall and precision in document retrieval.

## Foundational Learning

**Conversation Retrieval**: Understanding information needs in dialogue context
- Why needed: Multi-turn conversations contain implicit queries and evolving information needs
- Quick check: Verify conversation segmentation and context preservation methods

**Proactive Retrieval**: Anticipating user needs before explicit requests
- Why needed: Traditional reactive systems miss opportunities to provide timely information
- Quick check: Assess temporal reward structure in npDCG metric

**LLM Grounding**: Combining generative outputs with verification
- Why needed: LLMs can generate plausible but incorrect information
- Quick check: Validate grounding mechanisms against hallucinations

## Architecture Onboarding

**Component Map**: Conversation data → Preprocessing → LMGR candidate generation → Grounding verification → Retrieval output

**Critical Path**: Reddit conversation ingestion → Wikipedia link extraction → Human annotation pipeline → LMGR processing → npDCG evaluation

**Design Tradeoffs**: The framework trades computational overhead for accuracy by using two-stage LLM processing, balancing between generative coverage and grounded precision.

**Failure Signatures**: Performance degradation occurs when Wikipedia links don't capture true information needs, when LLM grounding fails to verify relevance, or when temporal reward structure in npDCG doesn't align with actual user satisfaction.

**First Experiments**:
1. Test LMGR with different LLM configurations to establish sensitivity
2. Validate npDCG performance on non-Reddit datasets
3. Measure inter-annotator agreement on human-labeled test set

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations

- npDCG metric generalizability across domains remains unverified
- Human annotation process lacks detailed validation and inter-annotator agreement metrics
- LMGR framework's performance variability based on LLM selection not thoroughly explored

## Confidence

**High Confidence**: Dataset construction methodology, basic statistics, technical implementation of retrieval methods, basic evaluation framework

**Medium Confidence**: npDCG metric effectiveness, comparative performance claims, limited ablation studies

**Low Confidence**: Generalizability across domains, robustness of human annotations, framework sensitivity to LLM variations

## Next Checks

1. Conduct cross-domain validation by applying npDCG to non-Reddit conversation datasets to verify metric generalizability and identify potential domain-specific biases.

2. Perform detailed ablation studies on the LMGR framework to understand the contribution of each component (LLM-based candidate generation vs. grounding) and test sensitivity to different LLM configurations.

3. Implement inter-annotator agreement analysis on the human-annotated test set and conduct annotation guideline refinement to ensure consistency and reliability of the benchmark's ground truth.