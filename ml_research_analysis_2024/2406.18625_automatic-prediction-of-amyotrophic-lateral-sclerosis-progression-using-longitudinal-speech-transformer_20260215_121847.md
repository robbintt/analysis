---
ver: rpa2
title: Automatic Prediction of Amyotrophic Lateral Sclerosis Progression using Longitudinal
  Speech Transformer
arxiv_id: '2406.18625'
source_url: https://arxiv.org/abs/2406.18625
tags:
- speech
- alst
- progression
- disease
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ALS transformer (ALST), a neural network-based
  predictor of amyotrophic lateral sclerosis (ALS) disease progression from longitudinal
  speech recordings. ALST takes advantage of high-quality pretrained speech features
  and longitudinal information in the recordings, and achieves 91.0% AUC, improving
  upon the previous best model by 5.6% relative on the ALS TDI dataset.
---

# Automatic Prediction of Amyotrophic Lateral Sclerosis Progression using Longitudinal Speech Transformer

## Quick Facts
- arXiv ID: 2406.18625
- Source URL: https://arxiv.org/abs/2406.18625
- Reference count: 0
- Primary result: ALST achieves 91.0% AUC for ALS progression prediction, improving upon previous best by 5.6% relative

## Executive Summary
This paper introduces ALS transformer (ALST), a neural network-based predictor of amyotrophic lateral sclerosis (ALS) disease progression from longitudinal speech recordings. ALST takes advantage of high-quality pretrained speech features and longitudinal information in the recordings, and achieves 91.0% AUC, improving upon the previous best model by 5.6% relative on the ALS TDI dataset. ALST is capable of fine-grained and interpretable predictions of ALS progression, especially for distinguishing between rarer and more severe cases. The code is publicly available.

## Method Summary
ALST uses pretrained speech encoders (wav2vec 2.0 or Whisper) to extract phoneme-level features from longitudinal speech recordings, then applies a transformer encoder with positional embeddings based on recording dates to capture temporal disease progression patterns. The model jointly predicts self-rated ALSFRS-R speech scores using a dual scorer with both classification (cross-entropy) and regression (MSE) objectives. The model is trained with Adam optimizer using warmup and multi-step learning rate decay, and evaluated on macro F1, AUC, Spearman ρ, Kendall τ, pairwise accuracy, and MSE metrics.

## Key Results
- ALST achieves 91.0% AUC, improving upon the previous best model by 5.6% relative on the ALS TDI dataset
- The model shows particular strength in predicting rarer and more severe ALS cases with fine-grained accuracy
- ALST demonstrates superior ranking performance (Kendall τ 0.785) compared to independent prediction approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using pretrained speech representations captures both linguistic and paralinguistic features needed for ALS progression prediction
- Mechanism: The ALST leverages high-quality speech features extracted from models like Whisper and wav2vec 2.0, which were pretrained on large speech datasets. These features encode both linguistic content and subtle acoustic variations that correlate with disease progression.
- Core assumption: Pretrained speech representations contain discriminative information for ALS severity that transfers effectively to the clinical prediction task
- Evidence anchors:
  - [abstract]: "By taking advantage of high-quality pretrained speech features and longitudinal information in the recordings, our best model achieves 91.0% AUC"
  - [section]: "recent advances in self-supervised speech representation [20, 21] and large-scale automatic speech recognition systems such as Whisper [22] have exploited the large amount of speech data online to obtain effective features that capture both linguistic and paralinguistic information in speech"
  - [corpus]: No direct evidence from corpus, but related work on dysarthria recognition suggests pretrained features help with speech disorders

### Mechanism 2
- Claim: Longitudinal modeling with transformer architecture captures temporal disease progression patterns better than independent predictions
- Mechanism: ALST treats ALS progression prediction as a sequence prediction problem, using positional embeddings based on recording dates to capture temporal relationships between observations from the same patient
- Core assumption: ALS progression follows consistent temporal patterns within patients that can be modeled through sequence learning
- Evidence anchors:
  - [abstract]: "ALST takes advantage of...longitudinal information in the recordings"
  - [section]: "Instead of predicting each ALSFRS-R score independently, we formulate the problem as a sequence prediction problem and predict the whole self-rated ALSFRS-R scores for each patient jointly"
  - [corpus]: No direct evidence from corpus, but related survival analysis work suggests temporal modeling is beneficial

### Mechanism 3
- Claim: Combining classification (CE) and regression (MSE) objectives provides both categorical accuracy and continuous progression estimation
- Mechanism: The training objective uses a weighted combination of cross-entropy for classification accuracy and MSE for continuous score prediction, balancing discrete class boundaries with fine-grained progression tracking
- Core assumption: ALS progression benefits from both categorical distinctions (for clinical interpretation) and continuous measures (for precision medicine)
- Evidence anchors:
  - [section]: "The training objective for ALST is then a linear combination of cross entropy and mean-squared error (MSE): LALST = nX i=1 |yˆi − yi|2 + λCE log pˆi(yi)"
  - [section]: "We also compute the MSE between predicted and true scores on the test set" to "provide a more fine-grained, continuous ALS progression measure"
  - [corpus]: No direct evidence from corpus, but multi-task learning approaches often benefit from combined objectives

## Foundational Learning

- Concept: Self-supervised speech representation learning
  - Why needed here: Understanding how models like wav2vec 2.0 and Whisper extract meaningful features from raw audio without explicit labels explains why their outputs are useful for downstream clinical tasks
  - Quick check question: What distinguishes self-supervised speech representations from traditional handcrafted features like MFCCs in terms of their ability to capture paralinguistic information?

- Concept: Transformer positional embeddings for temporal data
  - Why needed here: The ALST uses date-based positional embeddings to model longitudinal progression, so understanding how transformers handle sequence information is critical
  - Quick check question: How do relative positional embeddings differ from absolute ones, and why might order embeddings work better than day-count embeddings for this ALS dataset?

- Concept: Multi-task learning with combined classification and regression objectives
  - Why needed here: The ALST's training objective combines CE and MSE losses, requiring understanding of how to balance different loss functions for complementary tasks
  - Quick check question: What are the trade-offs between optimizing for classification accuracy versus regression MSE in a multi-task setting, and how does the λCE hyperparameter control this balance?

## Architecture Onboarding

- Component map: Raw audio -> Pretrained encoder (wav2vec 2.0/Whisper) -> Forced alignment -> Phoneme-level features -> Positional embeddings -> Transformer encoder -> Mean pooling -> Dual scorer (classification + regression) -> Output (categorical probabilities + continuous score)
- Critical path: The most important components for ALS progression prediction are the pretrained feature extractor, the longitudinal transformer encoder with date positional embeddings, and the dual scorer that combines classification and regression outputs
- Design tradeoffs:
  - Pretrained encoder choice: Whisper-medium vs large vs wav2vec 2.0 - smaller models may retain more paralinguistic information
  - Positional embeddings: Order-based vs day-based - simpler order embeddings may be more robust to irregular recording intervals
  - Loss weighting: λCE hyperparameter - requires balancing classification accuracy with regression precision
- Failure signatures:
  - Poor ranking performance despite good classification accuracy suggests the model captures cross-patient differences but not within-patient progression
  - Degraded performance with larger Whisper models suggests loss of paralinguistic information
  - Sensitivity to recording date embeddings suggests the model overfits to temporal artifacts rather than true progression
- First 3 experiments:
  1. Compare ALST performance using different pretrained encoder layers to identify which ones capture the most relevant ALS progression features
  2. Test the impact of removing positional embeddings to quantify the value of longitudinal modeling
  3. Vary the λCE hyperparameter to find the optimal balance between classification and regression objectives for the specific ALS progression task

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions.

## Limitations
- Dataset specificity: The model is validated on a single ALS TDI dataset with a specific speech task, raising questions about generalizability to other speech tasks or ALS patient populations
- Pretrained model dependence: The ALST relies heavily on pretrained speech encoders whose effectiveness for ALS-specific paralinguistic features is assumed but not directly validated
- Temporal modeling assumptions: The effectiveness of positional embeddings assumes consistent temporal patterns, but ALS progression is known to be heterogeneous

## Confidence
- High confidence: The technical implementation of the ALST architecture is well-specified and reproducible
- Medium confidence: The claim of 91.0% AUC improvement is credible but depends on proper dataset handling
- Low confidence: The generalizability of ALST to other ALS speech datasets, different speech tasks, or clinical settings is not established

## Next Checks
1. Test ALST on an independent ALS speech dataset from different clinical centers to assess generalizability beyond the ALS TDI dataset
2. Conduct ablation studies removing different layers from the pretrained encoder to validate that the current pretrained features are optimal for ALS progression detection
3. Analyze model predictions for patients with irregular recording intervals or non-monotonic progression patterns to verify that the model captures true disease progression rather than temporal artifacts