---
ver: rpa2
title: Why Don't Prompt-Based Fairness Metrics Correlate?
arxiv_id: '2406.05918'
source_url: https://arxiv.org/abs/2406.05918
tags:
- bias
- metrics
- fairness
- prompts
- correlation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that popular prompt-based fairness metrics
  show poor correlation, undermining their reliability for bias assessment. The authors
  identify six factors contributing to this low correlation, including prompt structure,
  verbalization, and distribution.
---

# Why Don't Prompt-Based Fairness Metrics Correlate?

## Quick Facts
- arXiv ID: 2406.05918
- Source URL: https://arxiv.org/abs/2406.05918
- Reference count: 40
- Key outcome: Popular prompt-based fairness metrics show poor correlation, but CAIRO method significantly improves this through data augmentation and prompt selection

## Executive Summary
This paper demonstrates that popular prompt-based fairness metrics (BOLD, HONEST, HolisticBias) exhibit poor correlation when assessing bias in language models, undermining their reliability. The authors identify prompt structure, verbalization, and distribution as key factors contributing to this inconsistency. To address this, they propose CAIRO, a method that uses data augmentation to generate paraphrases of original prompts and selects combinations that maximize correlation across metrics. CAIRO significantly improves Pearson correlation from 0.3 and 0.18 to 0.90 and 0.98 for gender and religion biases respectively, with high statistical significance.

## Method Summary
CAIRO addresses poor correlation between fairness metrics by generating multiple paraphrases of original prompts using different language models (ChatGPT, LLaMa 2, Mistral), then selecting prompt combinations that maximize correlation across metrics. The method standardizes bias quantification by using toxicity as a common proxy across all metrics. For each combination of augmented prompts, the approach measures bias scores across multiple language models and calculates Pearson correlation between metrics, selecting the combination that yields the highest correlation.

## Key Results
- Prompt-based fairness metrics show poor baseline correlation (0.3 for gender, 0.18 for religion)
- CAIRO improves correlation to 0.90 for gender and 0.98 for religion biases
- Improvements are statistically significant (p < 0.001)
- Method works across different language models (GPT-2, GPT-J, GPT-Neo, OPT, Pythia)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt-based fairness metrics exhibit poor correlation because they are highly sensitive to prompt structure, verbalization, and distribution.
- Mechanism: The same bias metric can yield different scores when applied to the same model using different prompt formulations, leading to inconsistency across metrics that use different prompts.
- Core assumption: Prompt sensitivity is the primary driver of low correlation between fairness metrics.
- Evidence anchors:
  - [abstract] This paper demonstrates that popular prompt-based fairness metrics show poor correlation, undermining their reliability for bias assessment. The authors identify six factors contributing to this low correlation, including prompt structure, verbalization, and distribution.
  - [section] Having established that the bias assessment of a given metric significantly fluctuates given the prompt's sentence structure and verbalization (Sections 4.2.1 and 4.2.2), averaging the bias scores across multiple prompt variations arises as a natural mitigation for this issue.
- Break condition: If prompts are standardized across all metrics, the correlation between them would improve, suggesting prompt sensitivity is the dominant factor.

### Mechanism 2
- Claim: CAIRO increases correlation by selecting prompt combinations that yield consistent bias assessments across different metrics.
- Mechanism: By generating multiple paraphrases of the original prompts using different language models and then selecting the combination that maximizes correlation across metrics, CAIRO finds a "common ground" where different metrics agree.
- Core assumption: There exist prompt formulations that lead to consistent bias assessments across different metrics, and these can be found through data augmentation and selection.
- Evidence anchors:
  - [abstract] To address this, they propose CAIRO, a method that uses data augmentation to generate paraphrases of original prompts, then selects combinations that maximize correlation across metrics.
  - [section] Lastly, we select the prompt combinations that achieved the highest correlation across different metrics. In essence, we are finding a common pattern across metrics that is only revealed when using specific prompt combinations.
- Break condition: If no prompt combination yields high correlation across metrics, CAIRO would fail, suggesting the metrics measure fundamentally different aspects of bias.

### Mechanism 3
- Claim: Standardizing bias quantification across metrics is necessary for achieving high correlation.
- Mechanism: By using the same proxy for bias (toxicity) and the same classifier across different metrics, CAIRO eliminates differences in bias quantification as a source of low correlation.
- Core assumption: Differences in bias quantification methods are a significant source of low correlation between metrics.
- Evidence anchors:
  - [abstract] The experiments are conducted using the following prompt-based fairness metrics: BOLD, HONEST, and HolisticBias. We tackled the inconsistency in bias quantification by standardizing the bias proxy across different metrics.
  - [section] We followed the work by Zayed et al. (2024) measuring bias as the difference in toxicity levels exhibited by the model across various subgroups.
- Break condition: If metrics continue to show low correlation even after standardizing bias quantification, other factors must be more significant.

## Foundational Learning

- Concept: Prompt sensitivity in language models
  - Why needed here: Understanding that language models can produce different outputs for semantically equivalent prompts is crucial for grasping why prompt-based fairness metrics show poor correlation.
  - Quick check question: If two prompts are semantically equivalent but worded differently, will a language model always produce the same output for both? (Answer: No)

- Concept: Data augmentation through paraphrasing
  - Why needed here: CAIRO relies on generating multiple paraphrases of the original prompts to find combinations that yield high correlation across metrics.
  - Quick check question: What is the purpose of generating multiple paraphrases of the original prompts in CAIRO? (Answer: To find prompt combinations that maximize correlation across different fairness metrics)

- Concept: Correlation as a measure of agreement between metrics
  - Why needed here: The paper's central claim is that prompt-based fairness metrics show poor correlation, and CAIRO significantly improves this correlation.
  - Quick check question: What does a high Pearson correlation between two fairness metrics indicate about their agreement? (Answer: High correlation indicates that the metrics tend to rank models similarly in terms of bias)

## Architecture Onboarding

- Component map:
  Original prompts from fairness metrics (BOLD, HolisticBias, HONEST) -> Paraphrasing models (ChatGPT, LLaMa 2, Mistral) -> Language models to be evaluated (GPT-2, GPT-J, GPT-Neo, OPT, Pythia) -> Toxicity classifier (BERT-based) -> CAIRO algorithm for prompt selection

- Critical path:
  1. Generate paraphrases of original prompts using three different language models
  2. Create all possible combinations of these augmented prompts
  3. For each combination, measure bias scores for each metric
  4. Calculate Pearson correlation between metrics for each combination
  5. Select the combination that yields the highest correlation

- Design tradeoffs:
  - Using more paraphrasing models increases the search space for high-correlation prompt combinations but also increases computational cost
  - Standardizing bias quantification (using toxicity) may reduce the richness of bias assessment but improves metric agreement
  - The method assumes that high correlation between metrics indicates reliable fairness assessment, which may not always be true

- Failure signatures:
  - Low correlation between metrics even after applying CAIRO (suggests metrics measure fundamentally different aspects of bias)
  - CAIRO selecting prompt combinations that yield inconsistent bias assessments across different models
  - High computational cost with diminishing returns as more paraphrasing models are added

- First 3 experiments:
  1. Verify that changing prompt structure/verbalization affects bias scores by paraphrasing original prompts and comparing results
  2. Test CAIRO's effectiveness by comparing correlation before and after applying the method on a small set of models and metrics
  3. Analyze which paraphrasing models contribute most to high-correlation prompt combinations by tracking their usage in selected combinations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of CAIRO vary across different bias dimensions (e.g., gender, religion, race) when subgroups targeted by fairness metrics have minimal overlap?
- Basis in paper: [explicit] The paper notes that CAIRO may fail when there is no significant overlap between subgroups targeted by different metrics, as demonstrated in the HolisticBias-BOLD comparison for gender and race biases.
- Why unresolved: The paper only provides one example of minimal subgroup overlap, but does not systematically explore how CAIRO performs across a broader range of bias dimensions with varying degrees of subgroup overlap.
- What evidence would resolve it: Empirical studies testing CAIRO's performance across multiple bias dimensions with different levels of subgroup overlap would clarify its limitations and effectiveness in diverse scenarios.

### Open Question 2
- Question: To what extent does the choice of paraphrasing models (e.g., ChatGPT, Llama 2, Mistral) influence the quality and diversity of augmented prompts, and how does this affect the correlation between fairness metrics?
- Basis in paper: [inferred] The paper suggests that using multiple models to generate paraphrases from different distributions is beneficial, but does not deeply analyze the impact of individual paraphrasing models on the quality and diversity of augmented prompts.
- Why unresolved: The paper acknowledges the contributions of each paraphrasing model but does not provide a detailed analysis of how the choice and combination of these models specifically affect the correlation outcomes.
- What evidence would resolve it: A detailed comparative analysis of the augmented prompts generated by each paraphrasing model, and their subsequent impact on fairness metric correlation, would provide insights into optimizing the selection of paraphrasing models.

### Open Question 3
- Question: How does the performance of CAIRO scale with the size and complexity of the language models being evaluated for fairness?
- Basis in paper: [inferred] The paper evaluates CAIRO on a range of model sizes but does not explicitly discuss how its performance scales with increasing model complexity or size.
- Why unresolved: While the paper provides results for various model sizes, it does not address whether CAIRO's effectiveness in improving correlation is consistent across different scales of language models.
- What evidence would resolve it: Systematic testing of CAIRO across a wider range of model sizes and complexities, with a focus on correlation improvements, would determine its scalability and effectiveness for larger models.

### Open Question 4
- Question: What are the potential unintended consequences of using CAIRO to maximize correlation between fairness metrics, such as the risk of overlooking specific biases that may be important but less correlated?
- Basis in paper: [explicit] The paper mentions that having correlated metrics does not eliminate the chance of measuring the wrong proxy for bias.
- Why unresolved: The paper highlights the risk of focusing on correlation at the expense of specific bias detection but does not explore the potential trade-offs or unintended consequences in detail.
- What evidence would resolve it: A comprehensive analysis of the trade-offs between achieving high correlation and detecting specific biases would help in understanding the broader implications of using CAIRO in fairness assessments.

## Limitations
- CAIRO's effectiveness may be limited by the relatively small sample of paraphrasing models and specific bias metrics tested
- High correlation between metrics doesn't necessarily indicate they measure the "correct" aspects of bias
- Standardizing bias quantification through toxicity proxies may oversimplify complex bias phenomena

## Confidence
**High Confidence**: The empirical demonstration that prompt-based fairness metrics show poor correlation (from 0.3 to 0.18) is well-supported by the experimental results. The methodology for improving correlation through data augmentation and selection is clearly described and reproducible.

**Medium Confidence**: The claim that prompt sensitivity is the primary driver of low correlation is supported but not definitively proven. While the paper shows that prompt variations affect bias scores, it doesn't conclusively establish that this is the dominant factor compared to other potential sources of metric disagreement.

**Low Confidence**: The assumption that high correlation between metrics necessarily indicates more reliable fairness assessment is not empirically validated. The paper doesn't test whether CAIRO-selected prompt combinations actually improve the quality or validity of bias measurement, only that they improve metric agreement.

## Next Checks
1. **Cross-domain validation**: Apply CAIRO to a different set of fairness metrics (e.g., those measuring different bias types like cultural or socioeconomic bias) to test generalizability beyond gender and religion biases.

2. **Ground truth correlation**: Compare CAIRO's bias assessments against human-annotated ground truth data to verify that high-correlation prompt combinations actually measure meaningful bias rather than just agreeing on potentially incorrect assessments.

3. **Model-specific behavior analysis**: Investigate whether CAIRO-selected prompt combinations perform consistently across different model families (e.g., comparing results between GPT models and LLaMA models) to ensure the method isn't exploiting model-specific quirks rather than finding truly robust prompt formulations.