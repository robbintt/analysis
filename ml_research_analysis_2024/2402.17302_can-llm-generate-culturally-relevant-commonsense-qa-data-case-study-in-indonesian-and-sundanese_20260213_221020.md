---
ver: rpa2
title: Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian
  and Sundanese
arxiv_id: '2402.17302'
source_url: https://arxiv.org/abs/2402.17302
tags:
- data
- question
- concept
- sundanese
- indonesian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLMs can generate Indonesian and Sundanese commonsense QA data
  with adequate general knowledge, but struggle with culturally deep content. Automatic
  data adaptation from English is less effective, especially for Sundanese.
---

# Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese

## Quick Facts
- **arXiv ID:** 2402.17302
- **Source URL:** https://arxiv.org/abs/2402.17302
- **Authors:** Rifki Afina Putri; Faiz Ghifari Haznitrama; Dea Adhista; Alice Oh
- **Reference count:** 40
- **Key outcome:** LLMs can generate Indonesian and Sundanese commonsense QA data with adequate general knowledge, but struggle with culturally deep content. Automatic data adaptation from English is less effective, especially for Sundanese. Direct generation in target languages yields better quality but still contains fluency errors, particularly in Sundanese. Human-generated data shows higher lexical diversity and cultural depth. GPT-4 Turbo achieves ~80% accuracy on LLM-generated data and ~90% on multiple-choice settings, dropping to ~75% in open-ended generation. Open-source LLMs lag behind proprietary models, with larger performance gaps in Sundanese. Combining LLM and human efforts can scale dataset creation but requires careful quality control.

## Executive Summary
This paper investigates whether large language models (LLMs) can generate culturally relevant commonsense question-answering (QA) datasets for Indonesian and Sundanese languages. The authors compare three methods: automatic data adaptation from English, manual data generation, and automatic data generation using GPT-4 Turbo. They find that direct generation in target languages produces higher quality data than translation-based adaptation, especially for Sundanese. While GPT-4 Turbo performs well on the generated datasets, human-generated data exhibits superior cultural depth and lexical diversity. The study highlights both the potential and limitations of using LLMs for creating culturally nuanced datasets in underrepresented languages.

## Method Summary
The authors created approximately 4,500 questions per language (9,000 total) using three methods: automatic data adaptation (translating/adapting English CommonsenseQA to Indonesian and Sundanese), manual data generation (human annotators creating questions in target languages), and automatic data generation (GPT-4 Turbo directly generating questions in target languages). They evaluated data quality through human review and benchmarked various LLMs using zero-shot prompts. The evaluation focused on accuracy rates and cultural relevance, comparing LLM-generated data against human-generated benchmarks. The study specifically examined differences between medium-resource Indonesian and lower-resource Sundanese.

## Key Results
- Direct generation in target languages produces significantly higher quality data than translation-based adaptation, especially for Sundanese
- GPT-4 Turbo achieves ~80% accuracy on LLM-generated data and ~90% on multiple-choice settings, dropping to ~75% in open-ended generation
- Human-generated data shows higher lexical diversity and cultural depth compared to LLM-generated data
- Open-source LLMs significantly underperform proprietary models, with larger performance gaps in Sundanese

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLM-generated datasets can achieve reasonable cultural relevance in target languages when using direct generation methods.
- **Mechanism:** Direct generation in target language (Indonesian/Sundanese) with explicit cultural category and concept inputs allows LLMs to produce contextually relevant question-answer pairs, even without explicit translation.
- **Core assumption:** LLMs trained on multilingual data can synthesize culturally relevant content when prompted with local categories and concepts, rather than relying on translation of culturally biased source data.
- **Evidence anchors:**
  - [abstract] "using the direct generation method on the target language, GPT-4 Turbo can generate questions with adequate general knowledge in both languages, albeit not as culturally 'deep' as humans."
  - [section 5.1.2] "the LLM_G EN dataset, which involves generating synthetic data directly in the target language, shows a significantly higher number of correct questions, particularly for Sundanese."
- **Break condition:** When LLM lacks sufficient training data in target language or when cultural concepts have no direct equivalent in training corpus, leading to hallucinations or generic responses.

### Mechanism 2
- **Claim:** Translation-based adaptation of English commonsense datasets introduces significant noise in lower-resource languages like Sundanese.
- **Mechanism:** Machine translation from adapted Indonesian data to Sundanese fails to preserve cultural nuance and introduces fluency errors, reducing data quality.
- **Core assumption:** Translation systems for lower-resource languages have limited linguistic coverage and struggle with morphological differences, leading to degraded output quality.
- **Evidence anchors:**
  - [abstract] "automatic data adaptation from an existing English dataset is less effective for Sundanese."
  - [section 5.1.1] "for Indonesian to Sundanese (Table 3), the correct adaptation drops to 77.22%, reflecting weak machine translation (MT) performance in Sundanese."
- **Break condition:** When translation quality improves (e.g., with better MT models or larger parallel corpora), or when target language has closer structural similarity to source language.

### Mechanism 3
- **Claim:** Human-generated data exhibits higher lexical diversity and cultural depth compared to LLM-generated data.
- **Mechanism:** Human annotators from diverse cultural backgrounds generate more varied vocabulary and culturally specific concepts, while LLMs tend to default to popular or general concepts.
- **Core assumption:** Humans draw from personal experience and regional knowledge, while LLMs rely on learned statistical patterns that may overgeneralize.
- **Evidence anchors:**
  - [abstract] "human-generated data shows higher lexical diversity and cultural depth."
  - [section 5.2] "humans generate more token variations that are not produced by LLMs, such as some unique terms like kalis or cimol."
- **Break condition:** When LLM training data becomes sufficiently diverse and culturally rich, or when prompting strategies are improved to elicit more varied outputs.

## Foundational Learning

- **Concept:** Cultural nuance in language models
  - **Why needed here:** Understanding how cultural context affects commonsense reasoning is critical for evaluating LLM-generated datasets
  - **Quick check question:** Can you explain why a question about "snow" might be irrelevant in Indonesian commonsense QA data?

- **Concept:** Low-resource language challenges
  - **Why needed here:** Sundanese has limited training data, affecting both translation quality and LLM performance
  - **Quick check question:** What are the key differences between medium-resource (Indonesian) and lower-resource (Sundanese) language processing challenges?

- **Concept:** Data adaptation vs. direct generation
  - **Why needed here:** The paper compares two strategies for creating culturally relevant datasets
  - **Quick check question:** When would direct generation be preferable to translation-based adaptation for creating multilingual datasets?

## Architecture Onboarding

- **Component map:** Data generation pipeline → quality control → benchmark evaluation → error analysis
- **Critical path:** Human annotation → LLM generation → quality filtering → benchmark testing
- **Design tradeoffs:** Translation vs. direct generation (speed/quality), human vs. LLM generation (cost/quality), multiple-choice vs. open-ended formats (evaluation simplicity/complexity)
- **Failure signatures:** High error rates in translation, over-reliance on generic concepts, fluency issues in lower-resource languages
- **First 3 experiments:**
  1. Compare LLM-generated vs. human-generated question quality using human evaluation
  2. Test different prompting strategies for direct generation in target languages
  3. Evaluate translation quality impact by comparing adapted vs. directly generated Sundanese data

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the limitations and discussion, several key unresolved issues emerge:

### Open Question 1
- **Question:** How does the performance of open-source LLMs in generating culturally relevant commonsense QA data for low-resource languages compare to proprietary models like GPT-4, and what specific improvements could close this gap?
- **Basis in paper:** [inferred]
- **Why unresolved:** The paper demonstrates that GPT-4 Turbo significantly outperforms open-source models like Merak-v4 in generating culturally relevant data for Indonesian and Sundanese. However, it does not explore the specific architectural or training enhancements that could enable open-source models to achieve comparable performance, such as fine-tuning strategies, dataset composition, or model scaling.
- **What evidence would resolve it:** Comparative studies evaluating open-source models before and after targeted fine-tuning on culturally diverse datasets, or ablation studies isolating the impact of specific model components on cultural relevance.

### Open Question 2
- **Question:** What is the optimal balance between human and LLM-generated data in creating high-quality, culturally nuanced commonsense QA datasets for underrepresented languages?
- **Basis in paper:** [explicit]
- **Why unresolved:** The paper suggests that combining human and LLM efforts can scale dataset creation but acknowledges the need for careful quality control. It does not quantify the ideal ratio of human to LLM-generated data or provide a framework for dynamically adjusting this balance based on dataset quality metrics or resource constraints.
- **What evidence would resolve it:** Experiments systematically varying the proportion of human-generated data in training and evaluation sets, measuring downstream model performance and dataset quality across different ratios.

### Open Question 3
- **Question:** How do fluency errors in LLM-generated data for lower-resource languages like Sundanese impact downstream model performance, and what targeted interventions could mitigate these errors?
- **Basis in paper:** [explicit]
- **Why unresolved:** The paper identifies higher fluency error rates in Sundanese compared to Indonesian but does not investigate how these errors specifically affect model accuracy or explore targeted solutions, such as specialized language models for low-resource languages or improved data augmentation techniques.
- **What evidence would resolve it:** Correlation studies linking fluency error types to model performance drops, and controlled experiments testing the effectiveness of targeted interventions like monolingual fine-tuning or synthetic data cleaning on downstream task accuracy.

### Open Question 4
- **Question:** Can LLM-generated commonsense QA data for underrepresented languages achieve cultural depth comparable to human-generated data, and what metrics best capture this cultural relevance?
- **Basis in paper:** [explicit]
- **Why unresolved:** The paper finds that GPT-4 Turbo generates questions with adequate general knowledge but lacks the cultural depth of human-generated data. It does not define or validate metrics for quantifying cultural depth or explore methods to enhance cultural relevance in LLM outputs.
- **What evidence would resolve it:** Development and validation of cultural relevance metrics, such as human evaluations or automated cultural concept coverage analysis, applied to both LLM and human-generated datasets to measure and compare cultural depth.

## Limitations
- The study relies on zero-shot evaluation without extensive cross-validation, limiting generalizability
- Only one open-source LLM (BLOOMZ) is tested against proprietary models, providing limited comparison
- Human-generated data comes from only 3 annotators per language, raising concerns about bias and representativeness
- The evaluation methodology depends heavily on automated metrics and limited human review, potentially missing nuanced cultural errors

## Confidence
- **High:** Direct generation outperforms translation-based adaptation for Sundanese
- **Medium:** LLM performance benchmarks (~80% accuracy on generated data)
- **Low:** Claims about open-source LLM performance gaps

## Next Checks
1. Conduct a systematic study varying prompt structures and instructions to identify optimal strategies for eliciting culturally relevant content from LLMs.
2. Expand evaluation to include multiple open-source LLM models and compare their performance across different cultural categories and languages.
3. Implement a more rigorous human evaluation protocol with larger annotator pools and inter-annotator agreement metrics to validate cultural relevance assessments.