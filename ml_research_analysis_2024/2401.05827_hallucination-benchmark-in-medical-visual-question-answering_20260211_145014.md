---
ver: rpa2
title: Hallucination Benchmark in Medical Visual Question Answering
arxiv_id: '2401.05827'
source_url: https://arxiv.org/abs/2401.05827
tags:
- llav
- question
- option
- answer
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study addresses the challenge of hallucination in medical
  visual question answering (Med-VQA), where models may produce factually incorrect
  responses. To tackle this, the authors created a hallucination benchmark dataset
  by modifying three publicly available VQA datasets (PMC-VQA, PathVQA, and VQA-RAD)
  with three scenarios: FAKE questions, None of the Above (NOTA), and Image SWAP.'
---

# Hallucination Benchmark in Medical Visual Question Answering

## Quick Facts
- arXiv ID: 2401.05827
- Source URL: https://arxiv.org/abs/2401.05827
- Reference count: 4
- Primary result: Created hallucination benchmark for Med-VQA and evaluated state-of-the-art models across three hallucination scenarios

## Executive Summary
This study addresses the critical challenge of hallucination in medical visual question answering (Med-VQA), where models may generate factually incorrect responses when processing medical images. The authors created a comprehensive hallucination benchmark dataset by modifying three existing VQA datasets (PMC-VQA, PathVQA, and VQA-RAD) with three specific scenarios: FAKE questions (unanswerable questions), None of the Above (NOTA) questions, and Image SWAP scenarios where questions are paired with mismatched images. They evaluated several state-of-the-art models including LLaVA variants and GPT-4-turbo-vision using various prompting strategies, with the best-performing strategy (L+D0) identified through systematic ablation studies.

## Method Summary
The authors systematically constructed a hallucination benchmark by modifying existing medical VQA datasets through three scenarios: FAKE questions where queries have no correct answers, NOTA questions where all provided options are incorrect, and Image SWAP where questions are paired with semantically mismatched images. They evaluated multiple state-of-the-art models including LLaVA-13B, LLaVA-v1.5-13B, and GPT-4-turbo-vision using five different prompting strategies ranging from basic text-only prompts to complex combinations of images and options. An ablation study was conducted to identify the optimal prompting strategy (L+D0), and model performance was measured using accuracy metrics along with analysis of irrelevant predictions.

## Key Results
- LLaVA-v1.5-13B achieved the highest average accuracy of 55.44% among evaluated models with zero irrelevant predictions
- GPT-4-turbo-vision outperformed LLaVA-v1.5-13B on average but produced more irrelevant predictions
- The L+D0 prompting strategy (combining image and option inputs) was identified as optimal through ablation studies
- All models showed significant performance variation across the three hallucination scenarios, with Image SWAP being particularly challenging

## Why This Works (Mechanism)
The systematic construction of hallucination scenarios through controlled modifications of existing datasets allows for precise measurement of model behavior when faced with unanswerable or mismatched inputs. The ablation study approach to prompt engineering enables identification of optimal strategies for mitigating hallucination, while the comparative evaluation across multiple state-of-the-art models provides insights into their relative capabilities and limitations in handling challenging medical visual question answering tasks.

## Foundational Learning
- **Med-VQA Systems**: Medical visual question answering combines image analysis with natural language processing to answer clinical questions about medical images; essential for evaluating AI systems in healthcare contexts
- **Hallucination Detection**: The ability to identify when models generate incorrect or fabricated information; critical for ensuring reliability in medical applications where errors can have serious consequences
- **Prompt Engineering**: Strategic formulation of input prompts to guide model responses; fundamental for optimizing performance and reducing hallucination in complex tasks
- **Ablation Studies**: Systematic removal or modification of components to identify their impact on overall system performance; necessary for understanding which elements contribute most to success

## Architecture Onboarding
Component map: Input images and questions -> Prompt Engineering module -> Multimodal Model (LLaVA or GPT-4) -> Answer Generation -> Hallucination Detection
Critical path: Image processing and question encoding flow through the prompt engineering module to the multimodal model, which generates responses that are evaluated for accuracy and hallucination
Design tradeoffs: Model size vs. computational efficiency (LLaVA variants vs. GPT-4), prompt complexity vs. response quality, and accuracy vs. hallucination rates
Failure signatures: High rates of irrelevant predictions, significant performance drops on Image SWAP scenarios, and inconsistent accuracy across hallucination types
First experiments: 1) Test baseline model performance on original non-modified datasets, 2) Evaluate individual hallucination scenarios in isolation, 3) Compare prompting strategies using a held-out validation set

## Open Questions the Paper Calls Out
None identified in the source material

## Limitations
- Benchmark relies on modified existing datasets rather than capturing full diversity of real-world medical scenarios
- Evaluation focuses on accuracy metrics without extensive exploration of clinical implications of different hallucination types
- Limited investigation of trade-offs between model capabilities and hallucination rates, particularly regarding clinically significant errors
- Study focuses on English-language medical images and questions, limiting applicability to multilingual or culturally diverse medical settings

## Confidence
High confidence in methodology for creating hallucination benchmark and comparative evaluation framework. Medium confidence in absolute performance metrics due to synthetic nature of some test scenarios. Medium confidence in generalizability of findings to real-world clinical applications.

## Next Checks
1. Validate benchmark on real clinical cases with ground truth answers verified by medical experts, rather than modified existing datasets
2. Conduct systematic evaluation of additional hallucination scenarios including ambiguous questions, incomplete image information, and clinically rare conditions
3. Implement and test broader range of prompt engineering strategies including chain-of-thought prompting and retrieval-augmented generation approaches to assess impact on hallucination rates in medical contexts