---
ver: rpa2
title: Were RNNs All We Needed?
arxiv_id: '2410.01201'
source_url: https://arxiv.org/abs/2410.01201
tags:
- state
- minlstm
- mingru
- lineardh
- hidden
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper revisits traditional RNNs (LSTMs and GRUs) by simplifying
  them to remove previous state dependencies in their gates, enabling parallel training.
  The resulting minimal models (minLSTM and minGRU) use fewer parameters, are fully
  parallelizable, and achieve competitive performance on various tasks, including
  language modeling, reinforcement learning, and synthetic benchmarks, often matching
  or outperforming modern models like Transformers and Mamba.
---

# Were RNNs All We Needed?

## Quick Facts
- arXiv ID: 2410.01201
- Source URL: https://arxiv.org/abs/2410.01201
- Authors: Leo Feng; Frederick Tung; Mohamed Osama Ahmed; Yoshua Bengio; Hossein Hajimirsadeghi
- Reference count: 40
- Primary result: Simplified LSTM/GRU variants (minLSTM/minGRU) achieve competitive performance through parallel training while using fewer parameters

## Executive Summary
This paper challenges the notion that complex architectures like Transformers are necessary for state-of-the-art sequence modeling by revisiting and simplifying traditional RNNs. The authors demonstrate that by removing previous state dependencies from LSTM and GRU gates, these architectures become fully parallelizable while maintaining competitive performance across various tasks. The resulting minLSTM and minGRU models use fewer parameters than their predecessors and can be trained efficiently without the sequential bottleneck that traditionally plagued RNNs.

## Method Summary
The authors systematically simplify LSTM and GRU architectures by removing dependencies on previous hidden states in their gate mechanisms, enabling full parallelization during training. This architectural modification allows the models to be trained using standard parallel computing techniques rather than requiring sequential processing. The simplified models (minLSTM and minGRU) are evaluated across multiple domains including language modeling (PTB, Wikitext-2), reinforcement learning (Meta-World), and synthetic sequence tasks. The experiments compare these simplified RNNs against various baselines including Transformers, Mamba, and traditional LSTM/GRU implementations.

## Key Results
- minLSTM and minGRU achieve competitive performance on language modeling tasks compared to Transformers and Mamba
- The simplified models use fewer parameters while maintaining or improving upon traditional LSTM/GRU performance
- Parallel training enables faster training times compared to sequential RNN implementations
- Strong performance is demonstrated across diverse tasks including reinforcement learning and synthetic benchmarks

## Why This Works (Mechanism)
By removing dependencies on previous hidden states in the gate mechanisms, the simplified RNNs eliminate the sequential processing requirement that traditionally made RNN training slow. This architectural change transforms the training process from inherently sequential to fully parallel, similar to how Transformers process sequences. The gates can now be computed independently for each time step, allowing for massive parallelization on modern hardware while maintaining the core recurrent structure that makes RNNs effective at modeling sequential dependencies.

## Foundational Learning

**RNN Fundamentals**: Understanding how traditional RNNs process sequences sequentially through hidden state updates - needed to appreciate the parallelization breakthrough, quick check: can you trace through one time step of an LSTM?

**Gate Mechanisms**: Knowledge of how LSTM and GRU gates control information flow - needed to understand what dependencies were removed, quick check: can you explain the purpose of forget gates?

**Parallel Computing**: Basics of parallel processing in neural networks - needed to grasp why parallelization matters, quick check: can you contrast sequential vs parallel training approaches?

**Sequence Modeling**: Core concepts of how models handle temporal dependencies - needed to evaluate the effectiveness of simplified architectures, quick check: can you describe the vanishing gradient problem?

## Architecture Onboarding

**Component Map**: Input -> Embedding -> minLSTM/minGRU Layers -> Output Projection -> Loss Function

**Critical Path**: The forward pass through the simplified RNN layers, where gate computations can now be parallelized across time steps, replacing the sequential state updates of traditional RNNs.

**Design Tradeoffs**: The paper trades sequential processing capability for parallelizability, accepting that the simplified gates may have reduced temporal modeling capacity in exchange for massive speedups and simpler training dynamics.

**Failure Signatures**: Potential issues include loss of fine-grained temporal dependencies that might be captured by traditional sequential processing, and possible instability during training if the simplified gates cannot adequately control information flow.

**3 First Experiments**:
1. Train minLSTM on PTB language modeling task to establish baseline performance
2. Compare training speed of minLSTM vs traditional LSTM on a medium-sized dataset
3. Evaluate minLSTM performance on a synthetic sequence task with known temporal dependencies

## Open Questions the Paper Calls Out
None

## Limitations

- Generalization Across Tasks: While minLSTM and minGRU show strong performance on specific benchmarks, their effectiveness across diverse real-world applications remains to be thoroughly validated.
- Training Dynamics and Stability: The removal of previous state dependencies represents a significant architectural change, and the stability and convergence properties across different training regimes require further investigation.
- Comparison Context: Performance comparisons are made against specific model configurations, and the absolute advantage may vary depending on implementation details and computational resources.

## Confidence

**High Confidence**: The architectural modifications are clearly described and implemented. The parallelizability claim is well-supported through theoretical analysis and empirical evidence.

**Medium Confidence**: The competitive performance claims are supported by experimental results, but the extent of superiority over established models may be context-dependent and require broader validation.

**Medium Confidence**: The assertion that simpler architectures can be highly effective is compelling but needs validation across a wider range of applications and scales.

## Next Checks

1. Cross-Domain Evaluation: Test minLSTM and minGRU on diverse sequence modeling tasks beyond language and reinforcement learning, including time-series forecasting, audio processing, and multimodal applications.

2. Scaling Studies: Evaluate model performance and efficiency at scale, comparing minLSTM/minGRU with Transformers and other modern architectures as model size increases to billions of parameters.

3. Robustness Analysis: Conduct systematic ablation studies and sensitivity analyses to understand how minLSTM/minGRU performance varies with different initialization strategies, learning rates, and regularization techniques.