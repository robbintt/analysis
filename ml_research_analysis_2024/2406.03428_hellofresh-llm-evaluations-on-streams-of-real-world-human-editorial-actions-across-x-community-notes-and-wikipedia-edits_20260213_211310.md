---
ver: rpa2
title: 'HelloFresh: LLM Evaluations on Streams of Real-World Human Editorial Actions
  across X Community Notes and Wikipedia edits'
arxiv_id: '2406.03428'
source_url: https://arxiv.org/abs/2406.03428
tags:
- wikipedia
- notes
- edits
- note
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HelloFresh, a dynamic benchmark for evaluating
  large language models (LLMs) using real-world human editorial actions from X (formerly
  Twitter) community notes and Wikipedia edits. The benchmark addresses key challenges
  in LLM evaluation, including data contamination, benchmark overfitting, and temporal
  inconsistency.
---

# HelloFresh: LLM Evaluations on Streams of Real-World Human Editorial Actions across X Community Notes and Wikipedia edits

## Quick Facts
- arXiv ID: 2406.03428
- Source URL: https://arxiv.org/abs/2406.03428
- Reference count: 18
- Key outcome: Introduces dynamic benchmark using real-world human editorial actions from X and Wikipedia

## Executive Summary
HelloFresh presents a novel approach to LLM evaluation by leveraging streams of real-world human editorial actions from X (formerly Twitter) community notes and Wikipedia edits. The benchmark addresses key challenges in LLM evaluation, including data contamination, benchmark overfitting, and temporal inconsistency, by using intrinsically motivated human labelers to generate fresh data streams covering recent events. The core method involves two evaluation regimes: a zero-shot classifier and a web-search agent, which show that GPT-4 and GPT-3.5 consistently rank highest for X notes and Wikipedia edits, respectively.

## Method Summary
The HelloFresh benchmark introduces a dynamic evaluation framework that uses streams of real-world human editorial actions from X community notes and Wikipedia edits. The method employs two evaluation regimes: a zero-shot classifier that directly prompts LLMs to classify notes and edits, and a web-search agent that first generates a web search query, summarizes retrieved information, and then uses this context for classification. This approach aims to mitigate risks associated with static evaluation data by leveraging intrinsically motivated human labelers to generate fresh data streams covering recent events.

## Key Results
- HelloFresh yields temporally consistent ranking of LLMs across different time periods
- GPT-4 and GPT-3.5 consistently rank highest for X notes and Wikipedia edits, respectively
- Under simulated deployment, best models achieve 75% precision on X notes and 90% precision on Wikipedia edits at a recall of more than 20%

## Why This Works (Mechanism)
HelloFresh works by leveraging intrinsically motivated human labelers to generate fresh data streams of real-world editorial actions. This approach addresses key challenges in LLM evaluation, including data contamination, benchmark overfitting, and temporal inconsistency. By using recent events and human-generated content, the benchmark provides a more realistic and up-to-date evaluation of LLM performance.

## Foundational Learning
- Data contamination mitigation: Why needed - Prevents models from memorizing evaluation data; Quick check - Compare model performance on known vs. unknown data
- Temporal consistency: Why needed - Ensures benchmark relevance over time; Quick check - Evaluate model rankings across different time periods
- Human editorial actions: Why needed - Provides realistic evaluation scenarios; Quick check - Analyze distribution of human editorial decisions

## Architecture Onboarding
Component map: Human labelers -> Data streams -> Zero-shot classifier / Web-search agent -> LLM evaluation
Critical path: Data collection → Classification → Performance measurement
Design tradeoffs: Fresh data vs. evaluation consistency; Search augmentation vs. potential confusion
Failure signatures: Inconsistent rankings across time periods; Poor performance on specific content types
First experiments:
1. Evaluate LLM performance on X notes without search augmentation
2. Compare zero-shot classifier vs. web-search agent performance
3. Analyze model rankings across different time periods

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on single source of human editorial actions may limit generalizability
- Web-search agent introduces potential variability and bias in information retrieval
- Evaluation focuses on specific precision and recall thresholds, not full performance range

## Confidence
- Major claims about consistent LLM rankings: Medium
- Claims about benchmark's real-world applicability: Medium
- Claims about web-search agent's performance: Medium

## Next Checks
1. Evaluate HelloFresh benchmark on broader range of human editorial actions from other platforms and content types
2. Conduct sensitivity analysis of web-search agent's performance with varying search query generation and information retrieval methods
3. Compare HelloFresh benchmark results with established LLM evaluation benchmarks like BIG-bench or GLUE