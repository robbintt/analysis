---
ver: rpa2
title: 'BM25S: Orders of magnitude faster lexical search via eager sparse scoring'
arxiv_id: '2407.03618'
source_url: https://arxiv.org/abs/2407.03618
tags:
- bm25s
- bm25
- which
- sparse
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BM25S is a Python implementation of BM25 that achieves up to 500x
  speedup compared to existing Python frameworks by eagerly computing BM25 scores
  during indexing and storing them in sparse matrices. The approach reformulates BM25
  scoring to allow pre-computation of all possible token-document scores, enabling
  fast retrieval via matrix slicing and summation.
---

# BM25S: Orders of magnitude faster lexical search via eager sparse scoring

## Quick Facts
- arXiv ID: 2407.03618
- Source URL: https://arxiv.org/abs/2407.03618
- Reference count: 16
- Primary result: Achieves up to 500x speedup over existing Python BM25 implementations through eager sparse scoring

## Executive Summary
BM25S introduces a novel approach to BM25-based lexical search by precomputing all possible token-document scores during indexing and storing them in sparse matrices. This reformulation transforms the traditional on-the-fly BM25 scoring into simple matrix operations during retrieval, enabling orders of magnitude faster search performance. The implementation supports multiple BM25 variants through a score shifting method and requires only Numpy and Scipy dependencies, making it accessible for deployment on resource-constrained devices.

## Method Summary
BM25S reformulates BM25 scoring to enable pre-computation of all possible token-document scores during indexing. These scores are stored in sparse matrix format, where most entries are zero due to the nature of BM25 scoring. During retrieval, query tokens are used to select relevant rows from this matrix, and scores are summed across documents. The approach extends to multiple BM25 variants through a differential scoring method that handles non-zero scores for absent tokens. Top-k selection uses Quickselect algorithm for efficient retrieval of the most relevant documents.

## Key Results
- Achieves over 100x higher throughput (queries per second) than Rank-BM25 in 10 out of 14 datasets
- Demonstrates up to 500x speedup compared to existing Python BM25 implementations
- Maintains retrieval accuracy with minimal impact on NDCG@10 scores
- Requires only Numpy and Scipy, enabling deployment on resource-constrained devices

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Precomputing all possible token-document scores during indexing and storing them in a sparse matrix structure enables faster retrieval by replacing on-the-fly BM25 scoring with simple matrix slicing and summation.
- Mechanism: The paper reformulates BM25 scoring into a sparse matrix format where each entry represents the BM25 score of a token-document pair. During retrieval, the query tokens are used to select the relevant rows from this matrix, and the scores are summed across the document dimension to obtain the final relevance scores.
- Core assumption: Most token-document pairs have a BM25 score of zero, making sparse matrix representation memory-efficient and computationally advantageous for retrieval.
- Evidence anchors:
  - [abstract] "eagerly computing BM25 scores during indexing and storing them into sparse matrices"
  - [section 2] "we can reformulate S(t, D) as: S(t, D) = TF(t, D) · IDF(t, C) / D" and "for most tokens in vocabulary V, we can simply set the relevance score to 0"
  - [corpus] Weak evidence - the corpus does not directly support this mechanism, but related works on sparse representations for retrieval suggest its feasibility.
- Break condition: If the vocabulary size becomes too large relative to the number of documents, or if the documents are extremely long, the sparse matrix may become dense, negating the performance benefits.

### Mechanism 2
- Claim: Using a novel score shifting method, BM25S can be extended to support multiple BM25 variants that have non-zero scores for tokens not present in a document.
- Mechanism: For BM25 variants where the score is not zero when a token is absent from a document, the paper introduces a differential score S∆(t, D) = S(t, D) - Sθ(t), where Sθ(t) is the score of token t when it does not occur in document D. This allows the use of sparse matrices for efficient computation, with the non-occurrence scores being added during retrieval.
- Core assumption: The non-occurrence scores can be precomputed and stored separately, allowing the main computation to still benefit from sparse matrix operations.
- Evidence anchors:
  - [section 2.1] "For BM25L (Lv and Zhai, 2011), BM25+ (Lv and Zhai, 2011) and TFl◦δ◦p×IDF (Rousseau and Vazirgiannis, 2013), we notice that when TF(t, D) = 0, the value of S(t, D) will not be zero"
  - [section 2.1] "we can still achieve sparsity by subtracting Sθ(t) from each token t and document D in the score matrix"
  - [corpus] No direct evidence, but the approach is consistent with the general idea of extending sparse representations to handle more complex scoring functions.
- Break condition: If the non-occurrence scores vary significantly across tokens or documents, the memory savings from using sparse matrices may be reduced.

### Mechanism 3
- Claim: Using an efficient top-k selection algorithm (e.g., Quickselect) instead of sorting the entire score vector significantly reduces the time complexity of retrieving the top-k most relevant documents.
- Mechanism: After computing the BM25 scores for all documents, BM25S uses a partitioning algorithm like Quickselect to find the k-th largest score in O(n) time, and then sorts only the top k documents in O(k log k) time, resulting in an average time complexity of O(n) for top-k selection.
- Core assumption: The k-th largest score can be found efficiently using a partitioning algorithm, and sorting a small subset of the documents is much faster than sorting the entire set.
- Evidence anchors:
  - [section 2] "we take the partition of the array, selecting only the last k documents (unordered). Using an algorithm such as Quickselect (Hoare, 1961), we can accomplish this in an average time complexity of O(n)"
  - [section 2] "If the user wishes to receive the top-k results in order, sorting the partitioned documents would take an additional O(k log k)"
  - [corpus] No direct evidence, but the use of Quickselect for top-k selection is a well-known technique in computer science.
- Break condition: If k is close to n, the time complexity approaches O(n log n), similar to sorting the entire array.

## Foundational Learning

- Concept: Sparse matrix representations and operations
  - Why needed here: BM25S relies heavily on sparse matrix operations to store and retrieve precomputed BM25 scores efficiently. Understanding how sparse matrices work and how to perform operations like slicing and summation on them is crucial for implementing and optimizing BM25S.
  - Quick check question: What is the time complexity of summing all elements in a sparse matrix compared to a dense matrix of the same size?

- Concept: Information retrieval and ranking functions
  - Why needed here: BM25S is an implementation of the BM25 ranking function, which is a fundamental concept in information retrieval. Understanding how BM25 works, including its components like term frequency (TF), inverse document frequency (IDF), and document length normalization, is essential for extending BM25S to other BM25 variants and for interpreting its performance.
  - Quick check question: How does the BM25 score of a document change if the document length increases while keeping all other factors constant?

- Concept: Top-k selection algorithms
  - Why needed here: BM25S uses an efficient top-k selection algorithm (Quickselect) to retrieve the most relevant documents. Understanding how this algorithm works and its time complexity is important for implementing the retrieval process and for comparing the performance of BM25S with other methods.
  - Quick check question: What is the worst-case time complexity of Quickselect, and how does it compare to the average-case complexity?

## Architecture Onboarding

- Component map: Tokenizer -> Indexer -> Sparse matrix construction -> Query processor -> Top-k selector
- Critical path: Tokenization → Indexing → Sparse matrix construction → Query processing → Top-k selection
- Design tradeoffs:
  - Memory vs. speed: Using a sparse matrix representation saves memory but may introduce some overhead in matrix operations compared to dense matrices.
  - Flexibility vs. performance: Supporting multiple BM25 variants through the score shifting method adds flexibility but may slightly increase the complexity of the implementation.
- Failure signatures:
  - Out-of-memory errors: If the vocabulary size or document length becomes too large, the sparse matrix may consume too much memory.
  - Slow retrieval: If the query contains many tokens or the top-k parameter is large, the retrieval process may become slow.
  - Incorrect results: If the tokenization or indexing process is not implemented correctly, the BM25 scores may be inaccurate, leading to incorrect retrieval results.
- First 3 experiments:
  1. Test the tokenization process on a small set of documents to ensure it correctly splits text into tokens and handles stemming and stopword removal as expected.
  2. Create a small index with a few documents and manually verify that the precomputed BM25 scores in the sparse matrix match the expected values.
  3. Perform a simple retrieval with a small query and a small top-k parameter, and verify that the retrieved documents are the expected ones based on their BM25 scores.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would BM25S perform on larger datasets that exceed available memory, and what optimizations could be made to handle such cases?
- Basis in paper: [inferred] The paper mentions that experiments were limited to free and readily available hardware, and some memory-intensive experiments terminated with out-of-memory errors.
- Why unresolved: The paper only tested on datasets that fit within the available 30GB RAM, leaving performance on larger datasets unexplored.
- What evidence would resolve it: Benchmark results comparing BM25S on datasets larger than 30GB, along with performance metrics and memory usage data.

### Open Question 2
- Question: How does BM25S compare to other lexical search implementations in terms of query latency and memory efficiency when deployed on edge devices?
- Basis in paper: [inferred] The paper highlights BM25S's minimal dependencies and potential for edge deployments, but doesn't provide direct comparisons or performance metrics for edge scenarios.
- Why unresolved: While the paper mentions suitability for edge devices, it lacks concrete data on latency and memory usage in such environments.
- What evidence would resolve it: Comparative benchmark results showing query latency and memory usage of BM25S versus other implementations on edge devices with limited resources.

### Open Question 3
- Question: What is the impact of different tokenization schemes on BM25S's performance across various languages and domains?
- Basis in paper: [explicit] The paper mentions using a customized Python-based tokenizer and explores the impact of stemming and stopwords, but doesn't extensively test different tokenization schemes across languages.
- Why unresolved: The paper focuses on English datasets and a specific tokenization approach, leaving the performance impact of other tokenization methods across different languages unexplored.
- What evidence would resolve it: Comprehensive benchmark results comparing BM25S performance using various tokenization schemes across multiple languages and domains.

## Limitations

- The sparse matrix approach may become inefficient for extremely large vocabularies or very long documents where the matrix becomes less sparse
- Limited empirical validation of the score shifting method beyond three specific BM25 variants
- Focus on retrieval speed rather than search quality improvements means accuracy gains are not explored

## Confidence

- **High Confidence**: The core speedup mechanism through eager sparse scoring is well-supported by the mathematical formulation and empirical benchmarks across 14 datasets. The 100x+ QPS improvements are consistently demonstrated.
- **Medium Confidence**: The score shifting method for extending to multiple BM25 variants is theoretically sound but has limited empirical validation beyond the three specific variants mentioned.
- **Medium Confidence**: The claim of maintaining minimal accuracy impact is supported by NDCG@10 comparisons, though the paper doesn't explore edge cases where sparse representations might affect ranking quality.

## Next Checks

1. **Memory Efficiency Validation**: Test BM25S on progressively larger datasets (varying both document count and average document length) to identify the threshold where sparse matrix representation becomes less efficient than traditional inverted index approaches. Measure both memory usage and retrieval speed across this spectrum.

2. **Cross-Variant Generalization**: Implement and benchmark BM25S with additional BM25 variants beyond the three mentioned (BM25L, BM25+, TFl◦δ◦p×IDF) to verify that the score shifting method generalizes effectively to other non-zero scoring variants and to identify any variants where the approach breaks down.

3. **Hardware Scaling Analysis**: Evaluate BM25S performance on different hardware configurations (multi-core CPUs, GPUs if applicable) to determine whether the single-threaded implementation's advantages persist when scaling across cores, and whether there are opportunities for parallelization that maintain the sparse matrix benefits.