---
ver: rpa2
title: Cleaner Pretraining Corpus Curation with Neural Web Scraping
arxiv_id: '2402.14652'
source_url: https://arxiv.org/abs/2402.14652
tags:
- neuscraper
- pages
- data
- language
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents NeuScraper, a neural web scraper that extracts
  primary content from webpages for language model pretraining. The method converts
  HTML into textual sequences via DOM tree traversal, then uses a shallow neural architecture
  with XLM-Roberta and a transformer to classify nodes as primary content.
---

# Cleaner Pretraining Corpus Curation with Neural Web Scraping

## Quick Facts
- arXiv ID: 2402.14652
- Source URL: https://arxiv.org/abs/2402.14652
- Reference count: 21
- Key outcome: NeuScraper achieves 86.66% accuracy on ClueWeb22, surpassing state-of-the-art scrapers by over 20%, and demonstrates lower perplexity and better downstream performance for language models pretrained on its extracted data.

## Executive Summary
This paper introduces NeuScraper, a neural web scraper that extracts primary content from webpages for language model pretraining. The method converts HTML into textual sequences via DOM tree traversal, then uses a shallow neural architecture with XLM-Roberta and a transformer to classify nodes as primary content. Evaluated on ClueWeb22, NeuScraper achieves 86.66% accuracy, surpassing state-of-the-art scrapers by over 20%, and significantly reduces processing latency on GPU. Language models pretrained on NeuScraper-extracted data show lower perplexity and better downstream performance, demonstrating its ability to generate higher-quality pretraining corpora. The open-source tool offers an efficient, data-driven alternative to rule-based scrapers for large-scale corpus curation.

## Method Summary
NeuScraper converts HTML documents into textual sequences through DOM tree traversal, then employs a shallow neural architecture combining XLM-Roberta embeddings with a transformer classifier to identify primary content nodes. The system processes webpages by first parsing the HTML structure, then extracting features from each node including text content, HTML tags, and positional information. These features are fed into the neural classifier, which predicts whether each node belongs to the primary content or non-essential elements like navigation, advertisements, or boilerplate. The architecture is designed to be lightweight enough for GPU acceleration while maintaining high accuracy in content extraction.

## Key Results
- Achieves 86.66% accuracy on ClueWeb22 dataset, outperforming state-of-the-art scrapers by over 20 percentage points
- Significantly reduces processing latency compared to existing scrapers when running on GPU hardware
- Language models pretrained on NeuScraper-extracted data demonstrate lower perplexity and improved downstream task performance compared to models trained on traditionally scraped corpora

## Why This Works (Mechanism)
The method works by leveraging the hierarchical structure of HTML documents and the semantic richness of transformer-based representations. By converting HTML into sequential textual representations through DOM traversal, NeuScraper preserves both the structural relationships between elements and their semantic content. The XLM-Roberta component provides multilingual text understanding capabilities, while the transformer classifier learns to distinguish between primary content and non-essential elements based on their textual and structural features. This combination allows the system to make context-aware decisions about content relevance rather than relying on rigid rule-based heuristics.

## Foundational Learning
- **DOM tree traversal**: Understanding how HTML documents are structured as tree hierarchies is essential for converting webpages into sequential representations that preserve structural relationships. Quick check: Verify that the traversal order maintains parent-child relationships and sibling ordering.
- **Transformer-based classification**: The classifier learns to distinguish content types based on contextual embeddings rather than fixed rules. Quick check: Ensure the transformer architecture can handle variable-length input sequences from different webpage structures.
- **Multilingual embeddings**: XLM-Roberta provides cross-lingual understanding necessary for processing diverse web content. Quick check: Validate that embeddings capture semantic similarities across different languages present in the corpus.
- **GPU acceleration for web scraping**: Parallel processing capabilities enable efficient handling of large-scale web corpora. Quick check: Measure throughput improvements when processing multiple documents simultaneously on GPU versus CPU.
- **Binary node classification**: Each HTML element is classified independently as primary or non-primary content. Quick check: Confirm that the classifier maintains high precision on edge cases like mixed-content nodes.
- **End-to-end document reconstruction**: Aggregating classified nodes back into coherent documents requires careful handling of structural information. Quick check: Verify that extracted documents maintain logical flow and readability.

## Architecture Onboarding

Component map: HTML -> DOM Traversal -> Feature Extraction -> XLM-Roberta Embeddings -> Transformer Classifier -> Content Labels -> Document Reconstruction

Critical path: The feature extraction and embedding generation stages form the critical path, as they transform raw HTML into semantic representations that the classifier uses for decision-making. Any bottleneck in these stages directly impacts overall system throughput.

Design tradeoffs: The shallow architecture prioritizes inference speed over model complexity, making it suitable for large-scale corpus curation but potentially limiting its ability to capture very subtle content distinctions. The binary classification approach simplifies the problem but may struggle with nuanced content categories.

Failure signatures: Common failure modes include misclassification of content-heavy navigation elements, failure to recognize content embedded in non-standard HTML structures, and over-aggressive filtering that removes contextually important but structurally peripheral content.

First experiments:
1. Test classification accuracy on webpages with mixed content types (articles with embedded advertisements)
2. Measure processing latency on GPU for varying webpage sizes and complexities
3. Evaluate document reconstruction quality by comparing extracted text against original content using ROUGE metrics

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluation relies entirely on a single annotated dataset (ClueWeb22), limiting generalizability to other web domains or languages
- Reported accuracy metric measures binary classification per node rather than end-to-end extraction quality, potentially masking aggregation errors
- Latency improvements are measured only on GPU hardware, with no CPU-based comparison under realistic deployment scenarios

## Confidence
- High confidence in the core architectural approach combining DOM parsing with transformer classification
- Medium confidence in the quantitative claims about accuracy and latency improvements
- Low confidence in the downstream LM benefits until validated with larger-scale pretraining experiments

## Next Checks
1. Evaluate NeuScraper performance on multilingual web corpora beyond English to assess cross-lingual generalization capabilities
2. Conduct ablation studies comparing the full NeuScraper architecture against simplified versions to identify the most critical components
3. Perform large-scale pretraining experiments (multiple epochs, larger model sizes) to verify that downstream LM performance improvements scale with model capacity and training duration