---
ver: rpa2
title: 'M2SA: Multimodal and Multilingual Model for Sentiment Analysis of Tweets'
arxiv_id: '2404.01753'
source_url: https://arxiv.org/abs/2404.01753
tags:
- sentiment
- language
- multimodal
- languages
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of multilingual and multimodal
  sentiment analysis of tweets by creating a new dataset and evaluating several models.
  The core method idea involves curating existing Twitter sentiment datasets in 21
  languages and transforming them into a multimodal format by collecting associated
  images.
---

# M2SA: Multimodal and Multilingual Model for Sentiment Analysis of Tweets

## Quick Facts
- arXiv ID: 2404.01753
- Source URL: https://arxiv.org/abs/2404.01753
- Reference count: 0
- Primary result: XLM-RoBERTa-Sentiment-Multilingual + CLIP (X-SM+C) achieves best performance across 21 languages with F1-scores from 18.5% to 98.4%

## Executive Summary
This paper introduces M2SA, a multilingual and multimodal dataset and model for tweet sentiment analysis. The authors curated 21 language-specific Twitter sentiment datasets and enhanced them with associated images to create a unified multimodal corpus. They evaluate various configurations combining text encoders (Multilingual-BERT, XLM-RoBERTa, sentiment-tuned XLM-RoBERTa-Sentiment-Multilingual) with vision encoders (CLIP, DINOv2) using concatenation-based fusion. Results show multilingual models outperform monolingual ones, and multimodal models improve accuracy when visual content provides complementary sentiment cues. The best-performing configuration uses sentiment-tuned XLM-RoBERTa with CLIP, achieving strong performance across most languages.

## Method Summary
The method involves curating existing Twitter sentiment datasets across 21 languages and collecting associated images for each tweet. The text is processed using multilingual transformers (MBERT, XLM-R, XLMR-SM) and vision encoders (CLIP, DINOv2). Features from both modalities are concatenated and projected to sentiment classes (positive, negative, neutral). Models are trained both monolingually and multilingually, with machine translation used to augment low-resource languages. The sentiment-tuned XLM-RoBERTa-Sentiment-Multilingual model, pre-trained on tweet sentiment data, is expected to perform better at classification tasks.

## Key Results
- Multilingual models outperform monolingual models on average across 17 languages
- XLM-RoBERTa-Sentiment-Multilingual + CLIP (X-SM+C) achieves highest F1-scores for most languages
- Multimodal models show superior performance for high-resource languages when visual content provides complementary sentiment cues
- F1-scores range from 18.5% to 98.4% depending on language and model configuration

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal fusion improves sentiment accuracy when visual content provides complementary sentiment cues.
- Mechanism: Concatenates high-level text features from multilingual transformers with visual features from CLIP/DINOv2, jointly projecting to sentiment classes for cross-modal interactions.
- Core assumption: Image and text features are semantically aligned and capture independent but correlated aspects of sentiment.
- Evidence anchors: Abstract mentions sentiment-tuned LLM performs exceptionally well; section 6.1 shows multimodal models superior for higher-resourced languages; corpus shows weak image-sentiment correlation.
- Break condition: If images are neutral or unrelated to sentiment, fusion can introduce noise and degrade performance.

### Mechanism 2
- Claim: Sentiment-tuned multilingual transformer (XLMR-SM) improves classification over generic multilingual models.
- Mechanism: Fine-tuning on tweet sentiment datasets aligns transformer representations with sentiment-specific linguistic cues.
- Core assumption: Sentiment-specific training data captures domain-relevant patterns that transfer to new languages.
- Evidence anchors: Section 4.2.3 states XLMR-SM should perform better due to tweet and sentiment dataset training; section 6.1 shows XLMR-SM yields higher F1 than MBERT or XLM-R base; corpus lacks domain alignment quality signals.
- Break condition: If target languages or domains differ significantly from pre-training data, fine-tuning gains may vanish.

### Mechanism 3
- Claim: Multilingual joint training improves performance over monolingual training by sharing cross-lingual representations.
- Mechanism: Merging datasets across languages allows learning universal sentiment features that generalize across languages.
- Core assumption: Languages share sufficient sentiment-related semantic structure for transfer learning.
- Evidence anchors: Section 6.1 shows training single model for all languages yields best performance for 17 languages; section 4.3 mentions vision encoder models; corpus shows weak cross-language transfer signals.
- Break condition: If language pairs are too dissimilar or data too low-resource, joint training may degrade performance.

## Foundational Learning

- Concept: Multilingual Transformers and tokenization
  - Why needed here: Models like XLM-R and MBERT process multiple languages; tokenization affects feature extraction
  - Quick check question: How does a multilingual tokenizer differ from a monolingual one, and why does it matter for sentiment analysis?

- Concept: Multimodal feature fusion
  - Why needed here: Combining text and image embeddings is central to the proposed architecture
  - Quick check question: What are the common fusion strategies (concatenation, attention, gating) and when does each help?

- Concept: Machine translation for low-resource augmentation
  - Why needed here: Authors translate English tweets to fill gaps in lower-resourced languages
  - Quick check question: What are the risks of using MT for sentiment tasks, and how might they manifest in model outputs?

## Architecture Onboarding

- Component map: Input (tweet text + image) → Text encoder (MBERT/XLM-R/XLMR-SM) → 768-dim features → Image encoder (CLIP/DINOv2) → feature vector → Concatenation → Linear projection → Softmax (3 classes)

- Critical path: Text → encoder → features → concat → projection → prediction; same for image; then merge

- Design tradeoffs:
  - Unimodal vs. multimodal: added complexity vs. performance gain; image quality and relevance matter
  - Monolingual vs. multilingual: simplicity vs. generalization; data imbalance across languages
  - Sentiment-tuned vs. generic encoders: domain specificity vs. broader applicability

- Failure signatures:
  - Low F1 on certain languages: likely data sparsity or poor MT quality
  - Multimodal performance worse than unimodal: images are noisy or irrelevant
  - Translation artifacts degrading results: MT introducing semantic drift

- First 3 experiments:
  1. Train unimodal X-SM (XLMR-SM) on high-resource languages only; check baseline F1
  2. Add CLIP features to X-SM for same languages; compare F1 to unimodal baseline
  3. Repeat step 2 with multilingual training across all languages; analyze cross-language performance changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do machine-translated tweets affect the performance of sentiment analysis models in low-resource languages compared to original data?
- Basis in paper: Explicit mention that including machine-translated instances for lower-resourced languages did not yield significant performance improvements and sometimes even decreased performance
- Why unresolved: Paper only provides initial observations without detailed analysis of translation quality, context preservation, or other influencing factors
- What evidence would resolve it: Comprehensive study comparing models trained on original vs. machine-translated data across various low-resource languages, considering translation quality, context preservation, and domain specificity

### Open Question 2
- Question: What are the specific challenges in detecting figurative language and sarcasm in multimodal sentiment analysis of tweets?
- Basis in paper: Explicit mention that models cannot comprehend cases such as sarcasm where textual models predict neutral while multimodal models predict positive, despite original class being negative
- Why unresolved: Paper only briefly mentions this issue without delving into specific challenges or potential solutions
- What evidence would resolve it: Detailed analysis of challenges in detecting figurative language and sarcasm, including common patterns, potential solutions, and evaluation metrics for model performance

### Open Question 3
- Question: How does the performance of sentiment analysis models vary across different types of social media platforms (e.g., Twitter, Instagram, YouTube) and content formats (e.g., text, images, videos)?
- Basis in paper: Inferred from focus on Twitter data and mention that tweets sometimes include images, videos, etc., without exploring other platforms or formats
- Why unresolved: Paper only provides insights into Twitter data performance without investigating other social media platforms or content formats
- What evidence would resolve it: Comparative study evaluating model performance across different social media platforms and content formats, considering platform-specific features, user behavior, and content characteristics

## Limitations

- Image collection relies on external retrieval, potentially introducing biases based on image availability across languages and sentiment categories
- Machine translation for low-resource languages creates uncertainty about whether performance gains come from translation quality or model learning to ignore MT noise
- Corpus analysis revealed no explicit cross-lingual transfer mechanism validation, leaving unclear whether models learn shared sentiment representations or memorize language-specific patterns
- Dataset composition varies dramatically across languages (2,500 to 75,000 tweets), potentially biasing "best overall" multilingual model toward high-resource languages

## Confidence

**High Confidence**: Multilingual models outperforming monolingual ones is well-supported by consistent F1-score patterns across 17 languages with clearly specified methodology

**Medium Confidence**: XLMR-SM+C as optimal configuration has moderate support but primarily driven by high-resource language performance; results for low-resource languages are less conclusive and may be influenced by MT quality

**Low Confidence**: Assertion that images consistently provide complementary sentiment information is weakly supported; corpus signals show only weak correlation between images and positive sentiment, and paper doesn't systematically analyze when multimodal fusion helps versus hurts

## Next Checks

1. **Cross-lingual Transfer Analysis**: Conduct ablation studies where models are trained on subsets of languages and tested on unseen languages to quantify actual cross-lingual transfer versus language-specific memorization

2. **MT Quality Impact Measurement**: Create controlled experiment comparing model performance on original vs. machine-translated tweets for same sentiment instances, measuring translation confidence scores to identify when MT degradation affects sentiment classification

3. **Image Relevance Classification**: Implement automated pipeline to classify whether images are sentiment-relevant or neutral/background, then train separate unimodal and multimodal models on subsets of sentiment-relevant vs. neutral images to quantify actual contribution of visual information