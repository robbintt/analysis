---
ver: rpa2
title: Representation Surgery for Multi-Task Model Merging
arxiv_id: '2402.02705'
source_url: https://arxiv.org/abs/2402.02705
tags:
- uni00000013
- uni00000011
- surgery
- representation
- merging
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a critical issue in model merging for multi-task
  learning (MTL) called "representation bias," where the merged model's feature representations
  significantly differ from those of the individual models. To address this, the authors
  propose a lightweight, task-specific module called "Surgery" that filters out representation
  biases in the merged model.
---

# Representation Surgery for Multi-Task Model Merging

## Quick Facts
- **arXiv ID**: 2402.02705
- **Source URL**: https://arxiv.org/abs/2402.02705
- **Reference count**: 40
- **Primary result**: Lightweight Surgery module filters representation bias in merged multi-task models, achieving results close to traditional MTL

## Executive Summary
This paper addresses a critical issue in model merging for multi-task learning called "representation bias," where merged models produce feature representations that significantly differ from those of individual models. The authors propose a lightweight, task-specific Surgery module that filters out this bias through an unsupervised optimization objective that minimizes representation distance between the merged model and individual models. Extensive experiments across eight tasks and three architectures demonstrate significant performance improvements when applying Surgery to state-of-the-art merging methods, achieving results comparable to traditional multi-task learning while avoiding data management and privacy concerns.

## Method Summary
The proposed method introduces a lightweight, task-specific Surgery module that addresses representation bias in merged multi-task models. The Surgery module is inserted into the merged model architecture and uses an unsupervised optimization objective to minimize the distance between the merged model's representations and those of the individual models. This approach requires no additional labeled data and can be applied to existing model merging pipelines. The module operates by filtering out representation biases that emerge during the merging process, effectively aligning the merged model's feature space with that of the original individual models. The method is architecture-agnostic and can be integrated with various model merging techniques.

## Key Results
- Surgery module significantly improves multi-task performance when applied to state-of-the-art merging methods
- Achieved results close to traditional multi-task learning baselines across eight tasks and three architectures
- Demonstrated effectiveness without requiring additional labeled data or extensive computational overhead

## Why This Works (Mechanism)
The Surgery module works by identifying and filtering out representation biases that emerge when combining models trained on different tasks. During model merging, the combined model's feature representations often drift from those of the original individual models due to interference between task-specific features. The Surgery module uses an unsupervised optimization objective to minimize this representation drift by aligning the merged model's feature space with that of the original models. This alignment ensures that the merged model maintains the representational quality of the individual models while benefiting from the efficiency gains of model merging.

## Foundational Learning

**Representation Bias**
*Why needed*: Understanding how merged models can produce feature representations that differ significantly from individual models
*Quick check*: Compare activation patterns between merged and individual models on same inputs

**Model Merging Fundamentals**
*Why needed*: Grasping how different task models are combined into a single architecture
*Quick check*: Verify that merged model can perform all constituent tasks

**Unsupervised Representation Alignment**
*Why needed*: Comprehending how to optimize representation similarity without labeled data
*Quick check*: Measure representation distance metrics between models

**Multi-Task Learning Trade-offs**
*Why needed*: Understanding the balance between efficiency and performance in MTL
*Quick check*: Compare computation costs and accuracy between traditional MTL and merged models

## Architecture Onboarding

**Component Map**
Input -> Individual Models -> Merging Process -> Surgery Module -> Merged Model with Filtered Representations -> Task Outputs

**Critical Path**
The critical path involves the Surgery module's optimization loop where representation distances are computed and minimized between the merged model and individual models. This occurs after the initial model merging but before final fine-tuning.

**Design Tradeoffs**
The method trades off additional module complexity for improved representation fidelity. The lightweight nature of the Surgery module minimizes computational overhead while addressing the core issue of representation drift.

**Failure Signatures**
- Poor performance on individual tasks despite successful merging
- Increased representation distance between merged and individual models
- Inconsistent task outputs when compared to original models

**First Experiments**
1. Measure representation distance between merged and individual models before and after applying Surgery
2. Compare task-specific performance metrics with and without Surgery module
3. Test scalability by applying Surgery to increasingly complex model architectures

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions in its discussion section.

## Limitations
- Effectiveness may be architecture-dependent and not generalize equally across all model types
- Lacks ablation studies to quantify the contribution of individual components
- Does not thoroughly investigate scalability limits on larger architectures

## Confidence
- **Medium confidence**: The existence and characterization of "representation bias" as a distinct issue in model merging
- **High confidence**: The technical feasibility and implementation of the Surgery module as a lightweight task-specific filter
- **Medium confidence**: The quantitative improvements achieved when applying Surgery to state-of-the-art merging methods
- **Low confidence**: The claim that results are "close to traditional MTL" without considering computational overhead or practical deployment constraints

## Next Checks
1. Conduct ablation studies removing individual components of the Surgery module to quantify their relative contributions to performance improvements
2. Test the method on larger-scale architectures (e.g., BERT-large or beyond) and more diverse task combinations to assess scalability limits
3. Compare against alternative representation alignment techniques (e.g., knowledge distillation, contrastive learning) to establish whether Surgery provides unique advantages