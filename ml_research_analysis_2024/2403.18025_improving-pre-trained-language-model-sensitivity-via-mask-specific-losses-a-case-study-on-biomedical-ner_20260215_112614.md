---
ver: rpa2
title: 'Improving Pre-trained Language Model Sensitivity via Mask Specific losses:
  A case study on Biomedical NER'
arxiv_id: '2403.18025'
source_url: https://arxiv.org/abs/2403.18025
tags:
- masking
- ds-terms
- arxiv
- mslm
- mask
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Mask Specific Language Modeling (MSLM) to improve
  the sensitivity of fine-tuned language models to domain-specific terms (DS-terms).
  MSLM jointly masks DS-terms and generic words, then learns mask-specific losses
  by ensuring larger penalties for inaccurate predictions of DS-terms.
---

# Improving Pre-trained Language Model Sensitivity via Mask Specific losses: A case study on Biomedical NER

## Quick Facts
- arXiv ID: 2403.18025
- Source URL: https://arxiv.org/abs/2403.18025
- Reference count: 37
- Primary result: MSLM improves biomedical NER exact match scores by 3.2 percentage points on average

## Executive Summary
This paper proposes Mask Specific Language Modeling (MSLM) to improve the sensitivity of fine-tuned language models to domain-specific terms (DS-terms). MSLM jointly masks DS-terms and generic words, then learns mask-specific losses by ensuring larger penalties for inaccurate predictions of DS-terms. Experiments on biomedical NER datasets show that MSLM improves exact match scores by an average of 3.2 percentage points compared to vanilla fine-tuning. The optimal masking rate depends on the dataset and sequence length. MSLM also outperforms advanced masking strategies like PMI and span-based masking.

## Method Summary
MSLM jointly masks domain-specific terms (DS-terms) and generic words during pre-training, then applies mask-specific losses that impose larger penalties for inaccurate predictions of DS-terms compared to generic words. The method uses a masking rate parameter that can be optimized for different datasets and sequence lengths. During training, the model learns to prioritize accurate reconstruction of DS-terms while maintaining general language modeling capabilities. The approach is evaluated on biomedical NER tasks where DS-terms are particularly important for model performance.

## Key Results
- MSLM improves exact match scores by an average of 3.2 percentage points compared to vanilla fine-tuning on biomedical NER datasets
- The optimal masking rate varies depending on dataset characteristics and sequence length
- MSLM outperforms advanced masking strategies including PMI and span-based masking approaches
- The method improves language model sensitivity to domain-specific terms without negatively impacting downstream performance

## Why This Works (Mechanism)
MSLM works by creating a differentiated learning signal where domain-specific terms receive higher loss weights during training. This forces the model to pay more attention to DS-terms during reconstruction, improving their representation in the learned embeddings. By jointly masking both DS-terms and generic words, the model maintains its general language modeling capabilities while developing enhanced sensitivity to domain-specific vocabulary. The mask-specific loss mechanism ensures that errors in predicting DS-terms are penalized more heavily, leading to better retention and reconstruction of these critical terms.

## Foundational Learning
- Domain-specific term identification: Needed to distinguish DS-terms from generic vocabulary; quick check: evaluate precision/recall of DS-term detection
- Masked language modeling: Core pre-training objective that MSLM builds upon; quick check: verify standard MLM performance as baseline
- Loss weighting mechanisms: Required for implementing mask-specific penalties; quick check: test different weighting schemes
- NER task requirements: Understanding why DS-terms are critical for biomedical NER performance; quick check: analyze error types when DS-terms are mispredicted

## Architecture Onboarding
**Component Map:** Input text -> DS-term identification -> Joint masking -> Mask-specific loss computation -> Language model fine-tuning
**Critical Path:** DS-term identification → Joint masking → Mask-specific loss computation → Model weight updates
**Design Tradeoffs:** Higher masking rates improve DS-term sensitivity but may hurt general language modeling; lower rates maintain general capabilities but may not sufficiently emphasize DS-terms
**Failure Signatures:** Poor DS-term identification leads to ineffective masking; overly aggressive masking rates degrade overall model performance; improper loss weighting fails to differentiate DS-terms from generic words
**First Experiments:** 1) Compare MSLM vs vanilla fine-tuning on biomedical NER exact match scores; 2) Test different masking rates (10%, 30%, 50%) to find optimal value; 3) Evaluate DS-term vs generic word reconstruction accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- Performance improvements measured only on biomedical NER datasets, limiting generalizability to other domains
- Optimal masking rate varies by dataset but no clear guidelines provided for practitioners
- Evaluation focuses on exact match scores without examining other metrics like precision-recall balance
- Comparison baselines may not represent current state-of-the-art masking strategies

## Confidence
- MSLM improves biomedical NER exact match scores: High
- Optimal masking rate varies by dataset: Medium
- MSLM outperforms advanced masking strategies: Medium
- No negative impact on downstream performance: Medium

## Next Checks
1. Test MSLM on non-biomedical specialized domains (e.g., legal, financial) to assess cross-domain generalization
2. Evaluate performance on additional NER metrics beyond exact match (e.g., F1, precision-recall curves) and on non-NER downstream tasks
3. Conduct ablation studies to determine the relative contribution of mask-specific loss weighting versus domain-specific term identification