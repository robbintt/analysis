---
ver: rpa2
title: Contrastive Sequential Interaction Network Learning on Co-Evolving Riemannian
  Spaces
arxiv_id: '2401.01243'
source_url: https://arxiv.org/abs/2401.01243
tags:
- space
- curvature
- interaction
- learning
- riemannian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CSincere, a novel approach for learning representations
  of sequential interaction networks on co-evolving Riemannian manifolds. It addresses
  the limitations of existing methods by modeling users and items in separate Riemannian
  spaces that evolve over time, capturing their inherent differences and dynamic patterns.
---

# Contrastive Sequential Interaction Network Learning on Co-Evolving Riemannian Spaces

## Quick Facts
- arXiv ID: 2401.01243
- Source URL: https://arxiv.org/abs/2401.01243
- Authors: Li Sun; Junda Ye; Jiawei Zhang; Yong Yang; Mingsheng Liu; Feiyang Wang; Philip S. Yu
- Reference count: 14
- Primary result: CSincere significantly outperforms state-of-the-art baselines on sequential interaction network tasks, achieving up to 8.7% improvement in MRR and 6.5% in Recall@10.

## Executive Summary
This paper introduces CSincere, a novel approach for learning representations of sequential interaction networks on co-evolving Riemannian manifolds. The method addresses limitations of existing approaches by modeling users and items in separate Riemannian spaces that evolve over time, capturing their inherent differences and dynamic patterns. The core method combines a co-evolving graph neural network with cross-space aggregation and a neural curvature estimator, enabling message passing across different geometries and modeling space evolution. Additionally, it introduces a Riemannian co-contrastive learning strategy that interacts user and item spaces for interaction prediction without labels.

## Method Summary
CSincere models sequential interaction networks by representing users and items in separate co-evolving Riemannian spaces. The approach consists of three main components: a co-evolving GNN with cross-space aggregation for message passing across different geometric spaces, a neural curvature estimator (CurvNN) that dynamically estimates curvature evolution using Ricci curvature, and a reweighted co-contrastive learning strategy that enables self-supervised interaction prediction by contrasting temporal views of user and item embeddings simultaneously.

## Key Results
- Achieves up to 8.7% improvement in MRR compared to state-of-the-art baselines
- Demonstrates 6.5% improvement in Recall@10 on benchmark datasets
- Shows effectiveness across 5 public datasets including MOOC, Wikipedia, Reddit, LastFM, and Movielen

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modeling users and items in separate co-evolving Riemannian manifolds better captures their inherent structural differences compared to a single Euclidean space.
- Mechanism: Separate κ-stereographic spaces (user space and item space) allow each type of node to evolve according to its own curvature over time, while the Cross-Space Aggregation module enables message passing between them using gyrovector operations that respect the different geometries.
- Core assumption: Users and items have fundamentally different structural patterns that cannot be adequately represented in a shared space, and these patterns evolve differently over time.
- Evidence anchors:
  - [abstract]: "On the bipartite nature, is it appropriate to place user and item nodes in one identical space regardless of their inherent difference?"
  - [section]: "Rather than a single space, it is more rational to model the users and items in two different spaces. Riemannian geometry provides the notion of curvature to distinguish the structural pattern between different spaces."
  - [corpus]: Weak evidence - the corpus contains related Riemannian manifold work but no direct evidence for this specific bipartite separation claim.
- Break condition: If user and item structures are sufficiently similar or if the computational overhead of separate spaces outweighs the representational benefits.

### Mechanism 2
- Claim: Neural curvature estimation based on Ricci curvature enables dynamic modeling of space evolution that improves prediction accuracy.
- Mechanism: The CurvNN component estimates sectional curvature from Ricci curvature observations on sampled subgraphs, allowing the representation spaces to adapt their geometry as the network evolves over time.
- Core assumption: The underlying structure of the sequential interaction network changes over time in ways that can be captured by curvature evolution, and Ricci curvature provides sufficient information to estimate this change.
- Evidence anchors:
  - [abstract]: "On the network dynamics, instead of a fixed curvature space, will the representation spaces evolve when new interactions arrive continuously?"
  - [section]: "We learn the temporal evolvement of user/item space curvatures via a neural curvature estimator (CurvNN). Co-evolving GNN utilizes CurvNN for both user and item spaces, so that they co-evolve with each other over time."
  - [corpus]: Moderate evidence - related work on dynamic graph learning in Riemannian spaces exists, but the specific neural curvature estimation approach appears novel.
- Break condition: If the computational cost of curvature estimation becomes prohibitive or if the structural changes are too subtle to be captured by curvature alone.

### Mechanism 3
- Claim: Co-contrastive learning between user and item spaces improves interaction prediction by capturing cross-type relationships without requiring labels.
- Mechanism: The reweighted co-contrastive loss contrasts temporal views of user and item embeddings simultaneously, with hard sample reweighting that emphasizes difficult-to-distinguish pairs, allowing the two spaces to inform each other during self-supervised learning.
- Core assumption: The correlation between users and items contains valuable information for interaction prediction that can be exploited through contrastive learning, and hard sample mining improves learning efficiency.
- Evidence anchors:
  - [abstract]: "thereafter, we present a Reweighed Co-Contrast between the temporal views of the sequential network, so that the couple of Riemannian spaces interact with each other for the interaction prediction without labels."
  - [section]: "In the co-contrast, user space and item space interact with each other for interaction prediction. In the meantime, we pay more attention to both hard negative and hard positive samples with the reweighing mechanism on Riemannian manifolds."
  - [corpus]: Moderate evidence - contrastive learning is well-established for graph representation, but co-contrastive learning between separate spaces appears to be a novel contribution.
- Break condition: If the co-contrastive learning fails to converge or if the hard sample mining strategy leads to unstable training dynamics.

## Foundational Learning

- Concept: Riemannian geometry and κ-stereographic model
  - Why needed here: The paper relies on representing nodes in curved spaces rather than Euclidean space, requiring understanding of how distances, angles, and vector operations work in hyperbolic and spherical geometries.
  - Quick check question: What is the relationship between sectional curvature and the type of geometry (hyperbolic, spherical, Euclidean) in the κ-stereographic model?

- Concept: Graph neural networks and message passing
  - Why needed here: The model extends standard GNN message passing to work across different Riemannian spaces, requiring understanding of how node representations are updated based on neighborhood information.
  - Quick check question: How does the Cross-Space Aggregation module modify standard GNN message passing to handle nodes in different geometric spaces?

- Concept: Contrastive learning and self-supervised objectives
  - Why needed here: The paper introduces a novel co-contrastive learning approach that requires understanding how to construct positive/negative pairs and design loss functions for representation learning without labels.
  - Quick check question: What is the difference between standard contrastive learning and the co-contrastive approach proposed in this paper?

## Architecture Onboarding

- Component map: Input interactions -> Time encoding and feature integration -> Co-evolving GNN with Cross-Space Aggregation -> Neural Curvature Estimator (CurvNN) -> Reweighted Co-Contrastive Learning -> Interaction prediction
- Critical path: Data flows from input interactions through time encoding and feature integration, then through the Co-evolving GNN layers with Cross-Space Aggregation, followed by curvature estimation, and finally through the co-contrastive learning module for prediction.
- Design tradeoffs: The choice of separate spaces increases representational capacity but adds computational complexity; the curvature estimation adds temporal modeling but requires expensive subgraph sampling; the co-contrastive approach captures cross-type relationships but may be harder to train than standard contrastive methods.
- Failure signatures: Poor performance may indicate issues with curvature estimation stability, co-contrastive learning convergence, or inappropriate choice of κ values for the stereographic model.
- First 3 experiments:
  1. Test Cross-Space Aggregation with fixed curvature values to verify basic message passing works across spaces.
  2. Validate Neural Curvature Estimator on synthetic data with known curvature evolution patterns.
  3. Evaluate co-contrastive learning with simplified loss (no reweighting) to establish baseline effectiveness before adding complexity.

## Open Questions the Paper Calls Out
None

## Limitations
- Limited empirical validation with only 5 public datasets without comprehensive ablation studies
- Insufficient details about neural curvature estimator implementation and training stability
- Lack of statistical significance testing and comparison against simpler baselines

## Confidence
- Core claims about separate co-evolving Riemannian spaces: Medium
- Neural curvature estimation component: Low
- Co-contrastive learning effectiveness: Medium

## Next Checks
1. Conduct ablation studies removing the co-contrastive learning component to quantify its specific contribution to performance gains.
2. Test model robustness across varying κ values in the stereographic model to establish sensitivity to this hyperparameter.
3. Implement and evaluate the curvature estimation component on synthetic data with known ground-truth curvature evolution to validate its accuracy.