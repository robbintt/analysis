---
ver: rpa2
title: Explaining Multi-modal Large Language Models by Analyzing their Vision Perception
arxiv_id: '2405.14612'
source_url: https://arxiv.org/abs/2405.14612
tags:
- person
- image
- user
- output
- assistant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to explain multi-modal large language
  models (MLLMs) by analyzing their vision perception. The core idea is to combine
  an open-world localization model with a MLLM, creating a joint architecture that
  produces both text and object localization outputs from the same vision embedding.
---

# Explaining Multi-modal Large Language Models by Analyzing their Vision Perception

## Quick Facts
- arXiv ID: 2405.14612
- Source URL: https://arxiv.org/abs/2405.14612
- Authors: Loris Giulivi; Giacomo Boracchi
- Reference count: 40
- One-line primary result: This paper introduces a method to explain multi-modal large language models (MLLMs) by analyzing their vision perception.

## Executive Summary
This paper proposes a novel approach to explain MLLMs by combining an open-world localization (OWL) model with an MLLM in a joint architecture. The method enables generating saliency maps for any output token, detecting model hallucinations, and assessing biases through semantic adversarial perturbations. Experiments demonstrate the effectiveness of the approach across multiple interpretability tasks using standard datasets.

## Method Summary
The method combines LLaVa (MLLM) with OWL-ViT (open-world object localization) using an alignment layer that transforms OWL's vision embedding to be compatible with LLaVa's language model. The joint architecture produces both text and object localization outputs from the same vision embedding, enabling gradient-based saliency map generation, hallucination detection through correlation between outputs, and bias assessment via semantic adversarial perturbations.

## Key Results
- The joint OWL-MLLM architecture achieves GPT-4 scores comparable to ground truth and LLaVa alone
- The Gradient Alignment saliency map demonstrates high simulatability in user studies (0.81 accuracy)
- The method successfully detects hallucinations by identifying correlations between text and detection outputs
- Semantic adversarial perturbations effectively measure biases in the model's responses to different demographic groups

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The joint architecture enables saliency map explanations for any output token by leveraging the shared vision embedding between the OWL model and MLLM.
- Mechanism: By computing gradients of both the text output (OMLLM) and detection output (OO W L) with respect to the shared vision embedding tO W L i, the method measures cosine similarity between these gradients to identify which bounding boxes (and thus image regions) are relevant to the explained token.
- Core assumption: Gradients with respect to the shared embedding will reflect semantic correlation between the two outputs, even though the bounding box coordinates are independent of the query concept.
- Evidence anchors:
  - [abstract]: "enabled by the tight link between the two outputs, we design adversarial perturbations to OO W L that reflect a semantic change to the shared embedding tO W L i and thus to OMLLM"
  - [section 3.3]: "we measure the cosine similarity φ( ∂zm ∂EO W L, ∂ˆln[s] ∂EO W L) where zm indicates the relevance of the box to query c. If this value is high, then the image region covered by the m-th bounding box in OO W L is relevant to the output logit ln[s] in OMLLM."
  - [corpus]: Weak - no direct corpus evidence found for this specific gradient alignment method for MLLMs.
- Break condition: If the gradients of the two outputs with respect to the shared embedding are not semantically correlated, the saliency map will not accurately identify relevant image regions.

### Mechanism 2
- Claim: The architecture can detect hallucinations by exploiting the correlation between errors in the text and detection outputs.
- Mechanism: When the MLLM hallucinates an object (text output refers to non-existent object), the detection output will also identify this non-present object with high confidence scores zi, since both outputs are based on the same vision embedding.
- Core assumption: Errors in the MLLM's understanding of the image will be reflected in both outputs due to the shared vision embedding.
- Evidence anchors:
  - [abstract]: "the proposed architecture greatly promotes interpretability, enabling us to design a novel saliency map to explain any output token, to identify model hallucinations"
  - [section 3.2]: "when the MLLM outputs text referring to objects that are not present in the image, then the detection output also displays these objects"
  - [section 4.2]: "For each class ω∈Ω, we record the model's Yes/No response and the maximal detection maxq... It is clear that for classes where the model answers 'Yes', the average maxq is higher"
  - [corpus]: Weak - no direct corpus evidence found for this specific hallucination detection method for MLLMs.
- Break condition: If the MLLM and detection outputs are not strongly correlated, hallucination detection will not be reliable.

### Mechanism 3
- Claim: Semantic adversarial perturbations can assess and measure biases in the MLLM by altering the shared vision embedding to change perceived image semantics.
- Mechanism: By designing perturbations that substitute concepts in C- with concepts in C+ in the visual representation tO W L i, and observing the effects on the MLLM output OMLLM, the method can identify biases in how the model responds to different demographic groups.
- Core assumption: The adversarial perturbation can meaningfully alter the visual semantics in the embedding without affecting other aspects, and these changes will be reflected in the MLLM output.
- Evidence anchors:
  - [abstract]: "we design adversarial perturbations to OO W L that reflect a semantic change to the shared embedding tO W L i and thus to OMLLM, and exploit these perturbations to assess and measure MLLM biases"
  - [section 3.4]: "we can exploit the shared vision representation tO W L i to measure these effects... we design perturbations e applied to tO W L i to induce a particular effect on OO W L and, by analyzing its effects on OMLLM, we can gain insight on the MLLM's functioning"
  - [section 4.4]: "we construct datasets composed of 100 portrait images for each of the considered biases... we can assess susceptibility to biases by running J twice for each image, once using embedding tO W L 1 , . . . , tO W L 576 =E O W L(x), and once using the adversarial embedding"
  - [corpus]: Weak - no direct corpus evidence found for this specific semantic adversarial perturbation method for bias assessment in MLLMs.
- Break condition: If the adversarial perturbation cannot meaningfully alter the visual semantics or if the changes are not reflected in the MLLM output, bias assessment will not be effective.

## Foundational Learning

- Concept: Multi-modal large language models (MLLMs)
  - Why needed here: The paper focuses on explaining MLLMs, which combine image and text understanding. Understanding how MLLMs work is crucial for understanding the proposed interpretability methods.
  - Quick check question: What are the two main approaches for incorporating vision information into MLLMs, as discussed in the paper?

- Concept: Open-world object localization (OWL)
  - Why needed here: The proposed architecture combines an OWL model with an MLLM. Understanding OWL is essential for grasping how the joint architecture works and how it enables interpretability.
  - Quick check question: What is the output of an OWL model, and how does it differ from traditional object detection?

- Concept: Saliency maps and their evaluation
  - Why needed here: The paper proposes a novel saliency map method for explaining MLLM outputs. Understanding saliency maps and their evaluation methods is crucial for assessing the proposed approach.
  - Quick check question: What is simulatability in the context of saliency map evaluation, and why is it important?

## Architecture Onboarding

- Component map:
  OWL-ViT -> Alignment MLP (W) -> LLaVa, Detection heads (H)

- Critical path:
  1. Image is processed by OWL-ViT's vision encoder to get tO W L i
  2. Alignment MLP transforms tO W L i to tW i
  3. LLaVa processes the concatenated vision and text tokens to generate text output
  4. Detection heads process tO W L i with queries to generate bounding box outputs

- Design tradeoffs:
  - Using a frozen OWL-ViT model ensures good detection performance but may limit the flexibility of the joint architecture
  - The alignment MLP introduces information loss but enables compatibility between OWL's and LLaVa's embeddings
  - Treating vision tokens as part of the input sequence simplifies the architecture but may limit the model's ability to reason about complex visual relationships

- Failure signatures:
  - Poor alignment between OWL's and LLaVa's embeddings (indicated by low performance in GPT-as-a-judge evaluation)
  - Inconsistent or unreliable hallucination detection (indicated by low correlation between text and detection outputs)
  - Ineffective bias assessment (indicated by inconsistent results in the proposed benchmark)

- First 3 experiments:
  1. Train the alignment MLP W and evaluate its performance using the GPT-as-a-judge paradigm
  2. Test the hallucination detection capability by asking the model about objects in COCO images and comparing responses to detection outputs
  3. Validate the Gradient Alignment saliency map using a user study to assess simulatability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would fine-tuning the entire MLLM on OWL vision encoding instead of using an alignment layer affect performance and interpretability?
- Basis in paper: [explicit] The authors suggest this as a future work direction, noting that fine-tuning could potentially overcome limitations of the alignment layer and improve performance.
- Why unresolved: The paper only discusses the alignment layer approach and does not explore or compare it with a fine-tuned MLLM.
- What evidence would resolve it: Experiments comparing the proposed J model with a fine-tuned MLLM in terms of both performance (e.g., GPT-4 scores) and interpretability (e.g., hallucination detection, bias assessment) would resolve this question.

### Open Question 2
- Question: How does the Gradient Alignment (GA) saliency map perform compared to other saliency methods for explaining MLLM outputs?
- Basis in paper: [inferred] The paper introduces the GA saliency map as a novel method for explaining MLLM outputs, but does not compare it to other existing saliency methods.
- Why unresolved: The paper focuses on validating the GA saliency map through a user study and does not benchmark it against other saliency methods.
- What evidence would resolve it: A comparison of the GA saliency map with other saliency methods (e.g., Attention Visualization, Attention Rollout) in terms of their ability to explain MLLM outputs and enable simulatability would resolve this question.

### Open Question 3
- Question: How does the model's susceptibility to biases vary across different demographic groups and types of biases?
- Basis in paper: [explicit] The paper explores biological gender and ethnicity biases, but acknowledges that the selection of categories is not exhaustive.
- Why unresolved: The paper only examines two types of biases (biological gender and ethnicity) and uses a limited set of categories within each type.
- What evidence would resolve it: Experiments exploring a wider range of biases (e.g., age, disability, socioeconomic status) and using more diverse demographic categories would provide evidence to resolve this question.

## Limitations
- The method relies on a complex joint architecture where effectiveness depends on alignment between OWL and MLLM components
- The gradient correlation assumptions for saliency maps lack extensive empirical validation in MLLM literature
- The semantic adversarial perturbation approach for bias assessment may not generalize to all types of biases

## Confidence
- Saliency map explanations: Medium confidence - The gradient alignment mechanism is theoretically sound but lacks extensive validation on diverse MLLM architectures
- Hallucination detection: Medium confidence - The correlation assumption between text and detection outputs is reasonable but may not hold for all MLLM designs
- Bias assessment: Low confidence - The semantic adversarial perturbation approach is novel but has not been rigorously tested across different bias types and model architectures

## Next Checks
1. Test the saliency map method on a second MLLM architecture (e.g., GPT-4V or Gemini) to verify that the gradient alignment assumption holds across different model designs
2. Apply the hallucination detection method to multiple datasets beyond COCO (e.g., Visual Genome, Flickr30k) to assess robustness across different image distributions and object frequencies
3. Systematically remove components of the adversarial perturbation method (e.g., test with random perturbations, or perturbations without semantic constraints) to isolate which elements are essential for effective bias detection