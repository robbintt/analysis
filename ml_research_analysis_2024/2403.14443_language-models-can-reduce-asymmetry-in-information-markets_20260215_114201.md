---
ver: rpa2
title: Language Models Can Reduce Asymmetry in Information Markets
arxiv_id: '2403.14443'
source_url: https://arxiv.org/abs/2403.14443
tags:
- information
- answer
- agents
- each
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles the buyer\u2019s inspection paradox in information\
  \ markets, where buyers need access to information to assess its value, but sellers\
  \ must restrict access to prevent theft. To resolve this, the authors introduce\
  \ the Information Bazaar, a simulated marketplace where LLM-powered agents act on\
  \ behalf of buyers and vendors."
---

# Language Models Can Reduce Asymmetry in Information Markets

## Quick Facts
- arXiv ID: 2403.14443
- Source URL: https://arxiv.org/abs/2403.14443
- Authors: Nasim Rahaman; Martin Weiss; Manuel Wüthrich; Yoshua Bengio; Li Erran Li; Chris Pal; Bernhard Schölkopf
- Reference count: 30
- Primary result: LLM-powered agents can resolve the buyer's inspection paradox by previewing, evaluating, and selectively forgetting information in simulated marketplaces

## Executive Summary
This paper addresses the buyer's inspection paradox in information markets, where buyers need access to information to assess its value, but sellers must restrict access to prevent theft. The authors introduce the Information Bazaar, a simulated marketplace where LLM-powered agents act on behalf of buyers and vendors. These agents can preview privileged information, evaluate its relevance, and choose to forget non-purchased content, thereby reducing information asymmetry. Experiments show that agents can make rational purchasing decisions, with GPT-4 and debate prompting strategies yielding the best results.

## Method Summary
The Information Bazaar framework simulates an information marketplace where LLM agents act as buyers and vendors. Buyer agents post tenders for information, vendor agents respond with quotes, and buyer agents evaluate and purchase documents. The system uses BM25 and neural retrieval for document matching, debate prompting for rational decision-making, and GPT-4 evaluation for answer quality assessment. The framework is tested on a corpus of 725 ArXiv papers and 110 high-quality questions, with agents operating under budget constraints and inspection capabilities.

## Key Results
- Inspection capability significantly improves answer quality, especially at higher budgets
- Debate prompting substantially improves rational decision-making compared to direct prompting or chain-of-thought
- GPT-4 consistently outperforms other LLM models in making rational purchasing decisions
- Higher budgets enable better answers through more comprehensive information acquisition

## Why This Works (Mechanism)

### Mechanism 1: Dual Capability of Inspection and Forgetting
Language model agents can assess the value of privileged information and choose to forget non-purchased content, reducing information asymmetry. Buyer agents can preview document content before purchasing, then selectively retain only purchased information while forgetting rejected content. This mechanism assumes LLMs can reliably implement "forgetting" of content they choose not to purchase, satisfying vendor concerns about unauthorized retention.

### Mechanism 2: Debate Prompting for Rational Decision-Making
Debate prompting significantly improves LLM performance in making rational economic choices by balancing different value considerations. Simulating a debate between two characters (one focused on information quality, one on budget constraints) leads to more rational purchasing decisions than direct questioning or chain-of-thought methods.

### Mechanism 3: Inspection Enables Better Value Estimation
Allowing agents to inspect content before purchase leads to better answer quality, especially for higher budgets. Inspection allows agents to accurately assess document relevance and price appropriateness, leading to more efficient resource allocation and improved purchasing decisions.

## Foundational Learning

- **Concept**: Information asymmetry and the buyer's inspection paradox
  - **Why needed here**: Understanding why this problem is challenging and why traditional solutions (paywalls, subscriptions) are insufficient is crucial for appreciating the innovation of the Information Bazaar.
  - **Quick check question**: Why can't buyers simply trust vendor descriptions of document quality without inspection?

- **Concept**: Language model capabilities and limitations (context windows, reasoning, forgetting)
  - **Why needed here**: The entire system relies on LLMs making rational economic decisions and reliably forgetting content, so understanding their capabilities and limitations is essential.
  - **Quick check question**: What happens if an LLM "forgets" content but still has access to it through its context window in subsequent interactions?

- **Concept**: Economic decision-making and rational choice theory
  - **Why needed here**: The agents must make rational purchasing decisions based on price, quality, and budget constraints, requiring understanding of basic economic principles.
  - **Quick check question**: How should an agent decide between two documents of equal quality but different prices?

## Architecture Onboarding

- **Component map**: Principals -> Buyer agents -> Bulletin Board -> Vendor agents -> Retrieval systems -> Buyer agents -> Answer generation
- **Critical path**: Principal question → Buyer agent posts tender → Vendor agents respond with quotes → Buyer agent evaluates and purchases → Information retrieval and synthesis → Answer generation
- **Design tradeoffs**: Inspection vs. privacy (more inspection improves decisions but increases risk of unauthorized retention), budget allocation (higher budgets enable better answers but increase cost), model selection (more capable models make better decisions but are more expensive)
- **Failure signatures**: Agents consistently selecting suboptimal documents despite inspection, vendors refusing to grant inspection access, budget exhaustion without satisfactory answers, debate prompting failing to improve decision quality
- **First 3 experiments**: 1) Test rational decision-making with fungible information (identical documents at different prices), 2) Evaluate positional bias in quote selection across different LLM models, 3) Compare answer quality with and without inspection capability at fixed budget levels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would vendor agents adjusting prices in response to demand affect the overall market dynamics and information quality in the Information Bazaar?
- Basis in paper: [explicit]
- Why unresolved: The paper mentions this as a planned future direction but does not investigate it experimentally. Understanding price elasticity and its impact on information flow could be crucial for real-world applications.
- What evidence would resolve it: Experiments comparing market outcomes with and without dynamic pricing, measuring metrics like information quality, budget efficiency, and market stability.

### Open Question 2
- Question: Can fine-tuning Llama 2 (70B) based on human preferences improve its performance on rational decision-making tasks in the Information Bazaar?
- Basis in paper: [explicit]
- Why unresolved: The paper acknowledges Llama 2's performance limitations but suggests this as a future direction. Empirical validation is needed to determine if fine-tuning closes the performance gap with GPT models.
- What evidence would resolve it: Comparative experiments showing decision-making performance before and after fine-tuning on relevant tasks.

### Open Question 3
- Question: How effective would LLM-powered automatic interviews be for extracting and monetizing latent expert knowledge in real-world information markets?
- Basis in paper: [explicit]
- Why unresolved: This is mentioned as a potential future application but not explored. The technical challenges and practical viability of this approach remain untested.
- What evidence would resolve it: Case studies or experiments demonstrating the extraction of valuable information through automated interviews and its integration into the Information Bazaar framework.

## Limitations
- The "forgetting" mechanism is not technically specified, raising questions about whether vendors can truly trust that rejected information won't be retained
- Experimental setup uses synthetic corpus and pre-generated questions, limiting generalizability to real-world markets
- Heavy reliance on GPT-4 for both agent decisions and evaluation could introduce circular validation issues

## Confidence

- **High Confidence**: The observation that inspection improves answer quality and the experimental demonstration of debate prompting improving decision-making
- **Medium Confidence**: The claim that LLMs can reliably implement "forgetting" and that this mechanism sufficiently addresses vendor concerns
- **Low Confidence**: The generalizability of results to real-world information markets and the long-term viability of the inspection-forgetting mechanism

## Next Checks

1. **Technical Verification**: Test whether content truly cannot be retrieved after agents are instructed to forget it, using prompt injection and context window analysis to verify the forgetting mechanism works as claimed.

2. **Real-World Market Testing**: Deploy the Information Bazaar with actual vendors and buyers using real documents and questions to validate that vendors trust the forgetting mechanism and that the marketplace dynamics match theoretical predictions.

3. **Long-Term Behavior Analysis**: Run extended simulations with varying market conditions (information scarcity, different pricing strategies, agent model variations) to identify potential failure modes and emergent behaviors not captured in initial experiments.