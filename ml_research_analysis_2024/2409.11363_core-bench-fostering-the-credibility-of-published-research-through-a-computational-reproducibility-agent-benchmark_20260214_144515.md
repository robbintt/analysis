---
ver: rpa2
title: 'CORE-Bench: Fostering the Credibility of Published Research Through a Computational
  Reproducibility Agent Benchmark'
arxiv_id: '2409.11363'
source_url: https://arxiv.org/abs/2409.11363
tags:
- code
- agent
- task
- agents
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CORE-Bench, a benchmark designed to evaluate
  AI agents' ability to reproduce scientific research results. The benchmark consists
  of 270 tasks derived from 90 reproducible scientific papers across computer science,
  social science, and medicine.
---

# CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark

## Quick Facts
- arXiv ID: 2409.11363
- Source URL: https://arxiv.org/abs/2409.11363
- Reference count: 40
- Primary result: AI agents achieve 21% accuracy on hardest computational reproducibility tasks, highlighting significant room for improvement

## Executive Summary
This paper introduces CORE-Bench, a benchmark designed to evaluate AI agents' ability to reproduce scientific research results. The benchmark consists of 270 tasks derived from 90 reproducible scientific papers across computer science, social science, and medicine. Tasks vary in difficulty and require skills such as code execution, dependency installation, and result extraction from text and figures. Two baseline agents were evaluated: AutoGPT and a task-specific CORE-Agent, both using GPT-4o and GPT-4o-mini. The results demonstrate that while automating computational reproducibility is challenging, task-specific modifications to general-purpose agents can substantially improve performance.

## Method Summary
The paper develops CORE-Bench using 270 tasks from 90 reproducible scientific papers sourced from CodeOcean capsules. Each task requires agents to reproduce research results by navigating file systems, installing dependencies, executing code, and extracting results from text and figures. The evaluation harness runs each task in an isolated virtual machine, enabling parallel testing across hundreds of tasks simultaneously. Two baseline agents are evaluated: AutoGPT (general-purpose) and CORE-Agent (task-specific modifications), both using GPT-4o and GPT-4o-mini as language model backends. The benchmark measures task accuracy across three difficulty levels (Easy, Medium, Hard) based on the complexity of code execution and result extraction required.

## Key Results
- Best-performing agent (CORE-Agent with GPT-4o) achieved 21% accuracy on Hard tasks
- CORE-Agent with GPT-4o outperformed GPT-4o-mini by 10 percentage points overall
- Task-specific modifications to AutoGPT improved performance significantly with minimal changes
- Evaluation harness reduced testing time from 20+ days to hours through parallel VM execution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task-specific modifications to general-purpose agents significantly improve performance on computational reproducibility tasks.
- Mechanism: The paper shows that simple task-specific adaptations like adding programmatic checks for output files and specific prompts for common failure modes can substantially boost accuracy compared to unmodified agents.
- Core assumption: General-purpose agents have the fundamental capabilities needed for reproducibility tasks, but lack domain-specific guidance and error handling.
- Evidence anchors:
  - [abstract]: "Having agents that can reproduce existing work is a necessary step towards building agents that can conduct novel research"
  - [section 4.2]: "Comparing performance when fixing the LLM model, we observed that AutoGPT's performance improved substantially with only slight modifications"
  - [corpus]: Weak - corpus doesn't directly address this mechanism
- Break condition: If general-purpose agents lack fundamental capabilities for reproducibility (like basic code execution or file manipulation), task-specific modifications would not be sufficient.

### Mechanism 2
- Claim: Isolated virtual machine environments enable efficient parallel evaluation of reproducibility agents.
- Mechanism: The evaluation harness runs each task in an isolated VM, allowing hundreds of tasks to run in parallel while maintaining reproducibility and preventing agents from tampering with the benchmark.
- Core assumption: Agent-environment interaction is a key component of the task, requiring isolated testing environments.
- Evidence anchors:
  - [section 3]: "Since agents are evaluated on their ability to navigate, manipulate, and understand their environments, each task of CORE-Bench runs in an isolated virtual machine"
  - [section 3]: "The evaluation harness dramatically reduces evaluation time from over 20 days to mere hours by running on hundreds of parallel virtual machines"
  - [corpus]: Weak - corpus doesn't directly address this mechanism
- Break condition: If agent tasks don't require significant environment interaction, isolated VMs may be unnecessary overhead.

### Mechanism 3
- Claim: Building benchmarks on already-reproducible repositories (CodeOcean capsules) enables creation of challenging but construct-valid evaluation tasks.
- Mechanism: By sourcing tasks from CodeOcean capsules known to be reproducible, the benchmark can focus on agent capabilities rather than verifying paper reproducibility, while still maintaining real-world difficulty.
- Core assumption: CodeOcean capsules provide a reliable source of reproducible papers that represent real-world computational reproducibility challenges.
- Evidence anchors:
  - [section 2.1]: "Since the benchmark is measuring the ability of agents to reproduce the results of running the code associated with the paper, and not to ensure that the results reported directly in the paper are correct, we did not see a need to include irreproducible papers in the benchmark"
  - [section 2.1]: "To address this, we based our benchmark on CodeOcean capsules (See Figure 2), which are known to be reproducible with little effort"
  - [corpus]: Weak - corpus doesn't directly address this mechanism
- Break condition: If CodeOcean capsules don't adequately represent the diversity and difficulty of real-world computational reproducibility challenges.

## Foundational Learning

- Concept: Computational reproducibility
  - Why needed here: Understanding the core task agents must perform (reproducing research results using provided code and data)
  - Quick check question: What are the key challenges in computational reproducibility mentioned in the paper?

- Concept: Agent-environment interaction
  - Why needed here: Agents must navigate file systems, install dependencies, execute code, and extract results
  - Quick check question: Why does the evaluation harness use isolated VMs for each task?

- Concept: Vision-language model capabilities
  - Why needed here: Some tasks require extracting information from figures and plots
  - Quick check question: How do vision-based questions differ from text-based questions in terms of agent performance?

## Architecture Onboarding

- Component map: CORE-Bench dataset -> Evaluation harness -> Baseline agents (AutoGPT, CORE-Agent) -> LLM backends (GPT-4o, GPT-4o-mini)
- Critical path: Agent receives task → Sets up environment → Runs code → Extracts results → Submits report → Evaluation harness checks correctness
- Design tradeoffs: Isolated VMs provide security and reproducibility but add overhead; task-specific modifications improve performance but reduce generalizability
- Failure signatures: 
  - Timeout during dependency installation
  - Inability to locate relevant results in multi-file outputs
  - Incorrect extraction from figures vs text
  - Context window exhaustion on complex tasks
- First 3 experiments:
  1. Run CORE-Agent with GPT-4o on a single EASY task to verify basic functionality
  2. Test agent behavior on a MEDIUM task with Docker requirements
  3. Evaluate performance on a HARD task requiring dependency installation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can AI agents reliably determine which specific version of a dependency to install when reproducing scientific code, especially when documentation is incomplete?
- Basis in paper: [explicit] The paper describes a case where an agent attempted to install network-diffusion version 0.14.4, but the code required version 0.6 for a specific import statement.
- Why unresolved: The paper shows agents can identify the correct package name but struggle with version compatibility issues, and there's no systematic approach described for determining correct versions.
- What evidence would resolve it: Development of an agent that can consistently identify and install correct dependency versions across diverse codebases, or a benchmark demonstrating current agents' success rates at this task.

### Open Question 2
- Question: What is the optimal trade-off between model strength and task-specific modifications for AI agents performing computational reproducibility tasks?
- Basis in paper: [explicit] The paper shows GPT-4o outperforms GPT-4o-mini despite the latter's lower per-token cost, and that task-specific modifications improved performance more for weaker models.
- Why unresolved: While the paper demonstrates these relationships exist, it doesn't establish optimal configurations or determine how this trade-off scales with future model improvements.
- What evidence would resolve it: Systematic experiments varying model size, cost limits, and task-specific modifications to identify optimal configurations across different difficulty levels.

### Open Question 3
- Question: How can we develop effective guardrails to prevent AI agents from taking potentially harmful or unintended actions during computational reproducibility tasks?
- Basis in paper: [explicit] The paper describes an agent attempting to create accounts on CodeOcean and search for repository information online, highlighting safety concerns.
- Why unresolved: The paper identifies safety issues but doesn't propose comprehensive solutions, and as agents become more capable, these risks could increase.
- What evidence would resolve it: Development and validation of safety mechanisms that prevent agents from taking unintended actions while maintaining their ability to complete legitimate tasks.

## Limitations
- Best-performing agent achieved only 21% accuracy on hardest tasks, indicating substantial room for improvement
- Benchmark focuses on already-reproducible CodeOcean capsules rather than addressing the broader reproducibility crisis in science
- Task-specific modifications may limit generalizability to other domains beyond computational reproducibility

## Confidence

**High confidence**: The evaluation methodology using isolated VMs for parallel testing is sound and well-implemented, with clear improvements in evaluation efficiency.

**Medium confidence**: The claim that task-specific modifications significantly improve performance is supported by results, though the specific modifications are not fully detailed.

**Medium confidence**: The benchmark provides a valuable tool for advancing AI reproducibility capabilities, but its focus on already-reproducible papers may limit its ability to address the broader reproducibility crisis in science.

## Next Checks

1. **Generalizability Test**: Evaluate CORE-Agent performance on non-CodeOcean papers to assess whether task-specific modifications generalize beyond curated reproducible datasets.

2. **Failure Mode Analysis**: Conduct detailed error analysis on the 79% of Hard tasks that failed to identify specific bottlenecks (e.g., dependency installation failures vs. result extraction issues).

3. **Scalability Assessment**: Test the evaluation harness with larger agent fleets and more complex tasks to verify that parallel processing scales efficiently and maintains isolation guarantees.