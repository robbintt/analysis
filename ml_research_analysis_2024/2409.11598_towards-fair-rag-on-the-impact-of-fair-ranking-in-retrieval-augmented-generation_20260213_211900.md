---
ver: rpa2
title: 'Towards Fair RAG: On the Impact of Fair Ranking in Retrieval-Augmented Generation'
arxiv_id: '2409.11598'
source_url: https://arxiv.org/abs/2409.11598
tags:
- ranking
- fairness
- items
- retrieval
- utility
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the impact of fair ranking in retrieval-augmented
  generation (RAG) systems, addressing both ranking fairness and attribution fairness.
  The authors evaluate RAG models that integrate fairness-aware retrieval by experimenting
  with twelve RAG models across seven tasks, using stochastic rankers to ensure equitable
  exposure of relevant items.
---

# Towards Fair RAG: On the Impact of Fair Ranking in Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2409.11598
- Source URL: https://arxiv.org/abs/2409.11598
- Reference count: 40
- Primary result: Fair ranking in RAG systems can maintain or improve both retrieval quality and generation quality while ensuring equitable source attribution

## Executive Summary
This paper investigates the impact of fair ranking in retrieval-augmented generation (RAG) systems, addressing both ranking fairness and attribution fairness. The authors evaluate RAG models that integrate fairness-aware retrieval by experimenting with twelve RAG models across seven tasks, using stochastic rankers to ensure equitable exposure of relevant items. They find that incorporating fairness-aware retrieval often maintains or even enhances both ranking quality and generation quality, challenging the assumption that fairness compromises performance. Additionally, fair retrieval practices lead to more balanced attribution in the final responses, ensuring equitable exposure of the sources cited by the generator.

## Method Summary
The authors evaluate twelve RAG models on seven LaMP tasks using stochastic rankers (Plackett-Luce sampling) with fairness control parameter Œ± to sample multiple rankings. They compute utility labels for each corpus item based on marginal utility gain provided to a language model. The RAG models use different retriever-generator combinations (BM25, SPLADE, Contriever with Flan-T5 and Flan-UL2) and are evaluated using Expected Exposure Disparity (EE-D), Expected Utility (EU), Expected Attribution Rate (EAR), and Expected Attributed Exposure Disparity (EAE-D) metrics. Experiments are run with Œ± values of 1, 2, 4, and 8, sampling 100 rankings per query and truncating to top-5 items.

## Key Results
- Fair ranking often maintains or enhances both retrieval and generation quality, contradicting the assumption that fairness compromises performance
- Incorporating fairness-aware retrieval leads to more balanced attribution of sources in the final generated output
- Fair retrieval practices can reduce the inclusion of distracting items, improving overall system utility

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fair ranking via stochastic sampling can maintain or even improve both retrieval and generation quality.
- Mechanism: By adjusting the fairness control parameter ùõº, the stochastic retriever samples rankings that expose relevant items more equitably while still favoring higher-quality items, reducing the impact of fixed positional bias.
- Core assumption: Machine users (generators) benefit from fair exposure because it increases the chance of including useful but lower-ranked items.
- Evidence anchors:
  - [abstract] "incorporating fairness-aware retrieval often maintains or even enhances both ranking quality and generation quality"
  - [section] "RAG models equipped with a fair ranker can often preserve a significant level of retrieval and generation quality, and in some cases, even surpass the quality achieved by the traditional RAG setup with a deterministic ranker"
- Break condition: If the generator's attention mechanism strongly prefers only top-ranked items regardless of fairness, the benefit disappears.

### Mechanism 2
- Claim: Fair retrieval leads to more balanced attribution of sources in the final generated output.
- Mechanism: By ensuring equitable exposure of retrieved items, the stochastic ranker increases the likelihood that multiple relevant sources are considered, leading the generator to cite them more evenly.
- Core assumption: The generator's attribution decisions are influenced by the set of items it receives, not just the top item.
- Evidence anchors:
  - [abstract] "fair retrieval practices lead to more balanced attribution in the final responses, ensuring that the generator fairly cites the sources it relies on"
  - [section] "equitable retrieval frequently leads to more equitable usage of those sources by the generator"
- Break condition: If the generator only uses a small fixed subset of retrieved items, fairness in retrieval has limited impact on attribution.

### Mechanism 3
- Claim: Fair ranking can reduce the inclusion of distracting items, improving overall system utility.
- Mechanism: Stochastic sampling breaks the deterministic bias toward high-scoring but potentially irrelevant items, allowing the system to explore a wider range of potentially useful content.
- Core assumption: High retrieval scores do not always correlate with high utility for the generator.
- Evidence anchors:
  - [section] "generators are not robust to changes in the position of useful information" and "items with high retrieval scores often include distracting content that can reduce the system-effectiveness"
- Break condition: If all high-scoring items are also highly useful, randomization provides no benefit.

## Foundational Learning

- Concept: Stochastic ranking via Plackett-Luce sampling
  - Why needed here: To replace deterministic retrieval with a probabilistic approach that can ensure equitable exposure of relevant items.
  - Quick check question: How does the fairness control parameter ùõº influence the sampling distribution in Plackett-Luce sampling?

- Concept: Expected exposure disparity (EE-D) and expected exposure relevance (EE-R)
  - Why needed here: To measure how fairly items are exposed in rankings and how well those rankings align with utility.
  - Quick check question: What does a normalized EE-D of 1 represent in the context of this study?

- Concept: Natural Language Inference (NLI) for attribution measurement
  - Why needed here: To determine whether retrieved items are actually used by the generator in the final output.
  - Quick check question: How does NLI-based attribution differ from simple keyword matching in evaluating source usage?

## Architecture Onboarding

- Component map:
  Query ‚Üí Stochastic retriever (Plackett-Luce with fairness control) ‚Üí Sampled rankings ‚Üí Generator (Flan-T5/UL2) ‚Üí Output
  Utility labels computed per item by measuring marginal gain when added to generator context
  NLI model for attribution rate and attributed exposure metrics

- Critical path:
  1. Compute utility labels for all corpus items
  2. Set fairness parameter ùõº
  3. Sample N rankings per query
  4. Generate outputs and measure utility and attribution
  5. Compute fairness and quality metrics

- Design tradeoffs:
  - Higher fairness (lower ùõº) may reduce immediate utility but improves long-term fairness
  - Sampling size N affects metric stability vs. evaluation cost
  - Top-k truncation balances context length vs. retrieval breadth

- Failure signatures:
  - High EE-D with low EAR suggests retrieval fairness without generator usage
  - Low EE-D with high EAE-D indicates fair retrieval but unfair consumption
  - Consistently low utility across ùõº values may indicate utility labeling issues

- First 3 experiments:
  1. Run with ùõº=1 vs ùõº=8 on a small subset to observe EE-D change
  2. Compare EAR between deterministic and stochastic retriever for fixed queries
  3. Measure EAE-D vs EE-D to detect consumption fairness gaps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the utility labeling process need to be adjusted when multiple items in the retrieval set have interacting or conflicting utility gains for the generator?
- Basis in paper: [explicit] The paper acknowledges that their current utility labeling considers single items, while multiple items may yield contrasting utility gains.
- Why unresolved: The paper uses single-item utility labeling for simplicity, but recognizes this limitation and its potential impact on fairness evaluation.
- What evidence would resolve it: Experiments comparing single-item labeling versus multi-item interaction labeling across various RAG tasks and generator models.

### Open Question 2
- Question: What is the relationship between long-context generators and fairness metrics when the generator allocates unequal attention across retrieved items?
- Basis in paper: [explicit] The paper notes that with long-context models becoming prevalent, measuring exposure-based retrieval fairness becomes more challenging due to generators' tendency to allocate unequal attention to different items.
- Why unresolved: The current fairness evaluation assumes equal attention to top-k items, but long-context models may violate this assumption significantly.
- What evidence would resolve it: Comparative studies of fairness metrics using different attention-based browsing models for various long-context generator architectures.

### Open Question 3
- Question: How can string utility be effectively measured across diffuse predictions when different rankings lead to different but equally valid outputs?
- Basis in paper: [explicit] The paper recognizes the need for evaluating various valid output strings and notes that their current approach relies on a single target output string for comparison.
- Why unresolved: Current evaluation uses one target string per query, potentially missing valid alternative responses that fair rankings might produce.
- What evidence would resolve it: Development and validation of utility metrics that can compare multiple valid output distributions rather than single target strings.

## Limitations

- The study focuses on a specific dataset (LaMP) and set of tasks that may not represent all RAG applications
- The utility labeling process depends on specific string utility metrics that could vary across domains
- The stochastic sampling approach with fixed parameters (Œ± values 1, 2, 4, 8) may not generalize to all fairness requirements or retrieval contexts

## Confidence

**High Confidence**: The core finding that fair ranking maintains or improves system utility (EE-D and EU metrics) is well-supported by the experimental results across multiple RAG configurations and tasks. The attribution fairness improvements (EAR and EAE-D) are also consistently demonstrated.

**Medium Confidence**: The mechanism explaining why fair ranking improves utility - that it reduces distracting items and increases exposure to useful but lower-ranked content - is plausible but requires further validation across different generator architectures and tasks.

**Low Confidence**: The optimal fairness parameter (Œ±) appears to be task-dependent, and the paper doesn't provide clear guidance on how to select Œ± for new applications beyond the tested range.

## Next Checks

1. **Cross-Dataset Validation**: Test the fairness-aware RAG approach on datasets from different domains (e.g., medical, legal, or conversational) to verify if the observed benefits generalize beyond the LaMP benchmark.

2. **Generator Architecture Sensitivity**: Evaluate whether the fairness benefits persist when using different generator architectures (e.g., GPT-style models) or when the generator has different context window constraints.

3. **Dynamic Fairness Parameter**: Implement an adaptive mechanism to adjust the fairness parameter Œ± based on task characteristics or real-time performance metrics, rather than using fixed values.