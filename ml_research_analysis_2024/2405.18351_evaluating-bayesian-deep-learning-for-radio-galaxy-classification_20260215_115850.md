---
ver: rpa2
title: Evaluating Bayesian deep learning for radio galaxy classification
arxiv_id: '2405.18351'
source_url: https://arxiv.org/abs/2405.18351
tags:
- radio
- learning
- data
- galaxies
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates Bayesian deep learning methods for radio galaxy
  classification, addressing the need for uncertainty quantification in astronomical
  data analysis. The authors compare Hamiltonian Monte Carlo (HMC), Variational Inference
  (VI), Last-Layer Laplace Approximation (LLA), Monte Carlo Dropout, and Deep Ensembles
  on a binary radio galaxy classification task.
---

# Evaluating Bayesian deep learning for radio galaxy classification

## Quick Facts
- arXiv ID: 2405.18351
- Source URL: https://arxiv.org/abs/2405.18351
- Authors: Devina Mohan; Anna M. M. Scaife
- Reference count: 21
- Primary result: HMC without data augmentation achieves best uncertainty calibration (UCE: 12.65 ± 0.01) for radio galaxy classification

## Executive Summary
This paper evaluates five Bayesian deep learning methods for radio galaxy classification, focusing on uncertainty quantification critical for astronomical data analysis. The authors compare Hamiltonian Monte Carlo (HMC), Variational Inference (VI), Last-Layer Laplace Approximation (LLA), Monte Carlo Dropout, and Deep Ensembles on a binary radio galaxy classification task using the MiraBest Confident dataset. HMC emerges as the top performer for uncertainty calibration while maintaining competitive predictive accuracy, uniquely distinguishing in-distribution radio galaxies from out-of-distribution optical and MeerKAT observations through energy-based scoring.

## Method Summary
The study uses an expanded LeNet-5 architecture (232,444 parameters) trained on the MiraBest Confident dataset (1,256 150x150 radio galaxy images) to classify Fanaroff-Riley Type I/II galaxies. Five Bayesian inference methods are compared: HMC with HAMILTORCH, VI with Gaussian variational approximation, LLA with LAPLACE package, MC Dropout with 50% dropout rate, and Deep Ensembles with 10 models. Each method is evaluated on predictive accuracy, uncertainty calibration (UCE), and distribution shift detection using energy scores on MIGHTEE and GalaxyMNIST datasets. The training protocol uses an 80/20 train/validation split with 104 test samples withheld for evaluation.

## Key Results
- HMC without data augmentation achieves best uncertainty calibration (UCE: 12.65 ± 0.01) while maintaining 4.16% error rate
- VI provides lowest test error (3.94%) but higher variance in calibration compared to HMC
- HMC uniquely distinguishes in-distribution radio galaxies from out-of-distribution optical and MeerKAT observations through energy-based scoring
- Deep Ensembles and MC Dropout show poor calibration for radio astronomy classification despite computational efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HMC without data augmentation achieves the best uncertainty calibration (lowest UCE) for radio galaxy classification.
- Mechanism: HMC samples directly from the energy surface proportional to the log posterior, learning well-shaped energy functions that distinguish in-distribution from out-of-distribution samples.
- Core assumption: The radio galaxy dataset has sufficient samples for HMC to converge properly without augmentation.
- Evidence anchors:
  - [abstract] "HMC without data augmentation achieves the best uncertainty calibration (UCE: 12.65 ± 0.01)"
  - [section] "HMC is directly sampling from an energy surface that is proportional to the log of the posterior distribution"
  - [corpus] Weak evidence - no corpus papers directly compare HMC vs VI for radio astronomy

### Mechanism 2
- Claim: VI provides the best predictive accuracy despite lower calibration than HMC.
- Mechanism: VI optimizes a surrogate energy function through ELBO, finding good MAP estimates that generalize well to test data.
- Core assumption: The ELBO objective provides a good approximation to the true posterior for this classification task.
- Evidence anchors:
  - [abstract] "VI shows the lowest test error (3.94%)"
  - [section] "VI the ELBO provides a well optimised surrogate energy function"
  - [corpus] No direct corpus evidence for VI performance on radio astronomy datasets

### Mechanism 3
- Claim: Common methods like Deep Ensembles and MC Dropout are poorly calibrated for radio astronomy classification.
- Mechanism: NLL training alone cannot shape the energy functional sufficiently to distinguish between different types of distribution shifts in astronomical data.
- Core assumption: Radio astronomy data has unique distributional characteristics that require more sophisticated uncertainty quantification.
- Evidence anchors:
  - [abstract] "commonly used methods like Deep Ensembles and Dropout are poorly calibrated"
  - [section] "NLL training is not be able to shape the energy functional well enough to distinguish between the datasets"
  - [corpus] No corpus papers specifically address calibration issues in radio astronomy

## Foundational Learning

- Concept: Bayesian inference and posterior approximation
  - Why needed here: Different BNN methods approximate the posterior differently, affecting both accuracy and calibration
  - Quick check question: What's the key difference between how HMC and VI approximate the posterior distribution?

- Concept: Energy-based models and scoring functions
  - Why needed here: Energy scores provide a post-hoc mechanism to detect distribution shifts without additional computation
  - Quick check question: How do energy scores help distinguish between in-distribution and out-of-distribution samples?

- Concept: Uncertainty quantification metrics (UCE, mutual information, predictive entropy)
  - Why needed here: Different metrics capture different aspects of model uncertainty, critical for evaluating BNN performance
  - Quick check question: Why might predictive entropy be preferred over mutual information for this small-sample astronomical dataset?

## Architecture Onboarding

- Component map: Data preprocessing -> Expanded LeNet-5 CNN (232K parameters) -> Bayesian inference method (HMC/VI/LLA/Dropout/Ensembles) -> Uncertainty quantification (UCE, energy scores) -> Distribution shift detection
- Critical path: Data preprocessing → Model training/inference → Uncertainty quantification → Distribution shift detection
- Design tradeoffs: HMC offers best calibration but high computational cost vs VI's better efficiency but sensitivity to initialization
- Failure signatures: Poor calibration (high UCE) indicates inadequate uncertainty modeling; low energy separation suggests poor OOD detection
- First 3 experiments:
  1. Compare HMC vs VI calibration on MiraBest test set using UCE metric
  2. Test energy score separation between MiraBest and GalaxyMNIST datasets
  3. Evaluate predictive accuracy vs calibration tradeoff by varying VI temperature parameter

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What causes the cold posterior effect (CPE) in variational inference for radio galaxy classification, given that data augmentation alone does not explain it?
- Basis in paper: [explicit] The authors state "Our observations on the cold posterior effect (CPE) contradict the results presented in Izmailov et al. [2021]. They suggest that the CPE is largely due to data augmentation. While our HMC model does not require any tempering, the VI models require temperatures below T = 0.01 to produce good predictive performance. We also found that data augmentation does not have a significant impact on the CPE observed in our models."
- Why unresolved: Previous studies attributed CPE to data augmentation, but the authors' results contradict this explanation, suggesting other factors are involved in their specific astronomical application.
- What evidence would resolve it: Comparative experiments testing different prior specifications, data curation strategies, and divergence metrics for the ELBO objective function, as mentioned by the authors as future work.

### Open Question 2
- Question: Can variational inference be optimized to match or exceed Hamiltonian Monte Carlo performance in radio astronomy applications while maintaining computational efficiency?
- Basis in paper: [explicit] The authors conclude "Since HMC is very computationally heavy, optimising VI for future radio surveys might be the way forward" and plan to "develop and improve our VI implementation further by using alternate optimisation strategies based on natural gradient descent [Shen et al., 2024, Khan and Rue, 2021] and proximal gradient descent [Kim et al., 2023]."
- Why unresolved: The paper demonstrates HMC's superior performance but acknowledges its computational burden, while VI shows promise but needs optimization.
- What evidence would resolve it: Comparative studies using the proposed optimization strategies (natural gradient descent, proximal gradient descent) on radio astronomy datasets, measuring both performance and computational efficiency against HMC baselines.

### Open Question 3
- Question: How do Bayesian neural networks perform in self-supervised learning regimes for radio astronomy applications with large unlabelled datasets?
- Basis in paper: [explicit] The authors suggest "Future work could also develop BNNs for self-supervised learning to exploit larger unlabelled datasets in astronomy."
- Why unresolved: The current study focuses on supervised learning with limited labelled data, but radio astronomy has access to vast unlabelled datasets that could be leveraged.
- What evidence would resolve it: Implementation and evaluation of self-supervised BNN architectures on radio astronomy datasets, comparing performance against supervised BNNs and measuring scalability with increasing unlabelled data volumes.

## Limitations
- Limited generalizability due to single astronomical dataset and synthetic out-of-distribution samples
- Computational constraints limited HMC sampling to N=200 posterior samples, potentially affecting calibration estimates
- Study focuses on binary FRI/FRII classification without systematic comparison across different radio galaxy morphologies

## Confidence
- High confidence in comparative ranking of methods on MiraBest dataset with appropriate metrics
- Medium confidence in generalizability claims across different astronomical datasets
- Medium confidence in energy-based distribution shift detection mechanism requiring further validation

## Next Checks
1. Cross-dataset validation: Test HMC and VI calibration on additional radio astronomy datasets (e.g., LOFAR surveys) to assess generalizability across different telescope resolutions and frequencies.
2. Computational efficiency analysis: Benchmark the runtime and memory requirements of HMC vs VI for larger models (500K+ parameters) to determine practical scalability for real-time astronomical surveys.
3. Ablation study on augmentation: Systematically vary data augmentation intensity during HMC training to quantify its impact on calibration and determine minimal augmentation requirements for optimal performance.