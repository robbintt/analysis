---
ver: rpa2
title: Towards Comprehensive Preference Data Collection for Reward Modeling
arxiv_id: '2406.16486'
source_url: https://arxiv.org/abs/2406.16486
tags:
- data
- preference
- step
- human
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The authors propose a four-step framework for collecting high-quality
  preference data for training reward models in RLHF: Prompt Generation, Response
  Generation, Response Filtering, and Human Labeling. They claim this structured approach
  ensures better data quality while reducing human labor.'
---

# Towards Comprehensive Preference Data Collection for Reward Modeling

## Quick Facts
- arXiv ID: 2406.16486
- Source URL: https://arxiv.org/abs/2406.16486
- Authors: Yulan Hu; Qingyang Li; Sheng Ouyang; Ge Chen; Kaihui Chen; Lijun Mei; Xucheng Ye; Fuzheng Zhang; Yong Liu
- Reference count: 5
- One-line primary result: A four-step framework for collecting high-quality preference data improves reward model accuracy by ~1% and win rates by ~1.5-2% in BoN experiments.

## Executive Summary
This paper proposes a structured four-step framework for collecting high-quality preference data to train reward models in Reinforcement Learning from Human Feedback (RLHF). The framework incrementally filters and refines data through Prompt Generation, Response Generation, Response Filtering, and Human Labeling, aiming to improve data quality while reducing human labor. Experiments on preference benchmarks and best-of-N reranking show that reward models trained on data collected through this framework outperform those trained on data collected via simpler methods, with accuracy improvements of ~1% and win rates of ~1.5-2% in BoN experiments.

## Method Summary
The framework collects preference data through four sequential steps: (1) Prompt Generation uses a proxy reward model to filter challenging prompts where SFT underperforms GPT-4; (2) Response Generation creates diverse response pairs using multiple stronger models (65B SFT, GPT-4) for each prompt; (3) Response Filtering employs GPT-4 scoring to remove uninformative pairs with identical scores or extreme differences; and (4) Human Labeling finalizes the high-quality subset. This hierarchical filtering ensures only informative pairs remain for reward model training.

## Key Results
- Reward models trained on step 4 data (post-filtering) achieve ~1% higher accuracy on preference benchmarks compared to models trained on step 3 data
- BoN reranking experiments show ~1.5-2% win rate improvement when using step 4 data versus step 3 data
- The framework reduces human labeling burden while maintaining or improving data quality

## Why This Works (Mechanism)

### Mechanism 1
Structured four-step data collection pipeline improves reward model accuracy by ensuring high-quality, diverse preference pairs while reducing human labor. Each step incrementally filters and refines the data. Core assumption: High-quality preference data directly improves reward model performance, and human effort can be reduced by intelligent AI filtering without sacrificing data quality.

### Mechanism 2
Combining responses from multiple models with different configurations increases diversity and robustness of the preference data. By using stronger models to generate y+ and y−, the responses are more likely to exhibit meaningful quality differences. Core assumption: Responses generated by stronger models are inherently more diverse and of higher quality, leading to more informative preference pairs.

### Mechanism 3
GPT-4-based filtering removes uninformative pairs (identical scores or extreme differences) before human labeling, improving labeling efficiency and data quality. GPT-4 scores each pair on a 5-point scale; pairs with identical scores or extreme score differences are discarded as uninformative. Core assumption: GPT-4 scoring reliably identifies uninformative pairs, and removing them does not eliminate important preference nuances.

## Foundational Learning

- **Reinforcement Learning from Human Feedback (RLHF)**
  - Why needed here: The paper builds a data collection framework specifically for training reward models within the RLHF pipeline, so understanding RLHF's role is essential.
  - Quick check question: What is the role of the reward model in RLHF, and how does it guide the policy update?

- **Bradley-Terry Model for Preference Learning**
  - Why needed here: The reward model is trained as a binary classifier to distinguish preferred from dispreferred responses, analogous to pairwise ranking models like Bradley-Terry.
  - Quick check question: How does the binary cross-entropy loss in Equation 1 relate to pairwise preference modeling?

- **In-Context Learning with GPT-4**
  - Why needed here: GPT-4 is used to score response pairs before human labeling; understanding in-context learning is key to grasping how this filtering works.
  - Quick check question: How does GPT-4 perform scoring without fine-tuning, and what are the limitations of this approach?

## Architecture Onboarding

- **Component map**: Prompt Generation → Response Generation → Response Filtering → Human Labeling → Reward Model Training
- **Critical path**: Prompt Generation → Response Generation → Response Filtering → Human Labeling (any failure here directly impacts final data quality)
- **Design tradeoffs**: More aggressive filtering reduces human labor but risks losing nuanced preferences; using stronger models for response generation increases diversity but raises cost and latency; relying on GPT-4 for filtering leverages its capability but introduces dependency on an external API.
- **Failure signatures**: Low retention after filtering suggests overly aggressive criteria or poor initial data quality; no accuracy gain over simpler methods indicates filtering is discarding informative pairs or failing to improve data quality; high human labeling costs despite filtering suggest the criteria are not effectively reducing workload.
- **First 3 experiments**: 1) Compare reward model accuracy trained on step 3 vs step 4 data (as in Table 2) to validate filtering benefit; 2) Measure win rate improvement in BoN reranking when using step 4 data vs step 3 data (as in Figure 4); 3) Ablate the Response Filtering step: train models with and without GPT-4 filtering to quantify its impact on labeling efficiency and model performance.

## Open Questions the Paper Calls Out

### Open Question 1
How does the quality of preference data collected through the proposed framework compare to data collected solely by human annotators in terms of final reward model performance? The paper demonstrates the framework's effectiveness but does not directly compare to traditional human-only data collection approaches.

### Open Question 2
What is the optimal balance between AI filtering and human annotation in the preference data collection process to maximize reward model performance while minimizing human labor? The paper shows the framework reduces reliance on human labor but doesn't provide a detailed analysis of the trade-off between AI filtering and human annotation.

### Open Question 3
How does the proposed framework generalize to different domains or languages, and are there domain-specific considerations for prompt generation and response filtering? The paper focuses on a general framework without specific attention to domain-specific or language-specific adaptations.

## Limitations
- Claims about GPT-4 filtering effectiveness and multi-model response generation diversity lack direct empirical validation through ablation studies
- Heavy reliance on stronger models (65B, 175B, GPT-4) raises concerns about scalability and cost-effectiveness in production settings
- The framework's performance on out-of-distribution prompts and specialized domains is not evaluated

## Confidence
- **High confidence**: The four-step framework structure and its general effectiveness in improving reward model performance over simpler methods, as demonstrated by benchmark results
- **Medium confidence**: The specific mechanisms of GPT-4 filtering and multi-model response generation, as these rely on external components without full ablation analysis
- **Low confidence**: The scalability and cost-effectiveness of the approach in production settings, given the heavy reliance on stronger models for response generation and filtering

## Next Checks
1. Perform ablation studies removing the GPT-4 filtering step to quantify its exact contribution to accuracy gains and labeling efficiency improvements
2. Measure response diversity metrics (e.g., lexical overlap, embedding distance) when using single vs. multiple models to validate the claimed diversity benefits
3. Test the framework's performance on out-of-distribution prompts not seen during training to assess generalization beyond the curated dataset