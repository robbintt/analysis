---
ver: rpa2
title: Memory-efficient Energy-adaptive Inference of Pre-Trained Models on Batteryless
  Embedded Systems
arxiv_id: '2405.10426'
source_url: https://arxiv.org/abs/2405.10426
tags:
- exit
- layer
- memory
- accuracy
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of running deep neural networks
  (DNNs) on extremely resource-constrained batteryless embedded systems that frequently
  experience power failures. The key problem is that these systems have severe memory
  limitations and require energy-adaptive inference to handle intermittent execution.
---

# Memory-efficient Energy-adaptive Inference of Pre-Trained Models on Batteryless Embedded Systems

## Quick Facts
- arXiv ID: 2405.10426
- Source URL: https://arxiv.org/abs/2405.10426
- Authors: Pietro Farina, Subrata Biswas, Eren Yıldız, Khakim Akhunov, Saad Ahmed, Bashima Islam, Kasım Sinan Yıldırım
- Reference count: 40
- Primary result: Up to 95× compression rate on pre-trained DNN models while maintaining accuracy

## Executive Summary
This paper introduces FreeML, a framework for running pre-trained deep neural networks on extremely resource-constrained batteryless embedded systems that frequently experience power failures. The key innovation is a systematic approach that combines selective layer compression through SparseComp with a novel gNet architecture for energy-adaptive inference. FreeML achieves up to 95× compression rates while reducing early exit memory overhead by 2.03-19.65× compared to state-of-the-art methods, enabling successful deployment on MSP430FR5994 microcontrollers with only 256KB FRAM and 8KB SRAM.

## Method Summary
FreeML uses a two-phase workflow: first applying SparseComp compression to selectively retrain large layers while imposing sparsity constraints, then inserting gNet early exit architecture into the compressed model. SparseComp iteratively identifies the largest layers in pre-trained DNNs and retrains them with high sparsity (90% non-zero weights) using a small subset of training data. The gNet architecture uses a single global exit branch with zero-padding and 3D max-pooling to handle variable-length inputs from different layers, enabling anytime output with minimal memory overhead. The framework converts compressed models to C headers using CSR representation and integrates with intermittent computing runtimes for deployment on batteryless systems.

## Key Results
- Achieves up to 95× compression rate on pre-trained DNN models while maintaining accuracy
- Reduces memory overhead for early exit by 2.03-19.65× compared to state-of-the-art methods
- Successfully deployed on MSP430FR5994 microcontroller (256KB FRAM, 8KB SRAM) with model execution under intermittent power

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** SparseComp selectively retrains only the largest layers while imposing sparsity constraints, enabling aggressive compression without significant accuracy loss.
- **Mechanism:** SparseComp iteratively identifies the largest layer in a pre-trained DNN, applies a high initial sparsity constraint (90% non-zero weights), and jointly retrains that layer plus subsequent layers using a small subset of training data. The process repeats until the target compression rate is met.
- **Core assumption:** Large layers contribute disproportionately to model size, and selective retraining with sparsity constraints can maintain accuracy while achieving high compression.
- **Evidence anchors:**
  - [abstract]: "SparseComp minimizes accuracy degradation due to compression by retraining only the selected layers using a small percent of the training data."
  - [section]: "To compress the selected layer, we freeze all the preceding layers...and only retrain the selected layer and the following ones by imposing the selected sparsity constraint only on the layer to be compressed."
- **Break condition:** If sparsity constraints are too aggressive for fragile layers, causing significant accuracy drops that cannot be recovered through retraining.

### Mechanism 2
- **Claim:** gNet's single global exit branch architecture provides anytime output with minimal memory overhead compared to traditional multiple exit branches.
- **Mechanism:** gNet uses zero-padding to compensate for missing future layer outputs when exiting early, 3D max-pooling to standardize feature dimensions across layers, and a fully connected layer for classification that learns to emphasize real features over padded ones.
- **Core assumption:** A single exit architecture can effectively process variable-length inputs from different exit points while maintaining accuracy comparable to multiple specialized exit branches.
- **Evidence anchors:**
  - [abstract]: "gNet is the first plug-and-play early exit architecture that uses a single exit branch to exit from any layer of the network, introducing minimal memory overhead."
  - [section]: "gNet takes the output from all intermediate layers as input using a unique pooling mechanism that handles missing intermediate outputs from the later layers."
- **Break condition:** If the zero-padding approach fails to effectively compensate for missing information from future layers, leading to degraded accuracy for early exits.

### Mechanism 3
- **Claim:** FreeML's runtime optimizations reduce both time and energy overhead during intermittent execution by leveraging unstructured compressed computation and selective processing.
- **Mechanism:** The runtime performs max-pooling before storing layer outputs to reduce buffer size, skips MAC operations for zero-padded inputs during gNet execution, and uses pre-recorded time/energy overheads to select optimal exit points based on available harvested power.
- **Core assumption:** Microcontrollers can efficiently support unstructured compressed computation, and selective processing of non-zero inputs can significantly reduce execution time and energy consumption.
- **Evidence anchors:**
  - [section]: "FreeML runtime uses a smart approach to save time and energy by avoiding computations for zero-padded inputs that belong to layers not yet executed."
  - [section]: "Unlike traditional accelerators that only supports structured compressed computing, microcontrollers allow unstructured compressed computation."
- **Break condition:** If the overhead of checking for zero-padded inputs exceeds the savings from skipping computations, or if memory constraints prevent effective buffer reduction.

## Foundational Learning

- **Concept: Intermittent computing and power failures**
  - Why needed here: FreeML specifically targets batteryless systems that experience frequent power failures, requiring checkpointing and recovery mechanisms.
  - Quick check question: What happens to a DNN inference if power fails mid-computation and there's no checkpointing mechanism?

- **Concept: DNN compression techniques (pruning, quantization, separation)**
  - Why needed here: FreeML builds on existing compression methods but adapts them for extreme memory constraints and energy awareness.
  - Quick check question: How does tensor decomposition differ from unstructured pruning in terms of model structure preservation?

- **Concept: Early exit mechanisms in neural networks**
  - Why needed here: gNet's core innovation is a single global exit branch that provides anytime output, requiring understanding of how early exits work in traditional architectures.
  - Quick check question: In traditional early exit networks, what additional parameters must be stored for each exit branch?

## Architecture Onboarding

- **Component map:** Pre-trained DNN model -> SparseComp compression module -> gNet early exit architecture -> FreeML runtime library -> Target hardware platform (e.g., MSP430FR5994)

- **Critical path:**
  1. Input pre-trained model and dataset
  2. Apply SparseComp compression layer-by-layer
  3. Insert gNet architecture into compressed model
  4. Convert to C headers with CSR representation
  5. Link with FreeML runtime for intermittent execution

- **Design tradeoffs:**
  - Memory vs. accuracy: Higher compression rates reduce memory usage but may impact accuracy
  - Early exit timing vs. accuracy: Earlier exits save time/energy but provide lower accuracy
  - Runtime overhead vs. buffer size: Max-pooling reduces buffer requirements but adds processing overhead

- **Failure signatures:**
  - Accuracy degradation during compression indicates over-pruning of critical layers
  - Runtime errors during inference suggest buffer size mismatches or incorrect checkpointing
  - Power failure during execution without proper state recovery indicates missing backup mechanisms

- **First 3 experiments:**
  1. Test SparseComp compression on a simple CNN (e.g., MNIST) to verify layer-by-layer compression works
  2. Validate gNet architecture with a small pre-trained model to ensure early exit functionality
  3. Deploy compressed model with gNet on MSP430FR5994 to verify memory constraints are met and intermittent execution works

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SparseComp's iterative pruning and retraining strategy perform on deeper neural network architectures beyond ResNet34, such as DenseNet or MobileNet, particularly in terms of accuracy retention and compression ratio?
- Basis in paper: [explicit] The paper mentions evaluating SparseComp on ResNet34 and notes that deeper and more complex networks are less suitable for intermittent devices, suggesting potential limitations.
- Why unresolved: The paper only evaluates SparseComp on ResNet34, leaving uncertainty about its scalability and effectiveness on more complex architectures.
- What evidence would resolve it: Empirical results comparing SparseComp's performance on ResNet34, DenseNet, and MobileNet architectures with varying depths and complexities, showing accuracy retention and compression ratios.

### Open Question 2
- Question: What is the impact of the proposed attention-based max-pooling technique on the input size reduction to gNet and how does it affect the accuracy of early exit predictions compared to the current pooling strategy?
- Basis in paper: [explicit] The paper discusses the large input size to gNet as a shortcoming and mentions future plans to implement attention-based max-pooling.
- Why unresolved: The paper only proposes the attention-based max-pooling technique without evaluating its effectiveness, leaving uncertainty about its benefits.
- What evidence would resolve it: Comparative results showing the input size reduction, accuracy improvement, and computational efficiency of attention-based max-pooling versus the current pooling strategy in gNet.

### Open Question 3
- Question: How does the performance of SparseComp and gNet vary across different types of datasets, such as those with higher resolution images, audio with varying frequencies, or sensor data with different sampling rates?
- Basis in paper: [explicit] The paper evaluates the framework on CIFAR-10, KWS, and HAR datasets, suggesting potential limitations in generalizability to other dataset types.
- Why unresolved: The paper only evaluates the framework on three specific datasets, leaving uncertainty about its performance on diverse data types.
- What evidence would resolve it: Empirical results demonstrating SparseComp and gNet's performance on datasets with higher resolution images, audio with varying frequencies, and sensor data with different sampling rates, showing accuracy retention and compression ratios.

## Limitations
- SparseComp's layer-by-layer retraining approach may not generalize well to complex DNN architectures with non-standard layer structures or unconventional connectivity patterns
- gNet's single exit branch architecture effectiveness heavily depends on the quality of the pooling mechanism, which is not fully specified in terms of handling feature dimension mismatches
- Runtime optimizations assume microcontrollers can efficiently handle unstructured compressed computation, but actual performance may vary significantly across different hardware platforms

## Confidence

- **High confidence in compression mechanism (SparseComp):** Well-documented layer-by-layer approach with clear iterative retraining procedure
- **Medium confidence in early exit architecture (gNet):** Single-exit concept is sound, but pooling mechanism details are sparse
- **Low confidence in runtime optimizations:** Assumes specific hardware capabilities without comprehensive cross-platform validation

## Next Checks

1. Test SparseComp on diverse DNN architectures (ResNet, MobileNet, EfficientNet) to verify layer selection algorithm generalizes beyond simple CNNs
2. Implement and benchmark gNet's pooling mechanism with different kernel sizes and strides to identify optimal configuration for various layer separation distances
3. Profile runtime performance on multiple microcontroller platforms (ARM Cortex-M, RISC-V) to validate unstructured compressed computation efficiency assumptions across different hardware capabilities