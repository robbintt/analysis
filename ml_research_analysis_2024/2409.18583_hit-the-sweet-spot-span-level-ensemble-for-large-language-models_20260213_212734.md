---
ver: rpa2
title: Hit the Sweet Spot! Span-Level Ensemble for Large Language Models
arxiv_id: '2409.18583'
source_url: https://arxiv.org/abs/2409.18583
tags:
- ensemble
- span
- tasks
- methods
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a span-level ensemble method for large language
  models (LLMs) to balance real-time adjustments and information richness. The approach
  involves generating candidate spans independently by each model and selecting the
  optimal span based on filtered perplexity scores.
---

# Hit the Sweet Spot! Span-Level Ensemble for Large Language Models

## Quick Facts
- arXiv ID: 2409.18583
- Source URL: https://arxiv.org/abs/2409.18583
- Authors: Yangyifan Xu; Jianghao Chen; Junhong Wu; Jiajun Zhang
- Reference count: 12
- One-line primary result: Span-level ensemble method SWEET SPAN achieves consistent performance improvements over individual models and prior ensemble methods across diverse language generation tasks.

## Executive Summary
This paper introduces SWEET SPAN, a novel span-level ensemble method for combining multiple large language models (LLMs) that balances real-time adjustability with information richness. Unlike token-level methods that make frequent but poorly-informed decisions, or sample-level methods that lack adaptability, SWEET SPAN generates spans composed of complete words and uses perplexity-based mutual evaluation with adaptive filtering to select the optimal span. The approach requires no additional training and demonstrates robustness even when models have significant performance gaps, making it a versatile solution for improving LLM performance across various tasks.

## Method Summary
SWEET SPAN works by having each candidate model independently generate a span based on the current shared prefix, then using perplexity scores to facilitate mutual evaluation among models. An adaptive filtering strategy removes outlier perplexity scores that deviate significantly from the group, ensuring more reliable span selection. The method uses word-level spans rather than tokens to avoid tokenizer incompatibility issues across models, and operates without any parameter training, making it directly applicable to any LLM ensemble. The span length is set to match the average number of tokens per word (approximately 3 tokens) to balance real-time adjustability with sufficient contextual information.

## Key Results
- SWEET SPAN consistently improves performance over individual models and baseline ensemble methods across four diverse tasks (Natural Questions, GSM8K, MBPP, and Flores-101)
- The method demonstrates robustness in challenging settings with significant performance gaps between models, achieving stable positive improvements
- SWEET SPAN outperforms token-level and sample-level ensemble methods while avoiding their respective limitations of information bottleneck and lack of real-time adjustment
- The approach generalizes across different tasks and model architectures without requiring additional training or vocabulary alignment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Span-level ensemble avoids the information bottleneck of token-level methods while maintaining real-time adjustability.
- Mechanism: By generating spans composed of complete words rather than individual tokens, the method ensures that each decision in the ensemble process is informed by richer contextual information, while still allowing frequent updates during generation.
- Core assumption: The additional context provided by word-level spans significantly improves ensemble decision quality without sacrificing the benefits of real-time correction.
- Evidence anchors:
  - [abstract] "the information carried by an individual token is quite limited, leading to suboptimal decisions at each step"
  - [section] "Spans are composed of words rather than tokens. We ensure that spans do not cross word boundaries to prevent subsequent evaluations from being affected by different tokenizers across models."
- Break condition: If the span length becomes too long, it may delay corrections and reduce the benefits of real-time adjustment.

### Mechanism 2
- Claim: Perplexity-based mutual evaluation with adaptive filtering prevents underperforming models from skewing ensemble decisions.
- Mechanism: Each candidate model evaluates all spans using perplexity scores, then an adaptive filtering strategy removes outlier scores that deviate significantly from the group, ensuring more reliable span selection.
- Core assumption: Models that lack relevant knowledge for the current sample produce inconsistent perplexity scores that can be identified and filtered out based on statistical deviation.
- Evidence anchors:
  - [section] "Models lacking relevant knowledge of the current sample may assign unjustifiably perplexity scores, giving correct spans excessively high perplexity while assigning overly low perplexity to their own incorrect spans."
  - [section] "We design an adaptive filtering strategy that identifies outliers based on the discrepancy in evaluation scores for each span."
- Break condition: If all models perform poorly on a sample, filtering may remove all evaluations, leaving no basis for selection.

### Mechanism 3
- Claim: The method generalizes across diverse tasks and model architectures without additional training.
- Mechanism: By avoiding vocabulary-specific alignment and model-specific training, the approach can be directly applied to any LLM ensemble, making it versatile across different tasks and model configurations.
- Core assumption: The core ensemble logic based on perplexity and filtering is robust enough to work across different vocabularies, model architectures, and task types without task-specific adaptation.
- Evidence anchors:
  - [abstract] "SWEET SPAN is not constrained by vocabulary, model architecture, or task, and can be directly applied to any LLM ensemble without additional parameter training"
  - [section] "Our method has no limitations regarding vocabulary, model architecture, or task, and can be directly applied to any LLM ensemble without additional parameter training"
- Break condition: The method may fail if models use fundamentally incompatible tokenization schemes that cannot be evaluated using shared word-level spans.

## Foundational Learning

- Concept: Perplexity as a language model evaluation metric
  - Why needed here: The ensemble method relies on perplexity scores to evaluate and select spans, so understanding how perplexity measures model fit is essential.
  - Quick check question: If a span has lower perplexity under a language model, does this indicate better or worse fit to the model's learned distribution?

- Concept: Ensemble learning fundamentals
  - Why needed here: The method builds on ensemble learning principles to combine multiple models' strengths while mitigating individual weaknesses.
  - Quick check question: What is the primary advantage of ensemble methods over individual models in machine learning?

- Concept: Tokenization and its impact on language model outputs
  - Why needed here: The method specifically addresses tokenization differences across models by using word-level spans, so understanding tokenization is crucial.
  - Quick check question: How might different tokenization strategies across models affect the comparison of their outputs at the token level?

## Architecture Onboarding

- Component map: Shared prefix -> Span generation (parallel across N models) -> Perplexity calculation (parallel across N models) -> Filtering (per-span processing) -> Selection (minimal filtered perplexity) -> Output span -> Updated prefix for next round
- Critical path: Shared prefix → Span generation (parallel across N models) → Perplexity calculation (parallel across N models) → Filtering (per-span processing) → Selection (minimal filtered perplexity) → Output span → Updated prefix for next round
- Design tradeoffs: Longer spans provide more context but reduce real-time correction frequency; aggressive filtering removes noise but may discard valid evaluations; perplexity as metric is simple but may not capture all aspects of quality
- Failure signatures: Poor performance when all candidate models lack relevant knowledge; degradation when span length is mismatched to task complexity; filtering may fail when performance gaps are too extreme or too subtle
- First 3 experiments:
  1. Test with four diverse LLMs on a simple question-answering task with varying span lengths to observe the optimal balance.
  2. Compare performance with and without the filtering strategy on a task with known model quality disparities.
  3. Evaluate efficiency overhead by measuring generation time with different span lengths and comparing to token-level ensemble baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the span length in SWEET SPAN affect the trade-off between real-time adjustments and information richness across different tasks?
- Basis in paper: [explicit] The paper discusses that shorter spans provide more frequent corrections while longer spans offer richer information for better decisions, with optimal span length varying by task.
- Why unresolved: While the paper shows experimental results for various span lengths, it does not provide a theoretical framework for predicting the optimal span length for new, unseen tasks.
- What evidence would resolve it: A comprehensive study analyzing the relationship between span length, task characteristics, and ensemble performance, potentially including a predictive model for optimal span length selection.

### Open Question 2
- Question: What are the limitations of the perplexity-based filtering strategy in SWEET SPAN when dealing with models that have similar but incorrect knowledge bases?
- Basis in paper: [inferred] The paper introduces a filtering strategy to remove unfaithful perplexity scores, but does not explore scenarios where models might consistently agree on incorrect spans due to shared biases.
- Why unresolved: The filtering strategy assumes that outliers are due to individual model errors, but doesn't account for collective model biases that could lead to incorrect consensus.
- What evidence would resolve it: Experiments testing SWEET SPAN's performance when ensemble models share common misconceptions or biases, and analysis of how the filtering strategy handles such scenarios.

### Open Question 3
- Question: How does SWEET SPAN's performance compare to other ensemble methods when integrating models with vastly different architectures (e.g., transformers vs. other neural network types)?
- Basis in paper: [explicit] The paper states that SWEET SPAN is not constrained by model architecture, but only tests it with transformer-based LLMs of similar sizes.
- Why unresolved: The experiments are limited to 7B transformer models, leaving questions about performance with diverse architectures and sizes.
- What evidence would resolve it: Comparative experiments including models with different architectures (e.g., recurrent networks, convolutional networks) and sizes to evaluate SWEET SPAN's robustness across diverse model types.

### Open Question 4
- Question: What is the impact of incorporating external knowledge sources (e.g., knowledge graphs, domain-specific databases) on SWEET SPAN's performance and robustness?
- Basis in paper: [inferred] The paper focuses on ensemble of LLMs without external knowledge integration, suggesting potential for enhancement.
- Why unresolved: The study does not explore the integration of external knowledge sources, which could provide additional context and improve ensemble decisions.
- What evidence would resolve it: Experiments augmenting SWEET SPAN with external knowledge sources and comparing performance against the base method, particularly in tasks requiring factual accuracy or domain expertise.

## Limitations

- Unknown filtering parameters: The paper specifies using a filtering threshold λ=1.5 but does not detail how this value was chosen or whether it was tuned for specific tasks, which likely affects performance and could explain variations in results across different settings.
- Span length optimization: While the paper mentions span-level ensemble, it does not provide systematic analysis of how span length affects performance or how to optimally select span length for different tasks and model combinations.
- Resource overhead: The method requires each model to generate spans and calculate perplexities, but the paper lacks detailed analysis of computational overhead compared to baseline methods, particularly for longer spans.

## Confidence

**High confidence** in the claim that SWEET SPAN improves performance over individual models and baseline ensemble methods, as this is supported by consistent experimental results across four diverse tasks (NQ, GSM8K, MBPP, Flores-101).

**Medium confidence** in the robustness claim regarding significant performance gaps between models, as the paper provides theoretical reasoning but limited empirical evidence showing SWEET SPAN's behavior in extreme model quality disparity scenarios.

**Medium confidence** in the versatility claim (vocabulary/model/architecture agnostic), as the paper demonstrates the method works across different open-source LLMs but doesn't test with fundamentally different architectures or proprietary models.

## Next Checks

1. **Parameter sensitivity analysis**: Systematically vary the filtering threshold λ (0.5, 1.0, 1.5, 2.0) and span length (1-3 words, 4-6 words, 7+ words) to identify optimal configurations for different task types and model quality disparities.

2. **Extreme disparity testing**: Create controlled experiments where one model is significantly weaker than others (e.g., by using a smaller model or one trained on limited data) to rigorously test the robustness claims and observe how filtering performs in failure modes.

3. **Efficiency benchmarking**: Measure wall-clock time and computational resources required for SWEET SPAN versus baseline methods across different span lengths, and compare against theoretical estimates of N-fold increase in generation time.