---
ver: rpa2
title: Revisiting Code Similarity Evaluation with Abstract Syntax Tree Edit Distance
arxiv_id: '2404.08817'
source_url: https://arxiv.org/abs/2404.08817
tags:
- similarity
- tsed
- code
- metrics
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study extends TSED evaluation to 6 programming languages,
  demonstrating its effectiveness across Java, Python, JavaScript, TypeScript, Ruby,
  and Kotlin. The metrics show strong correlations with BLEU score and Jaccard similarity
  (0.6-0.8 range) and moderate correlation with GPT similarity, particularly in Java
  and Python.
---

# Revisiting Code Similarity Evaluation with Abstract Syntax Tree Edit Distance
## Quick Facts
- arXiv ID: 2404.08817
- Source URL: https://arxiv.org/abs/2404.08817
- Reference count: 7
- Primary result: TSED evaluation extended to 6 programming languages with strong performance metrics

## Executive Summary
This study evaluates the effectiveness of Abstract Syntax Tree Edit Distance (TSED) as a code similarity metric across six programming languages: Java, Python, JavaScript, TypeScript, Ruby, and Kotlin. The research demonstrates that TSED shows strong correlations with established metrics like BLEU score and Jaccard similarity (0.6-0.8 range) while also maintaining moderate correlation with GPT-based similarity measures. The findings indicate that TSED performs robustly across different language families, making it a versatile tool for code similarity evaluation in diverse programming contexts.

The study also reveals important trade-offs between TSED and GPT-based similarity metrics. While GPT scoring achieves higher F1 scores compared to TSED, it demonstrates significant instability across repeated runs with measurable variance in results. TSED shows better accuracy but requires careful tuning of penalty weight parameters that vary by language. These findings suggest that while TSED offers more stable performance, GPT-based methods may provide superior results at the cost of reliability and computational expense.

## Method Summary
The research methodology involves evaluating code similarity using three distinct metrics: Abstract Syntax Tree Edit Distance (TSED), GPT-based similarity scoring, and semantic metrics based on execution results. The evaluation framework tests these metrics across six programming languages using standardized benchmark datasets. For each metric, the study measures correlations with established similarity measures (BLEU score and Jaccard similarity) and evaluates matching accuracy when comparing execution results. The TSED implementation includes configurable penalty weights for different edit operations, allowing optimization for each language. GPT similarity is assessed through repeated runs to quantify stability and variance, while semantic metrics serve as a baseline for comparison. The comprehensive evaluation provides insights into the relative strengths and weaknesses of each approach across different programming paradigms.

## Key Results
- TSED shows strong correlations (0.6-0.8 range) with BLEU score and Jaccard similarity across all six tested languages
- GPT similarity demonstrates moderate correlation with TSED but exhibits instability across repeated runs (MSE 0.0527-0.0628)
- TSED and GPT both achieve higher F1 scores than semantic metrics when matching execution results, with GPT showing slightly better F1 but TSED better accuracy
- TSED performance is highly sensitive to penalty weight parameters, with optimal values varying significantly by language (e.g., insert operation at 0.8 for Java)

## Why This Works (Mechanism)
TSED works by measuring the minimum number of edit operations required to transform one AST into another, capturing structural similarity at the syntactic level. This approach is effective because it focuses on the underlying program structure rather than surface-level textual differences, making it robust to variable naming changes, whitespace differences, and code formatting variations. The edit distance calculation inherently accounts for insertions, deletions, and modifications of AST nodes, providing a fine-grained measure of structural similarity. The method's effectiveness across multiple languages stems from the universal nature of AST representations, which capture the fundamental syntactic structure common to all programming languages regardless of their specific syntax rules.

## Foundational Learning
- Abstract Syntax Trees (ASTs): Hierarchical representation of source code structure that abstracts away from surface syntax. Needed to understand how code structure is represented for similarity measurement. Quick check: Can you identify the AST nodes for a simple function declaration?
- Edit Distance Metrics: Algorithms for measuring the minimum number of operations needed to transform one structure into another. Needed to understand the core similarity measurement technique. Quick check: Can you calculate the edit distance between two simple code snippets?
- BLEU Score: Metric originally developed for machine translation that measures similarity between texts. Needed as a benchmark for evaluating code similarity metrics. Quick check: Can you explain how n-gram matching works in BLEU scoring?
- Jaccard Similarity: Measure of similarity between sets based on intersection over union. Needed as an alternative benchmark for code similarity evaluation. Quick check: Can you compute Jaccard similarity between two sets of code tokens?
- GPT Similarity Scoring: Using large language models to evaluate semantic similarity between code snippets. Needed to understand the state-of-the-art comparison baseline. Quick check: Can you describe how GPT might evaluate code similarity differently from syntactic metrics?
- AST Node Types: Different categories of nodes in an AST (expressions, statements, declarations, etc.). Needed to understand how structural differences are captured. Quick check: Can you list the main AST node types in your preferred programming language?

## Architecture Onboarding
Component map: Code input -> AST parser -> Edit distance calculator -> Similarity score -> Language-specific parameter tuning -> Result output
Critical path: Code parsing → AST construction → Edit distance computation → Parameter optimization → Similarity scoring
Design tradeoffs: TSED prioritizes structural accuracy over computational efficiency, while GPT similarity offers higher performance at the cost of stability and expense
Failure signatures: Poor performance on highly idiomatic code patterns, sensitivity to parameter tuning, and computational overhead for large codebases
First experiments:
1. Test TSED parameter sensitivity on a simple Java code transformation task with known structural changes
2. Compare TSED and GPT similarity scores on a small dataset of semantically equivalent but syntactically different code snippets
3. Measure correlation between TSED scores and execution result similarity on a controlled set of functionally equivalent implementations

## Open Questions the Paper Calls Out
None

## Limitations
- TSED parameter sensitivity requires language-specific tuning, limiting out-of-the-box applicability
- GPT scoring instability across runs raises concerns about reliability in production settings
- Evaluation framework may not capture edge cases in complex code structures or domain-specific programming patterns

## Confidence
- TSED effectiveness across languages: High - supported by consistent correlation patterns and F1 scores across all six languages tested
- GPT stability concerns: High - directly measured through repeated run comparisons with quantified variance
- Parameter sensitivity: Medium - well-documented but requires broader testing across additional language families and code complexity levels

## Next Checks
1. Test TSED parameter sensitivity across additional programming languages beyond the six studied, particularly focusing on languages with different syntactic structures (functional, scripting, or domain-specific languages)
2. Conduct longitudinal GPT scoring stability tests over extended timeframes to assess whether instability persists across different model versions and contexts
3. Validate TSED performance on code with complex nested structures and domain-specific patterns not represented in the current benchmark datasets