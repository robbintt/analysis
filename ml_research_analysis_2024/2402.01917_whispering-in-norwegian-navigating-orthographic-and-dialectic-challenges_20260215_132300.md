---
ver: rpa2
title: 'Whispering in Norwegian: Navigating Orthographic and Dialectic Challenges'
arxiv_id: '2402.01917'
source_url: https://arxiv.org/abs/2402.01917
tags:
- norwegian
- dataset
- openai
- whisper
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NB-Whisper, an adaptation of OpenAI's Whisper,
  specifically fine-tuned for Norwegian language Automatic Speech Recognition (ASR).
  We highlight its key contributions and summarise the results achieved in converting
  spoken Norwegian into written forms and translating other languages into Norwegian.
---

# Whispering in Norwegian: Navigating Orthographic and Dialectic Challenges

## Quick Facts
- **arXiv ID**: 2402.01917
- **Source URL**: https://arxiv.org/abs/2402.01917
- **Reference count**: 0
- **One-line result**: NB-Whisper improves Norwegian Bokmål transcription WER from 10.4 to 6.6 on Fleurs and from 6.8 to 2.2 on NST

## Executive Summary
This paper introduces NB-Whisper, a Norwegian adaptation of OpenAI's Whisper model fine-tuned for Norwegian language Automatic Speech Recognition (ASR). The authors address challenges specific to Norwegian, including orthographic variation and dialectal differences, through a two-stage training process with dataset cleaning. The resulting model demonstrates significant improvements in Word Error Rate (WER) compared to the base Whisper model, reducing WER from 10.4 to 6.6 on the Fleurs Dataset and from 6.8 to 2.2 on the NST dataset.

## Method Summary
NB-Whisper uses a two-stage training approach on Norwegian-specific datasets including NRK subtitles, audiobooks, NST, and Stortinget Speech Corpus. The process begins with 200k training steps on the full dataset, followed by dataset cleaning using model inference to identify and remove misalignments and hallucinations, then continues with 50k steps on the cleaned dataset. Key modifications include increased batch size (1024), BPE dropout (0.2), and adjusted learning rates. The model is evaluated on Fleurs (Norwegian Bokmål), NST, and Common Voice datasets using JiWER with lowercase and punctuation removal.

## Key Results
- Improved WER from 10.4 to 6.6 on Fleurs Dataset (Norwegian Bokmål)
- Reduced WER from 6.8 to 2.2 on NST dataset
- Demonstrated effectiveness in both transcription and translation tasks
- Showed improved handling of Norwegian orthographic and dialectal variations

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning Whisper on Norwegian-specific data significantly improves WER compared to the base model. By training on curated Norwegian datasets (NRK subtitles, audiobooks, NST, Stortinget corpus), the model learns the orthographic and dialectal patterns specific to Norwegian, overcoming the limitations of Whisper's general multilingual pretraining.

### Mechanism 2
Cleaning the dataset by removing misalignments and insertions reduces hallucinations in the model's output. The two-stage training process, with dataset cleaning in between, ensures that the model is trained on high-quality, aligned data, reducing the likelihood of generating incorrect or hallucinated text.

### Mechanism 3
Using a larger batch size (1024) and adjusted learning rates improves training efficiency and model performance. Increasing the batch size allows for more efficient use of computational resources and can lead to better generalization, while adjusting the learning rates ensures stable training and optimal convergence.

## Foundational Learning

- **Concept**: Automatic Speech Recognition (ASR)
  - Why needed here: Understanding the basics of ASR is crucial for comprehending the challenges and solutions presented in the paper.
  - Quick check question: What is the main difference between traditional ASR models and Whisper's approach?

- **Concept**: Transformer Architecture
  - Why needed here: The paper relies heavily on the Whisper model's transformer-based architecture, so understanding its components and functioning is essential.
  - Quick check question: What are the key components of the encoder-decoder transformer architecture used in Whisper?

- **Concept**: Norwegian Language and Orthography
  - Why needed here: The paper focuses on adapting Whisper for Norwegian, so familiarity with the language's unique characteristics, such as its two written standards and dialectal variation, is necessary.
  - Quick check question: What are the two written standards of Norwegian, and how do they differ?

## Architecture Onboarding

- **Component map**: 16kHz audio chunks → 128-channel Mel spectrogram → Encoder (2-layer conv + transformer) → Decoder (transformer) → Transcription/translation
- **Critical path**: Audio preprocessing → Encoder → Decoder → Output
- **Design tradeoffs**:
  - Large batch size (1024) for efficiency but requiring substantial hardware resources
  - Two-stage training process with dataset cleaning to reduce hallucinations but potentially removing valuable data
  - Fine-tuning on Norwegian-specific data for improved performance but potentially overfitting to the training data
- **Failure signatures**:
  - High WER on test datasets, indicating poor generalization
  - Hallucinations or incorrect transcriptions, suggesting issues with the training data or model architecture
  - Training instability or suboptimal convergence, possibly due to inappropriate hyperparameters
- **First 3 experiments**:
  1. Fine-tune the base Whisper model on the Norwegian datasets without any dataset cleaning and evaluate the WER on the test sets.
  2. Implement the two-stage training process with dataset cleaning and compare the WER and hallucination rates with the first experiment.
  3. Experiment with different batch sizes and learning rates to find the optimal combination for training efficiency and model performance.

## Open Questions the Paper Calls Out

### Open Question 1
How can the training process be optimized to reduce the risk of discarding valuable data during the multi-stage dataset cleaning approach? The paper mentions that the multi-stage approach inherently risks discarding valuable data due to the fine line between high-quality and faulty data, but does not provide specific strategies for optimization.

### Open Question 2
What are the specific challenges and limitations of the model's architecture in handling live transcription scenarios? The paper notes that the model's architecture, optimized for transcribing 30-second audio clips, presents a limitation in live transcription scenarios due to the reliance on an autoregressive decoder.

### Open Question 3
How can the issue of orthographic variation in Norwegian be effectively addressed to ensure consistent model outputs? The paper highlights the challenge of orthographic variation in Norwegian, where multiple correct spellings can lead to inconsistent model outputs, but does not provide concrete methodologies for implementation.

## Limitations

- Narrow dataset scope may not fully represent Norwegian speech diversity across all dialects and domains
- Cleaning process may have removed valuable edge cases that would improve real-world robustness
- Heavy reliance on hardware resources (TPU-v4-pods) with large batch size may limit reproducibility
- Two-stage training process with dataset cleaning introduces complexity that could lead to overfitting

## Confidence

- **High confidence**: The general approach of fine-tuning Whisper for Norwegian and the reported WER improvements are well-supported by the results.
- **Medium confidence**: The specific dataset cleaning methodology and its impact on reducing hallucinations, as the exact thresholds and criteria are not fully detailed.
- **Medium confidence**: The optimal batch size and learning rate configuration, as these were chosen based on available resources rather than systematic ablation studies.

## Next Checks

1. **Dataset diversity validation**: Evaluate NB-Whisper on additional Norwegian speech datasets (e.g., NorTED, NoTa) to assess generalization across different domains and recording conditions.
2. **Dialect robustness testing**: Test the model's performance on underrepresented Norwegian dialects not well-covered in the training data, using dialect-specific test sets.
3. **Ablation study on cleaning criteria**: Systematically vary the dataset cleaning thresholds and compare the resulting WER and hallucination rates to determine the optimal balance between data quantity and quality.