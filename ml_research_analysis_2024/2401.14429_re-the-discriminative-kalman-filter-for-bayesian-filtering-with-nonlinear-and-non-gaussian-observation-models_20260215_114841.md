---
ver: rpa2
title: '[Re] The Discriminative Kalman Filter for Bayesian Filtering with Nonlinear
  and Non-Gaussian Observation Models'
arxiv_id: '2401.14429'
source_url: https://arxiv.org/abs/2401.14429
tags:
- pnum
- zero
- kalman
- data
- trial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study reproduces and extends the Discriminative Kalman Filter
  (DKF) method for neural decoding in brain-computer interfaces. The DKF addresses
  the challenge of estimating latent motor intentions from highly non-linear and non-Gaussian
  neural signals by leveraging Bayes' theorem to approximate the observation model.
---

# [Re] The Discriminative Kalman Filter for Bayesian Filtering with Nonlinear and Non-Gaussian Observation Models

## Quick Facts
- arXiv ID: 2401.14429
- Source URL: https://arxiv.org/abs/2401.14429
- Reference count: 3
- DKF-NW achieves 14-20% nRMSE improvement over baseline Kalman filters in neural decoding

## Executive Summary
This study reproduces and extends the Discriminative Kalman Filter (DKF) method for neural decoding in brain-computer interfaces. The DKF addresses the challenge of estimating latent motor intentions from highly non-linear and non-Gaussian neural signals by leveraging Bayes' theorem to approximate the observation model. The method combines a Kalman filter's linear dynamics with a learned non-linear observation-to-state mapping. The study implements DKF variants using neural networks, Nadaraya-Watson kernel regression, and Gaussian processes, along with traditional Kalman, Extended Kalman, and Unscented Kalman filters. Results show DKF-NW achieves the best overall performance with 14-20% nRMSE improvement over baseline Kalman filters across six trials, though neural network regression without DKF filtering achieved the lowest nRMSE (17% improvement).

## Method Summary
The study implements DKF variants by first preprocessing publicly available primate reach data, then learning observation-to-state mappings f(Xi) and covariance functions Q(Xi) using neural networks, Nadaraya-Watson kernel regression, or Gaussian processes. The DKF applies Bayes' theorem to approximate the observation model p(Xi|Zi) by substituting it with p(Zi|Xi)/p(Zi), where p(Zi|Xi) is modeled as Gaussian with learned mean and covariance. Traditional Kalman filters and their variants serve as baselines. Performance is evaluated using normalized root mean square error (nRMSE) and mean absolute angle error (MAAE) across multiple trials and random seeds.

## Key Results
- DKF-NW achieves 14-20% nRMSE improvement over baseline Kalman filters across six trials
- Neural network regression without DKF filtering achieved the lowest nRMSE (17% improvement)
- DKF application for neural networks increases nRMSE while decreasing MAAE
- DKF-GP performance degraded compared to the original implementation due to GPML vs scikit-learn differences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The DKF improves over traditional Kalman filters by approximating the observation model using Bayes' theorem rather than assuming linearity.
- Mechanism: The DKF substitutes the intractable nonlinear observation likelihood p(Xi|Zi) with p(Zi|Xi)/p(Zi), where p(Zi|Xi) is modeled as a Gaussian with learned mean f(Xi) and covariance Q(Xi).
- Core assumption: The observation model p(Xi|Zi) is strongly nonlinear or non-Gaussian, and the dimensionality of observations X is much larger than the dimensionality of latents Z.
- Evidence anchors:
  - [abstract] "The DKF addresses the challenge of estimating latent motor intentions from highly non-linear and non-Gaussian neural signals by leveraging Bayes' theorem to approximate the observation model."
  - [section] "The DKF further models p(Zi|Xi) as Gaussian: p(Zi|Xi) ~ N(f(Xi), Q(Xi)), where f(·) and Q(·) are nonlinear functions that map the observation Xi to its corresponding elements in the state space Rd and the covariance space Sd, respectively."
  - [corpus] Weak - neighboring papers focus on general nonlinear filtering methods but don't specifically validate DKF's Bayes' theorem approach for neural decoding.
- Break condition: If the observation model is approximately linear or Gaussian, the computational overhead of learning f(·) and Q(·) provides no benefit over standard Kalman filtering.

### Mechanism 2
- Claim: Learning Q(Xi) via Nadaraya-Watson kernel regression on validation residuals provides accurate conditional covariance estimates for the DKF.
- Mechanism: After estimating f(·) with neural networks or other methods, the residuals Zi - f(Xi) on validation data are used to compute leave-one-out mean squared error and optimize the kernel bandwidth for NW regression, which then estimates Q(Xi).
- Core assumption: The conditional covariance structure can be effectively captured by local kernel regression on the validation residuals.
- Evidence anchors:
  - [section] "The optimal bandwidth of the radial basis kernel for covariance estimation is then found by minimizing the leave-one-out mean square error of Nadaraya-Watson (NW) kernel regression using the validation set and the outer product of the validation residuals (Zi - f(Xi) for all {Zi, Xi} pairs in the validation data)."
  - [corpus] Weak - corpus papers discuss kernel methods and covariance estimation but don't specifically validate this combination for DKF covariance learning.
- Break condition: If the validation data is too small or unrepresentative, the NW regression will fail to capture the true conditional covariance structure.

### Mechanism 3
- Claim: The DKF's performance gain over traditional methods is most pronounced when measured by mean absolute angle error (MAAE) rather than normalized RMSE (nRMSE).
- Mechanism: MAAE measures angular differences between predicted and true velocities, which is more relevant for neuroprosthetic control than nRMSE's absolute error metric.
- Core assumption: For BCI applications, the direction of movement (angle) is more critical than magnitude for effective control.
- Evidence anchors:
  - [section] "Interestingly, DKF application for neural networks increases nRMSE while decreasing MAAE."
  - [section] "the DKF-NW triumphed as the best regression method due to its superior performance with respect to the MAAE metric."
  - [corpus] Weak - neighboring papers don't discuss MAAE or angular metrics for BCI performance evaluation.
- Break condition: If the application requires precise magnitude control rather than direction, nRMSE would be the more relevant metric, potentially reversing performance rankings.

## Foundational Learning

- Concept: Bayesian filtering and Bayes' theorem
  - Why needed here: The DKF fundamentally relies on Bayes' theorem to approximate the observation model by substituting p(Xi|Zi) with p(Zi|Xi)/p(Zi)
  - Quick check question: Can you explain how Bayes' theorem allows us to work with p(Zi|Xi) instead of the intractable p(Xi|Zi)?

- Concept: Kalman filter variants (EKF, UKF, DKF)
  - Why needed here: Understanding the differences between these filters is crucial - the DKF keeps linear Gaussian dynamics but learns nonlinear observation mappings, unlike EKF which linearizes and UKF which uses sigma points
  - Quick check question: What are the key differences in how EKF, UKF, and DKF handle nonlinear observation models?

- Concept: Kernel regression and covariance estimation
  - Why needed here: The DKF uses Nadaraya-Watson kernel regression to estimate both the observation-to-state mapping f(·) and the conditional covariance Q(·) from validation residuals
  - Quick check question: How does leave-one-out cross-validation help optimize the kernel bandwidth for covariance estimation in the DKF framework?

## Architecture Onboarding

- Component map: Data preprocessing -> Regression method (NN/GP/NW/LSTM) -> DKF filtering (if applicable) -> Performance evaluation (nRMSE/MAAE)
- Critical path: Preprocess neural data -> Split into training/validation/test -> Learn f(·) on training -> Optimize Q(·) on validation -> Apply DKF filtering on test -> Evaluate performance
- Design tradeoffs: DKF-NW provides best overall performance but requires careful kernel bandwidth optimization; DKF-NN is faster but may overfit; LSTM avoids explicit filtering but performs worse on MAAE
- Failure signatures: Poor kernel bandwidth selection leads to unstable covariance estimates; insufficient validation data causes unreliable Q(·) estimation; using nRMSE alone may mask directional accuracy issues
- First 3 experiments:
  1. Implement Kalman filter baseline on preprocessed data to establish performance floor
  2. Add DKF-NW implementation and compare both nRMSE and MAAE metrics
  3. Test DKF-NN and DKF-GP variants to understand performance tradeoffs between different regression methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does the Discriminative Kalman Filter (DKF) application for neural networks increase nRMSE while decreasing MAAE?
- Basis in paper: [explicit] The paper states "Interestingly, DKF application for neural networks increases nRMSE while decreasing MAAE."
- Why unresolved: The paper observes this phenomenon but does not provide a theoretical explanation for why applying DKF to neural network outputs would improve angular error metrics while worsening normalized root mean square error.
- What evidence would resolve it: Additional theoretical analysis or controlled experiments varying network architectures, training objectives, and DKF parameters to understand the trade-off between these two error metrics.

### Open Question 2
- Question: How would transformer architectures perform compared to LSTMs for neural decoding when combined with DKF methods?
- Basis in paper: [inferred] The paper mentions "we also tried using a Transformer model to estimate f(Xi, Xi-1, Xi-2)" but found LSTMs superior "given our small input timestep size of 3" and that "it is certainly possible that such an architecture could surpass LSTM performance, especially in trials of much longer durations with longer-range dependencies."
- Why unresolved: The authors only briefly tested transformers with a very limited context window and found them inferior, but suggest they might perform better with longer sequences and dependencies.
- What evidence would resolve it: Systematic comparison of transformer vs LSTM architectures with varying context windows and sequence lengths, particularly for longer-duration trials with complex temporal dependencies.

### Open Question 3
- Question: What causes the significant variation in DKF performance across different trials and random seeds?
- Basis in paper: [explicit] The paper shows "Tables 4 and 5" demonstrating "average performance on all eight of the twelve trials that had the requisite number of samples (at least 6,000) using ten different random seeds," with results varying considerably across trials.
- Why unresolved: The paper observes performance variability across trials and seeds but doesn't investigate the underlying causes or identify which factors (trial characteristics, seed sensitivity, or method-specific issues) contribute most to this variability.
- What evidence would resolve it: Detailed analysis of trial characteristics that correlate with DKF performance, sensitivity analysis across more random seeds, and investigation of whether certain trials consistently challenge specific methods.

## Limitations

- The DKF-GP performance degraded compared to the original implementation due to differences between GPML and scikit-learn libraries
- Significant performance variation exists across different trials and random seeds, suggesting method sensitivity to data characteristics
- The superiority of DKF-NW on MAAE metrics lacks strong corpus evidence for angular metrics being the appropriate evaluation measure for BCI applications

## Confidence

- DKF performance improvements: High
- MAAE vs nRMSE metric importance: Medium
- Implementation-specific performance differences: Low

## Next Checks

1. Re-run DKF-NW experiments with multiple random seeds to verify the consistency of MAAE improvements
2. Implement a simplified neural network version using standard optimization to isolate effects of the Bayesian regularization
3. Conduct ablation studies removing the DKF filtering step from DKF-NN to quantify the contribution of filtering vs. regression alone