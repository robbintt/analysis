---
ver: rpa2
title: Less is More for Improving Automatic Evaluation of Factual Consistency
arxiv_id: '2404.06579'
source_url: https://arxiv.org/abs/2404.06579
tags:
- alignscore
- text
- data
- factual
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LIM-RA, an improved factual consistency evaluation
  model that demonstrates superior performance over previous methods. The key insight
  is that utilizing a smaller number of high-quality training samples can lead to
  better performance.
---

# Less is More for Improving Automatic Evaluation of Factual Consistency

## Quick Facts
- arXiv ID: 2404.06579
- Source URL: https://arxiv.org/abs/2404.06579
- Authors: Tong Wang; Ninad Kulkarni; Yanjun Qi
- Reference count: 14
- This paper proposes LIM-RA, an improved factual consistency evaluation model that demonstrates superior performance over previous methods. The key insight is that utilizing a smaller number of high-quality training samples can lead to better performance.

## Executive Summary
This paper introduces LIM-RA, a novel approach to automatic factual consistency evaluation that challenges conventional wisdom about training data quantity. By carefully cleaning and reducing the training dataset size, the authors demonstrate that models can achieve superior performance compared to those trained on larger, noisier datasets. LIM-RA leverages synthetic robustness data to handle name and number variations, and employs a DeBERTa-based architecture to achieve state-of-the-art results across multiple benchmarks. The approach shows particular effectiveness in evaluating outputs from large language models, outperforming strong baselines like AlignScore and ChatGPT.

## Method Summary
LIM-RA is trained on a cleaned and reduced version of AlignScore's training data, using only 10% of the original samples (452k samples) combined with synthetic robustness data. The synthetic data is generated by perturbing names and numbers in DocNLI samples using Mistral-7B. The model uses a DeBERTa architecture with a 3-way classification head for entailment, neutral, and contradiction relationships. Training is performed for 3 epochs with AdamW optimizer, and evaluation is conducted on four benchmarks (SummaC, TRUE, SummEdits, LLMR) across 33 test datasets.

## Key Results
- LIM-RA achieves the highest score on 24 out of 33 test datasets
- Outperforms strong baselines including AlignScore and ChatGPT on LLM output evaluation
- Demonstrates that using only 10% of cleaned training data can outperform models trained on full datasets
- Shows particular effectiveness on name and number perturbation robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cleaning and reducing training data size improves model performance
- Mechanism: Removing low-quality datasets and focusing on high-quality samples allows the model to learn more relevant patterns without noise interference
- Core assumption: Not all training data points contribute equally to model performance; some datasets introduce noise that degrades performance
- Evidence anchors:
  - [abstract] "utilizing a smaller number of data points can actually improve performance"
  - [section] "our ablation studies shown in Figure 1 indicate that the answer is 'No'"
  - [corpus] Weak - only 5 related papers found, average neighbor FMR=0.524 suggests moderate relevance but limited direct evidence for this specific mechanism
- Break condition: If cleaned data still contains subtle biases or if remaining samples don't capture the full distribution of factual consistency scenarios

### Mechanism 2
- Claim: Synthetic robustness data improves model handling of name/number variations
- Mechanism: Generating perturbed versions of training samples with name and number variations teaches the model to recognize semantically equivalent but syntactically different expressions
- Core assumption: The model's failure on name/number perturbations stems from insufficient exposure to such variations during training
- Evidence anchors:
  - [section] "We also notice AlignScore fails on name or number perturbations as illustrated in Table 1"
  - [section] "We create two synthetic datasets: Robust-Name and Robust-Number datasets"
  - [corpus] Weak - no direct evidence in neighbor papers about synthetic robustness data for factual consistency
- Break condition: If perturbations create unrealistic variations that don't reflect actual usage patterns or if the model overfits to specific perturbation patterns

### Mechanism 3
- Claim: DeBERTa architecture outperforms RoBERTa for this task
- Mechanism: DeBERTa's disentangled attention and enhanced mask decoder provide better representation learning for factual consistency evaluation
- Core assumption: The architectural improvements in DeBERTa translate to better performance on the specific task of factual consistency evaluation
- Evidence anchors:
  - [section] "DeBERTa achieves the best overall performance while DistilBERT has poor performance"
  - [section] "We train a pre-trained NLI DeBERTa model"
  - [corpus] Weak - neighbor papers don't provide direct comparison of DeBERTa vs RoBERTa for factual consistency
- Break condition: If the performance gain is due to factors other than architecture (like pre-training data differences) or if the task doesn't benefit from DeBERTa's specific improvements

## Foundational Learning

- Concept: Natural Language Inference (NLI) and factual consistency evaluation
  - Why needed here: The task requires understanding semantic relationships between context and claims
  - Quick check question: Can you explain the difference between entailment, contradiction, and neutral relationships in NLI?

- Concept: Data cleaning and quality assessment
  - Why needed here: The paper demonstrates that removing low-quality datasets improves performance
  - Quick check question: What metrics would you use to evaluate the quality of a training dataset for factual consistency?

- Concept: Synthetic data generation and augmentation
  - Why needed here: The paper uses synthetic perturbations to improve model robustness
  - Quick check question: How would you ensure that synthetic perturbations maintain semantic meaning while testing model robustness?

## Architecture Onboarding

- Component map: Context-claim pairs -> Context chunking (350 tokens), claim sentence splitting -> DeBERTa-based classifier with 3-way classification head -> Factual consistency score (0-1) based on aligned class probability -> Average sentence-level scores for final context-level score

- Critical path:
  1. Context and claim preprocessing
  2. Sentence-level alignment scoring
  3. Aggregation to context-level score
  4. Output formatting

- Design tradeoffs:
  - Model choice: DeBERTa vs RoBERTa - better performance vs. faster inference
  - Training data size: Smaller cleaned dataset vs. larger noisy dataset
  - Chunking strategy: 350-token chunks vs. alternative approaches

- Failure signatures:
  - Low performance on name/number variations (indicates need for robustness data)
  - Inconsistent scores across similar contexts (suggests data quality issues)
  - High variance in predictions (may indicate insufficient training or model capacity)

- First 3 experiments:
  1. Test LIM-RA on a simple dataset with clear name variations to verify robustness improvements
  2. Compare LIM-RA vs AlignScore on a dataset with mixed quality samples to verify cleaning benefits
  3. Test LIM-RA with and without synthetic data on a perturbation-focused dataset to isolate robustness impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of using different data cleaning techniques on model performance?
- Basis in paper: [explicit] The paper mentions data cleaning steps like removing certain datasets and filtering samples based on context length, but does not explore the impact of these specific techniques on performance.
- Why unresolved: The paper does not provide ablation studies or comparisons of different data cleaning methods to determine their individual effects on model performance.
- What evidence would resolve it: Conducting experiments with different data cleaning techniques and comparing their impact on model performance would help determine the most effective methods.

### Open Question 2
- Question: How does the model perform on out-of-domain datasets not included in the four benchmarks?
- Basis in paper: [inferred] The paper evaluates the model on four benchmarks but does not mention testing on datasets outside these benchmarks or exploring the model's generalizability to unseen domains.
- Why unresolved: The paper focuses on specific benchmarks and does not provide information on the model's performance on diverse, out-of-domain datasets.
- What evidence would resolve it: Evaluating the model on a wide range of out-of-domain datasets would provide insights into its generalizability and robustness.

### Open Question 3
- Question: What is the effect of using different pre-trained models (e.g., BERT, RoBERTa) as the base for LIM-RA?
- Basis in paper: [explicit] The paper mentions experimenting with different pre-trained models like RoBERTa and DeBERTa but does not provide a detailed comparison of their performance.
- Why unresolved: The paper does not explore the impact of using different pre-trained models on the final performance of LIM-RA.
- What evidence would resolve it: Conducting experiments with various pre-trained models and comparing their performance would help determine the optimal base model for LIM-RA.

### Open Question 4
- Question: How does the model handle long-form texts beyond the chunking strategy used in AlignScore?
- Basis in paper: [inferred] The paper mentions that AlignScore uses a chunking strategy for long texts, but it does not explore alternative approaches or the limitations of the chunking strategy.
- Why unresolved: The paper does not provide information on how the model performs with very long texts or explore alternative methods for handling long-form inputs.
- What evidence would resolve it: Evaluating the model's performance on long-form texts and comparing it with other approaches would help determine the effectiveness of the chunking strategy and potential limitations.

### Open Question 5
- Question: What is the impact of using different synthetic data generation techniques on the model's robustness?
- Basis in paper: [explicit] The paper introduces synthetic robustness data using name and number perturbations but does not explore other data generation techniques or their effects on model performance.
- Why unresolved: The paper focuses on specific synthetic data generation methods and does not provide insights into the impact of alternative techniques on model robustness.
- What evidence would resolve it: Conducting experiments with various synthetic data generation techniques and comparing their impact on model robustness would help determine the most effective approaches.

## Limitations

- The "less is more" principle for training data quality may not generalize to other tasks or domains
- Synthetic data robustness improvements are limited to name and number variations
- DeBERTa's superiority may be partially due to pre-training data differences rather than architecture alone

## Confidence

- High Confidence: The core empirical finding that cleaned training data outperforms larger noisy datasets
- Medium Confidence: The specific claim that LIM-RA establishes new state-of-the-art benchmarks across 24/33 datasets
- Low Confidence: The generalizability of the "less is more" principle to other factual consistency evaluation tasks

## Next Checks

1. **Cross-Domain Validation**: Test LIM-RA on factual consistency datasets from domains not represented in the training data (e.g., scientific literature, legal documents) to assess generalizability beyond news and summary domains.

2. **Training Data Sensitivity Analysis**: Systematically vary the percentage of training data used (10%, 25%, 50%, 75%, 100%) on held-out validation sets to determine if the 10% optimal point is consistent across different dataset splits and model initializations.

3. **Perturbation Robustness Testing**: Create a comprehensive evaluation suite with diverse perturbation types (synonym replacement, paraphrasing, entity substitution beyond names/numbers) to verify that LIM-RA's robustness extends beyond the specific synthetic perturbations used during training.