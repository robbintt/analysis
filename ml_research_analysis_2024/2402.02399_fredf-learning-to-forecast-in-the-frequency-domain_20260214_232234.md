---
ver: rpa2
title: 'FreDF: Learning to Forecast in the Frequency Domain'
arxiv_id: '2402.02399'
source_url: https://arxiv.org/abs/2402.02399
tags:
- uni00000013
- uni00000015
- uni00000017
- fredf
- uni00000019
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of label autocorrelation in multi-step
  time series forecasting, where the direct forecast (DF) paradigm overlooks correlations
  between future time steps, leading to suboptimal performance. The proposed method,
  FreDF, enhances DF by aligning forecasts and labels in the frequency domain using
  Fourier transform, thereby reducing the impact of label autocorrelation.
---

# FreDF: Learning to Forecast in the Frequency Domain

## Quick Facts
- arXiv ID: 2402.02399
- Source URL: https://arxiv.org/abs/2402.02399
- Authors: Hao Wang; Licheng Pan; Zhichao Chen; Degui Yang; Sen Zhang; Yifei Yang; Xinggao Liu; Haoxuan Li; Dacheng Tao
- Reference count: 40
- Primary result: FreDF significantly outperforms state-of-the-art forecasting methods by reducing label autocorrelation effects through frequency domain alignment

## Executive Summary
This paper addresses a critical limitation in multi-step time series forecasting: label autocorrelation, where future time steps are dependent on each other rather than being independent as assumed by the Direct Forecast (DF) paradigm. The authors propose FreDF, which enhances DF by incorporating frequency domain alignment using Fourier transform. By projecting forecasts and labels onto orthogonal frequency bases, FreDF effectively reduces the impact of autocorrelation while maintaining the practical advantages of DF. Experiments across multiple datasets demonstrate substantial improvements over state-of-the-art methods.

## Method Summary
FreDF combines temporal and frequency domain losses to address label autocorrelation in multi-step forecasting. The method applies FFT to both predictions and labels, computing a frequency domain loss based on absolute differences of Fourier coefficients. This is combined with the standard time domain loss using a hyperparameter α that controls the relative importance of each domain. The approach is compatible with various base forecasting models and orthogonal transforms beyond Fourier. The frequency domain transformation effectively bypasses autocorrelation by leveraging the orthogonality of Fourier bases, where components become independent given the input sequence.

## Key Results
- FreDF achieves 16.4% relative improvement in MSE and 15.7% in MAE over state-of-the-art methods on average across multiple datasets
- Outperforms iTransformer, FreTS, and other leading methods on ETT, Weather, and Traffic datasets
- Demonstrates effectiveness for both short-term forecasting and missing data imputation tasks
- Shows consistent improvements across different base models including iTransformer, DLinear, and Autoformer

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Label autocorrelation causes biased learning objectives in Direct Forecast (DF) paradigm
- Mechanism: DF assumes conditional independence among future steps, but label autocorrelation creates dependencies between future steps, violating this assumption and leading to biased estimation
- Core assumption: The label sequence Y is autoregressively generated with Y_{t+1} depending on Y_t
- Evidence anchors:
  - [abstract] "modern forecasting models primarily adhere to the Direct Forecast (DF) paradigm, generating multi-step forecasts independently and disregarding label autocorrelation over time"
  - [section 3.1] "The DF paradigm computes the forecast error at each step independently, treating them as separate tasks. This method, while practical, oversights the autocorrelation present within Y"
  - [corpus] Weak evidence - no direct mention of DF bias in corpus
- Break condition: When label autocorrelation is negligible or when using models that naturally handle temporal dependencies

### Mechanism 2
- Claim: Frequency domain transformation reduces autocorrelation effects through orthogonal bases
- Mechanism: Fourier transform projects the sequence onto orthogonal bases, making frequency components independent given the input sequence, thus bypassing autocorrelation
- Core assumption: Fourier bases are orthogonal and independent for different frequencies
- Evidence anchors:
  - [section 4.1] "By transforming to the frequency domain where bases are orthogonal and independent, the impact of autocorrelation is found to be effectively diminished"
  - [section 4.1] "In this context, L is treated as the confounder, F_k (the component of F corresponding to frequency k) as the treatment, and F_k' as the outcome... most non-diagonal elements show negligible values"
  - [corpus] No direct evidence about frequency domain reducing autocorrelation
- Break condition: When frequency components are not truly independent or when transform doesn't preserve relevant signal characteristics

### Mechanism 3
- Claim: Combining time and frequency domain losses improves performance over single-domain approaches
- Mechanism: Temporal loss captures overall trends while frequency loss handles high-frequency components and autocorrelation; their combination provides complementary information
- Core assumption: Both time and frequency domain representations contain useful information for forecasting
- Evidence anchors:
  - [section 5.3] "Learning to forecast in both domains generally showcase improvement compared to relying solely on one domain"
  - [section 5.2.4] "The forecasts generated under FreDF not only keep pace with the label sequence, accurately capturing high-frequency components, but also exhibit a smoother appearance"
  - [corpus] No evidence about combining domain losses
- Break condition: When one domain dominates the signal characteristics or when losses conflict rather than complement

## Foundational Learning

- Concept: Fourier Transform
  - Why needed here: Core transformation that enables FreDF to bypass label autocorrelation by projecting onto orthogonal frequency bases
  - Quick check question: What property of Fourier bases makes them effective for reducing autocorrelation in forecasting?

- Concept: Double Machine Learning (DML)
  - Why needed here: Method used to quantify causation strength while controlling for confounding effects from historical data
  - Quick check question: How does DML help distinguish true autocorrelation from pseudo-correlations caused by confounding?

- Concept: Direct Forecast (DF) Paradigm
  - Why needed here: Baseline forecasting approach that FreDF enhances by adding frequency domain alignment
  - Quick check question: What key assumption does DF make about future time steps that FreDF addresses?

## Architecture Onboarding

- Component map: Historical sequence L(n) → Model g → Time loss → Frequency transform → Frequency loss → Combined loss → Parameter update
- Critical path: Historical sequence → Model g → Time loss → Frequency transform → Frequency loss → Combined loss → Parameter update
- Design tradeoffs:
  - α parameter: Higher α emphasizes frequency domain (better autocorrelation handling) but may lose time-domain trends
  - Transform choice: Fourier vs Legendre vs Chebyshev affects orthogonality properties and signal preservation
  - Model compatibility: FreDF should work with any base model but may need different α tuning
- Failure signatures:
  - Performance worse than baseline: Likely α too high or frequency transform distorting signal
  - Training instability: Magnitude differences between time and frequency losses not properly handled
  - No improvement: Label autocorrelation not significant in dataset or transform not effective
- First 3 experiments:
  1. Test baseline model vs. model with FreDF (α=0.5) on a single dataset to verify improvement
  2. Vary α from 0 to 1 on same dataset to find optimal value
  3. Try different transforms (Fourier, Legendre) to compare effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does FreDF's effectiveness depend on the specific choice of Fourier transform versus other orthogonal transforms like Chebyshev or Legendre?
- Basis in paper: [explicit] The paper explores FreDF's compatibility with various transforms including Fourier, Chebyshev, Legendre, and Laguerre, but primarily focuses on Fourier for experimental results.
- Why unresolved: While the paper shows FreDF works with different transforms, it doesn't definitively establish whether Fourier is optimal or if other transforms could yield superior results in specific scenarios.
- What evidence would resolve it: Systematic comparison of FreDF's performance across multiple datasets and forecasting tasks using different orthogonal transforms, with statistical analysis of significance differences.

### Open Question 2
- Question: How does FreDF's performance scale with increasingly long forecast horizons beyond those tested in the experiments?
- Basis in paper: [inferred] The experiments test up to T=720 forecast length, but the paper discusses FreDF's potential for handling long-term forecasting without empirical validation beyond this point.
- Why unresolved: The experiments don't explore the limits of FreDF's effectiveness as forecast horizons extend into multi-year predictions, which would be relevant for many real-world applications.
- What evidence would resolve it: Experiments testing FreDF on datasets with multi-year forecast horizons, comparing performance degradation rates against baseline methods.

### Open Question 3
- Question: What is the computational overhead of FreDF compared to baseline methods, particularly for large-scale industrial applications?
- Basis in paper: [inferred] While the paper mentions FFT complexity (O(L log L)), it doesn't provide detailed computational benchmarks or comparisons of training/inference times between FreDF and baselines.
- Why unresolved: The paper focuses on accuracy improvements but doesn't quantify the trade-off between performance gains and computational costs, which is crucial for practical deployment.
- What evidence would resolve it: Comprehensive benchmarks measuring training time, inference latency, and memory usage for FreDF versus baselines across different hardware configurations and dataset sizes.

## Limitations
- Limited empirical validation of autocorrelation reduction mechanism beyond correlation matrix analysis
- Focus on specific dataset characteristics without exploring edge cases where frequency domain transformation might fail
- Computational overhead implications not fully quantified for large-scale industrial deployment

## Confidence

- Mechanism 1 (DF bias): Medium confidence - well-motivated theoretically but limited empirical quantification
- Mechanism 2 (Frequency independence): Medium confidence - supported by correlation analysis but lacks comprehensive validation
- Mechanism 3 (Combined domain losses): High confidence - directly supported by ablation studies showing consistent improvements

## Next Checks

1. Conduct ablation study measuring autocorrelation strength in forecasts with and without FreDF to quantify the reduction mechanism
2. Perform controlled experiments on synthetic data with varying autocorrelation levels to test FreDF's effectiveness threshold
3. Analyze failure cases where FreDF underperforms to identify dataset characteristics where frequency domain transformation is less effective