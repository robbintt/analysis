---
ver: rpa2
title: 'LLMs for XAI: Future Directions for Explaining Explanations'
arxiv_id: '2405.06064'
source_url: https://arxiv.org/abs/2405.06064
tags:
- explanations
- llms
- explanation
- narrative
- narratives
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores using LLMs to generate natural language explanations
  from SHAP-based ML explanations, aiming to improve interpretability for non-technical
  users. The authors propose five research directions: defining evaluation metrics,
  prompt design, comparing LLMs, training/fine-tuning, and integrating external data.'
---

# LLMs for XAI: Future Directions for Explaining Explanations

## Quick Facts
- arXiv ID: 2405.06064
- Source URL: https://arxiv.org/abs/2405.06064
- Reference count: 25
- Demonstrates GPT-4 outperforms GPT-3.5 in generating sound, complete, and context-aware XAI explanations

## Executive Summary
This paper explores using Large Language Models (LLMs) to transform technical SHAP-based machine learning explanations into natural language narratives for non-technical users. The authors propose five key research directions: defining evaluation metrics, prompt design, comparing LLMs, training/fine-tuning, and integrating external data. Initial experiments demonstrate that GPT-4 produces more accurate and contextually aware explanations than GPT-3.5, while user studies show a clear preference for narrative explanations over traditional plots.

## Method Summary
The authors conduct experiments comparing GPT-4 and GPT-3.5 in generating explanations from SHAP-based ML explanations. They evaluate performance using custom metrics for soundness, completeness, and context-awareness. A user study with 20 participants compares narrative explanations against traditional SHAP plots across subjective measures including usefulness, trustworthiness, and understandability.

## Key Results
- GPT-4 outperformed GPT-3.5 on soundness (1.789 vs 1.211), completeness (1.700 vs 1.422), and context-awareness (0.889 vs 0.522)
- GPT-3.5 generated shorter, more fluent responses despite lower metric scores
- 12 out of 20 participants preferred narrative explanations over traditional plots across all subjective measures

## Why This Works (Mechanism)
LLMs can bridge the gap between technical ML explanations and human comprehension by translating complex SHAP values into coherent narratives. Their natural language capabilities allow for context-aware interpretation that adapts to user needs, making AI decisions more accessible to non-technical stakeholders.

## Foundational Learning
- **SHAP values**: Game-theoretic approach to explain individual predictions - needed for understanding feature importance in ML models; quick check: verify SHAP implementations produce expected local explanations
- **Prompt engineering**: Crafting effective inputs for LLM responses - needed for optimal explanation generation; quick check: test different prompt templates on small examples
- **Evaluation metrics**: Soundness, completeness, and context-awareness measures - needed to objectively assess explanation quality; quick check: ensure metrics align with user comprehension goals
- **User-centered design**: Considering end-user needs in XAI - needed for practical deployment; quick check: conduct preliminary user interviews to identify pain points

## Architecture Onboarding
Component map: SHAP explanations -> LLM prompts -> Generated narratives -> User feedback
Critical path: Data → SHAP computation → Prompt formulation → LLM generation → User comprehension
Design tradeoffs: GPT-4 accuracy vs GPT-3.5 efficiency; technical precision vs narrative fluency
Failure signatures: Inaccurate SHAP inputs lead to misleading narratives; poor prompt design results in irrelevant explanations
First experiments: 1) Test single-feature vs multi-feature explanations; 2) Compare different LLM temperature settings; 3) Evaluate explanation quality across different ML models

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Small user study sample size (n=20) may limit generalizability
- Evaluation metrics may not capture all aspects of explanation quality
- Experiments focused on single SHAP-based example rather than diverse datasets and models

## Confidence
High confidence in GPT-4's superior performance metrics; Medium confidence in narrative preference findings due to sample size; Low confidence in proposed research directions without empirical validation.

## Next Checks
1. Conduct larger-scale user study (n≥100) across diverse user groups and decision contexts to validate narrative explanation preferences
2. Test LLM explanation generation across multiple ML models and diverse real-world datasets to assess generalizability
3. Develop and validate more comprehensive evaluation metrics that capture both technical accuracy and practical utility of generated explanations