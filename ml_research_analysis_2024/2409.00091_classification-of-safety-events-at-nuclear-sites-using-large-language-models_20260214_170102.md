---
ver: rpa2
title: Classification of Safety Events at Nuclear Sites using Large Language Models
arxiv_id: '2409.00091'
source_url: https://arxiv.org/abs/2409.00091
tags:
- safety
- scrs
- classification
- conference
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes using a large language model (GPT-4) to classify
  nuclear safety events from Station Condition Records (SCRs) as safety-related or
  not. With a highly imbalanced dataset (90% non-safety), initial attempts using traditional
  k-NN classifiers failed to achieve good recall on safety events.
---

# Classification of Safety Events at Nuclear Sites using Large Language Models

## Quick Facts
- **arXiv ID**: 2409.00091
- **Source URL**: https://arxiv.org/abs/2409.00091
- **Reference count**: 0
- **Primary result**: GPT-4 achieved 77% accuracy and 78% positive recall at 70% threshold for classifying safety events

## Executive Summary
This paper explores using large language models (LLMs) to classify nuclear safety events from Station Condition Records (SCRs) as safety-related or not. The dataset is highly imbalanced (90% non-safety events), which posed challenges for traditional machine learning approaches. After refining GPT-4 prompts to better reflect nuclear safety culture and incorporating a numerical safety score, the LLM achieved 77% accuracy with 78% positive recall at a 70% safety score threshold. The authors conclude the model shows promise for augmenting manual review processes and suggest further improvements including integrating GPT-4 Turbo and refining data collection methods.

## Method Summary
The authors employed a multi-stage approach to classify nuclear safety events using LLMs. They first attempted traditional k-NN classifiers but found poor recall on safety events due to dataset imbalance. They then leveraged GPT-4 with refined prompts that incorporated nuclear safety culture context. The methodology included manual labeling by domain experts to create ground truth data, prompt engineering through trial and error, and the addition of a numerical safety score to improve classification performance. The classification output was based on a threshold approach, with 70% determined as the optimal cutoff for positive recall.

## Key Results
- GPT-4 achieved 77% accuracy and 78% positive recall at 70% safety score threshold
- Traditional k-NN classifiers failed to achieve good recall on safety events due to dataset imbalance (90% non-safety)
- Prompt refinement incorporating nuclear safety culture significantly improved performance over baseline approaches

## Why This Works (Mechanism)
The LLM approach works because it can understand nuanced language and context in safety reports that traditional classifiers struggle with. GPT-4's ability to incorporate domain-specific knowledge about nuclear safety culture allows it to better distinguish subtle indicators of safety relevance that might be missed by statistical pattern matching alone. The numerical safety score provides an interpretable confidence metric that enables threshold-based decision making.

## Foundational Learning
- **Dataset Imbalance Management**: Why needed - safety events are rare compared to non-safety events; Quick check - verify class distribution and consider resampling techniques
- **Prompt Engineering**: Why needed - LLMs require careful instruction to perform specialized tasks; Quick check - test multiple prompt variations and track performance changes
- **Domain Knowledge Integration**: Why needed - nuclear safety has specific cultural and regulatory contexts; Quick check - involve subject matter experts in prompt development
- **Threshold Optimization**: Why needed - classification requires balancing precision and recall; Quick check - analyze ROC curves at different threshold values
- **Manual Labeling Processes**: Why needed - ground truth data requires expert validation; Quick check - implement inter-rater reliability measures
- **Performance Metrics Selection**: Why needed - accuracy alone can be misleading with imbalanced data; Quick check - report multiple metrics including recall, precision, and F1-score

## Architecture Onboarding

**Component Map**: Station Condition Records -> Pre-processing -> LLM (GPT-4) -> Safety Score Generation -> Threshold Decision -> Classification Output

**Critical Path**: The critical path involves the LLM inference step, where prompt engineering and domain knowledge integration directly impact classification quality. The safety score generation and threshold application are dependent on this core LLM output.

**Design Tradeoffs**: The authors traded computational simplicity (traditional classifiers) for contextual understanding capability (LLM), accepting higher inference costs for improved recall on the minority safety class. They also traded potential prompt overfitting for immediate performance gains.

**Failure Signatures**: Primary failure modes include: misclassification of nuanced safety events due to prompt limitations, threshold selection errors leading to high false negatives or false positives, and dataset bias from single-site data limiting generalizability.

**First 3 Experiments**:
1. Test GPT-4 with varying safety score thresholds (60%, 70%, 80%) to optimize precision-recall tradeoff
2. Apply the same prompt to multiple nuclear site datasets to assess generalizability
3. Compare GPT-4 performance against human expert classification on a subset of records

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset imbalance (90% non-safety events) significantly impacts classifier performance and requires careful threshold selection
- Reliance on manual labeling by domain experts introduces potential subjectivity and bias in ground truth data
- Single nuclear site focus limits generalizability across different operational contexts and reporting styles
- Prompts refined through trial and error suggest potential overfitting and lack of systematic engineering methodology

## Confidence
- **High confidence**: The basic methodology of using LLMs for safety event classification is sound and represents a valid approach
- **Medium confidence**: The reported accuracy and recall metrics are reliable for the specific dataset and prompts used, but may not generalize
- **Low confidence**: Claims about the model's readiness for deployment in safety-critical nuclear applications without further validation and refinement

## Next Checks
1. Test the GPT-4 classifier on data from multiple nuclear sites to assess generalizability across different operational contexts and reporting styles
2. Implement cross-validation with domain expert review to establish confidence intervals for the classification metrics and identify systematic error patterns
3. Conduct ablation studies to quantify the contribution of the numerical safety score versus prompt engineering to overall performance, and test alternative scoring thresholds to optimize the precision-recall tradeoff