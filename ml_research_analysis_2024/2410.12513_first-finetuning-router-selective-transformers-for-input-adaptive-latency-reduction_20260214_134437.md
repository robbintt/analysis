---
ver: rpa2
title: 'FiRST: Finetuning Router-Selective Transformers for Input-Adaptive Latency
  Reduction'
arxiv_id: '2410.12513'
source_url: https://arxiv.org/abs/2410.12513
tags:
- router
- lora
- skip
- skipping
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FiRST, a framework for reducing inference
  latency in large language models (LLMs) by adaptively skipping transformer layers
  during decoding. The method employs lightweight routers trained to decide, based
  on the input sequence, which layers can be safely skipped without significant quality
  loss.
---

# FiRST: Finetuning Router-Selective Transformers for Input-Adaptive Latency Reduction

## Quick Facts
- arXiv ID: 2410.12513
- Source URL: https://arxiv.org/abs/2410.12513
- Authors: Akriti Jain; Saransh Sharma; Koyel Mukherjee; Soumyabrata Pal
- Reference count: 40
- Primary result: Achieves 10-20% latency reduction while retaining 80%+ of base model performance across multiple tasks

## Executive Summary
FiRST introduces a framework for reducing inference latency in large language models by adaptively skipping transformer layers during decoding. The method employs lightweight routers that make sequence-level decisions about which layers can be safely skipped based on input representations. By combining router training with LoRA fine-tuning, FiRST maintains quality while achieving significant latency improvements across machine translation, summarization, and question answering tasks. The approach is model-agnostic and leverages KV caching compatibility to ensure efficient autoregressive generation.

## Method Summary
FiRST operates in two phases: first training lightweight router modules that make binary layer-skipping decisions at the sequence level, then applying LoRA fine-tuning to recover any quality degradation from skipping. Routers are single-layer neural networks that take mean-pooled input representations and output probabilities for each layer, with skipping occurring when probability falls below 0.5. The training uses a three-term loss function combining cross-entropy for quality, L2 regularization, and non-skip penalization to encourage layer skipping. Decisions made during prefill are cached and reused during decoding to maintain KV cache compatibility. LoRA adapters are then applied to FFN and attention modules with frozen router weights to smooth skipping decisions and compensate for quality loss.

## Key Results
- Achieves 10-20% latency reduction across machine translation, summarization, and question answering tasks
- Maintains 80% or more of base model performance while skipping 15% of layers on average
- Outperforms existing layer-skipping baselines including Random Skip, Skip Decode, and Unified Skip across all metrics
- Demonstrates effectiveness on both LLaMA-3-8B (32 layers) and LLaMA-3.2-3B (28 layers) architectures

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Sequence-level routing decisions enable KV cache compatibility during decoding
- **Mechanism**: Routers make binary layer-skipping decisions at the sequence level during prefill, then fix these decisions for all decoding tokens. This ensures every token in a sequence follows the same path through the transformer, avoiding the need to recompute KV cache values for preceding tokens.
- **Core assumption**: Sequence-level consistency is sufficient to maintain autoregressive computation while allowing layer skipping
- **Evidence anchors**:
  - [abstract] "Layer decisions are made at the sequence level to maintain compatibility with KV caching"
  - [section] "During the prefill phase, the decisions made by the routers are cached. During the decoding phase, every token adheres to the cached decision made during prefill."
  - [corpus] Weak - no direct neighbor evidence found
- **Break condition**: If sequence-level decisions cannot capture token-level variation in difficulty, some tokens may be processed unnecessarily or insufficiently

### Mechanism 2
- **Claim**: Router training with regularization encourages layer skipping while maintaining quality
- **Mechanism**: Routers are trained using a loss function that combines cross-entropy for quality preservation, L2 regularization on router weights, and a non-skip penalization term that sums router probabilities across layers. This pushes routers toward skipping more layers while still allowing quality-maintaining decisions.
- **Core assumption**: Simple linear classifiers can learn effective layer-skipping patterns from input representations
- **Evidence anchors**:
  - [section] "The training task is modeled as a language modeling task... The loss function comprises of 3 terms: Cross-entropy loss, Regularization loss, Non-skip penalization loss"
  - [abstract] "The method employs lightweight routers trained to decide, based on the input sequence, which layers can be safely skipped"
  - [corpus] Weak - no direct neighbor evidence found
- **Break condition**: If the regularization strength is mis-tuned, routers may either skip too aggressively (hurting quality) or too conservatively (losing latency benefits)

### Mechanism 3
- **Claim**: LoRA fine-tuning recovers quality degradation from layer skipping while preserving skipping behavior
- **Mechanism**: After router training, LoRA adapters are added to FFN and attention modules with frozen router weights. A non-skip penalization loss is added during LoRA training to prevent the model from reverting to full computation paths. This smooths the skipping decisions and compensates for quality loss.
- **Core assumption**: Low-rank adapters can adapt the model to layer skipping without needing to update the router
- **Evidence anchors**:
  - [abstract] "FiRST is model-agnostic and supports LoRA fine-tuning to recover any quality drop caused by skipping"
  - [section] "To compensate for the loss in performance caused by skipping layers, we finetune the router-augmented pretrained model on the downstream task using Low Rank Adapters (LoRA)"
  - [corpus] Weak - no direct neighbor evidence found
- **Break condition**: If LoRA fine-tuning changes the model's internal representations too much, it may invalidate the router's learned skipping decisions

## Foundational Learning

- **Concept**: KV caching in autoregressive transformers
  - **Why needed here**: Understanding why token-level early exits are incompatible with modern inference while sequence-level decisions preserve cache efficiency
  - **Quick check question**: What happens to the KV cache when a token exits early in a sequence where subsequent tokens haven't yet reached that layer?

- **Concept**: Transformer layer architecture and skip connections
  - **Why needed here**: Understanding how layer skipping modifies the standard transformer computation (Eq 3 vs Eq 1)
  - **Quick check question**: How does the computation change when a layer is skipped versus when it's processed normally?

- **Concept**: Router architectures and binary classification
  - **Why needed here**: Understanding the lightweight router design and how sequence-level aggregation works
  - **Quick check question**: How does taking the mean across tokens in a sequence affect the router's decision-making compared to token-level decisions?

## Architecture Onboarding

- **Component map**: Base LLM -> Router modules (one per layer) -> Conditional layer execution -> KV cache management -> Output generation
- **Critical path**: 
  1. Input tokenization and embedding
  2. Router decision making (sequence-level)
  3. Conditional layer execution based on router output
  4. KV cache management based on fixed sequence decisions
  5. Output generation
- **Design tradeoffs**:
  - Sequence-level vs token-level decisions (compatibility vs adaptivity)
  - Router complexity vs inference overhead
  - Skipping rate vs quality retention
  - Training stability vs end-to-end optimization
- **Failure signatures**:
  - Routers outputting ~0.5 consistently (unstable decisions)
  - KV cache recomputation during decoding (sequence-level assumption violated)
  - Quality drops exceeding acceptable thresholds
  - Latency improvements not materializing (routers not skipping effectively)
- **First 3 experiments**:
  1. Run inference with routers but no skipping (all decisions set to 1.0) to verify baseline functionality
  2. Test router outputs on validation data to check if they're producing reasonable probabilities
  3. Verify KV cache consistency by comparing outputs with and without skipping at very low skip rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the routers be effectively reused across different domains within the same task (e.g., summarization of news vs. scientific articles) without retraining?
- Basis in paper: [explicit] The paper demonstrates cross-dataset reusability for routers within the same task (e.g., routers trained on SQuAD used for Natural Questions), showing comparable performance to routers trained directly on the target dataset.
- Why unresolved: The paper only tests reusability across datasets of the same task (translation, summarization, QA), but doesn't explore cross-domain variations within these tasks, which could have different linguistic styles and structures.
- What evidence would resolve it: Experimental results comparing router performance on the same task across different domains (e.g., summarizing news vs. scientific articles) with and without retraining.

### Open Question 2
- Question: Would joint training of routers and LoRA modules improve performance compared to the sequential training approach used in the paper?
- Basis in paper: [inferred] The paper explicitly states that joint training is "theoretically possible" but was avoided due to "optimization instability" in initial experiments, suggesting potential benefits were observed but implementation challenges remained.
- Why unresolved: The paper does not provide quantitative comparisons between joint and sequential training approaches, leaving uncertainty about whether the optimization challenges could be overcome with better techniques.
- What evidence would resolve it: Head-to-head comparison of joint vs. sequential training approaches with metrics for both quality and stability across multiple tasks and model sizes.

### Open Question 3
- Question: How would FiRST perform on encoder-only transformer architectures compared to decoder-only architectures?
- Basis in paper: [inferred] The paper focuses on decoder-only models (LLaMA-3 variants) and mentions that early exit strategies work better for encoder-only models, but doesn't explore FiRST's application to encoder-only architectures like BERT.
- Why unresolved: The paper's methodology and experiments are limited to decoder-only models, leaving the effectiveness of FiRST on encoder-only models untested, particularly for tasks like classification or sequence labeling.
- What evidence would resolve it: Implementation and evaluation of FiRST on encoder-only models like BERT across various tasks, comparing performance metrics and latency improvements to the decoder-only results presented.

## Limitations

- The sequence-level routing assumption has not been empirically validated against token-level variation in task difficulty
- Regularization coefficients are tuned per dataset without clear generalization guidelines
- The paper's claim of being "model-agnostic" is based on testing only two LLaMA-3 variants without exploring other model families

## Confidence

**High Confidence**: The fundamental architecture of FiRST (router modules, LoRA fine-tuning, sequence-level decisions) is clearly specified and the experimental methodology follows standard practices. The latency reduction mechanism through layer skipping is well-established in prior work.

**Medium Confidence**: The quality retention claims (80%+ performance at 10-20% latency reduction) are supported by experiments across multiple tasks, but the comparison baselines are relatively limited. The effectiveness of the three-term loss function for router training is plausible but not independently verified.

**Low Confidence**: The specific values for regularization coefficients and the exact learning rate schedules are not fully specified, making exact reproduction challenging. The paper's claim that FiRST is "model-agnostic" is based on testing only two LLaMA-3 variants without exploring architectural differences in other model families.

## Next Checks

1. **Sequence-Level Decision Sensitivity Test**: Run FiRST with token-level routing decisions (violating KV cache compatibility) and compare quality/latency trade-offs to verify whether sequence-level aggregation is indeed necessary or optimal for the claimed performance gains.

2. **Router Decision Distribution Analysis**: Examine the distribution of router outputs across validation data to determine if routers consistently identify skippable layers or if they produce near-random decisions for certain input types. This would validate whether the learned skipping patterns generalize beyond the training distribution.

3. **KV Cache Consistency Verification**: Implement a debug mode that tracks KV cache recomputation during decoding. Compare outputs and latency when routers make sequence-level vs token-level decisions to confirm that the fixed-sequence approach actually prevents the cache invalidation issues mentioned in the mechanism section.