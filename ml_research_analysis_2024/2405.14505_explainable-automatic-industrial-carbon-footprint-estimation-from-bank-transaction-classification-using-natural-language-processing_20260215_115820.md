---
ver: rpa2
title: Explainable automatic industrial carbon footprint estimation from bank transaction
  classification using natural language processing
arxiv_id: '2405.14505'
source_url: https://arxiv.org/abs/2405.14505
tags:
- classification
- bank
- transaction
- automatic
- estimation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an explainable machine learning solution for
  estimating industrial carbon footprints from bank transaction classification. The
  method addresses the lack of transparency in existing automatic approaches.
---

# Explainable automatic industrial carbon footprint estimation from bank transaction classification using natural language processing

## Quick Facts
- arXiv ID: 2405.14505
- Source URL: https://arxiv.org/abs/2405.14505
- Reference count: 40
- Key outcome: 90% classification accuracy with 70% satisfactory explanations for industrial carbon footprint estimation

## Executive Summary
This paper presents a machine learning solution for estimating industrial carbon footprints through classification of bank transactions into activity sectors. The approach addresses the lack of transparency in existing automatic methods by incorporating an explainability module that extracts and validates relevant terms from transaction descriptions. Using Support Vector Machine, Random Forest, and LSTM models, the system achieves high classification performance while providing interpretable explanations that can be automatically validated against sector descriptions.

## Method Summary
The methodology combines text preprocessing with feature engineering to handle brief bank transaction descriptions (average 10 words). Classification is performed using SVM, RF, and LSTM models, while explainability is achieved through a model-agnostic approach using LIME for feature selection. The system extracts relevant terms from transaction descriptions and validates them against sector descriptions using similarity metrics. Enterprise-specific descriptors are incorporated when available to enrich explanations. The final carbon footprint estimation aggregates classification results weighted by transaction amounts.

## Key Results
- Classification models achieve 90% accuracy, precision, and recall
- Over 70% of explanations are satisfactory to human operators
- Approximately 60% of explanations are automatically validated
- The approach successfully handles brief transaction descriptions (10 words average)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model-agnostic explainability enables transparent industrial carbon footprint estimation
- Mechanism: By using LIME to generate local surrogate models, the system extracts relevant terms from transaction descriptions without relying on internal model structures. These terms are then validated against sector descriptions using similarity metrics, creating interpretable explanations for classification decisions.
- Core assumption: The relevant terms extracted by LIME accurately represent the decision factors used by the classification model
- Evidence anchors:
  - [abstract]: "The explainability methodology is based on an agnostic evaluation of the influence of the input terms extracted from the descriptions of transactions using locally interpretable models."
  - [section III-C]: "The model-agnostic feature selector performs recursive feature selection tests using the LIME Python library given its wide acceptance in the literature [44]."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.459, average citations=0.0. Top related titles include "Automatic generation of insights from workers' actions in industrial workflows with explainable Machine Learning"
- Break condition: If LIME fails to detect representative terms for certain transactions, leading to "empty" explanations with no informative content

### Mechanism 2
- Claim: The combination of classification accuracy and explainability validation creates trustworthy carbon footprint estimates
- Mechanism: High classification accuracy (90% range) ensures reliable sector assignment, while explainability validation confirms that the terms driving classification align with human understanding of those sectors. This dual validation builds trust in the automated system.
- Core assumption: The similarity between explanation terms and sector descriptions correlates with classification correctness
- Evidence anchors:
  - [abstract]: "Over 70% of explanations are satisfactory to human operators, with 60% automatically validated."
  - [section IV-D]: "Satisfactory explanations exceeded 70%, of which approximately 60% could be automatically 'validated'."
  - [corpus]: Weak evidence - no direct citations in corpus addressing this specific dual validation approach
- Break condition: If high classification accuracy occurs with poor explainability validation, indicating potential overfitting to irrelevant features

### Mechanism 3
- Claim: Feature engineering through preprocessing and enterprise term selection enhances explainability in short text classification
- Mechanism: The preprocessing pipeline removes noise while reconstructing meaningful terms, and enterprise term selection supplements explanations with company-specific descriptors. This enrichment process compensates for the brevity of bank transaction descriptions.
- Core assumption: Enterprise descriptors retrieved from the internet accurately represent the activity sectors of companies mentioned in transactions
- Evidence anchors:
  - [section III-C]: "Sometimes, the descriptions of transactions include explicit references to particular enterprises...these descriptions were pre-processed using the same method described in Section III-A."
  - [section IV-B4]: "The system creates a list of words for each enterprise from the resulting list of lemmas...only the ten most frequent terms are retained."
  - [corpus]: Weak evidence - corpus focuses on carbon footprint of AI systems rather than NLP explainability techniques
- Break condition: If enterprise descriptions are unavailable or inaccurate, reducing the enrichment effectiveness and explanation quality

## Foundational Learning

- Concept: Short-text classification challenges
  - Why needed here: Bank transaction descriptions are typically brief (average 10 words) with limited context, making accurate classification difficult without proper feature engineering
  - Quick check question: What preprocessing steps are applied to handle the brevity and noise in bank transaction descriptions?

- Concept: Model-agnostic explainability techniques
  - Why needed here: Different classification models (SVM, RF, LSTM) have different internal structures, requiring a unified approach to extract interpretable explanations
  - Quick check question: Which Python library is used for the model-agnostic feature selection process?

- Concept: Similarity metrics for explanation validation
  - Why needed here: Automatically validating explanations requires comparing extracted terms against sector descriptions using quantitative measures to ensure consistency with human knowledge
  - Quick check question: Which similarity metric showed better performance than Jaccard similarity for this application?

## Architecture Onboarding

- Component map: Pre-processing → Feature Engineering → Classification Module → Explainability Module → Carbon Footprint Estimation
- Critical path: Bank transaction → Pre-processing → Classification → Explanation → Validation → CF Estimation
- Design tradeoffs: Model-agnostic explainability provides flexibility across different classifiers but adds computational overhead compared to model-specific approaches
- Failure signatures: High classification accuracy with low explanation validation rates suggests overfitting; empty explanations indicate LIME failure to identify relevant features
- First 3 experiments:
  1. Test classification accuracy with and without preprocessing to quantify feature engineering impact
  2. Compare Jaccard vs linguistic proximity metrics for explanation validation
  3. Evaluate explainability performance across different enterprise descriptor availability scenarios

## Open Questions the Paper Calls Out
None

## Limitations

- Generalizability concerns due to reliance on quality and comprehensiveness of sector descriptions in reference corpus
- Enterprise descriptor enrichment depends on publicly available information that may be incomplete or biased
- Classification accuracy varies across sectors without detailed error analysis or confidence intervals

## Confidence

- **High confidence**: Classification accuracy metrics (90% precision/recall) and basic explainability framework implementation
- **Medium confidence**: The explainability validation methodology and its effectiveness for human interpretation
- **Low confidence**: Generalizability to unseen sectors and long-term robustness of enterprise descriptor enrichment

## Next Checks

1. **Error Analysis Across Sectors**: Conduct detailed analysis of classification errors by sector to identify systematic weaknesses and evaluate whether explainability performance correlates with classification difficulty.

2. **Temporal Robustness Test**: Validate the system's performance when processing transactions spanning multiple years to assess how well it handles evolving company descriptions and emerging industries.

3. **Human-in-the-Loop Validation**: Implement a controlled study where human experts evaluate explanations for transactions from companies they're familiar with, measuring both accuracy and interpretability compared to the automatic validation approach.