---
ver: rpa2
title: 'CriticAL: Critic Automation with Language Models'
arxiv_id: '2411.06590'
source_url: https://arxiv.org/abs/2411.06590
tags:
- critical
- test
- statistics
- statistic
- criticism
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CriticAL automates model criticism by using LLMs to generate summary
  statistics that capture discrepancies between model predictions and data, then applies
  hypothesis tests to evaluate their significance. This approach addresses the challenge
  of automatically identifying meaningful model-data discrepancies without relying
  on human expertise.
---

# CriticAL: Critic Automation with Language Models

## Quick Facts
- arXiv ID: 2411.06590
- Source URL: https://arxiv.org/abs/2411.06590
- Authors: Michael Y. Li; Vivek Vajipey; Noah D. Goodman; Emily B. Fox
- Reference count: 40
- Key outcome: Automates model criticism by using LLMs to generate summary statistics and hypothesis tests that identify discrepancies between model predictions and data

## Executive Summary
CriticAL addresses the challenge of automatically identifying meaningful model-data discrepancies without human expertise. The system uses LLMs to generate executable Python functions as test statistics that capture discrepancies, then applies hypothesis tests to evaluate their significance. It also produces natural language critiques that explain the findings in an actionable way. In experiments, CriticAL reliably identifies true discrepancies without hallucinating false ones, outperforming naive LLM-based approaches. When integrated into an LLM-based model discovery system, CriticAL's critiques enable significant improvements over human-designed models on real-world datasets, achieving >80% win rates at various standard error thresholds.

## Method Summary
CriticAL operates through a three-step pipeline: first, an LLM generates test statistics as Python functions by conditioning on dataset metadata and a symbolic model representation; second, these test statistics are converted into empirical hypothesis tests using model samples to approximate null distributions and compute p-values; third, the LLM synthesizes the test statistic results into natural language criticism. The framework leverages posterior predictive checks and hypothesis testing to provide rigorous validation of LLM-generated critiques, while maintaining transparency through executable code and interpretability through natural language explanations.

## Key Results
- CriticAL reliably identifies true model-data discrepancies without hallucinating false ones, with false positive rates closely tracking significance thresholds
- Human and LLM judges consistently prefer CriticAL's critiques over alternatives for transparency, actionability, and tailoring
- When integrated into model discovery, CriticAL's critiques enable >80% win rates over human-designed models at various standard error thresholds

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can generate tailored summary statistics that capture domain-specific discrepancies between models and data.
- Mechanism: By conditioning the LLM on dataset metadata and a symbolic model representation, it proposes test statistics specifically designed to reveal discrepancies relevant to that particular model-data pair.
- Core assumption: The LLM has sufficient domain knowledge and statistical reasoning capability to propose meaningful test statistics when given appropriate context.
- Evidence anchors: [abstract], [section 3.1], [corpus]
- Break condition: The LLM lacks sufficient domain knowledge or statistical expertise to propose meaningful test statistics, or the conditioning context is inadequate.

### Mechanism 2
- Claim: Hypothesis testing framework provides rigorous validation of LLM-generated critiques.
- Mechanism: Test statistics generated by the LLM are converted into empirical hypothesis tests using model samples to approximate null distributions, enabling quantitative assessment of discrepancy significance.
- Core assumption: Model samples can be generated to approximate the null distribution of test statistics, and the significance threshold controls false positive rate.
- Evidence anchors: [abstract], [section 3.1], [section 4.2]
- Break condition: Model samples cannot adequately approximate the null distribution, or the hypothesis testing framework is inappropriate for the model type.

### Mechanism 3
- Claim: Natural language critiques bridge the gap between statistical findings and actionable model revisions.
- Mechanism: LLMs synthesize the results of test statistics and p-values into interpretable natural language explanations that guide human or LLM scientists in revising models.
- Core assumption: Natural language explanations can effectively communicate statistical findings to scientists in a way that informs model revision decisions.
- Evidence anchors: [abstract], [section 3.2], [section 4.4]
- Break condition: Natural language explanations are too vague or technical to be actionable, or they misrepresent the statistical findings.

## Foundational Learning

- Concept: Posterior predictive checks
  - Why needed here: CriticAL builds upon posterior predictive checks as a framework for model criticism, extending it with LLM-generated test statistics
  - Quick check question: How does a posterior predictive check differ from traditional goodness-of-fit tests?

- Concept: Hypothesis testing and p-values
  - Why needed here: The empirical p-values computed from test statistics determine which discrepancies are statistically significant
  - Quick check question: What does an empirical p-value of 0.05 mean in the context of CriticAL's test statistics?

- Concept: Sliced statistics
  - Why needed here: Sliced test statistics reveal discrepancies that aggregate statistics cannot detect, improving critique quality
  - Quick check question: How might slicing test statistics by input features reveal different patterns than aggregate statistics?

## Architecture Onboarding

- Component map: LLM test statistic proposer -> Test statistic executor -> Hypothesis test calculator -> Natural language critic -> Revision system integrator

- Critical path:
  1. Input: dataset metadata + symbolic model representation
  2. LLM generates test statistics (Python functions)
  3. Execute test statistics on model samples to get null distributions
  4. Compute empirical p-values comparing data to null distributions
  5. Apply multiple testing correction
  6. Generate natural language criticism for significant test statistics
  7. Output: test statistics + p-values + natural language critiques

- Design tradeoffs:
  - LLM temperature vs. consistency of test statistics
  - Number of test statistics proposed vs. computational cost
  - Significance threshold vs. false positive/negative rates
  - Granularity of slicing vs. interpretability

- Failure signatures:
  - High false positive rate despite significance threshold
  - Test statistics that don't execute or produce errors
  - Natural language critiques that are too vague or misrepresent findings
  - No significant test statistics despite known model-data discrepancies

- First 3 experiments:
  1. Run CriticAL on a simple linear regression with known heteroscedasticity to verify it detects the discrepancy
  2. Test CriticAL on a well-specified model to verify it doesn't hallucinate false discrepancies
  3. Compare CriticAL's sliced statistics vs. aggregate statistics on a dataset with subgroup differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CriticAL's performance scale with larger and more complex datasets, particularly those with high-dimensional features or non-linear relationships?
- Basis in paper: [inferred] The paper mentions that CriticAL was tested on real-world datasets from the Stan PosteriorDB, but does not explicitly discuss performance on high-dimensional or non-linear datasets.
- Why unresolved: The paper focuses on the effectiveness of CriticAL in identifying discrepancies and generating critiques, but does not address how the system handles scalability challenges.
- What evidence would resolve it: Empirical results showing CriticAL's performance metrics (e.g., true positive rate, false positive rate) on increasingly complex datasets with varying dimensions and non-linear relationships.

### Open Question 2
- Question: Can CriticAL be extended to handle non-Bayesian models, such as deep learning models or models with non-differentiable components?
- Basis in paper: [explicit] The paper states that "While our evaluation was limited to Bayesian models, which are commonly used in scientific domains, CriticALâ€™s design is versatile: the only requirements are the ability to sample data from the model and a symbolic representation of the model."
- Why unresolved: The paper does not provide evidence or examples of CriticAL's application to non-Bayesian models.
- What evidence would resolve it: Case studies or experiments demonstrating CriticAL's application to non-Bayesian models, such as deep learning models or models with non-differentiable components.

### Open Question 3
- Question: How does the choice of LLM (e.g., gpt-4-turbo-2024-04-09) impact CriticAL's performance, and can the system be adapted to use different LLMs?
- Basis in paper: [explicit] The paper mentions that "For our test statistic proposer, we use gpt-4-turbo-2024-04-09."
- Why unresolved: The paper does not explore the impact of using different LLMs on CriticAL's performance or discuss the system's adaptability to other LLMs.
- What evidence would resolve it: Comparative analysis of CriticAL's performance using different LLMs, including metrics such as accuracy, transparency, and actionability of the generated critiques.

### Open Question 4
- Question: How does CriticAL handle cases where the initial model is fundamentally flawed or incorrect, rather than just having minor discrepancies?
- Basis in paper: [inferred] The paper discusses CriticAL's ability to identify discrepancies between the model and data, but does not explicitly address how the system handles fundamentally flawed models.
- Why unresolved: The paper focuses on CriticAL's ability to identify and critique discrepancies, but does not explore how the system handles cases where the model is fundamentally incorrect.
- What evidence would resolve it: Case studies or experiments demonstrating CriticAL's performance when applied to fundamentally flawed models, including metrics such as the system's ability to identify the core issues and suggest appropriate revisions.

## Limitations

- The effectiveness of LLM-generated test statistics depends heavily on prompt engineering and may not generalize well across diverse scientific domains
- Computational cost scales with the number of test statistics and model samples required for accurate empirical p-values
- The framework assumes access to model samples, which may not be available for all types of models (e.g., non-probabilistic or black-box models)

## Confidence

- High confidence: The empirical p-value approach for controlling false positives is well-established and rigorously validated
- Medium confidence: The qualitative evaluation of critiques by human and LLM judges is somewhat subjective and may not fully capture scientific utility
- Medium confidence: The improvement in model discovery when using CriticAL critiques is demonstrated, but the generality across different discovery algorithms remains to be tested

## Next Checks

1. Test CriticAL on a broader range of scientific domains (e.g., physics, biology, economics) to assess cross-domain generalizability of the LLM-generated test statistics

2. Evaluate the computational efficiency and scalability by measuring execution time and resource usage as the number of test statistics and model samples increases

3. Conduct a controlled experiment where human scientists attempt to revise models using CriticAL critiques versus traditional diagnostic plots, measuring revision quality and time-to-insight