---
ver: rpa2
title: Adapting Pretrained ViTs with Convolution Injector for Visuo-Motor Control
arxiv_id: '2406.06072'
source_url: https://arxiv.org/abs/2406.06072
tags:
- coin
- control
- pretrained
- tasks
- vits
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CoIn enhances pretrained Vision Transformers for visuo-motor control
  by injecting convolutional inductive biases through a lightweight CNN encoder and
  cross-attention mechanism. This allows pretrained ViTs to leverage spatial locality
  and translation equivariance features, improving performance across 12 tasks in
  three domains (Adroit, MetaWorld, DMC) when finetuned with CoIn.
---

# Adapting Pretrained ViTs with Convolution Injector for Visuo-Motor Control

## Quick Facts
- arXiv ID: 2406.06072
- Source URL: https://arxiv.org/abs/2406.06072
- Reference count: 27
- Primary result: CoIn consistently outperforms standard finetuning, with CLIP + CoIn achieving an 11.3 point increase in mean success rate

## Executive Summary
Vision Transformers (ViTs) have shown strong visual representation capabilities but lack the inductive biases needed for effective visuo-motor control. This paper introduces Convolution Injector (CoIn), a lightweight add-on module that injects convolutional inductive biases into pretrained ViTs through a CNN encoder and cross-attention mechanism. CoIn enables pretrained ViTs to leverage spatial locality and translation equivariance features while maintaining their strong visual representations. The method is evaluated across 12 tasks in three domains (Adroit, MetaWorld, DMC), demonstrating consistent improvements over standard finetuning and achieving state-of-the-art results with only 3.6% additional parameters.

## Method Summary
CoIn is a lightweight add-on module designed to adapt pretrained Vision Transformers for visuo-motor control tasks. The method consists of a CNN encoder that extracts spatial locality and translation equivariance features from input images, and a cross-attention mechanism that integrates these features into the ViT's patch embeddings. The enriched patch embeddings are then processed by the ViT's transformer encoder blocks, and the output is used by a control policy network to predict actions. The approach is evaluated through behavior cloning using expert demonstrations across 12 visuo-motor control tasks, with finetuning performed using AdamW optimizer, weight decay 0.05, cosine learning rate schedule, and layer-wise learning rate decay.

## Key Results
- CoIn consistently outperforms standard finetuning across all 12 tasks in Adroit, MetaWorld, and DMC domains
- CLIP + CoIn achieves an 11.3 point increase in mean success rate compared to standard finetuning
- CoIn adds only 3.6% additional parameters to standard ViT-B/16 while outperforming larger ViT models
- The method demonstrates efficiency and scalability, with performance improvements observed across different model scales up to ViT-L/14

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** CoIn improves ViT performance on visuo-motor control by injecting convolutional inductive biases that ViTs lack.
- **Mechanism:** The CNN encoder extracts spatial locality and translation equivariance features, and the cross-attention module integrates these features into the ViT's patch embeddings, allowing the ViT to leverage both pretrained knowledge and newly obtained spatial prior features.
- **Core assumption:** Spatial locality and translation equivariance biases are beneficial for visuo-motor control tasks.
- **Evidence anchors:** Weak evidence from related papers focusing on different aspects of ViTs and inductive biases.

### Mechanism 2
- **Claim:** CoIn allows pretrained ViTs to leverage strong visual representations while providing beneficial control-centric inductive biases.
- **Mechanism:** CoIn eliminates the need to retrain pretrained ViTs from scratch with control-specific datasets, instead adding a lightweight module that injects control-centric inductive biases.
- **Core assumption:** Pretrained ViTs have strong visual representations that can be leveraged for visuo-motor control with added control-centric biases.
- **Evidence anchors:** Weak evidence from related papers not directly addressing the combination of pretrained ViTs with control-centric inductive biases.

### Mechanism 3
- **Claim:** CoIn improves ViTs' ability to capture high-frequency signals and learn translation equivariant features important for visuo-motor control.
- **Mechanism:** The CNN encoder helps ViTs capture high-frequency elements like texture and edges, while also enhancing translation equivariance through injected features.
- **Core assumption:** High-frequency signals and translation equivariant features are important for visuo-motor control.
- **Evidence anchors:** Weak evidence from related papers not directly addressing the importance of these features for visuo-motor control.

## Foundational Learning

- **Concept: Vision Transformers (ViTs)**
  - **Why needed here:** Understanding ViTs is crucial for grasping how CoIn enhances their performance on visuo-motor control tasks.
  - **Quick check question:** What are the main components of a ViT architecture, and how do they process input images?

- **Concept: Convolutional Inductive Biases**
  - **Why needed here:** CoIn injects convolutional inductive biases into ViTs to improve their performance on visuo-motor control tasks.
  - **Quick check question:** What are the two main types of convolutional inductive biases that CoIn injects into ViTs, and why are they beneficial for visuo-motor control?

- **Concept: Cross-Attention Mechanism**
  - **Why needed here:** The cross-attention mechanism integrates features from the CNN encoder into the ViT's patch embeddings.
  - **Quick check question:** How does the cross-attention mechanism in CoIn use the features from the CNN encoder and the ViT's patch embeddings to improve performance on visuo-motor control tasks?

## Architecture Onboarding

- **Component map:**
  Input image -> CNN encoder (stem + Conv1 + Conv2 + Conv3) -> Multi-scale feature pyramid (F1, F2, F3) -> Cross-attention module -> ViT patch embedding -> Transformer encoder blocks -> Control policy network

- **Critical path:**
  1. Input image is processed by the CNN encoder to extract multi-scale feature maps
  2. Multi-scale feature maps are flattened and merged into a single tensor
  3. Cross-attention module uses ViT's patch embeddings as queries and merged feature tensor as keys/values to integrate CNN features
  4. Enriched patch embeddings are processed by ViT's transformer encoder blocks
  5. Output of ViT is used by control policy network to predict actions

- **Design tradeoffs:**
  - Using lightweight CNN encoder to minimize computational burden while maintaining effectiveness
  - Multi-scale feature maps capture objects at different scales but add complexity
  - Cross-attention integration chosen for effective feature incorporation

- **Failure signatures:**
  - CNN encoder fails to extract spatial locality and translation equivariance features
  - Cross-attention mechanism does not effectively integrate CNN features into ViT
  - Control policy network fails to use enriched ViT output effectively

- **First 3 experiments:**
  1. Train ViT with CoIn on simple visuo-motor control task and compare to vanilla ViT
  2. Visualize cross-attention attention maps to verify effective CNN feature integration
  3. Analyze Fourier transformed feature maps to verify CoIn's improvement in capturing high-frequency signals

## Open Questions the Paper Calls Out
- **Open Question 1:** How does CoIn's performance scale when applied to even larger Vision Transformer models beyond ViT-L/14, such as those with 1B+ parameters?
- **Open Question 2:** Can CoIn be effectively adapted for reinforcement learning scenarios, and how would its performance compare to behavior cloning?
- **Open Question 3:** How does CoIn perform on real-world robotic hardware, considering potential challenges not present in simulated environments?

## Limitations
- Claims rely heavily on the assumption that injecting convolutional inductive biases will consistently improve visuo-motor control performance across diverse tasks
- Evidence for the mechanism is weak, as related papers focus on different aspects of ViTs and inductive biases
- Insufficient implementation details provided for the CoIn module's CNN encoder architecture and cross-attention mechanism

## Confidence
- **High Confidence:** Effectiveness of CoIn in improving visuo-motor control performance across 12 tasks in three domains
- **Medium Confidence:** Mechanism of CoIn improving ViT performance by injecting convolutional inductive biases
- **Low Confidence:** Importance of high-frequency signals and translation equivariant features for visuo-motor control, and CoIn's effectiveness in capturing these features

## Next Checks
1. **Verify CoIn Implementation:** Implement CoIn module with detailed CNN encoder architecture and cross-attention mechanism, train ViT with CoIn on simple visuo-motor control task to compare with vanilla ViT
2. **Analyze Attention Maps:** Visualize cross-attention attention maps to ensure effective CNN feature integration, analyze Fourier transformed feature maps to verify improvement in capturing high-frequency signals
3. **Evaluate Generalization:** Test CoIn's generalization across different visuo-motor control tasks and domains, evaluate performance on unseen tasks to assess robustness and adaptability