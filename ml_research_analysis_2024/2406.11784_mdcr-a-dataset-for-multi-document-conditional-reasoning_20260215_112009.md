---
ver: rpa2
title: 'MDCR: A Dataset for Multi-Document Conditional Reasoning'
arxiv_id: '2406.11784'
source_url: https://arxiv.org/abs/2406.11784
tags:
- conditions
- condition
- answer
- documents
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MDCR is a new dataset for multi-document conditional reasoning
  that evaluates models' ability to answer optimization questions by exploring condition
  combinations and relationships across documents. The dataset includes 1,138 scenarios
  with 3,414 questions covering scholarships and job domains, requiring models to
  identify maximum outcomes by reasoning about unmentioned conditions and their cross-document
  relationships (conflicting, equivalent, or inclusive).
---

# MDCR: A Dataset for Multi-Document Conditional Reasoning

## Quick Facts
- arXiv ID: 2406.11784
- Source URL: https://arxiv.org/abs/2406.11784
- Reference count: 22
- Multi-document conditional reasoning dataset with 1,138 scenarios and 3,414 questions across scholarship and job domains

## Executive Summary
MDCR is a novel dataset designed to evaluate models' ability to perform conditional reasoning across multiple documents. The dataset focuses on optimization questions where models must identify maximum outcomes by exploring combinations of conditions and their relationships across documents. The task requires reasoning about unmentioned conditions and understanding cross-document relationships (conflicting, equivalent, or inclusive). Evaluation reveals that recent LLMs achieve only 69% short answer accuracy and around 40% conditional answer F1, demonstrating significant challenges in this task. The dataset shows that while performance improves with hints about document conditions, satisfiability, and relationships, models still struggle with perfect reasoning even with complete information.

## Method Summary
The MDCR dataset was constructed to evaluate multi-document conditional reasoning capabilities by creating scenarios that require models to answer optimization questions across multiple documents. The dataset includes 1,138 scenarios with 3,414 questions covering scholarship and job domains. Each scenario requires reasoning about condition combinations and relationships across documents, including identifying maximum outcomes based on unmentioned conditions. The evaluation framework includes three types of hints: document conditions, condition satisfiability, and condition relationships, to systematically assess how additional information impacts model performance.

## Key Results
- Recent LLMs achieve only 69% short answer accuracy and around 40% conditional answer F1 on MDCR
- Performance improves with hints: 4.96% accuracy gain from document conditions, 3.02% from satisfiability, and 3.77% from relationship hints
- Even with all hints, models remain imperfect, highlighting limitations in conditional reasoning capabilities
- Dataset successfully identifies challenging reasoning tasks with significant gaps between model and human performance

## Why This Works (Mechanism)
MDCR works by creating realistic multi-document scenarios that require genuine conditional reasoning across documents. The mechanism leverages optimization questions where models must identify maximum outcomes by exploring combinations of conditions and their relationships. The dataset design forces models to reason about unmentioned conditions and understand cross-document relationships (conflicting, equivalent, or inclusive), which cannot be solved through simple information retrieval or pattern matching.

## Foundational Learning
- **Multi-document reasoning**: Understanding how to integrate information across multiple sources is essential because the task requires synthesizing conditions from different documents to find optimal solutions. Quick check: Can you identify all conditions across documents without missing any?
- **Conditional logic**: Models must understand if-then relationships and how different conditions interact to affect outcomes. Quick check: Can you determine which conditions must be satisfied simultaneously?
- **Optimization reasoning**: The task requires finding maximum outcomes by exploring condition combinations, not just retrieving existing information. Quick check: Can you identify the best possible outcome given all constraints?
- **Cross-document relationship identification**: Models need to recognize conflicting, equivalent, or inclusive relationships between conditions across documents. Quick check: Can you determine if two conditions from different documents can both be satisfied?
- **Unmentioned condition inference**: The dataset requires reasoning about conditions that are not explicitly stated but affect outcomes. Quick check: Can you infer implicit constraints from the given information?

## Architecture Onboarding

**Component Map:**
Document Loader -> Condition Extractor -> Relationship Analyzer -> Optimization Engine -> Answer Generator

**Critical Path:**
Document Loader -> Condition Extractor -> Relationship Analyzer -> Optimization Engine
The critical path focuses on accurately extracting conditions from documents, identifying relationships between conditions across documents, and determining optimal combinations that satisfy all requirements.

**Design Tradeoffs:**
The dataset prioritizes realistic conditional reasoning scenarios over simpler retrieval tasks, accepting lower baseline performance in exchange for more meaningful evaluation of reasoning capabilities. This tradeoff ensures the dataset identifies genuine reasoning limitations rather than superficial pattern matching abilities.

**Failure Signatures:**
Common failure patterns include missing unmentioned conditions, incorrectly identifying cross-document relationships, and failing to optimize across all possible condition combinations. Models often struggle with scenarios requiring simultaneous satisfaction of multiple conditions from different documents.

**First Experiments:**
1. Test baseline retrieval performance without any hints to establish the difficulty of pure information extraction
2. Evaluate performance with document condition hints only to assess if models can reason once all conditions are provided
3. Test with relationship hints to determine if models can optimize once they understand condition interactions

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding the fundamental limitations of current models in multi-document conditional reasoning. Key questions include whether the observed performance gaps stem from architectural limitations in transformer-based models or from insufficient training data for conditional reasoning tasks. The paper also questions whether the dataset's focus on scholarship and job domains limits generalizability to other conditional reasoning scenarios.

## Limitations
- Dataset focuses on only two specific domains (scholarships and jobs), potentially limiting generalizability to other conditional reasoning scenarios
- Evaluation relies heavily on automated metrics like BLEU and ROUGE, which may not fully capture the nuances of conditional reasoning quality
- Performance gaps between models and human performance suggest challenges, but specific contributing factors require further investigation

## Confidence

**High Confidence:**
- Dataset construction methodology and basic statistics (1,138 scenarios, 3,414 questions) are well-documented and verifiable

**Medium Confidence:**
- Claim that MDCR uniquely requires reasoning about unmentioned conditions and their cross-document relationships, as similar requirements may exist in other multi-document QA datasets
- Reported model performance numbers, as they depend on specific evaluation settings and model versions that may evolve

## Next Checks

1. Test MDCR's generalizability by applying the dataset to additional domains beyond scholarships and jobs to assess whether the observed reasoning challenges persist across different contexts

2. Conduct human evaluation studies to validate whether automated metrics (BLEU, ROUGE) accurately capture the quality of conditional reasoning, particularly for complex cross-document condition relationships

3. Analyze error patterns in model responses to determine whether failures stem from fundamental reasoning limitations or specific implementation issues in the tested models