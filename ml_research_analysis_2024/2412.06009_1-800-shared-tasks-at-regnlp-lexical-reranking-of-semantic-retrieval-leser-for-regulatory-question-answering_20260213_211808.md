---
ver: rpa2
title: '1-800-SHARED-TASKS at RegNLP: Lexical Reranking of Semantic Retrieval (LeSeR)
  for Regulatory Question Answering'
arxiv_id: '2412.06009'
source_url: https://arxiv.org/abs/2412.06009
tags:
- retrieval
- leser
- regulatory
- answer
- passages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents LeSeR, a hybrid approach combining dense semantic
  retrieval with lexical reranking for regulatory question answering. The method fine-tunes
  embedding models on regulatory text and integrates BM25-based reranking to improve
  retrieval performance.
---

# 1-800-SHARED-TASKS at RegNLP: Lexical Reranking of Semantic Retrieval (LeSeR) for Regulatory Question Answering

## Quick Facts
- arXiv ID: 2412.06009
- Source URL: https://arxiv.org/abs/2412.06009
- Reference count: 2
- Primary result: LeSeR achieved Recall@10 of 0.8201 and mAP@10 of 0.6655 on regulatory question answering

## Executive Summary
This paper presents LeSeR, a hybrid approach combining dense semantic retrieval with lexical reranking for regulatory question answering. The method fine-tunes embedding models on regulatory text and integrates BM25-based reranking to improve retrieval performance. LeSeR achieved Recall@10 of 0.8201 and mAP@10 of 0.6655 on the RIRAG challenge dataset. For answer generation, Qwen2.5 7B achieved the best performance with RePASs score of 0.4340. The approach demonstrates significant improvements over pure semantic or lexical retrieval methods in handling complex regulatory texts, though challenges remain in balancing recall and precision.

## Method Summary
LeSeR (Lexical-Semantic Retrieval) combines fine-tuned embedding models with BM25 reranking to retrieve regulatory information. The approach uses Multiple Negative Symmetric Ranking Loss (MNSR) to fine-tune embedding models on regulatory text pairs from the ObliQA dataset. The system retrieves top-20 passages using dense semantic embeddings, then reranks them with BM25 scores to produce top-10 results. Qwen2.5 7B is used for answer generation from the retrieved passages, evaluated using the RePASs metric.

## Key Results
- LeSeR achieved Recall@10 of 0.8201 and mAP@10 of 0.6655 on the regulatory question answering task
- Qwen2.5 7B achieved the highest RePASs score of 0.4340 for answer generation
- Fine-tuned embedding models with MNSR loss significantly improved retrieval performance compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining dense semantic embeddings with BM25-based reranking (LeSeR) improves both recall and precision compared to using either method alone.
- Mechanism: The dense embedding model retrieves a broad set of candidate passages using semantic similarity, while BM25 reranking prioritizes passages with exact term matches and higher lexical relevance, balancing semantic breadth with lexical precision.
- Core assumption: Regulatory texts contain both domain-specific terminology requiring exact matching and semantic concepts requiring contextual understanding.
- Evidence anchors:
  - [abstract]: "LeSeR, which achieved competitive results with a recall@10 of 0.8201 and map@10 of 0.6655 for retrievals"
  - [section]: "We propose LeSeR (Lexical-Semantic Retrieval), a novel take on hybrid retrieval that uniquely decouples these phases. Semantic embeddings retrieve high-recall candidates, which are then reranked lexically for precision"
  - [corpus]: Weak evidence - corpus shows related work on hybrid retrieval but no direct evidence for this specific LeSeR mechanism

### Mechanism 2
- Claim: Fine-tuning embedding models with Multiple Negative Symmetric Ranking Loss (MNSR) on regulatory text pairs significantly improves recall performance.
- Mechanism: MNSR treats all in-batch examples as potential negatives, creating more effective contrastive learning signals. This improves the embedding model's ability to distinguish relevant regulatory passages from irrelevant ones.
- Core assumption: Regulatory text pairs have clear semantic relationships that can be learned through contrastive training.
- Evidence anchors:
  - [abstract]: "The embedding model was fine-tuned on a dataset derived from ObliQA for a maximum of 10 epochs"
  - [section]: "We used Multiple Negative Symmetric Ranking Loss (MNSR) for contrastive learning, which treats every in-batch example as a potential negative example for all other queries"
  - [corpus]: Weak evidence - corpus shows related work on fine-tuning but no direct evidence for MNSR effectiveness in regulatory domains

### Mechanism 3
- Claim: Using Qwen2.5 7B with BGE_LeSeR retrieval produces the best answer quality as measured by RePASs score.
- Mechanism: Qwen2.5 7B effectively synthesizes information from the top-10 retrieved passages, balancing entailment and avoiding contradictions while covering regulatory obligations.
- Core assumption: The retrieved passages contain sufficient relevant information for the model to generate comprehensive answers.
- Evidence anchors:
  - [abstract]: "For answer generation, Qwen2.5 7B achieved the best performance with RePASs score of 0.4340"
  - [section]: "Among the models tested, Qwen2.5 7B outperformed the others across all metrics, achieving the highest score for Entailment (0.5730), second lowest Contradiction score (0.3480), highest Obligation Coverage (0.0772), and highest RePASs (0.4340)"
  - [corpus]: No direct evidence in corpus about Qwen2.5 performance with this specific retrieval setup

## Foundational Learning

- Concept: Dense vs. Sparse Retrieval
  - Why needed here: Understanding the difference between semantic (dense) and keyword-based (sparse) retrieval is crucial for implementing LeSeR
  - Quick check question: What is the primary advantage of dense retrieval over sparse retrieval in regulatory text applications?

- Concept: Contrastive Learning with MNSR
  - Why needed here: MNSR is the fine-tuning approach used for embedding models in this system
  - Quick check question: How does MNSR differ from standard contrastive loss in handling negative samples?

- Concept: Hybrid Retrieval Systems
  - Why needed here: LeSeR represents a specific type of hybrid system with decoupled semantic and lexical phases
  - Quick check question: What is the key architectural difference between LeSeR and typical hybrid retrieval approaches?

## Architecture Onboarding

- Component map: Embedding model (BGE_MNSR) → Dense retrieval (FAISS) → Top-20 candidates → BM25 reranking → Top-10 passages → LLM (Qwen2.5 7B) → Answer generation
- Critical path: Query → Embedding → Dense retrieval → BM25 reranking → Answer generation
- Design tradeoffs: LeSeR prioritizes recall through dense retrieval then precision through lexical reranking, trading computational cost for improved performance; using smaller models (7B) balances inference speed with quality
- Failure signatures: Low recall@10 indicates embedding model or fine-tuning issues; low mAP@10 suggests reranking problems; poor RePASs scores indicate answer generation or context quality issues
- First 3 experiments:
  1. Test baseline BM25 retrieval performance on a small regulatory dataset
  2. Implement dense retrieval only with fine-tuned BGE model and measure recall@10
  3. Combine dense retrieval with BM25 reranking and compare metrics against individual approaches

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the LeSeR approach perform on datasets outside of regulatory domains, particularly in other specialized fields with complex terminology?
- Basis in paper: [explicit] The paper focuses on regulatory domains but mentions the potential for broader applications
- Why unresolved: The current evaluation is limited to the regulatory domain dataset, leaving performance in other specialized domains untested
- What evidence would resolve it: Testing LeSeR on diverse specialized datasets (medical, legal, technical) with similar evaluation metrics

### Open Question 2
- Question: What is the optimal balance between semantic and lexical scores in the weighted aggregation approach, and does this balance vary by domain or query type?
- Basis in paper: [inferred] The paper uses a weighted aggregation approach but doesn't explore optimal weighting or its variability across different contexts
- Why unresolved: The paper uses a fixed weighting scheme without investigating how different weight combinations affect performance
- What evidence would resolve it: Systematic experiments varying the semantic-lexical weight ratio across different query types and domains

### Open Question 3
- Question: How does the performance of LeSeR change when using different passage selection strategies beyond the top-10 or top-20 approaches mentioned in the paper?
- Basis in paper: [explicit] The paper mentions using top-10 results for answer generation but discusses limitations of using top-10 or top-20 retrievals
- Why unresolved: The paper doesn't explore alternative passage selection strategies or their impact on retrieval and answer quality
- What evidence would resolve it: Comparative experiments using different passage selection thresholds and strategies (dynamic selection, relevance scoring, etc.)

## Limitations
- The evaluation was conducted on a specific regulatory corpus from ADGM regulations, which may not represent the full diversity of regulatory domains
- The performance metrics show a notable gap between recall (0.8201) and precision (0.6655 mAP@10), indicating challenges in distinguishing the most relevant passages
- The computational cost of the hybrid approach, particularly the fine-tuning requirements and inference with both dense and lexical components, may limit scalability for real-time applications

## Confidence
- **High confidence**: The effectiveness of combining dense semantic retrieval with BM25 reranking for improving overall retrieval performance (Recall@10 = 0.8201, mAP@10 = 0.6655)
- **Medium confidence**: The superiority of Qwen2.5 7B for answer generation (RePASs = 0.4340) based on single dataset evaluation
- **Low confidence**: The generalizability of the MNSR fine-tuning approach to other regulatory domains without additional adaptation

## Next Checks
1. **Cross-domain validation**: Test LeSeR on regulatory corpora from different jurisdictions and domains (e.g., financial, healthcare, environmental regulations) to assess generalizability beyond ADGM regulations.

2. **Ablation study**: Systematically remove components (semantic retrieval, lexical reranking, fine-tuning) to quantify their individual contributions to the 0.8201 Recall@10 and 0.6655 mAP@10 performance.

3. **Scalability assessment**: Measure inference time and memory requirements for the hybrid approach on larger regulatory datasets to evaluate practical deployment constraints.