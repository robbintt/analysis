---
ver: rpa2
title: 'PARADISE: Evaluating Implicit Planning Skills of Language Models with Procedural
  Warnings and Tips Dataset'
arxiv_id: '2403.03167'
source_url: https://arxiv.org/abs/2403.03167
tags:
- inference
- language
- warning
- tasks
- goal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces PARADISE, a dataset for evaluating language
  models'' implicit planning abilities using warnings and tips from wikiHow procedural
  texts. The dataset consists of over 104K examples in two tasks: warning inference
  and tip inference.'
---

# PARADISE: Evaluating Implicit Planning Skills of Language Models with Procedural Warnings and Tips Dataset

## Quick Facts
- arXiv ID: 2403.03167
- Source URL: https://arxiv.org/abs/2403.03167
- Reference count: 40
- Primary result: Fine-tuned small models outperform zero-shot LLMs on warning/tip inference tasks, but all models fall short of human performance

## Executive Summary
This paper introduces PARADISE, a dataset for evaluating language models' implicit planning abilities through warning and tip inference tasks derived from wikiHow procedural texts. The dataset contains over 104K examples across two tasks: inferring implicit warnings and tips from procedural goals. The authors compare fine-tuned BERT variants against zero-shot prompting with various large language models including GPT-4. Results show that task-specific fine-tuning of smaller models generally outperforms zero-shot approaches with LLMs, though all models perform significantly below human baselines of 94% (warnings) and 96% (tips).

## Method Summary
The PARADISE dataset was constructed by extracting goals, steps, warnings, and tips from wikiHow articles. For the warning inference task, models must select the correct warning given only the goal and candidate warnings. The tip inference task follows a similar format. The authors fine-tuned several BERT variants (DistilBERT, BERT, RoBERTa, DeBERTa) using a question-answering format with [CLS] goal [SEP] candidate input, then evaluated using standard accuracy metrics. For zero-shot evaluation, they prompted LLMs (GPT-4, PALM-2, LLaMA-2, Mistral, Vicuna) with provided templates and sampling parameters. The study also examined knowledge transfer by evaluating whether fine-tuning on warning/tip inference improves performance on other procedural tasks.

## Key Results
- Fine-tuned BERT variants achieve 77-85% accuracy on warning inference and 77-84% on tip inference, outperforming zero-shot LLMs including GPT-4
- All models fall significantly short of human performance baselines (94% for warnings, 96% for tips)
- BERT-family models struggle more with physical goals while GPT-4 has difficulty with abstract, digital, and social objectives
- Fine-tuning on warning/tip inference tasks provides beneficial prior knowledge for other procedural tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Implicit planning is better captured through warning/tip inference than explicit step generation.
- Mechanism: By removing intermediary steps and requiring models to infer implicit knowledge directly from goals, the task forces deeper reasoning rather than pattern matching on procedural sequences.
- Core assumption: Implicit knowledge is distinct from explicit step sequences and requires abductive reasoning.
- Evidence anchors:
  - [abstract] "involves warning and tip inference tasks directly associated with goals, excluding intermediary steps"
  - [section] "focus on the implicit relationship between goals and warnings/tips, bypassing intermediate steps (instructions)"
- Break condition: If models can solve the task by simple keyword matching or surface-level semantic similarity without understanding the underlying plan.

### Mechanism 2
- Claim: Task-specific fine-tuning of smaller models outperforms zero-shot prompting of large LLMs.
- Mechanism: Fine-tuned models learn task-specific patterns and reasoning structures that general-purpose LLMs don't develop through pre-training alone.
- Core assumption: Task-specific training provides more relevant inductive biases than broad pre-training.
- Evidence anchors:
  - [abstract] "Our experiments...reveal the effectiveness of task-specific small models over large language models in most scenarios"
  - [section] "fine-tuning small models tailored to specific tasks proves more effective than zero-shot prompting across all LLMs"
- Break condition: If LLMs with sufficient reasoning capabilities could perform well without task-specific adaptation.

### Mechanism 3
- Claim: Cross-domain transfer from warning/tip inference improves performance on other procedural tasks.
- Mechanism: Warning and tip inference captures general reasoning patterns about procedural knowledge that transfer to related tasks like goal-step inference.
- Core assumption: Warning/tip inference develops transferable reasoning skills rather than task-specific memorization.
- Evidence anchors:
  - [abstract] "the proposed tasks offering valuable prior knowledge for other unseen procedural tasks"
  - [section] "prior fine-tuning on warning and tip inference tasks consistently improves performance during training for both goal and step inference tasks"
- Break condition: If improvements are due to model capacity increase rather than learned reasoning patterns.

## Foundational Learning

- Concept: Abductive reasoning
  - Why needed here: The task requires inferring implicit causes/warnings from goals without explicit procedural steps
  - Quick check question: What distinguishes abductive reasoning from deductive reasoning in the context of procedural planning?

- Concept: Semantic similarity and embedding spaces
  - Why needed here: Candidate sampling and evaluation rely on measuring semantic similarity between goals and warnings/tips
  - Quick check question: How does cosine similarity in embedding space relate to semantic relevance in this task?

- Concept: Fine-tuning vs zero-shot learning
  - Why needed here: The paper compares task-specific fine-tuning with zero-shot prompting approaches
  - Quick check question: What are the key differences in model behavior between fine-tuned and zero-shot approaches for procedural reasoning?

## Architecture Onboarding

- Component map: Fine-tuned PLM setup (BERT variants) with additional projection layer → softmax classification; Zero-shot LLM setup with prompt templates and sampling parameters
- Critical path: Input encoding → candidate scoring → selection of highest probability warning/tip
- Design tradeoffs: Small fine-tuned models vs large zero-shot LLMs - specialization vs generalization; manual prompt engineering vs automated learning
- Failure signatures: High accuracy drop when dropping keywords (PLMs); category-specific failures (DeBERTa vs GPT-4); poor performance on reverse inference task
- First 3 experiments:
  1. Run baseline random and majority baselines on test set
  2. Fine-tune BERT variant on training set and evaluate on validation set
  3. Test zero-shot prompting with GPT-4 using provided templates on validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific types of procedural reasoning tasks would benefit most from the implicit knowledge captured by warning and tip inference?
- Basis in paper: [explicit] The paper mentions that the proposed tasks offer valuable prior knowledge for other unseen procedural tasks.
- Why unresolved: While the paper demonstrates knowledge transfer to some procedural tasks, it doesn't provide a comprehensive analysis of which types of procedural reasoning tasks would benefit most from this implicit knowledge.
- What evidence would resolve it: A systematic study evaluating the performance of models fine-tuned on warning and tip inference tasks across a diverse set of procedural reasoning tasks, analyzing which task types show the most improvement.

### Open Question 2
- Question: How does the performance of fine-tuned models on warning and tip inference tasks generalize to real-world applications beyond the wikiHow domain?
- Basis in paper: [inferred] The paper evaluates models on the wikiHow dataset and demonstrates knowledge transfer to other procedural tasks, but doesn't explicitly test real-world applications.
- Why unresolved: The paper focuses on the wikiHow dataset and doesn't provide evidence of how well the models perform on real-world procedural reasoning tasks outside of this domain.
- What evidence would resolve it: Evaluating the fine-tuned models on a diverse set of real-world procedural reasoning tasks, such as medical procedures, cooking recipes, or technical manuals, to assess their generalization capabilities.

### Open Question 3
- Question: What are the key linguistic features or patterns in warnings and tips that make them challenging for language models to infer?
- Basis in paper: [explicit] The paper discusses keyword manipulation experiments and analyzes the failures of different model families, but doesn't provide a detailed linguistic analysis of the challenging aspects of warnings and tips.
- Why unresolved: While the paper identifies some patterns in model failures, it doesn't provide a comprehensive linguistic analysis of the specific features or patterns in warnings and tips that make them difficult for language models to infer.
- What evidence would resolve it: A detailed linguistic analysis of warnings and tips, identifying the specific linguistic features or patterns that contribute to their complexity and pose challenges for language models. This could involve analyzing syntactic structures, semantic relationships, or pragmatic aspects of warnings and tips.

## Limitations
- The evaluation methodology lacks detailed human evaluation process and inter-rater reliability analysis
- Comparison between fine-tuned models and zero-shot LLMs may be confounded by multiple factors beyond reasoning capabilities
- Dataset construction from a single source (wikiHow) may introduce domain-specific biases limiting generalization

## Confidence

**High Confidence**: The finding that human performance significantly exceeds all model performances (94-96% vs 50-85% for best models) is well-supported by the experimental results. The observation that BERT-family models struggle more with physical goals while GPT-4 has difficulty with abstract, digital, and social objectives is also robustly demonstrated through the analysis.

**Medium Confidence**: The claim that fine-tuned small models outperform zero-shot LLMs requires more careful interpretation. While statistically supported, the comparison conflates model architecture, training approach, and evaluation methodology. The conclusion that this demonstrates the effectiveness of task-specific fine-tuning over general-purpose LLMs is reasonable but not definitively proven.

**Low Confidence**: The assertion that the proposed tasks "provide beneficial prior knowledge for other procedural tasks" is based on limited evidence. The transfer experiments show correlation but do not establish causation or rule out alternative explanations such as model capacity effects.

## Next Checks
1. Conduct inter-rater reliability analysis and multiple human evaluations to establish confidence intervals around the 94% and 96% baselines
2. Design an experiment where a large LLM is fine-tuned on the same task and compared directly with small fine-tuned models of comparable final performance
3. Evaluate models on a held-out subset of procedural tasks from different domains (medical, technical, educational) to assess generalization beyond wikiHow-style instructions