---
ver: rpa2
title: 'MISS: A Generative Pretraining and Finetuning Approach for Med-VQA'
arxiv_id: '2401.05163'
source_url: https://arxiv.org/abs/2401.05163
tags:
- medical
- image
- encoder
- images
- image-text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of Medical Visual Question Answering
  (Med-VQA), where most methods treat the task as answer classification, limiting
  their applicability in real-world scenarios. The proposed method, MISS, treats Med-VQA
  as a generative task, enabling direct application in practical settings.
---

# MISS: A Generative Pretraining and Finetuning Approach for Med-VQA

## Quick Facts
- arXiv ID: 2401.05163
- Source URL: https://arxiv.org/abs/2401.05163
- Reference count: 31
- Primary result: Generative Med-VQA model achieving superior performance with fewer multimodal datasets through Joint Text-Multimodal encoder and Transfer-and-Caption method

## Executive Summary
MISS addresses the limitations of classification-based approaches in Medical Visual Question Answering by treating Med-VQA as a generative task. The method introduces a Joint Text-Multimodal (JTM) encoder that unifies text and multimodal representations through multi-task learning. Additionally, the Transfer-and-Caption (TransCap) technique leverages Large Language Models to generate captions for single-modal medical image datasets, enabling Vision-Language Pre-training with limited multimodal data. Experimental results demonstrate that MISS achieves excellent performance while requiring fewer specialized multimodal datasets, making it more practical for real-world medical applications.

## Method Summary
MISS proposes a generative approach to Medical Visual Question Answering by introducing the Joint Text-Multimodal (JTM) encoder architecture. This encoder unifies text and multimodal representations through multi-task learning, allowing the model to learn comprehensive feature representations from both modalities simultaneously. The method also introduces Transfer-and-Caption (TransCap), which uses Large Language Models to generate descriptive captions for single-modal medical image datasets, effectively extending the feature space of these datasets for Vision-Language Pre-training. This approach enables the model to leverage abundant single-modal medical image data while requiring fewer specialized multimodal datasets, making the training process more practical and scalable.

## Key Results
- Achieves state-of-the-art performance on Med-VQA benchmarks while using fewer multimodal datasets
- Demonstrates effectiveness of generative VQA models in medical domain applications
- Shows successful transfer learning from single-modal medical image datasets through LLM-generated captions

## Why This Works (Mechanism)
The generative approach allows direct application in real-world scenarios where answer spaces are open-ended and diverse. By treating Med-VQA as a generation task rather than classification, the model can produce varied, context-appropriate responses rather than selecting from predefined answer categories. The JTM encoder's unified architecture enables better cross-modal feature learning, capturing complex relationships between visual medical information and textual queries. The TransCap method effectively expands the training data universe by transforming abundant single-modal medical images into multimodal training examples through LLM-generated captions, addressing the scarcity of high-quality medical multimodal datasets.

## Foundational Learning
- **Vision-Language Pre-training (VLP)**: Required for learning cross-modal representations between medical images and text; quick check: verify pre-training datasets include diverse medical imaging modalities
- **Large Language Models for Caption Generation**: Essential for TransCap method to create synthetic multimodal data; quick check: evaluate caption quality through medical expert review
- **Multi-task Learning for Joint Representations**: Needed to unify text and multimodal encoders effectively; quick check: compare performance against separate encoder architectures
- **Generative vs. Classification Approaches**: Understanding the trade-offs between open-ended generation and constrained classification; quick check: measure response diversity and accuracy

## Architecture Onboarding

**Component Map:**
Image Encoder -> JTM Encoder -> Text Encoder -> Generation Head

**Critical Path:**
The critical path flows from the image encoder through the JTM encoder, which jointly processes visual features and textual queries, before passing through the text encoder and generation head to produce answers. The JTM encoder is the central innovation that enables effective cross-modal learning.

**Design Tradeoffs:**
- Generative output provides flexibility but may sacrifice precision compared to classification
- LLM-generated captions expand training data but introduce potential quality variability
- Unified JTM encoder simplifies architecture but may limit specialized optimization for each modality

**Failure Signatures:**
- Hallucinations or irrelevant answers from generation head
- Over-reliance on textual information at expense of visual features
- Degradation in performance when input domain shifts from training distribution

**First Experiments:**
1. Compare JTM encoder performance against separate text and multimodal encoders
2. Evaluate caption quality impact by varying LLM model size and prompting strategies
3. Test generation diversity by measuring answer variation across multiple runs with identical inputs

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Reliance on LLM-generated captions introduces potential quality and bias concerns across medical imaging domains
- Effectiveness of JTM encoder for handling diverse medical imaging modalities (X-ray, CT, MRI) requires more extensive validation
- Potential domain shift issues when applying models trained on general medical images to specific clinical contexts

## Confidence

**High confidence**: Architectural innovation of JTM encoder and theoretical justification for unifying text and multimodal representations

**Medium confidence**: Claims about performance improvements with fewer datasets, dependent on quality of LLM-generated captions

**Medium confidence**: Generalizability of results across different medical imaging modalities and clinical settings

## Next Checks
1. Conduct ablation studies to quantify impact of LLM-generated captions on model performance across different medical imaging types
2. Test model robustness on out-of-distribution medical images to assess real-world applicability
3. Compare inference speed and computational requirements against traditional classification-based Med-VQA approaches to evaluate practical deployment feasibility