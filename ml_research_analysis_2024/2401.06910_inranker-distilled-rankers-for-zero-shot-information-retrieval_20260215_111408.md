---
ver: rpa2
title: 'InRanker: Distilled Rankers for Zero-shot Information Retrieval'
arxiv_id: '2401.06910'
source_url: https://arxiv.org/abs/2401.06910
tags:
- effectiveness
- soft
- synthetic
- training
- distillation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces InRanker, a distilled version of the monoT5-3B
  model designed for zero-shot information retrieval tasks. The core method involves
  a two-phase distillation pipeline: first, training on soft labels from a larger
  teacher model using human-annotated data, and second, training on soft labels for
  synthetic queries generated by a language model.'
---

# InRanker: Distilled Rankers for Zero-shot Information Retrieval

## Quick Facts
- arXiv ID: 2401.06910
- Source URL: https://arxiv.org/abs/2401.06910
- Authors: Thiago Laitz; Konstantinos Papakostas; Roberto Lotufo; Rodrigo Nogueira
- Reference count: 40
- Key outcome: Distilled rankers achieve up to 50x size reduction while maintaining or improving zero-shot retrieval effectiveness on BEIR benchmark

## Executive Summary
This paper introduces InRanker, a knowledge distillation approach for creating smaller, efficient rankers that maintain effectiveness for zero-shot information retrieval tasks. The method employs a two-phase distillation pipeline that first trains on soft labels from a large teacher model using human-annotated data, then fine-tunes on synthetic queries generated by language models paired with teacher logits. The approach significantly improves the effectiveness of smaller models on out-of-domain datasets, with InRanker-60M and InRanker-220M achieving substantial improvements in nDCG@10 scores on the BEIR benchmark while being 50x and 13x smaller than the teacher model, respectively.

## Method Summary
InRanker uses a two-phase distillation pipeline to create efficient rankers for zero-shot information retrieval. First, smaller models are trained using soft labels from a monoT5-3B teacher model on human-annotated data from MS MARCO. Second, the models are further fine-tuned using synthetic queries generated by a language model for passages in target domains, paired with teacher logits. The training uses Mean Squared Error (MSE) loss to match the teacher and student logits, with zero-mean normalization applied to the teacher logits for each query-document pair. This approach enables the creation of models that are 50x smaller than the teacher while maintaining or improving effectiveness on out-of-domain retrieval tasks.

## Key Results
- InRanker-60M and InRanker-220M achieve substantial improvements in nDCG@10 scores on BEIR benchmark
- Models show 50x and 13x size reduction compared to teacher model while maintaining effectiveness
- Two-phase distillation (human soft labels + synthetic queries) outperforms single-phase approaches
- Out-of-domain datasets show improvements, suggesting enhanced generalization capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distilling with soft labels instead of hard labels improves zero-shot retrieval performance by preserving teacher model's nuanced relevance signals.
- Mechanism: The student model learns to mimic the teacher's probability distribution over relevant vs. non-relevant passages, capturing fine-grained relevance judgments rather than binary decisions.
- Core assumption: The teacher model's soft relevance scores contain information about relative relevance quality that is useful for zero-shot transfer.
- Evidence anchors: [abstract] "training on existing supervised soft teacher labels"; [section] "Distilling rerankers involves using the Mean Squared Error (MSE) loss to match the logits of the teacher and the student"; [corpus] "average neighbor FMR=0.395"

### Mechanism 2
- Claim: Synthetic query generation via language models enables domain adaptation without human annotation, improving zero-shot performance.
- Mechanism: The model is trained on synthetic queries generated for passages in the target domain, paired with teacher logits, effectively converting any corpus to an "in-domain" training set.
- Core assumption: Language models can generate queries that reasonably represent how humans would query about specific passages.
- Evidence anchors: [abstract] "using language models and rerankers to generate as much as possible synthetic 'in-domain' training data"; [section] "using synthetic queries generated by an LLM based on randomly sampled documents from the corpus"

### Mechanism 3
- Claim: Two-phase distillation (human soft labels followed by synthetic soft labels) is more effective than single-phase approaches.
- Mechanism: The first phase builds general ranking capability using human-curated data, while the second phase specializes the model for zero-shot effectiveness on specific domains.
- Core assumption: The knowledge from human-curated data provides a better foundation for learning domain-specific patterns than synthetic data alone.
- Evidence anchors: [section] "both distillation steps were essential for improving the average nDCG@10 score compared to the model trained solely using human hard labels"; [section] "we observe that the out-of-domain datasets also had improvements, suggesting that the model's generalization capabilities were enhanced"

## Foundational Learning

- Concept: Knowledge distillation
  - Why needed here: The core technique for transferring knowledge from a large teacher model to smaller student models while maintaining effectiveness.
  - Quick check question: What is the difference between training with hard labels and training with soft labels from a teacher model?

- Concept: Zero-shot learning
  - Why needed here: The goal is to create models that perform well on datasets they weren't explicitly trained on, without requiring additional human annotations.
  - Quick check question: How does zero-shot learning differ from few-shot learning in terms of data requirements?

- Concept: Cross-entropy vs. MSE loss for distillation
  - Why needed here: Understanding why MSE loss is used for matching teacher logits rather than classification loss.
  - Quick check question: Why would you use MSE loss instead of cross-entropy when distilling from teacher logits?

## Architecture Onboarding

- Component map: Query → Teacher model (monoT5-3B) → Soft logits → Student model training → Zero-shot evaluation
- Critical path: Query → Teacher model → Soft logits → Student model training → Zero-shot evaluation
- Design tradeoffs:
  - Model size vs. inference efficiency: Smaller models are more deployable but may lose some effectiveness
  - Synthetic vs. human data: Synthetic data is scalable but may be less representative
  - Single vs. multi-phase distillation: More complex but potentially more effective
- Failure signatures:
  - If student models don't improve over baselines: Teacher logits may be poor quality or student capacity too limited
  - If zero-shot performance lags behind in-domain: Synthetic queries may not capture target domain characteristics
  - If training is unstable: Learning rate or batch size may need adjustment
- First 3 experiments:
  1. Train student model using only human soft labels from MS MARCO and measure baseline effectiveness
  2. Add second distillation phase with synthetic queries for one BEIR dataset and compare in-domain vs. out-of-domain performance
  3. Compare MSE vs. KL divergence loss functions to identify optimal distillation objective

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of InRanker compare when using different teacher models with varying sizes and capabilities?
- Basis in paper: [explicit] The paper mentions that the methodology is model-agnostic and one could use a stronger teacher model to anticipate even stronger results.
- Why unresolved: The experiments only used monoT5-3B as the teacher model, so there is no direct comparison with other teacher models.
- What evidence would resolve it: Experiments using different teacher models (e.g., Promptagator, RankT5-3B) to train InRanker and comparing the effectiveness on the BEIR benchmark.

### Open Question 2
- Question: What is the impact of using real queries from BEIR instead of synthetic queries for the second distillation phase?
- Basis in paper: [explicit] The paper mentions that real queries from BEIR achieved a better out-of-domain effectiveness compared to the model trained solely on synthetic ones.
- Why unresolved: The paper only provides a brief comparison of using real vs. synthetic queries for a small number of models and datasets.
- What evidence would resolve it: A comprehensive comparison of using real vs. synthetic queries for all model sizes and datasets in the BEIR benchmark.

### Open Question 3
- Question: How does the zero-mean normalization of teacher logits affect the effectiveness of the distilled model?
- Basis in paper: [explicit] The paper mentions that zero-mean normalization is used to make the data distribution symmetric for each query-document pair, but does not provide an ablation study on its impact.
- Why unresolved: The paper does not compare the effectiveness of models trained with and without zero-mean normalization.
- What evidence would resolve it: An ablation study comparing the effectiveness of models trained with and without zero-mean normalization on the BEIR benchmark.

## Limitations

- Synthetic query quality uncertainty: The paper claims synthetic queries improve zero-shot performance but doesn't quantitatively evaluate synthetic query quality compared to human queries
- Teacher model dependency: The entire approach relies on the quality of the monoT5-3B teacher model's soft labels, which could propagate systematic biases
- Limited generalization evidence: While improvements are shown on BEIR benchmark datasets, the zero-shot capabilities across truly diverse domains remain untested

## Confidence

- Knowledge Distillation Improves Zero-Shot Performance (High confidence): Empirical results showing nDCG@10 improvements across multiple BEIR datasets provide strong evidence
- Two-Phase Distillation is Essential (Medium confidence): While combining human and synthetic data phases improves performance, ablation studies don't fully isolate whether the second phase adds unique value beyond providing more training data
- 50x Size Reduction Maintains Effectiveness (Medium confidence): The claim is supported by BEIR results, but the performance degradation curve as models get smaller isn't fully characterized

## Next Checks

1. **Synthetic Query Quality Analysis**: Conduct a human evaluation study comparing synthetic queries to actual user queries from the target datasets. Measure query relevance, diversity, and semantic alignment to quantify how well the synthetic queries represent real query distributions.

2. **Teacher Model Validation**: Evaluate the teacher model's soft label quality independently by comparing its rankings to human judgments on held-out data. Test whether the soft scores correlate with human relevance judgments and whether they provide meaningful ranking signals beyond binary relevance.

3. **Cross-Domain Generalization Test**: Apply the distilled models to information retrieval tasks outside the BEIR benchmark, such as medical literature search, e-commerce product search, or patent retrieval. Measure performance degradation across diverse domain types to validate true zero-shot capabilities.