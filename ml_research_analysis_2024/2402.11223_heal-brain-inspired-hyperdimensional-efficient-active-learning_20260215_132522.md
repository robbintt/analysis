---
ver: rpa2
title: 'HEAL: Brain-inspired Hyperdimensional Efficient Active Learning'
arxiv_id: '2402.11223'
source_url: https://arxiv.org/abs/2402.11223
tags:
- learning
- data
- acquisition
- heal
- hypervectors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HEAL, the first Active Learning (AL) framework
  designed specifically for Hyperdimensional Computing (HDC)-based classification.
  While HDC offers lightweight and efficient learning compared to deep neural networks,
  it still benefits from improved data efficiency.
---

# HEAL: Brain-inspired Hyperdimensional Efficient Active Learning

## Quick Facts
- arXiv ID: 2402.11223
- Source URL: https://arxiv.org/abs/2402.11223
- Reference count: 40
- First Active Learning framework designed specifically for Hyperdimensional Computing-based classification

## Executive Summary
HEAL introduces a novel Active Learning framework tailored for Hyperdimensional Computing (HDC) classifiers, addressing the critical challenge of data efficiency in HDC systems. While HDC offers lightweight and efficient learning compared to deep neural networks, it still benefits from improved data efficiency. HEAL achieves this through uncertainty and diversity-guided acquisition, proactively annotating unlabeled data points to lower annotation costs. The framework leverages an HDC ensemble with prior hypervectors for uncertainty estimation and employs hypervector memorization for diversity-aware batch acquisition.

## Method Summary
HEAL is an Active Learning framework that improves data efficiency in HDC classifiers through uncertainty and diversity-guided acquisition. The method uses an HDC ensemble with prior hypervectors to estimate model uncertainty without requiring gradients or probabilistic computations. For diversity-aware batch acquisition, HEAL maintains memory hypervectors per class and selects samples based on similarity thresholds. The framework integrates seamlessly with existing HDC classifier architectures and employs NeuralHD for dimensionality reduction when needed.

## Key Results
- Outperforms diverse baselines in AL quality and data efficiency across four datasets
- Achieves 11× to 40,000× speedup in acquisition runtime per batch compared to BNN-powered methods
- Demonstrates effective uncertainty estimation through ensemble disagreement and prior hypervector initialization
- Shows improved data efficiency through diversity-aware batch acquisition using memory hypervectors

## Why This Works (Mechanism)

### Mechanism 1: Prior Hypervector Initialization
HEAL improves uncertainty estimation by initializing HDC ensemble sub-models with non-zero prior hypervectors sampled from a Gaussian distribution. These priors serve as diverse initializations, creating better disagreement among sub-models for more accurate uncertainty estimation on unseen data.

### Mechanism 2: Diversity-Aware Acquisition
The framework prevents redundant sample selection through hypervector-based similarity comparison. Memory hypervectors encode previously acquired samples per class, and new candidates are compared against these memories to ensure batch diversity.

### Mechanism 3: Computational Efficiency
HEAL achieves significant runtime improvements through shared encoder architecture, pre-computed similarities, and NeuralHD dimensionality reduction. These design choices eliminate redundant computations and reduce the computational overhead compared to BNN-based AL methods.

## Foundational Learning

- **HDC Encoding and Operations**: Understanding bundling, binding, and similarity operations is essential as HEAL is built entirely on HDC primitives
  - Quick check: How does fractional power encoding differ from standard binding in HDC, and why is it used in HEAL?

- **Active Learning Strategies**: Knowledge of uncertainty sampling, margin sampling, and diversity metrics is crucial for understanding HEAL's design
  - Quick check: What is the difference between confidence sampling and margin sampling in AL, and how is margin sampling adapted for HDC in HEAL?

- **Ensemble Learning for Uncertainty**: Understanding how ensemble disagreement translates to predictive uncertainty is key to grasping HEAL's uncertainty estimation approach
  - Quick check: How does ensemble disagreement translate to predictive uncertainty in HEAL, and why are prior hypervectors important?

## Architecture Onboarding

- **Component map**: HDC Encoder (shared) -> HDC Ensemble (E sub-models) -> Uncertainty Acquisition -> Diversity Module -> Training Loop
- **Critical path**: 
  1. Pre-encode unlabeled pool
  2. Pre-compute similarities with prior hypervectors
  3. Train ensemble on current labeled data
  4. Score candidates using margin-based acquisition
  5. Filter via diversity module
  6. Query true labels and update datasets
- **Design tradeoffs**: 
  - Higher ensemble size improves uncertainty estimation but increases computation
  - NeuralHD reduces dimensionality at some accuracy cost
  - Similarity threshold balances diversity vs. information gain
- **Failure signatures**: 
  - Low ensemble diversity leads to poor uncertainty scores
  - High redundancy in batches indicates overly permissive similarity filtering
  - Slow acquisition suggests encoder sharing is broken or NeuralHD disabled unnecessarily
- **First 3 experiments**: 
  1. Train HEAL on UCIHAR with b=20, compare accuracy vs random acquisition
  2. Disable diversity module (γ=1.0) and observe batch redundancy, then tune γ
  3. Compare acquisition runtime with and without NeuralHD at 50% dimensionality reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does HEAL's performance change with different HDC encoding schemes beyond Fractional Power Encoding?
- Basis: Paper evaluates only FPE, leaving impact of other encodings unexplored
- Evidence needed: Experimental results comparing various HDC encoding schemes on same datasets

### Open Question 2
- Question: Can HEAL's uncertainty estimation technique be extended to HDC regression tasks?
- Basis: Paper focuses on classification and mentions prior HDC regression uncertainty methods are unsuitable
- Evidence needed: Experimental results showing HEAL's effectiveness for HDC regression uncertainty estimation

### Open Question 3
- Question: What is the impact of prior hypervectors' initialization distribution on HEAL's performance?
- Basis: Prior hypervectors initialized from standard Gaussian but other distributions not explored
- Evidence needed: Experimental results comparing different initialization distributions across datasets

## Limitations

- Exact parameter settings (dimensionality D, ensemble size E, similarity threshold γ) are not specified
- Dataset preprocessing details for DSADS and PAMAP are incomplete
- Prior hypervector initialization strategy lacks detailed specification

## Confidence

- **High**: Runtime efficiency claims (11× to 40,000× speedup) are well-supported by design analysis
- **Medium**: Accuracy improvements depend on proper hyperparameter tuning and are demonstrated but not fully detailed
- **Medium**: Novelty claims are strong as no prior work combines HDC ensemble uncertainty with hypervector-based diversity filtering

## Next Checks

1. Implement HEAL with configurable parameters and validate uncertainty estimation on a simple binary HDC classification task before scaling to full datasets
2. Systematically test the impact of NeuralHD dimensionality reduction on both accuracy and runtime across all four datasets
3. Compare HEAL's diversity module against standard AL diversity metrics (e.g., core-set) using the same uncertainty scores to isolate diversity effects