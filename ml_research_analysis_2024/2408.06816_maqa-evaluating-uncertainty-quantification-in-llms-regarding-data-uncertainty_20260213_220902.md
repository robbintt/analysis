---
ver: rpa2
title: 'MAQA: Evaluating Uncertainty Quantification in LLMs Regarding Data Uncertainty'
arxiv_id: '2408.06816'
source_url: https://arxiv.org/abs/2408.06816
tags:
- uncertainty
- answers
- question
- answer
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new benchmark, MAQA, to evaluate uncertainty
  quantification in large language models (LLMs) under data uncertainty, where questions
  inherently require multiple answers. The dataset consists of 2,042 non-ambiguous,
  multi-answer question-answer pairs across world knowledge, mathematical reasoning,
  and commonsense reasoning tasks.
---

# MAQA: Evaluating Uncertainty Quantification in LLMs Regarding Data Uncertainty

## Quick Facts
- arXiv ID: 2408.06816
- Source URL: https://arxiv.org/abs/2408.06816
- Reference count: 40
- Key outcome: MAQA benchmark reveals data uncertainty impacts traditional uncertainty measures, especially in reasoning tasks where LLMs become overconfident

## Executive Summary
This paper introduces MAQA, a new benchmark for evaluating uncertainty quantification in large language models (LLMs) under data uncertainty, where questions inherently require multiple valid answers. The authors assess five uncertainty quantification methods (three white-box and two black-box) across world knowledge, mathematical reasoning, and commonsense reasoning tasks using various models. Results show that data uncertainty negatively impacts traditional uncertainty measures, particularly in reasoning tasks where LLMs exhibit overconfidence. While entropy and consistency-based methods remain effective, the study highlights the need for new approaches that can separately estimate model and data uncertainty without requiring input clarification.

## Method Summary
The study evaluates uncertainty quantification methods on the MAQA dataset containing 2,042 non-ambiguous, multi-answer question-answer pairs across three task types. Five uncertainty quantification methods are assessed: white-box methods (max softmax logit, entropy, margin) and black-box methods (verbalized confidence, response consistency). Experiments are conducted using multiple LLM models including Llama3, Qwen1.5, Mistral, and Mixtral, comparing performance between single-answer and multi-answer settings using AUROC scores. The dataset is created by modifying existing QA datasets to generate multi-answer versions, with quality checks performed by LLM judges.

## Key Results
- Data uncertainty significantly degrades traditional uncertainty measures, especially in reasoning tasks where LLMs become overconfident
- Entropy- and consistency-based methods remain effective at estimating model uncertainty even under data uncertainty
- Verbalized confidence struggles with multi-answer datasets, showing relatively low AUROC scores compared to other methods
- Performance varies substantially across different task types, suggesting uncertainty quantification should be tailored to specific domains

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Data uncertainty from multi-answer questions impacts logit distributions, but entropy-based uncertainty quantification remains effective because LLMs prioritize a few tokens regardless of answer count.
- **Mechanism**: Even when a question requires multiple answers, the LLM tends to assign higher probability to a small set of candidate tokens. Entropy captures the spread of these probabilities, which correlates with uncertainty even when multiple answers are valid.
- **Core assumption**: The internal token prioritization of LLMs is relatively stable across different answer counts, so entropy still reflects the model's uncertainty about correctness.
- **Evidence anchors**:
  - [abstract] "Moreover, we observe that entropy- and consistency-based methods effectively estimate model uncertainty, even in the presence of data uncertainty."
  - [section] "Figure 2b, the sum of the top 5 softmax logits of the correct answers remains consistent regardless of the number of ground-truth answers."
  - [corpus] Weak: The corpus does not directly address entropy's robustness under multi-answer settings.

### Mechanism 2
- **Claim**: In reasoning tasks, LLMs become overconfident after generating the first answer, complicating uncertainty quantification.
- **Mechanism**: The reasoning process leads to increasingly confident logits for subsequent answers, masking uncertainty and making traditional measures less reliable.
- **Core assumption**: The confidence boost from intermediate reasoning steps persists and compounds across multiple answer generations.
- **Evidence anchors**:
  - [abstract] "Moreover, we observe that entropy- and consistency-based methods effectively estimate model uncertainty, even in the presence of data uncertainty."
  - [section] "As illustrated in Figure 2c, the average maximum softmax logit for each answer token tends to be high, exceeding 0.9 in some models (e.g., Llama3, Qwen1.5)."
  - [corpus] Missing: The corpus does not explicitly discuss reasoning task overconfidence dynamics.

### Mechanism 3
- **Claim**: Response consistency is highly effective for uncertainty quantification under data uncertainty because it measures agreement across multiple samples, which correlates with correctness regardless of answer multiplicity.
- **Mechanism**: By generating multiple responses to the same prompt and measuring their similarity, the method captures the model's internal confidence in its answer choice, which is less affected by the number of valid answers.
- **Core assumption**: The similarity between multiple sampled responses is strongly correlated with the correctness of the answer, even when multiple correct answers exist.
- **Evidence anchors**:
  - [abstract] "Moreover, we observe that entropy- and consistency-based methods effectively estimate model uncertainty, even in the presence of data uncertainty."
  - [section] "As shown in Table 4, verbalized confidence struggles, as evidenced by the relatively low AUROC scores, especially on the commonsense reasoning task... In contrast, consistency-based methods have significantly high AUROC scores on all tasks, especially on multi-answer datasets."
  - [corpus] Missing: The corpus does not contain specific findings on response consistency under multi-answer conditions.

## Foundational Learning

- **Concept**: Data uncertainty vs. model uncertainty
  - **Why needed here**: The paper distinguishes between irreducible randomness in inputs (data uncertainty) and insufficient model knowledge (model uncertainty). Understanding this distinction is critical for interpreting why multi-answer questions introduce unique challenges.
  - **Quick check question**: In a question like "What are the renewable energy sources?", which type of uncertainty is primarily at play: data uncertainty or model uncertainty?

- **Concept**: Logit distribution and softmax
  - **Why needed here**: Uncertainty quantification methods like max softmax logit, entropy, and margin operate on the model's output logits before normalization. Knowing how these relate to probabilities is essential for understanding the mechanisms.
  - **Quick check question**: If a model outputs logits [2.0, 1.0, 0.5] for three tokens, which token has the highest softmax probability?

- **Concept**: Response consistency
  - **Why needed here**: This black-box method estimates uncertainty by generating multiple responses and measuring their similarity. It is one of the few methods that remains effective under data uncertainty.
  - **Quick check question**: If a model generates the same answer five times in a row for the same prompt, what would the response consistency score be?

## Architecture Onboarding

- **Component map**: MAQA dataset -> LLM models (Llama3, Qwen1.5, Mistral, Mixtral) -> Uncertainty quantification methods (entropy, max logit, margin, verbalized confidence, response consistency) -> AUROC/AUPRC evaluation
- **Critical path**: For each model and task, generate responses, compute uncertainty scores using each method, compare against ground truth correctness, and aggregate AUROC/AUPRC scores
- **Design tradeoffs**: White-box methods are efficient but require model access and may be sensitive to data uncertainty; black-box methods are more general but computationally expensive due to multiple sampling
- **Failure signatures**: Low AUROC scores on multi-answer datasets indicate poor uncertainty estimation; large drops in performance when switching from single- to multi-answer settings suggest sensitivity to data uncertainty
- **First 3 experiments**:
  1. Run max softmax logit, entropy, and margin on a small subset of MAQA with a single model to verify logit extraction and score computation
  2. Generate 5 responses per prompt for a few questions and compute response consistency to ensure sampling and similarity calculation work
  3. Compare AUROC scores for single-answer vs. multi-answer datasets on one task to confirm the expected drop in performance

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can uncertainty quantification methods be designed to separately estimate model and data uncertainty in LLMs without requiring input clarification?
- **Basis in paper**: [explicit] The authors note that previous methods struggle to distinguish between model and data uncertainty, especially in multi-answer scenarios. They suggest that new approaches are needed to decompose these uncertainties.
- **Why unresolved**: The paper highlights the challenge but does not provide a concrete solution for separating model and data uncertainty in LLMs without input clarification.
- **What evidence would resolve it**: A proposed method or framework that effectively separates model and data uncertainty in LLMs, validated through experiments on datasets like MAQA, would resolve this question.

### Open Question 2
- **Question**: How do uncertainty quantification methods perform on long-form answers compared to short-form answers in multi-answer datasets?
- **Basis in paper**: [inferred] The authors mention that their dataset primarily consists of short-form answers and that analyzing long-form answers is challenging due to the vagueness of correctness assessment.
- **Why unresolved**: The paper does not explore the performance of uncertainty quantification methods on long-form answers, which could provide insights into their effectiveness in more complex scenarios.
- **What evidence would resolve it**: Experiments comparing the performance of uncertainty quantification methods on long-form versus short-form answers in multi-answer datasets would provide clarity on this issue.

## Limitations

- The study lacks a controlled experiment isolating model uncertainty from data uncertainty, making it difficult to definitively attribute performance degradation to data uncertainty alone
- The evaluation relies on AUROC scores which may not fully capture the practical implications of overconfidence in real-world applications
- The dataset creation process involved modifying existing datasets, which may introduce biases from the original sources

## Confidence

- **High Confidence**: The observation that entropy- and consistency-based methods remain effective under data uncertainty is well-supported by experimental results showing consistent AUROC scores across multi-answer settings
- **Medium Confidence**: The claim that reasoning tasks exhibit higher overconfidence is supported by logit analysis but requires more systematic investigation of the reasoning process dynamics
- **Low Confidence**: The assertion that new approaches are needed to separately estimate model and data uncertainty is based on observed performance drops but lacks comparative analysis with methods specifically designed for this separation

## Next Checks

1. **Controlled ablation study**: Create a synthetic dataset with known levels of model vs. data uncertainty to isolate their individual effects on uncertainty quantification performance
2. **Temporal consistency analysis**: Evaluate whether the observed overconfidence in reasoning tasks persists across different inference temperatures and sampling strategies
3. **Cross-dataset generalization**: Test the proposed uncertainty quantification methods on additional multi-answer datasets from different domains to verify robustness beyond the MAQA benchmark