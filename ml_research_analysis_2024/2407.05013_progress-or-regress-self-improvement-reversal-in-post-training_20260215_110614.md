---
ver: rpa2
title: Progress or Regress? Self-Improvement Reversal in Post-training
arxiv_id: '2407.05013'
source_url: https://arxiv.org/abs/2407.05013
tags:
- iterative
- post-training
- answer
- self-improvement
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies a "self-improvement reversal" phenomenon in
  post-training methods like iterative preference learning for Large Language Models
  (LLMs). While these methods show improved pass@1 accuracy on benchmarks, they paradoxically
  lead to declines in broader capabilities like output diversity and out-of-distribution
  (OOD) generalization.
---

# Progress or Regress? Self-Improvement Reversal in Post-training

## Quick Facts
- arXiv ID: 2407.05013
- Source URL: https://arxiv.org/abs/2407.05013
- Reference count: 40
- Key outcome: Iterative post-training methods show improved pass@1 accuracy but paradoxically lead to declines in output diversity and out-of-distribution generalization in LLMs.

## Executive Summary
This paper identifies a critical phenomenon called "self-improvement reversal" in post-training methods for Large Language Models (LLMs). While iterative preference learning methods like DPO show improved performance on benchmarks, they paradoxically lead to declines in broader capabilities including output diversity and out-of-distribution (OOD) generalization. The authors propose a comprehensive evaluation framework that goes beyond simple accuracy metrics to assess improvement problems, solution diversity, and OOD capabilities. Through rigorous experimentation, they find that iterative self-improvement primarily optimizes answer selection within the model's generation space rather than enhancing actual problem-solving abilities, with diversity reduction identified as a key bottleneck leading to capability collapse.

## Method Summary
The study evaluates three iterative post-training paradigms (Iterative SFT, Iterative DPO, and Iterative SFT-DPO) on multiple foundation models (LLaMA2-7B, Mistral-7B, LLaMA3-8B) across four datasets (CSQA, GSM8K, MATH, MBPP). Models are first fine-tuned with SFT on task-specific data, then iteratively refined through self-generated data. The authors propose comprehensive evaluation metrics including pass@1 accuracy, improvement problems (IS), solution diversity (Distinct N-grams, Sentence-BERT embedding cosine similarity, Distinct Equations), and OOD generalization. The experimental setup systematically compares performance across iterations to identify the self-improvement reversal phenomenon.

## Key Results
- Iterative post-training methods show improved pass@1 accuracy on benchmarks but lead to declines in output diversity and OOD generalization
- Solution diversity consistently decreases across all methods during iterations, identified as a bottleneck for generalization
- The effectiveness of iterative methods depends on initial correct answer coverage, with higher coverage yielding better improvements
- Iterative SFT-DPO outperforms other methods when initial coverage is high (>0.5), while Iterative SFT is more effective at lower coverage levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative self-improvement primarily enhances answer selection within the model's generation space rather than acquiring new problem-solving abilities
- Mechanism: The model improves by refining its ability to select the correct answer from its own generated outputs, rather than learning to solve more complex problems
- Core assumption: The model's generation space already contains correct answers, but the model initially struggles to select them accurately
- Evidence anchors:
  - [abstract] "models showing improved performance across benchmarks will paradoxically exhibit declines in broader, essential capabilities, like output diversity and out-of-distribution (OOD) generalization"
  - [section 5.1] "iterative self-improvement hardly entails the acquisition of new problem-solving abilities, but rather the enhancement of the model's correct answer selection within its generation space"
- Break condition: If the model's generation space does not contain correct answers for a given problem, this mechanism cannot improve performance

### Mechanism 2
- Claim: Reduction in solution diversity during iterations leads to decreased OOD generalization and capability collapse
- Mechanism: As iterations progress, the model's outputs become more homogeneous, focusing on fitting training data at the expense of broader generalization
- Core assumption: Diversity in solutions is necessary for robust generalization to unseen problems
- Evidence anchors:
  - [section 5.2] "all methods show a consistent decrease in diversity, significantly diminishing the diversity of model outputs over iterations"
  - [section 5.3] "All three iterative post-training methods can exacerbate the generalization disparities across groups"
- Break condition: If diversity metrics do not correlate with generalization performance, this mechanism breaks down

### Mechanism 3
- Claim: Correct answer coverage is a key factor determining the effectiveness of iterative post-training methods
- Mechanism: Models with higher correct answer coverage in their initial state achieve better improvements through iterative methods
- Core assumption: The initial model's ability to generate correct answers across the answer space predicts subsequent improvement potential
- Evidence anchors:
  - [section 4.2] "The table clearly demonstrates that when the correct answer coverage is high (> 0.5), Iterative DPO and Iterative SFT-DPO produce the best-performing M∗t"
  - [section 4.2] "When the coverage is lower (≤ 0.5), Iterative SFT is more effective in achieving the optimal M∗t"
- Break condition: If coverage does not predict improvement across different tasks and models, this mechanism is invalid

## Foundational Learning

- Concept: Supervised Fine-tuning (SFT)
  - Why needed here: SFT initializes the model with task-specific knowledge before iterative improvement
  - Quick check question: How does SFT differ from iterative SFT in the context of self-improvement?

- Concept: Preference Learning
  - Why needed here: Preference learning methods like DPO are used to align model outputs with human preferences
  - Quick check question: What is the key difference between DPO and traditional RLHF in terms of computational efficiency?

- Concept: Out-of-Distribution (OOD) Generalization
  - Why needed here: Evaluating OOD performance reveals whether improvements transfer to new problem types
  - Quick check question: Why might improved in-distribution performance not translate to better OOD generalization?

## Architecture Onboarding

- Component map: Foundation models (LLaMA2-7B, Mistral-7B, LLaMA3-8B) -> Training datasets (CSQA, GSM8K, MATH, MBPP) -> Post-training methods (Iterative SFT, Iterative DPO, Iterative SFT-DPO) -> Evaluation metrics (pass@1, diversity metrics, OOD generalization) -> Sampling and rewarding system

- Critical path:
  1. Initialize model with SFT on task-specific data
  2. Generate self-training data through sampling
  3. Apply post-training method to refine model
  4. Evaluate performance using comprehensive metrics
  5. Repeat iterations until convergence or degradation

- Design tradeoffs:
  - Accuracy vs. diversity: Improving accuracy often reduces output diversity
  - Computation vs. performance: More iterations yield better results but increase computational cost
  - Model size vs. generalization: Larger models may show better OOD generalization

- Failure signatures:
  - Plateauing or declining performance after several iterations
  - Significant reduction in output diversity
  - Worsening OOD generalization despite improved in-distribution performance

- First 3 experiments:
  1. Compare Iterative SFT vs. Iterative DPO on a simple dataset (CSQA) to observe basic performance trends
  2. Measure diversity metrics (Distinct N-grams, Sentence-BERT) across iterations to quantify solution diversity reduction
  3. Evaluate OOD generalization by training on GSM8K and testing on MATH to assess transfer capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific limitations of current evaluation metrics in capturing the full spectrum of model performance and behavior, particularly in real-world applications?
- Basis in paper: [inferred] The authors acknowledge that their evaluation metrics, while more holistic than traditional measures, may still not capture all dimensions of model performance and behavior, especially in real-world applications
- Why unresolved: The paper does not delve into the specific shortcomings of the proposed evaluation metrics or suggest alternative approaches to address these limitations
- What evidence would resolve it: A comprehensive analysis of the proposed evaluation metrics' performance in real-world scenarios, along with a comparison to alternative metrics and their effectiveness in capturing diverse aspects of model behavior

### Open Question 2
- Question: How can the computational efficiency of iterative post-training methods be optimized to make them more accessible for broader use, particularly in environments with limited resources?
- Basis in paper: [inferred] The authors mention that the iterative nature of their methodologies requires extensive training cycles, which can be computationally expensive and time-consuming, potentially limiting their practical applicability in environments with limited resources
- Why unresolved: The paper does not explore potential strategies or techniques to reduce the computational burden of iterative post-training methods or make them more resource-efficient
- What evidence would resolve it: Research and experimentation demonstrating effective approaches to optimize the computational efficiency of iterative post-training methods, such as model compression, distributed training, or more efficient optimization algorithms

### Open Question 3
- Question: What are the long-term impacts of iterative post-training methods on model robustness and adaptability, and how can these be ensured to achieve sustainable advancements in LLM capabilities?
- Basis in paper: [inferred] The authors suggest that investigating the long-term impacts of these methodologies on model robustness and adaptability will be crucial in ensuring sustainable advancements in LLM capabilities
- Why unresolved: The paper does not provide insights into the potential long-term effects of iterative post-training on model robustness and adaptability or discuss strategies to mitigate any negative impacts
- What evidence would resolve it: Longitudinal studies examining the performance and behavior of models subjected to iterative post-training over extended periods, along with an analysis of their robustness and adaptability to new tasks and domains

## Limitations
- The study focuses primarily on transformer-based models and may not generalize to other architectures
- The mechanisms behind self-improvement reversal are inferred from observed patterns rather than directly measured
- The computational cost of iterative post-training methods may limit their practical applicability in resource-constrained environments

## Confidence
- **High Confidence**: The empirical observation of performance degradation after multiple iterations of post-training methods is robust across different datasets and model architectures
- **Medium Confidence**: The proposed mechanism of answer selection refinement versus problem-solving ability acquisition is plausible but requires additional direct validation
- **Medium Confidence**: The correlation between diversity reduction and OOD generalization decline is supported by evidence but the causal relationship needs stronger experimental verification

## Next Checks
1. Cross-architecture validation: Test the self-improvement reversal phenomenon on non-transformer architectures and different model sizes to establish whether this is a fundamental limitation of current approaches
2. Controlled diversity experiments: Systematically manipulate output diversity during training to determine if maintaining diversity can prevent capability collapse while preserving accuracy improvements
3. Long-term capability assessment: Evaluate models after extended periods without further training to determine if capability degradation is permanent or reversible, distinguishing between catastrophic forgetting and structural limitations