---
ver: rpa2
title: 'Incremental Comprehension of Garden-Path Sentences by Large Language Models:
  Semantic Interpretation, Syntactic Re-Analysis, and Attention'
arxiv_id: '2405.16042'
source_url: https://arxiv.org/abs/2405.16042
tags:
- sentences
- chunk
- garden-path
- correct
- point
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the alignment between large language models
  (LLMs) and human processing of garden-path sentences, which are temporarily ambiguous
  sentences that lead readers to initially misinterpret their meaning. The researchers
  examined four LLMs (GPT-2, LLaMA-2, Flan-T5, and RoBERTa) as they processed 24 garden-path
  sentences in chunks, probing their semantic interpretations, syntactic parse trees,
  and attention mechanisms.
---

# Incremental Comprehension of Garden-Path Sentences by Large Language Models: Semantic Interpretation, Syntactic Re-Analysis, and Attention

## Quick Facts
- **arXiv ID**: 2405.16042
- **Source URL**: https://arxiv.org/abs/2405.16042
- **Reference count**: 5
- **Primary result**: LLMs show promising alignment with human processing of garden-path sentences, especially when extra-syntactic cues like commas guide interpretation

## Executive Summary
This study investigates how large language models process garden-path sentences - temporarily ambiguous sentences that initially lead readers to misinterpret their meaning. The researchers tested four different LLMs (GPT-2, LLaMA-2, Flan-T5, and RoBERTa) as they processed 24 garden-path sentences in incremental chunks. They tracked semantic interpretations, extracted syntactic parse trees, and visualized attention mechanisms to compare model processing with human data. The results show that LLaMA-2 and RoBERTa-large demonstrate human-like performance in correctly interpreting sentences with disambiguating commas and constructing appropriate parse trees, suggesting LLMs can serve as viable models for understanding human sentence processing in contexts of temporary ambiguity.

## Method Summary
The study used 24 garden-path sentences split into five chunks each, with yes/no comprehension questions probing whether models misinterpreted the sentences initially. Four LLMs with different architectures (GPT-2, LLaMA-2, Flan-T5, RoBERTa) processed sentences incrementally, with semantic interpretations tracked via question-answering probabilities, syntactic structures extracted using parse tree probes, and attention patterns visualized through heatmap analysis. The researchers compared model performance to human data from previous studies, focusing on whether models showed similar reinterpretation patterns when disambiguating information was present.

## Key Results
- LLaMA-2 showed human-like performance in correctly interpreting sentences with disambiguating commas
- RoBERTa-large demonstrated human-like accuracy in constructing correct parse trees comparable to human data
- Attention mechanisms in transformer models were sensitive to disambiguating information, with specific heads showing increased focus on correct syntactic attachments
- GPT-2, LLaMA-2, and Flan-T5 initially assigned higher probabilities to misinterpreted interpretations, aligning with human garden-path effects

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can represent semantic misinterpretations during ambiguous sentence regions and shift to correct interpretations after disambiguation when extra-syntactic cues (e.g., commas) are present.
- Mechanism: During incremental processing, LLMs initially assign higher probabilities to interpretations consistent with the misleading parse. When disambiguating information arrives (either via structural cues like commas or through later sentence content), models update their semantic state by reassigning higher probabilities to the correct interpretation.
- Core assumption: LLMs maintain distributed representations of multiple interpretations that can be dynamically weighted based on incoming evidence.
- Evidence anchors:
  - [abstract] "LLaMA-2 showed human-like performance in correctly interpreting sentences with disambiguating commas"
  - [section] "only LLaMA succeeded in switching to the correct semantic interpretation when a disambiguating comma was present"
  - [corpus] Weak evidence; corpus neighbors don't directly address comma-guided disambiguation mechanisms
- Break condition: Models fail to update semantic probabilities when disambiguating information arrives, maintaining high probability on misinterpretation throughout processing.

### Mechanism 2
- Claim: LLMs construct implicit parse trees that can be extracted and analyzed to reveal syntactic reanalysis at disambiguation points.
- Mechanism: The internal representations of LLMs encode hierarchical syntactic structure that can be probed using linear transformations to extract parse trees. These trees shift from incorrect to correct attachments when disambiguating information is processed.
- Core assumption: Parse tree information is implicitly encoded in the word embeddings and can be linearly decoded without explicit syntactic supervision.
- Evidence anchors:
  - [abstract] "RoBERTa-large demonstrated human-like accuracy in constructing correct parse trees"
  - [section] "RoBERTa-large...begin[s] to shift to the correct parse tree...comparable to the performance observed in...Christianson et al. (2001) study"
  - [corpus] No direct corpus evidence for parse tree extraction methodology
- Break condition: Extracted parse trees fail to show meaningful shifts at disambiguation points, suggesting the syntactic information is not well-encoded or extractable.

### Mechanism 3
- Claim: Attention mechanisms in transformer models are sensitive to disambiguating information, with certain attention heads showing increased focus on correct syntactic attachments when extra-syntactic cues are present.
- Mechanism: Attention weights between tokens change dynamically during processing, with specific attention heads showing stronger connections between disambiguated elements (e.g., noun phrases and their correct verbs) when clarifying information is available.
- Core assumption: Attention patterns correlate with syntactic dependencies and can be interpreted as evidence of structural reanalysis.
- Evidence anchors:
  - [abstract] "visualize the model components that attend to disambiguating information when processing the question probes"
  - [section] "heatmap of the sensitivities across the attention heads...indicate the attention heads sensitive to the point of disambiguation"
  - [corpus] No corpus evidence specifically addressing attention sensitivity to disambiguation
- Break condition: Attention patterns show no systematic changes at disambiguation points, indicating attention is not encoding structural reanalysis information.

## Foundational Learning

- Concept: Garden-path sentences and temporary ambiguity
  - Why needed here: Understanding how readers initially misinterpret syntactically ambiguous sentences and must reanalyze them is fundamental to interpreting the study's experimental design and results
  - Quick check question: What causes the "garden path" effect in sentences like "While the man hunted the deer ran through the woods"?

- Concept: Incremental sentence processing
  - Why needed here: The study processes sentences chunk by chunk to simulate how both humans and models build interpretations over time, making this processing model essential for understanding the methodology
  - Quick check question: Why does the study present sentences in chunks rather than whole sentences?

- Concept: Transformer attention mechanisms
  - Why needed here: Understanding how self-attention works in transformer models is crucial for interpreting the attention visualization results and what they reveal about syntactic processing
  - Quick check question: What information do attention weights between tokens provide about model processing?

## Architecture Onboarding

- Component map: Sentence → Chunked processing → Semantic interpretation tracking → Parse tree extraction → Attention visualization → Comparison with human data

- Critical path: Sentence → Chunked processing → Semantic interpretation tracking → Parse tree extraction → Attention visualization → Comparison with human data

- Design tradeoffs: Using multiple models with different architectures allows for broader generalization but increases computational complexity. The chunk-by-chunk processing approach enables fine-grained analysis but may not perfectly capture natural reading dynamics. Semantic probing via question-answering is direct but limited to yes/no interpretations.

- Failure signatures: Models that fail to show semantic probability shifts at disambiguation points, parse trees that don't show structural reanalysis, or attention patterns that don't correlate with disambiguating information all indicate failures in modeling human-like processing.

- First 3 experiments:
  1. Surprisal baseline calculation across all four models on both comma-present and comma-absent conditions
  2. Semantic interpretation tracking using comprehension question probabilities across the five processing chunks
  3. Parse tree extraction and analysis of structural shifts at the disambiguation point for GPT-2 and RoBERTa-large

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do larger language models show improved alignment with human processing of garden-path sentences compared to smaller models?
- Basis in paper: [inferred] The authors note that "larger models tend to show emergent abilities" and suggest that future work should evaluate larger models like LLaMA-70B and GPT-4.
- Why unresolved: The study only tested models ranging from millions to billions of parameters. The authors acknowledge that testing larger models could reveal whether size correlates with improved human alignment.
- What evidence would resolve it: Conducting the same experiments with significantly larger language models and comparing their performance to human data would provide evidence for or against improved alignment with increased model size.

### Open Question 2
- Question: Are the attention mechanisms in language models sensitive to disambiguating information in other types of temporary syntactic ambiguities beyond garden-path sentences?
- Basis in paper: [explicit] The authors state that their exploratory analysis of attention weights "raises new questions" and specifically ask whether "the same attention heads that are sensitive to disambiguating information for the garden-path sentences studied here are also sensitive to disambiguating information in other temporarily ambiguous sentence structures."
- Why unresolved: The study focused on a specific type of temporary ambiguity (garden-path sentences). The exploratory analysis of attention mechanisms was limited to these sentences.
- What evidence would resolve it: Testing the same attention analysis techniques on a broader range of temporarily ambiguous sentence structures would reveal whether the observed sensitivity to disambiguating information is generalizable across different types of syntactic ambiguity.

### Open Question 3
- Question: How does the incremental processing of garden-path sentences differ between language models trained with different architectures and training paradigms?
- Basis in paper: [explicit] The authors tested four models with different architectures (decoder-only, encoder-decoder, encoder-only) and training approaches (next-token prediction, instruction tuning, masked language modeling), noting that "they span a range of architectures, training approaches, and sizes."
- Why unresolved: While the authors compared these models, they acknowledge that "they are far from exhaustive" and suggest that future work should evaluate a "larger set of LLMs" to understand how architecture and training paradigm influence processing.
- What evidence would resolve it: Conducting the same experiments on a wider variety of language models with different architectures and training paradigms would reveal whether certain approaches lead to better alignment with human processing of garden-path sentences.

## Limitations

- The study's scope is limited to one specific type of garden-path sentence (NP/Z ambiguity) and relies on yes/no comprehension questions
- Parse tree extraction methodology assumes linear decodability of syntactic structure, which may not hold equally for all model architectures
- Attention analysis shows correlation but cannot establish causal links between attention patterns and interpretation shifts

## Confidence

**High Confidence:** LLaMA-2 shows human-like performance with disambiguating commas (based on direct probability tracking and clear semantic shifts).  
**Medium Confidence:** RoBERTa-large's human-like parse tree construction (dependent on the validity of the linear probe methodology).  
**Low Confidence:** Interpretation of attention mechanisms as directly reflecting syntactic reanalysis (correlation does not establish causation).

## Next Checks

1. **Cross-linguistic validation**: Test the same methodology on garden-path sentences from languages with different word orders (e.g., SOV languages) to assess whether findings generalize beyond English.

2. **Ablation of comma cues**: Systematically remove all comma-based disambiguation cues and measure whether models still show human-like reinterpretation patterns through other linguistic cues.

3. **Temporal dynamics analysis**: Track not just final interpretation probabilities but the rate and trajectory of semantic shifts across chunks to determine if models mirror human processing time courses.