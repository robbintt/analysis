---
ver: rpa2
title: On the Sample Complexity of One Hidden Layer Networks with Equivariance, Locality
  and Weight Sharing
arxiv_id: '2411.14288'
source_url: https://arxiv.org/abs/2411.14288
tags:
- networks
- group
- generalization
- equivariant
- bound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the generalization bounds for single-hidden-layer
  equivariant networks with weight sharing, equivariance, and locality. The authors
  use Rademacher complexity analysis to obtain dimension-free bounds that depend only
  on the norm of the filters.
---

# On the Sample Complexity of One Hidden Layer Networks with Equivariance, Locality and Weight Sharing

## Quick Facts
- arXiv ID: 2411.14288
- Source URL: https://arxiv.org/abs/2411.14288
- Reference count: 40
- This paper studies generalization bounds for single-hidden-layer equivariant networks, showing that equivariance provides dimension-free bounds while weight sharing offers similar benefits under certain conditions.

## Executive Summary
This paper provides a theoretical analysis of single-hidden-layer neural networks with equivariance, locality, and weight sharing properties. Using Rademacher complexity, the authors derive dimension-free generalization bounds that depend only on filter norms rather than input dimensionality. The analysis reveals that equivariance directly contributes to improved generalization bounds, while weight sharing can provide comparable benefits when specific conditions are met. The work also highlights a fundamental trade-off between locality and expressivity through the uncertainty principle, suggesting that while locality can enhance generalization, it may limit the network's representational capacity.

## Method Summary
The authors employ Rademacher complexity analysis to study generalization bounds for single-hidden-layer networks with equivariant architectures. They derive dimension-free bounds that depend solely on the norm of the filters rather than the input dimension, which is particularly valuable for high-dimensional data. The theoretical framework examines three key architectural properties: equivariance (which ensures transformation-consistent outputs), locality (restricting operations to local neighborhoods), and weight sharing (reusing filters across positions). The analysis shows that while these properties individually contribute to better generalization, there exists a trade-off between locality and expressivity governed by the uncertainty principle. The theoretical results are validated through numerical experiments on a rotated MNIST binary classification task.

## Key Results
- Derived dimension-free generalization bounds for single-hidden-layer equivariant networks that depend only on filter norms
- Demonstrated that equivariance provides direct generalization benefits, while weight sharing can achieve similar guarantees under specific conditions
- Revealed a trade-off between locality and expressivity via the uncertainty principle, showing that while locality enhances generalization, it may limit representational capacity

## Why This Works (Mechanism)
The theoretical guarantees stem from the mathematical properties of equivariant and localized architectures. Equivariance ensures that the network's output transforms consistently with input transformations, reducing the effective hypothesis space and thus the Rademacher complexity. Weight sharing reduces the number of independent parameters by reusing filters, which constrains the hypothesis space similarly to equivariance under certain conditions. Locality limits the receptive field, creating a form of inductive bias that favors local patterns and reduces overfitting, though this comes at the cost of reduced expressivity as dictated by the uncertainty principle.

## Foundational Learning

**Rademacher Complexity**
- Why needed: Provides a measure of hypothesis space complexity that directly relates to generalization bounds
- Quick check: Verify that the empirical Rademacher average converges to the expected value as sample size increases

**Group Convolution**
- Why needed: Enables equivariant operations by applying the same filter across transformed versions of the input
- Quick check: Confirm that the convolution output transforms correctly under group actions

**Uncertainty Principle in CNNs**
- Why needed: Establishes the fundamental trade-off between spatial localization and frequency representation
- Quick check: Measure the trade-off between kernel size and frequency resolution empirically

## Architecture Onboarding

**Component Map**
Input -> Group Convolution/Weight Sharing Layer -> Non-linearity -> Output

**Critical Path**
The critical path is from input through the equivariant/localized convolutional layer to the output, as this determines the generalization properties studied in the paper.

**Design Tradeoffs**
The main tradeoff is between locality (which improves generalization but limits expressivity) and the use of equivariance versus weight sharing (which can provide similar benefits under different conditions).

**Failure Signatures**
- Poor generalization on rotated/transformed test data indicates insufficient equivariance
- High variance across training runs suggests inadequate regularization from weight sharing
- Inability to capture long-range dependencies indicates excessive locality

**3 First Experiments**
1. Compare generalization performance of equivariant vs non-equivariant networks on rotated MNIST
2. Test the effect of varying kernel sizes on the locality-expressivity trade-off
3. Validate whether weight sharing provides similar generalization benefits to equivariance under different data distributions

## Open Questions the Paper Calls Out
None

## Limitations
- The analysis is limited to single-hidden-layer networks, which may not capture the generalization behavior of deeper architectures
- The theoretical bounds assume idealized conditions including Gaussian input distributions
- The practical impact of the locality-expressivity trade-off needs empirical validation beyond the rotated MNIST experiment

## Confidence
- **High Confidence**: The theoretical framework using Rademacher complexity is sound and the dimension-free bounds are mathematically rigorous for the stated assumptions
- **Medium Confidence**: The equivalence between weight sharing and group convolution benefits depends on specific conditions that may not always hold in practice
- **Medium Confidence**: The trade-off between locality and expressivity via the uncertainty principle is theoretically valid but its practical impact needs empirical validation

## Next Checks
1. Extend the analysis to multi-layer networks to verify if the theoretical insights about equivariance, locality, and weight sharing scale to deeper architectures.

2. Test the bounds empirically across diverse datasets and network architectures to validate the theoretical predictions, particularly the conditions under which weight sharing provides similar benefits to equivariance.

3. Investigate the practical implications of the locality-expressivity trade-off by systematically varying kernel sizes and measuring both generalization performance and approximation capabilities.