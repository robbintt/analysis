---
ver: rpa2
title: Uncertainty quantification for deeponets with ensemble kalman inversion
arxiv_id: '2403.03444'
source_url: https://arxiv.org/abs/2403.03444
tags: []
core_contribution: This paper proposes a novel approach for uncertainty quantification
  (UQ) in DeepONets using Ensemble Kalman Inversion (EKI). DeepONets are powerful
  tools for learning complex mappings between input and output functions, but assessing
  their uncertainty in noisy and limited data scenarios is crucial for mission-critical
  applications.
---

# Uncertainty quantification for deeponets with ensemble kalman inversion

## Quick Facts
- arXiv ID: 2403.03444
- Source URL: https://arxiv.org/abs/2403.03444
- Authors: Andrew Pensoneault; Xueyu Zhu
- Reference count: 36
- Primary result: EKI-based UQ for DeepONets achieves reasonable uncertainty estimates on benchmark problems with limited/noisy data

## Executive Summary
This paper addresses the critical challenge of uncertainty quantification (UQ) in DeepONets, which are powerful tools for learning mappings between input and output functions. The authors propose a novel approach using Ensemble Kalman Inversion (EKI) that provides derivative-free, noise-robust, and parallelizable UQ without the computational burden of traditional Bayesian methods. The methodology is demonstrated across three benchmark problems including anti-derivative operators, gravity pendulum dynamics, and diffusion-reaction systems.

## Method Summary
The proposed method leverages EKI to train ensembles of DeepONets while simultaneously obtaining uncertainty estimates. A mini-batch variant of EKI handles larger datasets by sampling subsets of data at each iteration, avoiding cubic scaling complexity. The approach includes a heuristic method for estimating the artificial dynamics covariance matrix Q, which adapts based on validation error to prevent ensemble collapse while maintaining conservative uncertainty estimates. The method is evaluated on three benchmark problems with varying noise levels, comparing mean relative error, uncertainty, and coverage metrics against deterministic DeepONet baselines.

## Key Results
- EKI-based DeepONets achieve mean relative errors of 0.2% (1% noise) and 1.2% (5% noise) on anti-derivative problems
- Coverage probabilities remain close to nominal 95% confidence levels across all test problems
- The proposed approach outperforms deterministic DeepONets in uncertainty quantification while maintaining competitive accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EKI enables derivative-free, parallelizable UQ for DeepONets without sacrificing efficiency
- Mechanism: EKI operates as an ensemble-based iterative method that updates parameter samples using artificial dynamics and measurement updates. This avoids gradient computations required in traditional Bayesian inference while leveraging ensemble statistics to estimate posterior uncertainty
- Core assumption: The ensemble size J is sufficiently large to capture posterior variance without excessive memory or compute overhead
- Evidence anchors: [abstract] "EKI, known for its derivative-free, noise-robust, and highly parallelizable feature"; [section 2.3] describes the iterative EKI update equations that rely only on forward evaluations of H; [section 2.6] confirms computational efficiency by measuring walltime for 814 iterations in seconds

### Mechanism 2
- Claim: Mini-batch EKI scales uncertainty quantification to large datasets by controlling cubic growth in complexity
- Mechanism: By sampling a subset mt of the full dataset at each iteration, the observation covariance matrices Cyy and Cθy are computed on reduced dimensions, keeping computational cost manageable while preserving statistical signal
- Core assumption: The mini-batch size mt is large enough to approximate the full-data gradients but small enough to avoid cubic scaling
- Evidence anchors: [abstract] "We deploy a mini-batch variant of EKI to accommodate larger datasets, mitigating the computational demand due to large datasets during the training stage"; [section 2.4] explicitly states "direct application of the EKI over the full dataset will result in a cubic growth of computational complexity"; [section 3] uses mt=500 and reports feasible walltimes for J=5000 ensembles

### Mechanism 3
- Claim: Heuristic covariance Q estimation prevents ensemble collapse while keeping uncertainty conservative
- Mechanism: The method monitors the discrepancy between predicted variance and validation error, adapting Q multiplicatively to maintain an appropriate spread in ensemble predictions without excessive perturbation
- Core assumption: The validation subset is representative of the full data distribution and provides a meaningful error signal
- Evidence anchors: [section 2.5] introduces fi comparing std(Hqi(θi)) to ||Dqi - mean(Hqi(θi))||2 and adapts ωi accordingly; [section 3] uses this learned Q and reports reasonable uncertainty metrics (e.g., uncertainty ≈ relative error for 1% noise case); [corpus] lacks direct comparisons to fixed-Q variants; evidence is primarily from reported results

## Foundational Learning

- Concept: Ensemble Kalman Filter (EnKF) basics
  - Why needed here: EKI builds directly on EnKF equations; understanding how ensemble means and covariances propagate is essential to grasping the update steps
  - Quick check question: In EnKF, what matrix is used to compute the Kalman gain, and how does it differ from classical KF?

- Concept: DeepONet architecture (branch/trunk fusion)
  - Why needed here: The forward operator H maps neural parameters to outputs via DeepONet; without this, one cannot see how EKI updates propagate to predictions
  - Quick check question: How do the branch and trunk networks combine to produce an output at a query location?

- Concept: Bayesian inference vs. optimization
  - Why needed here: EKI trades exact posterior sampling for approximate ensemble-based inference; understanding this tradeoff is critical for interpreting results
  - Quick check question: What is the main computational bottleneck in HMC that EKI avoids?

## Architecture Onboarding

- Component map: Data pipeline -> Mini-batches -> Observation operator H -> DeepONet (branch+trunk) -> EKI engine -> Validation loop -> Ensemble mean/std outputs
- Critical path: 1. Initialize J DeepONets with N(0,I) priors; 2. At each EKI iteration: Sample mt training outputs; Forward propagate all J networks → H(θ); Compute Cyy, Cθy; Update ensemble via (2.12); Update Q via (2.26); 3. Early stop via smoothed discrepancy (2.28)
- Design tradeoffs: Larger J → better uncertainty but higher memory/time; Larger mt → more stable updates but slower per iteration; Smaller Wq/K → quicker stopping but risk of premature convergence
- Failure signatures: Ensemble collapse: all θ(j) converge to same point → Q too small; Noisy training: walltime spikes, relative error high → mt too small or Q too large; Conservative uncertainty: coverage >> 95% → Q adaptation too aggressive
- First 3 experiments: 1. Run with fixed Q=σ²I (no adaptation) on anti-derivative example; compare coverage and walltime; 2. Vary mt (e.g., 100, 500, 1000) on same problem; measure convergence speed and stability; 3. Test early stopping with different W/K; check sensitivity of final error and uncertainty

## Open Questions the Paper Calls Out
- Question: How does the proposed EKI-based DeepONet approach scale to very large network architectures, and what are the computational bottlenecks?
- Question: How sensitive is the EKI-based DeepONet approach to the choice of hyperparameters, such as the artificial dynamics covariance matrix Q and the stopping criterion threshold?
- Question: How does the EKI-based DeepONet approach compare to other uncertainty quantification methods for DeepONets, such as Hamiltonian Monte Carlo (HMC) and Variational Inference (VI), in terms of accuracy, computational efficiency, and scalability?

## Limitations
- EKI provides approximate Bayesian inference rather than exact posterior sampling, which may under/overestimate uncertainty in highly multimodal posteriors
- The heuristic Q adaptation relies on a validation subset that may not be representative in all practical scenarios, potentially leading to biased uncertainty estimates
- Mini-batch size selection (mt) lacks theoretical guarantees and is determined empirically, making the method sensitive to dataset characteristics
- Computational cost scales linearly with ensemble size J, limiting applicability to very large-scale problems without additional acceleration strategies

## Confidence
- **High confidence**: EKI's derivative-free and parallelizable nature enabling efficient DeepONet training; mini-batch strategy for scalability; demonstrated performance improvements over deterministic DeepONets
- **Medium confidence**: Effectiveness of heuristic Q adaptation; coverage results across different noise levels; comparison with existing UQ methods given limited direct benchmarking
- **Low confidence**: Generalization to highly nonlinear operators beyond presented examples; robustness to severe data scarcity; behavior with multimodal posterior distributions

## Next Checks
1. Benchmark against MCMC: Implement Hamiltonian Monte Carlo or NUTS for the anti-derivative problem and compare posterior uncertainty estimates and coverage to EKI
2. Vary ensemble size systematically: Test EKI with J ∈ {50, 100, 500, 1000} on all three examples to quantify the tradeoff between uncertainty quality and computational cost
3. Stress test with biased validation data: Intentionally corrupt or undersample the validation subset used for Q adaptation and measure degradation in uncertainty calibration and coverage