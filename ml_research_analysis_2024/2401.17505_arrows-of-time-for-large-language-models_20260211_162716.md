---
ver: rpa2
title: Arrows of Time for Large Language Models
arxiv_id: '2401.17505'
source_url: https://arxiv.org/abs/2401.17505
tags:
- arxiv
- language
- training
- time
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates time directionality in autoregressive
  language models, addressing whether predicting the next token differs from predicting
  the previous one. Through extensive experiments across 8 languages, 6 model architectures,
  and various hyperparameters, the authors discover a consistent "Arrow of Time" (AoT)
  effect: forward models (predicting next tokens) consistently outperform backward
  models (predicting previous tokens), with differences ranging from 0.1% to 3.2%
  in perplexity.'
---

# Arrows of Time for Large Language Models

## Quick Facts
- arXiv ID: 2401.17505
- Source URL: https://arxiv.org/abs/2401.17505
- Reference count: 40
- Primary result: Forward language models consistently outperform backward models by 0.1-3.2% in perplexity across 8 languages and 6 architectures

## Executive Summary
This paper investigates whether autoregressive language models exhibit time directionality - specifically whether predicting the next token differs from predicting the previous token. Through extensive experiments across multiple languages, model architectures, and hyperparameters, the authors discover a consistent "Arrow of Time" (AoT) effect where forward models outperform backward models by measurable margins. The effect is most pronounced with larger context windows and model sizes, suggesting it reflects fundamental properties of natural language rather than implementation artifacts. The authors provide both theoretical and empirical evidence that certain operations are inherently harder to learn in reverse, supporting the existence of an intrinsic linguistic asymmetry.

## Method Summary
The authors conducted controlled experiments comparing forward and backward language models across 8 languages (Arabic, Chinese, English, French, German, Hindi, Russian, Spanish), 6 architectures (GPT-2, GPT-Neo, GPT-J, Pythia, OPT, LLaMA), and various model sizes. They trained identical models on the same datasets with reversed tokenization for backward models. To validate their theoretical framework, they created synthetic datasets based on prime factorization and linear circuits to test whether certain operations are inherently harder to learn in reverse. The experiments systematically varied context window sizes and model parameters to understand how the Arrow of Time effect scales with model capacity and context length.

## Key Results
- Forward models consistently outperform backward models by 0.1-3.2% in perplexity across all tested conditions
- The performance gap increases with larger context windows and model sizes
- Synthetic experiments confirm that prime factorization and linear circuit operations are inherently harder to learn in reverse
- The effect persists across 8 languages and 6 different model architectures

## Why This Works (Mechanism)
The observed asymmetry stems from fundamental properties of natural language and computational complexity. The authors propose that natural languages may evolve to favor sparse forward mappings - where next-token predictions can be computed efficiently from current context - while the inverse mappings (predicting previous tokens) require dense representations that are computationally harder to learn. This relates to the mathematical property that sparse matrices have dense inverses, making reverse computation more complex. The theoretical framework suggests that the Arrow of Time effect reflects an inherent property of how information flows in language rather than being an artifact of training methodology.

## Foundational Learning
- **Autoregressive language modeling**: Predicting tokens sequentially in one direction - needed to understand the baseline approach and why direction matters
- **Sparsity and computational complexity**: Relationship between sparse matrices and their dense inverses - needed to understand the theoretical framework explaining the asymmetry
- **Prime factorization complexity**: Known computational hardness of factoring large numbers - used as a synthetic benchmark to validate reverse computation difficulty
- **Linear circuit theory**: Mathematical framework for analyzing computational paths - used to create controlled synthetic experiments
- **Perplexity metric**: Standard evaluation measure for language models - needed to quantify and compare model performance
- **Tokenization directionality**: How reversing tokenization affects model training - fundamental to the experimental design

## Architecture Onboarding

**Component Map**: Tokenization -> Embedding -> Transformer layers -> Prediction -> Loss computation

**Critical Path**: Input tokens → Tokenizer (forward/backward) → Embedding layer → Stacked transformer blocks → Final prediction layer → Cross-entropy loss

**Design Tradeoffs**: The authors chose to keep all architectural components identical between forward and backward models, varying only the tokenization direction and prediction objective. This isolates the directional effect but may miss potential architectural modifications that could mitigate the asymmetry.

**Failure Signatures**: If the Arrow of Time effect were an artifact of training methodology rather than fundamental, we would expect the performance gap to vary significantly with training hyperparameters or disappear with sufficient model capacity. The consistent pattern across architectures and languages suggests the effect is robust.

**First Experiments**:
1. Train forward and backward GPT-2 models on English Wikipedia with varying context window sizes (128, 256, 512, 1024)
2. Create synthetic dataset using prime factorization of consecutive integers and train models to predict factors
3. Test whether reversing the input order (rather than tokenization) produces the same effect

## Open Questions the Paper Calls Out
None

## Limitations
- The exact mechanism driving the observed asymmetry remains partially speculative despite theoretical justification
- Synthetic datasets (prime factorization, linear circuits) are highly structured mathematical problems that may not fully capture natural language complexity
- The interaction between hyperparameters (context window, model size) and the Arrow of Time effect could be more systematically explored

## Confidence
- **High confidence**: The empirical observation that forward models consistently outperform backward models across 8 languages, 6 architectures, and various hyperparameters is robust and well-supported
- **Medium confidence**: The theoretical explanation involving sparsity and computational complexity is plausible but requires more formal mathematical treatment
- **Medium confidence**: The conclusion that the effect strengthens with larger context windows and model sizes is supported but could benefit from more granular analysis

## Next Checks
1. Conduct ablation studies varying the training objective (e.g., pure causal language modeling vs. masked language modeling) to determine if the Arrow of Time effect persists across different pretraining paradigms
2. Test whether the asymmetry reverses when training on languages with different structural properties (e.g., languages with consistent verb-final word order) to validate the linguistic versus computational origin hypothesis
3. Perform controlled experiments with synthetic languages designed to have known forward/backward complexity ratios to establish clearer mappings between theoretical complexity measures and observed performance differences