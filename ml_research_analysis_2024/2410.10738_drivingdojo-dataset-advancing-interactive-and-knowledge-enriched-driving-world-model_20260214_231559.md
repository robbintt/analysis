---
ver: rpa2
title: 'DrivingDojo Dataset: Advancing Interactive and Knowledge-Enriched Driving
  World Model'
arxiv_id: '2410.10738'
source_url: https://arxiv.org/abs/2410.10738
tags:
- dataset
- driving
- world
- video
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DrivingDojo, a large-scale driving video
  dataset designed to advance interactive world models for autonomous driving. Unlike
  existing datasets focused on perception tasks, DrivingDojo features complete driving
  maneuvers, multi-agent interactions, and rare open-world scenarios.
---

# DrivingDojo Dataset: Advancing Interactive and Knowledge-Enriched Driving World Model

## Quick Facts
- arXiv ID: 2410.10738
- Source URL: https://arxiv.org/abs/2410.10738
- Authors: Yuqi Wang; Ke Cheng; Jiawei He; Qitai Wang; Hengchen Dai; Yuntao Chen; Fei Xia; Zhaoxiang Zhang
- Reference count: 40
- Key outcome: Models trained on DrivingDojo achieve FID 19.20, FVD 343.91, lateral error 0.062m, and longitudinal error 0.100m on action instruction following benchmark

## Executive Summary
DrivingDojo introduces a large-scale driving video dataset specifically designed to advance interactive world models for autonomous driving. Unlike existing datasets focused on perception tasks, DrivingDojo features complete driving maneuvers, multi-agent interactions, and rare open-world scenarios across 17.8k video clips. The dataset enables world models to generate action-controlled predictions and includes a new action instruction following benchmark to evaluate both visual quality and motion controllability. Experimental results demonstrate that models trained on DrivingDojo significantly outperform those trained on existing datasets like nuScenes or ONCE in both visual quality and action-following accuracy.

## Method Summary
The paper presents DrivingDojo, a driving video dataset for training interactive world models. The dataset consists of 17.8k video clips at 1920x1080 resolution and 5 fps, covering complete driving maneuvers (7.9k clips), multi-agent interactions (6.2k clips), and open-world knowledge (3.7k clips). The proposed action instruction following (AIF) benchmark evaluates world models using both visual quality metrics (FID, FVD) and action-following accuracy (lateral and longitudinal errors). The baseline model uses Stable Video Diffusion with action encoding through an MLP encoder that conditions the U-Net backbone. Models are fine-tuned on 16 NVIDIA A100 GPUs for 50K iterations, and evaluation is performed on the OpenDV-2K benchmark using COLMAP for trajectory estimation.

## Key Results
- Models trained on DrivingDojo achieve FID score of 19.20 and FVD score of 343.91 on OpenDV-2K
- Action instruction following accuracy shows lateral error of 0.062m and longitudinal error of 0.100m
- DrivingDojo-Action subset provides significantly more balanced and complete ego action distributions compared to existing datasets
- Models trained on DrivingDojo exhibit significantly improved action-following ability versus those trained on nuScenes or ONCE

## Why This Works (Mechanism)

### Mechanism 1
Dataset diversity directly improves world model action-following accuracy by providing richer, more complete action distributions and multi-agent interactions. This allows models to learn more realistic physical dynamics and interaction patterns, translating into better action-controlled video predictions. The core assumption is that the model's architecture can effectively utilize the increased diversity and the training signal is strong enough to learn from it.

### Mechanism 2
Text descriptions of rare events improve model's ability to simulate open-world scenarios by providing additional semantic information that can guide the model to generate more realistic simulations of these long-tail scenarios. The core assumption is that the model can effectively incorporate and utilize the text information during training.

### Mechanism 3
Action instruction following benchmark enables quantitative evaluation of world model capabilities by measuring both visual quality (FID, FVD) and action-following accuracy (lateral and longitudinal errors). This allows for more comprehensive evaluation beyond just visual quality. The core assumption is that AIF errors are a meaningful and reliable measure of a world model's ability to follow action instructions.

## Foundational Learning

- Concept: Diffusion models for video generation
  - Why needed here: The paper uses Stable Video Diffusion (SVD) as a baseline world model, which is based on diffusion models
  - Quick check question: What is the key difference between diffusion models and other generative models like GANs or VAEs?

- Concept: Action-conditioned video generation
  - Why needed here: The AIF benchmark requires generating videos conditioned on action instructions, which is a key capability of world models
  - Quick check question: How does action conditioning affect the training process and the generated videos compared to unconditioned video generation?

- Concept: Multi-agent interactions in driving scenarios
  - Why needed here: Understanding and simulating multi-agent interactions is crucial for realistic driving world models, and DrivingDojo specifically focuses on this aspect
  - Quick check question: What are some common types of multi-agent interactions in driving scenarios, and how can they be modeled?

## Architecture Onboarding

- Component map: Input (initial frame + action sequence) → Encoder (MLP) → Fusion (concatenate with first-frame image feature) → Backbone (U-Net) → Output (predicted future frames)
- Critical path: Input → Encoder → Fusion → U-Net → Output
- Design tradeoffs:
  - Resolution vs. frame count: High-resolution (1024x576) for 14 frames vs. low-resolution (576x320) for 30 frames
  - Fine-tuning vs. training from scratch: Fine-tuning SVD-XT checkpoint vs. training a new model
- Failure signatures:
  - Poor visual quality: High FID/FVD scores
  - Inaccurate action following: High AIF errors
  - Hallucinations: Unrealistic or inconsistent elements in generated videos
- First 3 experiments:
  1. Fine-tune SVD on DrivingDojo and evaluate visual quality on OpenDV-2K
  2. Train action-conditioned model on DrivingDojo and evaluate AIF performance
  3. Compare AIF performance of models trained on different datasets (DrivingDojo, nuScenes, ONCE)

## Open Questions the Paper Calls Out

### Open Question 1
How can driving world models be extended to handle longer-horizon predictions beyond the current short video generation capabilities? The paper acknowledges that "Our baseline model is only capable of generating short videos" and identifies "Longer predictions [4, 57, 22] and faster generation [38, 36] are left for future research." This remains unresolved as the paper does not provide any technical solutions or approaches for extending the model's temporal prediction capabilities.

### Open Question 2
What specific architectural modifications would enable world models to reduce hallucination artifacts like object disappearance and road non-existence? The paper shows examples of hallucinations in Figure 8, noting "We observed that the model exhibits some hallucinations, such as the sudden disappearance of objects, and when an action is unrealistic given the scene, such as forcefully turning right, the model sometimes imagines a new road." While the problem is identified, the paper does not propose or test any architectural solutions to address these specific hallucination issues.

### Open Question 3
How can the DrivingDojo dataset be effectively utilized to improve end-to-end autonomous driving policies, not just visual prediction? The paper states "While this work focuses on visual prediction in world models, future studies can investigate how this data improves driving policy" in the limitations section. This remains unresolved as the paper only explores the dataset's value for world model visual prediction, not for policy learning or planning applications.

## Limitations

- The baseline model is only capable of generating short videos, with longer predictions identified as future research
- The paper observed hallucination issues including sudden object disappearance and unrealistic road generation
- While focusing on visual prediction, the paper acknowledges that investigating how the data improves driving policy is left for future studies

## Confidence

- Medium confidence: Dataset diversity improves action-following accuracy (supported by significant FID/FVD and AIF error improvements)
- Medium confidence: AIF benchmark effectiveness (quantitative metrics provided but lacking extensive human validation)
- Weak confidence: Text descriptions impact (mentioned but limited experimental validation)
- Medium confidence: Reproducibility (key details provided but action encoding scheme not fully detailed)

## Next Checks

1. Conduct human studies to validate that AIF error metrics correlate with perceived action-following quality in generated videos
2. Systematically evaluate the impact of text descriptions for rare events by training models with and without this information
3. Perform ablation studies on the action encoding architecture to determine optimal conditioning strategies