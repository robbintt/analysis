---
ver: rpa2
title: Knowing When to Ask -- Bridging Large Language Models and Data
arxiv_id: '2409.13741'
source_url: https://arxiv.org/abs/2409.13741
tags:
- data
- commons
- what
- queries
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents an approach to reduce hallucinations in Large
  Language Models (LLMs) by integrating them with Data Commons, a large repository
  of public statistical data. Two methods are explored: Retrieval Interleaved Generation
  (RIG), where the LLM generates natural language queries to retrieve data, and Retrieval
  Augmented Generation (RAG), where relevant data tables are fetched and used to augment
  the LLM''s prompt.'
---

# Getting When to Ask -- Bridging Large Language Models and Data

## Quick Facts
- arXiv ID: 2409.13741
- Source URL: https://arxiv.org/abs/2409.13741
- Authors: Prashanth Radhakrishnan; Jennifer Chen; Bo Xu; Prem Ramaswami; Hannah Pho; Adriana Olmos; James Manyika; R. V. Guha
- Reference count: 18
- One-line primary result: RIG improved factual accuracy from 5-17% to 58% for statistical queries; RAG achieved 98.6% accuracy for statistical claims

## Executive Summary
This paper addresses the challenge of hallucinations in Large Language Models (LLMs) when handling statistical and numerical queries by integrating them with Data Commons, a large repository of public statistical data. The authors propose two complementary approaches: Retrieval Interleaved Generation (RIG), where the LLM generates natural language queries to retrieve data, and Retrieval Augmented Generation (RAG), where relevant data tables are fetched and used to augment the LLM's prompt. Both methods significantly improve factual accuracy compared to base models, with RAG showing particularly strong performance on statistical claims.

The paper presents a practical framework for combining LLMs with external data sources to enhance factual accuracy, demonstrating that even with relatively small training datasets (400-635 examples), substantial improvements can be achieved. The approaches show promise for applications requiring accurate statistical reasoning and data citation, with user preference studies indicating strong preference for responses that include cited statistics from Data Commons.

## Method Summary
The paper introduces two methods for reducing hallucinations in LLMs when answering statistical queries. RIG involves fine-tuning the LLM to produce natural language queries describing statistical values, which are then converted to structured queries for Data Commons retrieval. The LLM is trained to insert `__DC__()` annotations around statistical values, describing the metric, place, and date. RAG involves fine-tuning an LLM to generate Data Commons natural language queries based on user queries, retrieving relevant tables, and using a long-context LLM (Gemini 1.5 Pro) to generate the final response with the retrieved data included as context.

## Key Results
- RIG improved factual accuracy from 5-17% to approximately 58% for statistical queries
- RAG achieved 98.6% accuracy for statistical claims and 71.9% for inferences based on those claims
- User preference studies showed RIG answers were preferred 62-76% of the time
- RAG answers were preferred 92-100% when statistical data was included
- RIG showed 94.7% accuracy for Data Commons natural language queries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can be fine-tuned to generate natural language queries that describe statistical values in context, enabling accurate data retrieval from external sources.
- Mechanism: The model learns to insert `__DC__()` annotations around statistical values in its output, describing the metric, place, and date. These annotations are then parsed into structured queries for Data Commons.
- Core assumption: Fine-tuning on a small set of examples (400) is sufficient to teach the model to reliably identify and annotate statistical values in context.
- Evidence anchors:
  - [abstract] "RIG, where the LLM is trained to produce natural language queries to retrieve data from Data Commons"
  - [section] "Our approach is to fine-tune the LLM to produce a natural language query describing the LLM-SV"
  - [corpus] Weak - corpus contains no direct evidence about RIG's effectiveness or the fine-tuning approach
- Break condition: If the fine-tuning dataset is too small or unrepresentative, the model may fail to identify relevant statistics or generate incorrect queries.

### Mechanism 2
- Claim: Retrieval Augmented Generation (RAG) can improve factual accuracy by providing relevant data tables to a long-context LLM, enabling it to cite accurate statistics and draw inferences.
- Mechanism: A fine-tuned model generates Data Commons natural language queries based on the user's query. These queries are executed to retrieve relevant tables, which are then included in the prompt to a long-context LLM (Gemini 1.5 Pro) for final response generation.
- Core assumption: Long-context LLMs can effectively process and utilize large amounts of tabular data to generate accurate, well-supported responses.
- Evidence anchors:
  - [abstract] "RAG, where relevant data tables are fetched from Data Commons and used to augment the LLM's prompt"
  - [section] "We fine-tune an LLM to accept-as-input a user query, and produce-as-output a set of Data Commons natural language queries"
  - [corpus] Weak - corpus contains no direct evidence about RAG's effectiveness or the specific implementation
- Break condition: If the retrieved tables are too large or complex for the LLM to process effectively, or if the fine-tuned model fails to generate relevant queries, the RAG approach may not improve accuracy.

### Mechanism 3
- Claim: User preference studies show that responses with cited statistics from Data Commons are preferred over base model responses without citations.
- Mechanism: The RIG and RAG approaches produce responses that include statistics from Data Commons, either as footnotes (RIG) or as part of the main response (RAG). These responses are then compared to base model responses in user preference studies.
- Core assumption: Users prefer responses with cited statistics, even if the base model's statistics are occasionally incorrect.
- Evidence anchors:
  - [abstract] "User preference studies showed that RIG answers were preferred 62-76% of the time, while RAG answers were preferred 92-100% when statistical data was included"
  - [section] "Preference was higher for the 27B model (76%) compared to the 7B model (62%)"
  - [corpus] Weak - corpus contains no direct evidence about user preferences or the specific user studies
- Break condition: If users do not value the added complexity of cited statistics, or if the citations are presented in a confusing or distracting manner, user preferences may not favor the RIG and RAG approaches.

## Foundational Learning

- Concept: Fine-tuning LLMs for specific tasks
  - Why needed here: The base LLMs need to be adapted to generate Data Commons queries and handle statistical data effectively.
  - Quick check question: What are the key differences between instruction-tuning and fine-tuning for a specific task like generating Data Commons queries?

- Concept: Retrieval Augmented Generation (RAG)
  - Why needed here: RAG allows the model to access and utilize external data sources to improve factual accuracy and support complex inferences.
  - Quick check question: How does RAG differ from traditional fine-tuning approaches, and what are the key considerations for implementing RAG effectively?

- Concept: Natural Language Interfaces for Data Querying
  - Why needed here: The Data Commons NL interface allows users to query statistical data using natural language, which is then translated into structured queries for data retrieval.
  - Quick check question: What are the key challenges in designing a natural language interface for querying statistical data, and how can these challenges be addressed?

## Architecture Onboarding

- Component map: Base LLM -> Fine-tuned LLM for RIG/RAG -> Data Commons NL interface -> Structured data query processor -> Data Commons database -> User interface

- Critical path:
  1. User query is processed by fine-tuned LLM to generate Data Commons queries (RAG) or identify statistical values to annotate (RIG)
  2. Data Commons queries are executed to retrieve relevant data or tables
  3. Retrieved data is processed and formatted for inclusion in the final response
  4. Final response is generated by base LLM, incorporating the retrieved data
  5. Response is displayed to the user, with citations or annotations as appropriate

- Design tradeoffs:
  - Fine-tuning vs. in-context learning for generating Data Commons queries
  - RIG vs. RAG approach for incorporating external data
  - Citation format and presentation (footnotes vs. inline)
  - Data Commons coverage and availability vs. query complexity and diversity

- Failure signatures:
  - Incorrect or irrelevant Data Commons queries generated by fine-tuned LLM
  - Missing or incomplete data from Data Commons
  - Base LLM fails to effectively utilize retrieved data in final response
  - User interface fails to clearly present citations or annotations

- First 3 experiments:
  1. Evaluate the accuracy of Data Commons queries generated by fine-tuned LLM on a small set of test queries
  2. Compare the factual accuracy of RIG and RAG responses to base model responses on a set of statistical queries
  3. Conduct user preference studies to assess the perceived helpfulness and clarity of RIG and RAG responses vs. base model responses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RAG approaches scale with the size of the training dataset?
- Basis in paper: [inferred] The paper mentions that the current training sets are quite small (~600 max) and suggests that creating a larger fine-tuning dataset (100ks or millions) is future work. This implies that the current performance may be limited by dataset size.
- Why unresolved: The paper does not provide experimental results comparing RAG performance with different training dataset sizes. It only mentions the need for larger datasets as future work.
- What evidence would resolve it: Conducting experiments with RAG approaches using training datasets of varying sizes (e.g., 600, 10,000, 100,000, 1,000,000 examples) and comparing their performance on the same evaluation set would provide insights into how dataset size affects RAG performance.

### Open Question 2
- Question: How does the accuracy of RIG and RAG approaches compare when handling complex queries involving multiple variables and entities?
- Basis in paper: [explicit] The paper mentions that there are 22 complex queries in the evaluation set, including complex list queries, interesting queries, peer group queries, and drill-down queries. However, the results section does not provide a breakdown of accuracy for these complex query types.
- Why unresolved: While the paper provides overall accuracy metrics for RIG and RAG, it does not specifically analyze how these approaches perform on the most challenging query types mentioned.
- What evidence would resolve it: Analyzing the accuracy of RIG and RAG approaches separately for simple queries and complex queries (as defined in the paper) would reveal how well these methods handle different levels of query complexity.

### Open Question 3
- Question: What is the impact of Data Commons data coverage on the overall accuracy and usability of RIG and RAG approaches?
- Basis in paper: [explicit] The paper mentions that Data Commons has varying levels of data coverage across different regions and topics. It also states that the main reason for missing statistics in both RIG and RAG evaluations is that Data Commons does not have all the relevant datasets currently.
- Why unresolved: While the paper acknowledges the limitations of Data Commons data coverage, it does not provide a quantitative analysis of how this coverage affects the performance of RIG and RAG approaches.
- What evidence would resolve it: Conducting experiments where RIG and RAG approaches are evaluated on queries that cover the full range of Data Commons data coverage (from high coverage areas like US counties to low coverage areas like non-OECD countries) would reveal the relationship between data coverage and method performance.

## Limitations
- Limited generalizability beyond statistical domains, as evaluation focused specifically on statistical queries
- Small training datasets (400-635 examples) may not capture the full complexity of real-world queries
- Dependence on Data Commons coverage, which varies significantly across regions and topics
- Computational costs of fine-tuning and inference with long-context LLMs not thoroughly analyzed

## Confidence

High confidence: The factual accuracy improvements (from 5-17% to 58% for RIG, and 98.6% for RAG on statistical claims) are well-supported by the evaluation metrics and methodology. The user preference results showing 62-76% preference for RIG and 92-100% for RAG when statistical data is included are also robust.

Medium confidence: The scalability of these approaches to other domains beyond statistical data, and the long-term effectiveness of the fine-tuning approach given the small training dataset (400 examples for RIG, 635 for RAG).

Low confidence: The potential biases introduced by the Data Commons coverage limitations, and how these approaches would perform on queries requiring multi-hop reasoning or combining multiple data sources.

## Next Checks

1. **Domain Generalization Test**: Evaluate RIG and RAG approaches on a diverse set of 200 queries spanning domains beyond statistics (e.g., technical, medical, and legal domains) to assess whether the query generation and data retrieval mechanisms generalize effectively.

2. **Fine-tuning Dataset Size Study**: Systematically vary the size of the fine-tuning dataset (from 100 to 1000 examples) to determine the minimum effective training size and assess whether the current 400-635 examples are truly sufficient for robust performance.

3. **Long-context LLM Comparison**: Compare Gemini 1.5 Pro's performance in the RAG approach against other long-context models (GPT-4 Turbo with 128K context, Claude 3 with 200K context) using identical prompts and data tables to determine if the performance gains are model-specific or represent a general advantage of the RAG architecture.