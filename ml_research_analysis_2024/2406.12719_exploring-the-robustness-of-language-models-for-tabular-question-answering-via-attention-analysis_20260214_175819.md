---
ver: rpa2
title: Exploring the Robustness of Language Models for Tabular Question Answering
  via Attention Analysis
arxiv_id: '2406.12719'
source_url: https://arxiv.org/abs/2406.12719
tags:
- table
- data
- llms
- answer
- tabular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the robustness of Large Language Models
  (LLMs) on Tabular Question Answering (TQA) tasks under various augmentations and
  perturbations. The study evaluates how in-context learning, model scale, instruction
  tuning, and domain biases impact LLM performance on Wikipedia-based WTQ and financial
  TAT-QA datasets.
---

# Exploring the Robustness of Language Models for Tabular Question Answering via Attention Analysis

## Quick Facts
- arXiv ID: 2406.12719
- Source URL: https://arxiv.org/abs/2406.12719
- Authors: Kushal Raj Bhandari; Sixue Xing; Soham Dan; Jianxi Gao
- Reference count: 10
- Primary result: Language models show varying robustness to tabular question answering tasks, with performance degradation under structural and value-based perturbations.

## Executive Summary
This paper investigates the robustness of Large Language Models (LLMs) on Tabular Question Answering (TQA) tasks under various augmentations and perturbations. The study evaluates how in-context learning, model scale, instruction tuning, and domain biases impact LLM performance on Wikipedia-based WTQ and financial TAT-QA datasets. Key findings include: instruction tuning and larger models like Llama3 show improved robustness; structural and value-based perturbations significantly degrade performance, with sensitivity peaking in middle model layers; and LLMs exhibit domain bias towards Wikipedia data, as seen in the contrast between WTQ and TAT-QA results. Despite decent performance on WTQ, models struggle with real-world reliability and generalization. The authors highlight the need for structure-aware self-attention mechanisms and domain-adaptive processing techniques to enhance LLM robustness for tabular data comprehension.

## Method Summary
The study employs a systematic evaluation of LLM robustness across two main datasets: Wikipedia-based WTQ and financial TAT-QA. Researchers apply various augmentation and perturbation techniques to test model performance under different conditions. The evaluation examines the effects of in-context learning, model scale variations, instruction tuning, and domain-specific biases. Attention mechanisms are analyzed to understand how models process tabular information. Performance metrics track accuracy degradation under structural modifications (row/column permutations) and value-based changes. The methodology includes comparative analysis between different model sizes and training approaches to identify robustness factors.

## Key Results
- Instruction tuning and larger models like Llama3 demonstrate significantly improved robustness to tabular question answering tasks
- Structural and value-based perturbations cause substantial performance degradation, with middle model layers showing peak sensitivity
- LLMs exhibit clear domain bias, performing notably better on Wikipedia data (WTQ) compared to financial domain data (TAT-QA)
- Despite reasonable WTQ performance, models struggle with real-world reliability and generalization across diverse tabular domains

## Why This Works (Mechanism)
The study's findings demonstrate that LLM robustness in tabular question answering depends on multiple interacting factors: model architecture, training methodology, and data domain characteristics. Instruction tuning appears to provide better structural understanding of tabular data, while larger models can maintain more context during processing. The attention mechanism analysis reveals that middle layers are particularly sensitive to perturbations, suggesting these layers play a crucial role in integrating tabular structure with question semantics. Domain bias emerges from the models' training data distribution, with Wikipedia-style tables being more prevalent than specialized financial tables in pretraining corpora.

## Foundational Learning
1. **Tabular Question Answering (TQA)**: The task of answering questions based on structured tabular data using language models
   - Why needed: Forms the core problem domain being studied
   - Quick check: Can the model accurately answer questions about simple tables?

2. **Attention Mechanisms**: The mathematical framework allowing models to focus on relevant parts of input when making predictions
   - Why needed: Central to understanding how models process tabular structures
- Quick check: Does attention weight distribution change under perturbations?

3. **Instruction Tuning**: The process of fine-tuning models on instruction-following datasets to improve task generalization
   - Why needed: Shown to significantly impact robustness in the study
   - Quick check: Does instruction-tuned model perform better on out-of-domain tasks?

4. **In-Context Learning**: The ability of models to perform tasks using only examples provided in the prompt, without parameter updates
   - Why needed: Key factor affecting model performance on TQA tasks
   - Quick check: How many examples are needed for optimal performance?

5. **Domain Bias**: The tendency of models to perform better on data from domains similar to their training distribution
   - Why needed: Explains performance differences between WTQ and TAT-QA
   - Quick check: Does pretraining data distribution correlate with downstream performance?

6. **Perturbation Analysis**: The systematic study of model behavior under controlled modifications to input data
   - Why needed: Reveals model robustness and failure modes
   - Quick check: Which perturbation types cause the most severe performance drops?

## Architecture Onboarding

Component Map:
Input Tables + Questions -> Preprocessing -> Tokenization -> LLM Architecture (Encoder/Decoder) -> Attention Layers -> Output Layer -> Answer Prediction

Critical Path:
The critical path flows through tokenization of tabular data, self-attention computation in middle layers, and final answer generation. Middle layers show highest sensitivity to perturbations, making them the most critical component for robustness.

Design Tradeoffs:
The study reveals tradeoffs between model size and robustness, instruction tuning benefits versus computational cost, and specialization for specific domains versus generalization across tabular data. Larger models with instruction tuning show better robustness but require more resources.

Failure Signatures:
Performance degradation under structural perturbations (row/column permutations), value-based changes, and domain shifts. Middle layers show highest sensitivity, with attention patterns becoming less focused under perturbations.

First Experiments:
1. Compare performance of base vs. instruction-tuned models on both WTQ and TAT-QA datasets
2. Apply systematic row and column permutations to test structural robustness
3. Analyze attention weight distributions across layers under normal and perturbed conditions

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis focuses primarily on two datasets (WTQ and TAT-QA), potentially limiting generalizability to other tabular domains
- Study examines relatively small sample of model variations, constraining broader applicability of findings
- Attention-based analysis, while insightful, doesn't fully explore alternative attention mechanisms for tabular structures
- Limited investigation of the relationship between pretraining data distribution and domain-specific performance

## Confidence
High: The consistent performance gap between instruction-tuned and non-tuned models; the observed degradation in performance under structural and value-based perturbations; the domain bias effects between Wikipedia and financial data

Medium: Layer-specific sensitivity findings, as these could vary with different model architectures; the generalizability of perturbation impact across diverse tabular domains

## Next Checks
1. Replicate the study using additional tabular datasets from varied domains (scientific, business, healthcare) to test domain generalization
2. Implement and test structure-aware attention mechanisms as suggested, comparing against baseline self-attention
3. Conduct ablation studies removing instruction tuning from larger models to isolate its specific contribution to robustness