---
ver: rpa2
title: Improving Clinical NLP Performance through Language Model-Generated Synthetic
  Clinical Data
arxiv_id: '2403.19511'
source_url: https://arxiv.org/abs/2403.19511
tags:
- synthetic
- data
- clinical
- tasks
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study demonstrates that large language models can generate
  high-quality synthetic clinical text data to improve downstream clinical NLP task
  performance, achieving results competitive with or exceeding those obtained from
  expert-annotated gold-standard datasets. The approach uses a two-step process: synthetic
  data generation followed by label correction via fine-tuned models, with performance
  evaluated across three DR.BENCH benchmark tasks and a real-world esophagitis classification
  task.'
---

# Improving Clinical NLP Performance through Language Model-Generated Synthetic Clinical Data

## Quick Facts
- **arXiv ID**: 2403.19511
- **Source URL**: https://arxiv.org/abs/2403.19511
- **Reference count**: 0
- **Key outcome**: Synthetic clinical text data generated by large language models can improve downstream clinical NLP task performance to match or exceed expert-annotated gold-standard datasets.

## Executive Summary
This study demonstrates that large language models can generate high-quality synthetic clinical text data to improve downstream clinical NLP task performance, achieving results competitive with or exceeding those obtained from expert-annotated gold-standard datasets. The approach uses a two-step process: synthetic data generation followed by label correction via fine-tuned models, with performance evaluated across three DR.BENCH benchmark tasks and a real-world esophagitis classification task. Results show that synthetic data alone without label correction leads to significant performance drops, but with label correction, augmentation with synthetic data consistently matches or surpasses gold-only performance. Notably, in the real-world task, models trained on synthetic data from just 200 gold-labeled notes achieved comparable results to those trained on 1,243 gold-labeled notes, highlighting the potential to reduce annotation requirements while maintaining performance.

## Method Summary
The study employs a two-stage pipeline for synthetic data augmentation in clinical NLP. First, GPT-3.5-turbo generates synthetic clinical notes by prompting it to create clinical documents based on available gold-labeled data. Second, a label correction step uses a fine-tuned base clinical BERT model to correct the labels on the synthetic data, addressing potential inaccuracies from the generative model. The augmented datasets (synthetic plus gold) are then used to train target clinical NLP models for downstream tasks. Performance is evaluated using standard metrics (accuracy, precision, recall, F1) across three DR.BENCH benchmark tasks (medication, obesity, fall risk) and one real-world esophagitis classification task.

## Key Results
- Models trained on synthetic data alone without label correction showed significant performance drops compared to gold-only models
- With label correction, augmented models consistently matched or exceeded gold-only performance across all evaluated tasks
- In the real-world esophagitis task, models trained on synthetic data from only 200 gold-labeled notes achieved comparable performance to models trained on 1,243 gold-labeled notes

## Why This Works (Mechanism)
The approach leverages large language models' ability to generate clinically coherent text that captures domain-specific patterns, while the label correction step ensures accuracy by using a fine-tuned model to validate and correct synthetic data labels. This two-stage process addresses the key challenge that while LLMs can generate syntactically and semantically plausible clinical text, they may introduce errors or inconsistencies in the labels. The correction step acts as a quality filter, maintaining high annotation standards while dramatically expanding the training dataset size.

## Foundational Learning
- **Synthetic data generation**: Creating artificial clinical text that mimics real patient documentation patterns; needed to expand training data beyond limited gold-labeled datasets; quick check: evaluate generated text for clinical coherence and domain accuracy
- **Label correction via fine-tuning**: Using a base clinical BERT model to validate and correct synthetic data labels; needed to maintain annotation quality when scaling dataset size; quick check: measure correction accuracy on a held-out validation set
- **Two-stage augmentation pipeline**: Combining synthetic generation with correction before training downstream models; needed to balance quantity with quality in training data; quick check: compare performance with and without correction step
- **DR.BENCH benchmark**: Standardized evaluation framework for clinical NLP tasks; needed to enable fair comparison across studies; quick check: verify task definitions and evaluation metrics match published standards
- **Gold-to-synthetic ratio optimization**: Determining optimal balance between real and synthetic data; needed to maximize performance while minimizing annotation burden; quick check: conduct ablation studies across different ratios
- **Task-specific evaluation**: Measuring performance on both benchmark and real-world clinical tasks; needed to demonstrate practical applicability beyond controlled experiments; quick check: ensure evaluation metrics align with clinical use cases

## Architecture Onboarding

**Component map**: Clinical notes -> GPT-3.5-turbo (synthetic generation) -> BERT fine-tuning (label correction) -> Augmented dataset -> Downstream NLP model

**Critical path**: The most critical components are the synthetic data generation and label correction steps. The quality of generated text directly impacts downstream model performance, while the correction step ensures that label errors don't propagate through the training pipeline.

**Design tradeoffs**: The approach trades computational cost for annotation savings. Generating large volumes of synthetic data and running correction requires significant compute resources, but reduces the need for expensive expert annotation. The choice of GPT-3.5-turbo versus more recent models represents another tradeoff between generation quality and accessibility.

**Failure signatures**: Poor synthetic data quality manifests as hallucinations or clinically implausible content. Inadequate label correction shows up as degraded performance compared to gold-only models, even with large synthetic datasets. Over-reliance on synthetic data without sufficient gold labels typically causes performance to plateau or decline.

**First experiments**:
1. Generate synthetic data from a small gold dataset and evaluate without correction to establish baseline degradation
2. Apply label correction to the same synthetic data and measure performance improvement
3. Conduct ablation study varying the gold-to-synthetic ratio to identify optimal augmentation strategy

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to specific DR.BENCH tasks and one real-world application, potentially limiting generalizability
- GPT-3.5-turbo was chosen without systematic comparison to alternative models
- Label correction relies on fine-tuned BERT, which may not be optimal compared to more recent methods
- Potential biases in synthetic generation and their impact on model performance for underrepresented populations were not addressed
- Computational costs and practical deployment feasibility were not discussed

## Confidence
- **High confidence**: The core finding that synthetic data alone degrades performance without label correction
- **Medium confidence**: The claim that augmented models match or exceed gold-only performance across all evaluated tasks
- **Medium confidence**: The assertion that synthetic data can significantly reduce annotation requirements while maintaining performance

## Next Checks
1. Evaluate the approach across a broader range of clinical NLP tasks, including those involving rare conditions and diverse patient populations
2. Conduct ablation studies comparing different base models for synthetic generation and label correction methods
3. Perform cost-benefit analysis comparing annotation savings against computational overhead for synthetic data generation and correction pipelines