---
ver: rpa2
title: Coreset Selection for Object Detection
arxiv_id: '2404.09161'
source_url: https://arxiv.org/abs/2404.09161
tags: []
core_contribution: CSOD addresses coreset selection for object detection, where traditional
  methods fail due to the presence of multiple objects per image. It generates imagewise-classwise
  representative feature vectors for each class in an image and applies submodular
  optimization to select a subset that balances representativeness and diversity.
---

# Coreset Selection for Object Detection

## Quick Facts
- arXiv ID: 2404.09161
- Source URL: https://arxiv.org/abs/2404.09161
- Authors: Hojun Lee; Suyoung Kim; Junhoo Lee; Jaeyoung Yoo; Nojun Kwak
- Reference count: 40
- Primary result: CSOD achieves +6.4% AP50 improvement over random selection when choosing 200 images from Pascal VOC

## Executive Summary
CSOD addresses coreset selection for object detection, where traditional methods fail due to the presence of multiple objects per image. It generates imagewise-classwise representative feature vectors for each class in an image and applies submodular optimization to select a subset that balances representativeness and diversity. Evaluated on Pascal VOC, CSOD achieved +6.4% AP50 improvement over random selection when choosing 200 images. The method was further validated on BDD100k and COCO2017 datasets, consistently outperforming random selection.

## Method Summary
CSOD generates imagewise-classwise representative feature vectors by averaging RoI features per class within each image, then applies submodular optimization to select a subset that balances representativeness and diversity. The method uses a greedy class-sequential selection strategy, where images are selected one by one for each class in order, ensuring coverage across all classes. The submodular score function combines cosine similarity to select highly representative features and negative cosine similarity to avoid redundancy with already selected features, controlled by hyperparameter λ.

## Key Results
- CSOD achieved +6.4% AP50 improvement over random selection when choosing 200 images from Pascal VOC
- Consistent performance gains across BDD100k and COCO2017 datasets
- Averaging approach proved more effective than using individual object features
- Method adapts well across different network architectures

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Imagewise-classwise averaging reduces the dimensionality of the selection problem while preserving class-level representativeness.
- **Mechanism**: Instead of selecting individual object instances, the method averages RoI features of the same class within each image, creating a single prototype per class per image. This reduces the combinatorial complexity of multi-object images.
- **Core assumption**: Averaging preserves sufficient information about class distribution while smoothing over intra-class variability.
- **Evidence anchors**:
  - [abstract]: "CSOD generates imagewise and classwise representative feature vectors for multiple objects of the same class within each image."
  - [section]: "the averaging approach is more effective than using individual object features"
- **Break condition**: If intra-class variability is too high within a single image, averaging may erase discriminative information, causing loss of representativeness.

### Mechanism 2
- **Claim**: Submodular optimization balances representativeness and diversity in a greedy, class-sequential manner.
- **Mechanism**: The score function uses cosine similarity to both select highly representative features (first term) and avoid redundancy with already selected features (second term), controlled by λ.
- **Core assumption**: Greedy selection by class, while not globally optimal, ensures coverage across all classes and maintains diversity.
- **Evidence anchors**:
  - [abstract]: "utilize the representative vectors in the submodular optimization process to select a subset"
  - [section]: "we introduce a mathematical tool known as a 'submodular function'"
- **Break condition**: If λ is poorly tuned, the method may overfit to either representativeness (high λ) or diversity (low λ), harming overall performance.

### Mechanism 3
- **Claim**: Selecting images instead of individual annotations preserves spatial and contextual information needed for object detection.
- **Mechanism**: The method focuses on selecting whole images, implicitly ensuring that context, scale, and spatial relationships are retained, which is crucial for detection.
- **Core assumption**: For detection, the image-level context is as important as individual object annotations.
- **Evidence anchors**:
  - [abstract]: "Our method, CSOD, is built upon a unique concept: the 'imagewise-classwise vector.'"
  - [section]: "We have proposed a Coreset selection method for Object Detection tasks, addressing the unique challenges presented by multi-object and multi-label scenarios."
- **Break condition**: If object density or diversity is extremely low in selected images, the retained context may not compensate for missing annotations.

## Foundational Learning

- **Concept**: RoI pooling and feature extraction
  - **Why needed here**: The method depends on extracting per-object features from ground truth boxes using RoI pooling.
  - **Quick check question**: How does RoI pooling handle variable-sized objects and what is the dimensionality of the resulting feature vector?

- **Concept**: Submodular optimization and greedy algorithms
  - **Why needed here**: Coreset selection is framed as maximizing a submodular set function with a greedy algorithm.
  - **Quick check question**: What is the formal definition of a submodular function and why does greedy selection provide a constant-factor approximation?

- **Concept**: Object detection pipeline (e.g., Faster R-CNN)
  - **Why needed here**: The method is tightly integrated with detection architecture, relying on backbone features and RoI processing.
  - **Quick check question**: What is the difference between ground truth RoI features and RPN-proposed RoI features, and why does CSOD use ground truth?

## Architecture Onboarding

- **Component map**: Backbone (ResNet50) -> Feature map -> RoI pooler -> RoI feature extraction from ground truth -> Global average pooling -> Compact RoI feature vectors -> Averaging step -> Imagewise-classwise vectors -> Submodular score computation -> Selection -> Greedy class-sequential selection -> Coreset construction

- **Critical path**:
  1. Extract RoI features for all ground truth objects
  2. Average per class per image
  3. Compute submodular scores
  4. Greedily select images by class order
  5. Train detector on selected subset

- **Design tradeoffs**:
  - Image selection vs. annotation selection: reduces computational overhead but may lose fine-grained annotation diversity
  - Averaging vs. keeping all objects: reduces dimensionality but risks information loss
  - Class-sequential greedy selection vs. joint selection: ensures coverage but ignores cross-class correlations

- **Failure signatures**:
  - Degraded performance when selected images lack object diversity
  - Sensitivity to λ hyperparameter
  - Instability in datasets with severe class imbalance
  - Reduced effectiveness when object instances are highly variable within an image

- **First 3 experiments**:
  1. Verify RoI feature extraction and averaging by visualizing imagewise-classwise vectors
  2. Tune λ on a small subset to observe trade-off between representativeness and diversity
  3. Compare image selection vs. random annotation selection on AP50 to validate core hypothesis

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does CSOD's averaging approach for multi-object images scale to datasets with more extreme class imbalance than Pascal VOC?
- **Basis in paper**: [explicit] The paper mentions that VOC has "relatively less severe class imbalance compared to other datasets" and that CSOD performed similarly across different λ values, suggesting class imbalance might not significantly affect results.
- **Why unresolved**: The paper only tested on VOC (relatively balanced) and BDD100k/COCO (larger but not explicitly analyzed for imbalance effects). No experiments systematically varied class imbalance to test CSOD's robustness.
- **What evidence would resolve it**: Experiments on highly imbalanced datasets (e.g., long-tail distributions) comparing CSOD with class-balanced variants or showing performance degradation with increasing imbalance.

### Open Question 2
- **Question**: How does CSOD's performance compare when using gradients instead of RoI features for coreset selection?
- **Basis in paper**: [explicit] Section S7 reports an experiment comparing RoI features (AP50=43.5) vs gradients (AP50=42.3) for 200-image selection, finding gradients slightly worse but still better than random.
- **Why unresolved**: The paper only tested one λ value (0.05) and one dataset (VOC). It's unclear if gradients could outperform features with different hyperparameters or on other datasets.
- **What evidence would resolve it**: Systematic comparison of RoI features vs gradients across multiple datasets, λ values, and network architectures to determine conditions where each approach excels.

### Open Question 3
- **Question**: Would incorporating background features improve CSOD's coreset selection for object detection?
- **Basis in paper**: [inferred] The paper explicitly states "we did not explicitly incorporate background features, which could provide additional context and potentially enhance coreset selection" as a limitation.
- **Why unresolved**: The authors acknowledge this as a limitation but didn't conduct experiments to test whether background features would actually help or hurt performance.
- **What evidence would resolve it**: Experiments comparing CSOD with and without background features, measuring impact on AP50 and other detection metrics across multiple datasets.

## Limitations
- Reliance on averaging RoI features may lose discriminative information when intra-class variability is high within a single image
- Effectiveness of class-sequential greedy selection strategy uncertain in scenarios with severe class imbalance
- Performance may degrade when object density or diversity is extremely low in selected images

## Confidence
- Mechanism 1: High
- Mechanism 2: High
- Mechanism 3: Medium

## Next Checks
1. Analyze intra-class variability: Examine the variance of RoI features within each class per image to assess whether averaging preserves sufficient discriminative information.
2. Tune λ hyperparameter: Conduct a sensitivity analysis of λ on a small subset to observe the trade-off between representativeness and diversity, and determine optimal values for different dataset sizes.
3. Compare with annotation selection: Evaluate the performance of CSOD against a random annotation selection baseline on AP50 to validate the core hypothesis that image selection preserves context and scale information.