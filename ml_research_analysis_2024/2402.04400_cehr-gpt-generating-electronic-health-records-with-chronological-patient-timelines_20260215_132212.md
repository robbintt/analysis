---
ver: rpa2
title: 'CEHR-GPT: Generating Electronic Health Records with Chronological Patient
  Timelines'
arxiv_id: '2402.04400'
source_url: https://arxiv.org/abs/2402.04400
tags: []
core_contribution: This work introduces CEHR-GPT, a framework for generating synthetic
  Electronic Health Records (EHR) with preserved chronological patient timelines.
  By extending the CEHR-BERT patient representation, CEHR-GPT incorporates demographic
  details, visit types, discharge facilities, and temporal intervals between visits
  and inpatient durations.
---

# CEHR-GPT: Generating Electronic Health Records with Chronological Patient Timelines

## Quick Facts
- arXiv ID: 2402.04400
- Source URL: https://arxiv.org/abs/2402.04400
- Reference count: 31
- Primary result: CEHR-GPT generates synthetic EHR data preserving chronological timelines with preserved ML utility and privacy

## Executive Summary
CEHR-GPT introduces a framework for generating synthetic Electronic Health Records that preserves complete chronological patient timelines. The approach extends CEHR-BERT patient representations by incorporating demographic details, visit types, discharge facilities, and temporal intervals between visits. Using Generative Pre-trained Transformers, the framework learns patient sequence distributions and generates synthetic data that matches real EHR statistical properties while maintaining patient privacy. Evaluations demonstrate strong performance across distribution preservation, machine learning utility, and privacy metrics.

## Method Summary
CEHR-GPT converts OMOP-format EHR data into patient sequences using a specialized patient representation that encodes temporal information through artificial time tokens. A GPT model with standard transformer decoders learns these sequences using Next Word Prediction objectives. The trained model generates synthetic patient sequences, which are then converted back to OMOP format. The framework is evaluated through distribution comparisons, co-occurrence analysis, and machine learning model performance benchmarks.

## Key Results
- Successfully replicates marginal and conditional distributions of real EHR data
- Preserves machine learning utility across predictive tasks
- Demonstrates no significant membership or attribute inference privacy risks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CEHR-GPT preserves complete patient timelines by encoding temporal information into a sequence of tokens.
- Mechanism: The patient representation uses artificial time tokens (ATT) and inpatient ATT tokens (IATT) to mark intervals between visits and inpatient spans, respectively. This allows GPT to learn the temporal distribution of events while maintaining their chronological order.
- Core assumption: Temporal tokens can fully capture the distribution of time intervals and inpatient durations without losing information.

### Mechanism 2
- Claim: CEHR-GPT effectively replicates the marginal and conditional distributions of real EHR data while preserving machine learning utility.
- Mechanism: By treating patient sequence generation as a language modeling problem, GPT learns the joint distribution of concepts and their temporal dependencies, enabling the generation of synthetic sequences that match the statistical properties of real data.
- Core assumption: The GPT model can learn the joint distribution of concepts and their temporal dependencies from the encoded patient sequences.

### Mechanism 3
- Claim: CEHR-GPT shows no significant membership or attribute inference risks, preserving patient privacy.
- Mechanism: The synthetic data generated by CEHR-GPT does not expose sensitive information about real patients, as demonstrated by privacy evaluations using membership and attribute inference attacks.
- Core assumption: The generative model can create synthetic data that is statistically similar to real data without exposing individual patient information.

## Foundational Learning

- Concept: Generative Pre-trained Transformers (GPT)
  - Why needed here: GPT is used to learn the distribution of patient sequences and generate synthetic EHR data.
  - Quick check question: What is the key difference between GPT and other generative models like GANs in the context of EHR data generation?

- Concept: Patient Representation
  - Why needed here: A well-designed patient representation is crucial for encoding temporal information and medical events into a format that can be processed by GPT.
  - Quick check question: How does the CEHR-GPT patient representation differ from traditional bag-of-words approaches in EHR data modeling?

- Concept: Observational Medical Outcomes Partnership (OMOP) Common Data Model
  - Why needed here: The OMOP format is used for easy dissemination and analysis of synthetic EHR data, ensuring compatibility with existing healthcare research tools.
  - Quick check question: Why is it important to convert synthetic patient sequences back to the OMOP format?

## Architecture Onboarding

- Component map:
  - OMOP Encoder: Converts real EHR data from OMOP format to patient sequences
  - GPT Model: Learns the distribution of patient sequences and generates synthetic sequences
  - OMOP Decoder: Converts synthetic patient sequences back to OMOP format
  - Evaluation Framework: Assesses the quality and utility of synthetic data

- Critical path:
  1. Encode real EHR data using OMOP Encoder
  2. Train GPT model on encoded patient sequences
  3. Generate synthetic patient sequences using trained GPT model
  4. Decode synthetic sequences back to OMOP format
  5. Evaluate synthetic data using the Evaluation Framework

- Design tradeoffs:
  - Context window size vs. computational efficiency: Larger context windows allow for longer patient histories but increase computational requirements
  - Sampling strategy (top-k vs. top-p) vs. data quality: Different sampling strategies can affect the diversity and realism of generated synthetic data

- Failure signatures:
  - Loss of temporal information: If the patient representation does not accurately encode time intervals, the synthetic data will fail to preserve patient timelines
  - Privacy leaks: If the generative model leaks information about real patients, the synthetic data will pose a privacy risk
  - Poor machine learning utility: If the synthetic data does not replicate the statistical properties of real EHR data, it will not be useful for training machine learning models

- First 3 experiments:
  1. Encode a small subset of real EHR data using the OMOP Encoder and verify the patient representation
  2. Train the GPT model on the encoded patient sequences and evaluate its ability to generate realistic synthetic sequences
  3. Decode the synthetic sequences back to OMOP format and assess the quality of the synthetic data using the Evaluation Framework

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CEHR-GPT's performance compare to GAN-based methods for synthetic EHR generation when evaluated on temporal aspects like patient timeline accuracy?
- Basis in paper: [inferred] The paper claims CEHR-GPT outperforms GANs in capturing temporal dependencies, but does not provide direct comparisons to state-of-the-art GAN methods on the same datasets and evaluation metrics.
- Why unresolved: The paper only compares against GPT variants with different patient representations, not against contemporary GAN-based methods that also aim to capture temporal aspects.
- What evidence would resolve it: Direct experimental comparison of CEHR-GPT and leading GAN-based methods (e.g., DAAE, EHR-M-GAN, SynTEG) on identical datasets using temporal evaluation metrics like timeline reconstruction accuracy.

### Open Question 2
- Question: What is the optimal patient representation balance between preserving temporal information and generating realistic synthetic data across different EHR use cases?
- Basis in paper: [explicit] The paper notes trade-offs between temporal information preservation (measured by LOTI) and predictive performance across different cohorts, suggesting different patient representations may be optimal for different tasks.
- Why unresolved: The paper tests several representations but doesn't systematically explore the design space of patient representations or provide guidelines for selecting representations based on intended use cases.
- What evidence would resolve it: A comprehensive study testing various patient representation designs across a wide range of EHR tasks, establishing which temporal aspects are most critical for different predictive tasks.

### Open Question 3
- Question: How can the over-representation of prevalent concepts in synthetic EHR data be effectively mitigated without compromising temporal dependencies?
- Basis in paper: [explicit] The paper identifies that GPT models tend to over-represent prevalent concepts and suggests adaptive regularization as a potential solution, but does not implement or validate this approach.
- Why unresolved: The proposed adaptive regularization technique is only described conceptually without experimental validation or comparison to alternative methods.
- What evidence would resolve it: Implementation and validation of adaptive regularization techniques, comparison with other debiasing approaches (e.g., frequency-based sampling, concept-specific temperature scaling), and evaluation of their impact on both temporal accuracy and concept distribution fidelity.

## Limitations
- Evaluation relies heavily on statistical comparisons rather than direct clinical validation
- Privacy assessment does not explicitly address sophisticated attacks like reconstruction attacks
- Limited evidence regarding scalability to larger datasets and diverse healthcare settings

## Confidence
**High Confidence Claims:**
- CEHR-GPT framework successfully generates synthetic EHR data preserving chronological timelines
- Framework effectively replicates marginal and conditional distributions of real EHR data
- Privacy assessments show no significant membership or attribute inference risks

**Medium Confidence Claims:**
- Machine learning utility is preserved when using synthetic data for training models
- OMOP format conversion process maintains data integrity and usability
- Patient representation adequately captures temporal dependencies

**Low Confidence Claims:**
- Framework's scalability to larger datasets and diverse healthcare settings
- Long-term stability of synthetic data quality across different time periods
- Generalizability of results to other healthcare systems beyond evaluated dataset

## Next Checks
1. **Clinical Pattern Validation**: Conduct systematic review of synthetic timelines by domain experts to identify whether clinically meaningful patterns and temporal dependencies are preserved beyond statistical measures.

2. **Robust Privacy Assessment**: Implement additional privacy evaluation methods including reconstruction attacks and membership inference against the generative model to comprehensively assess privacy guarantees.

3. **Cross-Institutional Generalization**: Test framework's performance on EHR data from multiple healthcare institutions to evaluate generalizability and robustness across different data distributions and medical practices.