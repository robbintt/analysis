---
ver: rpa2
title: 'From Complex to Simple: Enhancing Multi-Constraint Complex Instruction Following
  Ability of Large Language Models'
arxiv_id: '2404.15846'
source_url: https://arxiv.org/abs/2404.15846
tags:
- instructions
- constraints
- training
- data
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies how to enhance the ability of LLMs to follow
  complex instructions with multiple constraints. The authors conduct empirical studies
  to identify effective training data, finding that training with compositional data
  (instructions containing multiple constraints) enhances understanding of complex
  instructions, especially for lower complexity levels, and can generalize to out-of-domain
  constraints.
---

# From Complex to Simple: Enhancing Multi-Constraint Complex Instruction Following Ability of Large Language Models

## Quick Facts
- arXiv ID: 2404.15846
- Source URL: https://arxiv.org/abs/2404.15846
- Reference count: 18
- Primary result: Training with compositional data and using discrimination-based methods improves LLM performance on multi-constraint instruction following

## Executive Summary
This work addresses the challenge of enhancing Large Language Models' (LLMs) ability to follow complex instructions with multiple constraints. Through empirical studies, the authors identify that training with compositional data containing multiple constraints significantly improves understanding of complex instructions, particularly for lower complexity levels, and enables better generalization to out-of-domain constraints. The paper proposes a discrimination-based method for obtaining high-quality training data by refining student model outputs using a teacher model, and introduces a contrastive learning approach using reinforcement learning to effectively utilize both positive and negative samples. Experimental results demonstrate improved performance, training efficiency, and generalization across various settings while maintaining general capabilities.

## Method Summary
The authors propose a multi-faceted approach to enhance LLM instruction following ability. First, they generate training data by composing instructions with multiple constraints from seed instructions. Second, they implement a discrimination-based data synthesis method where a weaker student model generates initial outputs that are then refined by a stronger teacher model to correct failed constraints. Third, they introduce a contrastive training framework using reinforcement learning fine-tuning (RLFT) that leverages both positive samples (corrected outputs) and negative samples (initial failed outputs) through Direct Preference Optimization (DPO). This approach is evaluated across different model sizes (7B, 13B) and compared against baseline methods including standard supervised fine-tuning and atom/composition training strategies.

## Key Results
- Training with compositional data significantly improves performance on multi-constraint instructions, especially for lower complexity levels
- Discrimination-based data synthesis (student generation + teacher refinement) outperforms direct generation by advanced models
- Contrastive learning with both positive and negative samples achieves better performance with the same training steps compared to discrimination-only approaches
- The proposed method generalizes well to out-of-domain constraints while maintaining general capabilities

## Why This Works (Mechanism)

### Mechanism 1
Training LLMs with compositional data (instructions containing multiple constraints) enhances understanding of complex instructions, especially for lower complexity levels. When models are trained on instructions with multiple constraints, they learn to recognize and satisfy compositional patterns. This improves their ability to generalize to simpler instructions with fewer constraints because they develop a more robust understanding of how constraints interact. Core assumption: The compositional nature of constraints allows models to learn transferable patterns that apply across different complexity levels.

### Mechanism 2
The discrimination-based method for obtaining training data produces higher quality outputs than direct generation. By first generating outputs with a weaker student model and then correcting specific failed constraints with a stronger teacher model, the approach captures subtle instruction variations more effectively than generating directly with the advanced model. Core assumption: Small changes in instruction wording can cause substantial output differences, and iterative correction better captures these nuances than single-pass generation.

### Mechanism 3
Using both positive and negative samples through contrastive learning improves training efficiency and generalization. By training on contrastive triplets where positive samples (corrected outputs) are preferred over negative samples (initial outputs that failed constraints), the model learns more discriminative representations of constraint satisfaction, leading to better performance with fewer training steps. Core assumption: Negative samples provide valuable supervision signals that help the model understand the boundary between correct and incorrect constraint satisfaction.

## Foundational Learning

- Concept: Multi-constraint instruction following
  - Why needed here: The paper's core focus is on improving LLM ability to follow instructions with multiple constraints simultaneously, which is critical for real-world applications.
  - Quick check question: What distinguishes multi-constraint instruction following from single-constraint instruction following?

- Concept: Compositional data vs atomic data
  - Why needed here: Understanding the difference between training on instructions with multiple constraints (compositional) versus single constraints (atomic) is fundamental to the paper's empirical findings.
  - Quick check question: Why might training on compositional data improve performance on instructions with fewer constraints?

- Concept: Supervised fine-tuning vs reinforcement learning fine-tuning
  - Why needed here: The paper compares traditional SFT (using only positive samples) with RLFT (using both positive and negative samples), which is central to their proposed method.
  - Quick check question: What is the key difference between SFT and RLFT in terms of the samples they use during training?

## Architecture Onboarding

- Component map: Data synthesis pipeline -> Discrimination module -> Contrastive training framework -> Evaluation suite
- Critical path: Data synthesis → Discrimination correction → Contrastive training → Evaluation
- Design tradeoffs:
  - Using compositional data increases training complexity but improves generalization
  - Discrimination method requires two models (student and teacher) but produces higher quality data
  - Contrastive learning with negative samples improves efficiency but requires careful sample selection
- Failure signatures:
  - Poor performance on IFEval indicates issues with constraint satisfaction
  - High computational cost during discrimination suggests inefficient teacher-student interaction
  - Overfitting to training constraints visible as poor out-of-domain generalization
- First 3 experiments:
  1. Run ablation study comparing Backbone vs Atom vs Composition training to verify empirical findings
  2. Test discrimination vs generation methods on a small set of complex instructions to confirm quality differences
  3. Implement basic contrastive learning with positive and negative samples to establish performance baseline

## Open Questions the Paper Calls Out
None

## Limitations

- Generalization Scope: The paper demonstrates strong performance improvements on in-domain multi-constraint instructions, but the generalization to truly out-of-domain constraints remains an empirical claim without systematic validation.
- Data Quality Assumptions: The discrimination-based method assumes that teacher model corrections are always superior to student model outputs, which may not hold when the teacher model introduces systematic biases.
- Evaluation Scope: While using IFEval and FollowBench, these benchmarks may not fully capture real-world complexity where constraints interact in nuanced ways or where constraints conflict, requiring prioritization decisions.

## Confidence

- High Confidence: The empirical finding that compositional training data improves performance on lower-complexity instructions is well-supported by the ablation studies and consistent across multiple experimental settings.
- Medium Confidence: The superiority of discrimination-based data synthesis over direct generation has strong empirical support but relies on assumptions about teacher model quality that could break down in different domains.
- Low Confidence: The claim about significant out-of-domain generalization lacks systematic testing with truly novel constraint types, making it difficult to assess real-world applicability beyond the tested scenarios.

## Next Checks

1. **Cross-Domain Constraint Testing**: Evaluate the trained model on constraints from completely different domains (e.g., medical instructions, legal document processing) to test true out-of-domain generalization beyond the current controlled experiments.

2. **Teacher Model Quality Analysis**: Systematically compare outputs from the discrimination method against ground truth or human-annotated correct outputs to quantify when teacher corrections actually improve quality versus introducing new errors.

3. **Constraint Conflict Resolution Testing**: Design test cases with conflicting constraints to evaluate how well the model can prioritize or resolve constraint conflicts, which represents a critical real-world scenario not addressed in the current evaluation framework.