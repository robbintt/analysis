---
ver: rpa2
title: 'Consensus learning: A novel decentralised ensemble learning paradigm'
arxiv_id: '2402.16157'
source_url: https://arxiv.org/abs/2402.16157
tags:
- learning
- consensus
- slush
- which
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Consensus learning introduces a novel distributed ML paradigm combining
  classical ensemble methods with consensus protocols, enabling fully decentralised
  ensemble learning without data sharing. Theoretical analysis proves that the accuracy
  of a binary consensus learning classifier using the Slush protocol is at least equal
  to that of individual base learners, with potential for near-perfect accuracy as
  network size grows.
---

# Consensus learning: A novel decentralised ensemble learning paradigm

## Quick Facts
- arXiv ID: 2402.16157
- Source URL: https://arxiv.org/abs/2402.16157
- Reference count: 40
- Key outcome: Consensus learning introduces a novel distributed ML paradigm combining classical ensemble methods with consensus protocols, enabling fully decentralised ensemble learning without data sharing. Theoretical analysis proves that the accuracy of a binary consensus learning classifier using the Slush protocol is at least equal to that of individual base learners, with potential for near-perfect accuracy as network size grows. Numerical simulations on non-IID FEMNIST and beta-distributed synthetic data demonstrate that consensus learning outperforms centralised majority voting in heterogeneous settings and maintains resilience against Byzantine participants. The approach enhances privacy, interpretability, and robustness compared to traditional ensemble methods.

## Executive Summary
Consensus learning presents a novel decentralised ensemble learning paradigm that combines classical ensemble methods with consensus protocols. The framework enables multiple participants to train individual models locally and reach consensus on predictions without sharing data, addressing privacy concerns while maintaining high accuracy. The approach theoretically guarantees that ensemble accuracy matches or exceeds individual learner performance, with numerical simulations demonstrating superiority over centralized majority voting in heterogeneous settings.

## Method Summary
The consensus learning framework consists of three phases: individual learning where each participant trains their own model locally, prediction sharing where participants submit predictions for test data, and a communication phase using the Slush consensus protocol. The Slush protocol employs parameters k (sample size) and α (acceptance threshold) to reach consensus through iterative local aggregations. The method compares against centralized majority voting and supermajority rules, with theoretical analysis proving accuracy guarantees and Byzantine resilience when α > f (number of Byzantine participants).

## Key Results
- Theoretical proof that consensus learning accuracy using Slush protocol is at least equal to individual base learners
- Numerical simulations show consensus learning outperforms centralized majority voting on non-IID FEMNIST data
- Byzantine resilience maintained when α > f, with accuracy degradation controlled by the parameter relationship

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The ensemble's accuracy is guaranteed to be at least as good as any individual base learner when using the Slush consensus protocol.
- Mechanism: The Slush protocol's absorption probability in the correct outcome state (Bb) is always greater than or equal to the fraction of honest participants who initially predict correctly (b/n) when b ≥ n/2.
- Core assumption: All participants are honest and the base learner accuracies follow a binomial distribution.
- Evidence anchors:
  - [abstract]: "Theoretical analysis proves that the accuracy of a binary consensus learning classifier using the Slush protocol is at least equal to that of individual base learners"
  - [section 4.2.1]: "PS ≥ p" and the proof relies on Lemma 2.10 which states Bb ≥ b/n
  - [corpus]: No direct evidence found - corpus papers focus on distributed training challenges rather than consensus protocol accuracy guarantees
- Break condition: The assumption fails when Byzantine participants are present or when base learners are not independent.

### Mechanism 2
- Claim: Consensus learning can outperform centralized majority voting when base learners have diverse accuracies.
- Mechanism: The control ratio κb combines the asymmetry of the consensus protocol with the diversity of base learners. When diversity is high (large variance in accuracies), κb < 1 for certain configurations, allowing consensus learning to outperform majority voting.
- Core assumption: The variance in base learner accuracies is large enough to create configurations where κb < 1.
- Evidence anchors:
  - [abstract]: "consensus learning outperforms centralised majority voting in heterogeneous settings"
  - [section 4.3]: Definition of control ratio and discussion of how diversity affects performance
  - [section 5.2]: Numerical simulations showing consensus learning outperforms majority voting with beta-distributed base learners
  - [corpus]: No direct evidence found - corpus papers don't discuss ensemble diversity in consensus protocols
- Break condition: The assumption fails when base learners have similar accuracies or when the consensus protocol doesn't leverage diversity effectively.

### Mechanism 3
- Claim: Consensus learning maintains Byzantine resilience through the communication phase.
- Mechanism: The Slush protocol's threshold parameter α must be greater than the number of Byzantine participants f. This ensures that Byzantine participants cannot overwhelm the honest majority in any local aggregation.
- Core assumption: α > f and Byzantine participants follow extreme adversarial strategies.
- Evidence anchors:
  - [abstract]: "consensus learning... maintains resilience against Byzantine participants"
  - [section 4.4]: Discussion of Byzantine tolerance and Theorem 5 stating α > f is required
  - [section 5.1.1]: Numerical simulations showing consensus learning outperforms majority voting even with Byzantine participants
  - [corpus]: No direct evidence found - corpus papers don't discuss Byzantine resilience in consensus learning specifically
- Break condition: The assumption fails when f ≥ α or when Byzantine participants can collude to control more than α nodes in any sampling round.

## Foundational Learning

- Concept: Markov chain modeling of consensus protocols
  - Why needed here: The theoretical analysis of Slush protocol relies on modeling the protocol as a continuous-time Markov chain with absorption states
  - Quick check question: What are the two absorbing states in the Slush protocol's Markov chain model?

- Concept: Jury theorems and ensemble learning theory
  - Why needed here: The paper extends classical jury theorems to the consensus learning setting, requiring understanding of how individual accuracies combine in ensembles
  - Quick check question: What is the key assumption about base learner independence in Condorcet's jury theorem?

- Concept: Byzantine fault tolerance in distributed systems
  - Why needed here: The analysis of Byzantine resilience requires understanding of how consensus protocols handle malicious participants
  - Quick check question: What is the maximum fraction of Byzantine participants that traditional consensus protocols can tolerate?

## Architecture Onboarding

- Component map: Individual learning phase -> Prediction sharing -> Communication phase (Slush protocol) -> Final decision
- Critical path: Individual learning → Prediction sharing → Communication phase (Slush protocol) → Final consensus output
- Design tradeoffs:
  - Higher α values provide better Byzantine resilience but may reduce accuracy in homogeneous settings
  - Larger k values increase communication overhead but improve consensus quality
  - Local α parameters (based on individual accuracy) can improve performance but require knowledge of base learner accuracies
- Failure signatures:
  - Accuracy drops when f ≥ α (Byzantine participants overwhelm the system)
  - Performance degrades with highly correlated base learners
  - Communication overhead becomes prohibitive with large k values
- First 3 experiments:
  1. Implement basic Slush protocol with n=101, k=10, α=7 on synthetic data with known accuracies
  2. Compare consensus learning accuracy vs centralized majority voting on non-IID FEMNIST data
  3. Test Byzantine resilience by introducing f=10 malicious participants and measuring accuracy degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do hierarchical consensus protocols compare to the Slush protocol in consensus learning for ML tasks?
- Basis in paper: [inferred] from the conclusion suggesting that hierarchical consensus protocols could improve performance by delegating leaders for communication rounds.
- Why unresolved: The paper only briefly mentions hierarchical consensus protocols as a potential area for future research, without conducting any experiments or analysis.
- What evidence would resolve it: Comparative experiments between hierarchical consensus protocols and the Slush protocol in consensus learning for various ML tasks and network sizes.

### Open Question 2
- Question: Can consensus learning algorithms be effectively applied to regression and unsupervised learning problems?
- Basis in paper: [explicit] The authors state that consensus learning algorithms can be applied to regression problems using robust local aggregation rules, and briefly mention potential applications to unsupervised learning.
- Why unresolved: The paper focuses primarily on classification tasks and does not provide any theoretical analysis or experimental results for regression or unsupervised learning problems.
- What evidence would resolve it: Theoretical analysis and experimental results demonstrating the effectiveness of consensus learning algorithms for regression and unsupervised learning tasks.

### Open Question 3
- Question: How do reward mechanisms in blockchain implementations affect the performance and honesty of participants in consensus learning?
- Basis in paper: [inferred] from the suggestion that reward mechanisms in blockchain implementations could incentivize participants to be honest and improve ensemble performance.
- Why unresolved: The paper only briefly mentions blockchain implementations and reward mechanisms as a potential area for future research, without conducting any experiments or analysis.
- What evidence would resolve it: Experiments comparing the performance of consensus learning algorithms with and without reward mechanisms in blockchain implementations, measuring participant honesty and ensemble accuracy.

## Limitations
- Theoretical guarantees rely heavily on assumptions of base learner independence and worst-case Byzantine behavior
- FEMNIST experiments limited to three specific model architectures, potentially limiting generalizability
- Byzantine resilience analysis assumes extreme adversarial strategies but doesn't account for sophisticated collusion tactics

## Confidence
- **High Confidence**: The theoretical framework for binary consensus learning with honest participants. The Markov chain analysis and absorption probability proofs are mathematically sound given the stated assumptions.
- **Medium Confidence**: The Byzantine resilience claims. While the theoretical threshold condition (α > f) is established, the practical effectiveness against sophisticated Byzantine strategies needs further validation.
- **Medium Confidence**: The performance claims against centralized majority voting. The numerical simulations support the claims, but the theoretical conditions for when κb < 1 are not fully characterized.

## Next Checks
1. **Byzantine Strategy Testing**: Implement and test against more sophisticated Byzantine strategies beyond simple vote flipping, such as adaptive poisoning or model poisoning attacks, to validate the robustness claims.

2. **Protocol Parameter Sensitivity**: Conduct systematic experiments varying k and α parameters across different base learner accuracy distributions to identify optimal configurations and understand performance boundaries.

3. **Multi-class Extension Validation**: Extend the theoretical framework and implement numerical validation for multi-class classification scenarios, as the current analysis is limited to binary classification.