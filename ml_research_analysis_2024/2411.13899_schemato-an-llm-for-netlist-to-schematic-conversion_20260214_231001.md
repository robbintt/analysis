---
ver: rpa2
title: Schemato -- An LLM for Netlist-to-Schematic Conversion
arxiv_id: '2411.13899'
source_url: https://arxiv.org/abs/2411.13899
tags:
- schemato
- circuit
- schematic
- schematics
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Schemato is an LLM-based system that automatically converts circuit
  netlists into interpretable schematics. The method fine-tunes a large language model
  on human-created netlist-to-schematic pairs, supporting both LTSpice (.asc) and
  LaTeX (CircuitTikz) formats.
---

# Schemato -- An LLM for Netlist-to-Schematic Conversion

## Quick Facts
- arXiv ID: 2411.13899
- Source URL: https://arxiv.org/abs/2411.13899
- Authors: Ryoga Matsuo; Stefan Uhlich; Arun Venkitaraman; Andrea Bonetti; Chia-Yu Hsieh; Ali Momeni; Lukas Mauch; Augusto Capone; Eisaku Ohbuchi; Lorenzo Servadei
- Reference count: 35
- Key outcome: LLM-based system achieving 93% CSR in LaTeX conversion, 3× higher mean structural similarity to human designs

## Executive Summary
Schemato is an LLM-based system that automatically converts circuit netlists into interpretable schematics. The method fine-tunes a large language model on human-created netlist-to-schematic pairs, supporting both LTSpice (.asc) and LaTeX (CircuitTikz) formats. The model is trained using augmented data and evaluated on compilation success rate (CSR) and structural similarity metrics. Experiments show Schemato achieves up to 93% CSR in LaTeX conversion, significantly outperforming state-of-the-art models, and generates schematics with 3× higher mean structural similarity to human designs.

## Method Summary
Schemato fine-tunes Llama 3.1-8B models using human-generated netlist-schematic pairs from LTSpice files. Data augmentation through .asc line shuffling creates diverse training samples. The system generates schematics using autoregressive decoding with syntax-constrained prompts. Evaluation combines compilation-based CSR metrics with image-based MSSIM similarity measures. The approach uses FSDP training and LoRA for efficient fine-tuning.

## Key Results
- Achieves up to 93% CSR in LaTeX schematic conversion
- Generates schematics with 3× higher mean structural similarity to human designs
- Outperforms state-of-the-art models on netlist-to-schematic conversion tasks

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning an LLM on human-created netlist-schematic pairs enables learning of semantic mapping between circuit topology and schematic representation. The model leverages augmented training data where .asc files are shuffled to expose it to multiple valid schematic orderings, improving robustness and generalization across netlist formats.

### Mechanism 2
Structured prompts with syntax-specific keywords (e.g., "SYMBOL", "WIRE", "\draw") guide the LLM to produce syntactically valid schematic code. Zero-shot and one-shot prompts incrementally constrain the output format, ensuring compliance with LTSpice .asc and LaTeX CircuitTikz specifications.

### Mechanism 3
Evaluation using both compilation-based metrics (CSR) and image-based metrics (MSSIM) provides comprehensive assessment of schematic correctness and visual similarity. CSR ensures syntactic validity by checking if generated code compiles, while MSSIM measures structural similarity to human designs, capturing topological fidelity.

## Foundational Learning

- **Large Language Models and fine-tuning techniques**: Needed to adapt general-purpose LLMs to specialized netlist-to-schematic conversion task. Quick check: What is the difference between zero-shot, one-shot, and fine-tuning in the context of LLM adaptation?

- **Electronic Design Automation and schematic representation formats**: Essential for understanding LTSpice .asc and LaTeX CircuitTikz formats to generate valid schematic code. Quick check: How do the .asc and .tex formats differ in their representation of circuit components and connections?

- **Graph similarity metrics and compilation-based evaluation**: Used to assess quality of generated schematics, ensuring syntactic correctness and visual similarity to human designs. Quick check: Why is it important to use both CSR and MSSIM for evaluating schematic generation models?

## Architecture Onboarding

- **Component map**: Data preprocessing pipeline -> Data augmentation module -> LLM fine-tuning framework -> Inference engine -> Evaluation suite

- **Critical path**: 1) Load and preprocess circuit data. 2) Generate augmented training samples. 3) Fine-tune LLM on netlist-schematic pairs. 4) Generate schematics from test netlists. 5) Evaluate outputs using compilation and similarity metrics.

- **Design tradeoffs**: Model size vs. fine-tuning efficiency (larger models capture more complex patterns but require more resources); Data augmentation vs. overfitting (improves generalization but may introduce noise); Evaluation metrics vs. real-world utility (CSR and MSSIM provide quantitative measures but may not capture functional correctness).

- **Failure signatures**: Low CSR indicates syntactically invalid generated code; Low MSSIM suggests topological or visual discrepancies with human designs; High BLEU but low CSR/MSSIM implies superficial similarity without semantic correctness.

- **First 3 experiments**: 1) Evaluate baseline LLM performance on netlist-to-schematic conversion using zero-shot prompts. 2) Fine-tune LLM on small curated dataset and compare CSR and MSSIM to baselines. 3) Test impact of data augmentation on model generalization by varying number of augmented samples.

## Open Questions the Paper Calls Out

1. **Complex topology performance**: How does Schemato perform on circuit topologies not well-represented in training data, such as complex analog designs with many sub-blocks? The paper acknowledges Schemato struggles with complex examples like NAND2 and transistor-level OpAmps due to limited structural diversity in training dataset.

2. **Geometric component placement**: How can Schemato be improved to better handle geometric placement of components in schematics crucial for human interpretability? The paper notes that while Schemato generates schematics with identical topological features, geometric differences from reference designs may affect human interpretability.

3. **Graph-based evaluation metrics**: What are limitations of using image-based metrics like MSSIM for evaluating schematic similarity, and how can graph-based metrics improve evaluation? The paper highlights that MSSIM is highly sensitive to absolute component locations and does not consider topological structure, suggesting need for graph-based metrics.

## Limitations

- Evaluation metrics may not fully capture functional equivalence of generated circuits
- Data augmentation approach assumes line ordering doesn't affect semantic meaning
- Performance metrics based on limited dataset details without full diversity representation

## Confidence

**High confidence**: The core methodology of fine-tuning LLMs on netlist-schematic pairs is sound and technically feasible. The use of established training techniques (FSDP, LoRA) and evaluation metrics (CSR, MSSIM) is appropriate for this task.

**Medium confidence**: The performance claims are based on reasonable experimental design, but limited dataset details and lack of functional validation introduce uncertainty about real-world applicability.

**Low confidence**: Claims about Schemato's impact on ML-based analog circuit design workflows are speculative, as the paper does not demonstrate actual integration with downstream design processes or show improvements in circuit performance metrics.

## Next Checks

1. **Functional equivalence validation**: Implement SPICE simulation comparison between reference circuits and generated schematics to verify that high MSSIM scores correspond to functionally equivalent circuits, not just visually similar ones.

2. **Cross-format consistency test**: Generate both LTSpice and LaTeX versions of same circuits and verify they represent identical topologies. Test whether model can maintain semantic consistency across different output formats.

3. **Out-of-distribution robustness**: Test Schemato on circuit topologies not present in training data (e.g., complex analog blocks like oscillators, PLLs, or power supplies) to evaluate generalization beyond curated dataset.