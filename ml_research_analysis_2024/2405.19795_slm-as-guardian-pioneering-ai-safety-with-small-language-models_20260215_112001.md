---
ver: rpa2
title: 'SLM as Guardian: Pioneering AI Safety with Small Language Models'
arxiv_id: '2405.19795'
source_url: https://arxiv.org/abs/2405.19795
tags:
- harmful
- safety
- queries
- safe
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a multi-task learning approach using a small
  language model (sLLM) to both detect and generate safeguard responses for harmful
  user queries. The model is trained on a curated dataset of harmful and safe queries,
  with special tokens indicating task type and harmfulness.
---

# SLM as Guardian: Pioneering AI Safety with Small Language Models

## Quick Facts
- arXiv ID: 2405.19795
- Source URL: https://arxiv.org/abs/2405.19795
- Reference count: 31
- Small language models (7B parameters) can outperform larger LLMs in safety tasks

## Executive Summary
This paper introduces a multi-task learning approach using a small language model (sLLM) to simultaneously detect harmful user queries and generate appropriate safeguard responses. The model employs special tokens to distinguish between classification and generation tasks, achieving high precision and recall in detecting harmful queries while producing fluent safeguard responses. The approach demonstrates that carefully designed small models can be more effective for safety purposes than larger, general-purpose LLMs.

## Method Summary
The authors propose a unified sLLM that handles both harmful query detection and safeguard response generation through multi-task learning. The model uses five special tokens (<|pred|>, <|expl|>, <|safe|>, <|unsafe|>, <|harm|>) to manage task switching and output formatting. Training follows a progressive approach: first applying supervised fine-tuning (SFT) for general instruction following, then specialized safety fine-tuning on a curated dataset. The <|harm|> token is applied to 30% of harmful queries to encourage consistent safeguard response generation regardless of harm classification.

## Key Results
- sLLM approach outperforms larger LLMs and APIs for safety tasks
- High precision and recall achieved in harmful query detection
- Generated safeguard responses demonstrate fluency and appropriateness
- Multi-task learning improves both detection accuracy and response quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-task learning improves both harmful query detection and safeguard response generation
- Mechanism: Joint training on classification and response generation tasks allows the model to learn rationales for why queries are harmful, improving detection accuracy
- Core assumption: The two tasks are sufficiently related that shared representation learning benefits both
- Evidence anchors:
  - [abstract] "propose a multi-task learning mechanism fusing the two tasks into a single model"
  - [section] "the two tasks are closely related to each other, in that the supervision for safeguard answer generation enables the model to internalize proper rationales why a given input query is harmful"
  - [corpus] Weak evidence - no direct comparison to single-task baseline in corpus
- Break condition: If tasks become too dissimilar or conflict in their objectives

### Mechanism 2
- Claim: Special tokens (<|pred|>, <|expl|>, <|safe|>, <|unsafe|>, <|harm|>) improve task switching and response control
- Mechanism: Task-specific tokens act as prefixes that direct the model's attention to the current task and desired output format
- Core assumption: Adding task-specific tokens doesn't interfere with the pre-trained language modeling
- Evidence anchors:
  - [abstract] "introduce five special tokens... <|pred|> (prediction) and <|expl|> (explain) tokens are the respective task prefixes"
  - [section] "utilizes <|harm|> to elicit a safeguard response regardless of whether the question is determined to be harmful or not"
  - [corpus] Weak evidence - performance impact of special tokens mentioned but not quantified in corpus
- Break condition: If special tokens become too numerous or overlap in semantic space

### Mechanism 3
- Claim: Progressive learning (SFT → safety fine-tuning) improves safety task performance
- Mechanism: General instruction-following capability established first provides foundation for specialized safety tasks
- Core assumption: Instruction-following capability transfers to safety task contexts
- Evidence anchors:
  - [abstract] "Supervised fine-tuning (SFT) for general instructions... enhanced the instruction-following tendency"
  - [section] "there was a significant improvement from before and after the application of our proposed dataset configuration and training methodology"
  - [corpus] Weak evidence - ablation studies show improvement but don't isolate SFT contribution specifically
- Break condition: If safety tasks require fundamentally different capabilities than general instruction following

## Foundational Learning

- Concept: Multi-task learning tradeoffs
  - Why needed here: Understanding when joint training helps vs hurts performance
  - Quick check question: What happens if two tasks have conflicting objectives?

- Concept: Token-level classification vs sequence generation
  - Why needed here: Model must switch between predicting single tokens (class labels) and generating full responses
  - Quick check question: How does the model know when to stop generating vs when to output a single token?

- Concept: Instruction fine-tuning methodology
  - Why needed here: Progressive learning approach relies on SFT as foundation
  - Quick check question: What's the difference between regular fine-tuning and instruction fine-tuning?

## Architecture Onboarding

- Component map:
  Input -> Shared transformer encoder -> Classification head (for <|pred|>) or Generation head (for <|expl|>) -> Output
  Special token embeddings added to tokenizer

- Critical path:
  1. Tokenize input with special tokens
  2. Run through shared encoder
  3. If <|pred|> token present, output classification
  4. If <|expl|> token present, generate response
  5. Apply <|harm|> token 30% of time for harmful queries

- Design tradeoffs:
  - Model size vs performance: 7B SLM vs larger LLMs
  - Task coupling: Tightly coupled tasks vs separate models
  - Token strategy: Special tokens vs natural language prompts

- Failure signatures:
  - High false positive rate: Model too conservative
  - High false negative rate: Model too permissive
  - Generic responses: <|harm|> token not working
  - Task confusion: Special tokens not properly distinguishing tasks

- First 3 experiments:
  1. Test classification accuracy on balanced safe/harm dataset
  2. Test response generation quality with <|harm|> token
  3. Compare multi-task vs single-task baseline performance

## Open Questions the Paper Calls Out

Open Question 1
- Question: What is the minimum computing resources necessary for effective safety modeling with small language models?
- Basis in paper: Inferred
- Why unresolved: The study does not provide experimental data or insights regarding the minimum computing resources necessary for effective safety modeling.
- What evidence would resolve it: Experiments demonstrating the performance of safety modeling with various sizes of small language models, such as those with 1.3 billion or 760 million parameters.

Open Question 2
- Question: Can specialized safety large language models rival the performance of significantly larger LLMs?
- Basis in paper: Inferred
- Why unresolved: The study acknowledges the need for additional verification to determine if specialized safety large language models can rival the performance of significantly larger LLMs.
- What evidence would resolve it: Experiments comparing the performance of specialized safety large language models with significantly larger LLMs in various safety-related tasks.

Open Question 3
- Question: Is the data generation and multi-task learning structure proposed in this paper a generalized methodology that can be applied to solve other language’s safety issues or other NLP tasks with small language models?
- Basis in paper: Inferred
- Why unresolved: The study does not provide evidence or experiments demonstrating the applicability of the proposed methodology to other languages or NLP tasks.
- What evidence would resolve it: Experiments applying the proposed data generation and multi-task learning structure to other languages and NLP tasks, and evaluating their effectiveness.

## Limitations

- Data curation methodology not specified, making bias assessment difficult
- Performance on adversarial or novel harmful queries untested
- Lack of comprehensive human evaluation for response quality
- No single-task baseline comparison to quantify multi-task benefits

## Confidence

**High confidence** in: Technical feasibility of multi-task architecture and special token mechanism
**Medium confidence** in: Superiority of sLLM approach over larger LLMs for safety tasks
**Low confidence** in: Model robustness to adversarial inputs and generalization beyond curated datasets

## Next Checks

1. Conduct adversarial robustness testing using synonym substitution and paraphrasing to evaluate performance on obfuscated harmful queries
2. Perform comprehensive human evaluation of safeguard response quality across fluency, helpfulness, and appropriateness dimensions
3. Implement single-task baseline models to quantify the actual benefit of multi-task learning through ablation studies