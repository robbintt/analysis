---
ver: rpa2
title: 'Classical Statistical (In-Sample) Intuitions Don''t Generalize Well: A Note
  on Bias-Variance Tradeoffs, Overfitting and Moving from Fixed to Random Designs'
arxiv_id: '2409.18842'
source_url: https://arxiv.org/abs/2409.18842
tags:
- bias
- prediction
- error
- training
- in-sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This note addresses the discrepancy between classical statistical
  intuitions (such as bias-variance tradeoff and overfitting) and modern machine learning
  phenomena like double descent and benign overfitting. The author argues that the
  key issue is not overparameterization or model complexity, but rather the difference
  between fixed design (in-sample prediction) and random design (out-of-sample prediction)
  settings.
---

# Classical Statistical (In-Sample) Intuitions Don't Generalize Well: A Note on Bias-Variance Tradeoffs, Overfitting and Moving from Fixed to Random Designs

## Quick Facts
- arXiv ID: 2409.18842
- Source URL: https://arxiv.org/abs/2409.18842
- Reference count: 12
- The discrepancy between classical statistical intuitions and modern ML phenomena stems from the fixed vs random design distinction

## Executive Summary
This note argues that classical statistical intuitions about bias-variance tradeoffs, overfitting, and model complexity do not generalize well to modern machine learning settings. The key issue is the distinction between fixed design (in-sample prediction) and random design (out-of-sample prediction) settings. Using k-nearest neighbor estimators on low-dimensional data, the author demonstrates that textbook intuitions about bias-variance tradeoff can fail in random design settings, where both bias and variance can decrease when model complexity decreases. This explains why phenomena like double descent and benign overfitting were not observed historically in classical statistics literature.

## Method Summary
The author uses k-nearest neighbor estimators on low-dimensional data to demonstrate the differences between fixed and random design settings. By comparing prediction errors in both settings, the author shows that classical intuitions about bias-variance tradeoffs do not necessarily hold when moving from fixed to random designs. The analysis focuses on how model behavior at training versus test points affects out-of-sample generalization, particularly in the context of interpolation and overfitting.

## Key Results
- Classical intuitions about bias-variance tradeoff do not necessarily hold in random design settings
- Double descent shapes cannot be observed in fixed design settings using in-sample prediction error
- The term "benign overfitting" is a misnomer; the phenomenon should be called "benign interpolation"
- Interpolation can be benign in random design settings but not in fixed design settings due to out-of-sample bias considerations

## Why This Works (Mechanism)
The mechanism underlying this work is the fundamental distinction between fixed design (where predictors are deterministic) and random design (where predictors are stochastic). In fixed design settings, the focus is on in-sample prediction error, which naturally leads to classical intuitions about bias-variance tradeoffs. However, in random design settings, out-of-sample prediction becomes the relevant metric, and the behavior of models at training versus test points becomes crucial. This shift in perspective explains why phenomena like double descent and benign interpolation emerge in modern ML but were not observed in classical statistics.

## Foundational Learning
- Fixed vs random design distinction - Why needed: Understanding the difference between in-sample and out-of-sample prediction is crucial for modern ML
- Bias-variance tradeoff - Why needed: Classical intuition about this tradeoff needs to be re-examined in random design settings
- Double descent phenomenon - Why needed: Explains why classical intuitions about overfitting fail in modern ML contexts
- Interpolation and generalization - Why needed: Understanding when interpolating models can still generalize well
- k-NN estimators - Why needed: Simple model to demonstrate the fixed vs random design distinction

Quick check: Compare k-NN prediction errors in fixed vs random design settings for varying k values

## Architecture Onboarding
Component map: k-NN estimator -> Fixed design error calculation -> Random design error calculation -> Bias-variance analysis

Critical path: Model selection -> Error calculation method (fixed vs random) -> Bias-variance decomposition -> Generalization analysis

Design tradeoffs: Fixed design favors simplicity and interpretability, while random design requires consideration of model behavior at test points

Failure signatures: Assuming classical bias-variance tradeoffs hold in random design settings, missing double descent phenomena in fixed design analysis

First experiments:
1. Compare k-NN errors in fixed vs random design for simple 1D data
2. Plot bias-variance decomposition curves in both settings
3. Test interpolation behavior for varying k values

## Open Questions the Paper Calls Out
None

## Limitations
- Empirical demonstration relies on k-NN estimators on low-dimensional data
- Generalization to high-dimensional settings remains unclear
- Theoretical claims about impossibility of double descent in fixed design need more rigorous proof
- May oversimplify the relationship between interpolation and generalization in high-dimensional settings

## Confidence
- Generalizability to high-dimensional settings: Medium
- Mathematical rigor of double descent impossibility claim: Medium
- Connection between interpolation and generalization: Medium

## Next Checks
1. Replicate the k-NN experiments with varying dimensionalities to test whether the bias-variance patterns hold as dimensionality increases
2. Extend the fixed vs random design comparison to parametric models where double descent has been observed
3. Test whether the proposed "benign interpolation" framework can explain margin-based generalization bounds in high-dimensional linear models