---
ver: rpa2
title: Speaker- and Text-Independent Estimation of Articulatory Movements and Phoneme
  Alignments from Speech
arxiv_id: '2407.03132'
source_url: https://arxiv.org/abs/2407.03132
tags:
- speech
- phoneme
- alignment
- aptai
- hprc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a novel combination of acoustic-to-articulatory
  speech inversion (AAI) and phoneme-to-articulatory motion estimation (PTA) into
  a joint task called APTAI. Two approaches are proposed: one based on frame classification
  and the other on forced alignment.'
---

# Speaker- and Text-Independent Estimation of Articulatory Movements and Phoneme Alignments from Speech

## Quick Facts
- arXiv ID: 2407.03132
- Source URL: https://arxiv.org/abs/2407.03132
- Reference count: 0
- Primary result: Joint acoustic-to-articulatory speech inversion and phoneme-to-articulatory motion estimation achieves 0.73 mean correlation for articulatory estimation and 87% frame overlap with text-dependent phoneme aligners

## Executive Summary
This paper introduces APTAI, a novel joint approach combining acoustic-to-articulatory speech inversion (AAI) and phoneme-to-articulatory motion estimation (PTA) into a unified task. The authors propose two methods: APTAI (frame classification) and f-APTAI (forced alignment), both of which work speaker- and text-independently during inference. Using multi-task learning with pre-trained wav2vec2.0 models, the approach simultaneously predicts articulatory movements, phoneme sequences, and phoneme alignments from raw speech input, achieving state-of-the-art results in speaker-independent articulatory estimation.

## Method Summary
The authors present two approaches for acoustic phoneme-to-articulatory speech inversion (APTAI). Both use multi-task learning combining articulatory regression and phoneme prediction/alignment. APTAI uses frame classification where transformer layers fine-tune wav2vec2.0 features to predict both tract variables and phoneme probabilities. f-APTAI employs forced alignment via two-stage training: first training phoneme recognition with CTC loss, then using cross-attention alignment with frozen weights for articulatory regression. Both approaches transform EMA sensor coordinates to tract variables for speaker-independence and use weighted multi-task loss functions for optimization.

## Key Results
- Mean correlation of 0.73 for articulatory-to-articulatory speech inversion (AAI) on test sets
- Up to 87% frame overlap compared to state-of-the-art text-dependent phoneme aligners
- f-APTAI outperforms APTAI on text-independent phoneme recognition with PER of 9.8% vs 11.8%
- Speaker-independent models achieve comparable performance to speaker-dependent approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-task learning of articulatory regression and phoneme prediction/alignment enables complementary information sharing that improves both tasks
- Mechanism: Joint optimization of TV regression and phoneme-related tasks allows shared intermediate representations to capture both acoustic-phonetic and articulatory features
- Core assumption: The acoustic signal contains sufficient information to predict both articulatory movements and phoneme identities/alignments simultaneously
- Evidence anchors:
  - [abstract]: "We use a multi-task learning setup, with the end-to-end goal of taking raw speech as input and estimating the corresponding articulatory movements, phoneme sequence, and phoneme alignment."
  - [section]: "Both make use of MTL optimization, composed of articulator movement regression and phoneme prediction paired with alignment."
  - [corpus]: Weak evidence - the corpus mentions 25 related papers but none specifically validate the multi-task learning mechanism claimed

### Mechanism 2
- Claim: Speaker-independent modeling is achievable by transforming EMA sensor coordinates into tract variables
- Mechanism: The transformation from EMA coordinates to tract variables captures linguistically meaningful articulatory gestures that generalize across speakers
- Core assumption: Tract variables capture the essential articulatory features needed for phoneme production while being less sensitive to individual anatomical differences
- Evidence anchors:
  - [abstract]: "Tract Variables (TVs), introduced by Browman et. al. [1], on the other hand, combine multiple individual vocal tract articulator movements, that achieve a specific linguistic objective, into defined gestures relevant to articulation."
  - [section]: "Transformations were introduced by Ji [2] to convert EMA sensor coordinates into TVs, which were shown to be less speaker dependent [3] than the original measurements."
  - [corpus]: No direct evidence - the corpus mentions 25 related papers but none specifically address the tract variable transformation mechanism

### Mechanism 3
- Claim: Fine-tuning self-supervised learning models (wav2vec2.0) provides pre-trained speech representations that capture meaningful features relevant for both articulatory and phoneme prediction
- Mechanism: wav2vec2.0 pre-trains on large amounts of unlabeled speech data using contrastive loss to learn speech representations
- Core assumption: Pre-trained speech representations from wav2vec2.0 contain features that are transferable to articulatory inversion and phoneme prediction tasks
- Evidence anchors:
  - [abstract]: "Both approaches make use of self-supervised learning (SSL) models but in different setups."
  - [section]: "We chose wav2vec2, which optimizes a contrastive loss during pre-training to learn a finite set of speech representations. These can be fine-tuned for a broad set of applications, with ASR as the original intended use case."
  - [corpus]: Weak evidence - the corpus mentions related papers but none specifically validate the effectiveness of wav2vec2.0 for articulatory inversion

## Foundational Learning

- Concept: Multi-task learning (MTL)
  - Why needed here: The paper combines AAI and PTA tasks, requiring the model to learn multiple objectives simultaneously
  - Quick check question: What are the potential benefits and drawbacks of using a weighted sum versus alternating optimization for the multi-task loss?

- Concept: Self-supervised learning (SSL) for speech
  - Why needed here: The model uses wav2vec2.0, which is pre-trained on unlabeled speech data
  - Quick check question: How does the contrastive loss used in wav2vec2.0 pre-training encourage the model to learn meaningful speech representations?

- Concept: Articulatory phonetics and tract variables
  - Why needed here: The target of the model is articulatory movements represented as tract variables
  - Quick check question: Why are tract variables considered more speaker-independent than raw EMA sensor coordinates, and how does this benefit the proposed approach?

## Architecture Onboarding

- Component map:
  Raw speech (16 kHz) -> wav2vec2.0 feature extractor (frozen) -> Transformer layers (fine-tuned) -> Convolutional sinc filter (fixed) -> TV head/Phoneme head -> Output

- Critical path: Raw speech → wav2vec2.0 → Transformer layers → TV head/Phoneme head → Output
  - For f-APTAI: Raw speech → wav2vec2.0 → Transformer layers → CTC phoneme recognition → Cross-attention alignment → Bi-LSTM → TV head → Output

- Design tradeoffs:
  - Frame classification vs. forced alignment for phoneme prediction: Frame classification provides direct alignment but may be less robust, while forced alignment is more stable but requires a two-stage training process
  - Speaker-dependent vs. speaker-independent modeling: Speaker-independent modeling using tract variables enables generalization but may sacrifice some accuracy compared to speaker-dependent models
  - Fine-tuning vs. freezing wav2vec2.0 components: Fine-tuning the transformer layers allows adaptation to the specific task while freezing the feature extractor preserves the pre-trained representations

- Failure signatures:
  - Poor correlation between predicted and ground truth tract variables (PCC < 0.5)
  - High phoneme error rate (PER > 20%)
  - Low frame overlap with text-dependent force aligner (< 70%)
  - Training instability or convergence issues in multi-task learning setup

- First 3 experiments:
  1. Baseline evaluation: Train APTAI model on CommonPhone dataset and evaluate on HPRC using leave-one-speaker-out testing. Measure PCC, RMSE, PER, and frame overlap.
  2. Component ablation: Train models with either TV regression or phoneme prediction removed to assess the impact of multi-task learning on individual task performance.
  3. Architecture comparison: Compare APTAI (frame classification) and f-APTAI (forced alignment) approaches on the same datasets to quantify the performance differences in both articulatory and phoneme-related metrics.

## Open Questions the Paper Calls Out

- How can the performance of Tongue Mid. Constr. Deg. (TMCD) and Tongue Body Constr. Loc. (TBCD) articulatory targets be improved to match other tract variables?
- How does changing the output frame rate of wav2vec2 from 20ms to 10ms affect phoneme alignment performance and enable 100 Hz TV regression?
- Would incorporating additional articulatory features beyond the nine tract variables improve model performance?

## Limitations
- The paper does not investigate why certain tract variables (TMCD and TBCD) perform significantly worse than others
- The effectiveness of tract variable transformations for speaker-independence is assumed from prior work without direct validation
- The contribution of wav2vec2.0 pre-training versus architectural design is not isolated through ablation studies

## Confidence
- Mechanism 1 (Multi-task learning): Medium confidence
- Mechanism 2 (Tract variable transformation): Low confidence  
- Mechanism 3 (wav2vec2.0 pre-training): Medium confidence

## Next Checks
1. Conduct ablation study on multi-task learning by training separate models for articulatory regression and phoneme prediction only, then comparing their performance against the joint model on both tasks
2. Perform speaker-dependence analysis by training and evaluating models using raw EMA coordinates versus tract variables on the same dataset, measuring speaker-independent performance metrics
3. Test pre-training necessity by training the same architecture from scratch (without wav2vec2.0 pre-training) on the available labeled data, comparing performance to the fine-tuned pre-trained model