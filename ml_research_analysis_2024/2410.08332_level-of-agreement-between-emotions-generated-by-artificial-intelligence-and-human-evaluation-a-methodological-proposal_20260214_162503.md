---
ver: rpa2
title: 'Level of agreement between emotions generated by Artificial Intelligence and
  human evaluation: a methodological proposal'
arxiv_id: '2410.08332'
source_url: https://arxiv.org/abs/2410.08332
tags:
- images
- emotion
- emotions
- agreement
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a methodology to assess the level of agreement
  between emotions generated by artificial intelligence (AI) and human evaluations.
  The study focuses on analyzing the subjectivity inherent in emotional evaluation
  and addresses the challenge of validating AI-generated content.
---

# Level of agreement between emotions generated by Artificial Intelligence and human evaluation: a methodological proposal

## Quick Facts
- arXiv ID: 2410.08332
- Source URL: https://arxiv.org/abs/2410.08332
- Reference count: 40
- This paper proposes a methodology to assess the level of agreement between emotions generated by artificial intelligence (AI) and human evaluations.

## Executive Summary
This paper presents a methodology for evaluating the agreement between AI-generated emotions and human evaluations in the context of generated artwork. The study addresses the challenge of validating AI-generated content by training a generative model (StyleGAN2-ADA) on a dataset of artworks with associated emotional descriptions. The researchers created four emotional variants (contentment, amusement, fear, and sadness) for each generated image and conducted an online questionnaire with participants to classify these images based on their emotional content. The study employs various statistical analyses, including Krippendorff's Alpha coefficient, Cohen's Kappa coefficient, confusion matrices, and proportion analysis, to measure the level of agreement among participants and between participant responses and AI-generated emotions. The results demonstrate a generally good level of agreement, particularly for negative emotions, and highlight the importance of considering emotion binarization (positive vs. negative) to achieve higher agreement levels. This methodology provides a valuable tool for validating AI-generated content and contributes to understanding human emotional appreciation.

## Method Summary
The methodology involves training StyleGAN2-ADA on a dataset of artworks associated with landscape categories and emotional descriptions. Four emotional variants (contentment, amusement, fear, and sadness) are created for each generated image. An online questionnaire is designed where participants classify the images based on their emotions. Statistical analyses, including Krippendorff's Alpha coefficient, Cohen's Kappa coefficient, confusion matrices, and proportion analysis, are performed to determine the level of agreement among participants, between the participants' responses and the AI-generated emotions, and the consistency of the agreement for different images.

## Key Results
- Good level of agreement between AI-generated emotions and human evaluations, with better results for negative emotions
- Subjectivity inherent in emotional evaluation confirmed
- Binarization of emotions into positive and negative categories improves agreement metrics

## Why This Works (Mechanism)
The methodology works by leveraging the generative capabilities of StyleGAN2-ADA to create emotionally variant images based on a curated dataset of artworks. By presenting these variants to human evaluators and analyzing their responses using established statistical measures, the study quantifies the agreement between AI-generated emotions and human perception. The use of multiple statistical methods provides a comprehensive assessment of agreement, accounting for both inter-rater reliability and alignment with AI predictions. The binarization of emotions into positive and negative categories simplifies the emotional landscape, potentially reducing cognitive load on participants and improving agreement metrics.

## Foundational Learning
- **Krippendorff's Alpha coefficient**: Measures inter-rater reliability for categorical data. Needed to assess agreement among participants. Quick check: Ensure proper handling of missing data and appropriate selection of metric type.
- **Cohen's Kappa coefficient**: Measures inter-rater reliability for two raters. Needed to assess agreement between participants and AI-generated emotions. Quick check: Verify the assumption of independent ratings.
- **Confusion matrix**: Visualizes the performance of classification algorithms. Needed to analyze the distribution of responses across emotion categories. Quick check: Ensure proper normalization of row/column totals for accurate interpretation.

## Architecture Onboarding
- **Component map**: StyleGAN2-ADA -> Image Generation -> Online Questionnaire -> Statistical Analysis -> Agreement Metrics
- **Critical path**: Dataset Preparation -> Model Training -> Image Generation -> Data Collection -> Statistical Analysis
- **Design tradeoffs**: The use of StyleGAN2-ADA provides high-quality image generation but may not capture the full spectrum of human emotions. The binarization of emotions improves agreement metrics but potentially oversimplifies the nuanced nature of emotional responses.
- **Failure signatures**: Low agreement between participants and AI-generated emotions may indicate issues with model training, dataset quality, or emotional categorization. Inconsistent agreement across different images may suggest biases in the dataset or variations in image complexity.
- **First experiments**:
  1. Replicate the study with a larger and more diverse participant pool to assess the robustness of the agreement metrics.
  2. Expand the emotional categories tested beyond the four initial variants to include more nuanced or complex emotions.
  3. Apply the methodology to different generative models and datasets to evaluate its broader applicability.

## Open Questions the Paper Calls Out
None

## Limitations
- Sample size of participants not explicitly reported, raising questions about statistical power and generalizability
- Emotional categories used represent only a subset of possible human emotions
- Relies on StyleGAN2-ADA, which may not represent the most current or sophisticated generative models available

## Confidence
- Agreement between AI-generated emotions and human evaluations: Medium
- Effectiveness of the proposed methodology: Medium
- Generalizability to other emotional categories and contexts: Low

## Next Checks
1. Replicate the study with a larger and more diverse participant pool to assess the robustness of the agreement metrics
2. Expand the emotional categories tested beyond the four initial variants to include more nuanced or complex emotions
3. Apply the methodology to different generative models and datasets to evaluate its broader applicability