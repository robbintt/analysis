---
ver: rpa2
title: 'GuardAgent: Safeguard LLM Agents by a Guard Agent via Knowledge-Enabled Reasoning'
arxiv_id: '2406.09187'
source_url: https://arxiv.org/abs/2406.09187
tags:
- guardagent
- agents
- safety
- agent
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GuardAgent is the first guardrail agent that safeguards LLM agents
  by dynamically checking their actions against safety guard requests. It uses an
  LLM to generate a task plan and then translates this plan into executable guardrail
  code, leveraging a memory module to retrieve in-context demonstrations from past
  tasks.
---

# GuardAgent: Safeguard LLM Agents by a Guard Agent via Knowledge-Enabled Reasoning

## Quick Facts
- arXiv ID: 2406.09187
- Source URL: https://arxiv.org/abs/2406.09187
- Reference count: 37
- GuardAgent achieves over 98% and 83% guardrail accuracy on EICU-AC and Mind2Web-SC benchmarks respectively

## Executive Summary
GuardAgent is the first guardrail agent that safeguards LLM agents by dynamically checking their actions against safety guard requests. It uses an LLM to generate a task plan and then translates this plan into executable guardrail code, leveraging a memory module to retrieve in-context demonstrations from past tasks. GuardAgent is tested on two novel benchmarks: EICU-AC for healthcare agent access control and Mind2Web-SC for web agent safety control, demonstrating high accuracy without affecting target agents' task performance.

## Method Summary
GuardAgent safeguards LLM agents by dynamically checking their actions against safety guard requests. It uses an LLM to generate a task plan, translates this plan into executable guardrail code, and leverages a memory module to retrieve in-context demonstrations from past tasks. The method is evaluated on EICU-AC (healthcare access control) and Mind2Web-SC (web safety control) benchmarks, using metrics like label prediction accuracy and final response accuracy.

## Key Results
- GuardAgent achieves over 98% guardrail accuracy on the EICU-AC benchmark for healthcare agent access control
- GuardAgent achieves over 83% guardrail accuracy on the Mind2Web-SC benchmark for web agent safety control
- GuardAgent maintains target agents' task performance while providing safety guardrails

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GuardAgent achieves high accuracy by translating safety guard requests into executable code rather than relying on natural language reasoning alone.
- Mechanism: GuardAgent uses an LLM to generate an action plan based on the safety guard requests and target agent behavior, then translates this plan into guardrail code using a toolbox of callable functions. The code is executed to make deterministic safety decisions.
- Core assumption: Code-based guardrails are more reliable than natural language reasoning for complex safety rules involving multiple conditions and database access controls.
- Evidence anchors:
  - [abstract]: "GuardAgent first analyzes the safety guard requests to generate a task plan, and then maps this plan into guardrail code for execution."
  - [section]: "GuardAgent provides guardrails by code generation and execution, which is more reliable than guardrails solely based on natural language."
  - [corpus]: Weak - no direct corpus evidence for code reliability, though related papers mention "reasoning-based" approaches
- Break condition: If the toolbox lacks necessary functions for the specific safety rules, GuardAgent may fail to generate executable code or must define functions itself, potentially reducing reliability.

### Mechanism 2
- Claim: GuardAgent's memory module with in-context demonstrations enables effective knowledge-enabled reasoning for diverse safety guard requests.
- Mechanism: GuardAgent retrieves demonstrations from past tasks based on similarity to current inputs, using these as examples to guide both task planning and code generation steps.
- Core assumption: Past demonstrations relevant to current safety guard requests improve the LLM's ability to understand and respond to new, unseen requests.
- Evidence anchors:
  - [abstract]: "supplemented by in-context demonstrations retrieved from a memory module storing experiences from previous tasks"
  - [section]: "GuardAgent can understand different safety guard requests and provide reliable code-based guardrails with high flexibility and low operational overhead"
  - [corpus]: Weak - corpus mentions "reasoning-based" but doesn't specifically address memory/retrieval mechanisms
- Break condition: If retrieved demonstrations are not sufficiently relevant or diverse, GuardAgent's performance may degrade, especially for novel safety scenarios.

### Mechanism 3
- Claim: GuardAgent's non-invasive design allows it to safeguard target agents without affecting their task performance.
- Mechanism: GuardAgent operates as a separate agent that checks target agent inputs/outputs against safety rules, denying actions only when violations are detected, rather than modifying the target agent's core logic.
- Core assumption: Separating safety checking from task execution prevents interference with the target agent's original functionality while maintaining safety guarantees.
- Evidence anchors:
  - [abstract]: "without affecting the target agents' task performance"
  - [section]: "GuardAgent is 'non-invasive' to the target agents"
  - [corpus]: Weak - corpus mentions "step-level guardrail" but doesn't explicitly discuss non-invasive design
- Break condition: If GuardAgent's decision-making introduces significant latency or if the safety checking logic becomes too complex, it could indirectly impact task performance through timing or resource constraints.

## Foundational Learning

- Concept: Chain-of-thought reasoning
  - Why needed here: GuardAgent uses chain-of-thought reasoning in its action planning step to break down complex safety guard requests into manageable sub-steps
  - Quick check question: How does chain-of-thought reasoning help GuardAgent understand complex safety rules involving multiple conditions?

- Concept: In-context learning
  - Why needed here: GuardAgent relies on in-context learning through retrieved demonstrations to adapt to new safety guard requests without additional training
  - Quick check question: What advantage does in-context learning provide for GuardAgent compared to traditional fine-tuning approaches?

- Concept: Code generation and execution
  - Why needed here: GuardAgent generates executable code from natural language safety rules, enabling deterministic and reliable safety checking
  - Quick check question: Why might code-based guardrails be more reliable than pure natural language reasoning for complex safety rules?

## Architecture Onboarding

- Component map: Task Planning Module -> Code Generation Module -> Code Execution
- Critical path: Input safety requests → Task Planning (LLM + memory) → Code Generation (LLM + toolbox) → Code Execution → Output safety decision
- Design tradeoffs: GuardAgent trades computational overhead (code execution, memory retrieval) for increased reliability and flexibility compared to model-based guardrails. The non-invasive design prevents task performance degradation but requires careful synchronization with target agents.
- Failure signatures: Common failures include: (1) Generated code not executable due to missing toolbox functions, (2) Incorrect safety decisions due to irrelevant retrieved demonstrations, (3) Performance degradation if memory retrieval becomes too slow
- First 3 experiments:
  1. Test GuardAgent with a simple safety rule (e.g., age restriction) on a basic web agent to verify the core pipeline works
  2. Test memory retrieval by varying the number of demonstrations and measuring accuracy impact
  3. Test toolbox extensibility by removing critical functions and observing GuardAgent's ability to define them autonomously

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can GuardAgent's toolbox be automatically designed without manual specification of functions?
- Basis in paper: [explicit] The paper mentions that the toolbox is manually specified and suggests future research on automatic toolbox design.
- Why unresolved: Manual specification is labor-intensive and limits scalability when new safety guard requests emerge.
- What evidence would resolve it: Demonstrations of an auxiliary agent or automated system that analyzes guard requests and generates required functions without human intervention.

### Open Question 2
- Question: Can GuardAgent's reasoning capabilities be enhanced beyond simple chain-of-thought to improve reliability?
- Basis in paper: [explicit] The paper suggests future work on advanced reasoning strategies like self-consistency or reflexion.
- Why unresolved: Current reasoning may fail on complex guard requests or ambiguous scenarios where validation of reasoning steps is needed.
- What evidence would resolve it: Comparative studies showing GuardAgent with advanced reasoning (e.g., self-consistency) outperforming current chain-of-thought on complex safety guard requests.

### Open Question 3
- Question: How would a multi-agent design improve GuardAgent's performance on complex guardrail requests?
- Basis in paper: [explicit] The paper proposes future multi-agent frameworks where different agents handle task planning, code generation, and memory management separately.
- Why unresolved: Single-agent design may struggle with complex, multi-faceted guard requests requiring coordination across subtasks.
- What evidence would resolve it: Empirical results comparing single-agent GuardAgent with multi-agent variants on benchmarks with highly complex, multi-rule safety guard requests.

## Limitations
- Benchmark scale remains relatively small despite high accuracy claims
- Real-world deployment effectiveness in dynamic environments untested
- Potential performance overhead from code execution and memory retrieval not fully characterized

## Confidence
- High confidence: GuardAgent's code-based guardrail approach is more reliable than pure natural language reasoning for safety checking
- Medium confidence: The memory module with in-context demonstrations significantly improves GuardAgent's ability to handle diverse safety requests
- Low confidence: GuardAgent can be seamlessly deployed in production environments without affecting target agent performance

## Next Checks
1. Stress-test GuardAgent on a larger, more diverse benchmark with varying safety rule complexity to verify scalability beyond the reported results
2. Measure and analyze the computational overhead (latency, memory usage) introduced by GuardAgent's code execution and memory retrieval components in production-like scenarios
3. Conduct ablation studies removing the memory module to quantify the actual contribution of in-context learning to GuardAgent's performance across different types of safety guard requests