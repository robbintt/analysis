---
ver: rpa2
title: Extracting Training Data from Document-Based VQA Models
arxiv_id: '2407.08707'
source_url: https://arxiv.org/abs/2407.08707
tags: []
core_contribution: "This work investigates privacy risks in document-based VQA models,\
  \ showing they can memorize and regurgitate training data\u2014even sensitive PII\
  \ appearing only once\u2014when queried with partial context (image minus answer).\
  \ The authors define extractability, measure it across Donut, Pix2Struct, and PaLI-3,\
  \ and distinguish memorization from generalization using a counterfactual baseline."
---

# Extracting Training Data from Document-Based VQA Models

## Quick Facts
- **arXiv ID:** 2407.08707
- **Source URL:** https://arxiv.org/abs/2407.08707
- **Reference count:** 24
- **Primary result:** Document-based VQA models can memorize and regurgitate training data, including PII, when queried with partial context.

## Executive Summary
This paper investigates privacy risks in document-based visual question answering (VQA) models, demonstrating that these models can memorize training samples and regurgitate answers even when the relevant visual information is removed from the input. The authors introduce a systematic methodology to measure extractability—the ability to retrieve answers from partial contexts—and apply it to three popular models (Donut, Pix2Struct, and PaLI-3). They find that lower training resolution increases memorization, while better pretraining reduces it. The study also reveals that models can extract PII using single modalities and proposes an effective mitigation strategy that reduces extractable PII to zero without degrading performance.

## Method Summary
The authors define extractability as the ability of a model to produce correct answers when given partial context (image without answer text, original question). They conduct controlled experiments using the DocVQA dataset, measuring extractability across three models trained at different resolutions. The methodology includes introducing canaries (samples with manually removed answers), counterfactual memorization baselines to distinguish memorization from generalization, and ablation studies on context factors like OCR quality, question paraphrasing, and image perturbations. The proposed defense augments training data with "ANSWER NOT PRESENT" samples when answers are absent from images.

## Key Results
- Document-based VQA models exhibit significant extractability rates (10-25%) across all tested architectures
- Lower training resolution increases memorization rates, while better pretraining (PaLI-3) reduces them
- Image text and exact question phrasing are the most critical factors for successful extraction
- The proposed "Extraction Blocking" defense reduces extractable PII to zero without harming performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Models can extract answers even when the answer text is removed from the input image.
- Mechanism: The model has memorized the answer during training and can recall it when given the partial context (image without answer, original question).
- Core assumption: The answer is extractable only if it has been memorized during training.
- Evidence anchors:
  - [abstract] "we show these models can memorize responses for training samples and regurgitate them even when the relevant visual information has been removed."
  - [section 3] "Extractability of the answer a from a partial context (I −a, Q) Given a model f and a sample (I, Q, a) ∈ D, we say it is an extractable sample if the correct answer a is obtained from the partial context (I −a, Q)"
- Break condition: If the model cannot produce the correct answer when given partial context, the memorization does not work.

### Mechanism 2
- Claim: Lower training resolution increases the likelihood of memorization.
- Mechanism: Lower resolution makes it harder for the model to actually read the answers from the image, making it easier for the model to minimize loss by memorization.
- Core assumption: The resolution at which the model is trained is inversely proportional to the amount of memorized samples.
- Evidence anchors:
  - [section 4] "Training resolution: Given a fixed model architecture, the resolution at which the model is trained is inversely proportional to the amount of memorized samples."
  - [section 4] "While training at the maximum resolution possible is generally recommended to achieve better performance, lower resolutions might also be adopted in some settings to accelerate training, especially for the largest models."
- Break condition: If training at high resolution still results in high memorization, the mechanism is broken.

### Mechanism 3
- Claim: Better pre-training reduces reliance on memorization.
- Mechanism: Models with better pre-training can rely more on generalization rather than memorization, even at relatively low training resolutions.
- Core assumption: A better pre-trained model may rely less on memorization due to better generalization abilities.
- Evidence anchors:
  - [section 4] "Pretraining: ... we observe that, besides trivial answers like the ones extracted for Donut and Pix2Struct, the generalization baseline correctly responds to questions whose answer relies on general knowledge..."
  - [section 4] "This is attributable to the web-scale pretraining."
- Break condition: If a well-pretrained model still heavily relies on memorization, the mechanism is broken.

## Foundational Learning

- Concept: Counterfactual memorization and simplicity scores
  - Why needed here: To distinguish between memorization and generalization in extractable answers.
  - Quick check question: How do counterfactual memorization and simplicity scores help in identifying whether a model has memorized or generalized an answer?

- Concept: PII (Personally Identifiable Information)
  - Why needed here: To understand the privacy risks associated with model memorization.
  - Quick check question: What types of information are considered PII in the context of document-based VQA models?

- Concept: Document-based Visual Question Answering (VQA)
  - Why needed here: To understand the task the models are performing and the nature of the data they are trained on.
  - Quick check question: What is the difference between document-based VQA and other types of VQA?

## Architecture Onboarding

- Component map: Image encoder -> OCR module -> Language model -> Answer decoder
- Critical path: Training on DocVQA dataset -> Fine-tuning on document images -> Testing extractability with partial context
- Design tradeoffs: Higher resolution improves performance but increases computational cost; better pretraining reduces memorization but may increase model size
- Failure signatures: Inability to extract answers with partial context; extraction of answers not in training data
- First 3 experiments:
  1. Test model's ability to extract answers when given partial context (image without answer, original question)
  2. Test effect of training resolution on memorization by training at different resolutions and measuring extractability
  3. Test effect of pretraining on memorization by comparing models with different pretraining strategies and measuring extractability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of the "Extraction Blocking" (EB) defense strategy change when applied to models trained with differentially private (DP) training techniques?
- Basis in paper: [inferred] The paper discusses the potential of DP training to provide membership inference attack guarantees but notes it is beyond the scope of the work and complex to scale to VLMs without significant utility degradation.
- Why unresolved: The paper does not explore the combination of EB with DP training, leaving open whether this would provide additive or complementary protection against training data extraction.
- What evidence would resolve it: Experiments comparing the extractability of PII from VQA models trained with both EB and DP training versus either technique alone, measuring both privacy and utility impacts.

### Open Question 2
- Question: What is the impact of varying the frequency of "ANSWER NOT PRESENT" samples in the training set on the model's ability to generalize to unseen documents and its resistance to training data extraction attacks?
- Basis in paper: [explicit] The paper introduces EB by augmenting the training set with samples where the answer is marked as not present, but does not explore how the frequency of these samples affects performance or privacy.
- Why unresolved: The paper does not investigate the trade-off between the number of "ANSWER NOT PRESENT" samples and the model's utility or privacy guarantees.
- What evidence would resolve it: A systematic study varying the proportion of "ANSWER NOT PRESENT" samples in the training set, measuring the impact on test set performance and the extractability of PII under different attack scenarios.

### Open Question 3
- Question: How do different types of image perturbations (e.g., adversarial examples, natural image degradations) affect the extractability of training data from document-based VQA models, and can the model's robustness to these perturbations be improved without sacrificing utility?
- Basis in paper: [explicit] The paper investigates the impact of brightness changes, rotations, and translations on extractability, finding that spatial transformations have a stronger adverse effect than intensity changes.
- Why unresolved: The paper does not explore a broader range of image perturbations or strategies to improve the model's robustness to these perturbations while maintaining performance.
- What evidence would resolve it: Experiments evaluating the extractability of PII under various image perturbation techniques, including adversarial examples and natural degradations, and investigating methods to improve the model's robustness (e.g., adversarial training, data augmentation) without compromising utility.

## Limitations

- The findings may not generalize beyond the specific DocVQA dataset and three examined model architectures
- The manual verification process for canaries introduces potential human bias
- The EB defense's effectiveness against adaptive adversaries using multiple prompting strategies is not thoroughly explored
- The paper focuses specifically on document-based VQA, leaving open questions about other multimodal learning paradigms

## Confidence

**High Confidence:**
- Models can memorize and regurgitate training data when given partial context
- Lower training resolution increases memorization rates
- Better pretraining (as in PaLI-3) reduces memorization
- The EB defense effectively reduces extractable PII to zero in tested conditions

**Medium Confidence:**
- Image text and exact question phrasing are the most critical factors for extraction
- Models can extract PII using only single modalities (image or question alone)
- The counterfactual memorization framework reliably distinguishes memorization from generalization

**Low Confidence:**
- The specific numerical extractability rates would remain consistent across different document datasets
- The EB defense would maintain zero extractability against all possible adversarial query strategies
- The resolution-memorization relationship holds identically across all vision-language model families

## Next Checks

1. **Cross-Dataset Validation**: Test the same extractability methodology on a different document-based VQA dataset (e.g., OCR-VQA or InfographicVQA) to verify whether the observed memorization patterns generalize beyond DocVQA.

2. **Adaptive Attack Evaluation**: Design and execute an adaptive attack scenario where an adversary systematically varies prompt formulations, uses multiple queries to reconstruct answers, or employs different inference-time strategies to bypass the EB defense.

3. **Resolution Scaling Study**: Conduct a systematic study varying training resolution across a wider range (including resolutions beyond those tested) while controlling for other factors like model scale and pretraining.