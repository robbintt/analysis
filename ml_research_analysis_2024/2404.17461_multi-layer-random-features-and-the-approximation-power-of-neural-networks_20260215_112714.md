---
ver: rpa2
title: Multi-layer random features and the approximation power of neural networks
arxiv_id: '2404.17461'
source_url: https://arxiv.org/abs/2404.17461
tags: []
core_contribution: The paper investigates the approximation power of multi-layer neural
  networks by analyzing the connection between randomly initialized networks and Gaussian
  Random Fields defined by the Neural Network Gaussian Process (NNGP) kernel. The
  key insight is that a neural network with randomly initialized weights in the infinite
  width limit behaves like a Gaussian Random Field whose covariance function is the
  NNGP kernel.
---

# Multi-layer random features and the approximation power of neural networks

## Quick Facts
- arXiv ID: 2404.17461
- Source URL: https://arxiv.org/abs/2404.17461
- Authors: Rustem Takhanov
- Reference count: 40
- Key outcome: The paper proves that functions in the Reproducing Kernel Hilbert Space (RKHS) defined by the Neural Network Gaussian Process (NNGP) kernel can be approximated by corresponding neural network architectures, with approximation bounds determined by RKHS norms.

## Executive Summary
This paper establishes a theoretical framework connecting the approximation power of multi-layer neural networks to Gaussian Random Fields defined by Neural Network Gaussian Process (NNGP) kernels. The key insight is that randomly initialized neural networks in the infinite width limit behave as Gaussian Random Fields whose covariance is the NNGP kernel. The paper proves that functions in the RKHS of the NNGP kernel can be approximated by neural networks with randomly initialized weights in earlier layers and trained weights in the final layer.

The theoretical contributions include deriving approximation bounds based on RKHS norms and comparing the approximation power with Barron's theorem for 2-layer networks. The paper demonstrates that when NNGP eigenvalues decay slower than k^(-n-2/3), where k is the eigenvalue order, the proposed method provides more efficient neural network approximations than Barron's theorem. Computational experiments validate these theoretical findings on simple target functions.

## Method Summary
The paper analyzes the approximation power of neural networks through their connection to NNGP kernels in the infinite width limit. The key mechanism involves showing that a randomly initialized network approximates a Gaussian Random Field, and functions in the RKHS defined by the NNGP kernel can be approximated using a multi-layer random representation of the input combined with training of only the last layer's weights. The number of neurons required in each layer is determined by the RKHS norm of the target function. The theoretical analysis includes comparing this approach with Barron's theorem for 2-layer networks on an n-1-dimensional sphere in R^n, showing conditions under which the proposed method provides more succinct approximations.

## Key Results
- Functions in the RKHS of the NNGP kernel can be approximated by neural networks with randomly initialized weights in early layers and trained weights in the final layer
- The required number of neurons in each layer is determined by the RKHS norm of the target function
- When NNGP eigenvalues decay slower than k^(-n-2/3), the proposed method provides more efficient approximations than Barron's theorem
- Computational experiments validate that realistic networks can learn target functions even when theoretical guarantees don't apply

## Why This Works (Mechanism)
The paper leverages the connection between randomly initialized neural networks and Gaussian Random Fields in the infinite width limit. In this regime, the network's output distribution converges to a Gaussian process whose covariance function is the NNGP kernel. Since functions in the RKHS of this kernel can be approximated by finite linear combinations of the kernel function, and the random neural network provides a multi-layer random representation of the input, training only the final layer allows approximation of any function in the RKHS. The approximation quality depends on the decay rate of the NNGP kernel's eigenvalues, with slower decay enabling more efficient approximations.

## Foundational Learning
1. **Neural Network Gaussian Process (NNGP) kernel** - why needed: Provides the theoretical link between random neural networks and Gaussian processes; quick check: Verify understanding of how network architecture determines the kernel form

2. **Reproducing Kernel Hilbert Space (RKHS)** - why needed: Defines the function space where approximation guarantees hold; quick check: Confirm understanding of RKHS norm and its role in determining approximation bounds

3. **Eigenvalue decay of integral operators** - why needed: Determines the efficiency of approximation; quick check: Understand the relationship between eigenvalue decay rate and required network width

4. **Barron's theorem** - why needed: Provides the baseline comparison for approximation bounds; quick check: Know the key assumptions and guarantees of Barron's theorem

5. **Infinite width limit** - why needed: Enables the Gaussian process approximation; quick check: Understand how this limit is formalized and its implications

6. **RKHS norm** - why needed: Determines the number of neurons required for approximation; quick check: Calculate RKHS norm for simple functions

## Architecture Onboarding

**Component Map**: Input -> Random multi-layer representation -> Trained final layer weights -> Output

**Critical Path**: The random initialization of early layers creates a feature representation that maps inputs to a space where the final layer's linear combination can approximate any function in the RKHS. The critical path is ensuring the random representation has sufficient capacity (determined by RKHS norm) to capture the target function.

**Design Tradeoffs**: The method trades off training computational complexity (only final layer trained) against potentially larger networks needed to achieve the same approximation quality compared to fully trained networks. The choice of activation function significantly impacts eigenvalue decay and thus approximation efficiency.

**Failure Signatures**: Poor approximation when the target function has large RKHS norm requiring exponentially many neurons, or when the NNGP kernel's eigenvalues decay too rapidly. The infinite width assumption may break down for very deep networks or certain activation functions.

**First Experiments**:
1. Verify NNGP kernel computation for a simple architecture (e.g., ReLU network)
2. Test approximation of a simple RKHS function using random initialization + final layer training
3. Compare eigenvalue decay rates across different activation functions

## Open Questions the Paper Calls Out
None

## Limitations
- The analysis relies heavily on the infinite width limit, which may not fully capture finite-width network behavior
- The comparison with Barron's theorem assumes specific eigenvalue decay conditions that may not hold universally
- Computational experiments are limited to simple target functions and may not scale to complex real-world scenarios
- The focus on only training the final layer may not reflect typical deep learning practice

## Confidence
- **High confidence**: The theoretical framework connecting NNGP kernels to RKHS approximation is mathematically rigorous
- **Medium confidence**: The specific eigenvalue decay conditions required for improved approximation over Barron's theorem
- **Medium confidence**: The computational results demonstrating the theory on limited examples

## Next Checks
1. Test the eigenvalue decay conditions empirically across different activation functions to verify the claimed improvement over Barron's theorem holds broadly

2. Extend computational experiments to higher-dimensional problems and more complex target functions, particularly those with multiple output dimensions

3. Investigate how the approximation bounds scale with finite-width networks to bridge the gap between theoretical infinite-width results and practical implementations