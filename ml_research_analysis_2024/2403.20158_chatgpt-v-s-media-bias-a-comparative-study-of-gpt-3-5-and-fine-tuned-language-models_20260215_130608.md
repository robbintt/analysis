---
ver: rpa2
title: 'ChatGPT v.s. Media Bias: A Comparative Study of GPT-3.5 and Fine-tuned Language
  Models'
arxiv_id: '2403.20158'
source_url: https://arxiv.org/abs/2403.20158
tags: []
core_contribution: This study evaluates ChatGPT's ability to detect six types of media
  bias (racial, gender, cognitive, text-level context, hate speech, and fake news)
  using the MBIB dataset. It compares ChatGPT's zero-shot performance against fine-tuned
  models (BART, ConvBERT, GPT-2) using micro and macro average F1-scores.
---

# ChatGPT v.s. Media Bias: A Comparative Study of GPT-3.5 and Fine-tuned Language Models

## Quick Facts
- arXiv ID: 2403.20158
- Source URL: https://arxiv.org/abs/2403.20158
- Authors: Zehao Wen; Rabih Younes
- Reference count: 19
- ChatGPT performs comparably to fine-tuned models on hate speech and text-level context bias detection but underperforms on racial, gender, cognitive biases, and fake news detection

## Executive Summary
This study compares ChatGPT's zero-shot media bias detection capabilities against fine-tuned models (BART, ConvBERT, GPT-2) across six bias types: racial, gender, cognitive, text-level context, hate speech, and fake news. Using the MBIB dataset and micro/macro average F1-scores as metrics, the research finds that ChatGPT matches fine-tuned models on explicit bias types like hate speech but struggles significantly with subtle biases requiring deeper contextual understanding. The results demonstrate that while ChatGPT shows proficiency in identifying surface-level bias patterns, fine-tuned models remain more effective for comprehensive media bias detection tasks.

## Method Summary
The study evaluates six fine-tuned models (BART, ConvBERT, GPT-2) trained on MBIB dataset tasks with learning rate 5e-5, batch size 128, and 5 epochs. ChatGPT is evaluated using zero-shot prompting with temperature=0 and three prompts per task, selecting the best based on a 60-example subset. Both approaches use binary label conversion and micro/macro average F1-scores for comparison. The MBIB dataset includes 80-20 train-test splits for each task with specific instance counts for racial (7830/1958), gender (32072/8020), cognitive (39066/9768), text-level context (7213/1805), hate speech (27879/6972), and fake news (9063/2675) bias detection.

## Key Results
- ChatGPT performs at par with fine-tuned models on hate speech and text-level context bias detection
- ChatGPT significantly underperforms on racial, gender, cognitive biases, and fake news detection
- Fine-tuned models show advantage in detecting subtle biases requiring deeper contextual reasoning

## Why This Works (Mechanism)

### Mechanism 1
ChatGPT performs comparably to fine-tuned models on explicit bias types due to surface-level pattern matching. Explicit bias types like hate speech contain clear linguistic markers (aggressive tone, negative sentiment) that are detectable via pattern recognition without deeper contextual reasoning. The model's training data includes sufficient examples of explicit bias patterns for zero-shot detection.

### Mechanism 2
ChatGPT underperforms on subtle bias types due to lack of task-specific fine-tuning. Fine-tuned models like BART and ConvBERT learn specific bias labeling patterns from human evaluators, while ChatGPT relies only on general patterns from pretraining. Human labeling introduces subjectivity that models learn to emulate through fine-tuning.

### Mechanism 3
Over-sensitivity to bias in certain domains leads to high false positive rates for ChatGPT. Exposure to bias during training causes ChatGPT to associate certain words or phrases with bias, even when human evaluators consider them neutral. Training data contains sufficient bias examples that create strong word-bias associations.

## Foundational Learning

- Concept: Zero-shot vs fine-tuned learning
  - Why needed here: Understanding the fundamental difference between ChatGPT's general pretraining and task-specific fine-tuning explains performance gaps
  - Quick check question: If a model has never seen task-specific examples, what type of learning approach is it using?

- Concept: Bias subjectivity and labeling inconsistency
  - Why needed here: Explains why human-labeled data creates challenges for both model training and evaluation
  - Quick check question: Why might two human evaluators disagree on whether a statement contains bias?

- Concept: Pattern recognition vs contextual reasoning
  - Why needed here: Distinguishes between bias types that rely on surface features versus those requiring deeper understanding
  - Quick check question: What type of cognitive process would a model need to detect subtle cognitive bias?

## Architecture Onboarding

- Component map: MBIB dataset → preprocessing → train/test split → fine-tuning → evaluation → ChatGPT prompting → JSON output → automated scoring
- Critical path: Prompt engineering → model inference → JSON output → automated scoring
- Design tradeoffs:
  - Zero-shot convenience vs fine-tuned accuracy
  - General language understanding vs task-specific precision
  - Computational efficiency vs comprehensive bias detection
- Failure signatures:
  - High false positive rates in gender/racial bias detection
  - Low scores on cognitive bias and fake news tasks
  - Inconsistent performance across different datasets within the same task
- First 3 experiments:
  1. Test ChatGPT with few-shot examples to measure improvement over zero-shot baseline
  2. Compare human evaluation scores with model predictions on a subset of examples
  3. Analyze confusion matrices to identify specific types of errors in each bias category

## Open Questions the Paper Calls Out

### Open Question 1
Can few-shot prompting significantly improve ChatGPT's performance on media bias detection tasks? The study mentions that few-shot prompting could potentially improve ChatGPT's performance but was not tested due to concerns about inconsistency with dataset examples.

### Open Question 2
How does ChatGPT's performance on media bias detection vary across different domains or topics? The paper tested ChatGPT on general datasets but didn't analyze performance variations across specific domains like politics, health, or entertainment.

### Open Question 3
What specific aspects of ChatGPT's training data or architecture contribute to its overestimation of gender and racial bias? The paper notes that ChatGPT tends to overestimate gender and racial bias but didn't investigate the underlying causes or mechanisms in ChatGPT's training that lead to this bias.

## Limitations
- Subjective nature of bias annotation creates inherent variability in ground truth labels
- Reliance on single MBIB dataset may not capture full diversity of real-world media bias scenarios
- Fundamental fairness issue comparing zero-shot ChatGPT with fine-tuned models using different training paradigms

## Confidence

**High Confidence**: ChatGPT performs comparably to fine-tuned models on hate speech detection (explicit evidence showing similar F1-scores).

**Medium Confidence**: ChatGPT underperforms on racial, gender, cognitive biases, and fake news detection (moderate support but could vary with different datasets).

**Low Confidence**: ChatGPT's over-sensitivity to bias in certain domains (based on observed patterns but lacks direct evidence about underlying causes).

## Next Checks

1. Cross-dataset validation: Test both ChatGPT and fine-tuned models on additional media bias datasets to verify whether performance patterns hold across different data sources.

2. Human evaluation study: Conduct blinded human evaluation of model predictions to assess agreement rates and identify specific types of errors.

3. Few-shot learning experiment: Test ChatGPT with 5-10 examples per task to determine whether minimal task-specific examples can significantly improve performance on challenging bias types.