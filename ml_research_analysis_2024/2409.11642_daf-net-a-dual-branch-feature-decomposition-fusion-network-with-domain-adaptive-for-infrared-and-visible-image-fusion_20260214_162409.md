---
ver: rpa2
title: 'DAF-Net: A Dual-Branch Feature Decomposition Fusion Network with Domain Adaptive
  for Infrared and Visible Image Fusion'
arxiv_id: '2409.11642'
source_url: https://arxiv.org/abs/2409.11642
tags:
- fusion
- image
- visible
- infrared
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes DAF-Net, a dual-branch feature decomposition
  fusion network with domain adaptation for infrared and visible image fusion. The
  method addresses the challenge of preserving key features during fusion due to significant
  differences between infrared and visible images.
---

# DAF-Net: A Dual-Branch Feature Decomposition Fusion Network with Domain Adaptive for Infrared and Visible Image Fusion

## Quick Facts
- **arXiv ID:** 2409.11642
- **Source URL:** https://arxiv.org/abs/2409.11642
- **Reference count:** 34
- **Primary result:** Proposes DAF-Net with MK-MMD for infrared-visible fusion, achieving superior performance on TNO dataset with EN=7.16, SD=45.02, SF=12.63, MI=2.06, SCD=1.80, VIF=0.75, QAB/F=0.54, SSIM=0.68

## Executive Summary
DAF-Net addresses the challenge of infrared and visible image fusion by introducing a dual-branch architecture that combines global structural information extraction with detailed texture preservation. The method incorporates Multi-Kernel Maximum Mean Discrepancy (MK-MMD) into a Restormer-based encoder to align the feature spaces of infrared and visible images. A detail encoder based on Invertible Neural Networks (INN) captures fine texture information. Experimental results demonstrate superior performance across multiple fusion metrics on the TNO dataset, showing significant improvements over existing fusion techniques.

## Method Summary
The proposed DAF-Net architecture employs a dual-branch design to address the domain gap between infrared and visible images. The base encoder, built on the Restormer network, captures global structural information while the detail encoder, based on INN, focuses on extracting fine texture details. MK-MMD is integrated into the base encoder to align the latent feature spaces of the two domains. A hybrid kernel function is specifically designed for infrared-visible image fusion. The fused output combines information from both branches, preserving both structural integrity and detailed textures. The network is trained end-to-end with a composite loss function that balances domain alignment and fusion quality.

## Key Results
- Achieves EN of 7.16, SD of 45.02, SF of 12.63, MI of 2.06, SCD of 1.80, VIF of 0.75, QAB/F of 0.54, and SSIM of 0.68 on TNO dataset
- Demonstrates superior performance compared to existing fusion methods across multiple metrics
- Shows significant improvement in visual quality and fusion performance for infrared-visible image pairs

## Why This Works (Mechanism)
DAF-Net's effectiveness stems from its dual-branch architecture that separately handles global structural information and fine texture details. The MK-MMD component addresses the fundamental domain gap between infrared and visible images by aligning their latent feature spaces, reducing misalignment artifacts in the fused output. The hybrid kernel function is specifically tailored for the infrared-visible fusion task, improving the effectiveness of domain adaptation. By combining the Restormer network's global feature extraction with INN's invertible detail preservation, the method captures complementary information from both modalities while maintaining the ability to reconstruct fine details.

## Foundational Learning

**Multi-Kernel Maximum Mean Discrepancy (MK-MMD)**
- *Why needed:* Addresses domain shift between infrared and visible image feature distributions
- *Quick check:* Verify kernel selection impacts domain alignment effectiveness

**Restormer Network**
- *Why needed:* Provides efficient global structural feature extraction with attention mechanisms
- *Quick check:* Compare attention patterns between infrared and visible inputs

**Invertible Neural Networks (INN)**
- *Why needed:* Enables lossless detail extraction and reconstruction for texture preservation
- *Quick check:* Verify invertibility preserves input information during detail extraction

**Hybrid Kernel Design**
- *Why needed:* Adapts MMD to specific characteristics of infrared-visible fusion
- *Quick check:* Test different kernel combinations for optimal domain alignment

**Dual-Branch Architecture**
- *Why needed:* Separates global structure from local detail processing
- *Quick check:* Analyze feature maps from each branch independently

## Architecture Onboarding

**Component Map:** Input Images -> Base Encoder (Restormer + MK-MMD) -> Detail Encoder (INN) -> Fusion Layer -> Output Image

**Critical Path:** Image Input → Restormer Encoder → MK-MMD Alignment → Detail Encoder → Fusion Layer → Final Output

**Design Tradeoffs:** Global vs. detail information processing; domain alignment complexity vs. fusion quality; computational efficiency vs. metric performance

**Failure Signatures:** Poor domain alignment manifests as ghosting artifacts; inadequate detail extraction shows as blurred textures; suboptimal kernel selection reduces metric scores

**First Experiments:** 1) Test MK-MMD ablation to measure domain alignment contribution, 2) Compare single-kernel vs. multi-kernel MMD performance, 3) Evaluate different Restormer depth configurations

## Open Questions the Paper Calls Out
None

## Limitations
- Limited comparison with recent state-of-the-art fusion methods from 2022-2023
- No ablation studies isolating MK-MMD contribution from other architectural components
- Single dataset validation (TNO) raises concerns about generalization to other scenarios

## Confidence
- **High:** Technical soundness of combining Restormer-based encoder with INN-based detail extraction
- **Medium:** Effectiveness of MK-MMD for domain alignment in infrared-visible fusion application
- **Medium:** Claimed superiority over existing methods based on reported metrics
- **Low:** Generalizability across diverse real-world scenarios beyond TNO dataset

## Next Checks
1. Conduct cross-dataset evaluation using multiple infrared-visible fusion datasets (e.g., Oxford, CVC-14) to verify generalization
2. Perform ablation studies removing the MK-MMD component to quantify its specific contribution to performance gains
3. Test the method on real-time video sequences to assess temporal consistency and computational efficiency for practical deployment