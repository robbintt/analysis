---
ver: rpa2
title: 'ERD: A Framework for Improving LLM Reasoning for Cognitive Distortion Classification'
arxiv_id: '2403.14255'
source_url: https://arxiv.org/abs/2403.14255
tags:
- distortion
- debate
- cognitive
- reasoning
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of cognitive distortion classification
  from user utterances in psychotherapy, where existing methods tend to overdiagnose
  distortions. The proposed ERD framework introduces three steps: Extraction (identifying
  distorted parts of speech), Reasoning (using LLM-based diagnosis-of-thought), and
  Debate (multi-agent discussion to refine decisions).'
---

# ERD: A Framework for Improving LLM Reasoning for Cognitive Distortion Classification

## Quick Facts
- **arXiv ID**: 2403.14255
- **Source URL**: https://arxiv.org/abs/2403.14255
- **Reference count**: 10
- **Primary result**: ERD achieves 9%+ improvement in multi-class F1 score and over 25% improvement in binary specificity for cognitive distortion classification

## Executive Summary
This paper addresses the problem of cognitive distortion classification from user utterances in psychotherapy, where existing methods tend to overdiagnose distortions. The proposed ERD framework introduces three steps: Extraction (identifying distorted parts of speech), Reasoning (using LLM-based diagnosis-of-thought), and Debate (multi-agent discussion to refine decisions). Tested on a public dataset of 2530 samples, ERD achieves significant improvements in both classification accuracy and specificity, demonstrating that structured multi-agent reasoning can effectively reduce false positives while maintaining high sensitivity.

## Method Summary
ERD is a three-step framework for cognitive distortion classification that uses large language models to improve reasoning accuracy. The framework first extracts distorted segments from user utterances, then applies diagnosis-of-thought reasoning to generate thought processes, and finally uses multi-agent debate to refine decisions. The debate involves two opposing agents and a judge who summarizes and evaluates arguments before making the final classification. The system was implemented using gpt-3.5-turbo with temperature 0.1 and tested on a Kaggle cognitive distortion detection dataset containing 10 types of cognitive distortions plus neutral cases.

## Key Results
- ERD achieves 9%+ improvement in multi-class F1 score compared to baseline methods
- Binary specificity improves by over 25% compared to existing approaches
- Both Extraction and Debate modules contribute significantly to performance gains, with debate rounds and judge summarization further enhancing accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Extracting only the distorted part of the utterance improves classification performance by reducing noise and focusing the LLM's attention on the most informative features.
- **Mechanism**: The Extraction step isolates the relevant segment containing cognitive distortions, preventing the LLM from being distracted by neutral or unrelated content in the full utterance.
- **Core assumption**: The distorted parts contain the key features needed for accurate classification, and removing non-distorted content does not lose critical contextual information.
- **Evidence anchors**:
  - Table 1 shows the multi-class F1 score of Diagnosis-of-Thought (DoT) method for distortion classification problem... Putting only the distorted part significantly improves the classification performance, which motivates the Extraction step in ERD framework.
  - "Extraction (identifying distorted parts of speech)" as the first step in the three-step process.

### Mechanism 2
- **Claim**: Multi-agent debate reduces overdiagnosis by forcing LLMs to consider alternative viewpoints and justify their reasoning.
- **Mechanism**: The Debate step introduces two debaters with opposing views and a judge agent, creating a structured argumentation process that challenges initial assumptions and reduces false positives.
- **Core assumption**: LLMs can effectively simulate debate dynamics and the judge can fairly evaluate competing arguments to reach a more accurate conclusion.
- **Evidence anchors**:
  - "Debate (multi-agent discussion to refine decisions)" and results showing "over 25% improvement in binary specificity compared to baseline methods."
  - Table 2 shows the performances of ERD... adding Debate module not only improves the distortion classification score by around 7%, but also improves the distortion assessment specificity by more than 25%.

### Mechanism 3
- **Claim**: Judge summarization and validity evaluation improve final decision quality by integrating multiple perspectives and assessing argument strength.
- **Mechanism**: The judge agent performs two levels of processing: (1) summarizing the debate content to create a coherent overview, and (2) evaluating which arguments are more valid before making the final classification.
- **Core assumption**: LLMs can effectively summarize complex multi-turn debates and assess argument validity to improve decision-making.
- **Evidence anchors**:
  - Table 3 shows the effect of such prompts for three variants... Both summarization and validity evaluation steps improve the performance in terms of specificity.
  - Table 4 shows how the performance improves as we increase r, the number of Debate rounds used in ERD.

## Foundational Learning

- **Concept**: Cognitive distortions and their types
  - **Why needed here**: The system must distinguish between 10 different distortion types plus neutral cases, requiring understanding of what constitutes each distortion.
  - **Quick check question**: Can you name three common cognitive distortion types and explain how they differ from neutral thinking?

- **Concept**: Chain-of-thought reasoning and its application
  - **Why needed here**: The Reasoning step uses methods like Diagnosis-of-Thought that rely on structured reasoning processes, requiring understanding of how CoT prompting works.
  - **Quick check question**: What are the three critical stages of the Diagnosis-of-Thought method mentioned in the paper?

- **Concept**: Multi-agent debate dynamics
  - **Why needed here**: The Debate step requires understanding how multiple LLM agents can interact to improve decision quality through structured argumentation.
  - **Quick check question**: How does the judge agent in ERD ensure fair evaluation of the debaters' arguments?

## Architecture Onboarding

- **Component map**: User Speech → Extraction → Reasoning → Debate → Judge Decision → Final Output
- **Critical path**: User Speech → Extraction → Reasoning → Debate → Judge Decision → Final Output
- **Design tradeoffs**:
  - Extraction vs. full-context reasoning: Extraction improves specificity but may lose context; full-context reasoning preserves context but increases false positives
  - Number of debate rounds: More rounds improve performance but increase computational cost and latency
  - Judge complexity: Simple summarization is faster but less accurate than validity evaluation
- **Failure signatures**:
  - High false positives: Debate module not effectively challenging initial assumptions
  - Poor multi-class performance: Reasoning module not adequately distinguishing between distortion types
  - Low sensitivity: Extraction module too aggressive in removing content
- **First 3 experiments**:
  1. Test Extraction module alone with ground-truth distorted parts to verify the 10%+ improvement baseline
  2. Implement single-round debate with two debaters and simple judge to measure specificity improvement
  3. Compare judge prompting strategies (no summarization vs. summarization vs. summarization + validity evaluation) on the same dataset

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do the performance gains of ERD compare to human expert classification in cognitive distortion detection?
- **Basis in paper**: [explicit] The paper compares ERD to baseline methods but does not benchmark against human performance.
- **Why unresolved**: The paper focuses on comparing ERD to other LLM-based approaches without providing a comparison to human expert classification accuracy or specificity.
- **What evidence would resolve it**: A direct comparison study where ERD's classification performance is measured against human experts on the same dataset, including sensitivity, specificity, and F1 scores.

### Open Question 2
- **Question**: What is the optimal number of debate rounds in ERD for different types of cognitive distortions?
- **Basis in paper**: [explicit] The paper tests 1-3 debate rounds but notes performance plateaus after 2 rounds without exploring type-specific optimization.
- **Why unresolved**: The study only tests a general case without investigating whether different cognitive distortion types might benefit from different numbers of debate rounds.
- **What evidence would resolve it**: A detailed analysis comparing ERD performance across different cognitive distortion types with varying numbers of debate rounds to identify optimal configurations.

### Open Question 3
- **Question**: How does ERD's performance generalize to different languages and cultural contexts?
- **Basis in paper**: [explicit] The study is conducted on a single dataset without testing multilingual or cross-cultural validity.
- **Why unresolved**: The paper does not explore whether ERD's effectiveness transfers to different languages or cultural contexts where cognitive distortions might be expressed differently.
- **What evidence would resolve it**: Testing ERD on datasets from different languages and cultures, comparing performance across contexts to validate generalizability.

## Limitations

- The study relies on a single dataset and does not test generalizability across different populations or cultural contexts
- Performance is evaluated only on gpt-3.5-turbo with fixed parameters, limiting generalizability to other models
- Computational cost of multi-round debate and scalability for real-time clinical applications is not addressed

## Confidence

- **High confidence**: The core observation that Extraction improves specificity and reduces false positives is well-supported by comparative results across multiple baseline methods
- **Medium confidence**: The Debate module's effectiveness is demonstrated, but the specific mechanisms by which multi-agent discussion improves classification could benefit from more detailed analysis
- **Medium confidence**: The improvement from judge summarization and validity evaluation is shown, but the relative contribution of each component is not fully isolated

## Next Checks

1. **Ablation study on debate mechanics**: Systematically vary the number of debate rounds, debater expertise levels, and judge control methods to quantify their individual contributions to performance gains

2. **Cross-dataset validation**: Test ERD on multiple cognitive distortion datasets (including KoACD and other clinical conversation datasets) to assess generalizability across different populations and discourse styles

3. **Human evaluation of debate quality**: Have clinical experts review sample debate transcripts to assess whether the multi-agent discussions produce clinically meaningful reasoning improvements beyond what automated metrics capture