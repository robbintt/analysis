---
ver: rpa2
title: 'Unknown Script: Impact of Script on Cross-Lingual Transfer'
arxiv_id: '2404.18810'
source_url: https://arxiv.org/abs/2404.18810
tags:
- language
- script
- transfer
- cross-lingual
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates cross-lingual transfer to a new language
  (Amharic) with a unique script not seen during pre-training of language models.
  The study examines how factors like script, tokenizer, and language similarity affect
  transfer performance.
---

# Unknown Script: Impact of Script on Cross-Lingual Transfer

## Quick Facts
- arXiv ID: 2404.18810
- Source URL: https://arxiv.org/abs/2404.18810
- Authors: Wondimagegnhue Tsegaye Tufa; Ilia Markov; Piek Vossen
- Reference count: 10
- Primary result: Tokenizer type is a stronger factor than script similarity, language similarity, or model size in cross-lingual transfer to unseen scripts

## Executive Summary
This paper investigates how script differences affect cross-lingual transfer when moving from pre-trained models to a new language (Amharic) with a unique script not seen during pre-training. The study examines the impact of tokenizer type, language similarity, and romanization on transfer performance for Named Entity Recognition and Part-of-Speech tagging tasks. Experiments compare monolingual and multilingual models across different tokenizers (BPE, WordPiece, SentencePiece, character-based) using both the original Fidel script and romanized versions of Amharic.

## Method Summary
The study fine-tunes pre-trained monolingual and multilingual models on Amharic NER and POS tasks using a few-shot setting (1,750 training examples). Models tested include BERT, RoBERTa, ALBERT, BERT-base-arabic, CANINE-c, and m-BERT, all in base versions. Experiments are conducted in both the original Fidel script and a romanized version of Amharic, with performance measured using F1-score averaged over five runs. The study systematically compares different tokenizers (SentencePiece, BPE, character-based, WordPiece) and evaluates both monolingual and multilingual models to determine which factors most strongly influence cross-lingual transfer success.

## Key Results
- BPE tokenizer used in RoBERTa enables better cross-lingual transfer to unseen scripts compared to other tokenizers
- Romanization significantly improves transfer performance for models using sub-word tokenizers
- Model size alone does not determine cross-lingual transfer success - tokenizer-script compatibility is more critical

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BPE tokenizer enables better cross-lingual transfer to unseen scripts compared to other tokenizers.
- Mechanism: BPE tokenization operates on raw bytes rather than Unicode characters, creating subword units that can represent unseen scripts through byte-level decomposition.
- Core assumption: The subword structure learned on English data transfers effectively to morphologically complex languages with unique scripts.
- Evidence anchors:
  - [abstract] "the BPE tokenizer used in RoBERTa enables better transfer compared to other tokenizers"
  - [section] "RoBERTa is trained using BPE over raw bytes instead of Unicode characters. The results show that the BPE representation enables the model to leverage knowledge that benefits the downstream tasks"
  - [corpus] "Breaking the Script Barrier in Multilingual Pre-Trained Language Models with Transliteration-Based Post-Training Alignment" suggests BPE is effective for script transfer
- Break condition: If the target language has highly agglutinative morphology or character-level dependencies that require whole-character processing, BPE decomposition may lose critical information.

### Mechanism 2
- Claim: Romanization significantly improves transfer performance for subword-based models.
- Mechanism: Converting unique scripts to Latin script creates lexical overlap with pre-training data, allowing subword tokenizers to leverage learned representations.
- Core assumption: The semantic content is preserved during romanization, and the model can map romanized representations back to the original meaning.
- Evidence anchors:
  - [abstract] "Romanization also significantly improves transfer performance for models using sub-word tokenizers"
  - [section] "The difference in the obtained results for Fidel and romanized versions highlights the script's effect on model performance"
  - [corpus] "One Script Instead of Hundreds? On Pretraining Romanized Encoder Language Models" supports romanization benefits
- Break condition: If romanization introduces ambiguity or loses critical morphological distinctions, the performance gain may diminish or reverse.

### Mechanism 3
- Claim: Model size alone does not determine cross-lingual transfer success.
- Mechanism: While larger models have more parameters, the interaction between tokenizer type and script compatibility is more critical than parameter count.
- Core assumption: The representational capacity of subword tokenization patterns learned during pre-training outweighs raw parameter count.
- Evidence anchors:
  - [section] "RoBERTa-base is the largest model with 125 million parameters. However, the performance does not consistently correlate with the model size"
  - [abstract] "tokenizer as a stronger factor than the shared script, language similarity, and model size"
  - [corpus] "What Drives Performance in Multilingual Language Models?" investigates factors beyond size
- Break condition: If the target language has extreme morphological complexity requiring very deep representations, larger models may eventually outperform smaller ones regardless of tokenizer.

## Foundational Learning

- Concept: Cross-lingual transfer learning
  - Why needed here: The entire paper examines how knowledge transfers from pre-trained models to unseen languages
  - Quick check question: What is the difference between zero-shot and few-shot cross-lingual transfer?

- Concept: Subword tokenization (BPE, WordPiece, SentencePiece)
  - Why needed here: The paper directly compares different tokenization methods and their impact on transfer performance
  - Quick check question: How does BPE tokenization handle unseen characters compared to WordPiece?

- Concept: Script romanization and transliteration
  - Why needed here: The paper evaluates both original script and romanized versions to measure script impact
  - Quick check question: What information might be lost when romanizing a morphologically complex language like Amharic?

## Architecture Onboarding

- Component map: Pre-trained monolingual/multilingual models → Fine-tuning on target task → Evaluation on original/romanized script
- Critical path: Model selection (tokenizer type) → Fine-tuning configuration → Script choice (original vs romanized) → Performance measurement
- Design tradeoffs: BPE tokenization offers better cross-lingual transfer but may lose character-level information; romanization improves transfer but may introduce ambiguity
- Failure signatures: Complete failure on original script with minor improvements on romanized version suggests tokenizer-script mismatch; inconsistent performance across tasks suggests task-specific tokenization requirements
- First 3 experiments:
  1. Compare BPE vs WordPiece tokenization performance on romanized Amharic to isolate tokenizer effect
  2. Test character-level tokenizer (CANINE) on original Amharic script to evaluate character-level processing benefits
  3. Evaluate transfer from Arabic BERT (typologically similar) vs English BERT to measure language similarity impact independent of tokenizer

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of the BPE tokenizer generalize to other non-Latin scripts beyond Amharic?
- Basis in paper: [inferred] The paper demonstrates that the BPE tokenizer (used in RoBERTa) outperforms other tokenizers for Amharic, a non-Latin script. This suggests potential generalization, but it remains untested.
- Why unresolved: The study only examines Amharic. Without testing other non-Latin scripts, we cannot confirm if BPE's effectiveness is universal.
- What evidence would resolve it: Conducting similar experiments on multiple non-Latin scripts (e.g., Chinese, Japanese, Hindi) and comparing BPE performance to other tokenizers.

### Open Question 2
- Question: How does the size of the pre-training corpus in the target language's script affect cross-lingual transfer performance?
- Basis in paper: [explicit] The paper notes that Amharic is under-resourced, but doesn't explore how varying amounts of script-specific pre-training data might impact transfer.
- Why unresolved: The study uses existing models with fixed pre-training data. It doesn't isolate the effect of script-specific corpus size.
- What evidence would resolve it: Training multiple models with varying amounts of Amharic script data (while keeping other factors constant) and measuring transfer performance.

### Open Question 3
- Question: Is there an optimal tokenizer-subword unit size ratio for cross-lingual transfer to unseen scripts?
- Basis in paper: [inferred] The paper compares different tokenizers but doesn't analyze how their subword unit sizes (e.g., BPE merge operations) impact transfer to unseen scripts.
- Why unresolved: The study uses pre-trained models with fixed tokenizer configurations. It doesn't explore how varying subword unit sizes within the same tokenizer type affects performance.
- What evidence would resolve it: Training multiple models with the same tokenizer but different subword unit sizes, then testing transfer to unseen scripts.

## Limitations
- Single-target-language design limits generalizability to other morphologically complex languages with unique scripts
- Few-shot setting (1,750 training examples) may not reflect real-world transfer scenarios with larger datasets
- No confidence intervals or statistical significance testing provided for performance differences

## Confidence
- High Confidence: Tokenizer type being a stronger factor than script similarity or model size is well-supported by experimental design
- Medium Confidence: BPE tokenization advantage is demonstrated but mechanism explanation relies partly on external literature
- Medium Confidence: Romanization benefits are clearly shown but semantic preservation versus lexical overlap remains uncertain

## Next Checks
1. Cross-linguistic generalization test: Replicate experiments with 2-3 additional morphologically complex languages with unique scripts (e.g., Georgian, Armenian, Thai)
2. Ablation study on romanization quality: Compare different romanization schemes to quantify quality impact and morphological information loss correlation
3. Statistical significance validation: Perform paired statistical tests on five-run F1-score distributions to establish significant performance differences