---
ver: rpa2
title: 'This Looks Better than That: Better Interpretable Models with ProtoPNeXt'
arxiv_id: '2406.14675'
source_url: https://arxiv.org/abs/2406.14675
tags: []
core_contribution: This paper presents ProtoPNeXt, a framework for systematically
  developing and optimizing interpretable prototypical-part neural networks (ProtoPNets).
  The authors show that applying Bayesian hyperparameter tuning and an angular (cosine)
  prototype similarity metric to the original ProtoPNet is sufficient to achieve state-of-the-art
  accuracy on the CUB-200 dataset across multiple backbones.
---

# This Looks Better than That: Better Interpretable Models with ProtoPNeXt

## Quick Facts
- arXiv ID: 2406.14675
- Source URL: https://arxiv.org/abs/2406.14675
- Reference count: 40
- Primary result: Systematic optimization of prototypical-part networks with cosine similarity and joint accuracy-interpretability optimization achieves state-of-the-art performance

## Executive Summary
This paper presents ProtoPNeXt, a framework for systematically developing and optimizing interpretable prototypical-part neural networks (ProtoPNets). The authors demonstrate that applying Bayesian hyperparameter tuning and replacing Euclidean distance with angular (cosine) similarity in prototype comparison substantially improves accuracy on the CUB-200 dataset. They also show that jointly optimizing for accuracy and prototype interpretability metrics (sparsity, stability, and consistency) produces models with superior semantics while maintaining or slightly improving accuracy. The framework provides a principled approach to balancing model performance with interpretability in neural networks.

## Method Summary
ProtoPNeXt extends prototypical-part networks by incorporating three key innovations: (1) using cosine similarity instead of Euclidean distance for prototype comparison, (2) implementing Bayesian hyperparameter optimization to efficiently explore the design space, and (3) jointly optimizing accuracy with prototype interpretability metrics including sparsity, stability, and consistency. The framework uses a three-module architecture where a CNN backbone extracts latent features, prototypes are compared to these features using cosine similarity, and activations are aggregated into class predictions. Training includes clustering and separation losses plus projection steps to tie prototypes to image patches.

## Key Results
- Replacing Euclidean distance with cosine similarity in prototype comparison achieves state-of-the-art accuracy on CUB-200 across multiple backbones
- Joint optimization of accuracy and prototype interpretability metrics produces models with substantially superior semantics while maintaining accuracy within ±1.5%
- Bayesian hyperparameter tuning is essential for achieving optimal performance in ProtoPNets under fixed computational budgets

## Why This Works (Mechanism)

### Mechanism 1: Cosine Similarity for Prototype Comparison
Cosine similarity normalizes feature vectors to unit length, making comparisons invariant to scale and focusing on directional alignment between prototypes and image features. This aligns better with how prototypes represent semantic parts in the latent space. The mechanism breaks down if the latent space geometry does not benefit from angular comparison, such as when features are not normalized or scale differences are meaningful.

### Mechanism 2: Joint Optimization of Accuracy and Interpretability
By including prototype quality metrics in the objective function, the optimization explicitly encourages prototypes to be sparse (few unique prototypes used), stable (resistant to noise), and consistent (activate on the same semantic part). This regularizes the model toward more interpretable behavior while maintaining performance. The approach fails if prototype quality metrics do not correlate well with human interpretability or if the joint objective creates conflicting gradients.

### Mechanism 3: Bayesian Hyperparameter Optimization
ProtoPNets are highly sensitive to hyperparameters (learning rates, regularization coefficients, etc.). Bayesian optimization efficiently explores this high-dimensional, non-convex space under fixed computational budgets, finding configurations that balance accuracy and interpretability. This mechanism is ineffective if the hyperparameter space is simpler than assumed or if the computational budget is insufficient for meaningful exploration.

## Foundational Learning

- **Concept: Cosine similarity vs Euclidean distance**
  - Why needed here: The paper claims switching to cosine similarity is a key driver of performance gains
  - Quick check question: What is the main difference between cosine similarity and Euclidean distance in terms of what they measure between vectors?

- **Concept: Prototype sparsity, stability, and consistency metrics**
  - Why needed here: These metrics are used to quantify and optimize interpretability
  - Quick check question: How does prototype sparsity relate to the number of unique prototypes a model uses?

- **Concept: Bayesian hyperparameter optimization**
  - Why needed here: The paper uses this to efficiently tune ProtoPNet variants under fixed computational cost
  - Quick check question: What is the advantage of Bayesian optimization over random search in high-dimensional spaces?

## Architecture Onboarding

- **Component map:** Input image → CNN backbone (embedding layer) → Prototype layer with cosine similarity → Prediction head (aggregation) → Class logits
- **Critical path:** Embed → Compare (cosine) → Aggregate → Predict
- **Design tradeoffs:**
  - Cosine vs Euclidean: Cosine is more robust to scale but may lose magnitude information
  - Number of prototypes: More prototypes can improve accuracy but reduce sparsity and interpretability
  - Prototype spatial size: Larger prototypes cover more area but may be less precise
- **Failure signatures:**
  - Low sparsity: Model uses many redundant prototypes → increase regularization or reduce prototype count
  - Poor stability: Prototypes activate inconsistently → increase noise in training or adjust stability loss weight
  - Slow convergence: May indicate poor hyperparameter choice → revisit learning rates or clustering coefficients
- **First 3 experiments:**
  1. Train a basic ProtoPNet with cosine similarity on CUB-200; compare validation accuracy to Euclidean baseline
  2. Add prototype quality metrics to the loss and jointly optimize; measure changes in sparsity/stability/consistency
  3. Vary the number of prototypes per class and observe impact on accuracy and interpretability metrics

## Open Questions the Paper Calls Out

- **Open Question 1:** How do different prototype quality metrics correlate with human interpretability assessments?
  - Basis: The authors discuss optimizing for interpretability metrics but acknowledge these may not fully capture interpretability
  - Why unresolved: The paper relies on quantitative metrics rather than human studies
  - What evidence would resolve it: User studies comparing human understanding of models optimized for different interpretability metrics versus those optimized for accuracy alone

- **Open Question 2:** What is the optimal balance between accuracy and prototype quality in the joint optimization objective?
  - Basis: The authors use equal weighting (1:1 ratio) but note this may not be appropriate for all tasks
  - Why unresolved: The paper doesn't explore different weightings between accuracy and prototype quality
  - What evidence would resolve it: Experiments varying the weighting between accuracy and prototype quality in the joint objective across different datasets and tasks

- **Open Question 3:** How does the choice of latent space size affect the interpretability and accuracy of prototypical-part models?
  - Basis: The authors note they used a fixed 7x7 latent space size and suggest a more comprehensive study would tune this hyperparameter
  - Why unresolved: The paper focuses on other hyperparameters but doesn't explore the impact of latent space dimensions
  - What evidence would resolve it: Experiments training models with different latent space sizes and measuring their accuracy and interpretability metrics

## Limitations

- The claims about cosine similarity driving substantial accuracy gains lack external validation from the broader literature
- The evaluation of "superior semantics" is qualitative and could benefit from more rigorous human studies
- The computational cost of Bayesian optimization (200 model evaluations) is substantial and may limit reproducibility

## Confidence

- **High confidence:** Framework implementation and experimental methodology are clearly specified and reproducible
- **Medium confidence:** Quantitative results (accuracy improvements of 1.3% to -1.5%) are directly measured, but interpretation depends on baselines
- **Low confidence:** Claims about "substantially superior semantics" rely heavily on qualitative visual inspection without systematic human evaluation

## Next Checks

1. Replicate the cosine vs Euclidean ablation on CUB-200 with multiple random seeds to verify claimed accuracy differences are consistent
2. Conduct a user study comparing prototype interpretability between models optimized for accuracy alone versus joint accuracy-interpretability optimization
3. Test the framework on a held-out dataset (e.g., Stanford Dogs) to verify generalization beyond the development set