---
ver: rpa2
title: Empowering Embodied Visual Tracking with Visual Foundation Models and Offline
  RL
arxiv_id: '2404.09857'
source_url: https://arxiv.org/abs/2404.09857
tags:
- visual
- tracking
- offline
- target
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a framework for embodied visual tracking that
  combines visual foundation models (VFMs) with offline reinforcement learning (RL).
  The key idea is to use VFMs to generate text-conditioned segmentation masks as state
  representations, then train a recurrent policy network via offline RL using Conservative
  Q-Learning.
---

# Empowering Embodied Visual Tracking with Visual Foundation Models and Offline RL

## Quick Facts
- arXiv ID: 2404.09857
- Source URL: https://arxiv.org/abs/2404.09857
- Reference count: 40
- Key outcome: Achieves superior embodied visual tracking performance with 1-hour training on consumer GPU vs 12+ hours for online RL baselines

## Executive Summary
This paper introduces a framework that combines visual foundation models (VFMs) with offline reinforcement learning to solve embodied visual tracking. The key innovation is using VFMs to generate text-conditioned segmentation masks as state representations, which are then used to train a recurrent policy network via Conservative Q-Learning. This approach addresses the training inefficiency and poor generalization issues of existing methods, achieving state-of-the-art performance in accuracy, robustness to distractors, and generalization to unseen scenarios while requiring minimal training time.

## Method Summary
The method uses pre-trained VFMs to extract semantic segmentation masks from RGB images using text prompts that specify target and background objects. These masks serve as compact state representations that reduce domain gap. A recurrent policy network (CNN-LSTM) processes the mask sequence and previous hidden state to output actions. The policy is trained offline using Conservative Q-Learning on a dataset collected by an imperfect expert (PID controller with noise injection). This multi-level data collection strategy creates diverse demonstrations that improve generalization to imperfect tracking scenarios.

## Key Results
- Outperforms state-of-the-art methods in accuracy, robustness to distractors, and generalization
- Requires only 1 hour of training on consumer GPU vs 12+ hours for online RL baselines
- Successfully transfers from simulation to real-world robot tracking

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visual foundation models (VFMs) with text-conditioned segmentation masks reduce domain gap and improve generalization.
- Mechanism: VFMs abstract high-dimensional RGB images into semantic masks highlighting targets and obstacles while removing background noise. Text prompts specify object categories, and re-targeting ensures consistent target identification across episodes.
- Core assumption: VFMs can reliably segment targets and obstacles across diverse environments using text prompts without requiring fine-tuning.
- Evidence anchors:
  - [abstract] "We use a pre-trained VFM...to extract semantic segmentation masks with text prompts"
  - [section 4.1] "The open-vocabulary tracking model can abstract and generalize the state representation by combining semantic knowledge and visual features"
  - [corpus] Weak - no direct evidence found in related papers
- Break condition: If VFM segmentation fails to identify targets in new environments or text prompts are ambiguous, the mask representation becomes unreliable.

### Mechanism 2
- Claim: Offline RL with Conservative Q-Learning (CQL) enables training without online interactions while maintaining performance.
- Mechanism: CQL learns from fixed datasets by penalizing Q-values that deviate from the data distribution, preventing overestimation. A recurrent policy network (CNN-LSTM) captures temporal dependencies in the mask sequence.
- Core assumption: Collected demonstration data is diverse enough to cover relevant state-action pairs for good policy generalization.
- Evidence anchors:
  - [abstract] "We then train a recurrent policy network with offline RL, e.g., Conservative Q-Learning, to learn from the collected demonstrations without online interactions"
  - [section 4.3] "CQL-SAC consists of three neural networks: two critic networkQ1θ, Q2θ, used for estimate Q values and an actor networkVϕ"
  - [corpus] Weak - no direct evidence found in related papers
- Break condition: If the offline dataset lacks coverage of important state transitions or contains significant noise, the learned policy will fail in unseen scenarios.

### Mechanism 3
- Claim: Multi-level demonstration collection with perturbed expert policies creates diverse training data for better generalization.
- Mechanism: An imperfect expert policy (PID controller) generates trajectories with varying noise levels. Higher noise levels create less optimal demonstrations, while lower noise provides near-optimal data. This diversity helps the policy handle imperfect tracking situations.
- Core assumption: A state-based PID controller can serve as a reasonable expert policy for collecting initial demonstrations.
- Evidence anchors:
  - [section 4.2] "We use a state-based PID controller as the expert policy. To simulate the policy with different skill levels, we add noise to the output actions"
  - [section C.1] "We adopt PID controllers to move an agent to follow a target object and keep the target in a specific distance and relative angle"
  - [corpus] Weak - no direct evidence found in related papers
- Break condition: If the expert policy is too imperfect, the collected data may teach incorrect behaviors. If too perfect, the policy may overfit to ideal conditions.

## Foundational Learning

- Concept: Reinforcement Learning (RL) basics
  - Why needed here: The framework uses RL to learn a policy that maps observations to actions for visual tracking
  - Quick check question: What is the difference between value-based and policy-based RL methods?

- Concept: Visual representation learning
  - Why needed here: VFMs convert raw images into abstract representations that improve generalization across environments
  - Quick check question: How do text-conditioned masks differ from instance segmentation masks in terms of information content?

- Concept: Offline RL vs Online RL
  - Why needed here: The method avoids expensive online interaction by learning from pre-collected demonstrations
  - Quick check question: What is the main challenge in offline RL that CQL addresses?

## Architecture Onboarding

- Component map:
  Visual Foundation Model (VFM) → Segmentation Mask Encoder → Segmentation Mask → Text-Conditioned Representation → Mask + Previous Hidden State → Recurrent Policy Network (CNN-LSTM) → Action Output (angular and linear velocity) → Offline Dataset → Conservative Q-Learning (CQL) Training

- Critical path: VFM inference → Mask preprocessing → Policy network forward pass → Action output

- Design tradeoffs:
  - VFM choice: DEVA/SAM-Track offer real-time inference but may have accuracy differences
  - Dataset size vs noise level: Larger datasets improve performance but require more storage; higher noise increases diversity but may introduce poor demonstrations
  - Mask representation vs raw pixels: Masks reduce dimensionality and domain gap but may lose fine-grained information

- Failure signatures:
  - VFM fails to segment target: Mask shows no white region or incorrect object highlighted
  - Policy produces erratic actions: Agent moves randomly or oscillates instead of tracking smoothly
  - Offline RL doesn't converge: Training loss plateaus at high values or shows high variance

- First 3 experiments:
  1. Verify VFM inference: Run VFM on sample images and check mask quality with different text prompts
  2. Test policy inference: Feed precomputed masks through the policy network and verify action outputs are reasonable
  3. Validate offline training: Train policy on small dataset and check if loss decreases and policy produces sensible actions on validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of text prompts affect the segmentation mask quality and downstream tracking performance?
- Basis in paper: [explicit] The paper mentions using text prompts like {"person", "obstacles"} to specify target and background objects, but does not systematically analyze prompt sensitivity or provide guidelines for prompt selection.
- Why unresolved: While the paper demonstrates that text-conditioned masks improve performance, it does not explore the robustness of this approach to different prompt formulations or the potential need for prompt engineering.
- What evidence would resolve it: Systematic experiments varying prompt formulations and analyzing their impact on segmentation accuracy and tracking performance across different environments and object categories.

### Open Question 2
- Question: What is the optimal noise level and dataset size for collecting demonstrations for offline RL?
- Basis in paper: [explicit] The paper mentions using multi-level action noise and collecting 42k steps of data, but presents limited ablation studies on dataset size and noise level variations.
- Why unresolved: The paper provides some evidence that moderate noise levels work best, but does not explore the full trade-off space between dataset size, noise level, and policy performance.
- What evidence would resolve it: Comprehensive experiments varying both dataset size and noise level parameters, with detailed analysis of how these factors affect policy convergence and generalization.

### Open Question 3
- Question: How does the proposed method generalize to more complex environments with multiple moving targets and dynamic obstacles?
- Basis in paper: [inferred] The paper tests the method in environments with distractors and various obstacles, but does not explore scenarios with multiple moving targets or highly dynamic obstacle configurations.
- Why unresolved: While the method shows robustness to static and simple dynamic obstacles, the complexity of real-world environments often involves multiple interacting agents and unpredictable obstacle movements.
- What evidence would resolve it: Testing the method in simulation environments with multiple moving targets and complex dynamic obstacle interactions, analyzing tracking performance and failure modes.

## Limitations
- The method relies heavily on VFM performance; segmentation failures directly impact policy effectiveness
- Offline RL performance is bounded by the quality and diversity of collected demonstrations
- Real-world transfer success depends on sim-to-real gap bridging, which is not fully characterized

## Confidence
- **High confidence**: Offline RL framework with CQL can learn from demonstrations without online interaction (empirical results show 1 hour training vs 12+ hours for online methods)
- **Medium confidence**: VFM-based segmentation masks improve generalization across environments (supported by ablation showing raw RGB input performs worse, but limited cross-environment testing)
- **Medium confidence**: Multi-level imperfect expert data collection strategy enhances policy robustness (logical but lacks direct comparative ablation with uniform expert data)

## Next Checks
1. **VFM robustness test**: Evaluate segmentation performance across diverse environments with varying lighting, object appearances, and occlusions to quantify domain generalization limits
2. **Offline dataset coverage analysis**: Measure state-action space coverage of collected demonstrations and correlate with policy performance in novel scenarios
3. **Sim-to-real gap quantification**: Systematically compare tracking performance in simulation vs real robot under identical conditions to identify failure modes in physical deployment