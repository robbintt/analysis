---
ver: rpa2
title: 'Double-Dip: Thwarting Label-Only Membership Inference Attacks with Transfer
  Learning and Randomization'
arxiv_id: '2402.01114'
source_url: https://arxiv.org/abs/2402.01114
tags:
- learning
- double-dip
- target
- adversary
- transfer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Double-Dip introduces transfer learning (Stage-1) and randomization
  (Stage-2) to mitigate label-only membership inference attacks (MIAs) on overfitted
  DNNs without degrading classification accuracy. Stage-1 embeds a low-dimensional
  overfitted model into a higher-dimensional target model using transfer learning,
  reducing ASR while significantly improving nonmember classification accuracy.
---

# Double-Dip: Thwarting Label-Only Membership Inference Attacks with Transfer Learning and Randomization

## Quick Facts
- arXiv ID: 2402.01114
- Source URL: https://arxiv.org/abs/2402.01114
- Reference count: 40
- Primary result: Achieves ASR of 50% (near random guess) while maintaining high classification accuracy against label-only MIAs

## Executive Summary
Double-Dip is a two-stage defense mechanism designed to protect overfitted deep neural networks (DNNs) from label-only membership inference attacks (MIAs). The approach combines transfer learning (Stage-1) and randomization (Stage-2) to embed a low-dimensional overfitted model into a higher-dimensional target model while creating ambiguity in membership inference. By leveraging pretrained models and noise perturbation, Double-Dip effectively reduces adversary success rate to near-random levels without degrading classification accuracy. The method is evaluated on three (Target, Source) dataset pairs using four pretrained models, demonstrating superior performance compared to regularization and differential privacy defenses.

## Method Summary
Double-Dip employs a two-stage approach to mitigate label-only MIAs on overfitted DNNs. Stage-1 uses transfer learning to embed a lower-dimensional overfitted DNN into a higher-dimensional target model by freezing layers of a pretrained model and training the remaining layers on a limited target dataset. This reduces overfitting and adversary success rate. Stage-2 applies noise perturbation to input samples, averaging predictions to construct a high-dimensional region of constant output label, further reducing ASR. The combined approach achieves near-random ASR while maintaining high classification accuracy for nonmember samples.

## Key Results
- Reduces ASR to 50% (near random guess) while maintaining high classification accuracy
- Outperforms regularization and differential privacy defenses in both ASR reduction and accuracy preservation
- Effective across multiple dataset pairs and pretrained model architectures
- Maintains strong performance under both white-box and black-box adversary access scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transfer learning embeds a lower-dimensional overfitted DNN into a higher-dimensional target model, overcoming overfitting and reducing adversary success rate.
- Mechanism: By freezing layers of a pretrained model and training the remaining layers on a limited target dataset, the model gains better generalization and reduces overfitting.
- Core assumption: The source and target datasets share feature space or parameter values, enabling meaningful transfer.
- Evidence anchors:
  - [abstract] "Stage-1 uses transfer learning to embed features of a lower dimensional overfitted DNN into a target model that overcomes overfitting."
  - [section 4.1] "Our insight is that overcoming overfitting will enable resilience to MIAs by reducing success of an adversary even when size of the training dataset for the target model is limited."
- Break condition: If the source and target datasets have disjoint feature spaces, transfer learning will not effectively reduce overfitting or ASR.

### Mechanism 2
- Claim: Randomization through noise perturbation constructs a high-dimensional region of constant output label, making member and nonmember samples indistinguishable.
- Mechanism: Adding zero-mean Gaussian noise to input samples and averaging predictions smooths the decision boundary, reducing the effectiveness of label-only MIAs.
- Core assumption: Noise perturbation affects distance estimates to the decision boundary similarly for members and nonmembers.
- Evidence anchors:
  - [abstract] "Stage-2 employs noise perturbation to construct a high-dimensional region of constant output label, further reducing ASR."
  - [section 4.2] "The intuition underpinning Stage-2 is that such randomization will affect estimates of the distance of a data point to a decision boundary."
- Break condition: If the noise variance is too low, the decision boundary remains distinguishable; if too high, classification accuracy degrades.

### Mechanism 3
- Claim: Double-Dip reduces ASR to near 50% while maintaining high classification accuracy for nonmembers.
- Mechanism: Combining transfer learning (Stage-1) and randomization (Stage-2) addresses overfitting and creates ambiguity in membership inference.
- Core assumption: Both stages are necessary and complementary for effective defense.
- Evidence anchors:
  - [abstract] "After Stage-2, success of an adversary carrying out a label-only MIA is further reduced to near 50%, bringing it closer to a random guess."
  - [section 6.4] "Our results show that Double-Dip Stages-1&2 achieves low ASR while simultaneously ensuring high classification accuracy."
- Break condition: If either stage is omitted, ASR will not reach near-random levels or classification accuracy will suffer.

## Foundational Learning

- Concept: Overfitting in DNNs
  - Why needed here: Understanding overfitting is crucial to grasp why transfer learning and randomization are effective defenses against MIAs.
  - Quick check question: Why are overfitted DNNs more vulnerable to membership inference attacks?
- Concept: Transfer Learning
  - Why needed here: Transfer learning is the core mechanism in Stage-1 of Double-Dip, enabling embedding of a lower-dimensional model into a higher-dimensional one.
  - Quick check question: What are the key considerations when selecting a pretrained model for transfer learning?
- Concept: Membership Inference Attacks (MIAs)
  - Why needed here: Understanding MIAs, especially label-only MIAs, is essential to appreciate the effectiveness of Double-Dip.
  - Quick check question: How do label-only MIAs differ from confidence-score based MIAs?

## Architecture Onboarding

- Component map: Pretrained model -> Frozen layers -> Target dataset -> Transfer learning (Stage-1) -> Noise perturbation (Stage-2) -> ASR and accuracy evaluation
- Critical path: Pretrained model selection → Target dataset preparation → Stage-1 training → Stage-2 noise calibration → Evaluation of ASR and accuracy
- Design tradeoffs: Balancing noise variance in Stage-2 to reduce ASR without degrading classification accuracy. Choosing the number of frozen layers in Stage-1 to optimize transfer learning effectiveness.
- Failure signatures: High ASR after Stage-1 indicates poor feature transfer or insufficient dataset size. Low classification accuracy after Stage-2 suggests excessive noise perturbation.
- First 3 experiments:
  1. Implement Stage-1 using a pretrained VGG-19 model on a small target dataset (e.g., CIFAR-10) and measure ASR and accuracy.
  2. Add Stage-2 noise perturbation with varying variance and observe changes in ASR and accuracy.
  3. Compare results with baseline (no transfer learning, no randomization) and SOTA defenses (regularization, distillation).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is Double-Dip against membership inference attacks beyond label-only and entropy-based MIAs, such as attacks using confidence scores or model outputs?
- Basis in paper: [explicit] The paper mentions that Stage-1 of Double-Dip is more effective against label-only MIAs than entropy-based MIAs, and suggests that defenses against MIAs using confidence scores have been developed in other works.
- Why unresolved: The paper does not provide experimental results comparing Double-Dip's performance against other types of MIAs, such as those using confidence scores or model outputs.
- What evidence would resolve it: Experimental results comparing Double-Dip's performance against various types of MIAs, including those using confidence scores and model outputs.

### Open Question 2
- Question: How does the choice of the number of frozen layers in Stage-1 of Double-Dip affect the model's performance on different target datasets with varying levels of similarity to the source dataset?
- Basis in paper: [explicit] The paper discusses the role of the number of frozen layers in Stage-1 and how it affects the model's performance, but does not provide a comprehensive analysis of its impact on different target datasets with varying levels of similarity to the source dataset.
- Why unresolved: The paper does not provide experimental results analyzing the impact of the number of frozen layers on different target datasets with varying levels of similarity to the source dataset.
- What evidence would resolve it: Experimental results analyzing the impact of the number of frozen layers on different target datasets with varying levels of similarity to the source dataset.

### Open Question 3
- Question: How can Double-Dip be integrated with domain adaptation techniques to defend against label-only MIAs when the target dataset size is small and a pretrained model trained on a correlated dataset is not available?
- Basis in paper: [explicit] The paper suggests that domain adaptation techniques can be integrated with Double-Dip to defend against label-only MIAs when the target dataset size is small and a pretrained model trained on a correlated dataset is not available.
- Why unresolved: The paper does not provide experimental results or a detailed explanation of how to integrate domain adaptation techniques with Double-Dip.
- What evidence would resolve it: Experimental results or a detailed explanation of how to integrate domain adaptation techniques with Double-Dip to defend against label-only MIAs when the target dataset size is small and a pretrained model trained on a correlated dataset is not available.

## Limitations
- Limited evaluation on image datasets and specific pretrained models, leaving generalization to other domains and architectures uncertain
- Lack of extensive analysis on adaptive attacks and potential adversarial adaptation strategies
- Specific noise variance and number of noisy variants not fully specified for different dataset and model combinations

## Confidence

- High Confidence: The effectiveness of transfer learning (Stage-1) in reducing overfitting and ASR, supported by clear experimental evidence across multiple dataset pairs
- High Confidence: The mechanism of randomization (Stage-2) in constructing a high-dimensional region of constant output label, reducing ASR while maintaining accuracy
- Medium Confidence: The generalizability of Double-Dip to other domains beyond image datasets and different model architectures

## Next Checks

1. **Noise Calibration**: Experimentally determine the optimal noise variance (σ²) and number of noisy variants (s) for each dataset and pretrained model combination to ensure consistent ASR reduction without degrading accuracy
2. **Adaptive Attack Resilience**: Test Double-Dip against adaptive adversaries who are aware of the defense mechanism, to assess robustness under realistic attack scenarios
3. **Cross-Domain Applicability**: Evaluate Double-Dip on non-image datasets (e.g., text, tabular data) and diverse model architectures to validate generalizability and identify potential domain-specific adjustments