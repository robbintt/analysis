---
ver: rpa2
title: Joint-Task Regularization for Partially Labeled Multi-Task Learning
arxiv_id: '2404.01976'
source_url: https://arxiv.org/abs/2404.01976
tags:
- learning
- tasks
- multi-task
- mtpsl
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles multi-task learning with partially labeled data,
  a challenging problem where each input example may have labels for only a subset
  of tasks. The authors propose Joint-Task Regularization (JTR), which encodes predictions
  and labels from all tasks into a single joint-task latent space.
---

# Joint-Task Regularization for Partially Labeled Multi-Task Learning

## Quick Facts
- arXiv ID: 2404.01976
- Source URL: https://arxiv.org/abs/2404.01976
- Reference count: 40
- Multi-task learning method that achieves linear complexity while outperforming pair-wise approaches on partially labeled data

## Executive Summary
This paper introduces Joint-Task Regularization (JTR), a method for multi-task learning when each training example has labels for only a subset of tasks. JTR encodes predictions and labels from all tasks into a single joint-task latent space and applies a distance loss to regularize tasks jointly. The method achieves linear complexity with respect to the number of tasks, making it scalable compared to quadratic pair-wise approaches. Experiments on NYU-v2, Cityscapes, and Taskonomy demonstrate significant improvements over existing methods in partially labeled scenarios.

## Method Summary
JTR operates by concatenating predictions and labels from all tasks into a single tensor, encoding this through a learned encoder into a joint-task latent space, and applying a distance loss between encoded predictions and labels. The method uses an auto-encoder architecture with reconstruction loss to prevent trivial solutions. A key innovation is that gradients from the distance loss propagate to all task branches simultaneously, allowing unlabeled tasks to benefit from the supervision of labeled tasks. The method maintains linear complexity by using a single encoder-decoder rather than creating mapping spaces for each pair of tasks.

## Key Results
- On NYU-v2 randomlabels, JTR achieves 37.08 mIoU compared to 35.60 for the previous state-of-the-art method
- JTR demonstrates consistent improvements across multiple partially labeled settings: randomlabels, onelabel, and halflabels
- The method achieves 2.48% mean percentage improvement over the supervised baseline across all experimental settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint-Task Regularization (JTR) enables information flow across multiple tasks by encoding all task predictions and labels into a single joint-task latent space
- Mechanism: By stacking predictions and labels from all tasks into one tensor and encoding them through a learned encoder, JTR creates a shared latent representation that captures cross-task relationships. The distance loss in this latent space propagates gradients to all task branches simultaneously, allowing unlabeled tasks to benefit from the supervision of labeled tasks
- Core assumption: The cross-task relationships that exist in the original task spaces are preserved or enhanced in the learned joint-task latent space
- Evidence anchors:
  - [abstract] "encodes predictions and labels for multiple tasks into a single joint-task latent space"
  - [section] "JTR's distance loss term in the joint-task space propagates gradients to all tasks jointly"
  - [corpus] Weak - corpus papers focus on diffusion models and task grouping rather than joint-task latent spaces
- Break condition: If the learned encoder fails to preserve meaningful task relationships in the joint-task space, or if tasks are too dissimilar for useful information sharing, the regularization effect will degrade

### Mechanism 2
- Claim: JTR achieves linear complexity with respect to the number of tasks, making it scalable compared to quadratic pair-wise methods
- Mechanism: Instead of creating mapping spaces for each pair of tasks (which scales as O(K²)), JTR uses a single encoder-decoder architecture that operates on the concatenated tensor of all tasks. This means the number of parameters grows linearly with the number of tasks rather than quadratically
- Core assumption: A single joint-task encoder can effectively capture the relationships among all K tasks without needing separate pair-wise mappings
- Evidence anchors:
  - [abstract] "JTR stands out from existing approaches in that it regularizes all tasks jointly rather than separately in pairs—therefore, it achieves linear complexity"
  - [section] "Unlike previous methods which regularize tasks in a pair-wise fashion, JTR has linear complexity"
  - [corpus] Weak - corpus focuses on diffusion models and task grouping rather than computational complexity analysis
- Break condition: If the single encoder cannot capture complex task relationships when the number of tasks becomes very large, or if the linear scaling assumption breaks down due to other implementation factors

### Mechanism 3
- Claim: The reconstruction loss component prevents the encoder from learning a trivial solution (mapping all inputs to a single point)
- Mechanism: The auto-encoder architecture forces the encoder to create meaningful compressed representations by requiring it to reconstruct both the predictions and labels. Without this constraint, the encoder might learn to map all inputs to a single point in the latent space, making the distance loss meaningless
- Core assumption: The reconstruction task provides sufficient constraint to prevent trivial solutions while still allowing the encoder to learn useful joint-task representations
- Evidence anchors:
  - [section] "g forms an auto-encoder architecture which encodes and reconstructs Yx and Ŷx across the joint-task space as the bottleneck, preventing the encoder gθ1 from learning a trivial joint-task space"
  - [section] "LRecon measures the quality of the decoder's reconstruction using the task-specific loss functions Lt for all tasks t ∈ M"
  - [corpus] Weak - corpus doesn't discuss auto-encoder architectures for multi-task learning
- Break condition: If the reconstruction loss is too weak to prevent trivial solutions, or if the reconstruction task interferes with learning useful joint-task representations

## Foundational Learning

- Concept: Multi-task learning with partially labeled data
  - Why needed here: This paper specifically addresses the scenario where each training example has labels for only a subset of tasks, which is more realistic than assuming fully labeled datasets
  - Quick check question: If you have 10 tasks and each training example has labels for exactly 3 random tasks, what percentage of the total label budget are you using compared to fully labeled data?

- Concept: Auto-encoder architectures and bottleneck representations
  - Why needed here: JTR uses an auto-encoder to learn a compressed joint-task representation, and understanding how auto-encoders work is crucial for grasping why the reconstruction loss prevents trivial solutions
  - Quick check question: In an auto-encoder with bottleneck dimension 512×9×12, if the input has shape 2048×9×12, what compression ratio is being applied?

- Concept: Distance metrics in learned feature spaces
  - Why needed here: JTR uses cosine distance in the joint-task latent space, and understanding how distance metrics behave in high-dimensional learned spaces is important for interpreting the regularization effect
  - Quick check question: If two vectors in a learned feature space have cosine similarity of 0.95, what is their cosine distance value?

## Architecture Onboarding

- Component map: Input -> fϕ (shared feature extractor) -> hψt (task-specific decoders) -> Ŷx (predictions) -> gθ1 (JTR encoder) -> joint-task space -> gθ2 (JTR decoder) -> reconstruction -> LDist + LRecon -> gradients to all components

- Critical path: Input → fϕ → hψt → Ŷx → gθ1 → joint-task space → gθ2 → reconstruction → LDist + LRecon → gradients to all components

- Design tradeoffs:
  - Linear vs. quadratic scaling: JTR trades potential pair-wise specificity for computational efficiency
  - Single vs. multiple auto-encoders: Using one joint encoder vs. separate encoders for different task pairs
  - Distance metric choice: Cosine vs. L1/L2 norms for measuring similarity in latent space

- Failure signatures:
  - Poor performance despite correct implementation: Could indicate tasks are too dissimilar for effective cross-task regularization
  - High memory usage: May suggest need to reduce bottleneck dimensions or batch size
  - Training instability: Could result from improper weight balancing between supervised and JTR losses

- First 3 experiments:
  1. Run JTR on a small subset of NYU-v2 with only 2-3 tasks to verify basic functionality and understand the impact of different distance metrics
  2. Compare JTR with and without LRecon on the same small dataset to observe the effect of the reconstruction constraint
  3. Test JTR on a fully labeled dataset first to establish baseline performance before moving to partially labeled scenarios

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would JTR perform on heterogeneous multi-task learning problems combining instance-level classification, regression, and pixel-level dense prediction tasks?
- Basis in paper: [explicit] The authors acknowledge this as a limitation, stating that applying JTR to heterogeneous tasks may not be straightforward
- Why unresolved: The paper only evaluates JTR on dense prediction tasks (semantic segmentation, depth estimation, surface normal estimation) and does not explore its applicability to other task types
- What evidence would resolve it: Experiments applying JTR to multi-task learning problems with heterogeneous task types (e.g., object detection + semantic segmentation + depth estimation) would demonstrate its effectiveness or reveal limitations

### Open Question 2
- Question: What is the optimal bottleneck size for the JTR auto-encoder across different datasets and task combinations?
- Basis in paper: [explicit] The authors present an ablation study on bottleneck dimensions but only explore a limited range (256, 512, 1024) and find 512 to be optimal for NYU-v2 randomlabels
- Why unresolved: The optimal bottleneck size likely depends on the number of tasks, task complexity, and dataset characteristics. The current study only provides limited insight into this hyperparameter
- What evidence would resolve it: A comprehensive study varying the bottleneck size across multiple datasets with different numbers of tasks and task types would reveal patterns in optimal configuration

### Open Question 3
- Question: How does JTR perform in multi-domain settings where data comes from heterogeneous sources with domain gaps?
- Basis in paper: [explicit] The authors mention this as a limitation, noting that their experiments use single-domain data and that combining datasets from heterogeneous domains presents additional challenges
- Why unresolved: All experiments in the paper use single-domain datasets (NYU-v2, Cityscapes, Taskonomy), so JTR's performance in multi-domain settings remains unexplored
- What evidence would resolve it: Experiments training JTR on datasets from multiple domains (e.g., combining indoor and outdoor scenes, or synthetic and real data) would reveal its robustness to domain shifts and ability to leverage cross-domain information

## Limitations
- The method's effectiveness on tasks with very different data distributions or label spaces (e.g., combining visual and textual tasks) remains unexplored
- While experiments show strong performance, the ablation study only examines reconstruction loss importance but not the sensitivity to distance metric choice or bottleneck dimensionality
- The paper claims linear complexity over pair-wise methods but doesn't provide detailed computational analysis or runtime comparisons across different task counts

## Confidence
- **High confidence**: Linear complexity advantage over pair-wise methods, basic JTR architecture implementation
- **Medium confidence**: Effectiveness of cosine distance in joint-task space, reconstruction loss preventing trivial solutions
- **Low confidence**: Generalizability to tasks beyond computer vision, performance on datasets with natural partial labeling vs. synthetic

## Next Checks
1. **Computational scaling analysis**: Measure actual training time and memory usage of JTR versus pair-wise regularization methods as task count increases from 2 to 10+ tasks
2. **Distance metric ablation**: Systematically compare cosine distance against L1/L2 norms and other similarity measures in the joint-task latent space across all three datasets
3. **Cross-domain generalization**: Apply JTR to a mixed-modality multi-task setting (e.g., combining visual and textual tasks) to test the method's assumptions about cross-task relationships