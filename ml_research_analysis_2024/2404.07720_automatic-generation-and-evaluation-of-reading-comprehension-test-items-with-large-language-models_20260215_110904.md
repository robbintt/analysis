---
ver: rpa2
title: Automatic Generation and Evaluation of Reading Comprehension Test Items with
  Large Language Models
arxiv_id: '2404.07720'
source_url: https://arxiv.org/abs/2404.07720
tags: []
core_contribution: Large language models were evaluated for zero-shot generation of
  multiple-choice reading comprehension items in German. A new evaluation protocol
  measured text informativity by comparing answerability (with text) and guessability
  (without text).
---

# Automatic Generation and Evaluation of Reading Comprehension Test Items with Large Language Models

## Quick Facts
- **arXiv ID**: 2404.07720
- **Source URL**: https://arxiv.org/abs/2404.07720
- **Reference count**: 0
- **Key outcome**: GPT-4 generated German reading comprehension items with higher answerability and text informativity than Llama 2 and human writers

## Executive Summary
This study evaluates large language models for zero-shot generation of multiple-choice reading comprehension items in German, a low-resource language for NLP research. The authors introduce an innovative evaluation framework that measures text informativity by comparing answerability (with text access) versus guessability (without text). GPT-4 consistently outperformed Llama 2 in automatic evaluations, showing higher answerability scores and text informativity. Human evaluators also rated GPT-4 items as highest quality. The results demonstrate that LLM-based approaches are promising for reading comprehension item generation, particularly for languages with limited educational resources.

## Method Summary
The authors evaluated zero-shot question generation using GPT-4 and Llama 2 on 10 German reading passages from the B2-level German RC corpus. Each model generated 30 multiple-choice items (10 per passage), creating 60 total questions. A novel evaluation protocol measured text informativity through the difference between answerability (ability to answer with text) and guessability (ability to answer without text). The study employed both automated metrics (answerability, guessability, text informativity) and human evaluation with 12 university student evaluators who rated item quality on a 5-point scale. Generated items were compared against human-written items from the German RC corpus.

## Key Results
- GPT-4 achieved higher answerability scores than Llama 2 across all automated metrics
- Both LLMs showed greater text informativity than human-generated items in automatic evaluation
- Human evaluators rated GPT-4 items as highest quality on average, though sample size was limited

## Why This Works (Mechanism)
Large language models excel at question generation because they can leverage their understanding of language patterns, question-answer relationships, and contextual information from input texts. The zero-shot approach works by prompting the model to generate questions that are answerable from the given passage while maintaining distractor plausibility. The evaluation framework's focus on text informativity captures how well questions actually require passage information rather than being guessable, which is critical for effective reading comprehension assessment.

## Foundational Learning

**Answerability vs Guessability**
- *Why needed*: Distinguishes between questions that require text understanding versus those answerable through general knowledge
- *Quick check*: Compare model performance on answerable vs guessable items

**Text Informativity**
- *Why needed*: Measures how much unique information questions extract from passages
- *Quick check*: Calculate difference between answerability and guessability scores

**Zero-shot Generation**
- *Why needed*: Enables model use without task-specific training data
- *Quick check*: Evaluate prompt effectiveness across different text types

## Architecture Onboarding

**Component Map**
Input Text -> LLM (GPT-4/Llama 2) -> Multiple-choice Questions -> Automated Evaluation (Answerability/Guessability) -> Human Evaluation -> Quality Assessment

**Critical Path**
Text input → LLM prompt processing → Question generation → Answerability calculation → Text informativity scoring

**Design Tradeoffs**
Zero-shot generation prioritizes flexibility over fine-tuned accuracy, accepting variability for broader applicability across languages and domains

**Failure Signatures**
Low answerability with high guessability indicates questions relying on general knowledge rather than text; poor distractor quality reduces item validity

**First Experiments**
1. Compare answerability scores across different passage difficulty levels
2. Test cross-lingual transfer of zero-shot generation prompts
3. Evaluate impact of prompt engineering on question quality

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Small sample size with only 12 human evaluators limits statistical confidence
- Limited corpus size (10 passages) and question sample (60 total) may affect generalizability
- Convenience sampling from university setting raises questions about evaluator representativeness

## Confidence

**Major Claims and Confidence:**
- LLM superiority in automatic metrics: **High confidence** - Multiple automated evaluations consistently favored GPT-4
- Human preference for GPT-4 items: **Medium confidence** - Based on small evaluator sample and limited statistical testing
- Generalizability to other languages and domains: **Low confidence** - Results specific to German reading comprehension without cross-linguistic validation

## Next Checks

1. Conduct a larger-scale human evaluation with certified test item writers across multiple languages to validate quality assessments
2. Perform item response theory analysis to verify the psychometric properties of LLM-generated items match human-written standards
3. Test model performance across different text genres and complexity levels to establish robustness boundaries