---
ver: rpa2
title: 'Adaptation of Biomedical and Clinical Pretrained Models to French Long Documents:
  A Comparative Study'
arxiv_id: '2402.16689'
source_url: https://arxiv.org/abs/2402.16689
tags:
- biomedical
- clinical
- tasks
- french
- pre-training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares three adaptation strategies for applying Longformer
  models to French biomedical and clinical text. The strategies evaluated include
  converting a French biomedical BERT to Longformer, pre-training a French biomedical
  Longformer from scratch, and further pre-training an English clinical Longformer
  on French biomedical data.
---

# Adaptation of Biomedical and Clinical Pretrained Models to French Long Documents: A Comparative Study

## Quick Facts
- arXiv ID: 2402.16689
- Source URL: https://arxiv.org/abs/2402.16689
- Reference count: 20
- Primary result: Multi-phase adaptive pre-training enables effective cross-lingual transfer from English clinical models to French biomedical tasks

## Executive Summary
This paper compares three adaptation strategies for applying Longformer models to French biomedical and clinical text. The strategies evaluated include converting a French biomedical BERT to Longformer, pre-training a French biomedical Longformer from scratch, and further pre-training an English clinical Longformer on French biomedical data. Models were evaluated on 16 downstream tasks including NER, classification, and semantic similarity. Results showed that further pre-training an English clinical model with French biomedical texts (DrLongformer-CP) generally outperformed the other strategies, especially on document classification tasks. However, BERT-based models remained most effective for named entity recognition. The study highlights the potential of cross-lingual transfer through multi-phase adaptive pre-training, particularly when French clinical data is limited.

## Method Summary
The study pre-trained three Longformer variants using the NACHOS corpus (1.276 billion words, 2,367,419 documents) with Masked Language Modeling objective. DrBERT-4096 was created by converting DrBERT to Longformer architecture, DrLongformer-FS was pre-trained from scratch, and DrLongformer-CP was created by further pre-training Clinical-Longformer on French biomedical data. All models were fine-tuned on 16 downstream tasks with task-specific hyperparameters, then evaluated using appropriate metrics including F1 for NER/POS, weighted F1 for classification, and Spearman/EDRM for semantic tasks.

## Key Results
- DrLongformer-CP, which was pretrained from Clinical-Longformer weights, achieves better results in 11 of the 16 tasks
- BERT models outperform Longformer models on 5 of 16 tasks, particularly for named entity recognition
- Longformer's maximum sequence length of 4,096 tokens enables processing long clinical documents while maintaining performance
- Continual pre-training yields better results than training from scratch, with DrLongformer-FS never outperforming other variants

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-phase adaptive pre-training enables effective cross-lingual transfer from English clinical models to French biomedical tasks
- Mechanism: The model first learns general language patterns in English clinical data, then adapts to French biomedical domain through additional pre-training, allowing it to transfer specialized knowledge across languages
- Core assumption: The clinical knowledge learned in English is transferable to French biomedical contexts when combined with French domain data
- Evidence anchors:
  - [abstract] "further pre-training an English clinical model with French biomedical texts can outperform both converting a French biomedical BERT to the Longformer architecture"
  - [section] "DrLongformer-CP, which was pretrained from the weights of Clinical-Longformer, achieves better results in 11 of the 16 tasks"
  - [corpus] Corpus neighbors show related work on cross-lingual transfer, though direct evidence is limited
- Break condition: The transfer fails if the clinical concepts don't map well between languages or if the French biomedical domain is too different from English clinical data

### Mechanism 2
- Claim: Longformer architecture with sliding window attention enables processing of long clinical documents while maintaining performance
- Mechanism: The sliding window attention pattern reduces computational complexity from O(nÂ²) to O(n) while allowing the model to process sequences up to 4,096 tokens, enabling it to capture long-range dependencies in clinical documents
- Core assumption: The reduced attention pattern is sufficient to capture relevant dependencies in long clinical documents
- Evidence anchors:
  - [abstract] "long-sequence French biomedical models improve performance on a majority of biomedical and clinical tasks"
  - [section] "Longformer models perform better than BERT models on 11 of the 16 tasks" and "Longformer's maximum sequence length of 4,096 tokens"
  - [corpus] Related papers show LongFormer and LED-based models for long documents, supporting the mechanism
- Break condition: Performance degrades if the sliding window is too narrow to capture necessary context or if the attention pattern misses critical dependencies

### Mechanism 3
- Claim: Domain adaptation through continual pre-training is more effective than training from scratch for specialized language models
- Mechanism: Starting from a model already trained on relevant domain data and continuing pre-training on the target domain data allows the model to build on existing specialized knowledge rather than learning from scratch
- Core assumption: The initial domain knowledge provides a useful foundation that accelerates learning in the target domain
- Evidence anchors:
  - [abstract] "further pre-training an English clinical model with French biomedical texts can outperform... pre-training a French biomedical Longformer from scratch"
  - [section] "DrLongformer-FS model never outperforms either DrBERT-4096 or DrLongformer-CP" and "continual pre-training yields better results than training from scratch"
  - [corpus] DrBERT paper shows pre-training from scratch vs continual pre-training, providing supporting evidence
- Break condition: The initial domain knowledge becomes a liability if it conflicts with the target domain or if the domains are too dissimilar

## Foundational Learning

- Concept: Attention mechanisms in transformers
  - Why needed here: Understanding how Longformer's sparse attention differs from BERT's full attention is crucial for grasping why Longformer can handle longer sequences
  - Quick check question: What is the computational complexity difference between full attention and sliding window attention, and why does this matter for processing long sequences?

- Concept: Domain adaptation and transfer learning
  - Why needed here: The paper's core contribution involves adapting models across domains (clinical to biomedical) and languages (English to French), requiring understanding of these concepts
  - Quick check question: How does continual pre-training differ from training from scratch, and what advantages does it offer for domain adaptation?

- Concept: Named entity recognition evaluation metrics
  - Why needed here: The paper compares model performance on NER tasks using seqeval with IOB2 format, which requires understanding of entity-level vs token-level evaluation
  - Quick check question: Why might entity-level evaluation (seqeval) be more appropriate than token-level evaluation for NER tasks in biomedical contexts?

## Architecture Onboarding

- Component map: Input layer (WordPiece tokenization) -> Core (Longformer architecture: 12 layers, 768 hidden dimensions, 12 attention heads) -> Attention (Sliding window + global attention pattern) -> Output (Task-specific heads) -> Training (MLM objective with 15% masking)

- Critical path: 1. Load pre-trained weights (BERT, Clinical-Longformer, or scratch initialization) 2. Apply domain adaptation through continual pre-training on French biomedical data 3. Fine-tune on downstream tasks with task-specific heads 4. Evaluate using appropriate metrics

- Design tradeoffs:
  - Sequence length vs. computational cost: 4,096 tokens enables processing long documents but increases memory requirements
  - Attention pattern: Sliding window + global attention balances efficiency and performance but may miss some long-range dependencies
  - Pre-training strategy: Continual pre-training from specialized models vs. training from scratch involves tradeoffs between performance and computational cost

- Failure signatures:
  - Poor performance on NER tasks despite good overall results suggests attention pattern may not capture local context well
  - Degradation on very long sequences (>4,096 tokens) indicates need for different attention mechanism or truncation strategy
  - Unexpectedly poor cross-lingual transfer suggests domain mismatch or insufficient French biomedical data

- First 3 experiments:
  1. Compare DrLongformer-CP vs DrBERT-4096 on a representative classification task to validate cross-lingual transfer hypothesis
  2. Test attention pattern effectiveness by varying sliding window size and measuring impact on NER performance
  3. Evaluate model robustness by testing on sequences of varying lengths to identify performance degradation points

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on synthetic French clinical data for evaluation due to limited access to real clinical datasets
- Computational resource disparity between model variants (BERT models used 128 GPUs vs 4 GPUs for Longformer models)
- Data quality concerns with NACHOS corpus, where 5% of documents were removed due to noise without detailed analysis of what constitutes "noise"

## Confidence

**High Confidence (4/5):** Comparative performance analysis across 16 downstream tasks is methodologically sound with appropriate metrics and consistent results.

**Medium Confidence (3/5):** Cross-lingual transfer effectiveness has moderate support but requires validation on authentic clinical datasets.

**Low Confidence (2/5):** Universal superiority of Longformer architecture receives mixed support due to task-specific variations in performance.

## Next Checks

1. **Clinical Data Validation**: Test top-performing models on authentic French clinical datasets to compare synthetic versus real clinical data performance.

2. **Attention Pattern Ablation Study**: Conduct controlled experiments varying sliding window size to determine optimal parameters for different task types, especially examining why BERT outperforms Longformer on NER tasks.

3. **Computational Efficiency Analysis**: Perform comprehensive cost-benefit analysis comparing three adaptation strategies across pre-training time, fine-tuning convergence, inference latency, and memory requirements.