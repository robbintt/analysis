---
ver: rpa2
title: Improving Algorithm-Selection and Performance-Prediction via Learning Discriminating
  Training Samples
arxiv_id: '2404.05359'
source_url: https://arxiv.org/abs/2404.05359
tags: []
core_contribution: This paper proposes a meta-approach for improving algorithm selection
  and performance prediction in continuous optimization by generating discriminatory
  training data. The core idea is to use the irace algorithm configuration tool to
  tune a simple Simulated Annealing (SA) algorithm to produce short trajectories that
  maximize the performance metrics of machine learning models trained on this data.
---

# Improving Algorithm-Selection and Performance-Prediction via Learning Discriminating Training Samples

## Quick Facts
- **arXiv ID**: 2404.05359
- **Source URL**: https://arxiv.org/abs/2404.05359
- **Reference count**: 40
- **Primary result**: Models trained on tuned SA trajectories outperform those trained on ELA features and raw trajectory data for algorithm selection and performance prediction

## Executive Summary
This paper introduces a novel meta-approach for enhancing algorithm selection and performance prediction in continuous optimization through the generation of discriminatory training data. The core methodology employs the irace algorithm configuration tool to tune a simple Simulated Annealing (SA) algorithm, producing short trajectories that maximize the performance metrics of machine learning models trained on this data. The approach is evaluated on the BBOB benchmark suite, demonstrating significant improvements over traditional exploratory landscape analysis (ELA) features and raw trajectory data in both classification accuracy and root mean squared error (RMSE) for performance prediction.

The key innovation lies in using the irace tool to iteratively tune SA parameters to generate trajectories that are most informative for training predictive models, rather than relying on conventional feature extraction methods. This approach not only improves model performance but also requires considerably fewer function evaluations compared to ELA-based methods, making it particularly suitable for low-budget optimization scenarios where computational resources are constrained.

## Method Summary
The proposed method centers on using irace to configure a simple SA algorithm specifically to produce training data that maximizes machine learning model performance. The irace tool iteratively tunes SA parameters such as neighborhood size, temperature, and cooling schedule to generate short trajectories that contain maximal discriminatory information for subsequent model training. These tuned SA trajectories are then used as training data for machine learning models tasked with algorithm selection and performance prediction tasks.

The evaluation framework compares three approaches: models trained on tuned SA trajectories, models trained on raw trajectory data, and models trained on conventional ELA features. The BBOB benchmark suite provides the test environment, with performance measured through classification accuracy for algorithm selection and RMSE for performance prediction. The tuned SA approach requires multiple SA runs during the irace configuration phase, but produces more informative training data that leads to superior model performance compared to both raw data and ELA-based approaches.

## Key Results
- Models trained on tuned SA trajectories achieved significantly higher classification accuracy for algorithm selection compared to those trained on ELA features and raw trajectory data
- Performance prediction RMSE was substantially lower for models trained on tuned SA trajectories versus both baseline approaches
- The tuned SA method required fewer function evaluations than ELA feature extraction while producing superior model performance
- The approach demonstrates particular effectiveness in low-budget optimization scenarios where computational resources are limited

## Why This Works (Mechanism)
The effectiveness of this approach stems from the iterative tuning process that aligns the data generation mechanism (SA trajectories) with the ultimate goal of improving model performance. By using irace to optimize SA parameters specifically for generating informative training samples, the method creates a feedback loop where the trajectory generation process becomes increasingly discriminative. This targeted approach ensures that the training data contains maximal information for distinguishing between algorithm performances and predicting optimization outcomes, rather than simply exploring the search space as traditional SA would do.

## Foundational Learning
- **irace algorithm configuration**: Essential for understanding the iterative parameter tuning process that generates discriminatory trajectories. Quick check: Can you explain how irace selects and eliminates parameter configurations across iterations?
- **Exploratory Landscape Analysis (ELA)**: Provides context for understanding the baseline features and why tuned SA trajectories might outperform them. Quick check: What are the primary types of ELA features used in continuous optimization?
- **Simulated Annealing fundamentals**: Critical for understanding how the base algorithm is modified and why parameter tuning affects trajectory characteristics. Quick check: How do temperature and cooling schedule parameters influence SA exploration behavior?
- **BBOB benchmark suite**: Important for understanding the evaluation environment and its characteristics. Quick check: What types of functions and problem characteristics does BBOB encompass?
- **Algorithm selection and performance prediction**: Core concepts for understanding the tasks being improved. Quick check: What distinguishes algorithm selection from performance prediction in optimization contexts?
- **Trajectory-based feature extraction**: Understanding how optimization paths can be transformed into predictive features. Quick check: What information from optimization trajectories is typically used for machine learning model training?

## Architecture Onboarding

**Component map**: Data Generation (Tuned SA via irace) -> Feature Extraction (Trajectory Analysis) -> Model Training (ML Algorithms) -> Performance Prediction/Algorithm Selection

**Critical path**: The configuration process via irace serves as the critical path, as it directly determines the quality of generated trajectories and thus the downstream model performance. The iterative tuning process must successfully identify parameter configurations that produce maximally informative trajectories for the ML tasks.

**Design tradeoffs**: The primary tradeoff involves computational budget during configuration versus model performance gains. While tuned SA requires multiple runs for configuration, it ultimately produces more efficient training data compared to ELA. Another tradeoff exists between trajectory length and information density - shorter, more informative trajectories are preferred but must still contain sufficient information for accurate predictions.

**Failure signatures**: Poor model performance despite tuning suggests inadequate irace configuration or inappropriate ML model selection. If tuned SA trajectories fail to outperform raw data, this may indicate that the trajectory generation process isn't capturing relevant optimization characteristics. When performance gains are marginal, the SA algorithm may not be sufficiently expressive to generate diverse, informative trajectories.

**First experiments**: 1) Compare tuned SA trajectories against random SA trajectories to verify the configuration process adds value; 2) Test different ML model architectures (e.g., random forests, neural networks) on the same tuned trajectory data to assess model sensitivity; 3) Vary the number of irace iterations to determine the point of diminishing returns in trajectory quality improvement.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation confined to BBOB benchmark suite, limiting generalizability to other optimization problems
- Reliance on irace configuration tool may not be optimal or available for all problem domains
- Computational cost of tuning process, while lower than ELA, may still be prohibitive for large-scale applications
- Limited exploration of alternative base algorithms beyond simple SA for trajectory generation

## Confidence
- High confidence in improved performance of tuned SA trajectories over raw data
- Medium confidence in comparison with ELA features (limited benchmark scope)
- Medium confidence in computational efficiency claims (function evaluations vs. wall-clock time)

## Next Checks
1. Test the method on diverse optimization benchmarks beyond BBOB to assess generalizability
2. Conduct head-to-head comparison of wall-clock time versus function evaluations to validate computational efficiency claims
3. Explore alternative configuration tools beyond irace to determine if improvements are tool-specific or method-generalizable