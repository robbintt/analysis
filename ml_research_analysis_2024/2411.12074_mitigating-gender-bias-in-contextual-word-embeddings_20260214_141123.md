---
ver: rpa2
title: Mitigating Gender Bias in Contextual Word Embeddings
arxiv_id: '2411.12074'
source_url: https://arxiv.org/abs/2411.12074
tags:
- bias
- words
- gender
- embeddings
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses gender bias in both static and contextual
  word embeddings. For contextual embeddings, it proposes a novel Masked Language
  Modeling (MLM) objective with regularization to reduce gender bias in BERT while
  maintaining downstream task performance.
---

# Mitigating Gender Bias in Contextual Word Embeddings

## Quick Facts
- arXiv ID: 2411.12074
- Source URL: https://arxiv.org/abs/2411.12074
- Authors: Navya Yarrabelly; Vinay Damodaran; Feng-Guang Su
- Reference count: 38
- Primary result: Proposed methods reduce gender bias in both static and contextual embeddings while maintaining downstream task performance

## Executive Summary
This paper addresses gender bias in word embeddings by proposing novel methods for both static and contextual representations. For contextual embeddings, it introduces a regularized Masked Language Modeling (MLM) objective that reduces gender bias in BERT through gender-specific masking and probability equalization. For static embeddings, it identifies stereotypical names as the primary source of bias and proposes methods including representing gendered words as semantic concepts and masking stereotypical named entities during training. The methods show reduced bias (clustering accuracy of 0.5157 ± 0.0050 for gender-neutral professions) while maintaining strong performance on downstream tasks.

## Method Summary
The paper proposes two complementary approaches for mitigating gender bias. For contextual embeddings, it modifies BERT's MLM objective with gender-specific masking (separately masking nouns and attribute words) and adds a regularizer that equalizes prediction probabilities for gender-paired words. This breaks the direct dependency between attribute words and target words created by self-attention mechanisms. For static embeddings, it trains CBOW models with three proposed methods: representing gendered words as single semantic concepts, using explicit gender encoding with a regularizer, and masking stereotypical named entities during training. The NER-M + EGE combination achieves the best results for static embeddings.

## Key Results
- NER-M + EGE approach achieved clustering accuracy of 0.5157 ± 0.0050 for gender-neutral professions, significantly lower than baseline methods
- On contextual embeddings, the proposed methods reduced SEAT scores and maintained strong performance on downstream tasks (SST-2: 93.4%, CoLA: 54.08%, QNLI: 91.3%)
- The methods successfully reduced gender bias while preserving semantic understanding, with only a 3% drop on CoLA but gains on SST-2

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Masked Language Modeling with gender-specific masking reduces gender bias by decoupling gender attribute words from gender-neutral target words.
- Mechanism: The method masks nouns (gender-neutral target words) and predicts them using all tokens except attribute words (gendered words). It also masks attribute words and predicts them using all tokens except nouns. This breaks the direct dependency between attribute words and target words that self-attention normally creates.
- Core assumption: Gender bias in contextual embeddings arises from learned associations between attribute words (he/she) and target words (professions) through self-attention mechanisms.
- Evidence anchors:
  - [abstract]: "we propose a novel objective function for MLM(Masked-Language Modeling) which largely mitigates the gender bias in contextual embeddings"
  - [section 3.2.1]: "we propose a new MLM (Masked-Language Modeling) objective function that can successfully eliminate the dependencies and meanwhile retain the model performance"
  - [corpus]: Evidence shows clustering accuracy dropped from 0.5802 to 0.5157 for gender-neutral professions, indicating reduced bias.
- Break condition: If the model learns to infer masked tokens through other context words that still carry gender information, the bias reduction effect will diminish.

### Mechanism 2
- Claim: Regularization that equalizes prediction probabilities for gender-paired words reduces bias by forcing gender-neutral treatment.
- Mechanism: The regularizer penalizes differences in prediction scores between male and female token pairs (f(xi) - f(yi)), pushing the model to assign similar probabilities to both genders for the same context.
- Core assumption: Gender bias manifests as asymmetric prediction probabilities where certain words are more likely predicted with male vs female pronouns.
- Evidence anchors:
  - [section 3.2.1]: "we propose a newly-designed regularizer when calculating the cross-entropy losses of gender-neutral tokens. That is, Lossreg = Σ| f(xi) - f(yi) |"
  - [section 3.4.1]: Results show proposed reg has better performance with probabilities closer to 0.5 for most occupations.
  - [corpus]: Clustering accuracy dropped significantly (0.5157 ± 0.0050) compared to baselines, showing regularization effectiveness.
- Break condition: If the regularization term is too weak, it won't sufficiently equalize probabilities; if too strong, it may harm semantic understanding.

### Mechanism 3
- Claim: Gender prediction tasks with data augmentation expose models equally to male and female references, reducing bias from corpus imbalance.
- Mechanism: Strategy1 masks gendered words and predicts their labels while swapping gender labels during augmentation. Strategy2 masks all gendered words and predicts neutral gender for gender-neutral words.
- Core assumption: Gender bias stems partly from disproportionate male vs female references in training data.
- Evidence anchors:
  - [section 3.2.2]: "We propose a gender prediction task as a way to intelligently augment the data"
  - [section 3.4.1]: "By swapping genders of the masked words, we expose both male references and female references equally to gender neutral words"
  - [corpus]: Proposed aug achieved SEAT score improvements and maintained downstream task performance (92.08 on SST-2).
- Break condition: If the augmented data doesn't accurately represent the full distribution of gender references, residual bias may remain.

## Foundational Learning

- Concept: Masked Language Modeling (MLM)
  - Why needed here: MLM is the core training objective for BERT that the paper modifies to reduce gender bias. Understanding how standard MLM works is essential to grasp the proposed modifications.
  - Quick check question: What does MLM do in standard BERT training, and how does masking nouns vs attribute words differently help reduce gender bias?

- Concept: Self-attention mechanisms
  - Why needed here: The paper explicitly states that bias persists due to dependencies created by self-attention between attribute words and target words. Understanding self-attention is crucial to why the proposed methods work.
  - Quick check question: How does self-attention create associations between gendered words and gender-neutral words that lead to bias?

- Concept: Principal Component Analysis (PCA) for gender subspace
  - Why needed here: The paper references using PCA to identify gender subspaces in word embeddings, which is fundamental to understanding how bias is measured and mitigated in static embeddings.
  - Quick check question: How does computing principal components across gender word pairs help identify the gender subspace that captures bias?

## Architecture Onboarding

- Component map: Input text -> Gender-specific masking -> Regularized MLM objective -> BERT model -> Debiased embeddings -> Evaluation metrics
- Critical path:
  1. Data preprocessing: Create gender-neutral versions of corpus by concatenating gendered word pairs
  2. Model modification: Implement gender-specific masking and regularization in MLM objective
  3. Training: Run bias mitigation training on modified BERT
  4. Evaluation: Measure SEAT scores and downstream task performance
  5. Analysis: Compare clustering accuracy and neighbor analysis
- Design tradeoffs:
  - Masking strategy: Trade-off between removing bias and preserving semantic information
  - Regularization strength: Too weak won't reduce bias effectively, too strong may harm semantic understanding
  - Data augmentation: Balancing gender representation vs. maintaining natural language patterns
- Failure signatures:
  - High clustering accuracy (>0.6) for gender-neutral professions indicates persistent bias
  - Significant drop in downstream task performance (>2% decrease) suggests over-debiasing
  - SEAT scores remaining significantly different from 0 indicates bias still present
  - Names clustering separately from gender-neutral words in visualizations suggests names are bias carriers
- First 3 experiments:
  1. Run clustering accuracy test on 50 most stereotypically biased professions using cosine similarity and KMeans clustering
  2. Measure SEAT scores on Caliskan test sets to quantify gender bias in sentence embeddings
  3. Evaluate downstream task performance (SST-2, CoLA, QNLI) after bias mitigation training to ensure semantic preservation

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the text provided.

## Limitations

- The paper focuses primarily on gender bias in English language models, with limited discussion of generalization to other languages or types of bias (racial, age-related, etc.)
- The NER-M + EGE approach achieves only modest improvement in clustering accuracy (0.5157 ± 0.0050), indicating residual bias remains
- The paper doesn't fully explore the trade-offs between debiasing effectiveness and semantic preservation, particularly for static embedding methods

## Confidence

- **High confidence** in contextual embedding results: The proposed regularized MLM objective shows consistent improvements across multiple evaluation metrics with well-defined methodology
- **Medium confidence** in static embedding results: While NER-M + EGE shows improved clustering accuracy, the improvement is relatively small and lacks detailed hyperparameter specifications
- **Low confidence** in generalization claims: Limited evidence that methods work beyond specific datasets and bias types tested

## Next Checks

1. **Ablation study on static embeddings**: Run experiments to isolate the contribution of NER-Masking versus Explicit Gender Encoding to determine which component drives the observed improvements.

2. **Cross-lingual bias transfer evaluation**: Test the contextual embedding debiasing methods on multilingual BERT models to assess whether the regularization approach generalizes across languages and cultural contexts.

3. **Long-term semantic drift analysis**: Monitor the performance of debiased models on downstream tasks over extended training periods to detect any gradual degradation in semantic understanding.