---
ver: rpa2
title: Stance Detection on Social Media with Fine-Tuned Large Language Models
arxiv_id: '2404.12171'
source_url: https://arxiv.org/abs/2404.12171
tags:
- stance
- detection
- tweet
- fine-tuning
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive evaluation of fine-tuned large
  language models (LLMs) for stance detection on social media. The study compares
  three LLMs (ChatGPT, LLaMa-2, and Mistral-7B) across zero-shot, few-shot, and fine-tuned
  settings using three public datasets.
---

# Stance Detection on Social Media with Fine-Tuned Large Language Models

## Quick Facts
- arXiv ID: 2404.12171
- Source URL: https://arxiv.org/abs/2404.12171
- Reference count: 17
- Primary result: Fine-tuned LLMs significantly outperform traditional baselines for stance detection on social media, with Favg scores reaching up to 86.5.

## Executive Summary
This paper presents a comprehensive evaluation of fine-tuned large language models (LLMs) for stance detection on social media. The study compares three LLMs (ChatGPT, LLaMa-2, and Mistral-7B) across zero-shot, few-shot, and fine-tuned settings using three public datasets. Results demonstrate that fine-tuned LLMs significantly outperform traditional baselines, with Favg scores reaching up to 86.5. LLaMa-2 and Mistral-7B show remarkable efficiency despite their smaller sizes compared to ChatGPT. The study highlights the effectiveness of fine-tuning in enhancing model performance for stance detection, particularly on controversial topics and political figures, while also revealing that reduced training data can maintain near-peak performance.

## Method Summary
The study fine-tuned three LLMs (ChatGPT, LLaMa-2, and Mistral-7B) using LoRA with specific hyperparameters (rank 8, Î± 16, dropout 0.05), warmup with 10% training data, 3 epochs, learning rate 3e-4, batch size 128, on single NVIDIA A100 GPUs with bfloat16 precision. The evaluation was conducted on three public datasets: SemEval-2016, P-Stance, and Twitter Stance Election 2020. The primary metric was Favg (average of F1 scores for 'favor' and 'against' classes), with F1-macro used for the Twitter dataset. The study compared zero-shot, few-shot, and fine-tuned performance across all models and datasets.

## Key Results
- Fine-tuned LLMs achieved Favg scores up to 86.5, significantly outperforming traditional baselines
- LLaMa-2 and Mistral-7B showed remarkable efficiency despite their smaller sizes compared to ChatGPT
- Reduced training data volumes (down to 70%) maintained near-peak performance for stance detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuned LLMs achieve superior performance on stance detection tasks by adapting pre-trained knowledge to task-specific contexts.
- Mechanism: Fine-tuning adjusts the model's parameters using labeled datasets, enabling the LLM to better capture nuanced language patterns and contextual subtleties specific to stance detection.
- Core assumption: Pre-trained LLMs possess a foundational understanding of language that can be enhanced through task-specific fine-tuning.
- Evidence anchors:
  - [abstract]: "Results demonstrate that fine-tuned LLMs significantly outperform traditional baselines, with Favg scores reaching up to 86.5."
  - [section]: "ChatGPT-ft achieved an Favg score of 79.7 for the Feminist Movement (FM), which was a 5.1 point increase over its zero-shot performance."
  - [corpus]: Weak evidence; corpus neighbors focus on related stance detection studies but lack direct fine-tuning comparisons.
- Break condition: Fine-tuning fails if the dataset is too small or unrepresentative of the task's linguistic diversity.

### Mechanism 2
- Claim: Instruction-tuned variants of LLMs enhance zero-shot and few-shot learning capabilities for stance detection.
- Mechanism: Pre-training on instruction datasets with human annotations enables models to follow complex instructions without explicit task-specific fine-tuning.
- Core assumption: Models can generalize from instruction-tuned data to novel stance detection tasks.
- Evidence anchors:
  - [abstract]: "The instruction-tuned versions, LLaMa-2-7b-chat, LLaMa-2-13b-chat, and Mistral-7b-Instruct, were enhanced by pre-training on instruction datasets..."
  - [section]: "For our analysis, we conducted zero-shot and few-shot experiments with ChatGPT and the instruction-tuned models."
  - [corpus]: Limited direct evidence; corpus neighbors mention zero-shot learning but lack detailed performance metrics.
- Break condition: Instruction tuning is ineffective if the instruction dataset lacks diversity in stance-related contexts.

### Mechanism 3
- Claim: Reduced training data volumes can maintain near-peak performance for stance detection, indicating efficient learning.
- Mechanism: Models leverage pre-existing linguistic knowledge and pattern recognition to generalize effectively even with partial training data.
- Core assumption: Pre-trained models have sufficient foundational knowledge to require less task-specific data for effective fine-tuning.
- Evidence anchors:
  - [abstract]: "The study highlights the effectiveness of fine-tuning in enhancing model performance for stance detection... while also revealing that reduced training data can maintain near-peak performance."
  - [section]: "For example, the LLaMa-2-7b model trained with 70% of the data managed nearly equivalent performance to the fully trained model in the HC category (84.4 vs. 84.2)."
  - [corpus]: Weak evidence; corpus neighbors do not address data efficiency in fine-tuning.
- Break condition: Reduced data leads to performance degradation if the remaining data lacks critical examples of stance variations.

## Foundational Learning

- Concept: Fine-tuning vs. zero-shot learning
  - Why needed here: Understanding the difference helps in selecting the appropriate training strategy for stance detection tasks.
  - Quick check question: What is the primary advantage of fine-tuning over zero-shot learning for stance detection?

- Concept: Instruction tuning
  - Why needed here: Instruction tuning enables models to follow complex instructions, which is crucial for zero-shot and few-shot learning scenarios.
  - Quick check question: How does instruction tuning improve a model's ability to perform tasks without explicit fine-tuning?

- Concept: Dataset diversity and representation
  - Why needed here: Ensuring datasets cover a wide range of stance expressions is essential for robust model performance.
  - Quick check question: Why is dataset diversity important for training models on stance detection tasks?

## Architecture Onboarding

- Component map:
  Pre-trained LLM (ChatGPT, LLaMa-2, Mistral-7B) -> Fine-tuning pipeline (LoRA, Lit-GPT framework) -> Datasets (SemEval-2016, P-Stance, Twitter Stance Election 2020) -> Evaluation metrics (Favg, F1-macro)
- Critical path:
  1. Load pre-trained model
  2. Prepare dataset with structured prompts
  3. Apply LoRA fine-tuning using Lit-GPT
  4. Evaluate model performance on test set
- Design tradeoffs:
  - Model size vs. computational efficiency
  - Training data volume vs. model performance
  - Fine-tuning duration vs. resource constraints
- Failure signatures:
  - Poor performance on nuanced stance detection
  - Overfitting on training data
  - Inability to generalize to unseen targets
- First 3 experiments:
  1. Compare zero-shot performance of instruction-tuned models on stance detection.
  2. Evaluate fine-tuning impact with varying dataset sizes.
  3. Assess model performance on controversial vs. non-controversial topics.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the zero-shot performance differences between ChatGPT-1106 and the instruction-tuned LLaMa-2 and Mistral models reflect their underlying architectural differences?
- Basis in paper: [explicit] The paper compares zero-shot performance across models, showing ChatGPT-1106 generally outperforms instruction-tuned LLaMa-2 and Mistral models.
- Why unresolved: While the paper demonstrates performance differences, it does not analyze the underlying architectural reasons for these differences.
- What evidence would resolve it: Detailed architectural analysis comparing how different model designs (e.g., attention mechanisms, parameter counts, pre-training approaches) influence zero-shot performance on stance detection tasks.

### Open Question 2
- Question: What specific aspects of fine-tuning strategy (e.g., learning rate, batch size, number of epochs) contribute most significantly to stance detection performance improvements?
- Basis in paper: [explicit] The paper mentions fine-tuning hyperparameters but doesn't systematically isolate which parameters matter most.
- Why unresolved: The study provides a fixed fine-tuning setup but doesn't explore sensitivity to individual hyperparameter changes or their relative importance.
- What evidence would resolve it: Controlled experiments varying individual hyperparameters while holding others constant, measuring their impact on stance detection performance.

### Open Question 3
- Question: How does the performance of fine-tuned LLMs on stance detection generalize to other languages and cultural contexts beyond English social media?
- Basis in paper: [inferred] The study focuses exclusively on English-language social media data from specific datasets.
- Why unresolved: The paper doesn't test cross-lingual or cross-cultural generalization of the fine-tuned models.
- What evidence would resolve it: Testing the fine-tuned models on stance detection datasets from different languages and cultural contexts to measure performance transfer.

## Limitations
- The study's reliance on three specific social media datasets may not capture the full diversity of online discourse, particularly informal or multimodal content.
- Computational efficiency claims for smaller models lack direct comparisons of training time, memory usage, and inference latency across different hardware configurations.
- The study does not address potential biases in stance detection, such as political or demographic skew, which could impact real-world deployment.

## Confidence
- High confidence: The superiority of fine-tuned LLMs over traditional baselines for stance detection is well-supported by the reported Favg scores (up to 86.5) and consistent performance improvements across all three models and datasets.
- Medium confidence: Claims about the efficiency of smaller models (LLaMa-2, Mistral-7B) relative to ChatGPT are supported but lack comprehensive resource usage metrics.
- Low confidence: The generalizability of results to other stance detection tasks beyond the three studied datasets remains uncertain.

## Next Checks
1. Evaluate the fine-tuned models on an additional, independently curated stance detection dataset to assess robustness and generalization beyond the three studied datasets.
2. Conduct an analysis of model predictions across different demographic groups, political affiliations, and controversial topics to identify and mitigate potential biases in stance detection.
3. Measure and compare the training time, memory usage, and inference latency of ChatGPT, LLaMa-2, and Mistral-7B on standardized hardware to quantify the computational efficiency claims.