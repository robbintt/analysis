---
ver: rpa2
title: 'ProtoS-ViT: Visual foundation models for sparse self-explainable classifications'
arxiv_id: '2406.10025'
source_url: https://arxiv.org/abs/2406.10025
tags:
- prototypes
- dataset
- size
- prototype
- prototypical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ProtoS-ViT, a novel architecture that leverages
  frozen pre-trained Vision Transformer (ViT) backbones to build self-explainable
  models for both general and biomedical image classification tasks. ProtoS-ViT achieves
  classification performance comparable to state-of-the-art deep learning models while
  producing compact and interpretable explanations.
---

# ProtoS-ViT: Visual foundation models for sparse self-explainable classifications

## Quick Facts
- arXiv ID: 2406.10025
- Source URL: https://arxiv.org/abs/2406.10025
- Reference count: 40
- Primary result: ProtoS-ViT achieves classification performance comparable to state-of-the-art deep learning models while producing compact and interpretable explanations using frozen ViT backbones and prototype matching.

## Executive Summary
This paper introduces ProtoS-ViT, a novel architecture that leverages frozen pre-trained Vision Transformer (ViT) backbones to build self-explainable models for both general and biomedical image classification tasks. ProtoS-ViT achieves classification performance comparable to state-of-the-art deep learning models while producing compact and interpretable explanations. The key innovation is the use of a frozen ViT backbone combined with a novel prototypical head that learns to match input features to learned prototypes. This approach enables the model to reuse concepts across classes, resulting in a small number of discriminative prototypes.

## Method Summary
ProtoS-ViT uses frozen DINOv2, OpenClip, or BioMedCLIP backbones with a projection head, prototype matching via cosine similarity, and a prototypical head with depthwise convolutions. The model is trained with cross-entropy loss, sparsity loss (Hoyer-Square), and tanh loss for 80 epochs with cosine decay learning rate schedule. Evaluation includes accuracy, global/local prototype sizes, and explainability metrics like faithfulness, compactness, semantic coherence, completeness, and contrastivity.

## Key Results
- Achieves classification accuracy comparable to state-of-the-art deep learning models on general and biomedical image datasets
- Produces compact explanations with small global and local prototype sizes (e.g., ~39 global prototypes, ~6-7 local prototypes per image)
- Outperforms existing prototypical models in terms of explanation faithfulness, compactness, and semantic coherence

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Using frozen vision transformer (ViT) backbones preserves spatial alignment between patch embeddings and input pixels, enabling interpretable prototypes.
- **Mechanism**: ViT backbones produce patch embeddings with strong spatial correspondence to input image locations. By freezing these embeddings, the model ensures that similarity scores computed in the embedding space map reliably back to specific regions in the input image.
- **Core assumption**: The frozen ViT backbone maintains spatial correspondence between embeddings and input patches without retraining.
- **Evidence anchors**: [abstract] "frozen pre-trained ViT backbones" and "strong spatial features" support the spatial alignment claim; [section] "Given that the ViT output has a reduced spatial dimension...we interpolate the similarity map back to the original input resolution" confirms the interpolation process preserves spatial alignment.
- **Break condition**: If the ViT backbone's spatial alignment degrades under different pretraining regimes or if fine-tuning disrupts the embedding-patch correspondence, the interpretability of prototypes would break down.

### Mechanism 2
- **Claim**: The prototype matching head uses depthwise convolutions to compute prototype scores that reflect the spatial distribution of similarity, not just the maximum activation.
- **Mechanism**: Instead of taking the max similarity per prototype across the image (as in previous methods), the head applies separate depthwise convolutions (1x1 and 3x3 kernels) to the normalized similarity maps. This aggregates similarity values across the image while preserving spatial sensitivity.
- **Core assumption**: Independent kernels per prototype prevent cross-contamination and allow accurate score aggregation.
- **Evidence anchors**: [section] "Independent kernels are key for the score to properly reflect the importance of a single prototype presence with no interactions between prototypes" explicitly supports the mechanism; [section] "two convolutions were introduced following insights from the Inception architecture [36]; a convolution with a kernel of size 1 × 1 and another one with size 3 × 3" describes the exact mechanism.
- **Break condition**: If kernel size or number is insufficient to capture the spatial extent of prototypes, the score aggregation will misrepresent prototype importance.

### Mechanism 3
- **Claim**: Hoyer-Square regularization plus tanh-loss forces sparsity in both global prototype count and local prototype usage per image, yielding compact explanations.
- **Mechanism**: The Hoyer-Square loss penalizes the norm ratio of the importance matrix, driving many entries toward zero (global sparsity). The tanh-loss ensures each prototype appears at least once per batch, preventing collapse during early training.
- **Core assumption**: Sparsity regularization does not hurt classification accuracy if prototypes are well-distributed across classes.
- **Evidence anchors**: [section] "In our work, to achieve compactness, we use a regularization loss applied on the importance matrix I, namely the Hoyer-Square (HS) [38]" and the loss formula confirm the mechanism; [section] "In addition to these terms, we also adopt the tanh-loss LT devised by [15]" explains the training stability component; [section] Table 1 and figures show global size ~39, local size ~6–7, demonstrating compactness.
- **Break condition**: If the regularization strength is too high, the model may underfit and lose accuracy; if too low, sparsity benefits vanish.

## Foundational Learning

- **Concept**: Vision Transformers (ViTs) and their patch embedding mechanism
  - Why needed here: Understanding how ViTs split images into patches, embed them, and maintain spatial correspondence is crucial to grasp why frozen backbones work for interpretability.
  - Quick check question: What is the stride of DINOv2 ViT-L/14, and how does it affect the spatial resolution of the similarity maps?

- **Concept**: Prototype-based classification and similarity matching
  - Why needed here: The core of ProtoS-ViT is matching input features to learned prototypes; knowing how cosine similarity and softmax normalization work is essential.
  - Quick check question: How does the temperature τ in the softmax affect prototype matching convergence?

- **Concept**: Sparsity-inducing regularization (Hoyer metric, L1, etc.)
  - Why needed here: The compactness of explanations depends on sparsity losses; understanding how they drive zero entries in the importance matrix is key.
  - Quick check question: What is the difference between Hoyer-Square and L1 regularization in terms of sparsity behavior?

## Architecture Onboarding

- **Component map**: Input → Frozen ViT backbone (DINOv2/OpenCLIP/BioMedCLIP) → Projection head (3 conv layers, 1x1 kernels) → Cosine similarity to prototypes → Normalization → Prototype head (depthwise convs + LayerNorm) → Classification head (positive weights) → Output

- **Critical path**: Input → ViT → Projection → Similarity → Prototype head → Classification. Any bottleneck here directly affects accuracy and explainability.

- **Design tradeoffs**:
  - Frozen backbone vs. trainable: Freezing preserves spatial alignment but limits adaptation; training allows better task fit but risks losing interpretability.
  - Number of prototypes (300 initial) vs. compactness: More prototypes can improve accuracy but hurt sparsity; fewer hurt expressiveness.
  - Kernel sizes in prototype head (1x1 and 3x3) vs. receptive field: Larger kernels capture broader context but may blur fine details.

- **Failure signatures**:
  - If prototypes collapse to single patches (all similarity concentrated in one location), the model has lost spatial interpretability.
  - If global size stays high (>200), sparsity regularization is ineffective.
  - If accuracy drops significantly compared to baseline, the architecture may be too restrictive.

- **First 3 experiments**:
  1. Train ProtoS-ViT on CUB-200-2011 with DINOv2 backbone; verify global size <50 and accuracy >85%.
  2. Remove the prototypical head (replace with max op) and compare global/local size and accuracy; confirm head improves sparsity.
  3. Remove Hoyer-Square loss and observe global size increase; confirm sparsity loss is necessary for compactness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ProtoS-ViT perform on other visual foundation models beyond DINOv2, OpenClip, and BioMedCLIP?
- Basis in paper: [inferred] The paper demonstrates ProtoS-ViT's effectiveness with these three specific backbone models but does not explore its performance with other visual foundation models.
- Why unresolved: The paper's scope is limited to these three backbones, and no comparative analysis with other visual foundation models is provided.
- What evidence would resolve it: Experiments comparing ProtoS-ViT's performance using various other visual foundation models as backbones, such as CLIP, BEiT, or MAE, on the same datasets.

### Open Question 2
- Question: Can ProtoS-ViT be extended to handle multi-modal data beyond image classification?
- Basis in paper: [inferred] The paper focuses on image classification tasks, but the concept of leveraging visual foundation models and prototypes could potentially be applied to other data types.
- Why unresolved: The paper does not explore the applicability of ProtoS-ViT to other data modalities like text, audio, or video.
- What evidence would resolve it: Studies applying ProtoS-ViT's architecture to multi-modal tasks, such as image-text classification or video action recognition, and comparing its performance to existing methods.

### Open Question 3
- Question: How does the number of prototypes impact ProtoS-ViT's performance and explainability?
- Basis in paper: [explicit] The paper uses 300 prototypes for all experiments, but it does not investigate the effect of varying the number of prototypes on the model's accuracy, compactness, and explainability.
- Why unresolved: The paper does not provide insights into the optimal number of prototypes for different tasks or the trade-offs involved in increasing or decreasing the number of prototypes.
- What evidence would resolve it: Experiments analyzing ProtoS-ViT's performance and explainability metrics across a range of prototype numbers, identifying the optimal number for different datasets and tasks.

### Open Question 4
- Question: How does ProtoS-ViT compare to other self-explainable models in terms of computational efficiency and scalability?
- Basis in paper: [explicit] The paper mentions that ProtoS-ViT is less computationally expensive to train than other architectures due to its frozen backbone, but it does not provide a detailed comparison of computational efficiency and scalability with other self-explainable models.
- Why unresolved: The paper does not quantify the computational resources required for training and inference, nor does it explore the model's scalability to larger datasets or more complex tasks.
- What evidence would resolve it: Comparative studies measuring the training and inference time, memory usage, and scalability of ProtoS-ViT against other self-explainable models on various datasets and tasks.

## Limitations

- The frozen backbone assumption is critical but not explored when fine-tuning is allowed or when using other frozen architectures like ConvNets.
- Sparsity benefits from Hoyer-Square + tanh-loss are shown qualitatively but not ablated across datasets, leaving open whether gains are task-specific.
- Explainability is measured using proxy metrics without ground-truth human explanations, so the "self-explainable" label remains partially unverified.

## Confidence

- **High**: The use of frozen ViT backbones with spatial alignment and the general architecture pipeline are well-justified and reproducible.
- **Medium**: The sparsity regularization effects and their impact on compactness are supported by ablation but lack extensive cross-dataset validation.
- **Medium**: Explainability metric superiority is demonstrated but not validated against human judgments or alternative interpretability baselines.

## Next Checks

1. Retrain ProtoS-ViT on CUB-200-2011 with a trainable ViT backbone and compare global/local prototype sizes and accuracy to frozen variants.
2. Conduct an ablation study on Hoyer-Square vs. L1 regularization across all datasets to quantify sparsity trade-offs.
3. Run human studies where annotators rank prototype-based explanations from ProtoS-ViT against Grad-CAM and existing prototypical methods for semantic coherence.