---
ver: rpa2
title: Softmax Attention with Constant Cost per Token
arxiv_id: '2404.05843'
source_url: https://arxiv.org/abs/2404.05843
tags:
- attention
- compute
- constant
- space
- elementwise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a modification to the conventional attention
  mechanism in Transformers that linearizes the computation while maintaining constant
  time and space complexity per token. Instead of using scaled dot-products to quantify
  pairwise query-key similarity, the proposed method uses logarithms of scaled dot-products
  of exponentials.
---

# Softmax Attention with Constant Cost per Token

## Quick Facts
- arXiv ID: 2404.05843
- Source URL: https://arxiv.org/abs/2404.05843
- Reference count: 3
- Implemented a 125M parameter generative language model with modified attention, achieving a competitive cross-entropy loss of 2.47 on The Pile dataset.

## Executive Summary
This paper introduces a novel modification to the conventional attention mechanism in Transformers that achieves constant time and space complexity per token while maintaining competitive performance. Instead of using scaled dot-products for query-key similarity, the method employs logarithms of scaled dot-products of exponentials, which can be expressed as log-sums of exponentials with a fixed-size latent space. The approach was implemented in a small generative language model and achieved a cross-entropy loss of 2.47 on The Pile dataset, suggesting its potential as a viable alternative to standard softmax attention.

## Method Summary
The paper proposes replacing conventional scaled dot-product attention with a linearized version using log-sums of exponentials (LSE). The method transforms attention computation by applying exponential kernel feature maps to queries and keys, then computing log-sums of exponentials over the transformed values. This allows sequential processing with constant time and space complexity per token. The implementation includes both autoregressive and non-autoregressive variants, with the autoregressive case using a parallel scan operation. To maintain numerical stability, the method restricts values to non-negative elements, avoiding complex floating-point numbers while preserving the real-valued nature of softmax mixtures.

## Key Results
- Achieved cross-entropy loss of 2.47 on The Pile dataset with a 125M parameter model
- Demonstrated constant time and space complexity per token for attention computation
- Showed competitive performance compared to state-of-the-art generative language models of similar size
- Validated practical implementation of the proposed attention mechanism

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Linearizes attention computation by replacing scaled dot-products with log-sums of exponentials
- Mechanism: Transforms computation into associative and commutative log-sums of exponentials, enabling sequential processing
- Core assumption: Transformation preserves essential attention information while enabling efficient computation
- Evidence anchors:
  - [abstract] "Our modification is expressible as a composition of log-sums of exponentials, with a latent space of constant size"
  - [section] "We walk step-by-step through a sequence of algebraic manipulations...We obtain a form of linear attention with exponential kernel feature maps"
- Break condition: If exponential kernel feature maps cannot adequately capture similarity relationships, or numerical stability becomes problematic

### Mechanism 2
- Claim: Maintains numerical stability by restricting values to non-negative elements
- Mechanism: Limiting V to non-negative elements keeps log V in the real domain, avoiding complex floating-point overhead
- Core assumption: Non-negative restriction doesn't significantly impact model expressiveness
- Evidence anchors:
  - [section] "We restrict V to elements â‰¥ 0 to avoid dealing with complex floating-point numbers"
  - [section] "If any of V's elements are negative, log V is complex, but all Softmax mixtures of V remain over R"
- Break condition: If negative values in V are crucial for representation, or restriction leads to significant information loss

### Mechanism 3
- Claim: Achieves competitive performance despite attention modifications
- Mechanism: Small generative language model with modified attention achieves 2.47 cross-entropy loss on The Pile
- Core assumption: Cross-entropy loss sufficiently indicates method effectiveness as attention mechanism
- Evidence anchors:
  - [abstract] "We implement our modification...and conclude that it is a promising alternative to conventional attention"
  - [section] "We apply our implementation...and obtain a cross-entropy loss of 2.47, competitive with state-of-the-art models"
- Break condition: If competitive performance is limited to specific tasks or domains, or scaling reveals significant limitations

## Foundational Learning

- Concept: Logarithmic and exponential functions
  - Why needed here: Method relies on transforming dot-products into logarithms of scaled dot-products of exponentials
  - Quick check question: What is the derivative of log(exp(x) + exp(y)) with respect to x?

- Concept: Matrix operations and broadcasting
  - Why needed here: Understanding elementwise operations, broadcasting, and matrix multiplications is crucial for implementation
  - Quick check question: Given two matrices A (2x3) and B (3x4), what is the shape of the result when computing A @ B?

- Concept: Numerical stability in deep learning
  - Why needed here: Method involves log-sums of exponentials, which can be numerically unstable for large values
  - Quick check question: Why do we often subtract the maximum value from inputs before applying the softmax function?

## Architecture Onboarding

- Component map: Input Q, K, V -> LSE(K^T + log V) and LSE(K^T) -> log S = LSE(Q + result) -> log Z = LSE(Q + result) -> Attention output = exp(log S - log Z) * V

- Critical path:
  1. Compute log V (if not already provided)
  2. Compute LSE(K^T + log V) and LSE(K^T)
  3. Compute log S = LSE(Q + result from step 2)
  4. Compute log Z = LSE(Q + result from step 2)
  5. Compute attention output = exp(log S - log Z) * V

- Design tradeoffs:
  - Space vs. time: Parallel scan for autoregressive attention is space-inefficient but easier to implement
  - Numerical stability: Restricting V to non-negative values avoids complex numbers but may limit expressiveness
  - Accuracy vs. efficiency: Linearization provides constant cost per token but may not capture all conventional attention nuances

- Failure signatures:
  - Numerical overflow/underflow in log-sums of exponentials
  - Degraded performance on tasks requiring long-range dependencies
  - Increased training instability or convergence issues

- First 3 experiments:
  1. Implement basic non-autoregressive attention and verify reasonable attention weights on small dataset
  2. Test autoregressive version with simple sequence modeling, comparing performance and efficiency to conventional attention
  3. Evaluate performance on masked language modeling task, comparing to conventional attention and other efficient attention alternatives

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does proposed attention mechanism's performance scale with model size compared to conventional attention and other efficient variants?
- Basis in paper: [explicit] Authors state "An adequate comparison requires... evaluating models with one to several orders of magnitude more parameters"
- Why unresolved: Only tested on 125M parameter model, limiting generalizability
- What evidence would resolve it: Comprehensive benchmarking across model sizes (small to extremely large) on diverse tasks

### Open Question 2
- Question: What is the impact on ability to recall arbitrary parts of input context compared to conventional attention?
- Basis in paper: [inferred] Authors mention "methods with a fixed-size latent space cannot outperform conventional attention" on certain tasks
- Why unresolved: No testing on tasks specifically evaluating context recall ability
- What evidence would resolve it: Systematic evaluation using LRA benchmarks and synthetic tasks for arbitrary context retrieval

### Open Question 3
- Question: How does method perform in non-autoregressive applications compared to other efficient attention variants?
- Basis in paper: [explicit] Authors mention implementation supports both autoregressive and non-autoregressive applications but only demonstrate autoregressive results
- Why unresolved: Paper lacks comprehensive evaluation of non-autoregressive applications
- What evidence would resolve it: Extensive benchmarking on non-autoregressive tasks including masked language modeling and image-to-image translation

## Limitations

- Evaluation limited to single 125M parameter model on The Pile dataset, limiting scalability insights
- Restriction to non-negative values in V may constrain expressiveness for certain information types
- Practical performance gains depend heavily on implementation details and hardware characteristics not thoroughly explored

## Confidence

- High confidence: Theoretical foundation is mathematically sound; constant complexity claim is directly supported by algebraic manipulations
- Medium confidence: Competitive performance claim based on single experimental result; more extensive evaluation would strengthen this
- Low confidence: "Promising alternative" claim is premature without broader empirical validation across diverse tasks and model sizes

## Next Checks

1. **Scalability Test**: Implement and evaluate on larger models (1B+ parameters) to assess whether constant complexity benefit scales effectively and whether performance degrades with increased model size

2. **Task Diversity Evaluation**: Test on diverse NLP tasks beyond language modeling (machine translation, summarization, reasoning) to evaluate generalizability and identify potential weaknesses in handling different attention patterns

3. **Comparison with Other Efficient Attention Methods**: Conduct systematic comparison with other constant-time attention alternatives (linear attention, Performers, FlashAttention) on same benchmarks to establish relative strengths and weaknesses