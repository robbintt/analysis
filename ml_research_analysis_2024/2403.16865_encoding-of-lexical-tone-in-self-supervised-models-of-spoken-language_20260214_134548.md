---
ver: rpa2
title: Encoding of lexical tone in self-supervised models of spoken language
arxiv_id: '2403.16865'
source_url: https://arxiv.org/abs/2403.16865
tags:
- tone
- mandarin
- language
- speech
- slms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how self-supervised spoken language models
  (SLMs) encode lexical tone, a suprasegmental phonological feature used in over 60%
  of the world's languages. The authors analyze SLMs trained on tonal (Mandarin, Vietnamese,
  Cantonese) and non-tonal (English, French) languages by probing their internal representations
  with linear classifiers on Mandarin and Vietnamese tone data.
---

# Encoding of lexical tone in self-supervised models of spoken language

## Quick Facts
- arXiv ID: 2403.16865
- Source URL: https://arxiv.org/abs/2403.16865
- Authors: Gaofei Shen; Michaela Watkins; Afra Alishahi; Arianna Bisazza; Grzegorz ChrupaÅ‚a
- Reference count: 25
- Key outcome: Self-supervised spoken language models encode lexical tone to a significant degree even when trained on non-tonal languages, with fine-tuning effects differing by language type.

## Executive Summary
This paper investigates how self-supervised spoken language models (SLMs) encode lexical tone, a suprasegmental phonological feature used in over 60% of the world's languages. The authors analyze SLMs trained on tonal (Mandarin, Vietnamese, Cantonese) and non-tonal (English, French) languages by probing their internal representations with linear classifiers on Mandarin and Vietnamese tone data. They find that SLMs encode tonal information to a significant degree regardless of whether they are trained on tonal or non-tonal languages, with tonal-language models showing stronger encoding. Fine-tuning these models for automatic speech recognition enhances tone encoding for tonal-language models but reduces it for non-tonal ones. The models exhibit perceptual patterns similar to human listeners in distinguishing Mandarin tones and consonants but do not follow the same developmental trajectory observed in children.

## Method Summary
The study uses wav2vec2-based SLMs pre-trained/fine-tuned on English, French, Mandarin, Vietnamese, and Cantonese. For analysis, hidden state activations are extracted from each model layer and average-pooled by syllable. Linear Ridge classifiers are trained to predict tone labels from these syllable-level vectors, using exclusive train-test splits with non-overlapping phoneme strings. Performance is compared against baselines using F0 contours, MFCC features, and text (BERT). For consonant analysis, classifiers predict consonant categories from syllable onsets using a similar setup.

## Key Results
- SLMs encode lexical tone significantly better than chance, even when trained on non-tonal languages
- Models trained on tonal languages show stronger tone encoding than those trained on non-tonal languages
- Fine-tuning for ASR enhances tone representations for tonal-language models but reduces them for non-tonal models
- SLMs show perceptual patterns similar to native and non-native human listeners in tone and consonant perception, but follow different developmental trajectories

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Self-supervised spoken language models encode tonal information in their internal representations, regardless of the training language's tonality.
- **Mechanism**: The models learn general acoustic-phonetic patterns during self-supervised pre-training, which include pitch-related features relevant to tone perception.
- **Core assumption**: Pitch contours and other acoustic cues for tone are learnable from raw audio without explicit labels.
- **Evidence anchors**:
  - [abstract]: "We show that SLMs encode lexical tone to a significant degree even when they are trained on data from non-tonal languages."
  - [section]: "We find that SLMs are capable of capturing tonal information, regardless of whether they are trained on tonal or non-tonal languages."
  - [corpus]: Weak evidence - the corpus search found no papers specifically analyzing tone encoding in self-supervised models, suggesting this is a novel finding.
- **Break condition**: If the pitch cues used for tone in tonal languages are fundamentally different from those learned for other phonetic contrasts, or if the models only capture segmental information.

### Mechanism 2
- **Claim**: Fine-tuning for ASR enhances tone encoding for models trained on tonal languages but reduces it for models trained on non-tonal languages.
- **Mechanism**: ASR fine-tuning optimizes the model for language-specific information needed to transcribe the audio. For tonal languages, tone is crucial for correct character output, so fine-tuning enhances tone encoding. For non-tonal languages, tone may not contribute much to transcription, so fine-tuning may remove it.
- **Core assumption**: The ASR fine-tuning objective is language-specific and depends on the role of tone in the target language's orthography.
- **Evidence anchors**:
  - [section]: "We find that fine-tuning enhances tone representations for models trained on tonal languages, but reduces them for models trained on non-tonal languages."
  - [section]: "Tonal information may not contribute much to this objective in non-tonal languages, and thus fine-tuning would tend to remove it. In tonal-language ASR however, tone information may be crucial to correctly transcribe the input audio."
  - [corpus]: Weak evidence - the corpus search did not find papers discussing the impact of ASR fine-tuning on tone encoding, indicating this is a novel finding.
- **Break condition**: If the ASR fine-tuning process does not prioritize language-specific information, or if tone plays a different role in the orthography of tonal languages than assumed.

### Mechanism 3
- **Claim**: Self-supervised spoken language models show perceptual patterns similar to native and non-native human listeners in tone and consonant perception, but do not follow the same developmental trajectory.
- **Mechanism**: The models learn to distinguish phonetic contrasts based on acoustic cues present in the training data. For tones, this may include F0 contours and other pitch-related features. For consonants, it may include voicing, place, and manner of articulation.
- **Core assumption**: The acoustic cues used by the models for tone and consonant perception are similar to those used by humans.
- **Evidence anchors**:
  - [abstract]: "We further find that SLMs behave similarly to native and non-native human participants in tone and consonant perception studies, but they do not follow the same developmental trajectory."
  - [section]: "We find that models show patterns similar to humans in discrimination of Mandarin tones and consonants, but find no evidence that they follow a similar developmental trajectory."
  - [corpus]: Weak evidence - the corpus search did not find papers comparing the perceptual patterns of self-supervised models to humans, suggesting this is a novel finding.
- **Break condition**: If the models use different acoustic cues than humans for tone and consonant perception, or if their learning process is fundamentally different from human language acquisition.

## Foundational Learning

- **Concept**: Tone and segmental phonology
  - Why needed here: Understanding the distinction between segmental features (phonemes like vowels and consonants) and suprasegmental features (tone, stress, intonation) is crucial for interpreting the results of this study.
  - Quick check question: What is the primary difference between segmental and suprasegmental phonology?

- **Concept**: Self-supervised learning in speech models
  - Why needed here: The study uses self-supervised spoken language models (SLMs), so understanding how these models learn from raw audio without explicit labels is important.
  - Quick check question: How do self-supervised speech models learn to represent linguistic information without labeled data?

- **Concept**: Probing classifiers
  - Why needed here: The study uses linear probing classifiers to analyze the internal representations of the SLMs. Understanding how probing works is essential for interpreting the results.
  - Quick check question: What is the purpose of using probing classifiers in the analysis of self-supervised models?

## Architecture Onboarding

- **Component map**: Feature encoder -> Transformer layers -> Probing classifiers
- **Critical path**:
  1. Pre-train SLMs on tonal and non-tonal languages using self-supervised objectives.
  2. Extract hidden state activations from the SLMs for each syllable in the test data.
  3. Train linear probing classifiers on the extracted activations to predict tone labels.
  4. Evaluate the tone classification accuracy across different layers of the SLMs.
- **Design tradeoffs**:
  - The choice of self-supervised pre-training objective (e.g., contrastive loss) affects the type of linguistic information encoded in the SLMs.
  - The architecture of the SLMs (e.g., wav2vec2 vs. HuBERT) influences their ability to capture suprasegmental features like tone.
  - The probing classifier setup (e.g., exclusive train-test split) determines the extent to which the classifiers can exploit lexical cues for tone prediction.
- **Failure signatures**:
  - Low tone classification accuracy across all layers of the SLMs, suggesting that the models do not encode tonal information.
  - High tone classification accuracy only in the final layers of the SLMs, indicating that tonal information is only captured in the later stages of processing.
  - Inconsistent tone classification accuracy across different tonal languages, suggesting that the models do not generalize well to different tone systems.
- **First 3 experiments**:
  1. Replicate the tone classification experiments using the same setup as the paper to verify the findings.
  2. Investigate the impact of different self-supervised pre-training objectives on the encoding of tonal information in SLMs.
  3. Compare the tone classification accuracy of SLMs with different architectures (e.g., wav2vec2, HuBERT) to identify the most effective architecture for capturing suprasegmental features.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do self-supervised spoken language models encode tone information in a way that mirrors human developmental trajectories in tone perception?
- Basis in paper: [explicit] The paper states that while SLMs show patterns similar to humans in tone and consonant perception, they do not follow the same developmental trajectory observed in children.
- Why unresolved: The study compares tone and consonant classification accuracy in SLMs at different training steps but does not find a differential trajectory similar to that observed in human infants, where tone sensitivity develops earlier than consonant perception.
- What evidence would resolve it: Further experiments tracking SLM performance on tone vs. consonant tasks across training stages, with a focus on early-stage learning, could reveal whether SLMs eventually develop a human-like pattern or maintain a distinct trajectory.

### Open Question 2
- Question: How does the amount and quality of training data influence the encoding of suprasegmental features like tone in self-supervised spoken language models?
- Basis in paper: [inferred] The paper acknowledges that the training data for the models tested varies in size and quality, and that the Vietnamese model was trained on a significantly larger dataset (13k hours) compared to others (around 1000 hours).
- Why unresolved: The study does not control for the amount and quality of training data across different models, making it difficult to isolate the effect of data characteristics on tone encoding.
- What evidence would resolve it: Training multiple SLMs on datasets of varying sizes and qualities, while keeping other factors constant, and comparing their tone encoding capabilities, would clarify the impact of data characteristics.

### Open Question 3
- Question: Can multilingual self-supervised spoken language models encode suprasegmental features from different languages more robustly than monolingual models?
- Basis in paper: [explicit] The paper suggests that it would be interesting to investigate if multilingual SLMs encode segmentals and suprasegmentals from different languages more robustly.
- Why unresolved: The study focuses on monolingual SLMs and does not explore the capabilities of multilingual models in encoding suprasegmental features.
- What evidence would resolve it: Training and testing multilingual SLMs on datasets containing multiple tonal and non-tonal languages, and comparing their tone encoding performance to that of monolingual models, would provide insights into the advantages of multilingual training.

## Limitations

- The study relies on forced alignments for syllable segmentation, which may introduce errors in tone annotation, particularly for tonal languages with complex tone sandhi rules.
- The exclusive train-test split construction method is not fully specified, raising concerns about potential leakage of lexical information between splits.
- The comparison with human perceptual studies uses different datasets and experimental paradigms, limiting direct comparability.
- The corpus analysis reveals no prior work specifically examining tone encoding in self-supervised models, suggesting this is a novel contribution but also indicating limited external validation.

## Confidence

- **High confidence**: Models encode tonal information regardless of training language - supported by consistent experimental results across multiple models and languages.
- **Medium confidence**: Fine-tuning effects differ between tonal and non-tonal models - results are consistent but the interpretation of ASR optimization mechanisms remains somewhat speculative.
- **Low confidence**: Similarity to human perceptual patterns without shared developmental trajectory - comparison methodology differs significantly from human studies, and developmental trajectory analysis is limited.

## Next Checks

1. Verify forced alignment accuracy for tonal languages by comparing automatic alignments against manual annotations on a subset of data.
2. Reconstruct the exclusive train-test split procedure to confirm no lexical information leakage between splits.
3. Conduct ablation studies removing pitch-related features from model inputs to determine if tone encoding depends on F0 information specifically.