---
ver: rpa2
title: Weakly Semi-supervised Tool Detection in Minimally Invasive Surgery Videos
arxiv_id: '2401.02791'
source_url: https://arxiv.org/abs/2401.02791
tags:
- detection
- tool
- labels
- refinement
- supervised
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a weakly semi-supervised tool detection method
  for minimally invasive surgery videos. The approach addresses the high annotation
  cost of fully supervised object detection by using a small amount of fully annotated
  images and a large amount of image-level labeled data.
---

# Weakly Semi-supervised Tool Detection in Minimally Invasive Surgery Videos

## Quick Facts
- arXiv ID: 2401.02791
- Source URL: https://arxiv.org/abs/2401.02791
- Authors: Ryo Fujii; Ryo Hachiuma; Hideo Saito
- Reference count: 0
- Primary result: Proposes weakly semi-supervised approach using teacher model, MIL refinement, and co-occurrence loss for surgical tool detection

## Executive Summary
This paper addresses the high annotation cost of fully supervised object detection for surgical tools by proposing a weakly semi-supervised approach. The method combines a small amount of fully annotated images with a large amount of image-level labeled data to train a tool detection model. The key innovation is a refinement model that uses multiple instance learning and co-occurrence statistics to improve the quality of pseudo-labels generated by a teacher model. Experiments on the EndoVis2018 dataset demonstrate significant improvements over semi-supervised baselines, achieving up to 10.7 percentage points improvement in mAP when using only 27% fully labeled data.

## Method Summary
The approach consists of three stages: a teacher model (Faster R-CNN) trained on fully annotated data generates pseudo-labels for weakly labeled images, a refinement model uses MIL and image-level labels to improve pseudo-label quality, and a student model is trained on both fully labeled data and refined pseudo-labels. The refinement model incorporates a co-occurrence loss based on point-wise mutual information between tool pairs to leverage their tendency to appear together. This semi-supervised framework significantly reduces annotation costs while maintaining high detection performance.

## Key Results
- Achieves up to 10.7 percentage points improvement in mAP over semi-supervised baselines
- Outperforms fully supervised approach when using only 27% fully labeled data
- Refined pseudo-labels lead to better student model performance compared to unrefined pseudo-labels
- Co-occurrence loss enhances refinement model performance by leveraging tool pair relationships

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The refinement model improves pseudo-label quality by leveraging image-level labels through MIL.
- Mechanism: The refinement model uses a transformer encoder to aggregate instance-level predictions from the teacher model, then applies max-pooling across instances to predict image-level labels. This allows the model to learn which instance-level predictions are correct without requiring ground truth boxes.
- Core assumption: At least one instance in each image has the correct label, and the max-pooling operation can effectively identify this instance.
- Evidence anchors:
  - [abstract] "We conduct MIL regarding instance labels as the category of proposals in a frame and a bag as a weakly annotated image-level label."
  - [section] "We use the aggregation function g(·) to aggregate the set of instance-level probabilities to predict the bag-level probabilities: p(yk = 1|x1, x2, ..., xN) = g(p1, p2, ..., xN)"
- Break condition: If the teacher model consistently generates incorrect pseudo-labels, or if the image-level labels are unreliable, the refinement model will learn incorrect mappings and degrade performance.

### Mechanism 2
- Claim: The co-occurrence loss captures statistical relationships between tool pairs to improve classification accuracy.
- Mechanism: The co-occurrence loss uses point-wise mutual information between tool pairs to create a matrix Sij, which represents how often tools appear together. This information is incorporated into the loss function to encourage the model to consider these relationships when making predictions.
- Core assumption: The co-occurrence statistics observed in the training data are representative of the actual surgical workflow and can be used as a reliable prior for classification.
- Evidence anchors:
  - [abstract] "we introduce a co-occurrence loss to enhance the performance of the refinement model, leveraging the observation that certain surgical tool pairs often co-occur."
  - [section] "We then transform the scores using the logit function: Si,j = (1/(1+exp(si,j)) if si,j > 0, 0 if otherwise.)"
- Break condition: If the co-occurrence statistics are not representative of actual surgical practice, or if they change significantly between training and deployment scenarios, the model may make incorrect predictions based on spurious correlations.

### Mechanism 3
- Claim: The semi-supervised approach achieves comparable performance to fully supervised methods with significantly less annotation cost.
- Mechanism: By combining a small amount of fully labeled data with a large amount of weakly labeled data, the model can learn from both precise box annotations and broader image-level context. The teacher model generates pseudo-labels, which are then refined using the MIL approach.
- Core assumption: The small amount of fully labeled data is sufficient to train an initial teacher model that can generate useful pseudo-labels for the larger weakly labeled dataset.
- Evidence anchors:
  - [abstract] "Experiments on the EndoVis2018 dataset show that the proposed method significantly outperforms semi-supervised baselines, achieving up to 10.7 percentage points improvement in mAP when using 27% fully labeled data."
  - [section] "Figure 3 (left) summarizes the results of the methods in different data split settings. The proposed and semi-supervised frameworks that utilize the pseudo-labels outperform the supervised framework"
- Break condition: If the ratio of fully labeled to weakly labeled data is too skewed, or if the weakly labeled data is of poor quality, the semi-supervised approach may not provide sufficient supervision to match fully supervised performance.

## Foundational Learning

- Concept: Multiple Instance Learning (MIL)
  - Why needed here: MIL allows learning from image-level labels without requiring individual instance labels, which is crucial when only weak supervision is available.
  - Quick check question: In MIL, if an image is labeled as containing "scissors" but we don't know which bounding box contains scissors, how do we train a classifier?

- Concept: Transformer-based feature interaction
  - Why needed here: The transformer encoder helps the model learn relationships between different surgical tools in the same image, which is important for tools that often appear together.
  - Quick check question: Why might a transformer encoder be more effective than simple concatenation for combining features from multiple surgical tools?

- Concept: Co-occurrence statistics and mutual information
  - Why needed here: Understanding how often tools appear together helps the model disambiguate between similar-looking tools based on their likely companions.
  - Quick check question: If bipolar forceps and curved scissors have high mutual information, what does this tell us about their usage patterns in surgery?

## Architecture Onboarding

- Component map: Teacher model (Faster R-CNN) → Pseudo-label generation → Refinement model (transformer-based MIL) → Refined pseudo-labels → Student model (Faster R-CNN)
- Critical path: The refinement model is the critical component that distinguishes this approach from standard semi-supervised methods. It directly impacts the quality of pseudo-labels and thus the final student model performance.
- Design tradeoffs: The use of a transformer encoder in the refinement model adds computational complexity but enables better modeling of tool relationships. The co-occurrence loss adds another hyperparameter (α) to tune.
- Failure signatures: Poor performance on tool pairs that have similar appearances but different co-occurrence patterns, or degraded performance when the ratio of fully to weakly labeled data changes significantly.
- First 3 experiments:
  1. Compare mAP with and without the refinement model using the same amount of fully labeled data
  2. Test the impact of different values of α in the co-occurrence loss on overall performance
  3. Evaluate performance when using different proportions of fully labeled data (e.g., 10%, 27%, 50%) to understand the scaling behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the proposed method perform on surgical tool detection tasks in other types of surgeries beyond minimally invasive procedures, such as open surgeries or robot-assisted surgeries with different tool sets?
- Basis in paper: [inferred] The paper focuses specifically on minimally invasive surgery videos from the EndoVis2018 dataset, which contains a specific set of surgical tools used in laparoscopic procedures. The method's generalizability to other surgical contexts is not explored.
- Why unresolved: The paper only evaluates the method on one specific dataset and surgical type. There's no discussion of how well the approach might transfer to different surgical scenarios with varying tools, imaging conditions, or surgical workflows.
- What evidence would resolve it: Experiments applying the same framework to different surgical datasets (e.g., cataract surgery, neurosurgery, or open abdominal surgery) would demonstrate its generalizability. Comparative analysis showing performance across different surgical domains would provide evidence of transferability.

### Open Question 2
- Question: How does the proposed method's performance scale with increasing amounts of weakly labeled data versus fully labeled data, and what is the optimal ratio for different annotation budgets?
- Basis in paper: [explicit] The paper mentions that it "strikes a balance between the extremely costly annotation burden and detection performance" and evaluates different proportions of fully annotated data (27% to 81%). However, it doesn't systematically explore the trade-off curve or determine optimal ratios.
- Why unresolved: The paper only tests fixed proportions of labeled data but doesn't analyze how performance scales with different ratios of fully vs. weakly labeled data, or what the diminishing returns are for additional annotations.
- What evidence would resolve it: A comprehensive study varying both the total annotation budget and the ratio of fully to weakly labeled data would reveal optimal allocation strategies. Performance curves showing mAP versus annotation cost for different ratios would provide actionable guidance.

### Open Question 3
- Question: How robust is the proposed method to noisy or incorrect image-level labels in the weakly annotated dataset, and what mechanisms could be implemented to handle label noise?
- Basis in paper: [inferred] The paper uses image-level labels in the MIL framework but doesn't discuss scenarios where these labels might be incorrect or noisy. In real-world surgical settings, weak labels could contain errors due to human oversight or variability in surgical practices.
- Why unresolved: The paper assumes perfect image-level labels for the MIL training but doesn't address the common real-world scenario of label noise in weakly supervised learning. There's no discussion of how noise might propagate through the pseudo-label generation and refinement stages.
- What evidence would resolve it: Experiments deliberately introducing noise into the image-level labels and measuring degradation in performance would quantify robustness. Implementation and evaluation of noise-robust MIL methods or label correction mechanisms would demonstrate potential solutions.

## Limitations
- Relies heavily on accurate image-level labels and representative co-occurrence statistics from training data
- Assumes teacher model generates reliable pseudo-labels, which may not hold if model performance is poor
- Limited evaluation to single surgical dataset may not reflect performance in diverse clinical environments

## Confidence
- **High**: Overall framework effectiveness with 10.7 percentage point mAP improvement
- **Medium**: Specific contribution of co-occurrence loss without ablation studies
- **Low**: Scalability and generalizability to other surgical datasets and clinical settings

## Next Checks
1. **Ablation study on co-occurrence loss**: Remove the co-occurrence loss component and retrain the refinement model to quantify its specific contribution to overall performance improvements.

2. **Teacher model quality assessment**: Evaluate the teacher model's performance on weakly labeled data to determine the reliability of initial pseudo-labels, as this directly impacts refinement model effectiveness.

3. **Cross-dataset generalization test**: Apply the trained model to a different surgical video dataset (e.g., Cholec80 or M2CAI) to assess generalizability beyond the EndoVis2018 dataset used in the paper.