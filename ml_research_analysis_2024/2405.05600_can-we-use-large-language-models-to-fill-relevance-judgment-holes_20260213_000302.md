---
ver: rpa2
title: Can We Use Large Language Models to Fill Relevance Judgment Holes?
arxiv_id: '2405.05600'
source_url: https://arxiv.org/abs/2405.05600
tags:
- relevance
- judgments
- test
- labels
- chatgpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates using Large Language Models (LLMs) to fill
  "holes" in incomplete test collections caused by unjudged documents. The study focuses
  on conversational search using the TREC iKAT dataset, where information needs are
  highly dynamic and result in larger holes.
---

# Can We Use Large Language Models to Fill Relevance Judgment Holes?

## Quick Facts
- arXiv ID: 2405.05600
- Source URL: https://arxiv.org/abs/2405.05600
- Authors: Zahra Abbasiantaeb; Chuan Meng; Leif Azzopardi; Mohammad Aliannejadi
- Reference count: 34
- Primary result: LLM-generated relevance judgments combined with human judgments result in substantially lower correlations when ranking retrieval models

## Executive Summary
This paper investigates using Large Language Models (LLMs) to fill "holes" in incomplete test collections caused by unjudged documents, focusing on conversational search using the TREC iKAT dataset. The authors evaluate both commercial (ChatGPT) and open-source (LLaMA) LLMs in zero-shot, few-shot, and fine-tuned settings. While previous work showed LLMs correlate well with human judgments in absolute terms, this study finds that LLM-generated judgments combined with human judgments result in substantially lower correlations when ranking retrieval models. The authors observe that the choice of LLM significantly impacts the ranking of new systems, with this effect magnified by the size of the holes.

## Method Summary
The study evaluates LLMs for filling relevance judgment holes in the TREC iKAT 2023 conversational search dataset using 28 retrieval runs. The authors test zero-shot, one-shot, and two-shot prompting for ChatGPT, and fine-tune LLaMA-3 on partial human-labeled data using 4-bit QLoRA. They compare LLM-generated judgments with human judgments using Cohen's Kappa, and evaluate ranking consistency using Kendall's Tau, Spearman's Rho, and Rank-Biased Overlap (RBO) metrics. The experiments examine whether generating judgments on entire document pools yields more consistent rankings than filling individual holes.

## Key Results
- LLM-generated judgments combined with human judgments result in substantially lower correlations when ranking retrieval models
- Generating LLM annotations on the entire document pool yields more consistent rankings with human-generated labels than filling individual holes
- The choice of LLM significantly impacts the ranking of new systems, with this effect magnified by the size of the holes

## Why This Works (Mechanism)
Assumption: The mechanism likely relates to how LLMs handle context and judgment consistency across different document subsets, though the paper doesn't explicitly explain the underlying causes of the observed phenomena.

## Foundational Learning
- Conversational search: Information needs that evolve dynamically through dialogue; why needed to understand dataset context; quick check: TREC iKAT involves multi-turn conversations
- Relevance judgment holes: Unjudged documents in test collections that prevent fair evaluation; why needed to understand the core problem; quick check: missing judgments create evaluation gaps
- Kendall's Tau and Spearman's Rho: Rank correlation metrics for comparing model rankings; why needed to evaluate ranking consistency; quick check: values range from -1 to 1
- Cohen's Kappa: Agreement metric for comparing labelers; why needed to measure LLM-human agreement; quick check: values >0.6 indicate substantial agreement
- 4-bit QLoRA: Parameter-efficient fine-tuning method; why needed for efficient LLM adaptation; quick check: reduces memory requirements while maintaining performance

## Architecture Onboarding

Component map:
Human judgments -> LLM generation -> Combined judgments -> Ranking evaluation -> Correlation metrics

Critical path:
TREC iKAT dataset → Prompt engineering → LLM judgment generation → Kendall's Tau/Spearman's Rho/RBO calculation → Ranking comparison

Design tradeoffs:
- Zero-shot vs. fine-tuned LLMs: Simplicity vs. accuracy
- Hole-by-hole vs. entire pool generation: Targeted vs. consistent judgments
- Binary vs. graded judgments: Simplicity vs. granularity

Failure signatures:
- Low Cohen's Kappa values indicate poor LLM-human agreement
- Large differences in Kendall's Tau between subsets suggest unstable rankings
- Inconsistent rankings across different LLMs indicate sensitivity to model choice

3 first experiments:
1. Compare zero-shot ChatGPT judgments with human judgments using Cohen's Kappa
2. Generate LLM judgments on entire document pool and evaluate ranking consistency
3. Fine-tune LLaMA-3 and compare ranking stability with zero-shot results

## Open Questions the Paper Calls Out
Unknown: The paper doesn't explicitly identify open questions, suggesting potential areas for future research might include: how different hole sizes affect ranking stability across diverse datasets, whether selective hole-filling strategies could improve evaluation quality, or how to develop more robust LLM-based judgment generation methods that maintain ranking consistency.

## Limitations
- Findings may be dataset-specific to TREC iKAT conversational search and may not generalize to other domains
- Limited exploration of different hole sizes and distributions affecting ranking stability conclusions
- Study does not comprehensively evaluate selective hole-filling strategies or their potential benefits

## Confidence
- High confidence: Empirical finding that combining LLM and human judgments reduces ranking consistency
- Medium confidence: Conclusion that fine-tuning LLMs is crucial for improving judgment quality
- Medium confidence: Observation that hole size affects ranking stability

## Next Checks
1. Test ranking consistency findings across multiple IR datasets beyond TREC iKAT to assess generalizability
2. Conduct controlled experiments varying hole sizes and distributions to better understand their impact on ranking stability
3. Implement and evaluate comprehensive fine-tuning approaches to determine if LLM-generated judgments can match human judgment quality for ranking evaluation