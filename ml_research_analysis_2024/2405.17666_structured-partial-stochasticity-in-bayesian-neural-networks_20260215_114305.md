---
ver: rpa2
title: Structured Partial Stochasticity in Bayesian Neural Networks
arxiv_id: '2405.17666'
source_url: https://arxiv.org/abs/2405.17666
tags: []
core_contribution: This paper tackles the problem of poor performance in approximate
  Bayesian inference for neural networks due to posterior symmetries. Specifically,
  many neural network posterior modes are functionally equivalent under parameter
  permutations, which complicates inference and can lead to pathologies like underfitting
  or overconfidence.
---

# Structured Partial Stochasticity in Bayesian Neural Networks

## Quick Facts
- arXiv ID: 2405.17666
- Source URL: https://arxiv.org/abs/2405.17666
- Authors: Tommy Rochussen
- Reference count: 10
- One-line primary result: Structured partial stochasticity improves Bayesian neural network inference by removing permutation symmetries through selective weight fixing

## Executive Summary
This paper addresses a fundamental challenge in Bayesian neural network inference: posterior symmetries that arise from neuron permutation equivalence. These symmetries create unidentifiability issues and complicate approximate inference methods like mean-field variational inference (MFVI). The proposed solution, structured partial stochasticity, selectively fixes certain weights as constants in a structured manner to break permutation symmetries while preserving functional expressivity. The approach is evaluated on 1D regression and the UCI Energy Efficiency dataset, demonstrating improved predictive accuracy and uncertainty calibration compared to vanilla MFVI.

## Method Summary
The method involves identifying layers where neuron permutation symmetries can be broken (Dℓ(f c) ≤ 1) and applying structured pruning schemes to remove specific connections. Two schemes are proposed: light pruning removes connections between top and bottom neuron pairs, while heavy pruning removes triangular sections of connections. Fixed weights can take values of zero (pruning), a constant ±c, or their maximum a posteriori (MAP) values. The remaining weights are then learned using mean-field variational inference. This approach simplifies the posterior distribution by eliminating factorial numbers of equivalent modes, allowing approximate inference methods to converge more reliably.

## Key Results
- Structured partial stochasticity achieves RMSE of 0.076 vs 0.115 for vanilla MFVI on UCI Energy Efficiency dataset
- Heavy pruning scheme shows particularly strong performance in both 1D regression and UCI experiments
- Improved uncertainty calibration demonstrated through higher log posterior predictive (LPP) probabilities
- Light pruning scheme also improves performance but less dramatically than heavy pruning

## Why This Works (Mechanism)

### Mechanism 1: Unique Connection Patterns
Structured pruning removes permutation symmetries by forcing unique neuron connection patterns. By selectively removing connections between neurons (light pruning removes top/bottom pairs, heavy pruning removes triangular sections), each neuron in a layer is forced to have a unique pattern of incoming and outgoing connections. This eliminates the ability to permute neurons without changing the network function.

### Mechanism 2: Non-zero Constant Fixing
Setting weights to non-zero constants breaks symmetries more effectively than pruning alone. Instead of setting pruned weights to zero, they are fixed to ±c or MAP values. This forces non-pruned weights to take larger values to maintain network output scale, pushing them away from zero where symmetries would otherwise persist.

### Mechanism 3: Simplified Posterior Distribution
Removing symmetries simplifies the posterior distribution, making approximate inference more effective. Bayesian neural network posteriors have factorially many modes corresponding to functionally equivalent weight configurations due to permutation symmetries. By removing these symmetries through structured partial stochasticity, the posterior becomes less complex with fewer redundant modes, allowing approximate inference methods to converge more reliably.

## Foundational Learning

- **Permutation symmetries in neural networks**: Understanding that different weight configurations can represent the same function is crucial to grasping why structured partial stochasticity is necessary. Quick check: If you swap two neurons in the same layer and correspondingly permute the weight matrices on either side, does the network function change?

- **Bayesian neural network posterior modes**: The paper's core argument is that posterior modes corresponding to equivalent functions create inference challenges that structured partial stochasticity addresses. Quick check: Why does the Bayesian posterior for a network with permutation symmetries have factorially many modes?

- **Approximate inference methods (MFVI)**: The experiments use mean-field variational inference, and understanding its limitations with symmetric posteriors is key to appreciating the results. Quick check: What are the known pathologies of MFVI in Bayesian neural networks (e.g., underfitting, collapsing to prior)?

## Architecture Onboarding

- **Component map**: MLP with L layers, each with Dℓ neurons -> Weight sets W = {Wℓ}Lℓ=1 for layer weights, B = {bℓ}Lℓ=1 for biases -> Structured pruning schemes (light/heavy) -> Fixed weight policies (zero/±c/MAP) -> MFVI over remaining stochastic weights

- **Critical path**: 1) Identify layers where Dℓ(f c) ≤ 1 using Equation 2, 2) Apply structured pruning scheme (light or heavy), 3) Fix pruned weights according to chosen policy, 4) Perform MFVI on remaining stochastic weights, 5) Evaluate predictive performance and uncertainty calibration

- **Design tradeoffs**: Light vs. Heavy pruning (heavy removes more weights but may reduce model capacity), Zero vs. non-zero fixing (non-zero values force larger weights in remaining parameters but add hyperparameters c), Stochastic vs. deterministic weights (more fixed weights simplify inference but reduce flexibility)

- **Failure signatures**: Persistent poor performance despite structured pruning (may indicate remaining symmetries or insufficient model capacity), Numerical instability (large values of c can cause optimization issues), Over-regularization (too many fixed weights may prevent the network from fitting the data)

- **First 3 experiments**: 1) Implement light pruning with zero-fixing on a simple 1D regression task and compare to vanilla MFVI, 2) Test heavy pruning with constant c = 5 on the UCI Energy Efficiency dataset, 3) Compare structured pruning with random weight fixing (same number of fixed weights but no symmetry-breaking structure) to isolate the effect of symmetry removal

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions. However, it mentions that investigating the effects of the proposed symmetry-removal schemes on the true posterior would be useful, and that the procedures can be adapted for other feed-forward architectures.

## Limitations
- Experiments limited to MLPs with only two datasets (1D regression and UCI Energy Efficiency) with relatively small sample sizes
- Unclear how the approach generalizes to larger-scale problems and more complex architectures like CNNs or RNNs
- Missing direct comparison to alternative symmetry-breaking approaches such as bias-ordering constraints or skip connections

## Confidence
- **High**: Permutation symmetries exist in neural network posteriors and create inference challenges
- **Medium**: Structured partial stochasticity effectively removes these symmetries in the tested cases
- **Low**: The proposed approach generalizes robustly across different network architectures and datasets

## Next Checks
1. Test structured partial stochasticity on larger datasets (e.g., CIFAR-10, ImageNet) with convolutional architectures to verify generalization beyond MLPs
2. Compare against alternative symmetry-breaking approaches like input augmentation or learned weight initializations
3. Perform ablation studies to quantify the contribution of structured vs. random weight fixing to performance improvements