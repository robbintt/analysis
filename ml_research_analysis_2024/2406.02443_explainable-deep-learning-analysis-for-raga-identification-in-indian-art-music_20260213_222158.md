---
ver: rpa2
title: Explainable Deep Learning Analysis for Raga Identification in Indian Art Music
arxiv_id: '2406.02443'
source_url: https://arxiv.org/abs/2406.02443
tags:
- raga
- music
- audio
- dataset
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of Raga identification in Indian
  Art Music by developing an interpretable deep learning model. The authors curated
  a dataset of 191 hours of Hindustani Classical Music recordings, manually annotated
  for Raga and tonic labels, and trained a CNN-LSTM model achieving a chunk-wise F1-score
  of 0.89 for 12 Raga classes.
---

# Explainable Deep Learning Analysis for Raga Identification in Indian Art Music

## Quick Facts
- arXiv ID: 2406.02443
- Source URL: https://arxiv.org/abs/2406.02443
- Authors: Parampreet Singh; Vipul Arora
- Reference count: 39
- Primary result: CNN-LSTM model achieves 0.89 F1-score for 12-class Raga classification with XAI methods aligning with expert annotations

## Executive Summary
This paper addresses the problem of Raga identification in Indian Art Music by developing an interpretable deep learning model. The authors curated a dataset of 191 hours of Hindustani Classical Music recordings, manually annotated for Raga and tonic labels, and trained a CNN-LSTM model achieving a chunk-wise F1-score of 0.89 for 12 Raga classes. They then applied XAI techniques (SoundLIME and GradCAM++) to visualize and validate whether the model's predictions align with human expert understanding of Ragas. The results showed significant overlap between model explanations and expert annotations, with SoundLIME achieving higher precision than GradCAM++. This work demonstrates the effectiveness of interpretable models in understanding complex musical structures like Ragas and sets a foundation for future research in music analysis and pedagogy.

## Method Summary
The authors developed a CNN-LSTM model for Raga identification using 191 hours of Hindustani Classical Music recordings. Audio was split into 30-second chunks (23,005 total) and processed into tonic-normalized chromagram features (938x12). The model architecture consists of two convolutional layers followed by LSTM and dense layers for 12-class classification. Post-hoc explanations were generated using GradCAM++ and SoundLIME, then compared with expert annotations of salient regions using precision metrics. The dataset was curated with manual annotations for Raga and tonic labels, and speech segments were removed during preprocessing.

## Key Results
- CNN-LSTM model achieves 0.89 F1-score on 12-class Raga classification
- SoundLIME explanations show higher precision than GradCAM++ when compared with expert annotations
- Significant overlap found between model-identified salient regions and expert-annotated musical patterns
- Model demonstrates ability to capture musically meaningful patterns that align with human understanding of Ragas

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tonic normalization enables cross-performer raga identification by aligning microtonal pitch variations to a common reference.
- Mechanism: By shifting chromagram pitch bins based on each recording's tonic value, the model learns features invariant to absolute pitch, focusing instead on the relative pitch relationships that define each raga's melodic structure.
- Core assumption: The tonic-labeled by experts accurately reflects the pitch reference used in the performance.
- Evidence anchors:
  - [abstract] "tonic normalization as explained in Section IV-B"
  - [section] "tonic normalization...aligning all performances to a common reference note"
- Break condition: If tonic labels are incorrect or if a performance uses a pitch other than the stated tonic, normalization will misalign features and degrade classification accuracy.

### Mechanism 2
- Claim: SoundLIME provides more reliable explanations than GradCAM++ for the CNN-LSTM architecture because it incorporates the full model's decision process.
- Mechanism: SoundLIME treats the model as a black box and generates explanations by masking audio segments and observing prediction changes, capturing the influence of temporal patterns processed by both CNN and LSTM layers.
- Core assumption: The model's prediction is sensitive to the order and combination of audio features over time, which GradCAM++ misses by focusing only on CNN features.
- Evidence anchors:
  - [abstract] "SoundLIME achieving higher precision than GradCAM++"
  - [section] "SL outperforms GC, particularly for the top few seconds"
- Break condition: If the LSTM layers do not significantly contribute to classification, the advantage of SoundLIME would diminish.

### Mechanism 3
- Claim: High XAI precision correlates with model accuracy, validating that explanations reflect true musical understanding.
- Mechanism: When XAI methods identify time frames that experts also mark as salient, it indicates the model is using the same musically meaningful features humans use for raga identification.
- Core assumption: Expert annotations of salient regions capture the essential musical elements that define a raga.
- Evidence anchors:
  - [abstract] "We compare the generated explanations with human expert annotations"
  - [section] "a significant overlap, affirming that the patterns...are present and are being used by the Automatic Raga Identification (ARI) model"
- Break condition: If experts disagree significantly on salient regions, or if the model relies on non-musical cues, the correlation between XAI precision and accuracy would break down.

## Foundational Learning

- Concept: Tonic normalization in music information retrieval
  - Why needed here: Different performers sing the same raga in different keys; normalization aligns these to a common reference so the model learns raga-specific patterns rather than key-specific ones.
  - Quick check question: If a raga is performed in C and another in G, what does tonic normalization do to their chromagram features before classification?

- Concept: Chromagram feature extraction and interpretation
  - Why needed here: Chromagrams capture pitch class content over time, which is critical for identifying raga-specific melodic patterns like pakad and aaroh-avroh.
  - Quick check question: How does a chromagram represent the presence of the note "Sa" across different octaves in a 30-second audio clip?

- Concept: XAI methods for audio (SoundLIME vs gradient-based methods)
  - Why needed here: Understanding how different XAI techniques reveal model decision processes, especially when the model includes temporal layers (LSTM) that gradient methods might miss.
  - Quick check question: Why might SoundLIME be more suitable than GradCAM++ for explaining a CNN-LSTM model's predictions?

## Architecture Onboarding

- Component map: Input audio (30-second chunks) -> Chromagram extraction (938x12) -> Tonic normalization -> CNN feature extraction (256 feature maps) -> LSTM temporal modeling -> Dense layer (12 classes) -> Softmax output -> XAI (GradCAM++ or SoundLIME) -> Saliency map/attribution -> Comparison with expert annotations

- Critical path: Feature extraction and normalization must be consistent between training and inference; CNN layers must learn meaningful spatial patterns in the chromagram; LSTM layers must capture temporal dependencies critical for raga identification; XAI methods must align with expert annotations to validate model decisions.

- Design tradeoffs: Using tonic normalization improves cross-performer generalization but requires accurate tonic labeling; SoundLIME provides better explanations for CNN-LSTM but is computationally heavier than GradCAM++; Limiting to 12 raga classes ensures high performance but reduces dataset diversity.

- Failure signatures: Poor performance on cross-dataset tests indicates overfitting to training data or insufficient generalization; Low XAI precision suggests the model may be using non-musical cues or irrelevant features; High disagreement between XAI methods and expert annotations indicates potential issues with model interpretability.

- First 3 experiments: 1) Train the model with and without tonic normalization on a small subset to quantify its impact on accuracy; 2) Generate explanations using both GradCAM++ and SoundLIME on correctly classified examples to compare precision with expert annotations; 3) Test the trained model on an external dataset (e.g., Saraga) to evaluate generalization and identify failure modes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the identified salient regions in Raga identification models relate to specific musical concepts like "Khatka" and "murki"?
- Basis in paper: [explicit] The authors suggest incorporating musical concepts like "Khatka" and "murki" to refine Raga classifiers in future research.
- Why unresolved: The paper does not explore the specific relationship between these musical concepts and the model's identified salient regions.
- What evidence would resolve it: An analysis of how the model's salient regions align with these specific musical concepts in actual Raga performances.

### Open Question 2
- Question: Can the model's explanations be made more interpretable by generating listenable or musical explanations?
- Basis in paper: [explicit] The authors propose developing methods to generate listenable or musical explanations that could serve as standalone identifiers of Raga.
- Why unresolved: The paper does not implement or evaluate such methods.
- What evidence would resolve it: A method to extract and present musical phrases from audio clips that correspond to the Raga, along with an evaluation of its effectiveness in music pedagogy.

### Open Question 3
- Question: How does the model's performance on distinguishing between closely related Ragas like Bihag and Maru-Bihag vary with different audio representations?
- Basis in paper: [inferred] The paper highlights the difficulty in distinguishing between closely related Ragas and the model's performance on such cases.
- Why unresolved: The paper does not explore the impact of different audio representations on the model's performance for closely related Ragas.
- What evidence would resolve it: An experiment comparing the model's performance on different audio representations (e.g., different feature sets or preprocessing methods) for distinguishing between closely related Ragas.

## Limitations

- Analysis constrained to 12 raga classes from a single dataset, limiting generalizability across the broader raga system
- Manual annotation of salient regions introduces subjective variability, though inter-annotator agreement was not reported
- Effectiveness of tonic normalization depends on accuracy of expert-labeled tonic values, which was not independently verified

## Confidence

- High Confidence: Model achieves F1-score of 0.89 on 12-class raga classification task
- Medium Confidence: XAI methods successfully align with expert annotations, though subjective annotation introduces variability
- Medium Confidence: SoundLIME outperforms GradCAM++ for this architecture, though the advantage may depend on specific model characteristics

## Next Checks

1. Test the model on an external dataset (e.g., Saraga) to verify generalization beyond the curated dataset
2. Conduct ablation studies removing the LSTM component to quantify its contribution to SoundLIME's advantage
3. Perform cross-validation with multiple expert annotators to establish the reliability of salient region annotations and calculate inter-annotator agreement scores