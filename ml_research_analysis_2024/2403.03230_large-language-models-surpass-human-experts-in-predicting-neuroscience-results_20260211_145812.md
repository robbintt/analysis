---
ver: rpa2
title: Large language models surpass human experts in predicting neuroscience results
arxiv_id: '2403.03230'
source_url: https://arxiv.org/abs/2403.03230
tags:
- llms
- human
- neuroscience
- experts
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) trained on scientific literature can
  accurately predict neuroscience experimental outcomes, outperforming human experts
  by a significant margin. A novel benchmark, BrainBench, was developed to evaluate
  LLMs' ability to select the correct results from altered versions of neuroscience
  abstracts.
---

# Large language models surpass human experts in predicting neuroscience results

## Quick Facts
- arXiv ID: 2403.03230
- Source URL: https://arxiv.org/abs/2403.03230
- Reference count: 34
- Large language models (LLMs) trained on scientific literature can accurately predict neuroscience experimental outcomes, outperforming human experts by a significant margin

## Executive Summary
Large language models (LLMs) trained on scientific literature can accurately predict neuroscience experimental outcomes, outperforming human experts by a significant margin. A novel benchmark, BrainBench, was developed to evaluate LLMs' ability to select the correct results from altered versions of neuroscience abstracts. On average, LLMs achieved 81.4% accuracy compared to human experts' 63.4%. Notably, LLMs' confidence in their predictions was well-calibrated with their accuracy. Fine-tuning a base LLM, Mistral-7B, on neuroscience literature using LoRA further improved performance, creating "BrainGPT." These results demonstrate LLMs' potential to integrate complex scientific information and predict novel findings, suggesting their utility as tools for scientific discovery and complementing human expertise.

## Method Summary
The authors developed BrainBench, a novel benchmark consisting of 2,000 questions derived from 1,000 neuroscience abstracts, where LLMs must select the correct results from altered versions. They evaluated multiple LLMs including GPT-4, GPT-3.5, Claude-2, Llama-2, Vicuna, and Mistral-7B on this benchmark. Additionally, they fine-tuned Mistral-7B using LoRA on neuroscience literature to create "BrainGPT." Human experts were recruited to complete the same benchmark for comparison. The study measured both accuracy and confidence calibration of the models.

## Key Results
- LLMs achieved 81.4% accuracy on BrainBench compared to human experts' 63.4%
- LLMs showed well-calibrated confidence, with higher confidence correlating with higher accuracy
- Fine-tuned BrainGPT achieved the highest accuracy among all models tested
- The performance gap between LLMs and human experts was consistent across different question types

## Why This Works (Mechanism)
The success of LLMs in predicting neuroscience results likely stems from their ability to process and integrate vast amounts of scientific literature, capturing patterns and relationships that may not be immediately apparent to human experts. By training on extensive scientific corpora, these models develop a comprehensive understanding of experimental methodologies, theoretical frameworks, and historical trends in neuroscience research. The multiple-choice format of BrainBench may also play a role, as LLMs can leverage their knowledge of common experimental outcomes and research trajectories to eliminate incorrect options. The well-calibrated confidence suggests that the models have developed an internal representation of their own certainty, which is crucial for practical applications in scientific discovery.

## Foundational Learning
1. **Neuroscience experimental design** - Understanding of common experimental paradigms and methodologies is essential for predicting outcomes. Quick check: Model correctly identifies experimental approach from abstract.
2. **Scientific literature patterns** - Recognition of typical results patterns and research trajectories in neuroscience. Quick check: Model can distinguish between expected and surprising results.
3. **Multiple-choice reasoning** - Ability to eliminate incorrect options based on knowledge and logic. Quick check: Model accuracy on single-answer vs. multiple-answer questions.
4. **Confidence calibration** - Understanding of the model's own certainty in predictions. Quick check: Correlation between confidence scores and actual accuracy.

## Architecture Onboarding

Component Map:
Mistral-7B -> LoRA Fine-tuning -> BrainGPT -> BrainBench Evaluation
Human Experts -> BrainBench Evaluation
Multiple LLMs -> BrainBench Evaluation

Critical Path:
1. Abstract selection and BrainBench question construction
2. LLM inference on BrainBench questions
3. Human expert evaluation on BrainBench questions
4. Performance comparison and analysis

Design Tradeoffs:
- Multiple-choice format simplifies evaluation but may not capture full reasoning complexity
- Fine-tuning on neuroscience literature improves performance but requires extensive computational resources
- Human expert comparison provides validation but introduces variability in expertise levels

Failure Signatures:
- LLMs consistently selecting incorrect options may indicate overfitting to training data patterns
- Poor confidence calibration suggests inadequate understanding of uncertainty
- Human expert performance matching or exceeding LLMs would indicate benchmark limitations

First Experiments:
1. Ablation study removing neuroscience literature from fine-tuning to measure contribution
2. Testing LLMs on out-of-domain scientific abstracts to assess generalization
3. Evaluating human experts with extended time limits to determine if performance gap is due to time constraints

## Open Questions the Paper Calls Out
None

## Limitations
- The BrainBench benchmark consists of only 2,000 questions from 1,000 abstracts, raising questions about generalizability
- Human expert group composition and expertise levels are not fully characterized
- The fine-tuning methodology and data selection process for the neuroscience literature corpus are not sufficiently detailed
- The performance gap may not translate to practical significance in real-world scientific discovery

## Confidence

High Confidence:
- LLMs can predict neuroscience experimental outcomes better than average human performance on the BrainBench benchmark
- The confidence calibration of LLMs is well-aligned with their accuracy on this task
- The BrainBench benchmark is a novel contribution for evaluating scientific reasoning in LLMs

Medium Confidence:
- The performance improvement from fine-tuning demonstrates genuine reasoning enhancement rather than memorization
- The benchmark captures meaningful aspects of neuroscience experimental reasoning
- The human expert comparison is fair and representative

Low Confidence:
- The 17.6% absolute performance gap represents a practically significant difference in real-world scientific discovery
- The results generalize to neuroscience subfields not represented in the benchmark
- The LLM performance translates to actual scientific insight generation beyond multiple-choice selection

## Next Checks
1. Conduct a cross-validation study using multiple independent sets of neuroscience abstracts to test the robustness of the BrainBench benchmark and ensure the performance gap is not an artifact of the specific abstract selection or question construction process.

2. Perform ablation studies on the fine-tuning process, systematically varying the composition, quantity, and quality of the neuroscience literature corpus to determine the minimum requirements for achieving the observed performance improvements and test for potential overfitting.

3. Design a follow-up study where both LLMs and human experts must generate predictions for entirely new neuroscience experiments (not multiple-choice selection) and compare the quality, novelty, and accuracy of their predictions through expert evaluation and subsequent experimental validation.