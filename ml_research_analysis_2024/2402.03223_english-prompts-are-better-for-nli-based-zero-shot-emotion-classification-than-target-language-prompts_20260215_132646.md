---
ver: rpa2
title: English Prompts are Better for NLI-based Zero-Shot Emotion Classification than
  Target-Language Prompts
arxiv_id: '2402.03223'
source_url: https://arxiv.org/abs/2402.03223
tags: []
core_contribution: This paper investigates whether English prompts or translated target-language
  prompts perform better for zero-shot emotion classification using NLI-based multilingual
  language models. The authors conduct experiments across 18 languages, 7 prompt types,
  and 6 NLI models on three emotion corpora.
---

# English Prompts are Better for NLI-based Zero-Shot Emotion Classification than Target-Language Prompts

## Quick Facts
- **arXiv ID:** 2402.03223
- **Source URL:** https://arxiv.org/abs/2402.03223
- **Reference count:** 40
- **Primary result:** English prompts outperform translated target-language prompts for zero-shot emotion classification across 18 languages and multiple NLI models

## Executive Summary
This paper investigates whether English prompts or translated target-language prompts perform better for zero-shot emotion classification using NLI-based multilingual language models. The authors conduct experiments across 18 languages, 7 prompt types, and 6 NLI models on three emotion corpora. They find that English prompts consistently outperform translated prompts for all languages and models tested, with average macro-F1 improvements of 0.025 on the largest dataset. The performance differences are stable across different prompt types and models, with emo-s and emo-name prompts performing best overall. Analysis of individual examples reveals cases where English prompts avoid spurious correlations present in translated prompts.

## Method Summary
The authors systematically evaluate English versus translated prompts for zero-shot emotion classification using NLI-based multilingual models. They test 18 languages (including Arabic, Chinese, English, Hindi, and others) with 7 different prompt types (such as "The author is angry" and "The author feels angry") across 6 NLI models (including XLM-RoBERTa, InfoXLM, and others). The experiments use three emotion corpora: GoEmotions, EmpatheticDialogues, and Crowdflower. For each language and prompt type, they generate both English prompts and prompts translated using Google Translate, then measure performance using macro-F1 and accuracy metrics.

## Key Results
- English prompts outperform translated target-language prompts for all 18 languages tested
- Average macro-F1 improvement of 0.025 on the largest dataset (GoEmotions)
- Performance differences are stable across different prompt types, with emo-s and emo-name prompts performing best overall
- Analysis reveals cases where English prompts avoid spurious correlations present in translated prompts

## Why This Works (Mechanism)
The paper suggests that English prompts leverage the training bias of multilingual NLI models, which are predominantly trained on English data. When prompts are translated, they may introduce spurious correlations between words and emotions that don't exist in the original English prompts. For example, certain words in translated prompts might coincidentally appear more frequently with specific emotions, leading the model to learn these spurious associations rather than genuine emotional content. The English prompts, being closer to the model's training distribution, avoid these artifacts and allow the model to rely on more robust, generalizable patterns learned during pretraining.

## Foundational Learning
- **Zero-shot learning**: Why needed - enables classification without task-specific training data; Quick check - can the model classify emotions in a language it hasn't been fine-tuned on?
- **Natural Language Inference (NLI)**: Why needed - provides a framework for transforming classification into entailment tasks; Quick check - does the model correctly identify entailment relationships in emotion prompts?
- **Cross-lingual transfer**: Why needed - allows knowledge transfer between languages; Quick check - can a model trained on English data perform well on other languages?
- **Prompt engineering**: Why needed - determines how effectively the task is communicated to the model; Quick check - do different prompt formulations affect classification accuracy?
- **Multilingual language models**: Why needed - enables processing of multiple languages with a single model; Quick check - does the model handle linguistic variations across languages effectively?
- **Spurious correlations**: Why needed - identifies unintended patterns that may bias model predictions; Quick check - are there unintended word-emotion associations in translated prompts?

## Architecture Onboarding

**Component map:** NLI model -> Prompt generator (English/Translated) -> Emotion classifier -> Evaluation metrics

**Critical path:** Prompt generation → NLI inference → Emotion classification → Performance evaluation

**Design tradeoffs:** English prompts leverage model training bias but may miss cultural nuances; translated prompts attempt cultural alignment but introduce translation artifacts and spurious correlations

**Failure signatures:** Degraded performance on languages with significant cultural differences in emotional expression; systematic errors where translated prompts introduce unintended associations

**First experiments:**
1. Compare English vs translated prompts on a single language with controlled prompt variations
2. Test performance across different NLI model architectures with identical prompts
3. Analyze error patterns to identify specific types of spurious correlations in translated prompts

## Open Questions the Paper Calls Out
None

## Limitations
- Results may not generalize to other semantic tasks beyond emotion classification
- Analysis of spurious correlations relies on qualitative examples rather than systematic error analysis
- Does not investigate the impact of prompt translation quality or cultural differences in emotional expression

## Confidence
- **Core finding (English prompts outperform translated):** High
- **Explanation regarding spurious correlations:** Medium
- **Broader implications for multilingual NLP:** Low

## Next Checks
1. Replicate experiments with additional semantic tasks (e.g., sentiment analysis, topic classification) to assess task generality
2. Conduct systematic error analysis to quantify spurious correlations in translated prompts across all languages
3. Test whether prompt quality (rather than language) drives performance differences by comparing professionally translated prompts against automated translations