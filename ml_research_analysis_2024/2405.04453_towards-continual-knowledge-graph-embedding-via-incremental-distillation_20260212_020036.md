---
ver: rpa2
title: Towards Continual Knowledge Graph Embedding via Incremental Distillation
arxiv_id: '2405.04453'
source_url: https://arxiv.org/abs/2405.04453
tags:
- knowledge
- uni00000013
- uni00000015
- incde
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles continual knowledge graph embedding (CKGE),
  where a KGE model must adapt to new triples while preserving old knowledge. Existing
  CKGE methods ignore the graph structure, learning new triples randomly and preserving
  old knowledge with equal priority, leading to catastrophic forgetting.
---

# Towards Continual Knowledge Graph Embedding via Incremental Distillation

## Quick Facts
- arXiv ID: 2405.04453
- Source URL: https://arxiv.org/abs/2405.04453
- Reference count: 5
- Primary result: IncDE significantly outperforms strong baselines on continual KGE tasks, with incremental distillation contributing 0.2%-6.5% MRR improvement

## Executive Summary
This paper addresses continual knowledge graph embedding (CKGE), where models must adapt to new triples while preserving old knowledge without catastrophic forgetting. The authors identify that existing CKGE methods ignore graph structure, leading to random learning of new triples and indiscriminate preservation of old knowledge. They propose IncDE, a method that uses hierarchical ordering to prioritize new triples based on graph structure, incremental distillation to preserve old entity representations layer-by-layer, and a two-stage training strategy to prevent early corruption of old knowledge. Experiments on seven datasets show that IncDE significantly outperforms strong baselines, demonstrating effective learning of new knowledge while preserving old knowledge across all time steps.

## Method Summary
IncDE tackles CKGE through three key mechanisms: hierarchical ordering, incremental distillation, and two-stage training. Hierarchical ordering uses BFS expansion from the old graph to group new triples into layers, with intra-layer ordering based on entity centrality and relation betweenness. Incremental distillation preserves old entity representations by transferring them layer-by-layer using importance-weighted loss during training. Two-stage training first freezes old entity/relation embeddings while training only new ones, then trains all embeddings to prevent early corruption. The method was evaluated on seven datasets using TransE as the base KGE model, with performance measured by MRR, Hits@1, and Hits@10.

## Key Results
- IncDE significantly outperforms strong baselines across all seven datasets
- The incremental distillation mechanism alone contributes 0.2%-6.5% improvement in MRR
- IncDE effectively learns new knowledge while preserving old knowledge across all time steps
- Performance degradation over time steps is minimized compared to existing methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incremental distillation preserves old entity representations layer-by-layer during continual learning.
- Mechanism: When training on new triples, the model distills the representation of entities that have appeared in previous layers using their closest prior representation, with importance-weighted loss.
- Core assumption: Entity representations change gradually and can be preserved through incremental distillation without catastrophic forgetting.
- Evidence anchors:
  - [abstract]: "we devise a novel incremental distillation mechanism, which facilitates the seamless transfer of entity representations from the previous layer to the next one, promoting old knowledge preservation"
  - [section]: "During training, we use incremental distillation to preserve the old knowledge... If entity e in the j-th (j > 0) layer has appeared in a previous layer, we distill it with the representation of e from the nearest layer"
- Break condition: If entity representations change drastically between layers, distillation becomes ineffective and catastrophic forgetting occurs.

### Mechanism 2
- Claim: Hierarchical ordering optimizes the learning sequence of new triples based on graph structure.
- Mechanism: New triples are grouped into layers using BFS expansion from the old graph, then further ordered within layers by entity centrality and relation betweenness.
- Core assumption: The graph structure determines the optimal learning order for new knowledge.
- Evidence anchors:
  - [abstract]: "we introduce a hierarchical strategy, ranking new triples for layer-by-layer learning. By employing the inter- and intra-hierarchical orders together, new triples are grouped into layers based on the graph structure features"
  - [section]: "We use the bread-first search (BFS) algorithm to progressively partition ∆Ti from Gi−1... we compute the node centrality of the head entity h, the node centrality of the tail entity t, and the betweenness centrality of the relation r"
- Break condition: If the graph structure is not representative of learning dependencies, hierarchical ordering may prioritize irrelevant triples.

### Mechanism 3
- Claim: Two-stage training prevents early corruption of old knowledge by under-trained new knowledge.
- Mechanism: First stage freezes old entity/relation embeddings while training only new ones, then second stage trains all embeddings.
- Core assumption: New knowledge needs initial stabilization before it can safely update old representations.
- Evidence anchors:
  - [abstract]: "we adopt a two-stage training paradigm to avoid the over-corruption of old knowledge influenced by under-trained new knowledge"
  - [section]: "In the first training stage, IncDE freezes the embeddings of all old entities Ei−1 and relations Ri−1 and trains only the embeddings of new entities ∆Ei and relations ∆Ri"
- Break condition: If new knowledge is stable enough initially, two-stage training may unnecessarily slow down adaptation.

## Foundational Learning

- Concept: Knowledge Graph Embedding (KGE)
  - Why needed here: IncDE builds upon standard KGE models like TransE, extending them for continual learning
  - Quick check question: What is the scoring function in TransE, and how does it measure triple plausibility?

- Concept: Catastrophic Forgetting
  - Why needed here: CKGE's primary challenge is preserving old knowledge while learning new triples
  - Quick check question: What are the main causes of catastrophic forgetting in neural networks during continual learning?

- Concept: Graph Centrality Measures
  - Why needed here: Used to determine the importance of entities and relations for ordering and distillation
  - Quick check question: How do node centrality and betweenness centrality differ in measuring entity importance in graphs?

## Architecture Onboarding

- Component map: Hierarchical Ordering -> Triple Grouping -> Layer-by-Layer Training -> Incremental Distillation -> Entity Representation Preservation -> Two-Stage Training -> Knowledge Protection Strategy -> Base KGE Model (e.g., TransE) -> Embedding Generation

- Critical path: Hierarchical ordering -> layer assignment -> incremental distillation during training -> two-stage training -> final embeddings

- Design tradeoffs:
  - Complexity vs. performance: Hierarchical ordering adds preprocessing overhead but improves learning order
  - Preservation vs. adaptation: Two-stage training protects old knowledge but may slow new knowledge integration
  - Layer size (M) vs. training efficiency: Larger layers reduce distillation frequency but may overwhelm single training steps

- Failure signatures:
  - Performance degradation over time steps indicates catastrophic forgetting
  - Inconsistent results across runs suggest instability in distillation process
  - Slow convergence may indicate overly restrictive two-stage training

- First 3 experiments:
  1. Run IncDE on ENTITY dataset with default parameters and measure MRR improvement over Fine-tune baseline
  2. Test ablation by removing incremental distillation and compare performance loss
  3. Vary maximum layer size M (128, 512, 1024, 2048) to find optimal configuration for GraphHigher dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does IncDE perform on continual learning tasks when old knowledge is deleted as knowledge graphs evolve?
- Basis in paper: [explicit] The authors mention that in future work, they will consider handling situations where old knowledge is deleted as knowledge graphs evolve.
- Why unresolved: The paper focuses on scenarios where new knowledge is added, but does not address the case where existing knowledge is removed.
- What evidence would resolve it: Experimental results comparing IncDE's performance on knowledge graph evolution with both additions and deletions of triples, entities, and relations.

### Open Question 2
- Question: Can IncDE effectively integrate cross-domain and heterogeneous data into expanding knowledge graphs?
- Basis in paper: [explicit] The authors mention that it is imperative to address the integration of cross-domain and heterogeneous data into expanding knowledge graphs as future work.
- Why unresolved: The paper focuses on continual learning within a single knowledge graph, but does not explore how IncDE would handle the integration of diverse, multi-domain data.
- What evidence would resolve it: Experimental results demonstrating IncDE's performance on knowledge graph embedding tasks involving cross-domain and heterogeneous data sources.

### Open Question 3
- Question: What is the impact of different base KGE models on IncDE's performance in continual knowledge graph embedding tasks?
- Basis in paper: [inferred] The paper uses TransE as the base KGE model for IncDE, but does not explore the effects of using other KGE models such as DistMult, ComplEx, or RotatE.
- Why unresolved: The choice of base KGE model can significantly influence the overall performance of IncDE, but the paper does not provide insights into how different base models would affect the results.
- What evidence would resolve it: Comparative experiments using IncDE with various base KGE models, analyzing the impact on key performance metrics like MRR, Hits@1, and Hits@10.

## Limitations

- The specific formulas for hierarchical ordering (node/relation centrality computation) are not fully detailed
- The exact implementation details for dynamic distillation weight matrix W are unclear
- The two-stage training schedule lacks precise details on when to transition between stages

## Confidence

- Hierarchical ordering effectiveness: Medium - The concept is sound but specific ordering metrics need validation
- Incremental distillation mechanism: High - The layer-by-layer preservation approach is well-justified
- Two-stage training benefits: Medium - While intuitively correct, optimal timing parameters are unclear

## Next Checks

1. Reproduce hierarchical ordering: Implement BFS-based layer partitioning and centrality-based triple ranking, then verify that high-centrality entities appear in earlier layers

2. Test distillation sensitivity: Vary the importance weight computation (using different centrality metrics) to determine if performance changes significantly

3. Analyze stage transition: Experiment with different freezing durations in two-stage training to find optimal balance between old knowledge preservation and new knowledge integration