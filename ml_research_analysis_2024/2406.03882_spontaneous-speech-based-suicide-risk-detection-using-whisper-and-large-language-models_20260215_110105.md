---
ver: rpa2
title: Spontaneous Speech-Based Suicide Risk Detection Using Whisper and Large Language
  Models
arxiv_id: '2406.03882'
source_url: https://arxiv.org/abs/2406.03882
tags:
- suicide
- speech
- risk
- fusion
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates automatic suicide risk detection using
  spontaneous speech from adolescents. A Mandarin dataset with 15 hours of speech
  from over 1000 adolescents aged 10-18 was collected.
---

# Spontaneous Speech-Based Suicide Risk Detection Using Whisper and Large Language Models

## Quick Facts
- arXiv ID: 2406.03882
- Source URL: https://arxiv.org/abs/2406.03882
- Reference count: 0
- Achieved F1-score of 0.846 on a test set of 119 subjects

## Executive Summary
This paper presents a novel approach to automatic suicide risk detection using spontaneous speech from Mandarin-speaking adolescents aged 10-18. The study collected a 15-hour dataset from over 1000 adolescents and explored the use of Whisper speech models combined with large language models for suicide risk detection. The system achieved promising results with an accuracy of 0.807 and F1-score of 0.846 on the test set, demonstrating the potential of speech-based AI models for real-world suicide risk assessment applications.

## Method Summary
The study collected spontaneous speech from over 1000 Mandarin-speaking adolescents aged 10-18, creating a dataset of approximately 15 hours of audio. The approach utilized Whisper speech models to transcribe the spontaneous speech, which was then processed by large language models for suicide risk detection. The research explored various tuning strategies including all-parameter finetuning and parameter-efficient finetuning, along with different fusion approaches. The model was evaluated on a test set of 119 subjects, comparing different training and fusion methodologies to optimize performance for suicide risk detection.

## Key Results
- Achieved accuracy of 0.807 and F1-score of 0.846 on test set with 119 subjects
- Demonstrated effectiveness of combining Whisper speech models with large language models for suicide risk detection
- Explored multiple tuning strategies and fusion approaches to optimize model performance

## Why This Works (Mechanism)
The proposed system works by leveraging the strengths of both speech and language models in a complementary fashion. Whisper transcribes spontaneous speech into text with high accuracy, capturing nuances in speech patterns that may indicate suicide risk. The large language model then analyzes this transcribed text, detecting linguistic markers, semantic patterns, and contextual cues associated with suicide risk. This two-stage approach allows the system to process natural, spontaneous speech rather than relying on structured responses, potentially capturing more authentic indicators of mental state and risk.

## Foundational Learning
- **Whisper Speech Model**: Needed to convert spontaneous audio speech into accurate text transcriptions; quick check: verify transcription accuracy on test audio samples
- **Large Language Models**: Required for analyzing transcribed text and identifying suicide risk indicators; quick check: evaluate LLM's ability to detect known suicide risk markers in text
- **Parameter-Efficient Fine-tuning**: Used to adapt large models to suicide risk detection task without full retraining; quick check: compare performance with different parameter-efficient methods
- **Multi-modal Fusion**: Combines speech and language model outputs for enhanced prediction; quick check: test fusion strategies on validation set
- **Spontaneous Speech Processing**: Captures natural conversation patterns rather than structured responses; quick check: analyze model performance on different speech styles
- **Mandarin Language Processing**: Addresses language-specific challenges in suicide risk detection; quick check: validate performance across different Mandarin dialects

## Architecture Onboarding
Component map: Audio -> Whisper -> Text -> LLM -> Risk Score
Critical path: Spontaneous speech capture → Whisper transcription → LLM risk assessment → Output prediction
Design tradeoffs: Real-time processing vs. model accuracy; computational cost vs. performance; generalization vs. specificity to Mandarin adolescents
Failure signatures: Poor transcription quality from Whisper affecting downstream predictions; cultural/language biases in LLM affecting risk assessment
First experiments:
1. Validate transcription quality on a subset of audio samples using human raters
2. Test LLM performance on manually transcribed speech to isolate transcription errors
3. Evaluate model performance across different age groups within the 10-18 range

## Open Questions the Paper Calls Out
None

## Limitations
- Small test set size (119 subjects) may limit generalizability of results
- Single-site study with Mandarin-speaking adolescents may not represent diverse populations
- Limited detail on computational requirements and inference time for real-world deployment

## Confidence
- Model Performance: Medium confidence - Promising F1-score but needs validation on larger, more diverse datasets
- Generalizability: Low confidence - Results based on single demographic group and language
- Technical Implementation: Medium confidence - Innovative approach but lacks detailed reproducibility information

## Next Checks
1. Test the model on a multi-site, multi-language dataset with diverse demographic representation to assess generalizability
2. Conduct prospective clinical validation comparing model predictions against clinical assessments over time
3. Perform detailed bias analysis across age, gender, and cultural subgroups to ensure equitable performance