---
ver: rpa2
title: Learning Gaze-aware Compositional GAN
arxiv_id: '2405.20643'
source_url: https://arxiv.org/abs/2405.20643
tags:
- gaze
- images
- data
- image
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a generative adversarial network-based method
  for generating gaze-annotated facial images from limited labeled data and transferring
  the model to unlabeled in-the-wild datasets. The core idea is to train a gaze-aware
  compositional GAN that can generate realistic images with controlled gaze directions
  and then adapt the model to a larger unlabeled dataset to exploit its diversity.
---

# Learning Gaze-aware Compositional GAN

## Quick Facts
- **arXiv ID:** 2405.20643
- **Source URL:** https://arxiv.org/abs/2405.20643
- **Reference count:** 6
- **Primary result:** GAN-based method generates gaze-annotated facial images from limited labeled data and adapts to unlabeled datasets, improving gaze estimation accuracy.

## Executive Summary
This paper introduces a Gaze-aware Compositional GAN (GC-GAN) that learns to generate realistic facial images with controlled gaze directions from limited labeled data. The method first trains on a small labeled dataset to learn gaze-aware image generation, then adapts to a larger unlabeled dataset using segmentation masks as weak supervision. Experiments show that GC-GAN improves gaze estimation accuracy when used for data augmentation, reducing error from 4.54Â° to 3.86Â° on ETH-XGaze. The approach also enables applications like facial image editing and gaze redirection while maintaining good perceptual quality.

## Method Summary
The method employs a two-stage training approach. First, a Gaze-aware Compositional GAN is trained on a labeled dataset (ETH-XGaze) to learn generating gaze-annotated facial images. The model uses a compositional architecture with local generators for different facial components and a render net. In the second stage, the trained model is adapted to an unlabeled dataset (CelebAMask-HQ) by freezing certain layers and using a modified discriminator that enforces gaze consistency through segmentation masks. This enables the model to generate diverse synthetic images that improve gaze estimation DNN accuracy when used for augmentation.

## Key Results
- Reduces gaze estimation error from 4.54Â° to 3.86Â° on ETH-XGaze dataset using cross-domain augmentations
- Achieves FrÃ©chet Inception Distance of 15.3 and Inception Score of 1.92 for generated image quality
- Outperforms standard augmentation techniques in improving gaze estimation DNN accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** GC-GAN learns to generate gaze-annotated facial images by combining labeled limited data with unlabeled diverse data.
- **Mechanism:** The model first trains on a small labeled dataset to learn gaze-aware image generation. It then adapts to an unlabeled dataset by freezing certain layers and using a modified discriminator that enforces gaze consistency through segmentation masks.
- **Core assumption:** Gaze direction can be inferred from segmentation masks when annotations are unavailable.
- **Evidence anchors:**
  - [abstract] "The core idea is to train a gaze-aware compositional GAN that can generate realistic images with controlled gaze directions and then adapt the model to a larger unlabeled dataset."
  - [section 3.2] "We also add a mask loss ð¿ð‘”, which is minimized together with the same losses as the first stage. This loss helps to preserve the same generated mask for the same latent vector and, thus, the same gaze direction."
  - [corpus] Weak evidence; corpus papers do not directly discuss GAN-based gaze synthesis from unlabeled data.
- **Break condition:** If the segmentation mask does not reliably encode gaze direction, the adaptation stage will fail.

### Mechanism 2
- **Claim:** Disentangled latent space allows editing of specific facial components without affecting gaze.
- **Mechanism:** The model architecture separates latent codes for different facial components (e.g., eyes, nose, eyebrows). Gaze-related components are conditioned on the target gaze direction, while unrelated components are not.
- **Core assumption:** Certain facial components are gaze-independent and can be modeled separately.
- **Evidence anchors:**
  - [section 3.1] "To obtain a disentangled latent space for different face components, the latent code ð‘¤ is divided into ð¾ local latent codes and an additional base latent code wð‘ð‘Žð‘ ð‘’, common for all the face components."
  - [section 4.4] "The GC-GAN design enables independent control of components and a disentangled latent space."
  - [corpus] Weak evidence; no direct mention of component-wise disentanglement for gaze control in corpus papers.
- **Break condition:** If gaze becomes entangled with other components, editing will not preserve gaze direction.

### Mechanism 3
- **Claim:** GAN-based augmentations improve gaze estimation DNN accuracy more than standard augmentations.
- **Mechanism:** The model generates realistic synthetic images with varied gaze directions and facial appearances, providing diverse training data that better represents real-world conditions.
- **Core assumption:** Synthetic data generated by the GAN is realistic enough to improve model generalization.
- **Evidence anchors:**
  - [abstract] "Experiments demonstrate the effectiveness of the approach in generating within-domain and cross-domain image augmentations that improve the accuracy of gaze estimation deep neural networks."
  - [section 4.2] "While all augmentations lead to an error drop, the reduction is significantly higher when using GAN-based augmentations."
  - [corpus] Weak evidence; corpus papers do not evaluate GAN-based data augmentation for gaze estimation.
- **Break condition:** If generated images are not realistic enough, they may not improve or could degrade DNN performance.

## Foundational Learning

- **Concept:** Generative Adversarial Networks (GANs)
  - **Why needed here:** GANs are the core technology used to generate realistic synthetic images with controlled gaze directions.
  - **Quick check question:** What are the two main components of a GAN, and what are their roles?
- **Concept:** Latent space manipulation and disentanglement
  - **Why needed here:** Understanding how to manipulate latent vectors to control specific facial features and gaze direction is crucial for the model's functionality.
  - **Quick check question:** How does separating latent codes for different components enable more precise control over image generation?
- **Concept:** Domain adaptation techniques
  - **Why needed here:** The model needs to adapt from a labeled dataset to an unlabeled one, which requires knowledge of domain adaptation methods.
  - **Quick check question:** What are common strategies for adapting a model to a new domain without labels?

## Architecture Onboarding

- **Component map:** Latent vector mapping (MLP) -> Local Generators -> Render Net -> Image and Mask -> Discriminator
- **Critical path:** Latent vector â†’ Local Generators â†’ Render Net â†’ Image and Mask â†’ Discriminator
- **Design tradeoffs:**
  - Freezing layers during adaptation improves transfer but may limit fine-tuning.
  - Conditioning all generators on gaze increases complexity but may improve coherence.
  - Using segmentation masks for gaze supervision avoids needing annotations but relies on mask quality.
- **Failure signatures:**
  - Poor image quality or artifacts indicate issues in the generator.
  - Inaccurate gaze direction suggests problems in the conditioning mechanism or adaptation stage.
  - Mode collapse in the GAN training indicates instability.
- **First 3 experiments:**
  1. Train the GC-GAN on the labeled ETH-XGaze dataset and evaluate FID and IS metrics.
  2. Perform gaze redirection on synthetic images and measure gaze estimation accuracy.
  3. Conduct ablation study by freezing different components during the adaptation stage and compare gaze preservation.

## Open Questions the Paper Calls Out
- **Open Question 1:** How does the GC-GAN model's performance generalize to real-world gaze estimation scenarios beyond the controlled ETH-XGaze and MPIIFaceGaze datasets?
- **Open Question 2:** Can the GC-GAN model be extended to handle multi-person gaze estimation scenarios?
- **Open Question 3:** How does the GC-GAN model's performance vary with different levels of gaze annotation accuracy in the training data?

## Limitations
- Reliance on segmentation masks to infer gaze direction during adaptation stage lacks direct validation
- Claims about component-wise disentanglement are not well supported by evidence in the paper or corpus
- Superiority of GAN-based augmentations over standard methods is asserted but not rigorously compared

## Confidence
- **High:** The two-stage training approach (labeled to unlabeled data) and the overall framework for gaze-annotated image generation.
- **Medium:** The use of mask loss for gaze preservation during adaptation, given limited direct evidence in the corpus.
- **Low:** Claims about component-wise disentanglement and the specific advantage of GAN-based augmentations over standard methods.

## Next Checks
1. **Mask Gaze Inference Validation:** Conduct experiments to verify that segmentation masks reliably encode gaze direction across diverse datasets.
2. **Ablation on Disentanglement:** Perform an ablation study isolating the effect of component-wise disentanglement on gaze preservation and editing accuracy.
3. **Augmentation Comparison:** Compare GAN-based augmentations against standard augmentation techniques (e.g., rotation, scaling) in terms of their impact on gaze estimation accuracy and diversity.