---
ver: rpa2
title: Enriching Ontologies with Disjointness Axioms using Large Language Models
arxiv_id: '2410.03235'
source_url: https://arxiv.org/abs/2410.03235
tags:
- disjointness
- classes
- disjoint
- knowledge
- ontology
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates using large language models (LLMs) to automatically
  identify and assert disjointness axioms in ontologies, addressing the common lack
  of explicit disjointness declarations. The authors propose a method combining LLM-based
  classification with logical inference to maintain consistency and reduce LLM interactions.
---

# Enriching Ontologies with Disjointness Axioms using Large Language Models

## Quick Facts
- arXiv ID: 2410.03235
- Source URL: https://arxiv.org/abs/2410.03235
- Reference count: 40
- LLMs successfully identified over 170,000 disjointness axioms in DBpedia ontology

## Executive Summary
This study investigates using large language models (LLMs) to automatically identify and assert disjointness axioms in ontologies, addressing the common lack of explicit disjointness declarations. The authors propose a method combining LLM-based classification with logical inference to maintain consistency and reduce LLM interactions. Their approach leverages subclass relationships to deduce additional disjointness statements and ensures satisfiability by checking for common instances. Evaluated on the DBpedia ontology using open-source LLMs (Gemma 2, Llama 3, Mistral 0.3, Qwen 2), the methodology successfully identified over 170,000 disjointness axioms, demonstrating that LLMs can effectively support ontology enrichment when guided by appropriate prompt strategies.

## Method Summary
The methodology combines LLM-based classification with logical inference to identify disjointness axioms in ontologies. The process starts by generating all possible class pairs from the ontology, then applying logical inference rules to label pairs based on existing disjointness and subclass relationships. For remaining "unknown" pairs, an LLM is queried using carefully designed prompts. The LLM results are then propagated through logical inference to maintain consistency. The approach uses negative question-answering prompts (asking if an individual can belong to both classes) which outperform positive prompts, and finds that task description prompts work better than few-shot examples.

## Key Results
- The approach identified over 170,000 disjointness axioms in the DBpedia ontology
- Gemma 2 consistently outperformed other LLMs (Llama 3, Mistral 0.3, Qwen 2) across evaluation metrics
- Negative QA prompts (asking if an individual can belong to both classes) consistently outperformed positive QA prompts
- Logical inference successfully reduced LLM queries while maintaining consistency and enabling additional disjointness deductions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM background knowledge is sufficient to recognize class disjointness in ontologies.
- Mechanism: The LLM leverages semantic and linguistic knowledge from pre-training to classify pairs of classes as disjoint or not disjoint based on natural language understanding.
- Core assumption: Ontological class descriptions are recorded as or associated with terms in natural language, and LLMs possess wide linguistic and semantic working knowledge.
- Evidence anchors:
  - [abstract]: "Our approach aims at leveraging the implicit knowledge embedded in LLMs, using prompt engineering to elicit this knowledge for classifying ontological disjointness."
  - [section 3]: "Given that (i) ontological class descriptions are often recorded as (or associated with) terms in natural language and (ii) LLMs have been found to possess wide linguistic and semantic working knowledge, we aim to assess the potential of LLMs to decide on the question which classes ought to be disjoint"
  - [corpus]: Weak - corpus neighbors focus on ontology embeddings and benchmarks, not directly on disjointness learning via LLMs
- Break condition: If class names lack meaningful natural language labels or if the LLM's pre-training data lacks relevant semantic context for the domain.

### Mechanism 2
- Claim: Logical inference can reduce LLM interactions while maintaining consistency.
- Mechanism: Subclass relationships from the ontology are used to deduce additional disjointness statements, reducing the need for LLM queries and ensuring the resulting ontology remains satisfiable.
- Core assumption: The original knowledge base is consistent, and subclass relationships can be used to propagate disjointness information.
- Evidence anchors:
  - [section 4]: "Proposition 1... We exploit this property to use subclass relationships from ð’¦ to deduce class disjointness statements from existing class disjointness statements."
  - [section 4]: "Proposition 2... We exploited this property indirectly under the assumption that any named class ð¶ in the considered ontology is supposed to have instances"
  - [corpus]: Missing - corpus doesn't address logical inference for disjointness
- Break condition: If the original knowledge base contains inconsistencies or if subclass relationships are incomplete or incorrect.

### Mechanism 3
- Claim: Prompt engineering significantly impacts LLM performance for disjointness classification.
- Mechanism: Different prompting strategies (naive, task description, few-shot) and QA formats (positive vs negative) affect the LLM's ability to correctly classify disjointness.
- Core assumption: The LLM's response quality depends on how the question is framed and what examples are provided.
- Evidence anchors:
  - [section 5]: "Notably, the best prompting technique is not providing few-shot examples, but rather providing the LLM with little to no description of the task."
  - [section 5]: "Interestingly, framing the problem as a negative QA task â€“ i.e. asking whether an individual of a class can also be an instance of another class â€“ consistently outperforms the positive QA prompt."
  - [section 5]: "On average, Gemma 2 performs better than the other LLMs."
  - [corpus]: Weak - corpus neighbors focus on ontology embeddings, not prompt engineering strategies
- Break condition: If the LLM lacks sufficient context to understand the task regardless of prompt strategy.

## Foundational Learning

- Concept: Description Logic (DL) axioms and their semantics
  - Why needed here: Understanding disjointness axioms requires familiarity with DL foundations, as disjointness is a fundamental DL axiom type
  - Quick check question: What does it mean for two classes to be disjoint in a Description Logic ontology?

- Concept: Model-theoretic semantics for ontologies
  - Why needed here: The paper uses interpretations and models to define when disjointness holds across conceivable worlds
  - Quick check question: How does the paper define disjointness using the concept of interpretations and models?

- Concept: Automated reasoning and logical inference
  - Why needed here: The approach uses logical inference to propagate disjointness information and maintain consistency
  - Quick check question: What logical properties (Propositions 1-3) are used to deduce disjointness from subclass relationships?

## Architecture Onboarding

- Component map:
  Knowledge base (ontology) with class hierarchy -> LLM classification component -> Logical reasoning engine -> Enriched ontology with disjointness axioms

- Critical path:
  1. Generate list of all class pairs (C1, C2) where C1 is lexicographically smaller than C2
  2. Apply logical inference to label pairs based on existing disjointness and subclass relationships
  3. For remaining "unknown" pairs, query LLM with appropriate prompt
  4. Propagate LLM results through logical inference
  5. Prune redundant axioms to minimize output size

- Design tradeoffs:
  - Query efficiency vs. completeness: Using logical inference reduces LLM calls but requires reasoning engine overhead
  - Conservative vs. aggressive classification: Negative QA prompts tend to be more conservative in asserting disjointness
  - Model selection: Gemma 2 performs best on average but other models may be better for specific requirements

- Failure signatures:
  - Inconsistent ontology: Original knowledge base has contradictions that propagate through inference
  - High false negative rate: LLM fails to identify actual disjointness (recall issues)
  - High false positive rate: LLM incorrectly asserts disjointness (precision issues)
  - Excessive LLM queries: Logical inference fails to reduce query count as expected

- First 3 experiments:
  1. Run Algorithm 1 on DBpedia ontology to generate the initial list of class pairs and test logical inference alone
  2. Test different prompt strategies (naive, task description, few-shot) with Gemma 2 on a small subset of pairs
  3. Evaluate the complete Algorithm 2 pipeline with Gemma 2 and task description prompt on the full ontology

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of disjointness axioms generated by LLMs compare to those created by human experts in terms of accuracy and consistency?
- Basis in paper: [inferred] The paper demonstrates that LLMs can identify disjointness axioms but notes instances of misclassification, suggesting a need for comparison with human-generated axioms.
- Why unresolved: The paper does not provide a direct comparison between LLM-generated and human-created axioms, focusing instead on the potential and limitations of LLMs.
- What evidence would resolve it: A study comparing LLM-generated disjointness axioms with those created by human experts, evaluating both accuracy and consistency.

### Open Question 2
- Question: What impact does the size and complexity of an ontology have on the effectiveness of LLM-based disjointness axiom generation?
- Basis in paper: [explicit] The paper mentions using DBpedia ontology and suggests future work on testing other ontologies, implying that size and complexity may affect results.
- Why unresolved: The study is limited to the DBpedia ontology, and no analysis is provided on how different ontology sizes or complexities might influence LLM performance.
- What evidence would resolve it: Experiments testing LLM performance across ontologies of varying sizes and complexities, measuring accuracy and efficiency.

### Open Question 3
- Question: How can prompt engineering be optimized to improve the reliability of LLM-generated disjointness axioms, particularly in domain-specific contexts?
- Basis in paper: [explicit] The paper discusses the role of prompt engineering in improving LLM performance but notes that further research is needed, especially for domain-specific scenarios.
- Why unresolved: While the paper explores basic prompt strategies, it does not delve into advanced techniques or domain-specific optimizations.
- What evidence would resolve it: Development and testing of advanced prompt engineering techniques tailored to specific domains, assessing their impact on axiom reliability.

## Limitations

- The approach depends heavily on the quality of class names and descriptions in the ontology - ontologies with cryptic or domain-specific terminology may yield poorer LLM performance
- The assumption that all named classes should have instances may not hold for all ontologies, potentially affecting the consistency checks
- The study only evaluated on DBpedia ontology, which may not generalize to other ontologies with different characteristics

## Confidence

- High confidence in the core methodology combining LLM classification with logical inference for disjointness identification
- Medium confidence in the prompt engineering findings - while results show task description with negative QA works best for DBpedia, this may vary across domains and ontologies
- Low confidence in generalization to other ontologies without further validation on diverse knowledge bases

## Next Checks

1. Apply the methodology to additional ontologies (e.g., from BioPortal, Wikidata) to assess generalizability across different domains and naming conventions

2. Systematically analyze the specific class pairs where LLMs fail, particularly distinguishing between false positives (non-disjoint classes marked as disjoint) and false negatives (disjoint classes marked as non-disjoint)

3. Remove the logical inference component and measure the impact on LLM query count and classification accuracy to quantify the contribution of the reasoning engine to overall performance