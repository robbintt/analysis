---
ver: rpa2
title: Spatial and social situation-aware transformer-based trajectory prediction
  of autonomous systems
arxiv_id: '2406.02767'
source_url: https://arxiv.org/abs/2406.02767
tags:
- social
- trajectory
- prediction
- time
- tensor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a transformer-based trajectory prediction
  model that incorporates both spatial and social context for autonomous systems.
  The core method replaces LSTM-based social tensor processing with a transformer-based
  Social Tensor Transformer that considers interactions between target and surrounding
  agents at each time step.
---

# Spatial and social situation-aware transformer-based trajectory prediction of autonomous systems

## Quick Facts
- arXiv ID: 2406.02767
- Source URL: https://arxiv.org/abs/2406.02767
- Reference count: 20
- Primary result: Transformer-based model achieves 31.69 ± 40.58 meters FDE after 5 minutes prediction for inland vessel trajectories

## Executive Summary
This paper introduces a transformer-based trajectory prediction model for autonomous systems that incorporates both spatial and social context. The model uses a Social Tensor Transformer to directly model inter-agent relationships at each time step, replacing LSTM-based social tensor processing. Spatial awareness is achieved through navigation-area-specific dislocation features rather than separate map processing modules. The model is evaluated on inland vessel trajectory prediction, showing improved performance over context-agnostic and spatially-aware baselines, particularly for longer prediction horizons.

## Method Summary
The sosp-CT model uses AIS vessel navigation data to predict future trajectories by considering both social interactions with surrounding agents and spatial constraints of navigable areas. The model processes lateral and longitudinal dislocation features relative to navigable boundaries through a transformer-based architecture with a Social Tensor Transformer module that captures inter-agent relationships. The model is trained using weighted cross-entropy loss and evaluated using average and final displacement errors over 5-minute prediction horizons.

## Key Results
- sosp-CT model achieves FDE of 31.69 ± 40.58 meters after 5 minutes prediction
- Model outperforms context-agnostic (38.55 ± 46.8 meters) and spatially-aware (31.95 ± 44.32 meters) baselines
- Handles partially observed surrounding agents and demonstrates interpretable behavior in traffic situations
- Better performance for longer prediction horizons (>3 minutes) compared to context-agnostic approach

## Why This Works (Mechanism)

### Mechanism 1
The Social Tensor Transformer directly models inter-agent relationships at each time step rather than relying on LSTM hidden states. At each observation time step t, the model creates a 4D social tensor S (W × L × Tobs × 2) where each grid position contains the lateral and longitudinal distance change rates between surrounding agents and the target agent. This tensor is embedded, positionally encoded, and fused with the target agent's embedding through a transformer decoder's attention mechanism. The attention allows the model to weight surrounding agents based on their relevance to the target agent's state at that specific time step.

### Mechanism 2
Navigation area-specific dislocation features provide implicit spatial awareness without requiring additional map processing modules. The model defines lateral and longitudinal dislocation as changes in relative distance to the right fairway border and changes in waterway kilometers between subsequent time steps. This creates a target-agent-centric coordinate system where grid positions correspond to waterway kilometer distances from the target vessel and differences in relative fairway border distance. The model learns to predict trajectories within navigable boundaries by understanding these spatial constraints through the dislocation features alone.

### Mechanism 3
The transformer-based architecture provides computational efficiency and better handling of partially observed agents compared to LSTM-based approaches. By processing social information through a transformer rather than LSTM, the model can handle agents entering, re-entering, or disappearing from the scene without the modeling problems encountered with LSTM hidden states. The transformer's parallel processing capabilities and attention mechanism allow for more efficient computation compared to sequential LSTM processing.

## Foundational Learning

- Concept: Transformer attention mechanisms
  - Why needed here: The model relies on transformer attention to fuse social context with target agent features and to predict future trajectories based on historical observations
  - Quick check question: Can you explain how multi-head attention in transformers differs from simple weighted averaging, and why this matters for trajectory prediction?

- Concept: Social tensor construction for multi-agent systems
  - Why needed here: Understanding how to represent surrounding agents' influence on a target agent is crucial for implementing the Social Tensor Transformer
  - Quick check question: How does the 4D social tensor (W × L × Tobs × 2) capture the relationship between surrounding agents and the target agent at each time step?

- Concept: Coordinate system transformations for spatial awareness
  - Why needed here: The model uses navigation area-specific coordinates rather than global reference systems, requiring understanding of how to transform between different coordinate frames
  - Quick check question: How would you convert global UTM coordinates to the navigation area-specific coordinate system used in this model?

## Architecture Onboarding

- Component map: Input embeddings -> Social Tensor Transformer fusion -> Transformer encoding -> Trajectory prediction
- Critical path: Input embeddings → Social Tensor Transformer fusion → Transformer encoding → Trajectory prediction
- Design tradeoffs: Uses transformer-based social processing instead of LSTM for computational efficiency, but requires careful tuning of attention mechanisms. Avoids explicit map processing by using navigation area-relative coordinates, but may miss some spatial details
- Failure signatures: Poor performance on short prediction horizons (less than 3 minutes), high variance in error distribution, inability to predict correct reactions to traffic situations while maintaining individual agent behaviors
- First 3 experiments:
  1. Compare performance with and without Social Tensor Transformer to validate social awareness contribution
  2. Test different time resolutions (30s, 60s, 90s) to find optimal input granularity
  3. Evaluate model behavior on edge cases where surrounding agents enter or leave the scene to test partial observability handling

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but the following areas remain unexplored:

### Open Question 1
How does the proposed transformer-based social tensor definition and processing compare in computational efficiency to LSTM-based social tensor approaches across different sequence lengths and agent densities?

### Open Question 2
What is the performance impact of using navigation-area-specific dislocation features versus global coordinate-based features across different waterway configurations and vessel types?

### Open Question 3
How does the proposed model's performance degrade with varying levels of partial observability of surrounding agents, and what is the minimum observation requirement for maintaining acceptable prediction accuracy?

## Limitations
- Limited validation to inland vessel navigation on the Rhine, may not generalize to other autonomous systems
- Computational efficiency claims lack direct empirical validation against LSTM-based approaches
- No systematic evaluation of performance degradation under varying levels of partial observability
- Limited comparison to alternative approaches for capturing spatial and social information

## Confidence

- High confidence: The technical implementation details of the transformer architecture and loss function are well-specified
- Medium confidence: The performance improvements over baseline models are demonstrated but could benefit from additional ablation studies
- Low confidence: Claims about computational efficiency advantages over LSTM-based approaches are not empirically validated

## Next Checks

1. Implement and compare the Social Tensor Transformer against an LSTM-based social tensor approach on the same dataset to verify computational efficiency claims
2. Test the model on an autonomous vehicle dataset to evaluate generalization across different autonomous systems
3. Conduct ablation studies to quantify the individual contributions of social awareness, spatial awareness, and transformer architecture to overall performance