---
ver: rpa2
title: Reinforcing Competitive Multi-Agents for Playing 'So Long Sucker'
arxiv_id: '2411.11057'
source_url: https://arxiv.org/abs/2411.11057
tags:
- player
- game
- learning
- agents
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces So Long Sucker (SLS) as a novel multi-agent
  reinforcement learning (MARL) benchmark emphasizing coalition formation, strategic
  deception, and dynamic elimination. A computational framework with a graphical user
  interface and benchmarking tools is provided.
---

# Reinforcing Competitive Multi-Agents for Playing 'So Long Sucker'

## Quick Facts
- arXiv ID: 2411.11057
- Source URL: https://arxiv.org/abs/2411.11057
- Reference count: 10
- Agents achieve approximately half the maximum possible reward (~100-120) and outperform random baselines, but require long training (~2000 episodes) and still occasionally commit illegal moves.

## Executive Summary
This paper introduces So Long Sucker (SLS) as a novel multi-agent reinforcement learning benchmark emphasizing coalition formation, strategic deception, and dynamic elimination. A computational framework with a graphical user interface and benchmarking tools is provided. Classical deep reinforcement learning methods—DQN, DDQN, and Dueling DQN—are trained in a centralized cumulative learning setup with shared replay buffers. Agents achieve approximately half the maximum possible reward (~100-120) and outperform random baselines, but require long training (~2000 episodes) and still occasionally commit illegal moves. DDQN shows the most stable convergence. The results establish SLS as a negotiation-aware testbed for MARL and highlight the need for advanced architectures to capture its complex coalition and betrayal dynamics.

## Method Summary
The method trains DQN, DDQN, and Dueling DQN agents to play SLS using centralized cumulative learning with shared replay buffers. The SLS environment encodes board state, player chips, eliminated players, current player, game phase, and step count into a fixed-length vector. Agents select from 10 possible actions with dynamic masking to prevent illegal moves. Zero-sum reward shaping provides clearer feedback. Training runs for 10000 episodes with Adam optimizer (learning rate 0.001), batch size 64, discount factor 0.95, and epsilon-greedy exploration.

## Key Results
- Trained agents achieve ~100-120 reward points out of maximum 200
- Agents outperform random baselines but require ~2000 episodes to reach reasonable performance
- DDQN demonstrates most stable convergence among tested architectures
- Agents still commit occasional illegal moves despite action masking

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shared replay buffer and centralized learning improve sample efficiency in SLS.
- Mechanism: By sharing transitions among all four player agents, the replay buffer contains a richer, more diverse set of experiences from different perspectives of the same game, enabling faster learning and better coverage of the state space.
- Core assumption: All agents are learning the same policy and can benefit equally from each other's experiences without harming individual strategy specialization.
- Evidence anchors:
  - [abstract] "Using classical deep reinforcement learning methods (e.g., DQN, DDQN, and Dueling DQN), we train self-playing agents to learn the rules and basic strategies of SLS."
  - [section] "We employ a centralized cumulative learning setup [10], where all four player agents share a common learning network, such as DQN, and update a shared replay buffer."
  - [corpus] Weak/no direct evidence in neighbors about centralized replay buffers in multi-agent SLS-like games.
- Break condition: If agents require distinct policies or if strategic deception demands asymmetric learning, shared experience could slow specialization or leak strategic information.

### Mechanism 2
- Claim: Dynamic action masking preserves rule compliance while maintaining fixed action space size.
- Mechanism: Invalid actions are masked at each decision phase based on game state, preventing illegal moves during training and inference while keeping the neural network output layer constant (10 actions).
- Core assumption: The masking logic is correctly implemented and consistently synchronized with the current game phase and state.
- Evidence anchors:
  - [section] "Actions A0–A5 correspond to pile selection and are valid only during the choose pile phase... Actions A6–A9 correspond to player/color-based decisions... At each phase, only a subset of actions is legal, and invalid actions are masked to maintain consistency during training."
  - [abstract] "they require long training horizons (~2000 games) and still commit occasional illegal moves, highlighting both the promise and limitations of classical reinforcement learning."
  - [corpus] No direct evidence in neighbors about action masking in multi-agent games with coalition dynamics.
- Break condition: Masking errors or desynchronization between phase detection and action availability can lead to agents selecting masked actions, resulting in illegal moves.

### Mechanism 3
- Claim: Zero-sum reward shaping accelerates convergence by providing clearer feedback.
- Mechanism: Instead of ambiguous intermediate scores, the zero-sum {0,0,0,ù} payoff structure gives immediate, unambiguous signals tied to elimination order, helping agents distinguish beneficial from detrimental actions.
- Core assumption: The reward signal is dense enough to guide learning despite the sparse nature of terminal outcomes in SLS.
- Evidence anchors:
  - [section] "For reinforcement learning, however, we introduce a Zero-Sum Generalized Hofstra’s version, where the payoff structure is modified to {0,0,0,ù} with ù∈N +. This zero-sum formulation eliminates ambiguity in intermediate scores, providing a clearer reward signal and enabling faster convergence of baseline DRL agents."
  - [abstract] "they require long training horizons (~2000 games)... highlighting both the promise and limitations of classical reinforcement learning."
  - [corpus] No direct evidence in neighbors about zero-sum reward shaping in coalition-aware MARL benchmarks.
- Break condition: If the reward signal remains too sparse relative to the game's long horizons and complex coalition dynamics, learning may still be slow or unstable.

## Foundational Learning

- Concept: Markov Decision Process (MDP) formulation for MARL.
  - Why needed here: SLS is modeled as an MDP to formalize state transitions, actions, and rewards for reinforcement learning algorithms.
  - Quick check question: What are the five components of an MDP, and how does SLS map to each?

- Concept: Deep Q-Network (DQN) and its variants (DDQN, Dueling DQN).
  - Why needed here: These value-based methods are used as baseline agents to learn policies in SLS, with DDQN showing most stable convergence.
  - Quick check question: How does DDQN address Q-value overestimation compared to standard DQN?

- Concept: Centralized learning with decentralized execution.
  - Why needed here: Agents share a learning network and replay buffer but act independently during gameplay, improving sample efficiency while maintaining individual decision-making.
  - Quick check question: What are the benefits and risks of using a shared replay buffer in multi-agent reinforcement learning?

## Architecture Onboarding

- Component map:
  - SLS game environment (state, action, reward interfaces) -> Centralized Q-network (DQN/DDQN/Dueling DQN architecture) -> Shared replay buffer -> Action masking logic -> Training loop with cumulative updates -> GUI for visualization and benchmarking

- Critical path:
  1. Game state -> State representation vector
  2. State -> Q-network -> Q-values for 10 actions
  3. Action masking -> Legal action selection
  4. Action -> Game environment -> Next state, reward
  5. (State, Action, Reward, Next State) -> Shared replay buffer
  6. Replay buffer -> Batch sampling -> Network update

- Design tradeoffs:
  - Fixed 10-action space with masking vs. variable action space per phase
  - Centralized learning vs. independent agent learning
  - Zero-sum reward vs. progressive scoring
  - Simplified sequential variant vs. full negotiation-enabled SLS

- Failure signatures:
  - Persistent illegal moves despite masking (masking logic bug)
  - Slow or unstable learning (poor reward shaping or insufficient exploration)
  - Overfitting to specific agent strategies (insufficient diversity in replay buffer)
  - High variance in rewards (inadequate exploration or unstable updates)

- First 3 experiments:
  1. Run random agent baseline to establish performance floor and validate environment mechanics.
  2. Train DQN agent with fixed hyperparameters to verify learning curve and rule compliance.
  3. Compare DDQN vs. Dueling DQN on reward convergence and stability to identify best baseline architecture.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different reward shaping strategies (e.g., dynamic decay, shaping via coalition formation, or shaping via elimination speed) affect the learning efficiency and strategic depth of DRL agents in SLS?
- Basis in paper: [inferred] The paper notes that the current reward function is dynamic but fixed, and agents still exhibit suboptimal strategies and occasional illegal moves, suggesting that reward shaping could be a key factor.
- Why unresolved: The study uses a single reward shaping method without comparing it to alternatives, so the impact of different shaping strategies on strategic learning remains unknown.
- What evidence would resolve it: Comparative experiments training agents under multiple reward shaping functions and measuring both convergence speed and qualitative gameplay differences.

### Open Question 2
- Question: Can multi-agent reinforcement learning agents be trained to explicitly model and exploit coalition formation and betrayal opportunities in SLS, rather than merely learning rule compliance and reactive strategies?
- Basis in paper: [explicit] The authors explicitly state that agents achieve "roughly half of the maximum attainable reward" and "struggle with long-horizon reasoning and coalition dynamics," indicating a gap in coalition-aware strategic reasoning.
- Why unresolved: Current DRL baselines (DQN, DDQN, Dueling DQN) are value-based and do not inherently model social or coalition structures, limiting strategic depth.
- What evidence would resolve it: Training agents with architectures designed to model coalition dynamics (e.g., graph neural networks or multi-agent attention mechanisms) and evaluating whether they can outperform current baselines in coalition-based metrics.

### Open Question 3
- Question: Does incorporating game-theoretic reasoning modules (e.g., minimax or equilibrium computation) alongside deep reinforcement learning improve agent performance in SLS compared to purely DRL approaches?
- Basis in paper: [explicit] The authors propose SLS as a benchmark for future work integrating "game-theoretic reasoning" and "coalition-aware strategies" with DRL, implying current methods lack such integration.
- Why unresolved: The study only benchmarks classical DRL algorithms without hybrid architectures, leaving the potential benefits of game-theoretic integration unexplored.
- What evidence would resolve it: Empirical comparison between hybrid agents (DRL + game-theoretic reasoning) and baseline DRL agents in terms of reward, legality compliance, and strategic sophistication.

## Limitations
- Centralized replay buffer may not support asymmetric learning needed for strategic deception
- Action masking effectiveness is limited, with agents still committing occasional illegal moves
- Zero-sum reward formulation may oversimplify coalition dynamics and miss intermediate strategic signals

## Confidence
- **High confidence**: Basic framework implementation, baseline performance metrics, and fundamental architectural choices
- **Medium confidence**: Claims about shared replay buffer benefits and zero-sum reward effectiveness
- **Low confidence**: Generalization claims about SLS as a coalition-aware MARL benchmark

## Next Checks
1. **Ablation study on replay buffer sharing**: Train identical agents with separate vs. shared replay buffers under identical conditions to quantify the actual benefit of centralized experience storage
2. **Extended illegal move analysis**: Instrument the training loop to log every illegal move attempt with state context to identify systematic masking failures or phase detection errors
3. **Reward shaping sensitivity analysis**: Systematically vary the zero-sum reward parameters (ù value, α decay rate) and measure impact on convergence speed and final performance to establish robust hyperparameter ranges