---
ver: rpa2
title: Brain-Like Replay Naturally Emerges in Reinforcement Learning Agents
arxiv_id: '2402.01467'
source_url: https://arxiv.org/abs/2402.01467
tags:
- replay
- reward
- figure
- learning
- during
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper develops a modular reinforcement learning agent with
  a hippocampus-like (HF) and prefrontal cortex-like (PFC) component that naturally
  generates offline replay during rest periods. The two key conditions are: (1) replay
  serves reward maximization, and (2) replay occurs via communication between HF and
  PFC.'
---

# Brain-Like Replay Naturally Emerges in Reinforcement Learning Agents

## Quick Facts
- arXiv ID: 2402.01467
- Source URL: https://arxiv.org/abs/2402.01467
- Reference count: 14
- A modular RL agent with hippocampus-like and prefrontal cortex-like components naturally generates offline replay during rest periods, reproducing biological replay patterns observed in rodents.

## Executive Summary
This paper presents a modular reinforcement learning agent that naturally generates offline replay during rest periods, mirroring biological patterns observed in rodents. The agent consists of hippocampus-like (HF) and prefrontal cortex-like (PFC) components that communicate only during rest when rewards are received. The model successfully navigates a dynamic reward environment and reproduces key biological findings about how replay distribution changes over learning, shifting from exploring plausible paths to finding optimal paths.

## Method Summary
The model uses a modular architecture with separate HF and PFC components. The HF employs a GRU to predict next locations and remember recent rewards, while the PFC uses a GRU to estimate values and select actions. An information passage between HF and PFC activates only during rest periods when rewards are received. The HF and Encoder are pre-trained on random trajectories and frozen, while the PFC is trained using PPO to maximize reward. The model is tested in a 5×5 grid world where a checkpoint reward location dynamically changes, and replay patterns are analyzed over time.

## Key Results
- The model reproduces biological replay patterns, with replay distribution shifting from initial focus on finding plausible paths to finding optimal paths
- Ablation studies show replay significantly improves exploration efficiency compared to models without replay
- Replay contains information about context and action plans, and manifold analysis reveals replay bridges between different context states during learning

## Why This Works (Mechanism)
The replay emerges naturally because it serves the dual purpose of reward maximization while being constrained by the communication structure between HF and PFC. The HF's role in location prediction and reward memory, combined with the PFC's value estimation and action selection, creates a functional need for offline replay during rest periods. The information passage that activates only during rewards ensures replay is temporally gated and contextually relevant.

## Foundational Learning
- **Modular RL architecture**: Separates spatial memory (HF) from decision-making (PFC) to mirror brain organization. Needed for biological plausibility and functional specialization. Quick check: Verify modules can be independently ablated without complete system failure.
- **Offline replay generation**: Occurs during rest periods when rewards are received, not during active navigation. Needed to conserve computational resources and focus on relevant experiences. Quick check: Confirm replay only happens after reward events, not during exploration.
- **Information gating**: HF-to-PFC communication only opens during rest periods. Needed to create temporal separation between experience and consolidation. Quick check: Verify no information flow during active navigation phases.

## Architecture Onboarding
- **Component map**: Encoder -> HF (GRU) <-> PFC (GRU) with gated information passage
- **Critical path**: Visual input → Encoder → HF (location prediction) → gated passage → PFC (value estimation/action selection) → environment action
- **Design tradeoffs**: Separating HF and PFC enables biological realism but requires careful synchronization; gated communication prevents interference but may limit real-time adaptation
- **Failure signatures**: No biologically plausible replay sequences; ablation experiments fail to show replay's functional importance; manifold analysis doesn't reveal context bridging
- **3 first experiments**: 1) Verify information passage activates only during rest periods with rewards, 2) Test replay distribution changes match biological observations after reward relocation, 3) Compare multi-step vs single-step information exchange efficiency

## Open Questions the Paper Calls Out
None

## Limitations
- Missing critical implementation details including exact network architectures, hyperparameters, and precise information passage mechanism details
- Metrics for measuring exploration efficiency and replay content analysis could be interpreted differently
- Biological plausibility claims depend on accurate reproduction of replay patterns requiring precise timing and content control

## Confidence
- Modular architecture design: High
- Biological plausibility of replay patterns: Medium
- Ablation study results: Medium
- Manifold analysis interpretation: Low

## Next Checks
1. Implement the information passage mechanism and verify it activates only during rest periods when rewards are received, with no communication during active navigation
2. Reproduce the replay distribution analysis by tracking the frequency of replayed states over time after reward relocation, ensuring the shift from original path to shortcut replays matches biological observations
3. Run ablation experiments comparing models with different information exchange frequencies (single-step vs multi-step) and verify the multi-step variant shows superior exploration efficiency as claimed