---
ver: rpa2
title: On Vision Transformers for Classification Tasks in Side-Scan Sonar Imagery
arxiv_id: '2409.12026'
source_url: https://arxiv.org/abs/2409.12026
tags:
- classification
- vits
- performance
- vision
- imagery
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Vision Transformers (ViTs) outperform traditional CNNs for binary
  classification of man-made objects in side-scan sonar imagery, achieving superior
  performance across f1-score, precision, recall, and accuracy metrics. The study
  rigorously compares ViTs (ViT and SwinViT) with popular CNN architectures (ResNet
  and ConvNext) on a balanced dataset of diverse seafloor types.
---

# On Vision Transformers for Classification Tasks in Side-Scan Sonar Imagery

## Quick Facts
- arXiv ID: 2409.12026
- Source URL: https://arxiv.org/abs/2409.12026
- Authors: BW Sheffield; Jeffrey Ellen; Ben Whitmore
- Reference count: 14
- Vision Transformers outperform traditional CNNs for binary classification of man-made objects in side-scan sonar imagery across multiple performance metrics

## Executive Summary
This paper investigates the application of Vision Transformers (ViTs) for binary classification of man-made objects in side-scan sonar (SSS) imagery. The study compares ViTs (including ViT and SwinViT) against traditional CNN architectures (ResNet and ConvNext) on a balanced dataset of diverse seafloor types. Results demonstrate that ViTs achieve superior performance across f1-score, precision, recall, and accuracy metrics, primarily due to their self-attention mechanisms that capture global context more effectively than CNNs' local receptive fields. However, this performance advantage comes at the cost of significantly higher computational resources, making CNNs more suitable for deployment in resource-constrained underwater vehicles.

## Method Summary
The study evaluates multiple deep learning architectures on a balanced dataset of dual-channel (low/high frequency) SSS snippets resized to 224x224. Four model families are compared: ViT, SwinViT, ResNet, and ConvNext. All models are pretrained on ImageNet-1K and fine-tuned using AdamW optimizer with binary cross entropy loss. The training procedure employs a learning rate of 0.00008, early stopping with patience 20, and data augmentation including horizontal flip, ±5° rotation, affine transformations, and zoom. Performance is assessed using f1-score, precision, recall, and accuracy metrics, while computational efficiency is measured through inference time, throughput, FLOPS, parameter count, and model size on NVIDIA A6000 GPU.

## Key Results
- ViTs achieve superior classification performance across all four metrics (f1-score, precision, recall, accuracy) compared to CNNs
- SwinViT architecture demonstrates the best overall performance among all tested models
- CNNs maintain advantage in computational efficiency with faster inference, lower memory usage, and fewer parameters
- Self-attention mechanisms in ViTs help reduce false positives from complex seafloor textures like rocks and ripple sand

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ViTs capture global context more effectively than CNNs in SSS imagery.
- Mechanism: ViTs process images as sequences of patches and use self-attention to capture relationships between all patches simultaneously, unlike CNNs that rely on local receptive fields.
- Core assumption: Global contextual understanding is crucial for distinguishing man-made objects from complex seafloor textures.
- Evidence anchors:
  - [abstract] "ViTs leverage the power of self-attention mechanisms to process images as sequences of patches, allowing for a more flexible and comprehensive understanding of the spatial hierarchies within the data."
  - [section] "Unlike CNNs, which primarily focus on local features due to their convolutional nature, ViTs can process entire image patches at once, allowing them to better understand the broader context of a scene."
- Break condition: If seafloor textures are uniform and man-made objects are distinctly local features, CNNs may perform equally well without global context.

### Mechanism 2
- Claim: CNNs have built-in inductive biases that make them more computationally efficient than ViTs.
- Mechanism: CNNs incorporate local connectivity, weight sharing, hierarchical processing, and spatial invariance as design choices, reducing the number of parameters needed to learn patterns.
- Core assumption: Inductive biases encode prior knowledge about image structure that reduces learning requirements.
- Evidence anchors:
  - [section] "It is widely accepted that ViTs require more training data than CNNs to obtain good predictive performance. The requirement of larger training data for ViTs distill down to what are called inductive biases which encode assumptions or prior knowledge into the design of each machine learning algorithm."
  - [section] "Because CNNs have more inductive biases than ViTs, less training data is required to learn patterns or features in an input image."
- Break condition: If training data is abundant and computational resources are unlimited, the efficiency advantage of CNNs diminishes.

### Mechanism 3
- Claim: Self-attention mechanisms in ViTs help reduce false positives from seafloor textures like rocks and ripple sand.
- Mechanism: The self-attention mechanism weighs the importance of different image regions, allowing the model to focus on man-made object features while down-weighting confusing seafloor textures.
- Core assumption: False positives in SSS imagery are primarily caused by confusing seafloor textures that resemble man-made objects.
- Evidence anchors:
  - [abstract] "The self-attention mechanism in ViTs may be of benefit for particular classification scenarios where seafloor bottom types such as rocky and ripple sand negatively impact CNNs in reporting false alarms when searching for man-made objects."
  - [section] "ViTs can process entire image patches at once, allowing them to better understand the broader context of a scene."
- Break condition: If seafloor textures are well-separated from man-made objects in feature space, the attention mechanism provides minimal benefit.

## Foundational Learning

- Concept: Self-attention mechanism
  - Why needed here: Understanding how ViTs weigh different image regions is crucial for interpreting their superior performance on complex seafloor textures.
  - Quick check question: How does self-attention differ from convolution in terms of information flow between image patches?

- Concept: Inductive biases in neural networks
  - Why needed here: Recognizing why CNNs are more computationally efficient helps in making informed architecture choices for resource-constrained underwater vehicles.
  - Quick check question: What are the key inductive biases built into CNN architectures and how do they reduce learning requirements?

- Concept: Binary classification metrics (precision, recall, F1-score)
  - Why needed here: Understanding these metrics is essential for interpreting the performance results and their implications for underwater object detection.
  - Quick check question: In a balanced dataset, how do precision and recall relate to each other when F1-score is maximized?

## Architecture Onboarding

- Component map:
  Input layer -> Patch embedding and positional encoding -> Transformer encoder layers -> MLP classification head -> Output

- Critical path:
  1. Load and preprocess dual-channel SSS imagery
  2. Apply patch embedding and positional encoding
  3. Process through transformer encoder layers
  4. Apply classification head
  5. Output binary classification result

- Design tradeoffs:
  - Higher parameter count in ViTs vs. built-in inductive biases in CNNs
  - Better global context understanding vs. slower inference speed
  - More training data required vs. superior performance on complex textures

- Failure signatures:
  - High false positives on complex seafloor textures (indicates attention mechanism not learning properly)
  - Poor generalization to new seafloor types (indicates insufficient training data or overfitting)
  - Slow inference speed on embedded systems (indicates computational resource constraints)

- First 3 experiments:
  1. Compare ViT performance on simple vs. complex seafloor textures to validate attention mechanism benefits
  2. Test computational efficiency on target embedded hardware to verify deployment feasibility
  3. Evaluate performance with reduced training data to quantify data requirements compared to CNNs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Vision Transformers degrade when trained on smaller datasets compared to CNNs?
- Basis in paper: [inferred] The paper mentions that ViTs require more training data than CNNs due to their fewer inductive biases, suggesting a potential performance gap with smaller datasets.
- Why unresolved: The paper does not provide experimental results comparing ViT performance on small vs. large datasets.
- What evidence would resolve it: Controlled experiments training ViTs and CNNs on datasets of varying sizes and measuring classification performance metrics.

### Open Question 2
- Question: What specific self-supervised learning techniques would be most effective for ViTs in SSS imagery?
- Basis in paper: [explicit] The paper explicitly suggests exploring self-supervised learning for ViTs as a future research direction.
- Why unresolved: The paper does not specify which self-supervised learning methods should be tested or how they would improve performance.
- What evidence would resolve it: Experiments applying different self-supervised learning methods (e.g., contrastive learning, masked image modeling) to ViTs for SSS tasks and comparing results to supervised learning baselines.

### Open Question 3
- Question: How can model optimization techniques like quantization and pruning affect the trade-off between ViT classification performance and computational efficiency?
- Basis in paper: [inferred] The paper mentions potential future research into model optimization techniques to examine performance vs. efficiency trade-offs.
- Why unresolved: The paper does not provide experimental results showing how quantization or pruning affects ViT performance and efficiency.
- What evidence would resolve it: Experiments applying quantization and pruning to ViT models and measuring changes in classification accuracy, inference speed, and memory usage.

## Limitations
- Limited generalizability to underwater environments not represented in the dataset
- Computational efficiency trade-offs may not be acceptable for real-time deployment on resource-constrained AUVs
- Potential overfitting to specific seafloor types in the training data

## Confidence
- **High Confidence**: ViTs outperform CNNs on binary classification metrics (f1-score, precision, recall, accuracy) for man-made object detection in SSS imagery
- **Medium Confidence**: Self-attention mechanisms specifically reduce false positives from complex seafloor textures
- **Medium Confidence**: Computational efficiency trade-offs make CNNs more suitable for embedded deployment despite lower classification performance

## Next Checks
1. Test model performance on independent SSS datasets from different geographic regions to validate generalization claims
2. Benchmark inference speed and memory usage on target embedded hardware to verify deployment feasibility claims
3. Conduct ablation studies with reduced training data to quantify the data efficiency advantage of CNNs vs ViTs