---
ver: rpa2
title: A Study of Optimizations for Fine-tuning Large Language Models
arxiv_id: '2406.02290'
source_url: https://arxiv.org/abs/2406.02290
tags:
- memory
- fine-tuning
- optimizations
- runtime
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a comprehensive study on fine-tuning optimizations\
  \ for large language models (LLMs), focusing on balancing memory usage and runtime\
  \ efficiency. The authors evaluate four key techniques\u2014Gradient Checkpointing,\
  \ Low-Rank Adaptation (LoRA), DeepSpeed\u2019s ZeRO, and FlashAttention\u2014and\
  \ analyze their impact under different scenarios, including fine-tuning very large\
  \ models, long context lengths, and resource-constrained environments."
---

# A Study of Optimizations for Fine-tuning Large Language Models

## Quick Facts
- arXiv ID: 2406.02290
- Source URL: https://arxiv.org/abs/2406.02290
- Reference count: 5
- One-line primary result: ZeRO-2 combined with LoRA offers an optimal default configuration for most LLM fine-tuning scenarios

## Executive Summary
This paper presents a comprehensive study on fine-tuning optimizations for large language models (LLMs), focusing on balancing memory usage and runtime efficiency. The authors evaluate four key techniques—Gradient Checkpointing, Low-Rank Adaptation (LoRA), DeepSpeed's ZeRO, and FlashAttention—and analyze their impact under different scenarios, including fine-tuning very large models, long context lengths, and resource-constrained environments. The primary finding is that ZeRO-2 combined with LoRA offers an optimal default configuration for most use cases, achieving a strong balance between memory efficiency and runtime performance. For extremely large models (e.g., tens of billions of parameters), ZeRO-3 with LoRA and Gradient Checkpointing is essential to avoid out-of-memory errors. FlashAttention-2 significantly reduces memory usage and runtime for long context lengths on compatible GPUs. The study provides practical guidance for selecting fine-tuning optimizations tailored to specific model sizes, context lengths, and hardware constraints.

## Method Summary
The authors evaluate fine-tuning optimizations using Hugging Face Transformers, DeepSpeed, and FlashAttention-2 on Llama 2 (7B, 13B, 70B), Falcon (180B), and the Samsum dataset. The training setup uses AdamW optimizer with mixed-precision, linear scheduler, learning rate 4e-4, sequence length 256 (with context length experiments up to 4096), effective batch size 8, LoRA rank 64, alpha 32, and CPU offloading of optimizer states. The evaluation systematically compares memory usage (peak allocated) and fine-tuning runtime across different optimization combinations on 8xA100 and 8xV100 GPUs.

## Key Results
- ZeRO-2 combined with LoRA offers an optimal default configuration for most use cases, achieving a strong balance between memory efficiency and runtime performance
- For extremely large models (e.g., tens of billions of parameters), ZeRO-3 with LoRA and Gradient Checkpointing is essential to avoid out-of-memory errors
- FlashAttention-2 significantly reduces memory usage and runtime for long context lengths on compatible GPUs (A100 and newer)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ZeRO-2 combined with LoRA offers an optimal default configuration for balancing memory and runtime.
- Mechanism: ZeRO-2 partitions model states (parameters, gradients, optimizer states) across data-parallel processes, reducing memory usage by 2-4x compared to no optimization. LoRA reduces trainable parameters by orders of magnitude (e.g., 131 million vs 70 billion for Llama 2 70B), further lowering memory and computation.
- Core assumption: The model states and activation memory dominate GPU memory usage, and partitioning/sharding these effectively reduces peak memory without excessive runtime overhead.
- Evidence anchors:
  - [abstract] "ZeRO-2 combined with LoRA offers an optimal default configuration for most use cases, achieving a strong balance between memory efficiency and runtime performance."
  - [section] "The overall best combination of optimizations from the standpoint of both runtime and memory is configuration (c) i.e. ZeRO-DP + LoRA."
  - [corpus] Weak corpus signal: Only one neighbor paper directly addresses LoRA optimization (LoRA-SP: Streamlined Partial Parameter Adaptation).
- Break condition: If model states are not the dominant memory component (e.g., if temporary buffers or fragmentation are significant), or if partitioning overhead becomes prohibitive.

### Mechanism 2
- Claim: FlashAttention-2 significantly reduces memory usage and runtime for long context lengths on compatible GPUs.
- Mechanism: FlashAttention-2 optimizes attention computation using tiling and recomputation techniques, reducing memory from quadratic to linear scaling with sequence length and minimizing reads/writes between SRAM and HBM.
- Core assumption: Attention calculations form a significant memory bottleneck for long sequences, and optimizing them yields substantial gains.
- Evidence anchors:
  - [abstract] "FlashAttention-2 significantly reduces memory usage and runtime for long context lengths on compatible GPUs."
  - [section] "FlashAttention helps achieve attention calculations in linear instead of quadratic complexity with respect to the sequence/context length."
  - [corpus] Weak corpus signal: No neighbor papers directly address FlashAttention or attention optimization.
- Break condition: If context lengths are short (e.g., < 1024 tokens), or if the GPU architecture does not support FlashAttention-2 optimizations.

### Mechanism 3
- Claim: Gradient Checkpointing allows fine-tuning much larger models with modest runtime increase.
- Mechanism: Gradient Checkpointing saves only a subset of activations during forward pass and recomputes them during backward pass, reducing memory from linear to square root of network depth.
- Core assumption: The memory required to store all activations during forward pass is a major bottleneck, and recomputation is cheaper than storing all activations.
- Evidence anchors:
  - [section] "Gradient Checkpointing (GC) (Chen et al., 2016) makes judicious use of GPU memory by not preserving all the activations computed during the forward pass (FP) of a Deep Neural Net (DNN)."
  - [section] "The most memory efficient strategy saves checkpoints every √n steps, where n is number of layers (depth) of the DNN."
  - [corpus] Weak corpus signal: No neighbor papers directly address Gradient Checkpointing.
- Break condition: If recomputation cost outweighs memory savings (e.g., very deep models with expensive layers), or if activation memory is not the dominant factor.

## Foundational Learning

- Concept: GPU Memory Hierarchy and Tensor Layouts
  - Why needed here: Understanding how model states, activations, and optimizer states map to GPU memory (HBM, SRAM) is crucial for reasoning about optimization effectiveness.
  - Quick check question: Why does mixed-precision training reduce memory usage compared to full-precision?

- Concept: Data and Model Parallelism Trade-offs
  - Why needed here: DeepSpeed's ZeRO leverages both data and model parallelism; understanding their differences and when to apply each is key to selecting the right stage.
  - Quick check question: What is the primary difference between ZeRO-1 and ZeRO-2 in terms of model state partitioning?

- Concept: Low-Rank Matrix Approximation
  - Why needed here: LoRA relies on low-rank decomposition of weight updates; understanding this concept explains why it drastically reduces trainable parameters.
  - Quick check question: How does the rank parameter in LoRA affect the number of trainable parameters and model quality?

## Architecture Onboarding

- Component map: Forward pass with checkpointing -> Backward pass with recomputation -> Parameter update with ZeRO sharding
- Critical path: Forward pass with checkpointing → Backward pass with recomputation → Parameter update with ZeRO sharding
- Design tradeoffs:
  - Memory vs. Runtime: Checkpointing and ZeRO stages trade memory for recomputation or communication overhead
  - Model Quality vs. Efficiency: LoRA reduces parameters but may slightly impact fine-tuning quality
  - Hardware Compatibility: FlashAttention-2 only on A100s, not V100s
- Failure signatures:
  - Out-of-memory errors: Likely due to insufficient ZeRO stage, missing LoRA, or excessive context length without FlashAttention-2
  - Slow runtime: Possible over-optimization (e.g., ZeRO-3 without need), missing FlashAttention-2 on long sequences
  - Poor model quality: LoRA rank too low, insufficient fine-tuning data
- First 3 experiments:
  1. Fine-tune Llama 2 7B with ZeRO-2 + LoRA on 8xA100s: Verify memory/runtime balance and baseline performance.
  2. Enable FlashAttention-2 and increase context length to 4096: Confirm memory/runtime reduction for long sequences.
  3. Test Gradient Checkpointing with Llama 2 13B on 8xV100s: Validate ability to handle larger models with limited memory.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Gradient Checkpointing vary with different checkpointing strategies (e.g., periodic vs. sqrt(n)) for models with non-standard architectures or varying layer depths?
- Basis in paper: [explicit] The paper mentions that the most memory-efficient strategy saves checkpoints every √n steps but does not empirically compare different strategies or their impact on non-standard architectures.
- Why unresolved: The study focuses on standard transformer architectures and does not explore alternative checkpointing strategies or their effectiveness on models with varying layer depths or non-standard designs.
- What evidence would resolve it: Empirical results comparing different checkpointing strategies (e.g., periodic vs. sqrt(n)) on models with varying layer depths and non-standard architectures, along with their impact on memory usage and runtime.

### Open Question 2
- Question: What is the impact of combining Low-Rank Adaptation (LoRA) with other parameter-efficient fine-tuning methods (e.g., prefix tuning or prompt tuning) on memory usage and runtime?
- Basis in paper: [inferred] The paper discusses LoRA as a standalone parameter-efficient fine-tuning method but does not explore its combination with other methods like prefix tuning or prompt tuning.
- Why unresolved: The study focuses on LoRA in isolation and does not investigate potential synergies or trade-offs when combining it with other parameter-efficient fine-tuning techniques.
- What evidence would resolve it: Experimental results comparing the memory usage and runtime of LoRA combined with other parameter-efficient fine-tuning methods (e.g., prefix tuning or prompt tuning) against using LoRA alone.

### Open Question 3
- Question: How do the optimizations scale when fine-tuning models with