---
ver: rpa2
title: Ensemble Adversarial Defense via Integration of Multiple Dispersed Low Curvature
  Models
arxiv_id: '2403.16405'
source_url: https://arxiv.org/abs/2403.16405
tags:
- ensemble
- adversarial
- attacks
- robustness
- defense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving adversarial robustness
  in ensemble deep learning models by reducing attack transferability among sub-models.
  The core method introduces a novel regularizer that leverages second-order gradients
  (Hessian) to control the curvature of the loss function, alongside a first-order
  gradient alignment term to increase dispersion among ensemble members.
---

# Ensemble Adversarial Defense via Integration of Multiple Dispersed Low Curvature Models

## Quick Facts
- arXiv ID: 2403.16405
- Source URL: https://arxiv.org/abs/2403.16405
- Reference count: 40
- Primary result: EDLCM achieves 64.549% accuracy on CIFAR-100 under PGD attacks vs 26.646% for ADP and 41.589% for PDD

## Executive Summary
This paper addresses the problem of improving adversarial robustness in ensemble deep learning models by reducing attack transferability among sub-models. The core method introduces a novel regularizer that leverages second-order gradients (Hessian) to control the curvature of the loss function, alongside a first-order gradient alignment term to increase dispersion among ensemble members. By integrating multiple low-curvature models and minimizing shared adversarial subspaces, the approach aims to make adversarial examples less transferable across sub-models.

Experiments on CIFAR-10, CIFAR-100, and TinyImageNet demonstrate that the proposed EDLCM method outperforms state-of-the-art ensemble defense techniques. For example, under PGD attacks with perturbation strength 0.01 on CIFAR-100, the method achieves a classification accuracy of 64.549%, compared to 26.646% for ADP and 41.589% for PDD. It also shows superior robustness under black-box attacks, maintaining high accuracy even against advanced attacks like APGD. The method significantly reduces the transferability success rate (TSR) among ensemble members, indicating enhanced defense capability. However, the computational cost is higher due to Hessian approximation, which remains a limitation for future optimization.

## Method Summary
The EDLCM method trains an ensemble of multiple ResNet-18 models simultaneously using ensemble cross-entropy loss plus two regularization terms. The Lr term controls curvature by minimizing the largest eigenvalues of the Hessian matrix through second-order gradient information, while the Lg term increases gradient dispersion among ensemble members to reduce adversarial transferability. The Hessian-vector products are approximated using differential approximation, and the ensemble is trained via standard SGD updates. The method aims to create diverse sub-models that are robust individually while having minimal overlap in their adversarial subspaces.

## Key Results
- EDLCM achieves 64.549% accuracy on CIFAR-100 under PGD attacks (ε=0.01), significantly outperforming ADP (26.646%) and PDD (41.589%)
- Under black-box attacks, EDLCM maintains high accuracy while ADP and PDD degrade substantially
- Transferability Success Rate (TSR) between ensemble members is significantly reduced, indicating effective attack dispersion
- The method shows consistent improvements across CIFAR-10, CIFAR-100, and TinyImageNet datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reducing Hessian curvature (low curvature) improves adversarial robustness by increasing the perturbation threshold needed for misclassification.
- Mechanism: High curvature in the loss landscape allows small perturbations to induce large changes in loss, making models more susceptible to adversarial attacks. By minimizing the largest eigenvalues of the Hessian matrix, the model's loss landscape becomes flatter, requiring larger perturbations to cross decision boundaries.
- Core assumption: The relationship between curvature and robustness is monotonic; lower curvature consistently correlates with higher robustness.
- Evidence anchors:
  - [abstract] "Given that low curvature provides better robustness, our ensemble model was designed to consider the influence of curvature among different sub-models."
  - [section] "Based on the boundary values of perturbations, we observe that the upper and lower limits of ∥δ∗∥ increase as ν decreases. An increase in ∥δ∗∥ increases the minimum L2 norm ball required to find an adversarial example for input x, thereby enhancing robustness."
- Break condition: If the correlation between curvature and robustness becomes non-monotonic or if low curvature leads to underfitting, the defense effectiveness may degrade.

### Mechanism 2
- Claim: Reducing attack transferability among ensemble members increases overall ensemble robustness.
- Mechanism: Adversarial examples often transfer between models with similar decision boundaries. By promoting diversity in both first-order and second-order gradients, the overlap in adversarial subspaces is minimized, making it harder for a perturbation effective against one model to succeed against others.
- Core assumption: Gradient alignment is a primary factor in adversarial transferability; more dispersed gradients lead to less transferable attacks.
- Evidence anchors:
  - [abstract] "We introduce a novel regularizer to train multiple more-diverse low-curvature network models."
  - [section] "The Lg regularization term aims to control greater dispersion among the sub-models of the ensemble, thereby minimizing the overlap in the common adversarial subspace between the sub-models."
- Break condition: If the diversity introduced by gradient dispersion does not translate to practical robustness gains, or if it leads to conflicting decision boundaries that harm ensemble performance.

### Mechanism 3
- Claim: Using second-order gradient information (via Hessian-vector products) provides a more accurate and comprehensive defense than first-order methods alone.
- Mechanism: First-order methods only consider the linear term of the Taylor expansion of the loss function, while second-order methods account for curvature. This leads to a more precise approximation of the loss landscape and better identification of vulnerable directions.
- Core assumption: The computational approximation of Hessian-vector products is sufficiently accurate to capture meaningful curvature information.
- Evidence anchors:
  - [abstract] "We identify second-order gradients, which depict the loss curvature, as a key factor in adversarial robustness."
  - [section] "In this study, we attempt to promote ensemble diversity from a novel perspective – attack transferability. Specifically, our goal is to construct diverse sub-models that exhibit low transferability of adversarial examples between them."
- Break condition: If the approximation error in Hessian-vector products becomes too large, or if the added complexity does not yield significant robustness improvements.

## Foundational Learning

- Concept: Second-order optimization and Hessian matrix
  - Why needed here: The method relies on second-order gradients to measure and control loss curvature, which is central to improving robustness.
  - Quick check question: What does the Hessian matrix represent in the context of neural network training, and how does it relate to the loss landscape?

- Concept: Adversarial attacks and transferability
  - Why needed here: Understanding how adversarial examples transfer between models is key to designing ensemble defenses that minimize shared vulnerabilities.
  - Quick check question: Why do adversarial examples crafted for one model often fool another model, and what factors influence this transferability?

- Concept: Ensemble learning and diversity
  - Why needed here: The method builds an ensemble of diverse sub-models to improve robustness, so understanding ensemble diversity is essential.
  - Quick check question: How does diversity among ensemble members improve robustness against adversarial attacks, and what metrics can measure this diversity?

## Architecture Onboarding

- Component map:
  - Base ensemble models (e.g., ResNet-18) -> Loss function (Ensemble cross-entropy + Lr + Lg) -> Hessian-vector product approximation -> Simultaneous SGD updates -> Evaluation metrics (Accuracy, TSR)

- Critical path:
  1. Initialize ensemble models
  2. Compute gradients and Hessian-vector products
  3. Apply Lr and Lg regularization
  4. Update model parameters via SGD
  5. Evaluate robustness on adversarial examples

- Design tradeoffs:
  - Higher robustness vs. increased computational cost due to Hessian approximation
  - Diversity vs. potential loss of individual model accuracy
  - Second-order methods vs. scalability to larger models/datasets

- Failure signatures:
  - Low accuracy on clean data (over-regularization)
  - High TSR (failure to reduce transferability)
  - Training instability or divergence
  - Excessive training time or memory usage

- First 3 experiments:
  1. Validate curvature regularization (Lr) alone on a single model and measure robustness improvement.
  2. Test gradient dispersion (Lg) on an ensemble and measure TSR reduction.
  3. Combine Lr and Lg in an ensemble and compare robustness against white-box and black-box attacks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of ensemble members that balances robustness gains against computational cost?
- Basis in paper: [explicit] The paper states "we observed that the defense effectiveness of the ensemble model reached its optimal level when the number of ensemble members was 5" and notes that "as the number of ensemble model members increased, the improvement in the ensemble model's defense became less significant."
- Why unresolved: The paper only tested up to 5 models and found diminishing returns, but did not explore whether even larger ensembles might provide additional benefits or whether the optimal number varies with dataset complexity.
- What evidence would resolve it: Systematic experiments varying ensemble size (e.g., 3, 5, 10, 15 members) across multiple datasets and attack scenarios, measuring both robustness improvements and computational overhead.

### Open Question 2
- Question: Can the computational efficiency of Hessian-vector product approximation be improved without sacrificing robustness?
- Basis in paper: [explicit] The paper acknowledges "the computation of second-order gradients increases the time cost of our training process, rendering it less efficient than traditional approaches" and mentions optimizing this is a "future research direction."
- Why unresolved: The current method uses differential approximation which is computationally expensive (596s/epoch vs 46s/epoch for baseline), but the paper does not propose or evaluate alternative approximation methods.
- What evidence would resolve it: Comparative evaluation of different Hessian approximation techniques (e.g., randomized methods, low-rank approximations) measuring both computational time and robustness performance.

### Open Question 3
- Question: How does the proposed EDLCM method perform against adaptive attacks that specifically target its defense mechanism?
- Basis in paper: [inferred] The paper evaluates against standard attacks (PGD, BIM, FGSM, APGD) but does not test against attacks designed to circumvent this specific ensemble defense strategy.
- Why unresolved: The paper demonstrates superiority against conventional attacks but does not address whether attackers could design strategies that exploit weaknesses in the curvature-based regularization or gradient alignment approach.
- What evidence would resolve it: White-box attacks specifically engineered to minimize the effectiveness of the Lr (curvature) and Lg (gradient alignment) regularization terms, testing whether the ensemble can maintain robustness under such targeted attacks.

## Limitations
- The computational overhead from Hessian-vector product approximation is substantial but not fully quantified in detail
- Implementation details for the Hessian approximation are sparse, particularly regarding the choice of h and normalization process
- The method focuses on ensemble defense without comparing against the strongest single-model adversarial training baselines on equal computational budgets
- Results are primarily shown on ResNet-18; generalization to other architectures remains untested

## Confidence
- High confidence in the theoretical framework linking curvature to robustness and transferability to ensemble vulnerability
- Medium confidence in experimental results due to limited architectural diversity and missing computational cost analysis
- Medium confidence in practical applicability given the computational complexity of second-order methods

## Next Checks
1. Reproduce the curvature regularization (Lr term) on a single model to verify the theoretical claim that lower curvature improves robustness, independent of ensemble effects.
2. Implement the full ensemble method on CIFAR-10 with multiple random seeds to assess result stability and variance in TSR reduction.
3. Conduct a computational efficiency analysis comparing wall-clock training time per epoch between the proposed method and standard ensemble training with adversarial training.