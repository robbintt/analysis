---
ver: rpa2
title: 'MCMChaos: Improvising Rap Music with MCMC Methods and Chaos Theory'
arxiv_id: '2401.07967'
source_url: https://arxiv.org/abs/2401.07967
tags:
- music
- software
- voice
- sample
- simulation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents MCMChaos, a novel freestyle rap generation software
  that employs Markov Chain Monte Carlo (MCMC) methods and chaos theory. Three versions
  of the software use different mathematical simulations - collapsed Gibbs sampler
  and Lorenz attractor - to dynamically alter text-to-speech parameters (rate, volume,
  and voice) on a line-by-line basis.
---

# MCMChaos: Improvising Rap Music with MCMC Methods and Chaos Theory

## Quick Facts
- arXiv ID: 2401.07967
- Source URL: https://arxiv.org/abs/2401.07967
- Authors: Robert G. Kimelman
- Reference count: 15
- One-line primary result: Software generates rap music using MCMC methods and chaos theory to dynamically alter text-to-speech parameters

## Executive Summary
MCMChaos is a novel freestyle rap generation software that combines Markov Chain Monte Carlo methods and chaos theory to create dynamically varied rap performances. The system uses mathematical simulations - collapsed Gibbs sampler and Lorenz attractor - to generate text-to-speech parameters that change line-by-line, creating unique rhythmic variations. The software incorporates a real-time GUI for user control and is built on the MCFlow corpus of rap transcriptions. Results demonstrate that the collapsed Gibbs sampler converges to a normal distribution as intended, while a simpler within-line segmentation approach produces more musically realistic results than pure mathematical simulations.

## Method Summary
The method employs MCMC techniques (collapsed Gibbs sampler) and chaotic dynamical systems (Lorenz attractor) to generate parameter sequences for text-to-speech synthesis. The Gibbs sampler draws correlated samples from conditional normal distributions to maintain rhythmic coherence between lines, while the Lorenz attractor produces bounded chaotic trajectories for parameter variation. The MCFlow corpus provides rap transcriptions as input, which are converted to text format and processed through the mathematical simulations to generate speech rate, volume, and voice parameters. A real-time GUI allows users to control simulation parameters and observe their effects on the generated rap output.

## Key Results
- Collapsed Gibbs sampler converges to a normal distribution as intended, with trace plots confirming proper convergence
- Lorenz attractor implementation successfully adapts chaotic trajectories to fit within permissible TTS parameter ranges
- Within-line segmentation approach produces more musically realistic rap patterns than line-by-line parameter changes
- All three implementations successfully generate audible rap music with dynamic parameter variations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The collapsed Gibbs sampler produces voice rate, volume, and pitch values that follow a normal distribution while maintaining rhythmic correlations between lines.
- Mechanism: The sampler draws samples from conditional distributions of (X|Y), (Y|X), and (Z|X,Y) where each is normally distributed with mean based on correlation ρ and variance scaled by ρ+ρ³. This creates correlated outputs that preserve musical coherence between consecutive lines.
- Core assumption: The target distribution for musical parameters is multivariate normal with correlation ρ between variables, and sampling from conditional normals maintains this structure.
- Evidence anchors:
  - [abstract] "Results show that the collapsed Gibbs sampler converges to a normal distribution as intended"
  - [section] "Our sampling method did indeed converge to the target normal distribution, as can be seen in the histograms below"
  - [corpus] Weak evidence - corpus mentions "MCMC-driven learning" but no direct connection to musical applications

### Mechanism 2
- Claim: The Lorenz attractor generates chaotic but bounded parameter sequences that create interesting rhythmic variations.
- Mechanism: The system of differential equations dx/dt = σ(y-x), dy/dt = x(ρ-z) - y, dz/dt = xy - βz produces a bounded chaotic trajectory. The implementation extracts first digits of x and y, scales them by 100 and 10 respectively, and divides z by 100 to fit within TTS parameter ranges.
- Core assumption: Chaotic trajectories from the Lorenz system, when properly scaled, produce musically interesting variations in speech parameters that are neither too random nor too predictable.
- Evidence anchors:
  - [abstract] "Lorenz attractor implementation is adapted to fit within permissible parameter ranges"
  - [section] "This is a bit of an adaptation of the original lorenz attractor system and takes a simple approach to solving the differential equations"
  - [corpus] Weak evidence - corpus contains "Hallucination Level of Artificial Intelligence Whisperer: Case Speech Recognizing Pantterinousut Rap Song" but no clear connection to Lorenz attractor usage

### Mechanism 3
- Claim: Segmenting lines into rhythmic parts with randomized rate values creates more musically realistic rap patterns than line-by-line parameter changes.
- Mechanism: The software splits each line into two parts (1/2 size version), applies different speech rates (170-200) to each part, and adjusts volume for emphasis. This mimics natural rap flow where rhythm changes within lines rather than only between lines.
- Core assumption: Human listeners perceive rap as having rhythmic variations within lines, not just between lines, and this internal variation is crucial for realistic rap generation.
- Evidence anchors:
  - [abstract] "A simpler implementation that segments lines into rhythmic parts is found to produce more musically realistic results"
  - [section] "This version was made with the intention to achieve a more 'realistic' (i.e., rhythmic) version that sounds more like a human rapper"
  - [corpus] Weak evidence - corpus contains "ComposeOn Academy: Transforming Melodic Ideas into Complete Compositions Integrating Music Learning" suggesting line-segmentation approaches may be musically effective

## Foundational Learning

- Concept: Markov Chain Monte Carlo sampling
  - Why needed here: Provides a principled way to generate correlated parameter sequences that maintain musical coherence across lines
  - Quick check question: What is the key difference between collapsed Gibbs sampling and standard Gibbs sampling, and why is it beneficial for this application?

- Concept: Chaotic dynamical systems (Lorenz attractor)
  - Why needed here: Generates bounded yet unpredictable parameter sequences that create interesting rhythmic variations without becoming completely random
  - Quick check question: How do the parameters σ, ρ, and β affect the behavior of the Lorenz system, and what musical properties do these changes produce?

- Concept: Text-to-speech parameter mapping
  - Why needed here: Understanding how numerical values map to perceptual qualities (rate to speed, volume to loudness, voice ID to timbre) is essential for creating musically meaningful outputs
  - Quick check question: What are typical acceptable ranges for TTS rate, volume, and voice parameters, and how do extreme values affect speech intelligibility?

## Architecture Onboarding

- Component map: MCFlow corpus -> Text-to-text conversion -> Mathematical simulation modules (Gibbs sampler, Lorenz attractor) -> TTS parameter mapping -> pyttsx3 TTS engine -> Audio output
- Critical path: Corpus text → mathematical simulation → parameter mapping → TTS synthesis → audio output
  The most performance-critical path is the real-time parameter adjustment through the GUI to audio synthesis, requiring efficient mathematical computations and responsive TTS control.
- Design tradeoffs:
  - Line-by-line vs. within-line parameter changes: Line-by-line changes are computationally simpler but less musically realistic; within-line changes sound better but require more complex segmentation logic
  - Deterministic vs. stochastic parameter generation: Deterministic approaches are more controllable but may become predictable; stochastic approaches sound more natural but are harder to control
  - Mathematical simulation complexity vs. parameter range constraints: More sophisticated simulations may produce better musical properties but are harder to constrain within TTS parameter limits
- Failure signatures:
  - Parameter values outside TTS engine range → distorted or no speech output
  - Poor convergence of Gibbs sampler → incoherent parameter sequences
  - Inappropriate Lorenz parameters → convergence to fixed points or unbounded growth
  - Segmentation at inappropriate boundaries → choppy, unnatural rhythm
- First 3 experiments:
  1. Test Gibbs sampler convergence with different correlation values (ρ = 0.5, 0.9, 0.99) and verify normal distribution of outputs using histograms and trace plots
  2. Experiment with Lorenz attractor parameters (σ, ρ, β) to find chaotic regimes that produce bounded outputs within TTS parameter ranges
  3. Compare line-by-line vs. within-line parameter variation using user studies to measure perceived rhythmic quality and musicality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the mathematical simulation methods be adapted to produce more musically realistic and rhythmic rap generation?
- Basis in paper: [explicit] The paper mentions that the authors' simplest implementation, which segments lines into rhythmic parts, produces more musically realistic results compared to the mathematical simulations.
- Why unresolved: The paper acknowledges that the voices used in the mathematical simulation versions are not as rhythmic as desired, and finding simulations that both obey mathematical laws and fit within the allowed range of text-to-speech parameters was challenging.
- What evidence would resolve it: Developing and testing new mathematical models or adapting existing ones to better capture the rhythmic patterns and nuances of rap music, while ensuring the generated speech parameters fall within the permissible ranges.

### Open Question 2
- Question: How can the software be improved to make the rapping nearly indistinguishable from a human rapper?
- Basis in paper: [explicit] The paper states that to achieve a rapping voice that is nearly indistinguishable from a human, a novel text-to-rap algorithm that does not rely on previous text-to-speech processors would likely be needed.
- Why unresolved: The current implementation uses existing text-to-speech engines, which may not fully capture the unique characteristics and expressiveness of human rap performances.
- What evidence would resolve it: Developing and evaluating a new text-to-rap algorithm that incorporates features such as pitch variation, emphasis, and rhythmic patterns specific to rap music, and comparing its output to human rap performances.

### Open Question 3
- Question: How can the software be integrated with human-computer interaction and perceptual studies to evaluate its effectiveness and user experience?
- Basis in paper: [explicit] The paper suggests that future research could involve integrating human-computer interaction and perceptual studies, such as surveys measuring users' level of expressivity, enjoyment, and control of the software.
- Why unresolved: The current version of the software focuses on the technical aspects of rap generation, but does not include mechanisms for gathering user feedback or evaluating the perceived quality of the generated rap music.
- What evidence would resolve it: Designing and conducting user studies that involve participants interacting with the software, providing feedback on their experience, and assessing the perceived quality and enjoyment of the generated rap music.

## Limitations
- MCFlow corpus conversion process is underspecified, with unclear implementation details for the R function
- Lorenz attractor parameter scaling uses arbitrary values without mathematical justification for musical relevance
- Claims about within-line segmentation superiority are based on subjective assessment rather than empirical evidence
- Real-time performance capabilities and GUI responsiveness are not validated through benchmarks or user testing

## Confidence
- High Confidence: The collapsed Gibbs sampler converges to a normal distribution as intended
- Medium Confidence: The Lorenz attractor implementation produces bounded chaotic parameter sequences
- Low Confidence: Segmenting lines into rhythmic parts produces more musically realistic results

## Next Checks
1. **Empirical User Study:** Conduct a randomized controlled trial with 30-50 participants comparing the three implementations (line-by-line Gibbs, Lorenz attractor, and within-line segmentation) using a 7-point Likert scale for rhythmic quality, musicality, and human-likeness.
2. **Parameter Scaling Analysis:** Systematically vary the Lorenz attractor scaling factors (100, 10, 1/100) across a range of values (50-200, 5-50, 0.01-0.5) and measure the resulting parameter distributions, TTS output quality, and musical coherence using both quantitative metrics and qualitative assessments.
3. **Convergence Robustness Testing:** Test the collapsed Gibbs sampler with different correlation values (ρ = 0.5, 0.7, 0.9, 0.99) and initial conditions to verify convergence properties hold across a broader parameter space, including quantitative convergence diagnostics.