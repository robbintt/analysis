---
ver: rpa2
title: 'CALICO: Confident Active Learning with Integrated Calibration'
arxiv_id: '2407.02335'
source_url: https://arxiv.org/abs/2407.02335
tags: []
core_contribution: The paper proposes CALICO, a method that integrates confidence
  calibration into active learning by jointly training a classifier and an energy-based
  model. The approach aims to address the overconfidence issue in deep neural networks
  used for active learning, where unreliable confidence outputs can lead to poor sample
  selection.
---

# CALICO: Confident Active Learning with Integrated Calibration

## Quick Facts
- arXiv ID: 2407.02335
- Source URL: https://arxiv.org/abs/2407.02335
- Reference count: 34
- The paper proposes CALICO, a method that integrates confidence calibration into active learning by jointly training a classifier and an energy-based model.

## Executive Summary
This paper addresses the overconfidence problem in deep neural networks used for active learning by proposing CALICO, which integrates confidence calibration through joint training of a classifier and an energy-based model (EBM). The method aims to produce well-calibrated confidence outputs without requiring additional labeled validation data, enabling more effective sample selection in active learning loops. Experiments on medical imaging datasets demonstrate that CALICO improves classification accuracy and reduces calibration error compared to baseline methods using softmax-based classifiers.

## Method Summary
CALICO jointly trains a neural network classifier and an EBM to simultaneously estimate class posteriors and input data distribution. The method uses a multi-head architecture where one head produces class probabilities via softmax and another produces energy values for the EBM. During training, the model optimizes a combined loss function that includes cross-entropy for classification and negative log-likelihood for the EBM, with sampling performed using Stochastic Gradient Langevin Dynamics (SGLD). The calibrated confidence outputs are then used with a least confidence query strategy to select informative samples for labeling in the active learning loop.

## Key Results
- CALICO achieves better classification accuracy with fewer labeled samples compared to baseline softmax classifiers
- The method significantly reduces Expected Calibration Error (ECE) on medical imaging datasets
- Self-calibration eliminates the need for separate labeled validation data for post-hoc calibration
- Performance improvements are consistent across multiple MedMNIST medical image datasets (Blood, Derma, OrganS, OrganC, Pneumonia)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint training of classifier and EBM calibrates confidence by learning the true data distribution.
- Mechanism: The EBM estimates p(x) while the classifier estimates p(y|x). The loss function forces both models to account for the frequency of data occurrences, naturally lowering posterior probabilities for ambiguous samples near the decision boundary.
- Core assumption: The joint model can accurately estimate both the input data distribution and class posteriors simultaneously without overfitting.
- Evidence anchors:
  - [abstract] "simultaneously estimating the input data distribution with an EBM and class probabilities with a classifier"
  - [section] "Estimating the input distribution using a generative model allows us to account for the frequency of data occurrences. This approach naturally lowers the posterior probabilities for ambiguous data points near the decision boundary"
- Break condition: If the EBM cannot accurately model the data distribution, the calibration effect fails. Also, if the classifier overfits before the EBM converges, miscalibration persists.

### Mechanism 2
- Claim: Self-calibration during training eliminates need for validation data.
- Mechanism: By using unlabeled data in the loss function (equation 4), the model learns calibration directly from the data distribution rather than requiring a separate labeled validation set for post-hoc calibration.
- Core assumption: Unlabeled data is representative of the true data distribution and contains sufficient information for calibration.
- Evidence anchors:
  - [abstract] "improving calibration without needing an additional labeled dataset"
  - [section] "To calibrate the confidence output during the AL loop without relying on validation samples, our key idea involves leveraging the distribution of unlabeled training data"
- Break condition: If unlabeled data is not representative (e.g., contains outliers or is heavily biased), calibration will be poor. Break also occurs if the model overfits to the small labeled set before leveraging unlabeled data.

### Mechanism 3
- Claim: Least confidence query strategy selects most informative samples when using calibrated confidence.
- Mechanism: After self-calibration, confidence scores more accurately reflect uncertainty. The least confidence strategy then selects samples with lowest p(y|x), which are genuinely uncertain rather than artificially overconfident.
- Core assumption: The calibration process produces confidence scores that correlate with true uncertainty.
- Evidence anchors:
  - [section] "we utilize a least confidence strategy" and "The least confidence query strategy is designed to acquire samples with the smallest probability among the maximum activations"
- Break condition: If calibration is incomplete or unstable, the least confidence strategy may still select samples based on misleading confidence values.

## Foundational Learning

- Concept: Energy-Based Models and their partition function
  - Why needed here: The EBM component requires understanding of energy functions, partition functions, and sampling techniques like SGLD
  - Quick check question: What is the mathematical relationship between the energy function Eθ(x) and the probability density pθ(x) in an EBM?

- Concept: Semi-supervised learning with joint models
  - Why needed here: CALICO trains on both labeled and unlabeled data simultaneously, requiring understanding of how to construct loss functions that incorporate both
  - Quick check question: How does equation (4) combine classification loss with EBM negative log-likelihood?

- Concept: Confidence calibration metrics (ECE)
  - Why needed here: The paper evaluates performance using Expected Calibration Error, which requires understanding how to compute calibration error from binned confidence values
  - Quick check question: How is ECE calculated from accuracy and confidence within each bin according to equation (5)?

## Architecture Onboarding

- Component map:
  - Wide-ResNet backbone with Swish activation
  - Classification head (softmax output for p(y|x))
  - EBM head (energy-based output for p(x))
  - Joint loss function combining cross-entropy and negative log-likelihood
  - SGLD sampler for EBM training
  - Query strategy module (least confidence)

- Critical path:
  1. Initialize model with multi-head output
  2. Train on labeled data using cross-entropy
  3. Use SGLD to sample from EBM and compute negative log-likelihood
  4. Combine losses and update parameters
  5. Compute calibrated confidence from classifier head
  6. Select samples using least confidence strategy
  7. Query oracle for labels
  8. Add to labeled set and repeat

- Design tradeoffs:
  - Computational cost: EBM training with SGLD is expensive vs. simple softmax
  - Hyperparameter sensitivity: Learning rates, SGLD step size, and temperature affect calibration
  - Model capacity: Joint model must be large enough to learn both tasks but not overfit
  - Sampling strategy: Informative initialization vs. random noise for SGLD

- Failure signatures:
  - High ECE despite training completion (calibration failed)
  - Unstable accuracy during AL iterations (poor sample selection)
  - EBM training divergence (partition function estimation issues)
  - Overfitting to small labeled set (insufficient regularization)

- First 3 experiments:
  1. Train baseline softmax classifier on full labeled set, compute ECE - establishes reference point
  2. Train CALICO on small labeled set with all unlabeled data, monitor ECE vs. baseline - tests calibration capability
  3. Run AL loop with CALICO, track accuracy and ECE per iteration - tests active learning performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CALICO scale to high-resolution medical images beyond 28x28 resolution?
- Basis in paper: [inferred] The paper explicitly states that scalability is a limitation because training EBMs on high-resolution images requires considerable hyperparameter tuning, and CALICO was only evaluated on small-resolution images.
- Why unresolved: The paper only evaluated CALICO on 28x28 preprocessed medical images from the MedMNIST collection, leaving the performance on larger, more realistic medical images unknown.
- What evidence would resolve it: Experiments evaluating CALICO on high-resolution medical imaging datasets (e.g., chest X-rays at 512x512 or higher) with proper hyperparameter tuning and computational resources would provide evidence.

### Open Question 2
- Question: What is the impact of class distribution balancing on CALICO's performance across different types of imbalanced datasets beyond binary classification?
- Basis in paper: [explicit] The paper discusses the impact of class distribution balancing on CALICO's performance, particularly noting that balancing helped on the imbalanced binary PneumoniaMNIST dataset but had mixed results on multi-class datasets.
- Why unresolved: The experiments only tested one binary imbalanced dataset (PneumoniaMNIST) and several relatively balanced multi-class datasets, without exploring the effects of class balancing on other types of imbalanced datasets (e.g., multi-class imbalanced, long-tailed distributions).
- What evidence would resolve it: Systematic experiments on various types of imbalanced datasets (multi-class imbalanced, long-tailed, etc.) with and without class distribution balancing would clarify the generalizability of this finding.

### Open Question 3
- Question: How does CALICO compare to Bayesian active learning methods that use uncertainty quantification for sample selection?
- Basis in paper: [inferred] The paper mentions that recent advances in uncertainty quantification have received interest for use in AL paradigms and notes that CALICO avoids the computational overhead of Bayesian methods while still achieving calibration.
- Why unresolved: The paper does not directly compare CALICO to Bayesian active learning methods, leaving questions about whether the self-calibration approach provides advantages in terms of computational efficiency, sample efficiency, or calibration quality compared to uncertainty quantification approaches.
- What evidence would resolve it: Direct experimental comparisons between CALICO and Bayesian active learning methods (e.g., BALD, Monte Carlo dropout-based methods) on the same datasets with identical computational constraints would provide evidence of relative performance.

## Limitations
- EBM training with SGLD sampling is computationally expensive compared to simpler calibration methods
- Performance has only been demonstrated on small-resolution medical imaging datasets (28x28), with unclear scalability to high-resolution images
- The method requires careful hyperparameter tuning, particularly for EBM training stability and convergence

## Confidence
- Mechanism 1 (joint training calibration): Medium - theoretical justification is sound, but empirical validation across diverse datasets is limited
- Mechanism 2 (self-calibration without validation data): Medium - the approach is novel but untested against established calibration methods requiring validation sets
- Mechanism 3 (least confidence with calibrated outputs): High - straightforward application of established query strategy with improved inputs

## Next Checks
1. Test CALICO on non-medical image datasets (e.g., CIFAR-10/100) to assess generalizability beyond medical imaging domain
2. Compare calibration performance against post-hoc calibration methods (temperature scaling, histogram binning) that use validation data
3. Evaluate computational overhead by measuring training time and resource requirements versus baseline softmax classifiers