---
ver: rpa2
title: 'Condense, Don''t Just Prune: Enhancing Efficiency and Performance in MoE Layer
  Pruning'
arxiv_id: '2412.00069'
source_url: https://arxiv.org/abs/2412.00069
tags:
- experts
- layer
- layers
- expert
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ConDense-MoE (CD-MoE), a method that compresses
  large sparse MoE layers into smaller, dense layers by selectively retaining only
  the most important experts and removing the routing mechanism. Specifically designed
  for fine-grained MoE with shared experts (e.g., DeepSeekMoE, QwenMoE), it uses a
  greedy search algorithm to identify which layers and experts to condense while preserving
  shared experts.
---

# Condense, Don't Just Prune: Enhancing Efficiency and Performance in MoE Layer Pruning

## Quick Facts
- arXiv ID: 2412.00069
- Source URL: https://arxiv.org/abs/2412.00069
- Authors: Mingyu Cao, Gen Li, Jie Ji, Jiaqi Zhang, Xiaolong Ma, Shiwei Liu, Lu Yin
- Reference count: 25
- Primary result: 27.5% memory reduction and 1.26x inference speedup while maintaining 90% accuracy on DeepSeekMoE-16B

## Executive Summary
This paper introduces ConDense-MoE (CD-MoE), a novel approach to compressing Mixture-of-Experts (MoE) layers by condensing them into smaller, dense layers rather than traditional pruning. The method selectively retains the most important experts while eliminating the routing mechanism, achieving significant efficiency gains without sacrificing performance. Specifically designed for fine-grained MoE models with shared experts, CD-MoE demonstrates that intelligent condensation can outperform conventional pruning techniques in both speed and accuracy preservation.

## Method Summary
CD-MoE uses a greedy search algorithm to identify which MoE layers and experts to condense based on Jensen-Shannon divergence analysis. The method preserves shared experts and top-k routing experts (totaling 8 experts per layer), removes the routing mechanism, and replaces it with fixed gate values computed from calibration data. Lightweight fine-tuning is then applied only to the condensed layers, enabling rapid recovery of model performance with minimal computational overhead.

## Key Results
- 27.5% memory usage reduction on DeepSeekMoE-16B
- 1.26x inference speed improvement
- 90% accuracy retention after condensation
- 98% performance recovery through 5-hour lightweight fine-tuning on single A100 GPU

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Condensing MoE layers by removing routing and retaining a small subset of experts preserves most representational power.
- Mechanism: Greedy search algorithm identifies most impactful experts by minimizing Jensen-Shannon divergence between pre- and post-condensation outputs.
- Core assumption: Top-k selected experts capture essential information processing capabilities.
- Evidence: JS divergence analysis and experimental results showing 90% accuracy retention.

### Mechanism 2
- Claim: Lightweight fine-tuning on condensed layers recovers up to 98% of original performance with minimal overhead.
- Mechanism: Condensed layers retain sufficient representational capacity that can be reactivated through targeted fine-tuning of only 3.8% of parameters.
- Core assumption: Preserved parameters maintain enough information for effective recovery.
- Evidence: 5-hour fine-tuning achieving 98% performance recovery on single A100 GPU.

### Mechanism 3
- Claim: Selective layer condensation based on impact analysis maximizes efficiency gains while preserving functionality.
- Mechanism: JS divergence measurements identify which layers can be condensed with minimal performance impact, preserving critical layers.
- Core assumption: Different layers contribute unequally to model performance.
- Evidence: Layer-wise analysis showing some layers can be condensed more aggressively than others.

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) architecture and routing mechanisms
  - Why needed here: Understanding MoE fundamentals is essential for grasping why removing routing and condensing layers is effective
  - Quick check question: What is the difference between shared experts and routing experts in an MoE layer?

- Concept: Expert selection and importance ranking
  - Why needed here: The method relies on identifying which experts to retain during condensation
  - Quick check question: How does the Jensen-Shannon divergence help identify the most important experts?

- Concept: Model compression and pruning techniques
  - Why needed here: CD-MoE represents a novel form of model compression distinct from traditional pruning
  - Quick check question: What distinguishes layer condensation from expert pruning or layer pruning?

## Architecture Onboarding

- Component map: Original MoE Layer -> Condensation Algorithm -> Condensed Layer -> Fine-tuning Module
- Critical path: Data → Expert Selection → Layer Selection → Condensation → Inference/Fine-tuning
- Design tradeoffs:
  - Memory vs. Accuracy: More aggressive condensation saves more memory but risks performance loss
  - Computation vs. Quality: Greedy search adds overhead but improves condensation quality
  - Fine-tuning Scope vs. Recovery: Larger fine-tuning scope improves recovery but increases computational cost
- Failure signatures:
  - Performance degradation exceeding 10% on downstream tasks
  - Inability to recover performance through fine-tuning
  - Excessive memory usage compared to expected savings
- First 3 experiments:
  1. Implement basic condensation on single layer with greedy expert selection, measure JS divergence
  2. Apply layer selection algorithm to identify which layers can be condensed, verify impact analysis
  3. Perform lightweight fine-tuning on condensed layers, measure recovery percentage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CD-MoE perform on MoE architectures without shared experts?
- Basis: Paper states CD-MoE is "primarily designed for fine-grained MoE models with shared experts" and notes performance changes may occur in traditional MoE architectures without shared experts.
- Why unresolved: Only tested on DeepSeekMoE-16B and Qwen1.5-MoE-A2.7B, both using shared experts.
- Resolution evidence: Experiments on Switch Transformer or GLaM with performance comparisons.

### Open Question 2
- Question: What is the optimal ratio of shared to routing experts for different downstream tasks?
- Basis: Paper mentions Qwen1.5-MoE-A2.7B's better performance due to larger proportion of shared experts but doesn't systematically explore different ratios.
- Why unresolved: Only tested with default expert ratios from pre-trained models.
- Resolution evidence: Systematic experiments varying shared/routing expert ratios across task types.

### Open Question 3
- Question: How does CD-MoE compare to quantization and distillation when applied together?
- Basis: Paper mentions "exploring synergy of CD-MoE with quantization and knowledge distillation" but provides no empirical comparison.
- Why unresolved: Only evaluates CD-MoE in isolation against traditional pruning methods.
- Resolution evidence: Experiments comparing CD-MoE alone versus combined approaches measuring memory, speed, and accuracy.

## Limitations
- Greedy search algorithm may not capture all aspects of expert importance
- Method specifically targets fine-grained MoE with shared experts, limiting generalizability
- Lightweight fine-tuning may not fully restore performance for all model variants

## Confidence

**High Confidence:**
- Memory usage reduction of 27.5% (experimental results)
- Inference speed increase of 1.26x (experimental results)
- 90% accuracy retention after condensation (validated across tasks)
- 98% performance recovery through fine-tuning (controlled experiments)

**Medium Confidence:**
- Generalizability to different MoE architectures
- Scalability to larger models
- Fine-tuning recovery robustness across diverse tasks

## Next Checks
1. Cross-Architecture Validation: Test CD-MoE on different MoE architectures (e.g., without shared experts, different routing mechanisms) to verify method generalizability.

2. Extended Fine-tuning Analysis: Evaluate fine-tuning recovery process with varying durations and compute resources to determine optimal trade-offs between recovery quality and computational cost.

3. Robustness Testing: Assess model performance under adversarial inputs and out-of-distribution data to verify condensation doesn't introduce new vulnerabilities.