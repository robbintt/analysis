---
ver: rpa2
title: 'Investigating Calibration and Corruption Robustness of Post-hoc Pruned Perception
  CNNs: An Image Classification Benchmark Study'
arxiv_id: '2405.20876'
source_url: https://arxiv.org/abs/2405.20876
tags:
- pruning
- calibration
- corruption
- robustness
- uncertainty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the impact of post-hoc CNN pruning on\
  \ uncertainty calibration and natural corruption robustness in image classification.\
  \ The study benchmarks three pruning methods\u2014unstructured weight pruning, structured\
  \ filter pruning, and structured channel pruning\u2014on VGG and ResNet architectures\
  \ using CIFAR-10 and CIFAR-10-C datasets."
---

# Investigating Calibration and Corruption Robustness of Post-hoc Pruned Perception CNNs: An Image Classification Benchmark Study

## Quick Facts
- **arXiv ID**: 2405.20876
- **Source URL**: https://arxiv.org/abs/2405.20876
- **Reference count**: 40
- **Key outcome**: Post-hoc CNN pruning improves or maintains uncertainty calibration and natural corruption robustness across compression levels

## Executive Summary
This paper investigates how post-hoc pruning affects uncertainty calibration and natural corruption robustness in CNNs for image classification. The study benchmarks three pruning methods—unstructured weight pruning, structured filter pruning, and structured channel pruning—on VGG and ResNet architectures using CIFAR-10 and CIFAR-10-C datasets. Results show that pruning consistently improves or maintains uncertainty calibration across compression levels, with unstructured pruning showing the most significant improvements. Natural corruption robustness is preserved or enhanced, except for high pruning ratios in channel pruning of ResNet-164. Importantly, pruning does not negatively affect uncertainty calibration even under corrupted inputs.

## Method Summary
The study trains VGG-19 and ResNet-164 models on CIFAR-10 using SGD with a learning rate schedule (0.1→0.01→0.001) over 160 epochs. Three pruning methods are applied at 10% intervals from 0-70% pruning ratio: unstructured weight pruning (Han et al.), structured filter pruning (Li et al.), and structured channel pruning (Liu et al.). Pruned models are fine-tuned for 40 epochs at constant learning rate (0.001). Evaluation includes ECE (using 10 equal-mass bins) and accuracy on clean CIFAR-10 test set, plus mPC on CIFAR-10-C across 15 corruption types with 5 severity levels.

## Key Results
- Post-hoc pruning substantially improves model uncertainty calibration across all tested methods and compression levels
- Unstructured pruning provides the most significant calibration improvements compared to structured methods
- Natural corruption robustness is preserved or enhanced, with channel pruning showing degradation only at high ratios for ResNet-164
- Pruning maintains or improves calibration even when evaluating on corrupted inputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Post-hoc pruning improves uncertainty calibration by removing weights that contribute to overconfidence without harming model expressiveness.
- Mechanism: During pruning, low-magnitude weights associated with over-confident predictions are eliminated, leading to more calibrated confidence scores.
- Core assumption: Low-magnitude weights in CNNs are primarily responsible for producing overconfident predictions.
- Evidence anchors:
  - [abstract]: "post-hoc pruning substantially improves the model's uncertainty calibration"
  - [section]: "The results suggest that even high pruning ratios do not impact the uncertainty calibration compared to that of the original unpruned model"
- Break condition: If pruning removes weights critical for maintaining diverse prediction distributions, calibration could degrade.

### Mechanism 2
- Claim: Structured pruning preserves natural corruption robustness by maintaining architectural patterns that capture robust features.
- Mechanism: By removing entire filters or channels rather than individual weights, structured pruning preserves the network's ability to capture spatial and feature relationships important for handling corrupted inputs.
- Core assumption: The spatial organization of filters and channels in CNNs encodes important information for corruption robustness.
- Evidence anchors:
  - [abstract]: "post-hoc pruning has no negative impact on natural corruption robustness"
  - [section]: "the natural corruption robustness of weight pruned and filter pruned models... is better or similar compared to the original unpruned model"
- Break condition: If pruning removes critical channels that capture invariant features across corruption types.

### Mechanism 3
- Claim: Unstructured pruning enhances calibration by creating sparsity patterns that regularize overconfident predictions.
- Mechanism: Weight sparsity introduced by unstructured pruning creates a more regularized network where remaining weights must carry more predictive responsibility, leading to better-calibrated outputs.
- Core assumption: Sparse networks inherently produce more calibrated predictions than dense networks.
- Evidence anchors:
  - [abstract]: "post-hoc unstructured pruned models exhibit substantial improvements in calibration"
  - [section]: "unstructured pruning can even enhance the uncertainty calibration of the VGG-19 and ResNet-110 model"
- Break condition: If excessive sparsity causes information loss that outweighs regularization benefits.

## Foundational Learning

- **Concept: Expected Calibration Error (ECE)**
  - Why needed here: ECE is the primary metric used to quantify uncertainty calibration in this study.
  - Quick check question: How is ECE calculated and what does a lower value indicate about model calibration?

- **Concept: Natural corruption robustness**
  - Why needed here: The study evaluates how pruning affects model performance under corrupted inputs.
  - Quick check question: What are the main categories of natural corruptions tested in CIFAR-10-C?

- **Concept: Structured vs unstructured pruning**
  - Why needed here: The paper compares these two pruning paradigms for their effects on safety metrics.
  - Quick check question: What is the fundamental difference between structured and unstructured pruning approaches?

## Architecture Onboarding

- **Component map**: CIFAR-10 (clean) -> Pruning methods (weight, filter, channel) -> CIFAR-10-C (corrupted) -> Evaluation (ECE, mPC, accuracy)

- **Critical path**:
  1. Train original models on CIFAR-10
  2. Apply pruning methods at various ratios
  3. Fine-tune pruned models
  4. Evaluate on clean data (accuracy, ECE)
  5. Evaluate on corrupted data (mPC, ECE)

- **Design tradeoffs**:
  - Unstructured pruning offers better calibration but lacks hardware acceleration
  - Structured pruning enables efficient inference but may sacrifice some calibration benefits
  - Higher pruning ratios improve efficiency but may degrade corruption robustness

- **Failure signatures**:
  - Calibration degradation: ECE increases significantly with pruning ratio
  - Corruption robustness loss: mPC drops sharply for specific corruption types
  - Accuracy collapse: Performance on clean data falls below acceptable thresholds

- **First 3 experiments**:
  1. Run unstructured pruning on VGG-19 with 10-70% ratios, measure ECE and accuracy
  2. Apply filter pruning to ResNet-110, test corruption robustness across severity levels
  3. Compare channel pruning effects on ResNet-164 vs VGG-19 for corruption handling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does post-hoc pruning improve uncertainty calibration differently for structured versus unstructured pruning methods?
- Basis in paper: [explicit] The paper compares ECE results across weight, filter, and channel pruning methods and notes that unstructured pruning shows substantial improvements while structured pruning shows improvements only up to certain compression levels.
- Why unresolved: The paper doesn't fully explore why these differences exist or whether they generalize to other architectures beyond the tested VGG and ResNet models.

### Open Question 2
- Question: How does the compression ratio threshold affect natural corruption robustness differently between channel pruning and other pruning methods?
- Basis in paper: [explicit] The paper finds that channel pruning degrades mPC for ResNet-164 around 60% compression, while weight and filter pruning maintain robustness across all tested ratios.
- Why unresolved: The paper doesn't explain the mechanism behind why channel pruning specifically causes this degradation or whether this threshold varies by architecture depth.

### Open Question 3
- Question: Do the benefits of post-hoc pruning on uncertainty calibration persist when models are deployed in real-world safety-critical applications with concept drift?
- Basis in paper: [inferred] The paper notes that safety-critical applications require both proper uncertainty calibration and robustness against naturally occurring perturbations, but the experiments only use synthetic corruptions from CIFAR-10-C.
- Why unresolved: Real-world concept drift involves semantic changes to input distributions that may affect calibration differently than synthetic corruptions.

## Limitations
- Findings are limited to VGG and ResNet architectures on CIFAR-10 and CIFAR-10-C datasets
- Results may not generalize to other computer vision tasks like object detection or semantic segmentation
- The study uses synthetic corruptions from CIFAR-10-C rather than real-world domain shifts

## Confidence
- **High confidence**: The claim that pruning does not negatively impact uncertainty calibration
- **Medium confidence**: The assertion that unstructured pruning provides the most significant calibration improvements
- **Medium confidence**: The observation that channel pruning shows degradation in corruption robustness at high ratios

## Next Checks
1. Test the pruning methods on additional architectures (e.g., EfficientNet, MobileNet) to verify if the observed calibration improvements generalize beyond VGG and ResNet families
2. Evaluate the impact of pruning on out-of-distribution datasets beyond CIFAR-10-C to assess robustness to domain shifts and adversarial examples
3. Investigate the computational efficiency gains of pruned models on actual hardware to validate the practical deployment benefits suggested by the theoretical findings