---
ver: rpa2
title: Selecting Large Language Model to Fine-tune via Rectified Scaling Law
arxiv_id: '2402.02314'
source_url: https://arxiv.org/abs/2402.02314
tags:
- scaling
- fine-tuning
- arxiv
- language
- phase
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of selecting the most appropriate
  large language model (LLM) for fine-tuning given resource constraints. The authors
  formulate this as a resource-constrained prediction task and draw a natural connection
  to scaling law.
---

# Selecting Large Language Model to Fine-tune via Rectified Scaling Law

## Quick Facts
- arXiv ID: 2402.02314
- Source URL: https://arxiv.org/abs/2402.02314
- Reference count: 40
- The paper proposes a novel algorithm for selecting optimal LLMs for fine-tuning that achieves 62.7% correlation between predicted and actual performance while consuming hundreds of times less resources than traditional methods.

## Executive Summary
This paper addresses the critical challenge of selecting the most appropriate large language model (LLM) for fine-tuning when constrained by computational resources. The authors establish a novel connection between resource-constrained model selection and scaling laws, discovering a previously unobserved "pre-power phase" in fine-tuning scaling curves. By introducing the concept of "pre-learned data size" into their Rectified Scaling Law, they develop an efficient algorithm that can identify near-optimal models with dramatically reduced resource consumption compared to traditional full fine-tuning approaches.

## Method Summary
The authors formulate LLM selection as a resource-constrained prediction problem and develop a Rectified Scaling Law that incorporates pre-learned data size to better fit experimental observations. Their selection algorithm leverages this law to predict full fine-tuning performance from limited resource budgets, enabling efficient model selection. The approach specifically addresses the computational challenge of evaluating multiple LLMs under resource constraints by exploiting the predictable relationship between training data size, model parameters, and performance.

## Key Results
- Achieves 62.7% average Pearson correlation between predicted and true full-fine-tuning performance
- Reduces resource consumption by hundreds of times compared to other selection methods
- Demonstrates superior performance compared to baseline selection methods, which can produce negatively correlated predictions under demanding constraints

## Why This Works (Mechanism)
The method works by recognizing that LLM fine-tuning performance follows predictable scaling patterns that can be mathematically modeled. The discovery of the pre-power phase in scaling curves reveals that model performance evolves through distinct phases during fine-tuning, which existing scaling laws fail to capture. By incorporating pre-learned data size into their rectified scaling law, the authors create a more accurate model of how LLMs acquire knowledge during fine-tuning, enabling better performance predictions from limited data.

## Foundational Learning

### Scaling Laws in Machine Learning
- Why needed: Understanding how model performance scales with data and parameters is crucial for predicting fine-tuning outcomes
- Quick check: Verify that the pre-power phase observation holds across different model families and task types

### Resource-Constrained Optimization
- Why needed: Real-world applications require balancing model performance with computational budget limitations
- Quick check: Confirm that the algorithm maintains efficiency gains across varying constraint levels

### Model Selection Theory
- Why needed: Choosing optimal models requires understanding trade-offs between model capacity and data requirements
- Quick check: Validate that selection criteria remain robust when applied to models outside the tested parameter range

## Architecture Onboarding

### Component Map
Dataset -> Scaling Law Model -> Resource Constraint Evaluator -> Model Selector -> Performance Predictor

### Critical Path
The critical path flows from dataset analysis through scaling law fitting to resource-constrained model selection. The scaling law model serves as the central component that connects resource constraints to performance predictions.

### Design Tradeoffs
The primary tradeoff involves accuracy versus resource efficiency. While more extensive fine-tuning would provide better performance estimates, the rectified scaling law enables accurate predictions with dramatically reduced computational overhead. The authors chose to prioritize resource efficiency while maintaining acceptable prediction accuracy (62.7% correlation).

### Failure Signatures
- Poor scaling law fit indicates the pre-power phase characterization may not generalize to new domains
- Negative correlation between predicted and actual performance suggests the pre-learned data size estimation is inaccurate
- Resource constraint violations occur when the algorithm underestimates the computational requirements for adequate model evaluation

### First Experiments
1. Test the scaling law on a diverse set of datasets beyond code instruction-following tasks
2. Evaluate the selection algorithm with LLMs outside the 7B-70B parameter range
3. Assess performance predictions when fine-tuning with noisy or partially labeled data

## Open Questions the Paper Calls Out
None

## Limitations
- The study only evaluates 4 LLMs (Llama3-8B, Llama3-70B, Qwen2-7B, Qwen2-72B), limiting generalizability
- All experiments use a single code instruction-following dataset, which may not represent other domains
- The 62.7% correlation between predicted and actual performance leaves substantial room for improvement
- Real-world deployment scenarios with dynamic constraints and varying data quality were not fully explored

## Confidence

### High Confidence
- The theoretical framework connecting resource-constrained LLM selection to scaling laws is sound
- The mathematical formulation and concept of incorporating pre-learned data size are well-justified

### Medium Confidence
- The discovery of the pre-power phase in fine-tuning scaling curves is supported by empirical evidence
- The pre-power phase characterization requires validation across diverse datasets and model families

### Low Confidence
- The practical effectiveness of the Rectified Scaling Law-based selection algorithm in real-world, production-grade scenarios remains to be thoroughly validated

## Next Checks
1. Evaluate the algorithm and scaling law observations across multiple diverse datasets (e.g., general language understanding, medical text, legal documents) to assess domain generalizability
2. Test the algorithm with a broader range of LLMs including different architectures and sizes (from 1B to 100B+ parameters) to validate scalability
3. Implement the selection algorithm in a simulated production environment with dynamic resource constraints, variable data quality, and multiple concurrent tasks to assess practical robustness