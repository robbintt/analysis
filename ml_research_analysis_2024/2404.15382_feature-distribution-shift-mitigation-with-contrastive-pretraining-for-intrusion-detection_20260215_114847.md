---
ver: rpa2
title: Feature Distribution Shift Mitigation with Contrastive Pretraining for Intrusion
  Detection
arxiv_id: '2404.15382'
source_url: https://arxiv.org/abs/2404.15382
tags:
- feature
- pretraining
- distribution
- data
- shift
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of feature distribution shift
  in network intrusion detection (NID), where changes in feature distributions over
  time negatively impact model performance. The authors propose SwapCon, a machine
  learning model that leverages contrastive pretraining with a swapping augmentation
  strategy to compress shift-invariant feature information during pretraining and
  refine it during finetuning.
---

# Feature Distribution Shift Mitigation with Contrastive Pretraining for Intrusion Detection

## Quick Facts
- arXiv ID: 2404.15382
- Source URL: https://arxiv.org/abs/2404.15382
- Authors: Weixing Wang; Haojin Yang; Christoph Meinel; Hasan Yagiz Ã–zkan; Cristian Bermudez Serna; Carmen Mas-Machuca
- Reference count: 11
- Key outcome: This paper addresses the problem of feature distribution shift in network intrusion detection (NID), where changes in feature distributions over time negatively impact model performance. The authors propose SwapCon, a machine learning model that leverages contrastive pretraining with a swapping augmentation strategy to compress shift-invariant feature information during pretraining and refine it during finetuning. Using the Kyoto2006+ dataset, the authors demonstrate that SwapCon, when pretrained with an appropriate model size and numerical embedding strategy (Piecewise Linear Embedding), increases robustness against feature distribution shifts by over 8% compared to non-pretrained models. SwapCon also outperforms traditional ML models like XGBoost and KNN by a large margin, especially in scenarios with significant feature distribution shifts.

## Executive Summary
This paper tackles the critical challenge of feature distribution shift in network intrusion detection systems, where temporal changes in network traffic patterns degrade model performance. The authors introduce SwapCon, a novel approach that combines contrastive pretraining with a swapping augmentation strategy to learn shift-invariant feature representations. By pretraining on a large, unlabeled dataset and then fine-tuning on labeled data, SwapCon demonstrates significantly improved robustness to feature distribution shifts compared to traditional training methods and classical machine learning baselines.

## Method Summary
The authors propose SwapCon, a neural network model that leverages contrastive pretraining with a swapping augmentation strategy to mitigate feature distribution shifts in network intrusion detection. The method involves three key stages: numerical feature embedding (using strategies like Piecewise Linear Embedding), contrastive pretraining on unlabeled data to learn shift-invariant representations, and supervised fine-tuning on labeled data. The model is evaluated on the Kyoto2006+ dataset, which is split into IID, NEAR, and FAR sets to simulate different levels of feature distribution shift over time.

## Key Results
- SwapCon increases robustness against feature distribution shifts by over 8% compared to non-pretrained models
- SwapCon outperforms traditional ML models like XGBoost and KNN by a large margin, especially in scenarios with significant feature distribution shifts
- Model size affects generalization ability against feature distribution shifts, with smaller models showing better robustness in distant data (FAR split)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pretraining with contrastive learning helps the model learn shift-invariant feature representations.
- Mechanism: During pretraining, the model is trained on a large, unlabeled dataset to capture general patterns. The contrastive learning objective with swapping augmentation creates positive and negative pairs, forcing the model to learn representations that are invariant to feature order and temporal shifts. These learned representations are then fine-tuned for the specific intrusion detection task.
- Core assumption: The underlying patterns in network traffic data remain consistent over time, even if the specific feature distributions change.
- Evidence anchors:
  - [abstract] "compresses shift-invariant feature information during the pretraining stage and refines during the finetuning stage"
  - [section] "In pretraining, CL is leveraged to inject invariant latent features in the model weights in a self-supervised way"
  - [corpus] Weak evidence - no direct mentions of contrastive pretraining for intrusion detection in neighboring papers.
- Break condition: If the underlying network traffic patterns change significantly over time, the shift-invariant representations learned during pretraining may no longer be relevant.

### Mechanism 2
- Claim: Appropriate numerical feature embedding strategies enhance the performance of pretrained models.
- Mechanism: Different numerical embedding strategies (Exponential Binning, Piecewise Linear Embedding, Learnable Embedding) convert scalar features into higher-dimensional representations. PLE and LE provide more fine-grained representations that preserve magnitude relationships and allow for contextual learning, respectively. These enriched representations facilitate better learning during both pretraining and fine-tuning.
- Core assumption: The meaning and relationships of numerical features can be better captured through appropriate embedding techniques.
- Evidence anchors:
  - [abstract] "Moreover, we show how an adequate numerical embedding strategy also enhances the performance of pretrained models"
  - [section] "Guo et al. demonstrated that numerical feature embedding often brings better performance for ML models"
  - [corpus] No direct evidence in neighboring papers.
- Break condition: If the embedding strategy does not align with the data distribution or if the learned embeddings become outdated due to feature distribution shifts.

### Mechanism 3
- Claim: Model size affects the generalization ability against feature distribution shifts.
- Mechanism: Smaller models (fewer layers) with fixed pretrained weights show better robustness against feature distribution shifts in distant data (FAR split). This is because they are less prone to overfitting to the specific feature distributions in the training data. Larger models, while achieving higher performance on IID and NEAR splits, generalize worse on FAR splits.
- Core assumption: Simpler models are more robust to distribution shifts than complex models.
- Evidence anchors:
  - [section] "From the previous results, we found that although larger models can perform better in the IID and NEAR splits, they generalize worse in the FAR splits"
  - [section] "Tbl. II, as more layers are fixed, the model performance decreases in the IID and NEAR split. In the FAR split, the model performs better in the beginning but then worsens in the end"
  - [corpus] No direct evidence in neighboring papers.
- Break condition: If the feature distribution shifts are minimal or if the model is only deployed on data similar to the training data.

## Foundational Learning

- Concept: Contrastive Learning
  - Why needed here: Contrastive learning is used to create shift-invariant feature representations during pretraining. It helps the model learn to distinguish between similar and dissimilar samples, which is crucial for detecting intrusions in network traffic data with changing distributions.
  - Quick check question: How does contrastive learning differ from traditional supervised learning, and why is it beneficial for pretraining in scenarios with feature distribution shifts?

- Concept: Feature Distribution Shift
  - Why needed here: Feature distribution shift refers to the change in the distribution of features between the training and testing data. Understanding this concept is essential for designing models that are robust to temporal changes in network traffic patterns.
  - Quick check question: What are the potential causes of feature distribution shifts in network intrusion detection, and how can they impact the performance of machine learning models?

- Concept: Numerical Feature Embedding
  - Why needed here: Numerical feature embedding strategies are used to convert scalar features into higher-dimensional representations. This allows the model to capture more nuanced relationships and patterns in the data, which can improve its performance on the intrusion detection task.
  - Quick check question: How do different numerical embedding strategies (e.g., Exponential Binning, Piecewise Linear Embedding, Learnable Embedding) affect the representation of numerical features, and what are their trade-offs?

## Architecture Onboarding

- Component map:
  - SwapCon: A neural network model with a stack of blocks, each consisting of a linear layer followed by a Batch Normalization layer and an activation function (Tanh or ReLU).
  - Contrastive Pretraining: An unsupervised pretraining stage that uses contrastive learning with swapping augmentation to learn shift-invariant feature representations.
  - Fine-tuning: A supervised fine-tuning stage that adapts the pretrained model to the specific intrusion detection task.
  - Numerical Feature Embedding: Strategies to convert scalar features into higher-dimensional representations (EB, PLE, LE).

- Critical path:
  1. Data preprocessing and feature extraction from network traffic data.
  2. Numerical feature embedding using the chosen strategy (EB, PLE, or LE).
  3. Contrastive pretraining of the SwapCon model on the IID split.
  4. Fine-tuning of the pretrained SwapCon model on the training set.
  5. Evaluation of the fine-tuned model on the IID, NEAR, and FAR splits.

- Design tradeoffs:
  - Model size vs. robustness: Smaller models with fewer layers are more robust to feature distribution shifts but may have lower performance on IID and NEAR splits.
  - Numerical embedding strategy: Different embedding strategies have trade-offs in terms of representation quality and computational complexity.
  - Pretraining vs. training from scratch: Pretraining provides better initialization and robustness but requires additional unlabeled data and computational resources.

- Failure signatures:
  - Poor performance on the FAR split: Indicates that the model is not robust to feature distribution shifts and may be overfitting to the training data.
  - Degradation in performance on IID and NEAR splits when using a smaller model: Suggests that the model may be too simple to capture the complexity of the data.
  - Inconsistent results across different numerical embedding strategies: May indicate that the chosen embedding strategy is not well-suited for the data distribution.

- First 3 experiments:
  1. Train and evaluate the SwapCon model with different model sizes (number of layers) on the IID, NEAR, and FAR splits to assess the trade-off between performance and robustness.
  2. Compare the performance of the SwapCon model with different numerical embedding strategies (EB, PLE, LE) on the IID, NEAR, and FAR splits to determine the best embedding approach.
  3. Evaluate the SwapCon model with and without pretraining on the IID, NEAR, and FAR splits to quantify the benefits of contrastive pretraining in terms of robustness against feature distribution shifts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the relationship between model size and pretraining gain for feature distribution shift mitigation?
- Basis in paper: [explicit] The authors state "An in-depth study of the relationship between model size and pretraining gain remains for future work."
- Why unresolved: The paper only explores a limited range of model sizes and fixing strategies. The optimal model size for balancing performance across different distribution shifts is not determined.
- What evidence would resolve it: Systematic experiments varying model depth and width, testing performance on IID, NEAR, and FAR splits to identify the optimal architecture for robustness.

### Open Question 2
- Question: How does SwapCon perform on datasets with feature distribution shifts beyond the temporal domain?
- Basis in paper: [inferred] The study focuses on temporal distribution shifts in the Kyoto2006+ dataset. The authors don't test other types of distribution shifts.
- Why unresolved: The paper doesn't evaluate SwapCon on datasets with shifts due to different network topologies, attack types, or feature engineering changes.
- What evidence would resolve it: Testing SwapCon on multiple NID datasets with various types of distribution shifts and comparing performance to other methods.

### Open Question 3
- Question: What is the impact of different contrastive learning objectives on SwapCon's performance?
- Basis in paper: [explicit] The authors use NCE loss but note that "contrastive pretraining is applied with a different data augmentation method" compared to prior work.
- Why unresolved: The paper only experiments with one contrastive objective function. The impact of alternative objectives like InfoNCE or Barlow Twins is unknown.
- What evidence would resolve it: Comparing SwapCon variants using different contrastive objectives on the same dataset splits and measuring performance differences.

## Limitations
- The evaluation is based solely on the Kyoto2006+ dataset, which may not generalize to other network environments
- The paper does not explore the impact of different contrastive learning architectures or loss functions
- The computational overhead of contrastive pretraining versus training from scratch is not quantified

## Confidence

**High Confidence**: The core finding that contrastive pretraining with appropriate numerical embeddings improves robustness against feature distribution shifts is well-supported by the experimental results across multiple data splits (IID, NEAR, FAR). The comparison with traditional ML models (XGBoost, KNN) and the ablation studies on model size provide strong evidence for the effectiveness of the approach.

**Medium Confidence**: The specific mechanisms by which contrastive pretraining creates shift-invariant representations are plausible but not directly measured. While the swapping augmentation strategy is explained, the paper does not provide direct evidence that the learned representations are indeed invariant to the types of distribution shifts observed in the FAR split.

**Low Confidence**: The generalizability of the results to other intrusion detection datasets or real-world deployment scenarios is uncertain. The paper does not address how the model would perform on data with different characteristics or whether the pretraining benefits would persist in production environments with continuous data drift.

## Next Checks
1. **Dataset Generalization Test**: Evaluate SwapCon on at least two additional network intrusion detection datasets (e.g., CICIDS2017, UNSW-NB15) to verify that the performance gains and robustness to distribution shifts are not specific to Kyoto2006+.

2. **Temporal Drift Analysis**: Conduct a more granular analysis of performance degradation over time by creating multiple NEAR and FAR splits with varying temporal distances, and measure AUC scores to quantify the rate of performance decay.

3. **Pretraining Cost-Benefit Analysis**: Measure and compare the total training time, computational resources, and memory requirements for SwapCon with contrastive pretraining versus training from scratch on the same dataset, to assess the practical trade-offs of the approach.