---
ver: rpa2
title: A Conformal Prediction Score that is Robust to Label Noise
arxiv_id: '2405.02648'
source_url: https://arxiv.org/abs/2405.02648
tags:
- prediction
- noise
- conformal
- score
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of applying conformal prediction
  (CP) to neural networks when the validation set contains noisy labels, which is
  common in medical imaging where expert annotations may vary. The authors propose
  a noise-robust conformal score that estimates the noise-free score from noisy data
  by incorporating the known noise level.
---

# A Conformal Prediction Score that is Robust to Label Noise

## Quick Facts
- arXiv ID: 2405.02648
- Source URL: https://arxiv.org/abs/2405.02648
- Authors: Coby Penso; Jacob Goldberger
- Reference count: 17
- Primary result: Proposed noise-robust conformal score (NR-CP) significantly outperforms existing methods on medical imaging datasets with noisy labels

## Executive Summary
This paper addresses the challenge of applying conformal prediction (CP) to neural networks when validation sets contain noisy labels, which is common in medical imaging where expert annotations may vary. The authors propose a noise-robust conformal score that estimates the noise-free score from noisy data by incorporating the known noise level. Specifically, they modify the conformal score by combining the observed noisy score with an average over all possible classes weighted by the noise model. Experiments on four medical imaging datasets show that NR-CP significantly outperforms existing methods in terms of prediction set size while maintaining the required coverage probability.

## Method Summary
The method modifies the standard conformal prediction framework to handle label noise in the validation set. It introduces a noise-robust conformal score that estimates the noise-free score using the observed noisy score and the known noise level. The approach involves computing the expected noise-free score given the noisy observation, then using this estimated score during calibration to determine the conformal threshold. At test time, the original noise-free score is used to form prediction sets, ensuring both valid coverage and efficient prediction sets. The method is particularly designed for uniform label noise where with probability ε the true label is replaced by a random class uniformly chosen from all k classes.

## Key Results
- NR-CP achieves significantly smaller prediction set sizes than standard CP, Noisy-CP, and NRES-CP while maintaining target coverage (90%) on medical imaging datasets
- The method remains effective even when noise level must be estimated from data rather than being known
- NR-CP shows consistent performance across different conformal scores (APS, RAPS, HPS) and four medical imaging datasets (TissueMNIST, PathMNIST, HAM10000, OrganSMNIST)

## Why This Works (Mechanism)

### Mechanism 1
The noise-robust conformal score (NR-CP) maintains valid coverage even when the validation set contains uniform label noise. By estimating the expected noise-free score from the noisy score using the known noise level, the method corrects for the bias introduced by noisy labels during calibration. The expected noise-free score is computed as a weighted combination of the noisy score and the average score over all classes, with weights determined by the noise model. The mechanism fails if the noise model is misspecified (e.g., non-uniform noise) or if the noise level is severely underestimated.

### Mechanism 2
NR-CP produces smaller prediction sets than standard CP methods while maintaining the required coverage probability when labels are noisy. The estimated noise-free score is used during calibration to set a threshold that is more accurate than using the noisy score directly. At test time, the original (noise-free) conformal score is used to form the prediction set, which is smaller than using the estimated score directly. The mechanism breaks if the network's ranking of classes changes significantly between training with noisy labels and testing on clean data.

### Mechanism 3
NR-CP remains effective even when the noise level must be estimated from the data rather than being known. The method can be combined with state-of-the-art noise-robust network training techniques that estimate the noise level as part of the training process. This estimated noise level is then used in the noise-robust conformal score calculation. The mechanism fails if the noise level estimation is highly inaccurate, which could lead to miscalibration of the conformal score.

## Foundational Learning

- **Conformal Prediction (CP) framework**
  - Why needed here: CP is the underlying statistical framework that provides coverage guarantees for prediction sets. Understanding CP is essential to grasp how the noise-robust score modifies the standard CP approach.
  - Quick check question: What is the key difference between Adaptive Prediction Score (APS) and Homogeneous Prediction Sets (HPS) in CP?

- **Label noise models and their impact on model calibration**
  - Why needed here: The method explicitly addresses the problem of label noise in the validation set and assumes a specific noise model. Understanding different noise models and their effects on model performance is crucial for applying this method correctly.
  - Quick check question: How does uniform label noise affect the distribution of predicted probabilities from a neural network?

- **Expected value and conditional probability**
  - Why needed here: The noise-robust score is derived by computing the expected noise-free score given the noisy observation, which involves conditional probability calculations. A solid grasp of expected values is necessary to understand the score estimation process.
  - Quick check question: If the true label Y is uniformly distributed and the observed noisy label Ỹ follows a uniform noise model with parameter ε, what is P(Y=i|Ỹ=j)?

## Architecture Onboarding

- **Component map**: Neural network classifier -> Noise model estimator -> Conformal score calculator -> Calibration module -> Prediction set generator
- **Critical path**:
  1. Train classifier on noisy labeled data
  2. Estimate noise level ε (if unknown)
  3. Compute estimated noise-free scores on validation set
  4. Determine CP threshold qε from estimated scores
  5. At test time, compute original scores and form prediction sets using qε

- **Design tradeoffs**:
  - Using estimated noise-free scores for calibration vs. original scores for prediction: This approach ensures accurate calibration while maintaining small prediction sets at test time
  - Assuming uniform noise vs. modeling more complex noise patterns: Uniform noise assumption simplifies the score estimation but may not capture all real-world noise scenarios

- **Failure signatures**:
  - Prediction sets are much larger than expected despite correct noise level: This could indicate that the network's ranking of classes has changed significantly due to noisy training data
  - Coverage is consistently below the target level: This might suggest that the noise level is underestimated or the noise model is misspecified

- **First 3 experiments**:
  1. Apply NR-CP to a simple synthetic dataset with known uniform noise and compare prediction set sizes and coverage to standard CP methods
  2. Vary the noise level ε and evaluate how well NR-CP maintains coverage while keeping prediction sets small compared to Noisy-CP and NRES-CP
  3. Test NR-CP on a real medical imaging dataset (e.g., HAM10000) with artificially injected label noise and assess performance across different conformal scores (APS, RAPS, HPS)

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but raises several implicit ones through its limitations and scope. The method focuses on uniform label noise and marginal coverage guarantees, leaving questions about performance under more complex noise patterns and potential extensions to conditional conformal prediction methods.

## Limitations
- Assumes uniform label noise in validation set, which may not reflect real-world noise patterns in medical imaging datasets
- Performance depends critically on accurate noise level estimation, which can be challenging in practice
- Assumes network predictions maintain same ranking as true probabilities, which may break down with heavy noise

## Confidence

- **High confidence**: The theoretical framework for noise-robust score estimation and its mathematical derivation are sound and well-grounded in conformal prediction theory
- **Medium confidence**: Empirical results show improvement over baseline methods, but the experiments are limited to uniform noise and relatively small datasets
- **Low confidence**: Generalization to non-uniform noise patterns and real-world noisy annotations remains unproven

## Next Checks
1. Test the method on datasets with non-uniform label noise patterns (e.g., class-conditional noise, asymmetric noise) to evaluate robustness beyond the assumed uniform noise model
2. Evaluate performance when the noise level is severely underestimated or overestimated to determine sensitivity to noise level estimation errors
3. Apply the method to larger, more diverse medical imaging datasets with naturally occurring label noise to assess real-world applicability and potential failure modes