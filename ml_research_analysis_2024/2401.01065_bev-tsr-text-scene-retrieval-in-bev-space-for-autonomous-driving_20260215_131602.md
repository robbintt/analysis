---
ver: rpa2
title: 'BEV-TSR: Text-Scene Retrieval in BEV Space for Autonomous Driving'
arxiv_id: '2401.01065'
source_url: https://arxiv.org/abs/2401.01065
tags: []
core_contribution: This paper addresses the challenge of retrieving complex autonomous
  driving scenes using text descriptions. The authors propose BEV-TSR, a framework
  that leverages descriptive text to retrieve corresponding scenes in Bird's Eye View
  (BEV) space.
---

# BEV-TSR: Text-Scene Retrieval in BEV Space for Autonomous Driving

## Quick Facts
- arXiv ID: 2401.01065
- Source URL: https://arxiv.org/abs/2401.01065
- Authors: Tao Tang, Dafeng Wei, Zhengyu Jia, Tian Gao, Changwei Cai, Chengkai Hou, Peng Jia, Kun Zhan, Haiyang Sun, Jingchen Fan, Yixing Zhao, Fu Liu, Xiaodan Liang, Xianpeng Lang, Yang Wang
- Reference count: 40
- Key result: 85.78% and 87.66% top-1 accuracy on scene-to-text and text-to-scene retrieval respectively

## Executive Summary
BEV-TSR addresses the challenge of retrieving autonomous driving scenes using text descriptions by leveraging Bird's Eye View (BEV) features. The framework combines a frozen BEVFormer encoder for spatial feature extraction, a Llama2 LLM with LoRA fine-tuning for text processing, knowledge graph embeddings for semantic enhancement, and a Shared Cross-modal Prompt (SCP) module for alignment. The approach achieves state-of-the-art performance on the nuScenes-Retrieval dataset, demonstrating the effectiveness of BEV representations for complex scene understanding.

## Method Summary
BEV-TSR processes text descriptions through a Llama2 LLM with LoRA fine-tuning, concatenates the output with knowledge graph embeddings from a Graph Neural Network, and aligns these features with BEV features extracted from multi-view camera images using a frozen BEVFormer encoder. The Shared Cross-modal Prompt module maps both modalities into the same hidden space, enabling contrastive learning with caption generation as an auxiliary task. The method operates on the nuScenes dataset with caption complement strategy and QA augmentation.

## Key Results
- Achieves 85.78% top-1 accuracy for scene-to-text retrieval
- Achieves 87.66% top-1 accuracy for text-to-scene retrieval
- Outperforms existing methods on the nuScenes-Retrieval dataset
- Ablation studies confirm the importance of SCP and knowledge graph embeddings

## Why This Works (Mechanism)

### Mechanism 1
BEV features provide unified global representation of autonomous driving scenarios by aggregating spatial and temporal information from multiple camera views into a single Bird's Eye View representation. This captures global context that 2D images cannot represent adequately.

Core assumption: BEV features effectively encode global spatial relationships and dynamics of road participants critical for understanding complex autonomous driving scenes.

Break condition: If BEV feature extraction fails to capture critical spatial relationships or temporal dynamics, the global representation would be incomplete, leading to poor retrieval performance.

### Mechanism 2
Shared Cross-modal Embedding (SCP) effectively aligns BEV features with language embeddings by mapping both modalities into the same hidden space using learnable tokens. This enables direct comparison through cosine similarity, bridging the modality gap.

Core assumption: A shared embedding space can meaningfully represent both BEV features and language embeddings while preserving their respective semantics.

Break condition: If the learned embedding space fails to preserve critical semantic information from either modality, the alignment would be meaningless.

### Mechanism 3
Knowledge graph embeddings enhance semantic richness of text representations by incorporating structured domain knowledge about autonomous driving concepts. Graph neural network embeddings are concatenated with language model output.

Core assumption: The knowledge graph contains relevant and comprehensive information about autonomous driving scenarios that can supplement the language model's understanding.

Break condition: If the knowledge graph is incomplete or contains irrelevant information, concatenated embeddings could introduce noise rather than enhancing semantic understanding.

## Foundational Learning

- **Cross-modal contrastive learning**: Needed to train the model to associate text descriptions with corresponding BEV features by maximizing similarity between matching pairs and minimizing similarity between non-matching pairs. Quick check: How does contrastive learning differ from traditional supervised classification when dealing with paired text-image data?

- **BEV feature representation**: Understanding how BEV features capture spatial relationships and temporal dynamics is crucial for appreciating why they work better than 2D images for global scene understanding. Quick check: What specific spatial information does BEV representation preserve that is typically lost in 2D image projections?

- **Knowledge graph embeddings**: Understanding how structured domain knowledge can be incorporated into text representations to enhance semantic understanding beyond what language models can achieve alone. Quick check: How do knowledge graph embeddings differ from word embeddings in terms of capturing relationships between concepts?

## Architecture Onboarding

- **Component map**: Text → Language Encoder → Knowledge Graph → SCP → Contrastive Loss ← BEV Encoder ← Images

- **Critical path**: Text → Language Encoder → Knowledge Graph → SCP → Contrastive Loss ← BEV Encoder ← Images
  The critical path involves processing text through the language encoder and knowledge graph, then using SCP to align with BEV features from the image encoder.

- **Design tradeoffs**:
  - Frozen BEVFormer vs. fine-tuning: Using frozen parameters reduces training cost but may limit adaptation to retrieval-specific features
  - Knowledge graph vs. pure language model: Adding structured knowledge improves semantic richness but increases complexity
  - SCP vs. direct feature concatenation: SCP provides better alignment but requires learning additional parameters

- **Failure signatures**:
  - Low retrieval accuracy despite high training accuracy: Potential overfitting or misalignment in SCP
  - Disproportionate performance difference between B2T and T2B: Asymmetry in feature processing or loss calculation
  - Poor performance on specific scene types: Insufficient knowledge graph coverage for those scenarios

- **First 3 experiments**:
  1. Baseline test: Run the complete pipeline on a small subset of NuScenes data to verify all components work together
  2. Ablation study: Remove SCP to confirm its contribution to performance improvement
  3. Knowledge graph impact: Test with and without knowledge graph embeddings to measure semantic enhancement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does BEV-CLIP handle rare or long-tail scenarios in autonomous driving, and what are its limitations in this context?
- Basis: Paper mentions BEV-CLIP is designed to handle complex scenes and long-tail scenarios but lacks specific details on how it addresses these challenges or its limitations.
- What evidence would resolve it: Experimental results comparing BEV-CLIP's performance on common vs. rare scenarios, and analysis of its limitations in handling long-tail cases.

### Open Question 2
- Question: How does SCP contribute to alignment of BEV features and language embeddings, and what are its advantages over other cross-modal alignment techniques?
- Basis: Paper introduces SCP but does not provide detailed comparison with other alignment techniques.
- What evidence would resolve it: Detailed comparison of SCP with other alignment techniques including quantitative results and discussion of advantages.

### Open Question 3
- Question: How does knowledge graph prompting strategy improve semantic richness of language embedding, and what are potential drawbacks of this approach?
- Basis: Paper proposes using knowledge graph embeddings but does not discuss potential drawbacks or limitations.
- What evidence would resolve it: Analysis of potential drawbacks or limitations of knowledge graph prompting strategy including negative impacts on performance or scalability issues.

### Open Question 4
- Question: How does BEV-CLIP's performance on nuScenes-Retrieval dataset compare to other state-of-the-art retrieval methods, and what are key factors contributing to its success?
- Basis: Paper reports BEV-CLIP's performance but lacks detailed comparison with other state-of-the-art methods.
- What evidence would resolve it: Detailed comparison of BEV-CLIP's performance with other state-of-the-art retrieval methods on nuScenes-Retrieval dataset, including analysis of key factors contributing to success.

## Limitations

- Narrow scope of nuScenes dataset may not fully capture diversity of real-world autonomous driving scenarios
- Knowledge graph embeddings could introduce domain-specific biases limiting generalization to new environments
- SCP module introduces significant complexity without clear evidence that simpler alignment methods would not suffice
- Performance metrics may not demonstrate robustness across varying weather conditions, lighting scenarios, or rare edge cases

## Confidence

**High Confidence**: Core architectural design of using BEV features for global scene representation is well-established, and approach to aligning these with text embeddings through contrastive learning follows standard practices. Experimental methodology is transparent and reproducible.

**Medium Confidence**: Specific contribution of knowledge graph embeddings to performance improvements is supported by ablation studies but lacks comparative analysis against alternative semantic enhancement methods. Claim that SCP significantly improves alignment is supported by ablation results but underlying mechanism could be more rigorously explained.

**Low Confidence**: Claims about handling complex driving scenarios are based on performance metrics that may not translate to real-world robustness. Assumption that single knowledge graph can adequately represent all relevant autonomous driving concepts is not empirically validated across diverse driving environments.

## Next Checks

1. **Generalization Testing**: Evaluate BEV-TSR on diverse autonomous driving datasets (Waymo Open Dataset, Argoverse) to assess whether knowledge graph and SCP module maintain performance across different geographic regions, driving cultures, and environmental conditions.

2. **Ablation on Knowledge Graph Scope**: Systematically vary size and specificity of knowledge graph to determine optimal trade-off between semantic richness and noise introduction, including tests with domain-general knowledge graphs.

3. **Real-world Robustness**: Implement BEV-TSR in realistic simulation environment (CARLA) with varying weather, lighting, and rare traffic scenarios to validate whether top-1 accuracy translates to practical retrieval reliability in safety-critical situations.