---
ver: rpa2
title: 'Consistency Matters: Explore LLMs Consistency From a Black-Box Perspective'
arxiv_id: '2402.17411'
source_url: https://arxiv.org/abs/2402.17411
tags:
- arxiv
- llms
- evaluation
- consistency
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the problem of LLM consistency, which refers
  to the need for large language models to maintain unchanged internal parameters
  and capabilities across various stages of research and deployment. The authors propose
  a lightweight automated evaluation tool based on LightGBM to assess model consistency
  by analyzing response pairs.
---

# Consistency Matters: Explore LLMs Consistency From a Black-Box Perspective

## Quick Facts
- arXiv ID: 2402.17411
- Source URL: https://arxiv.org/abs/2402.17411
- Authors: Fufangchen Zhao; Guoqiang Jin; Jiaheng Huang; Rui Zhao; Fei Tan
- Reference count: 17
- Primary result: LightGBM-based tool achieves 72.41% consistency evaluation accuracy vs 55.86% manual evaluation and 48.97% GPT-3.5

## Executive Summary
This paper introduces the problem of LLM consistency, defined as the need for large language models to maintain unchanged internal parameters and capabilities across various stages of research and deployment. The authors propose a lightweight automated evaluation tool based on LightGBM to assess model consistency by analyzing response pairs. The tool uses traditional NLG metrics (ROUGE, BLEU, METEOR) and semantic similarity as features. Experiments show that LightGBM outperforms both manual evaluation and GPT-3.5 baselines, achieving significantly higher accuracy in consistency assessment.

## Method Summary
The authors propose a two-stage approach for LLM consistency evaluation. First, they generate response pairs by querying different LLMs or APIs with the same questions. Second, they extract features from these response pairs including ROUGE, BLEU, METEOR, and semantic similarity scores. A LightGBM model is trained to predict whether the underlying LLMs are consistent based on these features. The overall consistency score is calculated by aggregating predictions across multiple test cases, with a threshold mechanism to reduce noise from misleading response pairs.

## Key Results
- LightGBM achieves 72.41% overall consistency evaluation accuracy
- Manual evaluation baseline scores 55.86%
- GPT-3.5 zero-shot evaluation scores 48.97%
- Question features significantly improve evaluation performance
- Traditional NLG metrics combined with semantic similarity provide effective feature representation

## Why This Works (Mechanism)

### Mechanism 1
- LightGBM outperforms LLMs at consistency evaluation because LLMs lack model-level evaluation capability
- Core assumption: Surface-level similarity metrics capture differences between response pairs better than deep semantic understanding
- Evidence anchors: Paper states LLMs lack black-box model-level evaluation capability; LightGBM uses engineered features
- Break condition: High semantic similarity but different model origins may fail LightGBM's surface-level approach

### Mechanism 2
- Overall metric aggregates individual response pair evaluations to reduce noise
- Core assumption: Consistency is a property of the model as a whole, not individual responses
- Evidence anchors: Overall metric elevates evaluation to model level and significantly alleviates misleading response pairs
- Break condition: All response pairs in a test case are misleading or ambiguous

### Mechanism 3
- Question features critical for accurate consistency evaluation, especially for closed questions
- Core assumption: Question context determines expected similarity between responses
- Evidence anchors: Experiments show eliminating question features decreases all models' evaluation ability
- Break condition: Noisy or misleading question features introduce bias rather than improvement

## Foundational Learning

- Feature engineering for text similarity: LightGBM relies on traditional NLG metrics as features rather than raw text. Quick check: Can you calculate BLEU, ROUGE, and METEOR scores between two text passages?

- Binary classification with threshold tuning: LightGBM predicts consistency (True/False) and overall metric uses threshold λ. Quick check: How would you tune the threshold λ to balance precision and recall for this task?

- Aggregation of multiple predictions: Overall metric combines N test cases to produce final consistency score. Quick check: What aggregation method would you use if some test cases are more important than others?

## Architecture Onboarding

- Component map: Dataset construction → Feature extraction → LightGBM training → Evaluation tool deployment
- Critical path: 1) Generate response pairs using different LLMs/APIs 2) Calculate NLG metrics and semantic similarity 3) Train LightGBM model on labeled data 4) Use trained model to evaluate new response pairs 5) Calculate overall metric across test cases
- Design tradeoffs: Surface-level vs. deep semantic features; Model complexity vs. interpretability; Question dependence adds accuracy but complexity
- Failure signatures: High variance in predictions across similar response pairs; Low overall scores despite high individual pair scores; Bias toward positive or negative cases
- First 3 experiments: 1) Train LightGBM with and without question features to measure impact 2) Test different aggregation thresholds (λ, Λ) on validation set 3) Compare LightGBM performance against simple heuristic baselines

## Open Questions the Paper Calls Out

1. Can the consistency evaluation framework be extended to other domains beyond NLP, such as computer vision or speech recognition?

2. How does the choice of features (ROUGE, BLEU, METEOR, semantic similarity) impact the performance of the consistency evaluation model?

3. Can the consistency evaluation framework be adapted to handle more complex scenarios, such as evaluating the consistency of models that have been fine-tuned on different datasets or tasks?

4. How does the consistency of a model change over time as it is deployed and used in real-world applications?

5. Can the consistency evaluation framework be used to compare the consistency of different models or model architectures?

6. How does the size and complexity of the model impact its consistency?

7. Can the consistency evaluation framework be used to identify and diagnose issues with model deployment or fine-tuning?

8. How does the consistency of a model impact its performance on downstream tasks?

9. Can the consistency evaluation framework be used to evaluate the consistency of human-generated responses or annotations?

10. Are there other applications or use cases for the consistency evaluation framework beyond model deployment and monitoring?

## Limitations

- Relies heavily on surface-level similarity metrics that may not capture deeper semantic inconsistencies
- Dataset construction process and specific LightGBM hyperparameters are not fully detailed
- Does not address potential biases in the manual evaluation process serving as baseline

## Confidence

- High Confidence: Core finding that traditional NLG metrics combined with LightGBM effectively evaluate LLM consistency
- Medium Confidence: Claim that LLMs lack black-box model-level evaluation capability
- Low Confidence: Assertion that overall metric significantly reduces noise from misleading response pairs

## Next Checks

1. Cross-domain validation: Test LightGBM model on response pairs from different LLM architectures to verify generalizability

2. Error analysis: Systematically examine cases where LightGBM disagrees with manual evaluation to identify systematic biases

3. Semantic robustness test: Create response pairs with high semantic similarity but different model origins to test whether surface-level metrics are sufficient