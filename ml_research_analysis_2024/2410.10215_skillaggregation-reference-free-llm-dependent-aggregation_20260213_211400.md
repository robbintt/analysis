---
ver: rpa2
title: 'SkillAggregation: Reference-free LLM-Dependent Aggregation'
arxiv_id: '2410.10215'
source_url: https://arxiv.org/abs/2410.10215
tags:
- methods
- majority
- skillaggregation
- aggregation
- judges
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies aggregation methods, both context-free and context-dependent,
  in LLM-based evaluation. A new method called SkillAggregation is proposed, which
  learns to combine estimates from multiple LLM judges without needing additional
  data or ground truth.
---

# SkillAggregation: Reference-free LLM-Dependent Aggregation

## Quick Facts
- arXiv ID: 2410.10215
- Source URL: https://arxiv.org/abs/2410.10215
- Authors: Guangzhi Sun; Anmol Kagrecha; Potsawee Manakul; Phil Woodland; Mark Gales
- Reference count: 40
- Primary result: Achieved 4.9%, 1.3%, and 0.5% absolute accuracy improvements on HaluEval-Dialogue, TruthfulQA, and Chatbot Arena, respectively

## Executive Summary
This work addresses the challenge of aggregating predictions from multiple large language model (LLM) judges in evaluation tasks without requiring ground truth labels. The proposed SkillAggregation method extends the Crowdlayer aggregation framework to learn context-dependent weights for each LLM judge during inference. By incorporating a context encoder and regularization term, the approach learns to combine LLM estimates effectively while preventing over-confident skill estimates from dominating. The method demonstrates superior performance compared to standard aggregation techniques across three evaluation tasks.

## Method Summary
SkillAggregation learns to combine estimates from multiple LLM judges by mapping evaluation contexts to bottleneck representations, which are combined with learned skill-estimate vectors to predict each LLM's judgment. Unlike prior work, it doesn't require additional data or ground truth labels, making it a reference-free aggregation method. The approach extends Crowdlayer aggregation by exploiting judge estimates during inference and includes a regularization term to prevent over-confident skill estimates from dominating. The model is trained using cross-entropy loss on LLM judge outputs and can be applied to any binary or probabilistic LLM estimate aggregation problem.

## Key Results
- Outperforms Crowdlayer on all tested tasks (HaluEval-Dialogue, TruthfulQA, Chatbot Arena)
- Yields best performance over all approaches on majority of tasks
- Achieved 4.9%, 1.3%, and 0.5% absolute accuracy improvements on respective tasks
- Consistently outperforms DawidSkene and majority voting across different LLM judge subsets
- Mitigates positional bias effects in LLM judges

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SkillAggregation learns context-dependent weights for LLM judges that outperform static weighting methods.
- Mechanism: A context encoder maps each evaluation context to a bottleneck representation, which is combined with learned skill-estimate vectors to predict each LLM's judgment. This allows the aggregation to adapt weights based on the specific context rather than using fixed weights across all examples.
- Core assumption: The context encoder can learn a representation that captures information relevant to predicting LLM judgments, and that this representation is more informative than using the LLM judgments alone.
- Evidence anchors:
  - [abstract] "This method is more general as it can be applied to any problem where the LLM estimates are binary or probabilistic."
  - [section] "Unlike prior work, we do not prompt the judges for any information besides the estimates, nor does our algorithm need each judge to assess all others."
- Break condition: If the context encoder fails to capture task-relevant information or if LLM judgments are too noisy/correlated, the learned weights may not provide meaningful improvements over simple baselines.

### Mechanism 2
- Claim: The regularization term prevents over-confident skill estimates from dominating the aggregation.
- Mechanism: A regularization term penalizes large deviations of the sum of skill estimates from 1, preventing the model from learning extreme skill values that would overly weight certain LLM judgments.
- Core assumption: Some LLM judges produce over-confident but potentially incorrect judgments, and these need to be down-weighted during aggregation.
- Evidence anchors:
  - [section] "To mitigate this issue, a regularization term is proposed as shown below"
  - [section] "Without regularization, over-confident skill-estimate vectors are learned, dominating the posterior distribution and yielding a sub-optimal aggregation effect."
- Break condition: If the regularization parameter is set too high, it may prevent the model from learning meaningful skill differences between LLM judges, reducing the benefits of context-dependent weighting.

### Mechanism 3
- Claim: SkillAggregation uses posterior estimation during inference to combine context information with LLM judgments for better final predictions.
- Mechanism: After training, the model computes the posterior probability of each class given both the context representation and the actual LLM judgments, rather than just using the context representation alone.
- Core assumption: The LLM judgments contain additional information beyond what the context encoder captures, and combining both sources yields better predictions than either alone.
- Evidence anchors:
  - [section] "Posterior estimation enables us to produce better group estimates by using powerful LLM judgments in addition to the output of the bottleneck layer."
  - [section] "To derive the expression for the posterior distribution, we first make an assumption common in the crowdsourcing literature that the LLMs are conditionally independent given the same ground truth and context."
- Break condition: If the conditional independence assumption is violated (e.g., LLM judges make correlated errors), the posterior estimation may produce suboptimal results.

## Foundational Learning

- Concept: Conditional Independence Assumption
  - Why needed here: The posterior estimation formula assumes LLM judges are conditionally independent given the ground truth and context. This simplifies the computation of the posterior distribution.
  - Quick check question: If two LLM judges always make the same errors on certain types of questions, does the conditional independence assumption hold? (Answer: No)

- Concept: Bottleneck Layer in Neural Networks
  - Why needed here: The bottleneck layer forces the context encoder to compress information into a lower-dimensional representation, which the model uses to predict the ground truth distribution.
  - Quick check question: What is the purpose of using a bottleneck layer instead of using the full context representation? (Answer: To force the model to capture the most relevant information for predicting judgments)

- Concept: Cross-Entropy Loss for Training
  - Why needed here: The model is trained to minimize cross-entropy between predicted and actual LLM judgments, which encourages accurate prediction of each judge's behavior.
  - Quick check question: Why is cross-entropy loss appropriate for this task rather than mean squared error? (Answer: Because LLM judgments are probabilities that should be calibrated, and cross-entropy better handles probabilistic outputs)

## Architecture Onboarding

- Component map: Context encoder (GPT-2, RoBERTa, or Gemma-2B) → Bottleneck layer (2D output) → Skill-estimate vectors (K pairs of scalars) → Posterior estimation module (combines context and LLM judgments)
- Critical path: Context → Encoder → Bottleneck → Skill estimates → LLM judgment prediction → Posterior estimation → Final aggregation
- Design tradeoffs: Context-dependent weighting (more flexible but requires training) vs. context-independent methods (simpler but less adaptive); larger context encoders (potentially better representations but higher computational cost)
- Failure signatures: Degradation when dataset size is too small (context encoder cannot learn meaningful representations); performance plateaus when LLM judges are highly correlated; overfitting when regularization is too weak
- First 3 experiments:
  1. Replace majority voting with averaging probabilities as baseline to test if differential weighting helps
  2. Train with and without regularization term to observe impact on over-confident LLM judges
  3. Test with different context encoders (GPT-2, RoBERTa, Gemma-2B) to verify context-independence of the method

## Open Questions the Paper Calls Out
None

## Limitations
- Limited to binary classification tasks; performance on multi-class or continuous scoring remains untested
- Data requirements not explicitly analyzed; minimum dataset size for effective training is unknown
- Prompt sensitivity not investigated; performance may vary with different LLM judge prompting strategies

## Confidence

**High Confidence (8/10)**: The core mechanism of context-dependent weighting through SkillAggregation is well-supported by experimental results showing consistent improvements over baselines. The mathematical formulation and training procedure are clearly specified.

**Medium Confidence (6/10)**: The claim about mitigating positional bias is supported by ablation studies but requires more extensive analysis across different bias types and LLM judge configurations to establish robustness.

**Low Confidence (4/10)**: The assertion that performance is "agnostic to the choice of context encoder" is based on comparison with three specific models (GPT-2, RoBERTa, Gemma-2B). Testing with a broader range of context encoders would be needed to validate this claim more strongly.

## Next Checks

1. **Cross-task validation**: Apply SkillAggregation to non-binary classification tasks (e.g., multi-choice or continuous scoring) to test generalization beyond the current scope.

2. **Prompt sensitivity analysis**: Systematically vary the prompts used to elicit LLM judge outputs and measure the impact on aggregation performance to establish robustness to prompt engineering.

3. **Data efficiency study**: Train SkillAggregation on progressively smaller subsets of the training data to identify the minimum data requirements and characterize the performance degradation curve.