---
ver: rpa2
title: 'UNEX-RL: Reinforcing Long-Term Rewards in Multi-Stage Recommender Systems
  with UNidirectional EXecution'
arxiv_id: '2401.06470'
source_url: https://arxiv.org/abs/2401.06470
tags:
- recommender
- unex-rl
- systems
- learning
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of applying reinforcement learning
  to optimize long-term rewards in industrial multi-stage recommender systems, where
  different stages have different observation spaces. The authors propose UNEX-RL,
  a novel multi-agent reinforcement learning framework that uses unidirectional execution
  and a cascading information chain (CIC) approach to handle observation dependency
  and cascading effect problems.
---

# UNEX-RL: Reinforcing Long-Term Rewards in Multi-Stage Recommender Systems with UNidirectional EXecution

## Quick Facts
- arXiv ID: 2401.06470
- Source URL: https://arxiv.org/abs/2401.06470
- Reference count: 13
- Key outcome: 0.558% increase in users' usage time compared to single-agent RL algorithms in online A/B experiments with over 100 million users

## Executive Summary
This paper addresses the challenge of applying reinforcement learning to optimize long-term rewards in industrial multi-stage recommender systems, where different stages have different observation spaces. The authors propose UNEX-RL, a novel multi-agent reinforcement learning framework that uses unidirectional execution and a cascading information chain (CIC) approach to handle observation dependency and cascading effect problems. Practical variance reduction techniques like stopping gradient and category-quantile rescaling are also introduced. UNEX-RL shows significant improvement in optimizing long-term rewards compared to single-agent RL algorithms.

## Method Summary
UNEX-RL is a multi-agent reinforcement learning framework designed for multi-stage recommender systems. It uses a cascading information chain approach to handle observation dependency and cascading effect problems caused by unidirectional execution. The framework includes variance reduction techniques like stopping gradient and category-quantile rescaling. It employs a global critic that evaluates joint action-observation tuples and individual stage-specific policies for matching, pre-ranking, and ranking stages.

## Key Results
- 0.558% increase in users' usage time compared to single-agent RL algorithms
- Significant improvement in optimizing long-term rewards in online A/B tests with over 100 million users
- Better performance than traditional single-agent RL methods in multi-stage recommender systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unidirectional execution creates observation dependency and cascading effect problems that prevent standard CTDE from working.
- Mechanism: In multi-stage systems, actions at stage i affect candidate sets at stage i+1, so downstream agents cannot be trained with fixed observations from replay buffer. Actions also affect downstream observations, so policy gradients must account for this influence.
- Core assumption: Observations at each stage are deterministic functions of upstream actions and stage-specific inputs.
- Evidence anchors:
  - [abstract] "The unidirectional execution is a key feature of multi-stage recommender systems, bringing new challenges to the applications of multi-agent reinforcement learning (MARL), namely the observation dependency and the cascading effect."
  - [section] "The key problem is that CTDE assumes that observations of all the agents are simultaneously sampled from the replay buffer in the training stage, but in a multi-stage recommender system, even a slight change of the actions in upstream stages may cause different candidates in downstream stages, leading to different observations and actions in downstream stages."
  - [corpus] Weak: No direct neighbor papers discuss this specific OD/CE issue in MARL for recommender systems.
- Break condition: If observation generation becomes stochastic or non-deterministic, the cascading information chain approach would fail.

### Mechanism 2
- Claim: Cascading Information Chain (CIC) solves observation dependency by reconstructing downstream observations from upstream information and actions.
- Mechanism: CIC iteratively generates observations for each stage using only the first-stage observation and all upstream actions, eliminating the need to sample fixed downstream observations from replay buffer.
- Core assumption: Given first observation and all upstream actions, downstream observations are uniquely determined.
- Evidence anchors:
  - [section] "Based on this finding, we propose a cascade information chain (CIC) method... to obtain τ 1:N t and a1:N t only from the first observation τ 1 t."
  - [section] "Via CIC, the critic learning can be formulated as: L(ϕ) = ED[(yg − Qg(τ 1:N t , a1:N t ; ϕ))2]"
  - [corpus] Weak: No direct neighbor papers discuss CIC approach.
- Break condition: If information extraction process P i is non-deterministic or if upstream actions don't fully determine downstream observations.

### Mechanism 3
- Claim: Category Quantile Rescaling (CQR) reduces variance by normalizing rewards to uniform distribution within user/item groups.
- Mechanism: CQR transforms raw rewards into quantiles based on user-item group distributions, creating more stable reward signals for training.
- Core assumption: Reward variance primarily comes from user and item biases rather than content quality differences.
- Evidence anchors:
  - [section] "The CQR reward is the quantile of the reward conditioned on Gu and Gi: ˜rt,ui = Φ (rt,ui|Gu, Gi)"
  - [section] "CQR shows that the range of ˜rt,ui will be moderate compared with rt, which can reduce the variance of critic learning."
  - [corpus] Weak: No direct neighbor papers discuss this specific CQR variance reduction technique.
- Break condition: If user/item biases are not the primary source of reward variance, CQR would provide minimal benefit.

## Foundational Learning

- Concept: Multi-agent reinforcement learning with centralized training and decentralized execution (CTDE)
  - Why needed here: UNEX-RL extends CTDE to handle the unique challenges of multi-stage recommender systems
  - Quick check question: What are the key differences between standard CTDE and the CIC approach used in UNEX-RL?

- Concept: Markov Decision Processes and policy gradient methods
  - Why needed here: The framework optimizes long-term rewards through sequential decision making
  - Quick check question: How does the reward structure in recommender systems differ from typical MDP formulations?

- Concept: Variance reduction techniques in reinforcement learning
  - Why needed here: Large reward variance from user/item biases can destabilize training
  - Quick check question: What are alternative variance reduction techniques commonly used in RL, and how do they compare to CQR?

## Architecture Onboarding

- Component map:
  Stage agents (matching, pre-ranking, ranking) with individual policies -> Global critic Qg that evaluates joint action-observation tuples -> CIC module that reconstructs observations from upstream information -> Variance reduction modules (SG and CQR) -> Replay buffer for experience storage

- Critical path:
  1. User request triggers session
  2. First-stage observation processed by CIC
  3. CIC reconstructs full observation-action sequence
  4. Global critic evaluates Q-value
  5. Policy gradients computed via CIC-based backprop
  6. Actions executed through stage-specific selection functions

- Design tradeoffs:
  - CIC adds computational overhead but enables proper credit assignment
  - SG prevents gradient explosion but may slow learning
  - CQR reduces variance but adds preprocessing complexity

- Failure signatures:
  - Training instability suggests CIC reconstruction errors
  - Poor performance indicates variance reduction techniques aren't effective
  - Degraded user metrics suggest policy optimization problems

- First 3 experiments:
  1. Validate CIC reconstruction by comparing reconstructed vs actual observations
  2. Test variance reduction impact by comparing training stability with/without CQR
  3. Evaluate multi-stage vs single-stage performance to confirm cooperative benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of UNEX-RL compare to single-agent RL methods when the number of stages increases beyond three?
- Basis in paper: [inferred] The paper shows that UNEX-RL outperforms single-agent RL methods in a three-stage recommender system. However, it does not explore the performance of UNEX-RL when the number of stages increases.
- Why unresolved: The paper only experiments with a three-stage recommender system and does not provide insights into how UNEX-RL would perform in a system with more stages.
- What evidence would resolve it: Conducting experiments with a recommender system that has more than three stages and comparing the performance of UNEX-RL with single-agent RL methods would provide evidence to resolve this question.

### Open Question 2
- Question: How does the performance of UNEX-RL change when using different variance reduction techniques?
- Basis in paper: [explicit] The paper introduces stopping gradient (SG) and category-quantile rescaling (CQR) as variance reduction techniques for UNEX-RL. However, it does not explore the performance of UNEX-RL when using different combinations or variations of these techniques.
- Why unresolved: The paper only provides results for UNEX-RL with SG and CQR, but does not explore other possible variance reduction techniques or combinations.
- What evidence would resolve it: Conducting experiments with different combinations or variations of variance reduction techniques and comparing the performance of UNEX-RL would provide evidence to resolve this question.

### Open Question 3
- Question: How does the performance of UNEX-RL change when using different reward functions?
- Basis in paper: [inferred] The paper uses watch time and session length as reward functions for evaluating UNEX-RL. However, it does not explore the performance of UNEX-RL when using different reward functions.
- Why unresolved: The paper only experiments with watch time and session length as reward functions and does not provide insights into how UNEX-RL would perform with different reward functions.
- What evidence would resolve it: Conducting experiments with different reward functions and comparing the performance of UNEX-RL would provide evidence to resolve this question.

## Limitations
- Framework relies heavily on deterministic observation generation in multi-stage recommender systems
- Computational overhead of CIC reconstruction could become prohibitive in systems with many stages
- Limited validation from the paper alone for some of the proposed mechanisms

## Confidence
- Mechanism 1 (OD/CE problems): High confidence - well-established issue with clear theoretical grounding
- Mechanism 2 (CIC solution): Medium confidence - novel approach with limited validation from the paper alone
- Mechanism 3 (CQR variance reduction): Medium confidence - reasonable technique but limited comparative analysis

## Next Checks
1. Validate CIC reconstruction accuracy by measuring reconstruction error rates on held-out data
2. Conduct ablation studies to quantify the individual contributions of SG and CQR techniques
3. Test framework robustness by introducing controlled stochasticity in observation generation