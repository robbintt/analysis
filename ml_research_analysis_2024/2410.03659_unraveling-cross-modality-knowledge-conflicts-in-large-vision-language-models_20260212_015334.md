---
ver: rpa2
title: Unraveling Cross-Modality Knowledge Conflicts in Large Vision-Language Models
arxiv_id: '2410.03659'
source_url: https://arxiv.org/abs/2410.03659
tags:
- knowledge
- answer
- visual
- conflicts
- conflict
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates cross-modality parametric knowledge conflicts
  in large vision-language models (LVLMs). These conflicts arise due to inconsistencies
  between the visual and language components of LVLMs, which are separately trained
  on different data.
---

# Unraveling Cross-Modality Knowledge Conflicts in Large Vision-Language Models

## Quick Facts
- arXiv ID: 2410.03659
- Source URL: https://arxiv.org/abs/2410.03659
- Authors: Tinghui Zhu; Qin Liu; Fei Wang; Zhengzhong Tu; Muhao Chen
- Reference count: 23
- Key outcome: Dynamic contrastive decoding improves accuracy by 2.24% on LLaVA-34B for resolving cross-modality knowledge conflicts

## Executive Summary
This paper investigates cross-modality parametric knowledge conflicts in large vision-language models (LVLMs), which arise due to inconsistencies between separately trained visual and language components. The authors propose a systematic approach to detect, interpret, and mitigate these conflicts through a pipeline that compares answers from visual and textual inputs. They introduce a contrastive metric for effective conflict detection and propose mitigation strategies including dynamic contrastive decoding that removes unreliable logits based on answer confidence, along with prompt-based approaches for models without accessible logits.

## Method Summary
The authors propose a systematic approach to detect, interpret, and mitigate cross-modality parametric knowledge conflicts in LVLMs. They introduce a pipeline to identify conflicts by comparing answers from visual and textual inputs using a contrastive metric. For mitigation, they develop dynamic contrastive decoding that removes unreliable logits based on answer confidence, and two prompt-based strategies for models without accessible logits. The methods are evaluated on ViQuAE and InfoSeek datasets, demonstrating promising improvements in accuracy.

## Key Results
- Dynamic contrastive decoding method improves average accuracy by 2.24% using LLaVA-34B
- Proposed contrastive metric effectively distinguishes conflicting samples
- Mitigation methods show promising improvements on ViQuAE and InfoSeek datasets

## Why This Works (Mechanism)
The proposed approach works by identifying discrepancies between visual and textual knowledge representations in LVLMs. By comparing outputs from vision-only and text-only inputs, the method can detect when the model's visual and language components provide conflicting information. The dynamic contrastive decoding then selectively filters out unreliable predictions based on confidence scores, effectively resolving these conflicts by prioritizing more trustworthy modality-specific knowledge.

## Foundational Learning
- Cross-modality knowledge conflicts: Discrepancies between visual and language components in LVLMs that arise from separate training on different data types. Needed to understand the core problem being addressed.
- Contrastive metric: A quantitative measure to distinguish conflicting samples by comparing outputs from different modalities. Needed to identify conflicts systematically.
- Dynamic contrastive decoding: A method that removes unreliable logits during inference based on confidence scores. Needed to implement the mitigation strategy.
- Logit filtering: The process of selectively removing or down-weighting certain output predictions. Needed to understand how the mitigation works.
- Multimodal reasoning: The process by which LVLMs combine information from multiple modalities to generate answers. Needed to distinguish between genuine conflicts and legitimate multimodal reasoning.

## Architecture Onboarding

Component Map: Vision Encoder -> Language Model -> Contrastive Detector -> Conflict Resolver

Critical Path: Image input → Vision encoder → Cross-modal fusion → Language model → Output generation → Conflict detection → Mitigation

Design Tradeoffs: The approach trades computational overhead during inference for improved accuracy by adding conflict detection and mitigation steps. This represents a post-hoc solution rather than addressing conflicts during training.

Failure Signatures: The system may fail when conflicts arise from legitimate multimodal reasoning rather than parametric inconsistencies, potentially removing valid information. It may also struggle with edge cases where both modalities are unreliable.

First Experiments:
1. Test conflict detection on synthetic question-answer pairs with known discrepancies
2. Evaluate mitigation effectiveness on controlled conflict scenarios
3. Measure computational overhead of dynamic contrastive decoding compared to baseline inference

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation framework relies on synthetic question-answer pairs that may not fully capture real-world ambiguity patterns
- The conflict identification process assumes all discrepancies indicate genuine conflicts, potentially missing cases of legitimate multimodal reasoning
- Mitigation methods focus on post-hoc filtering rather than addressing root causes in training data or architecture
- Evaluation is limited to two datasets (ViQuAE and InfoSeek), which may not represent full diversity of knowledge conflict scenarios

## Confidence
High Confidence: The existence of cross-modality knowledge conflicts is well-supported through systematic analysis and multiple detection methods. The 2.24% accuracy improvement with dynamic contrastive decoding on LLaVA-34B is statistically significant and methodologically sound.

Medium Confidence: The proposed contrastive metric effectively distinguishes conflicting samples in controlled settings, though its generalizability to more complex, real-world scenarios requires further validation. The logit filtering mechanism appears robust but may have edge cases where it removes useful information.

Low Confidence: The claim that all detected discrepancies represent genuine knowledge conflicts (rather than legitimate multimodal reasoning differences) is not fully substantiated. The prompt-based mitigation strategies show promise but lack comprehensive evaluation across different model architectures and conflict types.

## Next Checks
1. Conduct ablation studies to determine whether the observed accuracy improvements stem primarily from conflict mitigation or from other factors such as increased computational budget during inference.

2. Test the mitigation methods on additional datasets representing diverse knowledge domains and conflict types, particularly focusing on cases where vision and language should legitimately disagree (e.g., optical illusions or linguistic ambiguities).

3. Implement a human evaluation framework to distinguish between genuine knowledge conflicts and legitimate multimodal reasoning differences, validating whether the automated conflict detection captures meaningful patterns.