---
ver: rpa2
title: Cross-lingual Named Entity Corpus for Slavic Languages
arxiv_id: '2404.00482'
source_url: https://arxiv.org/abs/2404.00482
tags:
- named
- entity
- should
- entities
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a cross-lingual named entity corpus for six
  Slavic languages, including Bulgarian, Czech, Polish, Slovenian, Russian, and Ukrainian.
  The corpus consists of 5,017 documents on seven topics, annotated with five classes
  of named entities: person, organization, location, event, and product.'
---

# Cross-lingual Named Entity Corpus for Slavic Languages

## Quick Facts
- **arXiv ID**: 2404.00482
- **Source URL**: https://arxiv.org/abs/2404.00482
- **Reference count**: 40
- **Primary result**: Transformer-based models achieved micro-averaged F-score of 0.8503 (single topic out) and 0.9222 (cross topics) on cross-lingual NER for Slavic languages

## Executive Summary
This paper presents a cross-lingual named entity corpus for six Slavic languages: Bulgarian, Czech, Polish, Slovenian, Russian, and Ukrainian. The corpus contains 5,017 documents across seven topics, manually annotated with five named entity classes (person, organization, location, event, product). Each entity is annotated with a category, lemma, and unique cross-lingual identifier to enable entity linking across languages. The authors provide two dataset splits - single topic out and cross topics - and establish baseline performance using transformer-based models (XLM-RoBERTa-large for NER, mT5-large for lemmatization), achieving strong F-scores of 0.8503 and 0.9222 respectively.

## Method Summary
The corpus was constructed by collecting news articles from various sources on seven topics (Brexit, Asia Bibi, Nord Stream, Ryanair, Covid-19, US 2020 Elections, Russia-Ukraine War) and manually annotating named entities using the Inforex web platform. Each entity mention was annotated with its surface form, lemma, cross-lingual identifier, and positional information. Two dataset splits were created: single topic out (testing generalization to unseen topics) and cross topics (evaluating performance on entities appearing across multiple topics). Baseline models were implemented using transformer architectures with XLM-RoBERTa-large for named entity recognition and categorization, and mT5-large for lemmatization and linking.

## Key Results
- Micro-averaged F-score of 0.8503 for named entity recognition on single topic out split
- Micro-averaged F-score of 0.9222 for named entity recognition on cross topics split
- Lemmatization accuracy of 96.13% (cross topics) and 88.89% (single topic out)
- Entity linking accuracy of 87.84% (cross topics) and 68.75% (single topic out)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-lingual entity linking enables entity disambiguation across Slavic languages with varying morphologies.
- Mechanism: Entities are annotated with a unique cross-lingual identifier that remains consistent across all language variants, allowing models to link mentions like "Donald Tusk" in Polish and "Дональд Туск" in Russian to the same entity.
- Core assumption: Surface forms in different Slavic languages referring to the same entity share a common cross-lingual identifier in the corpus.
- Evidence anchors:
  - [abstract]: "Each entity is described by a category, a lemma, and a unique cross-lingual identifier."
  - [section]: "Each mention of an entity is described with a category, a lemma, a cross-language identifier and positional information."
- Break condition: If cross-lingual identifiers are inconsistently assigned or missing, the linking mechanism fails.

### Mechanism 2
- Claim: Transformer-based multilingual models (XLM-RoBERTa-large, mT5-large) effectively handle morphological complexity in Slavic languages for NER tasks.
- Mechanism: Pre-trained multilingual transformers are fine-tuned on the Slavic NER corpus, leveraging their ability to capture subword patterns and contextual embeddings across languages.
- Core assumption: Multilingual transformers pre-trained on diverse languages can generalize to Slavic languages' inflectional morphology.
- Evidence anchors:
  - [abstract]: "For each split, we set benchmarks using a transformer-based neural network architecture with the pre-trained multilingual models —XLM-RoBERTa-largefor named entity mention recognition and categorization, andmT5-large for named entity lemmatization and linking."
  - [section]: "The baseline model follows the transformer-based neural network architecture presented by Devlin et al. (2019). The model consists of three core component's: a pre-trained language model, a dropout layer, and a linear layer that performs to-ken classification."
- Break condition: If the multilingual model's vocabulary doesn't cover Slavic language subwords adequately, performance degrades.

### Mechanism 3
- Claim: Topic-based dataset splits (single topic out vs cross topics) provide complementary evaluation of model generalization vs in-domain performance.
- Mechanism: Single-topic-out split tests model ability to recognize entities in unseen topics, while cross-topic split evaluates performance on entities appearing across multiple topics.
- Core assumption: Entities specific to certain topics (like "Asia Bibi") appear predominantly in documents about those topics.
- Evidence anchors:
  - [abstract]: "We provide two train-tune dataset splits—single topic out and cross topics."
  - [section]: "For the single topic out split, the baseline models achieved a micro-averaged F-score of 0.8503 across all language categories and categories."
  - [corpus]: The corpus statistics show distinct entity distributions across the seven topics (see Tables 12-18).
- Break condition: If entity distributions overlap significantly across topics, the distinction between the two splits becomes less meaningful.

## Foundational Learning

- Concept: Named Entity Recognition (NER)
  - Why needed here: NER is the fundamental task of identifying and classifying named entities in text, which this corpus directly supports.
  - Quick check question: What are the five entity categories annotated in this corpus?

- Concept: Cross-lingual entity linking
  - Why needed here: This corpus provides cross-lingual identifiers, enabling models to link entity mentions across different Slavic languages.
  - Quick check question: How does the corpus ensure mentions of the same entity across languages get the same identifier?

- Concept: Multilingual transformer models
  - Why needed here: The baseline models use XLM-RoBERTa-large and mT5-large, which are essential for handling multiple Slavic languages simultaneously.
  - Quick check question: What are the two pre-trained multilingual models used for NER and lemmatization tasks?

## Architecture Onboarding

- Component map: HTML parsing -> Text extraction -> Entity annotation (Inforex) -> Cross-lingual ID assignment -> Dataset split creation -> Model training (XLM-RoBERTa-large, mT5-large) -> Evaluation

- Critical path:
  1. Document collection and preprocessing
  2. Entity annotation with Inforex (surface form, lemma, cross-lingual ID)
  3. Dataset split creation (single-topic-out and cross-topic)
  4. Model training with transformers
  5. Evaluation on test splits

- Design tradeoffs:
  - Using multilingual transformers vs. language-specific models: Better cross-lingual generalization but potentially lower performance on individual languages
  - Including PRO and EVT categories vs. standard PER/ORG/LOC: More comprehensive coverage but increased annotation complexity
  - Manual annotation vs. automatic methods: Higher quality but more resource-intensive

- Failure signatures:
  - Low F1-scores on single-topic-out split indicate poor generalization to unseen topics
  - High discrepancy between cross-topic and single-topic-out performance suggests overfitting to topic-specific entities
  - Inconsistent cross-lingual IDs across languages indicate annotation errors

- First 3 experiments:
  1. Train XLM-RoBERTa-large on cross-topic split and evaluate on single-topic-out split to measure generalization
  2. Train mT5-large for lemmatization on cross-topic split and measure accuracy improvement over majority baseline
  3. Train separate monolingual models for each Slavic language and compare performance to multilingual approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the baseline models on this corpus compare to models trained on other Slavic NER corpora, particularly for the less common entity types like PRO and EVT?
- Basis in paper: [inferred] The paper states that "given that the taxonomy used for annotating the presented corpus is at least to some extent unique, a direct comparison to models trained on other Slavic NER corpora is not straightforward as well."
- Why unresolved: The authors acknowledge the unique taxonomy but do not provide any comparative analysis with existing Slavic NER corpora.
- What evidence would resolve it: A direct comparison of the baseline model results on this corpus with results obtained from models trained on other Slavic NER corpora, particularly focusing on the performance on PRO and EVT entity types.

### Open Question 2
- Question: How do the annotation guidelines for resolving ambiguities between ORG vs. PER and PRO vs. ORG categories impact the quality and consistency of the annotations across different languages?
- Basis in paper: [explicit] The paper mentions that "there were disagreements related, in particular, to the ambiguities between ORG vs. PER, and PRO vs. ORG in terms of deciding on the NE type" and provides specific disambiguation rules.
- Why unresolved: While the paper provides disambiguation rules, it does not evaluate the effectiveness of these rules in improving annotation consistency across languages.
- What evidence would resolve it: An analysis of the inter-annotator agreement scores for different entity types across languages, comparing the results before and after the implementation of the disambiguation rules.

### Open Question 3
- Question: What is the impact of the single-topic-out split on the model's ability to generalize to new topics, and how does it compare to the cross-topic split in terms of model performance?
- Basis in paper: [explicit] The paper states that the single-topic-out split "evaluates model generality, i.e., how it will perform on new topics" and provides baseline results for both splits.
- Why unresolved: The paper provides baseline results for both splits but does not analyze the impact of the single-topic-out split on model generalization in detail.
- What evidence would resolve it: A detailed analysis of the model performance on the single-topic-out split, comparing it with the cross-topic split, and identifying specific challenges or patterns in the model's ability to generalize to new topics.

## Limitations

- Corpus size of 5,017 documents may be insufficient for robust training on morphologically complex Slavic languages
- Topic-specific nature of the corpus could lead to overfitting to domain-specific entities
- Limited comparison with existing Slavic NER corpora due to unique annotation taxonomy

## Confidence

**High Confidence**: Technical description of corpus construction process, annotation scheme, entity categories, and dataset splits. Use of standard transformer architectures (XLM-RoBERTa and mT5) for baseline models.

**Medium Confidence**: Reported baseline performance metrics, which depend on specific implementation details not fully described in the paper (e.g., exact preprocessing, tokenization, hyperparameter settings).

**Low Confidence**: Claim that this corpus will significantly advance cross-lingual NER research for Slavic languages, requiring broader community adoption and comparison with existing resources.

## Next Checks

1. **Cross-lingual Identifier Audit**: Manually verify a sample of entity mentions across multiple languages to confirm that cross-lingual identifiers are consistently and correctly assigned.

2. **Baseline Reproduction**: Implement the exact baseline model architecture (XLM-RoBERTa-large for NER, mT5-large for lemmatization) using the provided dataset and compare the reproduced results with the reported metrics.

3. **Generalization Stress Test**: Design an experiment that systematically removes documents containing entities from specific topics to evaluate whether the model can truly generalize to unseen entity types.