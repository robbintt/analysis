---
ver: rpa2
title: 'Learning to Reduce: Optimal Representations of Structured Data in Prompting
  Large Language Models'
arxiv_id: '2402.14195'
source_url: https://arxiv.org/abs/2402.14195
tags:
- language
- context
- input
- data
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Learning to Reduce, a method to fine-tune language
  models to generate reduced context by identifying relevant evidence for downstream
  tasks. The approach uses on-policy reinforcement learning to train a model that
  reduces structured data like tables into relevant rows and columns, aiming to improve
  the performance of fixed large language models on downstream tasks.
---

# Learning to Reduce: Optimal Representations of Structured Data in Prompting Large Language Models

## Quick Facts
- arXiv ID: 2402.14195
- Source URL: https://arxiv.org/abs/2402.14195
- Authors: Younghun Lee; Sungchul Kim; Tong Yu; Ryan A. Rossi; Xiang Chen
- Reference count: 8
- Key outcome: Method achieves comparable performance in selecting relevant evidence and exhibits better generalizability on unseen data compared to baselines like RoBERTa and GPT-4

## Executive Summary
This paper proposes Learning to Reduce, a method to fine-tune language models to generate reduced context by identifying relevant evidence for downstream tasks. The approach uses on-policy reinforcement learning to train a model that reduces structured data like tables into relevant rows and columns, aiming to improve the performance of fixed large language models on downstream tasks. Experiments on the WikiTableQuestions dataset show the proposed method achieves comparable performance in selecting relevant evidence and exhibits better generalizability on unseen data compared to baselines like RoBERTa and GPT-4. The reduced context generated by the model helps improve the accuracy of GPT-4 on table QA tasks, especially when the context is lengthy.

## Method Summary
The method involves fine-tuning sequence-to-sequence language models (FLAN-T5-Large) using on-policy reinforcement learning with PPO to generate reduced context from structured data. The approach trains separate models for column and row reduction on the WikiTableQuestions dataset with SQUALL text-to-SQL annotations. The RL objective uses rewards for correct selections and penalties for missing relevant items, with a KL divergence term to prevent excessive deviation from the initial fine-tuned policy. The reduced context is then used to prompt a fixed LLM (GPT-4) for the final question answering task.

## Key Results
- Learning to Reduce achieves comparable accuracies in selecting relevant evidence from input contexts
- The method shows better generalizability on different datasets compared to baselines
- Reduced context helps improve GPT-4's performance on downstream tasks, especially when context is long

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The RL-based context reduction improves LLM reasoning accuracy on long structured data by providing more focused input.
- Mechanism: A fine-tuned language model is trained using on-policy reinforcement learning to generate a reduced version of the input context (table rows/columns) based on the task description and question. The model receives rewards for selecting relevant items and penalties for missing necessary information or including irrelevant data. This reduced context is then used to prompt a fixed LLM for the final task.
- Core assumption: A smaller, more relevant context improves the LLM's ability to reason accurately compared to the full, potentially lengthy context.
- Evidence anchors:
  - [abstract] "Experimental results illustrate that our model not only achieves comparable accuracies in selecting the relevant evidence from an input context, but also shows generalizability on different datasets. We further show that our model helps improve the LLM's performance on downstream tasks especially when the context is long."
  - [section] "We empirically show that our model shows comparable performance in reducing the input context on a widely used table QA dataset, WikiTableQuestions (Pasupat and Liang, 2015). Additionally, our model outperforms other baseline models including the most recent GPT-4 (OpenAI, 2023b) on the robustness test on unseen data distribution."

### Mechanism 2
- Claim: Fine-tuning the context reduction model on a dataset with relevant evidence annotations (like text-to-SQL annotations) enables it to learn effective selection criteria.
- Mechanism: The model is first fine-tuned on the WikiTableQuestions dataset with SQUALL text-to-SQL annotations to identify relevant rows and columns. This is done by iteratively removing rows/columns and checking if SQL queries still generate correct answers. The model learns to generate sequences of relevant columns and rows.
- Core assumption: Text-to-SQL annotations provide sufficient signal for the model to learn which parts of structured data are relevant for answering questions.
- Evidence anchors:
  - [section] "We use a table QA dataset which has a text-to-SQL annotation to automatically identify the relevant evidence. The general idea of annotation is executing SQL queries while iteratively removing rows and columns from the input tables; if executing a SQL query can generate an answer on the table even after removing some rows and columns, then the removed items are irrelevant to the input question."

### Mechanism 3
- Claim: The KL divergence penalty in the RL objective prevents excessive deviation from the initial fine-tuned policy, stabilizing training.
- Mechanism: During RL training, the model receives a reward based on its selection accuracy (positive for correct, negative for incorrect). A KL divergence term penalizes the policy for deviating too much from the initial fine-tuned model (π0 = θLM). The coefficient β is dynamically adjusted based on the KL divergence to control the strength of this penalty.
- Core assumption: Maintaining some similarity to the initial fine-tuned model prevents catastrophic forgetting and ensures the RL updates are incremental improvements rather than drastic changes.
- Evidence anchors:
  - [section] "Following the existing RL approaches on NLP applications (Ziegler et al., 2019), we employ KL divergence penalty rewards that dynamically adapt the coefficient β in different time steps to minimize excessive parameter updates from the initial policy."

## Foundational Learning

- Concept: Reinforcement Learning (RL) - specifically on-policy learning and policy optimization
  - Why needed here: The context reduction task is framed as a sequential decision-making problem where the model generates tokens representing relevant evidence. RL provides a framework to optimize the model's policy based on task-specific rewards (accuracy of reduced context).
  - Quick check question: What is the difference between on-policy and off-policy RL, and why is on-policy learning appropriate for this context reduction task?

- Concept: Text-to-SQL parsing and semantic parsing
  - Why needed here: The method relies on text-to-SQL annotations to automatically identify which rows and columns are relevant for answering questions. Understanding how semantic parsing works and how text-to-SQL annotations are generated is crucial for grasping the training data preparation.
  - Quick check question: How do text-to-SQL annotations indicate which parts of a table are relevant for answering a specific question?

- Concept: Prompt engineering and context management for LLMs
  - Why needed here: The ultimate goal is to improve LLM performance on structured data by managing the context provided in the prompt. Understanding the limitations of LLMs with long contexts and how prompt engineering can mitigate these issues is fundamental.
  - Quick check question: Why does providing a lengthy context to an LLM potentially degrade its performance on a task, even if the answer is present in the context?

## Architecture Onboarding

- Component map: Question + Original context/table -> Column reduction model -> Row reduction model -> Reduced context -> Fixed LLM (GPT-4) -> Answer

- Critical path: 1. Receive question and table 2. Pass through column reduction model 3. Pass through row reduction model (using reduced columns) 4. Generate reduced context 5. Prompt fixed LLM (GPT-4) with reduced context and question 6. Return LLM's answer

- Design tradeoffs:
  - Using two separate models for column and row reduction simplifies the task but may miss interactions between column and row selection
  - RL training adds complexity but allows task-specific optimization beyond supervised fine-tuning
  - Relying on text-to-SQL annotations for training data may limit generalizability to non-SQL-like reasoning tasks
  - The method requires access to a fixed LLM (GPT-4) for the final task, which may not be available or cost-effective in all settings

- Failure signatures:
  - Context reduction model consistently selects irrelevant columns/rows → Check training data quality and RL reward design
  - RL training fails to converge or causes performance degradation → Check KL penalty coefficient and reward scaling
  - Reduced context still exceeds LLM's context window → Check truncation strategy and context length estimation
  - Performance improvement is minimal → Check if the original context was already short enough or if the LLM's reasoning ability is the bottleneck

- First 3 experiments:
  1. Evaluate context reduction recall on WTQ test set with both column and row models
  2. Compare GPT-4 accuracy on WTQ questions with original vs. reduced context (varying context lengths)
  3. Test generalizability by evaluating context reduction performance on a different table QA dataset (e.g., Hybrid QA)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the Learning to Reduce model vary when applied to different types of structured data, such as knowledge graphs or databases, beyond tables?
- Basis in paper: [inferred] The paper primarily focuses on table QA tasks and mentions that structured data like knowledge graphs and databases are challenging for LLMs to integrate into their prompts.
- Why unresolved: The paper does not provide experiments or results on the application of the Learning to Reduce model to knowledge graphs or databases.
- What evidence would resolve it: Experiments applying the Learning to Reduce model to knowledge graph QA tasks or database QA tasks, comparing its performance to other methods and baseline models.

### Open Question 2
- Question: What is the impact of the Learning to Reduce model on the efficiency and cost-effectiveness of using LLMs for structured data QA tasks, considering the computational resources required for fine-tuning and inference?
- Basis in paper: [explicit] The paper mentions that the Learning to Reduce model aims to maximize the reasoning ability of LLMs as well as the cost efficiency of using them.
- Why unresolved: The paper does not provide quantitative data on the computational resources required for fine-tuning the Learning to Reduce model or the inference time compared to other methods.
- What evidence would resolve it: Experiments measuring the fine-tuning time, inference time, and computational resources required for the Learning to Reduce model and comparing them to other methods for structured data QA tasks.

### Open Question 3
- Question: How does the Learning to Reduce model perform when the input context contains multiple tables or when the tables have complex relationships?
- Basis in paper: [inferred] The paper focuses on single-table QA tasks and does not mention experiments with multiple tables or complex table relationships.
- Why unresolved: The paper does not provide experiments or results on the application of the Learning to Reduce model to scenarios with multiple tables or complex table relationships.
- What evidence would resolve it: Experiments applying the Learning to Reduce model to multi-table QA tasks or QA tasks with complex table relationships, comparing its performance to other methods and baseline models.

## Limitations
- The RL training methodology lacks detailed hyperparameter specifications, making exact reproduction challenging
- The generalizability results are based on a single out-of-distribution dataset (Hybrid QA), which may not capture all types of distributional shifts
- The performance improvement for GPT-4 with reduced context is modest (2-3% absolute improvement), raising questions about practical significance

## Confidence

**High confidence:** The core methodology of using RL for context reduction is sound and well-justified

**Medium confidence:** The approach's effectiveness on the WTQ dataset, as results are comparable to existing methods

**Medium confidence:** The generalizability claims, as they are supported by limited out-of-domain testing

## Next Checks
1. Evaluate the context reduction model's performance on additional table QA datasets beyond Hybrid QA to better assess generalizability
2. Conduct ablation studies to quantify the individual contributions of fine-tuning vs. RL training to the final performance
3. Test the approach with different fixed LLMs (e.g., Claude, Llama) to verify that the improvements are not specific to GPT-4's architecture