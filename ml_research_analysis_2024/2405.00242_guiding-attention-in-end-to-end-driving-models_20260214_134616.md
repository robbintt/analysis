---
ver: rpa2
title: Guiding Attention in End-to-End Driving Models
arxiv_id: '2405.00242'
source_url: https://arxiv.org/abs/2405.00242
tags:
- driving
- attention
- learning
- data
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an attention guidance method for vision-based
  end-to-end driving models trained by imitation learning. The approach adds an attention
  loss term during training to guide the model's focus toward salient regions using
  semantic maps, improving driving performance and producing more interpretable activation
  maps.
---

# Guiding Attention in End-to-End Driving Models

## Quick Facts
- **arXiv ID:** 2405.00242
- **Source URL:** https://arxiv.org/abs/2405.00242
- **Authors:** Diego Porres; Yi Xiao; Gabriel Villalonga; Alexandre Levy; Antonio M. López
- **Reference count:** 40
- **Primary result:** Attention guidance improves driving performance, especially in low-data regimes, without requiring saliency maps at test time.

## Executive Summary
This paper introduces an attention guidance method for vision-based end-to-end driving models trained by imitation learning. The approach adds an attention loss term during training to guide the model's focus toward salient regions using semantic maps, improving driving performance and producing more interpretable activation maps. Unlike prior work, it does not require saliency maps during testing or architectural changes. Experiments with the CIL++ model on CARLA show that the method improves driving quality, especially in low-data regimes. It outperforms baselines that require saliency maps at test time, achieving up to 79% success rate and higher driving scores, while also providing clearer attention visualizations.

## Method Summary
The method applies an attention loss during training using semantic segmentation and depth data to create binary attention masks highlighting safety-critical regions like vehicles, pedestrians, traffic signs, and lane markings. A Kullback-Leibler divergence loss is computed between the averaged multi-head self-attention maps from a transformer layer and these binary masks. This encourages the model to attend to relevant regions without architectural changes or requiring saliency maps at test time. The approach also incorporates noisy masks during training to improve robustness to imperfect segmentation outputs during deployment.

## Key Results
- The method achieves up to 79% success rate on CARLA benchmarks, outperforming baselines requiring saliency maps at test time
- Significant performance improvements are observed in low-data regimes (2-4 hours of training data) compared to the baseline
- Attention maps become more interpretable, clearly highlighting safety-critical objects like vehicles and pedestrians

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The attention loss guides the model to focus on safety-critical regions by matching the distribution of self-attention maps to pre-defined semantic masks.
- Mechanism: A Kullback-Leibler divergence loss is computed between the averaged multi-head self-attention maps from a transformer layer and a binary attention mask derived from semantic segmentation and depth data. This encourages the model to attend to regions of interest without architectural changes.
- Core assumption: The self-attention maps in the transformer encoder can approximate the distribution of human-relevant attention regions.
- Evidence anchors:
  - [abstract] states the method adds "a loss term during training using salient semantic maps" and achieves interpretable activation maps.
  - [section 3.3.2] explains the KL divergence between the attention maps and the binary masks.
  - [corpus] contains no direct citations, but related work on attention mechanisms supports the general idea.
- Break condition: If the self-attention maps do not correlate with the regions highlighted in the semantic masks, the loss will not effectively guide attention.

### Mechanism 2
- Claim: Using noisy semantic masks during training improves robustness and generalization.
- Mechanism: Realistic noise is injected into the attention masks using depth-aware Perlin noise, simulating real-world imperfections. The model is trained with these noisy masks, making it more robust to imperfect segmentation outputs during deployment.
- Core assumption: The model can learn to focus on relevant regions even when the ground truth attention masks are imperfect.
- Evidence anchors:
  - [section 3.3.2] mentions experiments with "perfect and noisy salient semantic maps" and notes "encouraging results in both."
  - [section 4.5] includes qualitative results showing the model segments objects of interest even with noisy masks.
  - [corpus] lacks direct citations but noise injection is a common robustness technique in computer vision.
- Break condition: If the noise level is too high, the model may learn to ignore the attention loss entirely or focus on irrelevant regions.

### Mechanism 3
- Claim: Attention guidance improves driving performance, especially in low-data regimes.
- Mechanism: By forcing the model to focus on task-relevant regions, the attention loss acts as a regularizer that reduces the need for large amounts of diverse training data. The model generalizes better from limited examples.
- Core assumption: Guiding attention reduces the complexity of the learning task by focusing on a subset of relevant features.
- Evidence anchors:
  - [abstract] states the method is "especially effective when data and computational resources are scarce."
  - [section 4.4.1] shows significant performance improvements with only 2-4 hours of training data compared to the baseline.
  - [corpus] has no direct citations but the idea aligns with literature on attention as a form of inductive bias.
- Break condition: If the attention masks are not representative of the most important driving features, the guidance could mislead the model and hurt performance.

## Foundational Learning

- **Concept:** Transformer self-attention
  - **Why needed here:** The model uses a transformer encoder to process multi-view camera inputs, and the attention loss is applied to the self-attention maps.
  - **Quick check question:** What is the difference between multi-head self-attention and single-head attention in a transformer?

- **Concept:** Imitation learning objective
  - **Why needed here:** The model is trained via imitation learning, minimizing the difference between predicted and expert actions.
  - **Quick check question:** In imitation learning, what is the role of the expert dataset and how does it differ from reinforcement learning?

- **Concept:** Kullback-Leibler divergence
  - **Why needed here:** The attention loss uses KL divergence to compare the distribution of attention maps to the binary mask distribution.
  - **Quick check question:** How does KL divergence differ from L2 loss when comparing two probability distributions?

## Architecture Onboarding

- **Component map:** Input (K=3 RGB images + speed + command) → ResNet backbone → flatten and concatenate → positional embeddings → Transformer Encoder (4 layers, 4 heads) → Global Average Pooling → MLP → Output (steering, acceleration). Attention loss is applied to the last transformer layer.
- **Critical path:** The attention loss is computed from the self-attention maps of the transformer and does not alter the forward pass for action prediction. The model still predicts actions via the same path as the baseline.
- **Design tradeoffs:** Using semantic masks for attention guidance avoids requiring saliency maps at test time but depends on access to semantic segmentation data during training. The method adds a small computational overhead during training but none at inference.
- **Failure signatures:** If the attention loss is too high, the model may overfit to the attention masks and ignore other important features. If too low, it may not provide meaningful guidance. Poor-quality semantic masks can lead to degraded performance.
- **First 3 experiments:**
  1. Train with λatt = 0 (baseline) and λatt = 10, compare success rate and driving score on a small validation set.
  2. Visualize the self-attention maps with and without the attention loss to confirm interpretability improvements.
  3. Test with noisy masks (f(Mi,t)) to verify robustness and compare performance to clean masks.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the proposed attention loss method perform when trained with real-world data and deployed in actual vehicles?
- **Basis in paper:** [explicit] The authors mention planning to test the Attention Loss with real data and deploy the model in a real car as future work.
- **Why unresolved:** The paper only evaluates the method in simulation (CARLA) using synthetic and noisy synthetic masks. Real-world deployment introduces challenges like sensor noise, dynamic environments, and hardware constraints that are not captured in simulation.
- **What evidence would resolve it:** Experimental results showing performance metrics (success rate, driving score, etc.) when the trained model is tested on real roads with a physical vehicle and real camera inputs.

### Open Question 2
- **Question:** Can the attention guidance method be effectively applied to non-pure-vision end-to-end driving models, such as those incorporating LiDAR or radar data?
- **Basis in paper:** [inferred] The authors suggest the method could be applied to non-pure-vision models in the future, but no experiments are conducted to verify this.
- **Why unresolved:** The method is only tested on vision-based models (CIL++). Different sensor modalities have different characteristics (e.g., sparsity of LiDAR, temporal coherence of radar) that may affect how attention guidance should be formulated.
- **What evidence would resolve it:** Comparative experiments showing performance of the attention loss when applied to multi-modal models (e.g., camera + LiDAR) versus single-modal baselines, with metrics like driving score and success rate.

### Open Question 3
- **Question:** How sensitive is the attention loss method to the choice of layer in the Transformer Encoder where the loss is applied?
- **Basis in paper:** [explicit] The authors mention that while they apply the loss at the last layer (l = L = 4), there is no limitation on which layer to apply it to, and they leave this for future work.
- **Why unresolved:** The paper only evaluates one choice (last layer) without exploring whether intermediate layers might yield better results or different trade-offs between performance and interpretability.
- **What evidence would resolve it:** Ablation studies showing driving performance and attention map quality when the loss is applied at different Transformer layers (e.g., l = 1, 2, 3, 4), with statistical significance tests.

## Limitations

- The method relies on semantic segmentation data during training, which may not be available in all driving scenarios or may introduce additional annotation costs
- The approach adds computational overhead during training and requires careful tuning of the attention loss weight parameter
- Evaluation is limited to the CARLA simulator, and real-world performance remains to be validated

## Confidence

- **High confidence:** The mechanism of using KL divergence between self-attention maps and semantic masks to guide attention is technically sound and well-supported by the results.
- **Medium confidence:** The claim that attention guidance significantly improves performance in low-data regimes is supported by experiments but would benefit from testing on additional datasets and driving scenarios.
- **Medium confidence:** The assertion that the method produces more interpretable attention maps is supported qualitatively but would be strengthened by user studies or quantitative interpretability metrics.

## Next Checks

1. Test the method on a real-world driving dataset (e.g., nuScenes or Waymo Open Dataset) to validate simulator performance translates to real-world conditions.
2. Conduct ablation studies varying the attention loss weight λatt across a wider range to identify optimal values for different data regimes and driving scenarios.
3. Implement and test alternative attention guidance mechanisms (e.g., using gradient-based saliency or object detection instead of semantic segmentation) to assess the generality of the approach.