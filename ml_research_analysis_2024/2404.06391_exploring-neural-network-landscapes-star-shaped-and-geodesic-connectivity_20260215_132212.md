---
ver: rpa2
title: 'Exploring Neural Network Landscapes: Star-Shaped and Geodesic Connectivity'
arxiv_id: '2404.06391'
source_url: https://arxiv.org/abs/2404.06391
tags:
- minima
- linear
- connectivity
- then
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the fine-grained structure of mode connectivity
  in neural network landscapes, revealing surprising geometric properties. The authors
  show that overparameterized networks exhibit not only standard mode connectivity
  but also star-shaped and geodesic connectivity patterns.
---

# Exploring Neural Network Landscapes: Star-Shaped and Geodesic Connectivity

## Quick Facts
- arXiv ID: 2404.06391
- Source URL: https://arxiv.org/abs/2404.06391
- Reference count: 40
- Key outcome: Neural network loss landscapes exhibit star-shaped and geodesic connectivity patterns, with global minima connected via simple paths that become increasingly convex-like as networks widen

## Executive Summary
This paper investigates the fine-grained structure of mode connectivity in neural network loss landscapes, revealing that overparameterized networks exhibit not only standard mode connectivity but also star-shaped and geodesic connectivity patterns. The authors demonstrate that global minima can be connected through simple two-piece linear paths with negligible loss barriers, and that geodesic distances approach Euclidean distances as network width increases. Their findings suggest neural network landscapes become increasingly convex-like as networks grow wider, providing new insights into optimization and generalization properties.

## Method Summary
The authors employ a combination of theoretical analysis and empirical validation to study neural network landscapes. Theoretically, they prove that for sufficiently wide two-layer ReLU networks and linear networks, global minima can be connected via simple two-piece linear paths. Empirically, they conduct experiments on MNIST and CIFAR-10 datasets using various architectures including fully connected networks, VGG16, and ResNet34. They compute loss values along connecting paths, measure geodesic distances using fast-marching methods, and compare these to Euclidean distances to quantify the convexity of the landscape.

## Key Results
- Global minima in neural networks can be connected via simple two-piece linear paths (fold-lines) with negligible loss barriers
- Geodesic distances between minima converge to Euclidean distances as network width increases, indicating increasingly convex-like landscapes
- Empirical results on MNIST and CIFAR-10 confirm theoretical predictions, showing star-shaped connectivity patterns across different architectures

## Why This Works (Mechanism)
The convergence of geodesic to Euclidean distance in wide networks occurs because the loss landscape becomes increasingly convex as the number of parameters grows. In the overparameterized regime, there are many paths of near-zero loss connecting different minima, allowing geodesics to take direct routes. The star-shaped connectivity emerges from the network's ability to smoothly interpolate between different solutions without encountering significant barriers, as the high dimensionality provides sufficient flexibility for the loss to remain flat along connecting paths.

## Foundational Learning
- **Mode connectivity**: The property that different local minima can be connected by low-loss paths; needed to understand generalization in neural networks, quick check: verify minima are connected by paths with bounded loss increase
- **Geodesic distance**: The shortest path on the loss manifold; needed to measure true landscape geometry, quick check: compare with Euclidean distance to assess convexity
- **Overparameterization**: When parameter count exceeds data requirements; needed to explain why landscapes become convex-like, quick check: verify width scaling affects connectivity properties
- **ReLU activation**: Piecewise linear activation function; needed for tractable theoretical analysis, quick check: confirm theoretical results hold for this activation type
- **Fold-lines**: Two-piece linear connecting paths; needed as the simplest connectivity structure, quick check: verify loss remains bounded along these paths
- **Fast-marching method**: Computational technique for finding geodesics; needed to measure true shortest paths, quick check: ensure accurate geodesic computation on discrete grids

## Architecture Onboarding
- **Component map**: Network initialization -> Training to convergence -> Identifying multiple minima -> Computing connecting paths -> Measuring geodesic distances -> Comparing to Euclidean distances
- **Critical path**: Training multiple minima → Computing fold-line connections → Measuring geodesic distances → Analyzing width scaling effects
- **Design tradeoffs**: Theoretical tractability vs. practical relevance (2-layer networks vs. deep architectures); computational cost of geodesic computation vs. insight gained
- **Failure signatures**: High loss barriers on connecting paths would indicate failure; large gaps between geodesic and Euclidean distances would suggest non-convexity
- **First experiments**: 1) Train multiple minima on simple dataset and verify connectivity; 2) Measure geodesic vs Euclidean distances as width varies; 3) Test star-shaped pattern visualization on 2D slices

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical results primarily limited to two-layer ReLU networks and may not extend to deeper architectures
- Analysis assumes Gaussian initialization and specific activation functions, limiting generalizability
- Focus on specific loss functions may not capture all possible loss landscape geometries

## Confidence
- Theoretical claims for two-layer networks: High
- Empirical observations across architectures: Medium
- Generalization to arbitrary network widths and depths: Low

## Next Checks
1. Test the star-shaped and geodesic connectivity properties across different activation functions (e.g., sigmoid, tanh, leaky ReLU) and initialization schemes to verify the robustness of findings.

2. Extend the theoretical analysis to deeper networks (3+ layers) and validate whether the convergence of geodesic distance to Euclidean distance still holds.

3. Investigate the impact of regularization techniques (weight decay, dropout) on the observed connectivity patterns and geodesic properties.