---
ver: rpa2
title: 'Text Classification using Graph Convolutional Networks: A Comprehensive Survey'
arxiv_id: '2410.09399'
source_url: https://arxiv.org/abs/2410.09399
tags:
- classification
- text
- graph
- learning
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This comprehensive survey systematically categorizes and analyzes
  graph convolutional network (GCN) approaches for text classification, covering fundamental
  techniques, integration with generative models (CNNs, RNNs, LSTMs, BERT, LLMs),
  and supervision-based categories (supervised, semi-supervised, self-supervised,
  weakly supervised). The authors provide detailed comparisons across benchmark datasets
  including 20NG, R8, R52, Ohsumed, MR, CoLA, and SST-2, reporting test accuracy and
  macro-averaged F1 scores.
---

# Text Classification using Graph Convolutional Networks: A Comprehensive Survey

## Quick Facts
- **arXiv ID**: 2410.09399
- **Source URL**: https://arxiv.org/abs/2410.09399
- **Reference count**: 40
- **Primary result**: Systematic survey of GCN approaches for text classification with performance comparisons across benchmark datasets

## Executive Summary
This survey provides a comprehensive analysis of graph convolutional network (GCN) approaches for text classification, systematically categorizing methods based on supervision types and integration strategies with other architectures. The authors trace the evolution from early TextGCN models to advanced hybrid approaches combining GCNs with CNNs, RNNs, LSTMs, BERT, and large language models. Through detailed performance comparisons on benchmark datasets including 20NG, R8, R52, Ohsumed, MR, CoLA, and SST-2, the survey identifies key research directions and challenges in the field.

The work highlights significant performance improvements achieved through GCN-based methods, with hybrid models like BERT-GCN and RoBERTa-GCN demonstrating superior accuracy over traditional approaches. The survey emphasizes the importance of semi-supervised learning in handling limited-labeled data scenarios and identifies promising directions including improved data augmentation techniques, graph diffusion methods, and privacy-preserving applications. The authors also discuss the challenges of noise in graph structures and the need for better handling of label scarcity in real-world applications.

## Method Summary
The survey employs a systematic literature review methodology to categorize and analyze GCN approaches for text classification. The authors classify methods into supervision-based categories (supervised, semi-supervised, self-supervised, weakly supervised) and integration-based categories (with CNNs, RNNs, LSTMs, BERT, LLMs). Performance metrics including test accuracy and macro-averaged F1 scores are extracted from published studies across benchmark datasets. The survey also identifies research directions and challenges through analysis of current limitations in the field, including issues with data augmentation, graph construction, and integration with emerging architectures.

## Key Results
- Evolution from early TextGCN (86.3% accuracy on 20NG) to advanced hybrid models like BERT-GCN (89.3% on 20NG) and RoBERTa-GCN (89.5% on 20NG)
- Semi-supervised approaches like HeteGCN showing strong performance across multiple datasets
- Identified research directions include improved data augmentation techniques, graph diffusion methods, and integration with large language models
- Challenges identified in handling limited-labeled data and noise in graph structures

## Why This Works (Mechanism)
GCNs excel at text classification by capturing complex relationships between words and documents through graph structures, where nodes represent words and documents with edges encoding semantic and syntactic relationships. The convolutional operations aggregate neighborhood information, allowing each node to incorporate contextual information from its neighbors through multiple layers. This enables the model to learn representations that reflect both local word patterns and global document structure simultaneously.

The effectiveness stems from GCNs' ability to model non-Euclidean relationships inherent in text data, where traditional Euclidean methods struggle. By treating text as a graph, GCNs can naturally capture long-range dependencies, hierarchical relationships, and semantic similarities that are difficult to model with sequential approaches alone. The message-passing mechanism allows information to flow through the graph, enriching node representations with context from related words and documents.

## Foundational Learning
**Graph Theory Basics**: Understanding nodes, edges, and graph representations is fundamental for GCNs, as text data is transformed into graph structures where words and documents become nodes with various relationship types. This is needed because GCNs operate on graph-structured data rather than sequential or Euclidean data, requiring a shift in how text is represented and processed.

**Convolutional Operations on Graphs**: Unlike image CNNs that operate on regular grids, GCN convolutions aggregate information from a node's neighborhood using adjacency matrices and learnable filters. This is needed because text graphs have irregular structures with varying node degrees and connectivity patterns, requiring specialized convolutional operations that can handle non-Euclidean geometry.

**Message-Passing Mechanism**: GCNs use iterative message-passing where each node updates its representation by aggregating information from its neighbors, with multiple layers allowing information to propagate further through the graph. This is needed because it enables nodes to incorporate context from distant parts of the graph, capturing both local and global relationships in the text data.

**Spectral Graph Theory**: Understanding graph Fourier transforms and graph signal processing provides theoretical foundations for GCN operations, explaining how information is filtered and propagated through the graph structure. This is needed for designing effective GCN architectures and understanding the mathematical principles behind their operation on graph-structured text data.

## Architecture Onboarding

**Component Map**: Text documents -> Graph Construction (word-word, word-document edges) -> GCN Layers (feature aggregation and transformation) -> Classification Layer -> Output predictions

**Critical Path**: Graph construction → GCN layers → Classification layer. The graph construction step is critical as it determines the relationships captured, GCN layers perform the core message-passing and feature learning, and the classification layer maps learned representations to final categories.

**Design Tradeoffs**: Word-word vs. word-document edges (connectivity vs. computational efficiency), GCN depth (receptive field vs. over-smoothing), feature initialization (pre-trained embeddings vs. learned features), and supervision type (fully supervised vs. semi-supervised). Deeper networks capture longer-range dependencies but risk over-smoothing, while different graph constructions emphasize different types of relationships.

**Failure Signatures**: Poor performance on semantically similar classes, sensitivity to graph construction quality, degradation with very deep networks due to over-smoothing, and suboptimal results when label information is sparse. These failures often manifest as high training accuracy but poor generalization, or inability to distinguish between closely related document categories.

**First Experiments**:
1. Compare TextGCN performance on 20NG dataset with different graph construction methods (word-word vs. word-document edges)
2. Evaluate GCN depth impact by testing 2-layer vs. 4-layer architectures on R8 dataset
3. Assess semi-supervised performance by varying label ratios on Ohsumed dataset

## Open Questions the Paper Calls Out
The survey identifies several open questions in GCN-based text classification, including how to effectively handle noise in graph structures and improve robustness to imperfect graph constructions. The authors question the scalability of GCN approaches to very large text corpora and the computational efficiency of graph construction methods. They also highlight the need for better integration strategies with emerging architectures like large language models and the development of more sophisticated data augmentation techniques specifically designed for graph-structured text data.

## Limitations
- Potential bias toward published GCN approaches showing positive results, with negative findings likely underrepresented
- Performance metric variations due to undisclosed implementation details, preprocessing differences, and hyperparameter tuning strategies across studies
- Categorization framework may not fully capture emerging hybrid approaches combining GCNs with newer architectures beyond explicitly mentioned ones
- Limited discussion of computational complexity and scalability challenges for real-world large-scale applications

## Confidence
- **High confidence**: Historical progression of GCN-based text classification from TextGCN to BERT-GCN variants, basic categorization into supervision types
- **Medium confidence**: Comparative performance across benchmark datasets, identified research directions
- **Low confidence**: Specific numerical improvements claimed for individual models, completeness of coverage for all relevant approaches

## Next Checks
1. Replicate the reported performance differences between TextGCN and hybrid models (BERT-GCN, RoBERTa-GCN) on 20NG dataset using publicly available implementations to verify claimed accuracy improvements
2. Conduct ablation studies on the impact of graph construction methods (word-word vs. word-document edges) across the three major datasets (20NG, R8, R52) to validate their influence on reported performance
3. Test the robustness of GCN-based approaches under varying levels of label scarcity on Ohsumed and MR datasets to verify claims about semi-supervised effectiveness compared to fully supervised methods