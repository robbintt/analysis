---
ver: rpa2
title: 'VLM Agents Generate Their Own Memories: Distilling Experience into Embodied
  Programs of Thought'
arxiv_id: '2406.14596'
source_url: https://arxiv.org/abs/2406.14596
tags:
- ical
- action
- task
- examples
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ICAL is a method that enables VLM agents to transform suboptimal
  trajectories into high-quality training data through self-reflection and human feedback.
  The approach abstracts noisy trajectories into generalized strategies and action
  annotations by correcting inefficiencies and capturing cognitive abstractions like
  causal relationships, object state changes, temporal subgoals, and task-relevant
  visual elements.
---

# VLM Agents Generate Their Own Memories: Distilling Experience into Embodied Programs of Thought

## Quick Facts
- arXiv ID: 2406.14596
- Source URL: https://arxiv.org/abs/2406.14596
- Authors: Gabriel Sarch; Lawrence Jang; Michael J. Tarr; William W. Cohen; Kenneth Marino; Katerina Fragkiadaki
- Reference count: 40
- Key outcome: ICAL achieves state-of-the-art results across multiple benchmarks, improving task success 1.6x-2.8x and reducing manual prompt engineering requirements.

## Executive Summary
ICAL is a method that enables VLM agents to transform suboptimal trajectories into high-quality training data through self-reflection and human feedback. The approach abstracts noisy trajectories into generalized strategies and action annotations by correcting inefficiencies and capturing cognitive abstractions like causal relationships, object state changes, temporal subgoals, and task-relevant visual elements. These annotations are iteratively refined through human feedback during execution in similar environments.

ICAL achieves state-of-the-art results across multiple benchmarks. In TEACh dialogue-based instruction following, it outperforms raw human demonstrations and expert examples by 17.5% in goal-condition success. In VisualWebArena, retrieval-augmented GPT-4V with ICAL improves task success 1.6x, while fine-tuned Qwen2-VL achieves 2.8x improvement over the base model. In Ego4D action forecasting, ICAL surpasses few-shot GPT-4V and remains competitive with supervised models. The approach scales 2x better than raw demonstrations and significantly reduces manual prompt engineering requirements.

## Method Summary
ICAL operates in two phases: abstraction and human-in-the-loop refinement. First, a VLM corrects action errors and generates language abstractions from noisy trajectories, capturing task understanding, causal relationships, and object state changes. Then, the optimized trajectory is executed in the environment while human observers provide natural language feedback on failures. The VLM revises both actions and abstractions based on this feedback. The resulting examples are stored in memory and used for retrieval-augmented generation or fine-tuning. ICAL processes noisy trajectories from TEACh, VisualWebArena, and Ego4D benchmarks, achieving 17.5% improvement in goal-condition success, 1.6x-2.8x task success improvement, and competitive performance with supervised models using 639x less training data.

## Key Results
- In TEACh dialogue-based instruction following, ICAL outperforms raw human demonstrations and expert examples by 17.5% in goal-condition success
- In VisualWebArena, retrieval-augmented GPT-4V with ICAL improves task success 1.6x, while fine-tuned Qwen2-VL achieves 2.8x improvement over the base model
- In Ego4D action forecasting, ICAL surpasses few-shot GPT-4V and remains competitive with supervised models using 639x less in-domain training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ICAL enables VLMs to transform noisy trajectories into high-quality training data by correcting inefficiencies and adding cognitive abstractions.
- Mechanism: The VLM first identifies and corrects action errors in the trajectory while simultaneously generating language annotations that capture task and causal relationships, state changes, temporal subgoals, and task-relevant visual elements. These abstractions are then refined through human feedback during execution.
- Core assumption: The VLM can identify suboptimal actions and generate meaningful abstractions without explicit human labeling.
- Evidence anchors:
  - [abstract] "ICAL, enabling VLM agents to transform suboptimal trajectories into high-quality training data through self-reflection and human feedback."
  - [section 3.2] "The abstraction function, Fabstract, modifies ξnoisy by correcting actions and generating language thoughts that encapsulate general knowledge and task-specific insights."
  - [corpus] Weak - corpus contains related work on feedback learning but no direct evidence of this specific abstraction generation mechanism.
- Break condition: If the VLM cannot accurately identify inefficiencies or generate meaningful abstractions, the quality of the resulting training data will degrade, limiting performance improvements.

### Mechanism 2
- Claim: Human-in-the-loop verification iteratively refines both actions and abstractions through natural language feedback.
- Mechanism: When the agent executes the optimized trajectory, a human observer monitors and intervenes when actions fail, providing natural language feedback. The VLM then revises the trajectory and updates the language abstractions based on this feedback.
- Core assumption: Natural language feedback from humans can effectively guide the refinement of both actions and abstractions.
- Evidence anchors:
  - [abstract] "These annotations are iteratively refined through human feedback during execution in similar environments."
  - [section 3.3] "Upon receiving feedback H(a t, ot), the VLM is provided with this input alongside the current state of ξoptimized and any existing language thoughts L."
  - [corpus] Weak - corpus contains related work on human feedback but not this specific iterative refinement process.
- Break condition: If human feedback is ambiguous or insufficient, the iterative refinement process may not converge to high-quality examples, reducing the effectiveness of the learned abstractions.

### Mechanism 3
- Claim: Retrieval-augmented generation with learned examples improves in-context learning efficiency and scales better than raw demonstrations.
- Mechanism: The agent retrieves relevant examples from memory based on textual and visual similarity with the current scene. These retrieved examples are used as context for action generation, allowing the agent to leverage past experiences effectively.
- Core assumption: The similarity-based retrieval system can effectively identify relevant examples from the memory set.
- Evidence anchors:
  - [abstract] "As the agent's example library grows, it becomes more efficient at abstracting new examples, requiring less human feedback and fewer environment interactions."
  - [section 3.4] "Given the learned example set M and a new instruction I, we prompt the VLM to carry out the instruction by producing action sequences from an action API that describes the skills set A by retrieving the top K examples from M to include in the prompt."
  - [corpus] Weak - corpus contains related work on retrieval-augmented generation but not this specific scaling analysis.
- Break condition: If the retrieval system cannot identify relevant examples or if the example library becomes too large, retrieval efficiency may degrade, limiting the scalability benefits.

## Foundational Learning

- Concept: Vision-Language Models (VLMs) and their in-context learning capabilities
  - Why needed here: ICAL relies on VLMs to generate and refine abstractions from trajectories, requiring understanding of how VLMs process multimodal information and learn from examples.
  - Quick check question: How do VLMs differ from traditional vision models in terms of processing instructions and generating actions?

- Concept: Program of Thought (PoT) generation and refinement
  - Why needed here: The core of ICAL involves generating and refining programs of thought that capture task understanding, which requires understanding how to represent and manipulate structured knowledge.
  - Quick check question: What are the key components of a program of thought for embodied agents, and how do they differ from simple action sequences?

- Concept: Human-in-the-loop learning and feedback integration
  - Why needed here: ICAL's effectiveness depends on how well human feedback can be integrated into the learning process, requiring understanding of feedback mechanisms and iterative refinement.
  - Quick check question: What are the key differences between traditional human feedback methods and the natural language feedback used in ICAL?

## Architecture Onboarding

- Component map:
  VLM abstraction module (Fabstract) -> Human-in-the-loop interface -> Memory management system -> Retrieval-augmented planning module -> Execution environment interface

- Critical path:
  1. Receive noisy trajectory and instruction
  2. Generate initial abstractions using Fabstract
  3. Execute trajectory in environment
  4. Collect human feedback on failures
  5. Revise trajectory and abstractions
  6. Store successful examples in memory
  7. Use memory for future in-context learning

- Design tradeoffs:
  - Tradeoff between feedback frequency and learning efficiency
  - Balance between abstraction complexity and practical usability
  - Memory size vs. retrieval efficiency
  - VLM model size vs. inference cost

- Failure signatures:
  - Poor abstraction quality → degraded performance despite successful execution
  - Insufficient human feedback → incomplete refinement of examples
  - Retrieval failures → inability to leverage learned examples effectively
  - Execution errors → corrupted example memory

- First 3 experiments:
  1. Test VLM abstraction generation on simple trajectories without human feedback
  2. Evaluate human-in-the-loop refinement process with controlled feedback
  3. Measure retrieval effectiveness on a small example library

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ICAL perform when the noisy trajectories come from extremely misleading demonstrations or adversarial feedback?
- Basis in paper: [explicit] The paper mentions that ICAL "may not be able to handle extremely misleading demonstrations or feedback"
- Why unresolved: The paper does not provide experimental results or theoretical analysis of ICAL's robustness to highly misleading demonstrations or adversarial feedback
- What evidence would resolve it: Experiments testing ICAL with intentionally misleading demonstrations or adversarial feedback, or theoretical analysis of ICAL's robustness bounds

### Open Question 2
- Question: What is the impact of GPT-4V's visual grounding deficiencies on ICAL's performance, and how can these limitations be addressed?
- Basis in paper: [explicit] The paper states that "GPT4V's visual grounding deficiencies [93, 85, 56, 9] cannot always be overcome by in-context learning"
- Why unresolved: The paper does not provide specific experiments or solutions to quantify or mitigate the impact of GPT-4V's visual grounding limitations on ICAL
- What evidence would resolve it: Experiments comparing ICAL performance with different vision-language models, or proposed methods to improve visual grounding within the ICAL framework

### Open Question 3
- Question: How does ICAL scale with the number and diversity of tasks in the learned example set?
- Basis in paper: [explicit] The paper shows ICAL scales 2x better than raw demonstrations, but doesn't explore the limits of scalability with increasing task diversity
- Why unresolved: The experiments only test ICAL on a fixed set of tasks and don't investigate how performance changes with increasingly diverse or numerous tasks
- What evidence would resolve it: Systematic experiments varying the number and diversity of tasks in the ICAL example set, measuring performance and learning efficiency across different scales

## Limitations
- The exact implementation details of the abstraction and human-in-the-loop phases require reference to the appendix
- ICAL's effectiveness heavily depends on human feedback quality and quantity, which can vary significantly
- Scalability analysis is based on limited experiments, and long-term effectiveness as the example library grows remains uncertain

## Confidence
- High Confidence: The core mechanism of transforming noisy trajectories into high-quality training data through VLM-based abstraction and human feedback
- Medium Confidence: The claim that ICAL scales 2x better than raw demonstrations
- Medium Confidence: The effectiveness of natural language feedback for iterative refinement

## Next Checks
1. **Ablation Study on Abstraction Quality:** Test the system's performance with and without the abstraction generation phase to quantify the specific contribution of the VLM-generated program-of-thought annotations to overall performance improvement.

2. **Human Feedback Robustness Test:** Evaluate the system's performance using different human feedback qualities (expert vs. non-expert, varying feedback frequencies) to determine the minimum viable human feedback requirements for effective ICAL operation.

3. **Memory Size Scaling Experiment:** Systematically test the retrieval effectiveness and overall performance as the example library grows from small (10 examples) to large (1000+ examples) to validate the claimed scalability benefits and identify potential retrieval efficiency bottlenecks.