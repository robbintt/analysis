---
ver: rpa2
title: 'MVMoE: Multi-Task Vehicle Routing Solver with Mixture-of-Experts'
arxiv_id: '2405.01029'
source_url: https://arxiv.org/abs/2405.01029
tags:
- gating
- mvmoe
- node
- neural
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MVMoE, a unified neural solver for vehicle
  routing problems (VRPs) that leverages mixture-of-experts (MoE) layers and a hierarchical
  gating mechanism. Unlike prior neural solvers that are tailored to individual VRP
  variants, MVMoE is trained on multiple VRP variants simultaneously, enabling zero-shot
  generalization to unseen VRPs without additional fine-tuning.
---

# MVMoE: Multi-Task Vehicle Routing Solver with Mixture-of-Experts

## Quick Facts
- arXiv ID: 2405.01029
- Source URL: https://arxiv.org/abs/2405.01029
- Authors: Jianan Zhou; Zhiguang Cao; Yaoxin Wu; Wen Song; Yining Ma; Jie Zhang; Chi Xu
- Reference count: 40
- One-line primary result: MVMoE achieves zero-shot generalization to unseen VRP variants through multi-task training with MoE layers and hierarchical gating.

## Executive Summary
This paper introduces MVMoE, a unified neural solver for vehicle routing problems (VRPs) that leverages mixture-of-experts (MoE) layers and a hierarchical gating mechanism. Unlike prior neural solvers that are tailored to individual VRP variants, MVMoE is trained on multiple VRP variants simultaneously, enabling zero-shot generalization to unseen VRPs without additional fine-tuning. The proposed hierarchical gating mechanism distributes computational load between sparse and dense layers during decoding, balancing empirical performance and computational efficiency. Experiments show that MVMoE significantly outperforms existing multi-task neural solvers on 10 unseen VRP variants, and also achieves competitive results on few-shot and real-world benchmark instances. The hierarchical gating is found to particularly enhance out-of-distribution generalization. The method is available at https://github.com/RoyalSkye/Routing-MVMoE.

## Method Summary
MVMoE employs an encoder-decoder architecture with MoE layers replacing standard feed-forward networks. The model is trained on 6 base VRP variants (CVRP, OVRP, VRPB, VRPL, VRPTW, OVRPTW) with 5 constraint types, enabling zero-shot generalization to 10 unseen variants. The hierarchical gating mechanism routes inputs through either sparse MoE layers or dense layers based on problem-level features, with a second gating stage for node-level routing when using MoEs. The model is trained using REINFORCE with auxiliary load-balancing loss for 5000 epochs on 100M instances.

## Key Results
- MVMoE achieves zero-shot generalization to 10 unseen VRP variants, significantly outperforming existing multi-task neural solvers
- The hierarchical gating mechanism delivers strong out-of-distribution generalization compared to standard node-level gating
- MVMoE shows competitive performance on few-shot learning and real-world benchmark instances from CVRPLIB

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MVMoE achieves zero-shot generalization to unseen VRP variants by learning compositional representations across a shared set of constraints.
- Mechanism: The model is trained on a multi-task dataset containing different combinations of five VRP constraints (capacity, open route, backhaul, duration limit, time window). By exposing the model to these compositional variants, it learns to represent and recombine constraint attributes, enabling generalization to any unseen combination at test time.
- Core assumption: The structural similarity between VRP variants means a unified architecture can capture the underlying patterns without per-variant retraining.
- Evidence anchors:
  - [abstract]: "Unlike prior neural solvers that are tailored to individual VRP variants, MVMoE is trained on multiple VRP variants simultaneously, enabling zero-shot generalization to unseen VRPs without additional fine-tuning."
  - [section]: "We define the static and dynamic features as the union set of attributes that exist in all VRP variants. By training on a few VRP variants with these attributes, the policy network has the potential to solve unseen variants..."
- Break condition: If unseen variants introduce constraints outside the training union set, or if the compositional structure is too sparse, the zero-shot capability will fail.

### Mechanism 2
- Claim: The mixture-of-experts (MoE) layers increase model capacity without proportional computational cost by conditionally activating subsets of parameters.
- Mechanism: Each MoE layer contains multiple expert networks (FFNs) and a gating network. Inputs are routed to the top K experts based on gating scores, activating only a fraction of parameters per forward pass. This conditional computation allows scaling to more experts without linearly increasing compute.
- Core assumption: Sparse activation of experts yields equivalent or superior performance compared to dense models while reducing compute.
- Evidence anchors:
  - [abstract]: "MVMoE, which greatly enhances the model capacity without a proportional increase in computation."
  - [section]: "An input to the MoE layer is routed to specific expert(s) by a gating network, and only parameters in selected expert(s) are activated (i.e., conditional computation)."
- Break condition: If the gating network fails to balance load among experts, some may be underutilized, reducing effective capacity.

### Mechanism 3
- Claim: The hierarchical gating mechanism improves out-of-distribution generalization by learning a two-stage routing policy that balances efficiency and adaptability.
- Mechanism: The first gating network routes inputs to either a sparse expert layer or a dense layer based on problem-level features. If routed to the sparse layer, a second gating network performs node-level routing. This design allows the model to selectively use MoEs only when beneficial, reducing overfitting to training distributions.
- Core assumption: Problem-level routing decisions can identify when MoE specialization is needed versus when a dense layer suffices.
- Evidence anchors:
  - [abstract]: "We further develop a hierarchical gating mechanism for the MVMoE, delivering a good trade-off between empirical performance and computational complexity... Surprisingly, it exhibits much stronger out-of-distribution generalization capability than the base gating."
  - [section]: "To tackle the challenges, we propose to employ MoEs only in partial decoding steps. Accordingly, we present a hierarchical gating..."
- Break condition: If the first-stage gating becomes biased toward one path, the regularization benefit disappears.

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) layers
  - Why needed here: MoEs allow scaling model capacity for complex multi-task VRP learning without exploding compute, crucial for handling diverse constraint combinations.
  - Quick check question: What determines which experts are activated for a given input in an MoE layer?

- Concept: Hierarchical gating in neural architectures
  - Why needed here: Hierarchical gating balances computational efficiency and model expressiveness, especially important in autoregressive decoding where each step is costly.
  - Quick check question: How does the two-stage gating in MVMoE differ from standard node-level gating?

- Concept: Zero-shot generalization in multi-task learning
  - Why needed here: The paper's core contribution is enabling the solver to handle unseen VRP variants without retraining, relying on compositional generalization from shared constraints.
  - Quick check question: What training design enables a model to generalize to unseen constraint combinations?

## Architecture Onboarding

- Component map:
  - Static features → Node embeddings (with MoE FFN layers) → Encoder → Context + embeddings → Decoder → Node selection probabilities (with MoE final linear layer) → Feasibility masks → Valid solutions

- Critical path:
  1. Input features projected to embeddings via linear layer
  2. Encoder refines embeddings through self-attention + MoE FFNs
  3. Decoder iteratively selects nodes using context + embeddings
  4. MoE layers activated conditionally via hierarchical gating
  5. Feasibility masks ensure valid solutions

- Design tradeoffs:
  - MoE vs dense: Higher capacity vs. load balancing complexity
  - Node-level vs. hierarchical gating: Better performance vs. computational efficiency
  - Training data diversity: Better generalization vs. increased training cost

- Failure signatures:
  - Poor load balancing in MoE layers → Underutilized experts
  - Overfitting to training variants → Poor zero-shot performance
  - Hierarchical gating always choosing dense path → No MoE benefit
  - Excessive decoding steps → Infeasible solutions

- First 3 experiments:
  1. Ablate MoE layers: Replace with dense FFNs, measure zero-shot performance drop
  2. Test gating levels: Compare node-level, instance-level, and problem-level gating on efficiency and accuracy
  3. Vary expert count: Train with 4, 8, 16 experts, measure scaling behavior and generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the generalization performance of sparse MoE models be improved for large-scale VRPs?
- Basis in paper: [explicit] The paper mentions that MVMoE/4E performs poorly on large-scale instances (n > 500) due to generalization issues of sparse MoEs when transferring to new distributions or domains.
- Why unresolved: The paper identifies this as an open question in the MoE literature, suggesting that the reasons behind this poor generalization and potential solutions are not well understood.
- What evidence would resolve it: Experiments showing improved performance of sparse MoE models on large-scale VRPs, potentially through techniques like cross-size and cross-distribution training, or modifications to the MoE architecture.

### Open Question 2
- Question: How can a unified neural solver be developed for a broader range of combinatorial optimization problems beyond VRPs?
- Basis in paper: [explicit] The paper suggests venturing into generic representations for different problems as a future direction.
- Why unresolved: The paper focuses on VRPs, and extending the approach to other COPs would require significant research into problem representation and model architecture.
- What evidence would resolve it: Development of a neural solver that can handle a variety of COPs, potentially through a common problem formulation or a more general neural network architecture.

### Open Question 3
- Question: How can the interpretability of gating mechanisms in MoE-based VRP solvers be improved?
- Basis in paper: [explicit] The paper mentions that interpretability of gating mechanisms is a challenging task in the MoE literature.
- Why unresolved: The paper acknowledges that understanding the learned gating policy and its impact on performance is still an open question.
- What evidence would resolve it: Techniques for visualizing and analyzing the gating decisions, potentially leading to insights into how the gating mechanism contributes to the solver's performance.

## Limitations

- The zero-shot generalization capability may degrade for VRPs with constraints outside the training union set or for highly sparse constraint combinations not well-represented in training data.
- The computational efficiency gains from MoE layers are demonstrated empirically but lack theoretical guarantees about load balancing across experts in the proposed hierarchical architecture.
- The paper doesn't provide detailed analysis of when the first-stage gating chooses sparse vs. dense paths, limiting understanding of its selective MoE activation.

## Confidence

- **High Confidence**: MVMoE's architecture design (encoder-decoder with MoE layers and hierarchical gating) and its competitive performance on trained VRP variants are well-supported by the experimental results.
- **Medium Confidence**: The claims about zero-shot generalization to unseen VRP variants are supported by results but may not generalize to constraint types outside the training union set.
- **Medium Confidence**: The computational efficiency benefits of MoE layers are demonstrated but not rigorously quantified against theoretical baselines.

## Next Checks

1. Test MVMoE on VRP variants with constraints not in the training union set (e.g., pickup-and-delivery constraints) to assess zero-shot generalization limits.
2. Analyze the gating decisions in the hierarchical mechanism to verify that the first-stage gating selectively activates MoEs based on problem characteristics rather than defaulting to one path.
3. Measure and report the actual computational cost (FLOPs, memory usage) of MVMoE with hierarchical gating versus dense alternatives to quantify the efficiency gains empirically.