---
ver: rpa2
title: Speed of Light Exact Greedy Decoding for RNN-T Speech Recognition Models on
  GPU
arxiv_id: '2406.03791'
source_url: https://arxiv.org/abs/2406.03791
tags:
- cuda
- rnn-t
- time
- decoding
- greedy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the high inference time of RNN-T models by
  optimizing greedy decoding. The key insight is that current implementations leave
  the GPU idle 80% of the time due to kernel launch latency overhead.
---

# Speed of Light Exact Greedy Decoding for RNN-T Speech Recognition Models on GPU

## Quick Facts
- arXiv ID: 2406.03791
- Source URL: https://arxiv.org/abs/2406.03791
- Authors: Daniel Galvez; Vladimir Bataev; Hainan Xu; Tim Kaldewey
- Reference count: 0
- Key outcome: 2.5x end-to-end speedup for 1.1B parameter RNN-T model using CUDA 12.4 conditional nodes

## Executive Summary
This paper addresses the high inference time of RNN-T models by optimizing greedy decoding. The key insight is that current implementations leave the GPU idle 80% of the time due to kernel launch latency overhead. The authors propose leveraging CUDA 12.4's conditional nodes feature to create a CUDA graph implementation of greedy decoding that eliminates this idle time. This results in a 2.5x end-to-end speedup for a 1.1 billion parameter RNN-T model, and 1.7x and 1.4x speedups when applied to 1.1 billion parameter RNN-T and Token and Duration Transducer models respectively. The optimized implementation enables RNN-T models to run only 16% slower than similarly sized CTC models, contradicting the common belief that RNN-T models are not suitable for high throughput inference.

## Method Summary
The authors implement CUDA graphs with conditional nodes in PyTorch to eliminate CPU-GPU synchronization during RNN-T greedy decoding. By using conditional nodes, the GPU can handle control flow autonomously without CPU intervention, eliminating the 80% idle time caused by kernel launch latency. The implementation uses the NVIDIA NeMo toolkit and targets 1.1B parameter Parakeet RNN-T, CTC, and TDT models on A100-80GiB GPUs with batch size 32. The method leverages CUDA 12.4's new conditional nodes feature to create graphs that can make decisions based on conditions, allowing for dynamic computation graphs that were previously only possible with CPU control flow.

## Key Results
- 2.5x end-to-end speedup for 1.1B parameter RNN-T model (from 367.25 RTFx to 145.45 RTFx)
- 1.7x speedup for 1.1B parameter RNN-T model (from 197.22 RTFx to 115.58 RTFx)
- 1.4x speedup for 1.1B parameter TDT model (from 1525.07 RTFx to 1088.58 RTFx)
- 16% slower than similarly sized CTC model after optimization
- Maintains word error rate (WER) while achieving speedups

## Why This Works (Mechanism)

### Mechanism 1
CUDA 12.4's conditional nodes eliminate GPU idle time in RNN-T greedy decoding by allowing the GPU to handle control flow autonomously without CPU intervention. Traditional RNN-T greedy decoding requires CPU-GPU synchronization for control flow decisions, leaving the GPU idle ~80% of the time. The conditional nodes feature in CUDA 12.4 can effectively handle the data-dependent control flow required for RNN-T decoding.

### Mechanism 2
CUDA Graphs eliminate kernel launch latency overhead by launching a group of kernels at once rather than individually, amortizing the per-kernel launch cost. This is particularly beneficial for RNN-T decoding where each iteration involves multiple small kernels, and the kernel launch latency overhead is significant enough to warrant the overhead of creating and managing CUDA Graphs.

### Mechanism 3
Label looping algorithm complements CUDA Graphs for further optimization by reordering the nested loops in RNN-T decoding, reducing thread divergence and making more efficient use of the GPU. When combined with CUDA Graphs, this provides multiplicative performance benefits as the label looping algorithm's benefits are compatible with and complementary to the CUDA Graphs approach.

## Foundational Learning

- Concept: CUDA Graphs and their role in GPU programming
  - Why needed here: Understanding how CUDA Graphs work is essential to grasp why this optimization is possible and effective
  - Quick check question: What is the primary benefit of using CUDA Graphs over traditional kernel launching?

- Concept: RNN Transducer (RNN-T) architecture and greedy decoding
  - Why needed here: The paper's optimizations are specific to RNN-T models and their decoding algorithms
  - Quick check question: What is the main difference between RNN-T decoding and CTC decoding in terms of computational complexity?

- Concept: Kernel launch latency and its impact on GPU utilization
  - Why needed here: The paper's core insight is that kernel launch latency causes GPU idle time, which this optimization addresses
  - Quick check question: Approximately how much time does a typical kernel launch take, and why does this matter for RNN-T decoding?

## Architecture Onboarding

- Component map: Encoder → Joint Network → Decoder
- Critical path: Encoder → Decoder (most time-consuming component)
- Design tradeoffs:
  - CUDA Graphs vs. traditional kernel launching: CUDA Graphs reduce latency but add complexity
  - Label looping vs. standard greedy decoding: Label looping improves batch efficiency but may increase implementation complexity
  - Batch size selection: Larger batches improve GPU utilization but may introduce adversarial slowdowns
- Failure signatures:
  - GPU utilization remains low despite CUDA Graphs implementation
  - Performance gains are not as expected when combining CUDA Graphs with label looping
  - Increased memory usage or out-of-memory errors due to CUDA Graph overhead
- First 3 experiments:
  1. Implement CUDA Graphs for a simple RNN-T model without label looping and measure GPU utilization
  2. Compare performance of CUDA Graphs implementation vs. baseline implementation on various batch sizes
  3. Add label looping to the CUDA Graphs implementation and measure end-to-end performance gains

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance improvement from CUDA graphs scale with increasing batch sizes beyond 32? The paper mentions that larger batch sizes may be slowed down by adversarial inputs that generate max symbols per outer loop time step, and that batch size 32 hits a sweet spot where GPU utilization is high while the probability of adversarial elements is low. The paper only benchmarks at batch size 32, and the authors mention that larger batch sizes may be negatively impacted by adversarial inputs, but do not provide data on how performance scales with larger batch sizes. Benchmarking the CUDA graphs implementation at various batch sizes (e.g., 64, 128, 256) and comparing the RTFx and speed-up factors to the baseline implementation at each batch size would resolve this.

### Open Question 2
How does the performance of CUDA graphs for RNN-T decoding compare to other autoregressive models like Whisper? The paper mentions that CUDA graphs can potentially benefit any autoregressive model, and gives an example of Whisper having 5% idle time while waiting for the CPU to decide whether to call the autoregressive decoder again. The paper only benchmarks RNN-T models, and while it mentions the potential for CUDA graphs to benefit other autoregressive models, it does not provide any concrete performance comparisons. Benchmarking CUDA graphs for RNN-T decoding and Whisper decoding on the same hardware and dataset, and comparing the RTFx and speed-up factors for each model would resolve this.

### Open Question 3
How does the performance of CUDA graphs for RNN-T decoding compare to other optimization techniques like weight quantization? The paper mentions that weight-only quantization of Bfloat16 weights to int8/int4 weights can result in speed-ups of up to 35%/56% without accuracy loss in Transformer encoders, and suggests that this could be a promising avenue for further accelerating the end-to-end pipeline. The paper does not provide any performance comparisons between CUDA graphs and weight quantization for RNN-T decoding. Benchmarking CUDA graphs for RNN-T decoding and weight quantization for the encoder on the same hardware and dataset, and comparing the RTFx and speed-up factors for each optimization technique would resolve this.

## Limitations

- Results are specific to CUDA 12.4 and may not generalize to earlier or future CUDA versions
- Performance gains may be hardware-dependent, particularly on A100-80GiB GPUs
- Implementation complexity of CUDA Graphs and conditional nodes may present adoption barriers
- Limited exploration of how optimizations scale across different model sizes and batch sizes

## Confidence

- High Confidence: The core insight that kernel launch latency causes GPU idle time in RNN-T decoding is well-established and clearly demonstrated through measurements showing ~80% idle time in baseline implementations
- Medium Confidence: The effectiveness of CUDA Graphs and conditional nodes for eliminating this idle time, while demonstrated, may not generalize equally well across different hardware configurations or decoding scenarios
- Medium Confidence: The end-to-end speedup measurements, while compelling, are based on specific models and datasets and may not reflect real-world deployment scenarios with varying audio characteristics

## Next Checks

1. **Hardware Generalization Test**: Reproduce the speedup measurements on different GPU architectures (e.g., H100, V100) and configurations to verify that the CUDA Graphs optimization provides consistent benefits across hardware generations.

2. **Model Size Scaling Analysis**: Evaluate the performance gains across a range of model sizes (from 100M to 2B parameters) to determine if the 2.5x speedup holds for smaller or larger models, and identify any scaling thresholds.

3. **Real-world Deployment Validation**: Test the optimized implementation with diverse audio inputs including noisy speech, accented speech, and varying speaking rates to verify that the speedup gains don't come at the cost of robustness in practical deployment scenarios.