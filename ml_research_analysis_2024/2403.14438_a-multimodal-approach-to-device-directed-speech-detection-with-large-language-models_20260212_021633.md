---
ver: rpa2
title: A Multimodal Approach to Device-Directed Speech Detection with Large Language
  Models
arxiv_id: '2403.14438'
source_url: https://arxiv.org/abs/2403.14438
tags:
- audio
- speech
- text
- system
- gpt2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of detecting device-directed
  speech without requiring a trigger phrase, aiming to enable more natural interactions
  with virtual assistants. The core method involves using a multimodal approach that
  combines acoustic features extracted from audio encoders (Whisper and CLAP), text
  transcriptions from an ASR system, and decoder signals to provide input to a large
  language model (LLM).
---

# A Multimodal Approach to Device-Directed Speech Detection with Large Language Models

## Quick Facts
- arXiv ID: 2403.14438
- Source URL: https://arxiv.org/abs/2403.14438
- Reference count: 0
- Primary result: Multimodal system achieves 7.45% EER, 61.1% relative improvement over audio-only baseline

## Executive Summary
This paper introduces a multimodal approach to detecting device-directed speech (DDS) in virtual assistant interactions without requiring trigger phrases. The system combines audio features from encoders (Whisper and CLAP), text transcriptions from ASR, and decoder signals as input to a large language model (LLM). The best-performing configuration integrates all three modalities with additional text data, achieving an equal error rate (EER) of 7.45%, representing substantial improvements over unimodal baselines. The approach enables more natural interactions by detecting when users are speaking to their devices without explicit wake words.

## Method Summary
The method uses a pretrained LLM (GPT2) as a foundation, enhanced with parameter-efficient fine-tuning via LoRA. Three modalities are processed: audio features extracted by Whisper or CLAP, ASR 1-best hypotheses, and four utterance-level decoder signals (graph cost, acoustic cost, word-level posterior confidence, and average number of alternative word options). Mapping networks translate audio and decoder signal representations into the LLM's token embedding space, allowing joint learning from all modalities through cross-entropy loss. The system is trained on a balanced dataset of ~80k utterances (~126 hours) with an additional 3M text utterances for augmentation.

## Key Results
- Multimodal system achieves 7.45% EER, representing 61.1% relative improvement over best audio-only baseline and 38.7% over best text-only model
- Parameter-efficient fine-tuning with LoRA and larger LLM reduces EER to 6.53% with Whisper audio encoder
- Whisper audio encoder outperforms CLAP, particularly when using parameter-efficient fine-tuning
- Additional text data improves EER by 4.3% relative to text-only model without augmentation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining acoustic features with ASR decoder signals and 1-best hypotheses in a pretrained LLM improves device-directed speech detection accuracy.
- Mechanism: The LLM learns to integrate multimodal information by conditioning generation on audio embeddings (from Whisper or CLAP), decoder signal embeddings, and text embeddings. The mapping networks translate the audio and decoder signal representations into the LLM's token embedding space, enabling joint learning from all modalities.
- Core assumption: The multimodal representations capture complementary information that improves over unimodal baselines, and the LLM can effectively fuse these signals when trained with the cross-entropy objective.
- Evidence anchors: [abstract] "Using multimodal information yields relative equal-error-rate improvements over text-only and audio-only models of up to 39% and 61%."; [section] "The system is finetuned to generate device-directedness decisions by jointly learning from all modalities (cf. Figure 1)."; [corpus] Weak: Only 8 related papers found; no direct evidence of this specific mechanism.
- Break condition: If the mapping networks fail to translate between latent spaces, or if the modalities provide redundant or conflicting information, the multimodal approach may not outperform unimodal baselines.

### Mechanism 2
- Claim: Using Whisper audio encoder representations is more effective than CLAP for device-directed speech detection.
- Mechanism: Whisper is trained on a large dataset of audio paired with transcripts, including diverse acoustic conditions. This training makes Whisper better at capturing information that aligns with the ASR system's 1-best hypotheses and decoder signals, which are also derived from speech data.
- Core assumption: The ASR system and Whisper share similar training domains and capture overlapping information relevant to device-directed speech detection.
- Evidence anchors: [abstract] "Using parameter-efficient fine-tuning with LoRA and a larger LLM further reduces the EER to 6.53% with Whisper, though CLAP performs less effectively under these conditions."; [section] "We assume that including unbiased acoustic representations can provide useful additional information that is less likely to contain the same mistakes as the lexical information."; [corpus] Weak: No direct evidence in corpus neighbors supporting this claim.
- Break condition: If the ASR system is changed to one with a different training domain, or if the audio encoder is switched to one with a different focus (e.g., general audio understanding), this advantage may disappear.

### Mechanism 3
- Claim: Increasing LLM size and using parameter-efficient fine-tuning with LoRA further improves device-directed speech detection accuracy.
- Mechanism: Larger LLMs have more capacity to learn complex relationships between modalities. LoRA allows efficient adaptation of large models by learning low-rank updates to the transformer layers, reducing the number of trainable parameters while maintaining performance.
- Core assumption: The additional capacity of larger models and the efficiency of LoRA enable better learning of the multimodal task without overfitting or losing the model's general language understanding capabilities.
- Evidence anchors: [abstract] "Increasing the size of the LLM and training with low-rank adaption leads to further relative EER reductions of up to 18% on our dataset."; [section] "Using the GPT2 1.5B model yields EERs of 6.53% and 8.41% (MM6.3), which is a considerable improvement over experiment MM6 in Table 1 with the Whisper backbone."; [corpus] Weak: No direct evidence in corpus neighbors supporting this claim.
- Break condition: If the model becomes too large relative to the dataset size, overfitting may occur. If LoRA is not properly configured (e.g., rank too low or scaling factor too high), it may not provide the expected efficiency gains.

## Foundational Learning

- Concept: Multimodal learning and fusion
  - Why needed here: The task requires integrating information from different modalities (audio, text, decoder signals) to make accurate device-directed speech detection decisions.
  - Quick check question: How does the model handle missing or noisy data from one modality during inference?

- Concept: Transformer architectures and pretraining
  - Why needed here: The LLM is based on a transformer architecture and leverages pretraining on large text corpora to understand language and perform the device-directed speech detection task.
  - Quick check question: What is the role of the attention mechanism in the transformer architecture, and how does it help in multimodal fusion?

- Concept: Parameter-efficient fine-tuning techniques
  - Why needed here: LoRA is used to efficiently adapt the large pretrained LLM to the device-directed speech detection task without fine-tuning all model parameters.
  - Quick check question: How does LoRA differ from other parameter-efficient fine-tuning methods like adapter layers or prompt tuning?

## Architecture Onboarding

- Component map: Audio waveform -> Audio encoder (Whisper/CLAP) -> M1 -> LLM prefix; ASR decoder signals -> M2 -> LLM prefix; ASR 1-best hypotheses -> LLM input; LLM -> Device-directedness decision
- Critical path: Audio waveform -> Audio encoder -> M1 -> LLM prefix; ASR decoder signals -> M2 -> LLM prefix; ASR 1-best hypotheses -> LLM input; LLM -> Device-directedness decision
- Design tradeoffs:
  - Using Whisper vs. CLAP as the audio encoder: Whisper may be more effective due to its training on speech data, but CLAP may provide complementary information when all modalities are used.
  - Full fine-tuning vs. LoRA: Full fine-tuning may yield better performance but is less efficient for large models. LoRA is more efficient but may not capture all task-specific nuances.
  - Model size: Larger models may have more capacity but require more computational resources and may be prone to overfitting with limited data.
- Failure signatures:
  - High EER: The model may not be effectively fusing multimodal information or may be overfitting to the training data.
  - Low EER on training data but high EER on evaluation data: The model may be overfitting to the training data and not generalizing well.
  - Slow inference: The model may be too large or the hardware may not be sufficient for real-time processing.
- First 3 experiments:
  1. Implement and evaluate the unimodal baselines (audio-only, text-only, decoder signals-only) to establish a performance baseline.
  2. Implement and evaluate the multimodal system with Whisper as the audio encoder and full fine-tuning to assess the benefits of multimodal fusion.
  3. Implement and evaluate the multimodal system with CLAP as the audio encoder and full fine-tuning to compare the effectiveness of different audio encoders.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed multimodal system perform in real-world scenarios with continuous audio streams compared to its performance on the evaluated datasets?
- Basis in paper: [inferred] The paper mentions future work will focus on enhancing the approach with additional tasks and exploring longer contexts, such as including features from previous interactions. This suggests that the current evaluation may not fully capture performance in real-world, streaming scenarios.
- Why unresolved: The paper evaluates the system on a fixed dataset without addressing how it would perform in a streaming, real-time environment where audio is continuously captured and processed.
- What evidence would resolve it: Conducting experiments with streaming audio data, simulating real-time interactions, and comparing the system's performance metrics (e.g., EER) between fixed datasets and streaming scenarios would provide insights into its real-world applicability.

### Open Question 2
- Question: What is the impact of using different audio encoder backbones (e.g., other than Whisper and CLAP) on the system's performance in device-directed speech detection?
- Basis in paper: [explicit] The paper compares two audio encoder backbones (Whisper and CLAP) and notes that Whisper generally performs better, especially when parameter-efficient fine-tuning is used. However, it does not explore other potential audio encoders.
- Why unresolved: The study focuses on Whisper and CLAP but does not investigate other audio encoder models that might offer different strengths or weaknesses for the task.
- What evidence would resolve it: Experimenting with a variety of audio encoder models, such as those based on different architectures or trained on diverse datasets, and comparing their performance metrics (e.g., EER) would clarify the impact of the choice of audio encoder on the system's effectiveness.

### Open Question 3
- Question: How does the system's performance scale with increasing amounts of additional text-only training data?
- Basis in paper: [explicit] The paper mentions that including additional in-domain text data improves EER by 4.3% relative to the text-only model without additional text data. However, it does not explore the effects of using varying amounts of additional text data.
- Why unresolved: While the paper demonstrates the benefit of including some additional text data, it does not investigate whether performance continues to improve with larger amounts of such data or if there is a point of diminishing returns.
- What evidence would resolve it: Conducting experiments with different quantities of additional text-only training data and analyzing the corresponding changes in performance metrics (e.g., EER) would reveal how the system's effectiveness scales with the amount of supplementary text data.

## Limitations
- Evaluation based on single dataset from one virtual assistant provider, limiting generalizability
- Specific ASR system used is not fully specified, making it difficult to assess impact of different transcription qualities
- Paper lacks ablation studies on individual decoder signals and detailed error analysis to understand failure modes

## Confidence
- **High Confidence**: The core finding that multimodal fusion improves device-directed speech detection compared to unimodal approaches is well-supported by experimental results
- **Medium Confidence**: The claim that Whisper is more effective than CLAP is supported by results but the mechanism explanation is not directly validated
- **Low Confidence**: The claim that increasing LLM size and using LoRA always improves performance is not fully supported by systematic studies of different model sizes or LoRA configurations

## Next Checks
1. Conduct ablation study to quantify individual contributions of the four decoder signals (graph cost, acoustic cost, word-level posterior confidence, and average number of alternative word options) to overall performance
2. Evaluate best-performing model on dataset from different virtual assistant provider or publicly available dataset to assess generalizability
3. Perform detailed error analysis on evaluation set to identify common failure modes and understand types of errors the model makes