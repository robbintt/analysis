---
ver: rpa2
title: A Novel Machine Learning Classifier Based on Genetic Algorithms and Data Importance
  Reformatting
arxiv_id: '2412.13350'
source_url: https://arxiv.org/abs/2412.13350
tags:
- data
- gadic
- dataset
- performance
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the issue of low performance of machine learning
  classifiers due to the nature of the data. A novel algorithm called GADIC is proposed
  that combines data importance reformatting and genetic algorithms to enhance classifier
  performance.
---

# A Novel Machine Learning Classifier Based on Genetic Algorithms and Data Importance Reformatting

## Quick Facts
- arXiv ID: 2412.13350
- Source URL: https://arxiv.org/abs/2412.13350
- Authors: A. K. Alkhayyata; N. M. Hewahi
- Reference count: 10
- Primary result: GADIC improves classifier performance by 8.9-16.8% across five ML algorithms on seven datasets

## Executive Summary
This paper presents GADIC, a novel machine learning classifier that addresses the challenge of low classifier performance due to data-related issues. The algorithm combines data importance reformatting with genetic algorithms to enhance the performance of traditional ML classifiers. GADIC operates through three distinct phases: data reformatting based on attribute importance, genetic algorithm optimization of the reformatted data, and testing through averaging similar instances. When tested on seven diverse datasets using five different classifiers, GADIC demonstrated significant improvements in accuracy and F1-score, with KNN showing the highest average improvement of 16.79% in accuracy.

## Method Summary
GADIC is a three-phase algorithm that first reformats data based on attribute importance values calculated from how much attribute values change across instances. In the training phase, genetic algorithms optimize the reformatted data through evolutionary operations like crossover and mutation, using a fitness function that evaluates whether these operations improve classification performance. The testing phase averages the attributes of each test instance with its similar instances from the training set to reduce noise and improve generalization. The method was evaluated on seven datasets (Cleveland heart disease, Indian liver patient, Pima Indian diabetes, employee future prediction, telecom churn prediction, bank customer churn, tech students) using five classifiers (SVM, KNN, LR, DT, NB).

## Key Results
- GADIC improved accuracy by 8.9-16.8% across five classifiers on seven datasets
- KNN showed the highest improvement with 16.79% average increase in accuracy
- Logistic Regression had the lowest improvement among tested classifiers
- SVM and KNN achieved the highest overall accuracy improvements
- The algorithm effectively overcomes data-related impediments that typically affect classifier performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data importance reformatting improves classifier performance by emphasizing attribute values that change more across instances.
- Mechanism: The data reformatting phase calculates a "changing factor" β for each attribute. Attributes with more fluctuating values (smaller β) are given higher importance weights. This transforms raw attribute values into importance scores that better reflect their influence on the output.
- Core assumption: Attribute values that change more frequently across instances are more informative for classification.
- Evidence anchors:
  - [abstract] "GADIC comprises three phases which are data reformatting phase which depends on DI concept"
  - [section] "Based on potential changes in the attribute's values across the dataset, the importance of each input for each attribute is determined...The importance of each input is calculated using equations established by Hewahi (2019)"
- Break condition: If the dataset contains attributes where stability (not variability) is actually more predictive, or if attribute importance is better captured through other methods like mutual information.

### Mechanism 2
- Claim: Genetic algorithms optimize the reformatted data by evolving better attribute combinations.
- Mechanism: The training phase treats the entire training dataset as a population. Through crossover and mutation operations, new attribute combinations are created. A fitness function evaluates whether these combinations improve classification accuracy compared to previous generations.
- Core assumption: The fitness landscape of reformatted data has local optima that GA can discover through evolutionary search.
- Evidence anchors:
  - [abstract] "training phase where GA is applied on the reformatted training dataset"
  - [section] "In this phase, the training dataset will be subjected to GA operators in order to fine-tune the data...The fitness function in the proposed algorithm is designed to assess whether applying GA operators on the reformatted dataset will yield to an increase in the performance or not"
- Break condition: If the search space is too large for GA to effectively explore within reasonable generations, or if the fitness function doesn't properly capture classification performance.

### Mechanism 3
- Claim: Averaging similar instances during testing reduces noise and improves generalization.
- Mechanism: For each test instance, the algorithm finds n closest training instances using Euclidean distance. It then averages their attribute values to create a "smoothed" test instance that represents the local data distribution.
- Core assumption: Similar instances share common characteristics that, when averaged, produce more robust feature representations.
- Evidence anchors:
  - [abstract] "testing phase where the instances of the reformatted testing dataset are being averaged based on similar instances in the training dataset"
  - [section] "In this phase, the attributes values of each instance in the testing test is averaged using the attributes values of their similar instances in the training dataset"
- Break condition: If averaging overly smooths important distinctions between classes, or if the similarity measure doesn't capture relevant differences for classification.

## Foundational Learning

- Concept: Data importance calculation based on attribute value variability
  - Why needed here: This is the core transformation that makes the data more informative before applying ML algorithms
  - Quick check question: How does the changing factor β relate to attribute importance, and why does a smaller β indicate higher importance?

- Concept: Genetic algorithm operators (selection, crossover, mutation) and fitness evaluation
  - Why needed here: GA is used to optimize the reformatted data by evolving better attribute combinations
  - Quick check question: What is the role of the fitness function in determining whether offspring replace parents in the population?

- Concept: Similarity measurement and instance averaging
  - Why needed here: This testing phase step reduces noise by averaging similar instances, creating more robust test representations
  - Quick check question: How does the Euclidean distance function determine which training instances are "closest" to a test instance?

## Architecture Onboarding

- Component map:
  - Data Reformatting Module -> Genetic Algorithm Engine -> Similarity Matching Component -> Averaging Processor -> Classifier Wrapper

- Critical path:
  1. Preprocess raw data (remove irrelevant attributes, handle missing values, encode categoricals)
  2. Apply data reformatting phase to calculate and replace attribute values with importance scores
  3. Split reformatted data into training and testing sets
  4. Apply GA optimization to training set across multiple generations
  5. For each test instance, find similar training instances and compute averages
  6. Apply ML classifier to averaged test instances and evaluate performance

- Design tradeoffs:
  - Sample size vs. computational cost in data reformatting (larger samples = more accurate β but slower)
  - Number of GA generations vs. diminishing returns in optimization
  - Number of similar instances (n) to average vs. risk of oversmoothing
  - Choice of ML classifier compatibility with reformatted data format

- Failure signatures:
  - No improvement across multiple generations suggests the GA parameters or fitness function need adjustment
  - Performance degradation after averaging indicates too many instances are being averaged or poor similarity measurement
  - Consistent underperformance on specific datasets may indicate data reformatting assumptions don't hold for that data type

- First 3 experiments:
  1. Run GADIC on a small, well-understood dataset (like Iris) with default parameters to verify all components work together
  2. Test sensitivity to the number of similar instances (n) by varying from 1 to 10 and measuring accuracy changes
  3. Compare GADIC performance with and without the GA optimization phase to isolate its contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does GADIC's performance scale with significantly larger datasets beyond 20,000 instances?
- Basis in paper: [inferred] The paper suggests future studies should test GADIC on larger datasets.
- Why unresolved: The current experiments were limited to datasets with up to 20,000 instances, leaving scalability for larger datasets unexplored.
- What evidence would resolve it: Experimental results showing GADIC's performance metrics (accuracy, F1-score) on datasets with millions of instances compared to baseline classifiers.

### Open Question 2
- Question: Can GADIC be effectively extended to improve performance of deep learning models?
- Basis in paper: [explicit] The paper suggests future studies should test GADIC's potential for enhancing performance of other ML and DL models.
- Why unresolved: GADIC was only tested on traditional ML classifiers (SVM, KNN, LR, DT, NB), with no evaluation on deep learning architectures.
- What evidence would resolve it: Comparative results showing GADIC's impact on DL models (CNNs, RNNs, Transformers) across various datasets versus standard DL training approaches.

### Open Question 3
- Question: What is the optimal balance between data reformatting intensity and genetic algorithm parameters for different types of datasets?
- Basis in paper: [inferred] The paper mentions extensive parameter tuning but doesn't provide a systematic analysis of how different dataset characteristics affect optimal parameter choices.
- Why unresolved: The experiments focused on finding working parameters for specific datasets rather than establishing a generalizable framework for parameter selection based on dataset properties.
- What evidence would resolve it: A comprehensive study correlating dataset characteristics (size, feature types, class balance) with optimal GADIC parameter configurations, potentially leading to an adaptive parameter selection method.

## Limitations

- The data importance calculation method relies heavily on Hewahi (2019) without full specification of implementation details, particularly for handling float attributes
- Genetic algorithm parameters show substantial variation across datasets without clear guidance on optimal settings for new datasets
- The averaging approach in testing could potentially smooth away important class distinctions, though this isn't empirically validated
- Performance improvements may be dataset-specific and not generalize well to all types of classification problems

## Confidence

- High confidence: The three-phase architecture of GADIC (data reformatting, GA optimization, averaging) is clearly described and implemented
- Medium confidence: The general improvement patterns across classifiers are reliable, but specific parameter settings may not generalize
- Low confidence: The exact mechanism by which data importance reformatting improves performance, particularly for datasets where attribute variability doesn't correlate with predictive power

## Next Checks

1. **Parameter sensitivity analysis**: Systematically vary GA parameters (population size, generations, crossover/mutation rates) across all datasets to determine optimal settings and identify overfitting to specific parameter combinations

2. **Averaging ablation study**: Compare performance with different numbers of averaged instances (1, 3, 5, 7, 10) and evaluate whether averaging consistently improves or sometimes degrades performance

3. **Cross-dataset generalization**: Apply GADIC to at least 3 additional datasets from different domains not used in the original study to validate that improvements aren't dataset-specific artifacts