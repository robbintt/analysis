---
ver: rpa2
title: A Closer Look at Claim Decomposition
arxiv_id: '2403.11903'
source_url: https://arxiv.org/abs/2403.11903
tags:
- decomposition
- subclaims
- claim
- factscore
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how claim decomposition methods affect
  metrics of textual support like FACTSCORE. The authors introduce DECOMP SCORE, which
  measures decomposition quality by counting how many subclaims are supported by the
  original claim.
---

# A Closer Look at Claim Decomposition

## Quick Facts
- **arXiv ID**: 2403.11903
- **Source URL**: https://arxiv.org/abs/2403.11903
- **Reference count**: 17
- **Primary result**: Introduces DECOMP SCORE to measure claim decomposition quality, showing that Russellian/neo-Davidsonian inspired LLM decompositions produce more atomic and complete subclaims, leading to higher confidence in textual support evaluation metrics like FACTSCORE.

## Executive Summary
This paper investigates how different claim decomposition methods affect the reliability of textual support evaluation metrics, particularly FACTSCORE. The authors introduce DECOMP SCORE as a measure of decomposition quality that counts how many subclaims are supported by the original claim. They propose an LLM-based decomposition approach inspired by Russellian logical atomism and neo-Davidsonian semantics that generates more atomic and complete subclaims than previous methods. Experiments show this approach achieves the highest DECOMP SCORE and improves downstream textual support evaluation reliability when used with FACTSCORE.

## Method Summary
The authors compare six decomposition methods (DR-ND, DChen et al., DWICE, DFACTSCORE, DCoNLL-U, DPredPatt) on 500 biographies generated by 12 different language models. DECOMP SCORE measures decomposition quality by counting supported subclaims using an LLM validator. FACTSCORE measures textual support against external knowledge sources. The DR-ND method uses Russellian/neo-Davidsonian principles to create more atomic decompositions by breaking claims into unary and binary predicates, while other methods use structured approaches or LLM prompting with varying degrees of atomicity and coherence.

## Key Results
- DR-ND (Russellian/neo-Davidsonian) achieves the highest DECOMP SCORE by producing more atomic and complete subclaims
- Methods with higher atomicity (DR-ND, DChen et al.) achieve higher DECOMP SCORE than less atomic methods (DWICE, DFACTSCORE)
- FACTSCORE reliability improves when using higher quality decompositions that filter out unsupported subclaims

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DECOMP SCORE measures decomposition quality by counting how many subclaims are supported by the original claim
- Mechanism: By evaluating whether each subclaim is entailed or supported by the original claim (using the same LLM validator), DECOMP SCORE quantifies how faithfully a decomposition method preserves the original claim's content
- Core assumption: An LLM can reliably determine whether a subclaim is supported by or entailed by the original claim
- Evidence anchors:
  - [abstract]: "DECOMP SCORE measures decomposition quality, an important step in determining the reliability of the downstream metric"
  - [section 3.2]: "DECOMP SCORE of a decomposition method is the average number of supported subclaims per passage produced by that decomposition method"
  - [corpus]: Weak - only mentions DECOMP SCORE indirectly through related papers
- Break condition: If the LLM validator cannot reliably determine support/entailment between subclaims and original claims, DECOMP SCORE becomes meaningless

### Mechanism 2
- Claim: Higher atomicity and coverage in decompositions lead to higher DECOMP SCORE
- Mechanism: More atomic decompositions break down claims into smaller, more specific subclaims, while better coverage ensures all parts of the original claim are captured, resulting in more supported subclaims
- Core assumption: Atomic and complete decompositions contain more subclaims that are supported by the original claim
- Evidence anchors:
  - [section 3.1]: "the subclaims should cover all parts of the claim and also be as atomic as possible"
  - [section 8.1]: "DR-ND...contains the greatest number of subclaims supported by the original claim"
  - [corpus]: Missing - no direct evidence about atomicity vs coverage tradeoff
- Break condition: If decompositions become too atomic (overly fragmented) or miss important information, DECOMP SCORE will decrease

### Mechanism 3
- Claim: Russellian/neo-Davidsonian inspired decompositions produce higher quality results
- Mechanism: By decomposing claims into unary predicates (properties of individuals) and binary predicates (relations between individuals), this approach creates more atomic and complete subclaims that better capture the original claim's meaning
- Core assumption: Russellian/neo-Davidsonian decomposition principles lead to more complete and atomic subclaims than other approaches
- Evidence anchors:
  - [section 5]: "we manually decompose the 21 in-context examples...into lists of such Russellian atomic propositions"
  - [section 8.1]: "DR-ND attains the highest DECOMP SCORE...followed by DChen et al. and DFACTSCORE"
  - [corpus]: Weak - only mentions DecMetrics which uses structured decomposition but not Russellian approach
- Break condition: If Russellian principles don't actually produce better decompositions than other methods, or if manual decomposition is required, this approach won't scale

## Foundational Learning

- Concept: Claim decomposition and its relationship to textual support evaluation
  - Why needed here: Understanding that decomposition quality directly affects downstream metrics like FACTSCORE is crucial for this work
  - Quick check question: If a decomposition method produces many subclaims but they're not supported by the original claim, what happens to DECOMP SCORE?

- Concept: Logical atomism and neo-Davidsonian semantics
  - Why needed here: These philosophical and linguistic theories provide the foundation for the proposed decomposition approach
  - Quick check question: How does decomposing claims into unary and binary predicates improve atomicity?

- Concept: Entailment and support relationships
  - Why needed here: The work relies on determining whether subclaims are supported by or entailed by the original claim
  - Quick check question: What's the difference between a subclaim being supported by vs entailed by the original claim?

## Architecture Onboarding

- Component map:
  Original claim → Decomposition method → Subclaims → Validator → DECOMP SCORE/FACTSCORE

- Critical path: Original claim → Decomposition method → Subclaims → Validator → DECOMP SCORE/FACTSCORE

- Design tradeoffs:
  - Flexibility vs structure: LLM prompting allows flexible decompositions but lacks control; structured methods are more controlled but may miss information
  - Atomicity vs coherence: More atomic decompositions may sacrifice coherence with the original claim
  - Manual vs automatic: Russellian approach requires manual decomposition examples but may produce better results

- Failure signatures:
  - Low DECOMP SCORE: Decomposition method is missing important subclaims or producing incoherent subclaims
  - Low FACTSCORE: Either the decomposition is poor or the generated text lacks support from external sources
  - High computational cost: LLM-based methods are expensive to run at scale

- First 3 experiments:
  1. Compare DECOMP SCORE across all decomposition methods on a small, manually-annotated dataset
  2. Evaluate the effect of in-context examples count on decomposition quality (similar to section 8.2)
  3. Test filtering out unsupported subclaims and measuring impact on FACTSCORE (as in section 8.2)

## Open Questions the Paper Calls Out
- How does the choice of decomposition method affect the reliability of FACTSCORE when evaluating generated text across different domains beyond biographies?
- What is the optimal balance between atomicity and coherence in claim decomposition for maximizing the reliability of downstream textual support metrics?
- How do different semantic representations (e.g., predicate-argument structures vs. dependency parses) impact the quality of claim decomposition and subsequent textual support evaluation?

## Limitations
- Reliance on LLM-based decomposition and validation raises scalability, cost, and reproducibility concerns
- Manual decomposition process for creating in-context examples (DR-ND) limits real-world deployment
- Evaluation constrained to biographies, limiting generalizability to other claim types or complex reasoning tasks

## Confidence
- High Confidence: The relationship between decomposition quality and downstream metric reliability (DECOMP SCORE concept)
- Medium Confidence: The Russellian/neo-Davidsonian approach producing better decompositions than alternatives, based on limited manual examples
- Low Confidence: The scalability and practical utility of the proposed LLM-based decomposition approach in real-world applications

## Next Checks
1. Test the decomposition methods on a diverse set of claim types beyond biographies (e.g., scientific claims, opinion statements, factual assertions) to assess generalizability.
2. Conduct a human evaluation study comparing the quality of subclaims generated by DR-ND versus other methods to validate the automatic DECOMP SCORE metric.
3. Measure the computational cost and latency of each decomposition method at scale to assess practical deployment feasibility.