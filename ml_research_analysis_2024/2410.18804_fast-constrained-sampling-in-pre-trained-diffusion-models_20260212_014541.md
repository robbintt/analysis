---
ver: rpa2
title: Fast constrained sampling in pre-trained diffusion models
arxiv_id: '2410.18804'
source_url: https://arxiv.org/abs/2410.18804
tags:
- image
- diffusion
- which
- should
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of efficiently generating high-quality
  images under arbitrary constraints using pre-trained diffusion models. Existing
  methods either rely on backpropagation through the denoiser network, leading to
  increased computation time and memory usage, or fail to capture long-range correlations
  in the generated image by only enforcing local constraints.
---

# Fast constrained sampling in pre-trained diffusion models

## Quick Facts
- arXiv ID: 2410.18804
- Source URL: https://arxiv.org/abs/2410.18804
- Authors: Alexandros Graikos; Nebojsa Jojic; Dimitris Samaras
- Reference count: 40
- Primary result: Achieves comparable or superior results to state-of-the-art training-free inference methods on both linear and non-linear constraints while requiring significantly less inference time.

## Executive Summary
This paper addresses the challenge of efficiently generating high-quality images under arbitrary constraints using pre-trained diffusion models. Existing methods either rely on backpropagation through the denoiser network, leading to increased computation time and memory usage, or fail to capture long-range correlations in the generated image by only enforcing local constraints. To overcome these limitations, the authors propose an algorithm that approximates Newton's optimization method using forward passes through the denoiser network, avoiding expensive backpropagation operations.

## Method Summary
The proposed algorithm approximates Newton's optimization method using forward passes through the denoiser network. It employs an inexact Newton approach, approximating the Jacobian inverse with the Jacobian matrix itself, leveraging the relationship between noisy input and clean output in denoising diffusion models. The method uses finite differences to compute Jacobian-vector products with only two forward passes through the denoiser, significantly reducing inference time while maintaining image quality. The algorithm achieves comparable or superior results to state-of-the-art training-free inference methods on both linear (inpainting, super-resolution) and non-linear (style-guided generation) constraints.

## Key Results
- Achieves FID score of 19.39 on free-form inpainting in 15 seconds
- Outperforms previous methods (46.72 FID in 8 minutes for LDPS)
- Demonstrates superior long-range correlation preservation compared to gradient descent approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The proposed update direction J e approximates Newton's method by leveraging the denoising diffusion model's Jacobian matrix as an inverse Jacobian approximation.
- Mechanism: In denoising diffusion models, the Jacobian J = ∇_x_t x̂_0(x_t) transforms a local change in the noisy input x_t to a change in the clean output x̂_0. The proposed update e_t = J e uses this same transformation instead of computing J⁻¹, approximating the Newton step.
- Core assumption: The input and output of the denoiser are closely related through additive Gaussian noise, making J approximately symmetric.
- Evidence anchors:
  - [abstract]: "we propose an algorithm that approximates Newton's optimization method using forward passes through the denoiser network"
  - [section]: "we propose to approximate the Jacobian inverse in the Newton step with the Jacobian matrix itself"
  - [corpus]: No direct evidence in corpus neighbors

### Mechanism 2
- Claim: Avoiding backpropagation through the denoiser model significantly reduces inference time and memory usage while maintaining image quality.
- Mechanism: The update direction e_t = J e can be computed using finite differences with only two forward passes through the denoiser: one to compute x̂_0(x_t) and another to compute x̂_0(x_t + δe).
- Core assumption: The denoiser model is a deterministic function that can be evaluated efficiently with forward passes.
- Evidence anchors:
  - [abstract]: "Our approach produces results that rival or surpass the state-of-the-art training-free inference methods while requiring a fraction of the time"
  - [section]: "requiring no backpropagation but instead two forward passes through the network"
  - [corpus]: No direct evidence in corpus neighbors

### Mechanism 3
- Claim: The proposed update direction J e captures long-range correlations better than gradient descent direction J^T e used in previous methods.
- Mechanism: The Jacobian J transforms local changes in the input to changes in the output, maintaining shapes and structures across the image. In contrast, J^T e only considers local gradient information.
- Core assumption: The denoising diffusion model has learned to preserve long-range correlations during training.
- Evidence anchors:
  - [section]: "the direction computed from J is sharper in some regions, like the outlines"
  - [section]: "the model is trying to change correlated parts of the image together, i.e. change both eyes or ears simultaneously"
  - [corpus]: No direct evidence in corpus neighbors

## Foundational Learning

- Concept: Linear algebra (matrix inversion, Jacobian matrices, spectral norms)
  - Why needed here: The algorithm relies on understanding how to compute and approximate Jacobian matrices and their inverses, as well as analyzing convergence using spectral norms.
  - Quick check question: What is the relationship between the spectral norm of a matrix and the convergence of iterative methods?

- Concept: Optimization methods (Newton's method, gradient descent, inexact Newton methods)
  - Why needed here: The algorithm is based on approximating Newton's method and understanding the differences between Newton steps and gradient descent steps.
  - Quick check question: How does the convergence rate of Newton's method compare to gradient descent, and what are the trade-offs?

- Concept: Denoising diffusion models (forward process, reverse process, score matching)
  - Why needed here: The algorithm operates on pre-trained denoising diffusion models and leverages their properties for constrained sampling.
  - Quick check question: How does the score function relate to the denoising process in diffusion models?

## Architecture Onboarding

- Component map: Pre-trained denoising diffusion model (denoiser network) -> Constraint function (linear or non-linear) -> Optimization algorithm (inexact Newton method) -> Sampling method (DDIM or other diffusion solvers)
- Critical path: Compute error direction e -> Approximate Jacobian-vector product J e -> Update noisy input x_t -> Apply diffusion step -> Repeat until convergence
- Design tradeoffs: Using forward passes instead of backpropagation reduces inference time but requires careful choice of finite difference step size δ. The inexact Newton approximation may converge slower than exact methods but is much more computationally efficient.
- Failure signatures: Poor convergence when learning rate λ is too high, blurry results when optimization steps are insufficient, artifacts when the Jacobian approximation is inaccurate.
- First 3 experiments:
  1. Implement the proposed algorithm on a simple denoising diffusion model (e.g., MNIST) with a linear constraint (e.g., inpainting a single pixel) and compare to gradient descent.
  2. Test the algorithm on a pre-trained diffusion model (e.g., Stable Diffusion) with a linear constraint (e.g., free-form inpainting) and measure inference time and image quality.
  3. Apply the algorithm to a non-linear constraint (e.g., style-guided generation) and evaluate the tradeoff between constraint satisfaction and image quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the spectral norm of the Jacobian matrix change across different diffusion model architectures and training objectives?
- Basis in paper: [explicit] The paper analyzes the spectral properties of the Jacobian matrix and discusses how the symmetric and skew-symmetric components affect convergence.
- Why unresolved: The analysis is limited to Stable Diffusion 1.4, and it's unclear if these spectral properties generalize to other architectures or training objectives.
- What evidence would resolve it: Spectral norm analysis of Jacobians for various diffusion model architectures and training objectives, showing patterns or differences.

### Open Question 2
- Question: Can the proposed algorithm be extended to handle non-differentiable constraints without resorting to gradient descent for the constraint Jacobian?
- Basis in paper: [inferred] The paper mentions using gradient descent for non-linear constraints and discusses potential inexact Newton methods for non-differentiable constraints.
- Why unresolved: The paper only provides a toy example of a non-differentiable constraint and doesn't explore practical applications or efficient methods.
- What evidence would resolve it: Demonstrations of the algorithm handling complex non-differentiable constraints in practical scenarios, with comparisons to gradient descent approaches.

### Open Question 3
- Question: What is the impact of the number of optimization steps K on the quality of generated images, and is there an optimal strategy for determining K dynamically?
- Basis in paper: [explicit] The paper ablates the number of optimization steps K and learning rate λ, finding that K=5 and λ=0.5 achieve the best results, but also discusses convergence behavior.
- Why unresolved: The paper uses a fixed number of steps for each task, and it's unclear if a dynamic strategy based on the current state or constraint satisfaction would improve results.
- What evidence would resolve it: Experiments comparing fixed vs. dynamic K strategies across various tasks, showing improvements in image quality or constraint satisfaction.

### Open Question 4
- Question: How does the proposed algorithm perform on constrained sampling tasks in domains other than images, such as audio or video generation?
- Basis in paper: [inferred] The paper focuses on image generation and mentions potential applications in medical imaging, but doesn't explore other domains.
- Why unresolved: The algorithm is designed for diffusion models, which have been applied to various domains, but its effectiveness on constrained sampling in these domains is unknown.
- What evidence would resolve it: Demonstrations of the algorithm on constrained sampling tasks in audio, video, or other domains, with comparisons to existing methods.

### Open Question 5
- Question: What are the limitations of the proposed algorithm when applied to extremely high-resolution images or very complex constraints?
- Basis in paper: [explicit] The paper discusses limitations with high classifier-free guidance and time-distilled models, but doesn't explore extreme resolution or complexity.
- Why unresolved: The experiments are conducted on standard resolution images and relatively simple constraints, and it's unclear how the algorithm scales to more demanding scenarios.
- What evidence would resolve it: Experiments on extremely high-resolution images and very complex constraints, showing performance degradation or identifying specific bottlenecks.

## Limitations

- The empirical nature of the Jacobian approximation means effectiveness depends heavily on specific denoising model architecture and training procedure
- Convergence guarantees assume bounded spectral norms that may not hold for all pre-trained models
- Performance on highly complex non-linear constraints beyond style transfer remains untested

## Confidence

- **High**: The computational efficiency gains (2-3x speedup) and basic linear constraint results are well-validated
- **Medium**: The theoretical approximation of Newton's method using Jacobian matrices is sound, but empirical performance varies across different constraints
- **Medium**: The qualitative improvements in long-range correlation preservation are demonstrated but not rigorously quantified

## Next Checks

1. Test the method on additional non-linear constraints (e.g., spatial layout constraints, multi-modal style mixing) to verify generalization beyond the demonstrated style transfer
2. Conduct ablation studies varying the finite difference step size δ and learning rate λ to establish robust hyperparameter ranges across different model architectures
3. Compare against exact Newton methods on smaller-scale problems where backpropagation is feasible to quantify the approximation error introduced by the Jacobian approximation strategy