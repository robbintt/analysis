---
ver: rpa2
title: Towards Probabilistically-Sound Beam Search with Masked Language Models
arxiv_id: '2402.15020'
source_url: https://arxiv.org/abs/2402.15020
tags:
- beam
- search
- standard
- infilling
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of performing beam search with
  masked language models (MLMs) by developing probabilistically-sound methods. MLMs,
  unlike autoregressive models, do not have joint probability distributions readily
  available, making it difficult to apply beam search directly.
---

# Towards Probabilistically-Sound Beam Search with Masked Language Models

## Quick Facts
- arXiv ID: 2402.15020
- Source URL: https://arxiv.org/abs/2402.15020
- Authors: Creston Brooks; Robert Calef; Charlie Cowen-Breen; Anna Sappington
- Reference count: 17
- Primary result: HCB beam search outperforms standard beam search with no additional computational complexity

## Executive Summary
This paper addresses the challenge of applying beam search to masked language models (MLMs), which lack explicit joint probability distributions unlike autoregressive models. The authors develop a probabilistically-sound modification to beam search by incorporating a correction term based on the Hammersley-Clifford-Besag theorem, which ensures consistent probability scoring. Empirical evaluations demonstrate superior performance across multiple domains including English text, ancient texts, and protein sequences, with improvements in top-k accuracy, BLEU scores, and BERTScore F1 metrics.

## Method Summary
The paper introduces HCB beam search, a modification to standard beam search that makes it probabilistically sound for MLMs. The key innovation is a correction term derived from the Hammersley-Clifford-Besag theorem that accounts for the conditional independence structure inherent in MLMs. This correction ensures that the scoring function properly accounts for the factorization of the joint probability distribution, eliminating inconsistencies that arise from naive application of beam search to MLMs. The method maintains the same computational complexity as standard beam search while improving search quality through more accurate probability estimation.

## Key Results
- HCB beam search outperforms standard beam search across English text, ancient texts, and protein sequence domains
- Higher top-k accuracy, BLEU scores, and BERTScore F1 metrics demonstrate improved generation quality
- The method shows contextual sensitivity to mask tokens and benefits from optimal pivot selection strategies
- No additional computational complexity compared to standard beam search despite probabilistic soundness guarantees

## Why This Works (Mechanism)
The method works by correcting the probability scoring in beam search to account for the conditional independence structure of MLMs. Standard beam search assumes explicit joint probabilities, but MLMs only provide conditional probabilities given observed context. The HCB theorem provides a way to factorize the joint distribution into conditional probabilities, and the correction term ensures that the accumulated scores properly represent the true joint probability. This eliminates the inconsistency where different search paths could lead to the same partial sequence with different scores, ensuring probabilistic soundness.

## Foundational Learning

**Hammersley-Clifford-Besag Theorem**: A result in probability theory that characterizes the factorization of joint probability distributions in terms of conditional probabilities on a graph structure. Needed to understand how to properly factorize MLM probabilities; check by verifying understanding of conditional independence in graphical models.

**Masked Language Models vs Autoregressive Models**: MLMs predict tokens given bidirectional context, while autoregressive models predict tokens sequentially. Needed to understand why standard beam search fails with MLMs; check by comparing probability factorization approaches.

**Beam Search Algorithm**: A heuristic search algorithm that explores the most promising candidates by maintaining a set of partial solutions. Needed as the baseline algorithm being modified; check by tracing through standard beam search on a simple example.

**Conditional Probability Factorization**: The process of decomposing joint probabilities into products of conditional probabilities. Needed to understand how MLMs represent probability distributions; check by working through a small MLM probability calculation.

**Pivot Selection in Beam Search**: The choice of which token positions to expand at each search step. Needed to understand the experimental design and optimization; check by analyzing how different pivot choices affect search efficiency.

## Architecture Onboarding

**Component Map**: MLM (provides conditional probabilities) -> HCB Correction (factorizes joint probabilities) -> Beam Search (maintains candidates) -> Output Sequence (final generation)

**Critical Path**: MLM inference provides conditional probabilities for masked positions → HCB correction computes joint probability estimates → Beam search maintains top-k candidates → Final sequence selected from completed candidates

**Design Tradeoffs**: Probabilistic soundness vs computational efficiency (addressed by maintaining O(1) complexity), accuracy vs search breadth (managed by beam size), and model generality vs domain-specific optimization (handled through pivot selection)

**Failure Signatures**: Degraded performance when pivot selection is suboptimal, loss of probabilistic soundness if HCB correction is omitted, and computational overhead if implementation doesn't properly leverage MLM efficiency

**First Experiments**: 1) Verify HCB correction produces consistent scores for equivalent partial sequences, 2) Compare standard vs HCB beam search on small synthetic datasets, 3) Test pivot selection sensitivity on a single MLM architecture

## Open Questions the Paper Calls Out
None

## Limitations
- Empirical validation is limited to a narrow set of tasks and metrics, potentially missing performance issues in other NLP applications
- The HCB-based correction assumes well-specified graphical model structure, which may not hold for all MLM architectures or domain adaptations
- Computational trade-offs for very large vocabularies or long sequences are not thoroughly addressed despite claims of no additional complexity
- Optimal pivot selection appears model-dependent, requiring careful hyperparameter tuning for different use cases

## Confidence

**Major Claims Confidence Labels:**
- Probabilistically-sound beam search formulation: High
- HCB correction eliminates scoring inconsistencies: Medium
- No additional computational complexity: Low
- Superior performance across diverse domains: Medium

## Next Checks
1. Test the HCB beam search on additional MLM architectures (e.g., ELECTRA, DeBERTa) and tasks beyond text generation, such as machine translation or summarization, to assess generalizability.
2. Conduct ablation studies to isolate the impact of the HCB correction term versus other factors (e.g., pivot selection, candidate filtering) on performance improvements.
3. Evaluate the method's scalability and runtime efficiency on large-scale vocabularies and long sequences to verify the claim of no additional computational complexity.