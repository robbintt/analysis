---
ver: rpa2
title: A Moonshot for AI Oracles in the Sciences
arxiv_id: '2406.17836'
source_url: https://arxiv.org/abs/2406.17836
tags:
- empirical
- theories
- mathematical
- scientific
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This perspective paper addresses the challenge of creating AI
  systems capable of generating revolutionary mathematical theories, a capability
  Nobel laureate Philip Anderson and Elihu Abrahams deemed unlikely for machines.
  The authors propose three necessary conditions for such AI theorists: the ability
  to formally conjecture and prove mathematical statements, represent and manipulate
  ontological concepts from scientific and manifest images, and combine these abilities
  to form empirical statements.'
---

# A Moonshot for AI Oracles in the Sciences

## Quick Facts
- arXiv ID: 2406.17836
- Source URL: https://arxiv.org/abs/2406.17836
- Authors: Bryan Kaiser; Tailin Wu; Maike Sonnewald; Colin Thackray; Skylar Callis
- Reference count: 40
- Key outcome: This paper proposes three necessary conditions for AI systems to generate revolutionary mathematical theories and introduces a heuristic "Galilean intelligibility" metric to quantify epistemic transparency.

## Executive Summary
This perspective paper addresses the challenge of creating AI systems capable of generating revolutionary mathematical theories in science, a capability Nobel laureate Philip Anderson and Elihu Abrahams deemed unlikely for machines. The authors propose three necessary conditions for such AI theorists: the ability to formally conjecture and prove mathematical statements, represent and manipulate ontological concepts from scientific and manifest images, and combine these abilities to form empirical statements. They introduce a heuristic definition of "Galilean intelligibility" to quantify the epistemic transparency of mathematical theories, defined as the ratio of empirical constants to ontologically grounded variables in empirical statements. Using this metric, the authors analyze various scientific theories and AI models, showing that conventional deep neural networks typically have very low intelligibility (e.g., I ≈ -2000 for AlphaFold 2) due to their large number of parameters relative to the number of ontological variables they relate.

## Method Summary
The paper presents a conceptual framework rather than a computational method. The authors define three necessary conditions for AI theorists: formal mathematical reasoning capability, ontological concept representation, and empirical statement formation. They then introduce a heuristic intelligibility metric (I) calculated as the ratio of empirical constants to ontologically grounded variables in empirical statements. This metric is applied to various scientific theories and AI models to assess their epistemic transparency. The approach is primarily theoretical, using mathematical analysis and logical argumentation to support their claims about the requirements for revolutionary AI scientific theories.

## Key Results
- Proposed three necessary conditions for AI theorists: formal mathematical reasoning, ontological concept manipulation, and empirical statement formation
- Introduced "Galilean intelligibility" metric I = empirical constants / ontologically grounded variables to quantify theory transparency
- Demonstrated that conventional deep neural networks have very low intelligibility (I ≈ -2000 for AlphaFold 2) due to parameter explosion relative to ontological variables

## Why This Works (Mechanism)
The proposed framework works by establishing a clear mathematical and conceptual foundation for what constitutes a "revolutionary" scientific theory. By defining intelligibility in terms of the ratio between empirically measurable quantities and ontologically meaningful variables, the authors create a quantitative framework for evaluating both human and AI-generated theories. The mechanism relies on the principle that truly revolutionary theories should be transparent and comprehensible, with a small number of ontological variables explaining a large number of empirical observations.

## Foundational Learning
1. **Galilean Intelligibility (I)** - A heuristic metric quantifying theory transparency as the ratio of empirical constants to ontologically grounded variables. Why needed: To provide a quantitative measure of what makes theories "revolutionary" and comprehensible. Quick check: Calculate I for F=ma (I=1) and compare to deep learning models (I ≈ -2000).

2. **Ontological Variables** - Fundamental conceptual entities in scientific theories that correspond to real-world phenomena. Why needed: To distinguish between meaningful scientific variables and mere mathematical parameters. Quick check: Identify ontological variables in Newtonian mechanics vs. neural network parameters.

3. **Scientific Image vs. Manifest Image** - The distinction between scientific theoretical constructs and observable phenomena. Why needed: To understand how theories bridge abstract concepts and empirical reality. Quick check: Map concepts from both images in a simple physical theory.

4. **Formal Mathematical Reasoning** - The ability to conjecture and prove mathematical statements. Why needed: As a necessary condition for generating rigorous scientific theories. Quick check: Verify proof capabilities in formal logic systems.

5. **Empirical Statement Formation** - Combining mathematical and ontological elements to create testable scientific claims. Why needed: To ensure AI-generated theories connect to observable phenomena. Quick check: Generate empirical predictions from theoretical frameworks.

## Architecture Onboarding

**Component Map:**
Formal Mathematical Reasoning -> Ontological Concept Representation -> Empirical Statement Formation

**Critical Path:**
1. Input scientific data and domain knowledge
2. Generate mathematical conjectures using formal reasoning
3. Map conjectures to ontological concepts
4. Form empirical statements connecting theory to observations
5. Evaluate intelligibility metric (I)

**Design Tradeoffs:**
- Expressiveness vs. Interpretability: More complex mathematical frameworks may capture more phenomena but reduce intelligibility
- Data-driven vs. Theory-driven: Balancing empirical observations with theoretical foundations
- Computational complexity vs. Transparency: More sophisticated models may be less intelligible

**Failure Signatures:**
- Low intelligibility scores (I << 0) indicating excessive parameters relative to ontological variables
- Inability to generate novel empirical predictions
- Failure to connect formal mathematics to observable phenomena
- Over-reliance on data-driven approaches without theoretical foundations

**3 First Experiments:**
1. Implement a simple ontology-aware theorem prover and measure its intelligibility on basic physics problems
2. Create a toy model that generates empirical statements from ontological concepts and evaluate its predictive power
3. Develop a proof-of-concept system that combines formal reasoning with empirical data to generate novel scientific hypotheses

## Open Questions the Paper Calls Out
None

## Limitations
- The heuristic intelligibility metric (I) lacks rigorous mathematical grounding and universal applicability
- The framework may over-emphasize mathematical formalism at the expense of empirical validation
- The focus on formal mathematical theories may not capture all forms of scientific discovery

## Confidence
- Medium: The three necessary conditions for AI theorists are well-reasoned and logically consistent
- Medium: The concept of ontological variables as a basis for measuring intelligibility
- Low: The quantitative intelligibility metric (I) and its practical applicability

## Next Checks
1. Test the intelligibility metric (I) on a broader range of established scientific theories to verify its predictive power in distinguishing between "transparent" and "opaque" theories.

2. Implement a computational framework that attempts to satisfy the three proposed conditions and evaluate its ability to generate novel, testable hypotheses in a specific scientific domain.

3. Conduct expert interviews with practicing scientists to assess whether the intelligibility metric aligns with their intuitive understanding of what makes a theory "comprehensible" or "revolutionary."