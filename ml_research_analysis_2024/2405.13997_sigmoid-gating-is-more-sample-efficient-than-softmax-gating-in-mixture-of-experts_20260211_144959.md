---
ver: rpa2
title: Sigmoid Gating is More Sample Efficient than Softmax Gating in Mixture of Experts
arxiv_id: '2405.13997'
source_url: https://arxiv.org/abs/2405.13997
tags:
- experts
- gating
- expert
- function
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a rigorous theoretical analysis of the sample
  efficiency of sigmoid gating in mixture of experts (MoE) models compared to the
  widely-used softmax gating. Under a regression framework where the true regression
  function is modeled as a mixture of experts, the authors study the convergence rates
  of least squares estimators when the number of fitted experts exceeds the true number.
---

# Sigmoid Gating is More Sample Efficient than Softmax Gating in Mixture of Experts

## Quick Facts
- arXiv ID: 2405.13997
- Source URL: https://arxiv.org/abs/2405.13997
- Authors: Huy Nguyen; Nhat Ho; Alessandro Rinaldo
- Reference count: 40
- Primary result: Sigmoid gating achieves faster convergence rates than softmax gating in MoE models

## Executive Summary
This paper provides a rigorous theoretical analysis of the sample efficiency of sigmoid gating in mixture of experts (MoE) models compared to the widely-used softmax gating. Under a regression framework where the true regression function is modeled as a mixture of experts, the authors study the convergence rates of least squares estimators when the number of fitted experts exceeds the true number. They identify two gating regimes based on whether over-specified parameters are zero or non-zero, and formulate identifiability conditions for expert functions in each regime.

The key finding is that sigmoid gating achieves faster convergence rates for expert estimation than softmax gating, particularly when experts are formulated as feed-forward networks with ReLU or GELU activations. Specifically, under the more practical regime where over-specified parameters are non-zero, sigmoid gating achieves parametric convergence rates of O(n^(-1/2)) compared to O(n^(-1/4)) for softmax gating with ReLU/GELU experts. The results demonstrate that sigmoid gating is more sample efficient than softmax gating for expert estimation in MoE models.

## Method Summary
The authors analyze mixture of experts models under a regression framework, comparing sigmoid and softmax gating mechanisms. They study the convergence rates of least squares estimators when the number of fitted experts exceeds the true number, identifying two distinct gating regimes. The theoretical analysis involves establishing identifiability conditions for expert functions in each regime and deriving convergence rates for parameter estimation. The framework assumes the true regression function can be modeled as a mixture of experts, and examines how different gating mechanisms affect the sample efficiency of learning these experts.

## Key Results
- Sigmoid gating achieves parametric convergence rates of O(n^(-1/2)) for expert estimation when over-specified parameters are non-zero
- Softmax gating converges at slower rate of O(n^(-1/4)) with ReLU/GELU expert networks
- The faster convergence of sigmoid gating holds particularly when experts are formulated as feed-forward networks with ReLU or GELU activations
- Theoretical analysis identifies two gating regimes based on whether over-specified parameters are zero or non-zero

## Why This Works (Mechanism)
The superior sample efficiency of sigmoid gating stems from its ability to handle over-specification in mixture of experts models more effectively than softmax gating. When the number of fitted experts exceeds the true number, sigmoid gating can better separate the contributions of individual experts due to its unbounded output range, allowing for clearer identifiability of expert parameters. In contrast, softmax gating's bounded outputs create competition between experts that can slow down the learning process, particularly when dealing with ReLU or GELU activation functions in the expert networks.

## Foundational Learning

**Mixture of Experts (MoE)**: A machine learning architecture that combines multiple specialized models (experts) with a gating mechanism to select or weight their contributions. Needed to understand the model structure being analyzed; check by verifying that the MoE framework can represent the true regression function as a mixture.

**Gating Mechanisms**: Functions that determine how expert outputs are combined, with sigmoid providing unbounded outputs and softmax providing normalized probabilities. Critical for understanding the difference in convergence rates; verify by examining how each gating function handles over-specification.

**Convergence Rates**: Measures of how quickly estimators approach the true parameter values as sample size increases. Essential for quantifying sample efficiency; check by calculating the rate at which parameter estimates converge to true values.

**Identifiability Conditions**: Requirements that ensure unique recovery of model parameters from observed data. Necessary for establishing theoretical guarantees; verify by confirming that expert functions can be uniquely determined from the data.

**ReLU/GELU Activation Functions**: Non-linear activation functions commonly used in neural network experts. Important for understanding practical implementation; check by confirming these activations are compatible with the theoretical framework.

## Architecture Onboarding

**Component Map**: Data -> Gating Mechanism (Sigmoid/Softmax) -> Expert Networks (ReLU/GELU) -> Weighted Combination -> Output

**Critical Path**: The gating mechanism determines expert weights, which are then applied to expert network outputs. The convergence rate of expert parameter estimation is the critical path, as it determines overall sample efficiency.

**Design Tradeoffs**: Sigmoid gating offers faster convergence but may require careful initialization to avoid numerical instability due to unbounded outputs. Softmax gating provides more stable training but at the cost of slower convergence rates.

**Failure Signatures**: Softmax gating may show slow convergence or poor expert specialization when over-specification occurs. Sigmoid gating might suffer from exploding gradients if not properly regularized.

**First Experiments**:
1. Compare convergence rates of sigmoid vs softmax gating on a simple MoE with synthetic data
2. Test identifiability conditions by varying the number of experts relative to the true model
3. Evaluate the impact of different expert network architectures (ReLU vs GELU) on convergence rates

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis assumes idealized conditions that may not fully translate to practical implementations
- Convergence rate comparisons are made under specific conditions where the number of experts exceeds the true number
- Identifiability conditions for expert functions may be difficult to verify in practice
- Theoretical framework focuses on regression settings and may not directly extend to classification or more complex task domains

## Confidence

**Theoretical convergence rate analysis**: High - The mathematical derivations appear rigorous and well-grounded in statistical learning theory

**Practical superiority of sigmoid gating**: Medium - While theoretically justified, real-world performance may vary due to implementation details and optimization challenges

**Applicability to modern deep learning architectures**: Medium - The theoretical framework may need adaptation for complex neural network structures

## Next Checks

1. Implement both sigmoid and softmax gating in a standard MoE architecture on benchmark datasets to empirically verify the theoretical convergence rate differences

2. Test the theoretical predictions under different expert network architectures (ReLU, GELU, and potentially other activation functions) to confirm the sample efficiency claims

3. Evaluate the impact of over-specification (when fitted experts exceed true experts) on convergence rates in practical settings to validate the identifiability conditions and their practical relevance