---
ver: rpa2
title: Multi-Granularity Guided Fusion-in-Decoder
arxiv_id: '2404.02581'
source_url: https://arxiv.org/abs/2404.02581
tags:
- passages
- passage
- answer
- evidence
- mgfid
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of Open-domain Question Answering
  (ODQA), where models need to discern relevant contexts from retrieved results and
  avoid spurious ones. The authors propose Multi-Granularity guided Fusion-in-Decoder
  (MGFiD), which improves upon the standard Fusion-in-Decoder (FiD) architecture by
  incorporating multi-task learning to identify evidence at both passage and sentence
  levels.
---

# Multi-Granularity Guided Fusion-in-Decoder
## Quick Facts
- arXiv ID: 2404.02581
- Source URL: https://arxiv.org/abs/2404.02581
- Reference count: 17
- Key outcome: MGFiD achieves 3.5% and 1.0% improvements in Exact Match on NQ and TQA datasets respectively, while reducing passages used by up to 76% and 61.5%

## Executive Summary
This paper addresses the challenge of Open-domain Question Answering (ODQA) by proposing Multi-Granularity guided Fusion-in-Decoder (MGFiD). The key insight is that retrieved contexts often contain both relevant and irrelevant information, and current models struggle to effectively distinguish between them. MGFiD improves upon the standard Fusion-in-Decoder architecture by incorporating multi-task learning at both passage and sentence levels, allowing the model to identify and focus on supportive contexts while filtering out spurious ones.

The proposed method achieves significant improvements in both accuracy and efficiency. By employing passage re-ranking for coarse-grained evidence identification and sentence classification for fine-grained evidence identification, MGFiD can better determine which contexts are truly relevant to answering questions. Additionally, the introduction of an anchor vector derived from sentence-level predictions guides the decoder more effectively, while threshold-based pruning reduces computational overhead without sacrificing performance.

## Method Summary
MGFiD introduces a multi-granularity approach to evidence identification in ODQA. The method consists of three main components: passage re-ranking to determine coarse-grained evidence, sentence classification for fine-grained evidence, and an anchor vector derived from sentence-level predictions to guide the decoder. The model uses threshold-based pruning to improve efficiency by reducing the number of passages processed. During training, MGFiD employs multi-task learning to simultaneously optimize passage re-ranking and sentence classification objectives. The anchor vector serves as a weighted representation of sentence-level predictions, providing additional guidance to the decoder when generating answers. This approach allows the model to better focus on relevant contexts while filtering out spurious information.

## Key Results
- Achieves 3.5% improvement in Exact Match on Natural Questions dataset
- Achieves 1.0% improvement in Exact Match on TriviaQA dataset
- Reduces number of passages used in decoder by up to 76% (NQ) and 61.5% (TQA)

## Why This Works (Mechanism)
The multi-granularity approach works by addressing the inherent challenge in ODQA where retrieved contexts contain both relevant and irrelevant information. By operating at both passage and sentence levels, MGFiD can more precisely identify which portions of the retrieved text are truly supportive of answering the question. The coarse-grained passage re-ranking filters out obviously irrelevant documents, while the fine-grained sentence classification identifies the most relevant sentences within the remaining passages. The anchor vector then provides a distilled representation of this fine-grained evidence to guide the decoder's answer generation process.

## Foundational Learning
- **Fusion-in-Decoder (FiD) architecture**: Needed for understanding the baseline model being improved upon. Quick check: Can you explain how FiD processes multiple retrieved passages?
- **Multi-task learning**: Required to understand how MGFiD jointly optimizes passage re-ranking and sentence classification. Quick check: What are the benefits of training multiple objectives simultaneously?
- **Passage re-ranking**: Essential for understanding the coarse-grained evidence identification component. Quick check: How does re-ranking differ from initial retrieval in information retrieval systems?
- **Sentence classification**: Critical for understanding the fine-grained evidence identification. Quick check: What makes sentence-level classification more precise than passage-level classification?
- **Threshold-based pruning**: Important for understanding the efficiency improvements. Quick check: How do different threshold values affect model performance and efficiency?
- **Anchor vectors**: Key to understanding how sentence-level predictions guide the decoder. Quick check: What properties should an effective anchor vector have?

## Architecture Onboarding
- **Component map**: Retriever -> Passage Re-ranker -> Sentence Classifier -> Anchor Vector Generator -> Decoder
- **Critical path**: Question + Retrieved passages → Passage re-ranking → Sentence classification → Anchor vector computation → Answer generation
- **Design tradeoffs**: The model trades additional computational complexity during training (for multi-task learning) for improved inference efficiency and accuracy. The threshold-based pruning requires careful hyperparameter tuning but provides significant efficiency gains.
- **Failure signatures**: Over-aggressive pruning may lead to loss of relevant information; under-pruning may not achieve desired efficiency gains. Sentence classification errors can propagate to anchor vector computation and affect final answers.
- **First experiments**: 1) Evaluate passage re-ranking performance independently, 2) Test sentence classification accuracy on annotated datasets, 3) Assess anchor vector quality through visualization of attention patterns

## Open Questions the Paper Calls Out
None

## Limitations
- The method relies on threshold-based pruning, which may introduce sensitivity to hyperparameter tuning
- Improvements on TriviaQA are relatively modest (1.0%) compared to Natural Questions (3.5%)
- The approach's effectiveness across diverse ODQA datasets beyond the two evaluated remains uncertain

## Confidence
- Exact Match improvements on NQ and TQA: Medium
- Efficiency gains through pruning: Medium
- Generalization to other ODQA datasets: Low

## Next Checks
1. Conduct ablation studies to quantify the individual contributions of passage re-ranking versus sentence classification components to the overall performance improvements.
2. Test the model's robustness across additional ODQA datasets with varying characteristics to assess generalization beyond the two evaluated datasets.
3. Evaluate the computational efficiency gains in terms of actual inference time rather than just reduction in passage count, as processing fewer passages may not necessarily translate to proportional time savings.