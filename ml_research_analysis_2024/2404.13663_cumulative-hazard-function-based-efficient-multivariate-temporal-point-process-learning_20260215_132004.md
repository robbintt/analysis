---
ver: rpa2
title: Cumulative Hazard Function Based Efficient Multivariate Temporal Point Process
  Learning
arxiv_id: '2404.13663'
source_url: https://arxiv.org/abs/2404.13663
tags:
- uni00000013
- function
- event
- point
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of modeling multivariate temporal
  point processes (MTPP) with a focus on scalability and accuracy. Traditional methods
  either require numerical approximations for likelihood evaluation or suffer from
  limited flexibility.
---

# Cumulative Hazard Function Based Efficient Multivariate Temporal Point Process Learning

## Quick Facts
- arXiv ID: 2404.13663
- Source URL: https://arxiv.org/abs/2404.13663
- Authors: Bingqing Liu
- Reference count: 40
- One-line primary result: Novel cumulative hazard function modeling achieves 100x parameter reduction while improving accuracy on six datasets

## Executive Summary
This paper proposes an efficient approach for modeling multivariate temporal point processes (MTPP) that addresses the fundamental tension between model expressiveness and computational scalability. The method directly models the cumulative hazard function (CHF) using neural networks, eliminating the need for numerical integration in likelihood evaluation. By learning a shared intensity function across all event types combined with a time-dependent type distribution, the model achieves significant parameter efficiency while maintaining competitive predictive performance.

The proposed approach overcomes key limitations of existing methods that either require complex numerical approximations or suffer from poor scalability when dealing with numerous event types. The model's architecture ensures mathematical correctness through dynamic bias subtraction and monotonic activation functions, while its parameter-efficient design enables deployment on large-scale datasets with thousands of event types.

## Method Summary
The method models the joint distribution of event types and timestamps by learning a total intensity function λ(t) and a time-dependent type distribution Pm(m|τ), factorizing P(mi, τi) = λ(τi) exp(-∫τi^0 λ(s)ds) Pm(mi|τi). The cumulative hazard function ϕ(τ) = ∫τ^0 λ(s)ds is modeled directly using a neural network, with dynamic bias subtraction ensuring ϕ(0)=0. An RNN encoder processes event history into hidden states, which are combined with inter-event times to compute the CHF. A separate two-layer neural network predicts the time-dependent type distribution using softmax. An auxiliary time predictor network with MSE loss accelerates inference during prediction.

## Key Results
- Outperforms state-of-the-art baselines in negative log-likelihood and event prediction accuracy across six datasets
- Achieves up to 100x reduction in parameters and memory usage compared to competitors
- Maintains competitive performance while being parameter-efficient, enabling scalability to datasets with many event types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Directly modeling the cumulative hazard function (CHF) eliminates the need for numerical integration in likelihood evaluation.
- Mechanism: The CHF is the integral of the intensity function over time. By modeling CHF directly with a neural network, its derivative yields the intensity exactly, bypassing the intractable integral in the likelihood.
- Core assumption: The neural network architecture can approximate any continuous CHF and its derivative is computationally tractable.
- Evidence anchors:
  - [abstract]: "By directly modelling the integral of the intensity function, i.e., the cumulative hazard function (CHF), the likelihood can be evaluated accurately"
  - [section III-B]: "The cumulative hazard function ϕ(τ) = Rτ0 λ(s)ds is monotonic with respect to the inter-event time τ since the intensity function is non-negative. FullyNN [26] simulates the CHF by a neural network where the neural weights are all positive and the activation functions are all monotonic. However, neural network defined that way doesn't meet the requisite ϕ(0) = 0, making it a flawed design."
- Break condition: If the network cannot guarantee ϕ(0)=0 or monotonicity is violated, the model will produce invalid probabilities and likelihoods.

### Mechanism 2
- Claim: The well-defined CHF network with dynamic bias subtraction ensures mathematical correctness and flexibility.
- Mechanism: A standard neural network computes f(τ), then subtracts f(0) to enforce ϕ(0)=0. This allows unconstrained architecture while preserving the necessary constraints.
- Core assumption: f(τ) - f(0) yields a valid CHF for any neural network f, provided activation functions are monotonic.
- Evidence anchors:
  - [section III-B]: "To repair it, we try not posing any extra restrictions or doing any modifications on the network architecture so as to not harm the expressiveness. For this sake, we reproduce a well-defined CHF network by dynamically subtracting the initial bias ϕ(0). We say the bias is dynamic because it's history-dependent rather than a fixed something."
- Break condition: If activation functions are not monotonic or parameters are unconstrained, the resulting function may not be a valid CHF.

### Mechanism 3
- Claim: Parameter-efficient multivariate modeling reduces complexity from O(M) to O(1) continuous variables per time step.
- Mechanism: Instead of learning individual intensity functions λm(t) for each event type m, the model learns a single total intensity λ(t) and a time-dependent type distribution Pm(m|τ).
- Core assumption: Event types are conditionally independent given the time and history, so a single intensity function suffices.
- Evidence anchors:
  - [section III-A]: "To overcome these shortcomings, we propose to model the joint distribution by only learning the total intensity function and the time-dependent type distribution, i.e., P(mi, τi) = λ(τi) exp(-∫τi0 λ(s)ds) Pm(mi|τi)"
  - [section III-B]: "Specifically, we use a two-layer neural network, i.e., Pm(m|τ) = softmax(W2my2m + b2m)"
- Break condition: If event types have strong independent dynamics, the shared intensity assumption may underfit.

## Foundational Learning

- Concept: Temporal Point Process
  - Why needed here: The entire model is built on TPP theory; understanding intensity functions, cumulative hazard, and likelihood is essential.
  - Quick check question: What is the relationship between intensity λ(t), cumulative hazard ϕ(t), and density p(t) in a TPP?

- Concept: Neural Network Design with Constraints
  - Why needed here: The CHF network must satisfy monotonicity and boundary conditions; this requires careful architectural choices.
  - Quick check question: Why does subtracting f(0) from f(τ) enforce ϕ(0)=0, and what constraints must the network satisfy?

- Concept: Multivariate Event Modeling
  - Why needed here: The model must predict both time and event type efficiently; understanding the factorization of joint distribution is key.
  - Quick check question: How does modeling a shared intensity and separate type distribution reduce parameter complexity compared to per-type intensities?

## Architecture Onboarding

- Component map:
  RNN encoder -> CHF network (3-layer with positive weights, tanh activation) -> Type predictor (2-layer softmax) -> Likelihood evaluation
  + Auxiliary time predictor network with MSE loss

- Critical path:
  1. Encode history with RNN
  2. Compute CHF and its derivative for intensity
  3. Evaluate likelihood using exact CHF values
  4. Update parameters via backpropagation

- Design tradeoffs:
  - Using tanh in CHF network: bounded, but avoids vanishing gradients better than relu
  - Adding residual base intensity: captures spontaneous events but adds parameters
  - Time predictor auxiliary loss: speeds inference but may distract from NLL optimization

- Failure signatures:
  - Loss not decreasing: check monotonicity constraints or activation function choice
  - Out of memory on large M: verify parameter-efficient design is implemented
  - Poor type prediction: inspect time-dependent type predictor and time prediction accuracy

- First 3 experiments:
  1. Train on synthetic Hawkes process data, compare NLL with FullyNN
  2. Evaluate type prediction F1 score on Retweet dataset
  3. Measure GPU memory usage vs UNIPoint baseline

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several limitations are implied:

### Open Question 1
- Question: How does the proposed model's performance scale when dealing with datasets containing thousands of event types, and what specific challenges arise in such scenarios?
- Basis in paper: [explicit] The paper mentions that the proposed model has fewer parameters and memory usage, but it does not provide specific results for datasets with thousands of event types.
- Why unresolved: The paper only provides results for datasets with up to 1641 event types, leaving the scalability for larger datasets unexplored.
- What evidence would resolve it: Experimental results demonstrating the model's performance on datasets with thousands of event types, including comparisons with baseline models in terms of accuracy, parameter count, and memory usage.

### Open Question 2
- Question: How does the choice of activation function in the cumulative hazard function network affect the model's performance, and are there other activation functions that could potentially improve results?
- Basis in paper: [explicit] The paper mentions that tanh is the best activation function among those tried, but it does not explore other potential activation functions.
- Why unresolved: The paper only tests a limited set of activation functions, leaving the possibility of better-performing functions unexplored.
- What evidence would resolve it: A comprehensive study comparing the performance of various activation functions in the cumulative hazard function network, including those not mentioned in the paper.

### Open Question 3
- Question: How does the proposed model handle missing or incomplete event sequences, and what strategies can be employed to improve its robustness in such cases?
- Basis in paper: [inferred] The paper does not discuss the handling of missing or incomplete event sequences, which is a common issue in real-world applications.
- Why unresolved: The paper focuses on the model's performance on complete event sequences, without addressing the challenges posed by missing or incomplete data.
- What evidence would resolve it: Experimental results demonstrating the model's performance on datasets with missing or incomplete event sequences, along with proposed strategies for handling such cases.

## Limitations

- The shared intensity assumption may underfit datasets where event types have strongly independent temporal dynamics
- Dynamic bias subtraction requires careful numerical implementation to maintain monotonicity and stability during training
- Performance benefits depend on dataset characteristics and may vary when event types are not conditionally independent

## Confidence

- **High Confidence**: The core mathematical framework of modeling CHF directly is well-established and correctly implemented. The elimination of numerical integration for likelihood evaluation is a clear technical advantage.
- **Medium Confidence**: The parameter efficiency claims are supported by experimental results, but the scalability benefits depend on dataset characteristics and may vary in practice.
- **Low Confidence**: The time prediction auxiliary loss and its impact on overall model performance could benefit from more extensive ablation studies to confirm its necessity.

## Next Checks

1. **Monotonicity Verification**: Implement gradient-based checks during training to ensure the CHF network maintains monotonicity constraints across all datasets and training epochs.

2. **Parameter Efficiency Benchmark**: Conduct controlled experiments comparing memory usage and parameter counts across varying numbers of event types (M) to verify the claimed O(1) scaling behavior.

3. **Type Independence Assumption**: Design synthetic datasets with varying degrees of type independence to test the model's robustness when the shared intensity assumption is violated, measuring degradation in NLL and prediction accuracy.