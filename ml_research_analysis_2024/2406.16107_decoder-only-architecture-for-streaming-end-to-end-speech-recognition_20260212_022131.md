---
ver: rpa2
title: Decoder-only Architecture for Streaming End-to-end Speech Recognition
arxiv_id: '2406.16107'
source_url: https://arxiv.org/abs/2406.16107
tags:
- prompts
- speech
- streaming
- decoder
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a streaming end-to-end speech recognition
  system using a decoder-only architecture, addressing the challenge of efficient
  and low-latency ASR. The core method involves blockwise processing of speech features,
  generating prompts from CTC outputs and context embeddings, and sequentially providing
  them to the decoder for prompt-based recognition.
---

# Decoder-only Architecture for Streaming End-to-end Speech Recognition

## Quick Facts
- arXiv ID: 2406.16107
- Source URL: https://arxiv.org/abs/2406.16107
- Reference count: 0
- Achieves 8% relative WER reduction on LibriSpeech test-other set while being twice as fast as baseline encoder-decoder model

## Executive Summary
This paper introduces a streaming end-to-end speech recognition system using a decoder-only architecture, addressing the challenge of efficient and low-latency ASR. The core method involves blockwise processing of speech features, generating prompts from CTC outputs and context embeddings, and sequentially providing them to the decoder for prompt-based recognition. A novel training scheme using random-length prefix prompts mitigates the mismatch between streaming inference and batch training. Experimental results on LibriSpeech and Switchboard datasets demonstrate competitive performance with significant speed improvements over baseline encoder-decoder models.

## Method Summary
The proposed approach processes speech in blocks using a conformer-based speech subnetwork that generates CTC outputs and context embeddings. These are converted into prompts and fed to a transformer decoder sequentially for streaming inference. The key innovation is a novel training scheme using random-length prefix prompts to simulate the streaming scenario where only partial prompts are available at each step. The model is trained on the target dataset, then fine-tuned with the prefix prompt method. Byte-pair encoding subword tokenization is used, and the decoder is pre-trained on external text-only corpora before fine-tuning with the speech subnetwork.

## Key Results
- Achieves 8% relative word error rate reduction in LibriSpeech test-other set
- Model runs twice as fast as baseline encoder-decoder model
- Successfully demonstrates streaming inference capability while maintaining competitive accuracy

## Why This Works (Mechanism)

### Mechanism 1
Blockwise processing of speech features reduces latency while maintaining accuracy by compressing acoustic information into prompts. The speech subnetwork processes input audio in blocks, producing context embeddings and CTC outputs. These are converted into prompts (compact audio representations) and fed to the decoder sequentially, enabling streaming inference.

### Mechanism 2
Random-length prefix prompts during training mitigate the mismatch between streaming inference and batch training. Instead of training with full prompts and entire transcriptions, a random-length prefix of prompts is selected during training. This simulates the streaming scenario where only partial prompts are available at each step.

### Mechanism 3
Combining CTC and context prompts improves accuracy by providing both local acoustic information and global context. Two types of prompts are generated: CTC prompts (based on greedy CTC outputs) and context prompts (based on context embeddings from previous blocks). These are concatenated and provided to the decoder.

## Foundational Learning

- **Transformer decoder architecture**: The decoder-only architecture relies on the transformer's self-attention mechanism to process prompts and generate transcriptions autoregressively. Why needed: Enables the decoder to process prompts and generate transcriptions without an encoder. Quick check: How does the self-attention mechanism in a transformer decoder differ from the source-target attention in an encoder-decoder model?

- **Connectionist Temporal Classification (CTC)**: CTC is used to align speech features with transcriptions and generate CTC prompts for the decoder. Why needed: Provides frame-level alignment between speech features and transcriptions. Quick check: What is the role of the blank token in CTC, and how does CTC greedy search work?

- **Blockwise processing**: Blockwise processing enables streaming ASR by processing speech in chunks and generating prompts sequentially. Why needed: Allows the model to process speech incrementally rather than waiting for the entire utterance. Quick check: How does the context embedding in blockwise processing help maintain continuity across blocks?

## Architecture Onboarding

- **Component map**: Audio input -> Speech subnetwork (block processing) -> CTC and context prompts -> Decoder -> Transcription output
- **Critical path**: The core pipeline processes audio blocks through the conformer speech subnetwork, generates prompts, and feeds them to the transformer decoder for streaming transcription generation.
- **Design tradeoffs**: Block size vs. latency (smaller blocks reduce latency but increase overhead), prompt length vs. accuracy (longer prompts provide more context but increase latency), CTC vs. context prompts (CTC provides local acoustic information, context provides global context)
- **Failure signatures**: High WER indicates issues with prompt generation or decoder accuracy, high latency suggests block size or prompt processing is too slow, training instability may be caused by improper prompt masking or alignment
- **First 3 experiments**:
  1. Test streaming inference with varying block sizes to find the optimal balance between latency and accuracy
  2. Compare WER with different prompt combinations (CTC only, context only, both) to assess their impact
  3. Evaluate the effect of random prefix prompt lengths on training stability and inference accuracy

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the content, several implicit open questions emerge:

1. How does the proposed decoder-only architecture perform compared to encoder-decoder models when scaled to much larger datasets or languages with longer sequences?
2. What is the impact of different block sizes on the performance and latency of the proposed streaming decoder-only architecture?
3. How does the proposed decoder-only architecture handle out-of-vocabulary words or rare words during streaming inference?

## Limitations

- Limited ablation studies on prompt components, making it unclear how much each component contributes to the 8% WER reduction
- Training methodology specifics are underspecified, including exact block sizes, hop sizes, and precise masking strategies
- Computational overhead analysis is incomplete, lacking detailed characterization of speed-accuracy-resource tradeoffs across different hardware configurations

## Confidence

**High Confidence Claims:**
- The decoder-only architecture with blockwise processing is a viable approach for streaming ASR
- Random-length prefix prompts during training help mitigate train-inference mismatch
- The proposed method achieves competitive WER on LibriSpeech and Switchboard datasets

**Medium Confidence Claims:**
- The 8% relative WER reduction on LibriSpeech test-other set
- The "twice as fast" speed improvement over baseline encoder-decoder models
- The effectiveness of combining CTC and context prompts

**Low Confidence Claims:**
- Exact latency measurements (EP50) across different block sizes
- Specific contribution of context embeddings versus CTC outputs
- Generalization performance beyond the tested datasets

## Next Checks

1. **Ablation study on prompt components**: Systematically evaluate WER performance using only CTC prompts, only context prompts, and their combination across different block sizes to quantify the contribution of each component.

2. **Parameter sensitivity analysis**: Test the model with varying block sizes (e.g., 50ms, 100ms, 200ms) and random prefix lengths during training to identify optimal configurations and understand the impact on both accuracy and latency.

3. **Cross-dataset generalization**: Evaluate the trained model on additional datasets (e.g., Common Voice, TED-LIUM) to assess how well the approach generalizes beyond LibriSpeech and Switchboard, particularly focusing on performance with different acoustic conditions and languages.