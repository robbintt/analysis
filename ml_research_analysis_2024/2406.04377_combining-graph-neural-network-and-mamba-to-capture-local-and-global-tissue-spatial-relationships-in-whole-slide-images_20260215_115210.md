---
ver: rpa2
title: Combining Graph Neural Network and Mamba to Capture Local and Global Tissue
  Spatial Relationships in Whole Slide Images
arxiv_id: '2406.04377'
source_url: https://arxiv.org/abs/2406.04377
tags: []
core_contribution: The paper addresses the challenge of predicting progression-free
  survival in early-stage lung adenocarcinoma using whole slide images (WSIs). The
  authors propose a novel method that combines a message-passing graph neural network
  (GAT) with a state space model (Mamba) to capture both local and global spatial
  relationships among tiles in WSIs.
---

# Combining Graph Neural Network and Mamba to Capture Local and Global Tissue Spatial Relationships in Whole Slide Images

## Quick Facts
- arXiv ID: 2406.04377
- Source URL: https://arxiv.org/abs/2406.04377
- Reference count: 40
- Outperforms several baseline models, including MLP, AttentionMIL, TransMIL, PatchGCN, and GTP, in terms of concordance index (C-index) and dynamic AUC.

## Executive Summary
This paper addresses the challenge of predicting progression-free survival in early-stage lung adenocarcinoma using whole slide images (WSIs). The authors propose a novel method that combines a message-passing graph neural network (GAT) with a state space model (Mamba) to capture both local and global spatial relationships among tiles in WSIs. The model uses a graph construction approach where tiles are nodes and edges are defined by k-nearest neighbors. The GAT branch processes node and edge features, while the Mamba branch selectively retains the most informative nodes based on both local and global contexts. The method outperforms several baseline models, including MLP, AttentionMIL, TransMIL, PatchGCN, and GTP, in terms of concordance index (C-index) and dynamic AUC. The best performance is achieved when using UNI node features extracted from a large-scale pathology image dataset. The authors also conduct experiments to assess the impact of different node features and tile sampling strategies on model performance.

## Method Summary
The proposed method combines a Graph Attention Network (GAT) with a Mamba state space model to capture both local and global spatial relationships in WSIs. The approach involves tiling WSIs into non-overlapping patches, extracting node and edge features, constructing a k-nearest neighbor graph (k=8), and processing the graph through GAT and Mamba branches. The GAT branch performs local aggregation through attention-weighted message passing, while the Mamba branch selectively processes the entire node sequence to capture global context. The outputs are concatenated and passed through pooling and MLP layers for survival prediction using Cox loss. The method is evaluated using 5-fold cross-validation on NLST and TCGA datasets with C-index and dynamic AUC as primary metrics.

## Key Results
- The GAT-Mamba model outperforms baseline models including MLP, AttentionMIL, TransMIL, PatchGCN, and GTP in both C-index and dynamic AUC metrics.
- Pathology foundation model features (UNI) provide better performance than ImageNet-pretrained features or hand-crafted features.
- Sampling only aggressive tissue subtypes results in slightly worse performance than random sampling, suggesting the importance of including both aggressive and non-aggressive regions.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GAT-Mamba outperforms standard GNNs by combining local neighborhood aggregation with global selective state space modeling.
- Mechanism: GAT provides local connectivity through attention-weighted message passing among 8-nearest-neighbor tiles. Mamba then processes all nodes as a sequence with selective state updates, retaining only the most informative nodes based on context. This combination avoids the oversmoothing of deep GNNs while maintaining computational efficiency compared to transformers.
- Core assumption: Local tile relationships (proximity in tissue space) contain useful prognostic information, and global context can be captured by selective state space modeling without quadratic complexity.
- Evidence anchors:
  - [abstract] "combines a message-passing graph neural network (GNN) with a state space model (Mamba) to capture both local and global spatial relationships among the tiles in WSIs."
  - [section] "GAT was used as the GNN, and Mamba was used as the state space model to capture both local and global tissue connectivity in the WSI."
  - [corpus] Weak: no direct comparison to Mamba+GNN combinations in corpus
- Break condition: If local tissue topology is irrelevant for survival prediction, or if Mamba's selective mechanism fails to distinguish informative from uninformative nodes in large WSI graphs.

### Mechanism 2
- Claim: Node features extracted from large-scale pathology foundation models (UNI) provide more robust representations than ImageNet-pretrained or smaller-domain features.
- Mechanism: Foundation models like UNI are trained on 100M+ pathology images, learning rich visual representations that transfer well to downstream tasks. These features capture domain-specific patterns better than general image models or hand-crafted features.
- Core assumption: Larger, domain-specific pretraining datasets produce more generalizable features for computational pathology tasks.
- Evidence anchors:
  - [section] "Results from our tile-sampling experiments suggest that one may not need to use all the tiles from the WSI to construct the tile-level graph when robust node features are used."
  - [section] "Our node feature ablation study shows that pathology foundation model-extracted features PLIP, CONCH, and UNI resulted in better model performance than ImageNet-pretrained-model-extracted features or LUAD domain-specific features."
  - [corpus] Weak: corpus doesn't discuss foundation model performance in pathology
- Break condition: If domain-specific pretraining data is biased or if the task requires features capturing specific molecular patterns not well-represented in foundation models.

### Mechanism 3
- Claim: Including both aggressive and non-aggressive tissue subtypes in graph construction improves model performance compared to sampling only aggressive regions.
- Mechanism: The model benefits from learning the spatial relationship between different tissue types. Aggressive regions (solid, micropapillary) provide progression signals while non-aggressive regions (lepidic, non-tumor) provide context and contrast. Sampling only aggressive tissue loses this complementary information.
- Core assumption: Spatial heterogeneity within WSIs contains prognostic information, and different tissue types interact meaningfully for survival prediction.
- Evidence anchors:
  - [section] "we found that the model-predicted low-risk patients tend to have a lepidic predominant subtype, whereas the high-risk patients tend to have a solid predominant subtype."
  - [section] "we discovered that, on average, the false negatives had larger tissues as compared to the true positives... the tissue's malignancy type is more important than the tissue size"
  - [section] "sampling only tiles of the least aggressive subtypes or the most aggressive subtypes generally resulted in models that have slightly worse performance than any of the random sampling methods"
- Break condition: If tissue subtype distribution is irrelevant for survival or if spatial relationships between subtypes don't contain predictive signal.

## Foundational Learning

- Concept: Graph neural networks and message passing
  - Why needed here: WSIs are modeled as graphs where tiles are nodes and edges capture spatial relationships. Understanding GNN mechanics is essential for implementing the GAT branch.
  - Quick check question: What is the difference between GCN and GAT in terms of how they aggregate neighbor information?

- Concept: State space models and selective state updates
  - Why needed here: Mamba replaces attention mechanisms with selective state space modeling to efficiently capture global dependencies in large WSI graphs.
  - Quick check question: How does Mamba's selective state mechanism differ from standard RNNs in handling long sequences?

- Concept: Survival analysis and concordance index
  - Why needed here: The task is predicting progression-free survival, requiring understanding of Cox proportional hazards loss and C-index evaluation.
  - Quick check question: What does a C-index of 0.7 mean in the context of survival prediction?

## Architecture Onboarding

- Component map:
  - WSI tiling → Node/edge feature extraction → Graph construction (8-NN) → GAT branch (local aggregation) → Mamba branch (global selective processing) → Concatenation → Pooling → MLP prediction → Cox loss

- Critical path:
  - Graph construction → Feature extraction → GAT processing → Mamba processing → Aggregation → Prediction
  - Performance bottleneck: Mamba layer computation on large node sequences

- Design tradeoffs:
  - GAT vs Transformer: GAT provides local structure preservation with linear complexity; Transformer captures global dependencies but at quadratic cost
  - Full graph vs sampled tiles: Full graph captures complete context but increases computation; sampling reduces cost but may lose information
  - Foundation model features vs domain-specific features: Foundation models provide robust representations but may lack task-specific nuance

- Failure signatures:
  - Overfitting: Poor generalization to external datasets, high variance across folds
  - Underfitting: Model performs similarly to simple baselines, high bias
  - Computational issues: Memory errors during Mamba processing of large graphs
  - Convergence problems: Training loss plateaus early or diverges

- First 3 experiments:
  1. Ablation: GAT-only vs Mamba-only vs combined to verify complementary benefits
  2. Feature comparison: UNI vs ResNet50IN vs hand-crafted features to validate foundation model advantage
  3. Sampling study: 100% tiles vs aggressive-only vs non-aggressive-only vs random subsets to optimize graph construction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GAT-Mamba change when using pathology-specific foundation models like CONCH or PLIP compared to the general-purpose UNI model?
- Basis in paper: [explicit] The paper compares UNI, CONCH, and PLIP features in ablation studies, showing UNI performs best, but does not explore fine-tuning these models for the specific task.
- Why unresolved: The paper only evaluates out-of-the-box features without exploring task-specific fine-tuning or adaptation of pathology foundation models.
- What evidence would resolve it: Fine-tuning CONCH and PLIL on lung adenocarcinoma datasets and comparing their performance against UNI in the GAT-Mamba pipeline.

### Open Question 2
- Question: What is the optimal balance between local (GAT) and global (Mamba) information processing for different types of tissue analysis tasks?
- Basis in paper: [inferred] The paper combines GAT and Mamba but doesn't explore varying their relative contributions or when one might be more beneficial than the other.
- Why unresolved: The current architecture uses equal weighting of GAT and Mamba outputs without exploring alternative architectures or task-specific optimizations.
- What evidence would resolve it: Systematic ablation studies varying the number of GAT and Mamba layers, or using attention mechanisms to dynamically weight their contributions based on task requirements.

### Open Question 3
- Question: How does tile sampling strategy interact with node feature quality across different cancer types and tissue architectures?
- Basis in paper: [explicit] The paper explores tile sampling strategies but only within lung adenocarcinoma, showing that robust features reduce the need for dense sampling.
- Why unresolved: The study is limited to one cancer type and doesn't explore how different tissue architectures might require different sampling strategies.
- What evidence would resolve it: Testing the same sampling experiments across multiple cancer types with varying tissue architectures to identify generalizable principles.

## Limitations

- Model Architecture Specificity: The exact implementation details of the GAT-Mamba block, particularly the Mamba layer configuration and how it integrates with GAT outputs, are not fully specified in the paper.
- Feature Extraction Dependencies: The paper relies on pretrained models (UNI, CONCH, PLIP) for feature extraction without providing these models or detailed specifications for obtaining them.
- Dataset Availability: While NLST and TCGA data sources are mentioned, specific access procedures and preprocessing pipelines are not fully detailed.

## Confidence

- **High Confidence**: The core hypothesis that combining GAT with Mamba provides complementary local and global feature extraction benefits is well-supported by the experimental results showing consistent improvements over baseline models.
- **Medium Confidence**: The claim that pathology foundation model features (UNI, CONCH, PLIP) outperform general ImageNet features is supported by ablation studies, but the specific magnitude of improvement may depend on implementation details.
- **Low Confidence**: The specific sampling strategy recommendations (8-NN graph construction, tile sampling percentages) may be dataset-dependent and require validation on independent cohorts.

## Next Checks

1. **Architecture Verification**: Implement the GAT-Mamba block according to the paper's description and verify that the combined model outperforms either component alone on a standard graph learning benchmark.

2. **Feature Ablation Replication**: Replicate the node feature ablation study using available foundation models to confirm that pathology-specific features consistently outperform general image features across multiple datasets.

3. **External Validation**: Test the model on an independent lung adenocarcinoma dataset (different from NLST and TCGA) to assess generalizability and identify potential overfitting to the original datasets.