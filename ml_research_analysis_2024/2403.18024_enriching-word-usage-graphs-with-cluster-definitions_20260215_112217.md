---
ver: rpa2
title: Enriching Word Usage Graphs with Cluster Definitions
arxiv_id: '2403.18024'
source_url: https://arxiv.org/abs/2403.18024
tags:
- definitions
- definition
- word
- english
- cluster
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the lack of human-interpretable sense definitions
  in word usage graphs (WUGs), which are widely used in semantic change modeling but
  only provide numerical cluster IDs. To make these clusters more useful for linguists
  and lexicographers, the authors generate contextualized definitions for each cluster
  using fine-tuned encoder-decoder language models.
---

# Enriching Word Usage Graphs with Cluster Definitions

## Quick Facts
- arXiv ID: 2403.18024
- Source URL: https://arxiv.org/abs/2403.18024
- Reference count: 13
- Key outcome: Human evaluation shows DefGen outperforms WordNet baselines in accuracy and relevance for generating contextualized cluster definitions

## Executive Summary
Word usage graphs (WUGs) are powerful tools for modeling semantic change, but their utility is limited by the lack of interpretable sense definitions for individual clusters. This work introduces a method to enrich WUGs with human-readable definitions by fine-tuning encoder-decoder language models to generate contextualized definitions from scratch. The approach, DefGen, is evaluated against WordNet-based baselines using human judgments across English, German, Russian, and Norwegian WUGs, demonstrating superior accuracy and relevance.

## Method Summary
The authors address the interpretability gap in WUGs by generating contextualized definitions for each cluster. They compare three approaches: selecting definitions from WordNet using Lesk and GlossReader baselines, versus generating definitions from scratch using a fine-tuned encoder-decoder model (DefGen). The DefGen model is trained to produce definitions that capture the specific sense of each cluster. Human evaluation is conducted to assess accuracy, relevance, and coverage of the generated definitions compared to the baselines.

## Key Results
- DefGen outperforms WordNet-based baselines in human evaluation for accuracy and relevance
- Generated definitions show better coverage of cluster-specific senses compared to baseline definitions
- DefGen produces less overly general definitions than baseline approaches
- Enriched WUG datasets with both English and native-language definitions are publicly released

## Why This Works (Mechanism)
The success of DefGen stems from its ability to generate definitions conditioned on the specific context of each cluster, rather than relying on static definitions from WordNet. By fine-tuning an encoder-decoder model on contextualized examples, DefGen can capture subtle sense distinctions that are often missed by baseline methods. The use of human evaluation ensures that the generated definitions are not only accurate but also relevant and interpretable for linguistic analysis.

## Foundational Learning
- **Word Usage Graphs (WUGs)**: Graph-based representations of word senses over time, used for semantic change modeling. Why needed: They provide a structured way to track semantic shifts but lack interpretability. Quick check: Can you explain how WUGs represent semantic change without definitions?
- **Encoder-decoder models**: Neural architectures that generate output sequences from input sequences, fine-tuned here for definition generation. Why needed: They enable context-aware definition generation beyond static lexical resources. Quick check: What is the advantage of using an encoder-decoder model over selecting definitions from WordNet?
- **Human evaluation**: Assessment of generated definitions by human annotators for accuracy, relevance, and coverage. Why needed: Automated metrics may not capture the nuances of definition quality for linguistic purposes. Quick check: Why is human evaluation preferred over automated metrics in this context?

## Architecture Onboarding

**Component map**: WUG clusters -> Encoder-decoder model (DefGen) -> Contextualized definitions

**Critical path**: 
1. Extract cluster-specific contexts from WUGs
2. Fine-tune encoder-decoder model on definition generation task
3. Generate contextualized definitions for each cluster
4. Human evaluation of generated definitions

**Design tradeoffs**:
- Fine-tuning an encoder-decoder model allows for context-aware generation but requires annotated training data
- Selecting from WordNet is faster and requires no training but lacks coverage of cluster-specific senses
- Human evaluation ensures quality but is resource-intensive and subjective

**Failure signatures**:
- Generated definitions are overly general or fail to capture cluster-specific senses
- Baseline definitions from WordNet are irrelevant or inaccurate for certain clusters
- Inter-annotator agreement is low, indicating subjective interpretation of definition quality

**First experiments**:
1. Compare DefGen-generated definitions against randomly selected WordNet definitions
2. Evaluate DefGen on a held-out test set of WUG clusters not seen during training
3. Conduct ablation study by removing context conditioning from DefGen to assess its importance

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- Evaluation limited to English, German, Russian, and Norwegian WUGs, with unknown performance on other languages
- Does not address potential biases in underlying WUG clustering methods
- No quantitative metrics for definition quality beyond human evaluation accuracy
- Uncertainty about capturing subtle semantic nuances in generated definitions

## Confidence

**High confidence**: DefGen outperforms WordNet-based baselines in human evaluation for accuracy and relevance.

**Medium confidence**: Generated definitions provide better coverage of cluster-specific senses compared to baseline definitions.

**Low confidence**: Generalizability of results to languages and domains beyond the four tested (English, German, Russian, Norwegian).

## Next Checks
1. Evaluate DefGen on additional languages and domain-specific corpora to test generalizability.
2. Conduct inter-annotator agreement analysis to quantify consistency in human evaluations.
3. Implement quantitative metrics (e.g., BLEU, ROUGE) to complement human evaluation and assess definition quality more systematically.