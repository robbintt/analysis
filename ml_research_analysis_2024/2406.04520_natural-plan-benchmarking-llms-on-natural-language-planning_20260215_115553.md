---
ver: rpa2
title: 'NATURAL PLAN: Benchmarking LLMs on Natural Language Planning'
arxiv_id: '2406.04520'
source_url: https://arxiv.org/abs/2406.04520
tags:
- planning
- days
- plan
- trip
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces NATURAL PLAN, a new benchmark for evaluating
  large language models'' planning capabilities in natural language. It includes three
  tasks: Trip Planning, Meeting Planning, and Calendar Scheduling, with real-world
  constraints and tool outputs.'
---

# NATURAL PLAN: Benchmarking LLMs on Natural Language Planning

## Quick Facts
- **arXiv ID:** 2406.04520
- **Source URL:** https://arxiv.org/abs/2406.04520
- **Reference count:** 15
- **Key result:** LLMs achieve only 31-35% accuracy on trip planning tasks, with performance dropping below 5% for 10-city trips

## Executive Summary
NATURAL PLAN is a new benchmark for evaluating large language models' planning capabilities in natural language. It consists of three tasks - Trip Planning, Meeting Planning, and Calendar Scheduling - each with varying levels of complexity and real-world constraints. The benchmark provides tool outputs (like flight information and travel times) as context rather than requiring models to use tools, allowing focused evaluation of planning reasoning. State-of-the-art models struggle significantly on this benchmark, with GPT-4 and Gemini 1.5 Pro achieving only 31.1% and 34.8% accuracy respectively on the Trip Planning task.

## Method Summary
The benchmark generates synthetic planning tasks with real tool outputs from Google Flights, Google Maps, and Google Calendar. Each task presents a planning problem with constraints (e.g., flight connectivity, duration requirements) and provides the relevant tool outputs as context. Models are evaluated using 5-shot in-context learning, where they must generate plans that satisfy all constraints. Performance is measured via exact match scoring against ground truth plans, using regex parsing to extract date, place, and time information from model outputs.

## Key Results
- GPT-4 and Gemini 1.5 Pro achieve only 31.1% and 34.8% accuracy on Trip Planning
- All models perform below 5% accuracy when planning involves 10 cities
- Gemini 1.5 Pro shows promise with in-context planning, achieving up to 40% accuracy using 800 examples
- Self-correction and few-shot generalization do not improve performance
- Performance degrades significantly as task complexity increases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can leverage long-context in-context learning to improve planning accuracy when sufficient examples are provided.
- Mechanism: When models like Gemini 1.5 Pro have access to extended context windows, they can incorporate hundreds of planning examples as in-context exemplars, which improves their ability to generalize and solve complex planning tasks.
- Core assumption: The model's long-context capability allows it to effectively process and learn from a large number of in-context examples without performance degradation.
- Evidence anchors:
  - [abstract] "in-context planning with long contexts shows promise, with Gemini 1.5 Pro achieving up to 40% accuracy on Trip Planning using 800 examples."
  - [section 4.5] "Due to the strong long context capabilities, Gemini Pro 1.5 is able to leverage more in-context examples up to 355K tokens, still showing steady improvements."

### Mechanism 2
- Claim: Decoupling tool-use from planning reasoning by providing tool outputs as context simplifies LLM planning evaluation.
- Mechanism: By providing pre-computed tool outputs (e.g., Google Flights API results, Google Maps travel times) directly in the context, the benchmark isolates the planning reasoning task from the tool-use task, making it easier to evaluate pure planning capabilities.
- Core assumption: The LLM can effectively parse and utilize the provided tool output information within its context window.
- Evidence anchors:
  - [abstract] "This eliminates the need for a tool-use environment for evaluating LLMs on Planning."
  - [section 2.1] "We decouple tool-use from the reasoning task and provide tool outputs in-context to keep focus of NATURAL PLAN on planning."

### Mechanism 3
- Claim: Increasing task complexity (more cities, people, or days) directly correlates with decreased LLM planning accuracy.
- Mechanism: As the number of constraints and variables in planning tasks increases, the computational complexity of finding valid solutions increases, exceeding the reasoning capacity of current LLMs.
- Core assumption: LLM planning performance scales inversely with task complexity in a predictable manner.
- Evidence anchors:
  - [abstract] "We find that model performance drops drastically as the complexity of the problem increases: all models perform below 5% when there are 10 cities"
  - [section 4.2] "Figure 4 shows that models start to fail with more cities involved in the planning. In Trip Planning, all five models perform below 5% when there are 10 cities."

## Foundational Learning

- Concept: Few-shot in-context learning
  - Why needed here: The benchmark uses few-shot exemplars to enable LLMs to learn the planning task format and constraints without fine-tuning.
  - Quick check question: How many examples are typically provided in the few-shot setup for each task in NATURAL PLAN?

- Concept: Constraint satisfaction problems
  - Why needed here: Planning tasks in NATURAL PLAN are essentially constraint satisfaction problems where the LLM must find solutions that satisfy all given constraints.
  - Quick check question: What types of constraints are used in the Trip Planning task (e.g., direct flights only, specific duration requirements)?

- Concept: Long-context processing
  - Why needed here: Gemini 1.5 Pro's performance improvement with 800 examples relies on its ability to process long contexts effectively.
  - Quick check question: What is the maximum context length that Gemini 1.5 Pro can handle according to the paper?

## Architecture Onboarding

- Component map: Task generation -> Context preparation (tool outputs + constraints) -> Model inference (few-shot prompting) -> Plan parsing -> Accuracy scoring
- Critical path: Task generation → Context preparation (tool outputs + constraints) → Model inference (few-shot prompting) → Plan parsing → Accuracy scoring
- Design tradeoffs: The decision to decouple tool-use from planning reasoning simplifies evaluation but may not reflect real-world scenarios where LLMs need to both use tools and plan. The synthetic data approach ensures controlled complexity variation but may not capture all real-world edge cases.
- Failure signatures: Models failing to respect constraints (e.g., violating flight connectivity), performance degradation with increasing complexity, and self-correction leading to worse performance are key failure modes.
- First 3 experiments:
  1. Test model performance on the simplest Trip Planning task (3 cities) to establish baseline accuracy.
  2. Gradually increase complexity (4, 5, 6 cities) to identify the complexity threshold where accuracy drops below 50%.
  3. Test in-context planning with long context by providing 100, 200, 400, 800 examples to Gemini 1.5 Pro to find the optimal number of examples.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different types of planning constraints (e.g., temporal, spatial, resource) affect model performance across various task complexities?
- Basis in paper: [explicit] The paper shows that model performance drops significantly as task complexity increases (e.g., more cities, people, or days).
- Why unresolved: The paper provides overall performance trends but doesn't break down how specific constraint types impact performance.
- What evidence would resolve it: Detailed ablation studies isolating different constraint types and their individual impact on model performance.

### Open Question 2
- Question: Can models improve planning performance through iterative refinement rather than single-pass generation?
- Basis in paper: [inferred] The self-correction experiments showed performance drops, suggesting single-pass correction may not be effective.
- Why unresolved: The paper only tested simple self-correction prompting without exploring more sophisticated iterative refinement strategies.
- What evidence would resolve it: Experiments testing various iterative refinement techniques and their impact on planning accuracy.

### Open Question 3
- Question: What is the relationship between context length and planning performance for different model architectures?
- Basis in paper: [explicit] The paper shows Gemini 1.5 Pro improves with longer contexts up to 800 examples, while other models peak at 20 examples.
- Why unresolved: The paper doesn't explore the full range of context lengths or compare different model architectures systematically.
- What evidence would resolve it: Comprehensive experiments varying context length and comparing different model architectures' scaling behavior.

## Limitations

- The synthetic nature of the benchmark datasets may not fully capture real-world planning complexity and variability
- Decoupling tool-use from planning reasoning creates an artificial evaluation setup that doesn't reflect practical LLM deployment
- Limited exploration of alternative approaches like fine-tuning or retrieval augmentation for improving planning performance

## Confidence

*High Confidence*: The claim that LLM planning performance degrades with increasing task complexity (Mechanism 3) is strongly supported by empirical results showing consistent performance drops across all models as cities, people, or days increase.

*Medium Confidence*: The effectiveness of long-context in-context learning (Mechanism 1) is supported by Gemini 1.5 Pro's performance improvement with 800 examples, but the paper doesn't fully explore the limits of this approach or compare it against other methods.

*Low Confidence*: The claim that decoupling tool-use from planning reasoning meaningfully simplifies evaluation (Mechanism 2) is more of a methodological assertion than an empirically validated finding.

## Next Checks

1. **Cross-validation with Real-World Data**: Test the same models on a subset of real, human-generated trip plans and meeting schedules to verify that the synthetic data performance correlates with real-world planning capability.

2. **Integrated Tool-Use Evaluation**: Create a variant of the benchmark where models must actually query APIs (rather than receiving pre-computed outputs) to measure the combined tool-use and planning capability.

3. **Alternative Complexity Scaling**: Systematically vary task complexity using different dimensions (e.g., constraint density vs. number of entities) to determine whether the observed performance degradation is linear, exponential, or follows another pattern.