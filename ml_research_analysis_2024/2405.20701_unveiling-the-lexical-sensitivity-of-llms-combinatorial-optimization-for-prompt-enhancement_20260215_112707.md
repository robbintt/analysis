---
ver: rpa2
title: 'Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for
  Prompt Enhancement'
arxiv_id: '2405.20701'
source_url: https://arxiv.org/abs/2405.20701
tags:
- task
- answer
- question
- prompt
- demo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reveals that large language models are highly sensitive
  to small lexical variations in prompts, where even minor changes imperceptible to
  humans can significantly impact performance. To address this, the authors propose
  COPLE, a black-box combinatorial optimization framework that iteratively refines
  prompt wording based on performance feedback from proxy tasks.
---

# Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement

## Quick Facts
- arXiv ID: 2405.20701
- Source URL: https://arxiv.org/abs/2405.20701
- Reference count: 40
- Primary result: LLMs are highly sensitive to small lexical variations in prompts, and COPLE optimizes these variations to improve performance.

## Executive Summary
This paper reveals that large language models are highly sensitive to small lexical variations in prompts, where even minor changes imperceptible to humans can significantly impact performance. To address this, the authors propose COPLE, a black-box combinatorial optimization framework that iteratively refines prompt wording based on performance feedback from proxy tasks. COPLE uses word influence scores to guide the search for better lexical choices. Experiments on GLUE and MMLU datasets show that COPLE improves accuracy over human-crafted prompts, recovering both instruct-following and task-solving ability.

## Method Summary
COPLE is a black-box combinatorial optimization framework that iteratively refines prompt wording based on performance feedback from proxy tasks. It uses word influence scores to guide the search for better lexical choices, employing a pre-trained MLM to generate semantically similar candidate words for substitution. The framework focuses on optimizing the task description component of prompts while maintaining semantic similarity to the original.

## Key Results
- COPLE significantly improves LLM performance on GLUE and MMLU datasets compared to human-crafted prompts
- Optimized prompts differ semantically but retain high semantic similarity to originals
- Performance gains are achieved without changing the core meaning of prompts
- The method recovers both instruct-following and task-solving abilities of LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs are highly sensitive to minor lexical variations in prompts, where semantically similar word substitutions can lead to significant performance changes.
- Mechanism: The model's internal representations of semantically similar prompts can be close in embedding space but still produce vastly different outputs due to the non-linear nature of the model's decision boundaries.
- Core assumption: The model's performance is sensitive to lexical choices in the task description portion of prompts.
- Evidence anchors:
  - [abstract] "we reveal that LLMs are over-sensitive to lexical variations in task instructions, even when the variations are imperceptible to humans"
  - [section] "semantically similar prompts have vastly different performances on downstream tasks, even if they differ by only one word"
  - [corpus] Weak - related papers discuss prompt sensitivity but not specifically this lexical variation mechanism

### Mechanism 2
- Claim: COPLE can find optimal lexical substitutions by using proxy reference tasks and word influence scores to guide the search.
- Mechanism: The algorithm iteratively substitutes the most influential words with semantically similar candidates, evaluating performance on a small set of proxy tasks to find the combination that minimizes expected loss.
- Core assumption: Word influence scores (performance difference when a word is deleted) can effectively guide the search for better lexical choices.
- Evidence anchors:
  - [abstract] "COPLE performs iterative lexical optimization according to the feedback from a batch of proxy tasks, using a search strategy related to word influence"
  - [section] "COPLE tries to iteratively find the optimal substitution for the most influential words in the descending order of their influence"
  - [corpus] Weak - related papers discuss black-box optimization but not specifically this word influence guided approach

### Mechanism 3
- Claim: Using a pre-trained MLM to generate semantically similar candidates for each word in the prompt provides an effective search space for lexical optimization.
- Mechanism: The MLM predicts probable fill-in words for masked positions in the prompt, creating a search space of semantically similar alternatives that can be evaluated for performance improvement.
- Core assumption: A pre-trained MLM can generate semantically similar words that are likely to improve prompt performance when substituted.
- Evidence anchors:
  - [abstract] "we employ a MLM...to obtain their sentence representations in the target model and project them into a two-dimensional space using t-SNE"
  - [section] "we reuse a pre-trained MLM to find semantically similar words...selecting the top-k words with the highest probabilities and a empty token (delete) as the candidates"
  - [corpus] Weak - related papers discuss using MLMs for various tasks but not specifically for generating lexical optimization candidates

## Foundational Learning

- Concept: Lexical sensitivity of LLMs
  - Why needed here: Understanding that small word changes can significantly impact model performance is crucial for appreciating why COPLE is necessary and effective.
  - Quick check question: If a prompt "Does this sentence make sense?" is changed to "Does this logically make sense?", would you expect the model's performance to change significantly?

- Concept: Combinatorial optimization
  - Why needed here: COPLE frames prompt optimization as a combinatorial optimization problem where the goal is to find the optimal combination of words from a search space.
  - Quick check question: In a combinatorial optimization problem with 10 words and 30 candidates per word, how many possible prompt combinations would need to be evaluated?

- Concept: Proxy reference tasks and word influence
  - Why needed here: Understanding how COPLE uses proxy tasks to evaluate prompt performance and word influence scores to guide the search is key to grasping the algorithm's approach.
  - Quick check question: If deleting a word from a prompt causes the performance to drop by 10%, what would its word influence score be according to the paper's definition?

## Architecture Onboarding

- Component map:
  Input (Original prompt, target model, proxy reference tasks, pre-trained MLM) -> COPLE iterative optimization -> Output (Optimized prompt)

- Critical path:
  1. Calculate word influence scores on original prompt
  2. Iteratively optimize most influential words using MLM-generated candidates
  3. Evaluate performance on proxy reference tasks
  4. Select best candidate and update prompt
  5. Repeat until convergence or all influential words optimized

- Design tradeoffs:
  - Search space size vs. computational cost: Using top-30 candidates per word balances exploration with efficiency
  - Number of proxy tasks vs. approximation accuracy: 100 sampled tasks provide good approximation without excessive cost
  - Number of iterations vs. performance gain: Optimizing 70% most influential words provides good balance

- Failure signatures:
  - No performance improvement after several iterations
  - High variance in performance across runs
  - Optimized prompts have very low semantic similarity to original
  - Performance on proxy tasks doesn't correlate with performance on validation set

- First 3 experiments:
  1. Run COPLE on a simple GLUE task (e.g., SST2) with Llama-2-7B-chat to verify basic functionality
  2. Compare performance with and without word influence guidance to validate the search strategy
  3. Test different sizes of proxy reference tasks (e.g., 20, 50, 100) to find optimal balance of cost and performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of COPLE vary across different model scales beyond 7 billion parameters?
- Basis in paper: [explicit] The paper mentions that experimental scope is primarily restricted to models around the 7-billion-parameter scale due to computational resource limitations.
- Why unresolved: The study did not evaluate COPLE on larger or smaller models, so its generalizability to different model scales is unknown.
- What evidence would resolve it: Conducting experiments with COPLE on a range of model sizes (e.g., 1B, 13B, 30B, 70B) and comparing performance improvements would clarify the impact of model scale on COPLE's effectiveness.

### Open Question 2
- Question: Can COPLE be effectively combined with other prompt engineering strategies to achieve further performance gains?
- Basis in paper: [inferred] The paper acknowledges that while COPLE is effective, it does not explore the combination of COPLE with other prompt engineering strategies that could potentially yield further improvements.
- Why unresolved: The study focused on the standalone effectiveness of COPLE without investigating potential synergies with other techniques.
- What evidence would resolve it: Implementing and evaluating combinations of COPLE with strategies like chain-of-thought prompting, few-shot learning, or emotional prompts on the same datasets would reveal if such combinations enhance performance.

### Open Question 3
- Question: Does lexical sensitivity in prompts affect components beyond the task description, such as verbalizers or demonstration examples?
- Basis in paper: [explicit] The paper states that while COPLE focuses on optimizing lexical choices within the task description component of prompts, it is possible that lexical sensitivity affects the entirety of a prompt.
- Why unresolved: The study did not extend optimization to the full prompt due to the significant increase in search space, leaving the impact on other components untested.
- What evidence would resolve it: Expanding COPLE to optimize lexical choices in verbalizers and demonstration examples, then measuring the performance changes on downstream tasks, would indicate the extent of lexical sensitivity across prompt components.

## Limitations
- The findings on lexical sensitivity are based on specific GLUE and MMLU tasks and may not generalize to other domains
- The effectiveness of COPLE depends on the proxy reference tasks providing a reliable signal for target task performance
- Automated semantic similarity metrics may not fully capture human perception of semantic equivalence

## Confidence
- **High confidence**: The core finding that LLMs exhibit sensitivity to lexical variations in prompts is well-supported by both the paper's experiments and the broader literature on prompt sensitivity
- **Medium confidence**: The effectiveness of COPLE in improving prompt performance is demonstrated on benchmark datasets, but the extent of improvement varies significantly across models and tasks
- **Low confidence**: Claims about the semantic similarity of optimized prompts to originals rely heavily on automated metrics without extensive human evaluation

## Next Checks
1. **Ablation study on proxy task size**: Systematically vary the number of proxy reference tasks (e.g., 20, 50, 100, 200) to quantify the relationship between proxy task quantity and optimization quality. Measure both performance improvement and computational cost to identify the optimal tradeoff point.

2. **Human evaluation of semantic similarity**: Conduct human studies where annotators rate the semantic similarity between original and optimized prompts. Compare human ratings with automated metrics (USE, BERTScore) to validate whether the model's notion of semantic similarity aligns with human perception.

3. **Cross-model generalization test**: Apply COPLE-optimized prompts from one model family (e.g., Llama-2) to different model architectures (e.g., Mistral, GPT variants) to assess how well lexical optimizations transfer across models with different training approaches and architectures.