---
ver: rpa2
title: 'Saving the legacy of Hero Ibash: Evaluating Four Language Models for Aminoacian'
arxiv_id: '2402.18121'
source_url: https://arxiv.org/abs/2402.18121
tags: []
core_contribution: This study evaluates four state-of-the-art language models (Llama2,
  ChatGPT, Mistral, and Ernie-bot) for their ability to process the Aminoacian language,
  an under-resourced and structurally complex language. The evaluation focuses on
  machine translation to Chinese, question answering, and entailment recognition tasks.
---

# Saving the legacy of Hero Ibash: Evaluating Four Language Models for Aminoacian

## Quick Facts
- **arXiv ID:** 2402.18121
- **Source URL:** https://arxiv.org/abs/2402.18121
- **Reference count:** 5
- **Primary result:** Four state-of-the-art language models failed to produce meaningful translations or task outputs for Aminoacian language tasks

## Executive Summary
This study evaluates four leading language models (Llama2, ChatGPT, Mistral, and Ernie-bot) on their ability to process Aminoacian, an under-resourced language with complex syntactic and tonal features. Despite employing standard evaluation metrics (BLEU, METEOR, ROUGE), all models uniformly failed to generate meaningful translations or perform question answering and entailment recognition tasks. The results highlight a critical gap in current language models' ability to handle structurally complex, low-resource languages, particularly those with rare features like OVS syntax and tonal systems.

## Method Summary
The study tested four state-of-the-art language models on three tasks: machine translation to Chinese, question answering, and entailment recognition. Standard evaluation metrics (BLEU, METEOR, ROUGE) were applied across all tasks. The models tested were Llama2, ChatGPT, Mistral, and Ernie-bot. The Aminoacian language was characterized as having rare OVS (Object-Verb-Subject) sentence patterns and a tonal system. No domain-specific adaptations were mentioned for the evaluation metrics, and details about the training data size or preprocessing steps were not disclosed.

## Key Results
- All four language models (Llama2, ChatGPT, Mistral, Ernie-bot) failed to produce meaningful translations for Aminoacian
- Standard evaluation metrics (BLEU, METEOR, ROUGE) yielded uniformly null results across all tasks
- The models' inability to capture Aminoacian's unique syntactic (OVS) and semantic (tonal) structures was identified as the primary failure mechanism

## Why This Works (Mechanism)
None provided in source material.

## Foundational Learning

**OVS syntax**: Object-Verb-Subject sentence structure, rare among world languages
*Why needed*: Aminoacian uses this uncommon word order, which most language models aren't trained to handle
*Quick check*: Verify sentence structure examples from Aminoacian corpus

**Tonal system**: Pitch variations that change word meaning
*Why needed*: Aminoacian's tonal nature affects semantic interpretation, which standard models struggle with
*Quick check*: Test model responses to minimal pairs (words differing only in tone)

**Low-resource language processing**: Handling languages with limited training data
*Why needed*: Aminoacian is under-resourced, limiting model exposure during training
*Quick check*: Compare model performance on aminoacian vs. other low-resource languages

## Architecture Onboarding

**Component map**: Input text -> Tokenizer -> Encoder layers -> Decoder layers -> Output generator -> Evaluation metrics

**Critical path**: Raw Aminoacian text → Tokenization → Model processing → Task-specific output generation → Metric evaluation

**Design tradeoffs**: Standard models prioritize high-resource languages with common SVO syntax, sacrificing ability to handle rare structures like OVS and tonal systems

**Failure signatures**: Null BLEU/METEOR/ROUGE scores, nonsensical output generation, inability to preserve syntactic relationships

**First experiments**:
1. Test zero-shot prompting with explicit OVS syntax instructions
2. Apply aminoacian-specific tokenization to preserve tonal information
3. Compare performance against other rare languages with similar structural features

## Open Questions the Paper Calls Out
None identified in source material.

## Limitations
- Standard evaluation metrics may not be appropriate for Aminoacian's unique structural features
- No information provided about Aminoacian training data size or composition for each model
- No details on preprocessing steps or prompt engineering attempts that might have improved performance

## Confidence
- Claim: All models uniformly failed - **High confidence**
- Claim: Aminoacian's structural complexity caused failure - **Medium confidence**
- Claim: Need for improved architectures and training datasets - **Medium confidence**

## Next Checks
1. Conduct controlled experiment testing same models on other rare languages with similar structural features
2. Implement Aminoacian-specific preprocessing and tokenization strategies, then re-evaluate
3. Test zero-shot and few-shot prompting strategies with prompts accounting for OVS syntax and tonal properties