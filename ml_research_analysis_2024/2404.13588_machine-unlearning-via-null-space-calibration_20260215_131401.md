---
ver: rpa2
title: Machine Unlearning via Null Space Calibration
arxiv_id: '2404.13588'
source_url: https://arxiv.org/abs/2404.13588
tags:
- unlearning
- space
- samples
- 'null'
- unsc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of over-unlearning in machine
  unlearning, where existing methods degrade model performance on remaining data after
  unlearning. The authors propose UNSC (Unlearning via Null Space Calibration), which
  confines the unlearning process to a null space tailored to remaining samples to
  avoid over-unlearning.
---

# Machine Unlearning via Null Space Calibration

## Quick Facts
- arXiv ID: 2404.13588
- Source URL: https://arxiv.org/abs/2404.13588
- Authors: Huiqiang Chen; Tianqing Zhu; Xin Yu; Wanlei Zhou
- Reference count: 29
- Primary result: UNSC achieves comparable/better performance than retraining with near-zero accuracy on unlearned classes

## Executive Summary
This paper addresses over-unlearning in machine unlearning, where existing methods degrade performance on remaining data. The authors propose UNSC (Unlearning via Null Space Calibration), which confines unlearning to a null space tailored to remaining samples and uses pseudo-labeling to improve model performance. UNSC achieves near-zero accuracy on unlearned classes while maintaining or improving accuracy on remaining classes, outperforming state-of-the-art methods.

## Method Summary
UNSC performs machine unlearning by first estimating class-specific subspaces using SVD on feature representations, then projecting gradients onto the null space orthogonal to remaining samples. Pseudo-labeling assigns each unlearning sample the most similar remaining class based on the original model's predictions. The method updates weights using null-space-projected gradients, theoretically preventing over-unlearning while improving forgetting through decision space calibration.

## Key Results
- UNSC achieves comparable or better performance than retraining across multiple datasets and architectures
- Near-zero accuracy (2.78%-10.01%) on unlearned classes while maintaining/improving accuracy on remaining classes
- Outperforms state-of-the-art methods like I-BIF and Top-K in forgetting and utility trade-off

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unlearning in the null space prevents over-unlearning by ensuring weight updates orthogonal to the remaining samples' subspace
- Mechanism: Gradients for unlearning are projected onto a null space perpendicular to remaining samples, so weight changes don't alter predictions on remaining data
- Core assumption: Gradients for classification lie in the subspace spanned by input features
- Evidence: [abstract] "confines the unlearning process to a specified null space" and [section 4.1] "We project the gradient to a space perpendicular to ˆS¯c"
- Break condition: If remaining samples' subspace is inaccurately estimated, projection may not fully avoid over-unlearning

### Mechanism 2
- Claim: Pseudo-labeling unlearning samples as most similar remaining class improves forgetting and decision-space calibration
- Mechanism: Assigns each unlearning sample the class most confidently predicted by the original model among remaining classes
- Core assumption: Original model's top-1 remaining-class prediction is a good proxy for most similar remaining class
- Evidence: [abstract] "pseudo-labeling to improve forgetting" and [section 4.2] "we label an unlearning sample as the most similar class"
- Break condition: If pseudo-labels mismatch ground truth too often, they may hurt utility or slow forgetting

### Mechanism 3
- Claim: Theoretical guarantee: unlearning in null space yields approximately same loss on remaining samples as original model
- Mechanism: Projection ensures weight changes orthogonal to remaining-sample gradients, so first-order Taylor expansion shows loss change ≈ 0
- Core assumption: Weight changes are small enough that higher-order terms can be ignored
- Evidence: [section 5.1] Theorem 1 and proof showing ∆θT∇θLr(θo) = 0 in null space
- Break condition: If unlearning is aggressive (large ∆θ), second-order terms become significant

## Foundational Learning

- Concept: Null space and orthogonal projection
  - Why needed here: Core to unlearning mechanism—ensures weight updates don't affect remaining samples
  - Quick check: Given a subspace S and its orthogonal complement S⊥, what is the projection matrix onto S⊥?

- Concept: Singular value decomposition (SVD) for subspace estimation
  - Why needed here: Used to extract dominant eigenvectors spanning class-specific subspaces from layer activations
  - Quick check: If R has dimensions 64×256 and we keep top-20 singular values, what is the dimension of the resulting subspace basis?

- Concept: Gradient flow in neural networks
  - Why needed here: Underpins claim that gradients lie in input-span subspace, justifying null-space projection
  - Quick check: For a linear classifier w⊤x, in which subspace do gradients with respect to w lie?

## Architecture Onboarding

- Component map: Layer-wise subspace estimator (SVD) -> Null-space projector (I - S(S⊤)) -> Pseudo-labeler (top-1 prediction) -> Unlearning optimizer (SGD)
- Critical path: 1. Estimate class subspaces (one-time SVD pass) 2. For each unlearning sample: find null space, assign pseudo-label 3. Update weights using null-space-projected gradients
- Design tradeoffs: Batch size for subspace estimation vs. accuracy, rank-k truncation vs. approximation fidelity, pseudo-label strategy vs. forgetting vs. utility
- Failure signatures: Accuracy drop on remaining data → over-unlearning, slow forgetting on unlearned classes → pseudo-labeling ineffective, unstable training → rank-k too low
- First 3 experiments: 1. Ablation: Remove null-space constraint, measure over-unlearning 2. Ablation: Remove pseudo-labeling, measure accuracy loss 3. Sweep: Vary rank-k threshold, plot AccDrt/AccDut trade-off

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does UNSC's performance scale when unlearning a large number of classes or samples, particularly on datasets with many classes like CIFAR-100?
- Basis: Paper shows performance on CIFAR-100 for 10-50 classes but doesn't explore extreme scenarios
- Why unresolved: Doesn't investigate UNSC's limits or performance degradation when unlearning 70-90% of classes
- What evidence would resolve it: Experiments showing UNSC's performance when unlearning 70-90% of classes on CIFAR-100

### Open Question 2
- Question: Does UNSC's pseudo-labeling strategy always align with optimal label assignment for unlearning?
- Basis: Paper assumes pseudo-labels based on original model's predictions are optimal but doesn't explore alternatives
- Why unresolved: Doesn't compare UNSC's approach with other strategies or analyze scenarios where it might fail
- What evidence would resolve it: Comparative experiments using different pseudo-labeling strategies and analysis of edge cases

### Open Question 3
- Question: How does UNSC's computational complexity scale with number of unlearning requests?
- Basis: Paper mentions efficiency but doesn't provide detailed complexity analysis or comparisons
- Why unresolved: Doesn't quantify UNSC's computational requirements or compare efficiency with other methods
- What evidence would resolve it: Detailed runtime and resource usage analysis compared to other unlearning methods

### Open Question 4
- Question: How robust is UNSC to adversarial attacks or poisoning during unlearning?
- Basis: Paper doesn't address UNSC's security against adversarial manipulation of pseudo-labeling process
- Why unresolved: Reliance on pseudo-labels could potentially be exploited by attackers
- What evidence would resolve it: Experiments testing UNSC's performance under adversarial attacks targeting pseudo-labeling

## Limitations
- Scalability untested on larger datasets and deeper architectures
- Theoretical guarantees rely on first-order approximations that may break with aggressive unlearning
- Method's robustness to batch-size variations in subspace estimation not thoroughly explored

## Confidence

- **High**: Null-space projection effectively prevents over-unlearning (supported by Theorem 1 and empirical results)
- **Medium**: Pseudo-labeling improves both forgetting and utility (supported by results but lacks quality ablation)
- **Low**: Method generalizes across all architectures and datasets (only tested on 4 datasets and 4 architectures)

## Next Checks

1. **Robustness sweep**: Vary batch size for subspace estimation and measure AccDrt stability across different dataset sizes
2. **Pseudo-label ablation**: Replace top-1 pseudo-labeling with ground-truth labels (if available) and measure change in forgetting rate vs accuracy trade-off
3. **Second-order effect measurement**: Compare first-order approximation error against actual loss change on remaining samples after aggressive unlearning steps