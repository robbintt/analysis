---
ver: rpa2
title: 'Exploring Performance Contrasts in TableQA: Step-by-Step Reasoning Boosts
  Bigger Language Models, Limits Smaller Language Models'
arxiv_id: '2411.16002'
source_url: https://arxiv.org/abs/2411.16002
tags:
- smaller
- bigger
- table
- reasoning
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a detailed step-by-step reasoning method called\
  \ Table-Logic to investigate performance differences between larger (\u226570B)\
  \ and smaller (\u223C7B) language models in TableQA tasks. The method guides models\
  \ through sequential steps: understanding table structure, identifying critical\
  \ columns and rows, and performing necessary aggregations or comparisons to answer\
  \ questions."
---

# Exploring Performance Contrasts in TableQA: Step-by-Step Reasoning Boosts Bigger Language Models, Limits Smaller Language Models

## Quick Facts
- arXiv ID: 2411.16002
- Source URL: https://arxiv.org/abs/2411.16002
- Authors: Haoyan Yang; Yixuan Wang; Keyue Tong; Hongjin Zhu; Yuanxin Zhang
- Reference count: 9
- Key outcome: Table-Logic improves accuracy by 7.8% for larger models (≥70B) but causes 11% decline for smaller models (~7B) in TableQA tasks

## Executive Summary
This paper investigates how step-by-step reasoning affects TableQA performance differently across model sizes. The authors propose Table-Logic, a sequential reasoning method that breaks down table question answering into five distinct steps: understanding table structure, identifying critical columns and rows, and performing necessary aggregations. Experimental results reveal a stark performance contrast: while Llama-3-70B achieves 7.8% accuracy improvement on HybridQA, Llama-2-7B suffers an 11% performance decline. Through detailed analysis of seven subtasks, the study demonstrates that smaller models struggle particularly with understanding table structures and locating relevant columns, while larger models effectively leverage intermediate reasoning steps.

## Method Summary
The Table-Logic method guides language models through sequential reasoning steps for TableQA tasks. The approach breaks down the problem into five stages: reading and understanding the table structure, identifying critical columns, identifying relevant rows, determining necessary aggregations or calculations, and generating the final answer. The method is evaluated across three datasets (TAT-QA, HybridQA, WikiTableQuestions) using three larger models (≥70B: Llama-3-70B, GPT-3.5-Turbo, Qwen1.5-72B) and three smaller models (~7B: Llama-2-7B, Vicuna-7B, QWEN1.5-7B). GPT-4 serves as the evaluation reference. The paper also implements seven subtask analyses to understand capability differences between model sizes.

## Key Results
- Table-Logic improves accuracy by 7.8% for larger models like Llama-3-70B on HybridQA
- The same method causes 11% performance decline for smaller models like Llama-2-7B
- Smaller models struggle particularly with understanding table structures and locating relevant columns
- Larger models can effectively leverage intermediate reasoning steps while smaller models cannot

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Table-Logic improves performance for larger models by providing structured intermediate reasoning steps that align with their superior capability to process complex sequential information.
- Mechanism: The method breaks down TableQA into sequential steps (table structure understanding, column/row identification, aggregation/calculation) with explicit prompts at each stage. Larger models (Llama-3-70B) can leverage these intermediate outputs to generate more accurate final answers.
- Core assumption: Larger models have better capacity to understand and utilize intermediate reasoning steps, while smaller models struggle with this multi-step approach.
- Evidence anchors:
  - [abstract] "Table-Logic improves accuracy by 7.8% for larger models like Llama-3-70B on HybridQA, but causes an 11% performance decline for smaller models like Llama-2-7B"
  - [section] "Experimental results show that this approach, as expected, enhances performance in bigger LMs but deteriorates in smaller LMs"
  - [corpus] Weak evidence - corpus contains related TableQA work but no direct evidence about step-by-step reasoning performance contrasts between model sizes

### Mechanism 2
- Claim: Smaller models perform worse with Table-Logic because they generate misleading intermediate information that propagates through subsequent steps.
- Mechanism: When smaller models struggle with early steps (like identifying critical columns/rows), they produce incorrect intermediate outputs. These errors compound through the reasoning chain, leading to wrong final answers. The sequential nature of Table-Logic amplifies these errors rather than mitigating them.
- Core assumption: The quality of intermediate steps directly impacts the final answer, and smaller models are more susceptible to error propagation in multi-step reasoning.
- Evidence anchors:
  - [abstract] "smaller models struggle particularly with understanding table structures and locating relevant columns"
  - [section] "smaller models struggle to correctly generate the answer, leading to subsequent reasoning based on misleading rows and columns, resulting in wrong results"
  - [corpus] Weak evidence - corpus discusses TableQA methods but doesn't specifically address error propagation in step-by-step reasoning

### Mechanism 3
- Claim: The gap between bigger and smaller models in Table-Logic performance reveals fundamental differences in their ability to understand structured tabular data versus treating it as unstructured text.
- Mechanism: Bigger models can better understand table structure (rows, columns, relationships) and use this understanding in the reasoning process. Smaller models tend to treat tables as paragraphs of information, struggling to map questions to specific table locations, which Table-Logic's structured approach exacerbates rather than helps.
- Core assumption: Understanding table structure is a prerequisite for effective step-by-step reasoning in TableQA, and this capability differs significantly between model sizes.
- Evidence anchors:
  - [section] "This suggests that smaller models are more likely to treat tables as paragraphs of information rather than understanding their structure"
  - [section] "smaller models struggle particularly with understanding table structures and locating relevant columns"
  - [corpus] Weak evidence - corpus contains TableQA research but lacks direct comparison of structural understanding between model sizes

## Foundational Learning

- Concept: Table structure understanding (rows, columns, headers)
  - Why needed here: Table-Logic relies on the model's ability to identify critical columns and rows, which requires understanding the table's two-dimensional structure
  - Quick check question: Given a table with 5 columns labeled A-E and 3 rows of data, can you identify which column contains the target information for the question "What is the value in column C for row 2?"

- Concept: Sequential reasoning and error propagation
  - Why needed here: The method's effectiveness depends on how errors in intermediate steps affect final outcomes, particularly for smaller models
  - Quick check question: If a model incorrectly identifies the critical column in step 2 of a 4-step reasoning process, how might this error impact the final answer?

- Concept: Model parameter scaling effects on reasoning capabilities
  - Why needed here: The performance contrast between bigger and smaller models in this task demonstrates how parameter count affects reasoning abilities
  - Quick check question: Based on the results showing a 7.8% improvement for Llama-3-70B but an 11% decline for Llama-2-7B, what does this suggest about the relationship between model size and step-by-step reasoning effectiveness?

## Architecture Onboarding

- Component map: Table → Sequential prompts (5 steps) → Intermediate outputs → Final answer
- Critical path: Table structure understanding → Column identification → Row identification → Aggregation determination → Final prediction
- Design tradeoffs: Multi-step reasoning improves accuracy for larger models but increases computational cost and can degrade performance for smaller models
- Failure signatures: Performance decline in smaller models, error propagation through intermediate steps, inability to understand table structure
- First 3 experiments:
  1. Run Table-Logic on Llama-3-70B and Llama-2-7B with the same table and question to observe performance contrast
  2. Test the 7 subtasks individually on both model sizes to identify specific capability gaps
  3. Implement the column finding task with ground truth columns to see if performance improves for smaller models, isolating the structure understanding issue

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can smaller models be effectively improved to handle step-by-step reasoning in TableQA tasks, given their current limitations with table structure understanding?
- Basis in paper: [explicit] The paper identifies that smaller models struggle with understanding table structures and locating relevant columns, which are critical for step-by-step reasoning.
- Why unresolved: While the paper highlights these limitations, it does not provide a concrete solution or methodology for enhancing these specific capabilities in smaller models.
- What evidence would resolve it: Development and testing of a targeted training regimen or architectural modification that demonstrably improves smaller models' ability to understand table structures and locate relevant columns, validated through improved performance on TableQA tasks.

### Open Question 2
- Question: What are the potential trade-offs between model size and reasoning capability when applying step-by-step reasoning methods to TableQA tasks?
- Basis in paper: [explicit] The paper contrasts the performance of larger (≥70B) and smaller (~7B) models, showing that step-by-step reasoning improves larger models but hinders smaller ones.
- Why unresolved: The paper does not explore the underlying mechanisms or trade-offs that cause larger models to benefit from step-by-step reasoning while smaller models do not.
- What evidence would resolve it: A detailed study analyzing the reasoning processes and computational efficiencies of different model sizes during step-by-step reasoning, identifying specific bottlenecks or advantages.

### Open Question 3
- Question: Can hybrid approaches combining strengths of both larger and smaller models improve TableQA task performance?
- Basis in paper: [inferred] The paper suggests that smaller models struggle with table structure understanding, while larger models excel, indicating potential for complementary use.
- Why unresolved: The paper does not explore or propose methods for leveraging the strengths of both model sizes in a single framework.
- What evidence would resolve it: Experimental results demonstrating improved TableQA performance using a hybrid model approach that integrates the structural understanding of larger models with the efficiency of smaller models.

## Limitations

- The paper identifies that smaller models struggle with table structure understanding but doesn't fully explain why this limitation exists or how to address it
- The performance contrast between model sizes remains partially unexplained, particularly regarding why intermediate reasoning steps help larger models but harm smaller ones
- The method's reliance on multiple sequential API calls introduces potential variability and computational overhead that isn't fully characterized

## Confidence

**High Confidence**: The claim that Table-Logic improves accuracy for larger models (7.8% improvement on Llama-3-70B for HybridQA) is well-supported by the experimental results presented in the abstract and discussion sections. The performance decline for smaller models (11% on Llama-2-7B) is also consistently reported across multiple evaluations.

**Medium Confidence**: The mechanism explaining why smaller models struggle with Table-Logic - specifically the error propagation hypothesis - is plausible given the evidence but requires additional validation. The paper provides good evidence that smaller models struggle with table structure understanding, but the causal chain linking this to overall performance decline needs more rigorous testing.

**Low Confidence**: The broader claim about fundamental differences in how larger versus smaller models understand structured tabular data versus unstructured text is interesting but not fully substantiated. While the paper observes behavioral differences, it doesn't definitively prove that larger models have genuinely superior structural understanding versus simply better task completion capabilities.

## Next Checks

1. **Isolate Error Sources**: Implement controlled experiments where ground truth intermediate steps are provided to smaller models to determine whether performance improves. This would help distinguish between inability to understand table structure versus inability to generate correct intermediate reasoning steps.

2. **Cross-Dataset Consistency**: Test Table-Logic across all three datasets (TAT-QA, HybridQA, WikiTableQuestions) with both model sizes to verify whether the performance patterns hold consistently or if they're dataset-specific artifacts.

3. **Step-by-Step Ablation**: Conduct experiments where individual steps of the Table-Logic process are removed or modified to identify which specific steps contribute most to the performance gap between larger and smaller models.