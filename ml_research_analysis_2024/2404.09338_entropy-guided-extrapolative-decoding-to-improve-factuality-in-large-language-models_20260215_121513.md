---
ver: rpa2
title: Entropy Guided Extrapolative Decoding to Improve Factuality in Large Language
  Models
arxiv_id: '2404.09338'
source_url: https://arxiv.org/abs/2404.09338
tags:
- layer
- layers
- decoding
- extrapolation
- truthfulqa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to improve the factuality of large
  language models (LLMs) by leveraging the hierarchical representation of factual
  knowledge within the model. The proposed approach, called extrapolative decoding,
  dynamically selects a contrasting layer based on token entropy and extrapolates
  critical token probabilities beyond the last layer to enhance the model's predictive
  maturity.
---

# Entropy Guided Extrapolative Decoding to Improve Factuality in Large Language Models

## Quick Facts
- arXiv ID: 2404.09338
- Source URL: https://arxiv.org/abs/2404.09338
- Reference count: 18
- This paper introduces entropy-guided extrapolative decoding to improve factuality in LLMs by dynamically selecting contrasting layers and extrapolating token probabilities beyond the final layer.

## Executive Summary
This paper proposes a novel method to improve the factuality of large language models by leveraging hierarchical knowledge representation within transformer layers. The approach combines contrastive decoding with entropy-guided layer selection and logit extrapolation to reduce hallucinations during inference. By dynamically selecting contrasting layers based on token entropy and extrapolating critical token probabilities beyond the last layer, the method enhances the model's predictive maturity and suppresses overconfident, factually incorrect predictions.

## Method Summary
The method employs entropy-guided extrapolative decoding that works by first computing token-wise entropy across candidate transformer layers. For each token, it selects a contrasting layer based on entropy (minimum for open-ended prompts, maximum for factual prompts). It then checks if the top-k token probabilities evolve monotonically over the last few layers. If monotonicity holds and entropy change exceeds a threshold, it applies linear regression to extrapolate probabilities to a later inference layer. Finally, it performs contrastive decoding using the mature final layer logits and the extrapolated contrasting layer logits to produce the final token distribution.

## Key Results
- Outperforms state-of-the-art techniques on TruthfulQA by large margins
- Demonstrates significant improvements on FACTOR dataset for factual accuracy
- Shows effectiveness across LLaMA model sizes (7B, 13B, 33B, 65B)

## Why This Works (Mechanism)

### Mechanism 1
Lower transformer layers contain less contextually mature but less overconfident representations that can correct hallucinations from higher layers. By contrasting logits from a dynamically selected lower layer with the final layer, the method suppresses overconfident, factually incorrect predictions. This works under the assumption that lower layers are less overconfident, making them useful for grounding final predictions.

### Mechanism 2
Logit extrapolation allows the final layer to produce more mature distributions by extending probability evolution beyond the last transformer layer. For tokens with monotonic probability changes over the last few layers, linear regression extrapolates their probabilities to a later inference layer, sharpening the distribution before contrastive decoding. This assumes probabilities evolve predictably and can be extended without overfitting.

### Mechanism 3
Entropy-guided layer selection removes dependence on the final layer's maturity, enabling effective contrast even when the last layer is unreliable. Token-wise entropy across candidate layers is computed, with minimum entropy selected for open-ended prompts and maximum entropy for factual prompts. This decouples selection from the final layer based on the assumption that entropy correlates with factuality differently for different prompt types.

## Foundational Learning

- **Entropy as uncertainty measure**: Used to quantify confidence of token predictions across layers for layer selection. Quick check: What happens to entropy when a distribution becomes more peaked around a single token?

- **Contrastive decoding**: Uses differences between mature and premature layer logits to steer generation and suppress hallucinations. Quick check: How does subtracting log probabilities from two layers influence the final token distribution?

- **Monotonicity in probability evolution**: Prerequisite for safe logit extrapolation; non-monotonic changes would invalidate linear regression extrapolation. Quick check: Why is it unsafe to extrapolate probabilities that first increase then decrease across layers?

## Architecture Onboarding

- **Component map**: Input tokens → Embedding layer → Stacked transformer layers → Final logits → Optional extrapolation → Contrastive decoding → Output token
- **Critical path**: 1) Forward pass through transformer layers, 2) Compute entropy for each candidate contrasting layer, 3) Select contrasting layer based on entropy strategy, 4) Check monotonicity of top-k token probabilities over last L layers, 5) If monotonicity holds and entropy change threshold met, extrapolate logits, 6) Apply contrastive objective using mature and contrasting layer logits, 7) Decode next token using resulting distribution
- **Design tradeoffs**: Using entropy vs other metrics trades simplicity for potential robustness; monotonicity filtering limits extrapolation to a subset of tokens, trading coverage for safety; linear regression extrapolation is lightweight but assumes linear evolution
- **Failure signatures**: Degraded performance on factual prompts when using minimum entropy selection; sudden performance drops when entropy fails to correlate with factuality; increased hallucination when extrapolation is applied to non-monotonic tokens
- **First 3 experiments**: 1) Run model on open-ended prompts, logging entropy per layer and checking if minimum entropy aligns with correct answers, 2) Apply logit extrapolation to monotonic tokens and compare perplexity before/after, 3) Compare contrastive decoding performance using entropy-guided vs fixed final layer selection on mixed validation set

## Open Questions the Paper Calls Out

- **Open Question 1**: How does extrapolation of critical token probabilities beyond the last layer affect the model's ability to generalize to unseen data? The paper demonstrates improved factuality but doesn't explore impact on generalization to new datasets.

- **Open Question 2**: What is the impact of different selection strategies (maximum entropy vs. minimum entropy) on performance for different prompt types? The paper provides a hypothesis but lacks empirical evidence for effectiveness across factual vs open-ended prompts.

- **Open Question 3**: How do the extrapolation factor (α) and inference extrapolation layer (Ei) influence factuality and informativeness? The paper provides some insights but lacks comprehensive analysis of their impact.

- **Open Question 4**: How does the proposed method compare to other state-of-the-art techniques for mitigating hallucination, such as retrieval-augmented generation? The paper compares only to specific baseline techniques, not other relevant approaches.

## Limitations
- The method's reliance on entropy as a proxy for factuality may not generalize across all prompt types or model architectures
- Linear regression extrapolation assumes monotonic probability evolution which may not hold for all tokens
- The method requires careful tuning of hyperparameters that are not fully specified in the paper

## Confidence
- **High confidence**: The core mechanism of using contrastive decoding between different transformer layers to improve factuality is well-established
- **Medium confidence**: The entropy-guided layer selection strategy shows promise but requires more extensive validation
- **Low confidence**: The logit extrapolation component, while theoretically sound, lacks extensive empirical validation

## Next Checks
1. Test the method on additional model architectures beyond LLaMA to verify improvements are not architecture-specific
2. Conduct a controlled ablation study isolating the impact of logit extrapolation across different token types and trigger thresholds
3. Systematically analyze the correlation between layer-wise entropy and factuality across diverse prompt categories with human evaluation