---
ver: rpa2
title: 'Learn to Tour: Operator Design For Solution Feasibility Mapping in Pickup-and-delivery
  Traveling Salesman Problem'
arxiv_id: '2404.11458'
source_url: https://arxiv.org/abs/2404.11458
tags:
- tour
- operator
- node
- pdtsp
- feasible
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the pickup-and-delivery traveling salesman
  problem (PDTSP), a variant of the classic TSP with precedence constraints between
  pickup and delivery nodes. The authors propose a reinforcement learning framework
  using operators that map one feasible solution to another, avoiding the infeasible
  solution space.
---

# Learn to Tour: Operator Design For Solution Feasibility Mapping in Pickup-and-delivery Traveling Salesman Problem

## Quick Facts
- arXiv ID: 2404.11458
- Source URL: https://arxiv.org/abs/2404.11458
- Reference count: 40
- One-line primary result: Reinforcement learning framework using feasibility-preserving operators outperforms baselines on pickup-and-delivery TSP, achieving up to 10.3% shorter tours on larger instances.

## Executive Summary
This paper introduces a reinforcement learning framework for solving the pickup-and-delivery traveling salesman problem (PDTSP), a variant of TSP with precedence constraints between pickup and delivery nodes. The key innovation is the use of operators that map feasible solutions to other feasible solutions, avoiding the infeasible solution space. The authors propose five operators (intra-block node-exchange, inter-block node-exchange, node pair-exchange, same-type block-exchange, and mixed-type block-exchange) that are proven to always generate feasible Hamiltonian cycles. The method is evaluated against existing baselines, including Google OR tools, Gurobi, and heuristic algorithms, demonstrating superior solution quality and computational efficiency across various problem scales.

## Method Summary
The method constructs initial feasible tours by randomizing pickup node order and appending corresponding delivery nodes. It then employs a reinforcement learning framework where five operators (intra-block node-exchange, inter-block node-exchange, node pair-exchange, same-type block-exchange, and mixed-type block-exchange) serve as actions. These operators are designed to preserve solution feasibility by construction. The RL agent, trained using PPO, learns to select operators that improve tour cost. The policy and value networks use a hidden dimension of 256, 4 heads, and two MLP layers of 256. Training uses a replay buffer to store experience tuples (state, action, reward, next state) and updates until convergence.

## Key Results
- Proposed method outperforms Google OR tools, Gurobi, and heuristic algorithms on solution quality and computational efficiency
- Achieves up to 10.3% shorter tours compared to best baseline on larger instances (up to 101 nodes)
- Demonstrates effectiveness across various problem scales from small to large instances

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The operator design framework efficiently explores feasible solutions by avoiding the infeasible space dominated by precedence constraint violations.
- Mechanism: The five operators (intra-block node-exchange, inter-block node-exchange, node pair-exchange, same-type block-exchange, mixed-type block-exchange) are proven to always map one feasible tour to another without violating precedence constraints.
- Core assumption: Any feasible solution in PDTSP can be reached from any other feasible solution through a sequence of these operators.
- Evidence anchors:
  - [abstract] "To restrict solution search within a feasible space, we utilize operators that always map one feasible solution to another, without spending time exploring the infeasible solution space."
  - [section] "We prove that operators in the unified operator set can always generate feasible Hamiltonian cycles in PDTSP."
- Break condition: If any operator violates precedence constraints or fails to map feasible solutions correctly, the entire framework would break down.

### Mechanism 2
- Claim: The reinforcement learning framework effectively evaluates and selects operators as policies to solve PDTSPs.
- Mechanism: The RL agent learns to choose the best operator at each step based on the current tour state, optimizing for minimal tour cost through policy and value networks.
- Core assumption: The operator selection process converges to an optimal or near-optimal policy that consistently improves solution quality.
- Evidence anchors:
  - [abstract] "Such operators are evaluated and selected as policies to solve PDTSPs in an RL framework."
  - [section] "We utilize the unified operator set as the action set in an RL framework for solving PDTSP, where operators are evaluated and selected as policies given the current state (i.e., a feasible solution of PDTSP)."
- Break condition: If the RL training fails to converge or consistently selects poor operators, the solution quality would degrade significantly.

### Mechanism 3
- Claim: The block representation of tours enables efficient operator design and execution.
- Mechanism: Tours are decomposed into pickup and delivery blocks, allowing operators to swap nodes and blocks while maintaining precedence constraints.
- Core assumption: Any feasible tour can be represented as a sequence of pickup and delivery blocks, and operators can be designed to work within this representation.
- Evidence anchors:
  - [section] "Proposition 3.2. A tour can always be represented by a sequence of P- and D-blocks."
  - [section] "Proposition 3.3. A block with sizeâ‰¥ 2 can be decomposed as two adjacent blocks. On the contrary, any two adjacent P- (D-) blocks can be combined as one P- (D-) block."
- Break condition: If the block representation fails to capture all feasible tours or operators cannot be designed to work within this representation, the entire framework would break down.

## Foundational Learning

- Concept: Pickup-and-Delivery Traveling Salesman Problem (PDTSP)
  - Why needed here: Understanding PDTSP is crucial for grasping the problem domain and the significance of the proposed solution.
  - Quick check question: What are the key differences between PDTSP and the classic TSP?

- Concept: Reinforcement Learning (RL) Framework
  - Why needed here: The RL framework is the core of the proposed solution, and understanding its components is essential for implementing and extending the method.
  - Quick check question: What are the main components of an RL framework, and how do they apply to the PDTSP problem?

- Concept: Operator Design for Constraint Satisfaction
  - Why needed here: The operator design is the key innovation in this paper, and understanding how operators can be designed to satisfy constraints is crucial for applying this approach to other problems.
  - Quick check question: How do the proposed operators ensure that precedence constraints are always satisfied in PDTSP?

## Architecture Onboarding

- Component map:
  - Initial tour construction -> Policy network -> Operator execution -> Reward calculation -> Replay buffer -> Value network

- Critical path:
  1. Construct initial feasible tour.
  2. Use policy network to select operator.
  3. Execute operator to generate new tour.
  4. Calculate reward (tour cost difference).
  5. Store experience in replay buffer.
  6. Update policy and value networks using PPO.
  7. Repeat until convergence or maximum steps.

- Design tradeoffs:
  - Operator set size vs. solution quality: More operators may improve solution quality but increase computational complexity.
  - RL algorithm choice: PPO is used for stable training, but other algorithms may be more suitable depending on the problem scale.
  - Neural network architecture: The current architecture balances model capacity and computational efficiency.

- Failure signatures:
  - Poor solution quality: Indicates issues with operator design or RL training.
  - Slow convergence: Suggests problems with the policy or value network architecture.
  - Inability to find feasible solutions: May indicate errors in operator implementation or initial tour construction.

- First 3 experiments:
  1. Implement and test each operator individually to verify they preserve solution feasibility.
  2. Train the RL agent with a small problem instance (e.g., 5 PD node pairs) to validate the overall framework.
  3. Compare the performance of the proposed method with a baseline (e.g., LKH3.0) on a medium-sized problem instance (e.g., 20 PD node pairs).

## Open Questions the Paper Calls Out
- None explicitly stated in the paper.

## Limitations
- Lack of detailed hyperparameter specifications for RL training creates significant barriers to faithful reproduction
- Limited exploration of performance on problem sizes significantly larger than 101 nodes
- No ablation study or sensitivity analysis of hyperparameters on method performance

## Confidence
- High confidence in the operator design mechanism and its ability to preserve feasibility
- Medium confidence in the RL framework's effectiveness due to missing hyperparameter details
- Low confidence in exact reproduction without additional implementation specifications

## Next Checks
1. Implement each of the five operators individually and verify through unit testing that they preserve solution feasibility across diverse initial tour configurations, particularly for edge cases involving adjacent blocks and mixed-type swaps.

2. Train the complete RL framework on a small PDTSP instance (5 PD node pairs) with controlled hyperparameters to validate that the policy network learns to select operators that improve solution quality and that the framework converges as expected.

3. Replicate the comparison with OR-Tools on medium-sized instances (20 PD node pairs) to verify the claimed computational efficiency improvements, using the same instance generation method and evaluation metrics.