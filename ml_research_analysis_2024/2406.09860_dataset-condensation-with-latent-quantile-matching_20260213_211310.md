---
ver: rpa2
title: Dataset Condensation with Latent Quantile Matching
arxiv_id: '2406.09860'
source_url: https://arxiv.org/abs/2406.09860
tags:
- dataset
- synthetic
- latent
- distribution
- matching
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes Latent Quantile Matching (LQM), a dataset\
  \ condensation method that addresses limitations of Maximum Mean Discrepance (MMD)\
  \ by matching latent distribution quantiles instead of means. LQM minimizes the\
  \ Cram\xE9r-von Mises test statistic between synthetic and real datasets by aligning\
  \ synthetic samples with optimal quantile points."
---

# Dataset Condensation with Latent Quantile Matching

## Quick Facts
- arXiv ID: 2406.09860
- Source URL: https://arxiv.org/abs/2406.09860
- Authors: Wei Wei; Tom De Schepper; Kevin Mets
- Reference count: 40
- Primary result: LQM improves accuracy by 1-2% compared to MMD on CIFAR datasets and shows 1-2% better average accuracy in continual graph learning settings

## Executive Summary
This paper introduces Latent Quantile Matching (LQM), a novel approach to dataset condensation that addresses limitations of Maximum Mean Discrepancy (MMD) by matching latent distribution quantiles instead of means. The method minimizes the Cramér-von Mises test statistic between synthetic and real datasets by aligning synthetic samples with optimal quantile points. LQM is applied to both image datasets (CIFAR-10, CIFAR-100, TinyImageNet) and graph datasets (CoraFull, Arxiv, Reddit, Product), demonstrating improved accuracy and reduced outlier values in latent spaces compared to MMD-based approaches.

## Method Summary
The paper proposes replacing MMD with Latent Quantile Matching (LQM) for dataset condensation tasks. LQM minimizes the Cramér-von Mises test statistic by aligning synthetic samples with optimal quantile points in latent space, providing a more robust distribution matching approach than mean-based MMD. The method is implemented on top of existing DC frameworks IDM (for images) and CaT (for graphs), replacing their MMD loss functions with the LQM quantile computation. The approach shows consistent improvements across multiple datasets and settings, particularly in continual learning scenarios where synthetic dataset size is limited.

## Key Results
- LQM improves accuracy by 1-2% compared to MMD on CIFAR-10 and CIFAR-100 image datasets
- In continual graph learning settings, LQM achieves 1-2% better average accuracy with more pronounced improvements when synthetic datasets are smaller
- LQM demonstrates reduced outlier values in latent spaces compared to MMD-based approaches

## Why This Works (Mechanism)
LQM addresses the fundamental limitation of MMD, which matches only the mean of distributions and can fail to capture higher-order moments and tail behaviors. By matching quantiles instead of means, LQM better captures the full distribution structure, including outliers and extreme values. The Cramér-von Mises test statistic provides a principled way to measure distribution differences based on quantile alignment, leading to more informative synthetic datasets that better preserve the original data's characteristics.

## Foundational Learning

**Distribution Matching Theory**: Understanding how to compare probability distributions is essential for dataset condensation. Quick check: Verify that MMD computes the squared difference between means in reproducing kernel Hilbert space.

**Quantile Function Computation**: Quantiles divide distributions into equal probability intervals. Why needed: LQM requires efficient computation of optimal quantile points for discrete samples. Quick check: Confirm that the k-point discrete approximation converges to true quantiles as k increases.

**Cramér-von Mises Test**: This statistical test measures the difference between empirical distribution functions. Why needed: LQM uses this test statistic as its optimization objective. Quick check: Verify that the test statistic is zero when distributions are identical.

## Architecture Onboarding

**Component Map**: Original Dataset -> Feature Extractor -> Latent Space -> LQM Loss -> Synthetic Dataset Generator -> Condensed Dataset

**Critical Path**: The optimization loop where synthetic samples are iteratively updated to minimize the Cramér-von Mises statistic between synthetic and real latent distributions represents the core computational path.

**Design Tradeoffs**: LQM provides better distribution matching than MMD but requires more computational overhead for quantile computation. The method trades increased training time for improved synthetic dataset quality and downstream performance.

**Failure Signatures**: Poor Cramér-von Mises statistics between synthetic and real datasets indicate failure to capture the true distribution. Large outlier values in latent space suggest the synthetic data doesn't adequately represent the original distribution's tails.

**Three First Experiments**:
1. Implement and verify the quantile computation function using the optimal k-point discrete approximation
2. Compare Cramér-von Mises statistics between LQM and MMD on a small synthetic dataset
3. Measure the impact of different numbers of quantile points (k) on distribution matching quality

## Open Questions the Paper Calls Out
None

## Limitations
- The exact pretrained model used for latent feature extraction is not specified, which may affect reproducibility
- The initialization scheme for synthetic datasets and specific random seeds used during training are not provided
- The generalizability of LQM to completely different domain types beyond images and graphs is not established

## Confidence

**High Confidence**: The core theoretical contribution of replacing MMD with quantile-based distribution matching is well-founded and the mathematical formulation is clearly presented.

**Medium Confidence**: The empirical results showing 1-2% accuracy improvements are reproducible in principle, but exact replication may vary due to unspecified implementation details.

**Low Confidence**: The generalizability of LQM to completely different domain types beyond images and graphs is not established.

## Next Checks

1. **Replicate the Latent Feature Extraction Setup**: Implement the exact pretrained model architecture and training procedure used to extract latent features for Cramér-von Mises computation. Verify that the synthetic-real distribution alignment produces the reported reduction in outlier values compared to MMD baseline.

2. **Ablation Study on Quantile Points**: Systematically vary the number of quantile points (k) used in the discrete approximation and measure the impact on both Cramér-von Mises statistics and downstream classification accuracy. This would validate the sensitivity of LQM to this hyperparameter.

3. **Cross-Domain Generalization Test**: Apply LQM to a third, distinct data modality (such as tabular data or time series) using the same methodology. Compare the performance against MMD and evaluate whether the quantile matching approach provides consistent benefits across diverse data types.