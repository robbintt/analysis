---
ver: rpa2
title: 'DFGNN: Dual-frequency Graph Neural Network for Sign-aware Feedback'
arxiv_id: '2405.15280'
source_url: https://arxiv.org/abs/2405.15280
tags:
- graph
- feedback
- negative
- recommendation
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel Dual-frequency Graph Neural Network
  (DFGNN) to address the challenge of modeling negative feedback in graph-based recommendation
  systems. The key idea is to treat positive and negative feedback as low-frequency
  and high-frequency signals, respectively, and design a dual-frequency graph filter
  (DGF) to capture both types of signals.
---

# DFGNN: Dual-frequency Graph Neural Network for Sign-aware Feedback

## Quick Facts
- arXiv ID: 2405.15280
- Source URL: https://arxiv.org/abs/2405.15280
- Authors: Yiqing Wu; Ruobing Xie; Zhao Zhang; Xu Zhang; Fuzhen Zhuang; Leyu Lin; Zhanhui Kang; Yongjun Xu
- Reference count: 40
- Primary result: Proposed DFGNN achieves 3.31% improvement in MRR and 2.33% improvement in HIT@10 on ML1M dataset

## Executive Summary
This paper introduces a novel Dual-frequency Graph Neural Network (DFGNN) to address the challenge of modeling negative feedback in graph-based recommendation systems. The key innovation lies in treating positive and negative feedback as distinct frequency signals - low-frequency for positive feedback and high-frequency for negative feedback. The model employs a dual-frequency graph filter consisting of low-passing and high-passing components, complemented by a signed graph regularization loss to prevent representation degeneration. Experimental results demonstrate significant improvements over state-of-the-art baselines on real-world datasets.

## Method Summary
The DFGNN framework is built on the observation that positive and negative feedback in recommendation systems can be effectively modeled as low-frequency and high-frequency signals, respectively. The core architecture comprises two main components: a low-passing graph filter (LGF) that captures positive feedback patterns, and a high-passing graph filter (HGF) that extracts negative feedback signals. These filters are designed to operate on the graph structure of user-item interactions, with the LGF focusing on smooth, densely connected regions while the HGF targets sharp transitions and sparse connections. The model incorporates signed graph regularization to address the representation degeneration problem commonly observed in graph neural networks. The overall training process optimizes both the dual-frequency filtering objectives and the regularization term to produce embeddings that effectively capture both types of feedback.

## Key Results
- Achieved 3.31% improvement in MRR on ML1M dataset for recommendation task
- Attained 2.33% improvement in HIT@10 on ML1M dataset
- Demonstrated superior performance over state-of-the-art baselines in both recommendation ranking and feedback type recognition tasks

## Why This Works (Mechanism)
The dual-frequency approach works because it explicitly models the complementary nature of positive and negative feedback signals in recommendation systems. Positive feedback typically exhibits smooth, densely connected patterns in the user-item graph, corresponding to low-frequency signals that can be effectively captured by graph convolutions. In contrast, negative feedback represents sparse, sharp transitions that manifest as high-frequency signals requiring specialized filtering. By separating these signals through dedicated filters, DFGNN can more precisely model the complex relationships in user behavior. The signed graph regularization addresses the inherent tendency of graph neural networks to produce similar embeddings for connected nodes, which is problematic when negative feedback needs to be distinguished. This mechanism ensures that the model can effectively leverage both types of feedback without one dominating the representation learning process.

## Foundational Learning
**Graph Neural Networks**: Neural networks that operate on graph-structured data by aggregating information from neighboring nodes. Needed to model the user-item interaction graph. Quick check: Verify that message passing updates properly aggregate neighborhood information.

**Graph Filtering**: Techniques for separating signals based on frequency components in graph domains. Essential for distinguishing between positive and negative feedback patterns. Quick check: Confirm that filter responses match expected frequency characteristics.

**Frequency Analysis in Graphs**: Understanding how graph signals can be decomposed into different frequency components based on graph Laplacian spectrum. Required to justify the dual-frequency separation approach. Quick check: Validate that the graph Laplacian captures the relevant structural properties.

**Signed Graph Embeddings**: Methods for learning node representations that preserve both positive and negative relationships. Needed to prevent representation collapse in the presence of mixed feedback. Quick check: Ensure that positive and negative connections are properly distinguished in the embedding space.

**Recommendation with Negative Feedback**: Approaches that explicitly incorporate negative interactions (skips, dislikes) rather than just implicit positive feedback. Critical for building more accurate recommendation models. Quick check: Verify that negative feedback is properly weighted in the loss function.

## Architecture Onboarding

**Component Map**: User-Item Graph -> Dual-Frequency Graph Filter (LGF + HGF) -> Embedding Layer -> Prediction Layer

**Critical Path**: Input graph -> Low-passing filter (captures positive feedback) -> High-passing filter (captures negative feedback) -> Concatenation/Combination -> Embedding generation -> Prediction

**Design Tradeoffs**: The dual-filter approach increases model complexity and computational cost compared to single-filter architectures, but provides more nuanced modeling of feedback signals. The signed regularization adds training stability at the expense of additional hyperparameters. The separation of frequency bands requires careful tuning to ensure both filters contribute meaningfully to the final predictions.

**Failure Signatures**: Poor performance on datasets with predominantly positive feedback (where high-frequency signals are weak), overfitting when the graph structure is too sparse, and degraded performance if the frequency separation is not properly calibrated. The model may also struggle with extremely noisy feedback signals that don't cleanly separate into frequency bands.

**First Experiments**: 1) Test individual LGF and HGF components separately to verify their respective contributions. 2) Evaluate the impact of signed graph regularization by training with and without this component. 3) Conduct frequency response analysis on synthetic graphs with known signal properties to validate the filter behavior.

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Experimental validation is limited to a small set of real-world datasets, raising questions about generalizability to other domains
- Theoretical justification for the dual-frequency separation approach lacks comprehensive mathematical analysis
- Technical details about filter implementation and parameter optimization are insufficient for practical replication

## Confidence
- Claims about performance improvements: Medium (based on limited dataset scope)
- Theoretical foundation of dual-frequency approach: Medium (lacks rigorous analysis)
- Practical applicability of architecture: Low (insufficient technical detail provided)

## Next Checks
1. Conduct extensive experiments on diverse datasets from different domains to evaluate the generalizability of DFGNN's performance improvements
2. Perform ablation studies to isolate and quantify the contributions of the dual-frequency filtering approach versus simpler alternatives
3. Investigate the theoretical underpinnings through mathematical analysis or extensive empirical validation to establish why frequency separation is necessary and optimal for recommendation systems