---
ver: rpa2
title: 'A Multi-Task Text Classification Pipeline with Natural Language Explanations:
  A User-Centric Evaluation in Sentiment Analysis and Offensive Language Identification
  in Greek Tweets'
arxiv_id: '2410.10290'
source_url: https://arxiv.org/abs/2410.10290
tags:
- explanations
- language
- sentiment
- text
- explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present a two-stage pipeline for interpretable text
  classification that first predicts a label and then generates a natural language
  explanation. They combine a Greek-BERT classifier with a BART explanation generator,
  conditioning the latter on both the input text and the predicted label to produce
  faithful rationales.
---

# A Multi-Task Text Classification Pipeline with Natural Language Explanations: A User-Centric Evaluation in Sentiment Analysis and Offensive Language Identification in Greek Tweets

## Quick Facts
- arXiv ID: 2410.10290
- Source URL: https://arxiv.org/abs/2410.10290
- Reference count: 34
- Authors: Nikolaos Mylonas; Nikolaos Stylianou; Theodora Tsikrika; Stefanos Vrochidis; Ioannis Kompatsiaris
- Primary result: Two-stage pipeline for interpretable Greek text classification with high plausibility and coherence of generated explanations

## Executive Summary
This paper presents a pipeline for interpretable text classification that combines a Greek-BERT classifier with a BART explanation generator. The system first predicts a label for Greek tweets and then generates natural language explanations conditioned on both the input text and predicted label. Since ground truth explanations were unavailable for their Greek datasets, the authors use a Greek LLM (Meltemi) to synthesize training rationales. They evaluate the pipeline on sentiment analysis and offensive language detection tasks using a user study with three metrics: Plausibility, Coherence, and Perfidiousness. The results show strong classifier performance and high-quality explanations, especially when more training data are available.

## Method Summary
The authors propose a two-stage pipeline for interpretable text classification in Greek. First, a Greek-BERT model classifies input text into sentiment or offensive language categories. Second, a BART model generates natural language explanations conditioned on both the input text and predicted label. Since no ground truth explanations exist for Greek tweets, they use a Greek LLM to generate synthetic rationales for training the explanation generator. The pipeline is trained in two stages: the classifier is fine-tuned on the Greek datasets, then the explanation generator is fine-tuned on text-label pairs concatenated with LLM-generated explanations. The system is evaluated through user studies measuring Plausibility (how convincing explanations are), Coherence (how well explanations relate to input), and Perfidiousness (whether explanations favor incorrect labels).

## Key Results
- The pipeline achieves high Plausibility and reasonable Coherence scores for generated explanations
- Classifier performance remains strong with F1-score and Balanced Accuracy metrics
- Explanation quality improves with more training data availability
- The two-stage approach maintains classifier accuracy while enabling interpretable explanations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The pipeline produces high plausibility explanations because the explanation generator is trained with conditioned inputs that include both the text and the predicted label.
- Mechanism: By concatenating the input text with its predicted label ("{input text} has {label} label"), the explanation generator learns to condition its output on the label context, which improves faithfulness to the predicted class.
- Core assumption: Conditioning the explanation generator on label information during training helps it generate explanations that are aligned with the predicted label rather than generic or misleading rationales.
- Evidence anchors:
  - [abstract]: "We create a new composite text by integrating the information of the input along with its label in the following fashion: '{input text} has {label} label'."
  - [section]: "The second model (explanation generator), explains in natural language the predictions of the previous model. The input of this model should contain information about the provided input text along with its corresponding label."
- Break condition: If the conditioning string format is inconsistent or too ambiguous, the generator may fail to learn the intended label-to-explanation mapping.

### Mechanism 2
- Claim: The pipeline maintains strong classifier performance because the classifier and explanation generator are trained independently.
- Mechanism: Training the classifier separately from the explanation generator prevents the explanation task from interfering with the classification objective, avoiding degradation in classifier accuracy.
- Core assumption: Multi-task models that combine classification and explanation in one architecture can suffer performance drops due to conflicting gradients or objectives.
- Evidence anchors:
  - [abstract]: "the performance of the model in the downstream task of text classification does not deteriorate."
  - [section]: "Unlike a single multi-task model that generates both labels and explanations simultaneously, our proposed pipeline allows for greater versatility by enabling the use of distinct models for each task, with independent performance, facilitating easier optimisation."
- Break condition: If the classifier becomes significantly less accurate due to training data shifts or if the explanation generator starts relying on classifier errors.

### Mechanism 3
- Claim: The use of a Greek LLM for rationale generation enables training the explanation generator even when human rationales are unavailable.
- Mechanism: The LLM is prompted to generate explanations for each training instance, providing synthetic rationales that the explanation generator can learn from, thus bypassing the need for expensive human annotation.
- Core assumption: LLM-generated explanations, while not perfect, are sufficiently coherent and plausible to serve as training targets for the explanation generator.
- Evidence anchors:
  - [abstract]: "we propose a way of obtaining machine-generated explanations that can be used for training the explanation generator."
  - [section]: "we exploit a generative LLM through a custom created prompts sequence. This approach, even though not ideal, can help us deal with scenarios where ground truth rationales are absent."
- Break condition: If LLM-generated explanations are too generic or inconsistent, the explanation generator may learn to produce low-quality or misleading rationales.

## Foundational Learning

- Concept: Text classification with transformer models
  - Why needed here: The classifier (Greek-BERT) must learn to map Greek tweets to sentiment or offensive labels accurately.
  - Quick check question: What is the difference between fine-tuning and pre-training a BERT model?

- Concept: Sequence-to-sequence generation with transformer models
  - Why needed here: The explanation generator (BART) must generate natural language explanations conditioned on input text and label.
  - Quick check question: How does BART differ from BERT in terms of architecture and task suitability?

- Concept: Prompt engineering for LLM-based rationale generation
  - Why needed here: The prompts must be carefully designed to elicit consistent and relevant explanations from the Greek LLM.
  - Quick check question: What are the risks of using a single prompt template for all instances in a dataset?

## Architecture Onboarding

- Component map:
  Input: Greek tweet text -> Greek-BERT classifier -> predicted label -> BART explanation generator -> natural language explanation

- Critical path:
  1. Input text → Greek-BERT → predicted label
  2. Concatenate text + label → BART → explanation

- Design tradeoffs:
  - Independent training allows strong classifier performance but requires careful alignment of label conditioning.
  - Using synthetic rationales reduces cost but may introduce noise into the explanation generator training.
  - Greek-specific models limit generalizability but improve performance on Greek language tasks.

- Failure signatures:
  - Explanations that contradict the predicted label → poor conditioning or training data noise.
  - Explanations that are grammatically incoherent → BART underfitting or poor prompt quality.
  - Classifier accuracy drop after pipeline integration → label leakage or data distribution shift.

- First 3 experiments:
  1. Train classifier alone on the Greek dataset and evaluate F1/Balanced Accuracy.
  2. Generate synthetic rationales with Greek LLM and train explanation generator on a small subset.
  3. Run end-to-end pipeline on test set and perform a mini user study on 10 examples to assess Plausibility and Coherence.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of explanations change when using ground truth rationales versus LLM-generated rationales?
- Basis in paper: [explicit] The paper states that explanations are generated by a Greek LLM (Meltemi) due to the lack of ground truth rationales, and mentions that the quality of explanations increases when more training data are available.
- Why unresolved: The paper does not directly compare the quality of explanations generated by the LLM against human-annotated rationales, which would provide a clearer measure of the LLM's effectiveness.
- What evidence would resolve it: A direct comparison of explanation quality (using Plausibility, Coherence, and Perfidiousness metrics) between LLM-generated and human-annotated rationales on the same dataset.

### Open Question 2
- Question: How does the proposed two-stage pipeline compare to a single multi-task model in terms of prediction accuracy and explanation quality?
- Basis in paper: [explicit] The paper mentions that the two-stage pipeline allows for greater versatility and independent performance of each model, and suggests comparing it with a single self-rationalizing model in future work.
- Why unresolved: The paper does not provide experimental results comparing the two-stage pipeline with a single multi-task model.
- What evidence would resolve it: Experimental results showing the performance of both approaches on the same datasets, using metrics like Balanced Accuracy, F1-Score, and the three explanation metrics.

### Open Question 3
- Question: How does the choice of explanation generator model (e.g., BART) affect the quality of generated explanations?
- Basis in paper: [explicit] The paper uses BART as the explanation generator but mentions testing different models for the explanation generator as a future research direction.
- Why unresolved: The paper does not explore the impact of using different explanation generator models on the quality of explanations.
- What evidence would resolve it: Experimental results comparing the performance of different explanation generator models (e.g., BART, T5, GPT) on the same datasets, using the same evaluation metrics.

## Limitations
- The pipeline's performance in low-resource scenarios is limited by the quality of LLM-generated rationales, which may not fully capture human reasoning patterns
- The user study metrics (Plausibility, Coherence, Perfidiousness) are novel and not yet standardized, making cross-study comparisons difficult
- The evaluation is limited to Greek tweets, restricting generalizability claims to other languages or domains

## Confidence
- High confidence: The two-stage pipeline architecture and training procedure are sound and well-documented
- Medium confidence: The classifier performance metrics are reliable, but explanation quality assessment depends heavily on user study design
- Low confidence: Claims about scalability to other languages or domains lack empirical validation beyond the Greek tweet context

## Next Checks

1. **Rationale Quality Validation**: Conduct a comparative study where human annotators rate both LLM-generated and human-written explanations on the same examples to quantify quality differences.

2. **Cross-Domain Testing**: Apply the pipeline to Greek text from different domains (e.g., news articles, product reviews) to assess robustness beyond Twitter data.

3. **Ablation Study on Conditioning**: Systematically test the impact of different label-conditioning strategies on explanation quality by comparing: no conditioning, different formatting approaches, and ground truth vs. predicted labels in the conditioning string.