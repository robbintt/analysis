---
ver: rpa2
title: Investigating Bias Representations in Llama 2 Chat via Activation Steering
arxiv_id: '2402.00402'
source_url: https://arxiv.org/abs/2402.00402
tags:
- bias
- steering
- gender
- figure
- vectors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates societal biases in Llama 2 7B Chat using
  activation steering. The authors employ Contrastive Activation Addition to probe
  for gender, race, and religion biases by constructing steering vectors from the
  StereoSet dataset and GPT-4-generated prompts.
---

# Investigating Bias Representations in Llama 2 Chat via Activation Steering

## Quick Facts
- arXiv ID: 2402.00402
- Source URL: https://arxiv.org/abs/2402.00402
- Authors: Dawn Lu; Nina Rimsky
- Reference count: 9
- One-line primary result: Gender bias persists in Llama 2 Chat post-RLHF, with RLHF increasing similarity between different bias representations

## Executive Summary
This study investigates societal biases in Llama 2 7B Chat using activation steering through Contrastive Activation Addition. The authors construct steering vectors from the StereoSet dataset and GPT-4-generated prompts to probe for gender, race, and religion biases. Key findings include persistent gender bias despite RLHF, a negative correlation between bias and refusal responses, and increased similarity in bias representations post-RLHF. The study demonstrates that racial bias steering vectors can elicit gender and religion biases, suggesting transferability across bias dimensions. These insights highlight the importance of integrating refusal vectors in red-teaming strategies and demonstrate the potential of activation steering as a diagnostic tool for evaluating LLM robustness against societal biases.

## Method Summary
The paper employs Contrastive Activation Addition to construct steering vectors by taking the average difference in residual stream activations between paired stereotype and anti-stereotype prompts from the StereoSet dataset. These vectors are added to the model's activations during the forward pass at specific layers to manipulate responses towards or away from biased outputs. The authors evaluate bias elicitation effectiveness using custom GPT-4 generated prompts and analyze cosine similarities between different bias vectors. The study compares the base model with the RLHF fine-tuned version to understand how fine-tuning affects bias representations.

## Key Results
- Gender bias persists in Llama 2 7B Chat despite RLHF, while racial and religious biases are largely refused
- RLHF increases similarity between different bias representations, with gender and racial bias vectors showing ~0.8 cosine similarity in the Chat model
- Racial bias steering vectors effectively elicit gender and religion biases, demonstrating cross-dimensional transferability
- A negative correlation exists between bias and refusal responses, suggesting interference between these mechanisms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Activation steering vectors can effectively elicit biased responses by manipulating model activations.
- Mechanism: The method constructs steering vectors by taking the average difference in residual stream activations between paired stereotype and anti-stereotype prompts. By adding these vectors to the model's activations during the forward pass, the model's responses are steered towards biased outputs.
- Core assumption: The difference in activations between paired prompts captures the "bias direction" in the model's latent space, allowing for targeted manipulation of responses.
- Evidence anchors:
  - [abstract] "This method manipulates model activations to direct responses towards or away from biased outputs, utilizing steering vectors derived from the StereoSet dataset..."
  - [section] "We perform this perturbation by adding a steering vector to the residual stream at a specific layer at every token position after an initial prompt. The steering vector is constructed by taking the average difference in residual stream activations between pairs of biased (stereotype) and unbiased (anti-stereotype) prompts at that layer."
  - [corpus] Weak evidence; neighboring papers discuss activation steering for bias mitigation but do not provide specific evidence for this mechanism.

### Mechanism 2
- Claim: There is a negative correlation between bias and refusal responses in the model.
- Mechanism: When attempting to steer the model towards biased responses, the model's refusal guardrails are triggered, resulting in refusal responses. This suggests that the model's internal representations of bias and refusal are negatively correlated.
- Core assumption: The model's refusal behavior is a learned response to detect and avoid potentially harmful or biased outputs.
- Evidence anchors:
  - [abstract] "We also observe a predictably negative correlation between bias and the model's tendency to refuse responses."
  - [section] "To obtain a baseline, we first sampled responses from Llama 2 7B Chat without any steering. We made the following observations: The model simply refuses to respond to the race and religion prompts."
  - [corpus] Weak evidence; neighboring papers discuss bias and refusal but do not provide specific evidence for this correlation.

### Mechanism 3
- Claim: RLHF increases the similarity in the model's representation of different forms of societal biases.
- Mechanism: After RLHF fine-tuning, the cosine similarity between bias steering vectors for different bias dimensions (e.g., gender, race, religion) increases compared to the base model. This suggests that RLHF causes the model to more broadly categorize different forms of societal biases.
- Core assumption: RLHF training signals cause the model to learn a more generalized representation of bias, leading to increased similarity between different bias dimensions.
- Evidence anchors:
  - [abstract] "Significantly, our study uncovers that RLHF tends to increase the similarity in the model's representation of different forms of societal biases..."
  - [section] "Interestingly, we found a very high correlation (~0.8) between gender bias and racial bias in the Chat model. This result is especially pronounced when contrasted with the respective cosine similarity of the bias vectors in the base model."
  - [corpus] Weak evidence; neighboring papers discuss RLHF but do not provide specific evidence for this mechanism.

## Foundational Learning

- Concept: Contrastive Activation Addition
  - Why needed here: This technique is the core method used to construct steering vectors by taking the average difference in activations between paired stereotype and anti-stereotype prompts.
  - Quick check question: How are steering vectors constructed using Contrastive Activation Addition?

- Concept: Residual Stream Activations
  - Why needed here: The steering vectors are added to the residual stream at specific layers during the forward pass to manipulate the model's responses.
  - Quick check question: At which point in the model's forward pass are the steering vectors added?

- Concept: Cosine Similarity
  - Why needed here: Cosine similarity is used to measure the relationship between different bias steering vectors and the refusal steering vector, as well as the similarity between bias vectors across different dimensions.
  - Quick check question: What does a high cosine similarity between two bias vectors indicate about the model's internal representation of those biases?

## Architecture Onboarding

- Component map: Llama 2 7B Chat model -> StereoSet dataset -> GPT-4 generated prompts -> Contrastive Activation Addition method -> Steering vectors -> Response evaluation
- Critical path: Construct steering vectors from StereoSet and GPT-4 prompts → Add steering vectors to residual stream at specific layers → Evaluate responses for bias elicitation → Analyze cosine similarities between bias vectors
- Design tradeoffs: The use of Contrastive Activation Addition allows for interpretable bias mitigation but may be limited by the quality and coverage of the prompt dataset. The method's effectiveness also depends on the model's internal representation of bias.
- Failure signatures: If the steering vectors do not effectively elicit biased responses, it may indicate that the vectors do not capture the true bias direction in the model's latent space. If the negative correlation between bias and refusal is not observed, it may suggest that the model's refusal behavior is not primarily driven by bias detection.
- First 3 experiments:
  1. Construct steering vectors using Contrastive Activation Addition from the StereoSet dataset and evaluate their effectiveness in eliciting biased responses.
  2. Analyze the cosine similarity between bias steering vectors and the refusal steering vector to confirm the negative correlation between bias and refusal.
  3. Compare the cosine similarity between bias vectors in the base model and the RLHF model to assess the impact of RLHF on the model's representation of different forms of societal biases.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of bias steering vectors vary across different model sizes and architectures?
- Basis in paper: [inferred] The paper focuses on Llama 2 7B Chat and mentions transferability of bias steering vectors. It would be valuable to understand if these findings generalize to other model sizes and architectures.
- Why unresolved: The study only examines Llama 2 7B Chat. Without testing other model sizes and architectures, it's unclear if the observed bias representations and transferability patterns are consistent across different models.
- What evidence would resolve it: Conducting similar experiments on various model sizes (e.g., Llama 2 13B, 70B) and different architectures (e.g., GPT-3, BERT) to compare the effectiveness of bias steering vectors and their transferability patterns.

### Open Question 2
- Question: What specific mechanisms in RLHF lead to increased similarity in bias representations across different dimensions?
- Basis in paper: [explicit] The paper observes that RLHF increases similarity in the model's representation of different forms of societal biases, but does not explain the underlying mechanisms.
- Why unresolved: While the paper identifies the phenomenon, it does not delve into the specific aspects of RLHF that cause this increased similarity in bias representations.
- What evidence would resolve it: Analyzing the RLHF process in detail, including the specific training data and human feedback used, to identify which aspects contribute to the convergence of bias representations across dimensions.

### Open Question 3
- Question: How does the model's understanding of nuanced bias concepts change as it undergoes RLHF?
- Basis in paper: [explicit] The paper raises questions about the model's nuanced understanding of different forms of bias after RLHF, suggesting a potential loss of distinction between bias types.
- Why unresolved: The paper observes a change in bias representation but does not investigate how this affects the model's ability to distinguish between subtle variations within each bias dimension.
- What evidence would resolve it: Designing tests that probe the model's ability to differentiate between nuanced concepts within each bias dimension (e.g., subtle gender stereotypes) before and after RLHF, comparing the results to understand the impact on nuanced understanding.

### Open Question 4
- Question: How can the findings on bias transferability be used to develop more effective bias mitigation strategies?
- Basis in paper: [explicit] The paper demonstrates that steering vectors from one bias dimension (e.g., race) can elicit biases in other dimensions (e.g., gender, religion), suggesting a potential interconnectedness in bias representations.
- Why unresolved: While the paper identifies this transferability, it does not explore how this knowledge can be leveraged to create more comprehensive bias mitigation approaches.
- What evidence would resolve it: Developing and testing bias mitigation strategies that target multiple bias dimensions simultaneously, using insights from the transferability patterns observed in the study.

## Limitations

- The study's findings are limited to a single model (Llama 2 7B Chat) and may not generalize to other architectures or model sizes
- Reliance on a single bias dataset (StereoSet) and GPT-4 for prompt generation may introduce coverage gaps and potential bias amplification
- The activation steering technique's effectiveness depends on specific layer selection and normalization procedures, which are not fully detailed

## Confidence

**High Confidence Claims:**
- The effectiveness of activation steering vectors in eliciting biased responses from Llama 2 7B Chat is well-supported by the experimental results.
- The negative correlation between bias and refusal responses is consistently observed across multiple experiments.

**Medium Confidence Claims:**
- The finding that RLHF increases similarity between different bias representations is supported but could benefit from additional architectural analysis.
- The transferability of racial bias vectors to elicit gender and religion biases is observed but may be model-specific.

**Low Confidence Claims:**
- The exact mechanism by which RLHF causes broader bias categorization remains speculative without further architectural investigation.
- Claims about the nuanced understanding of bias post-RLHF are interpretive rather than directly measured.

## Next Checks

1. **Cross-Model Validation**: Test the activation steering approach on multiple LLM architectures (GPT-3.5, Claude, Mistral) to determine if the observed bias patterns and steering effectiveness are consistent across models, or specific to Llama 2.

2. **Layer Sensitivity Analysis**: Systematically vary the steering vector application layer (not just layers 10-30) and measure how bias elicitation effectiveness changes, identifying whether certain transformer layers are more critical for bias representation.

3. **Bias Dataset Expansion**: Repeat the experiments using multiple bias datasets (CrowS-Pairs, Winogender, StereoSet) to verify that findings are not artifacts of a single dataset's construction or bias types, and to assess dataset-specific effects on steering vector construction.