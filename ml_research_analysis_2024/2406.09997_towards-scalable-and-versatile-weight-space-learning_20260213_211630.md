---
ver: rpa2
title: Towards Scalable and Versatile Weight Space Learning
arxiv_id: '2406.09997'
source_url: https://arxiv.org/abs/2406.09997
tags:
- sane
- learning
- sampling
- neural
- weights
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SANE, a novel approach to weight-space learning
  that addresses the limitations of previous methods by enabling scalable and task-agnostic
  representations of neural networks. SANE overcomes the constraints of fixed model
  sizes and task-specific learning by decomposing neural network weights into sequential
  token vectors, allowing for the embedding of larger and architecturally diverse
  models.
---

# Towards Scalable and Versatile Weight Space Learning

## Quick Facts
- arXiv ID: 2406.09997
- Source URL: https://arxiv.org/abs/2406.09997
- Authors: Konstantin Schürholt; Michael W. Mahoney; Damian Borth
- Reference count: 39
- One-line primary result: SANE achieves 25% improvement in accuracy for initialization on same task and 17% improvement for fine-tuning to new tasks on small CNN models, with even larger gains (31% and 28%) on ResNet architectures.

## Executive Summary
This paper introduces SANE (Sequential Autoencoder for Neural Embeddings), a novel approach to weight-space learning that overcomes limitations of previous methods by enabling scalable and task-agnostic representations of neural networks. SANE achieves this by decomposing neural network weights into sequential token vectors, allowing for the embedding of larger and architecturally diverse models. The method extends hyper-representation learning by processing subsets of neural network weights sequentially, preserving global model information in layer-wise embeddings. SANE demonstrates competitive or superior performance compared to state-of-the-art methods on various weight representation learning benchmarks, particularly excelling in initialization for new tasks and larger ResNet architectures.

## Method Summary
SANE extends hyper-representation learning by processing subsets of neural network weights sequentially rather than embedding entire flattened weight vectors at once. The method tokenizes neural network weights into manageable subsets, processes them through a transformer-based encoder to produce embeddings, and uses these embeddings for downstream tasks like property prediction and model sampling. The approach is trained on model zoos containing populations of trained neural networks, with the learned representations being task-agnostic and scalable to larger models of varying architectures. During inference, SANE can sample targeted models for new tasks and architectures with very few prompt examples by using Kernel Density Estimation (KDE) to model the distribution in latent space.

## Key Results
- Achieves 25% improvement in accuracy for initialization on the same task and 17% improvement in accuracy for fine-tuning to new tasks on small CNN models
- Shows 31% improvement in initialization accuracy and 28% improvement in fine-tuning accuracy for larger ResNet models
- Demonstrates competitive or superior performance compared to state-of-the-art methods on weight representation learning benchmarks
- Enables scalable embedding of larger neural networks through sequential processing of weight subsets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sequential decomposition of neural network weights into token vectors allows scaling to larger models while preserving global model information.
- Mechanism: By decomposing the entire weight vector into layers or smaller subsets and processing them sequentially, SANE can embed larger neural networks as a set of tokens into the learned representation space. This overcomes the limitation of previous methods that had to embed the entire flattened weight vector at once.
- Core assumption: Global model information is preserved in the layer-wise components of neural networks.
- Evidence anchors:
  - [abstract] "Our method extends the idea of hyper-representations towards sequential processing of subsets of neural network weights, thus allowing one to embed larger neural networks as a set of tokens into the learned representation space."
  - [section] "The change from processing the entire flattened weight vector to subsets of weights is motivated by Martin & Mahoney (2019a; 2021), who showed that global model information is preserved in the layer-wise components of NNs."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.44, average citations=0.0." (Weak corpus evidence, only indicates related work exists but not specific to this mechanism)
- Break condition: If global model information is not preserved in layer-wise components, the sequential decomposition would lose critical information and fail to represent the model accurately.

### Mechanism 2
- Claim: SANE learns task-agnostic representations of neural networks that can be used for both discriminative and generative downstream tasks.
- Mechanism: SANE extends hyper-representation learning by processing subsets of neural network weights sequentially, preserving global model information in layer-wise embeddings. This allows the learned representations to be used for various tasks without being task-specific.
- Core assumption: Representations learned from neural network weights contain information relevant to both discriminative and generative tasks.
- Evidence anchors:
  - [abstract] "SANE overcomes previous limitations by learning task-agnostic representations of neural networks that are scalable to larger models of varying architectures and that show capabilities beyond a single task."
  - [section] "To evaluate SANE on both discriminative and generative downstream tasks. For discriminate tasks, we evaluate on six model zoos by linear-probing for properties of the underlying NN models. For generative tasks, we evaluate on seven model zoos by sampling targeted model weights for initialization and transfer learning."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.44, average citations=0.0." (Weak corpus evidence, only indicates related work exists but not specific to this mechanism)
- Break condition: If the representations do not contain information relevant to both discriminative and generative tasks, the method would fail to perform well on one or both types of tasks.

### Mechanism 3
- Claim: SANE can sample targeted models for new tasks and architectures with very few prompt examples.
- Mechanism: SANE uses a sampling method that requires only a rough estimate of the distribution in latent space, which can be refined using the signal from sampled models. This reduces the requirements on prompt examples and allows targeting new model distributions.
- Core assumption: A rough estimate of the distribution in latent space, combined with refinement using sampled models, is sufficient to generate targeted models for new tasks and architectures.
- Evidence anchors:
  - [abstract] "Further, they are not bound to the distribution of prompt examples, but instead they can find the distribution that best satisfies the target performance metric, independent of the prompt examples."
  - [section] "Using E prompt examples We we compute the token sequence Te and corresponding embedding sequence ze = gθ(Te, P). Following previous work, we model the distribution P with a Kernel Density Estimation (KDE) per token as Pe∈E(ze n). We then draw k new token samples as: zk n ∼ P e∈E(ze n)."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.44, average citations=0.0." (Weak corpus evidence, only indicates related work exists but not specific to this mechanism)
- Break condition: If the rough estimate of the distribution in latent space is not sufficient to generate targeted models, or if the refinement process does not improve the sampled models, the method would fail to sample effectively for new tasks and architectures.

## Foundational Learning

- Concept: Sequential processing of subsets of neural network weights
  - Why needed here: This is the core mechanism that allows SANE to scale to larger models and handle varying architectures.
  - Quick check question: How does sequential processing of weight subsets differ from processing the entire weight vector at once, and what are the benefits?

- Concept: Tokenization of neural network weights
  - Why needed here: Tokenization is necessary to decompose the weight vector into manageable subsets for sequential processing.
  - Quick check question: What is the process for tokenizing neural network weights, and how does it ensure that global model information is preserved?

- Concept: Kernel Density Estimation (KDE) for sampling
  - Why needed here: KDE is used to model the distribution in latent space for sampling targeted models with few prompt examples.
  - Quick check question: How does KDE work in the context of SANE, and why is it effective for sampling with few prompt examples?

## Architecture Onboarding

- Component map:
  - Tokenizer: Reshapes and slices weight matrices into token vectors
  - Encoder: Transformer-based model that processes token sequences to produce embeddings
  - Decoder: Transformer-based model that reconstructs weight tokens from embeddings
  - Sampling module: Uses KDE and refinement to generate new model weights
  - Alignment module: Ensures consistent basis across models for easier learning

- Critical path:
  1. Tokenize input weights
  2. Process tokens sequentially through encoder
  3. Aggregate embeddings (e.g., using center of gravity)
  4. For sampling: use decoder and refinement to generate new weights

- Design tradeoffs:
  - Window size: Balances computational load and context information
  - Aggregation method: Center of gravity vs. other methods for combining token embeddings
  - Sampling strategy: Subsampling vs. bootstrapping for refining the distribution

- Failure signatures:
  - Poor reconstruction loss during training
  - Low correlation between embeddings and model properties
  - Sampled models perform no better than random initialization

- First 3 experiments:
  1. Train SANE on a small CNN model zoo and evaluate reconstruction loss
  2. Test property prediction on held-out models using learned embeddings
  3. Sample new models and evaluate their performance on the same task as the prompt examples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SANE scale when applied to even larger neural network architectures beyond ResNet-101, such as those used in state-of-the-art computer vision models?
- Basis in paper: [inferred] The paper discusses SANE's scalability up to ResNet-101 models and suggests that it is not fundamentally limited to that size, but does not provide empirical evidence for even larger models.
- Why unresolved: The paper does not include experiments or analysis on architectures larger than ResNet-101, leaving the question of scalability to larger models unanswered.
- What evidence would resolve it: Conducting experiments with SANE on architectures larger than ResNet-101, such as ResNet-152 or Vision Transformers, and comparing the performance and scalability to the results presented for ResNet-101 would provide the necessary evidence.

### Open Question 2
- Question: What is the impact of different tokenization strategies on the performance of SANE, and how do they affect the learned representations?
- Basis in paper: [explicit] The paper describes the tokenization process for neural network weights but does not explore alternative tokenization strategies or their impact on performance.
- Why unresolved: The paper focuses on a specific tokenization approach without investigating other possible methods or their effects on the quality of the learned representations.
- What evidence would resolve it: Experimenting with various tokenization strategies, such as different ways of splitting weight matrices or using alternative token sizes, and evaluating their impact on SANE's performance and the characteristics of the learned embeddings would provide insights into the importance of tokenization.

### Open Question 3
- Question: How does the choice of window size during pretraining affect the downstream task performance of SANE, and is there an optimal window size for different model sizes or tasks?
- Basis in paper: [explicit] The paper mentions that the window size is a critical parameter that balances computational load and context but does not provide a detailed analysis of its impact on downstream task performance.
- Why unresolved: The paper does not include a systematic study of how different window sizes during pretraining influence the quality of the learned representations and their effectiveness in downstream tasks.
- What evidence would resolve it: Conducting experiments with varying window sizes during pretraining and evaluating their impact on the performance of SANE in different downstream tasks would help determine the relationship between window size and task performance.

### Open Question 4
- Question: Can SANE be effectively applied to neural networks in domains other than computer vision, such as natural language processing or reinforcement learning, and how does its performance compare to domain-specific methods?
- Basis in paper: [explicit] The paper focuses on computer vision tasks and mentions that experiments are limited to this domain for simplicity, without exploring other domains.
- Why unresolved: The paper does not provide any experiments or analysis on the application of SANE to neural networks in domains outside of computer vision, leaving its generalizability and performance in other domains unknown.
- What evidence would resolve it: Applying SANE to neural networks in other domains, such as NLP or RL, and comparing its performance to existing domain-specific methods or baselines would demonstrate its effectiveness and generalizability beyond computer vision.

## Limitations
- Key implementation details underspecified, including exact transformer architecture parameters, window size selection methodology, and specific model alignment procedure
- Experimental results based on relatively small model zoos (20-100 models per zoo), raising questions about scalability to larger, more diverse populations
- Limited ablation studies for critical design choices like window size and aggregation methods

## Confidence

- **High Confidence:** The sequential decomposition mechanism for scaling to larger models is well-supported by theoretical motivation and experimental results showing improved performance on larger ResNet architectures.
- **Medium Confidence:** The task-agnostic nature of the representations is supported by evaluation across discriminative and generative tasks, though the breadth of tasks tested could be expanded.
- **Medium Confidence:** The sampling capability with few prompt examples shows promising results but would benefit from more extensive testing across diverse model architectures and tasks.

## Next Checks

1. **Scale Test:** Evaluate SANE on a significantly larger model zoo (500+ models) spanning multiple architectures to verify scalability claims and test the robustness of learned representations.

2. **Cross-Domain Generalization:** Test whether embeddings learned from one domain (e.g., vision) can effectively transfer to entirely different domains (e.g., language models) for property prediction and sampling.

3. **Architecture Transfer:** Systematically evaluate SANE's ability to sample effective models for architectures not present in the training zoo (e.g., sampling ResNet weights when trained only on CNNs) to validate true architectural versatility.