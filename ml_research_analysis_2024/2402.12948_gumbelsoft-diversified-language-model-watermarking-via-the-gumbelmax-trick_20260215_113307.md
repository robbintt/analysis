---
ver: rpa2
title: 'GumbelSoft: Diversified Language Model Watermarking via the GumbelMax-trick'
arxiv_id: '2402.12948'
source_url: https://arxiv.org/abs/2402.12948
tags:
- watermark
- text
- gumbelsoft
- logits-addition
- diversity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes GumbelSoft, a new text watermarking technique
  based on the GumbelMax-trick. The main limitation of existing GumbelMax-trick-based
  watermarking methods is their deterministic nature, leading to identical outputs
  for the same prompts.
---

# GumbelSoft: Diversified Language Model Watermarking via the GumbelMax-trick

## Quick Facts
- arXiv ID: 2402.12948
- Source URL: https://arxiv.org/abs/2402.12948
- Reference count: 40
- Key outcome: GumbelSoft achieves superior performance in diversity (Self-BLEU 0.828, Distinct-1 0.173, Distinct-2 0.377) and detectability (AUROC 0.983) compared to other watermarking methods while maintaining comparable quality (perplexity 7.86)

## Executive Summary
This paper addresses a fundamental limitation in LLM watermarking: the deterministic nature of GumbelMax-trick-based methods that produces identical outputs for the same prompts. GumbelSoft introduces randomness by replacing the argmax operation with softmax sampling, achieving significantly improved diversity in watermarked outputs while maintaining strong detectability. Experiments demonstrate that GumbelSoft outperforms existing watermarking approaches across multiple metrics, with particularly strong performance in high-entropy generation scenarios.

## Method Summary
GumbelSoft modifies the traditional GumbelMax-trick watermarking approach by replacing the deterministic argmax operation with softmax sampling parameterized by temperature τ. The method generates watermarked text through three components: a pseudo-random function that selects watermark tokens based on a key, a decoder that applies logits addition with Gumbel noise, and softmax sampling to introduce stochasticity in token selection. This creates diversified outputs while preserving the statistical properties needed for watermark detection through per-token scoring and z-test-based hypothesis testing.

## Key Results
- Achieves Self-BLEU of 0.828 (significant improvement over 0.989 for deterministic methods)
- Maintains strong detection performance with AUROC of 0.983
- Shows 76% AUROC retention against T5-span attacks, demonstrating robustness
- Preserves text quality with perplexity of 7.86, comparable to unwatermarked generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing argmax with softmax sampling introduces randomness that breaks deterministic output generation
- Mechanism: The GumbelSoft watermark replaces the deterministic argmax operation with softmax sampling parameterized by temperature τ, introducing stochasticity into token selection
- Core assumption: Sampling from softmax preserves the statistical detectability of the watermark while breaking deterministic patterns
- Evidence anchors:
  - [abstract] "replacing the argmax operation with softmax sampling, introducing randomness and improving diversity"
  - [section] "To mitigate this, we propose two strategies to introduce variability into the Decoder function and one strategy to the Pseudo-random function"
  - [corpus] No direct corpus evidence for this specific mechanism
- Break condition: If temperature τ is set too high, the sampling becomes too random and loses correlation with the watermark key, reducing detectability

### Mechanism 2
- Claim: The Logits-Addition watermark's detection mechanism is more effective than Exponential watermark's detection
- Mechanism: Logits-Addition watermark directly adds Gumbel noise to logits vector, with detection based on per-token scores st = ξt[wt], which shows stronger statistical separation between watermarked and unwatermarked texts
- Core assumption: Direct Gumbel noise addition provides better statistical properties than exponential transformation for detection
- Evidence anchors:
  - [abstract] "the softmax variant of the Logits-Addition watermark demonstrates superior performance in high diversity settings"
  - [section] "we introduce a new type of GM watermark that directly incorporates Gumbel noise into the logits vector for next-token sampling"
  - [corpus] No direct corpus evidence comparing detection mechanisms
- Break condition: If the language model's entropy is too low, the per-token score distribution may not provide sufficient separation for reliable detection

### Mechanism 3
- Claim: The variance in per-token scores increases detectability for watermarked texts
- Mechanism: Theorem 1 shows that for watermarked texts, tokens with low language model probability have significantly higher expected per-token scores, creating statistical separation
- Core assumption: The relationship between token probability and per-token score variance is strong enough to enable reliable detection
- Evidence anchors:
  - [section] "Theorem 1. Consider a text w1, . . . , wT embedded with a watermark using the Logits-Addition technique. When evaluated by the Logits-Addition watermark detector, the expected value and variance of the score for each token are given by..."
  - [section] "Increased detection token amounts also improve detectability, aligning with findings from Chakraborty et al. (2023)"
  - [corpus] No direct corpus evidence for variance-based detection
- Break condition: If attack methods successfully normalize token probabilities, the variance-based detection signal may be weakened

## Foundational Learning

- Concept: GumbelMax-trick and its unbiased sampling property
  - Why needed here: Understanding how the GumbelMax-trick enables unbiased watermarking is fundamental to the paper's approach
  - Quick check question: Why is the GumbelMax-trick mathematically equivalent to sampling directly from the categorical distribution?

- Concept: Statistical hypothesis testing for watermark detection
  - Why needed here: The detection mechanism relies on z-test principles to determine watermark presence
  - Quick check question: How does the z-test framework apply to comparing per-token score distributions between watermarked and unwatermarked texts?

- Concept: Temperature scaling in softmax sampling
  - Why needed here: Temperature τ controls the trade-off between diversity and detectability in the GumbelSoft approach
  - Quick check question: What happens to the token distribution when temperature τ approaches 0 versus infinity?

## Architecture Onboarding

- Component map: Context -> Pseudo-random Function -> Logits Addition -> Softmax Sampling -> Token Output -> Per-token Scoring -> Final Statistic Calculation
- Critical path: Context → Pseudo-random Function → Logits Addition → Softmax Sampling → Token Output → Per-token Scoring → Final Statistic Calculation
- Design tradeoffs: Temperature τ must balance diversity (higher τ) against detectability (lower τ)
- Failure signatures: 
  - AUROC drops below 0.8 indicates detection failure
  - Self-BLEU approaches 1.0 indicates lack of diversity
  - Perplexity increases significantly indicates quality degradation
- First 3 experiments:
  1. Run GumbelSoft with τ=0.3 on 20 high-entropy prompts, measure Self-BLEU and AUROC
  2. Compare GumbelSoft against vanilla GumbelMax-trick on identical prompts, measure output diversity
  3. Test robustness against T5-span attack by attacking 50% of tokens in generated text, measure AUROC retention

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the diversity of generated text vary across different tasks (e.g., completion, QA, code generation) when using GumbelSoft watermarking?
- Basis in paper: [inferred] The paper focuses on completion and QA tasks, but does not explore other tasks.
- Why unresolved: The paper does not provide experimental results or analysis for tasks beyond completion and QA.
- What evidence would resolve it: Conduct experiments with GumbelSoft watermarking on a variety of tasks (e.g., code generation, summarization, dialogue) and compare the diversity of generated text across these tasks.

### Open Question 2
- Question: How does the detectability of GumbelSoft watermarking compare to other watermarking techniques under different types of attacks (e.g., paraphrase, deletion, insertion)?
- Basis in paper: [explicit] The paper mentions that the T5-span attack is used to assess robustness, but does not explore other types of attacks.
- Why unresolved: The paper only evaluates the robustness of GumbelSoft watermarking against one specific attack (T5-span).
- What evidence would resolve it: Conduct experiments with GumbelSoft watermarking under various attack scenarios (e.g., paraphrase, deletion, insertion) and compare the detectability of the watermark against other techniques under these attacks.

### Open Question 3
- Question: What is the impact of the GumbelSoft watermarking technique on the quality of generated text, as measured by metrics beyond perplexity (e.g., human evaluation, downstream task performance)?
- Basis in paper: [explicit] The paper mentions that perplexity is used to assess the quality of generated text, but does not explore other quality metrics.
- Why unresolved: The paper only uses perplexity as a measure of text quality, which may not capture all aspects of quality.
- What evidence would resolve it: Conduct human evaluations or downstream task performance assessments of text generated with GumbelSoft watermarking and compare the results to text generated without watermarking or with other watermarking techniques.

## Limitations
- Trade-off between diversity and detectability: Softmax sampling improves diversity but slightly reduces detection performance (AUROC from 0.998 to 0.983)
- Attack robustness gaps: Limited evaluation against sophisticated watermark removal techniques beyond T5-span attacks
- Computational overhead: Multiple sampling iterations per token may increase inference latency

## Confidence

**High confidence**: The mechanism of replacing argmax with softmax sampling to improve diversity is well-supported by experimental results showing significant Self-BLEU reduction and maintained perplexity. The detection mechanism based on per-token score variance is mathematically sound per Theorem 1.

**Medium confidence**: The comparative advantage over Exponential watermark variants is demonstrated, but the paper lacks extensive ablation studies on temperature settings across different model architectures and entropy regimes.

**Low confidence**: The paper claims GumbelSoft can be applied to any autoregressive model without modification, but empirical validation is limited to Llama2-13B and GPT2-small. Cross-model generalization remains untested.

## Next Checks
1. **Temperature sensitivity analysis**: Systematically evaluate AUROC and Self-BLEU performance across τ ∈ [0.1, 1.0] on a diverse set of prompts (low, medium, and high entropy) to identify optimal operating points for different use cases.

2. **Cross-model generalization test**: Apply GumbelSoft to three additional model architectures (e.g., Mistral, Claude, and Gemma) and measure whether the diversity gains and detection performance transfer across different model families and sizes.

3. **Long-form generation evaluation**: Generate texts of 1000+ tokens under GumbelSoft and measure temporal correlation of watermark detection scores to assess whether the watermark remains detectable and diverse in extended generation scenarios.