---
ver: rpa2
title: 'Inference-Time Decontamination: Reusing Leaked Benchmarks for Large Language
  Model Evaluation'
arxiv_id: '2406.13990'
source_url: https://arxiv.org/abs/2406.13990
tags:
- data
- contamination
- mmlu
- evaluation
- gsm8k
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Inference-Time Decontamination (ITD) to mitigate
  evaluation inflation caused by memorization of leaked benchmarks in large language
  models. The method detects potentially contaminated samples and rewrites them while
  preserving the difficulty and knowledge points being tested.
---

# Inference-Time Decontamination: Reusing Leaked Benchmarks for Large Language Model Evaluation

## Quick Facts
- **arXiv ID**: 2406.13990
- **Source URL**: https://arxiv.org/abs/2406.13990
- **Reference count**: 40
- **Primary result**: Inference-Time Decontamination (ITD) reduces inflated accuracy by 22.9% on GSM8K and 19.0% on MMLU by detecting and rewriting memorized benchmark samples.

## Executive Summary
This paper introduces Inference-Time Decontamination (ITD), a framework to mitigate performance inflation in large language models caused by memorization of leaked evaluation benchmarks. ITD operates at inference time by detecting potentially contaminated samples using a probability-based method called MinKProb, then automatically rewriting them to break memorization while preserving the original difficulty and knowledge points being tested. The approach is validated on mathematical reasoning (GSM8K) and knowledge-based (MMLU) tasks, demonstrating significant reductions in inflated accuracy while maintaining the integrity of the evaluation.

## Method Summary
The ITD framework operates in three stages: detection, rewriting, and assurance. Detection uses MinKProb, which analyzes the average probability of the K lowest probability tokens in a sample - if this exceeds a threshold, the sample is flagged as potentially memorized. Rewriting is performed by GPT-4, with mathematical problems rewritten by changing contexts while preserving numbers and calculations, and knowledge problems paraphrased without altering core content. The assurance stage iteratively rewrites and re-detects samples until they pass contamination detection or reach maximum iterations. This process is designed to maintain the original difficulty and knowledge assessment while eliminating memorized responses.

## Key Results
- ITD reduces inflated accuracy by 22.9% on GSM8K and 19.0% on MMLU benchmarks
- When applied to Phi3 and Mistral models, ITD decreases accuracy by 6.7% and 3.6% respectively on MMLU
- The method successfully maintains difficulty and knowledge points while eliminating memorized responses
- ITD is effective across different model sizes and benchmark types, with performance improvements scaling with contamination levels

## Why This Works (Mechanism)

### Mechanism 1: Detection of memorized samples using probability-based filtering
- Claim: Models can be detected as having memorized samples by analyzing the probability distribution of their token generation
- Mechanism: The MinKProb detector calculates the average probability of the K lowest probability tokens in a text sample. If this average exceeds a threshold ϵ, the sample is flagged as potentially memorized
- Core assumption: Memorized samples have higher generation probabilities across tokens compared to unseen samples
- Break condition: If the model's probability distribution doesn't differ significantly between memorized and non-memorized samples, or if the threshold selection is poor

### Mechanism 2: Rewriting to break memorization while preserving task difficulty
- Claim: Rewriting problem contexts while maintaining numerical and logical structure can prevent models from relying on memorized answers
- Mechanism: For mathematical problems, contexts are rewritten (e.g., "eggs" to "candies") while numbers and calculations remain identical. For knowledge problems, questions are paraphrased without changing core content
- Core assumption: Models memorize specific problem formulations rather than abstract concepts and skills
- Break condition: If models have learned abstract reasoning patterns that generalize beyond specific formulations, or if rewriting inadvertently changes difficulty

### Mechanism 3: Iterative rewriting until detection passes
- Claim: Multiple rewriting iterations can progressively eliminate memorized responses until samples pass contamination detection
- Mechanism: After initial rewriting, samples are re-detected. If still flagged as contaminated, additional rewrites are performed until detection passes or maximum iterations are reached
- Core assumption: Each rewriting iteration progressively removes memorization cues while maintaining task integrity
- Break condition: If rewriting reaches maximum iterations without passing detection, or if iterative rewriting degrades task quality

## Foundational Learning

- Concept: Contamination detection in LLMs
  - Why needed here: Understanding how to detect when models have memorized training data is fundamental to the ITD approach
  - Quick check question: What is the difference between n-gram analysis and MinKProb for contamination detection?

- Concept: Prompt engineering for controlled generation
  - Why needed here: Effective rewriting requires precise prompts to maintain task difficulty while changing context
  - Quick check question: How would you design a prompt to rewrite a math problem while preserving its numerical structure?

- Concept: Statistical analysis of token probabilities
  - Why needed here: The MinKProb detector relies on analyzing probability distributions across tokens
  - Quick check question: Why might the average of the K lowest probability tokens be a good indicator of memorization?

## Architecture Onboarding

- Component map:
  - Detection module -> Rewriting module -> Assurance module -> Cache system -> Evaluation interface
  The pipeline processes samples from detection through rewriting to final evaluation

- Critical path: Detection → Rewriting → Re-detection → Output rewritten dataset
  The most computationally intensive part is the rewriting stage, especially with multiple iterations

- Design tradeoffs:
  - Detection accuracy vs. computational overhead (stricter detection requires more rewrites)
  - Rewriting quality vs. cost (GPT-4 API calls are expensive)
  - Iteration count vs. dataset freshness (more iterations = less memorization but higher cost)

- Failure signatures:
  - High contamination rate even after multiple rewrites → detection threshold too strict or rewriting ineffective
  - Performance degradation across all samples → rewriting inadvertently changing task difficulty
  - Low detection accuracy → poor threshold selection or inappropriate K value

- First 3 experiments:
  1. Run MinKProb detector on a small sample of GSM8K and MMLU to establish baseline contamination rates
  2. Test single-round rewriting on 10 samples from each dataset to verify difficulty preservation
  3. Apply full ITD pipeline to Llama2-contaminated model and compare accuracy before/after decontamination

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the upper bounds of ITD's effectiveness when applied to models with intentionally extensive in-domain training data contamination?
- Basis in paper: The paper discusses limitations, noting that ITD has limited effectiveness for models that have intentionally trained on large amounts of in-domain data to improve performance.
- Why unresolved: The paper acknowledges this limitation but doesn't provide empirical data on how effective ITD remains in these worst-case scenarios or what the upper bounds of correction might be.
- What evidence would resolve it: Experiments comparing ITD's performance on models with varying degrees of intentional contamination (from moderate to extreme) would establish the upper bounds of ITD's effectiveness.

### Open Question 2
- Question: How does the computational overhead of ITD scale with model size and dataset complexity?
- Basis in paper: The paper mentions that using the "All" detection scheme increases overhead by approximately 300%, but doesn't systematically analyze how computational costs scale with different model sizes or dataset characteristics.
- Why unresolved: While the paper demonstrates ITD's effectiveness, it doesn't provide a detailed analysis of the computational trade-offs across different model scales or benchmark complexities.
- What evidence would resolve it: Systematic experiments measuring computational time and resource usage of ITD across various model sizes (from 7B to 70B+ parameters) and dataset complexities would clarify the scaling properties.

### Open Question 3
- Question: What are the long-term effects of repeated ITD application on model performance and evaluation reliability?
- Basis in paper: The paper focuses on single-application effects of ITD but doesn't address whether repeated applications might lead to diminishing returns, model adaptation, or changes in evaluation reliability over time.
- Why unresolved: The paper demonstrates immediate effectiveness but doesn't explore whether ITD's benefits persist across multiple evaluation cycles or if models might adapt to ITD patterns.
- What evidence would resolve it: Longitudinal studies tracking model performance and ITD effectiveness across multiple evaluation cycles and over extended periods would reveal long-term impacts and potential adaptation effects.

## Limitations

- **Detection reliability**: The effectiveness of MinKProb contamination detection depends heavily on the specific distribution of training data in the target model, potentially limiting its generalizability across diverse architectures
- **Rewriting overhead**: The computational overhead is substantial, with approximately 300% increase in processing time when using stricter detection schemes, making large-scale evaluation campaigns potentially prohibitive
- **Difficulty preservation**: The rewriting quality assurance relies on GPT-4 and manual inspection, introducing subjectivity and lacking quantitative metrics for measuring how well rewritten problems maintain the original assessment objectives

## Confidence

- **Confidence: Medium** - The effectiveness of MinKProb contamination detection depends heavily on the specific distribution of training data in the target model. The paper reports detection accuracy around 88.7% but doesn't provide extensive validation across diverse model architectures or training regimes.
- **Confidence: Low** - The rewriting quality assurance relies on GPT-4 for automated rewriting, but the paper doesn't provide systematic evaluation of whether rewritten samples truly preserve the same difficulty and knowledge assessment as original problems.
- **Confidence: Medium** - The computational overhead is substantial - the paper reports approximately 300% increase in processing time when using stricter detection schemes. For large-scale evaluation campaigns, this could become prohibitive.

## Next Checks

1. **Cross-model validation**: Test ITD on models with different pretraining corpora (e.g., models trained on different subsets of CommonCrawl vs. specialized datasets) to assess how robust the MinKProb detection is across varying data distributions.

2. **Difficulty preservation metrics**: Develop quantitative measures for difficulty preservation beyond manual inspection, such as comparing model accuracy distributions across difficulty strata before and after rewriting, or using human evaluators to rate problem similarity on a standardized scale.

3. **Computational efficiency optimization**: Benchmark the ITD pipeline with different detection threshold configurations and iteration limits to identify the sweet spot where decontamination effectiveness plateaus while computational overhead remains manageable for practical evaluation scenarios.