---
ver: rpa2
title: Improving Vietnamese-English Medical Machine Translation
arxiv_id: '2403.19161'
source_url: https://arxiv.org/abs/2403.19161
tags:
- translation
- pairs
- medical
- sentence
- shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MedEV, a high-quality Vietnamese-English parallel
  dataset with approximately 360K sentence pairs in the medical domain. It compares
  various translation tools and models on this dataset, including Google Translate,
  ChatGPT, and fine-tuned NMT models.
---

# Improving Vietnamese-English Medical Machine Translation

## Quick Facts
- arXiv ID: 2403.19161
- Source URL: https://arxiv.org/abs/2403.19161
- Authors: Nhu Vo; Dat Quoc Nguyen; Dung D. Le; Massimo Piccardi; Wray Buntine
- Reference count: 0
- Primary result: Fine-tuning vinai-translate on the MedEV dataset achieves 4+ BLEU point improvements over Google Translate in English-to-Vietnamese translation and 3+ BLEU point improvements in Vietnamese-to-English translation.

## Executive Summary
This paper introduces MedEV, a high-quality Vietnamese-English parallel dataset containing approximately 360K sentence pairs in the medical domain. The authors conduct extensive experiments comparing various translation tools and models, including Google Translate, ChatGPT, and fine-tuned NMT models. The results demonstrate that fine-tuning the vinai-translate model on the MedEV dataset significantly outperforms Google Translate by 4+ BLEU points in English-to-Vietnamese translation and 3+ BLEU points in Vietnamese-to-English translation. The MedEV dataset is publicly released to promote further research in Vietnamese-English medical machine translation.

## Method Summary
The study involves collecting parallel documents from various sources, preprocessing and aligning sentences using NLP tools and translation toolkits, and fine-tuning pre-trained models on the resulting MedEV dataset. The vinai-translate model is fine-tuned for 5 epochs using AdamW with a learning rate of 5e-5, max sequence length of 256, and beam search with size 5. The performance is evaluated using BLEU, TER, and METEOR metrics on held-out validation and test sets.

## Key Results
- Fine-tuning vinai-translate on MedEV dataset improves BLEU scores by 4+ points in English-to-Vietnamese and 3+ points in Vietnamese-to-English translation compared to Google Translate.
- Using only 10K sentence pairs for fine-tuning improves baseline scores by 4+ points, and additional 330K+ pairs produce 4+ more points.
- Fine-tuned models consistently outperform pre-trained models without fine-tuning on both validation and test sets in both translation directions.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning pre-trained translation models on domain-specific data substantially improves translation quality in the medical domain.
- Mechanism: Domain-specific fine-tuning allows models to adapt to specialized vocabulary and linguistic patterns unique to medical texts, which are not well-represented in general-purpose training data.
- Core assumption: Medical terminology and sentence structures are sufficiently different from general text to warrant specialized training.
- Evidence anchors:
  - [abstract]: "Experimental results show that the best performance is achieved by fine-tuning 'vinai-translate' for each translation direction."
  - [section 4]: "When it comes to the 'fine-tuning' setting, all fine-tuned models outperform Google Translate on both validation and test sets in both translation directions."
  - [corpus]: Weak evidence; corpus lacks direct fine-tuning ablation studies.
- Break condition: If the fine-tuned model overfits to the small medical dataset, leading to poor generalization to unseen medical text.

### Mechanism 2
- Claim: Larger training datasets yield better translation performance in the medical domain.
- Mechanism: Increasing the number of training sentence pairs exposes the model to a broader range of medical terminology and context, improving its ability to translate accurately.
- Core assumption: The additional sentence pairs provide diverse and representative examples of medical language.
- Evidence anchors:
  - [section 4]: "Figure 1 presents BLEU scores on the validation set for both translation directions when fine-tuning vinai-translate with different numbers of training sentence pairs. Here, using only 10K sentence pairs helps substantially improve the baseline scores by 4+ points: from 44.24 to 48.23 for English-to-Vietnamese and from 33.28 to 37.97 for Vietnamese-to-English. Additional 330K+ pairs produce 4+ more points, increasing from 48.23 to 52.21 and from 37.97 to 42.66."
  - [corpus]: Weak evidence; corpus does not detail the distribution or diversity of the 360K pairs.
- Break condition: If the additional data introduces noise or irrelevant content, potentially degrading model performance.

### Mechanism 3
- Claim: Pre-trained bilingual/multilingual models serve as strong baselines but are outperformed by fine-tuned models on domain-specific tasks.
- Mechanism: Pre-trained models provide a good starting point due to their exposure to multiple languages, but fine-tuning tailors them to the specific nuances of medical translation.
- Core assumption: The pre-trained models have learned generalizable linguistic features that can be adapted to specialized domains.
- Evidence anchors:
  - [abstract]: "We conduct extensive experiments comparing Google Translate, ChatGPT (gpt-3.5-turbo), state-of-the-art Vietnamese-English neural machine translation models and pre-trained bilingual/multilingual sequence-to-sequence models on our new MedEV dataset."
  - [section 4]: "In the 'without fine-tuning' (w/o FT) setting, the automatic translation engine Google Translate consistently outperforms both vinai-translate and envit5-translation, achieving the best scores. In contrast, when it comes to the 'fine-tuning' setting, all fine-tuned models outperform Google Translate on both validation and test sets in both translation directions."
  - [corpus]: Weak evidence; corpus does not provide detailed performance comparisons across all model types.
- Break condition: If the pre-trained models are already highly specialized for the medical domain, fine-tuning may offer minimal improvement.

## Foundational Learning

- Concept: Neural Machine Translation (NMT)
  - Why needed here: NMT forms the basis for the translation models used in the study, including fine-tuning approaches.
  - Quick check question: What are the key components of an NMT model, and how do they contribute to translation quality?

- Concept: Fine-tuning
  - Why needed here: Fine-tuning is the primary method used to adapt pre-trained models to the medical domain, improving translation accuracy.
  - Quick check question: How does fine-tuning differ from training a model from scratch, and what are its advantages in low-resource scenarios?

- Concept: BLEU Score
  - Why needed here: BLEU is the primary metric used to evaluate translation quality in the study, guiding model selection and comparison.
  - Quick check question: How is the BLEU score calculated, and what are its limitations in assessing translation quality?

## Architecture Onboarding

- Component map: Data Collection -> Preprocessing -> Sentence Alignment -> Model Training -> Evaluation
- Critical path:
  1. Collect and preprocess parallel documents.
  2. Align sentences within document pairs.
  3. Fine-tune pre-trained models on the aligned dataset.
  4. Evaluate model performance on validation and test sets.
- Design tradeoffs:
  - Model Complexity vs. Performance: More complex models may offer better performance but require more computational resources.
  - Data Size vs. Quality: Larger datasets can improve model performance but may introduce noise if not carefully curated.
  - Fine-tuning vs. Training from Scratch: Fine-tuning is faster and requires less data but may be limited by the pre-trained model's capabilities.
- Failure signatures:
  - Overfitting: Model performs well on training data but poorly on validation/test data.
  - Underfitting: Model performs poorly on both training and validation/test data.
  - Data Leakage: Information from the test set inadvertently influences the training process.
- First 3 experiments:
  1. Fine-tune vinai-translate on a subset of the MedEV dataset and evaluate BLEU scores on the validation set.
  2. Compare the performance of fine-tuned models against pre-trained models without fine-tuning on the validation set.
  3. Analyze the impact of different sentence lengths on translation quality by evaluating BLEU scores across length buckets.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the fine-tuned models change when additional general domain parallel corpora (like PhoMT and MTet) are incorporated into the training data alongside the MedEV dataset?
- Basis in paper: [inferred] The paper mentions exploring the translation quality when combining MedEV with other general domain datasets as future work.
- Why unresolved: The authors have not conducted experiments to evaluate the impact of combining the MedEV dataset with other general domain parallel corpora on the translation quality.
- What evidence would resolve it: Conducting experiments that fine-tune the models on a combined dataset of MedEV and other general domain parallel corpora (like PhoMT and MTet) and comparing their performance with models trained solely on MedEV would provide evidence.

### Open Question 2
- Question: What is the impact of using different preprocessing techniques on the alignment quality of the parallel sentences in the MedEV dataset?
- Basis in paper: [inferred] The paper describes the data collection and preprocessing steps but does not evaluate the impact of different preprocessing techniques on the alignment quality.
- Why unresolved: The authors have not experimented with various preprocessing techniques to assess their influence on the alignment quality of parallel sentences.
- What evidence would resolve it: Conducting experiments that apply different preprocessing techniques (e.g., tokenization, normalization, etc.) to the collected parallel documents and comparing the alignment quality would provide evidence.

### Open Question 3
- Question: How does the performance of the fine-tuned models on the MedEV dataset generalize to other medical sub-domains not represented in the dataset?
- Basis in paper: [inferred] The MedEV dataset covers various medical sub-domains, but it is unclear how well the fine-tuned models would perform on sub-domains not included in the dataset.
- Why unresolved: The authors have not evaluated the generalization ability of the fine-tuned models to medical sub-domains not present in the MedEV dataset.
- What evidence would resolve it: Conducting experiments that test the fine-tuned models on parallel sentence pairs from medical sub-domains not represented in the MedEV dataset would provide evidence of their generalization ability.

## Limitations
- The study lacks detailed ablation studies on the fine-tuning process, including different configurations of learning rates, epochs, and batch sizes.
- The dataset construction process lacks transparency in terms of quality control measures for the 360K sentence pairs.
- The paper does not address potential domain-specific challenges in medical translation, such as handling rare medical terms or maintaining consistency across different medical specialties.

## Confidence
- High Confidence: The claim that fine-tuning pre-trained models on domain-specific data improves medical translation quality is well-supported by the experimental results.
- Medium Confidence: The assertion that larger training datasets yield better performance is supported by the data, but the study doesn't explore the point of diminishing returns or analyze the diversity of the additional sentence pairs.
- Low Confidence: The comparison between pre-trained and fine-tuned models assumes that pre-trained models provide a good baseline, but the study doesn't thoroughly investigate whether these pre-trained models were already partially optimized for medical translation tasks.

## Next Checks
1. Conduct a fine-tuning ablation study varying hyperparameters (learning rate, batch size, number of epochs) to determine the optimal configuration and understand the sensitivity of the results to these parameters.
2. Perform a detailed analysis of the MedEV dataset to assess the quality and diversity of the sentence pairs, including manual inspection of a random sample and evaluation of sentence alignment accuracy.
3. Test the fine-tuned models on medical texts from different specialties (e.g., cardiology, oncology) to evaluate their performance across various medical domains and identify any specialization-specific weaknesses.