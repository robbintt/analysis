---
ver: rpa2
title: Minimax optimality of deep neural networks on dependent data via PAC-Bayes
  bounds
arxiv_id: '2410.21702'
source_url: https://arxiv.org/abs/2410.21702
tags:
- deep
- neural
- networks
- class
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes minimax optimality of deep neural networks
  (DNNs) with ReLU activation for regression and classification on dependent data.
  The authors extend Schmidt-Hieber's (2020) results by relaxing the i.i.d.
---

# Minimax optimality of deep neural networks on dependent data via PAC-Bayes bounds

## Quick Facts
- arXiv ID: 2410.21702
- Source URL: https://arxiv.org/abs/2410.21702
- Authors: Pierre Alquier; William Kengne
- Reference count: 40
- Primary result: Establishes minimax optimality of DNNs with ReLU activation on dependent data, matching i.i.d. rates when pseudo-spectral gap is bounded away from zero

## Executive Summary
This paper establishes minimax optimality of deep neural networks (DNNs) with ReLU activation for regression and classification on dependent data. The authors extend Schmidt-Hieber's (2020) results by relaxing the i.i.d. assumption, allowing observations to follow a stationary ergodic Markov chain with a non-null pseudo-spectral gap. They develop a generalized Bayesian estimator based on PAC-Bayes oracle inequalities and Bernstein's inequality, applicable to both regression and logistic regression problems.

## Method Summary
The method uses a PAC-Bayes framework with spike-and-slab priors to derive oracle inequalities for DNNs on dependent data. The estimator replaces the sample size n with the effective sample size n·γn (where γn is the pseudo-spectral gap) in the theoretical bounds. The approach leverages Paulin's Bernstein inequality for Markov chains to extend PAC-Bayes bounds to the dependent setting, with architecture parameters scaled according to the effective sample size and function class complexity.

## Key Results
- DNNs achieve the same convergence rates on dependent data as in the i.i.d. case when the pseudo-spectral gap is bounded away from zero
- Oracle inequality matches the i.i.d. case up to a logarithmic factor when using the effective sample size
- Matching lower bound for classification with logistic loss proves minimax optimality
- Results hold for both Hölder and composition-structured function classes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DNNs with ReLU activation achieve minimax optimal convergence rates on dependent data, matching the i.i.d. case when the pseudo-spectral gap is bounded away from zero.
- Mechanism: The proof replaces the sample size n with the "effective sample size" n·γn, where γn is the pseudo-spectral gap of the underlying Markov chain.
- Core assumption: The observations form a stationary ergodic Markov chain with a non-null pseudo-spectral gap γn > 0, and n·γn → ∞ as n → ∞.
- Evidence anchors:
  - [abstract] "The results show that DNNs achieve the same convergence rates on dependent data as in the i.i.d. case, with the sample size effectively replaced by the 'effective sample size' n · γn"
  - [section] "Theorem 4.1. Assume that (A1)-(A5) and (A6) with κ = 1 hold and take λ = nγn / (32K + 10)"
- Break condition: If the pseudo-spectral gap γn is too small or decreases too rapidly with n, the effective sample size becomes insufficient for consistency.

### Mechanism 2
- Claim: The PAC-Bayes oracle inequality framework extends to dependent data through Paulin's Bernstein inequality for Markov chains.
- Mechanism: The authors derive a new PAC-Bayes oracle inequality for Markov chains using Paulin's Bernstein inequality, which handles the dependence structure while maintaining the risk concentration properties needed for oracle bounds.
- Core assumption: The loss function satisfies appropriate Bernstein conditions (A6) and local quadratic structure (A7) for the excess risk.
- Evidence anchors:
  - [abstract] "Leveraging on PAC-Bayes oracle inequalities and a version of Bernstein inequality due to Paulin (2015), we derive upper bounds on the estimation risk"
  - [section] "PAC-Bayes bounds were extended to non-i.i.d. settings under various assumptions... Here, we prove a new PAC-Bayes oracle inequality for Markov chains"
- Break condition: If the Bernstein conditions (A6) or local quadratic structure (A7) fail to hold, the oracle inequality derivation breaks down.

### Mechanism 3
- Claim: The spike-and-slab prior with sparsity parameter s enables optimal network architecture selection.
- Mechanism: The prior distribution (3.2) assigns probability mass to different active parameter sets, with the sparsity parameter s controlling the trade-off between model size and estimation accuracy.
- Core assumption: The target function can be well-approximated by a sparse DNN with bounded parameters, and the prior correctly reflects this structure.
- Evidence anchors:
  - [section] "The prior in (3.2) is referred to as a spike-and-slab prior in the literature... It is popular in high-dimensional Bayesian statistics as a sparsity inducing prior"
  - [section] "The spike-and-slab prior was used by [51, 14, 57, 9] among others... In PAC-Bayes bounds, it was used by [3, 55, 40]"
- Break condition: If the true function is not well-approximated by a sparse DNN or the sparsity parameter s is poorly chosen, the prior becomes suboptimal.

## Foundational Learning

- Concept: Markov chain theory and pseudo-spectral gap
  - Why needed here: The entire theoretical framework relies on understanding how temporal dependence affects statistical learning rates, with the pseudo-spectral gap quantifying the "effective independence" of observations.
  - Quick check question: What is the relationship between mixing time and pseudo-spectral gap, and why does this matter for statistical estimation?

- Concept: PAC-Bayes bounds and oracle inequalities
  - Why needed here: The paper uses PAC-Bayes techniques to derive generalization bounds and oracle inequalities that work in the dependent data setting, extending classical results.
  - Quick check question: How does the PAC-Bayes framework differ from classical frequentist bounds, and why is it particularly suited to neural network analysis?

- Concept: Function space theory and approximation rates
  - Why needed here: The optimality results depend on understanding how well DNNs can approximate functions in Hölder and composition-structured classes, and the corresponding minimax lower bounds.
  - Quick check question: What are the key differences between Hölder spaces and composition-structured function classes, and how do these affect approximation rates?

## Architecture Onboarding

- Component map: Markov chain modeling -> PAC-Bayes bound derivation -> DNN architecture selection -> Excess risk analysis
- Critical path: Start with verifying the Markov chain assumptions and pseudo-spectral gap conditions, then implement the PAC-Bayes bound framework, followed by DNN architecture selection, and finally excess risk computation.
- Design tradeoffs: The main tradeoff is between model complexity (network depth/width) and the effective sample size (pseudo-spectral gap). Larger networks require stronger dependence conditions to maintain optimality.
- Failure signatures: If the pseudo-spectral gap is underestimated, the effective sample size becomes too small and rates degrade; if Bernstein conditions fail, the PAC-Bayes bounds become vacuous.
- First 3 experiments:
  1. Verify pseudo-spectral gap estimation on simulated Markov chains with known spectral properties
  2. Test PAC-Bayes bound tightness on dependent data with known function class membership
  3. Validate excess risk bounds on synthetic data from Hölder and composition-structured functions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions can the pseudo-spectral gap γn be consistently estimated from a single trajectory of the Markov chain?
- Basis in paper: [explicit] The paper discusses that while γn estimation is feasible in finite state spaces, it remains problematic in infinite state spaces, noting that "it might remain feasible under additional assumptions on the Markov chain."
- Why unresolved: The paper acknowledges the difficulty of estimating γn from a single trajectory in infinite state spaces and suggests this will be the object of future works, without specifying what assumptions would make estimation possible.
- What evidence would resolve it: A theoretical framework establishing conditions under which γn can be consistently estimated from a single trajectory in infinite state spaces, along with a practical estimator and its convergence rate.

### Open Question 2
- Question: Does the convergence rate of the PAC-Bayes deep neural network estimator match the minimax lower bound when the pseudo-spectral gap γn decays to zero?
- Basis in paper: [inferred] The paper shows that when γn → 0, convergence rates become slower than the i.i.d. case, but does not establish whether these slower rates match the corresponding minimax lower bounds for dependent data.
- Why unresolved: The paper establishes upper bounds on the excess risk when γn → 0 but does not provide matching lower bounds for this case, leaving open whether the proposed estimator is minimax optimal under decaying spectral gaps.
- What evidence would resolve it: A minimax lower bound for deep learning on dependent data when the pseudo-spectral gap decays, showing whether the slower convergence rates established in the paper are tight.

### Open Question 3
- Question: How does the cold posterior effect manifest in the PAC-Bayes deep learning framework with dependent data?
- Basis in paper: [explicit] The paper mentions the cold posterior effect, noting that "it seems that λ can be taken much larger than what is prescribed by the PAC-Bayes theory" but does not investigate this phenomenon in the dependent data setting.
- Why unresolved: While the paper discusses the theoretical calibration of the temperature parameter λ for optimal rates, it acknowledges that practitioners often use larger values without understanding the implications for dependent data.
- What evidence would resolve it: Empirical studies comparing the performance of the theoretically calibrated λ versus larger values in PAC-Bayes DNNs on dependent data, along with theoretical analysis of the effect on generalization bounds.

## Limitations

- The framework requires precise conditions on the pseudo-spectral gap and Bernstein conditions that may be difficult to verify in practice
- Limited empirical validation of the theoretical results with only synthetic data examples
- The spike-and-slab prior sampling for DNNs is noted as non-trivial to implement
- Results assume known pseudo-spectral gap values, which may not be available in practice

## Confidence

- **High confidence**: The minimax optimality results for classification with logistic loss (Theorem 4.3) - these follow from matching upper and lower bounds in the established literature.
- **Medium confidence**: The PAC-Bayes oracle inequality extension to Markov chains (Section 3.1) - while the framework is sound, the specific Bernstein inequality adaptation to dependent data requires careful verification.
- **Medium confidence**: The excess risk bounds for Hölder and composition-structured classes (Theorems 4.1-4.2) - these depend on multiple technical conditions that may be difficult to verify in practice.

## Next Checks

1. **Empirical validation**: Test the proposed estimator on synthetic Markov chain data with known pseudo-spectral gap, comparing convergence rates against theoretical predictions across different dependence strengths.

2. **Robustness assessment**: Evaluate estimator performance when pseudo-spectral gap conditions are violated or only approximately satisfied, quantifying the impact on convergence rates.

3. **Practical implementation**: Develop and test algorithms for posterior sampling and pseudo-spectral gap estimation in the DNN context, addressing the noted computational challenges.