---
ver: rpa2
title: 'LexDrafter: Terminology Drafting for Legislative Documents using Retrieval
  Augmented Generation'
arxiv_id: '2403.16295'
source_url: https://arxiv.org/abs/2403.16295
tags: []
core_contribution: This paper introduces LexDrafter, a framework to assist drafting
  Definitions articles in EU legislative documents using retrieval-augmented generation
  (RAG). The framework builds a document and definition corpus from EUR-Lex legal
  acts, extracting definition elements and resolving citations.
---

# LexDrafter: Terminology Drafting for Legislative Documents using Retrieval Augmented Generation

## Quick Facts
- arXiv ID: 2403.16295
- Source URL: https://arxiv.org/abs/2403.16295
- Reference count: 0
- Primary result: Retrieval-augmented generation framework that extracts and generates legal definitions with BERTScore ~0.82 and BLEU-1 ~0.26

## Executive Summary
LexDrafter is a framework designed to assist in drafting Definitions articles for EU legislative documents by automatically retrieving existing definitions or generating new ones for selected terms. The system processes EUR-Lex legal acts from the Energy domain, extracts definition elements using pattern matching, and resolves citations to build a structured definition corpus. For terminology drafting, it employs a retrieval-augmented generation approach that combines lexical search retrieval with LLM generation to produce definitions that are semantically similar to ground truth definitions despite lower n-gram overlap.

## Method Summary
The LexDrafter framework extracts legal acts from EUR-Lex, processes them into structured document and definition corpora by identifying definition patterns and resolving citations, then uses a RAG approach for terminology drafting. When a user selects a term, the system either retrieves existing definitions from the corpus or generates new ones by retrieving relevant document fragments using lexical search and passing them with the term to an LLM (Vicuna-7b or LLaMA-2-7b) via a prompt template. The framework was evaluated on 1,007 legal terms from 551 Energy domain legal acts, showing high semantic similarity (BERTScore ~0.82) but lower n-gram overlap (BLEU-1 ~0.26) compared to ground truth definitions.

## Key Results
- High semantic similarity between generated and ground truth definitions (BERTScore ~0.82)
- Lower n-gram overlap indicating generated definitions use different phrasing (BLEU-1 ~0.26)
- Framework processes 551 legal acts from Energy domain, extracting 1,007 legal terms with 1,330 definition elements
- Generated definitions maintain fluency and coherence while capturing semantic meaning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LexDrafter improves definition consistency by automatically retrieving and generating definitions from existing legal documents
- Mechanism: The framework extracts legal acts from EUR-Lex, processes them into a structured document corpus, and builds a definition corpus by identifying and resolving citations. When a user selects a term, the system either retrieves an existing definition or uses a RAG approach with an LLM to generate a new definition based on retrieved fragments
- Core assumption: Legal terms in the EU context have standardized patterns for definition and citation, making automated extraction feasible
- Evidence anchors:
  - [abstract] "having a framework that provides insights about existing definitions and helps define new terms based on a document's context will support such harmonized legal definitions across different regulations"
  - [section] "Fragments defining a legal term are identified by the DefExtract component... using patterns as per 'Wording Laying Down Definitions' (Section C.7 in JPG)"
  - [corpus] "For example, consider the definition of the term 'energy from renewable sources' or 'renewable energy' present in the document on 'common rules for the internal market for electricity...' (Celex ID 32019L0944): 'energy from renewable sources' or 'renewable energy' means energy from renewable non-fossil sources..."
- Break condition: If legal documents deviate significantly from the assumed patterns or if citation formats are inconsistent, extraction accuracy will degrade

### Mechanism 2
- Claim: The RAG approach generates definitions that are semantically similar to ground truth definitions despite lower n-gram overlap
- Mechanism: The system uses a lexical search retriever to find relevant fragments based on term frequency, then passes these fragments along with the term to an LLM (Vicuna-7b or LLaMA-2-7b) via a prompt template to generate a definition within 25-45 words
- Core assumption: The LLM can understand the context provided by the retrieved fragments and generate a coherent definition that captures the essence of the term
- Evidence anchors:
  - [abstract] "Evaluation on 1,007 legal terms shows high semantic similarity (BERTScore ~0.82) but lower n-gram overlap (BLEU-1 ~0.26) compared to ground truth definitions"
  - [section] "The retrieved fragments and the term to be defined based on the fragments are then passed as a prompt to the generator. The prompt template below is used to generate a definition for the respective term"
  - [corpus] "The prompt, we specify a range of 25 to 45 words, considering the average number of tokens in the definitions in our corpus (approximately 32 words)"
- Break condition: If the retrieved fragments are irrelevant or insufficient, or if the LLM fails to understand the context, the generated definition will be inaccurate or nonsensical

### Mechanism 3
- Claim: LexDrafter reduces time and human error in drafting Definitions articles by automating the identification and generation of term definitions
- Mechanism: The system processes 551 legal acts from the Energy domain, extracts 1,007 legal terms with 1,330 definition elements, and provides a user interface where terms can be selected for definition retrieval or generation
- Core assumption: Automating the definition drafting process will reduce the manual effort required to ensure consistency and accuracy in legal documents
- Evidence anchors:
  - [abstract] "the advantages of LexDrafter are three-fold: first, it aims at harmonizing legal definitions across different legal acts by automating the drafting of Definitions articles. Second, human errors are reduced by providing existing term definitions... and third, time-consumption is reduced for drafting documents"
  - [section] "LexDrafter functions help users when drafting a legal act, in particular when drafting document sections has been completed, but the section with terminology definitions (the Definitions article) is missing"
  - [corpus] "For the creation of our document and definition corpus, 551 legal acts from the Energy domain between 1949 and 2023 were crawled... A total of 1330 fragments defining 1007 legal terms were detected and stored by the DefExtract component as definition elements"
- Break condition: If the system cannot handle the variety of legal domains or if the definition corpus is incomplete, users may still need to perform significant manual work

## Foundational Learning

- Concept: EUR-Lex document structure and legal terminology patterns
  - Why needed here: Understanding the structure of EUR-Lex documents and the patterns used in legal definitions is crucial for effective extraction and generation of definitions
  - Quick check question: What are the four major zones of a legal document as detailed by Chalkidis et al. (2019)?

- Concept: Retrieval Augmented Generation (RAG) and its components
  - Why needed here: The RAG approach combines retrieval of relevant fragments with generation by an LLM, which is central to the definition generation process
  - Quick check question: What are the two major components of the RAG approach used in LexDrafter?

- Concept: Evaluation metrics for text generation (BLEU, BERTScore, BLEURT)
  - Why needed here: These metrics are used to evaluate the quality of generated definitions against ground truth definitions
  - Quick check question: What does BERTScore measure in the context of evaluating generated definitions?

## Architecture Onboarding

- Component map: DocStruct -> DefExtract -> CiteResolver -> TermRetriever -> RAG
- Critical path: User selects term → TermRetriever checks for existing definitions → if none found, RAG generates new definition → generated definition is presented to user
- Design tradeoffs: Using lexical search retriever instead of dense embedding to reduce noise, but potentially missing semantically relevant fragments; using open-source LLMs to keep costs down, but potentially sacrificing performance
- Failure signatures: Low retrieval recall (few relevant fragments found), low generation quality (incoherent or irrelevant definitions), high manual intervention required
- First 3 experiments:
  1. Test definition extraction on a small set of EUR-Lex documents to verify pattern matching accuracy
  2. Test RAG generation with known terms and compare generated definitions to ground truth using evaluation metrics
  3. Conduct a user study to assess the usefulness and accuracy of generated definitions in a drafting context

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the fine-tuning of BLEURT on a legal domain corpus affect the evaluation of generated definitions in terms of fluency, coherence, and semantic accuracy?
- Basis in paper: [explicit] The authors mention that BLEURT is trained on non-legal domain corpus and suggest fine-tuning it on a legal domain corpus to improve insights into the fluency, coherence, and semantics of generated definitions
- Why unresolved: The authors state that fine-tuning BLEURT on a legal corpus is ongoing work and do not present the results of this fine-tuning
- What evidence would resolve it: Conducting the fine-tuning of BLEURT on a legal domain corpus and evaluating the generated definitions using the fine-tuned BLEURT to measure improvements in fluency, coherence, and semantic accuracy

### Open Question 2
- Question: What is the impact of using a dense embedding retriever instead of a lexical search retriever on the quality of retrieved fragments for the RAG approach in generating definitions?
- Basis in paper: [inferred] The authors mention that a dense embedding retriever often introduces noise by considering document fragments that have terms "closer" to the queried term(s), leading them to employ a lexical search retriever instead
- Why unresolved: The paper does not provide a comparison between the performance of a dense embedding retriever and a lexical search retriever in the context of the RAG approach for generating definitions
- What evidence would resolve it: Conducting experiments using both a dense embedding retriever and a lexical search retriever in the RAG approach and comparing the quality of the retrieved fragments and the generated definitions

### Open Question 3
- Question: How does the automatic identification of terms that need to be defined in a drafted document affect the efficiency and accuracy of the LexDrafter framework?
- Basis in paper: [explicit] The authors mention that they are working on automatically identifying terms that need to be defined in a drafted document as part of their ongoing work
- Why unresolved: The paper does not present the results or methodology for automatically identifying terms that need to be defined
- What evidence would resolve it: Developing and implementing an algorithm for automatically identifying terms that need to be defined in a drafted document and evaluating its performance in terms of efficiency and accuracy within the LexDrafter framework

## Limitations
- Performance relies heavily on the quality and completeness of the EUR-Lex corpus from the Energy domain, which may not generalize to other legal domains
- Evaluation focuses on semantic similarity metrics but lacks user studies to assess practical utility in real drafting workflows
- Reliance on lexical search rather than dense embeddings for retrieval may miss semantically relevant but lexically different fragments

## Confidence

- **High Confidence**: The RAG-based generation mechanism and the extraction of definitions from EUR-Lex documents are well-specified and demonstrated with concrete examples. The semantic similarity results (BERTScore ~0.82) provide strong evidence that the generated definitions capture the meaning of ground truth definitions.
- **Medium Confidence**: The practical impact on reducing drafting time and human error is claimed but not empirically validated through user studies or comparison with traditional drafting processes. The framework's effectiveness across different legal domains beyond Energy remains unproven.
- **Low Confidence**: The system's behavior with ambiguous or multi-sense terms is not explored. The paper does not address how LexDrafter handles situations where a term has multiple definitions across different legal contexts or how it resolves conflicts between retrieved definitions.

## Next Checks

1. **Domain Generalization Test**: Evaluate LexDrafter's performance on legal acts from domains other than Energy (e.g., environmental law, consumer protection) to assess cross-domain applicability of definition extraction and generation patterns
2. **User Study Implementation**: Conduct a controlled user study comparing drafting efficiency and definition quality between legal professionals using LexDrafter versus traditional methods, measuring time saved and accuracy of generated definitions
3. **Ambiguity Handling Evaluation**: Create a test set of legal terms with known multiple definitions across different contexts and evaluate how LexDrafter handles these cases, measuring precision of retrieved definitions and coherence of generated definitions