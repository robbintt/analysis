---
ver: rpa2
title: Training on the Edge of Stability Is Caused by Layerwise Jacobian Alignment
arxiv_id: '2406.00127'
source_url: https://arxiv.org/abs/2406.00127
tags:
- layer
- ar-10
- left
- middle
- bottom
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper demonstrates that training on the edge of stability in
  neural networks is caused by the alignment of layerwise Jacobian matrices, and that
  this alignment scales with dataset size following a power law with high coefficient
  of determination (0.74-0.98). The authors use an exponential Euler solver to train
  networks without entering the edge of stability, enabling accurate approximation
  of gradient flow dynamics.
---

# Training on the Edge of Stability Is Caused by Layerwise Jacobian Alignment

## Quick Facts
- arXiv ID: 2406.00127
- Source URL: https://arxiv.org/abs/2406.00127
- Authors: Mark Lowell; Catharine Kastner
- Reference count: 40
- One-line primary result: Training on the edge of stability is caused by layerwise Jacobian alignment, which scales with dataset size following a power law.

## Executive Summary
This paper investigates the phenomenon of "training on the edge of stability" in neural networks, where training exhibits oscillatory behavior with the learning rate near the stability threshold. The authors demonstrate that this phenomenon is caused by the alignment of layerwise Jacobian matrices during training. They show that as training progresses, the singular vectors of these Jacobians rotate to create alignment, causing small input changes to produce large output changes. This alignment is measured by the ratio r(∆i+1, ∂ˆxi+1/∂ˆxi), which increases during training. The study uses an exponential Euler solver to avoid edge of stability effects, enabling accurate measurement of gradient flow dynamics and the underlying causes of sharpness increase.

## Method Summary
The authors use an exponential Euler solver to train multi-layer perceptrons (MLPs) with 6 layers and 512 width using ELU activation, ensuring training remains stable while approximating gradient flow dynamics. They train on five datasets including CIFAR-10, SST, AtrialFibrillation, and a synthetic dice dataset. The method calculates top eigenvalues and eigenvectors of Hessian matrices using power iteration, measures sharpness and layerwise Jacobian alignment throughout training, and analyzes how these metrics scale with dataset size. Snapshots are saved every 100 iterations for post-training analysis of eigenvalues and alignments. The study decomposes the Hessian matrix into components to identify that the increase in sharpness is driven primarily by the G component and layerwise Jacobian alignment.

## Key Results
- Training on the edge of stability is caused by alignment of layerwise Jacobian matrices, where singular vectors rotate to allow small input changes to produce large output changes
- The degree of layerwise Jacobian alignment scales with dataset size following a power law with coefficient of determination between 0.74 and 0.98
- Using an exponential Euler solver prevents training from entering the edge of stability, enabling accurate approximation of gradient flow dynamics
- The increase in Hessian sharpness is driven primarily by the G component, which can be broken down into five components including the alignment ratio P1∆,J

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training on the edge of stability is caused by alignment of layerwise Jacobian matrices.
- Mechanism: As training progresses, the singular vectors of the layerwise Jacobians rotate so that a change along a narrow subspace at the network inputs can cause a large change in the outputs. This alignment is measured by the ratio r(∆i+1, ∂ˆxi+1/∂ˆxi), which increases during training.
- Core assumption: The increase in sharpness of the Hessian matrix is primarily driven by the G component, not the H component.
- Evidence anchors:
  - [abstract] "the increase in the sharpness of the Hessian matrix is caused by the layerwise Jacobian matrices of the network becoming aligned"
  - [section] "we observe that the highest operator norm is usually found in the first layer... the increase in the sharpness of Hθ ˜L(θ) can be analyzed by examining E||K 1||2
max = E||∆1||2
max"
  - [corpus] Weak evidence - no directly related work found that explicitly measures layerwise Jacobian alignment as the cause of edge of stability.
- Break condition: If the overlap ratio ρ(K) were the dominant factor instead of E||K||2
max, or if the sharpness increase were primarily from the H component.

### Mechanism 2
- Claim: The degree of layerwise Jacobian alignment scales with dataset size following a power law.
- Mechanism: Larger datasets create more complex decision boundaries, which require greater alignment between layerwise Jacobians to achieve the necessary output changes. This alignment follows the form max P1
∆,J ≈ c1Dc2 where D is dataset size.
- Core assumption: The complexity of the decision boundary is correlated with the degree of Jacobian alignment required.
- Evidence anchors:
  - [abstract] "the degree of alignment scales with the size of the dataset by a power law with a coefficient of determination between 0.74 and 0.98"
  - [section] "Consistent with [32], we observe that the highest operator norm is usually found in the first layer" (implying layer-specific behavior)
  - [corpus] Weak evidence - no directly related work found that connects dataset size to Jacobian alignment power laws.
- Break condition: If the power law relationship were not observed across different datasets and criteria, or if alignment did not correlate with decision boundary complexity.

### Mechanism 3
- Claim: Using an exponential Euler solver prevents training from entering the edge of stability, enabling accurate measurement of gradient flow dynamics.
- Mechanism: The exponential Euler method uses top eigenvalues and eigenvectors to adjust the step size, keeping training stable while still approximating the true gradient flow. This allows measurement of sharpness and Jacobian alignment without the confounding effects of edge of stability dynamics.
- Core assumption: The exponential Euler method accurately approximates gradient flow dynamics when top eigenvalues are used for step size control.
- Evidence anchors:
  - [section] "we trained a large number of multi-layer perceptrons (MLPs) using an exponential Euler solver [30, 15] to ensure training did not enter the edge of stability"
  - [section] "This is effectively the exponential Euler method [30, 15], but using only a small number of top eigenvectors instead of the full eigendecomposition"
  - [corpus] Weak evidence - no directly related work found that uses exponential Euler solvers specifically for edge of stability experiments.
- Break condition: If the exponential Euler method did not accurately approximate gradient flow, or if top eigenvalues were insufficient for step size control.

## Foundational Learning

- Concept: Hessian matrix and its role in optimization
  - Why needed here: The paper decomposes the sharpness of the Hessian (largest eigenvalue) into components to understand what causes training on the edge of stability
  - Quick check question: What is the relationship between the Hessian's largest eigenvalue and the maximum stable learning rate?

- Concept: Layerwise Jacobian matrices and their chain rule decomposition
  - Why needed here: The paper shows that sharpness increase is caused by alignment of layerwise Jacobians, requiring understanding of how Jacobians compose through layers
  - Quick check question: How does the chain rule relate the pre-activation Jacobians to the layerwise Jacobians?

- Concept: Eigenvalue decomposition and power iteration methods
  - Why needed here: The paper uses power iteration to calculate top eigenvalues of large matrices, and decomposes the Hessian into eigenvalues and eigenvectors
  - Quick check question: What is the computational advantage of using power iteration over full eigendecomposition for large matrices?

## Architecture Onboarding

- Component map:
  Data pipeline: Synthetic dice dataset + real datasets (CIFAR-10, AtrialFibrillation, SST) -> Model architecture: 6-layer MLP with 512 width, ELU activation -> Training engine: Custom exponential Euler solver with top eigenvalue-based step size -> Analysis tools: Power iteration for top eigenvalues, functorch for Jacobian calculations, LOBPCG for singular values -> Measurement system: Snapshot saving every 100 iterations, post-training eigenvalue calculation

- Critical path: Data → Model initialization → Exponential Euler training → Snapshot saving → Post-hoc analysis of eigenvalues and alignments
- Design tradeoffs:
  - Using exponential Euler vs standard gradient descent: More accurate dynamics but computationally expensive
  - Calculating full Hessian vs top eigenvalues only: Full Hessian is intractable, top eigenvalues give sufficient information for step size control
  - Using MLPs vs more complex architectures: MLPs allow clean layerwise analysis but may not capture all phenomena
- Failure signatures:
  - If sharpness doesn't increase during training: Model initialization or training configuration issue
  - If alignment ratios don't increase: Dataset complexity too low or model capacity insufficient
  - If exponential Euler becomes unstable: Learning rate too high or insufficient eigenvalues calculated
- First 3 experiments:
  1. Train a small MLP on synthetic dice data with cross-entropy loss, verify sharpness increases and alignment ratios grow
  2. Vary dataset size (e.g., 500, 1000, 2000 examples) and measure power law relationship for alignment
  3. Switch to MSE loss and verify same mechanisms operate with different scaling

## Open Questions the Paper Calls Out

- How does the layerwise Jacobian alignment phenomenon scale with network depth and width? The paper shows alignment increases with dataset size following a power law, but does not explore how network architecture affects this scaling. Systematic experiments varying network depth and width while measuring layerwise Jacobian alignment would clarify the architectural dependence.

- What is the relationship between layerwise Jacobian alignment and generalization performance? The paper hypothesizes that layerwise Jacobian alignment correlates with decision boundary complexity, but does not empirically verify this connection. Experiments correlating layerwise Jacobian alignment with generalization gap and decision boundary complexity metrics would establish this relationship.

- How do different activation functions affect the layerwise Jacobian alignment phenomenon? The experiments used ELU activation function, but the paper does not compare results with other activation functions. Training networks with different activation functions (ReLU, tanh, sigmoid) while measuring layerwise Jacobian alignment would reveal activation function dependencies.

## Limitations
- The study relies on synthetic and a limited set of real datasets, which may not fully represent the diversity of real-world problems
- Analysis is restricted to MLPs with ELU activation, potentially missing behaviors in more complex architectures
- The exponential Euler solver, while theoretically sound, introduces computational overhead and potential approximations

## Confidence
- High Confidence: The mathematical derivation showing how Hessian sharpness relates to layerwise Jacobian alignment
- Medium Confidence: The empirical observation that alignment ratios scale with dataset size following a power law (due to limited dataset diversity)
- Medium Confidence: The claim that exponential Euler solver accurately approximates gradient flow without edge of stability artifacts

## Next Checks
1. Test the alignment-scaling relationship across a broader range of dataset types and complexities (e.g., image, text, graph data) to verify generalizability
2. Replicate key findings using different network architectures (CNNs, Transformers) and activation functions to assess architectural dependence
3. Compare results between exponential Euler solver and standard gradient descent training, quantifying differences in measured alignment and sharpness metrics