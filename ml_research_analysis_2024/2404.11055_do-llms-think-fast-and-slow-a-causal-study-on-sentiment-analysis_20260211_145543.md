---
ver: rpa2
title: Do LLMs Think Fast and Slow? A Causal Study on Sentiment Analysis
arxiv_id: '2404.11055'
source_url: https://arxiv.org/abs/2404.11055
tags:
- causal
- review
- sentiment
- prompt
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a causal framework for sentiment analysis
  (SA) that distinguishes between two underlying causal processes: (1) review-driven
  sentiment (C1) where the review causes the sentiment, and (2) sentiment-driven review
  (C2) where the sentiment causes the review. The authors ground these processes in
  psychological theories of fast and slow thinking, using the peak-end rule to classify
  samples into C1 or C2 based on how well their overall sentiment aligns with either
  the average of all sentence-level sentiments or the average of peak and end sentiments.'
---

# Do LLMs Think Fast and Slow? A Causal Study on Sentiment Analysis

## Quick Facts
- arXiv ID: 2404.11055
- Source URL: https://arxiv.org/abs/2404.11055
- Reference count: 40
- Proposes causal framework distinguishing review-driven (C1) vs sentiment-driven (C2) sentiment analysis processes

## Executive Summary
This paper introduces a causal framework for sentiment analysis that distinguishes between two underlying processes: review-driven sentiment (C1) where the review causes the sentiment, and sentiment-driven review (C2) where the sentiment causes the review. The authors ground these processes in psychological theories of fast and slow thinking, using the peak-end rule to classify samples into C1 or C2 based on how well their overall sentiment aligns with either the average of all sentence-level sentiments or the average of peak and end sentiments. They propose causal prompts that align with the underlying causal direction, achieving up to 32.13 F1 points improvement on zero-shot five-class SA.

## Method Summary
The authors develop a causal framework for sentiment analysis by distinguishing between two processes: review-driven sentiment (C1) where the review causes the sentiment, and sentiment-driven review (C2) where the sentiment causes the review. They use the peak-end rule from psychology to classify samples into these categories based on alignment between overall sentiment and either the average of all sentence-level sentiments or the average of peak and end sentiments. The method involves constructing causal prompts that align with the underlying causal direction for each sample, then evaluating LLMs on these prompts. They also perform mechanistic interpretability to examine whether the models' internal representations align with the expected causal patterns.

## Key Results
- Achieved up to 32.13 F1 points improvement on zero-shot five-class SA using causal prompts
- LLMs perform better on C2 data under standard prompts, suggesting bias toward sentiment-driven generation
- Causal prompts substantially improve performance across both C1 and C2 samples
- Mechanistic interpretability reveals models still struggle to fully capture expected causal patterns despite prompt improvements

## Why This Works (Mechanism)
The framework works by explicitly aligning the prompting strategy with the underlying causal mechanism that generated each sentiment sample. By recognizing that sentiment can be either driven by the content of the review (C1 - slow thinking) or predetermined by the reviewer's emotional state (C2 - fast thinking), the method can better guide LLMs to extract the intended sentiment signal. The peak-end rule provides a principled way to classify samples into these categories based on how sentiment is distributed across the review text, allowing for targeted prompt engineering that respects the causal structure.

## Foundational Learning

**Causal inference in NLP**: Understanding how to model and reason about cause-effect relationships in language processing tasks. Needed to distinguish between different generative processes for sentiment. Quick check: Can identify whether a sentiment was generated from review content or predetermined emotion.

**Peak-end rule**: Psychological principle that people judge experiences largely based on how they felt at their peak and at the end. Needed to classify samples into C1 vs C2 categories. Quick check: Can compute whether overall sentiment aligns better with average sentiment or peak-end sentiment.

**Zero-shot learning**: Ability to perform tasks without task-specific training examples. Needed to evaluate the general effectiveness of causal prompts. Quick check: Can complete sentiment analysis tasks using only carefully crafted prompts without fine-tuning.

## Architecture Onboarding

**Component map**: Input reviews → Peak-end analysis → C1/C2 classification → Causal prompt generation → LLM prediction → Evaluation metrics

**Critical path**: Review → Sentiment classification (C1/C2) → Appropriate causal prompt → LLM output → Performance measurement

**Design tradeoffs**: Binary classification of causal mechanisms vs. continuous spectrum; zero-shot approach vs. fine-tuning; prompt complexity vs. generalization

**Failure signatures**: Misclassification of C1/C2 samples leads to suboptimal prompts; peak-end rule may not capture all sentiment generation mechanisms; interpretability analysis shows incomplete alignment with causal expectations

**First experiments**: 1) Classify samples using peak-end rule vs average sentiment alignment; 2) Test standard vs causal prompts on both C1 and C2 samples; 3) Perform mechanistic interpretability on LLM representations for causal alignment

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Binary causal framework (C1 vs C2) may oversimplify complex human sentiment generation processes
- Peak-end rule represents only one mechanism for sentiment formation, potentially missing other pathways
- Empirical validation relies on small test set of 100 samples, limiting statistical power

## Confidence
- High confidence in theoretical framework and psychological grounding
- Medium confidence in empirical results due to limited test set size
- Medium confidence in mechanistic interpretability findings showing partial but incomplete causal alignment

## Next Checks
1. Replicate experiments on larger, more diverse test set (minimum 500 samples) to verify stability of reported improvements
2. Test causal framework across multiple languages and cultural contexts to assess generalizability
3. Develop methods for handling samples with mixed causal mechanisms (both C1 and C2)