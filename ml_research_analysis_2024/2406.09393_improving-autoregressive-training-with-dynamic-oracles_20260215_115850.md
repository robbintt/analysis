---
ver: rpa2
title: Improving Autoregressive Training with Dynamic Oracles
arxiv_id: '2406.09393'
source_url: https://arxiv.org/abs/2406.09393
tags:
- dynamic
- sequence
- oracle
- word
- gold
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the challenge of training autoregressive models\
  \ in NLP tasks where the evaluation metrics (like span-based F1, ROUGE, BLEU) differ\
  \ from the training loss, leading to exposure bias and metric mismatch. The authors\
  \ propose using DAgger with dynamic oracles\u2014metrics-specific expert policies\
  \ that guide model training by providing optimal completions for partial sequences."
---

# Improving Autoregressive Training with Dynamic Oracles

## Quick Facts
- arXiv ID: 2406.09393
- Source URL: https://arxiv.org/abs/2406.09393
- Authors: Jianing Yang; Harshine Visvanathan; Yilin Wang; Xinyi Hu; Matthew Gormley
- Reference count: 40
- Primary result: Dynamic oracles with DAgger improve autoregressive training for NER and summarization tasks by 1.88 F1 points and 1.20 ROUGE-2 points respectively, but show mixed results for machine translation.

## Executive Summary
This paper addresses the fundamental mismatch between training loss functions and evaluation metrics in autoregressive sequence generation tasks. The authors propose using DAgger (Dataset Aggregation) with dynamic oracles - metric-specific expert policies that provide optimal completions for partial sequences during training. For decomposable metrics like span-based F1, they develop exact dynamic oracles, while for non-decomposable metrics like BLEU and ROUGE, they use approximate oracles computed via beam search. Experiments across three tasks (NER, MT, summarization) demonstrate that DAgger with dynamic oracles consistently outperforms teacher forcing and scheduled sampling on NER and summarization, though less favorably on machine translation.

## Method Summary
The authors develop dynamic oracle algorithms that guide autoregressive models during training by providing metric-optimized supervision for partial sequences. For decomposable metrics like span-based F1, they compute exact dynamic oracles that evaluate each token's contribution to the metric. For non-decomposable metrics (BLEU, ROUGE), they use beam search to approximate optimal completions. The DAgger algorithm iteratively trains the model on its own predictions (model-generated prefixes) rather than ground truth, using the dynamic oracle to generate supervision that optimizes the test-time metric. This approach directly addresses exposure bias and metric mismatch by training under conditions that mirror inference.

## Key Results
- DAgger with exact dynamic oracle improves partial F1 by up to 1.88 points on NER tasks compared to teacher forcing
- For summarization, ROUGE-2 improves by up to 1.20 points using approximate dynamic oracles
- Mixed results on machine translation where teacher forcing remains competitive with DAgger approach
- Approximate dynamic oracles using beam search show diminishing returns beyond beam size 20 for BLEU/ROUGE metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic oracles guide autoregressive models toward metric-optimized completions by providing exact supervision for partial sequences.
- Mechanism: At each decoding step, the dynamic oracle evaluates the current partial output against the gold sequence and selects the completion that minimizes the loss under the task-specific metric. This replaces static ground-truth supervision with metric-specific guidance.
- Core assumption: The dynamic oracle can compute the optimal completion given a partial sequence and the gold sequence.
- Evidence anchors:
  - [abstract] "DAgger provides a solution to mitigate these problems, yet it requires a metric-specific dynamic oracle algorithm, which does not exist for many common metrics like span-based F1, ROUGE, and BLEU."
  - [section] "The term dynamic oracle...represents the expert policy employed by DAgger. This dynamic oracle function acts as a guide to answer the question: Given a partial output sequence, which completion minimizes the loss when compared to the gold standard output?"
  - [corpus] Weak corpus evidence: neighbor papers focus on oracle computation for general optimization problems, not NLP sequence tasks. No direct NLP metric oracle evidence found.
- Break condition: For non-decomposable metrics like BLEU or ROUGE, the dynamic oracle becomes approximate and loses the no-regret guarantee, making it suboptimal compared to exact oracles.

### Mechanism 2
- Claim: Decomposable metrics enable exact dynamic oracle computation, while non-decomposable metrics require approximate solutions via beam search.
- Mechanism: For decomposable metrics like span-based F1, the dynamic oracle can be computed exactly by evaluating each token's contribution to the metric. For non-decomposable metrics, beam search approximates the optimal completion by exploring multiple candidate sequences.
- Core assumption: Decomposable metrics can be expressed as additive functions of individual tokens and partial sequences, enabling exact computation.
- Evidence anchors:
  - [section] "A decomposable metric can be expressed in the following form: score(x, y) = sum over t of score(x1:t-1 â—¦ xt, y)"
  - [section] "For decomposable metrics, we define the exact dynamic oracle to be the completion that gives the highest possible score given a partial sequence."
  - [corpus] Weak corpus evidence: neighbor papers discuss oracle computation for general optimization, not metric decomposition properties.
- Break condition: Beam search approximation quality degrades with metric complexity and sequence length, potentially missing the true optimal completion.

### Mechanism 3
- Claim: DAgger with dynamic oracles mitigates exposure bias and metric mismatch by training on model-generated prefixes with metric-optimized supervision.
- Mechanism: During training, DAgger uses the model's own predictions as input prefixes and applies the dynamic oracle to generate supervision that optimizes the test-time metric, rather than using static ground truth.
- Core assumption: Training on model-generated prefixes with dynamic oracle supervision better prepares the model for inference conditions than teacher forcing or scheduled sampling.
- Evidence anchors:
  - [abstract] "DAgger with dynamic oracle yields less favorable results in our MT experiments, but it outperforms the baseline techniques in NER and text summarization."
  - [section] "Scheduled sampling still relies on the ground truth as output supervision, even when the input prefix is no longer the ground truth. This approach can lead to suboptimal supervision during training."
  - [corpus] Weak corpus evidence: neighbor papers focus on oracle-based optimization but not on exposure bias or metric mismatch in NLP training.
- Break condition: If the dynamic oracle approximation is poor or the metric is non-decomposable, the training supervision may be misleading, leading to worse performance than simpler baselines.

## Foundational Learning

- Concept: Decomposable vs. non-decomposable metrics
  - Why needed here: Determines whether exact or approximate dynamic oracle computation is possible
  - Quick check question: Can you express ROUGE as a sum of individual token contributions? (No - it's non-decomposable due to global n-gram counts)

- Concept: Exposure bias in autoregressive training
  - Why needed here: Explains why teacher forcing fails at inference and why DAgger's approach is beneficial
  - Quick check question: What happens when a model trained with teacher forcing encounters its own prediction errors during inference? (It struggles because it was never trained to handle such errors)

- Concept: Beam search as an approximation method
  - Why needed here: Provides the mechanism for computing approximate dynamic oracles for non-decomposable metrics
  - Quick check question: How does beam search balance exploration and computational cost when searching for optimal completions? (It maintains a fixed-size set of top-scoring candidates at each step)

## Architecture Onboarding

- Component map: Encoder -> Decoder (autoregressive) -> Dynamic Oracle -> Training Loop
- Critical path:
  1. Initialize model with teacher forcing for warm-start
  2. At each training step, generate model predictions as prefixes
  3. Apply dynamic oracle to compute optimal completions
  4. Use dynamic oracle completions as supervision for cross-entropy loss
  5. Update model parameters via backpropagation
- Design tradeoffs:
  - Exact vs. approximate dynamic oracles: Exact oracles guarantee no-regret but only work for decomposable metrics; approximate oracles work for all metrics but lose guarantees
  - Beam size in approximate oracles: Larger beams improve approximation quality but increase computational cost
  - Warm-start duration: Longer teacher forcing warm-start may improve convergence but delays exposure to metric-optimized supervision
- Failure signatures:
  - Model performance worse than teacher forcing: Dynamic oracle approximation quality is poor or metric is non-decomposable
  - Training instability: Dynamic oracle introduces noisy supervision or incorrect completions
  - Slow convergence: Warm-start duration too short or beam search inefficient
- First 3 experiments:
  1. Implement exact dynamic oracle for span-based F1 on NER task and compare against teacher forcing baseline
  2. Add approximate dynamic oracle for ROUGE on summarization task with varying beam sizes
  3. Test DAgger with exact dynamic oracle on machine translation with BLEU metric to identify non-decomposable metric challenges

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DAgger with dynamic oracles compare when using larger beam sizes (beyond beam size 20) for non-decomposable metrics like BLEU and ROUGE?
- Basis in paper: [inferred] The paper notes that improvements begin to plateau beyond a beam size of 20, but does not explore larger beam sizes.
- Why unresolved: The authors did not test beam sizes larger than 20, leaving uncertainty about potential gains from even larger beam sizes.
- What evidence would resolve it: Experimental results comparing DAgger performance with beam sizes greater than 20 on tasks using BLEU and ROUGE metrics.

### Open Question 2
- Question: Would initiating DAgger training in earlier epochs consistently yield better dynamic oracle quality across different tasks and metrics?
- Basis in paper: [inferred] The analysis suggests that starting DAgger in early epochs gives better dynamic oracles, but the experiments started DAgger at different epochs for different tasks.
- Why unresolved: The paper does not provide a systematic comparison of starting DAgger at different epochs across multiple tasks.
- What evidence would resolve it: Comparative experiments showing DAgger performance when initiated at various early epochs across different tasks and metrics.

### Open Question 3
- Question: How would the runtime of the dynamic oracle for BLEU score change if optimized using multi-threading or other computational techniques?
- Basis in paper: [explicit] The authors mention that the runtime is approximately 6 times longer than teacher forcing and suggest multi-threading as a potential optimization.
- Why unresolved: The paper does not implement or measure the impact of multi-threading or other optimizations on runtime.
- What evidence would resolve it: Empirical data comparing the runtime of the dynamic oracle with and without multi-threading or other optimizations.

## Limitations

- Computational overhead from dynamic oracle computation is substantial, particularly for non-decomposable metrics using beam search
- Effectiveness of approximate dynamic oracles for BLEU and ROUGE is uncertain due to loss of theoretical guarantees
- Span-based F1 oracle assumes perfect metric decomposition which may not hold for all NER variants

## Confidence

- **High confidence**: Exact dynamic oracle computation for decomposable metrics (span-based F1) is well-defined and theoretically sound with substantial experimental improvements on NER tasks
- **Medium confidence**: Approximate dynamic oracle computation using beam search for non-decomposable metrics (BLEU, ROUGE) is reasonable but loses theoretical guarantees, with reduced effectiveness on machine translation suggesting approximation quality degrades for complex metrics
- **Low confidence**: The claim that DAgger with dynamic oracles consistently outperforms baselines across all tasks is not supported by experimental results, particularly for machine translation where teacher forcing remains competitive

## Next Checks

1. **Computational overhead analysis**: Measure and report the training time per epoch for DAgger with dynamic oracles versus teacher forcing baselines across all tasks. Quantify the beam search computation cost for BLEU/ROUGE oracles and assess whether the metric improvements justify the additional runtime.

2. **Approximate oracle quality validation**: Implement oracle quality metrics that measure how close the approximate dynamic oracle completions are to the optimal completion. Track the BLEU/ROUGE scores of dynamic oracle outputs versus ground truth during training to quantify approximation degradation.

3. **Hyperparameter sensitivity analysis**: Conduct experiments varying beam size, beam length, and warm-start duration for DAgger across tasks. Determine whether the observed performance differences are due to dynamic oracle quality or suboptimal hyperparameter choices specific to each task.