---
ver: rpa2
title: 'DCR: Divide-and-Conquer Reasoning for Multi-choice Question Answering with
  LLMs'
arxiv_id: '2401.05190'
source_url: https://arxiv.org/abs/2401.05190
tags:
- arxiv
- reasoning
- confidence
- choices
- choice
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a divide-and-conquer approach for enhancing\
  \ large language models\u2019 (LLMs) reasoning performance on multi-choice questions\
  \ (MCQs). The method divides questions into three confidence-based subsets and applies\
  \ different reasoning strategies to each subset: high-confidence questions are left\
  \ unchanged, medium-confidence questions are handled with filter choices reasoning,\
  \ and low-confidence questions are addressed with prior knowledge based reasoning."
---

# DCR: Divide-and-Conquer Reasoning for Multi-choice Question Answering with LLMs

## Quick Facts
- arXiv ID: 2401.05190
- Source URL: https://arxiv.org/abs/2401.05190
- Reference count: 13
- Improves LLM multi-choice question answering accuracy by 1.56% on average across nine datasets

## Executive Summary
This paper proposes a divide-and-conquer approach for enhancing large language models' reasoning performance on multi-choice questions (MCQs). The method divides questions into three confidence-based subsets and applies different reasoning strategies to each subset: high-confidence questions are left unchanged, medium-confidence questions are handled with filter choices reasoning, and low-confidence questions are addressed with prior knowledge based reasoning. The approach achieves an average accuracy improvement of 1.56% across nine datasets spanning arithmetic, commonsense, and logic tasks, while only requiring 85% of the computational cost compared to state-of-the-art methods.

## Method Summary
The DCR method divides questions into three confidence-based subsets using self-consistency to compute confidence scores. High-confidence questions (>μ) are left unchanged, medium-confidence questions (ν<CS≤μ) use filter choices reasoning (FCR), and low-confidence questions (CS≤ν) use prior knowledge reasoning (PKR). FCR constructs a new choices list by deduplicating answers from the divide stage, while PKR selects the longest rationale from the divide stage to guide reasoning. The approach was evaluated on nine datasets including AQuA, Algebra, CMSQA, OpenBookQA, ARC Challenge, RiddleSense, LogiDeduction, and Reclor.

## Key Results
- Achieves 1.56% average accuracy improvement across nine datasets
- Low-confidence subsets show 6.22-15.07% improvements depending on dataset
- Only requires 85% of computational cost compared to state-of-the-art methods
- Demonstrates consistent improvements across arithmetic, commonsense, and logic tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The method works by dividing questions into three confidence-based subsets and applying targeted reasoning strategies to each subset.
- **Mechanism**: The system uses self-consistency to compute a confidence score for each question based on the frequency of generated answers. Questions are then categorized into high-confidence, medium-confidence, and low-confidence subsets. High-confidence questions are left unchanged, medium-confidence questions use filter choices reasoning, and low-confidence questions use prior knowledge-based reasoning.
- **Core assumption**: Questions can be effectively categorized by difficulty using confidence scores derived from self-consistency, and different reasoning strategies are needed for different difficulty levels.
- **Evidence anchors**: [abstract]: "This paper proposes a divide-and-conquer approach for enhancing large language models' (LLMs) reasoning performance on multi-choice questions (MCQs)." [section]: "We first categorize questions into two subsets based on confidence score (CS), which is estimated by statistical frequency of generated answers."

### Mechanism 2
- **Claim**: Filtering choices based on prior inference results reduces irrelevant information and improves reasoning accuracy.
- **Mechanism**: The system constructs a new choices list for each question by deduplicating the answers generated during the divide stage. This filtered choices list is then used in subsequent reasoning, reducing the cognitive load on the model by removing irrelevant options.
- **Core assumption**: Irrelevant information in the choices list distracts the model and reduces reasoning accuracy, and removing these options will improve performance.
- **Evidence anchors**: [section]: "We make use of the results obtained during divide phase as alternative options for the subsequent queries of the LLM." [section]: "We conducted preliminary studies on four MCQs datasets... discovering a decrease in problem-solving accuracy as the number of choices increased."

### Mechanism 3
- **Claim**: Longer rationales provide more helpful knowledge for reference and prevent models from relying on harmful shortcuts.
- **Mechanism**: The system selects the longest rationale among those generated during the divide stage that correspond to the same answer. This longer rationale is then used as prior knowledge to guide subsequent reasoning, providing more detailed context for the model.
- **Core assumption**: Longer rationales contain more helpful knowledge that can guide reasoning, while shorter rationales may contain harmful shortcuts learned from training data.
- **Evidence anchors**: [section]: "We conducted extensive analysis on rationale length and verify that longer ones could prevent models from referring shortcuts generated by the influence of training data."

## Foundational Learning

- **Concept**: Self-consistency in LLMs
  - Why needed here: The method relies on self-consistency to compute confidence scores by sampling multiple reasoning paths and using majority voting.
  - Quick check question: How does self-consistency improve LLM reasoning compared to greedy decoding?

- **Concept**: Chain-of-Thought (CoT) prompting
  - Why needed here: The method builds upon CoT prompting by generating intermediate reasoning steps before producing final answers.
  - Quick check question: What is the difference between Zero-Shot-CoT and Few-Shot-CoT approaches?

- **Concept**: Multi-choice question answering
  - Why needed here: The entire method is designed specifically for multi-choice question answering tasks across various domains.
  - Quick check question: What are the key challenges in multi-choice question answering compared to open-ended question answering?

## Architecture Onboarding

- **Component map**: Question → Zero-Shot-CoT (multiple inferences) → Confidence score calculation → Subset categorization → Targeted reasoning strategy → Final answer
- **Critical path**: Question → Zero-Shot-CoT (multiple inferences) → Confidence score calculation → Subset categorization → Targeted reasoning strategy → Final answer
- **Design tradeoffs**:
  - Computational cost vs accuracy: Using self-consistency for confidence scoring increases cost but improves accuracy
  - Prompt length vs effectiveness: Longer rationales provide more context but may exceed model context limits
  - Choice filtering vs coverage: Removing choices reduces distraction but may eliminate correct answers
- **Failure signatures**:
  - Low correlation between confidence scores and actual accuracy
  - Performance degradation when applying PKR or FCR to subsets
  - Inconsistent results across different datasets or question types
  - Context window overflow when using longer rationales
- **First 3 experiments**:
  1. Test confidence score correlation: Run Zero-Shot-CoT multiple times on a small dataset and measure correlation between confidence scores and actual accuracy
  2. Validate choice filtering effectiveness: Compare accuracy with full choices vs filtered choices on medium confidence subset
  3. Test rationale length impact: Compare PKR performance using longest vs shortest vs random rationale selection on low confidence subset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal threshold for dividing confidence subsets (μ and ν) across different types of reasoning tasks?
- Basis in paper: [explicit] The paper mentions using μ=0.8 and ν=0.6 but states "The values for μ and ν are chosen from the quintile points with referring #DivideBase"
- Why unresolved: The paper only tested a specific pair of thresholds without exploring how different threshold values affect performance across various reasoning tasks (arithmetic, commonsense, logic). The selection criteria based on quintile points is not fully explained.
- What evidence would resolve it: Systematic experiments testing multiple threshold combinations across all nine datasets, with analysis of how threshold selection affects accuracy gains and computational efficiency for each task category.

### Open Question 2
- Question: How does the performance of DCR change with different LLM architectures beyond GPT-3.5-Turbo?
- Basis in paper: [explicit] The paper tested GPT-3.5-Turbo, Palm2, and Vicuna v1.5 but only for the low-confidence subset of two datasets
- Why unresolved: The paper only provides limited cross-model validation. It's unclear how DCR would perform with larger models like GPT-4 or open-source alternatives with different architectural properties.
- What evidence would resolve it: Comprehensive evaluation of DCR across multiple LLM architectures (different parameter counts, training methodologies, and reasoning capabilities) on all nine datasets.

### Open Question 3
- Question: What is the relationship between rationale length and reasoning quality in PKR, and is there an optimal length?
- Basis in paper: [explicit] "We conducted extensive analysis on rationale length and verify that longer ones could prevent models from referring shortcuts generated by the influence of training data"
- Why unresolved: The paper only compared shortest vs. longest rationales without exploring intermediate lengths or establishing a clear relationship between rationale length and reasoning quality.
- What evidence would resolve it: Experiments testing PKR performance with rationales of varying lengths (e.g., 10%, 25%, 50%, 75%, 100% of maximum length) across all datasets, with analysis of the trade-off between rationale length and reasoning accuracy.

## Limitations
- The confidence scoring mechanism relies entirely on self-consistency, which introduces computational overhead and potential instability
- The method's performance gains come at the cost of increased computational complexity during the divide stage
- The effectiveness assumes self-consistency scores reliably correlate with actual question difficulty across all question types

## Confidence
*High Confidence*: The core mechanism of dividing questions based on confidence scores and applying different reasoning strategies is well-supported by the empirical results showing consistent accuracy improvements across all nine datasets tested.

*Medium Confidence*: The claim that filtering choices based on prior inference results improves reasoning accuracy is supported by preliminary studies but lacks comprehensive ablation testing.

*Medium Confidence*: The rationale length analysis showing that longer rationales prevent harmful shortcuts is based on internal experiments, but the generalizability of this finding to different model architectures and question types remains uncertain.

## Next Checks
1. **Confidence Score Reliability Test**: Systematically measure the correlation between self-consistency confidence scores and actual accuracy across different question categories to validate the subset division strategy.

2. **Computational Cost Breakdown**: Conduct detailed profiling to verify the claimed 85% computational cost relative to state-of-the-art methods, including both inference time and memory usage across all stages.

3. **Cross-Domain Generalization**: Test the DCR approach on additional datasets outside the nine used in the study to assess whether the confidence-based division strategy maintains effectiveness across broader question types and difficulty distributions.