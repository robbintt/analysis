---
ver: rpa2
title: Vanilla Bayesian Optimization Performs Great in High Dimensions
arxiv_id: '2402.02229'
source_url: https://arxiv.org/abs/2402.02229
tags:
- optimization
- bayesian
- complexity
- vanilla
- high
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates why vanilla Bayesian optimization (BO) struggles
  in high-dimensional settings and proposes a simple fix. The core issue is that high-dimensional
  spaces cause GP lengthscales to be too short, leading to low correlation between
  data points and an overly complex model that hinders learning.
---

# Vanilla Bayesian Optimization Performs Great in High Dimensions

## Quick Facts
- arXiv ID: 2402.02229
- Source URL: https://arxiv.org/abs/2402.02229
- Authors: Carl Hvarfner; Erik Orm Hellsten; Luigi Nardi
- Reference count: 40
- The paper demonstrates that vanilla Bayesian optimization with a simple lengthscale scaling heuristic outperforms state-of-the-art high-dimensional BO methods on benchmarks up to 6392 dimensions.

## Executive Summary
This paper challenges the conventional wisdom that Bayesian optimization (BO) inherently struggles in high-dimensional spaces. The authors demonstrate that the core issue is not the curse of dimensionality itself, but rather the excessive model complexity assumed by vanilla BO. By scaling the Gaussian Process lengthscale prior by the square root of the dimensionality, they achieve global modeling that outperforms specialized high-dimensional BO methods without requiring structural assumptions like subspace embeddings or local modeling.

## Method Summary
The method modifies vanilla BO by scaling the GP lengthscale prior by √D for each dimension, using a LogNormal prior with base parameters µ0 = √2 and σ0 = √3. The GP uses an RBF or Matern-5/2 kernel with fixed signal variance σ²f = 1, initialized with 30 Sobol samples, and optimizes the Expected Improvement acquisition function through multi-start gradient optimization. This simple modification enables global modeling of high-dimensional functions without the complexity explosion that typically plagues vanilla BO in high dimensions.

## Key Results
- DSP (dimensionality-scaled prior) outperforms state-of-the-art HDBO methods on 24D and 389D synthetic benchmarks
- On real-world tasks, DSP significantly outperforms competitors on 388D SVM and 2250D MOPTA08 problems
- DSP achieves competitive performance on extreme-scale problems (6392D MuJoCo Ant, 6392D MuJoCo Humanoid) where other methods fail

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High dimensionality alone doesn't cause BO failure; it's the assumed model complexity that does.
- Mechanism: Vanilla BO assumes a high-complexity Gaussian process (GP) with short lengthscales, leading to low correlation between data points. This causes repeated queries near the incumbent instead of global exploration.
- Core assumption: The covariance function (e.g., RBF kernel) computes distances that grow with dimensionality, and standard priors place high density on short lengthscales.
- Evidence anchors:
  - [abstract]: "high-complexity, low-correlation issue presents itself most clearly in the high-dimensional setting"
  - [section 4.1]: "Assuming that all dimensions are of major importance may appear like a conservative and sensible choice. For moderately high dimensions, however, it practically guarantees that the problem will be impossible to model globally"
  - [corpus]: No direct evidence in corpus papers; this is the novel claim of the paper.
- Break condition: If the objective function truly has many important dimensions or is highly complex, scaling lengthscales may oversimplify the model and hurt performance.

### Mechanism 2
- Claim: The expected improvement (EI) acquisition function doesn't intrinsically seek high-variance boundary points in high dimensions.
- Mechanism: When the GP is uninformed (K ≈ I), EI prefers points with substantial correlation to the incumbent, leading to local search behavior rather than boundary exploration.
- Core assumption: The model is uninformed, meaning observations are nearly independent under the current lengthscale setting.
- Evidence anchors:
  - [section 4.2]: "Proposition 4.1 demonstrates that, when correlation in the model is low, EI does not seek out high-variance regions as described by Swersky (2017)"
  - [section 4.2]: "Rather, queries preferentially have substantial correlation with the incumbent"
  - [corpus]: No direct evidence in corpus papers; this contradicts the common belief about boundary issues.
- Break condition: If lengthscales are very long along some dimensions, EI may query boundary points that are low-variance but highly correlated with existing data.

### Mechanism 3
- Claim: Scaling the GP lengthscale prior by √D ensures manageable complexity growth and meaningful correlation between data points.
- Mechanism: By increasing lengthscales at a rate proportional to √D, the method counteracts the distance growth between points in high dimensions, maintaining correlation and preventing the GP from reverting to its prior.
- Core assumption: The objective function is simple enough to be modeled globally, and all dimensions have relatively small impact.
- Evidence anchors:
  - [section 5.1]: "increasing the lengthscales at this rate, ℓi√D, counteracts the complexity increase that stems from the increased distances"
  - [section 5.1]: "Our method takes the opposite approach, and simply assumes that a problem is simple enough to be modeled globally, for any dimensionality"
  - [section 6.3]: "The DSP does not heavily depend on identification of active variables"
- Break condition: If the objective has a low number of highly important dimensions (sparse structure), this assumption may be too strong and hurt performance.

## Foundational Learning

- Concept: Gaussian Process (GP) regression and kernel functions
  - Why needed here: The paper's core mechanism relies on understanding how GPs model functions and how kernel hyperparameters (especially lengthscales) affect correlation between data points.
  - Quick check question: How does the RBF kernel compute covariance between two points, and what role do lengthscales play in this computation?

- Concept: Bayesian Optimization (BO) and acquisition functions
  - Why needed here: The paper proposes a modification to vanilla BO, so understanding the standard BO setup (surrogate model, acquisition function) is crucial.
  - Quick check question: What is the purpose of the acquisition function in BO, and how does Expected Improvement (EI) balance exploration and exploitation?

- Concept: Curse of Dimensionality (CoD) and its implications
  - Why needed here: The paper argues that CoD is not the real issue for BO in high dimensions, but rather the assumed complexity. Understanding CoD helps grasp this distinction.
  - Quick check question: How does the expected distance between random points in a unit cube scale with dimensionality, and why is this relevant for BO?

## Architecture Onboarding

- Component map: Surrogate model (GP with scaled lengthscale prior) -> Acquisition function (EI) -> Hyperparameter optimization (MAP estimation) -> Data preprocessing (standardization) -> Model calibration (fixed σ²f)

- Critical path: 1) Initialize with 30 Sobol samples 2) Fit GP with scaled lengthscale prior 3) Optimize EI acquisition function 4) Evaluate selected point and add to data 5) Repeat until budget exhausted

- Design tradeoffs:
  - Complexity vs. performance: Assuming lower complexity enables global modeling but may oversimplify complex objectives
  - Exploration vs. exploitation: Scaled lengthscales promote more global exploration but may reduce local refinement
  - Fixed vs. learned signal variance: Fixing σ²f prevents degeneracy but may limit model flexibility

- Failure signatures:
  - Premature convergence to suboptimal solutions (too low complexity)
  - Poor performance on objectives with sparse active dimensions (assumption mismatch)
  - Instability in hyperparameter fitting (overparameterized regime)

- First 3 experiments:
  1. Run DSP on a simple synthetic function (e.g., 6D Hartmann) to verify it outperforms conventional MAP
  2. Test DSP on a high-dimensional real-world task (e.g., 388D SVM) to confirm scalability
  3. Compare DSP with and without learned signal variance on a mid-dimensional task to validate the fixed σ²f choice

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical upper bound on the maximum dimensionality at which the scaled lengthscale prior approach can effectively prevent the MIG from approaching independence?
- Basis in paper: [inferred] The paper shows that for D=24 with 5000 samples, the MIG closely follows an independent kernel, suggesting the approach's limitations at very high dimensions.
- Why unresolved: The paper demonstrates effectiveness up to 6392D but does not establish a theoretical limit where the approach would fail.
- What evidence would resolve it: Mathematical proof or empirical testing showing the maximum dimensionality where the scaled lengthscale maintains meaningful correlation.

### Open Question 2
- Question: How does the performance of vanilla BO with scaled lengthscales compare to specialized HDBO methods when the objective function has significant non-stationarity across the search space?
- Basis in paper: [inferred] The paper shows vanilla BO outperforms state-of-the-art methods but doesn't test against scenarios with non-stationary objectives where local methods might excel.
- Why unresolved: The benchmarks used are either stationary or don't explicitly test non-stationary behavior that could favor local modeling approaches.
- What evidence would resolve it: Comparative experiments on non-stationary high-dimensional functions showing relative performance.

### Open Question 3
- Question: What is the optimal scaling factor for the lengthscale prior when the effective dimensionality of the problem is known to be much lower than the ambient dimensionality?
- Basis in paper: [explicit] The authors use a square root scaling but note this is a conservative assumption that may not be optimal when only a few dimensions are truly important.
- Why unresolved: The paper assumes all dimensions are equally important and scales accordingly, but doesn't explore what happens when effective dimensionality is known to be low.
- What evidence would resolve it: Experiments varying the scaling factor based on known effective dimensionality to find optimal performance.

## Limitations

- The low complexity assumption may not hold for problems with sparse active dimensions or highly non-stationary behavior
- Fixed signal variance (σ²f = 1) prevents degeneracy but may limit model flexibility on certain objective landscapes
- The √D scaling heuristic, while effective empirically, lacks formal theoretical justification for all function classes

## Confidence

- High Confidence: The explanation of why vanilla BO fails in high dimensions (Mechanism 1)
- Medium Confidence: The claim that boundary issues are not the primary concern when the model is uninformed (Mechanism 2)
- Medium Confidence: The effectiveness of √D scaling across diverse function classes (Mechanism 3)

## Next Checks

1. Test DSP on problems with known sparse active dimensions to evaluate performance when the low-complexity assumption is violated
2. Compare performance with learned signal variance vs. fixed σ²f = 1 on a suite of mid-dimensional benchmarks
3. Investigate the theoretical properties of the √D scaling heuristic for different kernel classes and function regularity assumptions