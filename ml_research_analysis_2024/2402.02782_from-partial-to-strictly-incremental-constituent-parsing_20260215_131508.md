---
ver: rpa2
title: From Partial to Strictly Incremental Constituent Parsing
arxiv_id: '2402.02782'
source_url: https://arxiv.org/abs/2402.02782
tags:
- incremental
- parsing
- linguistics
- computational
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work evaluates strictly incremental constituent parsing by\
  \ integrating generative language models as encoders with two types of decoders\u2014\
  sequence labeling and transition-based\u2014across multiple languages. All components\
  \ process input strictly left-to-right, with optional small lookahead delays (k=0,1,2)\
  \ to mimic human reading."
---

# From Partial to Strictly Incrementally Constituent Parsing
## Quick Facts
- arXiv ID: 2402.02782
- Source URL: https://arxiv.org/abs/2402.02782
- Reference count: 14
- Incremental encoders significantly underperform bidirectional ones in strictly incremental constituent parsing.

## Executive Summary
This work evaluates strictly incremental constituent parsing by integrating generative language models as encoders with two types of decoders—sequence labeling and transition-based—across multiple languages. All components process input strictly left-to-right, with optional small lookahead delays (k=0,1,2) to mimic human reading. Experiments compare fully incremental models to non-incremental and partially incremental baselines. Results show that incremental encoders significantly degrade performance compared to bidirectional ones, confirming findings in dependency parsing. Among encoders, mGPT outperforms BLOOM and LSTM, especially on languages it was pre-trained on. Incremental decoders achieve reasonable F1 scores, with transition-based models outperforming sequence labeling by ~10 F1 points. Small delays improve performance, especially for longer constituents, though gains plateau quickly. The main bottleneck for strong incrementality lies in encoding; improving incremental encoders is key for closing the performance gap with non-incremental parsers.

## Method Summary
The paper evaluates strictly incremental constituent parsing using generative language models as encoders with two decoder types: sequence labeling and transition-based. All components process input strictly left-to-right, with optional lookahead delays (k=0,1,2). Experiments compare fully incremental models to non-incremental and partially incremental baselines across English, French, German, Spanish, and Vietnamese. Multiple encoder architectures are tested (mGPT, BLOOM, LSTM), along with different decoding strategies. The evaluation measures performance degradation when enforcing strict incrementality and investigates whether small delays can mitigate this gap.

## Key Results
- Incremental encoders significantly underperform bidirectional ones across all tested languages
- Transition-based decoders outperform sequence labeling decoders by ~10 F1 points in incremental settings
- Small lookahead delays (k=1,2) provide performance improvements that plateau quickly
- mGPT encoder outperforms BLOOM and LSTM, especially on languages it was pre-trained on

## Why This Works (Mechanism)
The paper demonstrates that strict incrementality in constituent parsing creates significant performance bottlenecks primarily at the encoding stage. While incremental decoders can achieve reasonable performance, the inability of unidirectional encoders to capture necessary context for phrase structure construction creates the main performance gap. Small lookahead delays help by allowing the model to gather more context before making parsing decisions, though the benefits diminish quickly. The superiority of transition-based decoders suggests that explicit action sequences better handle incremental parsing constraints than direct labeling approaches.

## Foundational Learning
- **Constituent parsing**: Task of identifying hierarchical phrase structure in sentences. Why needed: Forms the basis of the parsing task being evaluated. Quick check: Can you explain the difference between constituent and dependency parsing?
- **Incremental parsing**: Processing input strictly left-to-right without future context. Why needed: Central constraint being evaluated. Quick check: What's the difference between strictly incremental and partially incremental parsing?
- **Lookahead delays**: Allowing parser to process k additional tokens before making decisions. Why needed: Mechanism for bridging gap between incremental and non-incremental performance. Quick check: How do lookahead delays affect the incrementality of parsing?
- **Sequence labeling vs transition-based decoding**: Two approaches for generating parse trees from encoder representations. Why needed: Different strategies for handling incremental constraints. Quick check: What are the trade-offs between these two decoding approaches?
- **Generative language models as encoders**: Using models like mGPT and BLOOM for feature extraction. Why needed: Novel encoder choice for constituent parsing. Quick check: How do unidirectional LMs differ from bidirectional ones for parsing?

## Architecture Onboarding
**Component map**: Sentence -> Encoder (mGPT/BLOOM/LSTM) -> Decoder (Sequence Labeling/Transition-based) -> Parse Tree
**Critical path**: Encoder representation -> Decoder action/decision -> Parse structure
**Design tradeoffs**: Strict incrementality vs performance, lookahead delay length vs processing delay, encoder architecture vs language coverage
**Failure signatures**: High performance degradation with incremental encoders, transition-based decoders outperforming sequence labeling, diminishing returns from lookahead delays
**First experiments**: 1) Compare incremental vs bidirectional encoder performance, 2) Test different lookahead delay values (k=0,1,2), 3) Evaluate both decoder types on same encoder

## Open Questions the Paper Calls Out
The paper does not explicitly call out additional open questions beyond those addressed in the experimental analysis.

## Limitations
- Performance degradation may partly stem from architectural mismatches rather than purely from incrementality constraints
- Findings may not generalize to languages with very different syntactic properties (free word order, rich morphology)
- Lookahead delays blur the distinction between strictly incremental and partially incremental parsing
- Limited comparison between decoder types to only two specific architectures

## Confidence
High: (1) Incremental encoders perform worse than bidirectional ones across languages; (2) Transition-based decoders outperform sequence labeling decoders in incremental settings; (3) Small lookahead delays help but with diminishing returns
Medium: (4) mGPT being the best encoder among those tested, as this depends on pre-training overlap
Low: (5) The absolute ranking of incremental vs non-incremental parsing performance, as this conflates model architecture effects with incrementality constraints

## Next Checks
1. Replicate experiments with additional encoder architectures (e.g., ALBERT, ELECTRA) to test whether mGPT's superiority is due to pre-training or architectural properties
2. Evaluate strictly incremental parsing on typologically diverse languages (e.g., Japanese, Turkish, Finnish) to assess generalizability
3. Systematically ablate lookahead delays (k=0,1,2,3,4) to precisely quantify the trade-off between incrementality and performance across constituent lengths