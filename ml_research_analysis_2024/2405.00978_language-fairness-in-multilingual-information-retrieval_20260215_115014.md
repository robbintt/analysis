---
ver: rpa2
title: Language Fairness in Multilingual Information Retrieval
arxiv_id: '2405.00978'
source_url: https://arxiv.org/abs/2405.00978
tags:
- fairness
- language
- documents
- peer
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PEER, a novel metric to measure language
  fairness in multilingual information retrieval (MLIR) systems. Unlike previous metrics,
  PEER uses statistical equivalence testing (Kruskal-Wallis test) to evaluate whether
  documents of equal relevance across languages are ranked equally, without assuming
  any language as a protected group.
---

# Language Fairness in Multilingual Information Retrieval

## Quick Facts
- arXiv ID: 2405.00978
- Source URL: https://arxiv.org/abs/2405.00978
- Reference count: 34
- This paper introduces PEER, a novel metric to measure language fairness in multilingual information retrieval (MLIR) systems

## Executive Summary
This paper introduces PEER, a novel metric to measure language fairness in multilingual information retrieval (MLIR) systems. Unlike previous metrics, PEER uses statistical equivalence testing (Kruskal-Wallis test) to evaluate whether documents of equal relevance across languages are ranked equally, without assuming any language as a protected group. Experiments on synthetic and real MLIR benchmarks (CLEF 2003, NeuCLIR 2022) show that PEER captures fairness nuances better than existing metrics like AWRF. Results indicate that neural MLIR models trained on English text exhibit language bias, which PEER quantifies effectively.

## Method Summary
PEER (Probability of Equal Expected Rank) measures language fairness by testing whether documents of equal relevance across languages are ranked equally. It uses the non-parametric Kruskal-Wallis H test to compare mean ranks across languages for each relevance level, producing p-values that indicate fairness probability. The metric computes separate p-values for each relevance level and combines them with weighted averaging, making it robust to target distribution assumptions. PEER evaluates fairness at each relevance level independently, recognizing that bias may manifest differently for relevant versus non-relevant documents.

## Key Results
- PEER captures fairness nuances better than existing metrics like AWRF by using statistical equivalence testing
- Experiments show neural MLIR models trained on English text exhibit language bias that PEER effectively quantifies
- PEER is robust as it does not rely on target distributions, making it a reliable tool for evaluating MLIR fairness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PEER detects language bias in MLIR systems by testing whether documents of equal relevance across languages are expected to rank equally.
- Mechanism: Uses the Kruskal-Wallis H test to compare mean ranks across languages for each relevance level, producing a p-value that indicates the probability of fairness.
- Core assumption: Document ranks are random variables whose distributions depend on language and relevance, and these distributions can be compared non-parametrically.
- Evidence anchors:
  - [abstract] "PEER uses statistical equivalence testing (Kruskal-Wallis test) to evaluate whether documents of equal relevance across languages are ranked equally"
  - [section 3.1] "We define MLIR fairness using the following principle: Documents in different languages with the same relevance level, in expectation, should be presented at the same rank"
  - [corpus] Weak - no direct corpus support for the Kruskal-Wallis test in this context
- Break condition: If the rank distributions are not independent or if relevance judgments are unreliable, the test may produce false positives/negatives.

### Mechanism 2
- Claim: PEER is robust to target distribution assumptions, unlike AWRF.
- Mechanism: PEER operates directly on rank positions without requiring a predefined target distribution of document exposure across languages.
- Core assumption: Fairness can be measured purely through rank comparison without needing to specify an ideal exposure distribution.
- Evidence anchors:
  - [abstract] "PEER is robust, as it does not rely on target distributions, making it a reliable tool for evaluating MLIR fairness"
  - [section 3.3] "Overall, we report the average weighted PEER over all queries at rank X"
  - [section 4.3] "With a rank cutoff of 1000, AWRF strongly correlates with recall (Pearson r = 0.93 over both collections), while PEER does not (Pearson r = -0.55)"
- Break condition: If the system's ranking function is deterministic rather than probabilistic, the statistical test may lose power.

### Mechanism 3
- Claim: PEER captures fairness nuances at each relevance level independently.
- Mechanism: Computes separate p-values for each relevance level and combines them with weighted averaging, allowing different treatment of relevant vs. non-relevant documents.
- Core assumption: The impact of language bias differs across relevance levels and should be measured separately.
- Evidence anchors:
  - [section 3.2] "The impact of unfairly ranking documents in different languages may differ at each relevance level"
  - [section 4.2] "Figures 1(e) and 1(f) vary the sampling mean of the second language's relevant and nonrelevant documents, respectively, while keeping a first language sampling mean of 1.0"
  - [section 4.3] "While it is an artifact of the choice of target distribution, the need to define a target distribution reduces the robustness of AWRF in measuring MLIR Fairness"
- Break condition: If relevance judgments are coarse-grained or if the number of documents per relevance level is too small, the per-level p-values may be unreliable.

## Foundational Learning

- Concept: Non-parametric statistical testing
  - Why needed here: The Kruskal-Wallis test is non-parametric, making it robust to non-normal rank distributions and avoiding assumptions about score distributions.
  - Quick check question: What is the main advantage of using a non-parametric test like Kruskal-Wallis over a parametric ANOVA in this context?

- Concept: Rank-based evaluation in IR
  - Why needed here: PEER operates on document ranks rather than raw scores, which is fundamental to how IR effectiveness is traditionally measured.
  - Quick check question: Why might operating on ranks be more appropriate than raw scores when measuring language fairness?

- Concept: Relevance grading and multi-level evaluation
  - Why needed here: PEER evaluates fairness separately at each relevance level, recognizing that bias may manifest differently for relevant vs. non-relevant documents.
  - Quick check question: How does evaluating fairness at each relevance level separately improve the metric's sensitivity to different types of bias?

## Architecture Onboarding

- Component map: Rank collection module -> Relevance grouping module -> Kruskal-Wallis test executor -> Weighted aggregation component
- Critical path: For each query and relevance level, gather ranks by language → run Kruskal-Wallis test → collect p-values → apply relevance-level weights → aggregate to final PEER score
- Design tradeoffs: Non-parametric testing provides robustness but may be less powerful than parametric alternatives; per-relevance-level evaluation adds granularity but requires sufficient document counts at each level
- Failure signatures: PEER scores near 0 indicate strong language bias; very high scores (>0.95) may indicate either fairness or insufficient evidence due to small sample sizes; inconsistent results across relevance levels suggest relevance-dependent bias
- First 3 experiments:
  1. Run PEER on synthetic ranked lists with known fairness patterns (e.g., alternating vs. clustered languages) to verify detection capability
  2. Compare PEER scores for query translation vs. document translation systems on CLEF03 to validate alignment with prior analytical findings
  3. Test PEER's sensitivity by varying the number of documents per language and relevance level to identify minimum sample size requirements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PEER perform when applied to real-world MLIR systems with more than three languages or languages with significantly different script systems (e.g., Chinese vs. Arabic vs. Latin-based)?
- Basis in paper: [inferred] The paper evaluates PEER on two benchmarks (CLEF 2003 and NeuCLIR 2022) with up to three languages. It does not test scenarios with more languages or languages with vastly different scripts.
- Why unresolved: The current experiments are limited to European and Middle Eastern languages with similar script systems. Testing with more diverse languages would require new datasets and evaluations.
- What evidence would resolve it: PEER scores from MLIR systems evaluated on datasets with more than three languages or languages with different script systems (e.g., Chinese, Arabic, Hindi, etc.).

### Open Question 2
- Question: How sensitive is PEER to the choice of rank cutoff (X) in practical applications, and what is the optimal cutoff for balancing fairness detection and user utility?
- Basis in paper: [explicit] The paper discusses rank cutoffs at 20 and 1000 but does not explore the sensitivity of PEER to different cutoffs or provide guidance on optimal cutoff selection.
- Why unresolved: The paper demonstrates PEER's behavior at two cutoffs but does not analyze how varying the cutoff affects fairness detection or utility trade-offs.
- What evidence would resolve it: A systematic study of PEER scores across a range of cutoffs (e.g., 10, 50, 100, 500) and their correlation with user behavior or utility metrics.

### Open Question 3
- Question: Can PEER be extended to handle fairness across multiple dimensions (e.g., language, gender, or demographic attributes) in MLIR systems?
- Basis in paper: [inferred] The paper focuses on language fairness but does not explore how PEER could be adapted to evaluate fairness across multiple protected attributes simultaneously.
- Why unresolved: The current formulation of PEER is designed for a single dimension (language), and extending it to multiple dimensions would require new statistical methods and validation.
- What evidence would resolve it: A modified version of PEER that incorporates multiple dimensions and validation results showing its effectiveness in detecting multi-dimensional fairness issues.

### Open Question 4
- Question: How does PEER compare to other fairness metrics (e.g., those based on individual fairness) in detecting subtle biases in MLIR systems?
- Basis in paper: [explicit] The paper compares PEER to group fairness metrics like AWRF but does not evaluate its performance against individual fairness metrics.
- Why unresolved: The paper focuses on group fairness and does not explore how PEER performs relative to individual fairness metrics, which could detect different types of biases.
- What evidence would resolve it: A comparative study of PEER and individual fairness metrics (e.g., Lipschitz condition-based metrics) on the same MLIR datasets, highlighting their strengths and weaknesses.

## Limitations

- PEER's effectiveness depends on sufficient document counts per relevance level to ensure statistical test validity
- The metric assumes relevance judgments are accurate and complete across all languages
- PEER's performance may be affected by rank independence assumptions in deterministic ranking systems

## Confidence

- High Confidence: The core mechanism of using Kruskal-Wallis testing for comparing rank distributions across languages is well-established statistically and the synthetic experiments demonstrate expected behavior patterns.
- Medium Confidence: The metric's robustness claims relative to AWRF are supported by correlation analysis, but the choice of target distribution and its impact on AWRF performance could be explored more thoroughly.
- Medium Confidence: The application to real MLIR benchmarks shows consistent patterns with prior analytical findings, but the limited number of queries in CLEF 2003 (50 queries) constrains the generalizability of results.

## Next Checks

1. Systematically vary the number of documents per relevance level to identify minimum sample size thresholds where PEER maintains statistical validity and robustness.

2. Evaluate how PEER scores change when relevance judgments have known quality differences across languages, testing the assumption of uniform annotation quality.

3. Compare PEER's performance on MLIR systems with deterministic ranking functions versus those with probabilistic elements to assess the impact of rank independence assumptions.