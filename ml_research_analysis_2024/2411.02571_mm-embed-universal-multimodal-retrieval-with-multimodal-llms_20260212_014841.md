---
ver: rpa2
title: 'MM-Embed: Universal Multimodal Retrieval with Multimodal LLMs'
arxiv_id: '2411.02571'
source_url: https://arxiv.org/abs/2411.02571
tags:
- retrieval
- multimodal
- image
- qtxt
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents MM-Embed, the first universal multimodal retriever
  based on multimodal large language models (MLLMs). It addresses the challenge of
  modality bias in MLLM-based retrievers, where they tend to retrieve relevant text
  rather than images for cross-modal retrieval tasks.
---

# MM-Embed: Universal Multimodal Retrieval with Multimodal LLMs

## Quick Facts
- arXiv ID: 2411.02571
- Source URL: https://arxiv.org/abs/2411.02571
- Reference count: 40
- First universal multimodal retriever based on MLLMs achieving SOTA on M-BEIR benchmark

## Executive Summary
MM-Embed presents the first universal multimodal retriever built on multimodal large language models (MLLMs). The system addresses a critical limitation where MLLM-based retrievers exhibit modality bias, tending to retrieve relevant text rather than images in cross-modal scenarios. By introducing modality-aware hard negative mining and continuous fine-tuning techniques, MM-Embed achieves state-of-the-art performance on the M-BEIR multimodal retrieval benchmark while surpassing existing text retrieval models on MTEB. The paper also demonstrates that MLLMs can serve as effective zero-shot rerankers for complex interleaved text-image queries.

## Method Summary
The authors develop MM-Embed through two key innovations: modality-aware hard negative mining and continuous fine-tuning. The hard negative mining approach specifically targets modality bias by selecting challenging negative examples that force the model to correctly distinguish between text and image retrieval tasks. Continuous fine-tuning enhances text retrieval capabilities while preserving multimodal performance through a carefully balanced training schedule. The system is evaluated across multiple benchmarks, demonstrating superior performance in both pure text and multimodal retrieval scenarios.

## Key Results
- Achieves state-of-the-art results on M-BEIR multimodal retrieval benchmark
- Surpasses NV-Embed-v1 text retrieval model on MTEB benchmark
- Demonstrates MLLMs as effective zero-shot rerankers for complex interleaved queries

## Why This Works (Mechanism)
The success of MM-Embed stems from addressing the fundamental modality bias problem in MLLM-based retrievers. Traditional approaches struggle to balance text and image retrieval capabilities, often favoring one modality over the other. The modality-aware hard negative mining technique specifically forces the model to learn discriminative features for both modalities simultaneously, while continuous fine-tuning maintains this balance throughout training. This dual approach ensures the model can handle diverse retrieval scenarios without sacrificing performance in either modality.

## Foundational Learning

**Multimodal Large Language Models (MLLMs)**: Neural architectures capable of processing and generating both text and images. Why needed: Core foundation for universal retrieval. Quick check: Verify model accepts both modalities as input.

**Modality Bias**: Tendency of models to favor one modality over another in retrieval tasks. Why needed: Understanding the problem being solved. Quick check: Examine retrieval distributions across modalities.

**Hard Negative Mining**: Strategy of selecting challenging negative examples for training. Why needed: Improves model discrimination capability. Quick check: Verify negative samples are truly challenging.

**Continuous Fine-tuning**: Ongoing training process that adapts model parameters over time. Why needed: Maintains performance across multiple tasks. Quick check: Monitor performance drift during fine-tuning.

**Zero-shot Reranking**: Using pre-trained models to rerank retrieval results without task-specific training. Why needed: Improves retrieval accuracy without additional training. Quick check: Test reranking effectiveness on held-out data.

## Architecture Onboarding

**Component Map**: Input Query -> MLLM Encoder -> Embedding Space -> Retrieval Index -> Output Results

**Critical Path**: The MLLM encoder and embedding space formation represent the critical path, as these components directly determine retrieval quality and must handle both text and image inputs effectively.

**Design Tradeoffs**: The system balances between modality specialization (better performance but less flexibility) and universal capability (broader applicability but potential performance compromise). The modality-aware hard negative mining represents a middle ground that forces specialization while maintaining universal applicability.

**Failure Signatures**: Performance degradation typically manifests as modality bias (favoring text over images or vice versa), poor handling of complex interleaved queries, or catastrophic forgetting during continuous fine-tuning.

**First Experiments**:
1. Baseline retrieval accuracy test on M-BEIR benchmark to establish initial performance
2. Modality bias analysis comparing text vs image retrieval precision
3. Continuous fine-tuning stability test to verify absence of catastrophic forgetting

## Open Questions the Paper Calls Out
None

## Limitations
- Universal applicability claims lack comprehensive validation across diverse real-world datasets
- Architecture independence of hard negative mining effectiveness remains unverified
- Potential catastrophic forgetting risks during continuous fine-tuning are not thoroughly addressed

## Confidence
- Performance claims on M-BEIR: Medium
- Architecture generalization claims: Low
- Long-term stability of continuous fine-tuning: Low

## Next Checks
1. Test MM-Embed's performance on a broader range of real-world multimodal datasets beyond M-BEIR to verify universal applicability claims
2. Conduct ablation studies comparing different MLLM architectures with the proposed hard negative mining to establish architecture independence
3. Perform longitudinal studies to assess catastrophic forgetting when applying continuous fine-tuning over extended training periods