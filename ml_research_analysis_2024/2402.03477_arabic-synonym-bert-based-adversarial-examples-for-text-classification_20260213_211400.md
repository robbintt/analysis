---
ver: rpa2
title: Arabic Synonym BERT-based Adversarial Examples for Text Classification
arxiv_id: '2402.03477'
source_url: https://arxiv.org/abs/2402.03477
tags:
- adversarial
- examples
- attack
- bert
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first study of word-level adversarial attacks
  in Arabic. The authors develop a synonym-based attack using Masked Language Modeling
  (MLM) with a BERT model in a black-box setting.
---

# Arabic Synonym BERT-based Adversarial Examples for Text Classification

## Quick Facts
- arXiv ID: 2402.03477
- Source URL: https://arxiv.org/abs/2402.03477
- Authors: Norah Alshahrani; Saied Alshahrani; Esma Wali; Jeanna Matthews
- Reference count: 22
- Primary result: BERT models are more vulnerable to Arabic synonym-based adversarial attacks than WordCNN and WordLSTM models

## Executive Summary
This paper presents the first study of word-level adversarial attacks in Arabic, developing a synonym-based attack using Masked Language Modeling (MLM) with a BERT model in a black-box setting. The attack generates adversarial examples by ranking word importance, using BERT MLM to find synonyms, checking grammar and semantic similarity, and replacing words to flip model predictions. The authors evaluate the attack on three text classification models (BERT, WordCNN, WordLSTM) trained on two Arabic datasets (HARD and MSDA).

## Method Summary
The authors develop an adversarial attack methodology that works in three main phases: word importance ranking using a scoring function, synonym generation using BERT MLM, and validation through grammar checking (POS tagging) and semantic similarity assessment. The attack is evaluated in a black-box setting where the attacker does not have access to the target model's parameters. The attack's effectiveness is measured through attack success rate, transferability between models, and the impact of adversarial training as a defense mechanism.

## Key Results
- BERT models show significantly higher vulnerability to synonym attacks than WordCNN and WordLSTM models, with accuracy drops from 90.55% to 63.62% on MSDA dataset
- BERT models are more susceptible to transferred attacks from other models compared to vice versa
- Adversarial training helps BERT models regain at least 2% in accuracy as a defense mechanism
- Human evaluation confirms generated adversarial examples maintain grammatical correctness and semantic similarity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BERT models are more vulnerable to synonym-based adversarial attacks than WordCNN and WordLSTM models.
- Mechanism: BERT's contextual embeddings are more sensitive to word substitutions than the static embeddings used by WordCNN and WordLSTM models.
- Evidence anchors:
  - BERT models' accuracies dropped from 90.55% to 63.62% on MSDA dataset after attack
  - BERT models' accuracies dropped from 88.59% to 73.90% on HARD dataset after attack
- Break condition: BERT models regain accuracy when fine-tuned with adversarial training

### Mechanism 2
- Claim: BERT models are more susceptible to transferred attacks from other models.
- Mechanism: BERT's decision boundaries are more aligned with or closer to those of simpler models, making it easier for adversarial examples from those models to cross BERT's decision boundaries.
- Evidence anchors:
  - BERT has higher transferability scores than WordCNN or WordLSTM models
  - BERT (as victim) shows greater vulnerability to transferred attacks
- Break condition: Adversarial training reduces transferability scores

### Mechanism 3
- Claim: Adversarial training helps BERT models regain at least 2% in accuracy.
- Mechanism: Including adversarial examples in training data allows the model to learn more robust representations that are less sensitive to synonym substitutions.
- Evidence anchors:
  - BERT models regain at least 2% in accuracy after applying adversarial training
  - Adversarial training increases BERT models' accuracies on both datasets
- Break condition: If adversarial examples used for training are not representative of the attack space

## Foundational Learning

- Concept: Masked Language Modeling (MLM)
  - Why needed here: MLM is the core technique used to generate synonym candidates for creating adversarial examples
  - Quick check question: What is the primary purpose of using MLM in the context of generating adversarial examples?

- Concept: Transferability of adversarial examples
  - Why needed here: The paper studies how adversarial examples generated by one model can affect other models
  - Quick check question: What does it mean for an adversarial attack to have high transferability between models?

- Concept: Adversarial training
  - Why needed here: Adversarial training is used as a defense mechanism against the synonym-based attacks
  - Quick check question: How does adversarial training improve a model's robustness to adversarial examples?

## Architecture Onboarding

- Component map: Data preprocessing → Word importance ranking → Synonym generation (BERT MLM) → Grammar checking (POS tagging) → Semantic similarity checking → Adversarial example generation → Model evaluation
- Critical path: Word importance ranking → Synonym generation → Grammar checking → Semantic similarity checking → Adversarial example generation → Model evaluation
- Design tradeoffs:
  - BERT MLM provides contextually appropriate synonyms but requires more computational resources than static word embedding methods
  - 0.80 semantic similarity threshold balances between effective adversarial examples and naturalness
  - Black-box attack setting limits optimization but increases realism and applicability
- Failure signatures:
  - Low attack success rate: Issues with synonym generation, grammar checking, or semantic similarity thresholds
  - High semantic similarity but low attack success: Target model robustness to synonym substitutions
  - Low grammatical similarity: Issues with POS tagging or synonym selection process
- First 3 experiments:
  1. Run attack on small data subset with debugging output to verify each component works
  2. Vary semantic similarity threshold and observe effect on attack success rate and grammatical similarity
  3. Compare attack effectiveness on BERT vs. WordCNN/WordLSTM models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the synonym-based adversarial attack compare when using different Arabic pre-trained models (e.g., MARBERT, Arabic-BERT) instead of AraBERT?
- Basis in paper: [explicit] The authors acknowledge that their attack's effectiveness is limited by the performance of the pre-trained Arabic models used for synonym retrieval and grammatical/semantic checking
- Why unresolved: The paper only uses AraBERT and CAMeLBERT models
- What evidence would resolve it: Conducting the same experiments using different Arabic pre-trained models and comparing attack success rates and quality of generated adversarial examples

### Open Question 2
- Question: How does the transferability of adversarial examples differ when attacking models trained on the same dataset versus models trained on different datasets (e.g., HARD vs. MSDA)?
- Basis in paper: [inferred] The paper shows models trained on Dialectical Arabic (MSDA) are more vulnerable to transferred attacks than those trained on Modern Standard Arabic (HARD)
- Why unresolved: The paper only examines transferability within the same dataset
- What evidence would resolve it: Generating adversarial examples from one dataset and evaluating their effectiveness on models trained on a different dataset, and vice versa

### Open Question 3
- Question: What is the impact of using different data preprocessing techniques on the effectiveness of the synonym-based adversarial attack?
- Basis in paper: [explicit] The authors mention using different levels of data preprocessing for WordLSTM and WordCNN models compared to BERT models
- Why unresolved: The paper does not explore how different preprocessing techniques affect the attack's performance
- What evidence would resolve it: Conducting the attack with various preprocessing techniques and comparing attack success rates and quality of generated adversarial examples

## Limitations
- Small sample size: Main attack analysis relies on only 1000 randomly chosen examples
- Limited human evaluation: Only 5 participants per example, lacking statistical power
- Minimal defense improvement: Adversarial training shows only 2% accuracy recovery
- Single defense mechanism: Only adversarial training explored as defense, leaving uncertainty about better alternatives

## Confidence

**High confidence**: BERT models are more vulnerable to synonym attacks than WordCNN and WordLSTM models (supported by significant accuracy drops from 90.55% to 63.62% on MSDA dataset).

**Medium confidence**: BERT models are more susceptible to transferred attacks (supported by data but needs more experiments across model pairs and datasets).

**Low confidence**: Adversarial training helps BERT models regain "at least 2%" in accuracy (technically supported but represents minimal improvement that may not be practically significant).

## Next Checks

1. **Scale up human evaluation**: Replicate human evaluation with 20+ participants per example and use statistical tests to determine whether claimed grammatical and semantic similarity differences are statistically significant.

2. **Test adversarial training effectiveness more rigorously**: Implement k-fold cross-validation of adversarial training with multiple random seeds, measuring not just accuracy recovery but also false positive rates and robustness to different attack variations.

3. **Expand transferability analysis**: Generate adversarial examples using BERT as source model and measure attack success on WordCNN and WordLSTM targets, completing the transferability matrix and testing across different datasets.