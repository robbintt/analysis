---
ver: rpa2
title: 'Assessing Good, Bad and Ugly Arguments Generated by ChatGPT: a New Dataset,
  its Methodology and Associated Tasks'
arxiv_id: '2406.15130'
source_url: https://arxiv.org/abs/2406.15130
tags:
- arguments
- dataset
- chatgpt
- tasks
- essay
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents ArGPT, a new dataset of argumentative essays
  generated by ChatGPT designed to study the quality of LLM-generated arguments. The
  dataset was created by prompting ChatGPT to write essays on false or contradictory
  themes, followed by a professor correcting the student's argumentation.
---

# Assessing Good, Bad and Ugly Arguments Generated by ChatGPT: a New Dataset, its Methodology and Associated Tasks

## Quick Facts
- arXiv ID: 2406.15130
- Source URL: https://arxiv.org/abs/2406.15130
- Authors: Victor Hugo Nascimento Rocha; Igor Cataneo Silveira; Paulo Pirozelli; Denis Deratani MauÃ¡; Fabio Gagliardi Cozman
- Reference count: 24
- One-line primary result: ArGPT dataset created with 168 ChatGPT-generated argumentative essays annotated for argument mining and essay scoring tasks, establishing baselines with RoBERTa achieving F1-macro scores of 54.97% for argumentation quality and 77.35% for component classification

## Executive Summary
This paper introduces ArGPT, a novel dataset of 168 argumentative essays generated by ChatGPT designed to study the quality of LLM-generated arguments. The dataset was created through a simulated student-teacher interaction where ChatGPT was prompted to write essays on false or contradictory themes, followed by a professor correcting the student's argumentation. The resulting essays were annotated for five tasks: argumentation quality evaluation, span identification, component classification, relation classification, and essay scoring. The authors established baselines for these tasks using BERT and RoBERTa architectures, demonstrating the dataset's potential for training argument mining systems and studying problematic LLM argumentation.

## Method Summary
The ArGPT dataset was created by prompting ChatGPT to generate argumentative essays on false or contradictory themes, simulating a student writing with flawed reasoning. A professor then corrected these essays, creating a dialogue-based dataset. The 168 resulting essays were annotated for argument mining (AM) and automatic essay scoring (AES) tasks. Five specific tasks were defined: evaluating argumentation quality (with four sub-tasks), identifying argumentative spans, classifying components, classifying relations between components, and scoring essays. The authors trained BERT and RoBERTa models on this annotated data using standard hyperparameter configurations (15 epochs, batch size of 8, learning rate of 2e-5) and established baseline performance metrics for each task.

## Key Results
- RoBERTa achieved F1-macro scores of 54.97% for argumentation quality evaluation and 77.35% for component classification on the ArGPT dataset
- Models trained on ArGPT showed promising transferability to human-generated datasets, suggesting the dataset can train AM systems that generalize beyond LLM-generated text
- The dataset demonstrates potential for studying problematic argumentation patterns in LLMs while providing a foundation for improving argument mining and essay scoring systems

## Why This Works (Mechanism)
The methodology works by leveraging ChatGPT's tendency to generate coherent but potentially flawed arguments when given false premises. By having a professor correct these arguments, the dataset captures both problematic reasoning patterns and their corrections, creating rich annotation targets. The simulation of student-teacher dialogue provides natural context for argument components and their relationships. The use of false or contradictory themes ensures that generated arguments contain identifiable flaws, making them suitable for studying argumentation quality. The annotation scheme captures multiple aspects of argumentation structure, from component identification to quality assessment, enabling comprehensive evaluation of both the dataset and trained models.

## Foundational Learning
- **Argument Mining (AM)**: The task of automatically identifying and extracting argumentative structures from text, including claims, premises, and their relationships. *Why needed*: Essential for understanding how arguments are constructed and for building systems that can evaluate argumentative quality. *Quick check*: Verify understanding by identifying argument components in a sample essay.

- **Automatic Essay Scoring (AES)**: Computational methods for evaluating and scoring essays based on various criteria including argumentation quality, coherence, and structure. *Why needed*: Provides a framework for quantifying argumentative quality and enables automated assessment of large text corpora. *Quick check*: Understand how scoring rubrics translate qualitative judgments into numerical scores.

- **LLM-generated text characteristics**: Understanding how large language models generate text, particularly their tendencies to produce coherent but potentially flawed reasoning when given false premises. *Why needed*: Critical for interpreting results and understanding the limitations of datasets generated through LLM prompting. *Quick check*: Compare LLM-generated arguments with human arguments on the same topics.

## Architecture Onboarding

**Component Map**: ArGPT Dataset -> Annotation Schema -> BERT/RoBERTa Models -> Five Task Baselines

**Critical Path**: The most critical component is the annotation schema, as it defines how argumentative structures are identified and classified. This schema directly impacts model training and evaluation, determining what the models learn to predict. Without consistent, high-quality annotations, baseline performance becomes meaningless.

**Design Tradeoffs**: The dataset uses synthetic LLM-generated essays rather than human-generated ones, sacrificing naturalness for controllability and the ability to induce specific argumentation flaws. This tradeoff enables studying problematic argumentation patterns systematically but may limit generalizability to human discourse. The choice of RoBERTa over more recent architectures balances performance with computational efficiency for establishing baselines.

**Failure Signatures**: Models may overfit to ChatGPT's specific writing style, struggle with identifying subtle argumentation flaws, or fail to generalize to human-generated arguments with different structures. Annotation inconsistencies could lead to poor model performance even with adequate training data. The essay scoring task may show higher variance due to the subjective nature of quality assessment.

**Three First Experiments**:
1. Evaluate baseline models on a human-generated argumentative dataset to test transferability
2. Perform ablation studies removing different annotation layers to understand their contribution to model performance
3. Fine-tune models on subsets of ArGPT to determine minimum dataset size needed for reasonable performance

## Open Questions the Paper Calls Out
- How do arguments generated by other LLMs compare to those produced by ChatGPT in terms of quality, coherence, and ability to defend false claims? This requires comparative analysis of argumentative essays generated by multiple LLMs using the same methodology as ArGPT, followed by qualitative and quantitative assessment of argumentation quality, coherence, and ability to defend false claims.

- Can the ArGPT dataset and methodology be effectively used to train AM and AES systems for human-generated texts, and if so, to what extent? This needs extensive experiments training and evaluating AM and AES systems on ArGPT and testing them on various human-generated datasets, comparing performance to models trained on human-generated data.

- How can the ArGPT dataset be expanded and improved to create more robust baselines for the defined tasks and to support a broader range of research? This involves creating a larger, more diverse ArGPT dataset with improved annotations and more robust baselines for the defined tasks, along with experiments demonstrating the effectiveness of the expanded dataset and improved baselines.

## Limitations
- The exact prompts used to generate essays with ChatGPT are not disclosed, creating a significant reproducibility barrier for dataset generation
- The dataset's size (168 essays) is relatively small for training robust models across five different tasks, potentially limiting generalizability
- Inter-annotator agreement scores are not reported, raising questions about annotation consistency and reliability

## Confidence
- **High Confidence**: The dataset creation methodology and the establishment of baseline models for the five defined tasks are clearly described and reproducible given access to the dataset
- **Medium Confidence**: The transferability results showing performance on human-generated data are presented but lack statistical significance testing and detailed analysis of failure modes
- **Low Confidence**: The claim that this is the "first dataset for assessing problematic LLM-generated arguments" cannot be independently verified without a comprehensive literature review of similar datasets

## Next Checks
1. **Inter-annotator Agreement Analysis**: Calculate and report Cohen's Kappa or similar agreement metrics for all annotation tasks to quantify annotation consistency and reliability
2. **Statistical Significance Testing**: Perform paired t-tests or Wilcoxon signed-rank tests to determine if performance differences between models on ArGPT versus human-generated datasets are statistically significant
3. **Error Analysis on Boundary Cases**: Manually examine 20-30 essays where model predictions differ most from human annotations to identify systematic patterns in model failures and dataset limitations