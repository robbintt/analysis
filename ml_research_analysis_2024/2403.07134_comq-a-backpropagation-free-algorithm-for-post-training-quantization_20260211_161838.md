---
ver: rpa2
title: 'COMQ: A Backpropagation-Free Algorithm for Post-Training Quantization'
arxiv_id: '2403.07134'
source_url: https://arxiv.org/abs/2403.07134
tags:
- quantization
- comq
- neural
- accuracy
- post-training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: COMQ introduces a backpropagation-free post-training quantization
  (PTQ) algorithm for neural networks, including vision transformers and convolutional
  models. The method employs coordinate-wise minimization of layer-wise reconstruction
  errors by decomposing quantized weights into a shared floating-point scalar and
  integer bit-codes, updating one parameter at a time using a carefully designed greedy
  order to improve quantization accuracy.
---

# COMQ: A Backpropagation-Free Algorithm for Post-Training Quantization

## Quick Facts
- arXiv ID: 2403.07134
- Source URL: https://arxiv.org/abs/2403.07134
- Reference count: 40
- Primary result: State-of-the-art 4-bit PTQ for vision transformers with <1% accuracy loss

## Executive Summary
COMQ introduces a backpropagation-free post-training quantization algorithm that achieves state-of-the-art accuracy for 4-bit weight quantization, particularly excelling with vision transformers. The method uses coordinate-wise minimization of layer-wise reconstruction errors through a novel decomposition of quantized weights into shared floating-point scalars and integer bit-codes. By employing a carefully designed greedy update order and only requiring dot products and rounding operations, COMQ avoids hyper-parameter tuning while maintaining computational efficiency.

## Method Summary
COMQ performs post-training quantization by decomposing each quantized weight into a shared floating-point scalar (δ) and integer bit-codes (Q), then sequentially updating these parameters using coordinate-wise minimization of reconstruction error. The algorithm sorts coordinates by magnitude of quantization residuals and updates them in greedy order, with each update having a closed-form solution. This approach works for both per-layer and per-channel quantization schemes and supports various bit-widths including 4-bit, 3-bit, and 2-bit quantization.

## Key Results
- Achieves <1% accuracy loss in 4-bit weight quantization for vision transformers (ViT-S, DeiT-S, Swin-T)
- Maintains near-lossless accuracy for convolutional networks with only 0.3% drop
- Outperforms existing PTQ methods including GPTQ, AWQ, and ZeroQuant
- Successfully extends to 3-bit and 2-bit quantization with strong performance

## Why This Works (Mechanism)

### Mechanism 1
Coordinate-wise minimization improves quantization accuracy by iteratively reducing layer-wise reconstruction error without requiring gradient computation. The algorithm decomposes each quantized weight into a shared floating-point scalar and integer bit-codes, then performs sequential coordinate descent by updating one parameter at a time while holding others constant. The greedy coordinate selection order becomes ineffective if the magnitude-based importance heuristic fails to reflect true impact on reconstruction error, especially in highly non-convex or noisy quantization landscapes.

### Mechanism 2
Greedy selection of update order (based on magnitude of quantization residuals) accelerates convergence and reduces quantization loss compared to cyclic updates. The algorithm sorts coordinates by descending magnitude of ∥w_i · x_i∥ and updates them in this order, ensuring that the most significant coordinates are adjusted first. If quantization residuals are not well-aligned with magnitude-based importance, the greedy order may converge slower or to a worse local minimum than cyclic updates.

### Mechanism 3
Per-channel quantization with individual scaling factors per column reduces quantization error and improves accuracy, especially for columns with widely varying value ranges. Each column of the weight matrix has its own scale factor, allowing column-wise adaptation to the dynamic range. If column value ranges are similar or if memory overhead for storing multiple scaling factors is prohibitive, the per-channel approach may not provide significant benefit over per-layer quantization.

## Foundational Learning

- **Concept**: Coordinate descent optimization
  - Why needed here: COMQ relies on coordinate-wise minimization of reconstruction error, which is a form of coordinate descent. Understanding how coordinate descent works, its convergence properties, and its advantages over full gradient methods is essential.
  - Quick check question: What is the main advantage of coordinate descent over gradient descent in the context of large-scale optimization problems?

- **Concept**: Uniform quantization and bit-code decomposition
  - Why needed here: COMQ quantizes weights by decomposing them into a floating-point scalar and integer bit-codes. Understanding how uniform quantization maps continuous values to discrete levels and how this decomposition works is critical for implementing the algorithm.
  - Quick check question: In b-bit uniform quantization, what is the set of possible integer values for the bit-code, and how does the zero point affect the mapping?

- **Concept**: Layer-wise reconstruction error minimization
  - Why needed here: COMQ minimizes the squared error between the outputs of the original and quantized linear layers, ∥XW^q - XW∥². Understanding why this surrogate loss is used and how it relates to final model accuracy is important.
  - Quick check question: Why does minimizing the layer-wise reconstruction error help preserve the accuracy of the quantized model?

## Architecture Onboarding

- **Component map**: Pre-trained weights W -> Feature matrix X -> Coordinate descent loop (greedy order) -> δ-updates and Q-updates -> Quantized weights W^q -> Evaluation

- **Critical path**: Feature matrix X → Coordinate descent loop (greedy order) → δ-updates and Q-updates → Quantized weights W^q → Evaluation

- **Design tradeoffs**:
  - Per-layer vs. per-channel quantization: Simpler per-layer is faster but less accurate; per-channel is more accurate but requires more scaling factors and memory.
  - Greedy ordering vs. cyclic ordering: Greedy ordering accelerates convergence and improves accuracy but requires sorting overhead; cyclic is simpler but slower to converge.
  - Number of iterations K: More iterations can improve accuracy up to a point, but diminishing returns occur; too many iterations waste compute.

- **Failure signatures**:
  - Accuracy degradation: May indicate poor scaling factor initialization, inadequate iteration count, or suboptimal greedy ordering.
  - Runtime slowdown: Likely due to unvectorized loops, excessive iterations, or large batch sizes without benefit.
  - Numerical instability: Could arise from ill-conditioned feature matrices X or extreme weight ranges.

- **First 3 experiments**:
  1. Validate greedy vs. cyclic update order: Run COMQ with both orderings on a small CNN (e.g., ResNet18) and compare quantization error and accuracy.
  2. Test per-layer vs. per-channel scaling: Quantize ResNet50 with both approaches and measure accuracy drop and runtime.
  3. Ablation on initialization: Compare δ₀ = max|W| vs. δ₀ = average infinity norm for weight scaling and measure impact on convergence speed and final accuracy.

## Open Questions the Paper Calls Out
None

## Limitations
- Core mechanisms rely heavily on internal paper claims without independent verification
- Sparse corpus evidence for coordinate-wise minimization and greedy ordering effectiveness
- Implementation details for greedy ordering and scale factor initialization remain underspecified
- Break conditions for mechanisms are inferred from algorithmic reasoning rather than empirical validation

## Confidence

- **High**: Per-channel quantization with individual scaling factors reduces quantization error compared to per-layer approaches
- **Medium**: Greedy update ordering accelerates convergence and improves accuracy
- **Low**: Coordinate-wise minimization without gradients achieves state-of-the-art results

## Next Checks

1. Verify that coordinate-wise minimization converges faster and to better solutions than cyclic updates on a simple test case (e.g., linear regression with synthetic data).
2. Test whether magnitude-based greedy ordering consistently outperforms random ordering across different network architectures and bit-widths.
3. Validate that per-channel quantization provides consistent accuracy improvements over per-layer quantization when accounting for memory overhead in real deployment scenarios.