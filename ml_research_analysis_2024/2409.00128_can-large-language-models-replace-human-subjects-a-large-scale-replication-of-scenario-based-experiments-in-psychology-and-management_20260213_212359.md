---
ver: rpa2
title: Can Large Language Models Replace Human Subjects? A Large-Scale Replication
  of Scenario-Based Experiments in Psychology and Management
arxiv_id: '2409.00128'
source_url: https://arxiv.org/abs/2409.00128
tags:
- effects
- studies
- human
- llms
- replication
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study conducted a large-scale replication of 156 psychological
  experiments using three advanced LLMs (GPT-4, Claude 3.5 Sonnet, and DeepSeek v3),
  revealing that while LLMs successfully replicated main effects at rates of 73-81%
  and interaction effects at 46-63%, they consistently produced larger effect sizes
  than human studies, with Fisher Z values approximately 2-3 times higher. The LLMs
  showed notably lower replication rates for socially sensitive topics like race,
  gender, and ethics, and produced significant results in 68-83% of cases where original
  studies reported null findings.
---

# Can Large Language Models Replace Human Subjects? A Large-Scale Replication of Scenario-Based Experiments in Psychology and Management

## Quick Facts
- arXiv ID: 2409.00128
- Source URL: https://arxiv.org/abs/2409.00128
- Reference count: 0
- Primary result: LLMs replicate main effects at 73-81% but produce 2-3x larger effect sizes than human studies

## Executive Summary
This study conducted a large-scale replication of 156 psychological experiments using three advanced LLMs (GPT-4, Claude 3.5 Sonnet, and DeepSeek v3), revealing that while LLMs successfully replicated main effects at rates of 73-81% and interaction effects at 46-63%, they consistently produced larger effect sizes than human studies, with Fisher Z values approximately 2-3 times higher. The LLMs showed notably lower replication rates for socially sensitive topics like race, gender, and ethics, and produced significant results in 68-83% of cases where original studies reported null findings. Despite these systematic differences, LLMs demonstrated reliable directional consistency with human behavioral patterns, suggesting their value as tools for pilot testing and hypothesis validation while highlighting the need for careful interpretation when studying complex social phenomena.

## Method Summary
The study employed a "silicon replication" approach, treating LLMs as human subjects by generating equivalent numbers of responses to original experimental conditions. Researchers collected 156 psychological experiments from five top journals (2015-2024) focusing on text-based vignette studies, then designed standardized prompts with four components (context, scenario, variable measurement, response format) and validated them across three LLM models. Each model generated responses matched 1:1 with original human participant numbers, and results were analyzed using the same statistical methods as the original studies to assess replication success rates and effect size comparisons.

## Key Results
- LLMs achieved 73-81% replication success for main effects and 46-63% for interaction effects
- Effect sizes were consistently 2-3 times larger than human studies, driven by amplified between-group differences and decreased within-group variance
- Replication rates dropped significantly for socially sensitive topics (race, gender, ethics), with GPT-4 showing 41.5% success for race-related studies versus 76.8% for non-race studies
- LLMs produced significant results in 68-83% of cases where original studies reported null findings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can replicate main effects with high directional consistency but tend to amplify effect sizes.
- Mechanism: LLMs process experimental vignettes without fatigue, distraction, or response inconsistency, leading to cleaner data with narrower confidence intervals and amplified detection of psychological effects.
- Core assumption: LLM responses are analogous to human participant responses in controlled experimental contexts.
- Evidence anchors:
  - [abstract] "LLMs demonstrate high replication rates for main effects (73-81%) and moderate to strong success with interaction effects (46-63%), They consistently produce larger effect sizes than human studies, with Fisher Z values approximately 2-3 times higher than human studies."
  - [section] "For main effects, all models produced significantly smaller p-values than human studies... suggesting potential effect amplification, with Fisher Z values approximately 2-3 times higher than human studies."
  - [corpus] Weak - corpus papers focus on replication capability but not systematic effect size amplification.
- Break condition: When experimental stimuli require embodied experience, social interaction, or personal history that LLMs cannot simulate.

### Mechanism 2
- Claim: LLMs show reduced replication success for socially sensitive topics due to value alignment and social desirability bias.
- Mechanism: LLMs are trained to align with certain values and respond in socially desirable ways, making them more cautious and less prone to controversial responses in sensitive contexts.
- Core assumption: LLMs' training data and fine-tuning include value alignment that affects response patterns in socially sensitive scenarios.
- Evidence anchors:
  - [abstract] "LLMs show significantly lower replication rates for studies involving socially sensitive topics such as race, gender and ethics."
  - [section] "GPT-4's main effect replication rate dropped dramatically from 76.8% in studies without race variables to 41.5% in studies with race variables... This stark difference may be attributed to LLMs' alignment with certain values and their tendency to respond in socially desirable ways."
  - [corpus] Weak - corpus papers mention social simulations but don't specifically address value alignment in sensitive contexts.
- Break condition: When experimental design can isolate the effect of social desirability from the underlying psychological mechanism being tested.

### Mechanism 3
- Claim: LLMs maintain relative effect size relationships while amplifying absolute magnitudes, enabling reliable pilot testing.
- Mechanism: Despite producing larger absolute effect sizes, LLMs preserve the rank ordering and directional relationships between effects, allowing researchers to identify promising hypotheses and experimental designs.
- Core assumption: The correlation between original and LLM-generated effect sizes remains stable despite systematic amplification.
- Evidence anchors:
  - [abstract] "Despite these systematic differences, LLMs demonstrated reliable directional consistency with human behavioral patterns, suggesting their value as tools for pilot testing and hypothesis validation."
  - [section] "Across all models, psychology studies consistently show stronger correlations (GPT-4: ρ = 0.598) compared to management studies (GPT-4: ρ = 0.354)... suggesting that while all three LLMs tend to produce larger effect sizes, their ability to maintain relative effect size relationships remains consistent with human-based replication studies."
  - [corpus] Weak - corpus papers focus on replication rates but don't examine the relationship between relative effect sizes.
- Break condition: When the amplification pattern varies systematically across different types of psychological phenomena or experimental designs.

## Foundational Learning

- Concept: Fisher Z-transformation
  - Why needed here: To properly compare correlation coefficients between original studies and LLM replications while accounting for sampling uncertainty.
  - Quick check question: Why can't we directly compare raw correlation coefficients between studies with different sample sizes?

- Concept: Statistical power analysis in LLM contexts
  - Why needed here: To understand the implications of LLMs' more homogeneous response patterns for detecting effects and the conservatism of 1:1 sample size matching.
  - Quick check question: How does the reduced response variability in LLMs affect the required sample size for detecting psychological effects?

- Concept: Interaction effects in experimental design
  - Why needed here: To understand why LLMs show lower replication rates for interaction effects compared to main effects and the methodological challenges in comparing effect sizes across studies.
  - Quick check question: Why are interaction effects typically more difficult to detect and replicate than main effects in both human and LLM studies?

## Architecture Onboarding

- Component map: Prompt design and validation pipeline -> LLM API integration (GPT-4, Claude 3.5 Sonnet, DeepSeek v3) -> JSON response parsing and standardization -> Statistical analysis framework for replication assessment -> Effect size comparison and confidence interval analysis -> Temperature sensitivity testing module

- Critical path: 1. Article selection and screening 2. Prompt adaptation and validation 3. LLM response generation (1:1 sample matching) 4. Data cleaning and standardization 5. Statistical analysis and replication assessment 6. Effect size comparison and interpretation

- Design tradeoffs:
  - Strict vs. conceptual replication: Adaptations needed for LLM comprehension may move from literal to conceptual replication
  - Sample size matching: 1:1 matching ensures comparability but may be conservative given LLM homogeneity
  - Temperature settings: Default settings balance variability and consistency but may miss systematic effects of temperature adjustments

- Failure signatures:
  - Uniform responses across entire sample (zero variance)
  - Systematic deviation in socially sensitive topics
  - Temperature sensitivity in borderline cases
  - Format compliance issues in JSON responses

- First 3 experiments:
  1. Replicate a simple main effect study (e.g., anchoring effect) to validate basic prompt structure and response parsing
  2. Test a socially sensitive topic study to observe value alignment effects and replication rate differences
  3. Run temperature sensitivity analysis on a borderline significant case to understand the impact of temperature settings on statistical significance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms cause LLMs to produce systematically larger effect sizes compared to human studies, and can these be mitigated through model training or prompting strategies?
- Basis in paper: [explicit] The paper identifies that LLMs produce effect sizes approximately 2-3 times higher than human studies, driven by amplified between-group differences and decreased within-group variance
- Why unresolved: The paper demonstrates the phenomenon but does not investigate the underlying mechanisms or potential solutions
- What evidence would resolve it: Controlled experiments testing different model architectures, training approaches, or prompting strategies to identify which factors influence effect size inflation

### Open Question 2
- Question: How do LLMs handle studies involving complex individual differences and demographic profiles compared to simpler experimental designs?
- Basis in paper: [explicit] The paper notes that existing studies reveal notable divergences particularly in simulating individual-level behaviors and specific demographic profiles, but this study focused on text-based vignette studies
- Why unresolved: This study excluded studies involving individual-level behaviors and demographic profiling, leaving a gap in understanding LLM performance for these critical research areas
- What evidence would resolve it: Systematic replication of studies specifically designed to capture individual differences and demographic variations using the same LLM models

### Open Question 3
- Question: What are the implications of LLM-generated significant results for studies that originally reported null findings, and how should researchers interpret these discrepancies?
- Basis in paper: [explicit] The paper found that LLMs produced significant results in 68-83% of cases where original studies reported null findings
- Why unresolved: The paper identifies this pattern but does not explore whether it represents genuine detection of subtle effects, model artifacts, or other phenomena
- What evidence would resolve it: Comparative analysis of effect sizes, power calculations, and validation studies to determine whether LLM-generated significant results reflect real patterns or statistical artifacts

## Limitations

- Effect size amplification by LLMs (2-3x larger) creates challenges for direct quantitative comparisons with human studies
- 1:1 sample size matching approach may be overly conservative given LLMs' more homogeneous response patterns
- Study focused on text-based vignette studies, excluding research requiring embodied experience, social interaction, or individual-level behaviors

## Confidence

- **High confidence**: LLMs can replicate main effects with directional consistency (73-81% success rate); LLMs produce larger effect sizes than human studies
- **Medium confidence**: LLMs are valuable for pilot testing and hypothesis validation; value alignment affects replication rates in socially sensitive topics
- **Low confidence**: Quantitative effect size comparisons between LLMs and humans; generalizability across different LLM architectures and prompting strategies

## Next Checks

1. **Temperature sensitivity analysis**: Systematically test how different temperature settings affect replication rates and effect sizes across various experimental paradigms, particularly for borderline significant cases.

2. **Sample size efficiency testing**: Compare statistical power and effect detection capabilities between human and LLM samples using varying sample sizes to determine optimal matching ratios rather than strict 1:1 correspondence.

3. **Mechanism isolation study**: Design experiments specifically to disentangle whether effect size amplification stems from reduced response variability, absence of social desirability bias, or other systematic processing differences between LLMs and humans.