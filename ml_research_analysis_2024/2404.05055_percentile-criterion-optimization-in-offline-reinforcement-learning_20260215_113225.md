---
ver: rpa2
title: Percentile Criterion Optimization in Offline Reinforcement Learning
arxiv_id: '2404.05055'
source_url: https://arxiv.org/abs/2404.05055
tags:
- tvar
- ambiguity
- sets
- operator
- follows
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel dynamic programming framework for
  optimizing the percentile criterion in offline reinforcement learning. The proposed
  VaR Bellman operator enables robust policy optimization without explicitly constructing
  ambiguity sets, addressing the conservativeness of Bayesian credible regions.
---

# Percentile Criterion Optimization in Offline Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2404.05055
- **Source URL**: https://arxiv.org/abs/2404.05055
- **Reference count**: 40
- **Primary result**: Introduces a VaR Bellman operator that achieves higher robust performance than Bayesian credible region methods by constructing smaller ambiguity sets.

## Executive Summary
This paper presents a novel dynamic programming framework for optimizing the percentile criterion in offline reinforcement learning. The proposed VaR Bellman operator enables robust policy optimization without explicitly constructing ambiguity sets, addressing the conservativeness of Bayesian credible regions. By using the α-percentile of the transition model posterior directly, the method implicitly creates tighter ambiguity sets than traditional BCR approaches while maintaining the same confidence guarantees.

The framework theoretically demonstrates that VaR ambiguity sets are asymptotically smaller than Bayesian credible regions, leading to less conservative policies. Experiments on four domains show that the VaR framework achieves higher robust performance compared to baseline methods, particularly in high-confidence settings. The method effectively balances robustness and performance while providing probabilistic guarantees on expected returns.

## Method Summary
The VaR framework uses a dynamic programming approach that directly optimizes the α-percentile (Value-at-Risk) of returns under model uncertainty. Instead of constructing explicit ambiguity sets like Bayesian credible regions, it samples from the posterior distribution and computes the α-percentile of returns using a specialized Bellman operator. The VaR Bellman operator satisfies contraction properties, ensuring convergence to a unique robust value function. Implementation involves posterior sampling via MCMC, VaR estimation using Quick Select for non-normal distributions, and value iteration until convergence. The method is evaluated against 7 baseline approaches across 4 test domains using 80/20 train/test splits on posterior samples.

## Key Results
- VaR ambiguity sets are asymptotically smaller than BCR sets by a factor of √χ²_{S−1,1−α}/Φ⁻¹(1−α)
- VaR framework outperforms BCR ℓ1 and BCR ℓ∞ in most domains for δ=0.05
- The method achieves higher robust performance while maintaining same confidence guarantees
- Sample complexity O(M) where M must be sufficiently large for accurate percentile estimation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VaR Bellman operator implicitly constructs smaller ambiguity sets than Bayesian credible regions
- Mechanism: The VaR operator uses the α-percentile of the transition model posterior directly, avoiding the construction of a full uncertainty set. This leads to value-function-dependent ambiguity sets that are tighter than fixed BCR sets.
- Core assumption: The posterior transition distribution is sub-Gaussian (often normal in asymptotic regime).
- Evidence anchors:
  - [abstract] "Our theoretical and empirical results show that our algorithm implicitly constructs much smaller ambiguity sets and learns less conservative robust policies."
  - [section 4] Theorem 4.3 proves that asymptotic radii of VaR ambiguity sets are smaller than BCR sets by a factor of √χ²_{S−1,1−α}/Φ⁻¹(1−α).
  - [corpus] Weak; nearest-neighbor papers do not discuss VaR vs BCR comparisons explicitly.
- Break condition: If posterior is highly skewed or heavy-tailed, VaRα may be unstable and VaR Bellman operator may no longer yield valid contraction.

### Mechanism 2
- Claim: VaR Bellman operator is a valid contraction mapping with a unique fixed point
- Mechanism: It satisfies monotonicity and translation subvariance properties, ensuring convergence to a unique robust value function.
- Core assumption: Transition probabilities are sub-Gaussian (e.g., Dirichlet posteriors under mild conditions).
- Evidence anchors:
  - [section 3] Proposition 3.1: "The operator TVaRα is contraction mapping on RS: ‖TVaRαu − TVaRαv‖∞ ≤ γ‖u − v‖∞."
  - [corpus] No direct match; related papers focus on distributional robustness or risk-sensitive RL, not VaR contraction properties.
- Break condition: If the VaR update is estimated with too few samples, empirical error may break monotonicity and contraction.

### Mechanism 3
- Claim: VaR framework achieves higher robust performance than BCR RMDPs
- Mechanism: Smaller ambiguity sets yield less conservative policies while maintaining the same confidence guarantees.
- Core assumption: Empirical VaRα estimates are sufficiently accurate with enough samples.
- Evidence anchors:
  - [abstract] "Experiments on four domains demonstrate that the VaR framework achieves higher robust performance compared to baseline methods..."
  - [section 5] Table 1 shows VaR outperforming BCR ℓ1 and BCR ℓ∞ in most domains for δ=0.05.
  - [corpus] No direct evidence; related works discuss robustness but not VaR vs BCR performance comparison.
- Break condition: If confidence level α is too high, VaR ambiguity sets may become too small and under-cover the true model, breaking guarantees.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: VaR framework optimizes percentile returns in MDPs under model uncertainty.
  - Quick check question: What is the Bellman optimality operator for an MDP with known transition probabilities?

- Concept: Bayesian credible regions and posterior sampling
  - Why needed here: VaR framework uses posterior samples instead of constructing explicit ambiguity sets.
  - Quick check question: How does a Dirichlet posterior on transition probabilities arise from observed transitions?

- Concept: Value at Risk (VaR) measure
  - Why needed here: The framework maximizes the α-percentile (VaR) of returns under model uncertainty.
  - Quick check question: How does VaRα differ from Conditional Value at Risk (CVaRα) in terms of convexity?

## Architecture Onboarding

- Component map:
  - Posterior sampling module → generates transition probability samples from Dirichlet/posterior
  - VaR Bellman update engine → computes max_a VaRα[˜p^T_{s,a}(r + γv)] for each state
  - Value iteration loop → iteratively applies VaR Bellman update until convergence
  - Policy extraction → greedy policy w.r.t. final VaR value function

- Critical path:
  1. Sample M transition models from posterior
  2. For each state-action pair, compute VaRα of 1-step returns
  3. Apply max over actions to get next value iterate
  4. Repeat until value change < ε

- Design tradeoffs:
  - Sample count M vs. estimation error: More samples reduce VaR estimation error but increase computation
  - Confidence level α vs. conservativeness: Lower α increases robustness but may yield overly conservative policies
  - Assumed distribution (Gaussian vs. none) vs. computation: Gaussian assumption allows closed-form VaR update

- Failure signatures:
  - High variance in VaR estimates across iterations → insufficient samples
  - Policy performance degrades with higher δ → VaR sets too small relative to true uncertainty
  - Value iteration fails to converge → monotonicity/contraction violated due to sampling noise

- First 3 experiments:
  1. Implement VaR value iteration on Riverswim MDP; compare robust returns vs. BCR ℓ1
  2. Vary α and δ; measure performance gap between VaR and BCR methods
  3. Test VaR value iteration with Gaussian vs. empirical VaR estimates; compare computation time and accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the VaR framework perform in continuous state-action space domains compared to discrete ones?
- Basis in paper: [explicit] The paper mentions that empirical analysis of the VaRα framework in domains with continuous state-action spaces is an interesting avenue for future work.
- Why unresolved: The current experiments are limited to discrete state-action spaces, and the scalability and effectiveness of the VaR framework in continuous spaces remains untested.
- What evidence would resolve it: Experimental results comparing the VaR framework's performance in both continuous and discrete domains, particularly in terms of robustness and computational efficiency.

### Open Question 2
- Question: Can the VaR framework be extended to consider correlations in the uncertainty of transition probabilities across states and actions?
- Basis in paper: [explicit] The paper identifies that a limitation of the VaR framework is its failure to consider correlations in the uncertainty of transition probabilities across states and actions.
- Why unresolved: Due to the non-convex nature of the percentile-criterion, constructing a tractable VaRα Bellman operator that considers these correlations is not feasible with current methods.
- What evidence would resolve it: Development and testing of a modified VaR framework or alternative approach that can handle correlations, with empirical results showing improved performance over the current VaR framework.

### Open Question 3
- Question: How does the VaR framework compare to methods optimizing the Conditional Value at Risk (CVaR) in terms of conservativeness and performance?
- Basis in paper: [inferred] The paper mentions that CVaR Bellman operator is convex and lower bounds the Value at Risk measure, suggesting it could be a potential alternative to address the conservativeness of the VaR framework.
- Why unresolved: The paper does not provide a direct comparison between the VaR framework and CVaR-based methods in terms of their performance and conservativeness.
- What evidence would resolve it: Comparative analysis of the VaR framework and CVaR-based methods in various domains, focusing on their performance, conservativeness, and computational requirements.

## Limitations

- Theoretical advantage relies on asymptotic normality of posterior, which may not hold for small sample sizes or multimodal posteriors
- Empirical VaR computation requires O(M) samples where M must be sufficiently large for accurate percentile estimation, creating computational bottleneck
- Confidence guarantees assume perfect posterior estimation, but MCMC sampling introduces approximation errors not accounted for in theoretical bounds

## Confidence

- **High Confidence**: The contraction property of the VaR Bellman operator (Proposition 3.1) - supported by rigorous proof and standard MDP theory
- **Medium Confidence**: The asymptotic radius comparison between VaR and BCR ambiguity sets (Theorem 4.3) - relies on asymptotic assumptions that may not hold in finite samples
- **Medium Confidence**: Experimental performance improvements over baselines - results show consistent gains but sample complexity and hyperparameter sensitivity are not fully explored

## Next Checks

1. **Sample Complexity Analysis**: Systematically vary the number of posterior samples M and confidence level α to quantify the tradeoff between VaR estimation error and computational cost.

2. **Non-Normal Posterior Testing**: Evaluate the framework on domains with known non-normal posteriors (e.g., multimodal or heavy-tailed) to assess robustness beyond Gaussian assumptions.

3. **Robustness to Sampling Error**: Introduce controlled MCMC approximation errors and measure degradation in VaR Bellman operator properties and policy performance.