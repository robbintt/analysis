---
ver: rpa2
title: Are Large Language Models Good Statisticians?
arxiv_id: '2406.07815'
source_url: https://arxiv.org/abs/2406.07815
tags:
- data
- statistical
- test
- column
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: StatQA is a benchmark designed to evaluate LLMs' statistical analysis
  skills, focusing on method applicability assessment. The authors synthesized 11,623
  examples by generating questions from target answers, incorporating prerequisite
  checks to ensure quality.
---

# Are Large Language Models Good Statisticians?

## Quick Facts
- arXiv ID: 2406.07815
- Source URL: https://arxiv.org/abs/2406.07815
- Reference count: 40
- Large language models achieve only 64.83% accuracy on statistical method applicability assessment, significantly below human performance

## Executive Summary
This paper introduces StatQA, a benchmark designed to evaluate large language models' proficiency in statistical analysis, specifically focusing on method applicability assessment. The authors develop an automated pipeline to construct 11,623 statistical tasks by synthesizing questions from target answers, incorporating prerequisite checks for quality. Through systematic experiments, they find that even advanced models like GPT-4o achieve only 64.83% accuracy, while open-source models perform significantly worse. Human experiments reveal distinct error patterns: LLMs primarily make applicability errors, while humans struggle with statistical task confusion. The study suggests complementary strengths between humans and LLMs, inviting further exploration of their collaborative potential in statistical analysis.

## Method Summary
The authors introduce StatQA, a benchmark constructed through an automated pipeline that synthesizes statistical tasks and their corresponding answers. The process starts with tabular data from real-world domains (education, medicine, science, engineering, economy, life) and generates target answers based on statistical methods. Questions are then synthesized in reverse, ensuring alignment between questions and answers. The benchmark covers five statistical task categories: Correlation Analysis, Contingency Table Test, Distribution Compliance Test, Variance Test, and Descriptive Statistics. Experiments evaluate representative LLMs (GPT-3.5, GPT-4, GPT-4o, LLaMA-2/3 variants) using various prompting strategies, including domain-specific prompts and fine-tuning. Human experiments with post-graduate students provide comparative baselines using both closed-book and open-book approaches.

## Key Results
- GPT-4o achieves only 64.83% accuracy on StatQA, significantly below human performance
- Open-source models (LLaMA-2/3 variants) perform poorly without fine-tuning, but show marked improvements after fine-tuning
- Domain knowledge prompting proves effective, with six out of seven non-fine-tuned LLMs achieving their best results with this strategy
- Humans and LLMs exhibit complementary error patterns: LLMs make applicability errors while humans struggle with task confusion
- GPT-4o with domain knowledge outperforms GPT-4 without knowledge prompting in most task categories

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reversing the dataset construction pipeline from target answers to questions enables precise alignment between questions and answers.
- Mechanism: Instead of collecting data → asking questions → annotating answers, the authors start with target answers derived from tabular data, synthesize questions via templates, and include prerequisite checks.
- Core assumption: Statistical questions generated in reverse from target answers will maintain semantic and contextual fidelity to the original task.
- Evidence anchors:
  - [abstract] "we introduce an automated pipeline to construct StatQA by synthesizing statistical tasks and their corresponding answers"
  - [section] "To implement this, we design an efficient pipeline for constructing StatQA, as shown in Figure 4. Unlike traditional methods, we set target answers A based on tabular data D and then synthesize statistical questions Q in reverse."

### Mechanism 2
- Claim: Domain knowledge prompting improves LLM performance by explicitly encoding statistical prerequisites and applicability conditions.
- Mechanism: Prompts include structured information about when specific statistical methods apply, which supplements the LLM's internal reasoning.
- Core assumption: LLMs can effectively utilize structured prerequisite knowledge in prompts to make better method applicability judgments.
- Evidence anchors:
  - [abstract] "we explore several strategies, including domain-specific prompts and fine-tuning"
  - [section] "The introduced domain knowledge prompting has proven effective, for six out of the seven non-fine-tuned LLMs obtained their best results in our experiments."

### Mechanism 3
- Claim: Human-LLM complementarity arises because humans excel at recognizing applicability while LLMs excel at task classification.
- Mechanism: Humans are prone to task confusion errors (confusing CTT with CA) but good at checking prerequisites; LLMs make fewer task confusion errors but struggle with applicability assessment.
- Core assumption: The distinct error profiles reflect fundamentally different cognitive strategies between humans and LLMs.
- Evidence anchors:
  - [abstract] "LLMs primarily make applicability errors, whereas humans mostly make statistical task confusion errors"
  - [section] "Humans and LLMs have distinct proficiencies and weaknesses in different aspects of selecting applicable statistical tasks"

## Foundational Learning

- Concept: Statistical method applicability assessment
  - Why needed here: The benchmark evaluates whether LLMs can choose correct statistical methods based on data characteristics and prerequisites.
  - Quick check question: Given a dataset with non-normal distribution and two groups, which variance test is appropriate: Bartlett or Levene?

- Concept: Hypothesis testing categories and their methods
  - Why needed here: StatQA covers five categories (CA, CTT, DCT, VT, DS) each with specific applicable methods.
  - Quick check question: For categorical variables, which test should you use to check independence: Pearson Correlation or Chi-square Independence Test?

- Concept: Reverse dataset synthesis
  - Why needed here: StatQA's construction pipeline starts with target answers to generate questions, ensuring alignment.
  - Quick check question: In reverse synthesis, what comes first: selecting the statistical method or formulating the question?

## Architecture Onboarding

- Component map: Data collection → Metadata extraction → Target method selection → Prerequisite checking → Computation → Question synthesis → Difficulty balancing → Dataset splitting → Refinement → Expert review → Benchmark construction → LLM evaluation → Human comparison

- Critical path: Data collection → Prerequisite checking → Question synthesis → Expert review → Benchmark construction → LLM evaluation → Human comparison

- Design tradeoffs:
  - Manual review vs automation: Ensures quality but limits scale
  - Reverse synthesis vs traditional: Better alignment but requires careful template design
  - Open-book vs closed-book: Measures knowledge application vs recall

- Failure signatures:
  - Invalid answers from smaller LLMs indicate task comprehension issues
  - Column selection errors suggest poor data understanding
  - Applicability errors show difficulty with prerequisite assessment

- First 3 experiments:
  1. Evaluate GPT-4o with and without domain knowledge on mini-StatQA to measure knowledge prompting impact
  2. Compare LLaMA-3-8B with LLaMA-2-7B to assess architectural improvements
  3. Run human experiments in both closed-book and open-book modes to establish baseline performance and error patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLMs be further improved to better assess the applicability of statistical methods beyond what was explored in this paper?
- Basis in paper: [explicit] The paper explicitly states that current LLMs struggle with accurately assessing the applicability of statistical methods, even with explicit domain knowledge.
- Why unresolved: The paper identifies this as a significant limitation but does not propose specific solutions or future research directions beyond general suggestions like integrating sophisticated reasoning mechanisms or leveraging multi-agent frameworks.
- What evidence would resolve it: Development and evaluation of new LLM architectures or training strategies specifically designed to improve the understanding and application of methodological prerequisites and contexts in statistical analysis.

### Open Question 2
- Question: What are the specific mechanisms by which human-AI collaboration could lead to optimal performance in statistical tasks, as suggested by the paper's findings?
- Basis in paper: [explicit] The paper highlights that humans and LLMs have distinct strengths and weaknesses in statistical tasks, suggesting potential for complementary collaboration.
- Why unresolved: While the paper identifies the potential for collaboration, it does not provide concrete examples or models of how such collaboration could be structured or what specific tasks each party should handle.
- What evidence would resolve it: Empirical studies demonstrating effective human-AI collaborative workflows in statistical analysis, including clear delineation of roles and measurable performance improvements over individual human or LLM performance.

### Open Question 3
- Question: How does the performance of LLMs on StatQA compare to their performance on other statistical benchmarks like DAEval and QRData, and what accounts for any differences?
- Basis in paper: [inferred] The paper discusses StatQA's focus on method applicability assessment, which is not the primary focus of other benchmarks like DAEval and QRData that emphasize computational accuracy.
- Why unresolved: The paper does not provide a direct comparison of LLM performance across these different benchmarks, nor does it analyze the reasons for any potential differences in performance.
- What evidence would resolve it: Comparative studies evaluating LLMs on multiple statistical benchmarks, with detailed analysis of task types, performance metrics, and potential factors influencing LLM success or failure across different benchmarks.

## Limitations

- The automated benchmark construction process may not fully capture the complexity of real-world statistical reasoning
- The evaluation metrics prioritize complete accuracy, potentially underestimating LLMs' partial understanding of statistical concepts
- Human experiments involved a limited sample size (6 participants) with specialized backgrounds, limiting generalizability

## Confidence

- High Confidence: Claims about the benchmark construction methodology and the basic performance hierarchy across models (GPT-4o > GPT-4 > GPT-3.5 > open-source models) are well-supported by experimental evidence.
- Medium Confidence: Claims about human-LLM complementarity and the distinct error patterns are supported but could benefit from larger human studies and more diverse evaluation scenarios.
- Low Confidence: Claims about the general statistical reasoning capabilities of LLMs based solely on method applicability assessment should be viewed cautiously, as this represents a narrow slice of statistical analysis.

## Next Checks

1. **External Validity Assessment**: Evaluate the same models on established statistical reasoning benchmarks (e.g., BIG-bench's statistical tasks) to verify whether StatQA results generalize to other testing frameworks.

2. **Real-world Application Study**: Conduct a case study where LLMs assist with actual statistical analysis tasks on real datasets, measuring both accuracy and time efficiency compared to human statisticians working independently.

3. **Error Analysis Expansion**: Perform detailed error analysis on a larger sample of human-LLM responses to identify whether the complementary error patterns persist across different statistical domains and task complexities.