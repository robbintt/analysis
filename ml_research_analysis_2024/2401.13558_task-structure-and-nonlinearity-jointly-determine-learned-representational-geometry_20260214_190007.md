---
ver: rpa2
title: Task structure and nonlinearity jointly determine learned representational
  geometry
arxiv_id: '2401.13558'
source_url: https://arxiv.org/abs/2401.13558
tags:
- input
- networks
- alignment
- network
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how the geometry of learned representations
  in neural networks is shaped by the interplay between input structure, target structure,
  and the activation function. Using parameterized tasks, the authors show that ReLU
  networks preserve more input geometry while Tanh networks better reflect target
  geometry.
---

# Task structure and nonlinearity jointly determine learned representational geometry

## Quick Facts
- arXiv ID: 2401.13558
- Source URL: https://arxiv.org/abs/2401.13558
- Reference count: 21
- Primary result: ReLU networks preserve more input geometry while Tanh networks better reflect target geometry

## Executive Summary
This paper investigates how the geometry of learned representations in neural networks is shaped by the interplay between input structure, target structure, and the activation function. Using parameterized synthetic tasks, the authors demonstrate that ReLU networks tend to preserve more of the input geometry, while Tanh networks better reflect the structure of the target function. The study provides a theoretical framework to analyze these differences, attributing them to the asymmetric asymptotic behavior of ReLU and the bounded nature of Tanh.

## Method Summary
The authors use parameterized synthetic tasks to systematically study the effects of activation functions on learned representations. They analyze learning dynamics in weight space and employ dimensionality reduction techniques to visualize and quantify representational geometry. The framework is validated across various task structures and network architectures, including two-layer, multi-layer, and convolutional networks.

## Key Results
- ReLU networks preserve more input geometry compared to Tanh networks
- Tanh networks better reflect target geometry than ReLU networks
- The asymmetric asymptotic behavior of ReLU and the bounded nature of Tanh explain the observed differences

## Why This Works (Mechanism)
The paper explains the observed differences in representational geometry through the distinct properties of ReLU and Tanh activation functions. ReLU's unbounded positive side and linear asymptotic behavior lead to the preservation of input geometry, while Tanh's bounded nature and smooth saturation cause representations to better align with target structure.

## Foundational Learning
1. **Representational Geometry**: The study of how neural networks organize and structure learned representations in their hidden layers.
   - Why needed: Understanding representational geometry is crucial for interpreting and improving neural network behavior.
   - Quick check: Compare t-SNE or UMAP visualizations of representations learned by ReLU vs. Tanh networks.

2. **Activation Function Properties**: The mathematical characteristics of activation functions that influence how information flows through a network.
   - Why needed: Different activation functions can lead to vastly different learned representations and network behaviors.
   - Quick check: Analyze the asymptotic behavior and boundedness of ReLU and Tanh functions.

3. **Parameterized Tasks**: Synthetic tasks with controlled input and target structures used to systematically study network behavior.
   - Why needed: Allows for controlled experiments to isolate the effects of specific factors on learned representations.
   - Quick check: Create simple synthetic datasets with known geometric properties and train networks on them.

## Architecture Onboarding
**Component Map**: Input data -> Parameterized task structure -> Neural network (ReLU/Tanh) -> Learned representations -> Analysis of representational geometry

**Critical Path**: The critical path involves the interaction between input structure, activation function, and target structure in shaping learned representations. The activation function's properties mediate how much of the input geometry is preserved versus how much the representation aligns with the target structure.

**Design Tradeoffs**: The choice of activation function involves a tradeoff between preserving input geometry (favoring ReLU) and aligning with target structure (favoring Tanh). This tradeoff has implications for tasks requiring input preservation (e.g., autoencoders) versus those focused on target alignment (e.g., classification).

**Failure Signatures**: If the activation function choice does not align with the task requirements, the network may fail to learn useful representations. For example, using Tanh for tasks requiring input preservation might lead to loss of important input features.

**First Experiments**:
1. Train a two-layer network on a simple parameterized task (e.g., spiral classification) using both ReLU and Tanh activations and compare the learned representations.
2. Vary the input and target structures of the parameterized task and observe how the choice of activation function affects the representational geometry.
3. Extend the analysis to deeper networks and convolutional architectures to assess the scalability of the observed effects.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but potential areas for further investigation include:
- How do these findings generalize to more complex, real-world tasks beyond the parameterized synthetic tasks studied?
- What is the impact of different training regimes, regularization techniques, or optimization algorithms on the observed representational geometry effects?
- Do the observed geometric principles apply to recurrent neural networks and transformer architectures for sequence modeling and attention-based tasks?

## Limitations
- The analysis focuses on two-layer networks, and while some experiments extend to deeper and convolutional architectures, the core theoretical framework may not fully capture the dynamics in more complex network structures.
- The findings are based on controlled experiments with parameterized synthetic tasks, and their generalizability to real-world, high-dimensional datasets remains to be thoroughly investigated.
- The paper does not extensively explore the impact of different training regimes, regularization techniques, or optimization algorithms on the observed representational geometry effects.

## Confidence
- ReLU preserves more input geometry: High
- Tanh better reflects target geometry: High
- Implications for generalization and transfer learning: Medium

## Next Checks
1. Test the framework on larger-scale image classification tasks (e.g., CIFAR-10/100) to assess how activation function choice affects representational geometry in practical scenarios.
2. Investigate the effects of different training schedules (e.g., curriculum learning, fine-tuning) on the learned representational geometry to understand how optimization dynamics interact with activation function properties.
3. Extend the analysis to recurrent neural networks and transformer architectures to determine if the observed geometric principles apply to sequence modeling and attention-based models.