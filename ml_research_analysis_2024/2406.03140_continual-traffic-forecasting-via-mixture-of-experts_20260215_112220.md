---
ver: rpa2
title: Continual Traffic Forecasting via Mixture of Experts
arxiv_id: '2406.03140'
source_url: https://arxiv.org/abs/2406.03140
tags: []
core_contribution: This paper addresses continual traffic forecasting in expanding
  networks where new sensors are added over time, leading to catastrophic forgetting
  if models are incrementally trained. The proposed Traffic Forecasting Mixture of
  Experts (TFMoE) segments traffic flows into homogeneous groups and assigns an expert
  model to each group, minimizing interference and alleviating forgetting.
---

# Continual Traffic Forecasting via Mixture of Experts

## Quick Facts
- arXiv ID: 2406.03140
- Source URL: https://arxiv.org/abs/2406.03140
- Authors: Sanghyun Lee; Chanyoung Park
- Reference count: 40
- Primary result: Outperforms baselines on PEMSD3-Stream with MAE 12.48 vs 13.84 at 15-minute horizon using only 1% of pre-existing nodes

## Executive Summary
This paper addresses the challenge of continual traffic forecasting in expanding networks where new sensors are added over time. Traditional models suffer from catastrophic forgetting when incrementally trained on new data. The proposed Traffic Forecasting Mixture of Experts (TFMoE) approach segments traffic flows into homogeneous groups and assigns dedicated expert models to each group, minimizing interference and preventing forgetting. Experiments demonstrate significant performance improvements over baseline methods while requiring fewer computational resources.

## Method Summary
TFMoE uses a pre-training stage for reconstruction-based clustering to segment sensors into homogeneous groups, followed by a localized adaptation stage. Each expert consists of a reconstructor (VAE) and predictor (GNN + 1D-Conv). The method incorporates reconstruction-based knowledge consolidation, forgetting-resilient sampling using synthetic data generation, and reconstruction-based replay to detect unfamiliar patterns. Final predictions combine individual expert predictions through a reconstruction-based gating mechanism.

## Key Results
- TFMoE achieves MAE of 12.48, RMSE of 20.48, and MAPE of 16.72 at 15-minute horizon on PEMSD3-Stream
- Outperforms baselines by significant margins (MAE: 12.48 vs 13.84, RMSE: 20.48 vs 22.52, MAPE: 16.72 vs 28.99)
- Uses only 1% of pre-existing nodes compared to 10%+ required by other methods
- Shows superior resilience against catastrophic forgetting in long-term streaming traffic networks

## Why This Works (Mechanism)

### Mechanism 1
Segmenting traffic flow into homogeneous groups with dedicated experts minimizes interference and prevents catastrophic forgetting. Each expert learns and adapts to specific traffic patterns without diluting prior knowledge.

### Mechanism 2
Reconstruction-based knowledge consolidation preserves prior knowledge while adapting to new patterns. The reconstructor classifies new nodes into groups based on previous parameters, with consolidation loss ensuring group assignments are maintained during training.

### Mechanism 3
Forgetting-resilient sampling and reconstruction-based replay mitigate catastrophic forgetting through synthetic data generation and pattern detection. Decoders generate diverse synthetic samples, while reconstruction probabilities identify unfamiliar sensors for replay.

## Foundational Learning

- Concept: Variational Autoencoder (VAE)
  - Why needed here: Used to reconstruct traffic flow and generate synthetic samples for preventing catastrophic forgetting
  - Quick check question: How does the VAE's evidence lower bound (ELBO) objective function help in learning a meaningful latent representation of traffic patterns?

- Concept: Graph Neural Networks (GNNs)
  - Why needed here: Capture spatial dependencies among sensors in the traffic network
  - Quick check question: How does the diffusion convolution operation in GNNs differ from traditional graph convolution, and why is it suitable for modeling traffic flow?

- Concept: Continual Learning
  - Why needed here: Model must continuously learn and adapt to evolving traffic patterns without forgetting previously learned knowledge
  - Quick check question: What are the main challenges in continual learning, and how does the proposed TFMoE approach address these challenges?

## Architecture Onboarding

- Component map: Pre-training stage (VAE reconstruction-based clustering) -> Localized adaptation stage (Mixture of experts with reconstructors and predictors)

- Critical path:
  1. Pre-train reconstructor using VAE on first task data
  2. Cluster sensors into homogeneous groups based on reconstructed representations
  3. Assign expert (reconstructor + predictor) to each cluster
  4. For each subsequent task: generate synthetic samples, detect unfamiliar sensors, aggregate data, train experts with consolidation loss

- Design tradeoffs:
  - Number of experts: More experts enable better specialization but increase training complexity
  - Sampling ratio: Higher ratios provide more diverse data but increase computational cost
  - Replay ratio: Higher replay ratios help prevent forgetting but may increase computational overhead

- Failure signatures:
  - Poor clustering leading to suboptimal expert specialization
  - Ineffective consolidation resulting in forgetting of previous knowledge
  - Insufficient diversity in synthetic samples or replayed sensors causing overfitting

- First 3 experiments:
  1. Verify clustering effectiveness by visualizing t-SNE embeddings of reconstructed representations
  2. Test forgetting-resilient sampling by comparing performance with and without synthetic samples
  3. Evaluate reconstruction-based replay by analyzing reconstruction probabilities across experts

## Open Questions the Paper Calls Out

### Open Question 1
How does TFMoE performance scale with the number of experts when traffic network size increases significantly? The paper only tests up to 6 experts on a network with 871 nodes, showing performance plateaus at 4 experts. Experiments on larger datasets with varying expert counts are needed to identify optimal scaling.

### Open Question 2
What is the impact of sampling ratio on model performance, and is there an optimal balance between performance and computational efficiency? The paper explores sampling ratios up to 30% and finds 10% optimal, but doesn't investigate beyond this range or explain the optimal choice.

### Open Question 3
How does the choice of temporal data range in the autoencoder affect clustering and subsequent model performance? The paper compares one week of data to other ranges and finds one week optimal, but doesn't explore other temporal ranges or investigate why this choice is best.

## Limitations
- Performance scaling with network size and number of experts remains unexplored
- Optimal sampling ratio and its relationship to computational efficiency not fully characterized
- Limited investigation of temporal data range impacts on clustering quality

## Confidence

High: Experimental results demonstrate clear performance improvements over baselines on the PEMSD3-Stream dataset with significant margins in MAE, RMSE, and MAPE metrics.

Medium: The knowledge consolidation mechanism using VAE-based reconstruction shows theoretical soundness but lacks direct validation of its effectiveness in preventing forgetting versus regularizing training.

Low: The reconstruction-based replay mechanism's individual contribution to preventing catastrophic forgetting could benefit from dedicated ablation studies.

## Next Checks

1. Test model performance when traffic patterns shift significantly between tasks to assess true continual learning capability under substantial distribution changes.

2. Conduct ablation studies to isolate and quantify the individual impact of each component (reconstruction loss, synthetic sampling, replay) on preventing catastrophic forgetting.

3. Evaluate method performance on datasets with different temporal scales and network structures to assess generalizability beyond the PEMSD3-Stream highway data.