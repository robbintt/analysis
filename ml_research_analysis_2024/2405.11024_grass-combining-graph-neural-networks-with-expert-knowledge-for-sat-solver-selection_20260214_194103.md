---
ver: rpa2
title: 'GraSS: Combining Graph Neural Networks with Expert Knowledge for SAT Solver
  Selection'
arxiv_id: '2405.11024'
source_url: https://arxiv.org/abs/2405.11024
tags:
- solver
- graph
- clause
- instances
- instance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes GraSS, the first graph neural network (GNN)
  based approach for SAT solver selection. GraSS represents SAT instances as literal-clause
  graphs with node features encoding domain knowledge, and uses a heterogeneous GNN
  architecture tailored to this graph structure.
---

# GraSS: Combining Graph Neural Networks with Expert Knowledge for SAT Solver Selection

## Quick Facts
- arXiv ID: 2405.11024
- Source URL: https://arxiv.org/abs/2405.11024
- Reference count: 40
- First graph neural network (GNN) based approach for SAT solver selection

## Executive Summary
GraSS introduces a novel graph neural network approach for SAT solver selection by representing SAT instances as literal-clause graphs with node features encoding domain knowledge. The method uses a heterogeneous GNN architecture specifically designed for this graph structure, incorporating clause positional encodings to handle clause order sensitivity and training with a runtime-sensitive loss function. On industrial benchmarks and the 2022 SAT Competition data, GraSS demonstrates state-of-the-art performance, improving both average runtime and the percentage of solved instances compared to traditional baselines like SATzilla and ArgoSmArT.

## Method Summary
GraSS represents SAT instances as literal-clause graphs where nodes correspond to literals and clauses, with edges connecting literals to the clauses they appear in. The method employs a heterogeneous GNN architecture that processes this graph structure while incorporating node features derived from expert knowledge about SAT solving. A key innovation is the introduction of clause positional encodings to capture the sensitivity of SAT solvers to clause order. The model is trained using a runtime-sensitive loss function that accounts for the actual performance characteristics of different SAT solvers. This approach enables GraSS to effectively predict which solver will perform best on a given instance, combining the representational power of GNNs with domain-specific knowledge.

## Key Results
- Achieves state-of-the-art performance on industrial benchmarks and 2022 SAT Competition data
- Improves average runtime and solved instance percentage compared to strong baselines including SATzilla and ArgoSmArT
- Ablation studies confirm the importance of graph architecture, node features, and clause positional encodings
- Demonstrates effectiveness of GNN-based approach for SAT solver selection

## Why This Works (Mechanism)
GraSS works by leveraging the graph structure of SAT instances to capture complex relationships between literals and clauses that are difficult to represent with traditional feature engineering. The heterogeneous GNN architecture is specifically designed to process literal-clause graphs, allowing the model to learn solver-specific patterns directly from the instance structure. Clause positional encodings address the known sensitivity of SAT solvers to clause ordering, which is critical for industrial instances. The runtime-sensitive loss function ensures that the model is optimized for the actual performance metric of interest rather than proxy objectives, leading to better real-world performance.

## Foundational Learning

**Graph Neural Networks**: Deep learning models designed to operate on graph-structured data
*Why needed*: SAT instances have inherent graph structure that can be exploited for solver selection
*Quick check*: Verify GNNs can effectively process literal-clause graphs with appropriate message passing

**Literal-Clause Graph Representation**: Modeling SAT instances as bipartite graphs between literals and clauses
*Why needed*: Captures the fundamental structure of SAT problems in a form amenable to GNN processing
*Quick check*: Ensure graph construction preserves all necessary information for solver performance prediction

**Clause Positional Encodings**: Additional features indicating the position of clauses in the original instance
*Why needed*: SAT solvers often exhibit sensitivity to clause order, which affects performance
*Quick check*: Verify positional encodings improve prediction accuracy on clause-order-sensitive instances

**Runtime-Sensitive Loss Functions**: Loss functions designed to optimize for actual runtime performance
*Why needed*: Traditional losses may not align with the goal of minimizing solver runtime
*Quick check*: Compare runtime-sensitive loss against standard regression losses on solver selection task

## Architecture Onboarding

**Component Map**: SAT Instance -> Literal-Clause Graph Construction -> Heterogeneous GNN -> Runtime Prediction -> Solver Selection
- Literal-Clause Graph Construction: Builds bipartite graph representation
- Heterogeneous GNN: Processes graph with node features and positional encodings
- Runtime Prediction: Outputs predicted solver performance
- Solver Selection: Chooses best solver based on predictions

**Critical Path**: Instance Representation → GNN Processing → Solver Prediction
The critical path involves transforming the SAT instance into a graph representation, processing it through the heterogeneous GNN layers, and producing solver performance predictions that inform the selection decision.

**Design Tradeoffs**: 
- Graph vs. flat representation: Graph captures structure but increases complexity
- Heterogeneous vs. homogeneous GNN: Better modeling of different node types vs. simplicity
- Positional encodings: Captures order sensitivity vs. additional parameter complexity
- Runtime-sensitive vs. standard loss: Better alignment with goals vs. potential training instability

**Failure Signatures**:
- Poor generalization to instances with different structural properties
- Overfitting to specific clause orderings in training data
- Sensitivity to graph construction parameters (e.g., node feature choices)
- Suboptimal performance on instances where solver choice is less dependent on structure

**First Experiments**:
1. Validate graph construction preserves essential instance information
2. Test GNN performance with and without positional encodings
3. Compare runtime-sensitive loss against standard regression loss

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, focusing instead on presenting the GraSS methodology and empirical results. However, implicit questions remain about the generalizability of the approach to different SAT instance types and the potential for further architectural improvements.

## Limitations

- Generalizability beyond tested industrial benchmark and 2022 SAT Competition data is uncertain
- Scalability to larger and more complex SAT instances remains unclear
- Performance comparison limited to traditional baselines, lacking recent GNN-based approaches
- Specific graph representation may not capture all relevant features for solver selection

## Confidence

High confidence: Improved performance on tested benchmarks compared to traditional baselines like SATzilla and ArgoSmArT
Medium confidence: Importance of graph architecture, node features, and clause positional encodings based on ablation studies
Low confidence: "State-of-the-art" performance claim without broader comparison to recent approaches

## Next Checks

1. Test GraSS on additional diverse SAT benchmark sets to assess generalizability
2. Compare GraSS's performance with other recently developed GNN-based SAT solver selection methods
3. Evaluate GraSS's scalability to larger and more complex SAT instances beyond those tested