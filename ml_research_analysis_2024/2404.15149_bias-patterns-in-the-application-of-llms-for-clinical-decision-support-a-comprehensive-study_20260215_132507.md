---
ver: rpa2
title: 'Bias patterns in the application of LLMs for clinical decision support: A
  comprehensive study'
arxiv_id: '2404.15149'
source_url: https://arxiv.org/abs/2404.15149
tags:
- uni00000013
- uni00000011
- uni0000001c
- uni00000014
- uni00000044
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates social biases in Large Language Models
  (LLMs) used for clinical decision support. The study evaluates eight popular LLMs,
  including general-purpose and clinically-trained models, across three question-answering
  datasets using clinical vignettes.
---

# Bias patterns in the application of LLMs for clinical decision support: A comprehensive study

## Quick Facts
- arXiv ID: 2404.15149
- Source URL: https://arxiv.org/abs/2404.15149
- Authors: Raphael Poulain; Hamed Fayyaz; Rahmatollah Beheshti
- Reference count: 40
- Primary result: LLMs exhibit significant demographic disparities in clinical decision support, with Chain of Thought prompting showing promise for bias reduction

## Executive Summary
This paper investigates social biases in Large Language Models (LLMs) used for clinical decision support across eight popular models including both general-purpose and clinically-trained variants. The study evaluates models using three question-answering datasets with clinical vignettes, revealing significant disparities across protected groups such as gender and race. Notably, clinically-tuned models like Palmyra-Med and Meditron showed biases against Hispanic women in pain medication recommendations, while larger models and fine-tuned medical models were not necessarily less biased than general-purpose alternatives. The research demonstrates that specific prompting strategies, particularly Chain of Thought approaches, can effectively reduce biased outcomes.

## Method Summary
The study evaluates eight LLMs across three clinical QA datasets (Q-Pain, Nurse Bias, Treatment Recommendation) using three prompting techniques: zero-shot, few-shot, and Chain of Thought. Statistical analysis employs Welch's ANOVA, t-tests, and Chi-Squared tests to identify disparities across protected groups including gender and race. The research compares both general-purpose models (LLaMa, Gemma, Mixtral, GPT-4, PaLM-2, Galactica) and clinically-tuned models (Palmyra-Med, Meditron) to assess bias patterns across different model architectures and training approaches.

## Key Results
- Statistically significant disparities found across protected groups in LLM clinical decision outputs
- Chain of Thought prompting demonstrated effective bias reduction compared to traditional methods
- Clinically-tuned models (Palmyra-Med, Meditron) exhibited biases, with Hispanic women less likely to receive pain medication
- Larger models and fine-tuned medical models were not necessarily less biased than general-purpose models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chain of Thought prompting reduces bias in LLMs by forcing explicit reasoning steps.
- Mechanism: CoT requires LLMs to articulate intermediate reasoning, which exposes and mitigates reliance on biased statistical shortcuts learned during training.
- Core assumption: Biased shortcuts are statistical patterns that do not reflect true clinical reasoning and can be overridden by explicit justification.
- Evidence anchors:
  - [abstract] "reflection-type approaches (like Chain of Thought) can reduce biased outcomes effectively"
  - [section] "Chain of Thought (CoT) approach... by encouraging LLMs to articulate their reasoning steps, can demonstrably reduce bias compared to traditional prompting methods"
  - [corpus] Weak evidence - no direct citations in corpus about CoT bias mitigation
- Break condition: If CoT prompts do not require justification or if the LLM bypasses reasoning steps through pattern matching.

### Mechanism 2
- Claim: Domain-specific fine-tuning does not guarantee reduced bias compared to general-purpose models.
- Mechanism: Fine-tuning on medical data may amplify existing biases if the training corpus contains skewed demographic representations or biased clinical decision patterns.
- Core assumption: The quality and representativeness of the fine-tuning dataset determines bias outcomes more than the model architecture.
- Evidence anchors:
  - [abstract] "fined-tuned models on medical data not being necessarily better than the general-purpose models"
  - [section] "Palmyra-Med and Meditron, another clinically-tuned model, exhibited biases... with Hispanic women less likely to receive pain medication"
  - [corpus] No direct corpus evidence supporting this mechanism
- Break condition: If fine-tuning data is carefully curated for demographic balance and bias detection.

### Mechanism 3
- Claim: Prompt design significantly influences LLM bias patterns in clinical decision support.
- Mechanism: Different prompting strategies (zero-shot, few-shot, CoT) activate different reasoning pathways, with some strategies amplifying or mitigating demographic biases.
- Core assumption: The way questions are framed affects how LLMs retrieve and apply learned patterns.
- Evidence anchors:
  - [abstract] "specific phrasing can influence bias patterns"
  - [section] "zero-shot prompting tends to have the most extreme evidence of fairness... especially for Meditron"
  - [corpus] No direct corpus evidence supporting this mechanism
- Break condition: If prompt design does not affect the LLM's internal reasoning process or if all prompts activate the same bias pathways.

## Foundational Learning

- Concept: Statistical bias amplification in machine learning
  - Why needed here: Understanding how biases in training data can be amplified by model predictions is crucial for interpreting why LLMs exhibit demographic disparities
  - Quick check question: What is the difference between bias in training data versus bias in model predictions?

- Concept: Prompt engineering and its impact on LLM behavior
  - Why needed here: Different prompting strategies can activate different reasoning pathways, affecting both performance and bias patterns
  - Quick check question: How does Chain of Thought prompting differ from zero-shot prompting in terms of LLM reasoning?

- Concept: Fairness metrics and statistical testing
  - Why needed here: Evaluating bias requires appropriate statistical tests to determine if observed disparities are significant
  - Quick check question: When would you use Welch's ANOVA versus Pearson's Chi-Squared test for bias evaluation?

## Architecture Onboarding

- Component map: Clinical vignette → Prompt generation → LLM inference → Response analysis → Statistical testing → Bias assessment
- Critical path: Clinical vignette → Prompt generation → LLM inference → Response analysis → Statistical testing → Bias assessment
- Design tradeoffs: Model size vs. bias (larger models not necessarily less biased), domain-specific tuning vs. general capability, prompt complexity vs. interpretability
- Failure signatures: Significant p-values in demographic comparisons, inconsistent model behavior across similar tasks, unexpected model preferences for specific demographic groups
- First 3 experiments:
  1. Replicate the Q-Pain dataset evaluation with zero-shot prompting across all models to establish baseline bias patterns
  2. Implement Chain of Thought prompting on the same dataset to measure bias reduction effectiveness
  3. Compare clinically-tuned models (Meditron, Palmyra-Med) against general-purpose models (GPT-4, Gemma) on treatment recommendation tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do biases in LLMs for clinical decision support interact with clinician judgment and real-world healthcare workflows?
- Basis in paper: [inferred] The paper mentions this as a limitation, noting that their evaluation focuses on the inherent biases within the LLMs themselves and acknowledges that these biases might interact with factors like clinician judgment and real-world healthcare workflows in complex ways.
- Why unresolved: The paper's experimental design isolates LLM performance, making it difficult to determine the extent to which biases are amplified, mitigated, or transformed when LLMs are integrated into actual clinical settings where human decision-making is involved.
- What evidence would resolve it: Studies that deploy LLMs in real clinical environments, measuring the interplay between LLM outputs, clinician decisions, and patient outcomes across diverse demographics. This would require careful experimental design to isolate the impact of LLM bias on the overall decision-making process.

### Open Question 2
- Question: To what extent do biases in LLMs for clinical decision support stem from statistical patterns in training data that do not reflect reality?
- Basis in paper: [inferred] The paper suggests that biases might arise from LLMs learning "statistical patterns that don't necessarily reflect reality" and that Chain of Thought prompting can reduce reliance on these potentially biased shortcuts.
- Why unresolved: The paper does not delve into the specific sources of bias within the training data or provide a detailed analysis of how these biases manifest in LLM outputs. Further investigation is needed to understand the relationship between training data characteristics and observed biases.
- What evidence would resolve it: Analysis of LLM training data to identify overrepresented or underrepresented groups and their association with health outcomes. Correlation studies between training data biases and LLM output disparities would provide insights into the origin of these biases.

### Open Question 3
- Question: How can we develop more comprehensive frameworks for evaluating fairness in LLMs across key dimensions such as tasks, datasets, prompting techniques, and models?
- Basis in paper: [explicit] The paper explicitly states that "a comprehensive framework for evaluating LLM fairness across key dimensions such as different tasks, datasets, prompting techniques, and models remains necessary."
- Why unresolved: While the paper conducts a comprehensive study across various models, tasks, and prompting techniques, it acknowledges the need for more extensive evaluations to fully understand the scope of bias in LLMs for clinical decision support.
- What evidence would resolve it: Development and validation of standardized evaluation frameworks that incorporate diverse tasks, datasets, and prompting techniques. These frameworks should also consider additional factors such as model architecture, training data characteristics, and deployment contexts to provide a holistic assessment of fairness.

## Limitations

- Evaluation limited to three specific clinical vignette datasets that may not represent full diversity of real-world clinical scenarios
- Statistical significance testing establishes disparities exist but does not quantify clinical significance or potential harm
- Study does not investigate root causes of bias persistence in clinically-tuned models or examine training data characteristics
- Effectiveness of Chain of Thought prompting demonstrated but mechanism remains speculative without deeper analysis of model reasoning

## Confidence

- **High confidence**: The existence of demographic disparities in LLM clinical decision support outputs
- **Medium confidence**: Chain of Thought prompting reduces bias
- **Medium confidence**: Fine-tuned clinical models show similar or worse bias than general models
- **Low confidence**: Specific prompt design recommendations for bias mitigation

## Next Checks

1. Replicate with expanded datasets: Test the same LLMs on additional clinical vignette datasets covering diverse medical specialties and patient populations to verify if bias patterns persist across different clinical contexts.

2. Analyze bias mechanism in fine-tuning: Examine the training data used for clinically-tuned models (Meditron, Palmyra-Med) to identify whether demographic imbalances in the fine-tuning corpus correlate with observed bias patterns.

3. Evaluate clinical impact of bias reduction: Conduct expert review of LLM outputs with and without Chain of Thought prompting to assess whether bias reduction comes at the cost of clinical accuracy or appropriateness of recommendations.