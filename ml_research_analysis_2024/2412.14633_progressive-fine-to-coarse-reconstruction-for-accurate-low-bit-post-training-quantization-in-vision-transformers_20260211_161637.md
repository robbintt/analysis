---
ver: rpa2
title: Progressive Fine-to-Coarse Reconstruction for Accurate Low-Bit Post-Training
  Quantization in Vision Transformers
arxiv_id: '2412.14633'
source_url: https://arxiv.org/abs/2412.14633
tags:
- reconstruction
- quantization
- performance
- granularity
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of post-training quantization
  (PTQ) for vision transformers (ViTs) when quantized into low-bit representations,
  which often results in significant performance drops. The authors propose a Progressive
  Fine-to-Coarse Reconstruction (PFCR) method to mitigate this issue.
---

# Progressive Fine-to-Coarse Reconstruction for Accurate Low-Bit Post-Training Quantization in Vision Transformers

## Quick Facts
- arXiv ID: 2412.14633
- Source URL: https://arxiv.org/abs/2412.14633
- Authors: Rui Ding; Liang Yong; Sihuan Zhao; Jing Nie; Lihui Chen; Haijun Liu; Xichuan Zhou
- Reference count: 9
- Primary result: Achieves 75.61% top-1 accuracy for 3-bit quantized ViT-B in post-training quantization on ImageNet

## Executive Summary
This paper addresses the challenge of post-training quantization (PTQ) for vision transformers (ViTs) when quantized into low-bit representations, which often results in significant performance drops. The authors propose a Progressive Fine-to-Coarse Reconstruction (PFCR) method to mitigate this issue. PFCR progressively reconstructs the quantized model from fine granularity to coarse granularity, starting with multi-head self-attention (MHSA) and multi-layer perceptron (MLP) modules as the finest reconstruction units, and then iteratively combining and reconstructing coarser blocks. Additionally, a Progressive Optimization Strategy (POS) is introduced to further improve model performance. Experimental results on the ImageNet dataset demonstrate that the proposed method achieves state-of-the-art performance, particularly attaining 75.61% top-1 accuracy for 3-bit quantized ViT-B in PTQ. The effectiveness and generalization of the method are also validated on the COCO dataset for object detection and instance segmentation tasks.

## Method Summary
The paper proposes a Progressive Fine-to-Coarse Reconstruction (PFCR) method for post-training quantization of vision transformers. The approach works by first reconstructing fine-grained components (MHSA and MLP modules) independently, then progressively merging and reconstructing coarser blocks in a hierarchical manner. This fine-to-coarse approach addresses the challenge that low-bit quantization severely degrades ViT performance due to the complex interactions between different transformer components. The method also incorporates a Progressive Optimization Strategy (POS) that further refines the quantized model through iterative optimization. The reconstruction process is guided by maintaining the representational capacity of the quantized model while reducing quantization errors progressively from fine to coarse granularity.

## Key Results
- Achieves 75.61% top-1 accuracy for 3-bit quantized ViT-B on ImageNet, state-of-the-art for PTQ
- Demonstrates effectiveness on COCO dataset for object detection and instance segmentation tasks
- Shows progressive improvement from fine to coarse reconstruction stages

## Why This Works (Mechanism)
The method works by addressing the fundamental challenge that low-bit quantization introduces significant quantization errors that accumulate across transformer layers. By starting with fine-grained reconstruction of individual MHSA and MLP modules, the method can accurately capture their distinct characteristics before combining them. The progressive approach allows the model to first establish accurate low-level representations before building up to higher-level abstractions. The POS component provides additional refinement by iteratively optimizing the reconstructed model, helping to mitigate any residual quantization errors that persist after the initial reconstruction phase.

## Foundational Learning
- **Post-training quantization (PTQ)**: Why needed - enables efficient deployment without full retraining; Quick check - verify quantized model runs on target hardware
- **Vision transformers (ViTs)**: Why needed - foundation architecture being quantized; Quick check - confirm baseline ViT performance before quantization
- **Multi-head self-attention (MHSA)**: Why needed - core component of ViTs that is sensitive to quantization errors; Quick check - verify attention weights distribution before/after quantization
- **Multi-layer perceptron (MLP)**: Why needed - another key ViT component that requires careful quantization; Quick check - measure activation ranges for optimal bit allocation
- **Progressive reconstruction**: Why needed - enables hierarchical error correction; Quick check - verify reconstruction error decreases at each stage
- **Quantization error propagation**: Why needed - understanding how errors accumulate across layers; Quick check - measure error accumulation across transformer blocks

## Architecture Onboarding

**Component Map**
Quantized ViT -> PFCR Module (MHSA + MLP Reconstruction) -> Progressive Merging -> POS Refinement -> Final Quantized Model

**Critical Path**
1. Initial low-bit quantization of ViT
2. Independent reconstruction of MHSA and MLP modules
3. Progressive merging and reconstruction of transformer blocks
4. POS-based iterative optimization
5. Final evaluation on target task

**Design Tradeoffs**
- Granularity vs. reconstruction complexity: finer granularity allows more accurate reconstruction but increases computational cost
- Progressive stages vs. direct reconstruction: staged approach reduces error accumulation but requires more optimization steps
- POS iterations vs. training time: more iterations improve accuracy but increase deployment preparation time

**Failure Signatures**
- Performance degradation in early transformer layers
- Loss of attention diversity after quantization
- Activation distribution shift in MLP layers
- Accumulation of quantization errors across layers

**3 First Experiments**
1. Baseline comparison: 3-bit quantized ViT vs. PFCR-reconstructed 3-bit ViT on ImageNet
2. Ablation study: PFCR with vs. without POS on the same quantization task
3. Layer-wise analysis: Compare quantization error propagation in baseline vs. PFCR approach

## Open Questions the Paper Calls Out
None

## Limitations
- Experiments primarily conducted on ImageNet and COCO datasets with ViT-B architecture, limiting generalizability
- Computational overhead of progressive reconstruction process not thoroughly analyzed
- Performance on extremely low-bit quantizations (1-2 bits) not explored in depth
- No ablation studies isolating the impact of POS from PFCR components

## Confidence
- **High confidence**: The effectiveness of PFCR on standard benchmarks (ImageNet) for moderate bit-widths (3-4 bits)
- **Medium confidence**: Generalization to other ViT architectures and downstream tasks based on limited COCO results
- **Medium confidence**: The POS strategy's contribution to overall performance without proper ablation studies

## Next Checks
1. Conduct comprehensive ablation studies to isolate the contributions of PFCR and POS components
2. Evaluate performance across multiple ViT architectures (DeiT, Swin, PVT) and diverse datasets beyond ImageNet
3. Measure and report computational overhead and memory usage during the progressive reconstruction process