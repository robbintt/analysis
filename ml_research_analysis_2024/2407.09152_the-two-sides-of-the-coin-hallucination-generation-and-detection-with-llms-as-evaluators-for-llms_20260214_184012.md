---
ver: rpa2
title: 'The Two Sides of the Coin: Hallucination Generation and Detection with LLMs
  as Evaluators for LLMs'
arxiv_id: '2407.09152'
source_url: https://arxiv.org/abs/2407.09152
tags:
- hyp2
- hyp1
- source
- task
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work investigates the capability of LLMs to generate and
  detect hallucinations in both machine translation and paraphrasing tasks. The study
  employs four LLMs: Llama 3, Gemma, GPT-3.5 Turbo, and GPT-4, along with ensemble
  majority voting for detection.'
---

# The Two Sides of the Coin: Hallucination Generation and Detection with LLMs as Evaluators for LLMs

## Quick Facts
- arXiv ID: 2407.09152
- Source URL: https://arxiv.org/abs/2407.09152
- Reference count: 40
- Four LLMs (Llama 3, Gemma, GPT-3.5 Turbo, GPT-4) used for generation and detection tasks

## Executive Summary
This study investigates the dual capability of large language models to both generate and detect hallucinations in machine translation and paraphrasing tasks. Using four different LLMs and ensemble majority voting, the research demonstrates that GPT-4 achieves the highest accuracy in hallucination detection across both task types. The work highlights the importance of prompt engineering, identifies challenges such as gender bias and unit conversions, and shows that while LLMs show promise for hallucination detection, careful prompt design and awareness of limitations are essential for reliable performance.

## Method Summary
The study employs four LLMs (Llama 3, Gemma, GPT-3.5 Turbo, GPT-4) for both hallucination generation and detection tasks using multilingual datasets for paraphrasing (English, Swedish) and translation (de-en, en-de, fr-en, en-fr). For generation, a zero-shot text classification NLI model evaluates hyp+ entailment and hyp- contradiction. Detection uses four prompt types (simple, complex, with examples, full task description) and ensemble majority voting across all four models. Evaluation metrics include accuracy, F1-score, precision, recall, Matthews Correlation Coefficient, and Cohen's Kappa.

## Key Results
- GPT-4 achieved the highest accuracy in hallucination detection across both paraphrasing and translation tasks
- Majority voting improved overall detection performance by aggregating predictions from multiple models
- Llama 3 showed strong performance in generating coherent hyp+/hyp- pairs
- Prompt engineering significantly influenced LLM performance, with simpler prompts sometimes outperforming complex ones

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM hallucination detection works because the model can compare two hypotheses against the source text and identify contradictions or unsupported information.
- Mechanism: The LLM internally evaluates semantic consistency between the source and each hypothesis, flagging the one that introduces information not present or contradicted by the source.
- Core assumption: The LLM has sufficient world knowledge and reasoning capability to recognize when a hypothesis introduces novel or conflicting information relative to the source.
- Evidence anchors: The study employs four LLMs for hallucination detection, showing GPT-4 achieved highest accuracy; detection task involves presenting the LLM with a source sentence and two hypotheses; "Robust Hallucination Detection in LLMs via Adaptive Token Selection" suggests internal representations contain truthfulness hints.
- Break condition: The LLM lacks sufficient knowledge about the domain or encounters subtle linguistic ambiguities that make contradiction detection unreliable.

### Mechanism 2
- Claim: Ensemble majority voting improves detection reliability by aggregating predictions from multiple diverse models.
- Mechanism: Different LLMs may have varying strengths and weaknesses; combining their outputs through voting reduces individual model bias and increases overall accuracy.
- Core assumption: The models' errors are sufficiently independent so that majority voting can correct individual mistakes.
- Evidence anchors: "We employed ensemble majority voting to incorporate all four models for the detection task"; "We opted for a straightforward voting approach to ensemble model predictions"; The majority voting model achieved impressive performance with highest average MCC and Kappa.
- Break condition: Models make correlated errors or the dataset is too small for reliable voting patterns to emerge.

### Mechanism 3
- Claim: Prompt engineering significantly influences LLM performance in both generation and detection tasks.
- Mechanism: Carefully crafted prompts provide better task guidance, examples, and formatting requirements that help LLMs understand what constitutes hallucination versus factual content.
- Core assumption: LLMs are sensitive to prompt structure and can be guided to perform better through explicit instructions and examples.
- Evidence anchors: "We experimented with four distinct prompting techniques to provide better guidance to the LLMs"; "The performance of the 'Gemma' model varied significantly based on the complexity of the prompts used"; "We compared the performance of different models on the trial dataset using distinct prompts."
- Break condition: Prompts become too complex or introduce ambiguity, causing the LLM to focus on irrelevant aspects of the task.

## Foundational Learning

- Concept: Semantic consistency checking
  - Why needed here: Core mechanism for hallucination detection relies on comparing hypotheses against source text for contradictions
  - Quick check question: Can you explain how an LLM determines if a statement contradicts its source text?

- Concept: Ensemble methods and majority voting
  - Why needed here: Used to combine predictions from multiple LLMs to improve detection accuracy
  - Quick check question: What conditions must be met for majority voting to improve classification performance?

- Concept: Prompt engineering and few-shot learning
  - Why needed here: Different prompt structures significantly impact LLM performance in both generation and detection tasks
  - Quick check question: How do examples and explicit instructions in prompts influence LLM behavior?

## Architecture Onboarding

- Component map: Data ingestion -> LLM inference layer (4 models) -> Prompt management -> Voting mechanism -> Evaluation layer
- Critical path: 1. Load source-hypothesis pairs; 2. Apply appropriate prompt template to each LLM; 3. Collect predictions from all models; 4. Apply majority voting (or use individual model if needed); 5. Calculate evaluation metrics; 6. Log results and errors for analysis
- Design tradeoffs: Model diversity vs. cost (using both open-source and closed-source models); Prompt complexity vs. reliability (simpler prompts may work better for some models); Voting vs. weighted aggregation (simple majority voting chosen due to small trial set size)
- Failure signatures: High variance in predictions across models (indicates prompt issues or ambiguous samples); Systematic errors on specific hallucination types (e.g., gender bias, date conversions); Models failing to produce any output for certain samples; Performance degradation on specific language pairs
- First 3 experiments: 1. Test individual LLMs with simplest prompt on trial dataset to establish baseline; 2. Test same LLMs with most detailed prompt to see if performance improves; 3. Implement majority voting across all four models and compare against individual model performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ensemble majority voting compare to other ensemble methods like weighted voting or stacking in hallucination detection tasks?
- Basis in paper: The paper uses simple majority voting for ensemble learning but does not compare it to other ensemble methods.
- Why unresolved: The paper only implements and evaluates simple majority voting, leaving the comparative performance of other ensemble methods unexplored.
- What evidence would resolve it: A systematic comparison of ensemble methods (weighted voting, stacking, etc.) against majority voting on the same hallucination detection datasets would clarify the relative effectiveness of different approaches.

### Open Question 2
- Question: To what extent does the effectiveness of LLMs in hallucination detection depend on the quality and diversity of the training data used to fine-tune the models?
- Basis in paper: The paper uses zero-shot text classification NLI models for evaluation, implying that model performance may be sensitive to training data quality.
- Why unresolved: The paper does not investigate the relationship between training data quality/diversity and LLM performance in hallucination detection.
- What evidence would resolve it: Experiments varying the quality and diversity of training data for fine-tuning LLMs on hallucination detection tasks would reveal the impact of data characteristics on model performance.

### Open Question 3
- Question: Can the observed gender bias in Llama 3's translations be mitigated through targeted prompt engineering or fine-tuning, and what are the limitations of these approaches?
- Basis in paper: The paper notes instances where Llama 3 exhibits gender bias in translations, such as misinterpreting "Wirtschaftspr√ºferin" (female auditor) as gender-neutral.
- Why unresolved: The paper identifies the gender bias issue but does not explore potential mitigation strategies or their effectiveness.
- What evidence would resolve it: Experiments testing various prompt engineering techniques and fine-tuning approaches to address gender bias in Llama 3's translations would demonstrate the feasibility and limitations of these mitigation strategies.

## Limitations

- Dataset size limitations: Small sample sizes (average 245 samples per translation direction) may limit robustness of findings
- LLM-based evaluation reliability: Heavy reliance on automated LLM metrics rather than human judgment raises questions about assessment accuracy
- Prompt engineering transfer: Manual prompt tuning may not generalize well across different domains or languages
- Systematic model weaknesses: Gender bias and unit/date conversion issues suggest limitations in how LLMs handle certain linguistic features

## Confidence

- **High confidence**: GPT-4's superior performance in hallucination detection across both paraphrasing and translation tasks is well-supported by the data
- **Medium confidence**: Effectiveness of prompt engineering on Llama 3 for generation tasks is demonstrated, but variability across models suggests limited generalizability
- **Low confidence**: Reliability of LLM-based evaluation metrics for generation quality requires more validation against human judgments

## Next Checks

1. Conduct human evaluation validation on a subset of detection results to verify reliability of LLM-based evaluations, particularly focusing on cases where models disagree or where gender bias is suspected.

2. Systematically evaluate how detection accuracy scales with dataset size by testing on progressively larger subsets of available data to determine if current performance is limited by sample size.

3. Apply the same generation and detection pipeline to a different domain (e.g., medical or legal text) to assess whether prompt engineering techniques and model preferences transfer beyond current multilingual translation and paraphrasing tasks.