---
ver: rpa2
title: 'MathGAP: Out-of-Distribution Evaluation on Problems with Arbitrarily Complex
  Proofs'
arxiv_id: '2410.13502'
source_url: https://arxiv.org/abs/2410.13502
tags:
- problems
- problem
- apples
- proof
- than
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MathGAP, a framework for generating arithmetic
  word problems with arbitrary proof complexity to evaluate LLM reasoning generalization.
  The method constructs problems from proof trees using logical forms and inference
  rules, enabling controlled generation of problems with specified depth, width, and
  shape characteristics.
---

# MathGAP: Out-of-Distribution Evaluation on Problems with Arbitrarily Complex Proofs

## Quick Facts
- arXiv ID: 2410.13502
- Source URL: https://arxiv.org/abs/2410.13502
- Reference count: 40
- This paper introduces MathGAP, a framework for generating arithmetic word problems with arbitrary proof complexity to evaluate LLM reasoning generalization.

## Executive Summary
MathGAP is a framework for generating arithmetic word problems with controllable proof complexity to evaluate large language model reasoning capabilities. The method constructs problems from proof trees using logical forms and inference rules, enabling systematic generation of problems with specified depth, width, and shape characteristics. Experiments with multiple models show performance decreases as proof complexity increases, with nonlinear problems being particularly challenging and models showing sensitivity to sentence ordering.

## Method Summary
MathGAP generates arithmetic word problems by constructing proof trees using logical forms (quantities, agents, and their relationships) and inference rules that define how to derive new quantities. The framework builds problems by specifying proof tree shapes (linear, diamond, nonlinear) and then sampling logical forms to fill the tree structure. Inference rules like assign, add, and subtract operations are applied to combine quantities into intermediate results until the final answer is derived. The problems are then rendered into natural language text with controlled sentence ordering to test model robustness to presentation variations.

## Key Results
- Model performance decreases significantly as proof tree depth and width increase
- Nonlinear proof structures are more challenging than linear or diamond-shaped proofs
- Models are sensitive to sentence ordering, with performance varying based on permutation distance from canonical order
- Even state-of-the-art models like OpenAI o1 and DeepSeek-R1 struggle on the most complex MathGAP-generated problems

## Why This Works (Mechanism)
MathGAP works by providing a systematic method to generate arithmetic problems with precisely controlled complexity characteristics. By constructing problems from proof trees rather than natural language patterns, the framework can create genuinely novel problem structures that test reasoning capabilities rather than pattern matching. The controlled generation allows researchers to isolate specific aspects of problem complexity and measure their impact on model performance, revealing limitations in current LLMs' ability to handle multi-step reasoning with arbitrary proof structures.

## Foundational Learning
- **Proof trees and logical forms**: Understanding how mathematical proofs can be represented as tree structures with logical relationships between quantities is essential for grasping MathGAP's generation method.
- **Inference rules for arithmetic**: The specific rules (assign, add, subtract, multiply, divide) that define how quantities combine in mathematical reasoning form the core of MathGAP's problem construction.
- **Sentence permutation effects**: Recognizing how word order affects model performance is crucial for understanding the limitations of current LLMs in handling flexible problem presentations.
- **Proof complexity metrics**: Concepts like depth, width, and shape of proof trees provide the framework for measuring and comparing problem difficulty levels.
- **Out-of-distribution evaluation**: Understanding what makes problems truly out-of-distribution versus just rare or difficult is key to interpreting MathGAP's contribution to LLM evaluation.

## Architecture Onboarding

### Component Map
Logical Forms -> Proof Tree Generator -> Inference Rules -> Problem Renderer -> Evaluation Pipeline

### Critical Path
1. Define proof tree shape (depth, width, structure)
2. Sample logical forms to populate tree nodes
3. Apply inference rules to derive quantities
4. Render natural language problem
5. Evaluate model performance

### Design Tradeoffs
The framework trades computational complexity for control over problem generation. While generating problems through proof trees is more computationally intensive than sampling from existing datasets, it provides precise control over complexity characteristics and ensures true out-of-distribution generation.

### Failure Signatures
- Models fail more frequently on deeper proof trees
- Nonlinear structures cause disproportionate difficulty
- Sentence ordering significantly impacts performance
- In-context examples show inconsistent benefits

### First Experiments
1. Generate a linear proof tree with depth 3 and test model performance
2. Create a diamond-shaped proof tree with width 2 and evaluate sensitivity to sentence ordering
3. Compare performance on problems with identical logical forms but different proof tree structures

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does increasing the variety of logical forms and inference rules (beyond the current set) significantly impact the generalization capabilities of LLMs on complex arithmetic problems?
- Basis in paper: [inferred] The paper mentions that MathGAP can be extended by adding new logical forms and inference rules, but does not explore this systematically.
- Why unresolved: The current study focuses on a specific set of logical forms and inference rules. Exploring a broader range could reveal whether the observed generalization patterns are robust across different problem structures.
- What evidence would resolve it: Experiments with MathGAP using expanded logical form sets and varied inference rules, measuring how performance changes across different complexity metrics.

### Open Question 2
- Question: What is the relationship between the order of sentences in the problem text and the model's ability to solve nonlinear problems with deeper proof trees?
- Basis in paper: [explicit] The paper demonstrates that LLMs are sensitive to sentence ordering, with performance varying based on the distance of sentence movement from the canonical order.
- Why unresolved: While the study examines linear problems with permuted orderings, it does not investigate how sentence order affects performance on more complex nonlinear problems.
- What evidence would resolve it: Systematic experiments testing various permutations of sentence order in nonlinear problems with different proof tree depths and shapes.

### Open Question 3
- Question: How do token biases affect the performance of LLMs on MathGAP-generated problems, and can these biases be mitigated through prompt engineering?
- Basis in paper: [explicit] The paper mentions that token biases were not considered in the current study but could be investigated in future work.
- Why unresolved: The study does not analyze whether specific tokens, agents, or quantities disproportionately affect model performance.
- What evidence would resolve it: Analysis of model outputs to identify token-level patterns in errors, followed by experiments testing different prompting strategies to mitigate these biases.

## Limitations
- The evaluation relies primarily on open-source models alongside proprietary models, creating potential bias in generalizability of findings
- The study focuses on a limited set of models and does not explore whether different architectures might better handle complex proofs
- While sensitivity to sentence ordering is noted, the investigation lacks depth to determine if this reflects fundamental reasoning limitations or superficial pattern-matching behaviors

## Confidence
- **High Confidence**: The core methodology for generating arithmetic problems from proof trees is sound and well-documented
- **Medium Confidence**: The claim that MathGAP problems are genuinely out-of-distribution requires validation against potential training data patterns
- **Medium Confidence**: The assertion that even state-of-the-art models struggle with complex problems is supported but lacks sufficient statistical analysis for strong conclusions about relative capabilities

## Next Checks
1. **Distribution Analysis**: Conduct comprehensive statistical analysis to verify MathGAP problems are truly out-of-distribution by comparing linguistic patterns and problem structures against major benchmarks and potential training corpora
2. **Model Architecture Impact**: Test whether transformer-based models with different architectural modifications show systematically different performance patterns on MathGAP problems compared to standard architectures
3. **Generalization Transfer**: Evaluate whether models trained on MathGAP problems demonstrate improved performance on other established mathematical reasoning benchmarks, establishing whether MathGAP training provides general reasoning benefits or only narrow specialization