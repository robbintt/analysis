---
ver: rpa2
title: 'DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language
  Models'
arxiv_id: '2402.03300'
source_url: https://arxiv.org/abs/2402.03300
tags:
- training
- math
- mathematical
- arxiv
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DeepSeekMath 7B achieves a score of 51.7% on the competition-level
  MATH benchmark without external toolkits or voting, approaching the performance
  of Gemini-Ultra and GPT-4. Self-consistency over 64 samples further improves the
  score to 60.9%.
---

# DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models

## Quick Facts
- arXiv ID: 2402.03300
- Source URL: https://arxiv.org/abs/2402.03300
- Reference count: 37
- Key outcome: DeepSeekMath 7B achieves 51.7% on MATH benchmark, approaching Gemini-Ultra and GPT-4 performance.

## Executive Summary
DeepSeekMath introduces a 7B parameter language model that achieves state-of-the-art performance among open models on mathematical reasoning tasks. The key innovations include a meticulously engineered 120B-token math-specific corpus from Common Crawl, a two-stage training approach leveraging code training before math, and Group Relative Policy Optimization (GRPO) for reinforcement learning. The model achieves 51.7% accuracy on the MATH benchmark without external tools and 60.9% with self-consistency over 64 samples, approaching the performance of much larger closed models like Gemini-Ultra and GPT-4.

## Method Summary
DeepSeekMath is built through three stages: (1) pre-training on a 120B-token math-specific corpus constructed from Common Crawl using a fastText classifier and iterative filtering pipeline, (2) supervised fine-tuning on 776K instruction-tuning examples covering chain-of-thought, program-of-thought, and tool-integrated reasoning formats, and (3) reinforcement learning using Group Relative Policy Optimization (GRPO) on 144K questions with 64 sampled outputs per question. The model starts from DeepSeek-Coder-Base-v1.5 7B and leverages code training as a foundation for mathematical reasoning.

## Key Results
- Achieves 51.7% on MATH benchmark without external toolkits or voting
- Self-consistency over 64 samples improves score to 60.9%
- Outperforms all open-source models in the 7B-70B range
- Strong performance on both English (GSM8K) and Chinese (CMATH) mathematical benchmarks

## Why This Works (Mechanism)

### Mechanism 1
High-quality, large-scale math-specific pre-training data is a key driver of improved mathematical reasoning performance. The DeepSeekMath Corpus, built from Common Crawl using a fastText classifier, provides 120B math-related tokens. This scale is 7x larger than Minerva's math web pages and 9x larger than OpenWebMath. The iterative data selection pipeline (fastText classification, domain analysis, human annotation) ensures high quality by filtering low-quality content and enriching with diverse math sources.

### Mechanism 2
Code training before math training improves mathematical reasoning, both with and without tool use. Initializing with DeepSeek-Coder-Base-v1.5 7B and then training on math data leverages the reasoning patterns learned from code. Code training provides a foundation for structured, step-by-step problem-solving that translates to mathematical reasoning. Two-stage training (code → math) outperforms one-stage math training alone, and mixing code and math tokens in one-stage training mitigates catastrophic forgetting while synergizing coding and math performance.

### Mechanism 3
Group Relative Policy Optimization (GRPO) enhances mathematical reasoning while reducing computational overhead compared to PPO. GRPO replaces the value function in PPO with group-relative rewards, estimating the baseline from the average reward of multiple outputs for the same question. This simplifies training, reduces memory usage, and aligns with the comparative nature of reward models. GRPO achieves better performance than PPO and Online RFT, especially with process supervision (step-level rewards).

## Foundational Learning

- **Concept**: Reinforcement Learning with Proximal Policy Optimization (PPO) and its variants
  - **Why needed here**: DeepSeekMath uses GRPO, a variant of PPO, to further enhance mathematical reasoning after supervised fine-tuning. Understanding PPO's core concepts (policy gradient, advantage estimation, value function) is crucial for grasping GRPO's innovations and limitations.
  - **Quick check question**: What is the key difference between PPO and GRPO in terms of baseline estimation for advantage calculation?

- **Concept**: Data selection and filtering techniques for domain-specific corpus construction
  - **Why needed here**: The DeepSeekMath Corpus is built using a fastText classifier and an iterative pipeline. Understanding text classification, domain analysis, and human annotation is essential for replicating or improving the corpus construction process.
  - **Quick check question**: What are the three main steps in the iterative pipeline for building the DeepSeekMath Corpus?

- **Concept**: Mathematical reasoning benchmarks and evaluation metrics
  - **Why needed here**: DeepSeekMath is evaluated on benchmarks like GSM8K, MATH, and Chinese math tests. Understanding these benchmarks, their difficulty levels, and evaluation metrics (accuracy, pass@k, majority vote) is crucial for interpreting the model's performance and comparing it to other models.
  - **Quick check question**: What is the key difference between chain-of-thought reasoning and tool-integrated reasoning in the context of mathematical problem-solving?

## Architecture Onboarding

- **Component map**: DeepSeek-Coder-Base-v1.5 7B → Math Pre-training (120B tokens) → Supervised Fine-tuning (776K examples) → GRPO Reinforcement Learning (144K questions) → DeepSeekMath 7B
- **Critical path**: The most critical path for achieving high mathematical reasoning performance is: 1) Building a high-quality, large-scale math-specific corpus (DeepSeekMath Corpus). 2) Effective pre-training on this corpus, leveraging the code training foundation. 3) Applying GRPO reinforcement learning with process supervision to refine the model's reasoning capabilities.
- **Design tradeoffs**: The choice of GRPO over PPO trades off the potential accuracy of a learned value function for reduced memory usage and simplified training. The decision to focus on English and Chinese math content may limit performance on other languages. The use of a 7B model balances performance with computational efficiency.
- **Failure signatures**: If the model's performance on MATH or GSM8K is significantly lower than reported, it could indicate issues with the corpus quality, pre-training effectiveness, or GRPO implementation. Poor generalization to out-of-domain tasks (e.g., geometry, theorem proving) suggests limitations in the training data or model capacity.
- **First 3 experiments**:
  1. Replicate the DeepSeekMath Corpus construction pipeline on a smaller scale to verify the effectiveness of the iterative data selection process.
  2. Conduct ablation studies on the pre-training stage, comparing models trained on different data sources (e.g., DeepSeekMath Corpus vs OpenWebMath vs Proof-Pile-2) to isolate the impact of corpus quality and size.
  3. Implement and evaluate GRPO on a smaller dataset (e.g., GSM8K) to verify its performance gains over PPO and understand the impact of group-relative rewards vs learned value functions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the size of the DeepSeekMath Corpus continue to improve performance beyond 120B tokens, or is there a point of diminishing returns?
- Basis in paper: [explicit] The paper states "We end up with 35.5M mathematical web pages, totaling 120B tokens" and discusses performance improvements with larger corpus sizes.
- Why unresolved: The paper does not explore corpus sizes beyond 120B tokens to determine if performance plateaus or continues to improve.
- What evidence would resolve it: Experiments training models on math corpora larger than 120B tokens and comparing performance on benchmarks.

### Open Question 2
- Question: How does DeepSeekMath's performance on geometry and theorem-proof problems compare to closed models, and what specific data or techniques could improve this?
- Basis in paper: [explicit] The conclusion states "its capability on geometry and theorem-proof are relatively weaker than closed models" and mentions "the model cannot handle problems related to triangles and ellipses."
- Why unresolved: The paper does not provide detailed analysis of performance on geometry and theorem-proof problems or investigate specific improvements.
- What evidence would resolve it: Detailed benchmarking on geometry and theorem-proof problems, analysis of model failures, and experiments with targeted data or techniques for these areas.

### Open Question 3
- Question: What is the impact of including arXiv papers in the math pre-training data, and under what conditions (if any) do they provide benefits?
- Basis in paper: [explicit] The paper states "arXiv papers seem ineffective in improving mathematical reasoning" but notes limitations including "We have not yet studied: The impact of arXiv tokens on specific math-related tasks not included in this research."
- Why unresolved: The paper's experiments did not cover all possible scenarios where arXiv papers might be beneficial, such as specific task types or larger model scales.
- What evidence would resolve it: Experiments on arXiv's impact on specific math tasks, combining arXiv with other data types, and testing on larger model scales.

### Open Question 4
- Question: How does DeepSeekMath's few-shot learning capability compare to GPT-4, and what architectural or training modifications could close this gap?
- Basis in paper: [explicit] The conclusion states "restricted by the model scale, DeepSeekMath is worse than GPT-4 on few-shot capability" and notes that "GPT-4 could improve its performance with few-shot inputs, while DeepSeekMath shows similar performance in zero-shot and few-shot evaluation."
- Why unresolved: The paper does not investigate the specific reasons for this gap or test potential solutions.
- What evidence would resolve it: Detailed comparison of few-shot learning on various tasks, ablation studies on architectural components, and experiments with training techniques to improve few-shot performance.

## Limitations

- **Corpus Quality Uncertainty**: While the paper claims the DeepSeekMath Corpus is "high-quality," it lacks external validation beyond model performance comparisons.
- **GRPO Algorithm Specificity**: GRPO is presented as a key innovation, but the paper lacks detailed implementation specifics, making it difficult to assess its true novelty.
- **Generalization Concerns**: The model's performance is evaluated primarily on competition-level math problems, with unknown generalization to other mathematical domains.

## Confidence

- **High Confidence**: The overall methodology of using a math-specific corpus and reinforcement learning is sound. The reported performance improvements on established benchmarks (MATH, GSM8K) are significant and well-documented.
- **Medium Confidence**: The effectiveness of GRPO as a novel algorithm is plausible, but the lack of implementation details and comparison to other PPO variants limits confidence. The claim that code training improves math reasoning is supported by experiments but lacks a clear mechanistic explanation.
- **Low Confidence**: The quality and representativeness of the DeepSeekMath Corpus are asserted but not independently verified. The model's generalization to out-of-domain tasks is unknown.

## Next Checks

1. **Corpus Quality Validation**: Conduct an independent analysis of the DeepSeekMath Corpus to assess its quality, diversity, and representation of mathematical domains through human evaluation and token distribution analysis.

2. **GRPO Algorithm Replication**: Implement and evaluate GRPO on a smaller dataset (e.g., GSM8K) with detailed ablation studies, comparing its performance to standard PPO and other reinforcement learning algorithms.

3. **Out-of-Domain Generalization Testing**: Evaluate DeepSeekMath on a diverse set of mathematical tasks beyond competition problems, including geometry problems, theorem proving in formal systems, and real-world problem-solving scenarios.