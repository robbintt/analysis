---
ver: rpa2
title: 'MARS: Paying more attention to visual attributes for text-based person search'
arxiv_id: '2407.04287'
source_url: https://arxiv.org/abs/2407.04287
tags:
- loss
- text
- image
- attribute
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses text-based person search (TBPS), a task that
  requires retrieving images of a specific individual based on a textual description.
  The authors propose MARS, a novel TBPS architecture that tackles two main challenges:
  inter-identity noise and intra-identity variations.'
---

# MARS: Paying more attention to visual attributes for text-based person search

## Quick Facts
- arXiv ID: 2407.04287
- Source URL: https://arxiv.org/abs/2407.04287
- Reference count: 31
- Primary result: MARS achieves 71.41 mAP on CUHK-PEDES, outperforming previous SOTA

## Executive Summary
This paper addresses text-based person search (TBPS), where the goal is to retrieve images of a specific individual based on textual descriptions. The authors propose MARS, a novel architecture that tackles two main challenges: inter-identity noise (vagueness in descriptions) and intra-identity variations (pose, illumination, etc.). MARS introduces three key innovations: a masked autoencoder loss for better visual reconstruction, an attribute loss that balances the contribution of different attributes in the text, and full cross-attention layers in the cross-modal encoder. Experiments on three datasets (CUHK-PEDES, ICFG-PEDES, and RSTPReid) demonstrate that MARS outperforms state-of-the-art methods, with significant gains in the mean Average Precision (mAP) metric.

## Method Summary
MARS is built on a vision transformer (ViT) for image encoding and BERT for text encoding, with a cross-modal encoder that uses full cross-attention in all blocks. The model is trained with five losses: Relation-Aware (RA), Sensitive-Aware (SA), Contrastive Loss (CL), Masked AutoEncoder (MAE), and Attribute Loss (AL). The attribute loss balances the contribution of different types of attributes, defined as adjective-noun chunks of text, by classifying each chunk's embedding against the image. The MAE decoder reconstructs randomly masked image patches using text embeddings via cross-attention. The model is evaluated on three datasets with standard TBPS metrics: Rank@1/5/10 and mAP.

## Key Results
- MARS achieves 77.62% Rank@1, 90.63% Rank@5, 94.27% Rank@10, and 71.41 mAP on CUHK-PEDES
- Outperforms previous state-of-the-art models on all three benchmark datasets
- Significant improvement over baseline RaSa model (69.38 mAP vs 71.41 mAP on CUHK-PEDES)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Masked AutoEncoder loss improves visual-text alignment by reconstructing masked image patches with text guidance.
- Mechanism: Random patches of input images are masked and passed through the image encoder; the MAE decoder, aided by text embeddings via cross-attention, reconstructs the original image patches. This forces the model to retain fine-grained mutual information between visual and textual features.
- Core assumption: Cross-modal cross-attention in the decoder can effectively use text embeddings to guide image reconstruction.
- Evidence anchors:
  - [abstract] "a Masked AutoEncoder trained to reconstruct randomly masked image patches with the aid of the textual description"
  - [section 3.4] "we inject the text embeddings in the MAE decoder via cross attention layers"
- Break condition: If the text embeddings are not sufficiently informative or aligned, reconstruction quality degrades and the mutual information benefit vanishes.

### Mechanism 2
- Claim: Attribute Loss balances the contribution of each attribute in captions, reducing retrieval ambiguity.
- Mechanism: Attributes are defined as adjective-noun chunks. For each chunk, the cross-modal encoder outputs an embedding that is matched against the image via classification. This pushes the model to give equal attention to each attribute, not just the most discriminative ones.
- Core assumption: Each attribute chunk in the caption corresponds to a unique visual region in the image and can be accurately encoded.
- Evidence anchors:
  - [abstract] "Attribute Loss... balances the contribution of different types of attributes, defined as adjective-noun chunks of text"
  - [section 3.3] "For each attribute i.e. chunk ùëê‚Ñé of noun-adjective words in a given text ùëá , the average of the corresponding embeddings is calculated"
- Break condition: If attribute chunking is noisy or visual features are too ambiguous, the loss may misalign attributes and images, hurting retrieval.

### Mechanism 3
- Claim: Full cross-attention in every block of the cross-modal encoder improves fine-grained cross-modal matching.
- Mechanism: Unlike previous models that only equip the last 6 BERT blocks with cross-attention, MARS adds cross-attention to all 12 blocks, allowing richer fusion of image and text features at multiple abstraction levels.
- Core assumption: More cross-attention layers lead to better alignment without overfitting or gradient instability.
- Evidence anchors:
  - [section 3.1] "we equip all its blocks with cross-attention layers instead of only the last 6"
  - [section 4.4] "Our model outperforms all other SOTA models, achieving the highest performance on all the proposed metrics"
- Break condition: Excessive cross-attention without sufficient data may cause overfitting or optimization difficulty.

## Foundational Learning

- Concept: Vision Transformer (ViT) encoder and BERT-based text encoder
  - Why needed here: MARS uses ViT to encode image patches into embeddings and BERT to encode text tokens, forming the backbone for cross-modal alignment.
  - Quick check question: Can you explain how a ViT tokenizes an image into patch embeddings?
- Concept: Cross-modal contrastive learning
  - Why needed here: The model uses contrastive losses (e.g., InfoNCE) to push matching image-text pairs close and mismatched pairs apart in the shared embedding space.
  - Quick check question: What is the role of the momentum encoder in the contrastive loss formulation?
- Concept: Masked language modeling (MLM)
  - Why needed here: The baseline RaSa model uses MLM to predict masked words; MARS extends this with MLM + MRT (Momentum-based Replace Token Detection) to handle masked text during cross-modal encoding.
  - Quick check question: How does the momentum model differ from the online model in MLM training?

## Architecture Onboarding

- Component map:
  - Image Encoder (ViT) ‚Üí Image embeddings
  - Text Encoder (BERT) ‚Üí Text embeddings
  - Masked AutoEncoder Decoder ‚Üí Reconstructs masked patches using text
  - Cross-Modal Encoder (BERT w/ full cross-attention) ‚Üí Joint encoding for matching and attribute loss
  - Momentum Model ‚Üí Stabilizes contrastive and MLM training
- Critical path:
  1. Image + Text ‚Üí Encoders ‚Üí Embeddings
  2. Cross-modal encoder + attribute loss ‚Üí Refined embeddings
  3. MAE decoder + masked image + text ‚Üí Reconstruction loss
  4. Contrastive + ITM + MLM/MRT losses ‚Üí Joint training
- Design tradeoffs:
  - Full cross-attention improves matching but increases computation.
  - MAE decoder adds reconstruction supervision but requires masking logic.
  - Attribute loss improves fine-grained retrieval but relies on accurate chunk parsing.
- Failure signatures:
  - Poor R@1/mAP ‚Üí Attribute loss or cross-attention ineffective.
  - Unstable training ‚Üí Momentum model or loss balancing misconfigured.
  - Slow inference ‚Üí Large k in ITM ranking not tuned.
- First 3 experiments:
  1. Ablation: Train without MAE loss to measure reconstruction impact on retrieval.
  2. Ablation: Train with partial cross-attention (only last 6 blocks) to confirm full CA benefit.
  3. Ablation: Train without attribute loss to measure contribution of balanced attribute attention.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the attribute loss perform on text descriptions with varying lengths and attribute densities?
- Basis in paper: [inferred] The paper discusses the attribute loss's effectiveness in balancing contributions of different attributes, particularly in long descriptions with multiple attributes. However, it does not explicitly analyze performance across varying lengths and densities of text descriptions.
- Why unresolved: The paper does not provide a detailed analysis of how the attribute loss scales with different text description lengths and attribute densities, which could impact its effectiveness in real-world scenarios.
- What evidence would resolve it: Experiments comparing the attribute loss's performance on text descriptions of varying lengths and attribute densities, demonstrating its robustness and adaptability to different textual inputs.

### Open Question 2
- Question: What is the impact of the masked autoencoder loss on the model's ability to handle occlusions or missing visual information in the input images?
- Basis in paper: [inferred] The paper introduces a masked autoencoder loss to enhance mutual information between text and image embeddings. However, it does not explore how this loss affects the model's robustness to occlusions or missing visual information in the input images.
- Why unresolved: The paper does not investigate the model's performance in scenarios where parts of the image are occluded or missing, which is a common challenge in real-world applications.
- What evidence would resolve it: Experiments evaluating the model's performance on datasets with occluded or partially missing images, comparing it to models without the masked autoencoder loss.

### Open Question 3
- Question: How does the model perform on text descriptions that contain ambiguous or contradictory attributes?
- Basis in paper: [inferred] The paper mentions the challenges of inter-identity noise due to vague or ambiguous text descriptions. However, it does not explicitly test the model's performance on descriptions with ambiguous or contradictory attributes.
- Why unresolved: The paper does not provide evidence of the model's ability to handle text descriptions that may contain conflicting or unclear attributes, which could affect its retrieval accuracy.
- What evidence would resolve it: Experiments using text descriptions with ambiguous or contradictory attributes, measuring the model's retrieval accuracy and comparing it to other models.

## Limitations

- The evaluation focuses on three benchmark datasets with relatively constrained identity diversity and caption complexity, limiting generalizability to real-world applications.
- The attribute loss mechanism assumes clean extraction of adjective-noun chunks, but performance may degrade if SpaCy's parsing introduces noise or if visual features do not reliably correspond to textual attributes.
- The MAE decoder relies on cross-attention with text embeddings, but the paper does not provide quantitative evidence that the text embeddings are sufficiently informative for accurate reconstruction.

## Confidence

- **High**: The MARS architecture and its five-loss training framework are clearly described, and the reported metrics (R@1, R@5, R@10, mAP) are standard and reproducible.
- **Medium**: The attribute loss formulation is described but lacks implementation details for handling edge cases in chunk parsing and encoding, which could affect results.
- **Medium**: The benefit of full cross-attention is supported by comparisons to RaSa, but the exact contribution of deeper fusion versus other architectural changes is not isolated.

## Next Checks

1. Conduct a controlled ablation: train MARS without the attribute loss and with partial cross-attention (only last 6 blocks) to quantify the individual contributions of these two mechanisms.
2. Perform qualitative analysis of cross-attention maps using Grad-CAM to verify that attention is correctly distributed across attribute chunks rather than scattered or focused on irrelevant regions.
3. Evaluate MARS on a dataset with more diverse identities and noisier captions to assess robustness beyond the benchmark datasets.