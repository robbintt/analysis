---
ver: rpa2
title: CMA-R:Causal Mediation Analysis for Explaining Rumour Detection
arxiv_id: '2402.08155'
source_url: https://arxiv.org/abs/2402.08155
tags:
- cma-r
- rumour
- tweets
- detection
- causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies causal mediation analysis to explain the decision-making
  process of neural models for rumour detection on Twitter. Interventions at the input
  and network level reveal the causal impacts of tweets and words in the model output.
---

# CMA-R:Causal Mediation Analysis for Explaining Rumour Detection

## Quick Facts
- arXiv ID: 2402.08155
- Source URL: https://arxiv.org/abs/2402.08155
- Reference count: 18
- Key outcome: CMA-R applies causal mediation analysis to explain rumour detection model decisions by identifying causally implicated tweets and words through input and network-level interventions

## Executive Summary
This paper introduces CMA-R, a novel approach that applies causal mediation analysis to interpret rumour detection systems. By performing interventions at both input and network levels, CMA-R identifies causally implicated tweets and words in the final prediction, moving beyond correlation-based explanations. The method provides neuron-level interventions to highlight impactful words in salient tweets, offering deeper interpretability into black-box rumour detection models. Empirical results on a journalist-annotated dataset demonstrate that CMA-R aligns more closely with human judgments compared to traditional interpretation methods.

## Method Summary
CMA-R leverages causal mediation analysis to explain the decision-making process of neural models for rumour detection on Twitter. The approach performs interventions at two levels: input-level interventions modify tweet content, while network-level interventions target specific neurons. This dual intervention strategy identifies causally implicated tweets and words in the model's final prediction. By focusing on causal relationships rather than correlations, CMA-R provides a more accurate explanation of why a rumour detection model makes specific classifications, highlighting the most impactful words within salient tweets through neuron-level analysis.

## Key Results
- CMA-R demonstrates significantly higher turnaround accuracy compared to attention, local, and gradient-based interpretation methods
- The approach aligns more closely with human judgments on a journalist-annotated rumour dataset
- CMA-R can highlight impactful words in salient tweets through neuron-level interventions, providing multi-level interpretability

## Why This Works (Mechanism)
CMA-R works by addressing the fundamental limitation of correlation-based interpretation methods that fail to capture causal relationships in model decisions. By applying causal mediation analysis through targeted interventions at both the input and network levels, the method isolates the actual causal pathways that lead to specific predictions. This intervention-based approach effectively breaks spurious correlations while preserving true causal mechanisms, allowing the identification of genuinely impactful features rather than those that are merely correlated with the output. The neuron-level interventions provide granular insight into which specific model components drive the final decision, creating a hierarchical understanding from tweet-level to word-level causal effects.

## Foundational Learning
- **Causal Mediation Analysis**: A statistical framework for decomposing the effect of a treatment on an outcome through intermediate variables (mediators); needed to distinguish between direct and indirect causal effects in the model's decision process
- **Intervention-based methods**: Techniques that modify model inputs or parameters to assess causal impact; needed to move beyond correlation-based explanations that may capture spurious relationships
- **Neuron-level interventions**: Targeted modifications to individual neurons in neural networks; needed to identify specific model components that drive predictions and provide granular interpretability
- **Rumour detection in social media**: The task of classifying whether social media posts contain false information; needed as the application domain requiring interpretable explanations
- **Counterfactual reasoning**: Reasoning about alternative scenarios by changing inputs; needed to assess how model predictions would change under different conditions

## Architecture Onboarding

Component map: Tweet input -> Word embeddings -> Neural network layers -> Prediction output -> Intervention modules (input and neuron-level) -> Causal effect estimation

Critical path: The critical path flows from the input tweet through the neural network to the prediction, with intervention modules creating parallel paths for causal effect estimation. The most time-consuming components are the neuron-level interventions, which require forward passes through the network for each targeted neuron.

Design tradeoffs: The approach trades computational efficiency for causal accuracy, as multiple interventions are needed to estimate causal effects. The choice to implement both input and neuron-level interventions increases interpretability but also computational cost. The single-dataset evaluation represents a tradeoff between depth of analysis and generalizability across different rumour detection contexts.

Failure signatures: The method may fail when causal pathways are highly complex and non-linear, making intervention effects difficult to isolate. It may also struggle with cases where multiple words or tweets have redundant causal effects, leading to underestimation of individual contributions. The approach assumes that interventions can meaningfully perturb the model's internal representations, which may not hold for highly robust models.

First experiments: 1) Apply CMA-R to a simple linear model to verify that it correctly identifies known causal relationships. 2) Test the method on a synthetic dataset where ground truth causal effects are known. 3) Compare CMA-R's computational requirements with traditional interpretation methods on models of increasing complexity.

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- The empirical evaluation relies on a single journalist-annotated dataset, limiting generalizability across different rumour detection contexts and platforms
- The claim of better alignment with human judgments lacks detail about the specific human judgment metrics used and whether annotators were blinded to the interpretation method
- The turnaround accuracy comparison to other methods is not fully explained, making it difficult to assess whether the comparison is fair

## Confidence

**High Confidence**: The methodological framework of applying causal mediation analysis to rumour detection systems is technically sound and well-described

**Medium Confidence**: The empirical results showing improved alignment with human judgments, as the evaluation methodology lacks detail about human assessment procedures

**Low Confidence**: The generalizability claims across different rumour detection scenarios, given the single-dataset evaluation approach

## Next Checks

1. Conduct cross-platform validation using rumour datasets from different social media platforms (e.g., Facebook, Reddit) to assess generalizability

2. Perform ablation studies comparing CMA-R's performance with and without causal mediation analysis components to isolate the contribution of the causal framework

3. Implement a controlled human evaluation study with blinded annotators comparing CMA-R interpretations against ground truth causal relationships in model decisions