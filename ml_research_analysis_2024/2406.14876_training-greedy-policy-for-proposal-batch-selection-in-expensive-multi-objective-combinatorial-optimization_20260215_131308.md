---
ver: rpa2
title: Training Greedy Policy for Proposal Batch Selection in Expensive Multi-Objective
  Combinatorial Optimization
arxiv_id: '2406.14876'
source_url: https://arxiv.org/abs/2406.14876
tags:
- greedy
- batch
- policy
- algorithm
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a novel greedy-style subset selection algorithm
  for expensive multi-objective combinatorial optimization problems in active learning.
  The algorithm optimizes batch acquisition directly on the combinatorial space by
  sequential greedy sampling from a greedy policy trained to address all greedy subproblems
  concurrently.
---

# Training Greedy Policy for Proposal Batch Selection in Expensive Multi-Objective Combinatorial Optimization

## Quick Facts
- arXiv ID: 2406.14876
- Source URL: https://arxiv.org/abs/2406.14876
- Reference count: 40
- Key outcome: Achieves baseline performance with 1.69× fewer queries in red fluorescent protein design

## Executive Summary
This work introduces a novel greedy-style subset selection algorithm for expensive multi-objective combinatorial optimization problems in active learning. The method trains a single set-conditioned policy to handle all greedy subproblems concurrently, optimizing batch acquisition directly on the combinatorial space through sequential greedy sampling. By amortizing the computational burden of solving multiple greedy subproblems, the approach demonstrates significant query efficiency improvements while maintaining theoretical performance guarantees under various conditions including near-submodular functions and diversity constraints.

## Method Summary
The proposed method employs a set-conditioned policy architecture that extracts features from given sets using a deep set encoder and utilizes MLM logits for action decoding in batch Bayesian optimization scenarios. The training algorithm updates policy parameters based on the expected gain function through sequential sampling of set sizes and subsets. The method is evaluated on biological sequence design tasks including red fluorescent proteins, DNA aptamers, and bigrams, demonstrating superior performance compared to baseline methods across various acquisition functions and diversity metrics.

## Key Results
- Achieves baseline performance with 1.69× fewer queries in red fluorescent protein design experiments
- Consistently outperforms baseline methods in single-round subset selection tasks on bigram and DNA aptamer design problems
- Demonstrates higher batch acquisition values in batch Bayesian optimization scenarios across multiple acquisition functions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Greedy-style subset selection optimizes batch acquisition directly on the combinatorial space, avoiding the discrepancies of latent space optimization.
- Mechanism: The method trains a single greedy policy to handle all greedy subproblems concurrently, amortizing the burden of solving n separate subproblems. It uses sequential greedy sampling from this trained policy.
- Core assumption: The greedy policy can be trained to address all greedy subproblems effectively and the expected gain function has properties that allow convergence.
- Evidence anchors:
  - [abstract]: "optimize batch acquisition directly on the combinatorial space by sequential greedy sampling from the greedy policy"
  - [section]: "we introduce a novel greedy-style subset selection method utilizing the greedy policy, a set-conditioned policy trained to handle all subproblems, with its novel training algorithm"
  - [corpus]: Weak or missing direct evidence; this is a novel claim.
- Break condition: If the expected gain function does not satisfy the required conditions for convergence, the training of the greedy policy may fail.

### Mechanism 2
- Claim: The greedy policy achieves theoretical performance guarantees even when the batch acquisition function is near-submodular or includes diversity functions.
- Mechanism: The method extends theoretical bounds of the approximated greedy algorithm to include near-submodular functions and diversity functions, broadening its applicability.
- Core assumption: The batch acquisition function can be bounded in terms of its submodularity ratio and the approximation algorithm used in subproblems.
- Evidence anchors:
  - [abstract]: "Theoretical analysis provides bounds for the approximated greedy algorithm under various conditions"
  - [section]: "we extend the theoretical bounds of the approximated greedy algorithm to include both near-submodular functions and diversity functions"
  - [corpus]: Weak or missing direct evidence; this is a novel theoretical extension.
- Break condition: If the batch acquisition function does not exhibit properties that allow for such theoretical bounds, the guarantees may not hold.

### Mechanism 3
- Claim: The set-conditioned policy architecture enables effective conditioning on subsets and leverages learned features for action decoding.
- Mechanism: The architecture uses a deep set encoder to extract features from given sets and incorporates MLM logits for decoding actions in batch BO experiments.
- Core assumption: The set encoder can effectively capture the relevant information from subsets and the MLM model provides useful guidance for action decoding.
- Evidence anchors:
  - [section]: "We extract feat(x), the lower-dimensional continuous features to guide the policy, for each object x ∈ B, and utilize the set of extracted features as input to the deep set encoder"
  - [section]: "we further utilize the MLM model, trained on previously evaluated data, into the decoder, inspired by the architecture proposed in LaMBO"
  - [corpus]: Weak or missing direct evidence; this is a novel architectural component.
- Break condition: If the set encoder or MLM model fails to capture the necessary information or provide useful guidance, the policy's effectiveness may be compromised.

## Foundational Learning

- Concept: Reinforcement Learning (RL) for combinatorial optimization
  - Why needed here: The method uses RL to train the greedy policy, which is essential for addressing the greedy subproblems concurrently.
  - Quick check question: What is the role of the REINFORCE update rule in training the set-conditioned policy?

- Concept: Submodularity and submodularity ratio
  - Why needed here: The theoretical bounds of the method rely on the properties of submodular and near-submodular functions, which are characterized by the submodularity ratio.
  - Quick check question: How does the submodularity ratio quantify the degree of submodularity in a set function?

- Concept: Deep set architectures
  - Why needed here: The method employs a deep set encoder to extract features from given sets, which is crucial for the set-conditioned policy's effectiveness.
  - Quick check question: What is the purpose of using a deep set encoder in the set-conditioned policy architecture?

## Architecture Onboarding

- Component map: Set-conditioned policy -> Deep set encoder -> MLM model (optional) -> Action decoder
- Critical path:
  1. Initialize the set-conditioned policy randomly
  2. Sample a set size k and a subset B using the current policy
  3. Compute the marginal gain ∆a(x | B) for candidates sampled from the policy
  4. Update the policy parameters using the partial derivative step
  5. Repeat steps 2-4 for a fixed number of update steps

- Design tradeoffs:
  - Using a single policy for all subproblems vs. training separate policies for each subproblem
  - Incorporating MLM model for action decoding vs. relying solely on the set-conditioned policy
  - Computational efficiency of the training algorithm vs. the quality of the learned policy

- Failure signatures:
  - Policy fails to converge or produces poor results: Check the properties of the expected gain function and the training algorithm
  - Policy produces diverse but suboptimal solutions: Investigate the effectiveness of the set encoder and the MLM model (if used)
  - Policy is computationally expensive: Optimize the implementation of the training algorithm and the policy inference

- First 3 experiments:
  1. Verify the effectiveness of the set-conditioned policy on a simple synthetic task with known optimal solutions
  2. Evaluate the impact of the deep set encoder on the policy's performance by comparing with a baseline without the encoder
  3. Assess the contribution of the MLM model (if used) by comparing the policy's performance with and without the MLM guidance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed greedy policy approach scale with increasing problem size and complexity in expensive MOCO problems?
- Basis in paper: [explicit] The paper discusses the application of the greedy policy approach to various benchmarks, including biological sequence design tasks and red fluorescent proteins design, but does not provide extensive analysis of scaling behavior.
- Why unresolved: The paper focuses on demonstrating the effectiveness of the proposed method in specific tasks but does not explore its scalability to larger and more complex problems.
- What evidence would resolve it: Conducting experiments on larger datasets and more complex optimization problems would provide insights into the scalability of the approach.

### Open Question 2
- Question: What are the theoretical guarantees for the convergence of the greedy policy training algorithm in practice, especially when the assumptions on the objective function are not fully met?
- Basis in paper: [explicit] The paper presents theoretical analysis of the approximated greedy algorithm under various conditions but acknowledges that the assumptions may not always hold in practical scenarios.
- Why unresolved: The paper demonstrates empirical effectiveness but does not provide rigorous theoretical guarantees for the convergence of the training algorithm in real-world applications.
- What evidence would resolve it: Developing a more comprehensive theoretical analysis that accounts for practical scenarios and validating it through extensive experiments would strengthen the theoretical foundation of the approach.

### Open Question 3
- Question: How does the proposed greedy policy approach compare to other state-of-the-art methods in terms of query efficiency and solution quality for expensive MOCO problems?
- Basis in paper: [explicit] The paper compares the proposed method with baseline methods in specific tasks and demonstrates superior performance, but a comprehensive comparison with other state-of-the-art methods is not provided.
- Why unresolved: The paper focuses on showcasing the effectiveness of the proposed method but does not provide a thorough comparison with other competitive approaches.
- What evidence would resolve it: Conducting experiments on a wider range of benchmarks and comparing the proposed method with other state-of-the-art methods would provide a clearer understanding of its relative performance.

## Limitations

- The theoretical bounds rely heavily on the submodularity ratio assumptions, which may not hold for all practical batch acquisition functions
- The effectiveness of the MLM integration in the action decoder is not fully validated across different problem domains
- The scalability of the method to larger sequence lengths or more complex combinatorial spaces remains unexplored

## Confidence

- High confidence: Experimental results showing 1.69× query efficiency improvement on RFP design
- Medium confidence: Theoretical analysis of greedy algorithm bounds (limited empirical validation)
- Medium confidence: Generalization claims to different combinatorial optimization problems (based on 3 test cases)

## Next Checks

1. Test submodularity ratio empirically across different acquisition functions to verify theoretical assumptions
2. Evaluate performance degradation with increasing sequence lengths beyond the tested ranges
3. Compare training efficiency and solution quality against gradient-based optimization methods on the same problems