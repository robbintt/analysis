---
ver: rpa2
title: On Latency Predictors for Neural Architecture Search
arxiv_id: '2403.02446'
source_url: https://arxiv.org/abs/2403.02446
tags:
- latency
- predictor
- device
- neural
- architecture
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work systematically investigates latency predictors for hardware-aware
  neural architecture search (HW-NAS). The authors address shortcomings in prior few-shot
  latency predictors by studying (1) predictor architecture, (2) neural network sampling
  methods, (3) hardware device representations, and (4) neural network operation encoding
  schemes.
---

# On Latency Predictors for Neural Architecture Search

## Quick Facts
- arXiv ID: 2403.02446
- Source URL: https://arxiv.org/abs/2403.02446
- Reference count: 40
- Key outcome: NASFLAT latency predictor achieves 22.5% average accuracy improvement and 5.8× end-to-end NAS speedup

## Executive Summary
This work addresses the challenge of building accurate latency predictors for hardware-aware neural architecture search (HW-NAS) when few latency measurements are available on target hardware. The authors systematically study four key aspects: predictor architecture, sampling methods, hardware device representations, and operation encoding schemes. They introduce NASFLAT, which combines a graph neural network with operation-specific hardware embeddings, transfer learning from source devices, and supplementary architectural encodings. NASFLAT demonstrates significant improvements in prediction accuracy (22.5% average, up to 87.6% on hardest tasks) and achieves a 5.8× speedup in end-to-end NAS compared to existing methods.

## Method Summary
NASFLAT is a latency predictor for HW-NAS that uses a graph neural network (GNN) with operation-specific hardware embeddings to predict the latency of neural architectures on specific hardware devices. The predictor is pre-trained on a diverse set of source devices using thousands of samples, then transferred to target devices with only 20 samples. Key innovations include hardware-specific embeddings that capture device-operation interactions, correlation-based initialization of embeddings for new devices, and supplementary encodings (Arch2Vec, CATE, ZCP) that provide additional architectural context. The predictor uses an ensemble of DGF and GAT modules for graph processing, and an encoding-based sampler (ZCP+Arch2Vec+CATE) for selecting diverse architectures during transfer learning.

## Key Results
- 22.5% average improvement in latency prediction accuracy across 11 device types
- Up to 87.6% improvement on hardest tasks with high correlation between training and target devices
- 5.8× speedup in end-to-end HW-NAS wall-clock time compared to existing methods
- 7.8% improvement from hardware-specific operation embeddings alone
- 6.2% improvement from supplementary encodings (Arch2Vec, CATE, ZCP)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Operation-specific hardware embeddings capture device-specific execution dynamics better than generic embeddings.
- **Mechanism:** Each operation is concatenated with a hardware-specific embedding that models how that operation behaves on a particular device, capturing effects like layer pipelining and fusion optimizations.
- **Core assumption:** The interaction between an operation and hardware device is sufficiently represented by concatenation of operation and hardware embeddings.
- **Evidence anchors:**
  - [abstract] "We introduce hardware-specific NN operation embeddings to modulate NN encodings based on each hardware device, demonstrating a 7.8% improvement."
  - [section] "In this section, we discuss a methodology to initialize and utilize hardware embeddings to better model the dynamics of the target hardware."
  - [corpus] Weak - related papers focus on performance prediction but don't specifically address operation-specific hardware embeddings.
- **Break condition:** If operation behavior on hardware cannot be effectively encoded through concatenation, or if hardware optimizations don't significantly affect operation latency.

### Mechanism 2
- **Claim:** Hardware embedding initialization from most-correlated source device provides effective cold-start capability.
- **Mechanism:** When adding a new target device, its hardware embedding is initialized using the embedding of the source device with highest latency correlation, providing a good starting point for transfer learning.
- **Core assumption:** Latency correlation between devices implies similar hardware behavior patterns that can be captured in embeddings.
- **Evidence anchors:**
  - [abstract] "We introduce hardware-specific NN operation embeddings to modulate NN encodings based on each hardware device, demonstrating a 7.8% improvement."
  - [section] "For this, we gauge the computational correlation of the target device latency with each of the training devices."
  - [corpus] Weak - related papers discuss hardware-aware NAS but don't specifically address embedding initialization strategies.
- **Break condition:** If latency correlation doesn't reflect underlying hardware similarities, or if devices with similar latency patterns have different optimal architectures.

### Mechanism 3
- **Claim:** Supplementary encodings provide architectural context that improves predictor generalization with few samples.
- **Mechanism:** Additional encodings (Arch2Vec, CATE, ZCP) are concatenated with the GNN output to provide structural and computational information about neural architectures, helping the predictor contextualize target samples.
- **Core assumption:** Different encoding methods capture complementary aspects of neural architectures that enhance predictor understanding.
- **Evidence anchors:**
  - [abstract] "We additionally investigate the impact of supplementing with unsupervised (Arch2Vec), computationally aware (CATE), and metric based (ZCP) encodings resulting in 6.2% improvement in prediction accuracy."
  - [section] "To enhance the robustness and accuracy of our latency predictor, we integrate these encodings into its structure."
  - [corpus] Weak - related papers discuss encodings but don't specifically validate their use as supplementary inputs in latency predictors.
- **Break condition:** If supplementary encodings don't provide meaningful additional information beyond the base adjacency-operation encoding.

## Foundational Learning

- **Concept:** Neural Architecture Search (NAS) and hardware-aware optimization
  - Why needed here: The paper's work is specifically about improving latency predictors used within hardware-aware NAS systems.
  - Quick check question: What is the difference between standard NAS and hardware-aware NAS?

- **Concept:** Graph Neural Networks (GNNs) and their application to neural architectures
  - Why needed here: The latency predictor architecture uses GNNs to process neural network structures represented as graphs.
  - Quick check question: How does a GNN differ from a traditional neural network in processing graph-structured data?

- **Concept:** Transfer learning and few-shot learning in predictor adaptation
  - Why needed here: The predictor is first pre-trained on source devices and then fine-tuned on target devices with minimal samples.
  - Quick check question: What is the key challenge in transferring a predictor from source to target devices in hardware-aware NAS?

## Architecture Onboarding

- **Component map:**
  - Hardware embedding initialization (correlation-based initialization for new devices) → Operation-specific hardware embeddings (concatenation of operation and hardware embeddings) → GNN modules (DGF and GAT ensemble for graph processing) → Supplementary encodings (Arch2Vec, CATE, ZCP for additional architectural context) → Prediction head

- **Critical path:** Hardware embedding initialization → Operation-specific embeddings → GNN processing → Supplementary encoding integration → Prediction head

- **Design tradeoffs:**
  - Embedding dimensionality vs. model complexity
  - Number of GNN layers vs. over-smoothing
  - Choice of supplementary encodings vs. redundancy
  - Sample size for pre-training vs. computational cost

- **Failure signatures:**
  - Poor correlation between predicted and actual latency
  - High variance in predictor performance across different device sets
  - Inability to adapt to new devices (cold start problem)
  - Degradation in performance with increased pre-training samples

- **First 3 experiments:**
  1. Test operation-specific hardware embeddings vs. generic embeddings on a small device set
  2. Compare different hardware embedding initialization strategies (random vs. correlation-based)
  3. Evaluate the impact of different supplementary encodings on predictor performance

## Open Questions the Paper Calls Out

None

## Limitations

- Evaluation focuses on two search spaces (NASBench-201 and FBNet-C) which may not capture full diversity of real-world deployment scenarios
- Correlation-based hardware embedding initialization assumes latency correlation reflects underlying hardware behavior, which may not hold for all device architectures
- Individual contributions of supplementary encodings (Arch2Vec, CATE, ZCP) were not fully isolated in ablation studies

## Confidence

- **High confidence**: Operation-specific hardware embeddings improve prediction accuracy (supported by quantitative results and ablation studies)
- **Medium confidence**: Correlation-based hardware embedding initialization provides effective cold-start capability (mechanism plausible but limited validation across diverse device types)
- **Medium confidence**: Supplementary encodings enhance predictor generalization (supported by results but individual contribution unclear)

## Next Checks

1. Test NASFLAT's performance on additional search spaces beyond NASBench-201 and FBNet-C to assess generalizability across different architecture families
2. Evaluate the correlation-based hardware embedding initialization across a more diverse set of device types with varying underlying architectures (e.g., including ARM-based, DSP-based, and FPGA devices)
3. Conduct ablation studies isolating the individual contributions of each supplementary encoding method (Arch2Vec, CATE, ZCP) to understand their specific value-additions