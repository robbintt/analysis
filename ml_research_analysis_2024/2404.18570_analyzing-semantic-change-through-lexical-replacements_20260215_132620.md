---
ver: rpa2
title: Analyzing Semantic Change through Lexical Replacements
arxiv_id: '2404.18570'
source_url: https://arxiv.org/abs/2404.18570
tags: []
core_contribution: This paper investigates how semantic change impacts language model
  contextualization using a lexical replacement schema. By systematically substituting
  words with synonyms, antonyms, hypernyms, and random words, the authors measure
  changes in embedding representations.
---

# Analyzing Semantic Change through Lexical Replacements

## Quick Facts
- **arXiv ID**: 2404.18570
- **Source URL**: https://arxiv.org/abs/2404.18570
- **Reference count**: 23
- **Primary result**: Achieves Spearman correlation of 0.741 on SemEval-2020 Task 1, outperforming existing semantic change detection methods

## Executive Summary
This paper introduces a novel approach to semantic change detection by systematically replacing words in text with synonyms, antonyms, hypernyms, and random words to measure changes in language model embeddings. The method exploits the observation that random replacements cause larger increases in embedding distance than semantically related replacements, suggesting models rely more on pre-trained knowledge when encountering novel contexts. The authors develop an interpretable detection method that consistently outperforms existing approaches on benchmark datasets while providing insights into why semantic change occurs.

## Method Summary
The approach uses a lexical replacement schema where target words are substituted with different types of semantic relations (synonyms, antonyms, hypernyms) and random words. For each type of replacement, the authors measure the change in embedding representations using a pre-trained language model (BERT). They compute the self-embedding distance (SED) before and after replacement, with larger increases indicating greater semantic drift. The method aggregates these changes across different replacement types to produce a final semantic change score, with random replacements serving as a baseline to calibrate the detection threshold.

## Key Results
- Random word replacements cause the largest increase in self-embedding distance compared to semantic replacements
- The proposed method achieves Spearman correlation of 0.741 on SemEval-2020 Task 1 benchmark
- Outperforms existing semantic change detection methods and models that generate their own substitutes

## Why This Works (Mechanism)
The method works by exploiting the fundamental property that language models rely on both contextual information and pre-trained knowledge. When encountering random words that break semantic coherence, models must fall back more heavily on their pre-trained embeddings, causing larger representation shifts. Semantic replacements maintain some contextual coherence, resulting in smaller embedding changes. This differential response creates a measurable signal for detecting semantic change.

## Foundational Learning
- **Self-embedding distance (SED)**: Measures cosine distance between original and modified embeddings; needed to quantify representation changes from replacements; quick check: should increase with semantic drift
- **Lexical replacement schema**: Systematic substitution with different semantic relations; needed to create controlled perturbations; quick check: replacement types should produce distinct SED patterns
- **Language model contextualization**: How models generate representations based on context vs. pre-trained knowledge; needed to understand baseline behavior; quick check: random replacements should disrupt contextual signals most
- **Semantic change detection benchmarks**: SemEval-2020 Task 1 provides gold-standard annotations; needed for validation; quick check: results should correlate with human judgments
- **WordNet semantic relations**: Provides curated synonym/antonym/hypernym lists; needed for systematic replacements; quick check: coverage should be comprehensive for test vocabulary
- **Spearman correlation**: Non-parametric measure of rank correlation; needed for evaluation; quick check: should show monotonic relationship between predicted and actual change

## Architecture Onboarding
**Component Map**: WordNet relations -> Lexical replacement generator -> BERT model -> Embedding distance calculator -> Change score aggregator

**Critical Path**: Input text → Word replacement selection → BERT encoding → Cosine distance computation → SED aggregation → Semantic change score

**Design Tradeoffs**: Manual WordNet curation ensures semantic accuracy but limits coverage vs. automated generation methods that may introduce noise; random replacements provide clean baseline but may be too extreme compared to gradual semantic shifts

**Failure Signatures**: Poor coverage of semantic relations in WordNet leading to missed detections; random replacement baseline too noisy in short texts; model architecture mismatch causing inconsistent embedding distance patterns

**First Experiments**: 1) Test SED sensitivity across different replacement ratios (10%, 25%, 50%); 2) Compare BERT vs. RoBERTa embedding distance responses to same replacements; 3) Validate that random replacements consistently produce largest SED increases across diverse vocabulary

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on manually curated WordNet lists may introduce coverage gaps and biases
- Method primarily validated on English language data
- Does not investigate whether random replacement effects hold across different model architectures

## Confidence
- **Methodology validation**: High confidence - well-tested on SemEval-2020 benchmark with clear improvements
- **Interpretability claims**: Medium confidence - demonstrated but not extensively validated with human studies
- **Generalizability**: Medium confidence - results shown primarily on English may not transfer to other languages

## Next Checks
1. Test the semantic replacement schema across multiple language model architectures (GPT, RoBERTa, T5) to verify if random replacement effects consistently indicate reliance on pre-trained knowledge

2. Conduct human evaluation studies where linguists assess whether detected semantic changes align with their understanding of historical meaning shifts, particularly for edge cases

3. Apply the method to detect semantic change in non-English corpora, starting with morphologically rich languages like Finnish or Turkish, to test robustness across different linguistic structures