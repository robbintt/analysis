---
ver: rpa2
title: 'Semantic Sensitivities and Inconsistent Predictions: Measuring the Fragility
  of NLI Models'
arxiv_id: '2401.14440'
source_url: https://arxiv.org/abs/2401.14440
tags: []
core_contribution: This paper proposes a framework to measure semantic sensitivity
  in NLI models by generating semantics-preserving surface-form variations of hypotheses.
  The method involves creating variations using conditional generation, ensuring the
  NLI model predicts the original and generated hypotheses as symmetric entailments.
---

# Semantic Sensitivities and Inconsistent Predictions: Measuring the Fragility of NLI Models

## Quick Facts
- **arXiv ID:** 2401.14440
- **Source URL:** https://arxiv.org/abs/2401.14440
- **Reference count:** 18
- **Primary result:** NLI models exhibit performance degradations of 12.92% (in-domain) and 23.71% (out-of-domain) due to semantic sensitivity, leading to inconsistent predictions across variations.

## Executive Summary
This paper introduces a framework to measure semantic sensitivity in Natural Language Inference (NLI) models by generating semantics-preserving surface-form variations of hypotheses. The method uses conditional generation to create variations while ensuring symmetric entailment between original and generated hypotheses. Through systematic evaluation, the study reveals that NLI models experience significant performance degradation when processing semantically equivalent but surface-varied hypotheses. The inconsistency manifests consistently across different models, datasets, and inference variations, highlighting fundamental fragility in current NLI systems.

## Method Summary
The framework generates semantics-preserving variations of hypotheses using conditional generation, where the NLI model is prompted to create new hypotheses that maintain semantic equivalence with the original. The semantic sensitivity is then measured by evaluating the NLI model's predictions on both original and generated hypotheses. To ensure validity, the generated hypotheses must be classified as symmetric entailments with the original hypotheses. The study systematically evaluates this sensitivity across multiple NLI models and datasets, measuring performance degradation in both in-domain and out-of-domain settings.

## Key Results
- NLI models show performance degradations of 12.92% on in-domain settings and 23.71% on out-of-domain settings due to semantic sensitivity
- The phenomenon of inconsistent predictions is consistent across different NLI models, datasets, and inference variations
- Semantic sensitivity leads to major inconsistency within model predictions, even when hypotheses are semantically equivalent

## Why This Works (Mechanism)
The framework works by systematically exposing NLI models to semantically equivalent but surface-varied hypotheses, revealing their inability to maintain consistent predictions across such variations. The conditional generation process creates controlled perturbations while symmetric entailment validation ensures semantic preservation. This approach effectively measures the models' robustness to surface-level variations while maintaining core semantic content.

## Foundational Learning
- **Natural Language Inference (NLI):** Understanding the task of determining semantic relationships between premise and hypothesis sentences; needed to contextualize the evaluation framework and understand what constitutes correct model behavior.
- **Semantic Equivalence vs Surface Form:** Distinguishing between meaning preservation and syntactic variation; needed to design the generation process and evaluate whether variations truly preserve semantics.
- **Conditional Generation:** Using models to generate text conditioned on specific inputs; needed to create hypothesis variations while maintaining control over the generation process.
- **Symmetric Entailment:** Mutual inference relationships where both statements entail each other; needed to validate that generated hypotheses maintain semantic equivalence with originals.
- **Out-of-domain Evaluation:** Testing models on data distributions different from training data; needed to assess model robustness beyond familiar contexts.
- **Performance Degradation Measurement:** Quantifying drops in model accuracy or consistency; needed to systematically evaluate the impact of semantic sensitivity.

## Architecture Onboarding
**Component Map:** NLI Model -> Hypothesis Generator -> Symmetric Entailment Validator -> Performance Evaluator
**Critical Path:** The core workflow involves generating hypothesis variations → validating symmetric entailment → measuring prediction consistency across variations.
**Design Tradeoffs:** The method prioritizes semantic preservation through symmetric entailment validation over diversity of variations, potentially limiting the range of perturbations tested.
**Failure Signatures:** Major inconsistency in predictions for semantically equivalent hypotheses, with performance drops ranging from ~13% to ~24% depending on domain.
**First 3 Experiments:** (1) Generate variations for a sample hypothesis and manually verify semantic preservation, (2) Test symmetric entailment validation on a small set of original/generated hypothesis pairs, (3) Measure prediction consistency on 10 original and 10 generated hypotheses from a simple dataset.

## Open Questions the Paper Calls Out
None

## Limitations
- The semantic preservation validation relies on automated symmetric entailment checks rather than human evaluation, which may miss subtle semantic shifts.
- The framework focuses on surface-level variations, potentially missing deeper semantic sensitivities that might emerge with more substantial perturbations.
- Results may not generalize to newer NLI architectures like T5 or DeBERTa that employ different pretraining objectives.

## Confidence
- **Performance degradation measurements:** High confidence - consistent across multiple models and datasets
- **Consistency across inference variations:** Medium confidence - demonstrated but could benefit from additional validation
- **Semantic preservation of generated hypotheses:** Low confidence - relies on automated validation rather than human judgment

## Next Checks
1. Test the semantic sensitivity framework on additional NLI architectures including encoder-decoder models like T5 to assess architectural dependencies.
2. Conduct human evaluation studies to verify semantic preservation in generated hypothesis variations beyond automated symmetric entailment checks.
3. Extend the analysis to more substantial semantic perturbations including negation handling and quantification changes to understand deeper semantic sensitivities.