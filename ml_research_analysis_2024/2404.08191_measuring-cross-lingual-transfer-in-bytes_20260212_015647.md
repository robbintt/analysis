---
ver: rpa2
title: Measuring Cross-lingual Transfer in Bytes
arxiv_id: '2404.08191'
source_url: https://arxiv.org/abs/2404.08191
tags:
- language
- languages
- transfer
- target
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates cross-lingual transfer by measuring the
  amount of data (in bytes) transferred from diverse source languages to target languages
  using a byte-level tokenizer. Models pretrained on English, Russian, and Chinese
  and finetuned on 10 target languages (e.g., Spanish, Korean, Finnish) achieved similar
  perplexity and Data Transfer (DT) metrics across source languages, regardless of
  linguistic proximity or contamination.
---

# Measuring Cross-lingual Transfer in Bytes

## Quick Facts
- arXiv ID: 2404.08191
- Source URL: https://arxiv.org/abs/2404.08191
- Reference count: 14
- Key outcome: Byte-level tokenization enables language-agnostic cross-lingual transfer regardless of linguistic proximity, with consistent Data Transfer values (50-100 MB) across diverse source languages

## Executive Summary
This study investigates cross-lingual transfer by measuring data transferred between languages using byte-level tokenization. Models pretrained on English, Russian, and Chinese and finetuned on 10 target languages achieved similar performance metrics across source languages, regardless of linguistic proximity or contamination. The consistent Data Transfer values suggest language-agnostic representations, rather than language-specific or contamination effects, drive transfer. Downstream RTE task results support these findings, showing effective cross-lingual transfer even for distant languages.

## Method Summary
The study uses byte-level tokenization to enable language-agnostic embeddings across all languages. Three source language models (English, Russian, Chinese) are pretrained on the mC4 dataset using causal language modeling. These models are then finetuned on 10 target languages with varying dataset sizes (6 million to 6 billion tokens). Performance is evaluated using perplexity and a Data Transfer (DT) metric that measures additional tokens needed for from-scratch models to match finetuned performance.

## Key Results
- Cross-lingual transfer occurs regardless of linguistic proximity between source and target languages
- Data Transfer (DT) values remain consistent (50-100 MB) across different source languages for a given target
- Language contamination does not significantly impact transfer effectiveness
- Byte-level tokenization enables standardized representations across all languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Byte-level tokenization enables language-agnostic embeddings that do not depend on subword tokenization biases
- Mechanism: Using 256 byte embeddings provides identical input representations across all languages, avoiding unknown tokens and undertrained embeddings
- Core assumption: Byte representations capture sufficient linguistic structure to enable meaningful language modeling
- Evidence anchors: Abstract mentions standardization across languages; section 3.3 discusses byte vocabulary approach

### Mechanism 2
- Claim: Models learn both language-specific and language-agnostic components during pretraining
- Mechanism: Shared representations across languages allow transfer without relying on language proximity or contamination
- Core assumption: Language-agnostic component captures universal knowledge transcending linguistic boundaries
- Evidence anchors: Abstract discusses two-component representation hypothesis; section 5.1 supports language-agnostic and language-specific components

### Mechanism 3
- Claim: Data Transfer (DT) metric effectively quantifies transferable knowledge
- Mechanism: Measures additional tokens needed for from-scratch models to match finetuned performance, isolating pretraining contribution
- Core assumption: Linear interpolation method accurately estimates performance curve
- Evidence anchors: Section 3.1 describes DT metric calculation; section 5.2 shows consistent DT values across source languages

## Foundational Learning

- Concept: Cross-lingual transfer
  - Why needed here: Understanding how knowledge moves between languages is central to investigating language-agnostic representations
  - Quick check question: Can you explain the difference between zero-shot and few-shot cross-lingual transfer?

- Concept: Tokenization strategies
  - Why needed here: Choice between subword and byte-level tokenization directly impacts model's ability to handle diverse languages
  - Quick check question: What are the main advantages and disadvantages of byte-level tokenization compared to subword tokenization?

- Concept: Language contamination
  - Why needed here: Study explicitly investigates whether target language data in source pretraining datasets affects transfer performance
  - Quick check question: How would you detect and measure language contamination in a multilingual corpus?

## Architecture Onboarding

- Component map: Byte input → embedding layer → 10-layer decoder-only Transformer (640-dim embeddings, 10 heads, 2560-dim MLP) → output distribution → perplexity calculation
- Critical path: Byte input → embedding layer → transformer blocks → output distribution → perplexity calculation
- Design tradeoffs: Byte-level tokenization provides language-agnostic input but may lose some linguistic efficiency compared to subword tokenization
- Failure signatures: High perplexity across all languages suggests tokenization or model capacity issues; inconsistent DT values across source languages suggests language-specific rather than language-agnostic transfer
- First 3 experiments:
  1. Train byte-level models from scratch on each source language and evaluate on target languages to establish baseline perplexity
  2. Finetune each source language model on target language data and measure DT values
  3. Repeat experiments with different dataset sizes to verify scaling behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the size of the pretrained model affect the amount of cross-lingual transfer?
- Basis in paper: Paper mentions using a small model (65 million parameters) as a limitation, suggesting larger models may behave differently
- Why unresolved: Only tested one model size, leaving open whether larger models would show similar or different patterns
- What evidence would resolve it: Conducting experiments with larger models and comparing their DT metrics to the smaller model

### Open Question 2
- Question: What is the relative contribution of language-agnostic versus language-specific components in cross-lingual transfer?
- Basis in paper: Paper discusses hypothesis of language-agnostic and language-specific components and aims to investigate through experiments
- Why unresolved: While evidence supports language-agnostic representations, exact contribution of each component to transfer is not quantified
- What evidence would resolve it: Experiments isolating and measuring contribution of each component separately

### Open Question 3
- Question: How does the content of pretraining data (cultural and semantic information) influence cross-lingual transfer compared to linguistic form?
- Basis in paper: Paper suggests language-agnostic component may capture cultural and semantic content, but this is not explicitly tested
- Why unresolved: Paper does not investigate whether transfer is primarily driven by linguistic form or shared cultural/semantic information
- What evidence would resolve it: Experiments using artificial languages with controlled content and form, or analyzing impact of domain-specific pretraining

## Limitations
- Study focuses on perplexity and DT metrics without comprehensive downstream task evaluation across all language pairs
- Uses relatively small model (65 million parameters) which may not capture complexities of larger language models
- Does not quantify exact contribution of language-agnostic versus language-specific components

## Confidence

**High Confidence Claims:**
- Byte-level tokenization enables consistent input representations across languages
- Language-agnostic transfer occurs regardless of linguistic proximity
- DT metric values remain stable across source languages for given targets

**Medium Confidence Claims:**
- Two-component model representation hypothesis (language-agnostic + language-specific)
- Language proximity enhances but is not primary driver of transfer
- Byte-level tokenization avoids unknown tokens and undertrained embeddings

**Low Confidence Claims:**
- Specific mechanisms by which byte representations capture linguistic structure
- Generalizability of findings to larger models and different architectures
- Exact contribution of language-agnostic versus language-specific components

## Next Checks

1. **Cross-lingual Transfer on Diverse Downstream Tasks**: Evaluate models on a broader range of downstream tasks (e.g., question answering, named entity recognition) beyond NLI to verify that observed transfer patterns hold across different task types.

2. **Scaling Behavior Analysis**: Investigate whether observed language-agnostic transfer patterns persist when scaling model size to 1 billion or more parameters, revealing whether findings are architecture-dependent.

3. **Ablation Study on Language-Specific Components**: Design experiments to isolate and measure contribution of language-specific versus language-agnostic components by freezing or fine-tuning specific layers, providing clearer evidence for two-component hypothesis.