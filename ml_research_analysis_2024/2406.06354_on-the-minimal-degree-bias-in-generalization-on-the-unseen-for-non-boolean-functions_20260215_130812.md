---
ver: rpa2
title: On the Minimal Degree Bias in Generalization on the Unseen for non-Boolean
  Functions
arxiv_id: '2406.06354'
source_url: https://arxiv.org/abs/2406.06354
tags: []
core_contribution: This paper investigates the out-of-domain generalization of random
  feature models and Transformers in the "generalization on the unseen" setting, where
  training data covers part of the domain but testing is on a new, unseen part. The
  authors first prove that for random feature models in the small feature regime,
  convergence occurs to interpolators of minimal degree, similar to the Boolean case.
---

# On the Minimal Degree Bias in Generalization on the Unseen for non-Boolean Functions

## Quick Facts
- arXiv ID: 2406.06354
- Source URL: https://arxiv.org/abs/2406.06354
- Reference count: 40
- Primary result: Random feature models converge to minimum-degree interpolators in the generalization on the unseen setting, but this depends critically on data embeddings

## Executive Summary
This paper investigates the generalization on the unseen (GOTU) setting where models are trained on part of the domain but tested on unseen parts. The authors analyze random feature models and Transformers, proving that in the small feature regime, random features converge to minimum-degree interpolators for both Boolean and non-Boolean functions when data is embedded with roots of unity. However, for non-Boolean functions with other embeddings (like integers), random features and Transformers may learn higher-degree interpolators instead. This reveals that the Boolean setting with roots of unity embeddings is a special case where minimum-degree bias provides a characterization of learning behavior.

## Method Summary
The paper analyzes random feature models with polynomial activation functions in the generalization on the unseen setting. The key approach involves weight initialization with vanishing variance (wi ~ N(0, εId), ε → 0) to create small feature models, combined with gradient descent optimization. The authors derive theoretical guarantees showing convergence to minimum-degree interpolators when the data is embedded with roots of unity, using Hermite polynomial expansions to characterize the learned functions. They contrast this with experiments on integer embeddings where both random features and Transformers learn higher-degree interpolators instead.

## Key Results
- Random feature models in the small feature regime converge to minimum-degree interpolators in the GOTU setting
- This minimum-degree bias depends critically on data embeddings: roots of unity embeddings preserve the bias while integer embeddings break it
- Transformers consistently learn degree-4 interpolators instead of linear minimal degree interpolators for non-Boolean functions like f(x) = x1x2
- The Boolean setting with roots of unity is shown to be a special case where minimum-degree interpolators provide a rare characterization of learning

## Why This Works (Mechanism)
The mechanism behind minimum-degree bias in random feature models relies on the small feature initialization regime. When weights are initialized with vanishing variance (ε → 0), the model behaves like a linear combination of Hermite polynomials with small coefficients, naturally favoring lower-degree terms during training. The roots of unity embedding preserves this structure by maintaining orthogonality properties that align with the Hermite basis, while other embeddings like integers introduce correlations that favor higher-degree solutions.

## Foundational Learning
- **Generalization on the Unseen (GOTU)**: Learning from partial domain coverage and testing on unseen regions - needed for understanding the problem setup and why standard generalization theory doesn't apply directly
- **Hermite Polynomial Expansions**: Orthogonal polynomial basis for characterizing function approximations - needed for analyzing the learned interpolators and their degrees
- **Random Feature Models**: Linear models with random feature mappings - needed as the primary model class for theoretical analysis
- **Roots of Unity**: Complex numbers satisfying z^n = 1 - needed for understanding the special data embedding that preserves minimum-degree bias
- **Sparse Target Regime**: Setting where only few target coefficients are non-zero - needed for proving convergence to minimal-degree solutions
- **Weight Initialization Variance**: Scaling factor ε in wi ~ N(0, εId) - needed to control model capacity and bias toward lower-degree terms

## Architecture Onboarding

**Component Map:**
Random Feature Model -> Hermite Expansion -> Hermite Coefficients -> Minimum-Degree Interpolator

**Critical Path:**
Small weight initialization (ε → 0) -> Hermite polynomial expansion -> Sparse coefficient structure -> Convergence to minimum-degree interpolator

**Design Tradeoffs:**
- Small ε enables minimum-degree bias but may require more features N for accurate approximation
- Roots of unity embeddings preserve bias but limit applicability to real-world data
- Random features offer theoretical tractability but may underperform compared to learned feature models

**Failure Signatures:**
- Convergence to higher-degree interpolator indicates insufficient feature count or inappropriate data embedding
- Slow convergence suggests learning rate too low or initialization not small enough
- Oscillatory training loss indicates poor conditioning of the Hermite expansion

**3 First Experiments:**
1. Verify convergence to linear interpolator for f(x) = x with roots of unity embedding and ε = 0.01
2. Test convergence to degree-2 interpolator for f(x) = x^2 with integer embedding and ε = 0.01
3. Compare Transformer learning curves for f(x) = x1x2 with roots of unity vs integer embeddings at learning rates 10^-4 and 10^-5

## Open Questions the Paper Calls Out

**Open Question 1**
Does the min-degree bias generalize beyond roots of unity data embeddings to other structured groups or manifolds?
Basis: Authors show min-degree bias holds for roots of unity embeddings in RF models but note that more general integer and real-valued settings require further characterization.
Evidence needed: Experimental or theoretical results showing whether RF models and Transformers learn minimal degree interpolators when data is embedded in other structured groups or manifolds.

**Open Question 2**
What drives the bias of Transformers toward higher-degree interpolators in non-Boolean settings?
Basis: Experiments show Transformers consistently learn degree-4 interpolators instead of linear minimal degree interpolators for the function f(x) = x1x2 with appropriate constraints.
Evidence needed: Analysis of Transformer architecture components (attention mechanisms, positional embeddings) that could explain the preference for higher-degree solutions.

**Open Question 3**
Is there a learning rate threshold below which Transformers exhibit min-degree bias in non-Boolean settings?
Basis: The paper notes that a learning rate of 10^-5 leads to leaky min-degree bias in Boolean settings, and experiments with 10^-4 and 10^-5 rates show different behaviors in non-Boolean settings.
Evidence needed: Systematic experiments varying learning rates across multiple orders of magnitude to identify thresholds where Transformer behavior shifts from higher-degree to minimal-degree interpolators.

## Limitations
- Theoretical claims are primarily asymptotic with parameter regimes not precisely quantified for finite samples
- Analysis focuses on polynomial activation functions, limiting generalizability to other activation types
- Extension from Boolean to non-Boolean settings may not directly translate to practical scenarios with complex data distributions

## Confidence
- **High Confidence**: The core theoretical framework for random feature models converging to minimum-degree interpolators in the small feature regime
- **Medium Confidence**: The extension of these results from Boolean to non-Boolean settings, particularly the role of roots of unity embeddings
- **Low Confidence**: The practical implications for Transformers and other architectures beyond random feature models

## Next Checks
1. Conduct numerical experiments with finite feature counts N and finite ε to empirically verify convergence rate and minimum-degree property, comparing with theoretical predictions
2. Test the model on non-Boolean functions with various data embeddings (real numbers, complex embeddings other than roots of unity) to assess robustness of minimum-degree convergence
3. Evaluate whether similar minimum-degree biases emerge in other architectures (MLPs, CNNs) beyond random features and Transformers to determine if this is a universal phenomenon