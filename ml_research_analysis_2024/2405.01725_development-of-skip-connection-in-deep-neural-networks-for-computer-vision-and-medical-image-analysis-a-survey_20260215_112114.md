---
ver: rpa2
title: 'Development of Skip Connection in Deep Neural Networks for Computer Vision
  and Medical Image Analysis: A Survey'
arxiv_id: '2405.01725'
source_url: https://arxiv.org/abs/2405.01725
tags:
- residual
- skip
- learning
- networks
- connections
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively reviews the development of skip connections
  (residual learning) in deep neural networks for computer vision tasks. The paper
  traces the historical evolution from early neural network architectures to modern
  transformer-based models, highlighting how skip connections have become a fundamental
  building block for designing deep networks.
---

# Development of Skip Connection in Deep Neural Networks for Computer Vision and Medical Image Analysis: A Survey

## Quick Facts
- arXiv ID: 2405.01725
- Source URL: https://arxiv.org/abs/2405.01725
- Reference count: 40
- Primary result: Comprehensive survey of skip connection development in deep neural networks for computer vision

## Executive Summary
This survey comprehensively reviews the evolution and applications of skip connections (residual learning) in deep neural networks for computer vision tasks. The paper traces the historical development from early neural network architectures to modern transformer-based models, highlighting how skip connections have become fundamental building blocks for designing deep networks. It categorizes the development into five key areas: from short to long skip connections, widening residual blocks, strengthening discriminative feature learning, making residual networks more efficient, and incorporating skip connections in self-attention mechanisms. The survey also discusses theoretical explanations for why skip connections work effectively and summarizes applications across image classification, object detection, segmentation, and image reconstruction tasks.

## Method Summary
This is a survey paper that synthesizes and categorizes 40 references covering the development of skip connections in deep neural networks. The authors systematically organize the literature into five thematic areas of development, provide theoretical explanations for effectiveness, and summarize applications across various computer vision tasks. The survey does not present original experimental results but rather provides a comprehensive overview of existing work and future research directions.

## Key Results
- Skip connections have evolved from short residual blocks to long skip connections and are now integrated into self-attention mechanisms
- Theoretical explanations include gradient flow preservation, ensemble learning perspectives, and regularization effects
- Applications span image classification, object detection, segmentation, and image reconstruction tasks
- Future directions include large vision models, generative models, reinforcement learning, and image reconstruction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Skip connections enable easier optimization by allowing gradients to flow directly through the network via identity mappings, mitigating the vanishing gradient problem
- Mechanism: The identity mapping provides a direct path for gradients to bypass non-linear transformations, preventing gradient decay during backpropagation in deep networks
- Core assumption: The identity mapping preserves gradient magnitude while allowing the residual branch to learn corrections to the input
- Evidence anchors:
  - [abstract]: "enabling easier optimization through residual learning during the training stage"
  - [section III]: "The gating units allow feature maps to flow through multiple layers without saturation in Highway networks. Since the gating units are data-dependent and have learnable parameters, it might have a negative impact on the information (feature maps) flowing during training."
  - [corpus]: Weak evidence - corpus neighbors discuss gradient normalization and overlap but don't directly support the gradient flow mechanism
- Break condition: When residual blocks become too deep or complex, causing optimization to favor the residual branch over the identity path, potentially leading to gradient explosion

### Mechanism 2
- Claim: Skip connections improve representational power by creating ensemble-like pathways that combine shallow and deep features
- Mechanism: Multiple parallel paths through the network allow different levels of abstraction to be learned simultaneously, with skip connections preserving low-level features while deeper layers learn higher-level representations
- Core assumption: The network can effectively combine features from different depths without interference
- Evidence anchors:
  - [abstract]: "Many neural networks have inherited the idea of residual learning with skip connections for various tasks"
  - [section IV.C]: "In the context of ensemble learning, residual learning improves the overall system's robustness to noise and outliers by capturing residual information between models."
  - [corpus]: Weak evidence - corpus neighbors don't discuss ensemble properties directly
- Break condition: When feature distributions across depths become too dissimilar, making effective combination difficult or leading to feature interference

### Mechanism 3
- Claim: Skip connections act as regularization by smoothing the loss landscape and reducing overfitting
- Mechanism: The direct gradient paths created by skip connections reduce sharp minima in the loss landscape, leading to better generalization
- Core assumption: A smoother loss landscape correlates with better generalization performance
- Evidence anchors:
  - [section V.C]: "Skip connections could play as regularizations in training a deep neural network" and "placing the filters of skip connections into regularization groups and introducing variance-aware cross-layer regularization"
  - [abstract]: "improving accuracy during testing"
  - [corpus]: Weak evidence - corpus neighbors discuss gradient normalization but not regularization effects
- Break condition: When regularization becomes too strong, potentially underfitting the training data or preventing effective feature learning

## Foundational Learning

- Concept: Residual learning formulation (F(x) + x)
  - Why needed here: Understanding the mathematical formulation is crucial for implementing and modifying skip connections
  - Quick check question: If a residual block has input x and output F(x) + x, what does F(x) represent?

- Concept: Gradient flow in deep networks
  - Why needed here: Skip connections fundamentally change how gradients propagate through deep architectures
  - Quick check question: How does a skip connection prevent gradient vanishing in a 50-layer network compared to a standard feedforward network?

- Concept: Feature map dimensionality matching
  - Why needed here: Skip connections require proper alignment of feature dimensions for addition operations
  - Quick check question: What architectural components are typically used to match dimensions when skip connections connect layers with different channel counts?

## Architecture Onboarding

- Component map:
  - Input → Residual branch (conv → BN → activation → conv → BN) → Addition with identity → Activation → Output
  - Identity path may include projection (1x1 conv) when dimensions differ
  - Optional attention modules inserted before addition
  - Dilated/deformable convolutions can replace standard convolutions in residual branch

- Critical path: Input → [Projection if needed] → Residual branch processing → Addition with identity → Final activation

- Design tradeoffs:
  - Width vs depth: Wider residual blocks with more channels vs deeper networks with more layers
  - Short vs long skip connections: Local feature refinement vs global context aggregation
  - Standard vs dilated convolutions: Receptive field size vs feature resolution
  - Attention integration: Computational overhead vs feature discrimination improvement

- Failure signatures:
  - Performance degradation with increasing depth despite skip connections (suggests optimization issues)
  - Gradients becoming unstable (exploding or vanishing) during training
  - Feature maps losing spatial resolution too early in the network
  - Model overfitting despite regularization from skip connections

- First 3 experiments:
  1. Implement basic ResNet block with projection shortcuts and verify gradient flow using gradient norm monitoring
  2. Compare performance with and without attention modules in residual blocks on a small dataset
  3. Test different dilation rates in residual blocks for semantic segmentation task to find optimal receptive field size

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can skip connections be optimally integrated into large vision models to enable efficient fine-tuning of pre-trained language models while maintaining high performance on diverse vision tasks?
- Basis in paper: [explicit] The paper discusses the potential of using pre-trained large language models (LLMs) as feature extractors and designing lightweight networks to learn residual features for vision tasks
- Why unresolved: The paper identifies this as a future direction but does not provide specific architectural guidelines or empirical results for integrating skip connections in this context
- What evidence would resolve it: Experimental results comparing different skip connection integration strategies in large vision models, including performance metrics and computational efficiency analysis

### Open Question 2
- Question: Can residual learning be effectively applied to accelerate denoising diffusion models by reducing the number of iterative steps required during sampling?
- Basis in paper: [inferred] The paper suggests that residual learning could be introduced into diffusion models to accelerate running speed, as these models are currently slow due to high iteration requirements
- Why unresolved: The paper mentions this as a potential direction but does not explore specific residual learning techniques or demonstrate their effectiveness in diffusion models
- What evidence would resolve it: Implementation and evaluation of residual learning techniques in diffusion models, showing reduced sampling steps while maintaining or improving generation quality

### Open Question 3
- Question: How can residual correction be effectively implemented in reinforcement learning to improve policy learning and value network updates?
- Basis in paper: [explicit] The paper discusses residual reinforcement learning for robot control and residual policy learning for complex manipulation problems as promising directions
- Why unresolved: The paper identifies this as a future research area but does not provide specific methodologies or empirical validation of residual correction in RL
- What evidence would resolve it: Comparative studies of RL algorithms with and without residual correction, demonstrating improvements in convergence speed and policy performance across various control tasks

## Limitations
- The survey lacks specific quantitative comparisons between different skip connection variants
- Many claims about effectiveness are supported by references rather than direct experimental evidence within the paper
- The theoretical explanations often rely on intuitive arguments rather than rigorous mathematical proofs

## Confidence
- **High confidence**: Historical development trajectory of skip connections from ResNet to modern architectures; categorization of development areas into five themes
- **Medium confidence**: Theoretical explanations for skip connection effectiveness (gradient flow, regularization effects); applications across computer vision tasks
- **Low confidence**: Specific quantitative comparisons between different skip connection designs; precise conditions under which certain variants outperform others

## Next Checks
1. Implement gradient norm monitoring during training of deep networks with and without skip connections to empirically verify gradient preservation claims
2. Systematically test residual blocks with different configurations (projection shortcuts, attention modules, dilation rates) on a standard dataset to quantify their individual contributions
3. Compare generalization performance and loss landscape smoothness between networks with skip connections and equivalent networks using explicit regularization techniques like dropout or weight decay