---
ver: rpa2
title: Can Perplexity Predict Fine-tuning Performance? An Investigation of Tokenization
  Effects on Sequential Language Models for Nepali
arxiv_id: '2404.18071'
source_url: https://arxiv.org/abs/2404.18071
tags:
- language
- tokenization
- perplexity
- nepali
- used
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares six tokenization strategies for pretraining
  sequential language models in Nepali. The authors pretrained models using word-based,
  SentencePiece, WordPiece, BPE, morpheme, and morpheme+BPE tokenizers, all with 30k
  vocabularies.
---

# Can Perplexity Predict Fine-tuning Performance? An Investigation of Tokenization Effects on Sequential Language Models for Nepali

## Quick Facts
- **arXiv ID**: 2404.18071
- **Source URL**: https://arxiv.org/abs/2404.18071
- **Reference count**: 6
- **Primary result**: Perplexity fails to predict downstream performance; SentencePiece tokenization outperforms lower-perplexity methods for Nepali NLU tasks

## Executive Summary
This paper investigates whether perplexity during pretraining can predict downstream task performance for Nepali language models. The authors compare six tokenization strategies—word-based, SentencePiece, WordPiece, BPE, morpheme, and morpheme+BPE—all using 30k vocabularies. They pretrain models on 100k Nepali news articles and evaluate on four downstream NLU benchmarks: CPS, POS, NER, and CC. Surprisingly, lower perplexity does not correlate with better downstream performance. Byte-level BPE methods achieve the lowest perplexity but perform worst on downstream tasks, while SentencePiece (highest perplexity) achieves the best average performance across tasks.

## Method Summary
The study systematically compares six tokenization strategies for pretraining sequential language models in Nepali. All tokenizers were configured with 30k vocabularies and pretrained on the same 100k Nepali news articles. The tokenizers included word-based, SentencePiece, WordPiece, BPE, morpheme, and morpheme+BPE approaches. Downstream evaluation was conducted across four Nepali NLU benchmarks: CPS (1,446 samples), POS, NER, and CC (1,000 samples). The authors measured both perplexity during pretraining and task-specific performance metrics to examine the correlation between pretraining objectives and fine-tuning success.

## Key Results
- Perplexity fails to predict downstream task performance for Nepali language models
- Byte-level BPE methods achieve lowest perplexity but worst downstream performance
- SentencePiece tokenization achieves highest perplexity but best average performance across all NLU tasks
- The disconnect between pretraining and downstream performance suggests perplexity is a poor proxy for language understanding capabilities

## Why This Works (Mechanism)
The paper demonstrates that perplexity, while useful as a pretraining optimization objective, does not capture the semantic and syntactic understanding required for downstream NLU tasks. Byte-level BPE methods fragment words into subword units that minimize prediction uncertainty but lose morphological and semantic coherence. SentencePiece, despite higher perplexity, better preserves linguistic structure and morphological patterns crucial for Nepali's agglutinative characteristics. This suggests that language model evaluation should prioritize task-specific performance over pretraining objectives when developing NLP systems for morphologically rich languages.

## Foundational Learning

1. **Perplexity as Language Model Evaluation**
   - Why needed: Measures how well a model predicts the next token in a sequence
   - Quick check: Lower perplexity indicates better probability distribution modeling

2. **Tokenization Strategies for NLP**
   - Why needed: Determines how text is segmented into units for model processing
   - Quick check: Different strategies (word, subword, character) suit different languages

3. **Morphologically Rich Languages**
   - Why needed: Languages like Nepali have complex word formation through affixation
   - Quick check: Tokenization must preserve morphological boundaries for accurate understanding

## Architecture Onboarding

**Component Map**: Tokenizer -> Pretraining -> Downstream Task Evaluation -> Performance Metrics

**Critical Path**: Text Data → Tokenization → Language Model Pretraining → Fine-tuning → Task Performance Evaluation

**Design Tradeoffs**: 
- Byte-level BPE: Lowest perplexity, poor downstream performance
- SentencePiece: Higher perplexity, superior downstream performance
- Vocabulary size (30k) constrains all methods equally

**Failure Signatures**: Low perplexity with poor downstream performance indicates tokenization that fragments meaning rather than preserving linguistic structure

**First Experiments**:
1. Compare perplexity and downstream performance on a held-out validation set
2. Analyze tokenization output to verify morphological preservation
3. Evaluate model performance on out-of-domain Nepali text to test generalization

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Small dataset sizes across downstream tasks (CPS: 1,446 samples, CC: 1,000 samples)
- Exclusive focus on Nepali limits generalizability to other languages
- Fixed 30k vocabulary size may not optimize all tokenization methods
- Preprocessing pipeline details for handling Nepali linguistic features are incomplete

## Confidence
- **High**: Experimental methodology for comparing tokenization strategies on identical pretraining data
- **Medium**: Conclusion that perplexity cannot predict downstream performance due to limited task diversity and dataset sizes
- **Medium**: Claim that SentencePiece is optimal for Nepali NLU based on specific task combinations

## Next Checks
1. Replicate the study with significantly larger datasets (minimum 10k samples per task) to test whether the perplexity-performance disconnect persists at scale
2. Test the same tokenization strategies with varying vocabulary sizes (10k, 30k, 50k) to determine if optimal tokenizer depends on vocabulary configuration
3. Extend the comparison to include at least two additional morphologically rich languages from different language families to assess generalizability of findings