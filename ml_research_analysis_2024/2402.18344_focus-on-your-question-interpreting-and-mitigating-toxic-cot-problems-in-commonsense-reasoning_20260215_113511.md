---
ver: rpa2
title: Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense
  Reasoning
arxiv_id: '2402.18344'
source_url: https://arxiv.org/abs/2402.18344
tags:
- uni00000013
- uni00000011
- uni00000018
- uni00000014
- uni00000015
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies a critical issue in chain-of-thought (CoT)
  reasoning called Toxic CoT, where models that can directly answer correctly often
  produce wrong answers after applying CoT methods. This problem occurs because the
  model loses information from the question in shallow attention layers during CoT
  reasoning.
---

# Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning

## Quick Facts
- arXiv ID: 2402.18344
- Source URL: https://arxiv.org/abs/2402.18344
- Reference count: 22
- Primary result: Reduces Toxic CoT problems by 23.6% and improves accuracy by 5.5% using RIDERS method

## Executive Summary
This paper identifies a critical issue in chain-of-thought (CoT) reasoning called Toxic CoT, where models that can directly answer correctly often produce wrong answers after applying CoT methods. The problem stems from information loss in shallow attention layers during CoT reasoning, where the model fails to maintain sufficient attention flow from question tokens to rationale and answer tokens. To address this, the authors propose RIDERS, a method that compensates for information loss through residual decoding and serial-position swap. RIDERS significantly reduces Toxic CoT problems by 23.6% and improves overall accuracy by 5.5% on five commonsense reasoning benchmarks.

## Method Summary
The RIDERS method addresses Toxic CoT problems through two complementary mechanisms: residual decoding (RD) and serial-position swap (SPS). RD encourages the model to generate tokens with more attention to the question context by adding attention-based rewards during decoding. SPS reduces information loss by reordering the question and rationale positions, leveraging the serial-position effect to place question context closer to the answer prediction. The method is evaluated on five commonsense reasoning benchmarks using Llama2-13B and Baichuan2-13B models, measuring both accuracy and Toxic Rate (TR).

## Key Results
- Reduces Toxic CoT problems by 23.6% compared to baseline CoT methods
- Improves overall accuracy by 5.5% on commonsense reasoning benchmarks
- Effectively addresses both Rationale Drift and Answer Drift issues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Information loss from question context in shallow attention layers during CoT reasoning causes Toxic CoT problems.
- Mechanism: The model fails to maintain sufficient attention flow from question tokens to rationale and answer tokens in early transformer layers, leading to semantic drift.
- Core assumption: Attention mechanisms in shallow layers are responsible for contextual information retention during reasoning generation.
- Evidence anchors:
  - [abstract] "the model exhibits information loss from the question in the shallow attention layers when generating rationales or answers"
  - [section] "the model exhibits information loss from the question in the shallow attention layers when generating rationales or answers"
  - [corpus] Weak - no direct citation but related work on attention mechanisms in transformers supports this mechanism
- Break condition: If attention flow in shallow layers remains consistent between correct and drifting cases, this mechanism fails.

### Mechanism 2
- Claim: Residual decoding compensates for information loss by encouraging attention to question context during token generation.
- Mechanism: The decoding algorithm adds attention-based rewards that promote tokens with higher contextual alignment to the question, effectively "reconnecting" the rationale generation to the original question.
- Core assumption: Adding explicit attention-based rewards during decoding can overcome the model's inherent information loss tendencies.
- Evidence anchors:
  - [abstract] "compensates for the information deficit in the model from both decoding and serial-position perspectives"
  - [section] "we design a decoding algorithm, promoting the model to generate tokens that pay more attention to question contexts"
  - [corpus] Weak - no direct citation but attention-based decoding techniques have been explored in other contexts
- Break condition: If attention scores don't correlate with rationale quality or if the reward mechanism doesn't improve information flow.

### Mechanism 3
- Claim: Serial-position swap reduces information loss by leveraging the serial-position effect to place question context closer to the answer prediction.
- Mechanism: By swapping the order of question and rationale, the method reduces the distance between critical information and the final prediction, making it more likely to be utilized.
- Core assumption: Models utilize information better when it appears at the beginning or end of the input sequence.
- Evidence anchors:
  - [abstract] "we swap the positions of the output sequence, reducing the information loss from the question to the final prediction"
  - [section] "if we replace the original order of '[Question] + [CoT]' with the order of '[CoT] + [Question]', we can not only increase the intensity of information flow from the question to the final prediction, but also reduce the total information loss"
  - [corpus] Weak - related work on serial-position effects in NLP exists but not directly cited
- Break condition: If position swapping doesn't affect prediction accuracy or if the model doesn't show position-dependent information utilization.

## Foundational Learning

- Concept: Attention mechanisms in transformers
  - Why needed here: Understanding how attention flow between tokens determines information retention during reasoning
  - Quick check question: What happens to information flow when attention weights between question and rationale tokens are low in shallow layers?

- Concept: Causal mediation analysis
  - Why needed here: Tracing how different model components contribute to final predictions to identify information loss points
  - Quick check question: How does intervening on context tokens affect the final answer probability in correct vs. drifting cases?

- Concept: Serial-position effect in cognitive processing
  - Why needed here: Understanding why placing question context at specific positions in the input sequence affects prediction quality
  - Quick check question: Why would placing question tokens closer to the answer prediction improve utilization of that information?

## Architecture Onboarding

- Component map: Question → Attention layers → Rationale generation → Attention layers → Answer prediction
- Critical path: Question → Attention layers → Rationale generation → Attention layers → Answer prediction
- Design tradeoffs: Balancing decoding complexity (residual decoding) against computational cost, and input reordering (position swap) against prompt formatting
- Failure signatures: Persistent Toxic CoT rates above 20%, lack of attention flow improvement in shallow layers after intervention, position swapping that doesn't affect accuracy
- First 3 experiments:
  1. Compare attention flow in shallow layers between correct and drifting cases using attribution tracing
  2. Apply residual decoding to rationale generation and measure Toxic Rate reduction
  3. Test position swapping on answer prediction and verify information flow improvement using causal tracing

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the fundamental mechanisms underlying the Toxic CoT problem in other reasoning tasks beyond commonsense reasoning?
- Basis in paper: [explicit] The paper focuses on commonsense reasoning and acknowledges limitations in analyzing other reasoning tasks like math and logic
- Why unresolved: The paper explicitly states that current white-box models perform poorly on tasks like GSM8K (7.2% accuracy), preventing analysis of Toxic CoT in these domains
- What evidence would resolve it: Development of more capable white-box models for math/logic reasoning that would allow investigation of Toxic CoT mechanisms in these domains

### Open Question 2
- Question: How does the RIDERS method perform on open-ended commonsense reasoning tasks where multiple answers may be correct?
- Basis in paper: [inferred] The paper uses multi-choice question formats and acknowledges limitations with open-ended commonsense reasoning due to lack of effective evaluation methods
- Why unresolved: The paper specifically avoids open-ended tasks, focusing only on multiple-choice formats due to evaluation challenges
- What evidence would resolve it: Empirical evaluation of RIDERS on open-ended commonsense reasoning benchmarks with appropriate evaluation metrics

### Open Question 3
- Question: What is the optimal layer depth for residual decoding's attention score computation across different model architectures?
- Basis in paper: [explicit] The paper uses layer 15 for attention score computation based on findings that shallow layers (around 15) are crucial for contextual information exchange
- Why unresolved: The paper uses a fixed layer (15) based on observations from Llama2-13B and Baichuan2-13B, but different architectures may have different optimal layers
- What evidence would resolve it: Systematic ablation studies across different model architectures and layer depths to determine optimal attention layer for residual decoding

### Open Question 4
- Question: How do the effects of RIDERS vary with different model scales (e.g., 7B, 33B, 70B parameters)?
- Basis in paper: [inferred] The paper uses 13B parameter models and discusses limitations of current white-box models, implying model scale may affect results
- Why unresolved: The paper only tests on 13B parameter models, leaving the relationship between model scale and RIDERS effectiveness unexplored
- What evidence would resolve it: Comparative evaluation of RIDERS across different model scales while controlling for other variables

## Limitations

- Only tested on commonsense reasoning tasks, not other reasoning domains like math or logic
- Uses specific model architectures (Llama2-13B and Baichuan2-13B) which may limit generalizability
- Limited hyperparameter analysis for residual decoding and serial-position swap mechanisms

## Confidence

**Confidence: Low** for the causal claim that shallow attention layer information loss is the primary driver of Toxic CoT problems. While the paper presents evidence that attention patterns differ between correct and drifting cases, the causal chain from "shallow layer attention flow" to "rationale drift" to "wrong answer" requires stronger validation. The attribution tracing experiments show correlation but not definitive causation, and alternative explanations (such as insufficient reasoning depth or prompt formatting issues) are not fully ruled out.

**Confidence: Medium** for the RIDERS methodology effectiveness. The 23.6% reduction in Toxic Rate and 5.5% accuracy improvement are statistically significant and replicated across five benchmarks with two different model architectures. However, the specific hyperparameter choices (candidate_num n and weight ω) appear arbitrary, and ablation studies only partially justify these selections. The serial-position swap mechanism's effectiveness also depends heavily on prompt formatting conventions that may not generalize to all CoT applications.

**Confidence: Medium** for the generalizability of findings. While tested on five commonsense reasoning benchmarks, the paper doesn't address whether Toxic CoT problems manifest similarly in other reasoning domains (mathematical reasoning, logical inference, or domain-specific tasks). The method's reliance on specific model architectures (Llama2-13B and Baichuan2-13B) also raises questions about transfer to other transformer-based systems.

## Next Checks

1. **Attention Flow Validation**: Conduct controlled experiments varying the depth of attention tracing to determine whether information loss in specific transformer layers (not just "shallow layers") correlates most strongly with Toxic CoT occurrence. This would involve systematic layer-by-layer analysis of attention weights between question tokens and rationale/answer tokens.

2. **Mechanism Isolation**: Design ablation studies that test RIDERS components independently - first apply only residual decoding without position swapping, then only position swapping without residual decoding - to quantify each component's individual contribution to the 23.6% Toxic Rate reduction. This would clarify whether one mechanism dominates or if they provide complementary benefits.

3. **Domain Transfer Testing**: Apply RIDERS to at least two non-commonsense reasoning tasks (such as mathematical problem-solving or logical inference) to assess whether Toxic CoT problems and their mitigation strategies generalize beyond the current domain. This would validate whether the information loss mechanism is a universal property of CoT reasoning or specific to commonsense tasks.