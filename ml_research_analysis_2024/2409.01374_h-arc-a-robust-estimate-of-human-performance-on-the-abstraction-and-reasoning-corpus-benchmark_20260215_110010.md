---
ver: rpa2
title: 'H-ARC: A Robust Estimate of Human Performance on the Abstraction and Reasoning
  Corpus Benchmark'
arxiv_id: '2409.01374'
source_url: https://arxiv.org/abs/2409.01374
tags:
- tasks
- evaluation
- training
- participants
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study provides a comprehensive evaluation of human performance
  on the Abstraction and Reasoning Corpus (ARC) benchmark. The researchers recruited
  1,729 participants to solve 400 training and 400 evaluation tasks from the original
  ARC problem set.
---

# H-ARC: A Robust Estimate of Human Performance on the Abstraction and Reasoning Corpus Benchmark

## Quick Facts
- arXiv ID: 2409.01374
- Source URL: https://arxiv.org/abs/2409.01374
- Reference count: 16
- Human performance on ARC training set: 73.3-77.2% correct

## Executive Summary
This study provides the first comprehensive evaluation of human performance on the full Abstraction and Reasoning Corpus (ARC) benchmark, testing 1,729 participants on all 400 training and 400 evaluation tasks. The researchers found that humans achieve 73.3-77.2% accuracy on training tasks and 55.9-68.9% on evaluation tasks, with 98.8% of all tasks solvable by at least one person in three attempts. Notably, human performance significantly exceeds current state-of-the-art AI approaches, despite making different types of errors. The authors publicly release H-ARC, a dataset containing all human submissions and action traces, to facilitate further research into human problem-solving strategies and their application to AI development.

## Method Summary
The researchers recruited 1,729 participants through Amazon Mechanical Turk to solve ARC tasks using an interface that provided minimal feedback. Each participant received 5 randomly assigned tasks and could make up to 3 attempts per task, with natural language descriptions collected after each attempt. The study used the full ARC task set (400 training + 400 evaluation) rather than a subset, enabling robust performance estimates. Missing data (10-13% due to incomplete attempts) was handled through pessimistic and optimistic statistical estimates. The resulting H-ARC dataset includes all submissions, action traces, and performance metrics for further analysis.

## Key Results
- Average human performance on ARC training set: 73.3-77.2% correct
- Average human performance on ARC evaluation set: 55.9-68.9% correct
- 98.8% of all tasks were solvable by at least one person in three attempts
- Humans outperform current AI approaches on ARC despite different error patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Comprehensive task coverage eliminates sampling bias and provides accurate performance estimates
- Mechanism: Testing all 400 training and 400 evaluation tasks ensures representative coverage of ARC's difficulty spectrum
- Core assumption: Original ARC task distribution represents the full abstraction and reasoning problem space
- Evidence anchors:
  - [abstract] "In this work, we obtain a more robust estimate of human performance by evaluating 1729 humans on the full set of 400 training and 400 evaluation tasks from the original ARC problem set"
  - [section] "The current study aimed to close this gap by providing a robust estimate of human performance on the ARC benchmark, based on human attempts to solve all 400 training and evaluation tasks respectively"

### Mechanism 2
- Claim: Human fluid intelligence enables superior pattern recognition compared to current AI systems
- Mechanism: Humans leverage cognitive capabilities for abstraction and reasoning that current architectures cannot replicate
- Core assumption: Human cognitive processes for abstraction differ fundamentally from AI architectures
- Evidence anchors:
  - [abstract] "Notably, while these numbers are slightly lower than earlier estimates, human performance still greatly exceeds current state-of-the-art approaches for solving ARC"
  - [section] "Although people solve more tasks than state-of-the-art approaches, we find that machines outperform people on most error metrics we analyzed"

### Mechanism 3
- Claim: Minimal feedback enables effective human self-correction through trial-and-error
- Mechanism: Humans iteratively refine solution hypotheses using limited feedback information
- Core assumption: Human ability to learn from minimal feedback is a key differentiator from AI approaches
- Evidence anchors:
  - [abstract] "we also report that 98.8% of both the training and evaluation sets are solved by at least one person"
  - [section] "For humans, we see substantial improvements in accuracy after 2 attempts (+21.2% on the training set and +25.9% on the evaluation set)"

## Foundational Learning

- Concept: Visual program synthesis and abstract reasoning
  - Why needed here: ARC tasks require inferring transformation rules from minimal examples, central to the benchmark
  - Quick check question: Can you explain how visual program synthesis differs from traditional supervised learning?

- Concept: Statistical estimation with incomplete data
  - Why needed here: Study must handle missing participant data through pessimistic and optimistic estimates
  - Quick check question: How would you compute a confidence interval for performance when some data points are missing?

- Concept: Error analysis and pattern recognition
  - Why needed here: Comparing human and machine error patterns reveals fundamental differences in reasoning approaches
  - Quick check question: What metrics would you use to determine if two sets of errors are fundamentally different?

## Architecture Onboarding

- Component map: Data collection pipeline (AMT recruitment → task assignment → interface interaction → result submission), analysis framework (performance calculation → error pattern analysis → cross-dataset comparison), dataset release system (H-ARC public release)
- Critical path: Participant recruitment → task completion → data validation → performance estimation → error analysis → results publication
- Design tradeoffs: Mechanical Turk provides large sample size but may introduce quality variation; three-attempt limit balances thoroughness with participant fatigue
- Failure signatures: High dropout rates indicating task difficulty; systematic biases in incomplete data handling; interface usability issues affecting performance
- First 3 experiments:
  1. Validate data imputation method by simulating different dropout patterns and measuring estimation accuracy
  2. Compare error patterns between high-performing and low-performing participants to identify skill factors
  3. Test whether providing additional feedback types improves performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific cognitive processes and mental representations underlie human problem-solving strategies in ARC tasks?
- Basis in paper: [explicit] The authors state that "more in-depth analyses of essential elements of people's problem solving strategies such as state space trajectories, action traces and natural language descriptions are needed" and that "H-ARC affords answering questions about these aspects of behavior which are likely to be informative for understanding the underlying mental representations that support abstract reasoning and problem-solving in people."
- Why unresolved: While the study provides comprehensive dataset (H-ARC) with action traces and natural language descriptions, authors acknowledge that more in-depth analyses are required to fully understand cognitive processes involved.
- What evidence would resolve it: Detailed analysis of H-ARC dataset focusing on patterns in action traces, natural language descriptions, and state space trajectories could reveal common strategies and cognitive processes used by humans.

### Open Question 2
- Question: What factors contribute to increased difficulty of evaluation set tasks compared to training set tasks in ARC?
- Basis in paper: [explicit] The authors note that "it is still unclear why that is the case" and discuss potential factors such as output grid size and time spent thinking, but find these insufficient to fully explain difficulty difference.
- Why unresolved: Despite efforts to identify factors contributing to task difficulty, authors were unable to conclusively determine why evaluation set tasks are more challenging.
- What evidence would resolve it: Comparative analysis of task features (e.g., types of transformations, complexity of rules, primitive operations involved) between training and evaluation sets could reveal specific factors contributing to difficulty.

### Open Question 3
- Question: How can machine learning models be improved to better capture human-like problem-solving strategies in ARC tasks?
- Basis in paper: [explicit] The authors state that "Understanding how people achieve this is likely to be useful for improving machine intelligence in ARC tasks, and more generally for problem-solving" and that "we hope that the dataset we release along with this document will allow developments on two fronts: our understanding of how people solve novel abstract problems and how machines can be improved to reason about such problems in a more human-like and intelligent way."
- Why unresolved: While study demonstrates current AI approaches lag behind human performance on ARC tasks, it does not provide specific insights into how machine learning models can be improved to better emulate human problem-solving strategies.
- What evidence would resolve it: Detailed analysis of human strategies in H-ARC could reveal patterns or heuristics that could be incorporated into AI models.

## Limitations

- Mechanical Turk participants may not fully represent general population's reasoning abilities, introducing sampling bias
- 10-13% missing task data requires statistical imputation, which could affect accuracy estimates
- Three-attempt limit per task constrains ability to measure learning curves beyond initial attempts

## Confidence

- **High confidence** in overall finding that humans outperform current AI approaches on ARC tasks
- **Medium confidence** in specific performance estimates (73.3-77.2% training, 55.9-68.9% evaluation) due to data imputation uncertainty
- **Medium confidence** in claim about different error patterns, as analysis focuses on aggregate metrics rather than detailed error classification

## Next Checks

1. Replicate the study with a different participant pool (e.g., university students) to test generalizability of performance estimates
2. Conduct ablation studies by varying the number of attempts allowed (1, 2, 4, etc.) to understand impact on performance curves
3. Implement controlled experiment comparing error patterns between high-performing and low-performing participants to validate claims about different reasoning approaches