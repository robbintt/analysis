---
ver: rpa2
title: Map-based Modular Approach for Zero-shot Embodied Question Answering
arxiv_id: '2405.16559'
source_url: https://arxiv.org/abs/2405.16559
tags:
- object
- agent
- navigation
- question
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a map-based modular approach for zero-shot
  Embodied Question Answering (EQA), enabling real-world robots to explore and map
  unknown environments to answer natural language questions. The method combines object-goal
  navigation with Visual Question Answering (VQA), using frontier-based exploration
  and semantic mapping to locate target objects.
---

# Map-based Modular Approach for Zero-shot Embodied Question Answering

## Quick Facts
- arXiv ID: 2405.16559
- Source URL: https://arxiv.org/abs/2405.16559
- Authors: Koya Sakamoto; Daichi Azuma; Taiki Miyanishi; Shuhei Kurita; Motoaki Kawanabe
- Reference count: 40
- One-line primary result: Map-based modular approach enables real-world robots to explore, map, and answer natural language questions in zero-shot EQA tasks.

## Executive Summary
This paper introduces a map-based modular approach for zero-shot Embodied Question Answering (EQA), allowing real-world robots to navigate and map unknown environments to answer questions without prior training data. The method integrates object-goal navigation with Visual Question Answering (VQA), using frontier-based exploration and semantic mapping to locate target objects. Image-text matching with vision-language models (BLIP2, CLIP) verifies targets, and VQA modules (BLIP, LLaVA) provide answers based on collected images. The approach is evaluated on Matterport3D-EQA and in real-world settings, demonstrating competitive performance and robustness in open vocabulary scenarios, though challenges remain in navigation precision and complex reasoning.

## Method Summary
The proposed method combines object-goal navigation with VQA by first using frontier-based exploration and semantic mapping to locate target objects in an unknown environment. The robot collects images during navigation, and image-text matching with vision-language models (BLIP2, CLIP) verifies the presence of the target object. Once the target is found, a VQA module (BLIP or LLaVA) analyzes the collected images to generate an answer. This modular pipeline allows the robot to handle open vocabulary questions and operate in real-world settings without prior training on specific environments or question types.

## Key Results
- VQA top-1 accuracy around 0.43 on Matterport3D-EQA dataset
- Successful real-world navigation and answering in two houses
- Competitive performance compared to end-to-end methods
- Robustness demonstrated in handling open vocabularies and diverse questions

## Why This Works (Mechanism)
The method works by decomposing the EQA task into modular steps: exploration, mapping, object verification, and VQA. Each module leverages recent advances in vision-language models, enabling the system to generalize to new environments and questions. The modular design allows for interpretability and easier debugging, while the use of semantic mapping and image-text matching ensures robust target localization even in complex environments.

## Foundational Learning
- **Embodied Question Answering (EQA)**: Requires an agent to navigate and gather information to answer questions about its environment; needed for real-world robotic applications where agents must interact with and understand their surroundings.
- **Semantic Mapping**: Building a spatial representation of the environment with object labels; critical for localization and navigation in unknown spaces.
- **Vision-Language Models (e.g., BLIP2, CLIP)**: Enable zero-shot image-text matching and VQA; allow the system to generalize to new objects and questions without retraining.
- **Frontier-based Exploration**: Strategy for autonomous exploration by navigating to the boundary between known and unknown space; ensures efficient coverage of the environment.
- **Modular vs. End-to-End Approaches**: Modular systems offer interpretability and easier debugging, while end-to-end systems may be more efficient but less transparent.

## Architecture Onboarding

**Component Map**
- Perception (Semantic Mapping) -> Navigation (Object-Goal) -> Target Verification (Image-Text Matching) -> VQA (Answer Generation)

**Critical Path**
The critical path is: Semantic Mapping → Navigation → Target Verification → VQA. Errors or failures in early modules (e.g., inaccurate mapping or navigation) propagate downstream, potentially leading to incorrect answers or failed object detection.

**Design Tradeoffs**
The modular design offers interpretability and easier debugging, but may suffer from error accumulation across stages. Using vision-language models enables zero-shot generalization but can be sensitive to model quality and input data.

**Failure Signatures**
Failures may arise from incomplete or inaccurate semantic maps, navigation errors leading to missed targets, or VQA modules failing on ambiguous or rare queries. Complex reasoning tasks (e.g., counting objects across rooms) remain challenging.

**3 First Experiments**
1. Evaluate semantic mapping accuracy in a new, cluttered environment.
2. Test navigation robustness with dynamic obstacles or variable lighting.
3. Assess VQA performance on ambiguous or long-tail question types.

## Open Questions the Paper Calls Out
- How does the system generalize to environments not represented in the Matterport3D-EQA dataset?
- What is the impact of error accumulation across the modular pipeline on overall performance?
- Can the approach scale to multi-agent or collaborative EQA scenarios?

## Limitations
- Semantic mapping can be incomplete or inaccurate in complex real-world environments, affecting navigation and target localization.
- Error accumulation across modules may degrade performance, and VQA robustness on long-tail or ambiguous queries is not thoroughly assessed.
- Evaluation is limited to two real-world houses and a single dataset, raising generalizability concerns.

## Confidence

**High**
- The method's ability to combine object-goal navigation with VQA and semantic mapping is well-supported by results on Matterport3D-EQA and real-world tests.

**Medium**
- The claim of competitive performance versus end-to-end methods is plausible but limited by the small number of environments and questions tested.

**Low**
- The robustness to open vocabularies and diverse questions is asserted but not thoroughly validated, especially for rare or ambiguous cases.

## Next Checks
1. Test the system in a larger and more diverse set of real-world environments, including those with dynamic obstacles or variable lighting, to assess robustness.
2. Conduct ablation studies to quantify the impact of each module (navigation, mapping, VQA) on overall performance and identify error propagation.
3. Evaluate the system on a broader set of question types, especially those requiring complex reasoning or multi-step inference, to identify limitations in current VQA modules.