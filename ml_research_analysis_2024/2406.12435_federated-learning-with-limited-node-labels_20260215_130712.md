---
ver: rpa2
title: Federated Learning with Limited Node Labels
arxiv_id: '2406.12435'
source_url: https://arxiv.org/abs/2406.12435
tags:
- learning
- graph
- federated
- fedmpa
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces FedMpa, a federated learning framework for
  subgraph node classification with limited labeled data. FedMpa first trains a multilayer
  perceptron (MLP) model using a small amount of data to obtain global features, then
  propagates these features to local subgraph structures using approximate personalized
  PageRank.
---

# Federated Learning with Limited Node Labels

## Quick Facts
- arXiv ID: 2406.12435
- Source URL: https://arxiv.org/abs/2406.12435
- Authors: Bisheng Tang; Xiaojun Chen; Shaopu Wang; Yuexin Xuan; Zhendong Zhao
- Reference count: 40
- Key outcome: FedMpa outperforms FedSage+ by up to 64.7% in node classification accuracy on six graph datasets with limited labeled data

## Executive Summary
This paper introduces FedMpa, a federated learning framework for subgraph node classification when labeled data is scarce. The framework first trains a global MLP to extract node features across all clients, then uses approximate personalized PageRank (APPNP) to propagate these features to local subgraph structures. For handling cross-subgraph edges, FedMpae reconstructs local graph structure by pooling missing nodes and edges into super-nodes and relearning edge weights. Experiments demonstrate that FedMpa consistently outperforms baseline methods including FedSage+ across six datasets when label rates are low.

## Method Summary
FedMpa operates in three stages: (1) FedMLP trains a global MLP across all clients to learn node feature embeddings using FedAvg, (2) FedMpa applies APPNP diffusion to propagate these global features to local subgraphs for node classification, and (3) FedMpae reconstructs local graph structure by pooling missing cross-subgraph nodes/edges into super-nodes and relearning edge weights through a graph autoencoder. The framework combines cross-entropy classification loss with reconstruction loss to jointly optimize feature and structure learning.

## Key Results
- FedMpa achieves up to 64.7% improvement over FedSage+ on Cora dataset with 1% labeled data
- FedMpa consistently outperforms LocMpa across all six datasets, demonstrating the effectiveness of learned federated features
- FedMpae shows advantages over both LocMpa and FedMpa across all datasets by effectively reconstructing local structure

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FedMpa improves cross-subgraph node classification by decoupling global feature learning from local structure diffusion
- Mechanism: The model first trains a simple MLP (FedMLP) to extract global feature embeddings, then applies APPNP-style diffusion using these embeddings on local subgraphs. This separation allows the diffusion to benefit from global knowledge without requiring extensive local labels
- Core assumption: The MLP trained on limited labeled data can capture sufficient global structure to guide local diffusion
- Evidence anchors:
  - [abstract] "FedMpa first trains a multilayer perceptron (MLP) model using a small amount of data to obtain global features, then propagates these features to local subgraph structures using approximate personalized PageRank"
  - [section 3.1] "R(0) = MLP(X), where MLP is a multi-layer perceptron. With the learned feature representation R(0), APPNP then approximates the propagation of the low-dimension feature with personal PageRank..."
- Break condition: If the global feature embedding from MLP fails to capture meaningful cross-subgraph patterns, the subsequent diffusion will not improve local node classification

### Mechanism 2
- Claim: FedMpae reconstructs local subgraph structure by pooling missing cross-subgraph nodes and edges into super-nodes, avoiding the need to simulate missing neighbors
- Mechanism: Instead of generating missing neighbors as in FedSage+, FedMpae uses graph pooling to condense missing nodes and edges into super-nodes, then relearns edge weights for the augmented graph. This reduces computational overhead and avoids potential overfitting to synthetic neighbors
- Core assumption: Pooling missing structure into super-nodes preserves enough structural information for effective message passing while reducing complexity
- Evidence anchors:
  - [abstract] "To handle cross-subgraph edges, FedMpa proposes FedMpae, which reconstructs local graph structure by pooling missing nodes and edges into super-nodes and relearning edge weights"
  - [section 3.2] "Specifically, we relearn an adjacency matrix ˇA with super-nodes representing the missing nodes and edges during the training process..."
- Break condition: If pooling into super-nodes overly coarsens the graph, important local structure may be lost, harming classification accuracy

### Mechanism 3
- Claim: Combining FedMpa and FedMpae allows the framework to outperform FedSage+ in low-label scenarios by leveraging global features and efficient structure reconstruction
- Mechanism: FedMpa uses global features to improve local diffusion, while FedMpae refines the local graph structure via pooling and edge weight relearning. Together, they reduce reliance on extensive local labels and expensive cross-subgraph edge simulation
- Core assumption: The two-stage approach (feature extraction → diffusion → structure reconstruction) is more effective than FedSage+'s simulation-based approach when labeled data is scarce
- Evidence anchors:
  - [abstract] "Experiments on six graph datasets demonstrate that FedMpa outperforms baseline methods, including FedSage+, in node classification accuracy when labeled data is scarce"
  - [section 4.2] "FedMpa constantly outperforms LocMpa, indicating the effectiveness of the learned federated feature. Furthermore, our FedMpae demonstrates advantages in LocMpa and FedMpa across six datasets..."
- Break condition: If the global feature embedding or pooling strategy fails to generalize, the combined approach may not yield consistent gains over FedSage+

## Foundational Learning

- Concept: Personalized PageRank diffusion (APPNP)
  - Why needed here: Provides a scalable, label-efficient way to propagate global features across local subgraphs without requiring extensive labeled data
  - Quick check question: How does the teleport probability α in APPNP influence the balance between local and global information in node representations?

- Concept: Graph pooling for structure coarsening
  - Why needed here: Enables efficient reconstruction of cross-subgraph structure by condensing missing nodes/edges into super-nodes, reducing computational overhead compared to neighbor simulation
  - Quick check question: What trade-off exists between pooling granularity and preservation of local structural information?

- Concept: Federated averaging (FedAvg) for distributed training
  - Why needed here: Allows aggregation of model parameters across clients without sharing raw graph data, preserving privacy while enabling global feature learning
  - Quick check question: How does non-i.i.d data distribution across clients affect the convergence and effectiveness of FedAvg in this context?

## Architecture Onboarding

- Component map:
  - FedMLP (global feature extraction) → FedMpa (local diffusion with APPNP) → FedMpae (structure reconstruction with pooling)

- Critical path:
  1. Train FedMLP on all clients to obtain global feature parameters
  2. Each client runs FedMpa: diffuse global features via APPNP on local subgraph
  3. Each client runs FedMpae: reconstruct augmented graph and train with combined classification + reconstruction loss
  4. Aggregate gradients via FedAvg for each component

- Design tradeoffs:
  - Using MLP for global features trades expressiveness for label efficiency
  - Pooling missing nodes into super-nodes reduces computational cost but may coarsen local structure
  - Combining classification and reconstruction losses balances feature and structure learning but adds hyperparameter tuning complexity

- Failure signatures:
  - FedMpa degrades to near LocMpa performance → global features not informative
  - FedMpae shows no improvement over FedMpa → pooling not preserving useful structure
  - High variance in results across clients → non-i.i.d data distribution not handled well

- First 3 experiments:
  1. Compare FedMpa vs LocMpa on a small subgraph dataset with 1% label rate to verify global feature benefit
  2. Compare FedMpae vs FedMpa on the same dataset to verify pooling-based structure reconstruction
  3. Run FedMpa + FedMpae vs FedSage+ on Cora with varying label rates to confirm low-label advantage

## Open Questions the Paper Calls Out
None

## Limitations

- The paper claims significant accuracy gains over FedSage+ (up to 64.7%) but doesn't fully specify experimental conditions and hyperparameter tuning process
- The effectiveness of pooling missing nodes into super-nodes relies on assumptions about structural preservation that aren't empirically validated
- The framework's sensitivity to non-i.i.d data distribution across clients isn't quantified or systematically tested

## Confidence

- **High confidence**: FedMpa's core mechanism of decoupling global feature learning from local diffusion is technically sound and well-supported by the APPNP literature
- **Medium confidence**: The FedMpae structure reconstruction approach shows promise but lacks sufficient empirical validation of the pooling operation's effectiveness
- **Low confidence**: The claimed 64.7% improvement over FedSage+ is difficult to verify without access to the exact experimental setup and hyperparameter configurations

## Next Checks

1. Replicate the FedMpa vs FedSage+ comparison on Cora with varying label rates (1%, 5%, 10%) using the same MLP architecture and APPNP parameters to verify the claimed accuracy improvements
2. Conduct an ablation study comparing FedMpae with different pooling granularities to determine the optimal trade-off between computational efficiency and structural preservation
3. Test the framework's sensitivity to data heterogeneity by systematically varying the non-i.i.d distribution of node labels across clients and measuring performance degradation