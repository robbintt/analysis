---
ver: rpa2
title: 'Multi-Agent Training for Pommerman: Curriculum Learning and Population-based
  Self-Play Approach'
arxiv_id: '2407.00662'
source_url: https://arxiv.org/abs/2407.00662
tags:
- agent
- agents
- training
- reward
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a multi-agent training system for Pommerman
  using curriculum learning and population-based self-play. The approach addresses
  sparse rewards through an adaptive annealing factor that dynamically adjusts exploration
  rewards based on agent performance, and implements Elo-based matchmaking for effective
  opponent pairing.
---

# Multi-Agent Training for Pommerman: Curriculum Learning and Population-based Self-Play Approach

## Quick Facts
- **arXiv ID**: 2407.00662
- **Source URL**: https://arxiv.org/abs/2407.00662
- **Reference count**: 12
- **Primary result**: Trained agent achieves 982 Elo rating points and wins 98.85% of matches against Pommerman's baseline

## Executive Summary
This study presents a multi-agent training system for Pommerman using curriculum learning and population-based self-play. The approach addresses sparse rewards through an adaptive annealing factor that dynamically adjusts exploration rewards based on agent performance, and implements Elo-based matchmaking for effective opponent pairing. The system trains agents through three curriculum phases with increasing difficulty, followed by self-play against a population of agents. Experiments show the trained agent achieves 982 Elo rating points and wins 98.85% of matches against Pommerman's baseline, outperforming top learning agents from 2018 competitions.

## Method Summary
The method employs actor-critic architecture with Proximal Policy Optimization (PPO) to train agents in the Pommerman environment. Training proceeds through three curriculum phases: static agents, simple moving agents, and simple bomb agents, with an adaptive annealing factor that shifts focus from exploration to game rewards based on performance. After curriculum training, population-based self-play with Elo-based matchmaking continues training with a population of 8 agents (3 rule-based, 5 trained). The network architecture consists of 4 convolutional layers, LSTM for temporal memory, and separate value/policy heads.

## Key Results
- Trained agent achieves 982 Elo rating points, winning 98.85% of matches against Pommerman's baseline
- Outperforms top learning agents from 2018 competitions (Alpha-Pommerman, Fair-Pommerman, pPO, DGQ)
- Agent wins 77% of matches against tree-search-based opponents without requiring communication between allied agents

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptive annealing dynamically shifts learning focus from exploration to game rewards based on agent performance.
- Mechanism: The annealing factor α is calculated as α = 1 - tanh(k * x), where x is the average number of enemy deaths. At the beginning, when x = 0, α = 1, maximizing exploration rewards. As x increases (indicating better performance), α decreases toward 0, minimizing exploration rewards and maximizing game rewards.
- Core assumption: Agent performance (enemy deaths) is a reliable proxy for readiness to transition from exploration-focused to game-reward-focused learning.
- Evidence anchors:
  - [abstract] "we propose an adaptive annealing factor based on agents' performance to adjust the dense exploration reward during training dynamically"
  - [section] "the annealing factor α in Equation 1 is equal to 0. In this way, the agent prefers to do actions to explore the area... After about 5 million timesteps, the agent is able to locate and eliminate static agent, reaching the win rate of 55%."
  - [corpus] No direct evidence found for adaptive annealing in corpus papers.

### Mechanism 2
- Claim: Elo-based matchmaking ensures progressive learning by pairing agents with appropriate difficulty opponents.
- Mechanism: Matchmaking probability is calculated using softmax on expected win rates derived from Elo ratings. Higher Elo agents have higher probability of being selected as opponents, ensuring agents face increasingly challenging competition.
- Core assumption: Elo ratings accurately reflect agent strength and matchmaking probabilities derived from them create appropriate learning challenges.
- Evidence anchors:
  - [abstract] "we implement a matchmaking mechanism utilizing the Elo rating system to pair agents effectively"
  - [section] "the matchmaking probability of other agents to a selected agent is calculated by applying the softmax function to the expected win rate between those agents and the selected one"
  - [corpus] No direct evidence found for Elo-based matchmaking in corpus papers.

### Mechanism 3
- Claim: Curriculum learning with incremental difficulty rule-based agents provides structured skill progression.
- Mechanism: Three phases with increasingly difficult rule-based opponents (static, simple moving, simple bomb agents) ensure the agent learns essential skills in order: exploration, combat, and defensive strategies.
- Core assumption: The staged progression through increasingly difficult opponents builds necessary skills in the correct order for Pommerman success.
- Evidence anchors:
  - [abstract] "curriculum learning and population-based self-play"
  - [section] "we designed a curriculum learning with three phases: exploring the map and locating opponents, eliminating opponents, and surviving encounters while fighting opponents"
  - [corpus] No direct evidence found for curriculum learning in corpus papers.

## Foundational Learning

- **Proximal Policy Optimization (PPO)**: PPO provides stable policy updates for multi-agent reinforcement learning in competitive environments. How does PPO's clipped objective prevent destructive policy updates in this competitive setting?
- **Partially Observable Markov Decision Processes (POMDPs)**: Pommerman is partially observable (9x9 vision limit), requiring agents to handle incomplete state information. How does the 9x9 vision constraint affect the agent's ability to plan long-term strategies?
- **Self-play population dynamics**: Population-based self-play creates diverse opponents and prevents overfitting to specific strategies. What happens when an agent's Elo rating drops below the population average for extended periods?

## Architecture Onboarding

- **Component map**: State preprocessing → CNN feature extraction → LSTM memory → Action/policy selection → Environment interaction → Reward processing → Network update
- **Critical path**: State preprocessing → CNN feature extraction → LSTM memory → Action/policy selection → Environment interaction → Reward processing → Network update
- **Design tradeoffs**: Dense exploration rewards vs. sparse game rewards (solved by adaptive annealing), rule-based vs. learning-based opponents in curriculum (balanced for progressive difficulty)
- **Failure signatures**: Agent gets stuck in local optima (indicates curriculum difficulty mismatch), Elo ratings diverge wildly (indicates matchmaking instability), performance plateaus (indicates exploration-reward balance issues)
- **First 3 experiments**:
  1. Test adaptive annealing alone with static opponent to verify performance-based reward shifting
  2. Validate Elo-based matchmaking with fixed agents to ensure appropriate opponent selection
  3. Run full curriculum with one phase to confirm skill progression before adding population self-play

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the adaptive annealing factor compare to other exploration reward methods in multi-agent competitive environments?
- Basis in paper: [explicit] The paper introduces an adaptive annealing factor based on agent performance to dynamically adjust exploration rewards, contrasting it with linear annealing
- Why unresolved: The paper only compares adaptive annealing to linear annealing within their system, without testing against other exploration reward methods like intrinsic motivation or count-based exploration
- What evidence would resolve it: Comparative experiments testing the adaptive annealing factor against alternative exploration methods in Pommerman or similar multi-agent competitive environments

### Open Question 2
- Question: Would the Elo-based matchmaking mechanism scale effectively to larger populations of agents or different competitive game environments?
- Basis in paper: [explicit] The paper implements Elo-based matchmaking for 8 agents in Pommerman, claiming it ensures progressive learning
- Why unresolved: The paper only tests the matchmaking system on a specific population size and environment, without exploring its performance with larger populations or different competitive games
- What evidence would resolve it: Experiments testing the Elo matchmaking mechanism with varying population sizes and different competitive game environments

### Open Question 3
- Question: How would the training performance change if communication between allied agents was enabled?
- Basis in paper: [explicit] The paper notes their agent achieves 982 Elo rating without communication, while agents with communication outperform it
- Why unresolved: The paper deliberately excludes communication to focus on non-communicative agents, leaving the performance ceiling unknown
- What evidence would resolve it: Training and testing the same system architecture with communication enabled between allied agents

### Open Question 4
- Question: What is the long-term stability of the learned strategies against evolving opponent populations?
- Basis in paper: [inferred] The paper describes population-based self-play but only reports results after a fixed training period
- Why unresolved: The paper doesn't investigate whether the learned strategies remain effective as the opponent population continues to evolve over extended training periods
- What evidence would resolve it: Long-term training experiments tracking strategy evolution and performance against continuously evolving opponent populations

## Limitations
- Lacks detailed hyperparameter specifications, particularly PPO learning rate, batch size, entropy regularization, and the exact K-factor for Elo updates
- Timing of curriculum transitions and population replacement criteria are vaguely defined as "45% win rate" and "55% win rate" without specifying whether these are smoothed metrics or immediate thresholds
- Population-based self-play mechanism mentions 8 agents total (3 rule-based, 5 trained) but doesn't specify how often agents are replaced or how matchmaking diversity is maintained

## Confidence
- Adaptive Annealing Mechanism: **High** - The mathematical formulation is clearly specified and the performance correlation is demonstrated
- Elo-based Matchmaking: **Medium** - Mechanism is specified but lacks details on K-factor and convergence properties
- Curriculum Design: **Medium** - Three-phase progression is described but timing and transition criteria lack precision
- Final Performance Claims: **High** - Results are clearly quantified and benchmarked against known baselines

## Next Checks
1. Verify adaptive annealing dynamics by running ablation studies with fixed vs. adaptive annealing factors on the static agent curriculum phase
2. Test Elo rating stability by running matchmaking with fixed agents of known strength to confirm proper difficulty progression
3. Validate curriculum progression by measuring skill acquisition at each phase boundary using held-out opponents