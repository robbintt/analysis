---
ver: rpa2
title: A Reply to Makelov et al. (2023)'s "Interpretability Illusion" Arguments
arxiv_id: '2401.12631'
source_url: https://arxiv.org/abs/2401.12631
tags:
- head
- name
- layer
- makelov
- position
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper argues that distributed interchange interventions can
  reveal meaningful causal structure in neural networks, rather than producing mere
  "interpretability illusions" as previously claimed. It demonstrates that even intuitive
  explanations can be labeled as illusions under the strict definition used by Makelov
  et al.
---

# A Reply to Makelov et al. (2023)'s "Interpretability Illusion" Arguments

## Quick Facts
- arXiv ID: 2401.12631
- Source URL: https://arxiv.org/abs/2401.12631
- Reference count: 40
- Authors: Zhengxuan Wu, Atticus Geiger, Jing Huang, Aryaman Arora, Thomas Icard, Christopher Potts, Noah D. Goodman
- Key outcome: The paper argues that distributed interchange interventions can reveal meaningful causal structure in neural networks, rather than producing mere "interpretability illusions" as previously claimed.

## Executive Summary
This paper challenges Makelov et al.'s (2023) claim that distributed interchange interventions create "interpretability illusions" in neural networks. The authors demonstrate that the geometric relationships Makelov et al. consider problematic are actually natural features of how networks process information, and that nullspace decompositions need not be orthogonal to data-induced manifolds. Through extensive experiments on the indirect object identification task with GPT-2, they show that DAS can effectively identify causal variables like name position information, with interchange intervention accuracy reaching 70% in residual streams and revealing distributed representations across attention heads.

## Method Summary
The authors use Distributed Alignment Search (DAS) to find intervention subspaces in GPT-2 that correspond to high-level causal variables like name position information. DAS is trained with Adam optimizer (learning rate 0.01, batch size 20, 10 epochs) to learn single-dimensional subspaces representing causal variables. Interchange intervention accuracy (IIA) serves as the primary evaluation metric, measuring the percentage of counterfactual predictions matched under interchange interventions between the high-level causal model and the neural model. The methodology is applied to the indirect object identification (IOI) task and factual recall task, with experiments including leave-one-out head ablation and comparison with Boundless DAS.

## Key Results
- Interchange intervention accuracy for name position information reaches 70% in residual streams of GPT-2
- Name position information emerges in a distributed manner across attention heads rather than being localized
- Head 6 alone reaches 0% IIA, and less than 20% when considered together with head 10, indicating distributed processing
- The geometric relationships Makelov et al. consider problematic are actually natural features of how networks process information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distributed interchange interventions reveal genuine causal structure in neural networks rather than creating illusory effects.
- Mechanism: The interventions operate on data-induced submanifolds that naturally include variations along nullspace directions of downstream components, making these variations causally relevant rather than illusory.
- Core assumption: Neural network representations inherently include variation along nullspace directions of downstream components due to natural input-driven variation.
- Evidence anchors:
  - [abstract] "The geometric relationships Makelov et al. consider problematic are actually natural features of how networks process information"
  - [section] "unless the inputs and all subsequent representations are orthogonal to the nullspace of the relevant model components (unlikely in practice, and not something we would impose), so-called illusions are inevitable"
  - [corpus] Weak corpus evidence for "distributed interchange interventions" specifically - only 0 related papers found
- Break condition: If neural network representations were constrained to be orthogonal to nullspace directions, the mechanism would break down.

### Mechanism 2
- Claim: Interchange intervention accuracy (IIA) is a valid metric for evaluating model interpretability.
- Mechanism: IIA measures the percentage of counterfactual predictions matched under interchange interventions, providing confidence in high-level causal models as explanations of neural network behavior.
- Core assumption: High IIA values indicate that high-level causal models faithfully explain neural network behavior under performed interventions.
- Evidence anchors:
  - [section] "Interchange intervention accuracy (IIA) is the percentage of counterfactual predictions matched under interchange interventions between the high-level causal model and the neural model"
  - [section] "when IIA is 100%, the causal model is a faithful explanation of how the neural network behaves under the set of interventions we perform"
  - [corpus] Weak corpus evidence for "interchange intervention accuracy" specifically - only 0 related papers found
- Break condition: If IIA values were misleading or failed to correlate with actual causal understanding, this mechanism would fail.

### Mechanism 3
- Claim: Name position information in GPT-2 emerges in a distributed manner across attention heads rather than being localized to specific components.
- Mechanism: Learning a single DAS direction across concatenated head representations achieves higher IIA than individual head alignment, indicating distributed processing.
- Core assumption: Information can be processed across multiple neural components in ways that are only detectable through distributed interventions.
- Evidence anchors:
  - [section] "head 6 alone reaches 0% IIA, and it reaches less than 20% when considered together with head 10" - showing distributed nature
  - [section] "Our findings suggest that all heads are crucial to carrying out all the name position information"
  - [corpus] Weak corpus evidence for "distributed information processing" in neural networks - only 0 related papers found
- Break condition: If information processing were strictly localized rather than distributed, this mechanism would fail.

## Foundational Learning

- Concept: Causal abstraction
  - Why needed here: The paper relies on comparing high-level causal models with low-level neural network behavior through interventions
  - Quick check question: What is the difference between behavioral equivalence and structural equivalence in causal abstraction frameworks?

- Concept: Nullspace decomposition
  - Why needed here: Critical for understanding why interventions along certain directions might be labeled as "illusory" in Makelov et al.'s framework
  - Quick check question: If a vector v is decomposed into v_nullspace and v_rowspace, what happens to v_nullspace when multiplied by the downstream weight matrix W_out?

- Concept: Interchange intervention accuracy
  - Why needed here: The primary metric used to evaluate how well high-level causal variables align with neural representations
  - Quick check question: If an intervention achieves 70% IIA, what does this tell us about the alignment between the causal model and neural network?

## Architecture Onboarding

- Component map: GPT-2 Small -> DAS (Distributed Alignment Search) -> Interchange Intervention Framework -> IOI/Factual Recall Tasks
- Critical path: 1) Train DAS to find causal variables, 2) Apply interchange interventions, 3) Measure IIA, 4) Analyze distribution of learned weights
- Design tradeoffs: DAS trades computational complexity for the ability to find distributed representations versus traditional neuron-level interventions
- Failure signatures: Low IIA values (e.g., 0-4%) indicate failure to find relevant structure; overfitting when DAS only works on seen examples
- First 3 experiments:
  1. Replicate Makelov et al.'s IOI experiment with GPT-2 to verify name position information alignment at layer 8
  2. Perform leave-one-out head ablation to determine which attention heads carry name position information
  3. Apply Boundless DAS to compare with standard DAS performance on the IOI task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop metrics beyond IIA that better capture the causal relevance of distributed representations in neural networks?
- Basis in paper: [explicit] The paper discusses the limitations of FLDD and suggests that metrics comparing logit differences (e.g., KL divergence) could be promising alternatives to IIA.
- Why unresolved: The paper identifies the need for better metrics but does not propose or test specific alternatives that could more accurately measure the causal effects of distributed representations.
- What evidence would resolve it: Experiments demonstrating that new metrics more accurately predict counterfactual behavior and correlate better with known causal structures in neural networks.

### Open Question 2
- Question: What are the implications of distributed representations being naturally non-orthogonal to downstream nullspaces for the interpretability of neural networks?
- Basis in paper: [inferred] The paper argues that distributed representations reflecting natural variation in activations driven by inputs need not vanish in null directions, suggesting this is a general feature rather than an "illusion."
- Why unresolved: While the paper challenges the notion of "interpretability illusions," it does not fully explore how this geometric relationship affects our understanding of neural network computation and interpretability.
- What evidence would resolve it: Studies showing how these geometric relationships manifest across different network architectures and tasks, and how they impact the effectiveness of interpretability methods.

### Open Question 3
- Question: How can we design experiments to avoid the pitfalls of overfitting and generalization when using DAS to find alignments between neural networks and high-level causal models?
- Basis in paper: [explicit] The paper identifies two pitfalls in the factual recall experiment: overfitting to a single pair of facts and finding a direction that aligns with an uninteresting high-level causal model.
- Why unresolved: The paper critiques these issues but does not provide a framework for designing experiments that ensure meaningful generalizations and avoid alignment with trivial high-level models.
- What evidence would resolve it: Development and validation of experimental protocols that use diverse datasets and test for generalization across multiple high-level causal models, ensuring the discovered directions are both generalizable and meaningful.

## Limitations

- The evidence is largely confined to the indirect object identification task and factual recall, leaving open questions about generalizability to other architectures and tasks
- The geometric arguments about nullspaces and data-induced manifolds are plausible but not fully proven with rigorous mathematical proofs
- The paper does not address potential confirmation bias in interpreting distributed representations as meaningful versus coincidental correlations

## Confidence

**High confidence**: The empirical demonstration that name position information is distributed across attention heads (70% IIA) is well-supported by the experimental results and represents a clear, reproducible finding that challenges the notion of strict localization in neural representations.

**Medium confidence**: The argument that Makelov et al.'s "illusion" examples are artifacts of their evaluation framework is persuasive but relies heavily on reinterpretation rather than direct falsification of their claims. The geometric arguments about nullspaces and data-induced manifolds are plausible but not fully proven.

**Low confidence**: The broader claim that distributed interchange interventions should be the preferred methodology for interpretability work is not fully substantiated, as the paper focuses primarily on defending against criticism rather than demonstrating superior performance across diverse scenarios.

## Next Checks

1. **Cross-task generalization**: Apply the same DAS methodology to completely different tasks (e.g., arithmetic reasoning, code generation) to determine whether distributed representations consistently emerge across diverse domains, or if the IOI results are task-specific.

2. **Alternative intervention baselines**: Compare DAS against other intervention methods (neuron-level, feature-level) on identical tasks to quantify the actual benefit of distributed interventions, including computational cost-benefit analysis.

3. **Formal geometric analysis**: Develop rigorous mathematical proofs or simulations demonstrating that nullspace directions are indeed naturally aligned with data-induced manifolds in trained networks, rather than being coincidental artifacts of specific training runs or architectures.