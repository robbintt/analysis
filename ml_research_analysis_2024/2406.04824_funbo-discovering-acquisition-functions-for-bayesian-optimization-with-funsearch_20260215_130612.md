---
ver: rpa2
title: 'FunBO: Discovering Acquisition Functions for Bayesian Optimization with FunSearch'
arxiv_id: '2406.04824'
source_url: https://arxiv.org/abs/2406.04824
tags: []
core_contribution: This paper proposes FunBO, a method for discovering new acquisition
  functions for Bayesian optimization by using a large language model to iteratively
  modify the code of an initial acquisition function. FunBO is based on FunSearch,
  a recently proposed algorithm that combines an LLM with an evaluator to solve open
  problems in mathematics and algorithm design.
---

# FunBO: Discovering Acquisition Functions for Bayesian Optimization with FunSearch

## Quick Facts
- **arXiv ID**: 2406.04824
- **Source URL**: https://arxiv.org/abs/2406.04824
- **Reference count**: 40
- **Primary result**: FunBO discovers acquisition functions that generalize well across function classes, outperforming general-purpose AFs and achieving competitive performance against function-specific ones.

## Executive Summary
FunBO is a method that uses a large language model to iteratively modify acquisition function code for Bayesian optimization, guided by performance scores from full BO evaluations. Building on the FunSearch framework, FunBO can discover acquisition functions that generalize well both within and across function classes. The paper demonstrates that FunBO-discovered AFs outperform general-purpose acquisition functions and achieve competitive results against transfer-learned, function-specific AFs, while providing interpretable code snippets that can be easily deployed without additional infrastructure overhead.

## Method Summary
FunBO employs an LLM (Codey) to iteratively modify the code of an initial acquisition function, using prompts constructed from previously generated AFs stored in a program database. At each iteration, the LLM samples new AF code which is then evaluated through complete BO loops on a training set of auxiliary objective functions. The performance is scored based on proximity to the true optimum and evaluation efficiency, with better-performing AFs retained in the database. The method uses an island model structure to balance exploration and exploitation, and ultimately selects the best-performing AF based on validation set performance. FunBO operates within a 48-hour time budget and uses GP surrogates for BO evaluations.

## Key Results
- FunBO-discovered AFs outperform general-purpose acquisition functions across diverse benchmark classes
- The discovered AFs achieve competitive performance against function-specific AFs learned via transfer-learning methods
- FunBO generalizes effectively both within and across function classes, with discovered AFs transferring well to unseen functions
- The method produces interpretable code snippets that can be deployed without additional infrastructure overhead

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FunBO generates novel acquisition functions by using an LLM to iteratively modify code, guided by BO performance scores.
- Mechanism: The LLM proposes code changes to an initial AF, which is then evaluated via full BO loops on a training set of functions; better-performing code is retained in a program database and used to prompt further modifications.
- Core assumption: AF performance can be reliably scored by running full BO loops on a small set of functions.
- Evidence anchors:
  - [abstract] "FunBO, an LLM-based method that can be used to learn new AFs written in computer code by leveraging access to a limited number of evaluations for a set of objective functions."
  - [section 3] "At every algorithm iteration, a prompt is constructed by sampling two AFs, hi and hj, previously generated and stored in DB."
  - [corpus] Weak: corpus neighbors focus on alternative BO methods but none describe LLM-driven AF code generation.
- Break condition: If the LLM fails to generate compilable code or the BO evaluation metric does not correlate with true AF effectiveness.

### Mechanism 2
- Claim: FunBO generalizes well both within and across function classes by learning AFs that do not assume specific similarities between training and test functions.
- Mechanism: The training set G contains diverse functions; FunBO learns AFs that perform well on this set and validates on a held-out subset GV, ensuring robustness to distribution shifts.
- Core assumption: AFs that perform well on a diverse training set will also perform well on unseen functions.
- Evidence anchors:
  - [abstract] "FunBO identifies AFs that generalize well in and out of the training distribution of functions."
  - [section 3] "We consider acquisition_function in Fig. 2 (top) which takes the functional form of the EI and has as inputs the union of the inputs given to EI, UCB and PofI."
  - [corpus] Weak: corpus neighbors do not directly support generalization claims; they focus on other BO improvements.
- Break condition: If the training set G is not sufficiently diverse, learned AFs may overfit and fail to generalize.

### Mechanism 3
- Claim: FunBO's code-based AFs are more interpretable and easier to deploy than neural network-based AFs.
- Mechanism: Since the AFs are expressed in Python code, they can be inspected, modified, and deployed without additional infrastructure overhead.
- Core assumption: Code-based representations are inherently more interpretable than opaque neural networks.
- Evidence anchors:
  - [abstract] "FunBO outputs code snippets corresponding to improved AFs, which can be inspected to (i) identify differences with respect to known AFs and (ii) investigate the reasons for their observed performance, thereby enforcing interpretability, and (iii) be easily deployed in practice without additional infrastructure overhead."
  - [section 2] "Unlike general-purpose AFs, several works have proposed increasing the efficiency of BO for a specific optimization problem... by learning problem-specific AFs [14, 40, 43]."
  - [corpus] Weak: corpus neighbors do not address interpretability of AFs.
- Break condition: If the generated code is too complex or relies on undocumented functions, interpretability benefits are lost.

## Foundational Learning

- Concept: Bayesian Optimization (BO) basics
  - Why needed here: FunBO builds upon BO, so understanding its core components (surrogate model, acquisition function) is essential.
  - Quick check question: What is the role of the acquisition function in BO?

- Concept: Gaussian Processes (GPs) and their use as surrogate models
  - Why needed here: FunBO uses GPs to model objective functions and evaluate AF performance.
  - Quick check question: How does a GP provide a posterior mean and variance for a given input?

- Concept: Large Language Models (LLMs) and their ability to generate code
  - Why needed here: FunBO relies on an LLM to propose new AF code based on prompts.
  - Quick check question: What is the relationship between prompt quality and the relevance of generated code?

## Architecture Onboarding

- Component map: LLM (Codey) → Prompt builder → AF generator → BO evaluator → Score calculator → Program database (DB) → AF selector
- Critical path:
  1. Prompt construction from two previously generated AFs in DB
  2. LLM sampling of new AFs
  3. BO evaluation on training set GTr
  4. Scoring and storing in DB
  5. Final AF selection based on GV performance
- Design tradeoffs:
  - Evaluation cost vs. exploration: Full BO loops are expensive but necessary for accurate scoring.
  - Diversity vs. exploitation: DB structure balances exploring new AFs and exploiting known good ones.
  - LLM sampling vs. quality: More samples increase chances of good AFs but raise computational cost.
- Failure signatures:
  - LLM generates non-compilable or nonsensical code
  - AFs overfit to training set and perform poorly on GV
  - Evaluation is too slow, preventing timely AF discovery
- First 3 experiments:
  1. OOD-Bench: Test generalization across function classes with diverse synthetic benchmarks.
  2. ID-Bench: Test within-class generalization using scaled/translated versions of known functions.
  3. HPO-ID: Apply FunBO to hyperparameter optimization tasks and compare against MetaBO.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can FunBO scale effectively to larger sets of auxiliary objective functions G without prohibitive computational overhead?
- Basis in paper: [explicit] The paper discusses computational overhead as a limitation, noting that running a full BO loop for each function in G significantly increases evaluation time.
- Why unresolved: The paper does not provide experiments or analysis on FunBO's performance with varying sizes of G.
- What evidence would resolve it: Experiments showing FunBO's performance and computational requirements as the size of G increases, including comparisons to baseline methods.

### Open Question 2
- Question: How would FunBO perform on multi-objective optimization problems where multiple conflicting objectives need to be optimized simultaneously?
- Basis in paper: [inferred] The paper mentions that FunBO's current focus is on single-output BO and suggests potential extension to other adaptations, but does not explore multi-objective optimization.
- Why unresolved: The paper does not provide any experiments or theoretical analysis for multi-objective optimization scenarios.
- What evidence would resolve it: Experiments comparing FunBO's performance on multi-objective optimization problems against existing multi-objective BO methods, along with analysis of the resulting acquisition functions.

### Open Question 3
- Question: Would incorporating more sophisticated metrics beyond the simple score in Eq. (1) improve FunBO's ability to discover effective acquisition functions?
- Basis in paper: [explicit] The paper notes that the current scoring metric only captures distance from optimum and number of trials, suggesting more research is needed on alternative metrics.
- Why unresolved: The paper does not explore or test alternative scoring metrics for FunBO.
- What evidence would resolve it: Experiments comparing FunBO's performance using different scoring metrics, including those that better characterize convergence paths, and analysis of the resulting acquisition functions.

## Limitations

- The computational cost of full BO loop evaluations for each candidate AF may limit scalability to larger problem domains
- The reliance on Codey LLM access raises questions about reproducibility without similar resources
- The diversity and representativeness of the training set G could significantly impact generalization performance

## Confidence

- **Medium**: Claims about superior generalization across function classes, as validation was performed only on synthetic benchmarks
- **Medium**: Interpretability benefits of code-based AFs, as complexity of generated code is not systematically evaluated
- **High**: Core methodology of using LLM to modify AF code, as this is directly demonstrated and implemented

## Next Checks

1. Test FunBO-discovered AFs on real-world optimization problems beyond synthetic benchmarks to verify practical utility
2. Perform ablation studies on prompt diversity and its impact on AF discovery quality
3. Measure and report computational overhead of the FunBO pipeline compared to traditional AF optimization methods