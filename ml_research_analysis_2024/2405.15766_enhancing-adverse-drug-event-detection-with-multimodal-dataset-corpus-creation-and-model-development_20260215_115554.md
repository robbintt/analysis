---
ver: rpa2
title: 'Enhancing Adverse Drug Event Detection with Multimodal Dataset: Corpus Creation
  and Model Development'
arxiv_id: '2405.15766'
source_url: https://arxiv.org/abs/2405.15766
tags:
- text
- dataset
- image
- drug
- adverse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel multimodal dataset (MMADE) for adverse
  drug event (ADE) detection, combining images with textual descriptions. It presents
  a framework leveraging large language models (LLMs) and vision language models (VLMs)
  to generate detailed descriptions of medical images depicting ADEs.
---

# Enhancing Adverse Drug Event Detection with Multimodal Dataset: Corpus Creation and Model Development

## Quick Facts
- arXiv ID: 2405.15766
- Source URL: https://arxiv.org/abs/2405.15766
- Authors: Pranab Sahoo; Ayush Kumar Singh; Sriparna Saha; Aman Chadha; Samrat Mondal
- Reference count: 25
- Key outcome: Novel multimodal dataset (MMADE) for ADE detection combining images with text, fine-tuned InstructBLIP achieves BLEU up to 0.319, ROUGE up to 0.571, BERTScore F1 of 0.893, and MoverScore of 0.622

## Executive Summary
This paper introduces MMADE, a novel multimodal dataset for adverse drug event (ADE) detection that combines medical images with textual descriptions. The authors develop a framework using large language models (LLMs) and vision language models (VLMs) to generate detailed descriptions of medical images depicting ADEs. By fine-tuning models like InstructBLIP on this dataset, they demonstrate significant improvements in performance over text-only methods, showcasing the value of integrating visual cues for enhanced ADE detection in personalized healthcare applications.

## Method Summary
The study creates the MMADE dataset with 1,500 instances of patient-reported concerns containing both images and text descriptions. Vision language models (InstructBLIP, BLIP, GIT) are fine-tuned on this dataset using a query transformer (Q-Former) architecture to extract instruction-aware visual features. The models are trained to generate natural language descriptions that accurately capture both visual and textual information about adverse drug events. Training uses standard hyperparameters including learning rate of 1e-5, 50 epochs, and batch size of 2.

## Key Results
- Fine-tuned InstructBLIP achieves BLEU scores up to 0.319 and ROUGE scores up to 0.571 on MMADE dataset
- Multimodal approach significantly outperforms text-only methods for ADE description generation
- BERTScore F1 reaches 0.893 and MoverScore achieves 0.622, demonstrating strong alignment with reference descriptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Integrating visual cues from medical images improves ADE detection accuracy over text-only methods
- Mechanism: The model leverages fine-tuned InstructBLIP's Query Transformer (Q-Former) to extract instruction-aware visual features, which are then fused with textual descriptions to provide a richer, context-aware representation of adverse drug events
- Core assumption: Visual information captures symptom-specific details that text alone cannot convey, and InstructBLIP's fine-tuning adapts the model to domain-specific medical imagery
- Evidence anchors:
  - [abstract] "We showcase the significance of integrating visual cues from images to enhance overall performance."
  - [section 5] "Our findings suggest that domain-specific fine-tuning significantly enhances overall performance, emphasizing the importance of multimodal visual cues."
- Break condition: If visual features do not correlate with the textual description of ADE, or if fine-tuning does not adapt the model to medical imagery, performance will degrade

### Mechanism 2
- Claim: Fine-tuning VLMs with ADE-specific data improves BLEU, ROUGE, BERTScore, and MoverScore performance
- Mechanism: Pre-trained VLMs (InstructBLIP, BLIP, GIT) are fine-tuned on the curated MMADE dataset, enabling them to learn domain-specific patterns and associations between textual and visual ADE cues
- Core assumption: The MMADE dataset captures representative ADE instances with sufficient diversity to adapt the models effectively
- Evidence anchors:
  - [abstract] "fine-tuned InstructBLIP achieving BLEU scores up to 0.319, ROUGE scores up to 0.571, BERTScore F1 of 0.893, and MoverScore of 0.622."
  - [section 6] "Fine-tuning with domain-specific ADE data remarkably enhances model performance, reflecting its pivotal role in adapting models to the intricacies of adverse drug event detection."
- Break condition: If the dataset lacks sufficient diversity or the fine-tuning process overfits to limited patterns, performance gains will not generalize

### Mechanism 3
- Claim: Multimodal models outperform unimodal models in generating accurate ADE descriptions
- Mechanism: The fusion of image and text modalities allows the model to cross-validate information, reducing hallucination and improving contextual relevance compared to models using only one modality
- Core assumption: Visual and textual modalities are complementary and their integration leads to better contextual understanding
- Evidence anchors:
  - [abstract] "The proposed approach significantly improves performance over text-only methods."
  - [section 6.2] "Observations also revealed that models such as BLIP and GIT tended to hallucinate, as evidenced by Fig. 7, occasionally generating facts that were entirely unrelated to the context."
- Break condition: If either modality is noisy or irrelevant, the multimodal model may underperform compared to a well-tuned unimodal model

## Foundational Learning

- Concept: Vision-Language Models (VLMs) and their components (image encoder, text encoder, fusion mechanisms)
  - Why needed here: Understanding how VLMs process and integrate multimodal inputs is essential to grasp why fine-tuning and multimodal fusion improve ADE detection
  - Quick check question: What is the role of the Query Transformer (Q-Former) in InstructBLIP's architecture?

- Concept: Evaluation metrics for text generation (BLEU, ROUGE, BERTScore, MoverScore)
  - Why needed here: These metrics quantify the quality of generated ADE descriptions and justify the model's performance claims
  - Quick check question: How does BERTScore differ from BLEU in evaluating generated text?

- Concept: Domain adaptation and fine-tuning of pre-trained models
  - Why needed here: Fine-tuning is the key to adapting generic VLMs to the specialized ADE domain, enabling improved performance
  - Quick check question: Why is it important to freeze the image encoder during fine-tuning of InstructBLIP?

## Architecture Onboarding

- Component map: Image (I) -> Vision Transformer (ViT) -> Query Transformer (Q-Former) -> Cross-attention fusion -> LLM -> Text output (Y)
- Critical path:
  1. Load image and text pair
  2. Extract visual features using ViT
  3. Apply Q-Former for instruction-aware feature extraction
  4. Fuse with text features via cross-attention
  5. Generate output using LLM
- Design tradeoffs:
  - Freezing the image encoder reduces fine-tuning parameters but may limit adaptation to ADE-specific visual patterns
  - Using a larger dataset could improve generalization but may introduce privacy and annotation challenges
- Failure signatures:
  - Low BLEU/ROUGE scores indicate poor alignment with reference descriptions
  - High hallucination rates (e.g., irrelevant facts in output) suggest inadequate cross-modal grounding
- First 3 experiments:
  1. Compare fine-tuned InstructBLIP vs. base InstructBLIP on MMADE using all metrics
  2. Evaluate unimodal (text-only) vs. multimodal (text+image) performance on a held-out test set
  3. Test model robustness by introducing noisy or irrelevant images and measuring degradation in output quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of multimodal ADE detection models vary when trained on datasets with different proportions of external vs. internal body part images?
- Basis in paper: [explicit] The paper notes that the dataset primarily focuses on external body parts, omitting data about internal conditions, and acknowledges this as a limitation
- Why unresolved: The paper does not provide experimental results comparing model performance on datasets with varying proportions of external vs. internal body part images
- What evidence would resolve it: Conducting experiments with datasets that have different ratios of external to internal body part images and comparing the model's performance across these datasets would provide insights into how the proportion of image types affects ADE detection accuracy

### Open Question 2
- Question: What is the impact of instruction tuning on the model's ability to distinguish between similar ADE symptoms that appear on the same body part?
- Basis in paper: [explicit] The paper mentions that InstructBLIP, which uses instruction tuning, demonstrates superior performance compared to other models
- Why unresolved: The paper does not provide a detailed analysis of how instruction tuning specifically helps the model differentiate between similar symptoms on the same body part
- What evidence would resolve it: Conducting a detailed analysis of the model's performance on cases where similar ADE symptoms appear on the same body part, comparing the performance of models with and without instruction tuning, would provide insights into the effectiveness of instruction tuning in this specific context

### Open Question 3
- Question: How does the performance of multimodal ADE detection models change when trained on datasets with a more balanced distribution of different body parts affected by ADEs?
- Basis in paper: [inferred] The paper mentions that the dataset has an imbalanced distribution of body parts affected by ADEs, with skin-related ADEs being the most common
- Why unresolved: The paper does not provide experimental results showing how model performance is affected by the imbalance in the distribution of body parts in the dataset
- What evidence would resolve it: Conducting experiments with datasets that have a more balanced distribution of different body parts affected by ADEs and comparing the model's performance across these datasets would provide insights into the impact of dataset imbalance on ADE detection accuracy

## Limitations

- The MMADE dataset is relatively small (1,500 samples) and may not capture the full diversity of adverse drug events
- The study lacks detailed information about image quality, annotation reliability, and inter-rater agreement
- Evaluation focuses on generation quality metrics rather than clinical accuracy or downstream impact on patient outcomes

## Confidence

- **High Confidence**: The claim that fine-tuned InstructBLIP achieves superior performance on MMADE dataset compared to baseline models, supported by quantitative metrics (BLEU, ROUGE, BERTScore, MoverScore)
- **Medium Confidence**: The assertion that multimodal integration improves ADE detection accuracy over text-only methods, as this relies on dataset-specific observations without broader validation
- **Low Confidence**: The clinical utility and generalizability of the model, given the limited dataset size and lack of external validation

## Next Checks

1. **External Validation**: Test the fine-tuned model on an independent ADE dataset (e.g., MultiADE or ADEQA) to assess generalization beyond MMADE
2. **Bias Analysis**: Evaluate model performance across demographic subgroups (age, gender, ethnicity) to identify and mitigate potential biases in ADE detection
3. **Clinical Impact Study**: Collaborate with healthcare providers to assess whether the model's ADE descriptions improve clinical decision-making or patient outcomes in real-world settings