---
ver: rpa2
title: 'ITEM: Improving Training and Evaluation of Message-Passing based GNNs for
  top-k recommendation'
arxiv_id: '2407.07912'
source_url: https://arxiv.org/abs/2407.07912
tags:
- loss
- rank
- ndcg
- training
- item
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "ITEM introduces a smooth approximation of ranking metrics for\
  \ training GNNs in top-k recommendation, using sigmoid-based rank smoothing and\
  \ Personalized PageRank-based negative sampling. Evaluated across four datasets\
  \ (MovieLens-100K, MovieLens-1M, Yelp-2018, Amazon-book) with four GNN architectures,\
  \ ITEM outperforms the standard BPR loss by 10\u201355% relative improvements in\
  \ NDCG@20 and Recall@20."
---

# ITEM: Improving Training and Evaluation of Message-Passing based GNNs for top-k recommendation

## Quick Facts
- arXiv ID: 2407.07912
- Source URL: https://arxiv.org/abs/2407.07912
- Reference count: 25
- ITEM outperforms BPR loss by 10–55% in NDCG@20 and Recall@20 on top-k recommendation tasks

## Executive Summary
ITEM introduces a smooth approximation of ranking metrics for training GNNs in top-k recommendation, using sigmoid-based rank smoothing and Personalized PageRank-based negative sampling. Evaluated across four datasets (MovieLens-100K, MovieLens-1M, Yelp-2018, Amazon-book) with four GNN architectures, ITEM outperforms the standard BPR loss by 10–55% relative improvements in NDCG@20 and Recall@20. It also surpasses state-of-the-art methods including NeuralNDCG, MixGCF, and SGL-ED, and sets new inductive recommendation records. ITEM converges faster than BPR and MixGCF while delivering superior ranking quality, as validated by both quantitative metrics and qualitative top-k retrieval comparisons.

## Method Summary
ITEM improves GNN-based recommendation by introducing a smooth approximation of ranking metrics for training. The method employs a sigmoid-based rank smoothing technique to handle non-differentiable ranking metrics, enabling end-to-end optimization. Additionally, it utilizes Personalized PageRank-based negative sampling to generate more relevant and challenging negative examples during training. The approach is evaluated across multiple datasets and GNN architectures, demonstrating consistent improvements in ranking quality and faster convergence compared to standard BPR loss and other state-of-the-art methods.

## Key Results
- ITEM achieves 10–55% relative improvements in NDCG@20 and Recall@20 over BPR loss
- Surpasses state-of-the-art methods including NeuralNDCG, MixGCF, and SGL-ED
- Sets new inductive recommendation records while converging faster than BPR and MixGCF

## Why This Works (Mechanism)
ITEM's effectiveness stems from its ability to directly optimize ranking metrics through smooth approximations, rather than relying on pointwise or pairwise losses like BPR. The sigmoid-based rank smoothing transforms discrete ranking positions into differentiable scores, allowing gradient-based optimization of metrics like NDCG and Recall. This addresses the fundamental mismatch between traditional loss functions and evaluation metrics in recommendation systems. The Personalized PageRank-based negative sampling further enhances training by providing more challenging and relevant negative examples, leading to better generalization and ranking quality.

## Foundational Learning

**Personalized PageRank**: A random walk-based algorithm that computes the importance of nodes in a graph relative to a specific starting node. In ITEM, it's used to generate negative samples that are relevant to the user's context, improving the quality of training examples.

**Rank Smoothing**: The process of converting discrete ranking positions into smooth, differentiable scores using sigmoid functions. This allows the use of gradient-based optimization for ranking metrics that are otherwise non-differentiable, enabling end-to-end training of recommendation models.

**Inductive Recommendation**: A setting where the model must make recommendations for new users or items not seen during training. ITEM's approach generalizes well to unseen entities by leveraging the graph structure and learned representations.

## Architecture Onboarding

**Component Map**: User-Item Graph -> GNN Encoder -> Item Embeddings -> Sigmoid Rank Smoothing -> Ranking Loss -> Backpropagation

**Critical Path**: The critical path involves propagating information through the GNN encoder, applying the sigmoid rank smoothing to convert rankings to differentiable scores, and computing the ranking loss for backpropagation. The Personalized PageRank-based negative sampling is integrated into this path to enhance training quality.

**Design Tradeoffs**: ITEM trades increased computational complexity (due to rank smoothing and PPR-based sampling) for improved ranking quality and faster convergence. The smooth approximation of ranking metrics enables direct optimization but requires careful tuning of the smoothing factor.

**Failure Signatures**: Potential failures include suboptimal performance if the smoothing factor is poorly chosen, or if the PPR-based negative sampling fails to generate diverse enough examples. Over-smoothing in the GNN encoder could also degrade performance.

**First Experiments**:
1. Compare NDCG@20 and Recall@20 of ITEM against BPR on a small dataset (e.g., MovieLens-100K) with a simple GNN architecture.
2. Evaluate the impact of different smoothing factors on convergence speed and final ranking quality.
3. Assess the effectiveness of PPR-based negative sampling by comparing against uniform random negative sampling.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but potential areas for future research include extending ITEM to dynamic, temporal recommendation scenarios and exploring its performance in cold-start settings with limited user-item interactions.

## Limitations
- The rank smoothing technique adds computational overhead, and the trade-off between improved ranking quality and increased training time is not thoroughly discussed.
- Evaluation is primarily focused on static recommendation tasks; performance in dynamic, temporal scenarios remains unclear.
- The paper does not explore the impact of hyperparameter choices (e.g., smoothing factor, number of negative samples) on final performance.

## Confidence
- ITEM's improvements over BPR and other SOTA methods: High
- Faster convergence compared to BPR and MixGCF: High
- Effectiveness of PPR-based negative sampling: Medium (requires further validation)
- Generalization to dynamic, temporal recommendation: Low (not evaluated)

## Next Checks
1. Conduct experiments to quantify the computational overhead introduced by the rank smoothing technique and compare it against the performance gains.
2. Evaluate ITEM on dynamic, temporal recommendation datasets to assess its effectiveness in scenarios with changing user preferences and item popularity.
3. Perform a thorough hyperparameter sensitivity analysis to identify the optimal settings for different datasets and GNN architectures.