---
ver: rpa2
title: Can Input Attributions Explain Inductive Reasoning in In-Context Learning?
arxiv_id: '2412.15628'
source_url: https://arxiv.org/abs/2412.15628
tags:
- task
- input
- methods
- example
- attribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether conventional input attribution (IA)
  methods can identify the most influential in-context example in large language models'
  (LLMs) in-context learning (ICL). To address this, the authors design synthetic
  diagnostic tasks with ambiguous in-context examples and a single disambiguating
  "aha" example that determines the correct underlying rule.
---

# Can Input Attributions Explain Inductive Reasoning in In-Context Learning?

## Quick Facts
- arXiv ID: 2412.15628
- Source URL: https://arxiv.org/abs/2412.15628
- Reference count: 40
- Key outcome: Input erasure and gradient norm methods show promise for identifying influential in-context examples, but current interpretability tools have fundamental limitations in explaining ICL reasoning.

## Executive Summary
This study investigates whether conventional input attribution (IA) methods can identify the most influential in-context example in large language models' in-context learning (ICL). The authors design synthetic diagnostic tasks with ambiguous examples and a single disambiguating "aha" example to evaluate several IA methods including input erasure, gradient norm, input×gradient, and integrated gradients. Their findings reveal that simple IA methods can be effective in ICL settings, with input erasure frequently outperforming more sophisticated approaches. However, all methods exhibit limitations including positional bias and failure to handle model inductive biases, suggesting fundamental constraints on current interpretability tools for LLMs.

## Method Summary
The authors create synthetic diagnostic tasks where each task contains multiple ambiguous in-context examples plus one disambiguating "aha" example that determines the correct underlying rule. They evaluate several IA methods (input erasure, gradient norm, input×gradient, integrated gradients) along with baselines (attention weights, edit distance, post-hoc self-answer explanations) by measuring how well each method can identify the influential example. The evaluation uses metrics that compare attribution scores against ground truth influence, testing across different model sizes, context lengths, and task types to assess robustness and limitations of each approach.

## Key Results
- Input erasure frequently outperforms other interpretability methods in identifying influential examples
- Gradient norm (GN) often performs best among gradient-based methods, while integrated gradients shows no substantial improvement
- Larger models are generally harder to interpret with gradient-based IA methods
- IA methods maintain accuracy or improve with longer context, showing robustness in long-context scenarios

## Why This Works (Mechanism)
The study demonstrates that input attribution methods can capture some aspects of how LLMs identify influential examples during in-context learning, particularly when using simpler approaches like input erasure. The mechanism appears to work by measuring the marginal contribution of each example to the model's final prediction, though the effectiveness varies significantly based on model size and task complexity.

## Foundational Learning
- In-context learning (ICL): The ability of LLMs to learn from examples provided in the prompt without parameter updates
  - Why needed: Central phenomenon being studied - understanding how models reason from examples
  - Quick check: Verify that model performance improves when correct examples are provided in context

- Input attribution methods: Techniques that measure the contribution of input features to model predictions
  - Why needed: Primary tools for identifying which in-context examples influence model behavior
  - Quick check: Compare attribution scores against ground truth influence

- Gradient-based attribution: Methods using gradients to measure sensitivity of output to input changes
  - Why needed: Common approach for understanding model decision-making
  - Quick check: Verify gradient calculations are correctly implemented and normalized

## Architecture Onboarding

**Component Map:** Input examples -> Model processing -> Attribution calculation -> Influence ranking

**Critical Path:** Task construction → Model inference → Attribution computation → Performance evaluation

**Design Tradeoffs:** Simpler methods (input erasure) vs. sophisticated methods (integrated gradients); model size vs. interpretability; synthetic tasks vs. real-world complexity

**Failure Signatures:** Positional bias in attributions; inability to overcome model inductive biases; gradient-based methods failing on larger models

**3 First Experiments:**
1. Compare input erasure vs. gradient norm on binary classification tasks with clear "aha" examples
2. Test attribution methods across model sizes (small vs. large) on identical tasks
3. Evaluate robustness by varying context length while measuring attribution accuracy

## Open Questions the Paper Calls Out
The paper suggests several open questions regarding the fundamental limitations of input attribution methods in explaining inductive reasoning, particularly around positional bias and model inductive biases that current methods cannot overcome.

## Limitations
- All methods exhibit positional bias, favoring examples based on their position rather than true influence
- Methods fail to account for model inductive biases that affect reasoning patterns
- Synthetic binary tasks may oversimplify real-world ICL scenarios with more ambiguous multi-faceted problems

## Confidence
- **High confidence**: Input erasure and gradient norm methods perform better than other IA approaches in synthetic ICL tasks
- **Medium confidence**: Larger models are harder to interpret with gradient-based IA methods
- **Medium confidence**: IA methods show robustness to context length increases
- **Low confidence**: Post-hoc self-answer explanations are more reliable for larger models (based on limited comparisons)

## Next Checks
1. Test IA methods on real-world ICL tasks with naturally occurring ambiguous examples rather than synthetic binary tasks
2. Evaluate whether combining multiple IA methods or using ensemble approaches improves identification of influential examples
3. Investigate whether fine-tuning IA methods specifically for ICL scenarios (rather than using generic implementations) yields better performance