---
ver: rpa2
title: 'Key Design Choices in Source-Free Unsupervised Domain Adaptation: An In-depth
  Empirical Analysis'
arxiv_id: '2402.16090'
source_url: https://arxiv.org/abs/2402.16090
tags:
- domain
- sf-uda
- adaptation
- methods
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive empirical analysis of source-free
  unsupervised domain adaptation (SF-UDA) methods in image classification. The study
  examines a diverse set of SF-UDA techniques, assessing their consistency across
  datasets, sensitivity to hyperparameters, and applicability across different backbone
  architectures.
---

# Key Design Choices in Source-Free Unsupervised Domain Adaptation: An In-depth Empirical Analysis

## Quick Facts
- arXiv ID: 2402.16090
- Source URL: https://arxiv.org/abs/2402.16090
- Reference count: 15
- Primary result: SF-UDA performance highly depends on backbone architecture and pre-training dataset choice, with LayerNorm outperforming BatchNorm

## Executive Summary
This paper presents a comprehensive empirical analysis of source-free unsupervised domain adaptation (SF-UDA) methods for image classification. The study evaluates various SF-UDA techniques across diverse datasets, architectures, and pre-training strategies to identify key factors affecting performance. By systematically examining hundreds of architectures and multiple pre-training approaches, the research reveals that backbone architecture selection and pre-training dataset choice are critical determinants of SF-UDA success. The findings highlight the importance of normalization layer choice, with Layer Normalization consistently outperforming Batch Normalization, and demonstrate that fine-tuning on source data can sometimes degrade performance for certain architectures.

## Method Summary
The study employs a three-phase experimental pipeline: pre-training on large datasets (ImageNet, ImageNet21k, or self-supervised methods), optional fine-tuning on the source domain, and adaptation to the target domain using SF-UDA methods. The evaluation uses 7 domain adaptation datasets and tests 5 SF-UDA methods (SCA, SHOT, NRC, AAD, PCSR) across 500+ architectures. The analysis compares performance against lower bounds (LP-ODG, FT-ODG) and upper bounds (LP-IDG, FT-IDG) while examining the impact of batch size, GPU count, and hyperparameter sensitivity. The framework emphasizes reproducibility through distributed training and multi-seed experiments.

## Key Results
- High correlation exists between ImageNet top-1 accuracy, out-of-distribution generalization, and SF-UDA performance across hundreds of architectures
- Self-supervised pre-training methods can be effective but generally underperform supervised pre-training for SF-UDA
- Fine-tuning on source domain can lead to severe performance degradation for architectures with Batch Normalization
- Architectures with Layer Normalization consistently outperform those with Batch Normalization for SF-UDA

## Why This Works (Mechanism)

### Mechanism 1
Backbone architecture and pre-training dataset choice significantly influence final SF-UDA performance. Higher ImageNet top-1 accuracy correlates with better out-of-distribution generalization and SF-UDA accuracy; ImageNet21k pre-training provides additional gains beyond ImageNet1k. The pre-training dataset and architecture shape the feature space in a way that generalizes across domains. This correlation may break under extreme domain shifts or when target domains are highly dissimilar to ImageNet.

### Mechanism 2
Fine-tuning on source domain can severely degrade SF-UDA performance for architectures with Batch Normalization. Batch Normalization layers accumulate statistics from the source domain during fine-tuning, which may become misaligned when adapting to the target domain, causing failures. The statistics in Batch Normalization layers are domain-specific and their mismatch causes instability. This failure may be mitigated if Batch Normalization statistics are reset or re-estimated during adaptation.

### Mechanism 3
Layer Normalization-based architectures consistently outperform Batch Normalization-based ones in SF-UDA. Layer Normalization is less sensitive to domain shifts because it normalizes across the channel dimension within each sample, making it more robust to changes in input distribution. Layer Normalization's per-sample normalization provides better stability under domain shift than Batch Normalization's batch-level statistics. The difference may be negligible if the target domain is very similar to the source domain.

## Foundational Learning

- Concept: Domain adaptation and transfer learning
  - Why needed here: SF-UDA builds on transfer learning by adapting models from a source to a target domain without source data access
  - Quick check question: What is the key difference between traditional UDA and SF-UDA?

- Concept: Normalization layers (BatchNorm vs LayerNorm)
  - Why needed here: The choice of normalization layer affects model stability and performance under domain shift
  - Quick check question: How do BatchNorm and LayerNorm differ in their normalization strategy?

- Concept: Self-supervised learning pre-training
  - Why needed here: The paper evaluates self-supervised pre-training methods and their effectiveness compared to supervised pre-training for SF-UDA
  - Quick check question: What is the main advantage of self-supervised pre-training over supervised pre-training?

## Architecture Onboarding

- Component map: Pre-training -> Fine-tuning (source) -> Adaptation (target)
- Critical path: Backbone selection -> Pre-training dataset choice -> Fine-tuning decision -> SF-UDA method selection
- Design tradeoffs: BatchNorm offers faster convergence but may fail under domain shift; LayerNorm is more robust but potentially slower; self-supervised pre-training saves labeling cost but may underperform supervised
- Failure signatures: Sudden accuracy drop after fine-tuning (BatchNorm issue); method fails to improve over lower bound (hyperparameter or method choice issue); high variance across runs (random seed or data split sensitivity)
- First 3 experiments:
  1. Compare LP-ODG vs FT-ODG for a BatchNorm and a LayerNorm backbone to observe fine-tuning impact
  2. Test SCA on ResNet50 vs ViT-Large to see normalization layer effect
  3. Evaluate self-supervised pre-training (e.g., DINO) vs supervised pre-training for FT-SHOT

## Open Questions the Paper Calls Out

### Open Question 1
Does the performance gap between self-supervised and supervised pre-training methods for SF-UDA close as model sizes increase? The paper notes that while self-supervised methods like DINO v2 show competitive results, supervised pre-training generally outperforms them. It also observes that larger models benefit more from ImageNet21k pre-training, suggesting a potential trend. This remains unresolved as the study focused on ResNet50 and ViT-Base architectures. A large-scale empirical study evaluating various self-supervised methods across a diverse set of architectures and pre-training datasets would resolve this.

### Open Question 2
How can we effectively select hyperparameters for SF-UDA methods without access to target labels and without risking data leakage? The paper highlights the challenge of hyperparameter selection in SF-UDA due to the absence of target labels during adaptation. It mentions potential solutions like unsupervised hyperparameter selection techniques but notes their effectiveness needs further study. Current approaches either rely on heuristics, target accuracy (risking data leakage), or require additional labeled data, which contradicts the source-free setting. Development and evaluation of novel, principled methods for unsupervised hyperparameter selection specifically designed for SF-UDA would resolve this.

### Open Question 3
What architectural modifications or normalization strategies can improve the robustness of SF-UDA methods, particularly for Batch Normalization-based models? The paper demonstrates that architectures with Layer Normalization consistently outperform those with Batch Normalization in SF-UDA. It also shows that Batch Normalization can lead to severe performance degradation in some scenarios. While the paper identifies the problem, it does not provide a comprehensive solution or explore alternative normalization strategies beyond Layer Normalization. An extensive study comparing different normalization techniques and architectural modifications for SF-UDA would resolve this.

## Limitations
- Correlation between ImageNet accuracy and SF-UDA performance may not generalize to domains with minimal visual overlap with ImageNet
- Batch Normalization failure mechanism is observed empirically but lacks theoretical grounding in the SF-UDA context
- The analysis doesn't explore whether Layer Normalization advantage stems from normalization mechanics or other architectural differences

## Confidence

- High confidence in empirical observations about Batch Normalization degradation and Layer Normalization advantages
- Medium confidence in the ImageNet accuracy correlation mechanism (strong empirical support but limited theoretical explanation)
- Low confidence in the relative effectiveness of self-supervised vs supervised pre-training (results show mixed performance across methods)

## Next Checks

1. Test the ImageNet accuracy correlation on a domain with minimal ImageNet overlap (e.g., medical imaging) to validate cross-domain generalization
2. Implement Batch Normalization variants that reset statistics during adaptation to isolate the normalization mechanism as the failure cause
3. Compare self-supervised pre-training methods with varying degrees of ImageNet similarity to their target domains to identify when they outperform supervised approaches