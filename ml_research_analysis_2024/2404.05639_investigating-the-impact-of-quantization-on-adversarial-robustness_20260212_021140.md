---
ver: rpa2
title: Investigating the Impact of Quantization on Adversarial Robustness
arxiv_id: '2404.05639'
source_url: https://arxiv.org/abs/2404.05639
tags:
- robustness
- quantization
- adversarial
- training
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how quantization affects adversarial robustness
  in deep neural networks. The authors find that previous studies reached inconsistent
  conclusions due to different quantization pipeline settings, specifically regarding
  whether robust optimization is performed and at which quantization stage it occurs.
---

# Investigating the Impact of Quantization on Adversarial Robustness

## Quick Facts
- arXiv ID: 2404.05639
- Source URL: https://arxiv.org/abs/2404.05639
- Authors: Qun Li; Yuan Meng; Chen Tang; Jiacheng Jiang; Zhi Wang
- Reference count: 10
- This paper investigates how quantization affects adversarial robustness in deep neural networks, finding that inconsistent conclusions in prior work stem from different quantization pipeline settings.

## Executive Summary
This paper systematically investigates how different quantization pipelines affect adversarial robustness in deep neural networks. The authors demonstrate that previous contradictory findings about quantization's impact on robustness arise from varying pipeline settings, particularly regarding whether robust optimization is performed and at which quantization stage it occurs. Through comprehensive experiments, they show that quantization without robustness components can resist weaker attacks but fails against normal attacks, while adding robustness components can achieve similar robustness to full-precision models. The study provides clear guidance for choosing appropriate quantization pipelines when both efficiency and security are required.

## Method Summary
The authors investigate quantization's impact on adversarial robustness by comparing different quantization pipelines on CIFAR-10 with ResNet20 architecture. They implement PWLQ for Post-Training Quantization (PTQ) and PACT for Quantization-Aware Training (QAT), evaluating against PGD and AutoAttack adversarial methods. The experiments systematically vary initialization parameters (random, non-robust, robust) and training strategies (vanilla, adversarial training, adversarial transfer learning) to isolate the effects of quantization and robustness components on model performance.

## Key Results
- Quantization without robustness components can resist weaker attacks (ϵ ≤ 2/255) but fails against normal attacks (ϵ ≤ 8/255)
- When robustness components are added, quantized models can achieve similar robustness to full-precision models, though lower bit-width may reduce robustness
- Robust transfer learning from pre-trained robust models can achieve higher robustness in quantized models compared to full-precision robust models at higher bit-widths

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quantization without robustness components can resist weaker adversarial attacks but fails against normal attacks.
- Mechanism: Lower bit-width quantization introduces noise that acts as a form of regularization, making the model less sensitive to small perturbations. This effect is sufficient for weaker attacks (ϵ ≤ 2/255) but not for stronger attacks (ϵ ≤ 8/255).
- Core assumption: The noise introduced by quantization is sufficient to disrupt weaker adversarial perturbations but not strong enough to withstand normal attack intensities.
- Evidence anchors:
  - [abstract]: "quantization without robustness components can resist weaker attacks but fails against normal attacks"
  - [section]: "we observe that in quantized networks without any robustness components, they are unable to resist attacks with common attack settings (ϵ = 8/255)"
  - [corpus]: Weak evidence - corpus neighbors discuss quantization robustness but not specifically this mechanism.

### Mechanism 2
- Claim: Adding adversarial training to quantization can maintain or improve robustness compared to full-precision models.
- Mechanism: When adversarial training is incorporated during quantization-aware training (QAT), the model learns to be robust to adversarial examples while also adapting to the quantization process. This dual learning process preserves robustness.
- Core assumption: The adversarial training process can effectively transfer to the quantized model, maintaining the robustness properties learned in full-precision.
- Evidence anchors:
  - [abstract]: "When robustness components are added, quantized models can achieve similar robustness to full-precision models"
  - [section]: "When adversarial training is added to the training process of QAT, it can maintain the robustness provided by the initialization"
  - [corpus]: Weak evidence - corpus neighbors discuss quantization robustness but not specifically this mechanism.

### Mechanism 3
- Claim: Robust transfer learning from a pre-trained robust model can achieve higher robustness in quantized models compared to full-precision robust models.
- Mechanism: Knowledge distillation from a robust full-precision model to a quantized student model allows the quantized model to inherit both accuracy and robustness properties. The loss function used in transfer learning specifically targets both aspects.
- Core assumption: The teacher model's robustness can be effectively transferred to the student quantized model through the specified loss function.
- Evidence anchors:
  - [abstract]: "Adding adversarial training to quantization can gain robustness"
  - [section]: "It can be observed that this method still performs well when transferred to quantized models. At higher quantization bit-widths, it can even surpass the robustness of full-precision models"
  - [corpus]: Weak evidence - corpus neighbors discuss quantization robustness but not specifically this mechanism.

## Foundational Learning

- Concept: Adversarial robustness in deep learning
  - Why needed here: Understanding how adversarial attacks work and how to defend against them is crucial for interpreting the results of quantization on robustness
  - Quick check question: What is the difference between white-box and black-box adversarial attacks?

- Concept: Quantization techniques (PTQ and QAT)
  - Why needed here: The paper compares different quantization methods and their impact on robustness, so understanding these techniques is essential
  - Quick check question: What is the main difference between Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT)?

- Concept: Knowledge distillation in transfer learning
  - Why needed here: The paper discusses robust transfer learning, which uses knowledge distillation from a robust full-precision model to a quantized model
  - Quick check question: How does knowledge distillation work in the context of transferring robustness from a teacher model to a student model?

## Architecture Onboarding

- Component map:
  - Data preparation: CIFAR-10 dataset, ResNet20 architecture
  - Quantization methods: PWLQ (PTQ) and PACT (QAT)
  - Robustness evaluation: PGD attacks (white-box) and AutoAttack (black-box)
  - Transfer learning: Robust transfer learning with knowledge distillation

- Critical path:
  CIFAR-10 dataset and ResNet20 -> Apply quantization (PTQ or QAT) with/without robustness -> Evaluate against adversarial attacks -> Analyze results and compare pipelines

- Design tradeoffs:
  - Accuracy vs. robustness: Improving robustness may decrease accuracy, and vice versa
  - Bit-width vs. robustness: Lower bit-width may increase robustness against weaker attacks but decrease it against normal attacks
  - Time overhead: Adding adversarial training increases training time significantly

- Failure signatures:
  - Model converges to poor accuracy: Check initialization parameters and dataset size
  - Robustness doesn't improve with adversarial training: Verify that adversarial samples are being generated correctly
  - Transfer learning fails: Ensure teacher model is robust and student model is properly initialized

- First 3 experiments:
  1. Compare PTQ and QAT without any robustness components on CIFAR-10 with ResNet20
  2. Add adversarial training to QAT and compare robustness against normal attacks
  3. Implement robust transfer learning and evaluate its effectiveness compared to direct adversarial training

## Open Questions the Paper Calls Out
- How do quantization pipelines that incorporate robust optimization at different stages affect adversarial robustness under varying attack intensities?
- What is the optimal bit-width for quantized models that balances adversarial robustness, accuracy, and computational efficiency in resource-constrained scenarios?
- How does the size of the calibration/fine-tuning dataset affect the adversarial robustness of quantized models across different quantization pipelines?

## Limitations
- The study focuses primarily on CIFAR-10 with ResNet20, limiting generalizability to larger-scale models and datasets
- The experimental validation does not systematically characterize which specific pipeline combinations produce optimal robustness-efficiency tradeoffs across different threat models
- Findings may be method-specific as only PWLQ and PACT quantization methods were tested

## Confidence
- **High confidence**: Claims about PTQ/QAT pipeline inconsistencies explaining prior contradictory results, and the basic finding that quantization without robustness components fails against normal attacks
- **Medium confidence**: Claims about robust transfer learning potentially surpassing full-precision robustness at higher bit-widths, due to limited ablation studies
- **Low confidence**: Claims about specific robustness-efficiency sweet spots without broader validation across architectures and datasets

## Next Checks
1. Replicate experiments on ImageNet-scale datasets to verify scalability of findings
2. Systematically vary attack strengths (beyond the two fixed levels) to map the full robustness spectrum of quantized models
3. Test additional quantization methods (beyond PWLQ and PACT) to confirm findings aren't method-specific artifacts