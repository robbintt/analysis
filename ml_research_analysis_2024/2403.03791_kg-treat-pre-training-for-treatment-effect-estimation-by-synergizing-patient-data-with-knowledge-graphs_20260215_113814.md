---
ver: rpa2
title: 'KG-TREAT: Pre-training for Treatment Effect Estimation by Synergizing Patient
  Data with Knowledge Graphs'
arxiv_id: '2403.03791'
source_url: https://arxiv.org/abs/2403.03791
tags:
- data
- patient
- treatment
- pre-training
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: KG-TREAT is a pre-training and fine-tuning framework for treatment
  effect estimation (TEE) that synergizes large-scale patient data with biomedical
  knowledge graphs (KGs). It constructs dual-focus personalized KGs to capture treatment-covariate
  and outcome-covariate relationships, and introduces a deep bi-level attention synergy
  method (DIVE) for effective information fusion.
---

# KG-TREAT: Pre-training for Treatment Effect Estimation by Synergizing Patient Data with Knowledge Graphs

## Quick Facts
- arXiv ID: 2403.03791
- Source URL: https://arxiv.org/abs/2403.03791
- Authors: Ruoqi Liu; Lingfei Wu; Ping Zhang
- Reference count: 33
- Primary result: KG-TREAT outperforms state-of-the-art methods with 7% average AUC improvement and 9% average IF-PEHE improvement on TEE tasks

## Executive Summary
KG-TREAT is a pre-training and fine-tuning framework for treatment effect estimation that synergizes large-scale patient data with biomedical knowledge graphs. It constructs dual-focus personalized KGs to capture treatment-covariate and outcome-covariate relationships separately, reducing bias in TEE. The model uses a deep bi-level attention synergy method (DIVE) for effective information fusion between patient data and KGs. Pre-trained on 3M patient records and UMLS, KG-TREAT is fine-tuned on four CAD-related TEE tasks and achieves state-of-the-art performance.

## Method Summary
KG-TREAT constructs dual-focus personalized knowledge graphs (PKGs) capturing treatment-covariate and outcome-covariate relationships separately. It uses a BERT-base patient sequence encoder combined with a deep bi-level attention synergy method (DIVE) that first applies treatment/outcome attention to encode relationships, then uses multi-head co-attention to synergize patient data and PKG representations. The model is pre-trained on 3M patient records using masked code prediction and link prediction tasks, then fine-tuned for treatment effect estimation on four downstream datasets with shared representations for treatment and outcome prediction.

## Key Results
- 7% average AUC improvement over state-of-the-art methods on factual outcome prediction
- 9% average IF-PEHE improvement on counterfactual prediction accuracy
- Demonstrates strong performance across four CAD-related RCT-derived datasets comparing drug pairs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual-focus PKGs capture treatment-covariate and outcome-covariate relationships separately to reduce bias.
- Mechanism: By constructing one PKG for treatment-covariate relationships and another for outcome-covariate relationships, the model can explicitly encode distinct causal dependencies and reduce spurious correlations.
- Core assumption: Treatment-covariate and outcome-covariate relationships are meaningfully different and should be modeled separately.
- Evidence anchors:
  - [abstract] "KG-TREAT constructs dual-focus KGs and integrates a deep bi-level attention synergy method for in-depth information fusion, enabling distinct encoding of treatment-covariate and outcome-covariate relationships."
  - [section] "we propose to construct dual-focus PKGs that capture diverse medical contexts of treatment-covariate and outcome-covariate relationship, respectively."
- Break condition: If treatment and outcome effects are not separable or if confounding is symmetric across both relationships, dual-focus construction may not provide additional benefit.

### Mechanism 2
- Claim: Deep bi-level attention synergy (DIVE) enables effective fusion of patient data and PKGs.
- Mechanism: First level applies treatment/outcome attention to encode relationships among covariates, treatments, and outcomes. Second level uses multi-head co-attention to deeply synergize patient data and PKG representations across multiple layers.
- Core assumption: Deep information fusion across modalities improves representation quality compared to shallow concatenation.
- Evidence anchors:
  - [abstract] "KG-TREAT constructs dual-focus KGs and integrates a deep bi-level attention synergy method for in-depth information fusion"
  - [section] "The first level of attention applies a treatment(outcome) attention mechanism to patient data and specific treatment(outcome) information from PKGs, explicitly encoding the complex relationships among covariates, treatments, and outcomes."
- Break condition: If the attention mechanisms do not learn meaningful alignments between patient data and KG nodes, or if the model overfits to noise in the KGs.

### Mechanism 3
- Claim: Pre-training on large-scale patient data with KGs provides better generalization than task-specific training.
- Mechanism: Masked code prediction and link prediction tasks force the model to learn contextualized representations of medical codes and their relationships before fine-tuning on specific TEE tasks.
- Core assumption: Learning general medical knowledge from large-scale data transfers to specific TEE tasks.
- Evidence anchors:
  - [abstract] "KG-TREAT also incorporates two pre-training tasks to ensure a thorough grounding and contextualization of patient data and KGs."
  - [section] "Pre-training has a more significant impact on model performance than KGs, as indicated by the greater performance decline in the w/o pre-train scenario compared to the w/o KGs scenario."
- Break condition: If the pre-training tasks do not align with TEE objectives or if the pre-training data is too different from downstream tasks.

## Foundational Learning

- Concept: Causal inference assumptions (consistency, positivity, ignorability)
  - Why needed here: TEE relies on these assumptions to identify causal effects from observational data
  - Quick check question: What would happen if positivity assumption is violated in this patient population?

- Concept: Knowledge graph representation learning
  - Why needed here: Understanding how KG embeddings are constructed and used for downstream tasks
  - Quick check question: How does the link prediction task in pre-training help the model learn better KG representations?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The model uses multi-head self-attention and co-attention for both patient data and KG processing
  - Quick check question: Why might multi-head co-attention be more effective than simple concatenation for combining patient and KG representations?

## Architecture Onboarding

- Component map: Patient sequence encoder (BERT-base) → Deep bi-level attention synergy (DIVE) → Graph encoders (GNNs) for treatment-covariate and outcome-covariate PKGs → Prediction heads for treatment and outcome
- Critical path: Patient data → Patient sequence encoder → Treatment/Outcome attention → Co-attention with PKGs → Prediction heads
- Design tradeoffs: Dual PKGs add complexity but reduce bias vs. single PKG approach; pre-training requires large data but improves generalization
- Failure signatures: Poor treatment/outcome attention weights suggest misalignment; low co-attention scores indicate ineffective KG integration
- First 3 experiments:
  1. Ablation: Remove dual PKG construction and use single PKG - expect increased bias and reduced performance
  2. Ablation: Replace DIVE with simple concatenation - expect loss of nuanced relationship encoding
  3. Ablation: Remove pre-training - expect significant performance drop, especially on limited fine-tuning data

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the methods and results, several important questions arise:

### Open Question 1
- Question: How does the model perform on treatment effect estimation tasks beyond CAD, such as diabetes or cancer?
- Basis in paper: [inferred] The paper only evaluates KG-TREAT on four downstream datasets related to CAD. There is no mention of testing the model on other diseases.
- Why unresolved: The authors did not conduct experiments on other diseases, so the generalizability of KG-TREAT to different treatment effect estimation tasks remains unknown.
- What evidence would resolve it: Evaluating KG-TREAT on a diverse set of treatment effect estimation tasks for different diseases would provide insights into its generalizability.

### Open Question 2
- Question: How sensitive is KG-TREAT to the choice of biomedical knowledge graph used for pre-training?
- Basis in paper: [inferred] The paper uses UMLS as the biomedical knowledge graph for pre-training KG-TREAT. However, it is unclear how the model would perform if a different knowledge graph, such as MeSH or SNOMED CT, was used.
- Why unresolved: The authors did not experiment with different knowledge graphs, so the impact of the choice of knowledge graph on KG-TREAT's performance is unknown.
- What evidence would resolve it: Comparing KG-TREAT's performance when pre-trained on different biomedical knowledge graphs would reveal the sensitivity of the model to the choice of knowledge graph.

### Open Question 3
- Question: How does KG-TREAT handle missing or incomplete patient data during fine-tuning?
- Basis in paper: [inferred] The paper does not discuss how KG-TREAT handles missing or incomplete patient data during the fine-tuning process. This is an important consideration, as real-world patient data often contains missing values or incomplete records.
- Why unresolved: The authors did not address this issue in the paper, so the robustness of KG-TREAT to missing or incomplete data is unknown.
- What evidence would resolve it: Investigating KG-TREAT's performance when fine-tuned on patient data with varying levels of missingness or incompleteness would provide insights into its ability to handle such scenarios.

## Limitations
- Limited evaluation to CAD-related datasets, leaving generalizability to other diseases unclear
- Lack of real-world clinical validation beyond RCT-derived datasets
- No discussion of how the model handles violations of causal inference assumptions in observational data

## Confidence

- **High**: The dual PKG construction mechanism and its theoretical justification for reducing bias; the general architecture of using attention mechanisms for modality fusion.
- **Medium**: The specific implementation details of the DIVE method and its superiority over simpler fusion approaches; the transferability of pre-training benefits across different TEE tasks.
- **Low**: Real-world clinical validation beyond the RCT-derived datasets; performance under violations of causal inference assumptions like positivity and ignorability.

## Next Checks
1. Conduct an ablation study removing the dual PKG construction to quantify the specific contribution of separating treatment-covariate and outcome-covariate relationships.
2. Implement and compare against a simpler fusion method (e.g., concatenation with attention) to validate the necessity of the complex DIVE architecture.
3. Test the pre-trained model on a completely different TEE task domain (e.g., oncology) to assess the generalizability of the pre-training benefits.