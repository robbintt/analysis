---
ver: rpa2
title: Automatically Learning HTN Methods from Landmarks
arxiv_id: '2404.06325'
source_url: https://arxiv.org/abs/2404.06325
tags:
- methods
- learning
- planning
- landmarks
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CURRICULAMA is an HTN method learning algorithm that uses landmark
  analysis and curriculum learning to completely automate the learning process without
  requiring manual annotation. The algorithm generates curricula from landmarks and
  learns HTN methods by analyzing simpler to more complex subtraces of plan traces.
---

# Automatically Learning HTN Methods from Landmarks

## Quick Facts
- arXiv ID: 2404.06325
- Source URL: https://arxiv.org/abs/2404.06325
- Reference count: 18
- Key outcome: CURRICULAMA eliminates the need for expert-provided annotated tasks while maintaining learning effectiveness comparable to HTN-MAKER

## Executive Summary
CURRICULAMA is an HTN method learning algorithm that completely automates the learning process by using landmark analysis and curriculum learning to generate annotated tasks without manual annotation. The algorithm extracts landmarks from classical planning problems, creates a curriculum that progressively teaches simpler to more complex subtasks, and learns HTN methods that can solve equivalent hierarchical planning problems. Experimental results on five domains show CURRICULAMA achieves similar convergence rates, plan lengths, and planning times compared to HTN-MAKER while eliminating the need for human-provided task annotations.

## Method Summary
CURRICULAMA automates HTN method learning by generating curricula from landmarks and using curriculum learning to order method acquisition. The algorithm extracts landmarks from classical planning problems, creates a landmark graph with orderings, and generates curriculum steps that trace backward from each landmark to the start of the plan. CURRICU_LEARN then incrementally learns HTN methods from these curricula using a goal regression procedure, ensuring that learned methods can solve equivalent HTN planning problems. The approach eliminates manual annotation requirements while maintaining comparable learning effectiveness to HTN-MAKER.

## Key Results
- CURRICULAMA achieves similar convergence rates to HTN-MAKER in learning complete sets of methods
- Learned methods produce comparable plan lengths and planning times to HTN-MAKER across five domains
- The algorithm eliminates the need for expert-provided annotated tasks while maintaining learning effectiveness
- CURRICULAMA can learn methods that solve equivalent HTN planning problems, as proven by theoretical soundness analysis

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: CURRICULAMA uses landmark analysis to automatically generate curricula without manual annotation, which eliminates the need for human-provided task annotations required by HTN-MAKER.
- **Mechanism**: The algorithm extracts landmarks from classical planning problems, generates a landmark graph with orderings, and creates curriculum steps that progressively trace backward from each landmark to the start of the plan. This allows the learner to focus on simpler subtasks first, building up to more complex ones.
- **Core assumption**: Landmarks provide a natural structure for ordering learning tasks, and solving landmark-achieving subtasks provides a foundation for solving the overall problem.
- **Evidence anchors**:
  - [abstract] "uses landmark analysis to compose annotated tasks and leverages curriculum learning to order the learning of methods from simpler to more complex"
  - [section] "CURRICU GEN generates curricula from landmarks while CURRICU LEARN learns HTN methods from the curricula"
- **Break condition**: If landmark orderings are incomplete or incorrect, the generated curriculum may teach suboptimal methods, as shown in the Logistics domain example where unnecessary airplane movements occurred.

### Mechanism 2
- **Claim**: The learned methods can solve equivalent HTN planning problems, proving the soundness of the approach.
- **Mechanism**: The algorithm proves that methods learned from classical planning problems enable an HTN planner to solve equivalent hierarchical problems by showing that the solution trace π is a valid solution to the hierarchical problem Ph when methods are applied.
- **Core assumption**: The goal regression procedure used in learning methods guarantees that when preconditions are satisfied, the subtasks can be reduced using other learned methods.
- **Evidence anchors**:
  - [abstract] "We prove CURRICULAMA's soundness, and show experimentally that it has a substantially similar convergence rate in learning a complete set of methods to HTN-MAKER"
  - [section] "Proposition 1. Given P = (Σ, s0, g), π, τ, M, and Ph = ((Σ, M), s0, ⟨τ ⟩), π is a solution to Ph as a result of hierarchically decomposing g using the methods in M"
- **Break condition**: If the goal regression procedure fails to properly account for all dependencies, the learned methods may not be applicable to all initial states.

### Mechanism 3
- **Claim**: CURRICULAMA achieves comparable convergence rates and plan quality to HTN-MAKER while eliminating manual annotation requirements.
- **Mechanism**: The algorithm learns methods incrementally from training problems, and experimental results show similar fractions of problems solved, average plan lengths, and planning times compared to HTN-MAKER across five domains.
- **Core assumption**: The landmark-based curriculum provides sufficient guidance for learning effective methods without requiring expert annotations.
- **Evidence anchors**:
  - [abstract] "Experimental results on five domains... show that CURRICULAMA has a similar convergence rate to HTN-MAKER in learning complete sets of methods, with comparable plan lengths and planning times"
  - [section] "Figure 4 shows that CURRICULAMA's method learning exhibits a similar convergence rate and results in plan lengths and planning time comparable to HTN-MAKER"
- **Break condition**: If the domain contains complex dependencies not captured by landmarks, the algorithm may learn extraneous methods that don't improve planning performance.

## Foundational Learning

- **Concept: Hierarchical Task Network planning**
  - Why needed here: The entire algorithm operates within the HTN framework, where methods decompose tasks into subtasks
  - Quick check question: What is the difference between a primitive task and a compound task in HTN planning?

- **Concept: Curriculum learning**
  - Why needed here: The algorithm uses curriculum learning to order method learning from simpler to more complex tasks
  - Quick check question: How does curriculum learning improve learning performance compared to random task ordering?

- **Concept: Landmarks in planning**
  - Why needed here: Landmarks are the foundation for automatically generating curricula without manual annotation
  - Quick check question: What is the relationship between landmarks and solution plans in classical planning?

## Architecture Onboarding

- **Component map**: Problem → Landmark extraction → Curriculum generation → Method learning → HTN planning
- **Critical path**: Classical planning problem → Landmark graph generation → Curriculum creation → HTN method learning → HTN planning with learned methods
- **Design tradeoffs**:
  - Automatic vs. manual annotation: Eliminates human effort but may learn suboptimal methods
  - Landmark ordering: Uses reasonable orders but must enforce total ordering, potentially missing optimal strategies
  - Method reuse: Indexes methods by subtrace indices for reuse, but may learn redundant methods
- **Failure signatures**:
  - Learning extraneous methods (seen in Logistics, Satellite, Rover domains)
  - Non-convergence in method learning (observed in Blocks World, Logistics, Rover)
  - Suboptimal landmark orderings leading to inefficient plans
- **First 3 experiments**:
  1. Run CURRICU_GEN on a simple Blocks World problem with 3 blocks to verify landmark extraction and curriculum generation
  2. Test CURRICU_LEARN on the generated curriculum to see if it learns the expected Make-Clear methods
  3. Compare planning success rates of learned methods vs. ground truth methods on a small test set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does CURRICULAMA's suboptimal landmark ordering lead to overfitting or just inefficiency in method learning?
- Basis in paper: [inferred] The paper mentions that suboptimal landmark orderings can lead to learning extraneous methods, but notes this may be an indicator of overfitting or just suboptimal paths. It states "While it's possible that this may be an indication of overfitting, we believe this is more likely a result of partial orders in the landmark graph."
- Why unresolved: The paper does not experimentally test whether the additional methods learned due to suboptimal landmark orderings lead to overfitting or just inefficiency. It only hypothesizes that suboptimal paths are more likely the cause.
- What evidence would resolve it: Running experiments to measure CURRICULAMA's performance on out-of-distribution test problems compared to in-distribution test problems would indicate if overfitting is occurring. If performance degrades significantly on out-of-distribution problems, that would suggest overfitting. If performance remains consistent, it would support the suboptimal paths hypothesis.

### Open Question 2
- Question: How does the quality of automatically generated annotated tasks compare to manually annotated tasks when used with HTN-MAKER?
- Basis in paper: [explicit] The paper states "We are also interested in an empirical study that compares manually annotated task and automatically annotated tasks when directly applied to HTN-MAKER without any curricula." This is listed as future work.
- Why unresolved: The paper proposes an automated method for generating annotated tasks from landmarks, but does not empirically compare the quality of these automatically generated tasks to manually annotated tasks.
- What evidence would resolve it: Running experiments where both manually annotated and automatically generated tasks are used with HTN-MAKER on the same set of problems, and comparing the convergence rates, plan lengths, and planning times. This would provide a direct comparison of the quality of the two types of annotated tasks.

### Open Question 3
- Question: What is the time complexity of CURRICULAMA as a function of task domain problem or solution complexity?
- Basis in paper: [explicit] The paper states "We will theoretically and empirically analyse CURRICULAMA's time complexity as some measure of task domain problem or solution complexity increases." This is listed as future work.
- Why unresolved: While the paper provides some empirical running time data for CURRICULAMA, it does not provide a theoretical analysis of its time complexity or how it scales with problem complexity.
- What evidence would resolve it: Conducting a theoretical analysis to derive the time complexity of CURRICULAMA in terms of the number of landmarks, the length of the solution plan, and the number of curriculum steps. Additionally, running experiments to empirically measure how the running time scales with increasing problem complexity in terms of the number of objects, predicates, and goals.

## Limitations

- **Landmark Dependency**: The algorithm's effectiveness depends entirely on the quality of landmark extraction and ordering, potentially learning suboptimal methods when landmarks don't capture full complexity
- **Convergence Issues**: CURRICULAMA can fail to converge in learning complete sets of methods in some domains (Blocks World, Logistics, Rover), though it still achieves comparable plan quality to HTN-MAKER
- **Domain-Specific Performance**: The experimental evaluation covers only five domains, limiting generalizability to domains with different characteristics

## Confidence

**High Confidence**: The soundness proof for learned methods (Mechanism 2) - The theoretical analysis showing that methods learned by CURRICULAMA can solve equivalent HTN planning problems is well-founded and clearly demonstrated.

**Medium Confidence**: Comparable convergence rates and plan quality to HTN-MAKER (Mechanism 3) - While experimental results show similar performance, the lack of statistical significance testing and limited domain coverage reduces confidence in generalizability.

**Medium Confidence**: Landmark-based curriculum generation eliminates manual annotation (Mechanism 1) - The approach is logically sound, but the potential for learning extraneous methods due to suboptimal landmark orderings introduces uncertainty.

## Next Checks

1. **Statistical Significance Testing**: Conduct paired t-tests or Wilcoxon signed-rank tests comparing CURRICULAMA and HTN-MAKER performance across all metrics (convergence rate, plan length, planning time) to determine if observed differences are statistically significant.

2. **Landmark Quality Analysis**: Systematically analyze landmark orderings in each domain to identify cases where landmark-based curricula lead to suboptimal method learning, and quantify the impact on plan quality.

3. **Domain Transferability Test**: Apply CURRICULAMA to additional domains not in the original five (e.g., more complex logistics variants, manufacturing domains) to assess generalizability and identify domain characteristics that affect learning effectiveness.