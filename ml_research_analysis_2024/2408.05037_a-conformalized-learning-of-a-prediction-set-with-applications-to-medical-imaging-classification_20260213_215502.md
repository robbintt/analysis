---
ver: rpa2
title: A conformalized learning of a prediction set with applications to medical imaging
  classification
arxiv_id: '2408.05037'
source_url: https://arxiv.org/abs/2408.05037
tags:
- prediction
- sets
- medical
- network
- conformal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of quantifying uncertainty in
  medical imaging classifiers to enable their deployment in clinical settings. The
  proposed method, Conformalized Prediction Set Network (CPSN), trains a neural network
  to predict instance-specific conformal thresholds for generating prediction sets
  that contain the true label with a user-specified probability.
---

# A conformalized learning of a prediction set with applications to medical imaging classification

## Quick Facts
- arXiv ID: 2408.05037
- Source URL: https://arxiv.org/abs/2408.05037
- Authors: Roy Hirsch; Jacob Goldberger
- Reference count: 0
- Primary result: CPSN achieves smaller average prediction set sizes while maintaining required coverage levels on medical imaging datasets

## Executive Summary
This paper addresses the challenge of quantifying uncertainty in medical imaging classifiers to enable their deployment in clinical settings. The proposed method, Conformalized Prediction Set Network (CPSN), trains a neural network to predict instance-specific conformal thresholds for generating prediction sets that contain the true label with a user-specified probability. The approach is evaluated on OrganAMNIST and TissueMNIST medical imaging datasets, comparing against baseline methods including naive, APS, and RAPS.

## Method Summary
CPSN learns a regression network to predict conformal scores directly from input features, avoiding the need for true labels during prediction. The method uses temperature scaling for probability calibration, then calculates bias correction values δ1 and δ2 based on validation set residuals, separated by prediction confidence levels. For each test sample, the prediction set is generated by thresholding the learned conformal scores with the instance-specific bias correction, ensuring coverage while minimizing set size.

## Key Results
- CPSN achieves coverage of 0.935 with size 2.251 on OrganAMNIST with α=0.1, versus APS coverage 0.899 with size 4.096
- On TissueMNIST, CPSN demonstrates improved efficiency in prediction sets across multiple evaluation settings while satisfying theoretical coverage guarantees
- The method produces small prediction sets with low variance for all tested setups

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instance-specific conformal thresholds reduce prediction set sizes compared to fixed thresholds
- Mechanism: The network learns a residual score rt = s(xt, yt) - q(xt; θ) for each sample, then conformalizes by adding δ(x) that ensures coverage. This tailors the threshold to each sample's difficulty rather than using one global threshold.
- Core assumption: The residual distribution differs meaningfully between high-confidence and low-confidence predictions, justifying separate quantiles for each group.
- Evidence anchors:
  - [abstract] "The threshold is then conformalized to ensure the required coverage"
  - [section] "The residual scores act differently for predictions with higher and lower confidence levels. Hence, we can separate them into two sets based on whether the prediction confidence ˆp(x) = max i p(y = i|x) is larger or smaller than 1 − α."
  - [corpus] Weak - neighboring papers focus on different aspects of conformal prediction but don't directly address instance-specific thresholds

### Mechanism 2
- Claim: Separating residuals by confidence enables better bias correction
- Mechanism: The method calculates δ1 for samples where max predicted probability > 1-α and δ2 for others, capturing different bias patterns in high vs low confidence predictions.
- Core assumption: The relationship between predicted confidence and actual coverage differs across confidence levels, requiring different bias corrections.
- Evidence anchors:
  - [section] "The residual scores act differently for predictions with higher and lower confidence levels. Hence, we can separate them into two sets based on whether the prediction confidence ˆp(x) = max i p(y = i|x) is larger or smaller than 1 − α."
  - [section] "The bias correction values δ1 and δ2 in the case of TissueMNIST were 0.052 ± 0.15 and 0.134 ± 0.17. This empirical evidence highlights the need to calculate the conformal bias correction separately for each of the two sets."
  - [corpus] Missing - no direct evidence about confidence-based separation in related work

### Mechanism 3
- Claim: Learning conformal scores directly from features improves efficiency
- Mechanism: A neural network learns to predict the APS conformal score s(x, y) from input features x alone, then conformalization adjusts for coverage. This avoids using the true label during prediction.
- Core assumption: The input features contain sufficient information to predict conformal scores accurately, even without the true label.
- Evidence anchors:
  - [section] "We first train a neural network to predict the APS score s(x, y) (2) directly from the feature input vector x, without using the true classy"
  - [section] "Given a training set (x1, y1), ..., (xn, yn), we learn a regression network by minimizing the following Mean Squared Error (MSE) loss"
  - [section] "On average, CPSN produces small prediction sets with low variance for all the setups evaluated"

## Foundational Learning

- Concept: Conformal prediction framework and coverage guarantees
  - Why needed here: Understanding the theoretical foundation ensures correct implementation of coverage guarantees and proper interpretation of results
  - Quick check question: What is the coverage guarantee provided by conformal prediction, and how does it differ from conditional coverage?

- Concept: Adaptive Prediction Sets (APS) and Regularized APS (RAPS)
  - Why needed here: These are the baseline methods being compared against, so understanding their mechanisms is crucial for proper evaluation and implementation
  - Quick check question: How does the RAPS conformal score differ from APS, and when would RAPS be expected to outperform APS?

- Concept: Temperature scaling for calibration
  - Why needed here: The method uses calibrated probabilities as input, so understanding how temperature scaling affects prediction confidence is important
  - Quick check question: How does temperature scaling modify the softmax outputs, and why is calibration important for conformal prediction?

## Architecture Onboarding

- Component map:
  - Pre-trained medical image classifier (ResNet-50) providing features and probabilities
  - Temperature scaling module for probability calibration
  - Regression network (MLP) learning conformal score predictions
  - Conformalization module calculating δ values and generating prediction sets
  - Evaluation pipeline computing coverage and size metrics

- Critical path: Input → Temperature scaling → Feature extraction → Regression network → Conformalization → Prediction set output
  - Each stage must function correctly for the final prediction sets to meet coverage requirements while minimizing size

- Design tradeoffs:
  - Regression network complexity vs. generalization: Simpler networks may underfit conformal score patterns; complex networks may overfit to training data
  - Confidence threshold selection: The 1-α cutoff for separating high/low confidence samples affects bias correction effectiveness
  - Validation set size: Larger validation sets provide more stable quantile estimates but reduce training data

- Failure signatures:
  - Coverage consistently below 1-α: Indicates incorrect conformalization or poorly learned regression network
  - Extremely large prediction sets: Suggests the regression network is underestimating conformal scores
  - High variance in prediction set sizes: May indicate overfitting or unstable quantile estimation

- First 3 experiments:
  1. Verify coverage on validation set: Test that the method achieves the target coverage (1-α) on held-out validation data before testing
  2. Compare against naive baseline: Ensure the method provides improvement over the simple cumulative probability approach
  3. Ablation study on confidence separation: Test performance with single vs. dual δ values to confirm the benefit of separating by confidence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the CPSN method perform on datasets with more than 11 classes, and does its efficiency advantage persist as class count increases?
- Basis in paper: [explicit] The paper states "The RAPS algorithm became effective when the number of classes and the size of the prediction sets were both very large" and notes medical imaging typically involves "a moderate number of classes"
- Why unresolved: The paper only tested on datasets with 8-11 classes, so the method's scalability to high-cardinality classification problems remains unknown
- What evidence would resolve it: Systematic evaluation of CPSN on datasets with 50+, 100+, or 1000+ classes, comparing prediction set sizes and coverage rates against baseline methods

### Open Question 2
- Question: What is the impact of different network architectures for the regression network q(x; θ) on CPSN performance?
- Basis in paper: [explicit] The paper uses "a two-layered MLP with ReLU activation" but notes this is implementation detail, not an explored design choice
- Why unresolved: The paper uses a fixed architecture without exploring alternatives or conducting ablation studies on network depth, width, or activation functions
- What evidence would resolve it: Systematic comparison of different architectures (CNNs, transformers, varying depth/width) for the regression network while keeping other components fixed

### Open Question 3
- Question: How does CPSN performance change with different conformalization strategies beyond the two-set quantile approach?
- Basis in paper: [explicit] The paper separates scores into two sets based on prediction confidence and calculates quantiles separately, but this is presented as a single approach
- Why unresolved: The paper does not explore alternative conformalization methods (e.g., continuous functions, kernel density estimation, or learned conformalization) that might improve performance
- What evidence would resolve it: Comparative evaluation of CPSN using different conformalization strategies while keeping the regression network fixed

## Limitations
- The method's performance relies heavily on the quality of the pre-trained classifier features, and degraded classifier performance would propagate to the conformal prediction stage
- The confidence-based separation at threshold 1-α is heuristic and may not optimally capture the distribution of residuals across all difficulty levels
- The computational overhead of training an additional regression network could be prohibitive for resource-constrained clinical environments

## Confidence

- High confidence: Coverage guarantees are theoretically sound and empirically validated
- Medium confidence: Efficiency improvements over baselines are demonstrated but may vary across different medical imaging tasks
- Medium confidence: The heuristic confidence-based separation provides practical benefits but lacks theoretical optimality guarantees

## Next Checks

1. Test CPSN on additional medical imaging datasets with varying class imbalances and resolution characteristics to assess generalizability beyond OrganAMNIST and TissueMNIST
2. Evaluate the impact of classifier quality by testing CPSN with classifiers of varying accuracy levels to understand robustness to feature quality degradation
3. Perform ablation studies on the confidence threshold (1-α) to determine optimal separation points for different data distributions and whether adaptive thresholds would improve performance