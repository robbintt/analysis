---
ver: rpa2
title: Multimodal Learned Sparse Retrieval for Image Suggestion
arxiv_id: '2402.07736'
source_url: https://arxiv.org/abs/2402.07736
tags:
- image
- retrieval
- encoder
- sparse
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the application of Learned Sparse Retrieval
  (LSR) methods to the multimodal image suggestion task. LSR encodes queries and documents
  into sparse lexical vectors suitable for efficient retrieval.
---

# Multimodal Learned Sparse Retrieval for Image Suggestion

## Quick Facts
- arXiv ID: 2402.07736
- Source URL: https://arxiv.org/abs/2402.07736
- Reference count: 40
- Primary result: Caption-enriched models significantly outperform image-only models in multimodal image suggestion tasks

## Executive Summary
This paper explores Learned Sparse Retrieval (LSR) methods for multimodal image suggestion tasks, investigating how different encoder configurations handle queries and documents represented by images, captions, or both. Using the AToMiC dataset, the authors demonstrate that models incorporating captions substantially outperform those using only image content, highlighting captions' critical role in providing fine-grained concepts and context information. The study compares MLP and MLM encoders for both query and document representations, finding that caption information is essential for effective retrieval performance.

## Method Summary
The authors implement bi-encoder models with four configurations: M1 (MLM-MLM using images only), M2 (MLP-MLM using captions), M3 (MLP-MLP using captions), and M4 (MLP-MLP using both images and captions). Models are trained using InforNCE loss with in-batch negatives for 5 epochs on the AToMiC dataset. The dataset contains 11 million images and 10 million text queries from Wikipedia, with images paired with multilingual captions. Models encode queries and documents into sparse lexical vectors suitable for efficient retrieval using an inverted index.

## Key Results
- Models using only image content (M1) perform significantly worse than those incorporating captions (M2, M3, M4)
- Caption-enriched models show substantial improvements across all evaluation metrics (NDCG@k, MAP@k, Recall@k)
- Combining image and caption information (M4) does not significantly improve upon caption-only models (M2, M3)
- MLP encoders on the query side produce more interpretable and sparsely co-activated representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using captions in addition to image content significantly improves retrieval performance by providing fine-grained concepts and context information.
- Mechanism: Captions encode semantic information and contextual details that are difficult to extract from visual content alone, allowing the MLM encoder to project images into terms appearing in relevant queries.
- Core assumption: The additional semantic richness provided by captions is necessary and sufficient for solving the image suggestion task effectively.
- Evidence anchors: [abstract] "Enriching the image content with its caption improves the model performance significantly, implying the importance of image captions to provide fine-grained concepts and context information of images."

### Mechanism 2
- Claim: Using MLP encoder on the query side constrains the projection to semantically relevant terms, producing more interpretable and sparsely co-activated image representations.
- Mechanism: MLP encoder only scores the impact of input tokens without expansion capability, forcing the image encoder (MLM) to project images into terms appearing in the relevant query.
- Core assumption: Constraining the projection to semantically relevant terms prevents high co-activation and improves interpretability.
- Evidence anchors: [abstract] "One solution to the above issues is to constrain the projection to semantically relevant terms in the vocabulary. This constraint could be achieved by using the MLP encoder on the query side."

### Mechanism 3
- Claim: The MLM encoder can be applied to both text and image modalities and has the freedom to expand the input to any relevant terms in the vocabulary.
- Mechanism: MLM encoder uses masked language model logits for weighting and expansion, allowing flexible representation of both textual and visual inputs.
- Core assumption: The MLM encoder's expansion capability is necessary for effective multimodal retrieval.
- Evidence anchors: [abstract] "Unlike the MLP encoder, the MLM encoder can be applied to both text and image and has the freedom to expand the input to any relevant terms in the vocabulary."

## Foundational Learning

- Concept: Learned Sparse Retrieval (LSR)
  - Why needed here: LSR is the foundation for encoding queries and documents into sparse lexical vectors suitable for efficient retrieval using an inverted index.
  - Quick check question: What is the primary advantage of using LSR over dense retrieval methods?

- Concept: Multimodal retrieval
  - Why needed here: The task involves retrieving relevant images based on textual queries, requiring the model to handle both visual and textual modalities.
  - Quick check question: What are the key challenges in multimodal retrieval compared to unimodal retrieval?

- Concept: Bi-encoder architecture
  - Why needed here: The bi-encoder architecture allows for efficient encoding of queries and documents separately, which is crucial for large-scale retrieval tasks.
  - Quick check question: How does the bi-encoder architecture differ from a cross-encoder architecture in terms of computational efficiency?

## Architecture Onboarding

- Component map: Query encoder -> Document encoder -> Tokenizer/Image processor -> Sparse projection layer -> Inverted index
- Critical path: Input processing (tokenization/image processing) -> Encoding (query and document encoders) -> Sparse projection -> Inverted index lookup -> Ranking and output
- Design tradeoffs: MLP vs MLM encoders (Interpretability vs. flexibility) | Single vs. multiple modalities (Complexity vs. performance) | Sparse vs. dense representations (Efficiency vs. expressiveness)
- Failure signatures: Poor performance on image-only queries | High co-activation in output dimensions | Difficulty in interpreting sparse vectors
- First 3 experiments: 1) Compare performance of image-only vs. caption-only retrieval | 2) Evaluate the impact of using MLP vs MLM encoders on query side | 3) Test the effect of combining image and caption information on retrieval performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of different caption preprocessing strategies (e.g., removing stop words, using only nouns) on MLSR performance?
- Basis in paper: [inferred] The paper mentions that using MLP encoders on the query side helps produce more interpretable results but still relies on stop words and punctuation marks, suggesting caption preprocessing could be important.
- Why unresolved: The paper does not explore different caption preprocessing strategies and their effects on model performance.
- What evidence would resolve it: Experiments comparing MLSR performance using captions with different preprocessing approaches (e.g., raw captions, stop words removed, only nouns kept).

### Open Question 2
- Question: How does the performance of MLSR models vary across different types of images (e.g., landscapes, portraits, objects)?
- Basis in paper: [inferred] The paper discusses challenges in inferring fine-grained concepts from images alone and highlights the importance of captions, suggesting image type may influence the need for caption information.
- Why unresolved: The paper does not analyze MLSR performance across different image categories or types.
- What evidence would resolve it: Experiments evaluating MLSR models on subsets of images grouped by type (e.g., landscapes, portraits, objects) and comparing performance across these groups.

### Open Question 3
- Question: What is the effect of incorporating visual features (e.g., object detection, scene graphs) alongside captions in MLSR models?
- Basis in paper: [inferred] The paper explores using both image content and captions, with captions significantly improving performance, suggesting that combining visual features with captions could be beneficial.
- Why unresolved: The paper does not investigate the impact of incorporating additional visual features beyond raw image content.
- What evidence would resolve it: Experiments comparing MLSR performance using captions alone, raw image content alone, and captions combined with visual features (e.g., object detection results, scene graphs).

## Limitations
- Weak empirical validation of mechanisms - specific mechanisms lack direct ablation studies
- Missing architectural details - critical unknowns include specific encoder implementations and preprocessing steps
- Dataset-specific findings - AToMiC dataset may not generalize to other domains or image types

## Confidence
- High confidence: Core finding that caption-enriched models outperform image-only models
- Medium confidence: General claim that captions provide valuable semantic context
- Low confidence: Specific claims about MLP constraint benefits and MLM expansion freedom

## Next Checks
1. Ablation study on caption components - remove individual caption elements to determine which aspects contribute most to performance gains
2. Cross-domain evaluation - test best-performing models on different image-text datasets to assess generalization
3. Sparsity and interpretability analysis - quantify actual sparsity of output vectors and measure co-activation levels across dimensions