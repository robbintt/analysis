---
ver: rpa2
title: Bridging the Gap in Hybrid Decision-Making Systems
arxiv_id: '2409.19415'
source_url: https://arxiv.org/abs/2409.19415
tags:
- user
- bridget
- decision
- phase
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BRIDGET is a novel hybrid decision-making system that dynamically
  switches between human- and machine-led decision modes based on model reliability
  and user preference. The system employs a co-evolutionary training phase where a
  human expert makes all initial decisions while a machine model learns incrementally
  using Fading Empirical Accuracy to track both human and model performance over time.
---

# Bridging the Gap in Hybrid Decision-Making Systems

## Quick Facts
- arXiv ID: 2409.19415
- Source URL: https://arxiv.org/abs/2409.19415
- Reference count: 17
- Primary result: BRIDGET dynamically switches between human- and machine-led decision modes based on model reliability and user preference

## Executive Summary
BRIDGET is a novel hybrid decision-making system that bridges the gap between skeptical learning and learning-to-defer paradigms. The system employs a co-evolutionary training phase where a human expert makes all initial decisions while a machine model learns incrementally using Fading Empirical Accuracy (FEA) to track both human and model performance over time. When the model's accuracy surpasses a threshold and the human designates it as primary decision-maker, BRIDGET transitions to a machine-in-command mode where the model makes most decisions but can defer to the human when uncertain.

The system provides explanations through contrastive examples and decision rules in both modes and monitors for concept drift to trigger returns to human-led decision-making when reliability decreases. This unified framework adapts to different scenarios while maintaining interpretability, addressing the limitations of both skeptical learning (which questions human decisions) and learning-to-defer (which only learns when to defer).

## Method Summary
BRIDGET implements a co-evolutionary training approach where a human expert initially makes all decisions while an incremental learning model tracks performance using Fading Empirical Accuracy. The system transitions between Human-in-Command (HiC) and Machine-in-Command (MiC) phases based on FEA thresholds, user preferences, and concept drift detection. In HiC mode, the human makes decisions with skeptical machine suggestions, while in MiC mode, the machine makes most decisions but defers to humans when uncertain. The system provides explanations through contrastive examples and decision rules, and uses FEA with temporal weighting to monitor reliability and detect concept drift.

## Key Results
- Dynamic phase transitions between HiC and MiC modes based on model reliability and user preference
- Continuous learning through co-evolutionary training with FEA tracking
- Explanation mechanisms that maintain interpretability in both decision modes
- Concept drift monitoring that triggers returns to human-led decision-making

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BRIDGET dynamically switches between Human-in-Command (HiC) and Machine-in-Command (MiC) phases based on model reliability and user preference
- Mechanism: The system uses a co-evolutionary training phase where the human makes all initial decisions while the machine learns incrementally using Fading Empirical Accuracy (FEA) to track both human and model performance over time. When the model's accuracy surpasses a threshold and the human designates it as primary decision-maker, BRIDGET transitions to MiC mode where the model makes most decisions but can defer to the human when uncertain.
- Core assumption: The human is reliable and their decisions can serve as ground truth for training the machine learning model
- Evidence anchors:
  - [abstract] "BRIDGET transitions to a machine-in-command mode where the model makes most decisions but can defer to the human when uncertain"
  - [section] "the model's average FEA is higher than a certain threshold; H designates M as the primary decision-maker"
  - [corpus] No direct corpus evidence found for FEA or co-evolutionary training in related papers
- Break condition: The transition fails if the human's decisions are inconsistent or the model's FEA metric does not accurately reflect model reliability

### Mechanism 2
- Claim: BRIDGET provides explanations through contrastive examples and decision rules in both modes to maintain interpretability
- Mechanism: The system employs an incremental learning model that can explain its decisions through CAIPI model mechanisms, providing real and synthetic examples and counterexamples. In HiC mode, explanations help the human understand when the machine is skeptical of their decisions. In MiC mode, explanations help the human understand why the machine is uncertain.
- Core assumption: Explanations improve human trust and understanding of the system's decision-making process
- Evidence anchors:
  - [abstract] "The system provides explanations through contrastive examples and decision rules in both modes"
  - [section] "H can request an explanation for the suggestion, such as (counter-)exemplar records, local decision rules or an overview of the model's logic"
  - [corpus] Weak corpus evidence - while related papers mention explainability, specific mechanisms like contrastive examples are not well-documented
- Break condition: The explanation system fails if the explanations do not accurately reflect the model's reasoning or if they become too complex for users to understand

### Mechanism 3
- Claim: BRIDGET monitors for concept drift to trigger returns to human-led decision-making when reliability decreases
- Mechanism: The system continuously monitors the model's average FEA and triggers transitions back to HiC mode when the FEA drops below a threshold or when the user has been called back multiple times due to the model's low belief. This creates a feedback loop that maintains system reliability over time.
- Core assumption: Concept drift occurs in real-world decision-making scenarios and must be detected and addressed
- Evidence anchors:
  - [abstract] "monitors for concept drift to trigger returns to human-led decision-making when reliability decreases"
  - [section] "the user has been called back due to M's low b for more than pmax times; the model's average FEA is now lower than a certain threshold"
  - [corpus] No direct corpus evidence for concept drift monitoring in hybrid decision-making systems
- Break condition: The monitoring fails if the FEA metric does not accurately capture concept drift or if the thresholds are set too conservatively/aggressively

## Foundational Learning

- Concept: Incremental Learning (IL)
  - Why needed here: BRIDGET requires the model to learn continuously from each human decision without retraining on the entire dataset
  - Quick check question: What is the key difference between incremental learning and traditional batch learning?

- Concept: Empirical Accuracy (EA) and Fading Empirical Accuracy (FEA)
  - Why needed here: These metrics serve as proxies for model epistemic uncertainty and track the reliability of both human and machine decisions over time
  - Quick check question: How does FEA differ from traditional EA in terms of weighting historical data?

- Concept: Skeptical Learning
  - Why needed here: BRIDGET uses skeptical learning principles to determine when the machine should question human decisions in HiC mode
  - Quick check question: What role does skepticality play in the human-machine interaction within BRIDGET?

## Architecture Onboarding

- Component map: Human interface module -> Machine learning model -> FEA tracker -> Phase controller -> Explanation generator
- Critical path: Record → Human decision → Machine prediction → Skepticality check → Explanation request (if needed) → Final decision → Model update → FEA calculation → Phase transition check
- Design tradeoffs:
  - Accuracy vs. interpretability: More interpretable models may sacrifice some predictive accuracy
  - Responsiveness vs. stability: Frequent phase transitions may improve responsiveness but reduce stability
  - Complexity vs. usability: More sophisticated explanation mechanisms may increase system complexity
- Failure signatures:
  - Stuck in HiC mode: FEA never reaches threshold despite good performance
  - Stuck in MiC mode: FEA drops but system doesn't transition back to HiC
  - Explanation failure: Explanations don't match model reasoning or are too complex
  - Performance degradation: Model accuracy decreases over time without triggering phase transitions
- First 3 experiments:
  1. Test phase transition logic with synthetic data where human accuracy is known and controlled
  2. Evaluate explanation quality by measuring user understanding and trust with different explanation types
  3. Stress test concept drift detection by introducing gradual changes in data distribution and measuring transition accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the fading empirical accuracy (FEA) metric compare to traditional empirical accuracy in terms of prediction accuracy and human-AI collaboration efficiency in Bridget?
- Basis in paper: [explicit] The paper introduces FEA as a replacement for EA to weigh each record w.r.t. its temporal distance to the current one, aiming to provide a more accurate proxy for the model's epistemic uncertainty.
- Why unresolved: The paper does not provide empirical results comparing FEA to EA in terms of their impact on prediction accuracy and human-AI collaboration efficiency.
- What evidence would resolve it: Empirical studies comparing the performance of Bridget using FEA versus EA in terms of prediction accuracy, human-AI collaboration efficiency, and user satisfaction.

### Open Question 2
- Question: What are the optimal thresholds for skepticality (α) and belief (β) in Bridget, and how do they affect the system's performance?
- Basis in paper: [explicit] The paper mentions that the system uses thresholds α and β for skepticality and belief, respectively, but does not provide guidance on how to determine these thresholds or their impact on performance.
- Why unresolved: The paper does not discuss the sensitivity of the system's performance to different threshold values or provide a method for selecting optimal thresholds.
- What evidence would resolve it: Sensitivity analysis or optimization studies to determine the impact of different threshold values on the system's performance and user satisfaction.

### Open Question 3
- Question: How does Bridget handle concept drift, and what mechanisms are in place to ensure the model's reliability over time?
- Basis in paper: [explicit] The paper mentions that Bridget monitors for concept drift to trigger returns to human-led decision-making when reliability decreases, but does not provide details on the mechanisms used to detect and handle concept drift.
- Why unresolved: The paper does not discuss the specific techniques or algorithms used to detect and handle concept drift in Bridget.
- What evidence would resolve it: Empirical studies evaluating the effectiveness of Bridget's concept drift detection and handling mechanisms in maintaining model reliability over time.

## Limitations
- FEA implementation lacks specific distance function details critical for reproducibility
- Explanation generation mechanisms described at high level without technical implementation details
- Threshold selection for phase transitions lacks guidance for different applications

## Confidence
- High Confidence: Overall architecture and phase transition logic between HiC and MiC modes is clearly specified and reproducible
- Medium Confidence: FEA concept as reliability metric is understood but lacks specific implementation details
- Low Confidence: Explanation generation mechanisms lack sufficient technical detail for implementation

## Next Checks
1. **FEA Implementation Validation**: Implement multiple distance functions (Euclidean, cosine similarity, temporal decay) and evaluate their impact on phase transition accuracy using synthetic datasets with known human reliability patterns.

2. **Explanation Quality Assessment**: Conduct user studies comparing different explanation types (contrastive examples vs. decision rules) to measure which formats improve user trust and understanding while maintaining system usability.

3. **Concept Drift Detection Testing**: Create controlled experiments where data distributions gradually shift over time, measuring the system's ability to detect drift and trigger appropriate transitions back to human-led decision-making.