---
ver: rpa2
title: 'Reuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment'
arxiv_id: '2404.12318'
source_url: https://arxiv.org/abs/2404.12318
tags:
- language
- alignment
- data
- cross-lingual
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies cross-lingual alignment, where a reward model
  (RM) trained in a source language is directly applied to align a language model
  in a target language. On summarization and dialog generation, the approach consistently
  improves model quality, as judged by humans and LMs, with win rates up to 70% against
  unaligned models.
---

# Reuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment

## Quick Facts
- arXiv ID: 2404.12318
- Source URL: https://arxiv.org/abs/2404.12318
- Reference count: 29
- One-line primary result: Cross-lingual reward model transfer improves alignment quality with win rates up to >70% against unaligned models

## Executive Summary
This paper explores zero-shot cross-lingual alignment by transferring reward models trained in one language to align language models in another language. The approach leverages multilingual base models' ability to judge generation quality across languages, enabling effective alignment without target-language preference data. Experiments on summarization and dialog generation show consistent improvements, with cross-lingual alignment sometimes outperforming monolingual approaches due to regularization effects that prevent overfitting to language-specific artifacts.

## Method Summary
The method involves training a reward model on preference data in a source language, then using this model to guide alignment of a target-language language model through reward optimization (either best-of-n or reinforcement learning). The key insight is that multilingual base models can evaluate generation quality across languages, enabling zero-shot transfer. The approach requires only monolingual supervised finetuning (SFT) data in the target language, not preference data, making it particularly useful when target-language preference annotations are unavailable.

## Key Results
- Cross-lingual alignment consistently improves model quality with win rates up to >70% against unaligned models
- Different-language reward models sometimes yield better alignment than same-language reward models
- The method remains effective even when target-language SFT data is unavailable
- Cross-lingual alignment effectiveness is affected by data distribution and domain mismatches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual base models can judge generation quality across languages, enabling zero-shot cross-lingual reward model transfer
- Mechanism: A reward model trained on preference data in one language can generalize to evaluate outputs in other languages because the underlying multilingual model understands multiple languages and generation quality transfers across languages
- Core assumption: Quality of language model generations is language-agnostic (e.g., good English generations remain good when translated to Spanish)
- Break condition: If generation quality is highly culture-specific or language-dependent, the reward signal won't transfer effectively

### Mechanism 2
- Claim: Cross-lingual reward optimization sometimes outperforms monolingual alignment due to regularization effect
- Mechanism: A target-language reward model may contain language-specific spurious artifacts that cause the policy model to overfit, while a different-language reward model is less susceptible to these artifacts
- Core assumption: Target-language reward models can develop language-specific biases that policy models can exploit
- Break condition: If the source-language reward model is too different from target-language preferences, regularization benefit disappears

### Mechanism 3
- Claim: Reward model generalizability within the original task is necessary but not sufficient for downstream alignment success
- Mechanism: A reward model must be able to correctly rank generations in a different language (generalizability), but this alone doesn't guarantee effective alignment
- Core assumption: Cross-lingual generalizability in the reward modeling task correlates with alignment effectiveness
- Break condition: If the reward model can't generalize to the target language at all, alignment will fail regardless of other factors

## Foundational Learning

- Concept: Supervised Fine-Tuning (SFT)
  - Why needed here: SFT provides the initial policy model that is then aligned using the reward model; without SFT, there's no base model to optimize
  - Quick check question: What happens to alignment performance if the SFT model quality is poor in the target language?

- Concept: Reward Modeling (RM)
  - Why needed here: The RM provides the preference signal that guides alignment; cross-lingual transfer depends on RM generalizability
  - Quick check question: How does the choice between pointwise and pairwise feedback affect RM training and transfer?

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: RLHF is one method for reward optimization that updates the policy based on the RM signal; understanding its mechanics is crucial for cross-lingual alignment
  - Quick check question: What role does the KL-divergence regularization term play in preventing reward over-optimization?

## Architecture Onboarding

- Component map: Base multilingual model -> Source-language SFT model -> Source-language reward model -> Target-language SFT model -> Alignment pipeline -> Evaluation models
- Critical path: SFT → RM training → Reward optimization → Evaluation
  - For cross-lingual transfer: Source SFT → Source RM training → Target language reward optimization → Evaluation
- Design tradeoffs:
  - Using translated SFT data vs. organic target-language data (quality vs. availability)
  - Best-of-n vs. RL (simplicity vs. model change capability)
  - Monolingual vs. cross-lingual RM (potential overfitting vs. regularization)
- Failure signatures:
  - Poor alignment win rates against unaligned SFT model
  - High KL-divergence from SFT model (potential reward hacking)
  - Low RM generalizability accuracy on target language validation data
  - Large gap between monolingual and cross-lingual alignment performance
- First 3 experiments:
  1. Verify cross-lingual RM generalizability by testing source-language RM on target-language validation data
  2. Test best-of-n alignment with cross-lingual RM vs. no alignment baseline
  3. Compare cross-lingual alignment effectiveness vs. monolingual alignment using same-language RM

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do reward model transfer benefits scale with the size and quality of the source-language preference data?
- Basis in paper: [inferred] The paper notes that English is often the most effective source language, potentially due to higher annotator quantity and quality (Yu et al., 2022). However, it doesn't systematically investigate how different amounts or qualities of source-language preference data affect cross-lingual alignment success.
- Why unresolved: The paper focuses on comparing cross-lingual alignment effectiveness across different source-target language pairs rather than varying the size/quality of the source preference data itself.
- What evidence would resolve it: Controlled experiments varying the size and quality of source-language preference data while keeping target languages constant, measuring alignment effectiveness with different source data configurations.

### Open Question 2
- Question: Does reward model transfer work equally well for all types of alignment objectives (e.g., helpfulness, harmlessness, creativity) or are there differences in cross-lingual transferability across objectives?
- Basis in paper: [explicit] The paper focuses on helpfulness/harmlessness objectives in summarization and dialog tasks, but doesn't explore whether different alignment objectives show different cross-lingual transferability patterns.
- Why unresolved: The paper's evaluation framework assumes a single scalar reward for generation quality, without investigating whether different alignment objectives might have different cross-lingual generalization properties.
- What evidence would resolve it: Systematic comparison of cross-lingual alignment effectiveness across multiple alignment objectives using the same source-target language pairs.

### Open Question 3
- Question: What are the linguistic features that best predict successful reward model transfer between language pairs?
- Basis in paper: [inferred] The paper explicitly tests whether typological features (WALS features) predict alignment success and finds no statistically significant correlations, suggesting that other linguistic features might be more predictive.
- Why unresolved: While the paper rules out typological features as strong predictors, it doesn't identify what features actually do predict successful transfer, leaving the question of what makes some language pairs more transferable than others open.
- What evidence would resolve it: Analysis of additional linguistic features (e.g., semantic distance, cultural proximity, script similarity) and their correlation with alignment success across multiple language pairs.

## Limitations

- Limited to summarization and dialog generation tasks, with uncertain generalizability to other language generation tasks
- Effectiveness sensitive to quality and domain alignment of SFT data, with translated data potentially introducing artifacts
- Heavy reliance on automatic LM judges with limited human evaluation, raising questions about metric validity

## Confidence

- **High Confidence:** Cross-lingual reward models can be effectively transferred to align target-language models
- **Medium Confidence:** Cross-lingual reward optimization sometimes outperforms monolingual alignment due to regularization effects
- **Low Confidence:** Cross-lingual reward models can achieve higher win rates than same-language reward models

## Next Checks

1. **Human Evaluation Validation:** Conduct comprehensive human pairwise comparisons across multiple language pairs to validate the automatic LM judge results, particularly for the claim that cross-lingual RMs sometimes outperform same-language RMs.

2. **Task and Domain Generalization:** Test cross-lingual reward transfer on additional generation tasks (e.g., question answering, story generation) and domains (e.g., technical writing, social media dialogue) to assess limits of generalizability.

3. **Robustness to SFT Quality Variation:** Systematically vary the quality and domain alignment of target-language SFT data to quantify how SFT quality affects cross-lingual alignment effectiveness.