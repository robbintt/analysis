---
ver: rpa2
title: Direct Preference Optimization for Suppressing Hallucinated Prior Exams in
  Radiology Report Generation
arxiv_id: '2406.06496'
source_url: https://arxiv.org/abs/2406.06496
tags:
- prior
- exams
- report
- references
- reports
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a direct preference optimization (DPO) method\
  \ to reduce hallucinations of prior exams in radiology report generation by large\
  \ vision-language models (VLMs). The authors fine-tune a pretrained VLM on pairs\
  \ of reports\u2014one containing references to prior exams and another edited to\
  \ remove them\u2014using both standard and weighted DPO losses."
---

# Direct Preference Optimization for Suppressing Hallucinated Prior Exams in Radiology Report Generation

## Quick Facts
- **arXiv ID**: 2406.06496
- **Source URL**: https://arxiv.org/abs/2406.06496
- **Reference count**: 34
- **Primary result**: Achieves 3.2-4.8x reduction in hallucinated prior exam references while maintaining clinical accuracy in chest X-ray report generation

## Executive Summary
This paper introduces a direct preference optimization (DPO) method to reduce hallucinations of prior exams in radiology report generation by large vision-language models (VLMs). The authors fine-tune a pretrained VLM on pairs of reports—one containing references to prior exams and another edited to remove them—using both standard and weighted DPO losses. The weighted DPO loss selectively penalizes only the parts of the output that reference prior exams, while leaving other content unchanged. Experiments on chest X-ray report generation show that DPO fine-tuning achieves a 3.2-4.8x reduction in lines hallucinating prior exams while maintaining or improving clinical accuracy on specialized metrics like RadCliQ-V1. The method is data- and compute-efficient and represents the first application of DPO to medical VLMs.

## Method Summary
The method involves fine-tuning a pretrained vision-language model on pairs of radiology reports where one version contains references to prior exams and the other has been processed to remove them. The authors use both standard and weighted DPO losses, with the weighted version selectively penalizing only the tokens that reference prior exams by applying a scaling factor γ. This allows the model to focus optimization on the specific unwanted behavior while preserving other clinical content. The approach is applied to chest X-ray report generation using the MIMIC-CXR dataset, with GPT-4 used to create the preference pairs by editing reports to remove prior exam references.

## Key Results
- DPO fine-tuning achieves 3.2-4.8x reduction in lines hallucinating prior exams compared to the pretrained model
- Weighted DPO (γ=0.5) outperforms standard DPO (γ=1) in reducing hallucinations while maintaining clinical accuracy
- RadCliQ-V1 scores improve from 0.3268 to 0.3449 with weighted DPO fine-tuning, indicating better clinical accuracy
- Supervised fine-tuning alone on processed reports does not meaningfully reduce hallucinated prior exams

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DPO selectively penalizes hallucinated prior exam references while preserving clinical accuracy
- Mechanism: The weighted DPO loss multiplies the log-probabilities of irrelevant tokens by γ, reducing their contribution to the overall loss. This allows the model to focus optimization on the specific unwanted behavior (references to prior exams) without heavily penalizing correct content in other parts of the response.
- Core assumption: The behavior of interest can be isolated to specific parts of the output, allowing selective optimization
- Evidence anchors:
  - [abstract] "The weighted DPO loss selectively penalizes only the parts of the output that reference prior exams, while leaving other content unchanged"
  - [section] "When calculating log π(y|x), we multiply the log-probabilities of irrelevant tokens by γ, effectively reducing their importance in comparison to the relevant tokens"
- Break condition: If the unwanted behavior is distributed throughout the response rather than localized, the selective weighting approach would fail to properly suppress it

### Mechanism 2
- Claim: DPO fine-tuning achieves significant reduction in hallucinated prior exams while maintaining clinical accuracy
- Mechanism: By fine-tuning on pairs of reports where one version has references to prior exams removed, the model learns to generate content that matches the preference for prior-free reports without losing its clinical knowledge from pretraining
- Core assumption: The preference pairs contain sufficient signal to guide behavior change without degrading domain knowledge
- Evidence anchors:
  - [abstract] "DPO fine-tuning achieves a 3.2-4.8x reduction in lines hallucinating prior exams while maintaining model performance on clinical accuracy metrics"
  - [section] "We show that our method makes a pretrained VLM substantially less likely to hallucinate prior exams, while simultaneously maintaining the model's clinical accuracy"
- Break condition: If the preference pairs are too noisy or don't adequately represent the desired behavior, the fine-tuning could lead to either insufficient behavior change or loss of clinical accuracy

### Mechanism 3
- Claim: Supervised fine-tuning on processed reports alone is insufficient to suppress hallucinated prior exams
- Mechanism: Exposure to prior-free reports during fine-tuning without the preference optimization framework doesn't provide the necessary contrastive learning signal to prevent the model from generating references to prior exams
- Core assumption: The DPO loss provides a unique optimization signal beyond simple exposure to preferred outputs
- Evidence anchors:
  - [section] "Supervised fine-tuning on our processed training dataset improves clinical accuracy, perhaps because cases mentioning prior exams tend to be especially challenging, but it does not meaningfully reduce the number of lines referencing prior exams"
  - [section] "We therefore find that the DPO loss itself is necessary for suppressing unwanted behaviors"
- Break condition: If supervised fine-tuning with enough data and proper regularization could achieve similar results, this mechanism would be invalid

## Foundational Learning

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: DPO provides a framework for fine-tuning language models based on human preferences without requiring explicit reward modeling, making it suitable for medical applications where reward model construction would be challenging
  - Quick check question: What distinguishes DPO from traditional RLHF approaches?

- Concept: Multimodal Vision-Language Models (VLMs)
  - Why needed here: Understanding VLMs is essential because the work involves fine-tuning models that process both medical images and generate text reports, requiring knowledge of how vision encoders, adapters, and language models interact
  - Quick check question: How does the vision-language adapter component function in the VLM architecture?

- Concept: Clinical accuracy metrics for radiology reports
  - Why needed here: The work evaluates both reduction in hallucinations and maintenance of clinical accuracy, requiring understanding of specialized metrics like RadCliQ-V1 and RadGraph-F1 that are designed specifically for medical report evaluation
  - Quick check question: What is the key difference between RadCliQ-V1 and RadGraph-F1 in terms of what they measure?

## Architecture Onboarding

- Component map: Swin Transformer vision encoder → vision-language adapter → Llama2-Chat-7b LLM with LoRA tuning. Input: chest X-ray image + clinical context. Output: radiology report. Fine-tuning uses DPO with preference pairs.
- Critical path: Image → Vision encoder → Adapter → LLM → Report generation → Preference evaluation (RadCliQ/RadGraph) → DPO loss calculation → Parameter update
- Design tradeoffs: Standard vs. weighted DPO losses (γ values); parameter-efficient LoRA tuning vs. full fine-tuning; GPT-4 vs. human annotations for dataset creation
- Failure signatures: Increased hallucinated prior exams; decreased clinical accuracy; overfitting to processed dataset; poor generalization to original test distribution
- First 3 experiments:
  1. Compare pretrained model vs. standard DPO (γ=1) vs. weighted DPO (γ=0.5) on hallucinated prior exam reduction
  2. Evaluate clinical accuracy retention using RadCliQ-V1 and RadGraph-F1 across different γ values
  3. Test sensitivity to GPT-4 annotation quality by comparing models trained on different subsets of the preference dataset

## Open Questions the Paper Calls Out
None

## Limitations
- Annotation quality concerns: The preference dataset relies on GPT-4 annotations, which may introduce noise and systematic biases that could limit DPO performance
- Clinical metric validation: RadCliQ-V1 improvements are promising but this metric is relatively new and has not been extensively validated across diverse clinical scenarios
- Mechanistic understanding gaps: The paper demonstrates weighted DPO works better than standard DPO but doesn't fully explain why this improvement occurs

## Confidence
**High confidence**: The core claim that DPO fine-tuning reduces hallucinated prior exam references by 3.2-4.8x is supported by direct experimental measurements with clear baselines. The methodology for creating preference pairs and applying DPO is well-specified.

**Medium confidence**: The claim that clinical accuracy is maintained or improved, based primarily on RadCliQ-V1 improvements. This metric, while showing positive results, is newer and less established than traditional metrics.

**Low confidence**: The assertion that the weighted DPO loss is necessary rather than sufficient. The paper shows it works better than standard DPO, but doesn't explore whether other weighting schemes or fine-tuning approaches could achieve similar results.

## Next Checks
1. **Cross-validation with human annotations**: Have clinical radiologists independently review a subset of generated reports to verify that reduced prior exam references correspond to clinically appropriate content, and assess whether the RadCliQ-V1 improvements align with human clinical judgment.

2. **Ablation study on GPT-4 annotation quality**: Train models using different subsets of the preference dataset (e.g., reports where GPT-4 had high vs. low confidence scores) to quantify the impact of annotation quality on DPO performance and identify potential failure modes.

3. **Generalization testing across modalities**: Apply the fine-tuned model to different imaging modalities (CT, MRI) and body regions to evaluate whether the hallucination suppression generalizes beyond chest X-rays, and characterize any degradation in performance.