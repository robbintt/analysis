---
ver: rpa2
title: Personalized Multi-tier Federated Learning
arxiv_id: '2407.14251'
source_url: https://arxiv.org/abs/2407.14251
tags:
- global
- personalized
- rounds
- loss
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PerMFL, a personalized multi-tier federated
  learning framework that addresses statistical heterogeneity by learning customized
  models for teams and devices alongside a global model. The method uses squared Euclidean
  distance regularization to enforce proximity between team and device models and
  their respective parent models.
---

# Personalized Multi-tier Federated Learning

## Quick Facts
- arXiv ID: 2407.14251
- Source URL: https://arxiv.org/abs/2407.14251
- Reference count: 40
- Introduces PerMFL framework with team-level personalization achieving superior performance on multiple benchmark datasets

## Executive Summary
This paper presents PerMFL, a personalized multi-tier federated learning framework that addresses statistical heterogeneity through hierarchical model personalization at both team and device levels. The method introduces squared Euclidean distance regularization to enforce proximity between team and device models and their respective parent models, enabling more efficient local optimization without violating convergence. PerMFL demonstrates both linear convergence for strongly convex problems and sublinear convergence for non-convex problems, with extensive experiments showing superior performance compared to state-of-the-art methods across multiple benchmark datasets.

## Method Summary
PerMFL implements a three-tier federated learning architecture with global, team, and device levels. Each level maintains its own model, with squared Euclidean distance regularization enforcing proximity between parent and child models. The algorithm operates through iterative local optimization at each level: devices perform local gradient steps with regularization, team servers aggregate device updates and perform their own optimization, and the global server aggregates team updates. This hierarchical structure enables efficient personalization while maintaining global model quality, with theoretical guarantees for both strongly convex and non-convex settings.

## Key Results
- Achieves superior performance compared to state-of-the-art methods including FedAvg, pFedMe, Ditto, h-SGD, AL2GD, and DemLearn
- Demonstrates linear convergence for strongly convex problems and sublinear convergence for non-convex problems
- Particularly effective with full participation from both teams and devices, showing faster convergence and better accuracy
- Validated across multiple datasets including MNIST, FMNIST, EMNIST-10, FEMNIST, CIFAR100, and synthetic tabular data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PerMFL's three-tier structure enables more efficient local optimization without violating convergence.
- Mechanism: Squared Euclidean distance regularization between global, team, and device models creates a hierarchy where each level optimizes locally but remains tethered to its parent, avoiding drift that plagues flat FL architectures.
- Core assumption: Regularization parameters (λ, γ) can be tuned to balance personalization and stability.
- Evidence anchors: [abstract], [section 3.2]

### Mechanism 2
- Claim: The Moreau envelope interpretation provides a smooth approximation enabling provable convergence even with inexact gradients.
- Mechanism: Each local optimization subproblem is formulated as a Moreau envelope, which is continuously differentiable and allows gradient-based methods with controlled error bounds.
- Core assumption: Inexact gradient computed via finite local steps approximates true proximal gradient within proven error bounds.
- Evidence anchors: [section 3.2], [section 3.3]

### Mechanism 3
- Claim: Team-level personalization captures statistical heterogeneity more effectively than device-only personalization.
- Mechanism: Grouping devices with similar data distributions into teams allows team models to learn shared characteristics while individual device models specialize further.
- Core assumption: Team formation strategy groups devices with similar data distributions.
- Evidence anchors: [section 3.1], [section 4.1.4]

## Foundational Learning

- Concept: Moreau envelopes and proximal operators
  - Why needed here: Provide mathematical foundation for smoothing non-differentiable objectives and enabling gradient-based optimization in personalized FL
  - Quick check question: What is the relationship between a Moreau envelope and its corresponding proximal operator?

- Concept: Strong convexity and smoothness conditions
  - Why needed here: Essential for proving linear convergence rate in Theorem 1 and ensuring stable optimization
  - Quick check question: How does strong convexity of the Moreau envelope relate to the strong convexity of the original function?

- Concept: Hierarchical optimization and multi-tier architecture
  - Why needed here: Critical for understanding communication patterns and optimization structure for implementing PerMFL correctly
  - Quick check question: What is the difference between communication pattern in conventional FL and multi-tier FL?

## Architecture Onboarding

- Component map: Global server -> Team servers -> Devices
- Critical path:
  1. Global server broadcasts model to teams
  2. Each team server broadcasts to devices
  3. Devices perform L local gradient steps with regularization
  4. Devices send aggregated updates to team servers
  5. Team servers perform K gradient steps with regularization
  6. Team servers send aggregated updates to global server
  7. Global server performs T gradient steps with regularization

- Design tradeoffs:
  - More team iterations (K) improves personalization but increases latency
  - More device iterations (L) improves local accuracy but risks overfitting
  - Regularization parameters (λ, γ) control personalization-global trade-off

- Failure signatures:
  - Poor convergence: Check if L and K meet Ω(T) requirement from theorems
  - Degraded global model: Check if γ is too large relative to λ
  - Overfitting on devices: Check if λ is too small

- First 3 experiments:
  1. Implement basic PerMFL with T=10, K=5, L=5 on MNIST with random team formation
  2. Vary λ and γ to observe personalization-global trade-off
  3. Test with structured vs. random team formation to validate team-level benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PerMFL perform when the number of participating teams is significantly reduced (e.g., 1-2 teams) compared to a larger number of teams?
- Basis in paper: [explicit] The paper discusses impact of team participation on convergence but doesn't specifically address scenarios with very few teams.
- Why unresolved: Ablation study focuses on varying participation percentages (2% to 50%) but doesn't explore effect of minimal number of teams.
- What evidence would resolve it: Experiments comparing PerMFL's performance with 1-2 teams versus 5-10 teams, measuring convergence speed and final accuracy.

### Open Question 2
- Question: Can PerMFL's performance be further improved by dynamically adjusting hyperparameters β, γ, and λ during training based on current convergence state?
- Basis in paper: [inferred] The paper mentions hyperparameters β, γ, and λ exhibit interdependencies and significantly influence learning.
- Why unresolved: Experiments use fixed hyperparameter values and don't explore adaptive strategies.
- What evidence would resolve it: Experiments comparing PerMFL's performance with fixed versus dynamic adjustment strategies, measuring convergence speed and final accuracy.

### Open Question 3
- Question: How does PerMFL perform in scenarios with highly imbalanced data distributions across devices and teams?
- Basis in paper: [explicit] The paper uses datasets with non-IID distributions but doesn't specifically address highly imbalanced scenarios.
- Why unresolved: Experiments use datasets where each device has data from at most two classes, but paper doesn't explore extreme class imbalances.
- What evidence would resolve it: Experiments comparing PerMFL's performance on balanced versus highly imbalanced datasets, measuring convergence speed and final accuracy.

### Open Question 4
- Question: What is the impact of different team formation strategies (e.g., random, based on device similarity, or hierarchical clustering) on PerMFL's performance?
- Basis in paper: [explicit] The paper mentions PerMFL doesn't explicitly address team formation but can accommodate any mechanism.
- Why unresolved: Ablation study only compares worst-case and average-case team formations but doesn't explore other strategies.
- What evidence would resolve it: Experiments comparing PerMFL's performance with different team formation strategies, measuring convergence speed and final accuracy.

## Limitations
- Theoretical guarantees rely heavily on strong convexity and smoothness assumptions that may not hold in practical deep learning scenarios
- Sublinear convergence rate for non-convex problems provides weaker performance guarantees with significant gap between theory and practice
- Experimental evaluation focuses primarily on image classification tasks, limiting generalizability to other domains

## Confidence
- High: Convergence guarantees for strongly convex problems, experimental superiority over baselines
- Medium: Sublinear convergence for non-convex problems, team formation benefits
- Low: Generalizability to non-image datasets, performance under realistic communication constraints

## Next Checks
1. **Convergence under Communication Constraints**: Evaluate PerMFL performance when team-to-global communication is limited (e.g., sparse communication schedules) to validate practical deployment scenarios.

2. **Generalization to NLP Tasks**: Implement PerMFL on a text classification benchmark (e.g., Stack Overflow dataset) to assess performance outside image domains.

3. **Team Formation Robustness**: Systematically test PerMFL with random team formation vs. data-aware clustering to quantify importance of meaningful team structures.