---
ver: rpa2
title: 'TinyVQA: Compact Multimodal Deep Neural Network for Visual Question Answering
  on Resource-Constrained Devices'
arxiv_id: '2404.03574'
source_url: https://arxiv.org/abs/2404.03574
tags: []
core_contribution: TinyVQA is a compact multimodal deep neural network designed for
  Visual Question Answering (VQA) on resource-constrained devices. It integrates vision
  and language modalities for post-disaster damage assessment using the FloodNet dataset.
---

# TinyVQA: Compact Multimodal Deep Neural Network for Visual Question Answering on Resource-Constrained Devices

## Quick Facts
- arXiv ID: 2404.03574
- Source URL: https://arxiv.org/abs/2404.03574
- Authors: Hasib-Al Rashid; Argho Sarkar; Aryya Gangopadhyay; Maryam Rahnemoonfar; Tinoosh Mohsenin
- Reference count: 31
- Primary result: 79.5% accuracy on FloodNet dataset with 339 KB model size, 56 ms latency, and 693 mW power consumption on Crazyflie 2.0 drone

## Executive Summary
TinyVQA is a compact multimodal deep neural network designed specifically for Visual Question Answering (VQA) on resource-constrained devices. The model integrates vision and language modalities for post-disaster damage assessment using the FloodNet dataset. It employs supervised attention-based learning, knowledge distillation, and 8-bit quantization to achieve efficient performance while maintaining high accuracy. Deployed on a Crazyflie 2.0 drone equipped with GAP8 processor, TinyVQA demonstrates practical viability for edge deployment in resource-limited environments with low latency and power consumption.

## Method Summary
TinyVQA integrates vision and language modalities through a multimodal architecture that processes images and text questions for post-disaster damage assessment. The model employs supervised attention-based learning to focus on relevant image regions, knowledge distillation to compress larger models into compact representations, and 8-bit quantization to reduce computational requirements. The architecture is specifically optimized for deployment on the GAP8 processor within the Crazyflie 2.0 drone platform, achieving a balance between accuracy and resource constraints.

## Key Results
- Achieves 79.5% accuracy on FloodNet dataset for post-disaster damage assessment
- Model size of 339 KB suitable for resource-constrained devices
- Low latency of 56 ms and power consumption of 693 mW on Crazyflie 2.0 drone with GAP8 processor

## Why This Works (Mechanism)
Assumption: The combination of supervised attention learning, knowledge distillation, and 8-bit quantization enables TinyVQA to achieve efficient multimodal reasoning while maintaining accuracy. The supervised attention mechanism likely helps the model focus on relevant visual features for answering questions, while knowledge distillation transfers knowledge from larger models to the compact architecture. 8-bit quantization reduces computational overhead without significant accuracy loss, making the system viable for edge deployment on resource-constrained hardware.

## Foundational Learning
1. **Knowledge Distillation**: Transferring knowledge from larger teacher models to compact student models, needed to maintain accuracy while reducing model size; quick check: compare student accuracy with and without distillation
2. **8-bit Quantization**: Reducing model precision from 32-bit floating point to 8-bit integers, needed to minimize memory footprint and computational requirements; quick check: measure accuracy degradation at different quantization levels
3. **Supervised Attention Learning**: Training attention mechanisms with explicit supervision, needed to improve focus on relevant visual regions for VQA tasks; quick check: visualize attention maps on validation samples
4. **Multimodal Integration**: Combining visual and language representations for joint reasoning, needed to enable answering questions about images; quick check: test with single-modality ablations
5. **Edge Deployment Constraints**: Optimizing for specific hardware limitations (GAP8 processor), needed to ensure practical deployment feasibility; quick check: profile power consumption on target hardware
6. **Post-disaster Damage Assessment**: Specialized application domain focusing on flood damage recognition, needed to provide relevant real-world use case; quick check: validate on diverse disaster scenarios

## Architecture Onboarding
**Component Map**: Image Encoder -> Question Encoder -> Multimodal Fusion -> Attention Module -> Classification Head
**Critical Path**: Image and question processing feed into multimodal fusion, which passes through attention mechanisms before final classification
**Design Tradeoffs**: Model prioritizes size and speed over absolute accuracy, accepting 79.5% accuracy for 339 KB size and 56 ms latency
**Failure Signatures**: Potential issues include attention misalignment, quantization artifacts, and generalization failures on non-FloodNet datasets
**First Experiments**:
1. Measure accuracy on standard VQA benchmarks (VQA-v2, GQA) to assess generalization
2. Conduct ablation study comparing performance with individual optimizations disabled
3. Deploy on alternative edge platforms (ARM Cortex-M series) to verify hardware portability

## Open Questions the Paper Calls Out
Unknown: The paper does not explicitly call out open questions in the provided context.

## Limitations
- Narrow application domain focused on FloodNet dataset limits generalizability to other VQA tasks
- No benchmarking against larger models on the same dataset makes accuracy-cost tradeoff unclear
- Power consumption measurements are specific to GAP8 processor and may not generalize to other hardware

## Confidence
- High: Model feasibility on specified Crazyflie 2.0 hardware platform
- High: General methodology for achieving compact VQA models
- Medium: Accuracy metrics and power consumption measurements
- Low: Model generalization to other VQA tasks
- Low: Relative importance of individual optimization techniques

## Next Checks
1. Test TinyVQA on standard VQA benchmarks like VQA-v2 or GQA to assess cross-domain generalization
2. Conduct ablation study to quantify individual contributions of knowledge distillation, quantization, and attention mechanisms
3. Deploy model on alternative resource-constrained platforms (e.g., ARM Cortex-M series) to verify portability and performance consistency