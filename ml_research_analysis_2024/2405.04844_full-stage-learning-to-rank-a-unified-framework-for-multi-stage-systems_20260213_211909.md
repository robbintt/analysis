---
ver: rpa2
title: 'Full Stage Learning to Rank: A Unified Framework for Multi-Stage Systems'
arxiv_id: '2405.04844'
source_url: https://arxiv.org/abs/2405.04844
tags:
- stage
- stages
- items
- system
- ranking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses selection bias in multi-stage information retrieval
  systems, where ranking models at each stage are learned from biased data, leading
  to suboptimal results. The authors propose the Generalized Probability Ranking Principle
  (GPRP), which considers both user interest and selection bias from subsequent stages.
---

# Full Stage Learning to Rank: A Unified Framework for Multi-Stage Systems

## Quick Facts
- arXiv ID: 2405.04844
- Source URL: https://arxiv.org/abs/2405.04844
- Authors: Kai Zheng; Haijun Zhao; Rui Huang; Beichuan Zhang; Na Mou; Yanan Niu; Yang Song; Hongning Wang; Kun Gai
- Reference count: 40
- One-line primary result: FS-LTR improves engagement metrics by +0.69% CTR and +1.08% watch time in online A/B tests on short-video platform

## Executive Summary
Multi-stage information retrieval systems suffer from selection bias where ranking models at each stage are learned from increasingly filtered data, leading to suboptimal final rankings. The paper proposes the Generalized Probability Ranking Principle (GPRP) which considers both user interest and selection bias from subsequent stages. Full Stage Learning to Rank (FS-LTR) is a unified framework that relabels data from all stages to approximate GPRP, using a weighted ranking approach where items from higher stages are considered more important. Extensive experiments show FS-LTR outperforms traditional methods both in simulated environments and online A/B tests.

## Method Summary
FS-LTR addresses selection bias in multi-stage IR systems by first estimating the probability that items survive subsequent stages, then learning ranking models that comply with this downstream selection bias. The method collects data across all stages and relabels each user-item pair based on which stage it was filtered at and whether it received positive feedback. Instead of separately learning user interest and selection bias, FS-LTR directly learns their product through this relabeling strategy. The framework uses any standard LTR algorithm (LambdaRank, RankNet, etc.) trained on the relabeled data to optimize for items most likely to reach users.

## Key Results
- Online A/B tests show +0.69% improvement in click rate and +1.08% increase in watch time
- FS-LTR achieves the highest reveal ratio (16%) among retrieval algorithms in production
- Extensive offline experiments demonstrate superior performance across multiple IR metrics (HR, NDCG, R, P) compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1
The core innovation treats selection bias from subsequent stages as part of the relevance score in upstream stages. Instead of optimizing purely for user interest (r(u,v)), the algorithm learns a combined score wi+1,4(u,v) = pi+1,4(u,v) * r(u,v), where pi+1,4(u,v) is the probability of an item surviving all subsequent stages. This pointwise approximation assumes selection probability depends only on the user-item pair, not the entire candidate set.

### Mechanism 2
The relabeling technique assigns higher importance to items that survive more stages, making the learning process focus on items more likely to reach users. Items are relabeled with values z0 ≤ z1 ≤ ... ≤ z5 based on which stage they appear in and whether they receive positive feedback, with higher-stage items getting higher labels. This is theoretically justified by Assumption 2, which bounds the ratio of user interests between items in different stages by the ratio of their selection probabilities.

### Mechanism 3
FS-LTR unifies the learning of user interest and selection bias into a single optimization, avoiding error accumulation from learning them separately. Instead of separately learning r(u,v) and pi+1,4(u,v) and multiplying them, FS-LTR directly learns the product wi+1,4(u,v) from labeled data. This unified approach is more efficient and theoretically sound when the relabeling accurately reflects the true product.

## Foundational Learning

- Concept: Probability Ranking Principle (PRP)
  - Why needed here: Understanding PRP is essential because GPRP is presented as an extension of PRP for multi-stage systems
  - Quick check question: What is the key assumption of PRP that breaks down in multi-stage systems?

- Concept: Position bias and its correction methods
  - Why needed here: The paper draws an analogy between position bias in single-stage systems and selection bias in multi-stage systems
  - Quick check question: How does selection bias in multi-stage systems differ from position bias in single-stage systems?

- Concept: Learning to Rank (LTR) algorithms
  - Why needed here: FS-LTR uses LTR algorithms as its backbone, and understanding different LTR approaches is crucial for implementation
  - Quick check question: What are the main differences between pointwise, pairwise, and listwise LTR approaches?

## Architecture Onboarding

- Component map: Data collection -> Relabeling module -> LTR model -> Serving component
- Critical path: Data collection → Relabeling → Model training → Serving
- Design tradeoffs:
  - The relabeling scheme trades off between accurately capturing selection bias and computational efficiency
  - Using more data from higher stages improves learning but increases computational cost
  - The choice of LTR algorithm affects both training speed and online performance
- Failure signatures:
  - Poor online performance despite good offline metrics: The relabeling may not accurately reflect true selection bias
  - Slow training: Too much data from higher stages is being used
  - Model not learning: The LTR algorithm may not be suitable for the relabeled data distribution
- First 3 experiments:
  1. Compare FS-LTR with baseline LTR using only exposed data in a single stage
  2. Test different relabeling schemes (varying the z0-z5 values) to see impact on performance
  3. Evaluate the effect of using different amounts of data from each stage on model performance

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed FS-LTR framework perform in a multi-stage system with more than four stages? The paper discusses a four-stage system but does not explore the performance of FS-LTR in systems with more stages.

### Open Question 2
What is the impact of varying the values of c1, c2, c3, and c4 on the performance of FS-LTR? The paper assumes fixed values for these parameters but does not investigate how changes affect FS-LTR's performance.

### Open Question 3
How does FS-LTR handle situations where the selection bias of subsequent stages is not monotonically increasing with respect to the underlying interest r(u, v)? The paper mentions that Assumption 2 may not always hold in real-world systems.

## Limitations

- The theoretical framework relies on Assumption 2, which may not hold in systems with strong personalization where user preferences vary dramatically
- The pointwise approximation of selection bias assumes selection probability depends only on user-item pairs, which could break down with complex filtering criteria
- Results may not generalize beyond short-video recommendation due to specific platform and user behavior patterns

## Confidence

- **High confidence**: Online A/B test results showing +0.69% CTR and +1.08% watch time improvements
- **Medium confidence**: Theoretical framework (GPRP and Assumption 2) relying on idealized assumptions
- **Low confidence**: Generalizability of results beyond short-video recommendation platforms

## Next Checks

1. Test the relabeling strategy on a different multi-stage system (e.g., e-commerce search) to verify Assumption 2 holds across domains
2. Conduct ablation studies removing the selection bias component to quantify its exact contribution versus the unified learning framework
3. Implement the algorithm in a controlled simulation where the true selection probabilities are known to verify the accuracy of the pointwise approximation