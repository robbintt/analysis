---
ver: rpa2
title: 'MAG-V: A Multi-Agent Framework for Synthetic Data Generation and Verification'
arxiv_id: '2412.04494'
source_url: https://arxiv.org/abs/2412.04494
tags:
- trajectory
- questions
- data
- wang
- assistant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MAG-V, a multi-agent framework designed to
  generate synthetic customer queries and verify agent trajectories without relying
  on LLM-as-a-judge. The framework uses an investigator agent to create diverse questions
  mimicking real user queries, an assistant agent to generate responses and trajectories,
  and a reverse engineer agent to create alternate questions from responses.
---

# MAG-V: A Multi-Agent Framework for Synthetic Data Generation and Verification

## Quick Facts
- arXiv ID: 2412.04494
- Source URL: https://arxiv.org/abs/2412.04494
- Reference count: 10
- Outperforms GPT-4o judge baseline by 11% accuracy

## Executive Summary
MAG-V introduces a multi-agent framework that generates synthetic customer queries and verifies agent trajectories without relying on expensive LLM-as-a-judge approaches. The framework uses an investigator agent to create diverse questions, an assistant agent to generate responses and trajectories, and a reverse engineer agent to create alternate questions from responses. Trajectory verification is achieved through traditional ML models trained on engineered features extracted from base and alternate trajectories, achieving accuracy matching GPT-4 and exceeding GPT-4o by 11%. The approach demonstrates that synthetic data can improve agent performance on real customer queries through in-context learning while reducing dependence on costly LLM judgments.

## Method Summary
The MAG-V framework operates through three specialized agents: an investigator agent generates synthetic questions from seed trajectories using in-context learning, an assistant agent answers these questions and produces multi-call trajectories, and a reverse engineer agent creates alternate questions from the assistant's responses. For trajectory verification, the framework extracts six features (EDIT distance, semantic similarity, graph edit distance, argument overlap, longest common subsequence, and execution match) between base trajectories and alternate trajectories generated from the same responses. These features train discriminative ML models (k-NN, Random Forest, etc.) to classify trajectories as correct or incorrect, avoiding the need for LLM-based judgment while maintaining high accuracy.

## Key Results
- Trajectory verification methodology outperforms GPT-4o judge baseline by 11% accuracy
- Matches performance of GPT-4 judge while using traditional ML models
- Synthetic data improves assistant performance on real customer queries through in-context learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using trajectory similarity across multiple alternate trajectories enables robust verification of a base trajectory.
- Mechanism: For each base question, the reverse engineer agent creates alternate questions from the response. The assistant answers these, producing alternate trajectories. Feature vectors (EDIT, SS, etc.) are computed between the base trajectory and each alternate trajectory. These features train a discriminative ML model that classifies the base trajectory as correct or incorrect.
- Core assumption: If alternate questions are generated from the response and answered with similar trajectories, the base trajectory is likely correct; high variance in alternate trajectories indicates low confidence in the base trajectory.
- Evidence anchors:
  - [abstract] "our trajectory verification methodology, inspired by distant supervision and using traditional machine learning (ML) models, outperforms a GPT-4o judge baseline by 11% accuracy and matches the performance of a GPT-4 judge"
  - [section] "To verify the MCT of a question Q, we first ask the reverse engineer agent to create 3 alternate questions (AQ) based on the assistant's response using the MCT"
- Break condition: If alternate questions fail to capture the response semantics accurately, or if alternate trajectories diverge too much due to model instability, the similarity features become unreliable.

### Mechanism 2
- Claim: Traditional ML models with engineered trajectory features can match or exceed LLM-as-a-judge performance for trajectory verification.
- Mechanism: Features like EDIT distance, semantic similarity (SS), and argument overlap (AO) between trajectories provide discriminative signal. k-NN, Random Forest, and other models trained on these features achieve accuracy comparable to GPT-4 and superior to GPT-4o.
- Core assumption: Trajectory similarity metrics contain sufficient information to distinguish correct from incorrect tool call sequences without requiring LLM reasoning.
- Evidence anchors:
  - [abstract] "our trajectory verification methodology, inspired by distant supervision and using traditional machine learning (ML) models, outperforms a GPT-4o judge baseline by 11% accuracy and matches the performance of a GPT-4 judge"
- Break condition: If trajectories become too complex or long, simple distance metrics may lose discriminative power compared to LLM-based semantic understanding.

### Mechanism 3
- Claim: Synthetic data generated by multi-agent systems can improve agent performance on real customer queries through in-context learning.
- Mechanism: Investigator agent generates synthetic questions mimicking customer queries. These questions and their trajectories are added as in-context examples when querying the assistant. This guides the assistant to handle similar real queries correctly.
- Core assumption: Synthetic examples provide sufficient signal to correct systematic errors in the assistant's reasoning, even without parameter updates.
- Evidence anchors:
  - [abstract] "Initial results indicate that our synthetic data can improve agent performance on actual customer queries"
- Break condition: If synthetic examples don't adequately represent the diversity and complexity of real customer queries, in-context learning provides minimal benefit.

## Foundational Learning

- Concept: Trajectory similarity metrics (EDIT distance, semantic similarity, graph edit distance)
  - Why needed here: These metrics form the core features used to train verification models. Understanding their properties helps in feature engineering and debugging.
  - Quick check question: What's the difference between EDIT distance and graph edit distance when applied to tool call sequences?

- Concept: Distant supervision in machine learning
  - Why needed here: The verification approach uses alternate trajectories as "weak labels" to train models without requiring manual annotation of every trajectory.
  - Quick check question: How does distant supervision differ from traditional supervised learning in the context of trajectory verification?

- Concept: In-context learning mechanics
  - Why needed here: The synthetic data improves real query performance through in-context learning, so understanding how examples influence LLM behavior is crucial.
  - Quick check question: What factors determine whether in-context examples successfully guide an LLM's behavior?

## Architecture Onboarding

- Component map:
  - Investigator agent -> Synthetic questions
  - Assistant agent -> Trajectories
  - Reverse engineer agent -> Alternate questions
  - Feature extractor -> Trajectory features
  - ML classifier -> Classification
  - GPT-baseline -> LLM-as-a-judge comparison

- Critical path:
  1. Seed data → Investigator → Synthetic questions
  2. Synthetic questions → Assistant → Trajectories
  3. Base trajectory + Alternate trajectories → Feature extractor → ML model → Classification

- Design tradeoffs:
  - Model choice vs. cost: GPT-4o-mini for data generation vs. GPT-4 for verification
  - Feature complexity vs. performance: Simple EDIT distance vs. full feature set
  - Determinism vs. flexibility: ML models always consistent vs. LLMs show variability

- Failure signatures:
  - Low accuracy despite high feature quality: Feature engineering not capturing discriminative information
  - High variance in alternate trajectories: Model instability or poor question generation
  - ML model overfitting: Too few samples for complex feature space

- First 3 experiments:
  1. Baseline comparison: Run verification with all features vs. using only EDIT distance
  2. Alternate trajectory sensitivity: Vary number of alternate questions (1-5) and measure impact on accuracy
  3. Model robustness: Test k-NN with and without argument features to isolate feature importance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the MAG-V framework scale when generating and verifying trajectories for thousands or millions of customer queries?
- Basis in paper: [inferred] The paper mentions that the framework requires generating three alternate questions and trajectories for each base question, which could lead to high API call costs at scale.
- Why unresolved: The paper only demonstrates the framework on a small dataset of 45 questions. The authors acknowledge that at scale, using expensive LLMs like GPT-4 could accrue significant costs, but do not provide empirical data on performance and cost implications for larger datasets.
- What evidence would resolve it: Systematic experiments scaling the framework to datasets of varying sizes (100, 1,000, 10,000+ questions) with cost and performance metrics would clarify scalability limitations and potential optimizations.

### Open Question 2
- Question: How robust is the trajectory verification method when dealing with partially correct trajectories that contain both correct and incorrect tool calls?
- Basis in paper: [explicit] The authors note during annotation that some "incorrect" trajectories could be marked as correct given the complexity of questions, and they propose exploring a 3-class classification system (correct, partially correct, incorrect) in future work.
- Why unresolved: The current binary classification approach may oversimplify the evaluation of complex trajectories. The paper doesn't explore how well the framework handles trajectories that are neither fully correct nor fully incorrect.
- What evidence would resolve it: Experiments comparing the current binary approach against a 3-class system on a dataset explicitly labeled with partial correctness, along with analysis of common patterns in partially correct trajectories, would clarify the framework's robustness.

### Open Question 3
- Question: How do different embedding methods and feature engineering approaches compare in grounding trajectories to their associated questions for verification?
- Basis in paper: [explicit] The authors acknowledge that TF-IDF features are used to represent questions and suggest exploring better ways to ground trajectories to their questions in future work.
- Why unresolved: While the paper uses TF-IDF features and reports that removing question features doesn't significantly impact performance, it doesn't systematically compare different embedding methods (e.g., BGE, SBERT, custom embeddings) or feature engineering approaches for question-trajectory alignment.
- What evidence would resolve it: Comparative experiments using various question embedding methods and feature combinations, along with ablation studies showing the impact of question features on verification accuracy, would identify optimal approaches for trajectory-question grounding.

## Limitations

- Small seed dataset (19 questions) limits diversity and generalizability of synthetic data
- Binary classification may oversimplify trajectory correctness evaluation
- Reliance on specific trajectory similarity metrics may not capture all forms of correctness

## Confidence

- **High**: The 11% accuracy improvement over GPT-4o baseline is well-supported by experimental results
- **Medium**: Claims about matching GPT-4 performance require further validation on diverse datasets
- **Low**: The effectiveness of synthetic data for in-context learning improvements needs more extensive testing across different domains

## Next Checks

1. **Dataset Scaling Test**: Evaluate the framework's performance when scaling from 19 to 100+ seed questions to assess whether accuracy improvements persist with increased training data diversity

2. **Cross-Domain Transfer**: Apply the framework to a different tool-using agent domain (e.g., code generation or data analysis) to validate generalization beyond customer service queries

3. **Alternative Metric Comparison**: Test whether incorporating additional trajectory features (such as execution timing or resource usage) improves verification accuracy beyond the current six-metric baseline