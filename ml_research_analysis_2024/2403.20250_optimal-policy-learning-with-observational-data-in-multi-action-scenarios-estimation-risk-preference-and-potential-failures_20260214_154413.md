---
ver: rpa2
title: 'Optimal Policy Learning with Observational Data in Multi-Action Scenarios:
  Estimation, Risk Preference, and Potential Failures'
arxiv_id: '2403.20250'
source_url: https://arxiv.org/abs/2403.20250
tags:
- policy
- optimal
- learning
- reward
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper reviews optimal policy learning (OPL) with observational
  data in multi-action scenarios, focusing on estimation, risk preference, and potential
  failures. The author examines three main approaches to estimating the value function:
  regression adjustment, inverse probability weighting, and doubly-robust estimators.'
---

# Optimal Policy Learning with Observational Data in Multi-Action Scenarios: Estimation, Risk Preference, and Potential Failures

## Quick Facts
- arXiv ID: 2403.20250
- Source URL: https://arxiv.org/abs/2403.20250
- Authors: Giovanni Cerulli
- Reference count: 40
- Key outcome: Reviews optimal policy learning with observational data in multi-action scenarios, showing how decision-maker risk preferences influence optimal choice and identifying conditions where decision-making can fail due to violations of unconfoundedness and overlapping assumptions.

## Executive Summary
This paper provides a comprehensive review of optimal policy learning (OPL) with observational data in multi-action scenarios, focusing on three main aspects: estimation methods, risk preferences, and potential failures. The author examines regression adjustment, inverse probability weighting, and doubly-robust estimators for estimating value functions, and demonstrates how decision-maker attitudes toward risk can influence optimal choice through the trade-off between reward conditional mean and variance. The paper also highlights limitations of OPL, showing how violations of fundamental assumptions (unconfoundedness and overlapping) can lead to failures in decision-making, with an application to real data illustrating how average regret depends on the decision-maker's risk attitude.

## Method Summary
The paper examines optimal policy learning with observational data structured as triplets (environment signal, action, reward), where the goal is to estimate the optimal policy π* that maximizes expected reward. Three main estimation approaches are discussed: regression adjustment (RA), inverse probability weighting (IPW), and doubly-robust (DR) estimators. The DR estimator is shown to have theoretical advantages as it only requires correct specification of either the propensity score or the conditional mean, not both. The paper also incorporates risk preferences by introducing utility functions that consider both conditional mean and variance, allowing for risk-averse decision-making. The methodology is illustrated through theoretical analysis and a real data application to job training policy.

## Key Results
- Optimal policy selection depends on decision-maker risk preferences, with risk-averse decision-makers potentially selecting different actions than risk-neutral ones based on the trade-off between reward mean and variance
- Doubly-robust estimators offer theoretical advantages over regression adjustment and inverse probability weighting by requiring correct specification of only one of the two nuisance parameters
- Weak overlap between treatment groups can lead to spurious imputations of counterfactual outcomes and even inverted preference orderings between actions
- Weak unconfoundedness (presence of unmeasured confounders) can result in inconsistent estimates of the mapping between decisions and rewards

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OPL with observational data enables optimal decision-making by estimating the expected reward for each action and selecting the action with the highest expected reward
- Mechanism: The paper leverages the unconfoundedness and overlapping assumptions to identify the counterfactual outcomes, allowing for the estimation of the conditional expected reward for each action. By comparing these estimates, the optimal action can be selected
- Core assumption: The unconfoundedness and overlapping assumptions hold in the data
- Evidence anchors:
  - [abstract] "This paper reviews optimal policy learning (OPL) with observational data in multi-action scenarios, focusing on estimation, risk preference, and potential failures"
  - [section] "Under assumptions A1 and A2, we can prove that (Imbens & Rubin, 2015; Cerulli 2022): µi(j, xi) = E(Yi|Di = j, xi) (9)"
  - [corpus] Weak evidence; corpus neighbors focus on related topics but don't directly address this mechanism
- Break condition: If the unconfoundedness or overlapping assumptions are violated, the estimation of the conditional expected reward becomes biased, leading to suboptimal decisions

### Mechanism 2
- Claim: The optimal choice can be influenced by the decision maker's attitude towards risk, specifically in terms of the trade-off between reward conditional mean and conditional variance
- Mechanism: The paper introduces risk-averse preferences by incorporating the conditional variance into the decision-making process. By considering both the expected reward and its variability, the optimal action can be selected based on the decision maker's risk tolerance
- Core assumption: The decision maker's risk preferences can be modeled using a utility function that incorporates both the conditional mean and variance
- Evidence anchors:
  - [abstract] "This analysis reveals that the optimal choice can be influenced by the decision maker's attitude towards risks, specifically in terms of the trade-off between reward conditional mean and conditional variance"
  - [section] "We assume a risk-averse decision-maker, i.e. one preferring lower levels of risk for a given level of return. A utility function for a risk-averse decision-maker would reflect this preference by assigning a lower utility value to actions with higher levels of risk"
  - [corpus] Weak evidence; corpus neighbors focus on related topics but don't directly address this mechanism
- Break condition: If the decision maker's risk preferences are not accurately modeled or if the conditional variance is not properly estimated, the optimal action selection may not align with the decision maker's true preferences

### Mechanism 3
- Claim: The limitations of OPL arise from the failure of the unconfoundedness and overlapping assumptions, leading to potential failures in decision-making
- Mechanism: The paper discusses the consequences of violating the unconfoundedness and overlapping assumptions. Weak overlap can lead to spurious imputations of the conditional expectation, while weak unconfoundedness can result in inconsistent estimates of the mapping between decisions and rewards
- Core assumption: The unconfoundedness and overlapping assumptions are necessary for the validity of OPL
- Evidence anchors:
  - [abstract] "The third part of the paper discusses the limitations of optimal data-driven decision-making by highlighting conditions under which decision-making can falter. This aspect is linked to the failure of the two fundamental assumptions essential for identifying the optimal choice: (i) overlapping, and (ii) unconfoundedness"
  - [section] "More critically, figure 16 shows an illustrative example of an inverted preference ordering between two actions due to weak overlap"
  - [corpus] Weak evidence; corpus neighbors focus on related topics but don't directly address this mechanism
- Break condition: If the unconfoundedness or overlapping assumptions are violated, the estimates of the conditional expected reward become biased, leading to suboptimal decisions or even inverted preference orderings

## Foundational Learning

- Concept: Unconfoundedness and overlapping assumptions
  - Why needed here: These assumptions are the foundation of OPL with observational data, enabling the identification of counterfactual outcomes and the estimation of the conditional expected reward for each action
  - Quick check question: What are the unconfoundedness and overlapping assumptions, and why are they crucial for OPL with observational data?

- Concept: Risk-averse preferences and utility functions
  - Why needed here: The paper introduces risk-averse preferences by incorporating the conditional variance into the decision-making process. Understanding risk-averse preferences and utility functions is essential for modeling the decision maker's attitude towards risk
  - Quick check question: How can risk-averse preferences be modeled using utility functions that incorporate both the conditional mean and variance?

- Concept: Limitations and potential failures of OPL
  - Why needed here: The paper discusses the consequences of violating the unconfoundedness and overlapping assumptions, highlighting the potential failures in decision-making. Understanding these limitations is crucial for interpreting the results and limitations of OPL
  - Quick check question: What are the potential failures of OPL, and how do they arise from the violation of the unconfoundedness and overlapping assumptions?

## Architecture Onboarding

- Component map:
  - Data -> Assumptions -> Estimation Methods -> Risk Preferences -> Optimal Action Selection -> Limitation Assessment

- Critical path:
  1. Validate the unconfoundedness and overlapping assumptions in the data
  2. Estimate the conditional expected reward for each action using appropriate methods
  3. Incorporate risk preferences using utility functions
  4. Select the optimal action based on the estimated rewards and risk preferences
  5. Assess the limitations and potential failures of the OPL approach

- Design tradeoffs:
  - Bias-variance tradeoff: Different estimation methods (RA, IPW, DR) have different biases and variances
  - Computational complexity: More complex estimation methods may be more accurate but computationally expensive
  - Model misspecification: Incorrect functional forms for the propensity score or conditional mean can lead to biased estimates

- Failure signatures:
  - Inverted preference orderings due to weak overlap
  - Suboptimal decisions due to violation of unconfoundedness or overlapping assumptions
  - Inconsistent estimates of the mapping between decisions and rewards

- First 3 experiments:
  1. Simulate data with known causal effects and test the performance of different estimation methods (RA, IPW, DR) under various scenarios (strong/weak overlap, correct/incorrect functional forms)
  2. Generate synthetic data with different levels of risk aversion and evaluate the impact of incorporating risk preferences on the optimal action selection
  3. Analyze the sensitivity of the OPL approach to violations of the unconfoundedness and overlapping assumptions by introducing confounding variables or reducing overlap in the simulated data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the Doubly-Robust (DR) estimator compare to Regression Adjustment (RA) and Inverse Probability Weighting (IPW) estimators in finite samples when both the propensity score and the conditional mean are misspecified?
- Basis in paper: [explicit] The paper states that "Unlike the RA and IPW approaches, the DR does not require for its consistency that both the propensity score and the conditional mean are simultaneously correctly specified. Only one out of the two must be correctly specified, with the other being potentially also mispecified."
- Why unresolved: The paper does not provide a direct comparison of the finite sample performance of the DR estimator when both nuisance parameters are misspecified, compared to the RA and IPW estimators
- What evidence would resolve it: Simulation studies or empirical applications comparing the finite sample performance of the DR, RA, and IPW estimators under different degrees of misspecification of the propensity score and conditional mean models

### Open Question 2
- Question: How does the optimal choice in multi-action scenarios change under different risk preferences (linear vs. quadratic) in the presence of weak overlap?
- Basis in paper: [explicit] The paper discusses the impact of risk preferences on optimal choice selection and presents an example of how weak overlap can lead to an inverted preference ordering between two actions
- Why unresolved: The paper does not provide a detailed analysis of how the optimal choice changes under different risk preferences in the presence of weak overlap, particularly in multi-action scenarios
- What evidence would resolve it: Theoretical analysis and empirical simulations comparing the optimal choices under linear and quadratic risk preferences in multi-action scenarios with varying degrees of overlap

### Open Question 3
- Question: What are the most effective methods for addressing weak unconfoundedness in observational data, particularly when collecting more data on potential confounders is not feasible?
- Basis in paper: [explicit] The paper discusses the limitations of OPL due to weak unconfoundedness and presents several potential solutions, including collecting more data, using methods robust to unobservable selection, sensitivity analysis, and sensible assumptions
- Why unresolved: The paper does not provide a comprehensive evaluation of the effectiveness of these methods in addressing weak unconfoundedness in practice, nor does it discuss the trade-offs between these approaches
- What evidence would resolve it: Empirical studies comparing the performance of different methods for addressing weak unconfoundedness in various contexts, along with a discussion of the strengths and limitations of each approach

## Limitations

- The paper relies heavily on theoretical analysis and illustrative examples rather than empirical validation of the proposed mechanisms and their implications
- The confidence in the mechanisms described is primarily based on claims made in the reviewed paper and related work, lacking direct empirical validation or mathematical proofs
- The paper acknowledges limitations related to the fundamental assumptions of OPL (unconfoundedness and overlapping), which can lead to biased estimates and suboptimal decisions when violated

## Confidence

- **High confidence**: The basic framework of OPL with observational data and the three main estimation approaches (RA, IPW, DR) are well-established in the literature
- **Medium confidence**: The incorporation of risk preferences and the potential impact on optimal action selection is a novel contribution, but requires further empirical validation
- **Low confidence**: The specific failure modes and their implications are discussed, but the evidence provided is largely theoretical and illustrative rather than empirical

## Next Checks

1. **Empirical validation of risk preference impact**: Conduct a simulation study to quantify the impact of different risk preferences on the optimal action selection and compare it to theoretical predictions
2. **Robustness to assumption violations**: Analyze the sensitivity of the OPL approach to violations of the unconfoundedness and overlapping assumptions using simulated data with known causal effects
3. **Comparison of estimation methods**: Evaluate the performance of the three estimation methods (RA, IPW, DR) in terms of bias, variance, and rate of convergence under various scenarios (strong/weak overlap, correct/incorrect functional forms)