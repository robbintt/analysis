---
ver: rpa2
title: Pre-trained Large Language Models Use Fourier Features to Compute Addition
arxiv_id: '2406.03445'
source_url: https://arxiv.org/abs/2406.03445
tags:
- fourier
- components
- logits
- features
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how pre-trained large language models (LLMs)
  perform addition tasks by analyzing their internal representations. The authors
  show that LLMs compute addition using Fourier features - dimensions in the hidden
  state that represent numbers via features sparse in the frequency domain.
---

# Pre-trained Large Language Models Use Fourier Features to Compute Addition

## Quick Facts
- arXiv ID: 2406.03445
- Source URL: https://arxiv.org/abs/2406.03445
- Authors: Tianyi Zhou, Deqing Fu, Vatsal Sharan, Robin Jia
- Reference count: 40
- Primary result: Pre-trained LLMs compute addition using Fourier features, achieving 99.74% accuracy vs 94.44% for models trained from scratch

## Executive Summary
This paper demonstrates that pre-trained large language models perform addition by computing with Fourier features - dimensions in the hidden state that represent numbers through sparse frequency domain representations. The authors show that MLP layers primarily use low-frequency Fourier components for magnitude approximation, while attention layers use high-frequency components for modular operations like determining even/odd and mod 10. Critically, pre-training is essential for this mechanism: token embeddings from pre-trained models contain outlier Fourier components with periods 2, 5, and 10, which enables the model to learn precise addition algorithms. Without pre-trained embeddings, models achieve significantly lower accuracy (94.44% vs 99.74%). The same Fourier feature mechanism is present in both fine-tuned and frozen pre-trained models when prompted with arithmetic problems.

## Method Summary
The authors fine-tune GPT-2-XL (48-layer, 1.5B parameter decoder-only Transformer) on a synthetic addition dataset with 5 templates like "Put together 15 and 93. Answer: " for number pairs 0-260, using an 80/10/10 train/validation/test split. They use AdamW optimizer with learning rate 1e-5 for 50 epochs with batch size 16. To analyze the internal mechanisms, they apply Logit Lens to extract intermediate predictions, perform Fourier analysis on token embeddings and intermediate logits, and use frequency filters to ablate low/high frequency components in MLP and attention outputs separately.

## Key Results
- Pre-trained LLMs use Fourier features (sparse in frequency domain) to compute addition, with periods 2, 5, and 10 in token embeddings
- MLP layers primarily use low-frequency features for magnitude approximation, while attention layers use high-frequency features for modular operations
- Models trained from scratch achieve only 94.44% accuracy, while pre-trained models achieve 99.74%
- Introducing pre-trained token embeddings to randomly initialized models rescues performance to 100% accuracy
- The Fourier feature mechanism is present in both fine-tuned and frozen pre-trained models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-trained LLMs compute addition using Fourier features that represent numbers via sparse frequency domain representations.
- Mechanism: The model represents numbers as combinations of sine and cosine waves with specific periods (2, 5, 10, etc.). These Fourier components are sparse in the frequency domain, meaning only certain frequencies have significant magnitude. MLP layers primarily use low-frequency components to approximate the magnitude of answers, while attention layers use high-frequency components for modular operations.
- Core assumption: Fourier features are learned during pre-training and become embedded in token representations.
- Evidence anchors:
  - [abstract] "This paper shows that pre-trained LLMs add numbers using Fourier features—dimensions in the hidden state that represent numbers via a set of features sparse in the frequency domain."
  - [section 3.2] "We apply the discrete Fourier transform to represent the logits as the sum of sine and cosine waves of different periods" and "high-frequency components in Fourier space...are approximately sparse"

### Mechanism 2
- Claim: Different model components (MLP vs attention) have complementary roles using different frequency ranges.
- Mechanism: MLP layers primarily perform magnitude approximation using low-frequency Fourier components (periods like 520), while attention layers perform modular addition using high-frequency components (periods like 2, 5, 10). This division of labor allows the model to first estimate the answer's magnitude, then refine it to the exact value.
- Core assumption: The model can learn to separate approximation and classification tasks across different layers.
- Evidence anchors:
  - [section 3.3] "MLP layers primarily approximate the magnitude of the answer using low-frequency features, while attention layers primarily perform modular addition...using high-frequency features"
  - [section 3.2] "MLP layers contribute primarily to approximation, whereas attention layers contribute primarily to classification"

### Mechanism 3
- Claim: Pre-training is essential because it provides token embeddings with Fourier features that enable precise addition mechanisms.
- Mechanism: During pre-training, token embeddings learn to represent numbers using Fourier features with periods 2, 5, and 10. These pre-trained embeddings provide the inductive bias needed for the model to learn the Fourier-based addition mechanism. Without pre-trained embeddings, models trained from scratch achieve much lower accuracy (94.44% vs 99.74%).
- Core assumption: Pre-training naturally leads to Fourier feature representations in token embeddings.
- Evidence anchors:
  - [section 4.2] "models trained from scratch to add numbers only exploit low-frequency features, leading to lower accuracy (94.44% vs 99.74%)"
  - [section 4.1] "Across the pre-trained token embeddings of many different pre-trained models, Fourier analysis uncovers large magnitudes of components with periods 2, 5, and 10"

## Foundational Learning

- Concept: Discrete Fourier Transform (DFT)
  - Why needed here: The paper uses DFT to analyze model representations in frequency domain and identify Fourier features
  - Quick check question: What is the relationship between period T and frequency f in the Fourier basis used here?

- Concept: Residual stream architecture in Transformers
  - Why needed here: Understanding how token representations accumulate additive updates layer by layer is crucial for analyzing intermediate predictions
  - Quick check question: How does the residual connection formula h(ℓ) = h(ℓ-1) + Attn(ℓ) + MLP(ℓ) enable progressive computation?

- Concept: Modular arithmetic and its computational requirements
  - Why needed here: The paper distinguishes between magnitude approximation and modular operations (even/odd, mod 10) in addition computation
  - Quick check question: Why would high-frequency Fourier components be particularly useful for determining whether a number is even or odd?

## Architecture Onboarding

- Component map: Input token → token embedding (with Fourier features) → layer 1 (Attn + MLP) → ... → layer 48 (Attn + MLP) → output projection → softmax prediction

- Critical path: The critical path for addition computation is through the residual stream where Fourier features are progressively refined from magnitude approximation to exact answer

- Design tradeoffs: The model trades off between approximation (requiring coarse frequency resolution) and classification (requiring fine frequency resolution). MLP layers prioritize low-frequency components for magnitude estimation, while attention layers prioritize high-frequency components for precise modular operations

- Failure signatures: If Fourier features are absent, the model will show uniform logit distributions without periodic patterns, achieve lower accuracy (~94% vs ~99%), and produce errors that are multiples of 10 or 100. If frequency separation fails, ablating low or high frequencies will impact both magnitude and modular operations

- First 3 experiments:
  1. Apply Logit Lens to extract intermediate predictions and verify progressive refinement from magnitude approximation to exact answer
  2. Perform Fourier analysis on intermediate logits to identify sparse high-frequency components and verify periods around 2, 5, 10
  3. Apply frequency filters (low-pass/high-pass) to MLP and attention outputs separately to confirm their complementary roles in approximation vs classification

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do pre-trained token embeddings specifically develop Fourier features for numbers, and what aspects of the pre-training process are crucial for this development?
- Basis in paper: [explicit] The paper states that pre-trained token embeddings have outlier Fourier components with periods 2, 5, and 10, and that introducing these embeddings when training from scratch rescues performance.
- Why unresolved: The paper does not explain the mechanism by which pre-training leads to Fourier features in token embeddings, nor does it identify which aspects of pre-training (e.g., specific datasets, objectives, or architectural choices) are responsible.
- What evidence would resolve it: Experiments varying pre-training data, objectives, or architectures to isolate which factors induce Fourier features in number embeddings, combined with theoretical analysis of how these features emerge during pre-training.

### Open Question 2
- Question: Can the Fourier feature mechanism be generalized to other arithmetic operations beyond addition and multiplication, such as subtraction, division, or exponentiation?
- Basis in paper: [inferred] The paper demonstrates Fourier features in addition and multiplication tasks but does not explore other operations.
- Why unresolved: The paper only examines two arithmetic operations, leaving open whether the Fourier mechanism is a general feature of mathematical reasoning in LLMs or specific to certain operations.
- What evidence would resolve it: Systematic analysis of Fourier features in LLMs across various arithmetic operations, comparing the presence and role of Fourier components in each case.

### Open Question 3
- Question: What is the relationship between the Fourier features in token embeddings and the emergence of modular arithmetic capabilities in LLMs?
- Basis in paper: [explicit] The paper shows that high-frequency Fourier components in attention layers perform modular addition, while pre-trained embeddings have components with periods 2, 5, and 10.
- Why unresolved: While the paper identifies a correlation between Fourier features and modular arithmetic, it does not establish a causal relationship or explain how the token embedding features enable modular computation.
- What evidence would resolve it: Experiments ablating or modifying Fourier components in token embeddings and observing effects on modular arithmetic performance, combined with analysis of how these components interact with attention mechanisms.

### Open Question 4
- Question: How does the Fourier feature mechanism scale with model size, and are there diminishing returns or qualitative changes in larger models?
- Basis in paper: [explicit] The paper analyzes models ranging from GPT-2-small to GPT-4, showing similar Fourier feature patterns, but does not systematically study scaling effects.
- Why unresolved: The paper shows Fourier features exist across different model sizes but does not investigate whether the mechanism changes or becomes more/less prominent as models scale.
- What evidence would resolve it: Comparative analysis of Fourier features across a wide range of model sizes, examining how the strength, distribution, and role of Fourier components change with scale.

## Limitations

- Task specificity: Analysis focuses exclusively on addition of numbers ≤ 260, leaving behavior for other arithmetic operations unknown
- Architecture dependence: Findings based on GPT-2-XL may not generalize to other Transformer architectures
- Pre-training dataset influence: Paper doesn't analyze how different pre-training corpora affect Fourier feature emergence

## Confidence

**High confidence**: Claims about existence of Fourier features in pre-trained embeddings (periods 2, 5, 10) and their role in achieving high addition accuracy (99.74% vs 94.44% for scratch models).

**Medium confidence**: Claims about complementary roles of MLP (low-frequency, magnitude) vs attention (high-frequency, modular) layers. While supported by experiments, the mechanistic explanation for this division is somewhat speculative.

**Low confidence**: Claims about pre-training being the sole source of Fourier features. The paper shows pre-training provides these features, but doesn't rule out that careful initialization or architectural biases could achieve similar results.

## Next Checks

1. Apply the same Fourier analysis to different Transformer architectures (BERT, OPT, LLaMA) to verify if the MLP/high-frequency attention division generalizes beyond GPT-2-XL

2. Systematically ablate different frequency ranges (not just binary low/high) and measure the impact on both magnitude estimation and modular operations to verify the continuous nature of the frequency separation

3. Initialize token embeddings with random Fourier features (periods 2, 5, 10) and train from scratch to determine if the features themselves, rather than pre-training, are the key factor