---
ver: rpa2
title: Don't Touch My Diacritics
arxiv_id: '2410.24140'
source_url: https://arxiv.org/abs/2410.24140
tags:
- text
- diacritics
- language
- normalization
- unicode
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Preprocessing text with diacritics can degrade downstream NLP model
  performance if diacritic encoding is inconsistent or diacritics are removed. Inconsistent
  Unicode normalization leads to treating visually identical words as distinct, while
  stripping diacritics removes meaningful distinctions, harming tasks like POS tagging,
  dependency parsing, and diacritization.
---

# Don't Touch My Diacritics

## Quick Facts
- arXiv ID: 2410.24140
- Source URL: https://arxiv.org/abs/2410.24140
- Reference count: 5
- Preprocess text with consistent Unicode normalization and preserve diacritics to improve multilingual NLP model performance.

## Executive Summary
Inconsistent Unicode normalization and removal of diacritics during text preprocessing can degrade downstream NLP model performance. When diacritic encoding is inconsistent, visually identical words are treated as distinct, inflating vocabulary and fragmenting representations. Stripping diacritics removes meaningful phonological and morphological distinctions, leading to ambiguity and reduced task performance. Applying a consistent Unicode normalization form (NFC, NFD, NFKC, or NFKD) and preserving diacritics can mitigate these issues. For example, normalizing Hindi data improved dependency parsing LAS by 0.29 (2.25% relative error reduction). The authors call for adopting these practices to improve multilingual NLP equity.

## Method Summary
The authors demonstrate that inconsistent Unicode normalization and diacritic stripping degrade NLP model performance by causing visually identical words to be treated as distinct and removing meaningful phonological/morphological distinctions. They advocate for applying a consistent Unicode normalization form (NFC, NFD, NFKC, or NFKD) and preserving diacritics during preprocessing. The Hindi Dependency Treebank was used to show a 0.29 LAS improvement in dependency parsing through normalization. The method involves normalizing all training, validation, and inference text, preserving diacritics unless there is strong linguistic justification for removal, and validating that normalization does not introduce new ambiguities.

## Key Results
- Inconsistent Unicode normalization causes visually identical diacritized characters to be treated as distinct tokens, fragmenting vocabulary and degrading model performance.
- Stripping diacritics removes contrastive phonological and morphological distinctions, leading to ambiguity and reduced task performance in POS tagging, dependency parsing, and diacritization.
- Applying a consistent Unicode normalization form and preserving diacritics improved Hindi dependency parsing LAS by 0.29 (2.25% relative error reduction).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Inconsistent Unicode normalization causes visually identical diacritized characters to be treated as distinct tokens, fragmenting vocabulary and degrading model performance.
- Mechanism: When preprocessing does not apply a consistent normalization form, two sequences representing the same visual character (e.g., precomposed vs decomposed forms) are encoded as separate tokens. This inflates vocabulary size and forces the model to learn redundant representations for the same linguistic unit.
- Core assumption: The model's tokenizer or vocabulary does not collapse equivalent Unicode sequences during training.
- Evidence anchors:
  - [abstract] "Inconsistent Unicode normalization leads to treating visually identical words as distinct"
  - [section 3] "without normalization ⟨é⟩ and ⟨é⟩, and ⟨ड़⟩ and ⟨ड़⟩, are considered unequal by ordinary string comparison methods"
  - [corpus] Weak evidence: no explicit corpus metrics on token fragmentation provided; inference drawn from experimental performance drop.
- Break condition: The tokenizer applies Unicode normalization before tokenization, or the model's vocabulary explicitly merges equivalent forms.

### Mechanism 2
- Claim: Stripping diacritics removes contrastive phonological and morphological distinctions, leading to ambiguity and reduced task performance.
- Mechanism: Diacritics often encode lexical tone, vowel quality, or morphological markers. Removing them collapses distinct words into a single form (e.g., Greek νόμος vs νομός), forcing the model to infer distinctions from limited context or fail entirely.
- Core assumption: The target language uses diacritics to encode phonemic or morphological distinctions that are not recoverable from surrounding context.
- Evidence anchors:
  - [abstract] "stripping diacritics removes meaningful distinctions, harming tasks like POS tagging, dependency parsing, and diacritization"
  - [section 4] "Without these accents, ambiguities arise; e.g., νόμος ‘law, ordinance’ vs. νομός ‘county, district’"
  - [corpus] Weak evidence: no explicit corpus frequency analysis of diacritic-induced ambiguities provided; inference from reported task degradation.
- Break condition: The language's orthography is largely non-contrastive for diacritics, or the task does not require fine-grained morphological or phonological distinctions.

### Mechanism 3
- Claim: Inconsistent diacritic ordering within multigraphs (e.g., Arabic harakat before shadda) creates duplicate representations that harm model robustness.
- Mechanism: Unicode defines a canonical order for combining marks. When preprocessing does not enforce this, the same grapheme sequence can appear in multiple orders, causing the model to treat them as different tokens and reducing generalization.
- Core assumption: The script in use allows multiple diacritics on a single base character and Unicode's canonical ordering is not applied during preprocessing.
- Evidence anchors:
  - [section 2] "Unicode also defines a canonical ordering for sequences of diacritics... According to Unicode’s canonical ordering, the ḥarakāt precede shaddah."
  - [section 3] "Canonic ordering can be enforced by converting text to NFD or NFKD normalization forms."
  - [corpus] Weak evidence: no explicit corpus examples of non-canonical ordering beyond Table 1; inference from Arabic diacritization literature.
- Break condition: The script rarely combines multiple diacritics, or the preprocessing pipeline applies NFD/NFKD normalization before tokenization.

## Foundational Learning

- Concept: Unicode normalization forms (NFC, NFD, NFKC, NFKD)
  - Why needed here: These forms ensure consistent encoding of diacritized characters, preventing the model from learning redundant or conflicting representations of the same linguistic unit.
  - Quick check question: If you have the string "é" (U+00E9) and "e\u0301" (U+0065 U+0301), which normalization form will convert both to the same byte sequence?

- Concept: Diacritic stripping vs. diacritic preservation
  - Why needed here: Stripping removes phonological and morphological cues encoded in diacritics, while preservation maintains these distinctions and reduces ambiguity for the model.
  - Quick check question: In Greek, how many distinct words could be collapsed into one form if acute accents are stripped from "νόμος" and "νομός"?

- Concept: Script-specific diacritic usage patterns
  - Why needed here: Some scripts (e.g., Arabic, Hebrew) rely on diacritics for disambiguation, while others (e.g., Polish) use them for contrastive phonology; knowing this guides preprocessing decisions.
  - Quick check question: In Polish, does the character "ł" decompose into "l" plus a diacritic under Unicode normalization?

## Architecture Onboarding

- Component map:
  Raw text ingestion -> Unicode normalization (NFC/NFD) -> Tokenization -> Vocabulary building -> Model training
  Critical auxiliary components: Validation scripts for diacritic consistency, logging of normalization choices, language-specific diacritic usage profiling

- Critical path:
  1. Apply a consistent Unicode normalization (NFC or NFD) to all training, validation, and inference text.
  2. Preserve diacritics unless there is a strong linguistic justification for removal.
  3. Validate that normalization does not introduce new ambiguities (e.g., by sampling post-normalized text).
  4. Rebuild tokenizer vocabulary to reflect the normalized, diacritic-preserving text.

- Design tradeoffs:
  - NFC (precomposed) vs. NFD (decomposed): NFC may reduce token count but can increase byte length; NFD may increase token count but keep byte length stable.
  - Preserving diacritics increases vocabulary size and may require larger models, but reduces ambiguity and improves task performance in diacritic-rich languages.

- Failure signatures:
  - Performance degradation correlated with inconsistent diacritic encoding in the corpus.
  - High out-of-vocabulary (OOV) rates for visually identical but differently encoded words.
  - Unexpected model behavior on diacritic-rich languages (e.g., poor POS tagging or dependency parsing).

- First 3 experiments:
  1. Re-run dependency parsing on Hindi data with and without NFC normalization; measure LAS difference.
  2. Train a Greek POS tagger on text with diacritics preserved vs. stripped; compare accuracy.
  3. Build a tokenizer for Arabic text using NFD normalization and validate that harakat/shadda ordering is canonical.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do inconsistent diacritic encodings affect performance in other NLP tasks beyond dependency parsing, such as named entity recognition or machine translation?
- Basis in paper: [explicit] The authors mention that inconsistent encodings lead to degradation in downstream performance, but focus primarily on dependency parsing as a case study.
- Why unresolved: The paper provides detailed analysis for dependency parsing but does not extend the investigation to other tasks, leaving uncertainty about the breadth of the impact.
- What evidence would resolve it: Empirical studies measuring task-specific performance degradation across multiple NLP tasks when diacritic encoding is inconsistent.

### Open Question 2
- Question: What are the computational costs and practical challenges of implementing consistent Unicode normalization in large-scale NLP pipelines?
- Basis in paper: [inferred] The authors advocate for consistent normalization but do not discuss the practical implementation challenges or computational overhead in real-world systems.
- Why unresolved: While normalization is described as "conceptually simple and computationally efficient," the paper lacks details on integration challenges in production environments.
- What evidence would resolve it: Case studies or benchmarks comparing preprocessing time and resource usage before and after normalization in production NLP systems.

### Open Question 3
- Question: How does the removal of diacritics during pre-training affect model robustness to diacritized inputs during inference?
- Basis in paper: [explicit] The authors note that models pre-trained on stripped text often struggle with diacritized inputs, but the mechanisms and extent of this degradation are not fully explored.
- Why unresolved: The paper highlights the issue but does not provide a detailed analysis of how pre-training on stripped text impacts model generalization to diacritized data.
- What evidence would resolve it: Controlled experiments comparing model performance on diacritized vs. stripped inputs across different pre-training regimes.

### Open Question 4
- Question: Are there language-specific or script-specific best practices for handling diacritics in preprocessing, beyond the general recommendations provided?
- Basis in paper: [inferred] The authors discuss diacritic handling broadly but do not address whether different scripts (e.g., Arabic, Hebrew, Vietnamese) require tailored approaches.
- Why unresolved: The paper treats diacritic handling as a universal problem but does not explore whether script-specific nuances necessitate different preprocessing strategies.
- What evidence would resolve it: Comparative studies analyzing the impact of diacritic handling across diverse scripts and languages, identifying script-specific best practices.

## Limitations
- Limited empirical evidence on corpus-level effects of inconsistent normalization and diacritic stripping; most claims are inferred from task performance degradation rather than measured token-level fragmentation or ambiguity rates.
- Mixed results across languages: while Hindi and Greek benefited from diacritic preservation, Vietnamese saw no improvement, indicating the impact depends on the script's reliance on diacritics for contrast.
- The choice between NFC, NFD, NFKC, or NFKD normalization forms is not fully justified for all scripts, and the authors do not provide script-specific recommendations.

## Confidence
- **High**: Claims about the necessity of consistent Unicode normalization and the general benefit of preserving diacritics for diacritic-rich languages.
- **Medium**: Claims about script-specific mechanisms (e.g., Arabic diacritic ordering, Greek ambiguity) due to limited corpus-level evidence.
- **Low**: Claims about the universal benefit of preserving diacritics across all languages, given mixed results (e.g., Vietnamese).

## Next Checks
1. Measure the frequency of encoding inconsistencies and diacritic-induced ambiguities in the raw corpora used for Hindi, Greek, and Arabic experiments.
2. Conduct ablation studies across a broader set of languages (e.g., Vietnamese, Polish, Turkish) to quantify when diacritic preservation helps or hinders performance.
3. Evaluate the impact of different Unicode normalization forms (NFC vs. NFD) on vocabulary size and model performance across multiple scripts.