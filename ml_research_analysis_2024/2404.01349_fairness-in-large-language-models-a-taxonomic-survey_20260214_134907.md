---
ver: rpa2
title: 'Fairness in Large Language Models: A Taxonomic Survey'
arxiv_id: '2404.01349'
source_url: https://arxiv.org/abs/2404.01349
tags:
- arxiv
- bias
- language
- fairness
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive survey on fairness in large
  language models (LLMs), addressing the growing concern of bias in these models.
  The authors categorize bias metrics into embedding-based, probability-based, and
  generation-based, and bias mitigation techniques into pre-processing, in-training,
  intra-processing, and post-processing methods.
---

# Fairness in Large Language Models: A Taxonomic Survey

## Quick Facts
- arXiv ID: 2404.01349
- Source URL: https://arxiv.org/abs/2404.01349
- Reference count: 40
- Primary result: Comprehensive survey categorizing fairness metrics and mitigation techniques for large language models

## Executive Summary
This paper provides a comprehensive survey on fairness in large language models (LLMs), addressing the growing concern of bias in these models. The authors categorize bias metrics into embedding-based, probability-based, and generation-based types, and bias mitigation techniques into pre-processing, in-training, intra-processing, and post-processing methods. They also summarize available toolkits and datasets for evaluating bias. The survey highlights the need for developing more tailored datasets and methods to balance performance and fairness in LLMs, as well as addressing multiple types of fairness simultaneously.

## Method Summary
The survey systematically reviews existing literature on fairness in LLMs, organizing the findings into a taxonomic structure. It categorizes bias metrics based on the data format used (embeddings, probabilities, or generations) and mitigation techniques according to the stage in the LLM workflow where they are applied. The authors compile and summarize relevant toolkits and datasets for bias evaluation, providing a comprehensive resource for researchers and practitioners. The methodology involves extensive literature analysis to identify current approaches, challenges, and future directions in fair LLM development.

## Key Results
- Bias metrics are categorized into embedding-based, probability-based, and generation-based types, each targeting different stages of LLM processing
- Mitigation techniques are organized into four stages: pre-processing, in-training, intra-processing, and post-processing
- The survey identifies the need for more tailored datasets and methods to balance performance and fairness in LLMs
- Current evaluation resources are compiled and categorized for practical implementation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The survey's taxonomic structure enables systematic bias detection by categorizing fairness metrics into embedding-based, probability-based, and generation-based types, each addressing different data representations in LLMs.
- Mechanism: By mapping metrics to specific data formats (embeddings, probabilities, generations), the survey provides a clear framework for selecting appropriate evaluation tools based on the model's stage of processing, ensuring comprehensive bias coverage.
- Core assumption: Different stages of LLM processing (embedding, probability calculation, text generation) exhibit distinct bias characteristics that require tailored measurement approaches.
- Evidence anchors:
  - [abstract] "The authors categorize bias metrics into embedding-based, probability-based, and generation-based..."
  - [section] "Regarding metrics for quantifying biases in LLMs, they are further categorized based on the data format used by metrics: i) embedding-based metrics, ii) probability-based metrics, and iii) generation-based metrics."
- Break condition: If a single metric could capture all bias types across stages, the multi-category approach would be redundant.

### Mechanism 2
- Claim: The four-stage categorization of bias mitigation techniques (pre-processing, in-training, intra-processing, post-processing) provides a complete pipeline for addressing bias at every stage of LLM development and deployment.
- Mechanism: Each mitigation stage targets bias at a specific point in the LLM workflow, allowing practitioners to implement appropriate interventions based on their access level to the model and data.
- Core assumption: Bias can be introduced or amplified at multiple points in the LLM development pipeline, requiring targeted interventions at each stage.
- Evidence anchors:
  - [abstract] "bias mitigation techniques into pre-processing, in-training, intra-processing, and post-processing methods."
  - [section] "Concerning bias mitigation techniques, they are structured according to the different stages within the LLMs workflow: i) pre-processing, ii) in-training, iii) intra-processing, and iv) post-processing."
- Break condition: If bias were primarily introduced at only one stage, the multi-stage approach would be unnecessarily complex.

### Mechanism 3
- Claim: The survey's comprehensive resource compilation (toolkits and datasets) accelerates practical implementation of fairness evaluation by providing ready-to-use tools and benchmarks.
- Mechanism: By organizing resources into Toolkits and Datasets categories, the survey enables practitioners to quickly identify and deploy appropriate evaluation infrastructure without extensive research overhead.
- Core assumption: Access to standardized evaluation tools and datasets is a critical bottleneck in LLM fairness research and implementation.
- Evidence anchors:
  - [abstract] "They also summarize available toolkits and datasets for evaluating bias."
  - [section] "Subsequently, Section 7 summarizes existing datasets and related toolkits."
- Break condition: If custom-built evaluation tools were consistently superior to existing resources, the compilation approach would be less valuable.

## Foundational Learning

- Concept: LLM architecture and training pipeline
  - Why needed here: Understanding how LLMs process data (embeddings → probabilities → generation) is essential for grasping why different bias metrics target different stages
  - Quick check question: Why can't a single fairness metric evaluate all types of bias in LLMs?

- Concept: Types of bias in machine learning
  - Why needed here: The survey builds on traditional ML fairness concepts but adapts them for LLMs' unique characteristics, requiring understanding of both domains
  - Quick check question: How does individual fairness in LLMs differ from traditional ML?

- Concept: Counterfactual data augmentation
  - Why needed here: This technique is frequently mentioned as a bias mitigation strategy, and understanding its mechanics is crucial for implementing pre-processing interventions
  - Quick check question: What is the limitation of simple word substitution in counterfactual data augmentation?

## Architecture Onboarding

- Component map: LLM background → Fairness definitions → Bias metrics (embedding-based, probability-based, generation-based) → Mitigation techniques (pre-processing, in-training, intra-processing, post-processing) → Resources (toolkits + datasets) → Challenges/future directions
- Critical path: Understanding bias metrics → selecting appropriate mitigation techniques → deploying evaluation resources → addressing implementation challenges
- Design tradeoffs: Comprehensive coverage vs. depth in each category; academic survey vs. practical implementation guide
- Failure signatures: Over-reliance on probability-based metrics for generation-stage bias; applying pre-processing techniques to post-processing problems
- First 3 experiments:
  1. Implement WEAT (Word Embedding Association Test) on a pre-trained LLM to detect gender bias in embeddings
  2. Apply counterfactual data augmentation to a biased dataset and measure the impact on model outputs
  3. Use Perspective API to evaluate toxicity in LLM-generated text from different demographic prompts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop more rational counterfactual data augmentation strategies to avoid generating illogical or anomalous data that could detrimentally impact model performance?
- Basis in paper: [explicit] The paper mentions that straightforward replacement in counterfactual data augmentation may result in unnatural or irrational sentences, compromising model quality. It suggests exploring more rational replacement strategies or integrating alternative techniques to filter or optimize the generated data.
- Why unresolved: Existing counterfactual data augmentation methods often rely on simple attribute word substitutions, which can lead to inconsistencies and illogical data. Developing more sophisticated strategies requires further research into natural language processing and data generation techniques.
- What evidence would resolve it: Empirical studies comparing the performance of models trained on data augmented with rational counterfactual strategies versus those trained on data augmented with traditional methods, demonstrating improved model performance and fairness without compromising data quality.

### Open Question 2
- Question: What systematic approaches can be developed to achieve a balanced trade-off between model performance and fairness in large language models, considering the high costs associated with training?
- Basis in paper: [explicit] The paper highlights the challenge of balancing performance and fairness in LLMs, noting that it often involves manually tuning the optimal trade-off parameter. It emphasizes the need for systematic methods to achieve this balance, given the high costs of training LLMs.
- Why unresolved: Current methods for balancing performance and fairness rely on manual tuning, which is time-consuming and resource-intensive. Developing systematic approaches requires innovative optimization techniques that can efficiently explore the trade-off space.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of systematic approaches in achieving a balanced trade-off between performance and fairness, compared to traditional manual tuning methods, with reduced computational costs and improved model outcomes.

### Open Question 3
- Question: How can we develop more and tailored datasets to precisely evaluate specific social biases in large language models, beyond the current template-based methodology?
- Basis in paper: [explicit] The paper suggests that existing datasets for evaluating bias in LLMs largely adopt a similar template-based methodology, which may overlook the necessity for customizing template characteristics to address various forms of bias. It emphasizes the importance of devising more and tailored datasets to accurately evaluate specific social biases.
- Why unresolved: Current datasets may not fully capture the complexity and diversity of social biases, limiting the ability to assess and mitigate bias effectively. Developing more tailored datasets requires a deeper understanding of the nuances of different biases and the creation of more sophisticated evaluation methodologies.
- What evidence would resolve it: Comparative studies demonstrating the effectiveness of tailored datasets in capturing and evaluating specific social biases, compared to traditional template-based datasets, with improved accuracy in identifying and quantifying bias in LLMs.

## Limitations

- The survey's broad scope necessarily sacrifices depth in individual methodologies
- The resource compilation may not capture emerging or specialized evaluation tools
- The categorization framework assumes distinct bias manifestations across stages, though empirical validation is limited

## Confidence

- High confidence in the taxonomic structure and categorization logic (based on systematic literature review)
- Medium confidence in the completeness of resource compilation (depends on publication timing and accessibility)
- Medium confidence in the practical applicability of the framework (requires empirical validation in real-world settings)

## Next Checks

1. **Empirical Validation**: Test whether embedding-based, probability-based, and generation-based metrics capture distinct aspects of bias by applying them to the same LLM and comparing results
2. **Resource Evaluation**: Assess the practical usability of the compiled toolkits by attempting to implement bias evaluation using at least three different tools
3. **Framework Application**: Apply the four-stage mitigation framework to a specific LLM fairness problem and document which techniques are most effective at each stage