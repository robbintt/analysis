---
ver: rpa2
title: Hybrid Human-LLM Corpus Construction and LLM Evaluation for Rare Linguistic
  Phenomena
arxiv_id: '2403.06965'
source_url: https://arxiv.org/abs/2403.06965
tags:
- sentence
- sentences
- construction
- prompt
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a hybrid human-LLM annotation pipeline to
  enable large-scale data collection for rare linguistic phenomena, exemplified by
  the caused-motion construction (CMC). The pipeline uses dependency parsing and GPT-3.5
  to prefilter and classify sentences, reducing human annotation cost while preserving
  diversity.
---

# Hybrid Human-LLM Corpus Construction and LLM Evaluation for Rare Linguistic Phenomena

## Quick Facts
- **arXiv ID**: 2403.06965
- **Source URL**: https://arxiv.org/abs/2403.06965
- **Reference count**: 40
- **Key outcome**: LLM evaluation reveals models struggle with caused-motion construction semantics, with best model achieving 69.75% accuracy.

## Executive Summary
This paper introduces a hybrid human-LLM annotation pipeline to enable large-scale data collection for rare linguistic phenomena, exemplified by the caused-motion construction (CMC). The pipeline uses dependency parsing and GPT-3.5 to prefilter and classify sentences, reducing human annotation cost while preserving diversity. Applied to the CMC, it produced a manually verified dataset of 765 sentences and a semi-automatically expanded set of 127,955 high-confidence instances. Evaluation of GPT-3.5, GPT-4, Gemini Pro, Llama2, Mistral, and Mixtral models revealed all models struggle with understanding the motion component of CMC, with the best model (Mixtral 8x7b) achieving 69.75% accuracy and a 30.25% error rate. The work highlights both the potential and limitations of LLMs for capturing subtle syntactic-semantic patterns and suggests construction grammar as a promising area for further LLM research.

## Method Summary
The hybrid pipeline combines dependency parsing, GPT-3.5 classification, and human verification to annotate rare linguistic phenomena at scale. Starting with a Reddit corpus, spacy dependency parsing extracts sentences matching CMC syntactic patterns. GPT-3.5 then classifies these candidates using few-shot prompts, reducing the pool for expensive human annotation. Human annotators verify the remaining sentences, creating a gold standard. The pipeline exploits the observation that CMC meaning is largely determined by a 4-tuple of <verb, direct object, preposition, prepositional object>, enabling automatic expansion to similar sentences. This approach produces both a high-quality manually verified dataset and a much larger semi-automatically expanded corpus for evaluation.

## Key Results
- The hybrid pipeline produced 765 manually verified CMC instances from Reddit corpus
- Automatic expansion yielded 127,955 high-confidence CMC sentences using 4-tuple matching
- All evaluated models (GPT-3.5, GPT-4, Gemini Pro, Llama2, Mistral, Mixtral) struggled with CMC motion semantics
- Mixtral 8x7b achieved the highest accuracy at 69.75% with a 30.25% error rate
- Error analysis showed models confused CMC with resultatives and failed to recognize motion causation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Dependency parsing acts as a high-recall syntactic filter that preserves rare CMC instances while excluding structurally dissimilar sentences.
- **Mechanism**: By encoding the specific syntactic subtree pattern of CMC (verb → direct object → preposition → prepositional object), the parser can rapidly discard large swaths of non-CMC data without semantic analysis.
- **Core assumption**: The dependency parse correctly identifies the syntactic components of CMC even when rare verbs or unusual contexts are involved.
- **Evidence anchors**:
  - [section] "We run an automatic dependency parser from the spacy toolkit on the reddit corpus and extract all matching sentences."
  - [corpus] The pipeline begins with spacy dependency parsing on a Reddit corpus, filtering by a handcrafted subtree pattern.
- **Break condition**: If the parser systematically mislabels rare or creative CMC uses (e.g., with metaphorical verbs), the filter will either let too many negatives through or exclude true positives.

### Mechanism 2
- **Claim**: GPT-3.5 prompt-based classification concentrates true positives by exploiting the model's learned distributional knowledge of constructional semantics.
- **Mechanism**: The LLM, trained on massive web text, has implicitly learned that certain verb+preposition+object patterns encode motion causation; it can thus discriminate CMC from superficially similar structures.
- **Core assumption**: The LLM's semantic knowledge of the CMC generalizes well enough to classify unseen instances despite never being explicitly trained on construction grammar.
- **Evidence anchors**:
  - [abstract] "We show how dependency parsing and GPT-3.5 can be used to significantly reduce annotation cost and thus enable the annotation of rare phenomena at scale."
  - [section] "The aim of our pipeline is to minimise the cost of running GPT-3.5 and compensating human annotators, while maximising the number of positive, manually verified, linguistically diverse instances."
- **Break condition**: If the LLM overgeneralizes or confuses other constructions (e.g., resultatives) with CMC, it will introduce false positives that undermine the cost savings.

### Mechanism 3
- **Claim**: The 4-tuple <verb, direct object, preposition, prepositional object> acts as a sufficient identifier for CMC class membership, enabling large-scale semi-automatic expansion.
- **Mechanism**: Once a human annotator verifies a CMC instance, any other sentence sharing the same 4-tuple is automatically labeled as CMC, leveraging the assumption that this tuple fully determines constructional meaning.
- **Core assumption**: CMC semantics are entirely determined by the lexical items in these four syntactic slots, with no influence from surrounding context.
- **Evidence anchors**:
  - [section] "We observe that the 4-tuple of <verb, direct object, preposition, prepositional object> almost always perfectly determines the class."
  - [corpus] "We use this observation to extrapolate from our manually annotated sentences to all other sentences with the same 4-tuples."
- **Break condition**: If context beyond the 4-tuple changes the interpretation (e.g., irony, metaphor), the automatic expansion will produce false positives.

## Foundational Learning

- **Concept**: Dependency parsing fundamentals (subtree extraction, pattern matching)
  - Why needed here: The first filtering step relies on precise subtree patterns to isolate CMC candidates.
  - Quick check question: Can you write a spacy pattern that matches "verb → dobj → prep → pobj" and extracts those four tokens?

- **Concept**: LLM prompt engineering and few-shot learning
  - Why needed here: The pipeline's cost efficiency hinges on designing prompts that maximize precision while minimizing API tokens.
  - Quick check question: Given 5 positive and 5 negative CMC examples, how would you structure a prompt that alternates classes and includes explanations?

- **Concept**: Construction grammar semantics (form-meaning mapping)
  - Why needed here: Understanding that CMC meaning is constructional, not lexical, is key to interpreting why models struggle and how to test them.
  - Quick check question: Why does "She sneezed the foam off her cappuccino" mean motion is caused, even though "sneeze" alone doesn't encode motion?

## Architecture Onboarding

- **Component map**: Raw text corpus -> Dependency parser -> CMC subtree filter -> GPT-3.5 API -> Human annotator -> 4-tuple indexer -> Semi-automatic corpus

- **Critical path**: Corpus → Dependency parse → Filter → GPT classify → Human verify → 4-tuple index → Semi-automatic corpus

- **Design tradeoffs**:
  - Recall vs. precision in the dependency filter: stricter patterns reduce downstream cost but risk missing rare true positives.
  - Prompt complexity vs. API cost: longer, more explicit prompts improve precision but increase token count.
  - Manual vs. automatic verification: full human annotation is costly; 4-tuple extrapolation is efficient but assumes semantic sufficiency.

- **Failure signatures**:
  - Low precision in GPT classification → too many false positives → high human annotation cost.
  - Low recall in dependency filter → too few candidates → small final dataset.
  - Semantic ambiguity beyond 4-tuple → automatic expansion introduces noise.

- **First 3 experiments**:
  1. Run the dependency filter on a small held-out Reddit sample and compute precision/recall against a manually annotated gold set.
  2. Test 3-4 prompt variants on the same sample to find the best precision/API cost tradeoff.
  3. Verify the 4-tuple assumption by checking a random sample of automatically expanded sentences for semantic consistency.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the cost-benefit ratio of the hybrid human-LLM annotation pipeline change with different rare linguistic phenomena?
- **Basis in paper**: [explicit] The paper proposes a novel pipeline for collecting data on rare linguistic phenomena, using the caused-motion construction as an example.
- **Why unresolved**: The paper only demonstrates the pipeline on one specific construction, and the generalizability to other phenomena is not explored.
- **What evidence would resolve it**: Applying the pipeline to multiple rare linguistic phenomena and comparing the cost and effectiveness across them.

### Open Question 2
- **Question**: What is the impact of different instruction-tuned LLMs on the performance of the hybrid human-LLM annotation pipeline?
- **Basis in paper**: [explicit] The paper uses GPT-3.5 for the annotation pipeline, but other instruction-tuned LLMs like GPT-4, Llama2, and Mistral are evaluated in a separate experiment.
- **Why unresolved**: The paper does not investigate how using different instruction-tuned LLMs in the annotation pipeline would affect its performance.
- **What evidence would resolve it**: Implementing the pipeline with different instruction-tuned LLMs and comparing the results.

### Open Question 3
- **Question**: How does the performance of the hybrid human-LLM annotation pipeline scale with the size of the dataset?
- **Basis in paper**: [explicit] The paper mentions that the final size of the dataset is limited by the budget for prompting GPT-3.5 and the time available for annotation.
- **Why unresolved**: The paper does not explore how the pipeline's performance changes as the dataset size increases.
- **What evidence would resolve it**: Applying the pipeline to larger datasets and measuring the performance metrics.

### Open Question 4
- **Question**: What is the optimal balance between human annotation and LLM annotation in the hybrid pipeline?
- **Basis in paper**: [explicit] The paper aims to minimize the cost per true positive by jointly considering the costs of human annotation and LLM API calls.
- **Why unresolved**: The paper does not investigate the optimal ratio of human to LLM annotation that would minimize the overall cost while maintaining high accuracy.
- **What evidence would resolve it**: Experimenting with different ratios of human to LLM annotation and measuring the cost and accuracy trade-offs.

## Limitations
- The 4-tuple expansion assumption lacks quantitative validation across diverse linguistic contexts
- Evaluation focuses exclusively on English CMC, limiting cross-linguistic generalizability
- Different APIs and model versions used for evaluation may confound pure model capability comparisons

## Confidence
- **High confidence**: The overall pipeline architecture effectively reduces annotation costs while maintaining dataset quality
- **Medium confidence**: LLMs universally struggle with the motion component of CMC
- **Low confidence**: The 4-tuple expansion method sufficiently preserves semantic diversity

## Next Checks
1. **4-tuple semantic validation**: Randomly sample 100 automatically expanded sentences and manually verify whether the 4-tuple truly determines CMC membership, measuring false positive rate.
2. **Prompt sensitivity analysis**: Systematically vary the number and ordering of few-shot examples in the evaluation prompts to quantify performance variance attributable to prompt engineering.
3. **Cross-linguistic transferability**: Apply the pipeline to a non-English corpus (e.g., French or German) with similar caused-motion constructions to test language-agnostic applicability.