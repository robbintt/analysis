---
ver: rpa2
title: Poisoning Attacks on Federated Learning for Autonomous Driving
arxiv_id: '2405.01073'
source_url: https://arxiv.org/abs/2405.01073
tags: []
core_contribution: 'This paper introduces two novel poisoning attacks on federated
  learning tailored for regression tasks in autonomous driving: FLStealth and Off-Track
  Attack (OTA). FLStealth is an untargeted attack designed to deteriorate global model
  performance while appearing benign, achieved by training both honest and byzantine
  models with a weighted loss function.'
---

# Poisoning Attacks on Federated Learning for Autonomous Driving

## Quick Facts
- **arXiv ID**: 2405.01073
- **Source URL**: https://arxiv.org/abs/2405.01073
- **Reference count**: 35
- **Key outcome**: Introduces FLStealth and OTA poisoning attacks that effectively bypass federated learning defenses in autonomous driving scenarios.

## Executive Summary
This paper presents two novel poisoning attacks targeting federated learning systems in autonomous driving applications. The authors develop FLStealth, an untargeted attack designed to degrade global model performance while maintaining the appearance of benign behavior, and Off-Track Attack (OTA), a targeted backdoor attack that manipulates vehicle trajectory predictions through trigger pattern injection. The attacks are evaluated using the Zenseack Open Dataset, demonstrating significant vulnerabilities in federated learning systems for autonomous driving.

## Method Summary
The paper introduces two distinct poisoning attack strategies for federated learning regression tasks. FLStealth employs a weighted loss function during training, combining honest model objectives with byzantine model objectives to create attacks that appear benign while degrading overall performance. The Off-Track Attack injects specific trigger patterns into input images to manipulate vehicle trajectory predictions. Both attacks are designed to evade standard defense mechanisms commonly used in federated learning systems, with particular focus on the unique challenges presented by autonomous driving regression tasks.

## Key Results
- FLStealth successfully bypasses norm-based and Krum defenses while maintaining attack effectiveness
- OTA achieves targeted trajectory manipulation that evades all tested defense mechanisms
- Both attacks demonstrate significant vulnerabilities in federated learning systems for autonomous driving applications

## Why This Works (Mechanism)
These attacks exploit the fundamental trust assumptions in federated learning where participating clients are assumed to be either honest or can be identified through anomaly detection. By carefully crafting the training process, FLStealth creates byzantine models that produce similar outputs to honest models, making detection difficult. OTA leverages the spatial nature of autonomous driving data, using trigger patterns that manipulate model predictions while appearing as legitimate input variations to defensive systems.

## Foundational Learning
- **Federated Learning**: Distributed machine learning where multiple clients train models locally and share updates - needed to understand the attack surface and defense mechanisms
- **Byzantine Attacks**: Malicious clients sending corrupted model updates - needed to grasp attack objectives and defense strategies
- **Regression Tasks**: Predicting continuous values rather than classes - needed to understand the unique challenges in autonomous driving applications
- **Trigger Patterns**: Specific input modifications designed to manipulate model behavior - needed to comprehend OTA's targeted attack mechanism
- **Weighted Loss Functions**: Combining multiple objectives in model training - needed to understand FLStealth's evasion technique
- **Trajectory Prediction**: Forecasting vehicle movement paths - needed to appreciate the safety implications of OTA

## Architecture Onboarding

**Component Map**
Client Devices -> FL Server -> Global Model -> Safety Critical Decisions

**Critical Path**
Malicious Client Training -> Model Update Aggregation -> Global Model Update -> Autonomous Driving Decisions

**Design Tradeoffs**
The attacks balance stealth (avoiding detection) against effectiveness (achieving attack goals), requiring careful calibration of training parameters and trigger patterns.

**Failure Signatures**
FLStealth: Reduced model accuracy across multiple tasks without clear anomaly patterns
OTA: Consistent trajectory deviations when specific trigger patterns are present

**First 3 Experiments**
1. Baseline performance evaluation of honest federated learning system
2. FLStealth attack effectiveness with varying attack parameters
3. OTA trigger pattern optimization and targeted attack success rate

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation relies solely on Zenseact Open Dataset, limiting generalizability
- Attack scalability to different model architectures remains unclear
- Impact of varying federated learning parameters on attack success not fully explored

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| FLStealth effectiveness against standard defenses | High |
| OTA targeted attack success | Medium |
| Attack impact on heterogeneous client distributions | Medium |

## Next Checks
1. Test both attacks on additional autonomous driving datasets (nuScenes, Waymo Open Dataset) to assess generalizability
2. Evaluate attack robustness under varying federated learning configurations, including different client participation ratios and aggregation methods
3. Conduct ablation studies on OTA's trigger patterns to determine minimum effective trigger size and impact on stealthiness