---
ver: rpa2
title: 'GOMAA-Geo: GOal Modality Agnostic Active Geo-localization'
arxiv_id: '2406.01917'
source_url: https://arxiv.org/abs/2406.01917
tags:
- goal
- gomaa-geo
- aerial
- dataset
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses active geo-localization, where an agent navigates
  aerial views to locate a target described in multiple modalities (text, ground-level,
  or aerial images). The authors propose GOMAA-Geo, which combines cross-modal representation
  alignment, LLM pretraining with a novel goal-aware supervised strategy, and reinforcement
  learning.
---

# GOMAA-Geo: GOal Modality Agnostic Active Geo-localization

## Quick Facts
- arXiv ID: 2406.01917
- Source URL: https://arxiv.org/abs/2406.01917
- Reference count: 40
- Key outcome: Achieves up to 232% improvement in success rate over baselines for cross-modal active geo-localization

## Executive Summary
This paper introduces GOMAA-Geo, a method for active geo-localization where an agent navigates aerial views to locate a target described in multiple modalities (text, ground-level images, or aerial images). The approach combines cross-modal representation alignment, a novel goal-aware supervised pretraining strategy, and reinforcement learning. The method demonstrates significant performance improvements and zero-shot generalization to disaster scenarios, achieving strong results across all goal modalities despite being trained only on aerial goals.

## Method Summary
GOMAA-Geo addresses active geo-localization through a three-stage approach. First, it trains a cross-modal feature extractor (CLIP-MMFE) to align aerial image embeddings with CLIP's text and ground-level image space using contrastive learning. Second, it employs Goal-Aware Supervised Pretraining (GASP) to teach an LLM to generate action distributions useful for reinforcement learning by training on random trajectories with optimal action labels. Finally, it uses PPO-based planning with dense rewards to learn the navigation policy. The dense reward structure provides frequent feedback (+1 for moving closer, -1 for moving away or revisiting, +2 for reaching the goal), accelerating learning compared to sparse rewards.

## Key Results
- Achieves up to 232% improvement in success rate compared to baselines
- Demonstrates zero-shot generalization to disaster scenarios (xBD dataset) despite being trained only on building datasets
- Maintains strong performance across all goal modalities (text, ground-level, aerial) even though only trained on aerial goals

## Why This Works (Mechanism)

### Mechanism 1
- Cross-modal contrastive learning aligns aerial image embeddings with CLIP's text and ground-level image space using InfoNCE loss.
- Core assumption: Ground-level images and text are already aligned in CLIP's embedding space, so aligning aerial to ground-level implicitly aligns aerial to text.
- Evidence: The paper develops this approach explicitly and mentions InfoNCE loss implementation.
- Break condition: If ground-level images are not truly co-located or if CLIP's grounding to language is weak, the alignment may fail.

### Mechanism 2
- Goal-aware supervised pretraining (GASP) teaches the LLM to generate action distributions useful for RL.
- Core assumption: Random trajectories plus correct labeling of optimal actions will induce the LLM to learn a useful history-aware goal-conditioned policy structure.
- Evidence: The paper devises GASP strategy and mentions it enables learning history-aware goal-conditioned latent representation.
- Break condition: If optimal action labels are incorrect or the LLM capacity is insufficient, the learned representation may be useless for RL.

### Mechanism 3
- Dense reward function accelerates learning by providing more frequent feedback than sparse rewards.
- Core assumption: The dense reward provides a smoother optimization landscape and avoids local minima that sparse rewards might get stuck in.
- Evidence: The paper incorporates dense reward structure and mentions it expedites learning.
- Break condition: If the reward shaping is too aggressive, it could lead to suboptimal policies that exploit the reward structure rather than truly solving the task.

## Foundational Learning

- Concept: Cross-modal representation alignment via contrastive learning
  - Why needed here: The agent observes only aerial images but the goal can be specified in text or ground-level images; alignment allows the agent to understand goals regardless of modality.
  - Quick check question: If you have a ground-level image and an aerial image of the same location, should their embeddings be closer or farther in the aligned space?

- Concept: Goal-conditioned RL with history awareness
  - Why needed here: The agent must remember where it has been and what it has seen to efficiently navigate toward the goal within limited steps.
  - Quick check question: What information must the agent retain across timesteps to avoid revisiting the same location?

- Concept: Reward shaping for efficient learning
  - Why needed here: Sparse rewards would only provide feedback upon reaching the goal, making learning slow; dense rewards provide more frequent guidance.
  - Quick check question: If the agent moves from position A to position B, and B is closer to the goal than A, what reward should it receive?

## Architecture Onboarding

- Component map: CLIP-MMFE -> GASP-based LLM -> Planning module -> Policy output
- Critical path: CLIP-MMFE must be trained first to provide aligned embeddings, GASP can then learn from aligned representations, Planning module learns to act on GASP output
- Design tradeoffs:
  - Modality-agnostic vs. modality-specific: GOMAA-Geo sacrifices some aerial-only performance for cross-modal generalization
  - Dense vs. sparse rewards: Dense rewards speed learning but may introduce bias
  - Random vs. uniform sampling in training: Uniform sampling prevents bias toward certain start-goal distances
- Failure signatures:
  - Poor cross-modal alignment: Agent performs well on aerial goals but fails on text/ground goals
  - GASP failure: Agent acts randomly or shortsightedly without considering history
  - Reward shaping issues: Agent gets stuck in loops or takes inefficient paths
- First 3 experiments:
  1. Train CLIP-MMFE on paired aerial/ground images and verify alignment via nearest neighbor search
  2. Train GASP with random trajectories and visualize whether it learns meaningful action distributions
  3. Compare dense vs. sparse reward learning curves to quantify learning speed difference

## Open Questions the Paper Calls Out
- The authors note that their approach assumes noise-free pose estimation, which limits real-world deployment, and they leave exploration of noisy pose estimation for future work.

## Limitations
- The framework assumes the agent's ego-pose is known and noise-free, which is unrealistic for real-world deployment.
- Performance degrades as grid size increases beyond 10x10, motivating further research for larger search areas.
- The framework currently operates in 2D navigation, with elevation changes and 3D navigation left for future work.

## Confidence
- High confidence: Dense reward formulation and PPO-based planning approach
- Medium confidence: Cross-modal representation alignment effectiveness
- Medium confidence: GASP pretraining efficacy

## Next Checks
1. Conduct nearest-neighbor retrieval experiments to verify that aligned aerial embeddings correctly match corresponding text and ground-level images in the CLIP space.
2. Perform an ablation study comparing GASP pretraining against standard supervised pretraining and random initialization to isolate its contribution to performance gains.
3. Test alternative reward formulations (e.g., scaled rewards, different penalty structures) to determine the robustness of dense rewards to hyperparameter choices.