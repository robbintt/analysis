---
ver: rpa2
title: 'PruneSymNet: A Symbolic Neural Network and Pruning Algorithm for Symbolic
  Regression'
arxiv_id: '2401.15103'
source_url: https://arxiv.org/abs/2401.15103
tags: []
core_contribution: The paper proposes PruneSymNet, a symbolic neural network with
  a novel pruning algorithm for symbolic regression. PruneSymNet replaces activation
  functions in feedforward neural networks with common elementary functions and operators,
  enabling representation of any symbolic expression.
---

# PruneSymNet: A Symbolic Neural Network and Pruning Algorithm for Symbolic Regression

## Quick Facts
- arXiv ID: 2401.15103
- Source URL: https://arxiv.org/abs/2401.15103
- Reference count: 30
- Primary result: Outperforms EQL, GP, and DSR in symbolic regression accuracy and solution simplicity

## Executive Summary
PruneSymNet is a symbolic neural network architecture that replaces standard activation functions with elementary functions and operators to enable direct representation of symbolic mathematical expressions. The method combines this network structure with a greedy pruning algorithm that extracts minimal subnetworks while preserving accuracy, followed by coefficient post-processing to simplify the resulting expressions. Experiments on six benchmark datasets demonstrate that PruneSymNet achieves lower mean squared error and finds more optimal solutions compared to existing symbolic regression methods like EQL, GP, and DSR.

## Method Summary
PruneSymNet replaces activation functions in feedforward neural networks with common elementary functions (sin, cos, exp, log, square, identity) and arithmetic operators (+, -, ×, ÷). The network is trained using gradient descent with protection mechanisms against gradient explosions from sensitive operators. A greedy pruning algorithm extracts minimal subnetworks by preserving edges with least loss at each layer, optionally using beam search to avoid local minima. Post-processing with BFGS optimization refines coefficients and rounds near-integer values to simplify the final symbolic expression. The method is evaluated on six benchmark datasets with 128 input points sampled from [-5, 5].

## Key Results
- Achieves lower mean squared error than EQL, GP, and DSR on benchmark datasets
- Finds more optimal symbolic solutions across tested problems
- Produces simpler, more interpretable expressions through coefficient rounding and post-processing
- Successfully derives accurate symbolic expressions for polynomial, trigonometric, and mixed function datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replace activation functions with elementary functions/operators to enable symbolic expression representation
- Mechanism: By substituting standard neural activations with arithmetic and transcendental operators, the network structure itself becomes a symbolic expression tree where each subnetwork corresponds to a unique mathematical formula
- Core assumption: Elementary functions are differentiable and trainable via gradient descent without losing representational power
- Evidence anchors: [abstract] "replaces activation functions in feedforward neural networks with common elementary functions and operators"; [section] "activation function consists of common elementary functions and operators. The whole network is differentiable and can be trained by gradient descent method"
- Break condition: If operators cause gradient instability or NaN values, training collapses and symbolic extraction fails

### Mechanism 2
- Claim: Greedy pruning with beam search extracts minimal subnetworks yielding simple interpretable expressions
- Mechanism: Starting from output node, at each layer retain the edge with least loss (greedy), optionally preserving B candidates via beam search to avoid local minima
- Core assumption: Loss-based pruning preserves accuracy while reducing complexity; beam search mitigates greedy suboptimality
- Evidence anchors: [abstract] "greedy pruning algorithm prunes the network into a subnetwork while ensuring the accuracy of data fitting"; [section] "greedy pruning algorithm preserves the edge with the least loss in each pruning... combine beam search during pruning to obtain multiple candidate expressions"
- Break condition: If pruning removes essential connections, accuracy degrades; if beam size too small, suboptimal expressions persist

### Mechanism 3
- Claim: Post-processing via BFGS optimization and coefficient rounding yields simpler, more accurate symbolic expressions
- Mechanism: After pruning, solve for optimal coefficients using BFGS on the sparse subnetwork, then round coefficients close to integers and re-optimize iteratively to remove redundant terms and merge coefficients
- Core assumption: Many derived coefficients are near-integer multiples of each other; rounding simplifies without harming fit quality
- Evidence anchors: [abstract] "coefficient post-processing method... for the original expression obtained by pruning, so that a more concise expression can be obtained"; [section] "construct the objective function... solve it using the BFGS... if the error between a floating-point coefficient and its closest integer is less than a certain threshold, the coefficient will be rounded to the nearest integer"
- Break condition: If coefficients are not near-integer multiples, rounding introduces significant error

## Foundational Learning

- Concept: Gradient descent with protected operators to avoid NaN/overflow
  - Why needed here: Elementary functions like exp, log, division can produce undefined or extreme values during training, destabilizing convergence
  - Quick check question: How does the method limit operator outputs and gradients to maintain stable training?

- Concept: Beam search as exploration strategy in combinatorial pruning space
  - Why needed here: Pure greedy pruning may get stuck in local minima; beam search keeps multiple candidates to increase chance of finding minimal expression
  - Quick check question: What is the trade-off between beam size B and computational cost during pruning?

- Concept: BFGS optimization for post-processing symbolic coefficients
  - Why needed here: Pruned coefficients are approximate; BFGS refines them to improve fit accuracy and identify redundancies
  - Quick check question: How does iterative rounding of near-integer coefficients simplify the final symbolic expression?

## Architecture Onboarding

- Component map: Input layer → hidden layers (elementary functions/operators) → output layer
- Critical path:
  1. Initialize PruneSymNet with specified depth and width
  2. Train via gradient descent with operator protections
  3. Apply pruning (greedy + beam) every N epochs
  4. After training, extract minimal subnetwork
  5. Run BFGS post-processing on coefficients
  6. Output simplified symbolic expression
- Design tradeoffs:
  - More hidden layers → higher expressiveness but larger search space and risk of overfitting
  - Larger beam size → better pruning quality but higher memory/computation
  - Frequent pruning → earlier simplification but possible premature removal of useful connections
  - Operator set size → expressiveness vs training stability
- Failure signatures:
  - Training diverges (NaNs, exploding gradients) → operator protections insufficient or learning rate too high
  - Pruning yields overly complex expression → beam size too small or pruning frequency too low
  - Final expression inaccurate → insufficient data sampling or BFGS stuck in local minimum
- First 3 experiments:
  1. Train on a simple polynomial dataset (e.g., y = x² + 2x + 1) with minimal layers; verify symbolic extraction matches ground truth
  2. Apply pruning with B=1 and B=5 on a trigonometric dataset; compare expression simplicity and accuracy
  3. Run post-processing on a pruned expression with near-integer coefficients; confirm rounding reduces term count without degrading fit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the pruning algorithm's performance scale with increasing network depth and width in PruneSymNet?
- Basis in paper: [explicit] The paper mentions that the proposed greedy pruning algorithm may not yield the subnetwork with the least loss and introduces beam search to alleviate this problem. However, it does not discuss the scalability of the pruning algorithm with respect to network size.
- Why unresolved: The paper does not provide experiments or theoretical analysis on how the pruning algorithm's performance is affected by increasing network depth and width.
- What evidence would resolve it: Experiments comparing the pruning algorithm's performance on PruneSymNet networks of varying depths and widths, or a theoretical analysis of the algorithm's time and space complexity with respect to network size.

### Open Question 2
- Question: Can the proposed gradient descent method with protection for division operators and other sensitive functions be applied to other neural network architectures?
- Basis in paper: [explicit] The paper proposes an improved gradient descent method to avoid gradient explosion in PruneSymNet, which allows division operators to be included in every layer. However, it does not discuss the applicability of this method to other neural network architectures.
- Why unresolved: The paper focuses on the performance of the proposed method within PruneSymNet and does not explore its potential benefits for other neural network architectures.
- What evidence would resolve it: Experiments applying the proposed gradient descent method with protection to other neural network architectures, such as convolutional neural networks or recurrent neural networks, and comparing their performance with standard gradient descent methods.

### Open Question 3
- Question: How does the proposed coefficient post-processing method affect the interpretability and accuracy of the derived symbolic expressions?
- Basis in paper: [explicit] The paper introduces a coefficient post-processing method to obtain more accurate coefficients and remove redundant terms from the expressions derived by the pruning algorithm. However, it does not discuss the impact of this method on the interpretability and accuracy of the expressions.
- Why unresolved: The paper does not provide experiments or analysis on how the coefficient post-processing method affects the interpretability and accuracy of the derived symbolic expressions.
- What evidence would resolve it: Experiments comparing the interpretability and accuracy of the symbolic expressions derived with and without the coefficient post-processing method, or a theoretical analysis of how the method affects the sparsity and structure of the expressions.

## Limitations
- Protection mechanisms against gradient explosions are not fully detailed, leaving ambiguity in implementation
- Scalability to higher-dimensional problems and larger datasets is not thoroughly investigated
- Choice of operator set and its impact on expressiveness versus training stability requires further exploration

## Confidence
- Mechanism 1 (elementary functions replacement): High
- Mechanism 2 (greedy pruning with beam search): Medium
- Mechanism 3 (BFGS post-processing): Medium
- Experimental validation on benchmark datasets: Medium

## Next Checks
1. Systematically vary the operator set in PruneSymNet and evaluate the trade-off between expressiveness and training stability across multiple datasets
2. Apply PruneSymNet to higher-dimensional symbolic regression problems and assess its performance relative to state-of-the-art methods
3. Conduct an ablation study on the pruning algorithm, comparing greedy pruning with and without beam search, and evaluate the impact on final expression quality and computational cost