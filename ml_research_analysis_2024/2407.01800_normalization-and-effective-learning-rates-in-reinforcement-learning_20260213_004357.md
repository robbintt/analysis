---
ver: rpa2
title: Normalization and effective learning rates in reinforcement learning
arxiv_id: '2407.01800'
source_url: https://arxiv.org/abs/2407.01800
tags:
- learning
- normalization
- network
- rate
- norm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work analyzes the interaction between normalization layers
  and effective learning rates in neural networks, particularly in nonstationary learning
  settings like reinforcement learning and continual learning. The authors show that
  layer normalization introduces a subtle but important side effect: a relationship
  between parameter norm growth and decay in the effective learning rate.'
---

# Normalization and effective learning rates in reinforcement learning

## Quick Facts
- arXiv ID: 2407.01800
- Source URL: https://arxiv.org/abs/2407.01800
- Reference count: 40
- Key outcome: Normalize-and-Project (NaP) improves robustness to nonstationarity in reinforcement learning by maintaining constant effective learning rate through coupling normalization with weight projection

## Executive Summary
This paper analyzes how layer normalization affects learning rates in neural networks, particularly in nonstationary settings like reinforcement learning and continual learning. The authors show that layer normalization creates scale-invariant subnetworks where parameter norm growth leads to effective learning rate decay, potentially harming performance in nonstationary tasks. They propose Normalize-and-Project (NaP), which couples normalization with periodic weight projection to maintain constant effective learning rate. NaP improves performance in sequential Atari RL while maintaining or slightly improving performance on stationary tasks like CIFAR-10, ImageNet, and language modeling.

## Method Summary
Normalize-and-Project (NaP) is a reparameterization technique that combines layer normalization with weight projection to maintain constant effective learning rate during training. The method inserts LayerNorm before each nonlinearity in the network and periodically rescales parameters to their initial norm. This decouples parameter norm growth from the effective learning rate, preventing the decay that occurs when parameters grow unchecked. The technique is particularly effective in nonstationary settings where plasticity is crucial, and can be combined with explicit learning rate schedules for optimal performance in reinforcement learning.

## Key Results
- NaP improves robustness to nonstationarity in synthetic benchmarks and sequential Atari RL
- NaP maintains or slightly improves performance on stationary tasks (CIFAR-10, ImageNet, C4 language modeling)
- Explicit learning rate decay schedules outperform constant learning rates when using NaP in deep RL
- Implicit learning rate decay from parameter growth is beneficial in single-task RL but harmful in nonstationary settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Layer normalization introduces a relationship between parameter norm growth and effective learning rate decay, causing performance degradation in nonstationary settings.
- Mechanism: Normalization layers cause subnetworks to become scale-invariant, leading to gradients scaling inversely with parameter norm. As parameters grow, effective learning rate shrinks, slowing learning in nonstationary tasks.
- Core assumption: The parameter norm grows during training without regularization.
- Evidence anchors:
  - [abstract]: "an equivalence between growth in the norm of the network parameters and decay in the effective learning rate"
  - [section 3.2]: "when the norm of the parameters grows... the effective learning rate shrinks"
  - [corpus]: Weak - no direct citations found for this specific mechanism
- Break condition: If parameter norm remains bounded or regularization prevents norm growth.

### Mechanism 2
- Claim: Normalize-and-Project (NaP) maintains constant effective learning rate by coupling normalization with weight projection.
- Mechanism: NaP inserts normalization layers before nonlinearities and periodically projects weights to fixed norm, decoupling parameter norm from effective learning rate.
- Core assumption: The weight projection frequency is sufficient to prevent significant parameter norm growth between projections.
- Evidence anchors:
  - [abstract]: "couples the insertion of normalization layers with weight projection, ensuring that the effective learning rate remains constant"
  - [section 3.3]: "rescale the parameters of each layer to match their initial norm periodically throughout training"
  - [corpus]: Weak - no direct citations for this specific technique
- Break condition: If projection frequency is too low or projection implementation is incorrect.

### Mechanism 3
- Claim: Implicit learning rate decay from parameter growth is beneficial in single-task RL but harmful in nonstationary settings.
- Mechanism: In single-task RL, the implicit decay helps optimization by reducing learning rate as value function components are learned. In nonstationary settings, this decay happens too quickly relative to task duration.
- Core assumption: Different learning rate schedules are optimal for different task regimes.
- Evidence anchors:
  - [section 4.2]: "even without an explicit learning rate schedule, the implicit learning rate decay... is in fact critical to the optimization process"
  - [section 4.2]: "this implicit schedule can harm performance in nonstationary regimes"
  - [corpus]: Weak - no direct citations for this specific insight
- Break condition: If task duration matches implicit decay schedule or if explicit decay schedule is optimal.

## Foundational Learning

- Concept: Scale-invariance and effective learning rate
  - Why needed here: Understanding how normalization creates scale-invariance and affects effective learning rate is crucial for grasping NaP's mechanism
  - Quick check question: If a function f(θ) is scale-invariant, what happens to its gradient when you scale θ by c?

- Concept: Neural network initialization and trainability
  - Why needed here: NaP builds on the idea that maintaining pre-activation statistics is important for trainability
  - Quick check question: Why do we typically want mean-zero, unit-variance pre-activations in neural networks?

- Concept: Reinforcement learning and nonstationarity
  - Why needed here: The paper focuses on nonstationary learning settings where plasticity is crucial
  - Quick check question: What makes reinforcement learning a nonstationary learning problem?

## Architecture Onboarding

- Component map: LayerNorm -> Nonlinearity -> Projection -> Training
- Critical path: Insert normalization → Train with optimizer → Periodically project weights to initial norm → (Optional) Apply explicit learning rate schedule
- Design tradeoffs: Normalization before vs. after nonlinearities, frequency of weight projection, whether to include learnable scale/offset parameters
- Failure signatures: Performance degradation in nonstationary tasks, dead units in networks without normalization, unstable training due to exploding/vanishing gradients
- First 3 experiments:
  1. Apply NaP to a simple CNN on CIFAR-10 and verify no performance degradation
  2. Train NaP on a continual learning task with random label resets and observe improved robustness
  3. Apply NaP to a Rainbow agent on Atari and compare performance with/without learning rate schedule

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal learning rate schedule for NaP in deep reinforcement learning, and how should it be adapted for different environments or tasks?
- Basis in paper: [explicit] The paper shows that NaP with a constant learning rate performs worse than NaP with a linear decay schedule in Atari games, and notes that the optimal schedule may depend on the specific task.
- Why unresolved: The paper only tests a linear decay schedule and doesn't explore other potential schedules (e.g., cosine decay, adaptive schedules). The analysis suggests the implicit schedule from parameter growth is important but not optimal.
- What evidence would resolve it: Systematic comparison of different learning rate schedules (linear, cosine, adaptive) across multiple RL tasks and environments, measuring both learning speed and final performance.

### Open Question 2
- Question: How should scale and offset parameters be handled in NaP, and does their treatment affect performance in different network architectures or tasks?
- Basis in paper: [explicit] The paper discusses different approaches (projection, regularization, or omitting) but notes that the optimal choice may depend on the activation function and task, with regularization being recommended for non-homogeneous activations.
- Why unresolved: The paper doesn't provide a comprehensive comparison of these different approaches across multiple architectures and tasks. It mentions that regularization helped in sequential ALE but doesn't explore other combinations.
- What evidence would resolve it: Comparative study of NaP variants with different scale/offset treatments across multiple network architectures (CNNs, transformers, MLPs) and task types (stationary, sequential, reinforcement learning).

### Open Question 3
- Question: How does NaP interact with other techniques designed to improve plasticity, such as dead unit recovery methods or regularization techniques?
- Basis in paper: [explicit] The paper shows that NaP reduces the performance gaps between various plasticity-preserving methods in continual learning, suggesting these methods may be addressing similar underlying issues related to parameter norm growth.
- Why unresolved: The paper doesn't systematically investigate how NaP combines with specific plasticity techniques like ReDO, Shrink and Perturb, or dead unit recovery methods. It only shows that NaP improves robustness to nonstationarity.
- What evidence would resolve it: Experiments combining NaP with specific plasticity techniques in both synthetic and real-world nonstationary tasks, measuring whether the benefits are additive, redundant, or complementary.

## Limitations
- The analysis is primarily empirical with limited theoretical grounding for why NaP works optimally at certain projection frequencies
- The method requires tuning of projection frequency, which could be task-dependent and may limit its practical applicability
- The comparison with other normalization techniques (BatchNorm, GroupNorm) is incomplete

## Confidence
- High Confidence: The mathematical relationship between parameter norm growth and effective learning rate decay is well-established
- Medium Confidence: The empirical results demonstrating NaP's effectiveness in improving robustness to nonstationarity are compelling
- Low Confidence: The exact mechanism by which NaP improves performance is not fully explained

## Next Checks
1. Conduct ablation studies varying projection frequency across different network architectures and tasks to identify optimal schedules
2. Compare NaP against other normalization techniques on the same nonstationary benchmarks to determine if the effect is LayerNorm-specific
3. Develop theoretical analysis of the gradient flow in NaP networks to explain why certain projection frequencies work better than others