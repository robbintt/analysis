---
ver: rpa2
title: One system for learning and remembering episodes and rules
arxiv_id: '2407.05884'
source_url: https://arxiv.org/abs/2407.05884
tags:
- learning
- episodes
- rules
- capacity
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges traditional trade-offs in learning theory
  by demonstrating that a single computational system with excess representational
  capacity can simultaneously learn and remember both individual episodes and generalizable
  rules. The authors show this using a multi-layer perceptron with varying hidden
  layer widths trained on synthetic datasets where noise levels control the rule-to-episode
  ratio.
---

# One system for learning and remembering episodes and rules

## Quick Facts
- arXiv ID: 2407.05884
- Source URL: https://arxiv.org/abs/2407.05884
- Reference count: 5
- Key outcome: A single computational system with excess representational capacity can simultaneously learn and remember both individual episodes and generalizable rules, eliminating catastrophic forgetting observed in lower-capacity systems.

## Executive Summary
This paper challenges traditional learning theory trade-offs by demonstrating that a single computational system with excess representational capacity can simultaneously learn and remember both individual episodes and generalizable rules. Using a multi-layer perceptron with varying hidden layer widths trained on synthetic datasets where noise levels control the rule-to-episode ratio, the authors show that excess capacity models outperform constrained and sufficient capacity models in both learning episodes/rules (p < 0.001) and retaining previously learned associations during interference tasks. The work suggests that cognitive trade-offs in learning arise from capacity limitations rather than inherent incompatibility between cognitive processes.

## Method Summary
The authors used a multi-layer perceptron with two hidden layers of equal width, trained with stochastic gradient descent on synthetic datasets with controlled noise levels (0-1) to vary the rule-to-episode ratio. They tested four capacity regimes (0.5x, 1x, 10x, 100x relative to memorization requirements) across two learning blocks: first learning Atrain→B associations and testing generalization to Atest→C, then learning interfering Atrain→C associations while testing retention of Atrain→B knowledge and rule learning via Atest→D. The synthetic data generation created 10 5-dimensional samples each for Atrain, Atest, B, C, and D using Gaussian distributions with a weighted sum formula incorporating noise parameters.

## Key Results
- Excess capacity models (10x and 100x) eliminated catastrophic forgetting observed in constrained and sufficient capacity models
- Models with excess capacity significantly outperformed constrained and sufficient capacity models in both learning episodes/rules (p < 0.001)
- Sufficient capacity models (1x) showed good rule learning but catastrophic forgetting of previously learned episodes
- Constrained capacity models (0.5x) performed poorly on both rule learning and episode retention

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Excess representational capacity prevents interference between learning rules and memorizing episodes
- Mechanism: When a model has more hidden nodes than needed to memorize the data, it can simultaneously encode both the generalizable rule structure and the specific episode details without forcing them to compete for the same representational space
- Core assumption: The rule and episode information can be encoded in orthogonal subspaces within the high-dimensional hidden layer
- Evidence anchors: "models with excess capacity outperformed constrained and sufficient capacity models in both learning episodes/rules (p < 0.001)"
- Break condition: When noise level approaches 1.0, the rule component becomes negligible and even excess capacity models must primarily memorize noise patterns

### Mechanism 2
- Claim: Sufficient representational space eliminates catastrophic forgetting
- Mechanism: With excess capacity, previously learned associations (A-B pairings) can be preserved in unused representational dimensions while new associations (A-C pairings) are learned in separate dimensions, preventing interference
- Core assumption: The MLP architecture allows for distributed representation where different input-output mappings can occupy distinct regions of the hidden layer space
- Evidence anchors: "effectively eliminating catastrophic forgetting observed in lower-capacity systems"
- Break condition: When training data volume exceeds the representational capacity even of excess capacity models, forcing reuse of representational space

### Mechanism 3
- Claim: Noise level controls the rule-to-episode continuum
- Mechanism: The noise parameter in the data generation creates a continuum where 0 noise represents pure rule learning and 1 noise represents pure episode memorization, allowing systematic study of how capacity affects both learning modes simultaneously
- Core assumption: The transformation f represents a stable, learnable rule that can be extracted from noisy data when sufficient capacity exists
- Evidence anchors: "When noise = 0, the task amounts entirely to learning of the generalizable rule; when noise = 1, the task amounts entirely to learning arbitrary associations"
- Break condition: When noise exceeds a threshold where the signal-to-noise ratio prevents reliable extraction of the underlying rule structure

## Foundational Learning

- Concept: Catastrophic forgetting
  - Why needed here: Understanding why traditional models lose previously learned information when acquiring new knowledge is crucial for appreciating the breakthrough of excess capacity
  - Quick check question: In the A-B, A-C paradigm, what happens to B associations when the model learns C associations in a constrained capacity model?

- Concept: Representational capacity in neural networks
  - Why needed here: The core insight relies on understanding how the number of hidden nodes relative to data complexity affects learning dynamics and interference
  - Quick check question: If a dataset requires 10 hidden nodes to memorize perfectly, what happens when you train a model with only 5 hidden nodes on this data?

- Concept: Overfitting vs. generalization trade-off
  - Why needed here: The paper challenges the traditional view that models must choose between memorizing individual instances and learning generalizable patterns
- Quick check question: Why does traditional ML theory suggest that high-capacity models should overfit rather than generalize well?

## Architecture Onboarding

- Component map:
  - Input layer: 5-dimensional Gaussian samples (Atrain, Atest)
  - Hidden layers: Two hidden layers of equal width (capacity variable)
  - Output layer: Target datasets (B, C, D)
  - Loss function: Mean squared error
  - Optimizer: Stochastic Gradient Descent (learning rate = 0.01)

- Critical path:
  1. Data generation with controlled noise levels
  2. Training on Block 1 (Atrain → B)
  3. Testing generalization (Atest → C)
  4. Training on Block 2 (Atrain → C)
  5. Testing retention of Block 1 knowledge and rule extraction

- Design tradeoffs:
  - Capacity vs. computational cost: Higher capacity models require more training time and memory
  - Model simplicity vs. biological plausibility: Simple MLP vs. more complex neural architectures
  - Synthetic vs. real-world data: Controlled experiments vs. ecological validity

- Failure signatures:
  - Constrained models: Poor performance on both rule learning and episode retention
  - Sufficient models: Good rule learning but catastrophic forgetting of episodes
  - Excess models: High performance across all metrics but potential for inefficiency

- First 3 experiments:
  1. Replicate the constrained capacity condition (0.5× capacity) to observe catastrophic forgetting
  2. Test the sufficient capacity condition (1× capacity) to establish baseline performance
  3. Run excess capacity condition (10× capacity) to verify the elimination of trade-offs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do excess capacity systems perform when learning sequences of tasks with increasing interference rather than just two tasks?
- Basis in paper: The paper only tests a two-task paradigm (A-B followed by A-C), but does not explore how performance scales with more complex interference patterns or longer task sequences.
- Why unresolved: The current experiments only examine single interference events between two task blocks, leaving unclear whether excess capacity provides benefits for more complex continual learning scenarios.
- What evidence would resolve it: Testing models with 3+ sequentially learned tasks with varying degrees of interference, measuring retention of all previously learned associations.

### Open Question 2
- Question: What is the relationship between the number of hidden nodes and the amount of noise in the data that can be effectively learned without catastrophic forgetting?
- Basis in paper: The paper tests models with 0.5x, 1x, 10x, and 100x capacity but does not systematically explore how different noise levels interact with different capacity regimes.
- Why unresolved: While the paper varies noise levels (0 to 1) across experiments, it does not map out how different capacity levels handle different noise regimes in a systematic way.
- What evidence would resolve it: Systematic experiments varying both capacity ratios and noise levels to identify thresholds where excess capacity becomes necessary for successful learning and retention.

### Open Question 3
- Question: How does the excess capacity phenomenon change when using different neural network architectures beyond simple MLPs with two hidden layers?
- Basis in paper: The study uses a specific MLP architecture, but does not explore whether the excess capacity effects generalize to other architectures like convolutional networks or recurrent networks.
- Why unresolved: The current results are architecture-specific and may not generalize to other network types commonly used in cognitive modeling and deep learning.
- What evidence would resolve it: Replicating the experiments with different architectures (CNNs, RNNs, transformers) to test whether excess capacity benefits are architecture-dependent or general principles.

## Limitations

- The study relies on synthetic data with controlled noise parameters rather than real-world datasets, raising questions about ecological validity
- The research focuses exclusively on a specific MLP architecture with two hidden layers, limiting generalizability to other neural network architectures
- The capacity measurements are relative to synthetic dataset memorization requirements and may not translate directly to real-world problem domains

## Confidence

- **High confidence**: The core finding that excess capacity eliminates catastrophic forgetting in the synthetic learning paradigm (supported by strong statistical evidence p < 0.001)
- **Medium confidence**: The claim that cognitive trade-offs arise from capacity limitations rather than inherent incompatibility between processes (requires validation in real-world domains)
- **Low confidence**: The mechanism explanation that rule and episode information can be encoded in orthogonal subspaces (largely theoretical without empirical validation of the representational geometry)

## Next Checks

1. Test the excess capacity hypothesis on real-world datasets (e.g., image classification with fine-tuning, language models with continual learning tasks) to verify that catastrophic forgetting is eliminated when capacity is increased
2. Conduct ablation studies on the MLP architecture by varying the number of hidden layers, activation functions, and regularization methods to determine whether the excess capacity effect is architecture-dependent
3. Perform representational analysis (e.g., dimensionality reduction, probing tasks) on excess capacity models to empirically verify that rules and episodes occupy orthogonal subspaces in the hidden layer representation space