---
ver: rpa2
title: 'Getting Serious about Humor: Crafting Humor Datasets with Unfunny Large Language
  Models'
arxiv_id: '2403.00794'
source_url: https://arxiv.org/abs/2403.00794
tags:
- humor
- data
- unfun
- humorous
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models can be used to create aligned humor datasets
  by editing existing jokes to remove their humor, outperforming human crowd-workers
  at this task. We find that GPT-4 and GPT-3.5 can reliably edit satirical headlines
  and code-mixed English-Hindi tweets to remove humor while retaining coherence, as
  judged by human annotators.
---

# Getting Serious about Humor: Crafting Humor Datasets with Unfunny Large Language Models

## Quick Facts
- arXiv ID: 2403.00794
- Source URL: https://arxiv.org/abs/2403.00794
- Authors: Zachary Horvitz; Jingru Chen; Rahul Aditya; Harshvardhan Srivastava; Robert West; Zhou Yu; Kathleen McKeown
- Reference count: 20
- Primary result: GPT-4 can edit satirical headlines and code-mixed tweets to remove humor while preserving coherence, outperforming human crowd-workers

## Executive Summary
This paper presents a novel approach to creating humor datasets by using large language models (LLMs) to edit humorous texts and remove their humor while maintaining coherence. The authors demonstrate that GPT-4 and other LLMs can effectively transform satirical headlines and code-mixed English-Hindi tweets into unfunny versions that human annotators rate as coherent and non-humorous. This approach provides a scalable method for creating adversarial examples for humor detection research and challenging training data for humor classifiers.

## Method Summary
The authors use few-shot prompting to guide GPT-4, GPT-3.5, and Mistral models to edit humorous texts by removing humor while preserving coherence. They evaluate this approach on satirical headlines from The Onion and code-mixed English-Hindi tweets. The ROBERTA-SWAP method iteratively swaps low-probability tokens with model predictions based on the incongruity theory of humor. Human evaluations assess the quality of generated unfunny texts across dimensions of humor removal, coherence, and grammaticality. Humor classifiers are then trained on synthetic data and tested on human-vetted holdout sets.

## Key Results
- GPT-4 and GPT-3.5 significantly outperform human crowd-workers at the unfunning task on satirical headlines
- Humor classifiers trained on GPT-4's synthetic unfunny data perform nearly as well as those trained on human-edited data
- GPT-4 successfully generalizes humor removal to code-mixed English-Hindi tweets, with bilingual annotators rating the outputs as highly coherent and non-humorous
- ROBERTA-SWAP provides a computationally cheaper alternative but underperforms more sophisticated LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can effectively remove humor from jokes by replacing low-probability tokens with more probable alternatives, leveraging the incongruity theory of humor.
- Mechanism: The approach uses ROBERTA-SWAP to iteratively swap low-probability tokens in satirical headlines with model predictions, reducing humor by removing surprise elements.
- Core assumption: Humor is primarily driven by incongruous or unexpected word choices that can be detected and replaced by a language model.
- Evidence anchors:
  - [abstract] "ROBERTA-SWAP edits satirical headlines by iteratively swapping k original tokens with a ROBERTA model's predictions, based on the probability ratio between the predicted and original tokens."
  - [section] "ROBERTA-SWAP approach also performed comparably with Unfun players on funniness and real headline metrics, but underperformed on coherence."
- Break condition: If humor relies heavily on context or multi-word expressions rather than individual incongruous tokens, this token-swapping approach would fail to preserve coherence while removing humor.

### Mechanism 2
- Claim: GPT-4 can generalize its humor removal capability across languages and code-mixed text formats.
- Mechanism: GPT-4 demonstrates the ability to edit away humor in English-Hindi code-mixed tweets by identifying and modifying humor-specific elements while maintaining grammatical structure.
- Core assumption: The underlying patterns of humor removal are transferable across languages and text formats.
- Evidence anchors:
  - [abstract] "We extend our approach to a code-mixed English-Hindi humor dataset, where we find that GPT-4's synthetic data is highly rated by bilingual annotators and provides challenging adversarial examples for humor classifiers."
  - [section] "GPT-4 edited texts were rated comparably to non-humorous human tweets despite being derived from humorous tweets."
- Break condition: If humor relies on language-specific cultural references or linguistic structures that don't transfer between languages, GPT-4 would struggle to maintain coherence while removing humor in different languages.

### Mechanism 3
- Claim: Synthetic data generated by LLMs for humor detection tasks can be nearly as effective as human-curated data for training classifiers.
- Mechanism: Humor classifiers trained on synthetic unfunny data from GPT-4 perform nearly as well as those trained on human-edited data, demonstrating the quality of LLM-generated training data.
- Core assumption: LLMs can generate synthetic data that captures the essential characteristics of unfunny text while maintaining the structural properties needed for effective classifier training.
- Evidence anchors:
  - [abstract] "Humor classifiers trained on synthetic unfunny data from GPT-4 perform nearly as well as those trained on human-edited data."
  - [section] "When validated on human data, humor classifiers trained on GPT-4's synthetic unfun data are very performant, incurring the smallest accuracy drop relative to human-edited training data."
- Break condition: If the synthetic data fails to capture subtle nuances of humor that humans naturally include, classifiers trained on this data would perform poorly on real-world examples.

## Foundational Learning

- Concept: Understanding the incongruity theory of humor
  - Why needed here: The ROBERTA-SWAP approach is explicitly based on this theory, which associates humor with surprise and unexpected word choices.
  - Quick check question: Can you explain why replacing low-probability tokens with high-probability ones might reduce humor in a text?

- Concept: Few-shot learning and in-context prompting
  - Why needed here: The approach uses few-shot prompting with exemplar pairs to guide the LLM in editing humor, which is critical for the unfunning task.
  - Quick check question: How does providing in-context examples help guide the LLM's editing behavior?

- Concept: Code-mixed language processing
  - Why needed here: The English-Hindi dataset requires understanding how humor functions in code-mixed text and how to maintain coherence across language boundaries.
  - Quick check question: What challenges arise when trying to detect and remove humor from text that switches between languages?

## Architecture Onboarding

- Component map: Prompting module -> LLM (GPT-4, GPT-3.5, Mistral, ROBERTA-SWAP) -> Filtering mechanism -> Human evaluation pipeline -> Classifier training -> Evaluation
- Critical path: Prompting → LLM generation → Filtering → Human evaluation → Classifier training → Evaluation
- Design tradeoffs: Using ROBERTA-SWAP is computationally cheaper but may underperform more sophisticated LLMs; using GPT-4 provides better results but at higher cost; filtering with GPT-4 itself may introduce bias.
- Failure signatures: If humor classifiers trained on synthetic data perform significantly worse than those trained on human data, it indicates the LLM is not capturing essential humor characteristics; if coherence scores drop significantly, it suggests the editing process is too aggressive.
- First 3 experiments:
  1. Test ROBERTA-SWAP with different values of k (number of tokens to swap) to find the optimal balance between humor removal and coherence preservation.
  2. Compare GPT-4 unfunning performance on English satirical headlines versus code-mixed English-Hindi tweets to quantify cross-lingual generalization.
  3. Evaluate the impact of filtering low-quality outputs with GPT-4 versus human filtering on the overall quality of the synthetic dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different LLM architectures compare in their ability to remove humor from text while preserving coherence?
- Basis in paper: [explicit] The paper compares GPT-4, GPT-3.5, Mistral-7B, and a Roberta-based swap method on the Unfun task.
- Why unresolved: While the paper shows GPT-4 and GPT-3.5 outperform humans at unfunning satirical headlines, it doesn't provide a comprehensive comparison of different architectures on this specific task. The study focuses on a few popular models rather than exploring the full range of LLM capabilities.
- What evidence would resolve it: A systematic evaluation comparing multiple LLM architectures (including open-source models) on a diverse set of humor datasets, measuring both unfunning quality and preservation of coherence and meaning.

### Open Question 2
- Question: Can the unfunning capability of LLMs generalize to more diverse humor styles and cultural contexts beyond satirical headlines and code-mixed tweets?
- Basis in paper: [explicit] The paper tests generalization to English-Hindi tweets but acknowledges limitations in scope to two specific domains.
- Why unresolved: The study demonstrates generalization to a related task (English-Hindi tweets) but doesn't explore whether the unfunning ability extends to other forms of humor like puns, sarcasm, or culturally specific jokes. The authors themselves note this as a limitation.
- What evidence would resolve it: Experiments testing LLMs on diverse humor datasets including puns, sarcasm, cultural jokes, and multimodal humor, measuring unfunning performance across different humor styles and cultural contexts.

### Open Question 3
- Question: What are the fundamental differences in LLM behavior that enable successful humor removal but limit humor generation?
- Basis in paper: [explicit] The authors hypothesize that autoregressive training and maximum likelihood estimation enable high-probability substitutions for unfunning but lack the creative spark for joke generation.
- Why unresolved: This remains a hypothesis without empirical validation. The paper observes the asymmetry but doesn't investigate the underlying mechanisms or test interventions that might bridge this gap.
- What evidence would resolve it: Controlled experiments comparing attention patterns, probability distributions, and sampling strategies during humor generation versus unfunning tasks, potentially testing whether fine-tuning or prompting strategies can improve both capabilities simultaneously.

## Limitations

- The approach may not generalize well to humor styles beyond satirical headlines and code-mixed tweets, such as puns, sarcasm, or culturally-specific jokes.
- Computational cost is significant, particularly for GPT-4-based unfunning, creating scalability challenges for large-scale dataset creation.
- Human evaluation introduces subjective variability that isn't fully characterized, and the ROBERTA-SWAP approach underperforms more sophisticated LLMs in preserving coherence.

## Confidence

- High Confidence: The core claim that LLMs can be used to create aligned humor datasets by editing existing jokes to remove humor. This is well-supported by multiple experiments showing consistent performance across different models and datasets.
- Medium Confidence: The claim that synthetic data from GPT-4 performs nearly as well as human-edited data for training humor classifiers. While results are promising, the evaluation is limited to specific classifier architectures and would benefit from broader validation.
- Low Confidence: The implication that LLMs struggle to generate novel humor. This claim is based on comparisons with satirical writers but doesn't explore the full potential of different prompting strategies or more sophisticated generation techniques.

## Next Checks

1. **Cross-Cultural Generalization Test:** Evaluate the unfunning approach on humor datasets from different cultural contexts (e.g., Japanese puns, British irony, or Middle Eastern satire) to assess whether the token-swapping mechanism and LLM-based editing generalize beyond the tested Western and Indian contexts.

2. **Longitudinal Stability Analysis:** Track the performance of humor classifiers trained on synthetic unfunny data over time as the underlying LLMs are updated or as new humor styles emerge, to determine whether synthetic training data maintains its effectiveness as a long-term solution for humor detection.

3. **Adversarial Robustness Evaluation:** Test whether humor classifiers trained on synthetic data are more vulnerable to adversarial attacks that exploit the systematic patterns in LLM-generated unfunny text, compared to classifiers trained on human-edited data.