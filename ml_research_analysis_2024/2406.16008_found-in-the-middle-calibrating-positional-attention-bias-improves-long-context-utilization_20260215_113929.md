---
ver: rpa2
title: 'Found in the Middle: Calibrating Positional Attention Bias Improves Long Context
  Utilization'
arxiv_id: '2406.16008'
source_url: https://arxiv.org/abs/2406.16008
tags:
- attention
- bias
- arxiv
- document
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the "lost-in-the-middle" problem, where
  large language models struggle to locate relevant information placed in the middle
  of long input contexts. The authors establish that this phenomenon is linked to
  a U-shaped attention bias, where tokens at the beginning and end of the input receive
  higher attention, regardless of their relevance.
---

# Found in the Middle: Calibrating Positional Attention Bias Improves Long Context Utilization

## Quick Facts
- arXiv ID: 2406.16008
- Source URL: https://arxiv.org/abs/2406.16008
- Reference count: 21
- Key outcome: Attention calibration improves RAG performance by up to 15 percentage points

## Executive Summary
This work investigates the "lost-in-the-middle" problem where large language models struggle to locate relevant information placed in the middle of long input contexts. The authors establish that this phenomenon is linked to a U-shaped attention bias, where tokens at the beginning and end of the input receive higher attention, regardless of their relevance. To address this, they propose "found-in-the-middle," a calibration mechanism that disentangles positional bias from model attention, allowing the model to attend to contexts based on their true relevance. Experiments across multiple datasets and models demonstrate that this approach significantly improves retrieval-augmented generation performance.

## Method Summary
The proposed "found-in-the-middle" method calibrates attention scores by estimating and removing positional bias from the attention distribution. The approach involves computing attention scores between documents and the query, then subtracting the estimated positional bias term to obtain relevance-based attention. This calibrated attention is used for document selection and generation, allowing the model to focus on truly relevant content rather than being influenced by position in the input sequence.

## Key Results
- Attention calibration improves RAG performance by up to 15 percentage points
- U-shaped attention bias persists even after randomly shuffling document order
- Strong correlation between high-attention documents and their use in model generation

## Why This Works (Mechanism)

### Mechanism 1
LLMs exhibit a U-shaped attention bias where tokens at the beginning and end of the input receive higher attention, regardless of their relevance. The attention distribution in LLMs inherently prioritizes the first and last positions in the input sequence due to the way positional encodings interact with self-attention layers. This is an intrinsic property of the transformer architecture that persists across different model sizes and training regimes.

### Mechanism 2
The positional attention bias directly causes the lost-in-the-middle problem by making models rely on beginning/end documents regardless of relevance. The U-shaped attention distribution causes models to generate responses based on high-attention documents, even when these documents are irrelevant to the query. Model generation is strongly influenced by attention weights, and high-attention documents are preferentially used in output generation.

### Mechanism 3
Calibrating attention by removing positional bias allows models to attend to contexts based on true relevance rather than position. By estimating and subtracting the positional bias term from attention scores, the remaining attention reflects only the relevance of documents to the query. Attention can be decomposed into relevance and positional bias components that can be separated mathematically.

## Foundational Learning

- Concept: Self-attention mechanism in transformers
  - Why needed here: Understanding how attention weights are computed and how they influence token selection is crucial for grasping the positional bias problem.
  - Quick check question: How does self-attention compute attention weights between tokens, and what role do positional encodings play in this computation?

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: The paper's experiments and proposed solution are specifically designed for RAG tasks where models retrieve documents to answer queries.
  - Quick check question: What are the key components of a RAG system, and how does the lost-in-the-middle problem specifically affect RAG performance?

- Concept: Transformer positional encodings
  - Why needed here: The positional bias discussed in the paper is directly related to how positional information is encoded and used in transformer models.
  - Quick check question: What are the different types of positional encodings used in transformers, and how might they contribute to positional bias in attention?

## Architecture Onboarding

- Component map: Input layer -> Attention computation -> Calibration layer -> Generation layer -> Output layer
- Critical path: 1. Retrieve documents for query, 2. Compute original attention weights, 3. Apply calibration to remove positional bias, 4. Use calibrated attention for generation, 5. Produce final answer
- Design tradeoffs: Calibration accuracy vs computational overhead (requires extra forward passes), complete bias removal vs preserving potentially useful positional information, simple linear model vs more complex bias estimation
- Failure signatures: Calibration removes too much positional information, hurting performance; bias estimation is inaccurate, leading to poor calibration; computational overhead makes method impractical for real-time applications
- First 3 experiments: 1. Reproduce lost-in-the-middle phenomenon with Vicuna-7b on NaturalQuestions to confirm U-shaped attention bias, 2. Implement attention calibration and verify it improves document ranking accuracy, 3. Test calibration on full RAG task to measure end-to-end performance improvement

## Open Questions the Paper Calls Out

### Open Question 1
What are the exact mechanisms that cause the positional attention bias in LLMs? The paper states that the root cause of attention bias is unclear, and attributes it to potential factors like pretraining corpora distribution, transformer architecture, and optimization process. This remains unresolved as the paper does not provide a definitive explanation for why LLMs exhibit positional attention bias. Systematic experiments analyzing different pretraining corpora, architectural modifications, and optimization techniques to isolate the specific causes of positional bias would be needed.

### Open Question 2
How can the computational overhead of attention calibration be reduced while maintaining its effectiveness? The paper mentions that attention calibration introduces additional computational overhead, requiring extra O(K) model forward passes compared to vanilla generation. This remains unresolved as while the paper demonstrates the effectiveness of attention calibration, it does not explore methods to optimize the computational efficiency of the approach. Developing and testing more efficient calibration methods, such as approximations or selective application of calibration, would provide evidence for reducing computational overhead.

### Open Question 3
Are there specific tasks or scenarios where positional attention bias is beneficial, and how can we identify them? The paper acknowledges that positional bias might be beneficial in certain contexts but does not explore this possibility in detail. This remains unresolved as the paper focuses on mitigating positional bias without investigating potential scenarios where it could be advantageous. Conducting experiments across diverse tasks and analyzing performance with and without positional bias could identify scenarios where the bias is beneficial.

## Limitations

- The mathematical formulation of attention bias decomposition may be oversimplified
- Experiments focus primarily on two models and two datasets, limiting generalizability
- Computational overhead of calibration mechanism is not fully characterized for real-world deployment

## Confidence

**High Confidence**: The existence of U-shaped attention bias is well-supported by empirical evidence showing consistent positional patterns across shuffled documents and multiple models. The correlation between high-attention documents and generation output is demonstrated with clear experimental results.

**Medium Confidence**: The effectiveness of the calibration mechanism is demonstrated empirically, but the theoretical justification for why the bias removal formula works could be more rigorous. The claim that positional bias is the primary driver of lost-in-the-middle behavior is plausible but may oversimplify the problem.

**Low Confidence**: The generalizability of findings to models with different attention mechanisms or different positional encoding schemes remains untested. The claim that this is the "key factor" in long-context utilization may overstate the case.

## Next Checks

1. Test the calibration mechanism on models with different attention architectures (e.g., LongFormer, BigBird) and positional encoding schemes to verify that the U-shaped bias and calibration effectiveness generalize beyond standard transformers.

2. Systematically vary the positional bias estimation (e.g., use different reference documents, test different bias models) to determine which components are most critical for the calibration's success and whether the current approach is optimal.

3. Implement the calibration mechanism in a production-like setting to measure the actual computational overhead, latency impact, and memory requirements across different batch sizes and context lengths.