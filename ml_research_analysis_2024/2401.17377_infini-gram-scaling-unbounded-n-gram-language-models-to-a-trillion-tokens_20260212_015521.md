---
ver: rpa2
title: 'Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens'
arxiv_id: '2401.17377'
source_url: https://arxiv.org/abs/2401.17377
tags:
- gram
- data
- n-gram
- neural
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper modernizes n-gram language models by scaling training\
  \ data to 5 trillion tokens and extending n to unbounded values, creating an \u221E\
  -gram LM. A new engine, infini-gram, powered by suffix arrays, enables efficient\
  \ computation of \u221E-gram probabilities with millisecond latency."
---

# Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens

## Quick Facts
- **arXiv ID**: 2401.17377
- **Source URL**: https://arxiv.org/abs/2401.17377
- **Reference count**: 40
- **Primary result**: ∞-gram LM achieves 47% next-token prediction accuracy on human text and reduces perplexity by up to 73% when interpolated with neural LMs

## Executive Summary
This paper modernizes n-gram language models by scaling training data to 5 trillion tokens and extending n to unbounded values, creating an ∞-gram LM. A new engine, infini-gram, powered by suffix arrays, enables efficient computation of ∞-gram probabilities with millisecond latency. Experiments show ∞-gram achieves 47% next-token prediction accuracy on human text and significantly reduces perplexity when combined with neural LMs (up to 73% improvement). Analysis of machine-generated text reveals irregularities in agreement with ∞-gram across suffix lengths, suggesting issues in Transformer pretraining. The work provides a scalable, efficient n-gram/∞-gram engine with a public API and Python package, enabling novel analyses of large text corpora and improving neural LMs.

## Method Summary
The paper trains an ∞-gram language model on 5 trillion tokens from combined open-source corpora, using a suffix array-based engine for efficient n-gram counting without pre-computation. The suffix array stores starting positions of all suffixes lexicographically, allowing binary search to find n-gram occurrences in O(L + log N) time. During inference, the entire index stays on-disk and supports COUNT and QUERY operations with <20ms latency. The ∞-gram LM uses backoff from n=∞ until denominator > 0, and when interpolated with neural LMs using separate λ parameters for sparse and non-sparse estimates, achieves up to 73% perplexity reduction.

## Key Results
- ∞-gram achieves 47% next-token prediction accuracy on human-written text from Pile validation set
- When interpolated with neural LMs, ∞-gram reduces perplexity by up to 73% compared to neural LMs alone
- Analysis reveals irregularities in agreement with machine-generated text across suffix lengths, suggesting issues in Transformer pretraining
- The suffix array engine supports millisecond-level latency for n-gram counting on trillion-token corpora

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Suffix arrays enable efficient n-gram counting by exploiting lexicographical ordering of all suffixes
- Mechanism: The suffix array stores starting positions of suffixes ranked lexicographically. For any n-gram, all its occurrences lie in a single consecutive segment of the suffix array, so binary search finds the first and last occurrence positions, and the count is their difference
- Core assumption: The dataset is tokenized and documents are separated by a unique token (`\xff\xff`)
- Evidence anchors:
  - [abstract]: "we develop an engine named infini-gram—powered by suffix arrays—that can compute ∞-gram (as well as n-gram with arbitrary n) probabilities with millisecond-level latency."
  - [section]: "A suffix array represents the lexicographical ordering of all suffixes of an array... By construction, the occurrence positions of strings starting with x1...xn lies in a single, consecutive segment in the suffix array."

### Mechanism 2
- Claim: The ∞-gram LM avoids expensive pre-computation of count tables by computing n-gram counts on-demand
- Mechanism: Instead of storing all possible n-gram counts, the system builds a suffix array over the raw token array. When an n-gram query arrives, it uses the suffix array to count occurrences in O(L + log N) time, where L is the n-gram length and N is the corpus size
- Core assumption: n-gram counts can be computed quickly enough to support millisecond-level inference latency
- Evidence anchors:
  - [abstract]: "Instead of pre-computing n-gram count tables (which would be very expensive), we develop an engine named infini-gram—powered by suffix arrays—that can compute ∞-gram probabilities with millisecond-level latency."
  - [section]: "During inference, the entire infini-gram index can stay on-disk... On RedPajama, our most optimized infini-gram engine can count a given n-gram with an average latency of less than 20 milliseconds."

### Mechanism 3
- Claim: ∞-gram LMs improve neural LMs by providing complementary predictions on sparse contexts
- Mechanism: When neural LM confidence is low (low probability on actual next token), ∞-gram often agrees better, especially on sparse estimates where the next token is unique in the corpus. Interpolating with two separate λ parameters for sparse and non-sparse estimates captures this complementarity
- Core assumption: Sparse ∞-gram estimates correlate with higher agreement on human-written text
- Evidence anchors:
  - [abstract]: "we show that ∞-gram can complement neural LLMs and reach better performance when combined: heuristically interpolating between the estimates made by ∞-gram and neural LMs can greatly reduce perplexity (by up to 73%) compared to the neural LMs alone."
  - [section]: "When the neural LM performance is very poor (left side of the histogram), ∞-gram still gives a non-trivial agreement of above 20%; if only considering tokens with sparse ∞-gram estimates, the agreement is as high as 50%."

## Foundational Learning

- Concept: Suffix array construction and binary search
  - Why needed here: Core to enabling fast n-gram counting without pre-computed tables
  - Quick check question: How does the suffix array guarantee that all occurrences of an n-gram lie in a consecutive segment?

- Concept: N-gram backoff and probability normalization
  - Why needed here: ∞-gram LM uses backoff from n=∞ until denominator > 0; understanding ensures valid probability distributions
  - Quick check question: Why does ∞-gram not require additional discounting like Katz backoff?

- Concept: Interpolation of language models
  - Why needed here: Combining ∞-gram with neural LMs to reduce perplexity
  - Quick check question: What is the effect of using different λ values for sparse vs. non-sparse ∞-gram estimates?

## Architecture Onboarding

- Component map: Token array -> Suffix array -> Document offset index -> Document metadata index -> Query engine -> Web API -> Python client

- Critical path:
  1. Tokenize corpus → byte array
  2. Build suffix array (linear time)
  3. Store on-disk, memory-map during inference
  4. Handle query → binary search in suffix array → count → compute probability

- Design tradeoffs:
  - Storage vs. speed: Suffix array uses 7N bytes but avoids massive n-gram tables
  - Single shard vs. sharded: Sharding saves RAM but adds S·log N latency; parallelization mitigates
  - On-disk vs. in-memory: On-disk minimizes compute but adds I/O latency; pre-fetching helps

- Failure signatures:
  - High latency → shard count too large or insufficient parallelization
  - Incorrect counts → tokenization mismatch or document separator missing
  - Memory pressure → shard size exceeds RAM during suffix array construction

- First 3 experiments:
  1. Build suffix array on small synthetic corpus, verify counts match brute-force
  2. Benchmark COUNT query latency on shard counts 1, 2, 4, 8; measure speedup from parallelization
  3. Interpolate ∞-gram with small neural LM; tune λ1, λ2 on validation set; measure perplexity reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do ∞-gram's accuracy and agreement with human-written text vary across different domains and genres of text?
- Basis in paper: [explicit] The paper analyzes ∞-gram on Pile-val, which aggregates results from all domains, but does not provide domain-specific analysis
- Why unresolved: Domain-specific differences in language patterns and context usage could significantly impact ∞-gram's performance, but the paper does not explore this
- What evidence would resolve it: Analyzing ∞-gram's performance on individual domains of Pile-val or other datasets with clear domain distinctions would reveal domain-specific strengths and weaknesses

### Open Question 2
- Question: Can ∞-gram be effectively integrated into text generation tasks beyond perplexity improvement, and what are the potential benefits and drawbacks?
- Basis in paper: [explicit] The paper mentions preliminary experiments showing ∞-gram might harm open-ended text generation, but does not explore this further
- Why unresolved: The paper only briefly touches on this issue, and the reasons for potential harm are not fully investigated
- What evidence would resolve it: Systematic experiments comparing text generation quality with and without ∞-gram interpolation, along with qualitative analysis of generated text, would clarify the impact of ∞-gram on generation tasks

### Open Question 3
- Question: How does the performance of ∞-gram compare to other nonparametric language modeling approaches, such as those based on vector retrieval or chunk retrieval?
- Basis in paper: [explicit] The paper mentions related work on nonparametric LMs but does not provide direct comparisons
- Why unresolved: The paper focuses on ∞-gram's performance relative to neural LMs, but does not benchmark it against other nonparametric methods
- What evidence would resolve it: Experiments comparing ∞-gram to state-of-the-art nonparametric LMs on standard language modeling benchmarks would reveal relative strengths and weaknesses

## Limitations
- Scale and resource requirements: Suffix array approach requires ~10TB disk space and significant CPU time for building index over 5 trillion tokens
- Limited domain generalization: Evaluation focuses primarily on Pile dataset text and a limited set of neural models (Llama-2, GPT-Neo/J)
- Comparative analysis gaps: Limited direct comparison with contemporary n-gram methods using similar data scales or other exact-match retrieval methods

## Confidence
- **High Confidence**: Suffix array mechanism for efficient n-gram counting; 47% next-token prediction accuracy claim
- **Medium Confidence**: Perplexity reduction when interpolated with neural LMs; interpretation of agreement irregularities with machine-generated text
- **Low Confidence**: Assertion of superiority to all other language models in sparse contexts; practical utility of public API for novel analyses

## Next Checks
1. **Scalability Validation**: Reproduce suffix array construction and query latency benchmarks on 100B token corpus with different shard counts and parallelization strategies
2. **Cross-Domain Performance**: Evaluate ∞-gram next-token prediction accuracy and neural LM interpolation benefits on scientific papers, code repositories, and multilingual corpora
3. **Comparative Method Analysis**: Implement and compare ∞-gram with exact n-gram counting using hash tables and FM-index based methods on same corpus