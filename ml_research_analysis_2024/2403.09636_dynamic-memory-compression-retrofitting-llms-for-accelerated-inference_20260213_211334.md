---
ver: rpa2
title: 'Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference'
arxiv_id: '2403.09636'
source_url: https://arxiv.org/abs/2403.09636
tags:
- compression
- cache
- memory
- inference
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper proposes Dynamic Memory Compression (DMC) for auto-regressive\
  \ language models, addressing the inefficiency caused by the linear growth of key-value\
  \ cache memory with sequence length and batch size. DMC retrofits pre-trained LLMs\
  \ (Llama 2 7B, 13B, 70B) to compress the KV cache online during inference, achieving\
  \ up to 7\xD7 throughput increase on NVIDIA H100 GPUs with up to 4\xD7 cache compression\
  \ without adding parameters."
---

# Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference

## Quick Facts
- arXiv ID: 2403.09636
- Source URL: https://arxiv.org/abs/2403.09636
- Reference count: 40
- Key outcome: DMC achieves up to 7× throughput increase with 4× KV cache compression on NVIDIA H100 GPUs

## Executive Summary
Dynamic Memory Compression (DMC) addresses the inefficiency of auto-regressive language models by compressing the key-value cache during inference. The method learns to adaptively merge tokens, reducing memory usage while maintaining performance. DMC retrofits pre-trained LLMs (Llama 2 7B, 13B, 70B) without adding parameters, achieving significant throughput improvements through sublinear cache growth.

## Method Summary
DMC retrofits pre-trained LLMs to compress KV cache online during inference by learning to decide whether to append or accumulate key-value representations. The method uses Gumbel-softmax relaxation for training discrete decisions, with target compression ratios that increase linearly during continued pre-training. It can be combined with grouped-query attention for compounded benefits and requires approximately 2-8% of the original training data for fine-tuning.

## Key Results
- 350-390% inference throughput increase for Llama 2 7B and 13B models
- 4× KV cache compression without performance degradation
- 16× total compression when combined with GQA 8× on Llama 2 70B
- Maintains downstream performance on MMLU, HumanEval, and CS-QA benchmarks

## Why This Works (Mechanism)

### Mechanism 1
DMC enables sublinear growth of KV cache memory by learning to merge tokens during inference. During each time step, it predicts binary decisions to either append or accumulate current key-value representations with the top cache element using weighted averages based on importance scores. This learned accumulation reduces stored items while preserving semantic content.

### Mechanism 2
DMC improves inference throughput by reducing memory-bound operations. By compressing the KV cache, it decreases the data read from high-bandwidth memory during attention calculations, which dominate latency in auto-regressive generation. The ratio of FLOPS to input size remains constant, making MHSA layers memory-bound regardless of batch size.

### Mechanism 3
DMC can be combined with existing compression methods like GQA for compounded benefits. Since DMC operates independently of the underlying attention mechanism structure, it can further compress already-reduced KV caches from GQA, achieving multiplicative compression ratios.

## Foundational Learning

- **Gradient estimation for discrete decisions**: DMC requires binary append/accumulate decisions during inference but needs differentiable signals for training. The Gumbel-softmax trick enables gradient flow through these discrete decisions.

- **Attention mask manipulation**: During training, all intermediate KV states must be visible but masked to simulate inference behavior where only compressed states are accessible. Mathematical masking ensures queries can only attend to the last state of each compressed segment.

- **Memory-bound vs compute-bound operations**: Understanding why KV cache compression improves throughput requires knowing that attention calculations are memory-bound in auto-regressive generation, while linear layers eventually become compute-bound regardless of batch size.

## Architecture Onboarding

- **Component map**: Attention layer with modified KV cache update logic -> Decision predictor (α) using first dimension of kt -> Importance predictor (ω) using first dimension of qt -> Running sum tracker (z) for accumulation weighting -> Masking system for training-time simulation

- **Critical path**: Forward pass → α and ω prediction → KV cache update (append or accumulate) → attention calculation with compressed cache

- **Design tradeoffs**: Fixed vs learned compression boundaries (DMC vs Fixed Memory Pooling) vs global vs local (layer-wise) compression priors vs in-layer relaxation strategies (DMC-C vs DMC-HardC) vs training regime choices (SHORT vs LONG for CR annealing)

- **Failure signatures**: Performance degradation at high compression ratios vs training instability when CR increases too quickly vs suboptimal compression patterns (e.g., compressing early layers too heavily) vs memory overhead from window grouping approximation

- **First 3 experiments**: Implement basic DMC mechanism with append/accumulate decisions and verify cache size reduction vs add training pipeline with Gumbel-softmax relaxation and masking to simulate inference behavior vs test combined DMC+GQA on a pre-trained model to verify compatibility and measure compounded compression ratio

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on continued pre-training with undisclosed dataset composition and blending ratios
- Effectiveness depends heavily on quality of learned compression boundaries that may not generalize across all model architectures or domains
- Behavior in bidirectional or encoder-only settings remains unexplored

## Confidence
- **High Confidence**: Sublinear KV cache growth through learned accumulation boundaries and observed throughput improvements (350-390%) on H100 GPUs
- **Medium Confidence**: Successful combination with GQA for compounded compression ratios on Llama 2 70B
- **Low Confidence**: Assertion that DMC learns "meaningful" accumulation boundaries that preserve semantic content is primarily inferred from maintained downstream performance

## Next Checks
1. **Ablation Study on Compression Decision Features**: Replace the first dimension of qt and kt with random noise or alternative feature extraction methods to verify that learned compression boundaries genuinely capture semantic information.

2. **Cross-Domain Generalization Test**: Apply DMC-trained models to domains significantly different from continued pre-training data (e.g., medical or legal text) to evaluate whether learned compression patterns transfer or require domain-specific fine-tuning.

3. **Memory-Bandwidth Sensitivity Analysis**: Systematically vary HBM bandwidth and cache sizes on different GPU architectures to quantify how DMC's performance benefits scale with hardware constraints and identify break-even points where compression benefits plateau.