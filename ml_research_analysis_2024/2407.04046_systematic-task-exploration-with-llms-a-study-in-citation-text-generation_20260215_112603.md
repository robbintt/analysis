---
ver: rpa2
title: 'Systematic Task Exploration with LLMs: A Study in Citation Text Generation'
arxiv_id: '2407.04046'
source_url: https://arxiv.org/abs/2407.04046
tags:
- citation
- work
- generation
- text
- related
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores how task inputs and instructions impact citation
  text generation with large language models. Using a framework of systematic input
  manipulation, a reference dataset, and a comprehensive measurement kit, the authors
  investigate the effects of different input configurations and instructions on model
  outputs.
---

# Systematic Task Exploration with LLMs: A Study in Citation Text Generation

## Quick Facts
- arXiv ID: 2407.04046
- Source URL: https://arxiv.org/abs/2407.04046
- Reference count: 40
- The paper investigates how task inputs and instructions impact citation text generation with large language models, finding that free-form citation intents and example sentences improve performance compared to abstracts alone.

## Executive Summary
This paper presents a systematic framework for investigating how different input configurations and task instructions affect citation text generation with large language models. The authors develop a comprehensive measurement kit and conduct controlled experiments with Llama 2-Chat and GPT 3.5 Turbo to evaluate various combinations of input components including abstracts, citation intents, and example sentences. Their findings reveal that providing free-form citation intents and example sentences significantly improves generation quality compared to using abstracts alone, and that the type of evaluation metric used can reveal different aspects of model performance.

## Method Summary
The authors construct a dataset of citation paragraphs from the ACL Anthology, enriched with paper metadata and example citation sentences. They systematically manipulate input components (abstracts, categorical intents, free-form intents, example sentences) and task instructions using six different prompt templates. The study employs two state-of-the-art LLMs with greedy decoding to generate citation texts, which are then evaluated using a comprehensive measurement kit including surface metrics, conventional metrics (ROUGE, BERTScore, BLEURT), and NLI-based metrics (TRUE, SummaC). Human evaluation using the pyramid method assesses fact coverage for a subset of samples.

## Key Results
- Free-form citation intents and example sentences improve citation text generation performance compared to using abstracts alone
- The type of evaluation metric significantly impacts observed differences between model outputs
- Measurement type plays a significant role in revealing differences between model outputs
- Human evaluation confirms the importance of input components and highlights the complementary nature of different measurements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Providing free-form citation intents and example sentences improves citation text generation performance compared to using abstracts alone.
- Mechanism: The additional context from free-form intents and example sentences guides the LLM to generate more relevant and factually consistent citation text that better matches the reference paragraph.
- Core assumption: Free-form intents and example sentences provide meaningful additional information beyond what is available in the abstracts, steering generation toward the reference paragraph without leaking excessive information.
- Evidence anchors:
  - [abstract]: "Our results highlight the importance of systematically investigating both task instruction and input configuration when prompting LLMs, and reveal non-trivial relationships between different evaluation metrics used for citation text generation."
  - [section]: "Our experiments with two state-of-the-art LLMs – Llama 2-Chat (Touvron et al., 2023b) and GPT 3.5 Turbo (Ouyang et al., 2022) reveal that input components and task instructions both impact the generations, and their effects add up. Free-form citation intents, as illustrated in Figure 2, show promise as an alternative to categorical intents used in prior citation text generation work."
  - [corpus]: The dataset includes abstracts for both citing and cited papers, but the paper explicitly investigates whether additional components improve performance, suggesting abstracts alone are insufficient.
- Break condition: If free-form intents or example sentences contain excessive information from the reference paragraph, they would constitute data leakage rather than genuine guidance.

### Mechanism 2
- Claim: The type of evaluation metric used significantly impacts the observed differences between model outputs.
- Mechanism: Different metrics capture different aspects of generation quality - conventional metrics focus on surface similarity while NLI-based metrics assess factual consistency, leading to divergent rankings of model performance.
- Core assumption: The measurement kit includes diverse metrics that capture different aspects of generation quality, allowing for nuanced comparison of model outputs.
- Evidence anchors:
  - [abstract]: "Our results highlight the importance of systematically investigating both task instruction and input configuration when prompting LLMs, and reveal non-trivial relationships between different evaluation metrics used for citation text generation."
  - [section]: "We employ a range of measurements and metrics to characterize the citation texts generated by the LLMs in response to the prompt... To measure the factual consistency between the gold reference and the output, we use two NLI models trained on curated fact-checking datasets."
  - [corpus]: The paper compares multiple metrics (ROUGE, BERTScore, BLEURT, TRUE, SummaC) and finds they produce different results, with NLI-based metrics being more sensitive to differences between Llama 2-Chat and GPT 3.5 outputs.
- Break condition: If all metrics consistently rank models in the same order, the diversity of measurements would not be necessary to reveal performance differences.

### Mechanism 3
- Claim: The wording of task instructions significantly affects LLM output pragmatics, even when conventional metrics show similar scores.
- Mechanism: Different instruction templates can lead to different pragmatic interpretations by the LLM, affecting whether it generates comparative or descriptive text, which may not be captured by conventional evaluation metrics.
- Core assumption: The LLM interprets instructions semantically and generates text that follows the implied pragmatics of the instruction, not just the literal content requirements.
- Evidence anchors:
  - [abstract]: "Our results highlight the importance of systematically investigating both task instruction and input configuration when prompting LLMs..."
  - [section]: "We observe that providing models with free-form intents ( +IF) increases the correspondence between generated and reference citation texts more than example sentences (+E). Jointly providing free-form intent and example ( +IF+E) shows a combined effect and yields the best correspondence in 90.28% of 72 measurements."
  - [corpus]: The paper uses six different instruction templates with varying prompting strategies (direct instruction, chain-of-thought, role-playing, instruction list) to systematically investigate instruction effects.
- Break condition: If all instruction templates produce identical outputs with identical metric scores, instruction wording would not affect generation pragmatics.

## Foundational Learning

- Concept: Prompt engineering and its impact on LLM behavior
  - Why needed here: The paper systematically investigates how different prompt components (input configurations and instructions) affect citation text generation, requiring understanding of prompt engineering principles.
  - Quick check question: How does the combination of input components and instructions in a prompt affect LLM generation compared to using each component in isolation?

- Concept: Natural language generation evaluation metrics and their limitations
  - Why needed here: The paper uses a comprehensive measurement kit including conventional metrics (ROUGE, BERTScore, BLEURT) and NLI-based metrics (TRUE, SummaC) to evaluate generation quality.
  - Quick check question: What are the key differences between surface-level metrics like ROUGE and semantic metrics like BERTScore, and why might both be needed for comprehensive evaluation?

- Concept: Scholarly citation practices and related work section structure
  - Why needed here: The paper focuses on citation text generation for related work sections, requiring understanding of how citations function in academic writing and what information they typically contain.
  - Quick check question: What distinguishes citation text in related work sections from citations in other parts of academic papers, and how does this affect generation requirements?

## Architecture Onboarding

- Component map: Reference dataset (ACL Anthology based) -> Input component selection -> Instruction template selection -> Prompt construction -> LLM generation -> Output measurement -> Performance analysis
- Critical path: Reference dataset → Input component selection → Instruction template selection → Prompt construction → LLM generation → Output measurement → Performance analysis
- Design tradeoffs:
  - Single-citation vs. multi-citation paragraphs: Limiting to single citations simplifies input configuration but reduces task complexity and real-world applicability
  - Free-form vs. categorical intents: Free-form intents provide more guidance but risk information leakage; categorical intents are safer but less informative
  - Automatic vs. human evaluation: Automatic metrics are scalable but may miss pragmatic aspects; human evaluation is accurate but costly and subjective
- Failure signatures:
  - Poor performance with only abstracts (+A configuration) suggests input components are necessary
  - Similar metric scores across different instruction templates indicates metrics may not capture pragmatic differences
  - High correlation between conventional metrics suggests redundancy in measurement approach
- First 3 experiments:
  1. Baseline comparison: Generate citations using only abstracts (+A) vs. including all components (+A+IF+E) to establish baseline performance improvement
  2. Intent type comparison: Compare categorical intents (+IC) vs. free-form intents (+IF) to determine which provides better guidance
  3. Metric complementarity: Analyze correlation between different metric families to identify which measurements provide unique information about generation quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different types of instruction templates (e.g., direct instruction, chain-of-thought, role-playing) affect the performance of LLM-based citation text generation?
- Basis in paper: [explicit] The paper investigates six distinct human-written task instruction templates, including direct instruction, chain-of-thought, role-playing, and instruction list.
- Why unresolved: The paper focuses on the impact of input configurations and provides a comparison of different configurations, but does not delve deeply into the specific effects of different instruction templates on model performance.
- What evidence would resolve it: Systematic experiments comparing the performance of LLM-based citation text generation using different instruction templates, while controlling for other variables such as input configurations.

### Open Question 2
- Question: How do free-form citation intents compare to categorical intents in terms of their ability to guide generation and minimize information leaks?
- Basis in paper: [explicit] The paper introduces novel free-form citation intents and compares their performance to categorical intents, finding that free-form intents lead to better correspondence between generated and reference citation texts.
- Why unresolved: While the paper provides initial evidence suggesting the superiority of free-form intents, it does not fully explore the potential trade-offs between informativeness and information leaks.
- What evidence would resolve it: Further analysis of the relationship between the informativeness of citation intents and the risk of information leaks, potentially involving manual evaluation of generated texts and their similarity to gold references.

### Open Question 3
- Question: How do different evaluation metrics (e.g., conventional, NLI-based, surface-level) capture different aspects of citation text generation quality, and how should they be combined to provide a comprehensive assessment?
- Basis in paper: [explicit] The paper employs a wide range of measurements, including conventional metrics (ROUGE, BERTScore, BLEURT), NLI-based metrics (TRUE, SummaC), and surface-level measurements, and observes that these metrics capture different aspects of generation quality.
- Why unresolved: The paper highlights the complementary nature of different metrics but does not provide a definitive framework for combining them to obtain a holistic evaluation of citation text generation.
- What evidence would resolve it: Development of a principled approach for combining different evaluation metrics, potentially involving weighting schemes or machine learning models trained on human judgments of citation text quality.

## Limitations
- The evaluation framework relies heavily on automatic metrics, with human evaluation covering only 50 samples out of 8,048 total samples
- The dataset construction process for free-form intents using Flan-T5 introduces potential biases that aren't fully characterized
- The paper doesn't address potential information leakage when providing free-form intents and example sentences
- The focus on single-citation paragraphs limits applicability to real-world multi-citation scenarios

## Confidence

**High confidence**: The finding that input components significantly affect generation performance is well-supported by systematic experiments across multiple configurations and both evaluated LLMs.

**Medium confidence**: The claim about free-form intents being superior to categorical intents has strong empirical support but may be dataset-dependent.

**Low confidence**: The paper's assertions about the pragmatic effects of instruction wording are supported by metric analysis but lack direct human validation.

## Next Checks

1. **Human validation of instruction effects**: Conduct human evaluation specifically focused on whether different instruction templates produce citations with distinct pragmatic interpretations (comparative vs. descriptive) beyond what automatic metrics capture.

2. **Information leakage analysis**: Systematically measure n-gram overlap between generated free-form intents/example sentences and gold reference paragraphs to quantify potential data leakage and assess whether observed performance improvements reflect genuine guidance versus information copying.

3. **Multi-citation generation experiment**: Extend the framework to generate multi-citation paragraphs to evaluate whether input component effects scale to more realistic citation scenarios and to test the framework's generalizability beyond single-citation cases.