---
ver: rpa2
title: 'SeeGULL Multilingual: a Dataset of Geo-Culturally Situated Stereotypes'
arxiv_id: '2403.05696'
source_url: https://arxiv.org/abs/2403.05696
tags:
- stereotypes
- language
- languages
- multilingual
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SeeGULL Multilingual, a dataset of over 25,000
  stereotypes in 20 languages across 23 regions. It was created by combining LLM-generated
  candidate associations with culturally situated human annotations to ensure relevance
  and reliability.
---

# SeeGULL Multilingual: a Dataset of Geo-Culturally Situated Stereotypes

## Quick Facts
- arXiv ID: 2403.05696
- Source URL: https://arxiv.org/abs/2403.05696
- Authors: Mukul Bhutani; Kevin Robinson; Vinodkumar Prabhakaran; Shachi Dave; Sunipa Dev
- Reference count: 20
- Primary result: Dataset of 25,861 stereotypes across 20 languages and 23 regions, revealing higher model endorsement rates in multilingual contexts than English-only evaluations

## Executive Summary
This paper presents SeeGULL Multilingual, a comprehensive dataset of stereotypes in 20 languages across 23 regions, created by combining LLM-generated candidates with culturally situated human annotations. The dataset reveals significant regional variations in stereotype prevalence and offensiveness, demonstrating that stereotypes are not purely language-bound but depend on local cultural contexts. Crucially, the study shows that evaluating generative models in their native languages uncovers substantially higher stereotype endorsement rates compared to English translation, highlighting critical gaps in current AI safety assessments.

## Method Summary
The dataset was created through a two-phase process: first, few-shot prompting with a multilingual LLM generated stereotype candidates using nationality demonyms across 20 languages; second, native speakers from 23 regions validated these candidates, labeling them as stereotypical or not and rating offensiveness. The human annotation phase was essential for ensuring cultural relevance, as the same language can yield different stereotype profiles across countries (e.g., Portuguese stereotypes in Portugal vs. Brazil). The resulting dataset contains 25,861 stereotypes with comprehensive metadata including cultural context and offensiveness ratings.

## Key Results
- Regional variations in stereotype prevalence: Portuguese stereotypes marked as stereotypical by 45.4% of annotators in Portugal vs. 74.6% in Brazil
- Multilingual models show higher stereotype endorsement rates than English-only evaluations, with safety gaps exposed when testing in native languages
- 25,861 stereotypes across 20 languages and 23 regions, validated by culturally situated human annotators

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated candidates + culturally situated human validation yields higher coverage and cultural relevance than English-only or translation-based methods.
- Mechanism: LLMs produce large volumes of candidate stereotypes quickly; human annotators from each region filter and validate them in situ, capturing local nuance.
- Core assumption: LLMs can generate plausible stereotype associations in many languages, and local annotators can reliably judge cultural relevance.
- Evidence anchors:
  - [abstract] "combines LLM generations for scale with culturally situated validations for reliability"
  - [section 2.3] "human annotations across 23 regions"
  - [corpus] Weak - no explicit comparative evaluation vs. English-only or translated baselines.
- Break condition: If LLM generations are culturally inappropriate or annotators misjudge relevance, dataset quality drops.

### Mechanism 2
- Claim: Country-level annotation differences reveal that stereotypes are not purely language-bound but depend on regional socio-cultural context.
- Mechanism: Annotating the same language in two different countries surfaces divergent stereotype endorsement rates.
- Core assumption: Stereotypes reflect local cultural norms rather than just linguistic patterns.
- Evidence anchors:
  - [section 2.3] "region of annotator residence impacts socially subjective tasks like stereotype annotations"
  - [section 3] "Portugal vs Brazil: 45.4% vs 74.6% marked as stereotypical"
  - [corpus] Weak - corpus mentions the finding but no quantitative comparison metrics.
- Break condition: If annotators are not culturally embedded, regional differences will be missed.

### Mechanism 3
- Claim: Evaluating models in-language uncovers higher stereotype endorsement than English translation, exposing safety gaps.
- Mechanism: Prompts in native language yield different model behavior than translated prompts, due to multilingual model training bias toward English.
- Core assumption: Multilingual models embed different cultural biases in different languages.
- Evidence anchors:
  - [abstract] "higher endorsement rates in multilingual contexts than in English-only evaluations"
  - [section 4.2] "all models endorse stereotypes present in SGM... English-translated queries would have missed a significant fraction"
  - [corpus] Weak - no direct comparison with models evaluated solely in English.
- Break condition: If models are equally calibrated across languages, in-language vs. English difference disappears.

## Foundational Learning

- Concept: Cross-cultural evaluation design
  - Why needed here: Ensures that safety checks are not biased toward a single cultural perspective, which would miss region-specific harms.
  - Quick check question: What is the minimum number of distinct regions needed to capture meaningful cultural variation for a language used in multiple countries?

- Concept: Human-in-the-loop dataset creation
  - Why needed here: Scales stereotype coverage while maintaining cultural validity, avoiding the pitfalls of pure LLM generation.
  - Quick check question: How many annotator labels per candidate are required to achieve stable stereotype classification in subjective tasks?

- Concept: Prompt-based model evaluation for stereotypes
  - Why needed here: Enables standardized measurement of model endorsement across languages without retraining models.
  - Quick check question: How do you design a prompt that avoids leakage of the correct answer while still being natural in the target language?

## Architecture Onboarding

- Component map: LLM generation -> Human annotation -> Aggregation -> Dataset storage -> Model evaluation
- Critical path: Generation → Annotation → Aggregation → Model evaluation
  - Bottleneck: Human annotation cost and turnaround time
  - Mitigation: Parallel annotation across regions; sample subset for quality control
- Design tradeoffs:
  - Coverage vs. Cost: More languages/regions increase cost exponentially
  - Generality vs. Specificity: Broad stereotypes miss nuanced harms; narrow stereotypes limit utility
  - Prompt naturalness vs. Evaluation precision: Complex instructions may reduce response quality
- Failure signatures:
  - Low annotator agreement → Cultural ambiguity or unclear instructions
  - Model responses outside expected format → Prompt misalignment or translation issues
  - Skewed stereotype distribution → LLM bias or sampling imbalance
- First 3 experiments:
  1. Compare annotation agreement rates across languages to detect cultural ambiguity hotspots
  2. Test model stereotype endorsement with and without safety guardrails enabled
  3. Evaluate model behavior on in-language vs. translated prompts for a subset of stereotypes to quantify detection gap

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do stereotypes in SGM evolve over time as language models are trained on newer data and societal attitudes shift?
- Basis in paper: [inferred] The paper discusses the dynamic nature of stereotypes and their cultural context, but does not explore how these change over time.
- Why unresolved: The study is cross-sectional and does not track changes in stereotype prevalence or offensiveness across different time periods.
- What evidence would resolve it: Longitudinal studies tracking the same stereotypes in SGM over multiple years, comparing them with evolving model outputs and societal surveys.

### Open Question 2
- Question: Can stereotype mitigation techniques be effectively adapted for multilingual contexts using SGM?
- Basis in paper: [explicit] The paper mentions the need for diverse, language-specific datasets for fair AI safety assessments but does not explore mitigation strategies.
- Why unresolved: While the dataset enables evaluation of stereotypes, it does not provide insights into how to reduce their occurrence in model outputs.
- What evidence would resolve it: Experiments applying existing English-based stereotype mitigation techniques to multilingual models, measuring their effectiveness across languages in SGM.

### Open Question 3
- Question: How do intersectional identities (e.g., gender + nationality) contribute to stereotype complexity in multilingual contexts?
- Basis in paper: [explicit] The paper notes gendered demonyms in SGM but does not deeply analyze intersectional stereotype patterns.
- Why unresolved: The dataset captures some intersectional data but the analysis focuses primarily on single-axis identities (nationality or region).
- What evidence would resolve it: Systematic analysis of stereotype patterns when multiple identity dimensions are combined, comparing these to single-axis stereotypes in SGM.

## Limitations
- Cultural representativeness constrained by annotator availability, with sparse regional coverage for certain languages like French
- Human annotation process introduces subjectivity that may vary across regions and annotator groups
- LLM generation approach may inherit biases from training data affecting stereotype candidate quality

## Confidence

- **High Confidence:** The dataset creation methodology (LLM generation + human validation) is clearly specified and reproducible. The finding that multilingual evaluation reveals higher stereotype endorsement than English-only testing is well-supported by direct model evaluations.
- **Medium Confidence:** Claims about regional variation in stereotype prevalence are supported by cross-country comparisons (e.g., Portuguese vs Brazilian annotations) but would benefit from more granular statistical analysis across all regions.
- **Medium Confidence:** The assertion that multilingual models show higher stereotype endorsement rates requires more systematic comparison with English-only evaluations to establish statistical significance.

## Next Checks
1. Conduct inter-annotator agreement analysis across all regions to quantify cultural subjectivity and identify annotation reliability thresholds
2. Perform controlled experiments comparing model stereotype endorsement rates using in-language prompts versus professionally translated English prompts across a representative sample
3. Analyze LLM-generated stereotype candidates for cultural relevance using automated language model scoring before human validation to optimize the generation-validation pipeline