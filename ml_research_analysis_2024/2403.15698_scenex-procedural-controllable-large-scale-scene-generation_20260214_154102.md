---
ver: rpa2
title: 'SceneX: Procedural Controllable Large-scale Scene Generation'
arxiv_id: '2403.15698'
source_url: https://arxiv.org/abs/2403.15698
tags:
- generation
- scene
- generate
- arxiv
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SceneX, a framework for generating large-scale
  3D scenes using procedural modeling guided by natural language instructions. SceneX
  consists of PCGBench, a collection of procedural assets and API documentation, and
  PCGPlanner, an LLM-based agent that automatically generates Blender actions to create
  controllable 3D assets.
---

# SceneX: Procedural Controllable Large-scale Scene Generation

## Quick Facts
- arXiv ID: 2403.15698
- Source URL: https://arxiv.org/abs/2403.15698
- Reference count: 40
- Generates 2.5 km×2.5 km cities in hours vs days manually

## Executive Summary
SceneX introduces a framework for large-scale 3D scene generation using procedural modeling guided by natural language instructions. The system combines PCGBench (a collection of procedural assets and API documentation) with PCGPlanner (an LLM-based agent that generates Blender actions to create controllable 3D assets). SceneX achieves impressive scalability, generating 2.5 km×2.5 km cities in hours compared to days for manual procedural content generation. The framework demonstrates high executability (94%) and success rates (80%) in task execution, with strong aesthetic scores (7.83/10), while offering superior controllability compared to learning-based 3D generation approaches.

## Method Summary
SceneX leverages large language models to translate natural language prompts into executable Blender actions for procedural 3D scene generation. The system uses PCGBench as a knowledge base containing procedural assets and API documentation, which the PCGPlanner LLM agent queries to determine appropriate generation actions. This approach enables automated, controllable generation of complex 3D environments at unprecedented scales. The LLM-driven planning system breaks down high-level scene descriptions into specific procedural modeling operations, maintaining consistency and architectural coherence across large urban environments while allowing for customization through natural language instructions.

## Key Results
- Generates 2.5 km×2.5 km cities in hours, dramatically faster than manual PCG workflows
- Achieves 94% executability and 80% success rates in task execution with average aesthetic score of 7.83/10
- Outperforms learning-based 3D generation approaches in both quality and controllability for large-scale scenes

## Why This Works (Mechanism)
SceneX works by leveraging the planning capabilities of LLMs to decompose natural language scene descriptions into executable procedural modeling operations. The PCGPlanner agent interprets user prompts and translates them into specific Blender API calls using the PCGBench documentation as reference. This creates a feedback loop where the LLM can reason about spatial relationships, architectural styles, and scene composition while maintaining precise control over generation parameters. The procedural approach ensures consistency across large scales while the natural language interface provides intuitive control, combining the scalability of procedural generation with the flexibility of AI planning.

## Foundational Learning
- Procedural Content Generation (PCG): Why needed - enables scalable, consistent content creation; Quick check - verify understanding of noise functions, L-systems, and rule-based generation
- Blender Python API: Why needed - provides the actual 3D modeling operations; Quick check - confirm knowledge of bpy.context, bpy.ops, and mesh manipulation
- Large Language Model Planning: Why needed - translates natural language to executable actions; Quick check - understand chain-of-thought reasoning and function calling patterns
- 3D Scene Composition: Why needed - ensures generated scenes maintain spatial coherence; Quick check - verify understanding of spatial hierarchies and environmental storytelling
- Natural Language Processing: Why needed - enables intuitive user interaction; Quick check - confirm knowledge of prompt engineering and semantic understanding

## Architecture Onboarding

Component Map:
User Prompt -> LLM (PCGPlanner) -> PCGBench Documentation -> Blender Python Actions -> 3D Scene

Critical Path:
Natural Language Prompt → LLM Planning → API Documentation Lookup → Blender Action Generation → Scene Rendering

Design Tradeoffs:
- Procedural vs Learning-based: SceneX prioritizes controllability over the visual diversity of neural methods
- LLM-driven vs Rule-based: Uses LLM flexibility at the cost of potential planning inconsistencies
- Blender-specific vs Framework-agnostic: Tightly integrated with Blender ecosystem for maximum capability

Failure Signatures:
- Prompt ambiguity leading to inconsistent architectural styles
- LLM planning errors causing logical contradictions in scene layout
- API documentation gaps resulting in unsupported generation operations
- Memory constraints when scaling beyond demonstrated 2.5 km limits

First 3 Experiments:
1. Generate a simple urban block with specified building types and street layouts
2. Create a mixed-use neighborhood with residential, commercial, and park areas
3. Scale up to full city generation with zoning regulations and traffic patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily focused on urban environments, leaving performance on other scene types unclear
- Limited discussion of computational bottlenecks when scaling beyond 2.5 km×2.5 km limits
- Memory requirements and potential degradation patterns for extremely large scenes not addressed

## Confidence
- High Confidence: Core LLM-driven Blender action generation approach is technically sound
- Medium Confidence: Speed improvements over manual PCG are plausible but hardware-dependent
- Medium Confidence: Claims of outperforming learning-based approaches need more quantitative validation

## Next Checks
1. Scalability Testing: Validate performance on progressively larger scenes (5 km×5 km, 10 km×10 km) to identify bottlenecks and memory constraints
2. Cross-Domain Generalization: Test SceneX on non-urban environments including indoor spaces, natural landscapes, and fantasy settings
3. Long-Horizon Planning Robustness: Conduct stress tests with complex multi-constraint prompts to assess 94% executability rate under demanding scenarios