---
ver: rpa2
title: 'DiM: $f$-Divergence Minimization Guided Sharpness-Aware Optimization for Semi-supervised
  Medical Image Segmentation'
arxiv_id: '2411.12350'
source_url: https://arxiv.org/abs/2411.12350
tags:
- uni00000013
- domain
- segmentation
- uni00000011
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of semi-supervised medical image
  segmentation, where models must learn from limited labeled data and abundant unlabeled
  data. The authors propose DiM, a sharpness-aware optimization method guided by f-divergence
  minimization.
---

# DiM: $f$-Divergence Minimization Guided Sharpness-Aware Optimization for Semi-supervised Medical Image Segmentation

## Quick Facts
- **arXiv ID**: 2411.12350
- **Source URL**: https://arxiv.org/abs/2411.12350
- **Reference count**: 40
- **Primary result**: DiM achieves state-of-the-art performance on fundus and prostate datasets with improved Dice and Jaccard coefficients

## Executive Summary
This paper addresses the challenge of semi-supervised medical image segmentation, where models must learn from limited labeled data and abundant unlabeled data. The authors propose DiM, a sharpness-aware optimization method guided by $f$-divergence minimization. DiM combines Sharpness-Aware Minimization (SAM) with $f$-divergence constraints to enhance model stability and adaptability across different datasets. The method minimizes $f$-divergence between labeled and unlabeled data logits to align feature distributions, while SAM ensures optimization within low-loss neighborhoods. Experimental results on fundus and prostate datasets show that DiM outperforms state-of-the-art methods, achieving higher Dice and Jaccard coefficients, and lower Hausdorff and Average Surface Distances. The method demonstrates robustness in handling distribution shifts and preventing overfitting.

## Method Summary
DiM is a novel semi-supervised medical image segmentation framework that integrates $f$-divergence minimization with Sharpness-Aware Minimization (SAM). The approach addresses the challenge of limited labeled medical data by leveraging abundant unlabeled samples while maintaining model robustness. DiM operates by minimizing the $f$-divergence between labeled and unlabeled data logits, which aligns the feature distributions across domains. Simultaneously, SAM guides the optimization process to find parameters within low-loss neighborhoods, enhancing model stability. The method employs a teacher-student framework where the teacher model generates pseudo-labels for unlabeled data, and the student model learns from both labeled and pseudo-labeled data while optimizing for both segmentation accuracy and $f$-divergence minimization. The combined objective function balances supervised loss on labeled data, consistency loss between teacher and student predictions, and the $f$-divergence constraint.

## Key Results
- Achieves state-of-the-art performance on fundus and prostate segmentation datasets
- Improves Dice and Jaccard coefficients compared to existing semi-supervised methods
- Reduces Hausdorff and Average Surface Distances, indicating better boundary accuracy
- Demonstrates robustness across different datasets and distribution shifts

## Why This Works (Mechanism)
DiM leverages two complementary optimization strategies: $f$-divergence minimization aligns feature distributions between labeled and unlabeled data, reducing domain shift effects, while SAM ensures optimization within flat, low-loss regions of the parameter space, enhancing model generalization. The $f$-divergence constraint forces the model to produce consistent feature representations across labeled and unlabeled samples, effectively regularizing the learning process. SAM's neighborhood-based optimization prevents the model from converging to sharp minima that are sensitive to noise and perturbations. Together, these mechanisms create a robust optimization framework that balances learning from limited labeled data with generalization to unlabeled samples, resulting in improved segmentation performance and stability.

## Foundational Learning
- **$f$-divergence**: A measure of difference between probability distributions; needed to quantify distribution alignment between labeled and unlabeled data, quick check: verify KL divergence implementation matches theoretical formulation
- **Sharpness-Aware Minimization (SAM)**: Optimization technique that finds parameters in low-loss neighborhoods; needed to enhance model robustness against perturbations, quick check: confirm gradient ascent step size affects convergence stability
- **Teacher-student framework**: Semi-supervised learning paradigm where a teacher model generates pseudo-labels; needed to leverage unlabeled data effectively, quick check: monitor teacher-student consistency metrics during training
- **Semi-supervised learning**: Learning paradigm combining limited labeled and abundant unlabeled data; needed to address medical image annotation scarcity, quick check: verify performance improvement with increasing unlabeled data ratio
- **Medical image segmentation**: Task of partitioning medical images into anatomical regions; needed as the target application domain, quick check: ensure evaluation metrics align with clinical requirements

## Architecture Onboarding

**Component map**: Input images -> Backbone feature extractor -> Segmentation head -> $f$-divergence module + SAM optimizer -> Output predictions

**Critical path**: Raw medical images → Feature extraction → Segmentation head → Loss computation (supervised + consistency + $f$-divergence) → SAM optimization → Parameter updates

**Design tradeoffs**: The method trades increased computational complexity for improved segmentation accuracy and robustness. The $f$-divergence computation adds overhead but provides better domain alignment. SAM's neighborhood search increases training time but yields more stable models.

**Failure signatures**: Poor $f$-divergence minimization may indicate feature distribution mismatch between labeled and unlabeled data. Inconsistent teacher-student predictions suggest unreliable pseudo-label generation. Sharpness-aware optimization failure may manifest as sensitivity to input perturbations.

**Three first experiments**:
1. Baseline U-Net training with limited labeled data only
2. Teacher-student semi-supervised learning without $f$-divergence constraint
3. $f$-divergence minimization with standard optimization (no SAM)

## Open Questions the Paper Calls Out
None specified in the provided material.

## Limitations
- Lack of ablation studies isolating individual contributions of $f$-divergence minimization and SAM
- Limited evaluation to fundus and prostate datasets, untested on diverse medical imaging modalities
- Computational overhead from combining both techniques not thoroughly analyzed for scalability

## Confidence

| Claim | Confidence |
|-------|------------|
| Theoretical framework combining $f$-divergence minimization with SAM | High |
| Experimental results showing state-of-the-art performance | Medium |
| Claimed robustness to distribution shifts | Medium |

## Next Checks

1. Conduct comprehensive ablation studies to quantify the individual and combined effects of $f$-divergence minimization and SAM on segmentation performance across different datasets
2. Evaluate the method's performance on additional medical imaging modalities (e.g., MRI, CT) and anatomical structures to assess generalizability
3. Perform computational efficiency analysis comparing DiM with baseline methods in terms of training time, memory usage, and scalability to 3D volumes