---
ver: rpa2
title: Is Retain Set All You Need in Machine Unlearning? Restoring Performance of
  Unlearned Models with Out-Of-Distribution Images
arxiv_id: '2404.12922'
source_url: https://arxiv.org/abs/2404.12922
tags:
- unlearning
- scar
- forget
- data
- retain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SCAR introduces a novel machine unlearning method that removes
  targeted information from deep neural networks without using a retain set. The approach
  combines metric learning and knowledge distillation to shift feature vectors of
  forget samples toward incorrect class distributions while preserving model performance
  on retained data.
---

# Is Retain Set All You Need in Machine Unlearning? Restoring Performance of Unlearned Models with Out-Of-Distribution Images

## Quick Facts
- arXiv ID: 2404.12922
- Source URL: https://arxiv.org/abs/2404.12922
- Authors: Jacopo Bonato; Marco Cotogni; Luigi Sabetta
- Reference count: 40
- Primary result: SCAR achieves state-of-the-art machine unlearning performance without using retain sets

## Executive Summary
SCAR introduces a novel machine unlearning method that removes targeted information from deep neural networks without using a retain set. The approach combines metric learning and knowledge distillation to shift feature vectors of forget samples toward incorrect class distributions while preserving model performance on retained data. SCAR uses Mahalanobis distance to efficiently reorganize feature vectors and a distillation-trick mechanism with out-of-distribution images to maintain original model knowledge. The method demonstrates superior performance compared to retain-set-free approaches and achieves results comparable to state-of-the-art methods that rely on retain sets. SCAR also introduces a self-forget variant that operates without access to forget data, making it applicable in challenging scenarios where training or forget data are unavailable.

## Method Summary
SCAR is a machine unlearning method that eliminates the need for retain sets by using a combination of metric learning and knowledge distillation. The approach computes Mahalanobis distances between feature vectors of forget samples and class distributions to align them with the nearest incorrect class. A distillation-trick mechanism using out-of-distribution images preserves the original model's knowledge without access to retain data. SCAR also introduces a self-forget variant that classifies OOD data to create surrogate forget and retain sets, enabling unlearning without access to either forget or retain data.

## Key Results
- SCAR achieves superior performance compared to retain-set-free unlearning methods
- The approach matches or exceeds the performance of state-of-the-art methods using retain sets
- SCAR's self-forget variant successfully unlearns without access to forget data
- Validated across multiple datasets (CIFAR-10/100, TinyImageNet) and architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SCAR shifts feature vectors of forget samples toward the nearest wrong class distribution using Mahalanobis distance.
- Mechanism: For each forget sample, the method computes Mahalanobis distance between its feature vector and all other class distributions (excluding its own). It then aligns the feature vector to the nearest incorrect class distribution, effectively erasing class-specific information.
- Core assumption: The feature vector distributions of different classes are sufficiently separated in the feature space, and the Mahalanobis distance with covariance normalization captures this separation effectively.
- Evidence anchors:
  - [abstract] "Our approach utilizes a modified Mahalanobis distance to guide the unlearning of the feature vectors of the instances to be forgotten, aligning them to the nearest wrong class distribution."
  - [section] "the Mahalanobis distance is defined in eq. 6" with explicit formulation showing distance to class distributions.
  - [corpus] Weak - only 1 related paper mentions Mahalanobis distance, others use different approaches.

### Mechanism 2
- Claim: The distillation-trick mechanism preserves original model knowledge using out-of-distribution (OOD) images without requiring the retain set.
- Mechanism: SCAR uses Jensen-Shannon divergence between the original model (teacher) and unlearning model (student) predictions on OOD data to maintain knowledge. The OOD data triggers similar feature space clustering as the original training data.
- Core assumption: Deep neural networks exhibit similar feature space clustering patterns for both training data and OOD data, allowing knowledge transfer without access to retain data.
- Evidence anchors:
  - [abstract] "distills the knowledge of the original model into the unlearning model with out-of-distribution images for retaining the original model's test performance without using any retain set."
  - [section] "we leverage this characteristic behavior of DNNs on OOD datasets as a form of regularization during the unlearning process, which we term 'distillation-trick'."
  - [corpus] Weak - no direct evidence in related papers, but the concept is supported by references [21,25,34] mentioned in text.

### Mechanism 3
- Claim: SCAR's self-forget variant can perform class removal without access to either forget or retain data by using predicted class labels on OOD data.
- Mechanism: SCAR classifies OOD data with the original model, then uses samples predicted as the class to remove as surrogate forget data, and other samples as surrogate retain data.
- Core assumption: The original model's predictions on OOD data are sufficiently accurate to create valid surrogate forget and retain sets for the unlearning process.
- Evidence anchors:
  - [abstract] "we propose a self-forget version of SCAR that unlearns without having access to the forget set."
  - [section] "Given Kret, the set of retained classes, and Kfgt, the set of forget classes, SCAR initially employs the original model Φθ to classify Dsur."
  - [corpus] No evidence - this appears to be a novel contribution not present in related work.

## Foundational Learning

- Concept: Mahalanobis distance with covariance normalization
  - Why needed here: SCAR uses this distance metric to measure how far a forget sample's feature vector is from different class distributions, accounting for the spread and correlation of features within each class.
  - Quick check question: How does Mahalanobis distance differ from Euclidean distance when measuring distances between feature vectors and class distributions?

- Concept: Knowledge distillation using Jensen-Shannon divergence
  - Why needed here: SCAR preserves original model performance on test data by minimizing the divergence between original and unlearning model predictions on OOD data, without requiring access to retain data.
  - Quick check question: Why might Jensen-Shannon divergence be preferred over KL divergence for distillation in this context?

- Concept: Out-of-distribution data behavior in neural networks
  - Why needed here: The distillation-trick mechanism relies on DNNs producing similar feature space patterns for OOD data as they do for training data, enabling knowledge preservation without retain data.
  - Quick check question: What properties of neural network feature spaces make them responsive to OOD data in ways that enable this knowledge preservation?

## Architecture Onboarding

- Component map:
  - Feature extraction backbone (Φψ) -> Classification head (Φπ) -> Distribution prototypes -> OOD dataset handler -> Unlearning controller

- Critical path:
  1. Extract feature vectors for all training samples and compute class distribution prototypes
  2. For each forget sample, compute Mahalanobis distances to all class distributions
  3. Align forget sample feature vectors toward nearest wrong class distribution
  4. Apply distillation-trick using OOD data to preserve original model knowledge
  5. Iterate until convergence criteria are met

- Design tradeoffs:
  - Mahalanobis vs. simpler distance metrics: Mahalanobis captures distribution shape but is computationally heavier
  - OOD dataset selection: Must contain semantic information but not leak from original training data
  - Trade-off between unlearning effectiveness and performance preservation

- Failure signatures:
  - Poor forget set accuracy despite training: Class distributions may be too overlapping
  - Significant performance drop on test set: OOD data may not provide sufficient knowledge preservation
  - Slow convergence: Learning rate or batch size may need adjustment
  - High variance across runs: Random seed effects or insufficient OOD data

- First 3 experiments:
  1. Test Mahalanobis distance effectiveness: Compare SCAR with a baseline using only centroid distances on a simple dataset
  2. Validate distillation-trick: Train a model using only the distillation mechanism on OOD data and measure test performance
  3. Verify self-forget mechanism: Apply SCAR self-forget on a dataset where forget set access is available, then compare with standard SCAR

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SCAR's performance scale with dataset size and dimensionality, particularly for extremely large-scale datasets like ImageNet-21K?
- Basis in paper: [explicit] The paper mentions that using retain sets becomes impractical for large-scale datasets like ImageNet or ImageNet-21K when the forget set is a minor fraction, but does not provide experimental validation for extremely large-scale scenarios.
- Why unresolved: The experiments were conducted on CIFAR-10/100 and TinyImageNet (200 classes), which are significantly smaller than ImageNet-21K (21,841 classes). The computational and memory implications of SCAR's covariance matrix calculations at such scale remain unexplored.
- What evidence would resolve it: Experiments comparing SCAR's performance and computational requirements across multiple dataset scales (CIFAR, TinyImageNet, ImageNet, ImageNet-21K) would demonstrate scalability. Analysis of memory usage and runtime as a function of dataset size and feature dimensionality would provide insights into practical limitations.

### Open Question 2
- Question: What are the theoretical guarantees for data removal completeness in SCAR, and how do they compare to exact unlearning methods?
- Basis in paper: [explicit] The paper acknowledges that "like many approximate unlearning methods, future work should include a mathematical exploration of the algorithms' certifiability" but does not provide any formal analysis of removal completeness.
- Why unresolved: SCAR is positioned as an approximate unlearning method without theoretical bounds on how completely it removes information about forget samples. The paper shows empirical results but lacks formal guarantees about the extent of data removal.
- What evidence would resolve it: Mathematical proofs establishing bounds on information retention after unlearning, perhaps using information-theoretic measures or differential privacy guarantees, would provide theoretical validation. Empirical validation through rigorous membership inference attacks across multiple threat models would complement theoretical analysis.

### Open Question 3
- Question: How does SCAR perform when the forget set contains samples from multiple classes, and what is the impact on class relationships in the feature space?
- Basis in paper: [explicit] The paper primarily evaluates homogeneous removal (HR) where forget samples come from multiple classes, but does not extensively analyze how SCAR affects inter-class relationships in the feature space when unlearning is applied to multiple classes simultaneously.
- Why unresolved: The metric learning approach in SCAR shifts feature vectors toward the nearest wrong class distribution, but the paper does not investigate how this affects the overall geometric structure of the feature space, particularly when multiple classes are being unlearned simultaneously.
- What evidence would resolve it: Detailed analysis of feature space topology before and after unlearning, including visualization of decision boundaries and class separation metrics, would reveal how SCAR affects the geometric relationships between classes. Quantitative measures of class confusion and nearest-neighbor relationships would provide insights into the preservation of useful class structure.

## Limitations
- Computational overhead of computing Mahalanobis distances for all class pairs during unlearning, scaling quadratically with number of classes
- Effectiveness depends on sufficient separation between class feature distributions, which may not hold for datasets with high inter-class similarity
- Self-forget variant reliability heavily depends on quality of surrogate data selection through original model predictions on OOD data

## Confidence
- Mahalanobis distance-based feature vector alignment: Medium-High
- Distillation-trick mechanism using OOD data: Medium
- Self-forget variant effectiveness: Medium

## Next Checks
1. **Robustness to class distribution overlap**: Test SCAR on datasets with increasingly overlapping class distributions to determine the threshold where Mahalanobis distance alignment becomes ineffective, and compare performance against simpler distance metrics.

2. **OOD data sensitivity analysis**: Systematically vary the characteristics of OOD datasets used in the distillation-trick mechanism to quantify how differences in semantic content, domain shift, and data volume affect knowledge preservation and unlearning effectiveness.

3. **Self-forget variant reliability**: Conduct extensive experiments on diverse datasets to measure the consistency and accuracy of the self-forget mechanism, particularly focusing on scenarios where the original model's predictions on OOD data may be unreliable or biased.