---
ver: rpa2
title: 'BATON: Aligning Text-to-Audio Model with Human Preference Feedback'
arxiv_id: '2402.00744'
source_url: https://arxiv.org/abs/2402.00744
tags: []
core_contribution: 'BATON is a framework that uses human preference feedback to improve
  the alignment between text prompts and generated audio in text-to-audio models.
  The framework consists of three key stages: 1) curating a dataset of text-audio
  pairs and annotating them with human feedback, 2) training a reward model to predict
  human preferences based on the annotated dataset, and 3) fine-tuning an off-the-shelf
  text-to-audio model using the reward model to enhance audio integrity and temporal
  relationships.'
---

# BATON: Aligning Text-to-Audio Model with Human Preference Feedback

## Quick Facts
- arXiv ID: 2402.00744
- Source URL: https://arxiv.org/abs/2402.00744
- Reference count: 31
- BATON significantly improves text-to-audio model alignment through human preference feedback

## Executive Summary
BATON introduces a framework for aligning text-to-audio models using human preference feedback. The approach addresses limitations in existing text-to-audio models by incorporating human judgment to improve both audio integrity and temporal relationships. Through a three-stage process involving dataset curation, reward model training, and model fine-tuning, BATON demonstrates substantial improvements in audio generation quality.

## Method Summary
BATON employs a three-stage approach: first curating a dataset of text-audio pairs annotated with human feedback; second training a reward model to predict human preferences from this dataset; and third fine-tuning an existing text-to-audio model using the reward model. The framework specifically targets two aspects of audio generation - integrity (faithfulness to the text description) and temporal relationships (correct sequencing of audio events). The human preference annotations guide the model to generate more accurate and temporally coherent audio outputs that better match their textual descriptions.

## Key Results
- CLAP score improvements: +2.3% for integrity task, +6.0% for temporal relationship task
- Human evaluation: MOS-Q of 4.55 (integrity) and MOS-F of 4.41 (temporal relationship)
- Outperforms baseline by 0.58 MOS-F in temporal relationship task

## Why This Works (Mechanism)
BATON works by leveraging human preference feedback as a more nuanced quality signal than traditional automated metrics. Human annotators can identify subtle issues in audio generation that automated systems miss, particularly regarding temporal coherence and semantic fidelity. The reward model trained on this human data captures these complex preferences and guides the fine-tuning process to produce outputs that better align with human expectations for both what should be in the audio and when it should occur.

## Foundational Learning
- **Reward modeling**: A neural network that learns to predict human preference scores, needed to automate the human feedback process at scale; quick check: verify the reward model's correlation with human judgments on held-out samples
- **Fine-tuning with RLHF**: Using reinforcement learning from human feedback to update model parameters based on learned preferences; quick check: monitor KL divergence to prevent excessive deviation from original model
- **CLAP scoring**: Using Contrastive Language-Audio Pretraining metrics to evaluate audio-text alignment; quick check: ensure CLAP scores correlate with human preference scores
- **MOS evaluation**: Mean Opinion Score as a subjective human evaluation metric; quick check: establish inter-annotator agreement to validate consistency

## Architecture Onboarding

**Component Map**: Text Prompt -> Text-to-Audio Model -> Audio Output -> Reward Model -> Fine-tuned Model

**Critical Path**: Human feedback collection → Reward model training → Text-to-audio fine-tuning → Evaluation

**Design Tradeoffs**: The framework trades computational efficiency for quality by adding an intermediate reward modeling stage, and depends on human annotation quality for success.

**Failure Signatures**: Poor reward model training leads to misaligned fine-tuning; insufficient human annotation diversity causes bias; excessive fine-tuning can degrade original model capabilities.

**First Experiments**:
1. Validate reward model correlates with human preferences on held-out annotations
2. Test fine-tuning stability by monitoring both generation quality and adherence to original distribution
3. Evaluate ablation: compare results with and without human preference fine-tuning

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy dependence on human annotation introduces subjectivity and potential cultural bias
- Framework tested only on one baseline model architecture, limiting generalizability claims
- Substantial human annotation requirements may hinder scalability and practical deployment

## Confidence

**Major claim clusters and confidence:**
- *Framework effectiveness (High)*: Strong quantitative improvements and ablation studies support BATON's effectiveness
- *Human preference alignment (Medium)*: MOS improvements demonstrated but subject to annotation variability and cultural context
- *Scalability and generalization (Low)*: Limited testing scope and human annotation dependence create uncertainty about broader applicability

## Next Checks

1. Conduct cross-cultural human evaluation studies to assess whether the preference alignment generalizes beyond the original annotator pool
2. Test BATON's effectiveness on multiple off-the-shelf text-to-audio models with varying architectures to establish broader applicability
3. Perform ablation studies isolating the impact of reward model quality versus fine-tuning methodology on final audio quality improvements