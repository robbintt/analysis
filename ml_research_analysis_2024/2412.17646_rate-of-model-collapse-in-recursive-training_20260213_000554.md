---
ver: rpa2
title: Rate of Model Collapse in Recursive Training
arxiv_id: '2412.17646'
source_url: https://arxiv.org/abs/2412.17646
tags:
- training
- recursive
- collapse
- bound
- process
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how quickly models forget their training data
  in recursive training, where models are trained repeatedly on their own generated
  data. The authors analyze fundamental distributions including discrete, Gaussian,
  and Gaussian mixture models under maximum likelihood estimation.
---

# Rate of Model Collapse in Recursive Training

## Quick Facts
- arXiv ID: 2412.17646
- Source URL: https://arxiv.org/abs/2412.17646
- Reference count: 40
- One-line primary result: This paper proves that model collapse in recursive training occurs slowly, with discrete distributions forgetting symbols at rate ~1/k and Gaussian models converging to zero variance at rate ~1/ε · exp(-k/(4n)).

## Executive Summary
This paper provides rigorous theoretical analysis of model collapse in recursive training, where models are repeatedly trained on their own generated data. The authors prove convergence rates for discrete distributions (1/k forgetting rate), Gaussian models (exponential variance decay), and Gaussian mixture models (using approximate joint ML estimation). Their theoretical results show that model collapse occurs much more slowly than commonly assumed, especially when using many samples per training iteration. The findings have important implications for understanding the limitations of self-training approaches and the persistence of training data in recursive systems.

## Method Summary
The paper studies recursive training through a Markov chain framework where parameters Θk are estimated from synthetic data Sk generated by the previous model. For discrete distributions, Bernoulli/Poisson recursive training uses ML estimation to track symbol retention probability. For Gaussian models, recursive estimation of mean and variance shows exponential convergence to zero. For Gaussian mixture models, an approximate joint ML estimator based on tanh-based implicit equations is used to prove similar convergence properties. The theoretical analysis employs martingale convergence theorems, stochastic approximation techniques, and careful bounding of estimator variances to establish the rates of model collapse.

## Key Results
- For discrete distributions, the probability of forgetting a symbol decreases approximately as 1/k, where k is the number of training rounds
- For Gaussian models, variance converges to zero at rate approximately 1/ε · exp(-k/(4n)), where n is samples per round and ε is the variance threshold
- For Gaussian mixture models, similar exponential convergence rates are shown using an approximate joint ML estimator
- Experiments confirm theoretical predictions, showing model collapse occurs slowly especially with many samples per iteration

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model collapse under recursive training occurs when variance parameters converge to zero
- Mechanism: Recursive training creates a Markov chain of parameters where each iteration applies ML estimation to synthetic data. The estimator variance decreases monotonically, driving convergence to zero
- Core assumption: Estimator variance decreases monotonically with each iteration due to ML estimation structure
- Evidence anchors:
  - [abstract] "For Gaussian models, they prove that variance converges to zero at a rate of approximately 1/ϵ · exp(-k/(4n))"
  - [section] "Theorem 3. For the Gaussian recursive training process defined above with n≥3, Pr(Σk >ϵ)≤σ0/ϵexp{−k/(4n−1)}."
  - [corpus] Found 25 related papers; average neighbor FMR=0.457; average citations=0.0
- Break condition: If estimator variance doesn't decrease (e.g., due to bias or sampling error), convergence to zero may not occur

### Mechanism 2
- Claim: For discrete distributions, the probability of forgetting a symbol falls approximately as 1/k
- Mechanism: Under Bernoulli/Poisson sampling, each symbol's probability evolves independently. The decay of Pr(symbol ≠ 0) follows exp(-n p0 g(k)) where g(k) ≈ 1/k
- Core assumption: Estimator variance follows binomial/Poisson sampling with fixed n, and decay follows 1/k rate
- Evidence anchors:
  - [abstract] "For discrete distributions, the time to forget a word is approximately linearly dependent on the number of times it occurred in the original corpus"
  - [section] "Theorem 1. For the Bernoulli recursive training process... Pr(Pk≠0) ≥1−exp(−λi/k), where λi is the number of i's in the original dataset"
  - [corpus] No direct evidence in corpus for discrete distribution forgetting rate
- Break condition: Small n accelerates decay; biased estimators deviate from 1/k rate

### Mechanism 3
- Claim: Gaussian mixture models collapse when variance parameter converges to zero via approximate joint ML estimation
- Mechanism: GMM recursive training estimates mean and variance jointly via tanh-based implicit equations. The approximate estimator bounds variance above by (1+κ) times chi-squared scaled variance, which shrinks each iteration
- Core assumption: Approximate joint ML estimator variance bound is tight enough to preserve contraction behavior
- Evidence anchors:
  - [abstract] "For Gaussian mixture models, similar convergence rates are shown using an approximate joint maximum likelihood estimator."
  - [section] "Theorem 4. For the GMM recursive training process... Pr(Σk >ϵ)≤σ0/ϵexp{−k/(4n)}."
  - [corpus] No corpus evidence for GMM collapse mechanism
- Break condition: Large approximation error may prevent collapse; very low initial variance causes faster collapse

## Foundational Learning

- Concept: Maximum Likelihood Estimation (MLE) for discrete and continuous distributions
  - Why needed here: The paper's recursive training relies on MLE to estimate parameters at each iteration; understanding its properties is critical for analyzing convergence
  - Quick check question: In a Bernoulli model with p=0.2 and n=100 samples, what is the expected MLE of p?
    - Answer: 0.2 (unbiasedness property)

- Concept: Markov chains and martingales
  - Why needed here: Recursive training forms a Markov chain of parameters; martingale convergence theorems are used to prove almost sure convergence of variance parameters
  - Quick check question: If {Xk} is a non-negative martingale with E[Xk] bounded, what does the martingale convergence theorem guarantee?
    - Answer: Xk converges almost surely to a finite limit

- Concept: Stochastic approximation and iterated function systems
  - Why needed here: Recursive training is a stochastic recursion; knowledge of convergence conditions in stochastic approximation helps understand when parameters converge
  - Quick check question: In stochastic approximation θk+1 = θk + ak(f(θk)+ζk), what condition on f ensures convergence?
    - Answer: f must be a contraction on average, i.e., E[‖f(θ)‖] < 1 for θ in the parameter space

## Architecture Onboarding

- Component map: Data generator -> Estimator module -> Parameter updater -> Convergence monitor -> (loop back to Data generator)
- Critical path:
  1. Generate Sk from PΘk-1
  2. Compute Θk via estimator
  3. Check convergence criteria
  4. Repeat until convergence or max iterations
- Design tradeoffs:
  - n (samples per iteration) vs. speed of collapse: Larger n slows collapse but increases computational cost
  - Estimator choice: Exact ML is optimal but may be intractable (e.g., GMM); approximations trade accuracy for tractability
  - ε threshold: Smaller ε detects slower collapse but may require more iterations
- Failure signatures:
  - Estimator variance doesn't decrease (indicates non-contraction or biased estimator)
  - Pr(Σk > ε) doesn't decay exponentially (indicates estimator or sampling issues)
  - Variance increases or oscillates (suggests numerical instability or poor estimator)
- First 3 experiments:
  1. Implement Bernoulli recursive training with n=100, p0=0.01; plot Pr(Pk≠0) vs. k; verify 1/k decay
  2. Implement Gaussian recursive training with n=15, σ0=1; measure Pr(Σk>0.1) vs. k; compare to exp(-k/(4n)) bound
  3. Implement GMM recursive training with n=10, µ0=1, σ0=1; compare Pr(Σk>0.1) for exact vs. approximate joint ML; validate Theorem 4 bound

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions in a dedicated section. However, based on the content and discussion, several important questions emerge that the authors acknowledge require further investigation, particularly regarding the generalizability of results to more complex models and real-world scenarios.

## Limitations

- Theoretical analysis relies on idealized conditions (bounded parameters, sufficient samples) that may not hold in practice
- GMM convergence analysis depends on approximate estimators, introducing potential gaps between theory and practice
- Results are derived for simple distributions and may not directly translate to complex, high-dimensional models
- The slow rate of model collapse (1/k for discrete, exponential for continuous) may not hold under different training regimes or with mixed real/synthetic data

## Confidence

- **High Confidence**: Theoretical convergence rates for discrete and Gaussian models, supported by rigorous proofs and experimental validation
- **Medium Confidence**: GMM convergence analysis, as it depends on approximate estimators whose accuracy may vary with parameter choices
- **Low Confidence**: Real-world applicability of results, as synthetic data generation and ML estimation may not reflect the complexity of actual training scenarios

## Next Checks

1. Implement and validate the approximate joint ML estimator for GMM: Reproduce Algorithm 1 with varying parameter choices to assess the robustness of the GMM convergence bounds
2. Test convergence under non-ideal conditions: Run experiments with small sample sizes (n < 10) and high-variance initial parameters to evaluate the sensitivity of the theoretical bounds
3. Compare with alternative estimators: Implement biased or non-ML estimators in recursive training to determine if the 1/k and exponential decay rates hold under different estimation strategies