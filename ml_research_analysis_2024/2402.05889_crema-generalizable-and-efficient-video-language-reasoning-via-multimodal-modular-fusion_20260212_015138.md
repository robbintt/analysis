---
ver: rpa2
title: 'CREMA: Generalizable and Efficient Video-Language Reasoning via Multimodal
  Modular Fusion'
arxiv_id: '2402.05889'
source_url: https://arxiv.org/abs/2402.05889
tags:
- modalities
- modality
- crema
- video
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of integrating diverse sensory
  modalities into video-language reasoning models efficiently. The authors propose
  CREMA, a modular framework that uses parameter-efficient adapters for each modality
  and a novel self-gated fusion mechanism to combine multimodal features.
---

# CREMA: Generalizable and Efficient Video-Language Reasoning via Multimodal Modular Fusion

## Quick Facts
- arXiv ID: 2402.05889
- Source URL: https://arxiv.org/abs/2402.05889
- Authors: Shoubin Yu; Jaehong Yoon; Mohit Bansal
- Reference count: 37
- Primary result: Achieves superior or comparable performance on seven video-language reasoning benchmarks while reducing trainable parameters by over 90% compared to strong baselines

## Executive Summary
CREMA addresses the challenge of integrating diverse sensory modalities into video-language reasoning models efficiently. The proposed modular framework uses parameter-efficient adapters for each modality combined with a novel self-gated fusion mechanism to combine multimodal features. By supporting sequential modality training and adaptive early exits, CREMA improves both convergence and computational efficiency. The approach demonstrates strong empirical performance while maintaining significant parameter efficiency compared to existing methods.

## Method Summary
CREMA introduces a modular framework that processes multiple modalities through individual adapters, each with its own lightweight parameters. The framework employs a self-gated fusion mechanism to combine features from different modalities, allowing the model to adaptively weight contributions from each source. Sequential modality training enables progressive learning where each modality adapter can be trained independently before fusion. The design incorporates adaptive early exits that allow the model to terminate processing early when sufficient confidence is reached, improving efficiency without sacrificing accuracy.

## Key Results
- Achieves +2.4% improvement on MUSIC-A VQA benchmark
- Achieves +2.6% improvement on SQA3D benchmark
- Reduces trainable parameters by over 90% compared to strong baselines like BLIP-2 and SeViLA

## Why This Works (Mechanism)
The effectiveness of CREMA stems from its modular design that decouples modality-specific processing from cross-modal fusion. By using parameter-efficient adapters for each modality, the framework maintains flexibility while avoiding the computational burden of full fine-tuning. The self-gated fusion mechanism enables dynamic weighting of modality contributions based on task requirements and input characteristics. Sequential training allows each modality adapter to specialize before integration, while adaptive early exits prevent unnecessary computation when confidence thresholds are met.

## Foundational Learning
- **Parameter-efficient adapters**: Why needed - To reduce computational cost while maintaining model capacity; Quick check - Compare parameter counts between full fine-tuning and adapter-based approaches
- **Self-gated fusion mechanisms**: Why needed - To enable dynamic weighting of modality contributions; Quick check - Evaluate performance with fixed vs. adaptive fusion weights
- **Sequential modality training**: Why needed - To allow independent specialization before cross-modal integration; Quick check - Compare convergence rates between sequential and parallel training approaches

## Architecture Onboarding
- **Component map**: Input modalities -> Individual adapters -> Self-gated fusion -> Adaptive early exit decision -> Output
- **Critical path**: Modality processing → Fusion → Early exit evaluation → Final prediction
- **Design tradeoffs**: Parameter efficiency vs. fusion complexity; Sequential training vs. parallel processing; Early exit granularity vs. computational savings
- **Failure signatures**: Poor performance on tasks requiring tight cross-modal temporal alignment; Suboptimal results when modalities have highly interdependent features
- **First experiments**: 1) Ablation study removing self-gated fusion to quantify its contribution; 2) Comparison of sequential vs. parallel modality training; 3) Evaluation of early exit thresholds on efficiency-accuracy tradeoff

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on controlled benchmarks, potentially limiting assessment of real-world generalization
- Modular design may be less effective for tasks requiring tight cross-modal temporal alignment
- Computational overhead of sequential modality training is not fully characterized across different hardware configurations

## Confidence
- Performance gains over baselines: High
- Parameter efficiency claims: High
- Adaptive early exit effectiveness: Medium

## Next Checks
1. Evaluate CREMA's performance on open-domain video understanding datasets (e.g., HowTo100M, TVQA+) to assess real-world generalization beyond curated benchmarks
2. Conduct runtime analysis comparing sequential vs. parallel modality processing across different hardware setups to quantify practical efficiency trade-offs
3. Test CREMA's robustness to modality missingness patterns beyond those explicitly evaluated, including random vs. structured modality dropouts in both training and inference