---
ver: rpa2
title: 'TocBERT: Medical Document Structure Extraction Using Bidirectional Transformers'
arxiv_id: '2406.19526'
source_url: https://arxiv.org/abs/2406.19526
tags:
- pattern
- title
- segmentation
- text
- titles
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of hierarchical text segmentation,
  specifically the detection of titles and subtitles in medical documents. The authors
  propose TocBERT, a solution based on fine-tuning the Bio-ClinicalBERT model for
  named entity recognition.
---

# TocBERT: Medical Document Structure Extraction Using Bidirectional Transformers

## Quick Facts
- arXiv ID: 2406.19526
- Source URL: https://arxiv.org/abs/2406.19526
- Authors: Majd Saleh; Sarra Baghdadi; Stéphane Paquelet
- Reference count: 40
- Key outcome: Fine-tuned Bio-ClinicalBERT achieves 72.8% F1 for hierarchical and 84.6% F1 for linear title segmentation in medical discharge summaries

## Executive Summary
This paper addresses the challenge of hierarchical text segmentation in medical documents by detecting titles and subtitles in discharge summaries. The authors propose TocBERT, a solution that fine-tunes the Bio-ClinicalBERT model for named entity recognition to label tokens as titles, subtitles, or outside text. The model is trained on a semi-automatically labeled corpus from MIMIC-III, achieving significant improvements over rule-based baselines. The work also discusses potential applications in constructing topic ontologies from detected document structures.

## Method Summary
The authors frame hierarchical title detection as a token classification problem using named entity recognition with IOB labels (I-title, I-Stitle, O). Bio-ClinicalBERT is fine-tuned on discharge summaries from MIMIC-III, with training data semi-automatically labeled using a rule-based system (TocRegex) followed by manual curation. The model uses word-piece tokenization and projects full-word labels to subword tokens. Training involves 144k windows of 384 tokens each, with evaluation on a human-labeled test set of 250 reports.

## Key Results
- TocBERT achieves 72.8% F1-score for hierarchical segmentation (titles + subtitles) and 84.6% for linear segmentation (titles only)
- Significant improvement over the rule-based system baseline
- Bio-ClinicalBERT's domain-specific pretraining provides advantages for medical text processing
- Word-piece tokenization effectively handles specialized medical terminology

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning Bio-ClinicalBERT on token-level title/subtitle labels improves hierarchical segmentation performance compared to rule-based systems. The pretrained bidirectional transformer captures contextual semantic representations, allowing distinction between titles, subtitles, and regular text based on surrounding context rather than surface patterns.

### Mechanism 2
Treating document structure extraction as a sequence labeling problem enables the model to leverage bidirectional context for better predictions. By framing the problem as NER with I-title, I-Stitle, and O labels using IOB format, the model can attend to both left and right context when classifying each token's role in the document hierarchy.

### Mechanism 3
Word-piece tokenization and subsequent label projection from full words to subwords allows the model to handle out-of-vocabulary terms common in medical text. Breaking words into subword units enables representation of rare medical terms as combinations of more common subwords, with label projection ensuring each subword inherits the label of its parent word.

## Foundational Learning

- **Named Entity Recognition (NER)**: The document structure extraction task is framed as identifying entities (titles and subtitles) within text, making NER frameworks directly applicable. Quick check: What are the three entity types used in this NER formulation, and what does the "I-" prefix signify in the labeling scheme?

- **Bidirectional Transformers and Attention Mechanisms**: The BERT-based model needs to understand context from both directions to accurately classify tokens as titles, subtitles, or outside text. Quick check: How does the bidirectional nature of BERT differ from autoregressive models like GPT when processing sequences for classification tasks?

- **Tokenization and Subword Modeling**: Medical documents contain specialized terminology that may not appear in the pretraining corpus, requiring subword tokenization to handle out-of-vocabulary terms. Quick check: Why is word-piece tokenization particularly useful for medical text compared to character-level or word-level tokenization?

## Architecture Onboarding

- **Component map**: Text → Pre-tokenization → Label projection → BERT tokenization → Model inference → Post-processing → Final predictions

- **Critical path**: Raw text undergoes pre-tokenization for initial label assignment, followed by word-piece tokenization and label projection to subwords. The fine-tuned Bio-ClinicalBERT processes these tokens, producing subword-level predictions that are converted back to word-level predictions through post-processing.

- **Design tradeoffs**: Using 384-word windows balances GPU memory constraints with the need for sufficient context. Full fine-tuning allows adaptation of all parameters but requires more data and computation. Using IOB format enables distinction between titles and subtitles but adds complexity compared to binary classification.

- **Failure signatures**: High false positives on short phrases resembling titles but aren't, inability to detect nested or hierarchical structures beyond two levels, performance degradation on documents with unusual formatting or structure, overfitting to specific title patterns in training data.

- **First 3 experiments**: 1) Compare Bio-ClinicalBERT vs. general-domain BERT on small validation set to confirm domain adaptation benefits. 2) Test different window sizes (256, 384, 512 words) to find optimal balance between context and computational efficiency. 3) Evaluate impact of different label projection strategies (majority vote vs. first-subword assignment) on final performance.

## Open Questions the Paper Calls Out

1. **Ontology construction**: How can semantic vector representations of detected titles be effectively used to construct an ontology of topics for medical documents? The paper mentions this potential but lacks concrete methodology or implementation.

2. **Multi-level hierarchy extension**: Can the TocBERT model be adapted or extended to handle hierarchical segmentation beyond two levels (titles and subtitles)? The current model is designed for two-level hierarchical segmentation, but medical documents often have more complex structures.

3. **Cross-domain benchmarking**: How does TocBERT performance compare to other state-of-the-art models for text segmentation in different domains or on different types of documents? The paper compares only to a rule-based system on medical discharge summaries without benchmarking against other models or evaluating on diverse datasets.

## Limitations

- Limited external validation on single dataset (MIMIC-III discharge summaries) without testing on other medical document types or domains
- Semi-automatic labeling pipeline concerns regarding inter-annotator agreement and potential systematic biases from rule-based initial labeling
- Subword labeling ambiguity in handling complex medical terminology during word-piece tokenization and label projection

## Confidence

- **High confidence**: Experimental methodology is sound with appropriate evaluation metrics and clear comparison against rule-based baseline
- **Medium confidence**: Performance improvements are convincing within tested domain but limited scope reduces broader applicability confidence
- **Low confidence**: Claims about using detected titles/subtitles for constructing topic ontologies lack supporting evidence or experiments

## Next Checks

1. **Cross-domain generalization test**: Evaluate TocBERT on medical documents from different sources (clinical notes, radiology reports, pathology reports) to assess performance stability across document types and formatting variations.

2. **Rule-based system ablation study**: Analyze specific contributions of individual TocRegex patterns by systematically disabling different rules and measuring impact on both rule-based system and TocBERT performance.

3. **Label projection error analysis**: Conduct detailed examination of subword-level predictions to identify cases where label projection from full words to subwords may have introduced errors, particularly for medical terminology, and assess frequency and impact on overall performance.