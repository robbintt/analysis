---
ver: rpa2
title: 'Dusk Till Dawn: Self-supervised Nighttime Stereo Depth Estimation using Visual
  Foundation Models'
arxiv_id: '2405.11158'
source_url: https://arxiv.org/abs/2405.11158
tags:
- depth
- estimation
- disparity
- stereo
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of self-supervised stereo depth
  estimation in nighttime scenarios, where traditional methods struggle due to low
  texture and fluctuating illumination. The authors propose an approach that leverages
  pretrained visual foundation models, specifically DINO and DINOv2, to extract generalized
  features across challenging scenes.
---

# Dusk Till Dawn: Self-supervised Nighttime Stereo Depth Estimation using Visual Foundation Models

## Quick Facts
- arXiv ID: 2405.11158
- Source URL: https://arxiv.org/abs/2405.11158
- Authors: Madhu Vankadari; Samuel Hodgson; Sangyun Shin; Kaichen Zhou Andrew Markham; Niki Trigoni
- Reference count: 40
- One-line primary result: Proposes a self-supervised method for nighttime stereo depth estimation using visual foundation models that achieves state-of-the-art performance on Oxford RobotCar and Multi-Spectral Stereo datasets.

## Executive Summary
This paper addresses the challenge of self-supervised stereo depth estimation in nighttime scenarios where traditional methods struggle due to low texture and fluctuating illumination. The authors propose leveraging pretrained visual foundation models (DINO and DINOv2) to extract generalized features that are robust across challenging lighting conditions. They introduce a feature-level masking technique to filter out pixels violating photometric consistency assumptions and a distance regularizer to enhance feature descriptions. The method demonstrates significant improvements over state-of-the-art approaches, particularly in challenging regions like the sky, and generalizes well to unseen datasets.

## Method Summary
The method uses pretrained visual foundation models to extract features from stereo image pairs, projects these features to a lower dimension, and applies a cross-image transformer to enhance feature matching. A stereo matcher estimates coarse disparity through global matching followed by local refinement. The approach includes a feature-level masking technique based on intra-image feature distances to filter noisy pixels, and a distance regularizer that spreads features on the unit sphere to improve distinctiveness. The model is trained using photometric loss, distance regularizer loss, and edge-aware disparity smoothness loss in a self-supervised manner without ground truth depth labels.

## Key Results
- Achieves state-of-the-art performance on Oxford RobotCar and Multi-Spectral Stereo datasets for nighttime stereo depth estimation
- Demonstrates robust performance in challenging regions like sky where traditional methods struggle
- Shows competitive results compared to both supervised and self-supervised methods while requiring no ground truth depth labels during training
- Proposes novel evaluation metrics that account for non-uniform depth distribution in typical datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using pretrained visual foundation models (DINO and DINOv2) provides robust features that generalize across challenging nighttime conditions, where traditional classification-based networks fail.
- Mechanism: Visual foundation models are trained on large, diverse datasets without supervision, capturing more general visual patterns that are less sensitive to domain shifts like lighting changes. This allows them to extract meaningful features even from low-texture and noisy nighttime images.
- Core assumption: Features learned by foundation models on ImageNet and Facebook LVD-142M are transferable to nighttime stereo depth estimation tasks.
- Evidence anchors:
  - [abstract]: "Specifically, we use pretrained visual foundation models to extract generalised features across challenging scenes"
  - [section]: "Contemporary self-supervised feature extractors offer enhanced robustness and clarity in feature mapping, as they do not rely on labeled data"
  - [corpus]: Found 25 related papers discussing robust depth estimation across challenging conditions. Average neighbor FMR=0.389. Weak corpus evidence specific to foundation models in nighttime depth estimation.

### Mechanism 2
- Claim: The feature-level masking technique filters out pixels that violate photometric consistency assumptions, preventing them from negatively affecting depth predictions.
- Mechanism: By measuring intra-image feature description distances, the algorithm identifies pixels with low texture or high noise (like sky regions) and masks them out. This prevents erroneous depth estimates from these unreliable regions from corrupting the final depth map.
- Core assumption: Pixels with low minimum feature distances from their nearest neighbors are likely to be in poorly lit or noisy areas that violate photometric consistency.
- Evidence anchors:
  - [section]: "We conjecture that features belonging to noisy areas tend to have a lower minimum distance from their nearest neighbors, compared to the features that belong to well-lit areas"
  - [section]: "To address this, we propose a simple yet effective solution based on intra-image feature description distances, using them to mask the noisy areas"
  - [corpus]: Weak evidence specific to masking in nighttime depth estimation. General evidence for feature filtering exists in stereo matching literature.

### Mechanism 3
- Claim: The distance regularizer enhances feature descriptions by encouraging features to maximize their minimum distance from their nearest neighbors, improving depth estimation accuracy in poorly lit areas.
- Mechanism: This regularization loss spreads features out on the unit sphere, making them more distinctive and reducing confusion between similar features. This is particularly helpful in poorly lit areas where feature similarity is already a problem.
- Core assumption: Increasing feature distinctiveness through distance regularization will lead to more accurate depth estimation, especially in challenging lighting conditions.
- Evidence anchors:
  - [section]: "We also encourage all of the features to maximise the minimum distance from their nearest neighbour using a regularization loss"
  - [section]: "This allows the features from poorly lit areas to improve"
  - [corpus]: Weak specific evidence for distance regularization in nighttime depth estimation. General evidence exists in metric learning literature.

## Foundational Learning

- Concept: Photometric consistency assumptions in stereo depth estimation
  - Why needed here: Understanding these assumptions is crucial because they break down in nighttime scenarios with low texture and fluctuating illumination, which is the core problem this paper addresses.
  - Quick check question: What are the key assumptions behind photometric consistency in stereo depth estimation, and why do they fail in nighttime conditions?

- Concept: Visual foundation models and self-supervised learning
  - Why needed here: The paper leverages pretrained visual foundation models (DINO and DINOv2) instead of traditional classification networks, which is central to its approach for handling challenging conditions.
  - Quick check question: How do visual foundation models differ from traditional classification networks, and why might they be more robust to domain shifts like lighting changes?

- Concept: Feature matching and correspondence in stereo vision
  - Why needed here: The paper's approach relies on efficient matching and integration of features from stereo frames, including techniques like global and local matching, which are fundamental to stereo depth estimation.
  - Quick check question: What are the differences between global matching (using correlation volumes) and local matching approaches in stereo depth estimation, and when might each be preferable?

## Architecture Onboarding

- Component map: Stereo images -> DINO/DINOv2 feature extractor -> Projection head -> Cross-image transformer -> Stereo matcher -> Global matching -> Local refinement -> RAFT upsampling -> Disparity/depth map

- Critical path: Feature extraction → projection → stereo matching (with transformer) → coarse disparity → disparity filtering/masking → global disparity propagation → disparity refinement → upsampling

- Design tradeoffs:
  - Using foundation models vs. traditional classification networks: Better generalization vs. potentially higher computational cost
  - Global vs. local matching: More accurate but computationally expensive vs. faster but potentially less accurate
  - Masking approach: Prevents noise from affecting results vs. potential loss of valid information in masked regions
  - Distance regularizer: Improves feature distinctiveness vs. potential distortion of feature space

- Failure signatures:
  - Overexposed areas and lane markings create undesirable edges in disparity maps (mentioned in failure cases)
  - Performance drop when using DINOv2 despite larger pretraining dataset
  - Sky regions may still have some issues despite masking approach
  - Generalizability concerns when testing on datasets with different geographic locations and lighting conditions

- First 3 experiments:
  1. Implement and test the feature extractor with DINO on RobotCar dataset to verify feature quality and dimension reduction effectiveness
  2. Test the stereo matcher with global matching only (without local refinement) to establish baseline performance and identify the contribution of refinement
  3. Implement and evaluate the masking approach on a subset of challenging images (e.g., with sky regions) to verify it correctly identifies and masks noisy areas without losing valid depth information

## Open Questions the Paper Calls Out

- Question: How does the proposed method perform when trained and tested on datasets with significantly different environmental conditions (e.g., urban vs. rural, different weather patterns)?
  - Basis in paper: [inferred] The paper mentions that the model generalizes well to unseen datasets like MS2, but does not extensively explore performance across diverse environmental conditions.
  - Why unresolved: The paper focuses on nighttime depth estimation and evaluates on specific datasets, leaving the impact of varied environmental conditions unexplored.
  - What evidence would resolve it: Conducting experiments on datasets with diverse environmental conditions (e.g., KITTI, cityscapes) and comparing performance metrics would provide insights into the model's robustness and generalizability.

- Question: Can the feature-level masking technique be extended to other domains, such as object detection or segmentation, to improve performance in low-light or noisy conditions?
  - Basis in paper: [explicit] The paper introduces a feature-level masking technique to filter out pixels violating photometric consistency assumptions, which improves depth estimation accuracy.
  - Why unresolved: The paper does not explore the application of this masking technique beyond depth estimation.
  - What evidence would resolve it: Applying the masking technique to other computer vision tasks (e.g., object detection, segmentation) in low-light or noisy conditions and evaluating performance improvements would demonstrate its broader applicability.

- Question: How does the proposed method handle dynamic objects (e.g., moving vehicles or pedestrians) in the scene, which could violate the photometric consistency assumption?
  - Basis in paper: [inferred] The paper mentions that photometric consistency assumes the scene to be static, but does not explicitly address the handling of dynamic objects.
  - Why unresolved: The paper does not provide experimental results or analysis on the method's performance in the presence of dynamic objects.
  - What evidence would resolve it: Evaluating the method on datasets with dynamic objects (e.g., KITTI, Cityscapes) and comparing performance metrics with and without dynamic object handling techniques would provide insights into its robustness in such scenarios.

## Limitations
- Reliance on specific visual foundation models pretrained on datasets that may not fully represent nighttime scene diversity
- Feature masking approach introduces hyperparameters requiring dataset-specific tuning
- Remaining challenges with certain textureless regions like over-exposed areas and lane markings
- Computational overhead of transformer-based models may limit real-time deployment

## Confidence
- Feature extraction using foundation models: Medium-High
- Feature masking technique effectiveness: Medium
- Distance regularizer contribution: Medium
- Generalization to unseen datasets: Medium-High

## Next Checks
1. Cross-dataset generalization test: Evaluate the trained model on additional nighttime datasets with different geographic locations and sensor characteristics to verify true generalization beyond the Oxford RobotCar and Multi-Spectral Stereo datasets.

2. Ablation study on masking thresholds: Systematically vary the masking threshold zeta and distance regularizer strength gamma to quantify their impact on performance across different scene types, particularly in challenging regions like sky and overexposed areas.

3. Real-time feasibility assessment: Measure the computational requirements and inference latency of the full pipeline, including feature extraction with DINO/DINOv2, to determine practical deployment constraints and identify potential optimization opportunities.