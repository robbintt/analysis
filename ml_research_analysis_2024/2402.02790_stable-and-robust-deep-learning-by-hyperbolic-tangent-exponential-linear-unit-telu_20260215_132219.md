---
ver: rpa2
title: Stable and Robust Deep Learning By Hyperbolic Tangent Exponential Linear Unit
  (TeLU)
arxiv_id: '2402.02790'
source_url: https://arxiv.org/abs/2402.02790
tags: []
core_contribution: This paper introduces TeLU (Hyperbolic Tangent Exponential Linear
  Unit), a novel activation function for deep neural networks. TeLU combines the benefits
  of ReLU, GELU, and Mish while addressing their limitations like vanishing gradients
  and exploding gradients.
---

# Stable and Robust Deep Learning By Hyperbolic Tangent Exponential Linear Unit (TeLU)

## Quick Facts
- arXiv ID: 2402.02790
- Source URL: https://arxiv.org/abs/2402.02790
- Authors: Alfredo Fernandez; Ankur Mali
- Reference count: 40
- Key outcome: TeLU activation function (f(x) = x·tanh(ex)) outperforms ReLU, GELU, Mish across CIFAR-10, CIFAR-100, and TinyImageNet with lower variance

## Executive Summary
This paper introduces TeLU (Hyperbolic Tangent Exponential Linear Unit), a novel activation function for deep neural networks that combines the benefits of ReLU, GELU, and Mish while addressing their limitations like vanishing and exploding gradients. The proposed function f(x) = x·tanh(ex) is theoretically analyzed and empirically evaluated on image classification tasks. TeLU demonstrates superior performance and stability across multiple architectures and optimizers, achieving higher accuracy with lower variance compared to conventional activation functions.

## Method Summary
TeLU is implemented as f(x) = x·tanh(ex) with derivative f'(x) = tanh(ex) + x·(1-tanh²(ex))·ex. The method was evaluated on CIFAR-10 (45k train, 5k val, 10k test), CIFAR-100 (45k train, 5k val, 10k test), and TinyImageNet (100k train, 10k val) using SqueezeNet, ResNet-18/32/50 architectures with SGD, Momentum, AdamW, and RMSprop optimizers. A grid search was performed for learning rate, weight decay, and gamma across 200 epochs with batch size 128, comparing TeLU against ReLU, GELU, Mish, SiLU, Smish, and Logish.

## Key Results
- TeLU consistently achieved the highest accuracy across all tested architectures and datasets
- TeLU showed lower variance (typically < 0.2) compared to other activation functions
- Performance advantages were maintained across all optimizers (SGD, Momentum, AdamW, RMSprop)
- TeLU demonstrated stable convergence even under hyperparameter conditions optimized for other functions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TeLU avoids vanishing gradients by maintaining non-zero derivative for all x
- Mechanism: The derivative f'(x) = tanh(ex) + x·(1-tanh²(ex))·ex is always non-zero because tanh(ex) > 0 for all x and the second term is non-zero except at x = 0
- Core assumption: The derivative analysis correctly captures behavior across all real values of x
- Evidence anchors: Theorem 1 states f'(x) ≠ 0 for all x ∈ R; abstract claims TeLU addresses vanishing gradient problems

### Mechanism 2
- Claim: TeLU provides controlled growth for positive inputs and saturating behavior for negative inputs
- Mechanism: For x > 0, f(x) grows linearly as tanh(ex) approaches 1; for x ≤ 0, f(x) approaches 0 as x → -∞ due to tanh(ex) approaching 0
- Core assumption: Behavior of tanh and exponential functions under specified conditions is accurately characterized
- Evidence anchors: Theorem 2 proves controlled growth for x > 0 and saturating behavior for x ≤ 0

### Mechanism 3
- Claim: TeLU has zero-mean activation for symmetric distributions, leading to improved training stability
- Mechanism: For symmetric probability distributions around zero, the integral of f(x) over [-a, a] approaches zero as a → ∞
- Core assumption: Integral analysis correctly captures asymptotic behavior for symmetric distributions
- Evidence anchors: Theorem 3 proves expected value of f(x) is approximately zero for symmetric distributions

## Foundational Learning

- Concept: Fisher Information Matrix (FIM) and its role in optimization landscape
  - Why needed here: Paper claims TeLU leads to smoother FIM, correlating with enhanced training stability
  - Quick check question: How does a smoother Fisher Information Matrix affect gradient descent convergence properties?

- Concept: Lipschitz continuity and its implications for neural network stability
  - Why needed here: Paper proves TeLU is Lipschitz continuous, important for uniform continuity and stability
  - Quick check question: Why is Lipschitz continuity a desirable property for activation functions in deep networks?

- Concept: Gradient vanishing and exploding problems in deep learning
  - Why needed here: TeLU is specifically designed to address both vanishing and exploding gradient problems
  - Quick check question: What causes gradient vanishing and exploding problems in deep neural networks?

## Architecture Onboarding

- Component map: Input -> TeLU activation (f(x) = x·tanh(ex)) -> Output
- Critical path:
  1. Forward pass: Compute activation using f(x) = x·tanh(ex)
  2. Backward pass: Compute gradient using f'(x) = tanh(ex) + x·(1-tanh²(ex))·ex
  3. Parameter update: Standard gradient descent with computed gradients
- Design tradeoffs:
  - Computational cost: Requires computing tanh and exponential functions (3-4× more expensive than ReLU)
  - Memory usage: No additional memory overhead compared to standard activation functions
  - Numerical stability: Requires careful handling of large positive/negative values to avoid overflow/underflow
- Failure signatures:
  - NaN or Inf values in activations or gradients (numerical overflow)
  - Training instability or divergence (incorrect implementation or extreme hyperparameters)
  - Suboptimal performance compared to ReLU (data distribution not suitable for TeLU's properties)
- First 3 experiments:
  1. Simple MLP on synthetic data with varying input distributions to verify zero-mean property
  2. CIFAR-10 classification with small ResNet architecture to compare convergence speed vs ReLU
  3. Gradient norm analysis during training to verify absence of vanishing/exploding gradients

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does TeLU's performance compare to other activation functions on larger-scale datasets like ImageNet?
- Basis in paper: The paper mentions TeLU's effectiveness on CIFAR-10, CIFAR-100, and TinyImageNet, but doesn't provide results on ImageNet or other large-scale datasets
- Why unresolved: The paper focuses on smaller datasets due to computational limitations
- What evidence would resolve it: Conducting experiments on ImageNet or other large-scale datasets using TeLU and comparing its performance with other activation functions

### Open Question 2
- Question: What is the impact of TeLU on different neural network architectures beyond the ones tested (SqueezeNet and ResNet variants)?
- Basis in paper: The paper evaluates TeLU on SqueezeNet and ResNet architectures, but doesn't explore its effectiveness on other architectures like VGG, DenseNet, or Transformer-based models
- Why unresolved: The study's focus on specific architectures limits the generalizability of TeLU's performance across different network designs
- What evidence would resolve it: Implementing TeLU in various neural network architectures and comparing its performance with other activation functions

### Open Question 3
- Question: How does TeLU's performance change when applied to tasks outside of image classification, such as natural language processing or speech recognition?
- Basis in paper: The paper's experiments are limited to image classification tasks, leaving its effectiveness in other domains unexplored
- Why unresolved: The study's focus on computer vision tasks doesn't provide information about TeLU's potential in other areas of deep learning
- What evidence would resolve it: Applying TeLU to various deep learning tasks like NLP, speech recognition, or reinforcement learning and comparing its performance with other activation functions

## Limitations

- Computational overhead: TeLU requires computing both exponential and tanh functions, making it significantly more expensive than ReLU (approximately 3-4× higher per-activation cost)
- No ablation studies to isolate which component of TeLU contributes most to performance improvements
- Limited dataset diversity - evaluation restricted to image classification tasks without testing on NLP, speech, or structured data

## Confidence

- **High Confidence**: Empirical performance comparisons on CIFAR-10/CIFAR-100/TinyImageNet show TeLU consistently outperforms ReLU, GELU, and Mish with lower variance across multiple architectures and optimizers
- **Medium Confidence**: Theoretical claims about avoiding vanishing gradients and providing zero-mean activations rely on asymptotic analysis that may not fully capture finite-depth network behavior
- **Low Confidence**: Claims about "implicit regularization effect" and "smoother Fisher Information Matrix" lack quantitative validation or ablation studies demonstrating these mechanisms

## Next Checks

1. **Computational Cost Analysis**: Measure wall-clock training time per epoch for TeLU vs ReLU across different batch sizes to quantify the practical overhead
2. **Ablation Study**: Test variants like f(x) = x·tanh(x) and f(x) = x·tanh(k·x) for different k values to isolate the contribution of the exponential scaling
3. **Distribution Sensitivity**: Evaluate TeLU on non-symmetric input distributions (e.g., natural language embeddings) to verify the zero-mean property assumption holds in practice