---
ver: rpa2
title: 'Axe the X in XAI: A Plea for Understandable AI'
arxiv_id: '2403.00315'
source_url: https://arxiv.org/abs/2403.00315
tags:
- explanation
- understanding
- causal
- explanations
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This chapter challenges the idea that existing accounts of scientific
  explanation from the philosophy of science can be directly applied to deep neural
  networks in explainable AI. The author argues that these models fail to account
  for the complexity and opacity of DNNs, particularly in terms of factivity and interpretability.
---

# Axe the X in XAI: A Plea for Understandable AI

## Quick Facts
- arXiv ID: 2403.00315
- Source URL: https://arxiv.org/abs/2403.00315
- Reference count: 0
- Primary result: Traditional philosophical accounts of scientific explanation cannot be directly applied to deep neural networks; understanding should be the focus instead of explanation.

## Executive Summary
This paper argues that existing philosophical accounts of scientific explanation from the philosophy of science fail to adequately explain deep neural networks (DNNs) due to their complexity, opacity, and lack of lawlike structures. The author contends that these accounts cannot satisfy the factivity condition required for scientific explanation when applied to DNNs. Instead, the paper advocates for a pragmatic approach to understanding, where success in using the system and drawing correct inferences become the primary criteria. The author suggests that different stakeholders may require different levels of understanding depending on their context and needs, and that surrogate models can provide "true enough" functional representations that enable understanding without requiring full causal transparency.

## Method Summary
The paper conducts a philosophical analysis of four major accounts of scientific explanation (Deductive Nomological, Inductive Statistical, Causal Mechanical, and New Mechanist models) and systematically demonstrates their inapplicability to deep neural networks. Through logical argumentation, the author shows that DNNs lack the lawlike structures, factive grounding, and genuine causal mechanisms that these philosophical frameworks require. The analysis then proposes an alternative framework based on pragmatic understanding, drawing on Kuorikoski & Ylikoski's success-based criteria for understanding. The approach involves evaluating surrogate models as epistemic tools that capture functional similarities rather than exact causal mechanisms.

## Key Results
- Traditional philosophical accounts of scientific explanation (D-N, I-S, Causal Mechanical, New Mechanist) cannot be directly applied to deep neural networks due to lack of factivity and lawlike structures.
- Understanding, defined as success in using the system and drawing correct inferences, is more appropriate than explanation for XAI contexts.
- Surrogate models can provide "true enough" functional representations that enable stakeholder understanding without requiring full causal transparency.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Traditional philosophy-of-science accounts of explanation fail to capture ML explanation because DNNs lack lawlike statements, factive grounding, and genuine causal structure.
- Mechanism: The argument proceeds by showing that each model's conditions for explanation—deterministic/statistical laws, genuine causation, mechanistic entities—are absent in opaque DNNs. Without lawlike relations or verifiable mechanisms, explanations cannot satisfy factivity.
- Core assumption: Scientific explanation requires factivity (truth of explanans) and lawlike generality; DNNs provide neither.
- Evidence anchors:
  - [abstract] "These models fail to account for the complexity and opacity of DNNs, particularly in terms of factivity and interpretability."
  - [section] "The only viable alternative is to look at the model itself as composed of deterministic lawlike relationships...there is no way to verify which parameters are being used in the hidden layers."
  - [corpus] "Explainable artificial intelligence (XAI) is concerned with producing explanations indicating the inner workings of models... However explanations the..." (limited anchor—focus on model opacity).
- Break condition: If one adopts a purely pragmatic, non-factive notion of explanation, the mechanism no longer blocks application of these accounts.

### Mechanism 2
- Claim: Understanding, unlike explanation, can be pragmatic and success-based, making it better suited to ML contexts.
- Mechanism: By redefining understanding as the ability to use a model correctly and draw correct inferences (Kuorikoski & Ylikoski), the focus shifts from truth conditions to practical success. This allows surrogate models to serve as "true enough" epistemic tools without requiring factivity.
- Core assumption: Success in use (counterfactual reasoning, prediction) is a valid indicator of understanding even when models are not factive.
- Evidence anchors:
  - [abstract] "understanding should be the focus, rather than explanation, and that different stakeholders may require different levels of understanding."
  - [section] "Following Kuorikoski & Ylikoski (2015), the conditions of satisfaction for understanding an ML system are fleshed out in terms of an agent's success in using the system, in drawing correct inferences from it."
  - [corpus] "The field of 'explainable artificial intelligence' (XAI) seemingly addresses the desire that decisions of machine learning systems should be human-understandable." (weak anchor—general XAI framing).
- Break condition: If understanding must also be factive (as some epistemologists argue), the mechanism collapses back to the original problem.

### Mechanism 3
- Claim: Surrogate models provide "true enough" functional representations that enable understanding without requiring full fidelity or causal grounding.
- Mechanism: Surrogate models (e.g., sparse decision trees, rule lists) abstract away complexity, capturing functional dependencies relevant to the user's task. Their interpretability comes from simplicity and alignment with user cognitive models, not from mirroring DNN internals.
- Core assumption: Functional similarity for a specific task suffices for understanding; causal grounding is not always necessary for stakeholder needs.
- Evidence anchors:
  - [section] "surrogate models in ML are not factive. They are 'true enough' of their target, to use Elgin's (2017) suggestive phrase."
  - [section] "surrogate models as epistemic tools... designed to capture functional similarities of interest."
  - [corpus] "Beyond sparsity: Tree regularization of deep models for interpretability." (weak anchor—indirect evidence).
- Break condition: If the user requires full causal transparency (e.g., for safety-critical domains), surrogate models may be insufficient.

## Foundational Learning

- Concept: Difference between scientific explanation and understanding
  - Why needed here: The paper pivots from explaining DNNs via philosophy-of-science models to understanding via pragmatic success criteria.
  - Quick check question: Can you give an example where someone "knows" a fact but does not "understand" it in the ML context?

- Concept: Factivity vs. "true enough" in model interpretation
  - Why needed here: Central to why surrogate models can serve understanding without satisfying explanation's truth requirement.
  - Quick check question: Why might a simplified linear surrogate be "true enough" for a clinician but not for a researcher debugging the DNN?

- Concept: Success-based criteria for understanding
  - Why needed here: The pragmatic definition of understanding (Kuorikoski & Ylikoski) replaces traditional explanation conditions.
  - Quick check question: What operational behaviors would demonstrate a stakeholder "understands" a loan-approval DNN?

## Architecture Onboarding

- Component map: DNN model -> Surrogate model generator -> Stakeholder interface -> Evaluation metrics
- Critical path: Generate surrogate → present to stakeholder → assess understanding via task performance → iterate design
- Design tradeoffs: Simpler surrogates are more interpretable but less faithful; more faithful surrogates are harder to understand.
- Failure signatures: Stakeholders cannot predict new outputs; they fail counterfactual reasoning tasks; they cannot articulate model behavior.
- First 3 experiments:
  1. Compare rule-list surrogate vs. counterfactual explanation on loan-approval task; measure user prediction accuracy.
  2. Test whether "true enough" surrogates improve trust vs. full-fidelity surrogates.
  3. Vary surrogate complexity and measure understanding via behavioral tasks across novice and expert users.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can existing philosophical accounts of scientific explanation be meaningfully adapted to explain deep neural networks?
- Basis in paper: [explicit] The paper argues that four philosophical accounts of scientific explanation (Deductive Nomological, Inductive Statistical, Causal Mechanical, and New Mechanist models) cannot be directly applied to deep neural networks due to their complexity and opacity.
- Why unresolved: The author claims that these models fail to account for the complexity and opacity of DNNs, particularly in terms of factivity and interpretability. The author suggests that the concept of explanation in XAI is fundamentally different from traditional scientific explanation.
- What evidence would resolve it: Evidence could include successful applications of these philosophical accounts to DNNs, or demonstrations of their limitations in explaining DNN predictions and behaviors.

### Open Question 2
- Question: Is understanding a more appropriate goal than explanation in the context of explainable AI?
- Basis in paper: [explicit] The author advocates for a pragmatic approach to understanding, emphasizing the importance of success in using the system and drawing correct inferences. The author proposes that understanding should be the focus, rather than explanation, in XAI.
- Why unresolved: The paper argues that the concept of explanation in XAI is confusing and does not align with traditional scientific explanation. The author suggests that understanding is a more suitable goal for XAI, but does not provide a definitive answer on whether this is the case.
- What evidence would resolve it: Evidence could include empirical studies comparing the effectiveness of explanation-based and understanding-based approaches in XAI, or theoretical arguments supporting the superiority of one approach over the other.

### Open Question 3
- Question: How can we measure and evaluate understanding in the context of machine learning systems?
- Basis in paper: [inferred] The author discusses the importance of success in using the system and drawing correct inferences as indicators of understanding. However, the paper does not provide a concrete framework for measuring and evaluating understanding in ML.
- Why unresolved: The paper suggests that understanding is a success concept, but does not provide a clear definition or method for assessing it. The author mentions the need for contextual criteria and different levels of understanding for different stakeholders, but does not elaborate on how to implement these ideas.
- What evidence would resolve it: Evidence could include the development of a standardized framework for measuring understanding in ML, or empirical studies demonstrating the effectiveness of different methods for assessing understanding in various contexts and with different stakeholders.

## Limitations

- The analysis assumes a strong factive requirement for explanation that some philosophers reject, which would weaken the argument against applying traditional accounts to DNNs.
- The pragmatic understanding framework is not fully developed with operational criteria for measuring success across different stakeholder contexts.
- The treatment of surrogate models as "true enough" lacks rigorous validation of when functional similarity suffices versus when causal fidelity is necessary.

## Confidence

**High confidence**: That current DNNs lack the lawlike structure and verifiable causal mechanisms assumed by traditional explanation models; that stakeholders have varying understanding needs; that success-based criteria offer a pragmatic alternative framework.

**Medium confidence**: That surrogate models can provide "true enough" understanding without full factivity; that the shift from explanation to understanding meaningfully advances XAI practice.

**Low confidence**: That no variant of scientific explanation accounts could be adapted for DNNs with modified assumptions; that the pragmatic framework adequately captures all dimensions of stakeholder understanding needs.

## Next Checks

1. **Factivity threshold experiment**: Systematically vary the truth requirements of explanations for different stakeholder tasks (e.g., clinical diagnosis vs. model debugging) and measure the impact on task performance and trust calibration.

2. **Surrogate sufficiency test**: Compare understanding outcomes (measured via prediction accuracy, counterfactual reasoning, and explanation articulation) between "true enough" surrogates and higher-fidelity approximations across novice and expert users on identical prediction tasks.

3. **Explanation-to-understanding mapping**: Develop and validate a taxonomy that maps specific explanation types (feature importance, counterfactuals, surrogate models) to understanding requirements across stakeholder roles, testing whether this mapping improves XAI system design and evaluation.