---
ver: rpa2
title: "ArchesWeather: An efficient AI weather forecasting model at 1.5\xB0 resolution"
arxiv_id: '2405.14527'
source_url: https://arxiv.org/abs/2405.14527
tags: []
core_contribution: This paper identifies a computational inefficiency in 3D local
  attention mechanisms used in weather forecasting models like Pangu-Weather, where
  neighboring pressure levels are processed independently. The authors propose a Cross-Level
  Attention (CLA) mechanism that allows global vertical interactions while maintaining
  computational efficiency.
---

# ArchesWeather: An efficient AI weather forecasting model at 1.5° resolution

## Quick Facts
- arXiv ID: 2405.14527
- Source URL: https://arxiv.org/abs/2405.14527
- Reference count: 40
- Key outcome: Achieves competitive performance with IFS HRES and outperforms NeuralGCM ensemble for 1-day forecasts while requiring only few GPU-days to train

## Executive Summary
This paper presents ARCHES WEATHER, an efficient AI weather forecasting model that addresses computational inefficiencies in existing transformer-based approaches. The key innovation is a Cross-Level Attention (CLA) mechanism that enables global vertical interactions while maintaining computational efficiency, replacing the 3D local attention used in models like Pangu-Weather. Trained at 1.5° resolution, the model achieves competitive performance with operational weather forecasting systems while requiring significantly less training time - just a few GPU-days compared to orders of magnitude more for competing methods.

## Method Summary
The authors develop a 3D Swin U-Net transformer architecture with Cross-Level Attention (CLA) modules to improve vertical atmospheric interactions. The model is trained on ERA5 reanalysis data at 1.5° resolution using 6 upper air variables (temperature, geopotential, specific humidity, wind components U, V, W) at 13 pressure levels and 4 surface variables. The CLA mechanism processes column-wise data as a sequence along the vertical dimension, enabling information exchange across all pressure levels in a single attention operation. The model is fine-tuned on recent ERA5 data (2007-2018) to address distribution shifts caused by improvements in observation systems.

## Key Results
- Achieves competitive performance with IFS HRES and outperforms the 50-member NeuralGCM ensemble for 1-day forecasts
- Requires only a few GPU-days to train compared to orders of magnitude more for competing methods
- Fine-tuning on recent ERA5 data (2007-2018) provides additional performance benefits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-Level Attention (CLA) improves computational efficiency by allowing global vertical interactions while maintaining low parameter count.
- Mechanism: CLA processes column-wise data as a sequence along the vertical dimension, enabling information exchange across all pressure levels in a single attention operation.
- Core assumption: Vertical atmospheric interactions are more global than local at short timescales, and sharing computations across levels improves efficiency.
- Evidence anchors: [abstract] "Cross-Level Attention (CLA) mechanism that allows global vertical interactions while maintaining computational efficiency"; [section] "Global vertical interaction would allow sharing such computations, allocating resources more efficiently"
- Break condition: If atmospheric physics at short timescales requires truly local vertical interactions, or if vertical correlations are too weak to justify global attention

### Mechanism 2
- Claim: Removing 3D local attention's vertical component and replacing with CLA reduces parameter count while improving performance.
- Mechanism: Original 3D attention windows of shape (2,6,12) have O(d²Z²) parameters when processing concatenated columns. CLA with horizontal windows (1,6,12) and column-wise attention has only O(d²) parameters.
- Core assumption: The parameter reduction from O(d²Z²) to O(d²) doesn't sacrifice representational capacity for weather forecasting.
- Evidence anchors: [section] "With Z pressure levels... the linear and attention layer need O(d²Z²) parameters... By considering column data as a sequence of size Z, the number of parameters in this attention module is O(d²)"; [section] "ARCHES WEATHER with 16 layers reaches lower error than using 32 layers without CLA"
- Break condition: If the reduced parameter count limits the model's ability to capture complex vertical atmospheric patterns

### Mechanism 3
- Claim: Fine-tuning on recent ERA5 data (2007-2018) improves performance by addressing distribution shift.
- Mechanism: ERA5 data quality and observation systems changed around 2000, creating a distribution shift that degrades model performance on older data. Fine-tuning on recent data adapts the model to current conditions.
- Core assumption: The distribution shift is significant enough to impact forecasting accuracy and can be corrected through fine-tuning.
- Evidence anchors: [section] "we find that forecasting models have a larger error in the first half of the training period 1979-2018... which we attribute to ERA5 being less constrained in the past due to a lack of observation data"; [section] "To overcome this distribution shift, we use recent ERA5 data (2007-2018) for fine-tuning"
- Break condition: If the distribution shift is not the primary cause of performance degradation, or if fine-tuning overfits to recent patterns

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The entire ARCHES WEATHER architecture is built on transformer principles, with specific modifications to 3D attention
  - Quick check question: What is the computational complexity of standard self-attention, and how does window-based attention reduce this cost?

- Concept: Atmospheric physics and vertical coupling
  - Why needed here: Understanding why vertical interactions matter for weather prediction and how local vs. global vertical coupling affects forecast accuracy
  - Quick check question: How do atmospheric variables at different pressure levels typically correlate, and what physical processes drive these correlations?

- Concept: Distribution shift and fine-tuning
  - Why needed here: The paper's approach to handling ERA5 data quality changes requires understanding how temporal distribution shifts affect model performance
  - Quick check question: What are common causes of distribution shift in climate/weather datasets, and how can fine-tuning mitigate these effects?

## Architecture Onboarding

- Component map: Input embedding -> 2D attention with CLA -> convolutional head with bilinear upsampling -> adaptive LayerNorm for conditioning
- Critical path: Input embedding → 2D attention with CLA → convolutional head → output prediction
- Design tradeoffs: CLA reduces parameters and enables global vertical interactions but may lose some local vertical processing benefits; convolutional head avoids checkerboard artifacts but adds complexity
- Failure signatures: Poor vertical coupling performance suggests CLA implementation issues; checkerboard artifacts indicate convolutional head problems; degraded performance on recent data suggests fine-tuning insufficiency
- First 3 experiments:
  1. Compare baseline 3D local attention vs. CLA-only implementation on a small dataset to verify vertical interaction benefits
  2. Test convolutional head vs. strided deconvolution on synthetic data to confirm checkerboard artifact elimination
  3. Run ablation study with and without fine-tuning on recent data to quantify distribution shift impact

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- The CLA mechanism's effectiveness relies on assumptions about global vs. local vertical interactions that lack direct experimental validation
- Performance comparison against IFS HRES uses only a 1-day forecast horizon, potentially missing longer-term performance characteristics
- The explanation for distribution shift and fine-tuning benefits relies on qualitative reasoning without quantitative evidence of the temporal shift's magnitude

## Confidence
- **High confidence**: Computational efficiency claims and basic architectural improvements are well-supported by mathematical analysis
- **Medium confidence**: Forecasting performance claims are supported by experiments but could benefit from additional baselines and longer forecast horizons
- **Low confidence**: Explanation for distribution shift relies on qualitative reasoning about ERA5 data quality changes without quantitative evidence

## Next Checks
1. Conduct an ablation study comparing CLA with pure 2D attention and pure 3D local attention on the same architecture to isolate CLA's contribution to performance gains
2. Test the model across multiple forecast horizons (1-5 days) to verify consistent performance advantages over IFS HRES and other baselines
3. Perform sensitivity analysis on the CLA window size and depth parameters to determine optimal configurations for different atmospheric variables and pressure levels