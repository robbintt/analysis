---
ver: rpa2
title: Large Language Models Can Automatically Engineer Features for Few-Shot Tabular
  Learning
arxiv_id: '2404.09491'
source_url: https://arxiv.org/abs/2404.09491
tags: []
core_contribution: This paper introduces FeatLLM, a novel in-context learning framework
  that employs large language models (LLMs) as feature engineers to generate an input
  dataset optimally suited for tabular predictions. FeatLLM extracts rules from few-shot
  examples using LLMs, which are then parsed into binary features and used with a
  simple downstream model (e.g., linear regression) to infer class likelihoods.
---

# Large Language Models Can Automatically Engineer Features for Few-Shot Tabular Learning

## Quick Facts
- **arXiv ID**: 2404.09491
- **Source URL**: https://arxiv.org/abs/2404.09491
- **Reference count**: 40
- **Primary result**: FeatLLM achieves 10% average improvement over existing LLM-based approaches in few-shot tabular learning

## Executive Summary
This paper introduces FeatLLM, a novel framework that leverages large language models (LLMs) as feature engineers to automatically generate binary features from rules extracted in few-shot settings. The approach eliminates the need for LLM queries at inference time by converting natural language rules into executable code, which is then applied to create binary features for downstream prediction using a simple linear model. FeatLLM demonstrates significant performance improvements across 13 diverse tabular datasets, achieving an average 10% improvement in AUC metrics compared to existing LLM-based approaches.

## Method Summary
FeatLLM uses LLMs to extract rules from few-shot examples through structured prompts, then parses these rules into binary features that can be applied to training data without further LLM queries. The framework employs bagging to ensure diversity in rule generation across multiple iterations, with predictions combined through ensemble methods. A linear model with non-negative weights learns feature importance from the binary features, ensuring that generated features don't reduce class likelihood during training. The approach requires only API-level access to LLMs and handles datasets with varying numbers of features and samples across different domains.

## Key Results
- Achieves 10% average improvement in AUC across numerous tabular datasets from various domains
- Significantly outperforms existing LLM-based approaches for few-shot tabular learning
- Demonstrates effectiveness on datasets with varying numbers of features and samples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can extract meaningful rules from few-shot examples that outperform raw feature use
- Mechanism: FeatLLM uses structured prompts to guide LLMs to infer causal relationships between features and targets, then parse these into binary features
- Core assumption: LLMs possess sufficient prior knowledge to understand tabular data relationships when properly prompted
- Evidence anchors:
  - [abstract] "LLMs proficiently utilize both prior knowledge and the knowledge from the data, extracting high-quality features"
  - [section 3.1] "we guide the problem-solving process to mimic how an expert human might approach a tabular learning task"
  - [corpus] Weak - neighboring papers focus on feature engineering but don't directly validate rule extraction quality
- Break condition: If prompts fail to elicit meaningful rules or LLMs lack relevant prior knowledge for the domain

### Mechanism 2
- Claim: Ensemble with bagging improves robustness and handles large feature spaces
- Mechanism: Multiple iterations with feature/sample bagging create diverse rule sets, combined via ensembling to reduce variance
- Core assumption: Different prompt instantiations and data subsets will generate complementary rule sets
- Evidence anchors:
  - [section 3.2] "we adopt bagging to select a subset of features or instances for each trial" and "the ensemble method addresses the limitation of LLM's prompt size"
  - [section 4.2] "Our framework, employing both bagging and ensemble techniques, proved to be effective even on datasets with numerous features"
  - [corpus] Weak - neighboring papers mention ensemble methods but not specifically for LLM-generated features
- Break condition: If rule sets become too correlated across iterations or bagging fails to create diversity

### Mechanism 3
- Claim: Linear model with non-negative weights learns feature importance without negative interference
- Mechanism: Binary features from rules are weighted by positive coefficients learned via cross-entropy loss
- Core assumption: Simple linear combination of binary features can capture complex relationships when features are informative
- Evidence anchors:
  - [section 3.2] "we aim to learn this importance using a conventional machine learning (ML) model" with "projecting these weights into a positive subspace"
  - [section 4.2] "By projecting these weights into a positive subspace, we can ensure that the features generated by the LLM do not reduce the likelihood of a class during learning"
  - [corpus] Moderate - linear models with non-negative weights appear in related feature engineering work
- Break condition: If generated features are too sparse or uncorrelated with target classes

## Foundational Learning

- **Concept**: In-context learning
  - Why needed here: Enables LLMs to perform tasks without fine-tuning by providing examples in prompts
  - Quick check question: Can you explain how in-context learning differs from traditional fine-tuning approaches?

- **Concept**: Feature engineering
  - Why needed here: Transforms raw features into more informative representations for machine learning
  - Quick check question: What distinguishes rule-based features from traditional feature transformations?

- **Concept**: Ensemble methods
  - Why needed here: Combines multiple weak learners to create a stronger, more robust model
  - Quick check question: How does bagging differ from boosting in ensemble learning?

## Architecture Onboarding

- **Component map**: Prompt engineering -> LLM inference -> Rule parsing -> Feature transformation -> Linear model training -> Ensemble aggregation

- **Critical path**: 
  1. Generate prompts with task description, feature info, and examples
  2. Send to LLM and receive rule outputs
  3. Parse rules into executable code
  4. Apply rules to training data to create binary features
  5. Train linear model on binary features
  6. Ensemble predictions across multiple iterations

- **Design tradeoffs**:
  - Prompt complexity vs. LLM performance: More detailed prompts may improve rule quality but increase cost
  - Number of rules vs. overfitting: More rules provide more information but risk overfitting in low-shot settings
  - Ensemble size vs. computation: Larger ensembles improve robustness but increase inference time

- **Failure signatures**:
  - Poor performance despite high-quality prompts: May indicate LLM lacks relevant prior knowledge
  - High variance across ensemble members: Suggests prompts aren't generating diverse rule sets
  - Binary features show little correlation with targets: Indicates rule parsing or generation issues

- **First 3 experiments**:
  1. Run with minimal prompt (task description only) vs. full prompt to measure prompt contribution
  2. Test with different ensemble sizes (1, 5, 10, 20) to find optimal balance
  3. Compare performance with and without linear model training to validate feature importance learning

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the diversity of rules generated by FeatLLM impact the performance across different tabular datasets?
- **Basis in paper**: [explicit] The paper mentions using temperature, bagging, and instance shuffling to ensure diversity of features (rules) generated from the LLM in each iteration. It also provides a detailed analysis of the diversity of features in Appendix E.
- **Why unresolved**: While the paper discusses the strategies to ensure diversity and provides some analysis, it does not directly correlate the diversity of rules with performance across different datasets.
- **What evidence would resolve it**: A comprehensive analysis showing the correlation between the diversity of rules and the performance of FeatLLM on various datasets would provide insights into the impact of rule diversity on model performance.

### Open Question 2
- **Question**: What is the impact of using different LLMs as backbones for FeatLLM on its performance?
- **Basis in paper**: [explicit] The paper compares the performance of FeatLLM using GPT-3.5 and PaLM 2 Text-Unicorn as LLM backbones and presents the results in Appendix I.
- **Why unresolved**: Although the paper provides a comparison between two LLMs, it does not explore the full range of available LLMs or delve into the reasons behind the performance differences observed.
- **What evidence would resolve it**: Testing FeatLLM with a wider variety of LLMs and analyzing the performance differences in detail would help understand the impact of the choice of LLM on the framework's effectiveness.

### Open Question 3
- **Question**: How does FeatLLM handle the presence of missing data in tabular datasets?
- **Basis in paper**: [explicit] The paper discusses handling missing data through strategies like filling with zero, filling with 0.5, and imputation, and evaluates their performance in Appendix H.
- **Why unresolved**: The paper provides an analysis of different strategies for handling missing data but does not explore more advanced imputation techniques or the impact of missing data patterns on the model's performance.
- **What evidence would resolve it**: Implementing and evaluating more sophisticated imputation methods, as well as studying the effect of different missing data patterns on FeatLLM's performance, would provide a clearer understanding of how the framework deals with incomplete data.

## Limitations

- The exact prompt structure used for rule extraction is not provided, making it difficult to assess whether improvements stem from prompt design or overall framework architecture
- The methodology assumes LLMs possess relevant prior knowledge for diverse tabular domains, but this assumption is not empirically validated across all 13 datasets
- The evaluation relies heavily on AUC metrics without addressing computational efficiency or robustness to noisy data

## Confidence

- **High confidence**: The core architecture combining LLM-generated rules with linear models is technically sound and follows established ensemble learning principles
- **Medium confidence**: The mechanism by which LLMs extract meaningful rules from few-shot examples is plausible but not fully validated
- **Low confidence**: The generalizability of FeatLLM across diverse domains and scalability for larger, more complex datasets remain uncertain

## Next Checks

1. **Prompt Ablation Study**: Systematically vary the prompt structure and measure the impact on rule quality and downstream model performance to isolate the contribution of prompt design

2. **Cross-Domain Robustness**: Test FeatLLM on additional tabular datasets from domains not represented in current evaluation to assess generalizability

3. **Rule Quality Analysis**: Conduct detailed error analysis of generated rules, measuring parsing success rates and identifying common failure patterns