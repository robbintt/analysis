---
ver: rpa2
title: Universal randomised signatures for generative time series modelling
arxiv_id: '2406.10214'
source_url: https://arxiv.org/abs/2406.10214
tags:
- signature
- randomised
- data
- neural
- random
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel generative model for financial time
  series using randomised signatures, a computationally tractable alternative to path
  signatures. The authors propose a Wasserstein-type distance based on discrete-time
  randomised signatures to capture the distance between (conditional) distributions,
  justified by universal approximation results for randomised signatures.
---

# Universal randomised signatures for generative time series modelling

## Quick Facts
- arXiv ID: 2406.10214
- Source URL: https://arxiv.org/abs/2406.10214
- Reference count: 15
- Key outcome: Introduces a novel generative model for financial time series using randomised signatures as a computationally tractable alternative to path signatures

## Executive Summary
This paper proposes a novel approach to generative time series modeling using randomised signatures, which serve as a universal feature map for continuous functions on input paths. The authors develop a Wasserstein-type distance based on discrete-time randomised signatures to capture distributional differences, enabling non-adversarial training of a generator model based on a reservoir neural stochastic differential equation. The method is validated on both synthetic and real-world financial datasets, demonstrating superior performance compared to LSTM and Sig-W1 baselines.

## Method Summary
The proposed method uses randomised signatures as a feature map for continuous functions on paths, replacing traditional path signatures with a finite-dimensional approximation. A Wasserstein-type distance (RS-W1) is formulated using the terminal difference of randomised signatures, enabling non-adversarial training by avoiding the min-max optimization of GANs. The generator is implemented as a reservoir neural SDE with random feature neural networks as coefficient functions. The model is trained by minimizing the RS-W1 distance between real and generated data distributions, with extensions to conditional settings using C-RS-W1 for generating future trajectories given past information.

## Key Results
- Randomised signatures provide a universal approximation property for continuous functions on path spaces
- The RS-W1 metric enables non-adversarial training while maintaining discriminative power
- The conditional C-RS-W1 extension successfully generates future trajectories given past information
- Model outperforms LSTM and Sig-W1 baselines on both synthetic and real financial datasets including Brownian motion with drift, autoregressive processes, S&P 500 returns, and FOREX EUR/USD exchange rates

## Why This Works (Mechanism)

### Mechanism 1
Randomised signatures serve as a universal feature map for continuous functions on input paths. The randomised signature process inherits expressiveness from path signatures but is finite-dimensional, allowing approximation via linear readouts without truncation. Core assumption: Activation function σ is continuous, bounded, non-polynomial, injective, and satisfies σ(0)=0. Evidence anchors: Proposition 8 and 9 establish that linear readouts on the terminal difference ∆RST(x) can approximate continuous functions on the input path space. Break condition: If the activation function fails to be injective or non-polynomial, the approximation guarantee fails.

### Mechanism 2
The RS-W1 metric enables non-adversarial training by replacing the GAN discriminator with a Wasserstein-type distance computed on randomised signatures. RS-W1 is formulated as a linear functional on the expected randomised signature differences, avoiding the min-max optimization of traditional GANs. Core assumption: The randomised signature process is sufficiently expressive to distinguish distributions of interest. Evidence anchors: RS-W1(µ, ν) = ||E[∆RST(X)] - E[∆RST(X)]||₂, turning the problem into a convex minimization. Break condition: If the randomized signature fails to capture the relevant distributional differences, the discriminator loses discriminative power.

### Mechanism 3
The conditional C-RS-W1 metric enables generation of future trajectories given past information via a linear regression approximation of the conditional expectation. Uses Doob-Dynkin lemma to approximate the conditional expectation of future randomised signatures as a linear function of past randomised signatures, enabling supervised learning. Core assumption: There exists a measurable function l mapping past paths to the conditional expectation of future signature differences. Evidence anchors: Algorithm 1 and OLS formulation (38) show how to estimate the conditional expectation via linear regression on signature features. Break condition: If the linear approximation assumption fails, the conditional expectation estimation becomes inaccurate.

## Foundational Learning

- Path signatures and their universal approximation properties
  - Why needed here: The paper builds randomised signatures as a tractable alternative to path signatures, inheriting their universal approximation properties
  - Quick check question: What are the key mathematical properties that make path signatures universal feature maps for continuous functions on paths?

- Random feature neural networks and their universal approximation
  - Why needed here: The randomised signature construction is directly linked to random feature neural networks, and their approximation properties are used to prove the universality of randomised signatures
  - Quick check question: How do random feature neural networks differ from standard neural networks in terms of training and approximation capabilities?

- Wasserstein distances and their use in generative modeling
  - Why needed here: The paper introduces a Wasserstein-type metric based on randomised signatures as a discriminator for non-adversarial training
  - Quick check question: What are the advantages of using Wasserstein distances over other metrics (like MMD) in generative modeling?

## Architecture Onboarding

- Component map: Generator (Neural SDE with random feature neural networks) -> Reservoir (Randomised signature process) -> Readout (Linear function on signature differences) -> Discriminator (RS-W1/C-RS-W1 metric)

- Critical path:
  1. Sample random matrices/vectors for randomised signature (fixed during training)
  2. Compute randomised signature features for real and generated data
  3. Calculate RS-W1/C-RS-W1 distance as discriminator loss
  4. Update generator parameters via gradient descent
  5. Repeat until convergence

- Design tradeoffs:
  - Randomised signature dimension N vs. expressiveness: Higher N increases approximation power but computational cost
  - Choice of activation function: Must satisfy injectivity and non-polynomial conditions for universality
  - Sampling scheme complexity: Different schemes (4 vs 10) enable different approximation targets
  - Non-adversarial vs. adversarial training: Simpler optimization but potentially less expressive discriminator

- Failure signatures:
  - Discriminator loss plateaus early: Randomised signature may be insufficiently expressive
  - Generated data shows unrealistic temporal dependencies: Activation function or sampling scheme may be inappropriate
  - Training diverges: Learning rate may be too high or gradient estimates too noisy

- First 3 experiments:
  1. Implement randomised signature with Sampling Scheme 4 and verify universality for simple functions on synthetic paths
  2. Train unconditional generator on Brownian motion data and compare RS-W1 discriminator to Sig-W1 baseline
  3. Implement conditional generator and test on synthetic autoregressive processes with known conditional distributions

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of activation function in randomised signatures affect the universal approximation properties and the quality of generated financial time series? Basis in paper: The paper assumes activation functions satisfying specific properties (continuous, bounded, non-polynomial, injective, σ(0) = 0) and mentions Tanh and shifted Sigmoid as examples, but does not empirically compare different activation functions. Why unresolved: The theoretical framework allows for various activation functions, but the paper does not provide empirical evidence on how different choices impact the model's performance in practice. What evidence would resolve it: Comparative experiments using different activation functions (e.g., Tanh, Sigmoid, ReLU variants) on the same financial datasets, measuring both universal approximation accuracy and downstream generation quality metrics.

### Open Question 2
What is the theoretical relationship between the dimension N of randomised signatures and the time horizon T for optimal performance in generative time series modelling? Basis in paper: The paper mentions that "heuristically, Sampling Scheme 4 and 10 indicate that the relationship of N and T should be chosen linearly" but does not provide theoretical justification or empirical validation. Why unresolved: While the paper suggests a linear relationship, it does not derive this relationship theoretically or empirically validate it across different datasets and time horizons. What evidence would resolve it: Theoretical analysis deriving the optimal relationship between N and T, coupled with empirical studies varying both parameters across multiple financial datasets to determine the practical implications.

### Open Question 3
How do randomised signatures compare to other universal feature maps (e.g., neural ODEs, transformers) in terms of computational efficiency and approximation quality for financial time series generation? Basis in paper: The paper positions randomised signatures as computationally tractable alternatives to path signatures and demonstrates their effectiveness compared to LSTMs and Sig-W1, but does not compare against other modern architectures. Why unresolved: The paper establishes randomised signatures as effective but does not provide a comprehensive comparison with other state-of-the-art universal feature maps that have emerged in recent years. What evidence would resolve it: Benchmarking experiments comparing randomised signatures against neural ODEs, transformers, and other universal feature maps on the same financial datasets, measuring both computational efficiency (training time, memory usage) and generation quality metrics.

## Limitations

- The paper's main limitation lies in its assumption of weak stationarity for the financial time series data, which may not hold in real financial markets with regime changes and structural breaks
- While the randomized signature approach is theoretically elegant, its practical benefits over simpler methods for standard financial applications remain to be fully demonstrated
- The computational complexity of training reservoir neural SDEs with randomized signatures may limit scalability to very large datasets

## Confidence

**High confidence**: Theoretical foundations regarding randomized signatures as universal feature maps, supported by rigorous mathematical proofs

**Medium confidence**: Practical implementation and empirical results, as the paper provides comprehensive comparisons but doesn't explore edge cases or failure modes in depth

**Medium confidence**: Conditional modeling approach, as the linear approximation assumption for conditional expectations is theoretically justified but may not hold for complex financial dependencies

## Next Checks

1. Test the model's performance on non-stationary financial datasets with known structural breaks to validate the rolling window approach and identify failure modes

2. Compare computational efficiency and sample quality against simpler generative models (like standard GANs or VAEs) on the same financial datasets to establish practical advantages

3. Conduct ablation studies removing the reservoir structure or randomized signatures to quantify their individual contributions to model performance