---
ver: rpa2
title: A GAN-based data poisoning framework against anomaly detection in vertical
  federated learning
arxiv_id: '2401.08984'
source_url: https://arxiv.org/abs/2401.08984
tags: []
core_contribution: 'This paper proposes a GAN-based data poisoning framework for vertical
  federated learning (VFL) that can degrade the performance of the server-side model
  without access to it. The framework consists of two stages: 1) training a surrogate
  target model using semi-supervised learning, and 2) generating adversarial perturbations
  using a GAN-based method to degrade the surrogate target model''s performance.'
---

# A GAN-based data poisoning framework against anomaly detection in vertical federated learning

## Quick Facts
- arXiv ID: 2401.08984
- Source URL: https://arxiv.org/abs/2401.08984
- Reference count: 19
- Primary result: P-GAN framework outperforms state-of-the-art poisoning attacks in reducing target model F1 scores in vertical federated learning

## Executive Summary
This paper presents P-GAN, a GAN-based data poisoning framework designed to degrade the performance of server-side models in vertical federated learning (VFL) without requiring direct access to the target model. The framework operates in two stages: first training a surrogate model using semi-supervised learning, then generating adversarial perturbations through a GAN to maximize degradation of the surrogate's performance. The authors also propose a defense mechanism using deep auto-encoders to detect and mitigate poisoning attacks. Experimental results demonstrate that P-GAN effectively reduces F1 scores on standard image datasets while the DAE-based defense shows promise in mitigating such attacks, particularly on MNIST.

## Method Summary
The P-GAN framework attacks VFL systems by poisoning participant data to degrade the server-side model's performance. It first trains a surrogate target model using semi-supervised learning with available unlabeled data, creating a proxy for the actual target model. Then, a GAN-based method generates adversarial perturbations specifically designed to reduce the surrogate model's performance. These perturbations are applied to the participant's local data before federation. The attack is effective even without access to the actual target model architecture or parameters, making it particularly threatening in VFL settings where data remains decentralized.

## Key Results
- P-GAN outperforms existing poisoning attack methods in reducing F1 scores of target models on MNIST, CIFAR-10, and CIFAR-100
- The deep auto-encoder (DAE) based defense mechanism effectively mitigates poisoning attacks, especially on the MNIST dataset
- The framework successfully degrades model performance without requiring access to the actual target model architecture

## Why This Works (Mechanism)
The framework exploits the fundamental vulnerability of VFL systems where participants can manipulate their local data before federation. By training a surrogate model that approximates the target model's behavior on the poisoned data distribution, the GAN can generate perturbations specifically tailored to degrade performance on that distribution. The semi-supervised approach allows effective surrogate training even with limited labeled data, while the GAN's generative capability ensures perturbations remain realistic and effective against the defense mechanisms.

## Foundational Learning

**Vertical Federated Learning (VFL)**: Collaborative machine learning where different parties hold different feature sets for the same set of samples - needed because it defines the attack's operational context and constraints; quick check: understand data partitioning and aggregation in VFL.

**Data Poisoning Attacks**: Adversarial manipulation of training data to degrade model performance - needed as the fundamental attack paradigm; quick check: understand attack objectives and threat models.

**Semi-supervised Learning**: Learning from both labeled and unlabeled data - needed for surrogate model training when labeled data is scarce; quick check: understand how unlabeled data improves model generalization.

**Generative Adversarial Networks (GANs)**: Framework with generator and discriminator networks competing to generate realistic data - needed for creating effective adversarial perturbations; quick check: understand GAN training dynamics and convergence.

**Deep Auto-encoders (DAE)**: Neural networks trained to reconstruct inputs, used for anomaly detection - needed as the defense mechanism; quick check: understand reconstruction error as anomaly indicator.

## Architecture Onboarding

**Component Map**: Participant data -> Surrogate Model Training -> GAN Perturbation Generation -> Poisoned Data -> VFL Aggregation -> Server Model Degradation

**Critical Path**: Surrogate training → GAN optimization → perturbation application → model degradation

**Design Tradeoffs**: The framework balances attack effectiveness against perturbation realism - more aggressive perturbations may be more detectable by defenses, while subtle perturbations may be less effective.

**Failure Signatures**: Ineffective attacks show minimal F1 score reduction, failed surrogate training produces poor GAN guidance, and overly detectable perturbations trigger anomaly detection.

**First Experiments**: 1) Train surrogate model with varying amounts of labeled data to establish baseline performance; 2) Generate perturbations with different GAN training configurations to optimize attack effectiveness; 3) Test defense mechanism under varying attack intensities to measure robustness.

## Open Questions the Paper Calls Out

None

## Limitations
- Evaluation relies on benchmark image datasets that may not reflect real-world VFL applications with heterogeneous tabular or sequential data
- Semi-supervised surrogate training assumes availability of auxiliary unlabeled data from the same distribution, which may not be available in practical VFL settings
- Defense mechanism evaluated only against the proposed attack method without testing robustness against other poisoning strategies or adaptive attacks

## Confidence

**High confidence in**: Theoretical formulation of the P-GAN attack mechanism
**Medium confidence in**: Empirical effectiveness claims due to limited dataset diversity, defense evaluation scope

## Next Checks

1. Test the attack framework on heterogeneous tabular datasets typical of VFL applications (e.g., finance, healthcare)
2. Evaluate defense robustness against baseline poisoning attacks and adaptive adversaries
3. Measure computational overhead and communication costs during the GAN perturbation generation phase in realistic VFL topologies