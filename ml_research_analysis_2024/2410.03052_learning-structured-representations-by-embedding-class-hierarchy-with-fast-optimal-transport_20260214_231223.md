---
ver: rpa2
title: Learning Structured Representations by Embedding Class Hierarchy with Fast
  Optimal Transport
arxiv_id: '2410.03052'
source_url: https://arxiv.org/abs/2410.03052
tags:
- cpcc
- class
- coarse
- fine
- tree
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the limitation of \u21132-CPCC, which uses\
  \ class means and may misrepresent hierarchical relationships, especially for multi-modal\
  \ class distributions. To overcome this, the authors propose using Earth Mover's\
  \ Distance (EMD) within the CPCC framework, leading to the OT-CPCC family."
---

# Learning Structured Representations by Embedding Class Hierarchy with Fast Optimal Transport

## Quick Facts
- arXiv ID: 2410.03052
- Source URL: https://arxiv.org/abs/2410.03052
- Authors: Siqi Zeng; Sixian Du; Makoto Yamada; Han Zhao
- Reference count: 40
- Key outcome: FastFT-CPCC preserves more hierarchical information and outperforms ℓ2-CPCC in fine-level classification while maintaining coarse-level performance

## Executive Summary
This paper addresses the limitation of using class means in ℓ2-CPCC for hierarchical embedding, particularly for multi-modal class distributions. The authors propose using Earth Mover's Distance (EMD) within the CPCC framework, introducing the OT-CPCC family. They develop Fast FlowTree (FastFT), a linear-time approximation method, to improve computational efficiency while maintaining competitive performance. Experiments on 7 diverse datasets demonstrate that OT-CPCC methods, especially FastFT, preserve more hierarchical information as measured by TestCPCC scores and outperform ℓ2-CPCC in fine-level classification and retrieval tasks.

## Method Summary
The paper proposes OT-CPCC, a family of methods that uses optimal transport distances instead of ℓ2 distances to measure pairwise distances among classes in the feature space. The key innovation is FastFT, an approximation method that computes EMD in linear time by constructing an augmented label tree and using 1D greedy flow matching. The method operates within the CPCC framework, where the goal is to learn representations that preserve the hierarchical structure of class labels. During training, the model learns to minimize the difference between the tree metric of the label hierarchy and the learned distances between class distributions.

## Key Results
- FastFT achieves linear-time complexity (O((m+n)d)) while preserving hierarchical structure
- OT-CPCC methods consistently achieve higher TestCPCC scores than ℓ2-CPCC across all datasets
- FastFT shows superior performance in fine-level classification and retrieval tasks while maintaining coarse-level performance
- The method demonstrates robustness across varying batch sizes, regularization strengths, and backbone architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EMD-CPCC captures multi-modal class distributions better than ℓ2-CPCC
- Mechanism: EMD uses optimal transport to measure pairwise distances between all samples in two classes, weighted by flow, rather than just using class means
- Core assumption: Class conditional feature distributions are often multi-modal in fine-grained classification
- Evidence anchors:
  - [abstract] "class means may not be good representatives of the class conditional distributions, especially when they are multi-mode in nature"
  - [section 4.2] "We observe that this phenomenon occurs across most datasets for both ℓ2-CPCC and OT-CPCC training, suggesting that using optimal transport distances in CPCC computation is more appropriate"
- Break condition: If class distributions are truly unimodal and well-separated, ℓ2-CPCC would perform similarly

### Mechanism 2
- Claim: FastFT provides linear-time approximation of EMD while preserving hierarchical structure
- Mechanism: FastFT constructs an augmented label tree by extending leaf nodes with all samples, then uses 1D greedy flow matching which runs in O((m+n)d) time
- Core assumption: The tree metric between any two samples from different classes is identical in the augmented tree
- Evidence anchors:
  - [section 3.2] "We set all extended edges to have edge weight 1, and assign the instance weight ai for each data point"
  - [section 3.2] "Theorem 3.2 (Correctness of Fast FlowTree)... The Fast FlowTree can be computed in O((m + n)d)"
- Break condition: If the tree structure is not suitable for 1D greedy matching or if edge weights are heterogeneous

### Mechanism 3
- Claim: OT-CPCC preserves more hierarchical information as measured by TestCPCC
- Mechanism: OT-CPCC computes distances between full class distributions rather than centroids, leading to better alignment with tree metric
- Core assumption: Hierarchical information is better preserved when using full distribution distances
- Evidence anchors:
  - [abstract] "OT-CPCC methods, especially FastFT, preserve more hierarchical information, as measured by TestCPCC scores"
  - [section 4.4] "OT-CPCC methods achieves the best TestCPCC score... OT-CPCC preserves more hierarchical information, leading to better interpretability"
- Break condition: If the label hierarchy is flat or if the class distributions are too complex for any distance metric to capture

## Foundational Learning

- Concept: Optimal Transport (Earth Mover's Distance)
  - Why needed here: OT provides a principled way to measure distances between probability distributions, which is crucial for comparing class distributions
  - Quick check question: What is the computational complexity of exact EMD and why is approximation needed?

- Concept: Cophenetic Correlation Coefficient (CPCC)
  - Why needed here: CPCC measures how well the embedded distances preserve the hierarchical structure, serving as both objective and evaluation metric
  - Quick check question: How does CPCC differ from rank-based hierarchical metrics?

- Concept: Tree metrics and hierarchical embeddings
  - Why needed here: The label hierarchy provides the ground truth structure that the embedding should approximate
  - Quick check question: What is the difference between tree metric and Euclidean distance in terms of embedding capability?

## Architecture Onboarding

- Component map:
  - Feature encoder (fθ) -> Linear classifier (gw) -> CPCC regularizer -> FastFT algorithm
  - Flow weight vectors (a, b) control importance of individual samples

- Critical path:
  1. Forward pass through feature encoder
  2. Compute class-conditional feature distributions
  3. Calculate pairwise OT distances using FastFT
  4. Compute CPCC with tree metric
  5. Backpropagation through differentiable components

- Design tradeoffs:
  - Exact EMD vs FastFT: Accuracy vs computational efficiency
  - Uniform vs weighted flows: Simplicity vs incorporating prior knowledge
  - Tree depth: Expressiveness vs computational complexity

- Failure signatures:
  - Low TestCPCC scores: Poor hierarchical embedding
  - Training instability: Issues with OT computation or regularization strength
  - Suboptimal fine-level performance: Over-regularization of coarse structure

- First 3 experiments:
  1. Compare ℓ2-CPCC vs FastFT-CPCC on CIFAR10 with varying batch sizes
  2. Test effect of flow weight schemes (uniform vs distance-based) on TestCPCC
  3. Evaluate generalization on BREEDS with source-target splits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of flow weight distribution (uniform, distance-based, or inverse-distance) affect the learned hierarchical representations across different datasets and tasks?
- Basis in paper: [explicit] The paper investigates different flow weight schemes and observes their impact on CPCC values and downstream metrics.
- Why unresolved: While the paper shows that flow weights significantly influence CPCC values, it doesn't provide a comprehensive analysis across diverse datasets and tasks.
- What evidence would resolve it: A systematic study varying flow weight distributions across multiple datasets and tasks, analyzing both CPCC values and downstream performance metrics.

### Open Question 2
- Question: What is the theoretical relationship between the CPCC metric and the quality of hierarchical information embedded in the learned representations?
- Basis in paper: [inferred] The paper uses CPCC as a measure of hierarchical structure precision but doesn't provide theoretical justification for this relationship.
- Why unresolved: While empirical results demonstrate correlation, the paper doesn't explain why CPCC is suitable for measuring hierarchical information preservation.
- What evidence would resolve it: A theoretical analysis proving that maximizing CPCC leads to optimal hierarchical information embedding, or empirical evidence demonstrating correlation with other hierarchical evaluation metrics.

### Open Question 3
- Question: How does FastFT compare to other tree-based OT approximation methods in terms of accuracy and computational efficiency for large-scale datasets?
- Basis in paper: [explicit] The paper compares FastFT to other OT approximation methods on small synthetic datasets but doesn't evaluate scalability on large real-world datasets.
- Why unresolved: Performance on small synthetic datasets doesn't guarantee scalability to large-scale real-world datasets with complex hierarchical structures.
- What evidence would resolve it: Extensive experiments comparing FastFT to other tree-based OT approximation methods on large-scale datasets with varying characteristics.

## Limitations
- Relies on tree-structured label hierarchies which may not capture complex real-world relationships
- FastFT assumes homogeneous edge weights (all set to 1) which may not be appropriate for all hierarchical structures
- Experimental validation limited to seven datasets, with unclear scalability to very large label spaces

## Confidence
- High confidence: Computational efficiency of FastFT (O((m+n)d) complexity)
- Medium confidence: Preservation of hierarchical information as measured by TestCPCC
- Medium confidence: Practical utility for fine-grained classification tasks due to limited ablation studies

## Next Checks
1. **Edge Weight Sensitivity Analysis**: Evaluate FastFT performance with heterogeneous edge weights and compare against the uniform weight assumption to determine if the current approach is robust to tree structure variations.

2. **Scalability Benchmark**: Test the method on larger-scale hierarchical datasets (e.g., full iNaturalist with thousands of classes) to verify that the claimed linear-time complexity holds in practice and identify potential bottlenecks.

3. **Distribution Shape Dependency**: Conduct controlled experiments on synthetic datasets with varying class distribution shapes (unimodal, bimodal, multimodal) to isolate the effect of distribution shape on the performance gap between ℓ2-CPCC and OT-CPCC methods.