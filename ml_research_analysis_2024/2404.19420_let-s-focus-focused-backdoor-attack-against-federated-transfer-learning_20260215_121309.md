---
ver: rpa2
title: 'Let''s Focus: Focused Backdoor Attack against Federated Transfer Learning'
arxiv_id: '2404.19420'
source_url: https://arxiv.org/abs/2404.19420
tags:
- trigger
- learning
- attack
- federated
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the first backdoor attack against Federated
  Transfer Learning (FTL), named FB-FTL. The attack addresses the challenge of backdoor
  injection in FTL where the feature extractor is frozen and cannot be altered by
  malicious clients.
---

# Let's Focus: Focused Backdoor Attack against Federated Transfer Learning

## Quick Facts
- **arXiv ID:** 2404.19420
- **Source URL:** https://arxiv.org/abs/2404.19420
- **Reference count:** 40
- **One-line primary result:** FB-FTL achieves 80% attack success rate in Federated Transfer Learning while being robust against existing HFL defenses.

## Executive Summary
This paper introduces FB-FTL, the first backdoor attack designed specifically for Federated Transfer Learning (FTL) scenarios where the feature extractor is frozen and cannot be altered by malicious clients. The attack leverages GradCam to identify critical image regions used by the frozen feature extractor, then uses dataset distillation to embed target-class features into triggers positioned at these locations. Experiments across four datasets (CIFAR10, CINIC10, SVHN, GTSRB) demonstrate an average 80% attack success rate while maintaining model performance on clean data. The attack is shown to be robust against existing Horizontal and Vertical Federated Learning defenses.

## Method Summary
The FB-FTL attack operates by first using GradCam to identify the most important image regions for the frozen feature extractor's classification decisions. It then applies dataset distillation to encapsulate compressed target-class features into triggers positioned at these GradCam-identified locations. A perceptual similarity loss is employed to minimize trigger visibility while maintaining attack effectiveness. The attack specifically addresses the challenge that traditional backdoor methods fail in FTL because the frozen feature extractor cannot learn new trigger features.

## Key Results
- Achieves an average 80% attack success rate across CIFAR10, CINIC10, SVHN, and GTSRB datasets
- Outperforms static backdoor methods that don't use focused positioning or distillation
- Demonstrates robustness against existing HFL and VFL defense mechanisms
- Maintains clean data accuracy while achieving high attack success rates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The frozen feature extractor in FTL prevents traditional backdoor triggers from being effective because they introduce new features unknown to the pre-trained model.
- Mechanism: By using GradCam to identify the most important image regions for the frozen feature extractor, the attack can position a trigger that overrides these key features with distilled target-class information.
- Core assumption: The feature extractor only recognizes features it was pre-trained on, so new trigger features are discarded.
- Evidence anchors:
  - [abstract]: "State-of-the-art attack strategies assume the possibility of shifting the model attention toward relevant features introduced by a forged trigger... this is not feasible in FTL, as the learned features are fixed once the server performs the pre-training step."
  - [section 2.3]: "Because the feature extractor part of the classification model is learned in a different phase with respect to the federated learning one, crafting a backdoor implies a totally different approach than those typically adopted by existing backdoor attacks for FL."
  - [corpus]: Weak - no direct evidence found about frozen feature extractor preventing backdoor effectiveness.
- Break condition: If the feature extractor were not frozen, or if the pre-training included diverse enough features to recognize common trigger patterns.

### Mechanism 2
- Claim: Dataset distillation can encapsulate compressed target-class features into a trigger that the frozen feature extractor will recognize.
- Mechanism: The attack uses a distillation process where gradients from target-class images are matched to gradients from images containing the trigger, forcing the trigger to contain target-class features.
- Core assumption: The frozen feature extractor will respond to target-class features even when they appear in a trigger.
- Evidence anchors:
  - [section 3.3]: "We exploit a distillation strategy [36] to encapsulate inside the trigger the main information characterizing the features of the target class of the backdoor."
  - [section 4.2]: "Our approach, instead, we are substituting the original features with information from the target class distilled from the network itself."
  - [corpus]: Weak - no direct evidence found about dataset distillation for backdoor triggers.
- Break condition: If the distillation process fails to capture meaningful target-class features, or if the feature extractor cannot recognize these features when embedded in a trigger.

### Mechanism 3
- Claim: The focused positioning of triggers using GradCam heatmaps ensures the trigger overrides the most important features for classification.
- Mechanism: GradCam identifies high-attention regions that the feature extractor uses for classification, and the trigger is positioned to replace these features with target-class information.
- Core assumption: The GradCam heatmaps accurately identify the regions most critical for the feature extractor's classification decision.
- Evidence anchors:
  - [section 3.3]: "We leverage a powerful XAI tool, named GradCam [25], to find out the best way for an adversary to alter the images."
  - [section 4.5]: "The idea is to override the main features of an image, allowing its correct classification with a distilled trigger."
  - [corpus]: Weak - no direct evidence found about GradCam positioning effectiveness for backdoor attacks.
- Break condition: If GradCam fails to identify the correct important regions, or if the trigger cannot effectively override these features.

## Foundational Learning

- Concept: Federated Transfer Learning (FTL) - a federated learning scenario where clients have different feature and sample spaces, and a pre-trained feature extractor is frozen during collaborative training.
  - Why needed here: Understanding FTL is crucial because the attack exploits the frozen feature extractor and the transfer learning paradigm.
  - Quick check question: What makes FTL different from Horizontal or Vertical Federated Learning, and why does this difference enable the proposed attack?

- Concept: Explainable AI (XAI) and GradCam - techniques for identifying which parts of an input image are most important for a model's classification decision.
  - Why needed here: GradCam is used to identify the optimal positioning for triggers to override important features.
  - Quick check question: How does GradCam identify important regions in an image, and why is this useful for backdoor trigger positioning?

- Concept: Dataset distillation - a technique for compressing information from a dataset into a smaller representation, often synthetic data points.
  - Why needed here: Dataset distillation is used to encapsulate target-class features into the trigger.
  - Quick check question: What is the basic principle behind dataset distillation, and how can it be applied to create backdoor triggers?

## Architecture Onboarding

- Component map:
  Server -> Pre-trains feature extractor on public dataset -> Freezes feature extractor
  Clients -> Receive frozen feature extractor -> Train classification layers on private data
  Attacker -> Uses GradCam and dataset distillation -> Creates focused triggers
  Federated aggregation -> Combines local model updates

- Critical path:
  1. Server pre-trains feature extractor on public data
  2. Clients receive frozen feature extractor and train classification layers
  3. Attacker identifies important regions with GradCam
  4. Attacker distills target-class features into trigger using dataset distillation
  5. Attacker positions trigger in important regions
  6. Backdoored model achieves high attack success rate

- Design tradeoffs:
  - Trigger visibility vs. attack effectiveness: More visible triggers may be more effective but easier to detect
  - Region selection threshold: Higher thresholds reduce trigger size but may decrease effectiveness
  - Distillation iterations: More iterations may improve trigger quality but increase computational cost

- Failure signatures:
  - Low attack success rate despite correct implementation
  - Model accuracy drops significantly on clean data
  - GradCam heatmaps don't highlight meaningful regions
  - Distillation process fails to converge

- First 3 experiments:
  1. Test GradCam on frozen feature extractor to verify it identifies meaningful regions
  2. Implement basic dataset distillation to create target-class feature representations
  3. Combine GradCam positioning with dataset distillation to create initial focused backdoor triggers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the proposed attack perform against federated transfer learning scenarios where the feature extractor is not fully frozen but only partially frozen?
- Basis in paper: [inferred] The paper focuses on a scenario where the feature extractor is completely frozen, but mentions that "with minor tweaks, our approach could also work with different models."
- Why unresolved: The paper does not provide any experiments or analysis on scenarios where the feature extractor is partially frozen, which could be a more realistic setting in some applications.
- What evidence would resolve it: Experiments comparing the attack's performance on partially frozen feature extractors versus fully frozen ones, analyzing the impact of different degrees of freezing on attack success rates.

### Open Question 2
- Question: Can the proposed defense mechanisms (HFL and VFL defenses) be effectively adapted to specifically target the unique characteristics of the FB-FTL attack?
- Basis in paper: [explicit] The paper mentions that no ad-hoc defenses are available for the FTL scenario and that existing defenses from HFL and VFL were tested, with limited success.
- Why unresolved: The paper does not propose or test any defenses specifically designed to counter the FB-FTL attack, leaving open the question of whether existing defenses can be effectively adapted.
- What evidence would resolve it: Development and testing of new defense mechanisms specifically targeting the focusing and distillation aspects of the FB-FTL attack, comparing their effectiveness against existing HFL and VFL defenses.

### Open Question 3
- Question: How does the FB-FTL attack scale with increasingly complex datasets and larger model architectures?
- Basis in paper: [inferred] The paper tests the attack on four benchmark datasets and two additional model architectures (ConvNet and VGG16), but does not explore scaling to more complex datasets or deeper architectures.
- Why unresolved: The experiments are limited to relatively small-scale datasets and models, and it's unclear how the attack's performance would change with more complex data distributions or deeper neural networks.
- What evidence would resolve it: Extensive experiments on larger-scale datasets (e.g., ImageNet) and deeper architectures (e.g., ResNet-50, ResNet-101) to evaluate the attack's scalability and performance in more challenging scenarios.

## Limitations
- The paper does not validate whether GradCam heatmaps accurately identify the regions most critical for the frozen feature extractor's classification decisions
- Limited evaluation of defense mechanisms against the specific characteristics of the FB-FTL attack
- Experiments are conducted on relatively small-scale datasets and model architectures without exploring scalability

## Confidence

**Confidence: Low** for claims about GradCam positioning effectiveness. The paper assumes GradCam heatmaps accurately identify the most important regions for the frozen feature extractor, but provides no validation that these regions correspond to the features actually used for classification.

**Confidence: Medium** for dataset distillation claims. While the paper describes using distillation to encapsulate target-class features into triggers, the mechanism is not fully explained and lacks empirical validation specific to this application.

**Confidence: Medium** for defense robustness claims. The paper states the attack is robust against existing HFL defenses, but doesn't specify which defenses were tested or provide detailed results.

## Next Checks

1. **GradCam Verification**: Implement GradCam on the frozen feature extractor and systematically test whether the identified important regions actually correspond to features that influence classification decisions. Use ablation studies by occluding GradCam-identified regions and measuring accuracy changes.

2. **Distillation Generalization Test**: Create multiple triggers using the same distillation process but with different random initializations and positions. Measure whether attack success rates remain consistent across different trigger instantiations to verify the distillation process captures general target-class features rather than overfitting.

3. **Defense Evaluation Framework**: Implement and test against specific HFL defenses including: (a) anomaly detection on client updates, (b) robust aggregation methods like trimmed mean, and (c) input preprocessing defenses. Document which defenses are effective and at what thresholds, providing quantitative metrics for defense effectiveness.