---
ver: rpa2
title: Analyzing Adversarial Inputs in Deep Reinforcement Learning
arxiv_id: '2402.05284'
source_url: https://arxiv.org/abs/2402.05284
tags:
- adversarial
- inputs
- rate
- learning
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a comprehensive analysis of adversarial inputs
  in deep reinforcement learning (DRL) using formal verification techniques. The authors
  propose a novel metric, the Adversarial Rate, to quantify the susceptibility of
  DRL agents to adversarial perturbations.
---

# Analyzing Adversarial Inputs in Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2402.05284
- Source URL: https://arxiv.org/abs/2402.05284
- Authors: Davide Corsi; Guy Amir; Guy Katz; Alessandro Farinelli
- Reference count: 16
- Key outcome: Introduces Adversarial Rate metric and formal verification analysis of adversarial inputs in DRL agents, finding that larger networks and Swish activation are more susceptible

## Executive Summary
This paper presents a comprehensive analysis of adversarial inputs in deep reinforcement learning using formal verification techniques. The authors introduce the Adversarial Rate metric to quantify how susceptible DRL agents are to adversarial perturbations and employ the ProVe tool to identify and count these inputs. Through experiments on Jumping World and Robotic Mapless Navigation benchmarks, they demonstrate that even extensively tested models contain adversarial vulnerabilities that formal verification can detect but random sampling cannot.

The analysis reveals that adversarial inputs are concentrated in specific regions of the input space and that their locations can shift unpredictably during training. The authors also find that network architecture characteristics significantly impact vulnerability, with larger networks and certain activation functions like Swish being more susceptible. These findings highlight the importance of formal verification methods in assessing DRL safety and robustness against adversarial attacks.

## Method Summary
The paper introduces a formal verification-based approach to analyze adversarial inputs in DRL agents. The core methodology involves training DRL models using PPO and TD3 algorithms on two benchmark environments, then applying the ProVe verification tool to compute the Adversarial Rate metric. The authors also introduce Counting-ProVe for efficient approximation of adversarial input counts. The analysis examines how different network architectures, activation functions, and training parameters affect vulnerability to adversarial inputs, comparing formal verification results with empirical testing methods.

## Key Results
- Adversarial inputs can significantly impact DRL safety, even when models appear safe through extensive empirical testing
- Adversarial inputs are concentrated in specific regions of the input space, with locations shifting unpredictably during training
- Larger neural networks and certain activation functions (e.g., Swish) tend to be more susceptible to adversarial inputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Formal verification with ProVe can detect adversarial inputs that random sampling cannot.
- Mechanism: ProVe partitions the input domain into safe and unsafe regions using interval propagation and iterative splitting. It identifies counterexamples (adversarial inputs) by finding input intervals where the DNN output violates the safety property. Normalization of unsafe region size yields the Adversarial Rate.
- Core assumption: The DNN is Lipschitz-continuous and errors decrease with interval width.
- Evidence anchors:
  - [abstract] "They employ the ProVe tool for verification, counting, and enumeration of adversarial inputs, and introduce Counting-ProVe for efficient approximation of the Adversarial Rate."
  - [section] "In our analysis, the primary and foundational tool we employ is ProVe. This tool, when provided with a set of safety requirements, can effectively partition the input domain into two types of regions."
  - [corpus] Weak evidence for direct adversarial detection in DRL; corpus papers focus on attacks rather than detection.
- Break condition: When the DNN is too large for interval propagation to remain tractable, or when adversarial inputs are extremely sparse (<1% rate) so that approximate methods fail.

### Mechanism 2
- Claim: Adversarial inputs are concentrated in specific, small regions of the input space, making them hard to find by random sampling.
- Mechanism: The Adversarial Rate metric normalizes the unsafe region size, revealing concentration. Verification tools can locate these regions precisely, while random sampling only finds collisions if they happen to occur.
- Core assumption: Adversarial inputs are not uniformly distributed across the input space.
- Evidence anchors:
  - [abstract] "2) Adversarial inputs are concentrated in specific regions of the input space, and their locations can shift unpredictably during training."
  - [section] "Our results (summarized in Table 1) showcase the merits of employing verification-driven techniques to identify adversarial inputs. First, it can be observed that even after extensive testing, all models are susceptible, in varying levels, to adversarial inputs..."
  - [corpus] No direct evidence; this is an original claim supported by the paper's experiments.
- Break condition: If adversarial inputs are uniformly distributed, the Adversarial Rate would be high but random sampling would find them easily.

### Mechanism 3
- Claim: DNN architecture characteristics (size, activation functions) influence susceptibility to adversarial inputs.
- Mechanism: Larger networks and certain activation functions (e.g., Swish) provide more expressivity, which can create more opportunities for adversarial configurations. Smaller networks and simpler activations may be more constrained and less vulnerable.
- Core assumption: Network expressivity correlates with the number of possible adversarial configurations.
- Evidence anchors:
  - [abstract] "3) Larger neural networks and certain activation functions (e.g., Swish) tend to be more susceptible to adversarial inputs."
  - [section] "Our evaluation indicates that specific characteristics of a DNN architecture may increase the abundance of adversarial inputs, while other features do not have an apparent effect."
  - [corpus] No direct evidence; this is an original finding from the paper.
- Break condition: If adversarial inputs depend primarily on training data rather than architecture, then network characteristics would have little effect.

## Foundational Learning

- Concept: Deep Neural Networks (DNNs) and their formal verification
  - Why needed here: The paper relies on formal verification tools (ProVe, Marabou) to analyze adversarial inputs in DRL agents. Understanding DNN basics and verification is essential.
  - Quick check question: What is the difference between DNN verification and #DNN-verification?

- Concept: Deep Reinforcement Learning (DRL) and safety requirements
  - Why needed here: The paper analyzes adversarial inputs in DRL agents, so understanding how DRL works and how safety properties are defined is crucial.
  - Quick check question: How does the Jumping World environment encode safety requirements to guarantee collision avoidance?

- Concept: Adversarial examples and their properties
  - Why needed here: The paper focuses on characterizing adversarial inputs in DRL, so understanding what they are and their typical behaviors is fundamental.
  - Quick check question: Why are adversarial inputs hard to find through random sampling even when they exist?

## Architecture Onboarding

- Component map: DRL agent training (PPO/TD3) -> Safety property definition -> ProVe verification (interval propagation, iterative splitting) -> Adversarial Rate computation -> Counting-ProVe approximation -> Analysis of architecture effects

- Critical path:
  1. Train DRL agent on benchmark environment
  2. Define safety properties for the task
  3. Run ProVe to compute Adversarial Rate and identify unsafe regions
  4. Analyze spatial/temporal distribution of adversarial inputs
  5. Correlate findings with network architecture and training parameters

- Design tradeoffs:
  - Formal verification is sound but computationally expensive; approximation is faster but less precise
  - Small networks are less vulnerable but may have lower performance
  - Complex activation functions may improve performance but increase vulnerability

- Failure signatures:
  - ProVe returns "unknown" due to timeout or memory limits
  - Adversarial Rate is very low (<1%) but still non-zero, indicating sparse but existing vulnerabilities
  - High variance in Adversarial Rate across seeds/models, suggesting instability

- First 3 experiments:
  1. Train PPO agent on Jumping World, verify with ProVe, compute Adversarial Rate, and compare to random sampling collision rate
  2. Train agents with varying network sizes (2x32, 4x128, 4x256, 6x256) on Jumping World, analyze Adversarial Rate correlation
  3. Train agents with different activation functions (ReLU, Leaky ReLU, Swish, Tanh) on Jumping World, compare Adversarial Rates

## Open Questions the Paper Calls Out
None

## Limitations

- Analysis is limited to two specific environments (Jumping World and Robotic Mapless Navigation), raising questions about generalizability to other DRL tasks
- No comparison with existing adversarial detection methods in DRL literature to benchmark effectiveness
- The relationship between network architecture and vulnerability is correlational without establishing causal mechanisms

## Confidence

- **High Confidence**: Formal verification can detect adversarial inputs that random sampling misses (Mechanistic evidence from interval propagation)
- **Medium Confidence**: Adversarial inputs are concentrated in specific regions (Experimental evidence but limited to two benchmarks)
- **Medium Confidence**: Larger networks and Swish activation are more susceptible (Correlational findings but no causal mechanism established)

## Next Checks

1. **Benchmark Diversity Test**: Apply the same analysis framework to at least three additional DRL benchmarks (e.g., Atari games, robotic manipulation tasks) to assess generalizability of findings

2. **Architecture Ablation Study**: Systematically vary network depth, width, and activation functions across a wider range of combinations to establish clearer causal relationships between architecture and vulnerability

3. **Comparison with Black-box Methods**: Implement and compare standard adversarial attack detection methods (FGSM, PGD) against the formal verification approach to quantify relative effectiveness and computational costs