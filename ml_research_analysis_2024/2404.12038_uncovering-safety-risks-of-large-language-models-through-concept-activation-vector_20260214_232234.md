---
ver: rpa2
title: Uncovering Safety Risks of Large Language Models through Concept Activation
  Vector
arxiv_id: '2404.12038'
source_url: https://arxiv.org/abs/2404.12038
tags:
- money
- attack
- llms
- safety
- your
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces the Safety Concept Activation Vector (SCAV)
  framework to expose vulnerabilities in aligned large language models (LLMs). By
  extracting SCAVs from activation spaces using benign and harmful instruction datasets,
  the method enables effective adversarial attacks through computation flow perturbation.
---

# Uncovering Safety Risks of Large Language Models through Concept Activation Vector

## Quick Facts
- **arXiv ID:** 2404.12038
- **Source URL:** https://arxiv.org/abs/2404.12038
- **Reference count:** 40
- **Primary result:** SCAV framework achieves near 100% attack success rates on aligned LLMs by extracting safety concept activation vectors

## Executive Summary
This work introduces the Safety Concept Activation Vector (SCAV) framework to expose vulnerabilities in aligned large language models (LLMs). By extracting SCAVs from activation spaces using benign and harmful instruction datasets, the method enables effective adversarial attacks through computation flow perturbation. Evaluations show near 100% attack success rates on well-aligned open-source LLMs like LLaMA-2, with generated outputs confirmed harmful via GPT-4 rating and human evaluation. The SCAVs also demonstrate transferability across different models. These results indicate that current safety alignments are insufficient, highlighting significant risks in publicly released open-source LLMs.

## Method Summary
The SCAV framework extracts safety concept activation vectors by training binary classifiers on residual stream activations from harmful and benign instruction datasets. The method collects activation vectors from well-aligned LLMs during inference on paired instruction sets, then trains a linear classifier to distinguish between these activations. The decision boundary's normal vector becomes the SCAV, which is applied with negative coefficients during inference to perturb the model's computation flow and bypass safety mechanisms. The framework evaluates attack success through keyword matching, GPT-4-based rating, and human evaluation.

## Key Results
- Near 100% attack success rates on well-aligned open-source LLMs like LLaMA-2
- Generated outputs confirmed harmful through GPT-4 rating and human evaluation
- SCAVs demonstrate transferability across different models with same representation dimension

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SCAV extracts safety concept activation vectors by training a binary linear classifier on residual stream activations from harmful and benign instruction datasets.
- Mechanism: By feeding harmful and benign instructions to a well-aligned LLM, the method collects activation vectors at a chosen layer. A linear classifier learns to distinguish these activations, and its decision boundary normal vector becomes the SCAV. During inference, adding a negative coefficient of this vector to the residual stream effectively removes the safety mechanism.
- Core assumption: LLMs inherently encode safety concepts in their activation patterns, and these patterns are linearly separable between harmful and benign inputs.
- Evidence anchors:
  - [abstract] "...we extract safety concept activation vectors (SCA Vs) from activation spaces using benign and harmful instruction datasets..."
  - [section] "...We train a binary linear classifier to distinguish between the layer activation outputs of S+C and S-C. The decision boundary’s normal vectorv inherently signifies the direction of positive example activation. Hence, we designate v as the SCA V of layer k..."
  - [corpus] Weak: No direct evidence in neighbor papers, but the approach resembles representation bending techniques described in "Representation Bending for Large Language Model Safety."
- Break condition: If activation patterns for harmful and benign inputs are not linearly separable, the classifier cannot learn a meaningful SCAV.

### Mechanism 2
- Claim: Perturbing residual stream activations with the negative SCAV coefficient bypasses safety alignment mechanisms.
- Mechanism: During inference, the SCAV vector is subtracted (negative coefficient) from the residual stream activations at selected layers. This shifts the model's internal representation away from the safety-aligned direction, causing it to generate harmful content instead of refusing.
- Core assumption: Safety mechanisms in LLMs operate through activation patterns that can be mathematically inverted to remove safety constraints.
- Evidence anchors:
  - [section] "During model M's inference process, given a selected set of layers K, perturbations are applied at all token positions at each layer in K with different coefficients... A negative ϵ value signifies a diminution, thereby facilitating the potential attack degree of the model."
  - [corpus] Weak: No direct evidence in neighbor papers, but this resembles activation addition techniques described in "Activation addition: Steering language models without optimization."
- Break condition: If the safety mechanism relies on complex non-linear transformations or external guardrails not represented in residual activations, perturbation may not bypass them.

### Mechanism 3
- Claim: SCAVs show transferability across different LLMs with the same representation dimension.
- Mechanism: The SCAV extracted from one LLM (e.g., LLaMA-2) can be directly applied to perturb other models (e.g., Vicuna, Mistral) with the same activation dimension, achieving similar attack success rates.
- Core assumption: Different LLMs share similar underlying safety concept representations in their activation spaces, making SCAVs transferable.
- Evidence anchors:
  - [abstract] "Additionally, we find that our generated attack prompts may be transferable to GPT-4, and the embedding-level attacks may also be transferred to other white-box LLMs whose parameters are known."
  - [section] "We also observe that this method has a certain transferability. For example, the SCA Vs trained on LLaMA-2-7B-Chat [11] can directly take a certain effect on Mixtral-7B [12] and Vicuna-7B [13], etc."
  - [corpus] Weak: No direct evidence in neighbor papers, but this aligns with representation sharing concepts in "Representation Bending for Large Language Model Safety."
- Break condition: If different LLMs encode safety concepts in fundamentally different activation spaces or dimensions, transferability will fail.

## Foundational Learning

- Concept: Binary linear classification on activation vectors
  - Why needed here: The method relies on training a classifier to distinguish harmful vs. benign activations to extract the SCAV vector
  - Quick check question: If you have two sets of activation vectors from different classes, what supervised learning approach would you use to find the separating hyperplane?

- Concept: Residual stream manipulation in transformer models
  - Why needed here: The attack works by directly modifying the residual stream activations at specific layers during inference
  - Quick check question: In a transformer layer, where do residual connections occur and what is their purpose in the forward pass?

- Concept: Concept-based model explanation (CAVs)
  - Why needed here: SCAV extends CAVs from image models to language models for safety mechanism interpretation
  - Quick check question: How do CAVs in image models relate high-level concepts to neural network activations, and what's the equivalent in language models?

## Architecture Onboarding

- Component map: Input preprocessing → LLM forward pass (activation collection) → Binary classifier training (SCAV extraction) → Inference-time perturbation module (residual stream modification) → Output evaluation
- Critical path: Activation collection → SCAV extraction → Model perturbation → Attack execution
- Design tradeoffs: Linear classifier simplicity vs. potential non-linear safety mechanisms; direct residual manipulation vs. architectural constraints; transferability vs. model-specific optimization
- Failure signatures: Low classifier accuracy (activations not separable); minimal ASR improvement after perturbation (safety mechanisms not in residual stream); no transferability between models (different activation spaces)
- First 3 experiments:
  1. Collect activations from a small set of harmful/benign prompts on a base LLM, train classifier, visualize separability with t-SNE
  2. Apply extracted SCAV with negative coefficient to a single harmful prompt, compare outputs with and without perturbation
  3. Test transferability by applying SCAV from LLaMA-2 to Vicuna-7B on a held-out prompt set, measure ASR change

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can SCAV vectors extracted from one model type be effectively transferred to models with different architectures (e.g., from decoder-only LLMs to encoder-decoder models)?
- Basis in paper: [explicit] The paper demonstrates transferability across open-source LLMs with the same representation dimension but does not explore different architectures.
- Why unresolved: The experiments only tested transferability among similar decoder-only models with matching dimensions.
- What evidence would resolve it: Testing SCAV transferability on encoder-decoder models like BART or T5 would clarify if the approach generalizes beyond decoder-only architectures.

### Open Question 2
- Question: How does the SCAV attack method perform against frontier closed-source LLMs like GPT-4 or Claude, where the activation space is not directly accessible?
- Basis in paper: [explicit] The authors note SCAVs may be transferable to GPT-4 but only tested on open-source models with white-box access.
- Why unresolved: All experiments were conducted on open-source models where activation vectors could be directly extracted.
- What evidence would resolve it: Black-box attack experiments targeting GPT-4 or Claude would determine if SCAV principles apply without access to internal representations.

### Open Question 3
- Question: What is the minimum dataset size required to extract effective SCAVs, and how does performance degrade with smaller training sets?
- Basis in paper: [explicit] The authors mention that only "a few positive and negative samples" are needed but do not systematically evaluate the minimum requirements.
- Why unresolved: While the paper demonstrates effectiveness with their chosen dataset size, it does not explore the lower bounds of sample efficiency.
- What evidence would resolve it: Systematic experiments varying training set sizes from 1-100 pairs while measuring ASR would establish the minimum effective dataset size.

## Limitations
- Linear separability assumption may not hold across all safety mechanisms or model architectures
- Transferability shows inconsistent results depending on model family and training approaches
- Primarily demonstrated on open-source models, with limited validation on proprietary systems like GPT-4

## Confidence

**High confidence:** The core SCAV extraction methodology and its basic effectiveness on targeted models
**Medium confidence:** Transferability claims and effectiveness across diverse model families
**Low confidence:** Long-term robustness against evolving safety mechanisms and real-world deployment scenarios

## Next Checks
1. Test SCAV effectiveness on models with different architectural designs (e.g., Mamba, RWKV) to assess generalizability beyond transformer-based systems
2. Evaluate the framework's performance against adaptive safety mechanisms that actively detect and respond to activation pattern perturbations
3. Conduct longitudinal studies to measure how SCAV effectiveness changes as models receive safety updates and fine-tuning