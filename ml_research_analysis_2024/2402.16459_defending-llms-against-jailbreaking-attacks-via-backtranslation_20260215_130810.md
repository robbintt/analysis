---
ver: rpa2
title: Defending LLMs against Jailbreaking Attacks via Backtranslation
arxiv_id: '2402.16459'
source_url: https://arxiv.org/abs/2402.16459
tags: []
core_contribution: This paper proposes a new defense against jailbreaking attacks
  on large language models (LLMs) using backtranslation. The core idea is to infer
  a backtranslated prompt from the model's initial response, which reveals the harmful
  intent without adversarial manipulation.
---

# Defending LLMs against Jailbreaking Attacks via Backtranslation

## Quick Facts
- arXiv ID: 2402.16459
- Source URL: https://arxiv.org/abs/2402.16459
- Reference count: 14
- Primary result: Backtranslation defense improves attack success rate reduction from 61.4% to 28.6% against Advjam while maintaining generation quality

## Executive Summary
This paper proposes a novel defense mechanism against jailbreaking attacks on large language models using backtranslation. The core insight is that by inferring a backtranslated version of the model's initial response to a prompt, one can reveal the harmful intent without adversarial manipulation. The defense checks if the model refuses the backtranslated prompt, and if so, refuses the original prompt. Experiments show significant improvement in defense success rates compared to baselines while maintaining high generation quality on benign inputs, all without requiring additional model training.

## Method Summary
The defense mechanism works by intercepting the model's response to a potentially harmful prompt and applying backtranslation to reveal the underlying harmful intent. When the model is given a jailbreaking prompt, it initially generates a response that may contain harmful content. The defense then uses a backtranslation model to infer what prompt would have generated this response. If this backtranslated prompt is deemed harmful (by checking if the model refuses it), the defense refuses the original prompt as well. This approach is efficient, requiring only 0.02 seconds per prompt, and does not require any additional model training.

## Key Results
- Defense reduces attack success rate from 61.4% to 28.6% against Advjam attacks
- Maintains generation quality with minimal degradation on benign prompts
- Improves defense success rates across multiple attack types and model sizes
- No additional model training required for the defense mechanism

## Why This Works (Mechanism)
The defense exploits the fact that adversarial jailbreaking often relies on obfuscating harmful intent through prompt engineering. By backtranslating the model's initial response, the defense can reveal the underlying harmful request that the model was actually responding to, bypassing the obfuscation. This works because the model's response is generated based on the true semantic meaning of the prompt, not the surface-level text. The backtranslation acts as a semantic probe that can detect when the underlying request is harmful, even if the surface-level prompt appears benign.

## Foundational Learning
- **Jailbreaking attacks**: Techniques to bypass LLM safety filters by manipulating prompts - needed to understand the threat landscape
- **Backtranslation**: Translating text to another language and back to reveal semantic content - needed to understand the core defense mechanism
- **Semantic probing**: Using indirect methods to infer model behavior - needed to understand how the defense reveals hidden intent
- **Defense-aware attacks**: Attacks that adapt to known defense mechanisms - needed to understand the adversarial setting
- **Evaluation metrics**: Attack success rate, defense success rate, and generation quality - needed to assess defense effectiveness

## Architecture Onboarding

**Component Map:**
User Prompt -> LLM -> Backtranslation Model -> Defense Decision -> LLM Output

**Critical Path:**
User Prompt → LLM (initial response) → Backtranslation Model (inference) → Harmfulness check → Defense decision

**Design Tradeoffs:**
- Efficiency vs. accuracy: Faster backtranslation models may be less accurate
- False positives vs. false negatives: Stricter harmfulness thresholds reduce false negatives but increase false positives
- Model size vs. performance: Larger backtranslation models may be more accurate but slower

**Failure Signatures:**
- False negatives: Harmless prompts incorrectly flagged as harmful
- False positives: Harmful prompts incorrectly allowed
- Translation errors: Backtranslation fails to accurately capture semantic meaning

**3 First Experiments:**
1. Test defense against simple direct harmful prompts to establish baseline performance
2. Evaluate false positive rate on benign prompts from standard datasets
3. Measure latency impact on real-time user interactions

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the backtranslation threshold γ impact the balance between defense success rate and generation quality across different types of attacks and models?
- Basis in paper: [explicit] Section 5.5 discusses the impact of backtranslation threshold γ on generation quality, showing that a threshold of -2.0 or -1.0 achieves better quality than no filtering.
- Why unresolved: The paper only tests one threshold value (-2.0) for all main experiments and doesn't explore how optimal thresholds might vary across different attack types or target models.
- What evidence would resolve it: Empirical results showing defense success rates and generation quality across multiple threshold values for different combinations of attack types and target models.

### Open Question 2
- Question: How does the backtranslation model B's size and training affect the defense's computational efficiency and effectiveness?
- Basis in paper: [explicit] Section 5.4 shows that different backtranslation models (GPT-3.5-turbo, Llama-2-13B-Chat, Vicuna-13B) have little impact on defense success rates and generation quality, suggesting efficiency can be prioritized.
- Why unresolved: The paper only tests three specific models and doesn't explore the full trade-off space between model size, training requirements, and defense performance.
- What evidence would resolve it: Systematic comparison of defense performance using backtranslation models of varying sizes and training regimes, with analysis of computational costs.

### Open Question 3
- Question: How does the backtranslation defense perform against white-box attacks that can explicitly model and optimize against the defense mechanism?
- Basis in paper: [inferred] Section 6 mentions that the defense hasn't been tested against white-box attacks like GCG and AutoDAN in defense-aware settings, as these attacks rely on output probability which is nontrivial to define with backtranslation.
- Why unresolved: The paper only tests black-box attacks (PAIR) in defense-aware settings and doesn't explore how white-box attackers might adapt to the backtranslation mechanism.
- What evidence would resolve it: Experiments with white-box attacks specifically designed to optimize against the backtranslation defense, measuring both attack success rates and any necessary adaptations to the defense.

## Limitations
- Relies on backtranslation accurately revealing harmful intent, which may fail for highly obfuscated attacks
- Evaluated primarily on specific attack datasets and model architectures, limiting generalizability
- Computational efficiency claims not fully explored in production deployment contexts
- Potential for false positives on complex legitimate queries that backtranslation misinterprets

## Confidence
- **High Confidence**: The basic mechanism of using backtranslation to reveal harmful intent is technically sound and the experimental methodology is rigorous.
- **Medium Confidence**: The defense's effectiveness across diverse attack types and real-world scenarios, given the limited scope of evaluated attacks.
- **Medium Confidence**: The claimed computational efficiency and its practical implications in production environments.

## Next Checks
1. Test the defense against adaptive attacks specifically designed to evade backtranslation detection, including prompt variants that maintain harmful intent while breaking backtranslation reliability.
2. Evaluate the defense's performance on additional model architectures (beyond LLaMA and Vicuna) and different model sizes to assess generalizability.
3. Conduct a large-scale user study with benign prompts to measure false positive rates and assess impact on legitimate user interactions, particularly for complex technical or creative queries.