---
ver: rpa2
title: 'PC-LoRA: Low-Rank Adaptation for Progressive Model Compression with Knowledge
  Distillation'
arxiv_id: '2406.09117'
source_url: https://arxiv.org/abs/2406.09117
tags:
- pc-lora
- base
- pre-trained
- weights
- low-rank
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes PC-LoRA, a progressive compression method that
  uses low-rank adaptation to simultaneously perform model compression and fine-tuning.
  PC-LoRA gradually removes pre-trained weights during training, leaving only low-rank
  adapters at the end.
---

# PC-LoRA: Low-Rank Adaptation for Progressive Model Compression with Knowledge Distillation

## Quick Facts
- **arXiv ID:** 2406.09117
- **Source URL:** https://arxiv.org/abs/2406.09117
- **Reference count:** 40
- **Primary result:** Progressive compression method achieving 94.36% parameter and 89.1% FLOPs reduction for ViT-B with modest accuracy drops

## Executive Summary
PC-LoRA introduces a progressive model compression approach that integrates low-rank adaptation with knowledge distillation. The method gradually removes pre-trained weights during training while maintaining low-rank adapters, enabling simultaneous compression and fine-tuning. By leveraging knowledge distillation throughout the progressive removal process, PC-LoRA achieves substantial parameter and computational reductions while preserving model performance. The technique demonstrates effectiveness across both vision transformer and BERT architectures, offering a practical solution for deploying large models in resource-constrained environments.

## Method Summary
PC-LoRA operates by progressively removing pre-trained weights from the model during training, replacing them with low-rank adapters that capture the essential information. The process begins with the full pre-trained model and incrementally reduces the number of trainable parameters by replacing weight matrices with low-rank decompositions. Knowledge distillation is employed throughout training to ensure that the compressed model maintains the original model's predictive capabilities. This progressive approach allows the model to adapt gradually to the compressed representation, minimizing performance degradation while achieving significant compression ratios.

## Key Results
- Achieves 94.36% parameter reduction and 89.1% FLOPs reduction for ViT-B with only modest accuracy drops
- Demonstrates 93.42% parameter reduction and 84.2% FLOPs reduction for BERT
- Shows favorable trade-offs between compression and performance across vision and language tasks

## Why This Works (Mechanism)
PC-LoRA leverages the inherent redundancy in pre-trained models by progressively replacing full-rank weight matrices with low-rank approximations. The progressive removal strategy allows the model to adapt incrementally, preventing catastrophic forgetting that might occur with abrupt compression. Knowledge distillation ensures that the compressed model maintains the original model's predictive behavior by training the adapters to mimic the outputs of the full model throughout the compression process. This combination of gradual adaptation and distillation supervision enables the model to retain essential knowledge while significantly reducing parameter count and computational requirements.

## Foundational Learning

**Low-Rank Adaptation (LoRA)**
- Why needed: Enables parameter-efficient fine-tuning by decomposing weight updates into low-rank matrices
- Quick check: Verify that adapter matrices (A and B) have shape r × d and d × r where r << d

**Knowledge Distillation**
- Why needed: Transfers knowledge from the full model to the compressed model during progressive removal
- Quick check: Confirm distillation loss is computed between teacher (full model) and student (compressed model) outputs

**Progressive Training**
- Why needed: Gradual parameter removal prevents catastrophic forgetting during compression
- Quick check: Ensure removal schedule follows a predefined decay pattern across training epochs

## Architecture Onboarding

**Component Map**
Pre-trained model -> Progressive LoRA adapter insertion -> Knowledge distillation supervision -> Final compressed model

**Critical Path**
Full model initialization → LoRA adapter insertion → Progressive weight removal with distillation → Evaluation of compressed model

**Design Tradeoffs**
The progressive approach trades training time for better final accuracy compared to static compression methods. The choice of removal schedule affects both compression ratio and performance retention, requiring careful tuning based on application requirements.

**Failure Signatures**
- Rapid accuracy degradation indicates overly aggressive removal schedule
- Minimal compression suggests insufficient adapter capacity or conservative removal
- Training instability may result from poor initialization of adapter parameters

**First Experiments**
1. Baseline comparison: Train with no progressive removal to establish upper bound performance
2. Schedule sensitivity: Test different removal rates (linear vs exponential decay) on a validation set
3. Adapter capacity: Vary the rank r of LoRA adapters to find optimal compression-performance trade-off

## Open Questions the Paper Calls Out
None

## Limitations

**Generalization Concerns**
The method's effectiveness on specialized domains like medical imaging or scientific text remains unexplored, as experiments focus on standard vision and language benchmarks.

**Catastrophic Forgetting Risk**
Progressive weight removal introduces fundamental risks of catastrophic forgetting that are not fully characterized across different application domains.

**Computational Overhead**
The progressive training schedule's computational overhead versus static compression methods is not thoroughly evaluated for resource-constrained deployment scenarios.

## Confidence

**Compression Ratio Claims:** High
**Performance Trade-off Claims:** Medium
**Broad Applicability Claims:** Low

## Next Checks

1. Conduct ablation studies systematically varying the progressive removal schedule parameters to identify optimal trade-offs between compression ratio and accuracy preservation across multiple model architectures.

2. Perform comprehensive benchmarking against state-of-the-art static compression methods (pruning, quantization) and other adapter-based approaches on identical tasks to establish relative performance positioning.

3. Evaluate the method's robustness to domain shift by testing on out-of-distribution data and measuring catastrophic forgetting across the progressive training phases.