---
ver: rpa2
title: 'PEaCE: A Chemistry-Oriented Dataset for Optical Character Recognition on Scientific
  Documents'
arxiv_id: '2403.15724'
source_url: https://arxiv.org/abs/2403.15724
tags:
- text
- patch
- records
- real-world
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PEaCE addresses the lack of OCR datasets containing both scientific
  (e.g., chemical equations) and generic printed English text. The dataset comprises
  synthetic and real-world records from chemistry publications.
---

# PEaCE: A Chemistry-Oriented Dataset for Optical Character Recognition on Scientific Documents

## Quick Facts
- arXiv ID: 2403.15724
- Source URL: https://arxiv.org/abs/2403.15724
- Authors: Nan Zhang; Connor Heaton; Sean Timothy Okonsky; Prasenjit Mitra; Hilal Ezgi Toraman
- Reference count: 0
- Primary result: PEaCE dataset improves OCR performance on chemistry publications with BLEU scores up to 99.53% on synthetic tests and 81.24% on real-world records

## Executive Summary
This paper introduces PEaCE, a dataset designed to address the lack of OCR datasets containing both scientific (chemical equations) and generic printed English text from chemistry publications. The dataset includes 1.2 million synthetic records and 319 real-world images. Experiments with transformer-based OCR models (OCR-ViT and Pix2Tex) demonstrate that smaller patch sizes, multi-domain training (combining PEaCE and im2latex-100k), and proposed image transformations (pixelation, bolding, whitespace padding) significantly improve performance, with models trained on PEaCE outperforming single-domain models.

## Method Summary
The PEaCE dataset combines synthetic records (1M printed English, 100k numerical artifacts, 100k pseudo-chemical equations) with real-world images from chemistry publications. The authors train transformer-based OCR models (OCR-ViT and Pix2Tex) on this dataset and evaluate performance using BLEU-4 score, edit distance, and exact match metrics. Key methodological innovations include exploring the impact of patch size in ViT models, multi-domain training with im2latex-100k, and three proposed image transformations to simulate real-world artifacts.

## Key Results
- Models trained on PEaCE dataset achieve BLEU scores up to 99.53% on synthetic tests
- Real-world chemistry records show improved performance (81.24% BLEU) when trained on PEaCE
- Multi-domain training yields average improvements of 8.23% on PEaCE real-world test set
- Proposed image transformations improve performance by 3.61% on im2latex and 102.25% on PEaCE real-world

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Smaller patch sizes in ViT models improve OCR performance by lowering the burden on each patch embedding to capture complex text structures.
- Mechanism: Smaller patches cover fewer pixels and less text content, allowing more embedding dimensions to describe specific characters or symbols within each patch. This reduces information density per patch, making it easier to differentiate between similar characters.
- Core assumption: The trade-off between patch size and effective sequence length is acceptable because improved character differentiation outweighs increased computational cost.
- Evidence anchors: [section] "When using a20 ∗ 20 patch, then, the textaij must be described usingDM numbers. For a8 ∗ 8 patch, however, 6 × DM numbers can be used to describe the same text - in some sense, lowering the burden placed on the model."
- Break condition: If computational overhead becomes prohibitive or text images become extremely large, benefits may diminish.

### Mechanism 2
- Claim: Multi-domain training improves model performance by exposing the model to both scientific text and printed English.
- Mechanism: Training on datasets containing both scientific text (chemical equations) and printed English allows the model to handle the hybrid nature of the target domain, preventing inconsistencies in label formatting and enabling learning of temporal dependencies between different text types.
- Core assumption: The target domain contains documents with both scientific text and printed English, and the model benefits from seeing these together during training.
- Evidence anchors: [abstract] "Extracting text from chemistry publications requires an OCR model that is capable in both realms." [section] "We see that multi-domain training yields an average improvement of 825.39% on im2latex (primarily in EM) and 8.23% onPEaCE real-world."
- Break condition: If the target domain becomes predominantly one text type, multi-domain training benefits may decrease.

### Mechanism 3
- Claim: Proposed image transformations improve performance by simulating real-world artifacts present in scientific documents.
- Mechanism: Pixelation, bolding, and whitespace padding introduce noise and variations that mimic imperfections in real-world scientific documents (low-quality scans, certain fonts, uneven whitespace from table extraction), helping the model generalize better to unseen data.
- Core assumption: Real-world test set contains artifacts not present in synthetic training data, and the model benefits from exposure to these artifacts during training.
- Evidence anchors: [section] "To address this, we propose three transformations - pixelation, bolding, and whitespace padding - to mimic these artifacts." [section] "The transformations yield an average improvement of 3.61% and 102.25% on im2latex test set andPEaCE real-world test set, respectively, in BLEU, edit distance, and exact match."
- Break condition: If real-world artifacts differ significantly from simulated transformations or are not relevant to the target domain, benefits may be limited.

## Foundational Learning

- Concept: Vision Transformer (ViT) architecture
  - Why needed here: Understanding how ViT processes image patches and uses self-attention is crucial for grasping why patch size and sequence length matter in this OCR application.
  - Quick check question: How does the ViT handle the sequence of patch embeddings, and what role does the internal dimension (DM) play in this process?

- Concept: Multi-domain training
  - Why needed here: Recognizing the benefits and challenges of training a model on multiple related but distinct datasets is essential for understanding why combining im2latex-100k and PEaCE yields better performance.
  - Quick check question: What are the potential pitfalls of multi-domain training, and how can they be mitigated in this specific OCR application?

- Concept: Image transformations for data augmentation
  - Why needed here: Understanding how to simulate real-world artifacts through image transformations is key to appreciating why pixelation, bolding, and whitespace padding improve model generalization.
  - Quick check question: How do these specific transformations (pixelation, bolding, whitespace padding) introduce noise that mimics real-world imperfections, and why are they effective for this OCR task?

## Architecture Onboarding

- Component map: Image preprocessing -> Multi-Type-TD-TSR table segmentation -> Data augmentation (pixelation, bolding, whitespace padding) -> ViT or Pix2Tex model training -> Inference

- Critical path: 1) Convert PDF pages to images and segment tables into cells using Multi-Type-TD-TSR, 2) Apply pixelation, bolding, and whitespace padding transformations to training data, 3) Train ViT or Pix2Tex model on augmented dataset (im2latex-100k + PEaCE), 4) Process new images through trained model to extract text

- Design tradeoffs:
  - Patch size vs. sequence length: Smaller patches improve character differentiation but increase computational cost
  - Synthetic vs. real-world data: Synthetic data is abundant and clean, but real-world data is necessary to evaluate true performance
  - Multi-domain vs. single-domain training: Multi-domain training improves performance but may introduce inconsistencies in label formatting

- Failure signatures:
  - Poor performance on real-world test set despite good synthetic performance: Indicates overfitting to synthetic data and poor generalization to real-world artifacts
  - High edit distance or low exact match: Suggests the model struggles to accurately transcribe text, possibly due to patch size issues or insufficient exposure to complex text structures

- First 3 experiments:
  1. Compare OCR-ViT performance with patch sizes 10x10 and 16x16 on PEaCE synthetic test set to quantify patch size impact on character differentiation
  2. Train OCR-ViT on im2latex-100k alone, PEaCE alone, and their combination to evaluate multi-domain training benefits
  3. Apply proposed image transformations to training data and measure impact on PEaCE real-world test set performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does OCR performance vary when training on synthetic records that closely mimic real-world artifacts versus those without such transformations?
- Basis in paper: [explicit] The paper proposes three image transformations (pixelation, bolding, whitespace padding) to mimic real-world artifacts and observes their impact on model performance.
- Why unresolved: The paper only compares performance with and without these transformations but doesn't analyze the effect of varying their degree or combination.
- What evidence would resolve it: Systematic experiments varying intensity, frequency, and combination of each transformation on synthetic records, evaluating impact on real-world test performance.

### Open Question 2
- Question: What is the optimal patch size for OCR models when processing scientific documents with mixed text types?
- Basis in paper: [explicit] The paper finds that smaller patch sizes (10×10) improve performance on im2latex and PEaCE real-world test sets compared to larger sizes (16×16).
- Why unresolved: Experiments only test two patch sizes, and optimal size may depend on specific document characteristics like font size, spacing, and symbol complexity.
- What evidence would resolve it: Broader grid search over patch sizes (8×8, 12×12, 16×16) and their impact on performance across different document types and domains.

### Open Question 3
- Question: How does multi-domain training on PEaCE and im2latex-100k compare to training on domain-specific datasets in terms of generalization to unseen scientific domains?
- Basis in paper: [explicit] The paper observes that multi-domain training improves performance on both PEaCE and im2latex-100k test sets compared to single-domain training.
- Why unresolved: Experiments don't evaluate how well models generalize to other scientific domains (physics, mathematics) beyond chemistry and LaTeX equations.
- What evidence would resolve it: Evaluating trained models on datasets from other scientific domains (arXiv physics papers, math-heavy documents) to measure cross-domain generalization.

## Limitations

- Synthetic data may not fully capture real-world diversity, as the real-world test set (319 images) is much smaller than the synthetic training data (1.2M records)
- Multi-domain training benefits may be specific to the chemistry domain and may not extend to other scientific fields
- Computational costs of smaller patch sizes are not thoroughly analyzed, potentially limiting practical deployment in resource-constrained environments

## Confidence

**High Confidence**: The core findings about patch size effects and multi-domain training benefits are well-supported by experimental results with substantial and consistent improvements across metrics.

**Medium Confidence**: The proposed image transformations show promising results, particularly on the real-world test set, but evaluation could benefit from more diverse real-world examples to confirm generalizability.

**Low Confidence**: Long-term stability and generalization to entirely unseen chemistry publications remains uncertain due to the limited size of the real-world test set.

## Next Checks

1. Expand real-world testing by collecting a larger and more diverse set of real-world chemistry publication images (minimum 1000 additional samples) to better validate model generalization capabilities beyond the current 319-image test set.

2. Test trained models on scientific documents from other domains (physics, biology) to determine whether multi-domain training benefits extend beyond chemistry-specific content.

3. Conduct detailed benchmarking of training and inference times across different patch sizes and model configurations to quantify practical trade-offs between performance gains and computational costs.