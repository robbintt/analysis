---
ver: rpa2
title: 'NSINA: A News Corpus for Sinhala'
arxiv_id: '2403.16571'
source_url: https://arxiv.org/abs/2403.16571
tags:
- news
- sinhala
- language
- task
- corpus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces NSina, a comprehensive news corpus of over
  500,000 articles from popular Sinhala news websites, along with three NLP tasks:
  news media identification, news category prediction, and news headline generation.
  The authors address the challenge of low-resource languages like Sinhala, which
  lack substantial training data and limited benchmarking datasets for adapting large
  language models.'
---

# NSINA: A News Corpus for Sinhala

## Quick Facts
- arXiv ID: 2403.16571
- Source URL: https://arxiv.org/abs/2403.16571
- Reference count: 0
- Introduces NSina, a news corpus with over 500,000 Sinhala articles from ten Sri Lankan news sources, along with three NLP tasks

## Executive Summary
This paper addresses the challenge of low-resource languages by introducing NSina, a comprehensive news corpus of over 500,000 Sinhala articles from popular news websites. The authors create three NLP tasks—news media identification, news category prediction, and news headline generation—to evaluate transformer models on Sinhala language processing. The corpus and associated benchmarks are released publicly to advance research in Sinhala NLP, with the goal of improving adaptation of large language models to low-resource languages.

## Method Summary
The authors compiled a large news corpus by scraping articles from ten popular Sinhala news sources, then cleaned and processed the data to create three distinct NLP tasks. They evaluated multiple transformer models (XLM-R, SinBERT, mBART, mT5) on each task using standard training procedures with Adam optimizer, learning rates between 2e-5 and 1e-4, and early stopping. The models were fine-tuned on task-specific datasets derived from NSina, with evaluation using appropriate metrics for classification (F1 scores) and generation (BLEU and TER scores).

## Key Results
- NSina contains over 500,000 cleaned Sinhala news articles, making it the largest available Sinhala news corpus
- Transformer models achieve high performance on news media identification (above 0.88 Macro F1) and news category prediction (above 0.94 F1)
- XLM-R models perform comparably to or better than SinBERT on some tasks, suggesting multilingual models can effectively handle Sinhala

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training LLMs on large, high-quality monolingual corpora enables better performance on downstream NLP tasks in low-resource languages.
- Mechanism: NSina provides over 500,000 cleaned Sinhala news articles, substantially larger than previous corpora, allowing transformer models to learn richer language representations.
- Core assumption: Larger, cleaner corpora directly translate to improved model performance across diverse NLP tasks.
- Evidence anchors:
  - [abstract] "The introduction of large language models (LLMs) has advanced natural language processing (NLP), but their effectiveness is largely dependent on pre-training resources."
  - [section 2.2] "NSina is the most updated and the largest Sinhala news corpus available."
- Break condition: If the corpus contains excessive noise or domain-specific bias that limits generalization to other text types.

### Mechanism 2
- Claim: Transformer models (especially multilingual ones like XLM-R) can effectively handle low-resource languages when sufficient task-specific data is available.
- Mechanism: XLM-R models achieve near or better performance than Sinhala-specific models (SinBERT) on classification tasks, suggesting multilingual models can leverage cross-lingual knowledge.
- Core assumption: Multilingual transformers have learned transferable representations that benefit low-resource languages.
- Evidence anchors:
  - [section 3.1.3] "Notably, the SinBERT Large model outperformed the XLM-R Large model. However, the XLM-R Large model produces very close results to the SinBERT Large model."
  - [section 3.2.3] "Notably, XLM-R models could outperform SinBERT model in this task."
- Break condition: If the multilingual model's vocabulary coverage is insufficient for the target language's unique features.

### Mechanism 3
- Claim: Creating task-specific benchmarks from domain corpora enables meaningful evaluation of LLM capabilities in low-resource settings.
- Mechanism: The paper creates three distinct tasks with separate train/test splits, providing standardized evaluation frameworks.
- Core assumption: Task-specific datasets derived from the same corpus maintain internal consistency while testing different capabilities.
- Evidence anchors:
  - [section 3] "We compiled the following three tasks from NSina... We believe that creating benchmarks would help to evaluate the LLMs in Sinhala NLP tasks."
  - [section 3.1.1] "As all the news instances in NSina contained its news source, constructing the train/ test set for this task was straightforward."
- Break condition: If the task definitions are too narrow or domain-specific to generalize to broader language understanding.

## Foundational Learning

- Concept: Data preprocessing and cleaning for low-resource languages
  - Why needed here: The paper emphasizes cleaning news articles with fewer than ten Sinhala words and creating balanced datasets across sources/categories
  - Quick check question: What preprocessing steps would you take to handle inconsistent text encoding or missing metadata in a low-resource language corpus?

- Concept: Transformer architecture and fine-tuning strategies
  - Why needed here: The paper experiments with multiple transformer variants (XLM-R, SinBERT, mBART, mT5) and specific training configurations
  - Quick check question: How would you modify the learning rate schedule for fine-tuning a large multilingual model on a small target-language dataset?

- Concept: Evaluation metrics for classification vs generation tasks
  - Why needed here: The paper uses F1 scores for classification tasks and BLEU/TER for generation, reflecting different evaluation needs
  - Quick check question: When would you prefer macro F1 over weighted F1 for evaluating a multi-class classification model in a low-resource language?

## Architecture Onboarding

- Component map: Web scraping -> Cleaning -> Splitting -> Task-specific sampling -> Model training -> Evaluation -> Public release
- Critical path: 1. Corpus construction and cleaning, 2. Task definition and dataset creation, 3. Model selection and fine-tuning, 4. Evaluation and benchmarking, 5. Public release and documentation
- Design tradeoffs:
  - Model size vs performance: Larger models (XLM-R Large, mT5 Large) generally perform better but require more resources
  - Language-specific vs multilingual: SinBERT shows better results on some tasks but XLM-R offers broader applicability
  - Task complexity vs data availability: Simple classification tasks work well with limited data, while generation tasks require more sophisticated approaches
- Failure signatures:
  - Poor classification performance: Indicates insufficient language representation or class imbalance
  - Low BLEU scores in generation: Suggests vocabulary limitations or lack of generation-specific pre-training
  - Inconsistent cross-validation results: May indicate data leakage or insufficient train/test separation
- First 3 experiments:
  1. Fine-tune XLM-R Base on the news media identification task and compare with SinBERT Small to establish baseline performance
  2. Train a simple classification model (e.g., logistic regression) on TF-IDF features to understand the difficulty of each task
  3. Generate headlines using mBART and evaluate with human judges to establish a reference point beyond BLEU/TER metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal architecture for Sinhala-specific transformer models that can outperform multilingual models like XLM-R on various NLP tasks?
- Basis in paper: [explicit] The paper states that multilingual transformer models such as XLM-R provide very close results or sometimes even outperform language-specific models such as SinBERT, suggesting more research should be done to train Sinhala-specific transformer models.
- Why unresolved: The paper does not explore or propose specific architectural improvements for Sinhala-specific transformers. It only identifies the need for further research in this area.
- What evidence would resolve it: Training and evaluating Sinhala-specific transformer models with different architectures (e.g., larger models, different pre-training strategies, or domain-specific fine-tuning) on the NSina benchmark tasks and comparing their performance against multilingual models like XLM-R.

### Open Question 2
- Question: How can we develop advanced NLG metrics for Sinhala that are more suitable than BLEU and TER for evaluating text generation tasks?
- Basis in paper: [explicit] The paper mentions that all the experimented models perform poorly on the proposed NLG task and suggests that more language generation models should be explored for Sinhala. It also states that BLEU and TER scores cannot properly evaluate Sinhala text generation and there is a need for advanced NLG metrics for Sinhala.
- Why unresolved: The paper does not propose or explore any alternative NLG metrics specifically designed for Sinhala. It only highlights the limitations of current metrics.
- What evidence would resolve it: Developing and evaluating Sinhala-specific NLG metrics that take into account the unique linguistic features of the language, such as agglutination, and comparing their performance against traditional metrics like BLEU and TER on the NSina headline generation task.

### Open Question 3
- Question: What are the key factors contributing to the distinct stylistic approach of each news source in presenting their news content, as evidenced by the high performance of transformer models in news media identification?
- Basis in paper: [explicit] The paper shows that transformer models achieved high performance (above 0.88 Macro F1 score) in the news media identification task, implying that each news source exhibits a distinct stylistic approach in presenting their news content.
- Why unresolved: The paper does not analyze or identify the specific linguistic or stylistic features that contribute to the distinctiveness of each news source.
- What evidence would resolve it: Conducting a detailed linguistic analysis of the news articles from each source, identifying patterns in vocabulary usage, sentence structure, tone, and other stylistic features that may contribute to the high performance of the news media identification task.

## Limitations

- Headline generation performance is notably poor, with BLEU scores ranging from 0.1 to 0.17 and TER scores around 0.72-0.80, suggesting the task may be too challenging for current approaches
- The paper does not provide a comprehensive survey of all existing Sinhala corpora to definitively establish NSina as the largest
- Results are based on news domain data, which may not generalize to other types of Sinhala text

## Confidence

- High confidence: Corpus construction methodology and basic classification task results
- Medium confidence: Claims about NSina being the largest Sinhala news corpus, and general performance trends across tasks
- Low confidence: Headline generation results and their practical utility, as well as claims about transformer models' effectiveness for low-resource languages based solely on these experiments

## Next Checks

1. Cross-domain evaluation: Test the trained models on news articles from sources not included in NSina to assess generalization beyond the specific news websites used for corpus construction.

2. Human evaluation of generation quality: Conduct human judgment studies to evaluate the semantic quality and fluency of generated headlines, as automated metrics like BLEU and TER may not capture practical utility for this task.

3. Vocabulary coverage analysis: Analyze the vocabulary overlap between XLM-R's multilingual vocabulary and the unique characters/words in Sinhala to quantify how well the multilingual model can represent the language features.