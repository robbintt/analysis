---
ver: rpa2
title: 'PerkwE_COQA: Enhanced Persian Conversational Question Answering by combining
  contextual keyword extraction with Large Language Models'
arxiv_id: '2404.05406'
source_url: https://arxiv.org/abs/2404.05406
tags: []
core_contribution: This paper presents PerkwECOQA, a novel approach to enhance Persian
  conversational question-answering (CQA) systems by combining contextual keyword
  extraction with Large Language Models (LLMs). The method extracts keywords from
  user conversations using a graph-based approach (TopicRank), then uses these keywords
  alongside conversation content to guide the LLM in generating precise answers.
---

# PerkwE_COQA: Enhanced Persian Conversational Question Answering by combining contextual keyword extraction with Large Language Models

## Quick Facts
- arXiv ID: 2404.05406
- Source URL: https://arxiv.org/abs/2404.05406
- Reference count: 30
- Primary result: 8% improvement over LLM-only baseline on Persian CQA

## Executive Summary
This paper introduces PerkwE_COQA, a novel approach that enhances Persian conversational question-answering systems by integrating contextual keyword extraction with Large Language Models. The method leverages TopicRank, a graph-based keyword extraction technique, to identify salient terms from user conversations, which are then used alongside conversation content to guide LLM responses. This combination addresses challenges in handling implicit questions and maintaining contextual relevance in multi-turn conversations.

## Method Summary
The PerkwE_COQA approach operates by first extracting keywords from conversational context using the TopicRank algorithm, which builds a graph representation of the conversation and identifies important terms based on their connectivity and importance scores. These extracted keywords are then combined with the conversation content and passed to a Large Language Model for answer generation. The system was evaluated on the PCoQA dataset using standard CQA metrics including Exact Match, F1, BLEU, and ROUGE scores, demonstrating significant improvements over both baseline LLM-only approaches and traditional models like ParSQuAD+XLM-Roberta.

## Key Results
- Achieved 8% higher performance than LLM-only baseline on PCoQA dataset
- Outperformed ParSQuAD+XLM-Roberta baseline (EM: 0.37 vs 0.37, F1: 0.59 vs 0.62, BLEU: 0.48 vs 0.43, ROUGE: 0.60 vs 0.63)
- Demonstrated effectiveness in handling implicit questions and maintaining conversational context

## Why This Works (Mechanism)
The integration of keyword extraction with LLMs provides explicit contextual signals that help the model better understand the focus and intent of conversational queries. By identifying and emphasizing important terms from the conversation history, the approach compensates for the implicit nature of follow-up questions in multi-turn dialogues. This explicit context injection allows the LLM to generate more precise and contextually relevant answers, particularly for complex questions that require understanding of conversational history.

## Foundational Learning
- **TopicRank algorithm**: Graph-based keyword extraction method that identifies important terms by analyzing their relationships and importance scores in text
  - *Why needed*: Provides systematic way to extract salient terms from conversational context
  - *Quick check*: Verify graph construction and ranking accuracy on sample Persian conversations
- **Persian CQA characteristics**: Unique challenges of conversational question answering in Persian language including morphological complexity and context-dependent queries
  - *Why needed*: Understanding language-specific requirements for effective model design
  - *Quick check*: Compare performance on explicit vs implicit question types
- **LLM context handling**: How Large Language Models process and utilize extended conversational context
  - *Why needed*: Determines effectiveness of keyword-guided context injection
  - *Quick check*: Measure context retention across multiple conversation turns
- **Evaluation metrics for CQA**: Standard metrics including Exact Match, F1, BLEU, and ROUGE scores for conversational QA evaluation
  - *Why needed*: Provides quantitative assessment of system performance
  - *Quick check*: Validate metric calculations against published baselines
- **Persian language processing**: Specific challenges and considerations for NLP tasks in Persian
  - *Why needed*: Ensures approach accounts for language-specific characteristics
  - *Quick check*: Test keyword extraction accuracy on morphologically complex Persian sentences
- **Conversational AI evaluation**: Methods for assessing performance on multi-turn dialogue tasks
  - *Why needed*: Determines whether improvements are meaningful in practical scenarios
  - *Quick check*: Evaluate on conversations with varying context-dependence levels

## Architecture Onboarding
- **Component map**: TopicRank keyword extraction -> Context preprocessing -> LLM answer generation
- **Critical path**: Conversation text → Keyword extraction (TopicRank) → Keyword + context concatenation → LLM prompt → Answer generation
- **Design tradeoffs**: The approach trades computational overhead of keyword extraction for improved answer quality and contextual relevance
- **Failure signatures**: Poor performance on conversations with limited context, keyword extraction failures for certain Persian linguistic constructions, or LLM generation issues when keywords are irrelevant or misleading
- **First 3 experiments**: 1) Run TopicRank on sample Persian conversations to verify keyword quality, 2) Test LLM response quality with and without extracted keywords, 3) Evaluate system on conversations with explicit vs implicit questions to measure context handling capability

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Performance evaluation limited to Persian language CQA, limiting generalizability to other languages and domains
- Reliance on TopicRank algorithm may introduce performance dependencies on Persian text characteristics that are not well-documented
- Claims about outperforming human performance benchmarks require verification due to unusual nature of this claim

## Confidence
- High confidence in relative performance gains over LLM-only baseline (8% improvement)
- Medium confidence in absolute performance metrics due to potential variability in evaluation framework and dataset characteristics
- Medium confidence in cross-language generalizability claims given limited testing beyond Persian

## Next Checks
1. Replicate experiments on a multilingual CQA dataset to assess whether the keyword extraction + LLM approach generalizes beyond Persian
2. Conduct ablation studies removing the keyword extraction component to quantify its specific contribution to performance improvements
3. Test the system on conversational questions with varying degrees of context-dependence to better understand its limitations in handling implicit questions