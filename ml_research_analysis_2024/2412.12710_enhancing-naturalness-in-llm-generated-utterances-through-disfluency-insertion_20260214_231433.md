---
ver: rpa2
title: Enhancing Naturalness in LLM-Generated Utterances through Disfluency Insertion
arxiv_id: '2412.12710'
source_url: https://arxiv.org/abs/2412.12710
tags:
- speaker
- speech
- disfluencies
- utterances
- intelligibility
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the lack of disfluencies in large language
  model (LLM) outputs, which reduces perceived naturalness of synthesized speech for
  conversational agents. The authors propose a method to insert disfluencies into
  LLM-generated utterances by fine-tuning an LLM with LoRA on the Switchboard dataset
  and synthesizing the disfluent text using a text-to-speech model (Bark TTS) that
  supports spontaneous speech phenomena.
---

# Enhancing Naturalness in LLM-Generated Utterances through Disfluency Insertion

## Quick Facts
- **arXiv ID:** 2412.12710
- **Source URL:** https://arxiv.org/abs/2412.12710
- **Reference count:** 5
- **Primary result:** Fine-tuned LLM with LoRA on Switchboard data generates disfluencies at 29.1% rate, improving perceived spontaneity (2.91 vs 2.58) in user study

## Executive Summary
This paper addresses the unnatural fluency of LLM-generated utterances by inserting disfluencies to mimic spontaneous human speech. The authors fine-tune a Llama-2-7b model using LoRA on the Switchboard dataset and synthesize the output using Bark TTS. A user study with 41 participants found that disfluent utterances were rated significantly higher for spontaneity (2.91 vs 2.58 on 5-point scale) compared to fluent utterances, though intelligibility was slightly lower (4.23 vs 4.42). The approach achieves BLEU score of 0.55 and BERTScore F1 of 0.93 on the Switchboard test set.

## Method Summary
The authors fine-tuned Llama-2-7b-chat-hf with LoRA using the Switchboard dataset with disfluency annotations. The LoRA adapter was configured with rank 32, alpha 64, dropout 0.1, and learning rate 2×10⁻⁴. The fine-tuned model generates disfluent utterances which are then synthesized using Bark TTS. A user study compared fluent and disfluent utterances across two fictitious conversational scenarios, measuring intelligibility and perceived spontaneity through Likert-scale ratings.

## Key Results
- Disfluent utterances scored significantly higher for spontaneity (2.91 vs 2.58) than fluent utterances
- Intelligibility was slightly lower for disfluent utterances (4.23 vs 4.42)
- Fine-tuned model achieved BLEU score of 0.55 and BERTScore F1 of 0.93 on Switchboard test set
- Generated disfluencies at 29.1% rate (vs 24.5% in training data)

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning LLM with LoRA on disfluent Switchboard data enables insertion of contextually appropriate disfluencies. The LoRA adapter modifies the model's weights to learn patterns of disfluency insertion without retraining the full model, allowing efficient adaptation to spontaneous speech patterns. The assumption is that Switchboard disfluencies are representative enough to transfer to general conversational scenarios.

### Mechanism 2
Disfluencies enhance perceived spontaneity because they signal cognitive processing and hesitation, making speech sound more natural. When listeners hear disfluencies like "uh," "um," or false starts, they interpret this as evidence of real-time thinking rather than scripted delivery, increasing perceived authenticity. The assumption is that human listeners associate disfluencies with spontaneous speech rather than lack of preparation.

### Mechanism 3
Bark TTS's support for spontaneous speech phenomena allows accurate rendering of inserted disfluencies into natural-sounding audio. The TTS model's prosody and intonation capabilities enable it to pronounce disfluencies like false starts and hesitations in ways that sound natural rather than mechanical. The assumption is that Bark TTS's spontaneous speech features are sufficient to render the variety of disfluencies generated by the fine-tuned LLM.

## Foundational Learning

- **Concept:** Disfluency types and their linguistic functions
  - Why needed here: Understanding different categories of disfluencies (hesitations, fillers, repetitions, repairs) is essential for evaluating the quality and appropriateness of inserted disfluencies
  - Quick check question: What are the three main categories of disfluencies identified in linguistic research?

- **Concept:** Fine-tuning with LoRA vs full fine-tuning
  - Why needed here: Knowing the efficiency and effectiveness tradeoffs between LoRA and full fine-tuning helps understand why this approach was chosen and its limitations
  - Quick check question: What is the primary advantage of using LoRA over full fine-tuning for adapting LLMs?

- **Concept:** Evaluation metrics for natural language generation
  - Why needed here: Understanding BLEU, BERTScore, and human evaluation methods is crucial for interpreting the results and comparing with other approaches
  - Quick check question: Why might BLEU scores be less meaningful for evaluating disfluency insertion compared to fluency preservation?

## Architecture Onboarding

- **Component map:** Switchboard dataset → preprocessing → LoRA fine-tuning → disfluent text generation → Bark TTS → audio output → user evaluation
- **Critical path:** Disfluent text generation → TTS synthesis → user perception
- **Design tradeoffs:** Llama-2-7b chosen over larger models for balance; disfluency rate slightly overproduces (29.1% vs 24.5% training); Bark chosen over alternatives despite occasional artifacts
- **Failure signatures:** Over-insertion of disfluencies leading to unnatural speech; TTS artifacts degrading audio quality; disfluencies that don't match conversational context
- **First 3 experiments:**
  1. Compare disfluency insertion rates between fine-tuned model and ground truth Switchboard data
  2. Test Bark TTS with different disfluency types to identify which are rendered poorly
  3. Conduct A/B testing with different disfluency densities to find optimal naturalness-intelligibility tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal disfluency rate for maximizing perceived spontaneity without significantly compromising intelligibility? The paper notes that the fine-tuned LLM tends to overproduce disfluencies (29.1% vs 24.5% in training data) and suggests adjusting disfluency rates for specific scenarios. This remains unresolved as the study did not explore different disfluency rates.

### Open Question 2
How do different types of disfluencies (e.g., repetitions vs. hesitations) affect perceived naturalness and intelligibility differently? The paper mentions various types of disfluencies but does not analyze their individual effects, evaluating disfluencies collectively without distinguishing between types.

### Open Question 3
How does the effectiveness of disfluency insertion vary across different conversational contexts and domains? The user study used fictitious conversations generated by ChatGPT, which may not represent diverse real-world contexts. The study's limitations acknowledge this constraint but do not explore contextual variations.

## Limitations

- **Domain generalizability:** Switchboard is a telephone conversation corpus that may not capture disfluency patterns in other conversational contexts, potentially making inserted disfluencies sound awkward in non-telephone settings
- **Tradeoff optimization:** The modest improvement in spontaneity (0.33 points on 5-point scale) comes at cost of reduced intelligibility (0.19 points), with optimal balance point for different applications unclear
- **Evaluation scope:** The study focused on immediate perception of single utterances rather than longer conversational interactions, missing potential effects on engagement and cognitive load over extended dialogues

## Confidence

- **High confidence:** The LoRA fine-tuning approach effectively learns to insert disfluencies from the Switchboard dataset
- **Medium confidence:** Bark TTS adequately renders disfluencies into natural-sounding speech, though occasional artifacts suggest room for improvement
- **Medium confidence:** Disfluencies enhance perceived spontaneity in short utterances, but practical impact on longer conversational interactions remains uncertain

## Next Checks

1. **Cross-domain evaluation:** Test the fine-tuned model on disfluency generation for non-telephone conversational datasets to assess generalizability and identify domain-specific disfluency patterns
2. **Disfluency density optimization:** Conduct systematic study varying disfluency insertion rate (10%, 20%, 30%, 40%) to identify optimal tradeoff between naturalness and intelligibility for different contexts
3. **Longitudinal interaction study:** Evaluate approach in multi-turn conversational scenarios measuring not just perceived naturalness but also task completion, user engagement, and cognitive load over 10-15 exchanges