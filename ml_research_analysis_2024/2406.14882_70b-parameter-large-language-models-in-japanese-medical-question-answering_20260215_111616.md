---
ver: rpa2
title: 70B-parameter large language models in Japanese medical question-answering
arxiv_id: '2406.14882'
source_url: https://arxiv.org/abs/2406.14882
tags:
- medical
- japanese
- instruction
- arxiv
- llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study applies 70B-parameter large language models (LLMs) to
  Japanese medical question-answering tasks. It performs instruction tuning on multiple
  models using a Japanese medical question-answering dataset, improving their ability
  to solve Japanese medical license exams.
---

# 70B-parameter large language models in Japanese medical question-answering

## Quick Facts
- arXiv ID: 2406.14882
- Source URL: https://arxiv.org/abs/2406.14882
- Authors: Issey Sukeda; Risa Kishikawa; Satoshi Kodera
- Reference count: 11
- 70B-parameter LLMs achieve over 50% accuracy on Japanese medical license exams after instruction tuning

## Executive Summary
This study explores the application of 70B-parameter large language models to Japanese medical question-answering tasks. The researchers perform instruction tuning on multiple models using a Japanese medical QA dataset, demonstrating significant improvements in their ability to solve Japanese medical license exams. The results show that Japanese-centric models benefit more from instruction tuning compared to English-centric models, achieving over 50% accuracy. The study also highlights the importance of prompt engineering, with different prompt formats leading to performance differences of up to 8%.

## Method Summary
The researchers apply instruction tuning to 70B-parameter LLMs using a Japanese medical question-answering dataset. They evaluate the performance of Llama 2, Xwin-LM-70B-V0.1, and Swallow-70b-instruct-hf models on Japanese medical license exam questions. The fine-tuning process employs QLoRA to efficiently adapt the large models to the medical domain. The evaluation uses 1-shot Chain-of-Thought prompting with two different prompt formats to assess the impact of prompt engineering on performance.

## Key Results
- Instruction tuning using Japanese medical QA dataset significantly improves Japanese LLM performance on Japanese medical license exams, surpassing 50% accuracy
- Japanese-centric models (Swallow) show greater improvement through instruction tuning compared to English-centric models (Xwin-LM)
- Different prompt formats can lead to non-negligible performance differences, with accuracy gaps reaching up to 8%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction tuning on Japanese medical QA dataset significantly improves Japanese LLM performance on Japanese medical license exams.
- Mechanism: The Japanese medical QA dataset contains domain-specific knowledge and question-answer patterns that align with the Japanese medical license exam format. Fine-tuning on this dataset allows the LLM to learn relevant medical concepts and reasoning patterns specific to Japanese medical practice.
- Core assumption: The Japanese medical QA dataset is sufficiently representative of the knowledge and question styles found in the Japanese medical license exam.
- Evidence anchors:
  - [abstract] "instruction tuning using Japanese medical question-answering dataset significantly improves the ability of Japanese LLMs to solve Japanese medical license exams, surpassing 50% in accuracy."
  - [section 4.1] "Swallow performed better than Xwin, followed by Llama 2, around 9% difference each. This result exhibits the effect of suited continual pretraining."
- Break condition: If the Japanese medical QA dataset is not representative of the Japanese medical license exam content or if the exam content changes significantly, the instruction tuning may not yield similar improvements.

### Mechanism 2
- Claim: Japanese-centric models benefit more from instruction tuning than English-centric models in Japanese medical QA tasks.
- Mechanism: Japanese-centric models, having been trained on more Japanese data and optimized for Japanese language processing, can better leverage the Japanese medical QA dataset during instruction tuning. This leads to more effective knowledge transfer and improved performance on Japanese medical tasks.
- Core assumption: The tokenizer and pretraining data of Japanese-centric models are better suited for processing Japanese medical text compared to English-centric models.
- Evidence anchors:
  - [abstract] "the Japanese-centric models exhibit a more significant leap in improvement through instruction tuning compared to their English-centric counterparts."
  - [section 4.1] "Swallow passes continual pretraining with more than 90B tokens... thus its ability in Japanese should be better than English-centric Xwin."
- Break condition: If the Japanese medical QA dataset contains a significant amount of English medical terminology or if the English-centric models are extensively fine-tuned on Japanese data, the performance gap may diminish.

### Mechanism 3
- Claim: Different prompt formats can lead to non-negligible performance differences in Japanese medical QA tasks.
- Mechanism: The order and phrasing of instructions in prompts can influence how the LLM processes and responds to questions. Even slight differences in prompt structure can affect the model's reasoning and answer selection, leading to varying accuracy levels.
- Core assumption: The LLM's response is sensitive to the specific wording and structure of the prompt, even when the semantic meaning is similar.
- Evidence anchors:
  - [abstract] "different prompt formats can lead to non-negligible performance differences, with accuracy gaps reaching up to 8% in some cases."
  - [section 4.2] "this difference resulted in a non-negligible accuracy gap as large as 8.7% at most."
- Break condition: If the LLM is robust to prompt variations or if the prompts are designed to be semantically equivalent in all aspects, the performance differences may be negligible.

## Foundational Learning

- Concept: Domain adaptation
  - Why needed here: To improve the performance of LLMs on specific tasks or domains, such as Japanese medical QA, by fine-tuning on relevant datasets.
  - Quick check question: How does instruction tuning differ from standard fine-tuning in terms of objectives and dataset requirements?

- Concept: Tokenizer optimization
  - Why needed here: To ensure that the LLM can effectively process and understand the nuances of Japanese medical text, which may contain domain-specific terminology and language patterns.
  - Quick check question: What are the key considerations when designing a tokenizer for a specific language or domain?

- Concept: Prompt engineering
  - Why needed here: To guide the LLM's reasoning process and influence its answer selection, as demonstrated by the significant performance differences observed with different prompt formats.
  - Quick check question: How can prompt engineering be used to improve the reliability and consistency of LLM responses in specific tasks?

## Architecture Onboarding

- Component map: Base LLM (Llama 2, Xwin, or Swallow) -> Tokenizer -> Fine-tuning mechanism (QLoRA) -> Prompt format -> Evaluation dataset
- Critical path: Select base LLM -> Optimize tokenizer for Japanese medical text -> Fine-tune on Japanese medical QA dataset using QLoRA -> Evaluate performance with different prompt formats on Japanese medical license exam dataset
- Design tradeoffs: Choice of base LLM involves tradeoffs between model size, computational requirements, and performance on Japanese tasks. Japanese-centric models may offer better performance but may be less accessible or require more computational resources. Fine-tuning approach (QLoRA) trades off between parameter efficiency and ability to learn complex task-specific patterns.
- Failure signatures: If tokenizer is not well-suited for Japanese medical text, LLM may struggle to understand domain-specific terminology. If fine-tuning dataset is not representative of target task, LLM may not learn necessary knowledge. If prompt engineering is not effective, LLM may not be guided to provide accurate answers.
- First 3 experiments:
  1. Evaluate performance of different base LLMs on Japanese medical QA task without fine-tuning to establish baseline
  2. Fine-tune each base LLM on Japanese medical QA dataset using QLoRA and evaluate performance improvement
  3. Test impact of different prompt formats on performance of fine-tuned LLMs to identify most effective prompt structure

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Japanese medical LLMs compare to their English counterparts when evaluated on medical license exams from different countries?
- Basis in paper: [explicit] The study mentions that using USMLE for training and NMLE for evaluation may not be ideal since they are from different countries and languages.
- Why unresolved: The paper acknowledges this limitation but does not explore cross-country or cross-language performance comparisons in detail.
- What evidence would resolve it: Conducting experiments where Japanese LLMs are trained on NMLE and evaluated on USMLE, or vice versa, would provide insights into their performance across different medical systems and languages.

### Open Question 2
- Question: What is the impact of using more diverse and extensive Japanese medical corpora on the performance of Japanese medical LLMs?
- Basis in paper: [explicit] The paper suggests that curating available Japanese medical corpus is important for constructing a practical and useful LLM in a local environment.
- Why unresolved: The study uses a limited dataset (USMLE-JP) for instruction tuning and does not explore the effects of using larger or more varied Japanese medical datasets.
- What evidence would resolve it: Training models on larger, more diverse Japanese medical datasets and evaluating their performance on NMLE would show the impact of dataset size and diversity on LLM capabilities.

### Open Question 3
- Question: How do different prompt engineering techniques affect the performance of Japanese medical LLMs?
- Basis in paper: [explicit] The study notes that prompt engineering significantly impacts LLM performance and mentions techniques like multi-shot inference, self-consistency, ensemble refinement, and Medprompt as potential areas for improvement.
- Why unresolved: The research only explores two slightly different prompt formats and does not investigate other advanced prompt engineering techniques.
- What evidence would resolve it: Experimenting with various prompt engineering techniques and comparing their impact on model accuracy would provide insights into optimizing prompt design for Japanese medical LLMs.

## Limitations

- The study relies on the USMLE-JP dataset for instruction tuning, but the exact composition and representativeness of this dataset for Japanese medical license exams is not fully specified
- The significant performance differences based on prompt formats indicate that the LLM's responses are highly sensitive to prompt structure, raising questions about the robustness and reliability of the model's performance
- The use of 70B-parameter models requires substantial computational resources for fine-tuning and evaluation, which may limit the reproducibility of results and practical applicability in resource-constrained settings

## Confidence

- High confidence: The overall improvement in Japanese medical QA performance through instruction tuning is well-supported by the experimental results and aligns with established domain adaptation principles.
- Medium confidence: The superior performance of Japanese-centric models compared to English-centric models is supported by the data, but the exact reasons for this difference are not fully explored or explained.
- Low confidence: The specific impact of prompt format variations on performance is demonstrated, but the underlying mechanisms and potential for further optimization are not thoroughly investigated.

## Next Checks

1. Conduct a thorough analysis of the USMLE-JP dataset to assess its representativeness of Japanese medical license exam content and identify potential biases or gaps in coverage
2. Perform extensive experiments with various prompt formats and variations to quantify the sensitivity of the LLM's performance to prompt structure and identify the most robust prompt design for Japanese medical QA tasks
3. Investigate the performance trade-offs between full fine-tuning and parameter-efficient methods (e.g., QLoRA) for 70B-parameter models, and explore techniques to reduce computational requirements while maintaining performance levels