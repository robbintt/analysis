---
ver: rpa2
title: Influence Maximization via Graph Neural Bandits
arxiv_id: '2406.12835'
source_url: https://arxiv.org/abs/2406.12835
tags: []
core_contribution: The paper addresses the challenge of influence maximization in
  online social networks where the underlying diffusion network topology is unknown.
  The proposed method, IM-GNB, combines graph neural networks with contextual bandits
  to estimate users' probabilities of being influenced by potential influencers (seeds).
---

# Influence Maximization via Graph Neural Bandits

## Quick Facts
- arXiv ID: 2406.12835
- Source URL: https://arxiv.org/abs/2406.12835
- Reference count: 40
- Primary result: IM-GNB achieves up to 33,000 cumulative influenced users over 500 rounds, outperforming baselines by up to 28.8% with optimal 3-4 seeds per round

## Executive Summary
This paper introduces IM-GNB, a novel approach to influence maximization in online social networks where the underlying diffusion network topology is unknown. The method combines graph neural networks with contextual bandits to dynamically estimate user-user correlations and diffusion probabilities, enabling real-time seed selection that balances exploration and exploitation. Extensive experiments on Twitter and Weibo datasets demonstrate significant performance improvements over existing baselines, with the largest gains achieved when selecting 3-4 seeds per round and using up to 150 user clusters.

## Method Summary
IM-GNB addresses the challenge of influence maximization without known network topology by constructing exploitation and exploration graphs that capture user-user correlations based on estimated diffusion probabilities and potential gains. These graphs are refined using Graph Convolutional Networks (GCNs) to improve reward predictions in each contextual setting. The method estimates diffusion probabilities as functions of influencer features, message context, and user graph structure, allowing adaptation to different message types without retraining. Seed selection balances exploitation of known high-performing influencers with exploration of uncertain but potentially high-reward options through a neural bandit framework that incorporates uncertainty estimates derived from past gradients.

## Key Results
- IM-GNB achieves cumulative spreads of up to 33,000 influenced users over 500 rounds
- Performance advantage of up to 28.8% over baseline methods when selecting 3-4 seeds per round
- Effectiveness increases with user clustering granularity, with optimal performance at 150 groups
- Artificial exploration score boosting improves performance across all tested configurations

## Why This Works (Mechanism)

### Mechanism 1
The method learns user-user correlations dynamically to refine diffusion probability estimates without requiring a known network topology. At each round, for each influencer, the algorithm constructs exploitation and exploration graphs based on estimated diffusion probabilities and potential gains, refined using Graph Convolutional Networks to aggregate neighborhood information and improve reward predictions. The core assumption is that users influenced by the same influencer under the same context exhibit correlated diffusion probabilities that can be learned from observed activations. If this assumption fails (e.g., in networks where user behavior is entirely independent), the graph construction would add noise rather than improve estimates.

### Mechanism 2
Balancing exploration and exploitation is achieved through a neural bandit framework that incorporates uncertainty estimates into seed selection. The method uses two GCN-based networksâ€”one for exploitation (estimating expected reward) and one for exploration (estimating potential gain)â€”with their sum determining arm selection. The core assumption is that potential gain (uncertainty) can be effectively estimated from past gradients of the exploitation function, guiding exploration toward arms with uncertain but potentially high rewards. If the gradient-based exploration function fails to capture true uncertainty, the method may either over-explore (wasting budget) or under-explore (missing high-reward seeds).

### Mechanism 3
Contextual information is integrated into influence estimation to capture message-specific diffusion patterns. Each user-influencer pair's diffusion probability is estimated as a function of the influencer's feature vector, the message context, and user graph structure, allowing the model to adapt to different message types without retraining. The core assumption is that influence is context-dependent and this dependency can be captured by conditioning diffusion probability estimation on the message context. If the context representation is too coarse or irrelevant to diffusion behavior, contextual conditioning will not improve predictions and may add unnecessary complexity.

## Foundational Learning

- Graph Neural Networks (GNNs) and their ability to aggregate neighborhood information: Why needed here - The method uses GCNs to refine estimated rewards by aggregating user-user correlation information from exploitation and exploration graphs. Quick check: Can you explain how a GCN layer updates node representations using the adjacency matrix and feature matrix?

- Multi-armed bandits and the exploration-exploitation tradeoff: Why needed here - The method frames influence maximization as a contextual bandit problem, where arms (influencers) are selected to maximize cumulative reward (spread) over multiple rounds. Quick check: What is the difference between a standard multi-armed bandit and a contextual bandit, and why is the contextual version more suitable for influence maximization?

- Stochastic diffusion models (e.g., Independent Cascade, Linear Threshold): Why needed here - The method operates in a setting where the diffusion network is unknown, but understanding these models helps in interpreting estimated diffusion probabilities and their role in the algorithm. Quick check: How does the Independent Cascade model define the probability of influence propagation, and how does this relate to the estimated diffusion probabilities in the algorithm?

## Architecture Onboarding

- Component map: Data preprocessing -> User clustering -> Neural networks (â„(1), â„(2)) -> GCN models (ğ‘“(1), ğ‘“(2)) -> Arm selection -> Training loop

- Critical path: 1) Receive context ğ¶ğ‘¡ 2) For each arm ğ‘˜ğ‘–, construct exploitation and exploration graphs using user networks 3) Apply GCNs to refine reward and potential gain estimates 4) Select arm set ğ¼ğ‘¡ = arg maxğ¼ğ‘¡âŠ‚ğ¾,|ğ¼ğ‘¡|=ğ¿ (Ë†ğ‘Ÿğ‘–,ğ‘¡ + Ë†ğ‘ğ‘–,ğ‘¡ ) 5) Observe activations, update user networks and GCN models 6) Repeat for next round

- Design tradeoffs: User clustering vs. individual user modeling (clustering reduces computation but may lose granularity); GCN depth (hops ğ›¾) vs. overfitting (more hops capture broader context but may introduce noise); exploration score boosting vs. natural exploration (artificial boosting increases exploration but may deviate from optimal exploration policy)

- Failure signatures: Poor performance in early rounds (indicates slow convergence of user networks or GCNs); declining performance with more clusters (suggests overfitting or insufficient data per cluster); high variance in reward estimates (may indicate unstable graph construction or noisy user correlations)

- First 3 experiments: 1) Run with ğ¿ = 1 on a small synthetic network to verify basic functionality and convergence 2) Vary the number of user clusters (e.g., 10, 50, 100) to observe impact on performance and runtime 3) Compare with baseline that uses random exploration to quantify benefit of GCN-based exploration strategy

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- Computational complexity scales poorly with user group size, requiring careful clustering to remain practical
- Exploration-exploitation balance relies on gradient-based uncertainty estimates that may be unstable in highly non-stationary environments
- Artificial exploration score boosting mechanism lacks theoretical grounding and may lead to suboptimal exploration policies

## Confidence
- Dynamic correlation learning: Medium confidence - theoretically sound but limited empirical validation of graph construction process
- Uncertainty estimation: Medium confidence - framework is sound but effectiveness demonstrated primarily through aggregate metrics rather than component ablation
- Contextual integration: Low confidence - minimal evidence that context-aware conditioning meaningfully improves diffusion probability estimates

## Next Checks
1. Conduct ablation studies removing the GCN-based exploration component to quantify its contribution to performance gains
2. Test the method on synthetic networks where ground-truth diffusion probabilities are known to validate correlation learning accuracy
3. Evaluate sensitivity to the artificial exploration score boosting parameter across different network topologies and message contexts