---
ver: rpa2
title: 'Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure
  Queries'
arxiv_id: '2409.12640'
source_url: https://arxiv.org/abs/2409.12640
tags:
- context
- task
- latent
- evaluations
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Michelangelo, a new long-context evaluation
  framework called Latent Structure Queries (LSQ) that goes beyond simple retrieval
  tasks. The core idea is to require models to "chisel away" irrelevant information
  in a context to reveal underlying latent structures, then query these structures
  for details.
---

# Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries

## Quick Facts
- arXiv ID: 2409.12640
- Source URL: https://arxiv.org/abs/2409.12640
- Reference count: 31
- Primary result: New evaluation framework (LSQ) tests long-context understanding beyond retrieval, revealing significant performance drops even at 32K context length

## Executive Summary
Michelangelo introduces a novel evaluation framework for long-context understanding that goes beyond traditional retrieval tasks. The framework uses Latent Structure Queries (LSQ) to test models' ability to extract and reason about latent structures within long contexts, requiring models to "chisel away" irrelevant information. The evaluation includes three diagnostic tasks - Latent List, Multi-Round Coreference Resolution, and IDK - across code and natural language domains. Testing on 10 frontier models reveals significant performance drops beyond basic retrieval tasks, with models struggling to maintain performance even at 32K context length despite some generalization up to 1M tokens for certain models like Gemini 1.5.

## Method Summary
The Michelangelo framework introduces three synthetic diagnostic tasks designed to evaluate long-context understanding beyond simple retrieval. Latent List requires tracking list operations within long contexts, Multi-Round Coreference Resolution (MRCR) tests the ability to reproduce specific outputs from conversations by resolving references across turns, and IDK evaluates when information is missing from the context. These tasks are designed to be minimal, synthetic, un-leaked, and automatically scorable. The framework tests models across varying context lengths (4K to 1M tokens) to assess both performance and generalization capabilities, with a focus on structural reasoning rather than just memorization or retrieval.

## Key Results
- Most models show significant performance drops beyond basic retrieval tasks, even at 32K context length
- Different model families excel on different evaluation tasks, indicating orthogonal aspects of long-context understanding
- While some models like Gemini 1.5 show generalization up to 1M tokens, simple long-context reasoning primitives remain unsolved
- The synthetic nature of tasks enables controlled testing and automatic scoring, though questions remain about real-world applicability

## Why This Works (Mechanism)
The Michelangelo framework works by creating structured synthetic tasks that require models to perform reasoning over latent structures rather than simple pattern matching or retrieval. By designing tasks that demand "chisel away" irrelevant information to reveal underlying patterns, the framework forces models to demonstrate genuine understanding of long contexts. The automatic scorable nature of the synthetic tasks ensures consistent evaluation across different models and context lengths, while the controlled environment allows for precise measurement of performance degradation and generalization capabilities.

## Foundational Learning
- Latent Structure Queries (LSQ): A methodology for evaluating models' ability to extract and reason about implicit structures within long contexts - needed to move beyond simple retrieval testing, quick check: can the model identify and query latent patterns without explicit markers
- Synthetic Diagnostic Tasks: Controlled, automatically scorable evaluation tasks that isolate specific reasoning capabilities - needed for consistent and reproducible evaluation, quick check: task remains solvable with perfect attention but difficult with real-world limitations
- Context Length Generalization: Testing models' ability to maintain performance across exponentially increasing context windows - needed to understand true long-context capabilities, quick check: performance trends from 4K to 1M tokens show consistent degradation patterns
- Orthogonal Task Design: Creating evaluation tasks that measure distinct aspects of long-context understanding - needed to comprehensively assess different reasoning capabilities, quick check: different model families show varied strengths across tasks

## Architecture Onboarding
Component Map: Context Processing -> Latent Structure Extraction -> Query Resolution -> Performance Scoring
Critical Path: Input context → Latent structure identification → Information retrieval/chiseling → Answer generation → Automatic scoring
Design Tradeoffs: Synthetic tasks enable automatic scoring and controlled testing but may lack real-world ecological validity; varying context lengths reveal generalization but increase computational cost
Failure Signatures: Performance drops at 32K+ tokens for most models; inconsistent performance across different task types; inability to handle missing information scenarios
First Experiments:
1. Test baseline model performance on individual tasks at 4K context length to establish performance floors
2. Evaluate same models at 32K context to identify initial performance degradation patterns
3. Run cross-task consistency checks to verify orthogonal nature of different evaluation dimensions

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic nature of tasks raises questions about real-world applicability and ecological validity
- Focus on specific structural reasoning tasks may miss other important dimensions of long-context understanding
- Absence of ablation studies on different context window sizes and model architectures limits understanding of performance drivers
- Limited model coverage makes broader generalizations about long-context capabilities tentative

## Confidence
- **High confidence**: Most models struggle with simple long-context reasoning primitives and show significant performance drops beyond basic retrieval tasks
- **Medium confidence**: Orthogonal aspects of long-context understanding are measured by different tasks, though theoretical framework could be strengthened
- **Medium confidence**: Generalization capabilities up to 1M tokens, particularly for Gemini 1.5, but synthetic nature and limited coverage make broader claims tentative

## Next Checks
1. Test model performance on a subset of tasks using real-world document collections to assess ecological validity and practical applicability
2. Conduct ablation studies varying context window sizes, model architectures, and attention mechanisms to isolate key factors affecting long-context performance
3. Extend evaluation to include reasoning over heterogeneous document types (code, text, tables) simultaneously to test robustness across modalities