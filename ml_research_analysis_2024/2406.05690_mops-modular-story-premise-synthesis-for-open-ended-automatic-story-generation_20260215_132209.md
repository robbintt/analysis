---
ver: rpa2
title: 'MoPS: Modular Story Premise Synthesis for Open-Ended Automatic Story Generation'
arxiv_id: '2406.05690'
source_url: https://arxiv.org/abs/2406.05690
tags:
- story
- premises
- premise
- maximus
- will
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MoPS automates the design and creation of story premises by breaking
  them into modular components (theme, background, persona, plot) and generating diverse
  candidates for each. A key path sampled from a nested dictionary of these candidates
  is synthesized into a coherent premise sentence using a large language model.
---

# MoPS: Modular Story Premise Synthesis for Open-Ended Automatic Story Generation

## Quick Facts
- arXiv ID: 2406.05690
- Source URL: https://arxiv.org/abs/2406.05690
- Reference count: 40
- One-line primary result: MoPS generates more diverse and higher-quality story premises than baseline methods

## Executive Summary
MoPS (Modular Story Premise Synthesis) automates the design and creation of story premises by breaking them into modular components (theme, background, persona, plot) and generating diverse candidates for each. A key path sampled from a nested dictionary of these candidates is synthesized into a coherent premise sentence using a large language model. Evaluation shows MoPS-generated premises significantly outperform baselines in diversity, fascination, completeness, and originality. Extended stories from MoPS premises also exhibit higher quality.

## Method Summary
MoPS consists of three main phases: (1) Candidate induction - LLM generates candidates for each module (theme, background, persona, event, ending, twist) with dependencies enforced via prompts; (2) Premise synthesis - sample a key path from the nested dictionary and instruct LLM to synthesize into a coherent premise sentence; (3) Self-verification - LLM checks for inconsistencies or factual errors and discards invalid premises. The modular design enables combinatorial creativity while sequential dependencies ensure coherence, and self-verification prevents generation of flawed premises.

## Key Results
- MoPS-generated premises show higher diversity in semantic breadth and density compared to baselines
- Premises exhibit superior quality in fascination, completeness, and originality metrics
- Extended stories generated from MoPS premises demonstrate higher quality than those from baseline premises

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modular decomposition into theme, background, persona, and plot modules allows combinatorial creativity to generate unique and innovative premises.
- Mechanism: Each module captures a distinct narrative element (theme sets central idea, background provides context, persona drives plot, plot forms narrative backbone). Sampling from pre-collected candidates within each module and combining them creates novel premise designs.
- Core assumption: Creative combinations of existing modular components yield outputs perceived as diverse and original.
- Evidence anchors:
  - [abstract] "Our novelty lies in creative combinations of modules to generate a large number of diverse, fascinating, complete, and original story premises."
  - [section 3.1] "The effectiveness of MoPS primarily stems from its modular design, embodying the concept of combinatorial creativity... That is, while each component may represent existing ideas, their combination can boost unique and innovative outcomes."
  - [corpus] Weak - no direct corpus evidence cited for this mechanism specifically.
- Break condition: If module candidates become repetitive or lack sufficient diversity, combinatorial creativity fails to produce novel premises.

### Mechanism 2
- Claim: Sequential dependencies between modules ensure premise coherence and completeness.
- Mechanism: Each module's candidates depend on preceding module choices (e.g., persona depends on background and theme, event depends on persona/background/theme). This dependency chain constrains premise generation to maintain internal consistency.
- Core assumption: Enforcing logical dependencies between narrative elements prevents incoherent premise generation.
- Evidence anchors:
  - [section 3.1] "Dependency between Modules... arrows in Fig. 1 illustrate the dependency between and within modules, following the natural logic of story construction."
  - [section 5.2] "Premises from designs lacking sequential dependencies show decreased fascination and completeness but increased originality. This is because the inconsistency of design elements led to unique but subpar premises."
  - [corpus] Weak - no direct corpus evidence cited for this mechanism specifically.
- Break condition: If dependency enforcement is weakened or removed, premise quality degrades despite maintaining originality.

### Mechanism 3
- Claim: Self-verification step using LLM prevents generation of premises with obvious inconsistencies or factual errors.
- Mechanism: After synthesizing a premise from the sampled module path, LLM is prompted to check for inconsistencies (e.g., mismatched background/plot/characters) or factual errors (e.g., historical inaccuracies). Premises failing verification are discarded.
- Core assumption: LLM verification effectively filters out low-quality premises that would otherwise pass initial generation.
- Evidence anchors:
  - [section 3.3] "Self-Verification... we further instruct LLM to self-verify whether synthesized premises contain any obvious inconsistencies or factual errors... If so, that corrupt premise will be discarded."
  - [section 3.2] "All these story premises constitute the complete version... showing that injecting preceding premise modules into prompts can largely prevent inconsistencies and factual errors."
  - [corpus] Weak - no direct corpus evidence cited for this mechanism specifically.
- Break condition: If verification prompts become ineffective or LLM hallucination persists despite verification, premise quality suffers.

## Foundational Learning

- Concept: Story premise structure and components
  - Why needed here: Understanding what makes a story premise effective (theme, conflict, characters, plot) is essential for designing appropriate modular components.
  - Quick check question: What are the four main modules MoPS uses to decompose a story premise, and what narrative function does each serve?

- Concept: Combinatorial creativity
  - Why needed here: MoPS relies on generating novel outputs by combining existing modular components in new ways, rather than generating from scratch.
  - Quick check question: How does MoPS achieve diversity in generated premises without requiring LLM to generate entire premises from scratch?

- Concept: Sequential dependencies in narrative generation
  - Why needed here: MoPS enforces logical dependencies between modules to ensure premise coherence (e.g., persona choices depend on background/theme).
  - Quick check question: Why does MoPS enforce dependencies between modules, and what happens when these dependencies are removed (based on ablation results)?

## Architecture Onboarding

- Component map: Theme induction → Background induction → Persona induction → Event/Ending/Twist induction → Premise synthesis → Self-verification → Output valid premise
- Critical path: Theme induction → Background induction → Persona induction → Event/Ending/Twist induction → Premise synthesis → Self-verification → Output valid premise
- Design tradeoffs: Modular decomposition enables combinatorial creativity and focused generation but requires maintaining a large candidate dictionary; sequential dependencies ensure coherence but limit some creative freedom; self-verification adds quality control but introduces computational overhead.
- Failure signatures: Premises with obvious inconsistencies (mismatched elements), factual errors (historical inaccuracies), lack of fascination/completeness, or repetitive patterns suggest failures in candidate generation, dependency enforcement, or verification.
- First 3 experiments:
  1. Run candidate induction for all modules with a single theme to verify dependency enforcement and candidate diversity.
  2. Synthesize premises from sampled key paths and evaluate self-verification effectiveness by checking if obviously inconsistent premises are filtered out.
  3. Generate a small batch of premises and manually evaluate diversity and quality to validate the combinatorial creativity mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the modular design of MoPS impact the diversity of story premises compared to non-modular approaches?
- Basis in paper: [explicit] The paper states that MoPS consistently outperforms all baselines in semantic breadth and density, attributing this to its modular design.
- Why unresolved: While the paper shows that MoPS has higher diversity scores, it does not directly compare the diversity of premises generated by MoPS with those generated by a non-modular approach.
- What evidence would resolve it: A controlled experiment comparing the diversity of premises generated by MoPS with those generated by a non-modular approach, such as a simple prompt to an LLM.

### Open Question 2
- Question: How does the sequential dependency between modules in MoPS affect the quality of generated story premises?
- Basis in paper: [explicit] The paper mentions that MoPS's modules have sequential dependencies and conducts an ablation study to test the effectiveness of these dependencies.
- Why unresolved: The ablation study shows that removing sequential dependencies decreases premise quality, but it does not explore the impact of different dependency structures or the optimal level of dependency.
- What evidence would resolve it: An experiment comparing the quality of premises generated by MoPS with different dependency structures, such as varying the number of modules or the strength of their dependencies.

### Open Question 3
- Question: How does the quality of MoPS-generated story premises translate to the quality of extended stories?
- Basis in paper: [explicit] The paper shows that stories generated from MoPS premises have higher quality than those generated from baseline premises.
- Why unresolved: While the paper demonstrates a positive correlation between premise quality and story quality, it does not explore the specific aspects of premise quality that contribute most to story quality.
- What evidence would resolve it: A detailed analysis of the relationship between premise quality and story quality, identifying the key elements of high-quality premises that lead to high-quality stories.

## Limitations
- The quality and diversity of LLM-generated candidates forms the foundation of the entire system, creating potential for compounding errors
- Self-verification mechanism's effectiveness is uncertain as LLM hallucination could persist despite verification prompts
- Claim that MoPS outperforms all baselines in long-story generation quality is based on limited comparison with only one existing system (CAiGHT)

## Confidence
- Premise quality improvements (fascination, completeness, originality): Medium (based on ablation studies)
- Outperforms all baselines in long-story generation quality: Low (limited comparison with one system)
- Core modular design effectiveness: Medium (demonstrated through ablation studies but limited direct comparison)

## Next Checks
1. Perform ablation testing by systematically removing modules to quantify their individual contributions to premise quality and diversity
2. Test premise quality across different LLM model sizes and types to establish robustness of the modular approach
3. Evaluate long-term story generation quality by comparing MoPS-generated premises against human-written premises in extended narrative contexts