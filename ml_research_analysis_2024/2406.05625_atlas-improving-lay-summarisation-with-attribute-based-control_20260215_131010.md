---
ver: rpa2
title: 'ATLAS: Improving Lay Summarisation with Attribute-based Control'
arxiv_id: '2406.05625'
source_url: https://arxiv.org/abs/2406.05625
tags: []
core_contribution: "ATLAS improves lay summarisation by controlling four attributes\u2014\
  length, readability, background information, and content word entropy\u2014to tailor\
  \ summaries to different audience expertise levels. Using BART-base with fine-grained\
  \ control tokens, ATLAS outperforms state-of-the-art baselines in both automatic\
  \ and human evaluations across biomedical datasets."
---

# ATLAS: Improving Lay Summarisation with Attribute-based Control

## Quick Facts
- arXiv ID: 2406.05625
- Source URL: https://arxiv.org/abs/2406.05625
- Authors: Zhihao Zhang; Tomas Goldsack; Carolina Scarton; Chenghua Lin
- Reference count: 19
- Key outcome: ATLAS improves lay summarisation by controlling four attributes—length, readability, background information, and content word entropy—to tailor summaries to different audience expertise levels.

## Executive Summary
ATLAS introduces a novel approach to lay summarisation that controls four key attributes—length, readability, background information, and content word entropy—to generate summaries tailored to different audience expertise levels. Using BART-base with fine-grained control tokens, ATLAS outperforms state-of-the-art baselines in both automatic and human evaluations across biomedical datasets. Human evaluation shows ATLAS excels in comprehensibility, layness, and factuality. Controllability analysis confirms that extreme attribute settings produce significantly more technical or lay summaries.

## Method Summary
ATLAS uses BART-base as its foundation and prepends control tokens specifying length, readability, background information, and content word entropy to each input document. The model is trained on combined eLife (4.8k articles) and PLOS (27.5k articles) datasets, with control attributes extracted using SciBERT for entropy, BERT-based classifiers for background information, and textstat for readability scores. The model is fine-tuned for up to 5 epochs with batch size 1, generating summaries with specific characteristics based on the control tokens.

## Key Results
- ATLAS outperforms state-of-the-art baselines in both automatic metrics (ROUGE-1/2/L, BERTScore) and human evaluations
- Human evaluation confirms ATLAS superiority in comprehensibility, layness, and factuality
- Ablation studies show each attribute contributes to performance, with length and background information having the largest impacts
- Controllability analysis demonstrates extreme attribute values produce significantly different summary types

## Why This Works (Mechanism)

### Mechanism 1
Fine-grained control tokens enable tailored summary attributes for different audience expertise levels. The model prepends control tokens specifying length, readability, background information, and content word entropy to each input document, guiding BART-base to generate summaries matching desired "layness" levels. The core assumption is that discretizing attribute values into fixed-width bins and using them as control tokens allows the model to learn distinct summary characteristics associated with each bin.

### Mechanism 2
Multiple control attributes capture and effectuate differences between technical and non-technical summaries. The combination of length, readability, background information, and content word entropy attributes discriminates between summary types and guides the model to produce summaries with desired characteristics. The core assumption is that the selected control attributes effectively capture the differences between technical and non-technical summaries, and their combination is more effective than any single attribute.

### Mechanism 3
Human evaluation confirms the effectiveness of ATLAS in generating high-quality, lay summaries. Human judges rate the comprehensiveness, layness, and factuality of summaries generated by ATLAS and baseline models, demonstrating ATLAS's superiority in producing comprehensible, readable, and factually consistent summaries. The core assumption is that human evaluation is a reliable measure of summary quality, particularly for assessing the "layness" and comprehensibility of generated summaries.

## Foundational Learning

- Concept: Discretization of continuous attribute values
  - Why needed here: To convert continuous attribute values (e.g., readability scores) into discrete control tokens that can be used by the model to generate summaries with specific characteristics
  - Quick check question: How are the attribute values discretized, and what is the rationale behind the chosen discretization method?

- Concept: Fine-grained control in text generation
  - Why needed here: To enable the model to generate summaries tailored to different audience expertise levels by controlling multiple attributes that contribute to the overall "layness" of the generated summary
  - Quick check question: How do the fine-grained control tokens influence the generated summaries, and what is the relationship between the control tokens and the summary characteristics?

- Concept: Human evaluation in NLP
  - Why needed here: To assess the quality of generated summaries, particularly their comprehensibility, layness, and factuality, which are difficult to measure using automatic metrics alone
  - Quick check question: What are the evaluation criteria used in the human evaluation, and how are the human judges selected and trained to ensure reliable and consistent ratings?

## Architecture Onboarding

- Component map: Input Document -> Control Tokens (Length, Readability, Background Info, Content Entropy) -> BART-base Model -> Output Summary

- Critical path: Discretize attribute values into control tokens → Prepend control tokens to input document → Generate summary using BART-base → Evaluate summary using automatic and human metrics

- Design tradeoffs:
  - Fine-grained vs. coarse-grained control: Finer control allows for more tailored summaries but may require more training data and computational resources
  - Number of control attributes: More attributes can capture a wider range of summary characteristics but may also increase model complexity and training time

- Failure signatures:
  - Poor attribute discretization leading to ineffective control
  - Insufficient training data causing the model to fail to learn the influence of control tokens
  - Misalignment between control tokens and desired summary characteristics

- First 3 experiments:
  1. Ablation study: Remove each control attribute individually to assess its contribution to model performance
  2. Controllability analysis: Generate summaries using extreme values of control attributes to assess the model's ability to produce summaries with desired characteristics
  3. Human evaluation: Compare ATLAS-generated summaries with baseline models using human judges to assess the quality and effectiveness of the controlled summaries

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily focuses on biomedical domain data, raising questions about generalizability to other scientific domains
- Human evaluation relies on a relatively small sample size that may not fully represent the diversity of target lay audiences
- The mechanisms by which specific attributes capture layness versus technical content could benefit from deeper theoretical grounding

## Confidence

**High Confidence**: The core claim that ATLAS outperforms baseline models on both automatic and human evaluation metrics is well-supported by the experimental results.

**Medium Confidence**: The claim about fine-grained control tokens enabling tailored summaries is supported by controllability analysis, but the relationship between control token granularity and practical utility for end users remains less clear.

**Medium Confidence**: The assertion that multiple control attributes are more effective than single attributes is supported by ablation results, but the specific choice of four attributes over alternative sets is not thoroughly justified.

## Next Checks

1. Cross-domain validation: Test ATLAS on lay summarisation tasks outside the biomedical domain (e.g., physics, social science) to assess generalizability of the control attributes and overall approach.

2. User-centered evaluation: Conduct studies with actual target lay audiences to validate that attribute-controlled summaries genuinely improve comprehension and information retention compared to standard summaries.

3. Control attribute sensitivity analysis: Systematically vary the number and type of control attributes to determine whether the four chosen attributes represent an optimal set or whether alternative attribute combinations might yield superior performance.