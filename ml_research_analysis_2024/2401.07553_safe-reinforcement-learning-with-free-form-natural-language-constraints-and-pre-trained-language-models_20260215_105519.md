---
ver: rpa2
title: Safe Reinforcement Learning with Free-form Natural Language Constraints and
  Pre-Trained Language Models
arxiv_id: '2401.07553'
source_url: https://arxiv.org/abs/2401.07553
tags:
- cost
- constraints
- language
- natural
- constraint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses safe reinforcement learning with free-form
  natural language constraints, aiming to train agents that maximize rewards while
  obeying constraints without requiring ground-truth cost functions. The proposed
  method uses pre-trained language models, with a decoder LM to condense and clarify
  constraints and an encoder LM to extract semantic embeddings for cost prediction.
---

# Safe Reinforcement Learning with Free-form Natural Language Constraints and Pre-Trained Language Models

## Quick Facts
- arXiv ID: 2401.07553
- Source URL: https://arxiv.org/abs/2401.07553
- Authors: Xingzhou Lou; Junge Zhang; Ziyan Wang; Kaiqi Huang; Yali Du
- Reference count: 40
- Key outcome: Proposed method achieves safe policy learning without ground-truth costs using pre-trained LMs to handle free-form natural language constraints

## Executive Summary
This paper introduces a novel approach to safe reinforcement learning that uses pre-trained language models to handle free-form natural language constraints without requiring ground-truth cost functions. The method employs a decoder-based LM (GPT) to condense and clarify human constraints, followed by an encoder-based LM (BERT) with contrastive loss to extract semantic embeddings. By computing cosine similarity between constraint and observation embeddings, the approach predicts constraint violations and enables safe policy learning. Experiments on grid-world navigation and robot control tasks demonstrate strong performance in both reward maximization and constraint adherence.

## Method Summary
The proposed method addresses safe RL by using pre-trained language models to predict constraint violations from free-form natural language constraints. A decoder LM (GPT) condenses verbose human constraints into semantically clear instructions, which are then encoded by an encoder LM (BERT) fine-tuned with contrastive loss to produce embeddings. Text-based observations are similarly encoded, and cosine similarity between constraint and observation embeddings serves as a proxy for violation prediction. The RL agent is trained using PPO with Lagrange multiplier, replacing ground-truth costs with predicted costs from the LM-based system.

## Key Results
- Strong performance in both reward maximization and constraint adherence across grid-world and robot control tasks
- Outperforms direct prompting methods in safe policy learning without ground-truth costs
- Ablation studies confirm necessity of both decoder and encoder LMs for optimal performance

## Why This Works (Mechanism)

### Mechanism 1
Pre-trained decoder LMs condense free-form natural language constraints into semantically clear instructions that improve cost prediction accuracy. The decoder LM receives verbose human constraints and generates concise, unambiguous versions aligned with human values. Core assumption: Decoder LMs pre-trained on large corpora align with human semantic understanding. Evidence: Paper claims GPT aligns with human values but provides weak evidence. Break condition: If decoder LM generates semantically incorrect summaries, cost prediction accuracy drops.

### Mechanism 2
Encoder LM fine-tuned with contrastive loss learns to produce similar embeddings for semantically equivalent constraints, enabling robust cost prediction. The encoder LM is fine-tuned to pull embeddings of semantically similar constraints together and push apart dissimilar ones. Core assumption: Semantic similarity in natural language correlates with constraint violation likelihood. Evidence: Paper claims contrastive loss enables semantic similarity recognition but provides no direct evidence. Break condition: If contrastive loss is improperly balanced or training pairs are poorly chosen, encoder may fail to distinguish different constraints.

### Mechanism 3
Cosine similarity between constraint embeddings and text-based observation embeddings provides an effective proxy for constraint violation without requiring ground-truth cost functions. Both constraints and observations are transformed into text and encoded into embeddings. If cosine similarity exceeds a threshold, the model predicts a cost of 1 (violation). Core assumption: Semantic similarity between constraint text and observation text indicates constraint violation. Evidence: Paper presents this as insight but provides no direct evidence. Break condition: If text transformation is inaccurate or semantic space is misaligned, cosine similarity may not reflect actual violations.

## Foundational Learning

- Concept: Constrained Markov Decision Processes (CMDP)
  - Why needed here: Safe RL frameworks require understanding how to incorporate constraints alongside rewards in the optimization objective.
  - Quick check question: What is the key difference between MDP and CMDP formulation in terms of optimization objectives?

- Concept: Contrastive learning for semantic similarity
  - Why needed here: The method relies on contrastive loss to ensure the encoder LM produces meaningful embeddings that capture constraint semantics.
  - Quick check question: How does contrastive loss help in aligning embeddings of semantically similar constraints?

- Concept: Text-to-embedding transformation using pre-trained LMs
  - Why needed here: Both constraints and observations need to be converted to embeddings for similarity comparison, requiring understanding of how LMs encode semantic meaning.
  - Quick check question: Why is BERT particularly suited for encoding natural language constraints into semantic embeddings?

## Architecture Onboarding

- Component map: Decoder LM (GPT) → Text condensation → Encoder LM (BERT) with contrastive loss → Constraint embeddings; Text-based observation transformation → Observation embeddings → Cosine similarity computation → Cost prediction → RL policy update with predicted costs
- Critical path: Constraint → GPT condensation → BERT encoding → Observation encoding → Cosine similarity → Cost prediction → Policy learning. Any failure in this path breaks safe policy learning.
- Design tradeoffs: Using pre-trained LMs enables handling free-form constraints without ground-truth costs but introduces dependency on LM quality and may be computationally expensive compared to simpler approaches.
- Failure signatures: Poor cost prediction accuracy (high false positives/negatives), policy learning collapse when predicted costs are unreliable, or degraded performance when LM API access is limited.
- First 3 experiments:
  1. Verify GPT can condense a diverse set of free-form constraints into consistent, semantically equivalent forms.
  2. Test BERT with contrastive loss on constraint pairs to ensure semantically similar constraints have high cosine similarity.
  3. Validate the full pipeline on a simple grid-world with synthetic observations to confirm cost prediction accuracy before integrating with RL training.

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of decoder LM affect the performance of the proposed method in terms of safety and reward? The paper uses GPT-3.5 but does not explore other decoder LMs or compare their performance. This remains unresolved because the paper lacks ablation studies or comparisons of different decoder LMs. What evidence would resolve it: Conducting experiments with different decoder LMs (e.g., GPT-4, Llama 2) and comparing their performance would provide insights into the impact of the choice of decoder LM.

### Open Question 2
How does the proposed method scale to more complex environments with a larger number of constraints and objects? The paper evaluates on two environments with limited constraints and objects without exploring performance in more complex settings. This remains unresolved because the paper doesn't provide experiments or analysis in environments with more constraints and objects. What evidence would resolve it: Evaluating the method in more complex environments with a larger number of constraints and objects would provide insights into its scalability.

### Open Question 3
How does the proposed method handle novel or unseen natural language constraints that were not encountered during training? The paper mentions the method can handle novel constraints but doesn't provide evaluation or analysis of performance on unseen constraints. This remains unresolved because the paper lacks experiments or analysis on novel or unseen constraints. What evidence would resolve it: Evaluating the method on novel or unseen natural language constraints would provide insights into its ability to generalize.

## Limitations
- Core assumption that semantic similarity directly correlates with constraint violations lacks rigorous validation
- Dependency on quality of pre-trained language models introduces potential brittleness
- Computational overhead from using multiple large language models may limit practical deployment

## Confidence

**Confidence: Low** - The decoder LM constraint condensation mechanism relies heavily on the assumption that GPT aligns with human values, but lacks empirical validation across diverse domains.

**Confidence: Medium** - The encoder LM contrastive learning approach is well-grounded in NLP literature, but its effectiveness for safe RL cost prediction lacks direct evidence and optimal configuration details.

**Confidence: Low** - The fundamental insight that cosine similarity predicts constraint violations is presented without rigorous validation, particularly regarding false positive rates in semantically similar but non-violating cases.

## Next Checks

1. **Constraint condensation quality test**: Systematically evaluate GPT's constraint summaries against human-annotated ground truth for semantic equivalence across diverse constraint types to verify the condensation mechanism actually improves clarity.

2. **Contrastive loss sensitivity analysis**: Test how different training pair selections and loss weightings affect the encoder LM's ability to distinguish semantically similar vs. dissimilar constraints, identifying optimal training configurations.

3. **Semantic similarity correlation validation**: Conduct controlled experiments where semantically similar constraint-observation pairs are deliberately constructed to NOT represent actual violations, measuring false positive rates to assess the fundamental assumption about semantic similarity.