---
ver: rpa2
title: Towards Better Graph-based Cross-document Relation Extraction via Non-bridge
  Entity Enhancement and Prediction Debiasing
arxiv_id: '2406.16529'
source_url: https://arxiv.org/abs/2406.16529
tags:
- entity
- entities
- text
- relation
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses cross-document relation extraction (CoRE),\
  \ focusing on predicting relationships between target entities located in different\
  \ documents. The authors propose a graph-based model that integrates non-bridge\
  \ entities\u2014those co-occurring with only one target entity\u2014to strengthen\
  \ semantic associations between target entities, especially when bridge entities\
  \ are absent."
---

# Towards Better Graph-based Cross-document Relation Extraction via Non-bridge Entity Enhancement and Prediction Debiasing

## Quick Facts
- arXiv ID: 2406.16529
- Source URL: https://arxiv.org/abs/2406.16529
- Reference count: 8
- Ranks first in official CodRED leaderboard since December 2023

## Executive Summary
This paper addresses cross-document relation extraction (CoRE) by proposing a graph-based model that integrates non-bridge entities and prediction debiasing. The model constructs a unified entity graph using co-occurrence and semantic-related edges, encodes it with a Graph Recurrent Network (GRN), and applies a cross-path entity relation attention mechanism. To handle the predominance of NA instances in CodRED, the authors introduce a debiasing strategy that combines two distributions: one trained on non-NA instances and another derived from masking important non-target entities. Experiments show state-of-the-art performance on CodRED under closed and open settings, achieving 66.23% and 55.87% AUC respectively.

## Method Summary
The model constructs a unified entity graph for each document bag, including target entities, bridge entities, non-bridge entities, co-occurrence edges, and semantic-related edges (cosine similarity >0.6). A BERT encoder initializes entity representations, followed by GRN encoding to capture interdependency among connected nodes. Cross-path entity relation attention captures connections across text paths, and an MLP classifier predicts relation distributions. During inference, a debiasing strategy combines two distributions (yrela and ybias) to counteract NA class dominance. The model is trained using AdamW optimizer with a learning rate of 3e-5.

## Key Results
- Achieves 66.23% AUC on CodRED closed setting, ranking first in official leaderboard
- Achieves 55.87% AUC on CodRED open setting, ranking first in official leaderboard
- Outperforms baselines including GPT-3.5-turbo and fine-tuned InstructUIE
- Demonstrates effectiveness of non-bridge entity enhancement and prediction debiasing

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Non-bridge entities provide useful semantic associations between target entities when bridge entities are absent.
- **Mechanism:** Non-bridge entities co-occur with only one target entity but can be semantically related to non-bridge entities co-occurring with the other target entity. By connecting these via semantic-related edges, the model captures implicit relations that bridge entities would normally provide.
- **Core assumption:** Semantic relatedness between non-bridge entities implies semantic association between their respective target entities.
- **Evidence anchors:**
  - [abstract]: "these studies ignore the non-bridge entities, each of which co-occurs with only one target entity and offers the semantic association between target entities for relation prediction"
  - [section 1]: "Thus, it is difficult to correctly predict their relation due to the absence of bridge entities. If non-target entities 'Russian radio station' and 'Russian Soviet Republic' are semantically related, then there may also be some relationship between the target entities 'Europa Plus' and 'Soviet Union'"
- **Break condition:** If semantic relatedness between non-bridge entities does not imply target entity relation (e.g., when non-bridge entities are coincidentally related but targets are unrelated).

### Mechanism 2
- **Claim:** The prediction debiasing strategy calibrates the model to better predict non-NA relations by counteracting the NA class dominance.
- **Mechanism:** Two distributions are combined: yrela (trained on non-NA instances only, avoiding NA bias) and ybias (obtained by masking important non-target entities, making it more biased toward NA). Their difference is used to adjust the original prediction.
- **Core assumption:** The debiased distribution (ybias) is more NA-biased than the original, and yrela better captures non-NA relations.
- **Evidence anchors:**
  - [abstract]: "the commonly-used dataset–CodRED contains substantial NA instances, leading to the prediction bias during inference"
  - [section 2.4]: "Compared with the original prediction distribution, this distribution is more biased and thus can be used for debiasing in a manner of subtraction"
- **Break condition:** If the importance masking does not effectively create a more NA-biased distribution, or if yrela overfits to non-NA instances.

### Mechanism 3
- **Claim:** Graph Recurrent Network (GRN) effectively captures interdependency among connected nodes through recurrent gating mechanisms.
- **Mechanism:** GRN updates node representations by gathering context from neighboring nodes and applying GRU-based gating to control information flow, enabling iterative refinement of entity representations.
- **Core assumption:** The graph structure (co-occurrence and semantic-related edges) contains meaningful dependencies that GRN can capture through iterative message passing.
- **Evidence anchors:**
  - [section 2.2.2]: "it updates node representations using recurrent gating mechanisms, and thus has been widely used in many NLP tasks"
  - [section 2.2.1]: "we utilize Graph Recurrent Network (GRN) to encode this graph, where the interdependency among connected nodes is captured based on the recurrent gating mechanism"
- **Break condition:** If the graph structure is too noisy or sparse, or if GRU gates fail to capture relevant dependencies.

## Foundational Learning

- **Concept:** Graph neural networks and message passing
  - Why needed here: The model constructs a unified entity graph and uses GRN to encode it, requiring understanding of how information propagates through graph structures.
  - Quick check question: How does a Graph Recurrent Network differ from a standard Graph Convolutional Network in terms of information propagation?

- **Concept:** Cross-document relation extraction task
  - Why needed here: The paper addresses predicting relations between entities in different documents, which differs from sentence-level or document-level RE and requires specific handling of bridge and non-bridge entities.
  - Quick check question: What distinguishes cross-document RE from document-level RE, and why is the bridge entity concept important?

- **Concept:** Prediction debiasing techniques
  - Why needed here: The model introduces a debiasing strategy to counteract NA class dominance, requiring understanding of how to calibrate predictions using auxiliary distributions.
  - Quick check question: How does combining a non-NA-focused distribution with a more biased distribution help debias predictions?

## Architecture Onboarding

- **Component map:** BERT Encoder -> Entity Graph Construction -> GRN Encoder -> Cross-path Entity Relation Attention -> Classifier -> Prediction Debiasing
- **Critical path:**
  1. Input documents → BERT encoding → entity representations
  2. Entity graph construction → GRN encoding → refined entity representations
  3. Cross-path attention → relation representations → classifier → predictions
  4. Debiasing step (inference only) → calibrated predictions
- **Design tradeoffs:**
  - Using GRN vs. simpler GNN: GRN may capture longer-range dependencies but is more complex
  - Including non-bridge entities: Increases graph size and complexity but provides useful information
  - Debiasing strategy: Adds inference-time computation but improves non-NA prediction
- **Failure signatures:**
  - Poor performance on text paths without bridge entities: May indicate non-bridge entities not providing useful associations
  - Overfitting to NA instances despite debiasing: May indicate yrela or ybias not effectively counteracting bias
  - Degraded performance with larger graphs: May indicate GRN not scaling well or graph construction introducing noise
- **First 3 experiments:**
  1. **Ablation: Remove non-bridge entities** - Verify their contribution to performance, especially on text paths without bridge entities
  2. **Ablation: Remove debiasing strategy** - Confirm it improves non-NA prediction and overall AUC
  3. **Hyperparameter sweep: Similarity threshold η for semantic-related edges** - Find optimal threshold for connecting non-bridge entities

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the proposed prediction debiasing strategy perform on datasets with different class imbalance ratios compared to the CodRED dataset?
  - Basis in paper: The authors introduce a prediction debiasing strategy to mitigate the bias caused by the predominance of NA instances in the CodRED dataset, but they do not test its effectiveness on datasets with different class imbalance ratios.
  - Why unresolved: The effectiveness of the debiasing strategy might be specific to the CodRED dataset and may not generalize well to other datasets with different class imbalance ratios.
  - What evidence would resolve it: Conducting experiments on multiple datasets with varying class imbalance ratios and comparing the performance of the debiasing strategy to the baseline model.

- **Open Question 2:** What is the impact of dynamically adjusting the importance score of nodes in the graph on the model's performance?
  - Basis in paper: The authors mention that one limitation of their work is relying on the attention score to measure the importance of nodes in the graph without considering dynamically adjusting the importance score.
  - Why unresolved: Dynamically adjusting the importance score could potentially improve the model's performance by giving more weight to important nodes and less weight to less important ones.
  - What evidence would resolve it: Implementing a method to dynamically adjust the importance score and comparing its performance to the baseline model.

- **Open Question 3:** How does the model's performance change when using different edge connection methods in the entity-based graph?
  - Basis in paper: The authors mention that the edges in the entity-based graph are connected using a heuristic method, which may overlook useful information.
  - Why unresolved: The current edge connection method might not capture all relevant relationships between entities, and using a different method could potentially improve the model's performance.
  - What evidence would resolve it: Experimenting with different edge connection methods and comparing their performance to the baseline model.

## Limitations
- The paper does not provide detailed ablation studies on the semantic-related edge construction process
- The debiasing mechanism is only applied during inference, with no analysis of its impact on training dynamics
- The model's scalability to larger document collections is not evaluated

## Confidence
- Non-bridge entity enhancement mechanism: Medium
- Prediction debiasing effectiveness: Medium
- GRN's contribution to performance: High

## Next Checks
1. Conduct ablation study on semantic-related edge threshold (η) to determine sensitivity and optimal value
2. Test debiasing strategy on a balanced subset of CodRED to isolate its effect from class imbalance
3. Evaluate model performance on document bags with varying numbers of text paths to assess scalability