---
ver: rpa2
title: Auto FAQ Generation
arxiv_id: '2405.13006'
source_url: https://arxiv.org/abs/2405.13006
tags:
- questions
- question
- generation
- text
- sentences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study presents AutoFAQ, a system that generates Frequently
  Asked Questions (FAQs) from philosophical texts using a pipeline of text summarization,
  sentence ranking via TextRank, and question generation. The system uses BERT-based
  summarization to identify salient sentences, ranks them using cosine similarity
  and PageRank, and generates questions using transformer-based models.
---

# Auto FAQ Generation

## Quick Facts
- arXiv ID: 2405.13006
- Source URL: https://arxiv.org/abs/2405.13006
- Reference count: 25
- The system generates contextually relevant FAQs from philosophical texts with average grammar ratings of 4.38/5 and 71.28% meaningful questions.

## Executive Summary
AutoFAQ is a system that generates Frequently Asked Questions (FAQs) from philosophical texts using a pipeline of text summarization, sentence ranking via TextRank, and question generation. The system processes Stanford Encyclopedia of Philosophy articles through BERT-based summarization to identify salient sentences, ranks them using cosine similarity and PageRank, and generates questions using transformer-based models. Four heuristics filter invalid questions based on grammar, WH-word usage, named entities, and QA system validation. Human evaluation of 100 questions across four philosophical articles showed strong grammar quality but some vagueness due to lack of co-reference resolution.

## Method Summary
The AutoFAQ pipeline begins with HTML scraping of philosophical articles from the Stanford Encyclopedia of Philosophy, followed by section and paragraph splitting. BERT-based summarization identifies salient paragraphs, which are split into sentences and ranked using TextRank based on cosine similarity and PageRank. The top 100 ranked sentences are then used to generate questions via transformer-based models trained on SQuAD. Four heuristics filter the generated questions: grammar checking, WH-word count verification, named entity presence, and QA system validation. Questions failing two or more heuristics are removed before human evaluation.

## Key Results
- Generated questions received average grammar ratings of 4.38/5 and answerability ratings of 3.81/5
- 71.28% of questions were rated as meaningful by human evaluators
- Major weakness identified was lack of proper co-reference resolution leading to vague questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Summarization identifies key concepts that serve as proxy answers to aggregated FAQs.
- Mechanism: BERT-based summarization extracts salient paragraphs, then TextRank ranks sentences by salience using cosine similarity and PageRank.
- Core assumption: Salient sentences from a document represent the answers most likely needed by readers.
- Evidence anchors:
  - [abstract] "We suppose that salient sentences from a given document serve as a good proxy for the answers to an aggregated set of FAQs from readers."
  - [section] "We use existing text summarization, sentence ranking via the TextRank algorithm, and question generation tools to create an initial set of questions and answers."
  - [corpus] Weak: corpus contains only 25 related papers with average FMR=0.588, no direct evidence linking summarization quality to FAQ generation.
- Break condition: If salient sentences miss core concepts or include irrelevant details, generated questions will not align with reader needs.

### Mechanism 2
- Claim: Ranking sentences by salience ensures questions focus on most important content.
- Mechanism: TextRank computes sentence similarity matrix using GloVe embeddings, builds graph, applies PageRank to rank sentences, selects top 100.
- Core assumption: Higher-ranked sentences contain more central ideas worth questioning.
- Evidence anchors:
  - [abstract] "We use existing text summarization, sentence ranking via the TextRank algorithm, and question generation tools..."
  - [section] "The input to this module is a summarized article which is split into sentences. ... A graph is formed with sentences as Vertices and Similarity Score as Edges. ... This is input to a Page Rank algorithm and the sentences are ranked to form the summary."
  - [corpus] Weak: no corpus evidence directly measuring sentence ranking impact on FAQ quality.
- Break condition: If PageRank converges poorly or similarity measures are noisy, ranking may elevate unimportant sentences.

### Mechanism 3
- Claim: Heuristics filter out invalid questions, improving overall quality.
- Mechanism: Four heuristics—QA system validation, grammar check, WH-word count, named entity presence—remove questions failing at least two criteria.
- Core assumption: Simple structural and grammatical checks effectively eliminate poor questions.
- Evidence anchors:
  - [abstract] "Finally, we apply some heuristics to filter out invalid questions."
  - [section] "The other three heuristics are to check for decent grammar if the question has two WH words and checking if a named entity is present. If the questions fail any two heuristics among them, then it is filtered out."
  - [corpus] Weak: corpus contains no evidence on heuristic effectiveness; only mentions related FAQ generation work.
- Break condition: If heuristics are too strict, they may discard valid but unconventional questions; if too lax, they may allow low-quality questions.

## Foundational Learning

- Concept: Text summarization (extractive and abstractive)
  - Why needed here: Summarization reduces document size and surfaces key content that becomes FAQ answers.
  - Quick check question: Can you explain the difference between extractive and abstractive summarization and why both are used here?

- Concept: PageRank and graph-based ranking
  - Why needed here: PageRank on sentence similarity graph ranks sentences by importance for question generation.
  - Quick check question: How does PageRank assign scores to sentences in a similarity graph?

- Concept: Named entity recognition (NER)
  - Why needed here: NER heuristic ensures questions contain factual entities, improving answerability.
  - Quick check question: What is the role of NER in filtering generated questions?

## Architecture Onboarding

- Component map:
  HTML scraper -> Section/paragraph splitter -> BERT summarizer -> TextRank ranker -> Question generator -> Heuristics filter -> Human evaluation

- Critical path: Summarization -> Sentence ranking -> Question generation -> Heuristic filtering

- Design tradeoffs:
  - Using only top 100 sentences limits coverage but reduces computation
  - Multiple questions per sentence increases variety but may produce duplicates
  - Strict heuristics improve quality but may discard valid questions

- Failure signatures:
  - Low grammar ratings -> possible grammar checker misconfiguration
  - Low answerability ratings -> summarization may miss key content or ranking may be incorrect
  - Low meaningfulness ratings -> questions may be too generic or lack co-reference resolution

- First 3 experiments:
  1. Run pipeline on a single SEP article; inspect intermediate outputs (summarized paragraphs, ranked sentences)
  2. Vary the number of top sentences used (e.g., 50 vs 100) and measure impact on question quality
  3. Toggle each heuristic on/off individually to measure their individual impact on filtered question quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the system perform with articles from different domains (e.g., scientific, legal, or news articles) compared to philosophical texts?
- Basis in paper: [explicit] The paper mentions using philosophical documents from the Stanford Encyclopedia of Philosophy and notes that these articles have high reading complexity (Flesch Reading Ease score of ~50).
- Why unresolved: The system was only evaluated on philosophical texts, so its performance on other domains remains untested.
- What evidence would resolve it: Running the AutoFAQ system on articles from different domains and comparing the quality of generated questions (grammar, answerability, meaningfulness) to the results obtained with philosophical texts.

### Open Question 2
- Question: Would incorporating coreference resolution improve the quality of generated questions and reduce vagueness?
- Basis in paper: [inferred] The paper mentions that a major weakness found in many questions was the lack of proper coreference resolution, which led to vague questions.
- Why unresolved: The system did not implement coreference resolution, so the impact on question quality is unknown.
- What evidence would resolve it: Implementing coreference resolution in the AutoFAQ pipeline and comparing the quality of generated questions (grammar, answerability, meaningfulness) to the original results.

### Open Question 3
- Question: How would the system perform with different question generation models or fine-tuning approaches?
- Basis in paper: [explicit] The paper used a specific question generation model (Kriangchaivech et al., 2019) and mentions that the model was trained on the SQuAD dataset.
- Why unresolved: The paper only tested one question generation model and fine-tuning approach, so the impact of using different models or approaches is unknown.
- What evidence would resolve it: Testing the AutoFAQ system with different question generation models or fine-tuning approaches and comparing the quality of generated questions (grammar, answerability, meaningfulness) to the original results.

## Limitations

- Evaluation based on small sample (100 questions across 4 articles) making statistical significance unclear
- System lacks co-reference resolution leading to vague questions that human evaluators noted
- Filtering heuristics may be overly restrictive and discard valid questions
- External validation through related work is minimal with only 25 cited papers

## Confidence

- **High confidence**: The pipeline architecture is clearly specified and implemented. The BERT-based summarization and TextRank ranking mechanisms are well-established methods. Human evaluation results are explicitly reported.
- **Medium confidence**: The quality metrics (grammar 4.38/5, answerability 3.81/5, meaningfulness 71.28%) are reasonable but based on limited samples. The filtering heuristics improve quality but their exact thresholds are unspecified.
- **Low confidence**: External validation through related work is minimal. No error analysis of specific failure modes is provided. The impact of individual components on final quality is not systematically measured.

## Next Checks

1. Conduct statistical significance testing on the human evaluation results using larger samples across more articles to verify the reported quality metrics are not due to chance.
2. Implement co-reference resolution in the pipeline and measure the impact on question vagueness scores and answerability ratings through controlled experiments.
3. Perform ablation studies by systematically disabling each heuristic and measuring the tradeoff between quantity and quality of generated questions.