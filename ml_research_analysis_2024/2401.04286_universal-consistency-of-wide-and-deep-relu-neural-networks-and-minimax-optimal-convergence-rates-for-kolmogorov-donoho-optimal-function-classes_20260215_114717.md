---
ver: rpa2
title: Universal Consistency of Wide and Deep ReLU Neural Networks and Minimax Optimal
  Convergence Rates for Kolmogorov-Donoho Optimal Function Classes
arxiv_id: '2401.04286'
source_url: https://arxiv.org/abs/2401.04286
tags:
- neural
- classi
- function
- cation
- risk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proves that deep ReLU neural networks trained on logistic
  loss achieve universal consistency for classification, extending prior results that
  only covered shallow networks. The authors also establish conditions under which
  such networks attain minimax optimal convergence rates for a broad class of probability
  measures, including classical smoothness spaces.
---

# Universal Consistency of Wide and Deep ReLU Neural Networks and Minimax Optimal Convergence Rates for Kolmogorov-Donoho Optimal Function Classes

## Quick Facts
- arXiv ID: 2401.04286
- Source URL: https://arxiv.org/abs/2401.04286
- Authors: Hyunouk Ko; Xiaoming Huo
- Reference count: 4
- Primary result: Deep ReLU networks trained on logistic loss achieve universal consistency for classification, extending prior shallow network results

## Executive Summary
This paper establishes universal consistency and minimax optimal convergence rates for wide and deep ReLU neural networks in binary classification. The authors prove that overparameterized networks with sufficient width or depth can interpolate training data while maintaining strong theoretical guarantees. They extend existing consistency results beyond shallow networks and characterize conditions under which networks achieve optimal rates for broad function classes, including classical smoothness spaces. The work bridges practical empirical observations about overparameterized networks with rigorous learning theory by showing how interpolation can coexist with generalization.

## Method Summary
The paper analyzes deep ReLU neural networks trained via empirical risk minimization of logistic loss for binary classification. The key methodological approach involves two main components: (1) proving universal consistency by showing that sufficiently wide or deep networks can shatter any finite dataset while the excess risk converges to zero, and (2) establishing minimax optimal convergence rates by leveraging the connection between classification risk and Lp approximation error through the regression function. The analysis assumes the regression function belongs to a function class with known Kolmogorov-Donoho optimal exponent, allowing precise characterization of approximation power. The empirical risk minimizer is shown to achieve zero classification error on training data when the network is sufficiently overparameterized, leading to consistency guarantees.

## Key Results
- Deep ReLU networks achieve universal consistency by interpolating training data while maintaining generalization
- Excess classification risk decays at the same rate as Lp approximation error of the regression function
- Networks attain minimax optimal convergence rates for function classes with finite Kolmogorov-Donoho optimal exponent
- Results extend from shallow to deep networks and cover broader function classes than previous work

## Why This Works (Mechanism)

### Mechanism 1
Wide and deep ReLU networks can achieve universal consistency by interpolating training data while maintaining generalization. The networks have sufficient width and depth to shatter any finite dataset, meaning they can perfectly classify the training points. Despite this interpolation, the excess risk still converges to zero because the classification risk of the interpolating classifier converges to the Bayes risk.

### Mechanism 2
The excess classification risk decays at the same rate as the Lp approximation error of the regression function. By comparing the classification risk of a plug-in classifier based on a function fn with the Bayes risk, the excess risk is bounded by a constant times the Lp risk of the approximation error. This allows uniform convergence rates to be derived from approximation theory.

### Mechanism 3
The empirical risk minimizer of the logistic loss also minimizes the classification risk when the network can achieve zero classification error on the training set. Since there exists a network in the class that achieves zero classification error on the n training points, the empirical risk minimizer of the logistic loss will also achieve zero classification error.

## Foundational Learning

- **Concept: Kolmogorov-Donoho optimal exponent**
  - Why needed here: Quantifies the approximation power of neural networks for a function class, determining the rate at which approximation error decays with network complexity
  - Quick check question: If a function class has Kolmogorov-Donoho optimal exponent γ*, what is the decay rate of the L2 approximation error when using neural networks with connectivity M?

- **Concept: Tsybakov noise condition**
  - Why needed here: Provides a regularity condition on the regression function that allows tighter bounds on the excess classification risk in terms of the Lp approximation error
  - Quick check question: What is the form of the Tsybakov noise condition, and how does the parameter α affect the convergence rate?

- **Concept: Empirical process theory and localized complexity**
  - Why needed here: Provides tools to control the estimation error of the empirical risk minimizer, ensuring that network complexity is appropriate for the sample size
  - Quick check question: How does the VC-dimension of the network class relate to the sample size in controlling the estimation error?

## Architecture Onboarding

- **Component map**: Neural network architecture (width, depth, connectivity) -> Activation function (ReLU/periodic) -> Loss function (logistic) -> Function class F (with known Kolmogorov-Donoho optimal exponent) -> Excess risk bounds
- **Critical path**: 1) Choose function class F with known approximation properties. 2) Select network architecture with appropriate width/depth/connectivity. 3) Train network using empirical risk minimization of logistic loss. 4) Analyze excess risk using approximation and estimation error decomposition
- **Design tradeoffs**: Wider networks can approximate more complex functions but may have higher estimation error. Deeper networks can approximate certain functions more efficiently but may be harder to train. The choice of activation function affects approximation power and convergence rates
- **Failure signatures**: If excess risk doesn't decay as expected, it could be due to insufficient network complexity, poor approximation properties of chosen function class, or violation of Tsybakov noise condition
- **First 3 experiments**:
  1. Verify that wide enough network can achieve zero classification error on small dataset
  2. Measure L2 approximation error of network for known function from class F
  3. Train network on synthetic dataset and measure excess classification risk as function of sample size

## Open Questions the Paper Calls Out

### Open Question 1
Under what conditions does the empirical risk minimizer of logistic loss over deep ReLU networks achieve the exact minimax rate (not just optimal up to logarithmic factors)? The paper establishes logarithmic gaps in convergence rates between proposed methods and true minimax rates, but does not characterize when these gaps can be closed.

### Open Question 2
Can the universal consistency result be extended to other loss functions beyond logistic loss? The paper only proves universal consistency for logistic loss specifically, though it notes the property could extend to other losses.

### Open Question 3
How do the convergence rates change when the marginal distribution of X violates Assumption 6.2 (bounded density)? The paper assumes bounded density throughout but does not analyze what happens when this condition fails.

### Open Question 4
Can the Kolmogorov-Donoho optimal exponent framework be extended to function classes that are not optimally representable by neural networks? The paper focuses on function classes with γ* > 0 that are optimally representable by neural networks, but does not address other classes.

## Limitations
- The weight growth function π(n) must be sufficiently large for interpolation, but optimal scaling remains unclear
- Results assume regression function belongs to classes with known Kolmogorov-Donoho optimal exponents
- While paper shows existence of consistent classifiers, it doesn't address practical training dynamics or generalization beyond consistency

## Confidence
- **High confidence**: Universal consistency results - follows directly from interpolation capability
- **Medium confidence**: Minimax optimality results - depends on precise approximation power characterization
- **Medium confidence**: Connection to Tsybakov noise condition - requires specific regularity assumptions on η

## Next Checks
1. Empirical verification that ReLU networks with W(S_n) ≥ n can indeed shatter arbitrary n-point datasets
2. Systematic study of how different weight growth functions π(n) affect convergence rates
3. Testing the tightness of the connection between Kolmogorov-Donoho optimal exponents and actual network approximation performance on standard function classes