---
ver: rpa2
title: Modality-Agnostic fMRI Decoding of Vision and Language
arxiv_id: '2403.11771'
source_url: https://arxiv.org/abs/2403.11771
tags:
- language
- decoders
- gyrus
- vision
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a new large-scale fMRI dataset (~8,500 trials
  per subject) of people watching both images and text descriptions of such images,
  enabling the development of modality-agnostic decoders. These decoders can predict
  which stimulus a subject is seeing, irrespective of the modality (image or text).
---

# Modality-Agnostic fMRI Decoding of Vision and Language

## Quick Facts
- arXiv ID: 2403.11771
- Source URL: https://arxiv.org/abs/2403.11771
- Authors: Mitja Nikolaus, Milad Mozafari, Nicholas Asher, Leila Reddy, Rufin VanRullen
- Reference count: 40
- Primary result: Modality-agnostic fMRI decoders can predict which stimulus a subject is seeing (image or text) with performance matching or exceeding modality-specific decoders.

## Executive Summary
This study introduces a large-scale fMRI dataset (~8,500 trials per subject) where participants view both images and text descriptions of those images. Using this dataset, researchers developed modality-agnostic decoders that can identify which stimulus a subject is perceiving regardless of whether it's visual or linguistic. The key finding is that these decoders perform as well as or better than decoders trained separately for each modality, challenging the assumption that specialized decoders are necessary for different sensory inputs.

## Method Summary
The researchers collected fMRI data from three participants who viewed 1,024 unique image-caption pairs across 4-6 scanning sessions. Each pair was presented twice - once as an image and once as text. Participants performed a one-back matching task during scanning. For decoding, brain activity was mapped onto stimulus representations from various vision, language, and multimodal models including CLIP, LLaMA, and BERT. Three types of decoders were compared: modality-specific (trained separately for images and text), modality-agnostic (trained on both), and linear (simpler baseline).

## Key Results
- Modality-agnostic decoders achieved performance equal to or better than modality-specific decoders across all tested models
- Unimodal representations (from vision or language models) performed as well as multimodal representations for modality-agnostic decoding
- High-level visual (temporal) regions showed strong decoding performance for both image and text stimuli, suggesting amodal semantic processing

## Why This Works (Mechanism)
The success of modality-agnostic decoding suggests that high-level brain regions encode semantic content in an amodal format, independent of the sensory input channel. When we process either an image or its textual description, similar semantic representations are activated in temporal cortex regions. This allows decoders trained on combined data to capture these shared representations effectively. The finding that unimodal models can match multimodal ones suggests that the brain's semantic representations may be sufficiently captured by single-modality models when decoding from high-level regions.

## Foundational Learning
- **fMRI decoding basics**: Understanding how brain activity patterns can be mapped to stimulus features - needed to grasp the fundamental approach of predicting what someone is seeing from brain scans
- **Representational similarity analysis**: The method of comparing brain activity patterns to model representations - needed to understand how different brain regions encode different types of information
- **Multimodal learning**: How models integrate information from different sensory modalities - needed to contextualize why combining image and text processing is theoretically valuable
- **Searchlight analysis**: A voxel-wise approach to identify localized decoding regions - needed to understand the planned future work for more precise localization of amodal representations
- **One-back matching task**: The cognitive task used during fMRI scanning - needed to understand the experimental paradigm and its potential influence on results
- **ROI-based vs. searchlight analysis**: Different approaches to defining brain regions for analysis - needed to understand the current limitations and planned improvements

## Architecture Onboarding

**Component Map:**
Preprocessing -> Feature Extraction -> Decoding Model -> Performance Evaluation

**Critical Path:**
fMRI preprocessing → ROI selection → Model representation extraction → Decoder training → Cross-validation → Performance comparison

**Design Tradeoffs:**
The study chose a one-back matching task over passive viewing, trading ecological validity for controlled attentional engagement. They opted for broad anatomical ROIs rather than fine-grained searchlight analysis, prioritizing statistical power over spatial precision. The decision to use publicly available model representations rather than training custom models balanced computational efficiency with representational flexibility.

**Failure Signatures:**
Poor decoding performance would indicate either insufficient signal-to-noise ratio in the fMRI data, inadequate representational alignment between brain activity and model features, or that the semantic content is not encoded in an amodal format in the tested brain regions. Performance differences between modalities would suggest modality-specific processing dominates in certain regions.

**First Experiments:**
1. Train modality-agnostic decoder using only high-level visual (temporal) ROIs to test if these regions alone can support cross-modal decoding
2. Compare decoding performance using representations from increasingly large vision and language models to test scaling effects
3. Evaluate decoder performance when trained on combined image-caption pairs versus separate presentations to test integration requirements

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does the performance of modality-agnostic decoders depend on the specific task used during fMRI data collection (one-back matching vs. passive viewing)?
- Basis in paper: [inferred] The current study uses a one-back matching task, but previous studies used passive viewing. The paper notes this is a limitation and plans to investigate in future work.
- Why unresolved: The study only collected fMRI data using a one-back matching task, so there is no direct comparison to passive viewing data.
- What evidence would resolve it: Collecting fMRI data from the same subjects during both one-back matching and passive viewing tasks, then training and evaluating modality-agnostic decoders on each dataset to compare performance.

### Open Question 2
- Question: Are there specific brain regions that are particularly important for modality-agnostic decoding, and how do they differ from regions important for modality-specific decoding?
- Basis in paper: [explicit] The study found that high-level visual (temporal) regions perform well for both image and caption decoding, suggesting they contain "amodal" representations. However, a more fine-grained searchlight analysis was planned for future work.
- Why unresolved: The current ROI-based analysis used broad anatomical regions, which may not capture the specific locations of "amodal" representations.
- What evidence would resolve it: Conducting a searchlight-based analysis to identify specific voxels or small regions where modality-agnostic decoders outperform modality-specific ones, potentially revealing localized "amodal" areas.

### Open Question 3
- Question: Do multimodal representations from models trained on larger and more diverse datasets provide better performance for modality-agnostic decoding compared to unimodal representations?
- Basis in paper: [explicit] The study found that multimodal representations did not outperform unimodal language model representations for modality-agnostic decoding. However, it notes that previous studies used smaller language models, which could explain the discrepancy.
- Why unresolved: The study only tested a limited set of multimodal models and compared them to a range of unimodal models. It's possible that larger, more diverse multimodal models could perform better.
- What evidence would resolve it: Training and evaluating modality-agnostic decoders using multimodal representations from models trained on larger, more diverse datasets (e.g., CLIP trained on 400M image-text pairs) and comparing their performance to unimodal representations from similarly large models.

## Limitations
- The study relies on a single specialized dataset with only 3 subjects, limiting generalizability to broader populations
- Only one experimental paradigm (one-back matching task) was used, leaving open questions about task-dependence
- Potential confounds from stimulus modality-specific differences in attention or task engagement were not controlled for

## Confidence
- **Medium**: The claim that modality-agnostic decoders perform as well as (or better than) modality-specific decoders is based on a single dataset and may not hold across different fMRI datasets or experimental paradigms
- **Medium**: The finding that unimodal representations can match multimodal ones for decoding has medium confidence due to limited exploration of model diversity and lack of comparisons with other unimodal architectures
- **High**: The observation about high-level visual regions performing well on both stimulus types has high confidence as it aligns with established neuroscience literature on the role of temporal cortex in multimodal semantic processing

## Next Checks
1. Replicate the modality-agnostic decoding performance on independent fMRI datasets with different subjects and stimulus sets
2. Test the decoders' robustness to variations in stimulus complexity, presentation duration, and task demands
3. Compare performance when using alternative unimodal models (e.g., CLIP-based vision models or non-transformer language models) to ensure the results are not model-specific