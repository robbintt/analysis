---
ver: rpa2
title: Sharpness-Aware Minimization for Evolutionary Feature Construction in Regression
arxiv_id: '2405.06869'
source_url: https://arxiv.org/abs/2405.06869
tags:
- uni00000013
- uni00000048
- sharpness
- uni00000051
- uni00000003
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a sharpness-aware minimization framework
  for genetic programming-based evolutionary feature construction to address overfitting.
  The approach estimates the sharpness of symbolic models by perturbing the semantics/outputs
  of subtrees and optimizing both cross-validation loss and estimated sharpness.
---

# Sharpness-Aware Minimization for Evolutionary Feature Construction in Regression

## Quick Facts
- **arXiv ID**: 2405.06869
- **Source URL**: https://arxiv.org/abs/2405.06869
- **Reference count**: 40
- **Primary result**: Sharpness-aware minimization improves genetic programming feature construction, outperforming standard GP and six complexity measurement methods on 58 real-world regression datasets.

## Executive Summary
This paper introduces a sharpness-aware minimization (SAM) framework for genetic programming-based evolutionary feature construction to address overfitting in regression tasks. The approach estimates sharpness by perturbing subtree semantics and optimizes both cross-validation loss and estimated sharpness. Key innovations include a sharpness reduction layer to stabilize constructed features and bounded prediction to constrain predictions within training data ranges. Experimental results demonstrate superior performance compared to standard GP and state-of-the-art complexity measurement methods, with ensemble versions showing improved results against nine fine-tuned machine learning algorithms.

## Method Summary
The proposed method implements sharpness-aware minimization in genetic programming by estimating sharpness through semantic perturbations of subtrees, optimizing both cross-validation loss and estimated sharpness. A sharpness reduction layer stores subtree values during training and replaces test values with the nearest training data values using binary search to stabilize semantics. Bounded prediction constrains outputs within the range of training data labels. The approach also includes an ensemble learning strategy that maintains an archive of top-performing individuals for final predictions.

## Key Results
- Outperforms standard genetic programming and six state-of-the-art complexity measurement methods on 58 real-world regression datasets
- Sharpness reduction layer improves R2 by more than 0.01 on 14 datasets compared to not using the layer
- Ensemble version demonstrates superior performance compared to nine fine-tuned machine learning and symbolic regression algorithms
- Gaussian noise shows best default performance for sharpness estimation, though cross-validation may be needed to determine optimal noise type

## Why This Works (Mechanism)

### Mechanism 1
SAM optimizes GP-based feature construction by minimizing both cross-validation loss and estimated sharpness, leading to improved generalization on unseen data. The method perturbs subtree semantics and estimates sharpness based on maximum loss increase over multiple perturbation rounds, encouraging features with smooth loss landscapes that are robust to small perturbations.

### Mechanism 2
The sharpness reduction layer stabilizes constructed features by storing subtree values during training and replacing test values with the nearest training data values using binary search. This denoises corrupted results and makes the model more robust to unseen data by avoiding abrupt changes in feature semantics near training samples.

### Mechanism 3
Bounded prediction constrains predictions within the range of training data, assuming unseen data follows the same distribution as training data. During training, minimum and maximum target values are recorded and predictions are clipped to this range during testing, preventing unreasonable extrapolation on unseen data.

## Foundational Learning

- **Concept: Genetic Programming (GP)**
  - Why needed here: GP is the core technique used for evolutionary feature construction. Understanding GP is essential to grasp the proposed sharpness-aware minimization framework.
  - Quick check question: What is the main difference between GP and traditional parameter optimization techniques like gradient descent?

- **Concept: Sharpness-Aware Minimization (SAM)**
  - Why needed here: SAM is the key technique proposed to control overfitting in GP-based feature construction. Understanding SAM and its relationship to PAC-Bayesian theory is crucial.
  - Quick check question: How does SAM differ from traditional regularization techniques like L1/L2 regularization?

- **Concept: Sharpness Estimation**
  - Why needed here: Sharpness estimation is a critical component of the SAM framework. Understanding how sharpness is estimated in GP context is necessary for implementation.
  - Quick check question: Why is it challenging to estimate sharpness in GP compared to neural networks, and how does the proposed semantic perturbation method address this challenge?

## Architecture Onboarding

- **Component map**: GP-based feature construction -> Sharpness estimation -> Sharpness reduction layer -> Bounded prediction -> Ensemble learning (optional)
- **Critical path**: GP-based feature construction → Sharpness estimation → Sharpness reduction layer → Bounded prediction → Ensemble learning (optional)
- **Design tradeoffs**:
  - Accuracy vs. interpretability: Ensemble learning improves accuracy but reduces interpretability
  - Training time vs. generalization: Sharpness estimation and reduction layer increase training time but improve generalization
  - Complexity vs. robustness: Bounded prediction simplifies the model but may introduce bias
- **Failure signatures**:
  - Poor generalization indicates issues with sharpness estimation or reduction layer
  - High training time suggests inefficiencies in sharpness estimation or reduction layer implementation
  - Biased predictions may be caused by bounded prediction strategy or sharpness reduction layer
- **First 3 experiments**:
  1. Implement GP-based feature construction without sharpness estimation to establish baseline
  2. Add sharpness estimation to baseline and compare generalization performance
  3. Implement sharpness reduction layer and evaluate impact on generalization and training time

## Open Questions the Paper Calls Out

### Open Question 1
How does sharpness-aware minimization perform in the presence of highly noisy labels compared to other regularization techniques? While the paper shows SAM-GP outperforms standard GP and parsimony pressure on 31 and 21 datasets respectively in noisy conditions, it lacks comprehensive comparison with other regularization techniques specifically designed for noisy labels like label smoothing or robust loss functions.

### Open Question 2
What is the optimal ensemble size for SAM-EGP in terms of balancing performance and computational cost? The paper finds ensemble size of 100 yields best results but difference between 30 and 100 is not significant, without detailed analysis of trade-off between ensemble size, performance, and computational cost or exploration of greedy ensemble selection techniques.

### Open Question 3
How does the choice of noise type (Gaussian, uniform, Laplacian) affect SAM-GP's performance across different datasets or problem domains? While Gaussian noise is identified as good default choice, the paper lacks comprehensive analysis of how noise type impacts performance across diverse datasets to guide optimal noise type selection for specific problems.

## Limitations
- Sharpness estimation method using semantic perturbations may not capture all aspects of model robustness for complex GP structures
- Approach relies heavily on assumption that minimizing sharpness in semantic space correlates with improved generalization, with limited empirical validation beyond R² metrics
- Bounded prediction strategy may introduce bias when unseen data distribution significantly differs from training data

## Confidence

- **High confidence**: Effectiveness of sharpness reduction layer in stabilizing feature semantics and improving generalization, supported by direct experimental evidence showing R² improvements on 14 datasets
- **Medium confidence**: Overall superiority of proposed method over baseline GP and state-of-the-art complexity measurement techniques, though comparison could benefit from more diverse baseline methods
- **Medium confidence**: Bounded prediction strategy's effectiveness, with limited evidence suggesting it doesn't significantly impact training performance while potentially preventing unreasonable extrapolation

## Next Checks

1. Conduct ablation studies systematically removing each component (sharpness estimation, reduction layer, bounded prediction) to quantify individual contributions to overall performance

2. Test framework on datasets with known distribution shifts to evaluate whether bounded prediction assumption holds and identify potential failure modes

3. Implement cross-validation with varying perturbation magnitudes and sharpness reduction layer parameters to establish sensitivity analysis and identify optimal hyperparameter ranges