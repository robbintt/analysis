---
ver: rpa2
title: Unlocking Memorization in Large Language Models with Dynamic Soft Prompting
arxiv_id: '2409.13853'
source_url: https://arxiv.org/abs/2409.13853
tags:
- prompt
- uni00000013
- uni00000018
- memorization
- uni00000011
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the security risk posed by memorization in
  large language models (LLMs), where sensitive training data can be extracted, leading
  to privacy breaches and copyright infringement. The authors propose a novel method
  using dynamic, prefix-dependent soft prompts to estimate LLM memorization.
---

# Unlocking Memorization in Large Language Models with Dynamic Soft Prompting

## Quick Facts
- **arXiv ID:** 2409.13853
- **Source URL:** https://arxiv.org/abs/2409.13853
- **Reference count:** 14
- **Primary result:** Dynamic soft prompts achieve up to 112.75% relative improvement over baseline for text generation memorization extraction

## Executive Summary
This paper addresses the critical security risk of memorization in large language models, where sensitive training data can be extracted through carefully crafted prompts. The authors propose a novel approach using dynamic, prefix-dependent soft prompts to estimate and extract memorized data more effectively than existing methods. By training a transformer-based generator to produce prompts that adapt to input prefixes, the method demonstrates significant improvements in discoverable memorization rate across multiple model scales and datasets, achieving up to 112.75% relative improvement for text generation tasks.

## Method Summary
The method trains a transformer-based generator to produce dynamic soft prompts conditioned on input prefixes. The generator takes prefix tokens, maps them to a fixed-length representation using a naive mapping function, then generates a soft prompt that varies with the prefix content. This dynamic prompt is concatenated with prefix embeddings and fed into a frozen target LLM. Training uses aligned causal language modeling loss computed only on suffix tokens, with identity block initialization (zero matrices) to ensure stable training. The approach is evaluated on multiple pretrained models (GPT-Neo, Pythia, StarCoderBase) across text and code generation tasks using Exact Extraction Rate and Fractional Extraction Rate metrics.

## Key Results
- Dynamic soft prompts achieve maximum relative improvement of 112.75% over vanilla baseline for text generation tasks
- Code generation tasks show 32.26% relative improvement over baseline
- Performance gains observed across multiple model scales (125M to 7B parameters)
- Dynamic prompts consistently outperform constant prompts across all evaluated datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic soft prompts adapt to input changes, improving memorization extraction
- Mechanism: The generator produces prompts that vary with prefix content, allowing better alignment with memorized patterns
- Core assumption: Input-dependent prompts provide more relevant conditioning than static ones
- Evidence anchors:
  - [abstract] "Our approach involves training a transformer-based generator to produce soft prompts that adapt to changes in input, thereby enabling more accurate extraction of memorized data."
  - [section 3.3] "Since o depends on the prefix token p, it can adapt to the change in p."
  - [corpus] Weak evidence; no direct citations found on input-dependent prompt adaptation for memorization extraction
- Break condition: If the generator fails to produce meaningful variations, dynamic prompts revert to constant prompt performance

### Mechanism 2
- Claim: Identity block initialization prevents model collapse and aligns latent spaces
- Mechanism: Zero-initializing output matrices in transformer blocks ensures initial identity mapping, keeping prompt embeddings close to input embeddings
- Core assumption: Initializing generator blocks as identity mappings enables stable training convergence
- Evidence anchors:
  - [section 3.4] "To achieve this, the tokenizer and embedding layer of the generator gω should be initialized with those of the target LLM fθ... the identity block can be built by initializing WO and W3 as zero matrices, such that y = x."
  - [corpus] No corpus evidence found on identity block initialization for soft prompt generators
- Break condition: If gradients vanish or the mapping drifts far from identity, training may diverge

### Mechanism 3
- Claim: Aligned causal language modeling loss focuses optimization on suffix token prediction
- Mechanism: Loss is computed only over suffix tokens, ensuring generator optimization targets memorization-relevant outputs
- Core assumption: Training only on suffix tokens better reflects memorization extraction performance
- Evidence anchors:
  - [section 3.2] "the aligned CLM loss only focuses on the token prediction at the position of suffix tokens, which aligns with the definition of discoverable memorization."
  - [corpus] No corpus evidence found on CLM loss alignment for memorization extraction
- Break condition: If prefix tokens dominate loss contributions, suffix memorization may be underestimated

## Foundational Learning

- Concept: Memorization in large language models
  - Why needed here: Understanding memorization is essential to grasp why dynamic prompts improve extraction
  - Quick check question: What is discoverable memorization rate and how does it relate to data extraction?

- Concept: Soft prompt tuning
  - Why needed here: The method builds on soft prompt tuning principles but adds dynamic generation
  - Quick check question: How does soft prompt tuning differ from traditional fine-tuning in LLMs?

- Concept: Identity block initialization
  - Why needed here: Identity initialization ensures stable training of the prompt generator
  - Quick check question: Why would random initialization of transformer blocks lead to model collapse?

## Architecture Onboarding

- Component map:
  - Prefix → Naive mapping (m(·)) → Transformer-based generator → Dynamic soft prompt → Concatenation with prefix → Target LLM → Output tokens
  - Training loop: Generator forward pass → Aligned CLM loss computation → Generator parameter update (LLM frozen)

- Critical path:
  1. Map prefix tokens to fixed-length representation
  2. Generate dynamic soft prompt conditioned on prefix
  3. Concatenate prompt with prefix embeddings
  4. Feed through frozen LLM
  5. Compute aligned CLM loss on suffix tokens
  6. Update generator parameters

- Design tradeoffs:
  - Dynamic vs constant prompts: Dynamic provides adaptation but increases complexity
  - Identity initialization vs random: Identity ensures stability but may slow initial learning
  - Prompt length flexibility vs computational cost: Longer prompts increase extraction potential but require more memory

- Failure signatures:
  - Generator produces near-zero gradients (identity block initialization issue)
  - Performance plateaus at constant prompt baseline (lack of input adaptation)
  - Loss divergence (mismatched embedding spaces)

- First 3 experiments:
  1. Compare dynamic vs constant prompt extraction rates on a small dataset
  2. Test identity block initialization impact on training stability
  3. Vary prompt length to identify saturation points in extraction performance

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the saturation phenomenon observed in the ablation study on prompt length impact the extraction of memorized data, and what are the underlying causes?
  - Basis in paper: [explicit] The paper mentions a saturation phenomenon when increasing the length of the prompt beyond a certain point, but the reasons for this saturation remain unknown.
  - Why unresolved: The paper does not provide a detailed explanation for why the performance improvement diminishes as the prompt length increases, leaving the underlying mechanisms unclear.
  - What evidence would resolve it: Further experiments analyzing the relationship between prompt length and model performance, along with theoretical analysis of the attention mechanism, could shed light on the saturation phenomenon.

- **Open Question 2:** How effective is the proposed method for measuring memorization in fine-tuned large language models, and how does it compare to its effectiveness in pre-trained models?
  - Basis in paper: [inferred] The paper focuses on memorization in pre-trained models and does not explore its effectiveness in fine-tuned models, which also exhibit memorization as mentioned in the limitations.
  - Why unresolved: The paper does not provide any experimental results or analysis on the application of the method to fine-tuned models, leaving the effectiveness in this context unexplored.
  - What evidence would resolve it: Conducting experiments on various fine-tuning tasks and comparing the results with those obtained from pre-trained models would provide insights into the method's effectiveness in different scenarios.

- **Open Question 3:** What are the reasons for the observed discrepancy in improvement between exact extraction rate and fractional extraction rate, and how can the method be optimized for better performance on fractional extraction rate?
  - Basis in paper: [explicit] The paper notes that the improvement in fractional extraction rate is smaller and less robust compared to exact extraction rate, suggesting a potential area for improvement.
  - Why unresolved: The paper does not provide a detailed analysis of the reasons behind the discrepancy or propose specific strategies to enhance performance on fractional extraction rate.
  - What evidence would resolve it: Investigating the impact of different loss functions or optimization techniques on fractional extraction rate could help identify ways to improve the method's performance in this aspect.

## Limitations

- Identity block initialization efficacy lacks empirical validation through ablation studies
- Aligned CLM loss justification not supported by theoretical or comparative evidence
- Dynamic prompt improvements may not generalize across different model scales and domains

## Confidence

**High Confidence** (supported by clear experimental evidence):
- Dynamic soft prompts outperform static prompts in memorization extraction
- The proposed method achieves measurable improvements over baseline techniques
- The aligned CLM loss framework is correctly implemented and produces results

**Medium Confidence** (mechanism plausible but under-validated):
- Identity block initialization contributes to training stability
- Prefix-dependent adaptation provides meaningful performance gains
- The generator architecture is appropriate for the task

**Low Confidence** (claims not sufficiently supported):
- The theoretical justification for aligned CLM loss alignment
- Generalization of dynamic prompt benefits across different model scales
- Long-term stability of the training procedure

## Next Checks

1. **Ablation Study on Identity Initialization**: Train the generator with random initialization and compare training stability, convergence speed, and final performance against the identity-initialized version to quantify the contribution of this design choice.

2. **Cross-Domain Generalization Test**: Evaluate the dynamic prompt method on datasets from different domains (e.g., medical, legal, technical) and with different model architectures to assess whether the reported improvements transfer beyond the evaluated settings.

3. **Prompt Length Saturation Analysis**: Systematically vary prompt lengths beyond the tested ranges to identify whether performance improvements continue to scale or reach saturation points, providing insight into the optimal prompt length for memorization extraction.