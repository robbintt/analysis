---
ver: rpa2
title: Relative Value Biases in Large Language Models
arxiv_id: '2401.14530'
source_url: https://arxiv.org/abs/2401.14530
tags: []
core_contribution: This study tested whether large language models (LLMs) exhibit
  relative value biases in decision-making, similar to humans and animals. We adapted
  a bandit task where models learned to choose between options with different payoff
  distributions, then were tested on novel option pairings.
---

# Relative Value Biases in Large Language Models

## Quick Facts
- arXiv ID: 2401.14530
- Source URL: https://arxiv.org/abs/2401.14530
- Authors: William M. Hayes; Nicolas Yax; Stefano Palminteri
- Reference count: 0
- Primary result: LLMs exhibit context-dependent decision biases analogous to humans, preferring locally optimal options even when they have lower absolute payoffs

## Executive Summary
This study demonstrates that large language models exhibit relative value biases in decision-making similar to humans and animals. Using a bandit task adapted for LLMs, both GPT-4-Turbo and Llama-2-70B showed systematic preferences for locally optimal options within their respective contexts, even when these options had lower absolute payoffs. The bias was magnified when outcome comparisons were made explicit and eliminated when models were prompted to estimate expected outcomes. These findings reveal that LLMs can develop context-dependent decision biases through in-context learning, providing insights into potential mechanisms of value-based decision-making in AI systems.

## Method Summary
The study adapted a four-armed bandit task where LLMs learned to choose between options with different payoff distributions across four contexts. Each context contained a locally optimal (higher mean) and locally suboptimal (lower mean) option, with absolute payoffs varying across contexts. Models received 20 learning trials (five per context) with complete outcome histories, then were tested on 28 possible option pairs without feedback. The key manipulation tested whether models would prefer locally optimal options in transfer tests, revealing relative value biases. Additional conditions varied prompt structure to examine how explicit outcome comparisons and estimation instructions affected the bias.

## Key Results
- Both GPT-4-Turbo and Llama-2-70B exhibited relative value biases, preferring locally optimal options over higher-absolute-payoff options
- Bias magnitude increased when outcome comparisons were made explicit in prompts (Regret condition)
- Bias was eliminated when models were prompted to explicitly estimate expected outcomes
- GPT-4-Turbo showed no bias when contexts were presented separately rather than together

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs weight context-dependent comparisons over absolute numerical representations due to token prediction processes
- Mechanism: Embedding space forms relative distinctions between options in each context during learning, dominating choice behavior in transfer
- Core assumption: Models learn from statistical patterns rather than explicit arithmetic on numerical values
- Evidence anchors: Abstract and experimental results showing bias disappears with expected outcome estimation
- Break condition: Bias eliminated when explicitly prompted to estimate expected outcomes

### Mechanism 2
- Claim: Relative value bias emerges from processing structured outcome histories where comparisons are implicit in context pairing
- Mechanism: Models learn statistical associations between option pairs and their relative outcomes, persisting into transfer phase
- Core assumption: Models treat each context as distinct statistical pattern rather than extracting generalizable numerical relationships
- Evidence anchors: Abstract and experimental results showing Broken Contexts condition eliminates GPT-4-Turbo bias
- Break condition: Broken Contexts condition eliminates bias by separating outcome histories

### Mechanism 3
- Claim: Bias magnitude scales with explicitness of outcome comparisons in prompt structure
- Mechanism: Attention mechanisms amplify relative distinctions when prompted with explicit comparative language
- Core assumption: Attention mechanisms respond to explicit comparative language by reinforcing relative distinctions
- Evidence anchors: Abstract and experimental results showing Regret condition magnifies bias
- Break condition: Explicit expected outcome estimation overrides comparative processing

## Foundational Learning

- Concept: In-context learning
  - Why needed here: Study relies on LLMs learning from prompt examples rather than fine-tuning
  - Quick check question: How does in-context learning differ from traditional fine-tuning, and why is this distinction important for studying emergent behaviors?

- Concept: Attention mechanisms and contextual embeddings
  - Why needed here: Crucial for explaining why relative value representations dominate over absolute ones
  - Quick check question: What role do attention mechanisms play in shaping how LLMs represent and compare options within different contexts?

- Concept: Reinforcement learning concepts (bandit tasks, value estimation)
  - Why needed here: Experimental design uses bandit tasks adapted from human psychology
  - Quick check question: How does a bandit task structure create opportunities for relative value biases to emerge?

## Architecture Onboarding

- Component map: Prompt generation -> LLM interaction -> Outcome simulation -> Prompt update -> Response collection -> Statistical analysis
- Critical path: Prompt construction → LLM response → Outcome simulation → Prompt update → Response collection → Statistical analysis
- Design tradeoffs: Longer prompts ensure complete information but increase computational cost; shorter prompts reduce cost but risk omitting context
- Failure signatures: No learning trend during learning phase, no zig-zag pattern in transfer, responses not matching requested format
- First 3 experiments:
  1. Replicate baseline with different LLM architecture to test generalizability
  2. Test Broken Contexts condition with Llama-2-70B to confirm manipulation effects
  3. Test condition with explicit outcome comparisons throughout learning phase

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do relative value biases emerge in other types of decision-making tasks beyond the bandit task used in this study?
- Basis in paper: Authors suggest examining wider range of models and tasks
- Why unresolved: Study only tested one specific decision-making task
- What evidence would resolve it: Testing multiple LLMs on various decision-making tasks and comparing to human behavior

### Open Question 2
- Question: What are the underlying computational mechanisms that give rise to relative value bias in LLMs?
- Basis in paper: Authors suggest examining internals of models to understand emergent computational mechanisms
- Why unresolved: Study only examined behavioral outputs, not inner workings
- What evidence would resolve it: Analyzing internal representations at different learning phases

### Open Question 3
- Question: To what extent are relative value biases driven by specific model architecture versus training data patterns?
- Basis in paper: Authors note results replicated across two architectures but suggest examining wider range
- Why unresolved: Study only tested two model architectures
- What evidence would resolve it: Testing diverse LLMs with different architectures and training methods

## Limitations

- Limited generalizability due to artificial nature of payoff distributions and context pairings
- Only two model architectures tested, limiting claims about universality across LLM types
- Specific prompt constructions may create biases that wouldn't persist in more naturalistic decision contexts

## Confidence

- **High Confidence**: LLMs exhibit relative value biases when presented with structured outcome histories in paired contexts
- **Medium Confidence**: Bias magnitude scales with explicitness of outcome comparisons in prompts
- **Medium Confidence**: Bias elimination with expected outcome estimation suggests context-dependent rather than inherent mechanism

## Next Checks

1. Test whether relative value bias persists with continuous outcome values rather than discrete dollar amounts
2. Conduct cross-architecture validation using models with different attention mechanisms (Mamba, RWKV)
3. Implement transfer test with interleaved rather than blocked contexts during learning phase