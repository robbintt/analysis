---
ver: rpa2
title: 'CAMBranch: Contrastive Learning with Augmented MILPs for Branching'
arxiv_id: '2402.03647'
source_url: https://arxiv.org/abs/2402.03647
tags: []
core_contribution: The paper proposes CAMBranch, a framework for generating expert
  branching decisions in mixed integer linear programming (MILP) using contrastive
  learning with augmented MILPs. CAMBranch generates augmented MILPs by applying variable
  shifting to limited expert data from original MILPs.
---

# CAMBranch: Contrastive Learning with Augmented MILPs for Branching

## Quick Facts
- arXiv ID: 2402.03647
- Source URL: https://arxiv.org/abs/2402.03647
- Reference count: 40
- Primary result: CAMBranch achieves superior branching performance with only 10% of training data compared to state-of-the-art methods

## Executive Summary
CAMBranch introduces a novel framework for generating expert branching decisions in Mixed Integer Linear Programming (MILP) by combining contrastive learning with augmented MILPs. The approach generates augmented MILPs through variable shifting and leverages both original and augmented instances for imitation learning. By treating MILPs and their augmented counterparts as positive pairs in a contrastive learning framework, CAMBranch forces the model to learn invariant features while maintaining MILP structure. Experiments demonstrate that CAMBranch trained with only 10% of the data achieves superior performance on four combinatorial optimization problems compared to state-of-the-art methods.

## Method Summary
CAMBranch employs a contrastive learning framework that uses augmented MILPs to improve branching decision quality. The method generates augmented MILPs by applying variable shifts to the original problem's variables, then converts both MILPs and AMILPs into bipartite graph representations. A Graph Convolutional Neural Network processes these graphs, while contrastive learning enforces similarity between representations of corresponding MILP-AMILP pairs. The framework uses imitation learning via behavior cloning with cross-entropy loss, enhanced by contrastive learning with InfoNCE loss. The approach requires only limited expert data while achieving competitive performance through effective data augmentation and contrastive learning.

## Key Results
- CAMBranch trained with 10% of data outperforms GCNN trained with full dataset
- Outperforms state-of-the-art methods including PVR, RGNN, and NCF on 4 combinatorial optimization problems
- Achieves superior performance in terms of solving time, number of B&B nodes, and number of wins
- Contrastive learning component contributes 0.4% to 0.7% improvement in imitation learning accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Contrastive learning between MILPs and their AMILPs improves model performance by forcing the network to learn MILP-invariant features.
- **Mechanism:** By treating a MILP and its AMILP as positive pairs, the model learns representations that are robust to variable shifts while still capturing essential MILP structure.
- **Core assumption:** MILPs and their AMILPs share identical branching decisions, so their representations should be similar.
- **Evidence anchors:**
  - [abstract] "CAMBranch leverages both MILPs and AMILPs for imitation learning and employs contrastive learning to enhance the model's ability to capture MILP features, thereby improving the quality of branching decisions."
  - [section 3.3] "Within our proposed CAMBranch, we leverage this principle by viewing a MILP and its corresponding AMILP as positive pairs while considering the MILP and other AMILPs within the same batch as negative pairs."
- **Break condition:** If the variable shifting creates AMILPs with different branching decisions than the original MILP.

### Mechanism 2
- **Claim:** Data augmentation through variable shifting generates expert-labeled samples efficiently without expensive Strong Branching computations.
- **Mechanism:** Random shifts create new AMILPs that inherit expert branching decisions from the original MILP, multiplying the training data from limited expert samples.
- **Core assumption:** Theorem 3.1 proves that MILPs and their AMILPs have identical branching decisions.
- **Evidence anchors:**
  - [abstract] "CAMBranch generates Augmented MILPs (AMILPs) by applying variable shifting to limited expert data from their original MILPs."
  - [section 3.1] "This augmentation strategy enables the acquisition of a substantial number of labeled expert samples, even when expert data is limited."
- **Break condition:** If the variable shifting changes the problem structure significantly enough to affect branching decisions.

### Mechanism 3
- **Claim:** Using only 10% of expert data while achieving comparable performance to models trained on full datasets demonstrates the effectiveness of the augmentation and contrastive learning framework.
- **Mechanism:** The combination of data augmentation and contrastive learning allows the model to learn effectively from limited data by focusing on invariant features and generating additional training samples.
- **Core assumption:** The experimental results showing CAMBranch with 10% data performing comparably to GCNN with full data.
- **Evidence anchors:**
  - [abstract] "Experimental results demonstrate that CAMBranch, trained with only 10% of the complete dataset, exhibits superior performance."
  - [section 4.2.1] "CAMBranch, trained with 10% of the full training data, outperforms GCNN (10%), demonstrating that our proposed data augmentation and contrastive learning framework benefit the imitation learning process."
- **Break condition:** If the performance gap between CAMBranch with 10% data and GCNN with full data becomes too large.

## Foundational Learning

- **Concept:** Mixed Integer Linear Programming (MILP)
  - **Why needed here:** The paper is about improving branching strategies for MILP solvers.
  - **Quick check question:** What are the key components of an MILP formulation?

- **Concept:** Branch and Bound algorithm
  - **Why needed here:** The paper improves the branching component of the Branch and Bound algorithm.
  - **Quick check question:** How does the Branch and Bound algorithm use branching to solve MILPs?

- **Concept:** Graph Convolutional Networks (GCNs)
  - **Why needed here:** The paper uses GCNs to process the bipartite graph representation of MILPs.
  - **Quick check question:** How do GCNs process graph-structured data?

## Architecture Onboarding

- **Component map:** MILP bipartite graph and AMILP bipartite graph -> Graph Convolutional Neural Networks (GCNs) -> Contrastive learning module (InfoNCE loss) -> Variable selection probabilities

- **Critical path:**
  1. Generate AMILPs through variable shifting
  2. Convert MILPs and AMILPs to bipartite graphs
  3. Process graphs through GCNs
  4. Apply contrastive learning between positive pairs
  5. Output variable selection decisions

- **Design tradeoffs:**
  - Using contrastive learning adds computational overhead but improves performance with less data
  - Data augmentation through variable shifting may introduce noise but significantly increases training samples

- **Failure signatures:**
  - Poor performance on test data despite good training results (overfitting to augmented data)
  - Contrastive learning loss not decreasing (issues with positive/negative pair selection)
  - Slow convergence (learning rate too low or model capacity insufficient)

- **First 3 experiments:**
  1. Verify that variable shifting preserves branching decisions (Theorem 3.1)
  2. Test contrastive learning loss on synthetic data with known relationships
  3. Evaluate performance on a small subset of problems with varying data augmentation ratios

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- The framework relies on the assumption that variable shifting preserves branching decisions exactly, which may not hold for all problem types
- Performance depends heavily on the quality of expert data, even with augmentation
- Contrastive learning introduces additional computational overhead during training

## Confidence

- Mechanism 1 (Contrastive Learning Benefits): Medium - supported by theoretical framework and experimental results, but lacks extensive ablation studies
- Mechanism 2 (Data Augmentation Effectiveness): Medium - theoretically sound via Theorem 3.1, but empirical validation is limited
- Mechanism 3 (10% Data Performance): Medium - experimental results show promise but need more extensive testing across diverse problem types

## Next Checks

1. Test variable shifting across different MILP problem families to verify branching decision preservation
2. Conduct ablation studies comparing performance with and without contrastive learning across multiple problem types
3. Evaluate model generalization to unseen problem distributions beyond the training set