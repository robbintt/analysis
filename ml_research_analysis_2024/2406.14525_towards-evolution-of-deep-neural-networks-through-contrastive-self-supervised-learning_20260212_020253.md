---
ver: rpa2
title: Towards evolution of Deep Neural Networks through contrastive Self-Supervised
  learning
arxiv_id: '2406.14525'
source_url: https://arxiv.org/abs/2406.14525
tags:
- learning
- data
- supervised
- evolutionary
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates evolving deep neural networks using self-supervised
  learning to address the design complexity and data labeling challenges in deep learning.
  The proposed framework, EvoDeNSS, integrates neuroevolution with Barlow Twins algorithm
  to automatically evolve network architectures while reducing dependence on labeled
  data.
---

# Towards evolution of Deep Neural Networks through contrastive Self-Supervised learning

## Quick Facts
- arXiv ID: 2406.14525
- Source URL: https://arxiv.org/abs/2406.14525
- Reference count: 40
- Primary result: Self-supervised neuroevolution achieves competitive performance with reduced labeled data requirements

## Executive Summary
This work presents EvoDeNSS, a neuroevolutionary framework that combines deep neural network architecture evolution with contrastive self-supervised learning to address the challenges of manual DNN design and labeled data scarcity. The framework uses Barlow Twins for self-supervised pretraining and evaluates evolved architectures on CIFAR-10 under both supervised and self-supervised paradigms. Experiments demonstrate that self-supervised evolution produces more consistent network architectures across different data regimes and maintains competitive performance even with only 10% labeled data, offering a promising approach for reducing human intervention in deep learning model development.

## Method Summary
EvoDeNSS integrates neuroevolution with Barlow Twins self-supervised learning using a (1+λ)-ES strategy and Dynamic Structured Grammatical Evolution (DSGE) representation. Network architectures are evolved by optimizing both topology and learning paradigm, with individuals trained using either supervised learning or Barlow Twins pretext task on unlabelled data. The framework evaluates fitness on evolutionary test sets during evolution and final performance on the original test set after fine-tuning. Key components include a projector network for Barlow Twins, LARS optimizer, and standard data augmentations including padding, horizontal flip, color jitter, and grayscale transformations.

## Key Results
- Self-supervised evolution maintains competitive performance compared to supervised learning, particularly when labeled data is limited
- Evolved networks demonstrate resilience to data scarcity, achieving similar performance with 10% or 100% of labeled data in downstream tasks
- Structural analysis reveals self-supervised learning leads to more consistent network architectures regardless of labeled data availability
- Supervised learning produces more varied network structures compared to self-supervised evolution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neuroevolution automates the design of deep neural network architectures, reducing human effort and exploration time.
- Mechanism: The framework evolves network topologies using a grammar-based representation (DSGE), allowing mutation and variation operators to search the architecture space. Evolution uses a (1 + λ)-ES strategy with weight discarding to maintain diversity.
- Core assumption: Automated search via evolutionary algorithms can discover architectures as good as or better than human-designed ones when coupled with appropriate learning paradigms.
- Evidence anchors:
  - [abstract] "The proposed framework, EvoDeNSS, integrates neuroevolution with Barlow Twins algorithm to automatically evolve network architectures..."
  - [section III.A] "EvoDeNSS is a neuroevolutionary framework that can be considered as an extension of Fast-DENSER..."
  - [corpus] No direct evidence in corpus; this is foundational to NE literature.
- Break condition: If the search space is too large or poorly constrained, evolution may fail to converge to high-performing architectures, or performance may plateau early.

### Mechanism 2
- Claim: Self-supervised learning (SSL) reduces reliance on labeled data by learning representations from unlabelled inputs via pretext tasks.
- Mechanism: The framework uses Barlow Twins to maximize invariance and minimize redundancy between two augmented views of each image. Representations learned during pretext task are transferred to downstream classification with a small labeled subset.
- Core assumption: Representations learned without labels are useful for downstream tasks and retain generalization capability.
- Evidence anchors:
  - [abstract] "self-supervised learning has been used to leverage unlabelled data to learn representations."
  - [section III.B] "In order to train representations without using labels, we incorporated Barlow Twins algorithm..."
  - [corpus] Weak evidence; related works cite SSL for unlabeled data but not within NE context.
- Break condition: If the pretext task does not align well with the downstream task, representation quality may degrade, leading to poor transfer performance.

### Mechanism 3
- Claim: Combining neuroevolution with SSL creates architectures resilient to data scarcity, producing consistent structures regardless of labeled data availability.
- Mechanism: Evolution optimizes both architecture and learning paradigm; SSL-based evolution is less sensitive to the amount of labeled data, leading to stable architectures across data regimes.
- Core assumption: SSL-based training imposes a more consistent optimization pressure, reducing variability in evolved architectures.
- Evidence anchors:
  - [abstract] "Structural analysis reveals that self-supervised learning leads to more consistent network architectures regardless of labeled data availability..."
  - [section IV.B] "self-supervised evolution exhibits evolution curves that suggest that further improvements are still possible..."
  - [corpus] No corpus evidence for this claim; structural consistency is inferred from experimental results.
- Break condition: If SSL does not sufficiently constrain architecture search, or if downstream task performance varies widely, the claimed resilience may not hold.

## Foundational Learning

- Concept: Neuroevolution (NE) - use of evolutionary algorithms to automate design of neural network architectures.
  - Why needed here: NE removes the need for manual hyperparameter tuning and exhaustive architecture search, essential for automating DNN design.
  - Quick check question: What is the difference between neuroevolution and standard hyperparameter optimization?

- Concept: Self-Supervised Learning (SSL) - learning from unlabeled data by creating pretext tasks that generate pseudo-labels from the data itself.
  - Why needed here: SSL reduces dependence on costly labeled datasets, enabling model training with minimal supervision.
  - Quick check question: How does Barlow Twins encourage invariance and minimize redundancy in learned representations?

- Concept: Representation transfer - using features learned in a pretext task (SSL) for a downstream supervised task.
  - Why needed here: The evolved architectures are first trained on unlabelled data to learn useful features, then fine-tuned for the actual classification task using limited labels.
  - Quick check question: Why is it important to separate pretext and downstream training in SSL?

## Architecture Onboarding

- Component map:
  Evolutionary Engine (Fast-DENSER/EvoDeNSS) -> Evaluation Module -> Dataset Partitioning -> Grammar-based representation

- Critical path:
  1. Initialize population of network genotypes
  2. Map genotype to phenotype (trainable DNN)
  3. Evaluate fitness using chosen learning paradigm (supervised or SSL)
  4. Select parent, apply variation, generate next generation
  5. Repeat until convergence or max generations

- Design tradeoffs:
  - Evolve only feature extractor vs. whole network (including classifier): evolving only feature extractor reduces search space but may limit final performance
  - Use of fixed downstream layer: simplifies evolution but may restrict optimal architecture discovery
  - Weight-sharing vs. training from scratch: training from scratch ensures diversity but increases computation time

- Failure signatures:
  - Fitness plateaus early: likely search space too constrained or evolution stuck in local optima
  - High variance in best individual performance: insufficient diversity or unstable SSL training
  - Poor downstream accuracy despite high pretext task performance: weak transfer between pretext and downstream tasks

- First 3 experiments:
  1. Run EvoDeNSS with supervised learning on CIFAR-10, compare fitness evolution to baseline hand-designed CNN
  2. Run EvoDeNSS with Barlow Twins, vary labeled data percentage (10% vs 100%), observe fitness and structural consistency
  3. Modify grammar to include projector network evolution, evaluate impact on downstream accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of neuroevolution in self-supervised learning compare to traditional supervised learning approaches when the amount of labeled data is extremely limited (e.g., 1% or less)?
- Basis in paper: [explicit] The paper mentions that self-supervised evolution is resilient to data scarcity and can achieve similar performance to supervised learning with 10% labeled data.
- Why unresolved: The experiments only tested scenarios with 10% and 100% labeled data, leaving the performance at lower percentages unexplored.
- What evidence would resolve it: Conducting experiments with even smaller percentages of labeled data (e.g., 1%, 5%) and comparing the performance to supervised learning would provide insights into the robustness of self-supervised neuroevolution.

### Open Question 2
- Question: What specific architectural features or patterns emerge in the evolved networks when using self-supervised learning, and how do these differ from those evolved with supervised learning?
- Basis in paper: [explicit] The paper notes that self-supervised learning leads to more consistent network architectures regardless of labeled data availability, while supervised learning produces more varied structures.
- Why unresolved: The paper does not provide detailed analysis or visualization of the specific architectural features that distinguish self-supervised evolved networks from supervised ones.
- What evidence would resolve it: A detailed comparative analysis of the evolved network architectures, including layer types, connections, and activation functions, would reveal the distinctive features of self-supervised evolved networks.

### Open Question 3
- Question: How do different self-supervised learning algorithms, such as SimCLR or MoCo, perform when integrated with neuroevolution compared to Barlow Twins?
- Basis in paper: [explicit] The paper uses Barlow Twins as the self-supervised learning algorithm but does not explore other algorithms.
- Why unresolved: The paper focuses solely on Barlow Twins, leaving the performance of other self-supervised learning algorithms unexplored in the context of neuroevolution.
- What evidence would resolve it: Implementing and comparing the performance of various self-supervised learning algorithms (e.g., SimCLR, MoCo) with neuroevolution on the same dataset would highlight their relative strengths and weaknesses.

## Limitations
- Unknown grammar rules and projector network configuration prevent exact reproduction of the evolved architectures
- Experiments limited to CIFAR-10 dataset, requiring validation on diverse datasets and tasks
- No comparison with other self-supervised learning algorithms within the neuroevolution framework

## Confidence
- Mechanism 1 (neuroevolution automation): Medium - established principle but lacks direct corpus validation for this specific combination
- Mechanism 2 (SSL representation learning): Medium - well-supported by existing literature but weak evidence within NE context
- Mechanism 3 (resilience to data scarcity): Low - structural consistency claims inferred from experimental results without independent verification

## Next Checks
1. Implement EvoDeNSS with specified parameters and run evolutionary experiments across all four scenarios (supervised 100%, supervised 10%, BT 100%, BT 10%) to verify fitness evolution curves and structural consistency claims
2. Analyze evolved network architectures across data regimes to quantify structural similarity metrics and determine if SSL-based evolution truly produces more consistent designs
3. Test the evolved architectures on additional downstream tasks beyond CIFAR-10 classification to assess generalizability of learned representations