---
ver: rpa2
title: 'Exploring Boundary of GPT-4V on Marine Analysis: A Preliminary Case Study'
arxiv_id: '2401.02147'
source_url: https://arxiv.org/abs/2401.02147
tags:
- gpt-4v
- marine
- figure
- images
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the capabilities of GPT-4V for marine analysis,
  a domain requiring specialized knowledge. The researchers conducted a comprehensive
  evaluation of GPT-4V across various aspects of marine analysis, including object
  recognition, statistics, domain-specific question answering, cultural understanding,
  and advanced functions like coral coverage estimation and benthic composition analysis.
---

# Exploring Boundary of GPT-4V on Marine Analysis: A Preliminary Case Study

## Quick Facts
- **arXiv ID:** 2401.02147
- **Source URL:** https://arxiv.org/abs/2401.02147
- **Reference count:** 7
- **Primary result:** GPT-4V struggles with fine-grained marine object recognition and cannot reliably serve as a tool for marine analysis.

## Executive Summary
This study investigates GPT-4V's capabilities for marine analysis, revealing significant limitations despite some strengths. The researchers evaluated the model across multiple dimensions including object recognition, domain-specific question answering, and advanced functions like coral coverage estimation. While GPT-4V demonstrates strong OCR ability and can understand physical world knowledge, it falls short in critical areas such as fine-grained species identification and quantitative analysis tasks. The findings suggest that GPT-4V requires substantial development before it can serve as a reliable tool for marine professionals.

## Method Summary
The study conducted a comprehensive evaluation of GPT-4V using 300 diverse marine images from various sources including private collections, YouTube videos, and research articles. Researchers employed a range of prompts to test the model's capabilities in recognizing marine objects, answering domain-specific questions, understanding scientific figures, and performing advanced functions like coral coverage estimation. Results were compared against ground truth labels or human expert judgment, with all data and prompts made available at the GitHub repository https://github.com/hkust-vgd/Marine_GPT-4V_Eval.

## Key Results
- GPT-4V demonstrates strong OCR capability but struggles with fine-grained marine object recognition
- The model is easily misled by carefully crafted filenames and cannot accurately count objects in images
- GPT-4V fails to provide domain-specific information required by marine professionals and cannot perform accurate coral coverage estimation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** GPT-4V's strong OCR capability enables accurate extraction of text from scientific figures and tables in marine analysis.
- **Mechanism:** The model leverages its extensive pre-training on image-text pairs to recognize and parse text elements within visual inputs, even when embedded in complex layouts.
- **Core assumption:** OCR performance is robust across varied font styles, sizes, and backgrounds encountered in marine research figures.
- **Evidence anchors:**
  - [abstract] GPT-4V has "strong OCR ability" and "capacity to understand physical world knowledge"
  - [section] "GPT-4V has demonstrated a very strong OCR ability to extract text information from visual images"
  - [corpus] Weak - related papers focus on hallucination mitigation and retrieval, not OCR validation
- **Break condition:** OCR accuracy degrades with low image resolution, unusual fonts, or significant noise/artifacts.

### Mechanism 2
- **Claim:** GPT-4V's image-level understanding allows it to summarize relationships and detect events in marine imagery.
- **Mechanism:** By analyzing visual patterns and leveraging learned associations, the model generates coherent descriptions of ecological interactions or anomalous events.
- **Core assumption:** The training corpus includes sufficient examples of marine scenes and events to support generalization.
- **Evidence anchors:**
  - [abstract] GPT-4V has "robust visual-text comprehension capabilities and extensive knowledge"
  - [section] "GPT-4V has shown a promising potential to understand scenes from video and visual story generation"
  - [corpus] Weak - neighbor papers focus on hallucination and retrieval, not event detection
- **Break condition:** Complex or rare events not represented in training data lead to inaccurate or hallucinated descriptions.

### Mechanism 3
- **Claim:** GPT-4V can generate and execute Python code for quantitative marine analysis tasks like coral coverage estimation.
- **Mechanism:** The model translates visual observations into computational steps, leveraging its code generation ability to produce analysis scripts.
- **Core assumption:** Generated code is syntactically correct and produces reasonable numerical estimates based on visual input.
- **Evidence anchors:**
  - [abstract] GPT-4V "generates some computer vision processing codes for coral coverage estimation"
  - [section] "GPT-4V avoids directly outputting the coral coverage and instead attempts to generate some computer vision processing codes"
  - [corpus] Weak - no direct evidence in neighbor papers about code generation for marine tasks
- **Break condition:** Code execution fails due to undefined variables, incorrect logic, or incompatibility with input data.

## Foundational Learning

- **Concept:** Domain-specific knowledge gaps in large vision-language models
  - **Why needed here:** GPT-4V's limitations in fine-grained marine object recognition stem from insufficient training on marine-specific visual data and terminology.
  - **Quick check question:** Can GPT-4V distinguish between closely related coral species that differ in subtle morphological features?

- **Concept:** Multimodal model evaluation methodologies
  - **Why needed here:** Systematic assessment of GPT-4V across diverse marine tasks (perception, statistics, domain-specific QA) requires understanding of appropriate metrics and comparison protocols.
  - **Quick check question:** How does the study measure GPT-4V's performance on pairwise species comparison tasks?

- **Concept:** Prompt engineering and hallucination mitigation
  - **Why needed here:** GPT-4V's tendency to be misled by filenames and generate confabulated responses highlights the need for careful prompt design and validation strategies.
  - **Quick check question:** What experimental evidence shows GPT-4V relies on filename context rather than visual content?

## Architecture Onboarding

- **Component map:** Image input → Vision encoder → Multimodal embedding → Language model → Text output
- **Critical path:** Image input → Vision encoder → Multimodal embedding → Language model → Text output. Performance bottlenecks occur at the vision encoder when processing complex or low-quality images.
- **Design tradeoffs:** High model capacity enables broad generalization but introduces hallucination risks; fine-tuning on domain data could improve accuracy but reduce versatility.
- **Failure signatures:** Incorrect species identification, reliance on filename context, inability to count objects accurately, and generation of confabulated details are key failure modes.
- **First 3 experiments:**
  1. Test OCR accuracy on marine research figures with varied text complexity and layouts.
  2. Evaluate event detection performance on labeled marine video clips.
  3. Assess code generation quality for coral coverage estimation tasks with known ground truth.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can GPT-4V effectively recognize and classify a wide range of marine species under challenging conditions (e.g., low visibility, occlusion, camouflage)?
- **Basis in paper:** [explicit] The paper states that GPT-4V "has a poor ability to accurately recognize the visual elements under challenging conditions" and "struggles to recognize all the objects within the images and only lists very few common object categories."
- **Why unresolved:** The study tested GPT-4V on a diverse set of marine images, but the results show that it struggles with fine-grained marine object recognition and fails to accurately recognize marine objects that are not relatively common.
- **What evidence would resolve it:** A comprehensive evaluation of GPT-4V's performance on a large dataset of marine images with varying levels of difficulty and diverse species, using a robust evaluation metric that accounts for the complexity of the task.

### Open Question 2
- **Question:** Can GPT-4V accurately estimate coral coverage and benthic composition from visual images without relying on external tools or pre-trained data?
- **Basis in paper:** [explicit] The paper states that GPT-4V "nearly cannot achieve benthic composition statistics without utilizing an external professional analysis tool" and "avoids directly outputting the coral coverage and instead attempts to generate some computer vision processing codes."
- **Why unresolved:** The study tested GPT-4V on coral coverage estimation and benthic composition analysis, but the results show that it fails to provide accurate estimates and relies on external tools or pre-trained data for analysis.
- **What evidence would resolve it:** A comparison of GPT-4V's performance on coral coverage estimation and benthic composition analysis with and without the use of external tools or pre-trained data, using a dataset of visual images with ground truth labels for coral coverage and benthic composition.

### Open Question 3
- **Question:** Can GPT-4V effectively summarize relationships between marine creatures and detect events from visual images, such as predator-prey relationships, symbiosis, and abnormal behaviors?
- **Basis in paper:** [explicit] The paper states that GPT-4V "has shown a satisfactory ability to understand and describe some well-known relationships between recognized objects" but "generates totally irrelative responses" when it fails to recognize the marine objects accurately.
- **Why unresolved:** The study tested GPT-4V on relationship summarization and event detection, but the results show that it can only understand and describe some well-known relationships and fails to detect events when it fails to recognize the marine objects accurately.
- **What evidence would resolve it:** A comprehensive evaluation of GPT-4V's performance on relationship summarization and event detection using a dataset of visual images with ground truth labels for relationships and events, and a robust evaluation metric that accounts for the complexity of the task.

## Limitations

- The evaluation uses a relatively small sample size of 300 marine images, which may not fully represent marine ecosystem diversity.
- Ground truth labels generated by domain experts introduce potential subjectivity and variability in evaluation criteria.
- The study does not provide detailed information on specific prompts used for each evaluation task, affecting reproducibility.

## Confidence

*High Confidence:* GPT-4V's strong OCR ability and capacity to understand physical world knowledge are well-supported by experimental results and align with the model's general capabilities in processing visual information.

*Medium Confidence:* Conclusions about GPT-4V's limitations in fine-grained marine object recognition and susceptibility to filename manipulation are based on specific tasks and prompts, which may not generalize to all scenarios.

*Low Confidence:* The assertion that GPT-4V cannot serve as a reliable tool for marine analysis is a strong claim requiring further validation, as the evaluation covers limited tasks and may not capture the model's full potential.

## Next Checks

1. **Expand Dataset Diversity:** Evaluate GPT-4V's performance on a larger and more diverse set of marine images, including those from different geographic regions, seasons, and environmental conditions to assess robustness and generalizability.

2. **Prompt Variation Analysis:** Conduct experiments with different prompt formulations and variations to determine the impact on GPT-4V's performance and identify the most effective prompting strategies.

3. **Expert Review and Validation:** Engage a panel of marine experts to review and validate the evaluation criteria, ground truth labels, and overall methodology to ensure reliability and identify potential biases.