---
ver: rpa2
title: 'Evaluating Subword Tokenization: Alien Subword Composition and OOV Generalization
  Challenge'
arxiv_id: '2404.13292'
source_url: https://arxiv.org/abs/2404.13292
tags:
- word
- subword
- language
- alien
- umlabeller
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating subword tokenization
  methods, which often do not respect morpheme boundaries and can lead to suboptimal
  downstream performance. The authors propose a combined intrinsic-extrinsic evaluation
  framework for subword tokenization.
---

# Evaluating Subword Tokenization: Alien Subword Composition and OOV Generalization Challenge

## Quick Facts
- arXiv ID: 2404.13292
- Source URL: https://arxiv.org/abs/2404.13292
- Reference count: 19
- Key outcome: Proposes a combined intrinsic-extrinsic evaluation framework for subword tokenization, showing umLabeller achieves 98% accuracy and alien tokenization leads to poorer generalization than morphological tokenization.

## Executive Summary
This paper addresses the challenge of evaluating subword tokenization methods, which often do not respect morpheme boundaries and can lead to suboptimal downstream performance. The authors propose a combined intrinsic-extrinsic evaluation framework for subword tokenization. The intrinsic component is a new tool called UniMorph Labeller (umLabeller) that classifies subword tokenization as either morphological or alien. Extrinsic evaluation is performed using the Out-of-Vocabulary Generalization Challenge 1.0 benchmark, which consists of three newly specified downstream text classification tasks. The authors find that umLabeller has an accuracy of 98% and that alien tokenization leads to poorer generalizations compared to morphological tokenization for semantic compositionality of word meanings in all language models studied.

## Method Summary
The authors propose a combined intrinsic-extrinsic evaluation framework for subword tokenization. The intrinsic component is umLabeller, a Python tool that classifies subword compositions as vocab, morph, alien, or n/a based on morphological resources (UniMorph and compound morphology data). The extrinsic component is the OOV Generalization Challenge benchmark, consisting of three subtasks (WaD, WaM, WaW) designed to evaluate the impact of subword tokenization on downstream NLP tasks. The method involves generating morphological merge lists from UniMorph data, implementing the umLabeller algorithm, designing and implementing the benchmark tasks, and evaluating language models (BERT, RoBERTa, ALBERT, DeBERTa) on the benchmark.

## Key Results
- umLabeller achieves 98% accuracy in classifying subword compositions as morphological or alien
- Alien tokenization leads to poorer generalization than morphological tokenization for semantic compositionality of word meanings across all studied language models
- The OOV Generalization Challenge benchmark effectively evaluates the impact of subword tokenization on downstream NLP tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: umLabeller accurately classifies subword compositions by leveraging morphological resources (UniMorph and compound morphology data) to determine whether subwords are morphologically valid.
- Mechanism: The algorithm generates all possible morphological merges from UniMorph data, then checks if a given subword sequence matches these merges within a tolerance of one missing element. This allows it to distinguish between morphologically valid (morph) and invalid (alien) compositions.
- Core assumption: The morphological merge list comprehensively covers valid morphological combinations in English, and the tolerance of one missing element is sufficient to handle incomplete or altered roots.
- Evidence anchors:
  - [abstract]: "umLabeller has an accuracy of 98% and that, in all language models studied (including ALBERT, BERT, RoBERTa, and DeBERTa), alien tokenization leads to poorer generalizations compared to morphological tokenization for semantic compositionality of word meanings."
  - [section]: "The algorithm estimates Î¼(w, s), the number of morphological subwords in the subword sequence s by aligning all possible morphological merges of length n, retrieved from UniMorph."
  - [corpus]: Weak evidence. The corpus provides related papers but no direct evidence for umLabeller's mechanism.
- Break condition: If UniMorph data is incomplete or if the tolerance of one missing element is insufficient for certain morphological constructions, umLabeller's accuracy will decrease.

### Mechanism 2
- Claim: The OOV Generalization Challenge benchmark effectively evaluates the impact of subword tokenization on downstream NLP tasks by using out-of-vocabulary words and focusing on compositional and morphological generalization.
- Mechanism: The benchmark consists of three subtasks (WaD, WaM, WaW) that require models to generalize to unseen subwords, testing their ability to handle OOV words and understand morphological relationships. The tasks are designed to avoid context-based shortcuts, ensuring that performance reflects tokenization quality.
- Core assumption: OOV words and morphologically complex words are sufficient to reveal differences in tokenization methods' impact on model performance.
- Evidence anchors:
  - [abstract]: "Extrinsic evaluation, in turn, is performed via the Out-of-Vocabulary Generalization Challenge 1.0 benchmark, which consists of three newly specified downstream text classification tasks."
  - [section]: "To evaluate generalization abilities, words in the development and test splits are unseen on the training split."
  - [corpus]: Weak evidence. The corpus mentions related work on tokenization evaluation but does not provide direct evidence for the effectiveness of this specific benchmark.
- Break condition: If the benchmark tasks do not adequately capture the range of challenges posed by different tokenization methods, or if models can exploit other cues to achieve high performance, the benchmark will not accurately reflect tokenization quality.

### Mechanism 3
- Claim: Alien subword compositions lead to poorer generalization in language models compared to morphological compositions because they do not align with human understanding of word meaning composition.
- Mechanism: Language models trained on alien subword compositions struggle to compose meanings from individual subwords, leading to decreased performance on tasks requiring semantic understanding. Morphological compositions, on the other hand, allow models to leverage subword meanings to infer overall word meaning.
- Core assumption: Human understanding of word meaning composition is a valid benchmark for evaluating subword tokenization methods.
- Evidence anchors:
  - [abstract]: "Our empirical findings show that the accuracy of umLabeller is 98%, and that, in all language models studied (including ALBERT, BERT, RoBERTa, and DeBERTa), alien tokenization leads to poorer generalizations compared to morphological tokenization for semantic compositionality of word meanings."
  - [section]: "The empirical results in the experiments show that morphological composition supports better the semantic compositionality of OOV word meanings than alien composition."
  - [corpus]: Weak evidence. The corpus provides related work on tokenization but does not directly address the claim about alien vs. morphological compositions' impact on generalization.
- Break condition: If language models can learn to compose meanings from alien subword compositions through other means (e.g., contextual information), or if human understanding of word meaning composition is not a reliable benchmark, the claim will not hold.

## Foundational Learning

- Concept: Morphological segmentation
  - Why needed here: Understanding morphological segmentation is crucial for interpreting umLabeller's classification of subword compositions and the design of the OOV Generalization Challenge.
  - Quick check question: What is the difference between morphological segmentation and subword tokenization, and why is morphological segmentation considered more linguistically accurate?

- Concept: Out-of-vocabulary (OOV) words
  - Why needed here: OOV words are central to the OOV Generalization Challenge, as the benchmark aims to evaluate models' ability to handle unseen words and subwords.
  - Quick check question: How do different subword tokenization methods handle OOV words, and what are the potential consequences for downstream NLP tasks?

- Concept: Compositional semantics
  - Why needed here: Compositional semantics underlies the distinction between alien and morphological subword compositions and the rationale for evaluating their impact on model performance.
  - Quick check question: How does the concept of compositional semantics relate to the idea of alien vs. morphological subword compositions, and why is it important for evaluating tokenization methods?

## Architecture Onboarding

- Component map:
  - umLabeller: A Python tool that classifies subword compositions as vocab, morph, alien, or n/a based on morphological resources.
  - OOV Generalization Challenge: A benchmark consisting of three subtasks (WaD, WaM, WaW) designed to evaluate the impact of subword tokenization on downstream NLP tasks.
  - Morphological resources: UniMorph and compound morphology data used by umLabeller to determine morphological validity.
  - Language models: BERT, RoBERTa, ALBERT, and DeBERTa, which are evaluated on the OOV Generalization Challenge.

- Critical path:
  1. Generate morphological merge list from UniMorph data.
  2. Implement umLabeller algorithm to classify subword compositions.
  3. Design and implement the three subtasks of the OOV Generalization Challenge.
  4. Evaluate language models on the benchmark using umLabeller to categorize test instances.

- Design tradeoffs:
  - umLabeller's tolerance of one missing element vs. stricter morphological matching: The tolerance allows for incomplete or altered roots but may also lead to false positives.
  - Focus on English vs. multilingual support: The current implementation is limited to English but could be extended to other languages with available morphological resources.
  - Intrinsic vs. extrinsic evaluation: umLabeller provides intrinsic evaluation of tokenization quality, while the OOV Generalization Challenge offers extrinsic evaluation through downstream tasks.

- Failure signatures:
  - umLabeller: Incorrect classification of subword compositions, leading to misleading benchmark results.
  - OOV Generalization Challenge: Tasks that do not adequately capture the impact of tokenization on model performance, or tasks that can be solved without relying on subword composition understanding.
  - Language models: High performance on the benchmark despite poor tokenization quality, indicating that the benchmark is not sensitive enough to tokenization differences.

- First 3 experiments:
  1. Validate umLabeller's accuracy on a held-out set of subword compositions with known morphological validity.
  2. Compare the performance of different language models on the OOV Generalization Challenge to assess the impact of tokenization on generalization.
  3. Analyze the distribution of vocab, morph, and alien compositions in the test splits of the OOV Generalization Challenge to understand the types of subword compositions that models struggle with.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the accuracy of umLabeller be improved for languages with complex morphology beyond English?
- Basis in paper: [explicit] The paper notes that umLabeller's correctness and coverage depend on UniMorph data, which is primarily available for English. The authors welcome contributions to expand coverage through GitHub pull requests.
- Why unresolved: While umLabeller shows high accuracy for English, its performance for morphologically rich languages is untested. The paper does not provide any empirical results or methods for extending umLabeller to other languages.
- What evidence would resolve it: Empirical evaluations of umLabeller's accuracy across multiple languages with varying morphological complexity, along with proposed methods for adapting the tool to handle language-specific morphological phenomena.

### Open Question 2
- Question: What is the impact of different subword tokenization algorithms on the performance of downstream NLP tasks beyond the three tasks evaluated in the OOV Generalization Challenge?
- Basis in paper: [inferred] The paper evaluates the impact of tokenization on three specific downstream tasks (WaD, WaM, WaW) and compares it to the WiC task. However, it does not explore the effects on other common NLP tasks such as machine translation, sentiment analysis, or question answering.
- Why unresolved: The paper focuses on a limited set of tasks, leaving open the question of how tokenization choices might affect performance across the broader landscape of NLP applications.
- What evidence would resolve it: Extensive empirical studies evaluating various tokenization algorithms on a wide range of downstream NLP tasks, comparing performance metrics and identifying patterns or correlations between tokenization choices and task performance.

### Open Question 3
- Question: How can the trade-off between vocabulary size and morphological plausibility be optimized for different NLP applications?
- Basis in paper: [explicit] The paper presents statistics on BPE tokenizers with varying vocabulary sizes and shows that morphological plausibility peaks at 40,000-50,000 tokens before plateauing. However, it does not provide guidance on how to choose the optimal vocabulary size for specific NLP tasks or domains.
- Why unresolved: While the paper demonstrates the relationship between vocabulary size and morphological plausibility, it does not offer a systematic approach for determining the best vocabulary size for different NLP applications or discuss how this choice might impact other factors such as model size, computational efficiency, or task-specific performance.
- What evidence would resolve it: A comprehensive study exploring the effects of different vocabulary sizes on various NLP tasks, considering factors such as model performance, computational efficiency, and domain-specific requirements. This could include developing guidelines or algorithms for selecting optimal vocabulary sizes based on task characteristics and resource constraints.

## Limitations
- The study is limited to English language data and resources, which may not generalize to other languages with different morphological structures or writing systems.
- The evaluation relies on a single intrinsic evaluation method (umLabeller) and a single extrinsic evaluation benchmark (OOV Generalization Challenge), which may not fully capture the complexities of subword tokenization evaluation.
- The study's findings are dependent on the completeness and accuracy of the UniMorph database, which may not capture all morphological variations in English.

## Confidence
**High Confidence Claims:**
- umLabeller's accuracy in classifying subword compositions as morphological or alien is 98%
- Alien subword compositions lead to poorer generalization than morphological compositions for semantic compositionality of word meanings across all studied language models

**Medium Confidence Claims:**
- The OOV Generalization Challenge benchmark effectively evaluates the impact of subword tokenization on downstream NLP tasks

**Low Confidence Claims:**
- The tolerance of one missing element in umLabeller's morphological matching algorithm is sufficient to handle incomplete or altered roots

## Next Checks
1. Validate umLabeller's accuracy on a held-out set of subword compositions with known morphological validity.
2. Evaluate the effectiveness of the OOV Generalization Challenge benchmark on a diverse set of language models and tasks.
3. Investigate the impact of morphological resource quality on umLabeller's performance.