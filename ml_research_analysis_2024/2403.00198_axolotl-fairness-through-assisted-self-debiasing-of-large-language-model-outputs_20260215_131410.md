---
ver: rpa2
title: 'AXOLOTL: Fairness through Assisted Self-Debiasing of Large Language Model
  Outputs'
arxiv_id: '2403.00198'
source_url: https://arxiv.org/abs/2403.00198
tags: []
core_contribution: AXOLOTL is a novel post-processing framework that reduces bias
  in LLM outputs through assisted self-debiasing. It treats LLMs as black boxes, leveraging
  public APIs without requiring access to internal parameters.
---

# AXOLOTL: Fairness through Assisted Self-Debiasing of Large Language Model Outputs

## Quick Facts
- arXiv ID: 2403.00198
- Source URL: https://arxiv.org/abs/2403.00198
- Authors: Sana Ebrahimi; Kaiwen Chen; Abolfazl Asudeh; Gautam Das; Nick Koudas
- Reference count: 11
- Key outcome: AXOLOTL is a novel post-processing framework that reduces bias in LLM outputs through assisted self-debiasing. It treats LLMs as black boxes, leveraging public APIs without requiring access to internal parameters. The three-step process identifies bias orientation, proposes pleasant resolutions, and guides the model to self-debias its outputs. Across benchmark datasets and sensitive attributes (gender, race, profession), AXOLOTL significantly reduced toxicity (up to 31% reduction) and improved regard scores (e.g., 50% reduction in negative regard for gender). It achieved gender neutralization in co-reference resolution (up to 82% gender-neutral responses) and decreased stereotype scores in multi-choice question answering (e.g., 13.29% reduction for profession). The framework operates efficiently with minimal hardware requirements while preserving model performance.

## Executive Summary
AXOLOTL is a post-processing framework that reduces bias in LLM outputs through assisted self-debiasing. The approach treats LLMs as black boxes, leveraging public APIs without requiring access to internal parameters. Through a three-step process resembling zero-shot learning, AXOLOTL identifies biases, proposes resolutions, and guides the model to self-debias its outputs. The framework demonstrates significant reductions in toxicity and stereotype scores across multiple benchmarks while maintaining model performance and requiring minimal computational resources.

## Method Summary
AXOLOTL employs a three-step process to reduce bias in LLM outputs: first, it identifies bias orientation by comparing output embeddings to demographic group vectors using cosine similarity; second, it proposes pleasant resolutions by finding words that neutralize unpleasant characteristic vectors through vector rejection; and third, it guides the model to self-debias by constructing instructions containing the detected bias and pleasant resolution, prompting the model to regenerate the output. The framework operates on publicly available LLMs via APIs, requiring no internal parameter access, and uses vector-based operations for bias detection and neutralization.

## Key Results
- Reduced toxicity scores by up to 31% across benchmark datasets
- Achieved 82% gender-neutral responses in co-reference resolution tasks
- Decreased stereotype scores by 13.29% in multi-choice question answering for profession attribute
- Improved regard scores with 50% reduction in negative regard for gender attribute

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AXOLOTL identifies bias by comparing embedding vectors of model outputs to pre-defined demographic group vectors, detecting orientation when cosine similarity exceeds a threshold.
- Mechanism: Uses cosine similarity between output embeddings and demographic group embeddings; if similarity exceeds ε, classifies as bias orientation.
- Core assumption: Embedding vectors effectively capture semantic orientation toward demographic groups.
- Evidence anchors:
  - [abstract] "Through a three-step process resembling zero-shot learning, AXOLOTL identifies biases, proposes resolutions, and guides the model to self-debias its outputs."
  - [section] "To identify the orientation of a model response r towards a demographic group, we follow (Bolukbasi et al., 2016a) and calculate the cosine similarity of the vector representation of r, ⃗vr, with the vector representation of each demographic group gk ∈ G."
  - [corpus] Weak evidence - corpus papers discuss similar embedding-based bias detection but don't validate AXOLOTL's specific threshold approach.
- Break condition: Embedding vectors fail to capture semantic orientation accurately, or threshold ε is poorly calibrated.

### Mechanism 2
- Claim: AXOLOTL neutralizes bias by finding pleasant resolution words whose addition to the output vector makes it orthogonal to unpleasant characteristic vectors.
- Mechanism: Computes repair vector ⃗u* that makes output vector orthogonal to unpleasant word vector, then finds closest pleasant word from T+ set.
- Core assumption: Vector rejection formula can effectively neutralize bias direction while preserving semantic meaning.
- Evidence anchors:
  - [section] "Let ⃗w+ ∈ T+ be a vector such that, when added to the response vector ⃗vr, the resulting vector is (almost) orthogonal to ⃗w− ∈ T−k."
  - [section] "Although the addition of the vector ⃗u* to the response vector make the result orthogonal to ⃗w−, it does not correspond to a word in T+. Therefore, we identify the word that has the closest embedding vector to ⃗u* from the set T+."
  - [corpus] Moderate evidence - vector-based bias neutralization is established in literature, but AXOLOTL's specific implementation details are novel.
- Break condition: Vector rejection fails to preserve semantic coherence, or pleasant word selection produces nonsensical output.

### Mechanism 3
- Claim: AXOLOTL leverages model's self-debiasing capability by providing explicit instructions about detected bias and pleasant resolutions.
- Mechanism: Constructs instruction prompt containing bias orientation, unpleasant characteristic, and pleasant resolution, then asks model to regenerate biased response.
- Core assumption: LLMs can follow meta-instructions about their own outputs and modify them accordingly.
- Evidence anchors:
  - [abstract] "leverages public APIs to interact with LLMs without direct access to internal parameters."
  - [section] "Upon acquiring the pleasant resolution w+ and pinpointing the source of bias, we can formulate an instruction for the model to guide it in rewriting the original response r to incorporate the desired modifications."
  - [corpus] Strong evidence - self-debiasing through instruction is demonstrated in related work "Self-Debiasing Large Language Models."
- Break condition: Model fails to follow meta-instructions, or instructions introduce new biases.

## Foundational Learning

- Concept: Cosine similarity and vector space operations
  - Why needed here: Core mechanism for bias detection relies on comparing embeddings using cosine similarity
  - Quick check question: What does it mean when two vectors have cosine similarity of 0.9 vs 0.1?

- Concept: Zero-shot learning principles
  - Why needed here: AXOLOTL operates without fine-tuning, relying on model's ability to generalize from instructions
  - Quick check question: How does zero-shot learning differ from few-shot or fine-tuning approaches?

- Concept: Embedding-based bias detection
  - Why needed here: Foundation for identifying orientation toward demographic groups and pleasant/unpleasant characteristics
  - Quick check question: Why might embedding-based methods capture biases that token-level approaches miss?

## Architecture Onboarding

- Component map: Input prompt → Model M → Output r → Embedding generator → Vector ⃗vr → Demographic group vectors {⃗g1...⃃gn} → Cosine similarity calculation → Unpleasant/pleasant word sets T-/T+ → Vector rejection calculation → Instruction builder → Regenerated prompt → Model M (again) → Evaluation metrics (toxicity, regard, stereotype score)

- Critical path: Prompt → Model → Bias detection → Instruction generation → Model regeneration → Evaluation
- Design tradeoffs: Black-box approach trades access to internal parameters for broader model compatibility
- Failure signatures:
  - No bias detected when clearly present (threshold too high)
  - False positives on benign outputs (threshold too low)
  - Generated outputs lose semantic coherence
  - Self-debiasing instructions ignored by model

- First 3 experiments:
  1. Test bias detection on simple, known-biased sentences with single demographic attribute
  2. Verify vector rejection produces orthogonal output while maintaining meaning
  3. Validate instruction-following by asking model to rewrite clearly biased text

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effectively does AXOLOTL scale to larger language models beyond the tested parameter sizes (7, 13, 20, and 70 billion)?
- Basis in paper: [explicit] The paper states that AXOLOTL treats LLMs as black boxes and leverages public APIs, suggesting potential scalability, but does not explicitly test larger models.
- Why unresolved: The experiments only tested models up to 70 billion parameters, leaving uncertainty about performance with even larger models like GPT-4 or beyond.
- What evidence would resolve it: Testing AXOLOTL with larger models and comparing bias reduction metrics and computational efficiency to smaller models.

### Open Question 2
- Question: Can AXOLOTL effectively handle emerging demographic groups and sensitive attributes not covered in the current word sets (T+, T-)?
- Basis in paper: [inferred] The paper mentions that the method's success depends on the integrity and selection of word sets (T+, T-), implying limitations with unrepresented groups.
- Why unresolved: The study focuses on gender, race, and profession, but does not explore how well the framework adapts to new or intersectional demographic categories.
- What evidence would resolve it: Evaluating AXOLOTL's performance on newly defined demographic groups and sensitive attributes, measuring bias reduction and resolution accuracy.

### Open Question 3
- Question: What is the long-term impact of AXOLOTL's post-processing on model performance and user trust in real-world applications?
- Basis in paper: [explicit] The paper notes that AXOLOTL preserves model performance but does not investigate long-term effects on user perception or trust.
- Why unresolved: While immediate bias reduction is demonstrated, there is no data on how sustained use affects user confidence or whether performance degrades over time.
- What evidence would resolve it: Longitudinal studies tracking model outputs, user feedback, and performance metrics over extended periods in deployed applications.

## Limitations
- Implementation details for vector rejection formula and exact thresholds remain unspecified
- Performance on attributes beyond gender, race, and profession remains untested
- Limited analysis of trade-offs with other model capabilities and long-term effects

## Confidence
**High Confidence**: The three-step framework architecture is clearly defined and implementable; vector-based bias detection using cosine similarity is well-established; instruction-based self-debiasing leverages proven prompting techniques

**Medium Confidence**: Quantitative results showing bias reduction across benchmarks; claims about hardware efficiency and API-based implementation; comparative performance against baseline approaches

**Low Confidence**: Generalization to arbitrary demographic attributes; long-term stability of debiased outputs; preservation of nuanced semantic meaning during vector operations

## Next Checks
1. **Threshold Sensitivity Analysis**: Systematically vary the similarity threshold ε from 0.1 to 0.9 and measure impact on false positive/negative rates for bias detection. This would validate the claimed "optimal" threshold of 0.4.

2. **Cross-Attribute Generalization Test**: Apply AXOLOTL to additional demographic attributes (e.g., age, disability status, socioeconomic status) using the same framework to verify the claimed broad applicability beyond the three tested attributes.

3. **Semantic Preservation Benchmark**: Design a task-specific accuracy test (e.g., sentiment analysis, question answering) comparing model performance before and after AXOLOTL processing to quantify the claimed "minimal impact on model performance."