---
ver: rpa2
title: 'Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive Data
  Analysis Agents'
arxiv_id: '2403.05307'
source_url: https://arxiv.org/abs/2403.05307
tags:
- data
- code
- agents
- user
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Tapilot-Crossing is a new benchmark for evaluating LLM agents
  on interactive data analysis tasks, featuring 1024 interactions across four practical
  scenarios: Normal, Action, Private, and Private Action. The dataset is constructed
  using a cost-effective multi-agent environment called Decision Company, requiring
  minimal human effort.'
---

# Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents

## Quick Facts
- arXiv ID: 2403.05307
- Source URL: https://arxiv.org/abs/2403.05307
- Reference count: 35
- Key outcome: A new benchmark for interactive data analysis agents with 1024 interactions across four practical scenarios, showing that popular LLM agents struggle with interactive data analysis and proposing Adaptive Interaction Reflection (AIR) to achieve up to 44.5% relative performance improvement.

## Executive Summary
This paper introduces TAPILOT-CROSSING, a comprehensive benchmark for evaluating large language models (LLMs) as interactive data analysis agents. The benchmark is constructed using a cost-effective multi-agent environment called Decision Company, which simulates realistic user-agent interactions for data analysis tasks. The authors identify key challenges in interactive data analysis, including tool usage, reasoning, and learning from successful histories, and propose the Adaptive Interaction Reflection (AIR) strategy to address these challenges. Experiments across four practical scenarios (Normal, Action, Private, and Private Action) reveal that popular LLM agents struggle with interactive data analysis, highlighting the need for more advanced agents. AIR demonstrates significant performance improvements, particularly in ACTION modes, by enabling agents to learn from successful interaction histories.

## Method Summary
The TAPILOT-CROSSING benchmark is constructed using the Decision Company multi-agent environment, which simulates data analysis interactions using four specialized GPT-4 agents (Administrator, Client, Data Scientist, and AI Chatbot). The benchmark includes 1024 interactions across four practical scenarios: NORMAL, ACTION, PRIVATE, and PRIVATE ACTION. Three LLM settings are evaluated: Model Base, Agent with tools and reasoning (Chain-of-Thought and ReAct), and Inter-Agent with Adaptive Interaction Reflection (AIR). AIR enables agents to learn from successful user-code histories by generating pseudo code logic and reorganizing it into one-shot examples. The evaluation framework uses Accuracy (Acc) for correct code execution and multi-choice answers, with AccR extending Acc to include recall of private library functions. The method involves setting up the Decision Company environment, implementing the three LLM settings, and evaluating them using the specified metrics and toolkit (executor, user simulator, chart-to-table converter).

## Key Results
- Popular LLM agents struggle with interactive data analysis tasks, with performance varying significantly across different interaction modes.
- AIR achieves up to 44.5% relative performance improvement in ACTION modes by enabling agents to learn from successful interaction histories.
- PRIVATE mode evaluation reveals true semantic parsing capabilities, with models struggling to implement user-defined private functions.
- CSE (Code Similarity Equivalence) metric effectively captures code generation nuances, addressing the challenge of evaluating performance when minor differences shouldn't result in zero scores.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-agent simulation can generate realistic interactive data analysis logs with minimal human effort.
- Mechanism: DECISION COMPANY environment simulates four specialized agents that interact to produce authentic user-agent dialogues through role-playing and collaborative problem-solving.
- Core assumption: Specialized agents can maintain consistent personas and generate contextually appropriate interactions that reflect real-world data analysis scenarios.
- Evidence anchors:
  - [abstract] "TAPILOT-CROSSING is constructed by an economical multi-agent environment, DECISION COMPANY, with few human efforts"
  - [section 3.1] "DECISION COMPANY is a simulated environment where 4 GPT-4 agents communicate with each other to perform data analysis tasks"
  - [corpus] Weak evidence - no direct citations found for multi-agent data generation approaches
- Break condition: If agents fail to maintain consistent personas or generate unrealistic interactions that don't reflect actual user needs and constraints.

### Mechanism 2
- Claim: Adaptive Interaction Reflection (AIR) improves LLM performance by learning from successful interaction histories.
- Mechanism: AIR generates pseudo code logic from previous successful interactions and reorganizes them into one-shot examples that guide current problem-solving.
- Core assumption: Successful interaction histories contain transferable logic patterns that can be generalized to new but similar problems.
- Evidence anchors:
  - [abstract] "AIR can evolve LLMs into effective interactive data analysis agents, achieving a relative performance improvement of up to 44.5%"
  - [section 5.3] "AIR approach to enable data analysis agents to learn from successful user-code histories with two steps"
  - [corpus] Moderate evidence - some work on reflection and learning from history in AI agents
- Break condition: If historical interactions contain errors or if current problems differ significantly from historical patterns.

### Mechanism 3
- Claim: Private library evaluation reveals true semantic parsing capabilities beyond standard library memorization.
- Mechanism: Agents must generate code using user-defined private functions rather than relying on pre-trained knowledge of standard libraries.
- Core assumption: True understanding of data analysis tasks requires the ability to interpret and implement custom solutions rather than just recall standard patterns.
- Evidence anchors:
  - [abstract] "Private, which examines the true semantic parsing capability of agents when encountering unseen packages during the pre-training phase"
  - [section 6.3] "Understanding and implementing user-specific functions is a critical and urgent skill for LLM agents in real-world data analysis tasks"
  - [corpus] Moderate evidence - some work on private library evaluation exists but limited
- Break condition: If evaluation focuses too heavily on syntax rather than semantic understanding.

## Foundational Learning

- Concept: Interactive data analysis as collaborative problem-solving
  - Why needed here: The benchmark specifically evaluates agent performance in multi-turn interactions where user intent clarification and adaptive responses are crucial
  - Quick check question: What distinguishes interactive data analysis from single-turn code generation tasks?

- Concept: Tool usage and reasoning in LLMs
  - Why needed here: The Agent and Inter-Agent modes require effective tool integration (executors, simulators, converters) and reasoning strategies (COT, ReAct)
  - Quick check question: How do different reasoning strategies impact LLM performance in interactive settings?

- Concept: Code similarity evaluation
  - Why needed here: The CSE metric addresses the challenge of evaluating code generation when minor differences shouldn't result in zero scores
  - Quick check question: Why is traditional accuracy insufficient for evaluating data analysis code generation?

## Architecture Onboarding

- Component map:
  - Decision Company environment (4 GPT-4 agents) -> TAPILOT-CROSSING benchmark structure (1024 interactions across 4 modes) -> AIR reflection system (pseudo code generation + re-org one-shot reasoning) -> Evaluation framework (Acc, AccR, CSE metrics) -> Tool ecosystem (executor, user simulator, chart-to-table converter)

- Critical path: Data generation → Benchmark construction → Model evaluation → AIR implementation → Performance analysis

- Design tradeoffs:
  - Cost vs. quality in data generation (multi-agent vs. human crowdsourcing)
  - Model capability vs. prompt complexity (simpler instructions vs. comprehensive tool usage)
  - Evaluation granularity vs. computational efficiency (CSE vs. binary accuracy)

- Failure signatures:
  - Poor persona maintenance in multi-agent simulation
  - Inconsistent code generation across different interaction modes
  - AIR over-reliance on historical patterns leading to context blindness
  - Private library recall issues indicating shallow semantic understanding

- First 3 experiments:
  1. Compare DECISION COMPANY output quality with human-annotated interactions
  2. Test AIR effectiveness across different interaction mode distributions
  3. Evaluate private library integration success rate across model families

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can TAPILOT-CROSSING be extended to include noisy and realistic interaction histories, as opposed to the current clean and accurate interactions?
- Basis in paper: Inferred from the limitations section, which states that the current dataset assumes all human-machine interaction history is clean and correct, while real-world scenarios often contain noise or require multi-turn clarifications.
- Why unresolved: The paper acknowledges this limitation but does not provide a concrete methodology for incorporating noisy interactions into the benchmark.
- What evidence would resolve it: A detailed methodology for generating and incorporating noisy interaction histories into TAPILOT-CROSSING, along with experimental results demonstrating the impact on LLM agent performance.

### Open Question 2
- Question: What is the optimal balance between leveraging historical interactions and focusing on real-time user context in the Adaptive Interaction Reflection (AIR) strategy?
- Basis in paper: Inferred from the fine-grained results on ACTION modes, which show that AIR can make agents overly tractable to historical data, potentially hindering their ability to make assumptions based on instant user behaviors.
- Why unresolved: The paper highlights this trade-off but does not provide a concrete solution for striking the optimal balance.
- What evidence would resolve it: Experimental results comparing different weighting schemes for historical interactions vs. real-time context in the AIR strategy, along with ablation studies to determine the impact on agent performance.

### Open Question 3
- Question: How can the Code Similarity Equivalence (CSE) metric be further refined to better capture the nuances of code generation performance, especially for complex tasks with long-form code?
- Basis in paper: Inferred from the discussion on the challenges of evaluating performance in data analysis tasks, where execution results of code with a single error and a completely incorrect code are both determined as 0, making it difficult to discern subtle differences.
- Why unresolved: The paper proposes CSE as a solution but acknowledges that it is a preliminary metric and could be improved.
- What evidence would resolve it: A more comprehensive evaluation of CSE, including comparison with human evaluation and exploration of additional similarity metrics or techniques to better capture the nuances of code generation performance.

## Limitations
- The multi-agent simulation approach may not fully capture the complexity and nuance of real human-agent interactions in data analysis scenarios.
- The AIR mechanism's effectiveness is demonstrated but the underlying assumptions about transferable patterns in successful histories require further validation across diverse problem domains.
- The private library evaluation provides valuable insights but the limited scope of 5 open-source tables may not represent the full complexity of real-world data analysis tasks.

## Confidence
- High confidence in the benchmark construction methodology and evaluation framework
- Medium confidence in the AIR mechanism's generalizability beyond the tested scenarios
- Medium confidence in the conclusions about LLM agent capabilities given the limited table diversity

## Next Checks
1. Test the AIR mechanism on a broader range of data analysis problems from different domains to assess generalizability
2. Conduct human evaluation of Decision Company-generated interactions to validate their authenticity compared to real user-agent dialogues
3. Expand the private library evaluation to include more complex user-defined functions and nested dependencies to better assess semantic understanding