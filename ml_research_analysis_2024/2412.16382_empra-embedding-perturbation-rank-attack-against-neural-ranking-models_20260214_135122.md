---
ver: rpa2
title: 'EMPRA: Embedding Perturbation Rank Attack against Neural Ranking Models'
arxiv_id: '2412.16382'
source_url: https://arxiv.org/abs/2412.16382
tags:
- adversarial
- attack
- documents
- document
- empra
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EMPRA introduces a black-box adversarial attack method targeting
  neural ranking models by leveraging embedding perturbation at the sentence level.
  It manipulates sentence embeddings to align with query context while preserving
  semantic integrity, generating adversarial documents imperceptible to humans and
  machines.
---

# EMPRA: Embedding Perturbation Rank Attack against Neural Ranking Models

## Quick Facts
- **arXiv ID**: 2412.16382
- **Source URL**: https://arxiv.org/abs/2412.16382
- **Reference count**: 40
- **Primary result**: Achieved nearly 96% success in re-ranking documents originally ranked 51‚Äì100 into the top 10 positions

## Executive Summary
EMPRA introduces a black-box adversarial attack method targeting neural ranking models by leveraging embedding perturbation at the sentence level. It manipulates sentence embeddings to align with query context while preserving semantic integrity, generating adversarial documents imperceptible to humans and machines. Unlike prior methods, EMPRA does not rely on surrogate models, making it robust across both in-distribution and out-of-distribution settings. In experiments on the MS MARCO V1 passage collection, EMPRA achieved nearly 96% success in re-ranking documents originally ranked 51‚Äì100 into the top 10.

## Method Summary
EMPRA operates by iteratively perturbing sentence embeddings to align them with query context while preserving semantic integrity. The method uses a transporter function to update embedding representations and a transformer function to map perturbed embeddings back to lexical form. EMPRA employs generic neural ranking models trained on diverse datasets to score relevance without relying on surrogate models. An interpolated scoring mechanism balances semantic coherence and query relevance to select the final adversarial document. The approach generates high-quality, fluent adversarial documents that maintain naturalness while effectively deceiving neural ranking models.

## Key Results
- Achieved nearly 96% success rate in re-ranking documents from positions 51‚Äì100 to top 10
- Outperformed baseline methods in perplexity, particularly for hard target documents
- Maintained high grammatical quality and fluency in adversarial documents
- Demonstrated strong robustness across various victim models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EMPRA perturbs sentence embeddings iteratively to align them with query context while preserving semantic integrity.
- Mechanism: The transporter function updates the embedding representation of each sentence by moving it closer to the anchor text embedding within a constrained L‚àû radius (ùúñ = 0.01). The transformer function then maps the perturbed embedding back to lexical form.
- Core assumption: Sentence-level embedding perturbations can effectively deceive the victim neural ranking model while remaining imperceptible to humans and machines.
- Evidence anchors:
  - [abstract] "EMPRA manipulates sentence-level embeddings, guiding them towards pertinent context related to the query while preserving semantic integrity."
  - [section] "The transporter function calculates the new coordinates of the sentence embedding representation through ùê∏ (ùëÜ) (ùë° +1) = T (ùê∏ (ùëÜ) (ùë° ), ùê∏(ùê¥))."
- Break condition: If perturbation bounds are too large, semantic drift may become detectable.

### Mechanism 2
- Claim: EMPRA achieves high attack performance across both in-distribution and out-of-distribution settings by using surrogate-agnostic generic neural ranking models.
- Mechanism: EMPRA uses generic NRMs trained on diverse datasets to score relevance, eliminating dependence on specific surrogate models that mimic victim models.
- Core assumption: Generic NRMs can effectively approximate the relevance scoring criteria of diverse victim NRMs.
- Evidence anchors:
  - [abstract] "EMPRA does not depend on surrogate models for adversarial text generation, enhancing its robustness against different NRMs in realistic settings."
  - [section] "EMPRA not only eliminates the need for a surrogate model but also generates adversarial texts that are semantically related to both the target query context and the original target document."
- Break condition: If victim model uses fundamentally different relevance scoring mechanism.

### Mechanism 3
- Claim: EMPRA generates high-quality, fluent, and imperceptible adversarial documents by balancing semantic coherence and query relevance.
- Mechanism: EMPRA uses an interpolated scoring mechanism (ùõº ¬∑ùê∂coh + ( 1 ‚àí ùõº) ¬∑ ùê∂rel) to select the final adversarial document, balancing NSP-measured coherence with generic NRM-measured relevance.
- Core assumption: Balancing semantic coherence and query relevance is essential for generating effective and imperceptible adversarial documents.
- Evidence anchors:
  - [abstract] "EMPRA generates low-perplexity and fluent adversarial documents that can remain imperceptible under both human and automatic evaluations."
  - [section] "EMPRA outperforms IDEM in perplexity, particularly in Hard-5 target documents."
- Break condition: If interpolation coefficient ùõº is not properly tuned.

## Foundational Learning

- Concept: Sentence-level embedding perturbation
  - Why needed here: EMPRA operates by iteratively adjusting sentence embeddings to align them with query context, making this concept fundamental to understanding the attack mechanism.
  - Quick check question: How does EMPRA ensure that the perturbed sentence embeddings remain semantically similar to the original sentences?

- Concept: Generic neural ranking models
  - Why needed here: EMPRA uses generic NRMs trained on diverse datasets to score the relevance of query-document pairs, enabling surrogate-agnostic adversarial document generation.
  - Quick check question: What are the advantages of using generic NRMs over surrogate models in the context of adversarial attacks on neural ranking models?

- Concept: Semantic coherence and query relevance
  - Why needed here: EMPRA balances semantic coherence (measured by NSP) and query relevance (measured by a generic NRM) to generate high-quality, imperceptible adversarial documents.
  - Quick check question: How does EMPRA's interpolated scoring mechanism ensure that the generated documents are both effective and imperceptible?

## Architecture Onboarding

- Component map: Embedding function -> Transporter function -> Transformer function -> Generic NRM -> Interpolated scoring mechanism
- Critical path: 1) Generate adversarial texts by iteratively perturbing sentence embeddings using transporter and transformer functions 2) Construct adversarial documents by injecting generated texts and evaluating coherence/relevance 3) Select final adversarial document based on interpolated scoring
- Design tradeoffs:
  - Perturbation bounds (ùúñ): Larger bounds may increase attack effectiveness but also detection risk
  - Interpolation coefficient (ùõº): Higher ùõº values prioritize coherence over relevance
  - Generic NRM selection: Different NRMs may yield varying attack effectiveness across victim models
- Failure signatures:
  - High perplexity scores indicating unnatural or detectable documents
  - Low semantic coherence scores suggesting document incoherence
  - Poor attack performance indicating generic NRM fails to approximate victim behavior
- First 3 experiments:
  1. Evaluate impact of perturbation bounds (ùúñ) on attack effectiveness and semantic drift
  2. Assess performance of different generic NRMs in approximating victim relevance scoring
  3. Investigate effect of interpolation coefficient (ùõº) on balance between coherence and relevance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does EMPRA perform when attacking neural ranking models trained on datasets significantly different from MS MARCO V1?
- Basis in paper: [explicit] The paper mentions using out-of-distribution datasets (NQ, SQuAD2.0, SNLI, CommonQA) for training generic NRMs but does not evaluate EMPRA's performance against victim models trained on these datasets.
- Why unresolved: The paper only tests EMPRA against victim models fine-tuned on MS MARCO, leaving open whether its effectiveness generalizes to models trained on different data distributions.
- What evidence would resolve it: Experimental results showing EMPRA's attack success rate when targeting victim models trained on out-of-distribution datasets like NQ or SQuAD2.0.

### Open Question 2
- Question: What is the computational overhead of EMPRA compared to baseline methods in real-world attack scenarios?
- Basis in paper: [inferred] While the paper discusses the independence of EMPRA from surrogate models, it does not provide quantitative comparisons of computational resources required compared to baseline methods.
- Why unresolved: The paper focuses on attack performance metrics but omits detailed computational efficiency analysis, which is critical for practical deployment.
- What evidence would resolve it: Comparative analysis of training/inference time, memory usage, and GPU requirements for EMPRA versus baseline methods across multiple attack scenarios.

### Open Question 3
- Question: How effective are current defenses against EMPRA's adversarial attacks?
- Basis in paper: [explicit] The paper mentions the need for future research to develop defenses against EMPRA but does not test any existing defensive mechanisms against its attacks.
- Why unresolved: The paper demonstrates EMPRA's effectiveness but does not evaluate whether existing defense strategies can mitigate its impact.
- What evidence would resolve it: Experimental results showing the performance of various defense mechanisms when deployed against EMPRA's adversarial documents.

## Limitations

- Evaluation is constrained to a single dataset (MS MARCO V1 passage collection) and passage re-ranking scenarios
- Does not address potential defense mechanisms or EMPRA's robustness against them
- Effectiveness of generic NRMs in approximating diverse victim models remains uncertain for specialized or domain-specific ranking models

## Confidence

- **High confidence**: EMPRA's mechanism of iteratively perturbing sentence embeddings to align with query context while preserving semantic integrity is well-supported by evidence. The claim of achieving nearly 96% success in re-ranking documents from positions 51-100 to top 10 is also well-supported.
- **Medium confidence**: The claim of EMPRA's robustness across various victim models due to its surrogate-agnostic approach is partially supported by evidence, though specific mechanisms remain unclear.
- **Low confidence**: The claim of EMPRA's effectiveness in out-of-distribution settings is weakly supported by evidence, as specific characteristics of out-of-distribution scenarios are not clearly defined or tested.

## Next Checks

1. Evaluate EMPRA's performance on other IR tasks: Test EMPRA's effectiveness on document ranking, web search, and other IR tasks beyond passage re-ranking to assess generalizability.

2. Investigate the impact of perturbation bounds and interpolation coefficient: Conduct experiments to systematically explore effects of different perturbation bounds (ùúñ) and interpolation coefficient (ùõº) values on attack effectiveness and document quality.

3. Assess EMPRA's robustness against defense mechanisms: Evaluate EMPRA's performance against common defense strategies such as adversarial training, input preprocessing, and model ensembling to understand limitations and potential vulnerabilities.