---
ver: rpa2
title: 'FaBERT: Pre-training BERT on Persian Blogs'
arxiv_id: '2402.06617'
source_url: https://arxiv.org/abs/2402.06617
tags:
- persian
- fabert
- dataset
- language
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FaBERT, a Persian BERT-base model pre-trained
  on the HmBlogs corpus, which contains both informal and formal Persian texts. The
  model aims to improve performance on traditional Natural Language Understanding
  (NLU) tasks, addressing the complexities of diverse sentence structures and linguistic
  styles in the Persian language.
---

# FaBERT: Pre-training BERT on Persian Blogs

## Quick Facts
- arXiv ID: 2402.06617
- Source URL: https://arxiv.org/abs/2402.06617
- Reference count: 9
- Key outcome: FaBERT achieves state-of-the-art results on 12 Persian NLP datasets by pre-training on cleaned blog corpus HmBlogs

## Executive Summary
This paper introduces FaBERT, a Persian BERT-base model pre-trained on the HmBlogs corpus, which contains both informal and formal Persian texts. The model aims to improve performance on traditional Natural Language Understanding (NLU) tasks, addressing the complexities of diverse sentence structures and linguistic styles in the Persian language. FaBERT was evaluated on 12 datasets across various downstream tasks, including Sentiment Analysis, Named Entity Recognition, Natural Language Inference, Question Answering, and Question Paraphrasing. The model consistently demonstrated improved performance, achieving state-of-the-art results in many tasks. The findings highlight the importance of utilizing diverse and cleaned corpora, such as HmBlogs, to enhance the performance of language models like BERT in Persian NLP applications.

## Method Summary
FaBERT is a Persian BERT-base model pre-trained on the HmBlogs corpus, which contains 20 million Persian blog posts with over 6.8 billion tokens. The model uses BERT-base architecture with 12 layers, 12 attention heads, and 124M parameters. Pre-training employs WordPiece tokenizer with 50,000 tokens, dynamic masking (15% rate) with whole word masking, and omits the Next Sentence Prediction (NSP) task. The corpus undergoes extensive preprocessing to remove noisy posts and standardize characters. Fine-tuning is performed on 12 downstream task datasets using grid search for hyperparameter optimization.

## Key Results
- Achieved state-of-the-art results across multiple Persian NLP tasks including Sentiment Analysis, Named Entity Recognition, and Question Answering
- Demonstrated consistent performance improvements over baseline models when handling both formal and informal Persian texts
- Showed effectiveness in capturing linguistic diversity through training on HmBlogs corpus containing 6.8 billion tokens from diverse sources

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FaBERT improves performance by leveraging a cleaner and more diverse blog corpus.
- Mechanism: The HmBlogs corpus undergoes extensive preprocessing to remove noisy posts, standardize characters, and handle informal language, resulting in higher quality training data that better captures both formal and informal Persian text patterns.
- Core assumption: The cleaning process removes enough noise to significantly improve model learning while preserving linguistic diversity.
- Evidence anchors:
  - [abstract] The findings highlight the importance of utilizing diverse and cleaned corpora, such as HmBlogs, to enhance the performance of language models like BERT in Persian NLP applications.
  - [section 3.1] A series of pre-processing steps were performed on the corpus... A post-discriminator was implemented to filter out these improper and noisy posts.
  - [corpus] HmBlogs includes more than 6.8 billion tokens, covering a wide range of topics, genres, and writing styles, including both formal and informal texts together.
- Break condition: If the cleaning process removes too much linguistic diversity or fails to catch significant noise, the performance gains would diminish.

### Mechanism 2
- Claim: The smaller vocabulary size (50,000 tokens) improves efficiency without sacrificing performance.
- Mechanism: By choosing a conservative vocabulary size, FaBERT reduces model complexity and computational requirements while the WordPiece tokenizer effectively captures essential Persian linguistic features including half spaces.
- Core assumption: A vocabulary of 50,000 tokens is sufficient to represent Persian language adequately for downstream tasks.
- Evidence anchors:
  - [section 3.2] We opted for the WordPiece tokenizer... and with a conservative stance, we set the vocabulary size to 50,000 tokens.
  - [section 3.2] This decision aimed at finding a balance between capturing linguistic details and managing the computational demands associated with larger vocabularies.
  - [section 4.5] Both multilingual models faced challenges due to the lack of sufficient Persian tokens in their vocabularies, potentially impacting their performance on longer inputs.
- Break condition: If Persian requires more unique tokens to capture linguistic nuances, performance would degrade on complex tasks.

### Mechanism 3
- Claim: Excluding NSP and using whole word masking improves MLM effectiveness.
- Mechanism: Removing the NSP task simplifies pre-training while whole word masking ensures the model learns to predict complete words rather than fragments, improving semantic understanding.
- Core assumption: NSP contributes little to downstream performance while whole word masking provides meaningful improvement over standard MLM.
- Evidence anchors:
  - [section 3.2] We implemented dynamic masking... and omitted the Next Sentence Prediction task from our pre-training process, as it was demonstrated to have no discernible positive impact on performance.
  - [section 3.2] We also utilized the whole word masking approach for enhanced performance.
  - [abstract] FaBERT was evaluated on 12 datasets... consistently demonstrated improved performance.
- Break condition: If downstream tasks require sentence-level coherence understanding that NSP would provide, removing it could hurt performance.

## Foundational Learning

- Concept: Persian script characteristics (half spaces, non-standard characters)
  - Why needed here: The tokenizer must handle Persian-specific features that don't exist in English, affecting how text is segmented and processed
  - Quick check question: How does the presence of half spaces in Persian text affect tokenization compared to English?

- Concept: Domain adaptation in NLP
  - Why needed here: Understanding how training on blog data (informal) vs news data (formal) affects model performance on different task types
  - Quick check question: Why might a model trained on blogs perform better on informal text tasks than one trained on formal news articles?

- Concept: Pre-training objectives (MLM vs NSP)
  - Why needed here: The choice of pre-training tasks directly impacts what linguistic features the model learns
  - Quick check question: What aspect of language understanding does MLM capture that NSP does not, and why might NSP be less valuable for downstream NLU tasks?

## Architecture Onboarding

- Component map: Corpus preprocessing → Vocabulary building → Pre-training (MLM only) → Fine-tuning on downstream tasks
- Critical path: Corpus preprocessing → Vocabulary building → Pre-training (MLM only) → Fine-tuning on downstream tasks
- Design tradeoffs: Smaller vocabulary improves efficiency but may miss rare tokens; excluding NSP reduces training complexity but may lose sentence coherence learning; whole word masking improves semantic understanding but may be computationally heavier
- Failure signatures: Poor performance on informal text suggests insufficient domain adaptation; high perplexity during pre-training indicates corpus quality issues; tokenizer errors manifest as unexpected token counts
- First 3 experiments:
  1. Run tokenizer on sample Persian text to verify half space handling and check token count vs baseline models
  2. Evaluate MLM loss convergence on validation set to ensure pre-training stability
  3. Fine-tune on a simple sentiment analysis dataset to verify basic functionality before scaling to all 12 datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does FaBERT's performance on Persian NLP tasks compare to larger language models like LLMs when considering both efficiency and accuracy?
- Basis in paper: [explicit] The paper mentions that LLMs often have slower response times and increased latency compared to smaller models like FaBERT, and they require advanced hardware, creating accessibility challenges.
- Why unresolved: The paper does not provide a direct comparison between FaBERT and larger language models in terms of performance on Persian NLP tasks.
- What evidence would resolve it: Conducting a study comparing the performance of FaBERT and larger language models on a set of Persian NLP tasks, measuring both accuracy and efficiency (e.g., response time, hardware requirements).

### Open Question 2
- Question: How does the choice of vocabulary size impact the performance of BERT models on Persian NLP tasks, and what is the optimal vocabulary size for balancing performance and computational efficiency?
- Basis in paper: [inferred] The paper discusses the trade-off between vocabulary size and model performance, noting that a larger vocabulary can capture more unique tokens but requires more extensive training data, while a smaller vocabulary may struggle to capture all linguistic details.
- Why unresolved: The paper does not provide a systematic investigation of the impact of vocabulary size on model performance for Persian NLP tasks.
- What evidence would resolve it: Conducting a study varying the vocabulary size of BERT models for Persian NLP tasks and measuring the impact on performance and computational efficiency.

### Open Question 3
- Question: How does the use of informal text in the training corpus affect the performance of BERT models on downstream tasks involving informal language, such as sentiment analysis of social media posts?
- Basis in paper: [explicit] The paper mentions that FaBERT was trained on a corpus containing both informal and formal Persian texts, and it shows improved performance on tasks involving informal language, such as the ParsiNLU Question Paraphrasing dataset.
- Why unresolved: The paper does not provide a detailed analysis of the impact of informal text in the training corpus on model performance for specific downstream tasks involving informal language.
- What evidence would resolve it: Conducting a study comparing the performance of BERT models trained on corpora with varying proportions of informal text on downstream tasks involving informal language.

## Limitations

- Post-discriminator implementation details for corpus cleaning are not fully specified, making it difficult to assess whether noise removal was optimal or over-aggressive
- Comparison with multilingual BERT models raises questions about fair evaluation due to noted vocabulary limitations but lacks detailed ablations
- Pre-training corpus statistics are limited - lacks pre-training loss curves or validation metrics to assess model convergence
- Grid search ranges for fine-tuning hyperparameters across 12 datasets are not specified, making optimal configuration assessment difficult

## Confidence

**High Confidence**: The core mechanism that pre-training on diverse Persian blog data improves downstream task performance. The 12 dataset evaluation consistently shows FaBERT outperforming baselines on both formal and informal Persian texts, with state-of-the-art results achieved across multiple tasks.

**Medium Confidence**: The specific contribution of vocabulary size reduction to performance gains. While the paper argues that 50,000 tokens balances efficiency and coverage, the comparison with multilingual models suggests vocabulary limitations but doesn't definitively prove that the chosen size is optimal for Persian.

**Low Confidence**: The relative importance of individual architectural choices (removing NSP, whole word masking) to the overall performance improvements. The paper claims these modifications help but doesn't provide ablation studies isolating their individual contributions.

## Next Checks

1. **Corpus quality validation**: Run perplexity evaluation on held-out validation subsets from HmBlogs to verify that the cleaning process maintains linguistic diversity while reducing noise, comparing against raw corpus statistics.

2. **Vocabulary coverage analysis**: Tokenize a diverse sample of Persian texts (formal news, informal blogs, social media) and measure how many unique tokens fall outside the 50,000 vocabulary to assess whether this size is truly sufficient for Persian.

3. **Ablation study on pre-training choices**: Train three additional FaBERT variants - one with NSP retained, one with standard masking (not whole word), and one with both changes - then evaluate all four models on a representative subset of downstream tasks to isolate the contribution of each architectural decision.