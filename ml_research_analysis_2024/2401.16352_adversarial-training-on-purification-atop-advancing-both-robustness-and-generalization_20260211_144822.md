---
ver: rpa2
title: 'Adversarial Training on Purification (AToP): Advancing Both Robustness and
  Generalization'
arxiv_id: '2401.16352'
source_url: https://arxiv.org/abs/2401.16352
tags:
- adversarial
- purifier
- attacks
- accuracy
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Adversarial Training on Purification (AToP),
  a defense method that combines adversarial training and adversarial purification
  to achieve both high robustness and generalization. The key idea is to use random
  transforms to destroy adversarial perturbations, followed by fine-tuning a pre-trained
  generative purifier model with an adversarial loss derived from classifier outputs.
---

# Adversarial Training on Purification (AToP): Advancing Both Robustness and Generalization

## Quick Facts
- arXiv ID: 2401.16352
- Source URL: https://arxiv.org/abs/2401.16352
- Authors: Guang Lin; Chao Li; Jianhai Zhang; Toshihisa Tanaka; Qibin Zhao
- Reference count: 15
- Key outcome: AToP combines adversarial training with purification to achieve both high robustness and generalization, improving robust accuracy against various attacks while maintaining standard accuracy.

## Executive Summary
The paper proposes Adversarial Training on Purification (AToP), a defense method that integrates adversarial training with adversarial purification. By using random transforms to destroy adversarial perturbations and fine-tuning a pre-trained generative purifier model with an adversarial loss derived from classifier outputs, AToP generates high-quality, correctly classified purified examples while avoiding overfitting to known attacks. Extensive experiments on CIFAR-10, CIFAR-100, and ImageNette demonstrate significant improvements in robust accuracy against various attacks, including AutoAttack l∞ and l2, while maintaining good standard accuracy.

## Method Summary
AToP introduces a novel approach that combines adversarial training with purification to enhance both robustness and generalization. The method leverages random transforms to disrupt adversarial perturbations, followed by fine-tuning a pre-trained generative purifier model using an adversarial loss derived from classifier outputs. This ensures the purifier model generates high-quality, correctly classified purified examples while avoiding overfitting to known attacks. The approach is evaluated on CIFAR-10, CIFAR-100, and ImageNette, demonstrating significant improvements in robust accuracy against various attacks, including AutoAttack l∞ and l2, while maintaining good standard accuracy.

## Key Results
- AToP achieves up to 14.45% improvement in robust accuracy compared to existing generator-based purifier models.
- The method exhibits strong generalization ability against unseen attacks like StAdv.
- AToP maintains good standard accuracy while significantly improving robust accuracy against various attacks, including AutoAttack l∞ and l2.

## Why This Works (Mechanism)
AToP works by combining adversarial training with purification, leveraging random transforms to disrupt adversarial perturbations and fine-tuning a pre-trained generative purifier model with an adversarial loss derived from classifier outputs. This approach ensures the purifier model generates high-quality, correctly classified purified examples while avoiding overfitting to known attacks. The method enhances both robustness and generalization by integrating adversarial training with purification, leading to significant improvements in robust accuracy against various attacks while maintaining good standard accuracy.

## Foundational Learning
- Adversarial Training: Training models to be robust against adversarial attacks by incorporating adversarial examples during training. Why needed: To improve model robustness against adversarial attacks. Quick check: Verify the model's robustness against known attack methods.
- Generative Purification: Using generative models to remove adversarial perturbations from inputs. Why needed: To generate high-quality, correctly classified purified examples. Quick check: Assess the quality and correctness of purified examples.
- Random Transforms: Applying random transformations to inputs to disrupt adversarial perturbations. Why needed: To enhance the robustness of the purification process. Quick check: Evaluate the effectiveness of random transforms in disrupting adversarial perturbations.

## Architecture Onboarding
- Component Map: Input -> Random Transforms -> Pre-trained Generative Purifier -> Adversarial Loss -> Classifier -> Output
- Critical Path: Input -> Random Transforms -> Purifier Model -> Adversarial Loss -> Classifier -> Output
- Design Tradeoffs: Balancing robustness and generalization by integrating adversarial training with purification. The method aims to avoid overfitting to known attacks while improving robust accuracy.
- Failure Signatures: Poor purification quality, overfitting to known attacks, or failure to generalize to unseen attacks.
- First Experiments: 1) Evaluate AToP on CIFAR-10, CIFAR-100, and ImageNette. 2) Test against AutoAttack l∞ and l2. 3) Assess generalization against unseen attacks like StAdv.

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation scope is limited to specific datasets (CIFAR-10, CIFAR-100, ImageNette) and attack types.
- The claim of avoiding overfitting to known attacks needs stronger empirical validation across diverse attack families.
- The computational overhead of fine-tuning the purifier model is not discussed in detail, which could limit practical applicability.

## Confidence
- Claims about robustness improvements against tested attacks: **Medium**
- Claims about generalization to unseen attacks (e.g., StAdv): **Medium**
- Claims about avoiding overfitting to known attacks: **Low**

## Next Checks
1. Evaluate AToP on additional datasets (e.g., ImageNet, real-world data) to assess scalability and robustness in diverse settings.
2. Test against a broader range of attack families (e.g., optimization-based, query-based) to validate generalization claims.
3. Measure computational overhead and compare training/inference time with existing methods to assess practical feasibility.