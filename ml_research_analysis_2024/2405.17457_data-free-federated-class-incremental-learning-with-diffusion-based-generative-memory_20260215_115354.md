---
ver: rpa2
title: Data-Free Federated Class Incremental Learning with Diffusion-Based Generative
  Memory
arxiv_id: '2405.17457'
source_url: https://arxiv.org/abs/2405.17457
tags:
- learning
- federated
- diffusion
- data
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles catastrophic forgetting in federated class incremental
  learning (FCIL) by introducing a data-free framework called DFedDGM. The method
  leverages diffusion models to generate synthetic images for each client, avoiding
  the instability issues common in GAN-based approaches.
---

# Data-Free Federated Class Incremental Learning with Diffusion-Based Generative Memory

## Quick Facts
- **arXiv ID**: 2405.17457
- **Source URL**: https://arxiv.org/abs/2405.17457
- **Reference count**: 40
- **Primary result**: DFedDGM achieves over 4% higher average accuracy than baselines on Tiny-ImageNet in federated class incremental learning

## Executive Summary
This paper addresses catastrophic forgetting in federated class incremental learning (FCIL) through a novel data-free framework called DFedDGM. The method leverages diffusion models to generate synthetic images for each client, avoiding the instability issues common in GAN-based approaches. A balanced sampler addresses the non-IID problem in federated learning by ensuring class-balanced training of the diffusion model. Extensive experiments show DFedDGM significantly outperforms existing methods while maintaining communication costs comparable to FedAvg.

## Method Summary
DFedDGM operates in a federated setting where each client trains a local diffusion model to generate synthetic images that simulate the original data distribution. The framework employs a balanced sampler to ensure equal representation of all classes during diffusion model training, mitigating non-IID data distribution issues. Entropy-based filtering removes low-confidence synthetic samples before training. Knowledge distillation and feature distance loss preserve information from previous tasks while adapting to new classes. The global model aggregates local updates through standard federated averaging.

## Key Results
- Achieves over 4% higher average accuracy than baselines on Tiny-ImageNet
- Maintains communication costs comparable to FedAvg
- Demonstrates robust performance across CIFAR-100, EMNIST-Letters, and Tiny-ImageNet datasets
- Ablation studies confirm effectiveness of balanced sampler, entropy filtering, and knowledge distillation components

## Why This Works (Mechanism)

### Mechanism 1
Diffusion models provide stable, high-quality image generation compared to GANs in federated class incremental learning. The gradual denoising process avoids mode collapse and training instability common in GANs, enabling more reliable synthetic data generation to address catastrophic forgetting.

### Mechanism 2
Balanced sampling during diffusion model training mitigates the non-IID problem in federated learning. By ensuring equal numbers of samples from each class per batch, the diffusion model learns a balanced representation of all classes, preventing generation of biased or incomplete synthetic datasets.

### Mechanism 3
Entropy-based filtering removes low-confidence synthetic samples, improving training stability. Entropy quantifies uncertainty in global model predictions; samples with low entropy (high confidence) are retained while those with high entropy (low confidence) are filtered out, ensuring only reliable synthetic data is used.

## Foundational Learning

- **Concept**: Catastrophic forgetting in continual learning
  - **Why needed here**: FCIL requires the model to retain performance on previous classes while learning new ones; catastrophic forgetting would degrade overall performance.
  - **Quick check question**: What happens to a model's performance on old tasks when trained only on new tasks without any regularization or replay mechanism?

- **Concept**: Knowledge distillation for model compression and transfer
  - **Why needed here**: Distills knowledge from the global model to the local model, preserving information from previous tasks while adapting to new classes.
  - **Quick check question**: How does knowledge distillation differ from simple fine-tuning when transferring knowledge between models?

- **Concept**: Federated learning and non-IID data distribution
  - **Why needed here**: FCIL operates in a federated setting where data is distributed across clients with potentially different class distributions; understanding non-IID is crucial for designing effective algorithms.
  - **Quick check question**: Why does non-IID data distribution pose a challenge for federated learning, and how does it affect model convergence?

## Architecture Onboarding

- **Component map**: Local clients -> Train diffusion models with balanced sampling -> Generate synthetic images -> Apply entropy filter -> Train local models with knowledge distillation -> Central server aggregates models -> Distribute global model

- **Critical path**: 1) Train diffusion model on local data with balanced sampling, 2) Generate synthetic images and label with global model, 3) Apply entropy filter to remove low-confidence samples, 4) Combine synthetic and real data for local model training with knowledge distillation and feature distance loss, 5) Aggregate local models on server to form global model

- **Design tradeoffs**: Diffusion models vs. GANs (stability vs. computational efficiency), balanced sampling vs. random sampling (balanced data vs. computational overhead), entropy filtering vs. no filtering (higher quality data vs. potential loss of useful samples), knowledge distillation vs. no distillation (knowledge preservation vs. additional training complexity)

- **Failure signatures**: Model performance degrades on previous tasks (catastrophic forgetting not addressed), synthetic images are of poor quality or do not represent original data distribution, training becomes unstable or converges slowly due to imbalanced data or low-confidence labels, communication overhead increases significantly, impacting scalability

- **First 3 experiments**: 1) Train diffusion model with balanced sampling on non-IID data and evaluate synthetic image quality, 2) Apply entropy filter to synthetic samples and measure impact on training stability and performance, 3) Compare knowledge distillation with and without feature distance loss on catastrophic forgetting mitigation

## Open Questions the Paper Calls Out
No specific open questions are called out in the paper.

## Limitations
- Performance generalizability across extreme non-IID distributions with highly skewed class representations remains untested
- Computational overhead of training diffusion models locally on each client in large-scale deployments not fully characterized
- Optimal hyperparameter settings for diffusion models may be dataset-dependent and require extensive tuning

## Confidence

**High Confidence**: Knowledge distillation and feature distance loss mechanisms for mitigating catastrophic forgetting are well-established and directly applicable to this FCIL setting.

**Medium Confidence**: Entropy-based filtering shows promise but optimal threshold settings may be dataset-dependent.

**Low Confidence**: Balanced sampler's effectiveness relies heavily on specific implementation details and parameter choices.

## Next Checks

1. Validate DFedDGM's performance on additional datasets with different characteristics (medical imaging, satellite imagery) to assess robustness across domains.

2. Evaluate computational overhead and communication costs of training diffusion models on each client in large-scale federated settings with hundreds or thousands of clients.

3. Systematically vary non-IID data distribution parameters (Î² in Dirichlet distribution) to understand thresholds where balanced sampler and diffusion model approach become critical versus when simpler methods suffice.