---
ver: rpa2
title: Multimodality Invariant Learning for Multimedia-Based New Item Recommendation
arxiv_id: '2405.15783'
source_url: https://arxiv.org/abs/2405.15783
tags:
- modality
- item
- missing
- recommendation
- invariant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of new item recommendation in
  multimedia-based recommender systems where real-world items exhibit varying degrees
  of modality missing. The authors propose a novel framework called Multimodality
  Invariant Learning reCommendation (MILK) that tackles this issue from an invariant
  learning perspective.
---

# Multimodality Invariant Learning for Multimedia-Based New Item Recommendation

## Quick Facts
- arXiv ID: 2405.15783
- Source URL: https://arxiv.org/abs/2405.15783
- Reference count: 40
- New item recommendation performance improved by up to 14% in NDCG@20

## Executive Summary
This paper addresses the challenge of new item recommendation in multimedia systems where items exhibit varying degrees of modality missingness. The authors propose a novel Multimodality Invariant Learning framework (MILK) that tackles this problem from an invariant learning perspective. MILK consists of two key modules: a Cross-Modality Alignment Module (CMAM) that ensures semantic consistency across modalities, and a Cross-Environment Invariance Module (CEIM) that constructs heterogeneous environments using cyclic mixup to simulate any modality missing scenario. The framework is evaluated on three real-world datasets and demonstrates significant improvements over state-of-the-art methods, particularly in scenarios where modalities are missing during both training and testing.

## Method Summary
MILK addresses multimodal new item recommendation by learning invariant representations that remain stable across different modality missing scenarios. The framework uses two complementary modules: CMAM aligns representations across modalities to ensure semantic consistency and enable information transfer, while CEIM constructs M+1 heterogeneous environments via cyclic mixup sampling from Dirichlet distributions to simulate diverse missingness patterns. The model learns user preferences by optimizing for invariance across these environments, making it robust to any modality missing scenario encountered at test time. The final loss combines BPR loss, invariance variance penalty, alignment loss, and L2 regularization.

## Key Results
- MILK achieves up to 11.2% and 14.0% improvements in NDCG@20 on Amazon Baby and Clothing datasets respectively in the Full Training Missing Test setting
- The framework shows consistent improvements across three real-world datasets (Amazon Baby, Amazon Clothing, and TikTok)
- MILK significantly outperforms state-of-the-art methods including DUIF, multimodal variants of BPR, and other invariant learning approaches
- Both CMAM and CEIM modules contribute to performance gains, with CEIM providing the largest improvement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Invariant learning framework stabilizes recommendation performance under varying modality missingness
- Mechanism: By constructing heterogeneous environments via cyclic mixup and optimizing for invariance across these environments, the model learns user preference signals that remain stable regardless of which modalities are missing
- Core assumption: Users' underlying content preferences are stable and can be learned as invariant features across different modality combinations
- Evidence anchors:
  - [abstract] "We argue that users' inherent content preference is stable and better kept invariant to arbitrary modality missing environments"
  - [section 1] "Essentially, the difficulty is that there are multiple combinations with modality missing... In fact, users' inherent content preference is stable and better kept invariant with arbitraty modality missing"
  - [section 4.3] "We argue that users' inherent content preference is stable and better kept invariant to arbitrary modality missing scenarios"

### Mechanism 2
- Claim: Cyclic mixup creates diverse, unbalanced modality environments that generalize to any real-world missing pattern
- Mechanism: Sampling weights from Dirichlet distribution and applying cyclic shifts creates M+1 environments where different modalities dominate, ensuring full modality coverage while simulating real-world imbalances
- Core assumption: Real-world modality missingness follows patterns that can be approximated by Dirichlet-distributed modality proportions
- Evidence anchors:
  - [section 4.3] "We expect these environments to have some important properties: (1) Unbalance Proportion: On the one hand, all modalities should be included in each environment, otherwise, the model will be encouraged to ignore modality... (2) Full Modality Consideration: Different environments should be dominated by different modalities"
  - [section 4.3] "Inspired by mixup [ 27, 38]... we propose a novel cyclic mixup method to construct environments satisfying the above properties"
  - [section 5.3.3] Experimental comparison showing cyclic mixup outperforms alternatives

### Mechanism 3
- Claim: Cross-modality alignment allows each modality representation to supplement missing information from other modalities
- Mechanism: Alignment loss encourages semantic consistency between modality representations, creating redundancy that allows available modalities to compensate for missing ones
- Core assumption: Different modalities contain overlapping semantic information about items that can be aligned
- Evidence anchors:
  - [section 4.2] "We use alignment across modalities to guarantee the semantic consistency between representations of different modalities. At the same time, the alignment across modalities makes the information between the modalities be transferred to each other"
  - [section 4.2] "In MILK, we achieve this goal by optimizing the following alignment objective"
  - [section 5.4.1] Ablation study showing CMAM improves performance

## Foundational Learning

- Concept: Invariant Risk Minimization (IRM)
  - Why needed here: The core problem is distribution shift between training (complete modalities) and test (missing modalities), which IRM specifically addresses by learning features that generalize across environments
  - Quick check question: What is the key difference between empirical risk minimization and invariant risk minimization in terms of handling distribution shifts?

- Concept: Mixup and data augmentation strategies
  - Why needed here: The paper extends mixup beyond simple interpolation to create structured environments for invariant learning, addressing the challenge of limited training data for simulating missing modalities
  - Quick check question: How does cyclic mixup differ from standard mixup, and why is this difference important for modality missing scenarios?

- Concept: Modality alignment and representation learning
  - Why needed here: Without alignment, each modality representation would be isolated, preventing cross-modal information transfer when modalities are missing
  - Quick check question: What is the purpose of the alignment loss in the cross-modality alignment module, and how does it enable robustness to missing modalities?

## Architecture Onboarding

- Component map: Input multimodal features -> CMAM (extract + align) -> CEIM (environment construction + invariant optimization) -> Prediction
- Critical path: Input multimodal features → CMAM (extract + align) → CEIM (environment construction + invariant optimization) → Prediction
- Design tradeoffs:
  - More environments vs. computational cost (M+1 environments required)
  - Alignment strength vs. preserving modality-specific information (hyperparameter λ)
  - Dirichlet concentration (α) vs. environment diversity vs. stability
- Failure signatures:
  - Performance degrades significantly when certain modalities are missing in test data (alignment insufficient)
  - Model becomes unstable across different runs (invariant learning hyperparameters not tuned)
  - No improvement over baseline despite increased complexity (cyclic mixup not effective)
- First 3 experiments:
  1. Run baseline model (DUIF) on FTMT setting to establish performance drop baseline
  2. Test MILK without CEIM (only CMAM) to measure alignment contribution
  3. Test MILK without CMAM (only CEIM) to measure invariant learning contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different data imputation strategies affect model performance in the missing modality inference stage?
- Basis in paper: [explicit] The paper compares zero padding, mean imputation, and a mapping function approach in Table 3, showing mean and mapping function sometimes degrade performance.
- Why unresolved: The comparison is limited to three methods on two datasets. More sophisticated imputation techniques or domain-specific approaches are not explored.
- What evidence would resolve it: Comprehensive benchmarking of diverse imputation methods (e.g., k-NN, MICE, GAN-based) across multiple multimedia datasets with varying missingness patterns.

### Open Question 2
- Question: What is the optimal environment construction strategy for invariant learning in multimedia recommendation?
- Basis in paper: [explicit] The paper proposes cyclic mixup with Dirichlet distribution for environment construction, but also tests variants like fixed weights and no cyclic shift in Table 4.
- Why unresolved: The paper focuses on one specific construction method. Alternative environment construction approaches (e.g., clustering-based, adversarial) and their impact on performance remain unexplored.
- What evidence would resolve it: Systematic comparison of different environment construction methods (e.g., clustering, adversarial, curriculum learning) across multiple datasets and missingness scenarios.

### Open Question 3
- Question: How does the number of modalities impact the effectiveness of the proposed framework?
- Basis in paper: [inferred] The paper tests on datasets with 2-3 modalities (Baby, Clothing with visual/textual; TikTok with visual/acoustic/textual), but does not systematically vary the number of modalities.
- Why unresolved: The framework's performance across datasets with different numbers of modalities is not thoroughly analyzed. The impact of modality count on environment construction and alignment effectiveness is unclear.
- What evidence would resolve it: Controlled experiments varying the number of modalities in synthetic datasets and analyzing framework performance as modality count increases.

## Limitations

- The cyclic mixup approach assumes real-world modality missingness follows Dirichlet-distributed patterns, which may not capture all edge cases or structured missingness patterns
- The framework's performance relies on the assumption that user preferences remain stable across different modality combinations, which may not hold universally across all recommendation domains
- The computational cost increases with the number of environments (M+1) and modalities, potentially limiting scalability to datasets with many modalities

## Confidence

- **High confidence**: The technical implementation of the MILK framework and its experimental methodology are well-documented and reproducible.
- **Medium confidence**: The core claims about invariant learning effectiveness and the specific performance improvements, though supported by experiments.
- **Medium confidence**: The theoretical justification for cyclic mixup and cross-modality alignment, though empirical results are promising.

## Next Checks

1. **Stress Test Missing Modality Patterns**: Test MILK on datasets with structured modality missingness patterns (e.g., certain modalities always missing together) that may violate the Dirichlet assumption.
2. **Ablation on User Preference Stability**: Conduct experiments where user preferences are deliberately varied based on available modalities to test the invariance assumption.
3. **Scalability Analysis**: Evaluate MILK's performance and computational efficiency on datasets with more than three modalities to assess practical scalability.