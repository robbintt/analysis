---
ver: rpa2
title: 'NewsBench: A Systematic Evaluation Framework for Assessing Editorial Capabilities
  of Large Language Models in Chinese Journalism'
arxiv_id: '2403.00862'
source_url: https://arxiv.org/abs/2403.00862
tags:
- writing
- tasks
- safety
- journalistic
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces NewsBench, a comprehensive evaluation framework
  designed to assess the editorial capabilities of large language models (LLMs) in
  Chinese journalism. The framework focuses on two main criteria: journalistic writing
  proficiency and safety adherence.'
---

# NewsBench: A Systematic Evaluation Framework for Assessing Editorial Capabilities of Large Language Models in Chinese Journalism

## Quick Facts
- arXiv ID: 2403.00862
- Source URL: https://arxiv.org/abs/2403.00862
- Reference count: 34
- Key outcome: NewsBench framework evaluates LLM editorial capabilities in Chinese journalism, revealing GPT-4 and ERNIE Bot as top performers but highlighting weaknesses in journalistic ethics adherence

## Executive Summary
NewsBench is a comprehensive evaluation framework designed to assess the editorial capabilities of large language models (LLMs) in Chinese journalism. The framework focuses on two main criteria: journalistic writing proficiency and safety adherence, using 1,267 test samples across five editorial tasks, seven aspects, and 24 news domains. Two GPT-4-based automatic evaluation protocols were developed and validated against human evaluations. Testing 11 popular LLMs revealed that GPT-4 and ERNIE Bot achieved the highest scores in most categories, though all models showed relative weakness in adhering to journalistic ethics during creative writing tasks.

## Method Summary
The NewsBench framework constructs 1,267 test samples across five editorial tasks, seven aspects, and 24 news domains, using both open-ended generation tasks and multiple-choice questions. Two GPT-4-based automatic evaluation protocols assess LLM outputs for journalistic writing proficiency and safety adherence, validated through human evaluation by three annotators. The framework was tested on 11 LLMs capable of Chinese text generation, with performance measured on a 0-1 scale across the defined aspects.

## Key Results
- GPT-4 and ERNIE Bot achieved the highest scores across most evaluation categories
- All tested LLMs showed relative weakness in journalistic ethics adherence during creative writing tasks
- The GPT-4-based automatic evaluation protocols demonstrated high correlation with human evaluations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NewsBench enables automated, systematic evaluation of LLMs for Chinese journalistic writing proficiency and safety adherence by combining generative and multiple-choice tasks with GPT-4-based scoring protocols.
- Mechanism: The framework inputs tasks into targeted LLMs to generate responses, then uses GPT-4 to automatically score outputs against defined journalistic and safety aspects. Human validation confirms alignment with human judgment.
- Core assumption: GPT-4 can reliably and consistently evaluate Chinese journalistic writing quality and safety compliance.
- Evidence anchors:
  - [abstract] "We propose different GPT-4 based automatic evaluation protocols to assess LLM generations for short answer questions in terms of writing proficiency and safety adherence, and both are validated by the high correlations with human evaluations."
  - [section] "we have developed two distinctive automatic evaluation protocols... using GPT-4. Additionally, we've performed human validation to ensure the effectiveness of our evaluation framework and its alignment with human judgment."
  - [corpus] "Found 25 related papers... Large language models for automated PRISMA 2020 adherence checking" shows precedent for LLM-based automated evaluation in domain-specific tasks.
- Break condition: If GPT-4's scoring lacks correlation with human judgment in Chinese text evaluation, or if task complexity exceeds GPT-4's evaluation capacity.

### Mechanism 2
- Claim: The benchmark reveals specific weaknesses in LLM performance across editorial applications and safety aspects, guiding targeted model improvements.
- Mechanism: By systematically testing LLMs on 1,267 tasks across five applications, seven aspects, and 24 domains, the framework quantifies strengths and weaknesses, with detailed error analysis identifying problem areas like style alignment in creative writing.
- Core assumption: Task design effectively isolates and measures specific journalistic and safety capabilities.
- Evidence anchors:
  - [abstract] "Our comprehensive analysis of 11 LLMs highlighted GPT-4 and ERNIE Bot as top performers, yet revealed a relative deficiency in journalistic ethic adherence during creative writing tasks."
  - [section] "In the evaluation of Journalistic Writing Proficiency (JWP) multiple-choice tasks, GPT-4's performance in tasks requiring writing expansion and continuation was found to be unsatisfactory..."
  - [corpus] "Large language models for automated PRISMA 2020 adherence checking" demonstrates that systematic benchmarks can reveal performance gaps in specialized domains.
- Break condition: If tasks do not accurately reflect real-world journalistic challenges, or if evaluation metrics do not align with actual journalistic standards.

### Mechanism 3
- Claim: The framework's dual approach of generation and multiple-choice tasks provides comprehensive assessment coverage.
- Mechanism: Generation tasks test creative writing and safety under adversarial conditions, while multiple-choice tasks efficiently measure comprehension and discernment, together covering both generative and evaluative capabilities.
- Core assumption: Both task types are necessary to fully evaluate LLM capabilities in journalistic contexts.
- Evidence anchors:
  - [abstract] "our framework also incorporates both open-ended generation tasks and multiple-choice tasks."
  - [section] "The benchmark tasks are divided into two formats: open-ended generative questions and multiple-choice questions... The addition of multiple-choice questions enhances the ability to gauge LLMs' comprehension and discernment."
  - [corpus] "Found 25 related papers" suggests growing use of mixed-task approaches in LLM evaluation, though specific dual-task benchmarks are less common.
- Break condition: If one task type consistently fails to provide meaningful evaluation data, or if the combination does not improve overall assessment quality.

## Foundational Learning

- Concept: Chinese language characteristics (polysemy, context dependency)
  - Why needed here: Chinese text processing poses unique challenges for LLMs, affecting comprehension and output quality.
  - Quick check question: Why might a Chinese word with multiple meanings cause problems for an LLM in journalistic writing tasks?

- Concept: Journalistic ethics and safety standards (SPJ principles)
  - Why needed here: The framework evaluates adherence to specific journalistic and safety criteria relevant to editorial applications.
  - Quick check question: What are the four key principles of the Society of Professional Journalists (SPJ) that underpin journalistic ethics?

- Concept: Automated evaluation methodologies and correlation analysis
  - Why needed here: GPT-4-based scoring protocols require understanding of automated evaluation design and validation through human correlation.
  - Quick check question: What statistical measures would you use to validate that GPT-4's automated scores align with human judgments?

## Architecture Onboarding

- Component map:
  - Task Construction Module (10 junior journalists + 1 senior supervisor) -> Evaluation Protocol Engine (2 GPT-4-based protocols) -> Human Validation Layer (3 annotators) -> LLM Testing Interface (11 models) -> Results Aggregation and Analysis System

- Critical path: Task → LLM Input → GPT-4 Evaluation → Human Validation → Results Analysis

- Design tradeoffs:
  - Generation tasks provide depth but are harder to evaluate; multiple-choice tasks are easier to evaluate but may not capture full capabilities.
  - Relying solely on implicit LLM knowledge may limit evaluation of tasks requiring external verification.
  - Chinese-only focus enables deep specialization but limits multilingual applicability.

- Failure signatures:
  - Low correlation between GPT-4 scores and human annotations
  - Uneven task distribution across applications/aspects
  - LLM outputs consistently failing safety constraints
  - Human annotators unable to reach consensus

- First 3 experiments:
  1. Run a small pilot (100 tasks) across all 11 LLMs, measuring correlation between GPT-4 and human scores.
  2. Test task distribution balance by analyzing output across all applications and aspects.
  3. Evaluate safety protocol effectiveness by measuring LLM compliance rates on adversarial instructions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can evaluation protocols be adapted to handle multilingual journalistic content beyond Chinese?
- Basis in paper: [explicit] The authors note that the current dataset is exclusively in Chinese, limiting the framework's applicability to other languages.
- Why unresolved: The paper does not explore how the evaluation protocols would need to be modified or extended to support other languages, nor does it address the challenges of cross-linguistic evaluation in journalism.
- What evidence would resolve it: Experiments demonstrating the adaptation of the evaluation protocols to multiple languages, including validation against human evaluations in those languages.

### Open Question 2
- Question: What is the impact of incorporating external knowledge sources (e.g., fact-checking databases) on the accuracy of safety adherence evaluation?
- Basis in paper: [inferred] The authors mention that the current framework relies solely on the implicit knowledge within LLMs, which may lead to inaccuracies in scenarios requiring external evidence.
- Why unresolved: The paper does not investigate how integrating external knowledge sources would affect the evaluation results or the trade-offs involved.
- What evidence would resolve it: Comparative studies showing the difference in evaluation outcomes when using internal LLM knowledge versus external knowledge sources for safety adherence.

### Open Question 3
- Question: How can the evaluation framework be extended to assess AI-generated content for real-time news verification and plagiarism detection?
- Basis in paper: [inferred] The authors exclude applications like fact-checking and plagiarism detection due to their dependence on external knowledge, but acknowledge this as a gap in the framework.
- Why unresolved: The paper does not propose or test methods for incorporating real-time verification or plagiarism detection into the evaluation process.
- What evidence would resolve it: Development and testing of evaluation protocols that integrate real-time verification tools and plagiarism detection algorithms, with validation against human evaluators.

## Limitations

- The framework's GPT-4-based evaluation protocols require validation through specific correlation coefficients, which are not provided
- The benchmark focuses exclusively on Chinese language, limiting generalizability to other linguistic contexts
- Task construction relies on human annotators without specified inter-rater reliability measures

## Confidence

- High: The framework's comprehensive coverage of five editorial applications, seven aspects, and 24 news domains
- Medium: The identification of specific LLM weaknesses in journalistic ethics adherence
- Medium: The dual approach of generation and multiple-choice tasks for comprehensive assessment

## Next Checks

1. Request the exact correlation coefficients between GPT-4 automatic scores and human evaluations across all aspects and applications
2. Verify the inter-rater reliability scores among the three human annotators used for validation
3. Test the framework's sensitivity by evaluating whether it can detect known quality differences between established LLMs