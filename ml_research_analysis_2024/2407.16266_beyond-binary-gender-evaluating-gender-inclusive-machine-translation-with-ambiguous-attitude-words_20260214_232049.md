---
ver: rpa2
title: 'Beyond Binary Gender: Evaluating Gender-Inclusive Machine Translation with
  Ambiguous Attitude Words'
arxiv_id: '2407.16266'
source_url: https://arxiv.org/abs/2407.16266
tags:
- gender
- translation
- bias
- attitude
- words
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AmbGIMT, a gender-inclusive machine translation
  benchmark that evaluates bias beyond the binary male/female gender framework. The
  authors address the limitation of existing gender bias evaluations, which focus
  on binary genders and fail to capture bias against non-binary groups.
---

# Beyond Binary Gender: Evaluating Gender-Inclusive Machine Translation with Ambiguous Attitude Words

## Quick Facts
- arXiv ID: 2407.16266
- Source URL: https://arxiv.org/abs/2407.16266
- Reference count: 22
- Primary result: Non-binary gender contexts show significantly lower translation quality (over 10 COMET points lower) and more negative attitudes than binary-gender contexts

## Executive Summary
This paper introduces AmbGIMT, a gender-inclusive machine translation benchmark that evaluates bias beyond the binary male/female framework. The authors address the limitation of existing gender bias evaluations by proposing ambiguous attitude words and the Emotional Attitude Score (EAS) metric to quantify attitude tendencies in translations. Their benchmark includes 3,116 English-Chinese parallel sentences with 14 gender identity settings spanning sex, gender, and sexual orientation domains. Experimental results on three LLMs and one translation-specific model demonstrate that non-binary gender contexts exhibit significantly lower translation quality and more negative attitudes compared to binary-gender contexts, while lexical constraints can substantially reduce translation bias.

## Method Summary
The authors created the AmbGIMT dataset by collecting authentic English sentences containing ambiguous attitude words, synthesizing gender-neutral contexts, and replacing gender identity terms across 14 different settings. They evaluated translation quality using COMET and BLEU metrics, and introduced the EAS metric to quantify attitude shifts by comparing positive and negative pseudo-log-likelihood scores of ambiguous word translations. The study tested three LLMs (Llama-2-7b, Mistral-7B, MiniCPM-2B) and one translation-specific model (NLLB-200-3.3B) with and without lexical constraints to reduce bias.

## Key Results
- Non-binary gender contexts show over 10 COMET points lower translation quality in 5 out of 11 non-binary settings
- Lexical constraints decrease translation deviance by at least 40% and reduce shift bias rate to near zero in some models
- Word distribution analysis reveals that stereotypes impact translation mistakes and preferences across gender identity settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ambiguous attitude words are more sensitive to translation context than clear sentiment words
- Mechanism: Ambiguous words have multiple valid translations with different emotional valences. Translation models pick interpretations based on gender context, amplifying bias.
- Core assumption: The translation system's internal bias influences word choice even when semantic content is preserved
- Evidence anchors: [abstract] "These studies often rely on calculating the accuracy of gender pronouns or the masculine and feminine attributes of grammatical gender via the stereotypes triggered by occupations or sentiment words" [section 3.2] "The translations of ambiguous attitude words are more complicated than those that strongly convey positive or negative attitudes"

### Mechanism 2
- Claim: Non-binary gender contexts receive lower translation quality scores due to model training data imbalance
- Mechanism: Models trained primarily on binary gender data lack exposure to non-binary contexts, leading to poorer representations and translation outputs
- Core assumption: Training data distribution directly impacts translation performance on underrepresented groups
- Evidence anchors: [abstract] "non-binary gender contexts exhibit significantly lower translation quality (over 10 COMET points lower in 5 out of 11 non-binary settings)" [section 4.2] "the translation accuracy of neutral, male, and female is higher than all identity settings in the non-binary gender context"

### Mechanism 3
- Claim: Lexical constraints significantly reduce translation bias by providing explicit gender identity translations
- Mechanism: When models receive correct translations for identity terms, they don't need to rely on biased internal representations, improving both quality and reducing attitude shifts
- Core assumption: Models can follow explicit instructions when given clear constraints
- Evidence anchors: [abstract] "incorporating constraint context in prompts for gender identity terms can substantially reduce translation bias" [section 5.1] "The lexical constraint strategy shows significant improvements, which decrease the translation deviance of models by at least 40%"

## Foundational Learning

- Concept: Gender-inclusive language and its importance in NLP
  - Why needed here: Understanding why binary gender evaluation is insufficient requires grasping the broader context of gender diversity and inclusive language practices
  - Quick check question: What are the key differences between binary and non-binary gender evaluation in machine translation systems?

- Concept: Emotional Attitude Score (EAS) methodology
  - Why needed here: The EAS metric is central to quantifying attitude shifts in translations, requiring understanding of pseudo-log-likelihood and template-based evaluation
  - Quick check question: How does the EAS metric distinguish between positive and negative attitude shifts in translated ambiguous words?

- Concept: Machine translation quality metrics (COMET, BLEU)
  - Why needed here: Evaluating translation performance requires understanding different metrics and their strengths/limitations for bias assessment
  - Quick check question: When would COMET be more appropriate than BLEU for evaluating gender bias in machine translation?

## Architecture Onboarding

- Component map: Authentic sentence collection → Synthesis → Translation → Identity replacement → EAS calculation → Quality metric computation → Analysis
- Critical path: 1) Create gender-neutral source sentences with ambiguous attitude words, 2) Translate to target language while tracking identity contexts, 3) Apply EAS metric to quantify attitude shifts, 4) Analyze quality metrics across gender settings, 5) Test constraint-based bias mitigation
- Design tradeoffs: Ambiguous vs. clear sentiment words (nuanced bias detection vs. complex evaluation), manual vs. automated translation (quality vs. scalability), identity coverage vs. dataset size (diversity vs. sample size)
- Failure signatures: Translation quality gap persists despite constraints (bias too deeply embedded), EAS scores inconsistent across models (evaluation metric needs refinement), identity word translations skipped (model lacks internal knowledge)
- First 3 experiments: 1) Replicate COMET/BLEU comparison across binary vs. non-binary settings on a subset, 2) Test EAS consistency by having multiple models evaluate the same ambiguous word pairs, 3) Implement and measure lexical constraint effectiveness on a single model-dataset combination

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of machine translation models on gender-inclusive benchmarks change when using domain-specific fine-tuning versus general instruction-tuning?
- Basis in paper: [explicit] The paper shows that lexical constraints significantly improve translation quality and reduce gender bias, but questions remain about the optimal approach for model adaptation.
- Why unresolved: The paper only tests a few constraint strategies and does not explore domain-specific fine-tuning or instruction-tuning effects on gender bias reduction.
- What evidence would resolve it: Comparative experiments measuring translation quality and bias metrics (COMET, EAS) across models fine-tuned on gender-inclusive data versus those using lexical constraints.

### Open Question 2
- Question: Does the Emotional Attitude Score (EAS) metric capture nuanced cultural differences in emotional interpretation across languages?
- Basis in paper: [explicit] The paper uses EAS to quantify attitude tendencies but does not address potential cultural variations in how ambiguous words are interpreted across different languages and cultures.
- Why unresolved: The study focuses on English-Chinese translation pairs and assumes consistency in emotional interpretation without cross-cultural validation.
- What evidence would resolve it: Human evaluation studies across multiple language pairs to assess whether EAS scores align with cultural perceptions of emotional attitudes in translated texts.

### Open Question 3
- Question: How do non-binary gender identities impact the quality and bias of machine translation in languages with grammatical gender systems?
- Basis in paper: [explicit] The paper notes that Chinese is gender-neutral and questions the applicability of their approach to languages with grammatical gender.
- Why unresolved: The study only evaluates English-Chinese translation pairs, limiting insights into how non-binary identities are handled in gendered languages.
- What evidence would resolve it: Comparative experiments measuring translation quality and bias in languages with grammatical gender systems (e.g., German, French) versus gender-neutral languages like Chinese.

## Limitations
- The EAS metric relies on human judgment for attitude classification of ambiguous words, introducing potential subjectivity and reproducibility concerns
- The study demonstrates correlation between non-binary gender contexts and lower translation quality but cannot definitively establish causation from training data imbalance
- The lexical constraint effectiveness is shown quantitatively, but the underlying mechanism for why constraints reduce bias so substantially remains incompletely explained

## Confidence

- Translation quality gap between binary and non-binary contexts: Medium
- EAS metric validity for detecting attitude shifts: Medium
- Lexical constraints substantially reduce bias: High
- Model training data imbalance causes non-binary translation issues: Low-to-Medium

## Next Checks

1. Replicate the EAS metric validation by having multiple independent annotators classify the same ambiguous word pairs to assess inter-annotator agreement and metric reliability
2. Conduct ablation studies testing whether the translation quality gap persists when models are fine-tuned on balanced gender representation datasets
3. Implement controlled experiments varying the strength and specificity of lexical constraints to map the relationship between constraint granularity and bias reduction effectiveness