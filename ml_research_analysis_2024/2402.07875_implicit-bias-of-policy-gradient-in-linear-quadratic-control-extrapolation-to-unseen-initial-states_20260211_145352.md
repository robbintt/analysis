---
ver: rpa2
title: 'Implicit Bias of Policy Gradient in Linear Quadratic Control: Extrapolation
  to Unseen Initial States'
arxiv_id: '2402.07875'
source_url: https://arxiv.org/abs/2402.07875
tags:
- states
- initial
- training
- gradient
- extrapolation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how policy gradient methods for learning
  controllers in Linear Quadratic Regulator (LQR) problems generalize to unseen initial
  states. The authors analyze underdetermined LQR settings where the training objective
  admits multiple global minimizers, and show that the extent of extrapolation depends
  critically on the degree of exploration induced by the system when commencing from
  initial states seen in training.
---

# Implicit Bias of Policy Gradient in Linear Quadratic Control: Extrapolation to Unseen Initial States

## Quick Facts
- arXiv ID: 2402.07875
- Source URL: https://arxiv.org/abs/2402.07875
- Reference count: 40
- Policy gradient generalization to unseen initial states depends critically on exploration

## Executive Summary
This paper analyzes how policy gradient methods generalize in Linear Quadratic Regulator (LQR) problems when faced with unseen initial states. The authors investigate underdetermined settings where multiple optimal controllers exist and show that the implicit bias of policy gradient is fundamentally different from supervised learning - it does not minimize the Euclidean norm of controller parameters. Instead, the extent of extrapolation depends on the exploration induced by the system dynamics when starting from training initial states. Through theoretical analysis and experiments on both linear and nonlinear systems, the paper demonstrates that sufficient exploration is necessary and sufficient for near-perfect extrapolation, while insufficient exploration leads to poor generalization.

## Method Summary
The authors analyze policy gradient methods in underdetermined LQR settings where multiple globally optimal controllers exist. They establish theoretical bounds on the extrapolation error to unseen initial states based on the exploration properties of the system. The key insight is that policy gradient implicitly biases toward controllers that perform well under the exploration distribution induced by the training states. Experiments validate these findings using both linear systems and neural network controllers on nonlinear systems. The analysis reveals that unlike in supervised learning, policy gradient does not minimize the norm of controller parameters, and the exploration-induced implicit bias determines generalization performance.

## Key Results
- Policy gradient extrapolation depends critically on the degree of exploration induced by training initial states
- Insufficient exploration leads to no extrapolation, while sufficient exploration enables near-perfect generalization
- Policy gradient does not implicitly minimize the Euclidean norm of controller parameters, unlike in supervised learning
- Experiments confirm extrapolation behavior in both linear LQR and nonlinear systems with neural network controllers

## Why This Works (Mechanism)
The mechanism underlying the results is that policy gradient methods optimize controllers based on the exploration distribution induced by the system dynamics starting from training initial states. When multiple optimal controllers exist, the implicit bias of policy gradient selects the controller that performs best under this exploration distribution. If the exploration is insufficient (states concentrate near training initial states), the resulting controller may generalize poorly to truly unseen initial states. Conversely, sufficient exploration that covers the state space enables the policy gradient to find controllers that extrapolate well. This exploration-induced implicit bias is fundamentally different from the norm-minimizing bias observed in supervised learning.

## Foundational Learning

**Linear Quadratic Regulator (LQR)**: Optimal control framework with linear dynamics and quadratic cost functions. Needed to provide tractable theoretical analysis of policy gradient methods. Quick check: Verify the system matrices satisfy stabilizability and detectability conditions.

**Implicit Bias in Optimization**: The tendency of optimization algorithms to prefer certain solutions among multiple global optima. Critical for understanding generalization in underdetermined settings. Quick check: Compare solutions found by different optimization algorithms on the same underdetermined problem.

**Exploration in Control Systems**: The distribution of states visited during policy execution. Essential for understanding how training data coverage affects generalization. Quick check: Measure state visitation frequencies under different initial state distributions.

## Architecture Onboarding

Component Map: Training initial states -> System dynamics -> State exploration -> Policy gradient optimization -> Controller parameters -> Extrapolation to unseen states

Critical Path: Initial state distribution → State visitation distribution → Policy gradient updates → Controller parameters → Performance on unseen states

Design Tradeoffs: The paper highlights the tension between exploration (which improves generalization) and exploitation (which may lead to better performance on training states). This tradeoff must be carefully managed in practical applications.

Failure Signatures: Poor extrapolation occurs when exploration is insufficient, causing the policy gradient to converge to controllers that perform well only near training initial states but generalize poorly elsewhere.

First Experiments:
1. Compare extrapolation performance under different exploration intensities in linear LQR systems
2. Test the effect of regularization on implicit bias and extrapolation in underdetermined settings
3. Evaluate neural network controllers on nonlinear systems to verify extrapolation behavior beyond linear theory

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis assumes linear dynamics and quadratic costs, limiting applicability to complex real-world systems
- Underdetermined setting requires careful exploration design that may be difficult to implement in practice
- Extension to neural networks is empirical rather than rigorously analyzed, limiting theoretical guarantees

## Confidence
High: Role of exploration in determining extrapolation quality, failure of implicit norm minimization in policy gradient
Medium: Experimental validation on nonlinear systems, mechanisms for achieving optimal exploration
Low: None identified

## Next Checks
1. Test extrapolation bounds on nonlinear systems with varying model mismatch to assess robustness beyond theoretical assumptions
2. Implement and evaluate alternative exploration strategies to determine practical methods for achieving theoretical exploration conditions
3. Analyze sensitivity of extrapolation performance to regularization parameters in the cost function to understand effects on implicit bias properties