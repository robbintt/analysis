---
ver: rpa2
title: 'BASS: Batched Attention-optimized Speculative Sampling'
arxiv_id: '2404.15778'
source_url: https://arxiv.org/abs/2404.15778
tags:
- draft
- batch
- decoding
- latency
- bass
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces BASS, a batched speculative decoding system\
  \ that improves multi-sequence generation latency for large language models. By\
  \ extending speculative decoding to batch processing and implementing custom CUDA\
  \ kernels for ragged tensors, BASS achieves 2.15\xD7 speedup over optimized regular\
  \ decoding, with per-token latency of 5.8ms for 7.8B models on A100 GPUs at batch\
  \ size 8."
---

# BASS: Batched Attention-optimized Speculative Sampling

## Quick Facts
- arXiv ID: 2404.15778
- Source URL: https://arxiv.org/abs/2404.15778
- Reference count: 6
- Primary result: Achieves 2.15× speedup over optimized regular decoding with 5.8ms per-token latency for 7.8B models at batch size 8 on A100 GPUs

## Executive Summary
BASS introduces a batched speculative decoding system that significantly improves multi-sequence generation latency for large language models. By extending speculative decoding to batch processing and implementing custom CUDA kernels for ragged tensors, BASS achieves 2.15× speedup over regular decoding while reaching peak GPU utilization of 15.8%, more than 3× that of regular decoding. The system uses a dynamic heuristic to adjust draft lengths and employs two variants (BASS-PAD and BASS-SPLIT) to handle variable sequence lengths in batched attention calculations.

## Method Summary
BASS extends speculative decoding to batched settings by parallelizing across both batch and draft-token dimensions. The system uses a draft model to generate multiple tokens in parallel, then verifies them with the main model. Custom CUDA kernels handle ragged tensors that arise from variable draft acceptance rates across sequences. Two variants are implemented: BASS-PAD pads tensors to maximum sequence length, while BASS-SPLIT launches per-sequence kernels. A dynamic heuristic adjusts draft lengths based on acceptance rates to optimize performance for varying prompt characteristics.

## Key Results
- Achieves 2.15× speedup over optimized regular decoding
- Reaches peak GPU utilization of 15.8%, more than 3× regular decoding
- Generates sequences with HumanEval Pass@First of 43% and Pass@All of 61% within 2.5s time budget
- Per-token latency of 5.8ms for 7.8B models on A100 GPUs at batch size 8

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BASS improves GPU utilization by parallelizing across both batch and draft-token dimensions.
- Mechanism: Regular decoding processes tokens sequentially, limiting parallelism to one token at a time. BASS uses a draft model to generate multiple tokens in parallel, then verifies them with the main model. By batching multiple sequences, it amortizes memory I/O across sequences and draft tokens simultaneously.
- Core assumption: Speculative decoding correctness is preserved when applied to batched sequences with variable draft acceptance rates.
- Evidence anchors:
  - [abstract] "BASS increases GPU utilization by parallelism across both the batch dimension and the draft-token dimension."
  - [section] "BASS increases GPU utilization by parallelism across both the batch dimension and the draft-token dimension."
  - [corpus] Weak - no direct corpus evidence comparing GPU utilization between batched and single-sequence speculative decoding.
- Break condition: If draft model and main model alignment is poor, draft tokens get rejected frequently, reducing the parallelization benefit.

### Mechanism 2
- Claim: Custom CUDA kernels for ragged tensors enable efficient attention computation with variable sequence lengths.
- Mechanism: When batching speculative decoding, different sequences accept different numbers of draft tokens, creating ragged K, V, and P tensors. BASS-PAD pads these tensors to maximum sequence length, while BASS-SPLIT launches per-sequence kernels. Both approaches handle the ragged structure efficiently.
- Core assumption: The overhead of padding (BASS-PAD) or launching additional kernels (BASS-SPLIT) is outweighed by the benefits of processing variable-length sequences in batch.
- Evidence anchors:
  - [abstract] "We implement customized CUDA kernels to handle ragged tensors during attention calculation, which are a challenge posed by batched speculative decoding."
  - [section] "We propose two variants: BASS-PAD and BASS-SPLIT as illustrated in Figure 4(b) and Figure 4(c) respectively. They implement the two GEMMs differently and share the same softmax operation."
  - [corpus] Weak - no direct corpus evidence comparing CUDA kernel implementations for ragged tensors.
- Break condition: If sequence lengths are very similar across batch, the padding overhead in BASS-PAD becomes negligible and BASS-SPLIT's kernel launch overhead dominates.

### Mechanism 3
- Claim: Dynamic draft length adjustment heuristic optimizes performance for varying prompt characteristics.
- Mechanism: Algorithm 1 increases draft length when all sequences accept all draft tokens, and decreases it otherwise. The adjustment speed depends on current draft length and consecutive decrease steps. This adapts to prompt-specific draft-main model alignment.
- Core assumption: The alignment between draft and main models varies systematically with prompt characteristics, and this variation can be captured by draft acceptance rates.
- Evidence anchors:
  - [abstract] "We design a heuristic to dynamically adjust draft length for each step."
  - [section] "The rationale is to increase draft length when at least one sequence has accepted all draft tokens in the last speculative decoding step and to decrease it otherwise."
  - [corpus] Weak - no direct corpus evidence comparing dynamic vs fixed draft lengths.
- Break condition: If prompt characteristics are too diverse within a batch, the heuristic may settle on suboptimal draft lengths for some sequences.

## Foundational Learning

- Concept: Speculative decoding and its correctness criteria
  - Why needed here: BASS extends speculative decoding to batched setting, so understanding the original mechanism is essential.
  - Quick check question: What is the mathematical condition for accepting draft tokens in speculative decoding?

- Concept: GPU memory hierarchy and compute utilization
  - Why needed here: BASS's performance gains come from better GPU utilization, which requires understanding memory bandwidth vs compute throughput tradeoffs.
  - Quick check question: Why does regular auto-regressive decoding have low GPU utilization despite high theoretical compute capability?

- Concept: CUDA programming and kernel fusion
  - Why needed here: BASS uses custom CUDA kernels for ragged tensor operations and quantization, requiring understanding of GPU programming patterns.
  - Quick check question: What is the benefit of kernel fusion in the context of quantization and dequantization operations?

## Architecture Onboarding

- Component map: Draft model inference -> Main model verification -> Ragged tensor handling (BASS-PAD/BASS-SPLIT) -> Dynamic draft length controller -> Attention computation with custom CUDA kernels

- Critical path: Generate draft tokens → Verify with main model → Accept/reject tokens → Update KV cache → Continue generation
  - The attention computation in verification step is the most compute-intensive part

- Design tradeoffs:
  - BASS-PAD vs BASS-SPLIT: Padding vs kernel launch overhead
  - Dynamic vs fixed draft length: Adaptation overhead vs simplicity
  - Batch size vs latency: Larger batches improve utilization but increase per-sequence latency

- Failure signatures:
  - Low draft acceptance rate → Poor speedup despite batching
  - High padding overhead in BASS-PAD → Degraded performance with diverse sequence lengths
  - Suboptimal draft length → Frequent rejections or wasted computation

- First 3 experiments:
  1. Measure draft acceptance rate vs draft model size to find optimal draft model architecture
  2. Compare BASS-PAD vs BASS-SPLIT performance across different batch sizes and sequence length distributions
  3. Evaluate dynamic draft length adjustment vs fixed draft length across diverse prompts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal strategy for dynamically adjusting draft lengths in batched speculative decoding across heterogeneous sequences?
- Basis in paper: [explicit] The paper mentions using a heuristic algorithm to dynamically adjust draft lengths but notes this is based on empirical parameter choices (l0=7, lincre=2, lmod=10, llimit=32) without systematic optimization.
- Why unresolved: The current heuristic balances multiple competing factors (acceptance rates, compute efficiency, sequence diversity) but lacks theoretical grounding or automated optimization framework.
- What evidence would resolve it: Comparative experiments with alternative draft length adjustment strategies (reinforcement learning, adaptive gradient-based methods) across diverse task distributions and batch compositions.

### Open Question 2
- Question: How does the performance trade-off between BASS-PAD and BASS-SPLIT vary with different attention head counts and sequence length distributions?
- Basis in paper: [explicit] The paper observes that BASS-PAD is better when sequence lengths are similar while BASS-SPLIT excels with heterogeneous lengths, but doesn't systematically characterize this relationship.
- Why unresolved: The analysis is limited to specific model configurations without exploring the full design space of attention architectures and sequence length patterns.
- What evidence would resolve it: Comprehensive benchmarking across models with varying head counts (8, 16, 32, 64) and controlled sequence length distributions (uniform, bimodal, exponential) measuring both latency and GPU utilization.

### Open Question 3
- Question: What is the relationship between draft model architecture parameters (depth vs width) and the optimal latency-quality trade-off in speculative decoding?
- Basis in paper: [explicit] The paper shows that wider-but-shallow draft models outperform narrow-but-deep ones at fixed parameter count, but doesn't explore the full architectural design space.
- Why unresolved: The analysis is limited to three specific draft models without systematic exploration of how different architectural choices affect acceptance rates, latency, and overall system efficiency.
- What evidence would resolve it: Controlled experiments varying draft model depth (2-16 layers) and width (256-4096 hidden dimensions) while measuring token acceptance rates, generation latency, and accuracy metrics across multiple tasks.

## Limitations
- Performance gains are measured specifically at batch size 8 on A100 GPUs and may not scale linearly
- All experiments conducted on A100 GPUs; performance on other architectures may differ
- Evaluation uses HumanEval benchmarks and standard prompts, but real-world applications may have more diverse prompt characteristics

## Confidence

**High confidence**: The core claim that batched speculative decoding can improve throughput compared to single-sequence speculative decoding is well-supported by experimental results. The mathematical framework for speculative decoding correctness is sound, and observed GPU utilization improvements are consistent with parallelization mechanisms.

**Medium confidence**: The specific performance numbers (2.15× speedup, 5.8ms latency, 15.8% GPU utilization) are well-documented for tested configuration but may not generalize across different hardware, batch sizes, or model architectures.

**Low confidence**: The long-term stability and behavior of the dynamic draft length heuristic across diverse, real-world workloads remains unproven.

## Next Checks
1. **Batch size scaling analysis**: Systematically measure performance across batch sizes from 1 to 32 to determine optimal batch size for different model scales and identify scaling behavior of the 2.15× speedup claim.

2. **Hardware architecture portability**: Replicate experiments on different GPU architectures (H100, L4, RTX 4090) to validate performance improvements and GPU utilization gains are consistent across hardware generations.

3. **Diverse prompt benchmarking**: Evaluate BASS on broader set of prompts including long-form content generation, code with mixed complexity, and domain-specific tasks to assess robustness of dynamic draft length heuristic and identify failure modes.