---
ver: rpa2
title: 'Text-to-Song: Towards Controllable Music Generation Incorporating Vocals and
  Accompaniment'
arxiv_id: '2404.09313'
source_url: https://arxiv.org/abs/2404.09313
tags:
- text
- music
- synthesis
- melodist
- accompaniment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Melodist, a novel two-stage text-to-song
  synthesis method that incorporates both singing voice and accompaniment generation.
  The proposed method addresses the challenges of synthesizing complex musical compositions
  by leveraging a tri-tower contrastive pretraining framework to learn effective text
  representations for controllable accompaniment generation.
---

# Text-to-Song: Towards Controllable Music Generation Incorporating Vocals and Accompaniment

## Quick Facts
- arXiv ID: 2404.09313
- Source URL: https://arxiv.org/abs/2404.09313
- Reference count: 29
- Primary result: Text-to-song synthesis with vocals and accompaniment using two-stage generation and tri-tower contrastive pretraining

## Executive Summary
This paper introduces Melodist, a novel two-stage text-to-song synthesis method that generates singing voice from music scores in Stage 1, then generates accompaniment conditioned on the vocal and natural language prompts in Stage 2. The approach leverages a tri-tower contrastive pretraining framework to learn effective text representations for controllable accompaniment generation. Experiments on a newly constructed Chinese song dataset demonstrate that Melodist can synthesize songs with comparable quality and style consistency compared to state-of-the-art baselines, achieving a MOS score of 3.90 and a FAD score of 3.80.

## Method Summary
Melodist employs a two-stage generation approach where Stage 1 uses a multi-scale Transformer to generate singing voice from music scores (lyrics, notes, duration), and Stage 2 generates accompaniment conditioned on the vocal output and natural language prompts. The system uses acoustic tokens extracted via SoundStream's residual vector quantizer as model targets, with a unit-based vocoder (modified BigVGAN) for waveform reconstruction. A key innovation is the tri-tower contrastive pretraining framework that jointly trains text, vocal, and accompaniment encoders to align representations in a shared embedding space, enabling better controllability through natural language prompts.

## Key Results
- Achieved MOS score of 3.90 and FAD score of 3.80 on Chinese song dataset
- Outperformed baseline models in both objective (FAD, KLD, CLAP) and subjective metrics (MOS, SMOS, FFE, OVL, REL, MEL)
- Demonstrated effectiveness of tri-tower contrastive pretraining for text-to-song controllability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-stage synthesis reduces modeling burden by decomposing complex song generation into manageable subtasks
- Mechanism: Stage 1 generates singing voice from music score; Stage 2 generates accompaniment conditioned on vocal and prompts
- Core assumption: Vocals and accompaniments have strong semantic and acoustic alignment within the same song
- Evidence anchors:
  - [abstract]: "Melodist generates singing voice from the music score in Stage 1, then generates accompaniment given vocal in Stage 2."
  - [section 3.2]: "The fundamental ideas behind the two-stage generation can be summarized as follows: 1) The accompaniment and voice signals inside the same song strongly relate to each other."
  - [corpus]: Weak - related works focus on single-stage models without strong comparative evidence for this decomposition approach.
- Break condition: If vocal-accompaniment alignment is weak or varies significantly across genres, the independence assumption fails and quality degrades.

### Mechanism 2
- Claim: Tri-tower contrastive pretraining improves text-to-song controllability by learning aligned embeddings across text, vocal, and accompaniment modalities
- Mechanism: Jointly trains text encoder with vocal and accompaniment encoders using contrastive loss to align representations in shared embedding space
- Core assumption: Global characteristics of text prompts can be effectively mapped to both vocal and accompaniment patterns through contrastive learning
- Evidence anchors:
  - [abstract]: "Melodist leverages tri-tower contrastive pretraining to learn more effective text representation for controllable V2A synthesis."
  - [section 4]: "We introduce a tri-tower training scheme with contrastive loss that jointly embeds text, vocals, and accompaniments into an aligned space."
  - [corpus]: Weak - only mentions similar contrastive approaches (CLAP, MuLan) but not tri-modal contrastive learning specifically.
- Break condition: If the embedding space cannot capture the complex relationships between text concepts and audio characteristics, controllability suffers.

### Mechanism 3
- Claim: Using acoustic tokens instead of raw waveforms enables efficient modeling while maintaining sufficient audio quality
- Mechanism: Extracts discrete acoustic tokens via SoundStream's residual vector quantizer, models token sequences with multi-scale transformer, reconstructs waveforms via unit-based vocoder
- Core assumption: Discrete acoustic tokens preserve essential audio information while being more tractable for sequence modeling than raw waveforms
- Evidence anchors:
  - [section 3.3]: "Acoustic tokens, as the predicted target, are extracted by SoundStream... These compressed representations can be used to reconstruct waveforms by the decoder subsequently."
  - [section 3.6]: "Instead of the decoder of Soundstream, we adopt a unit-based vocoder utilizing GAN-based architecture for waveform generation from acoustic units."
  - [corpus]: Weak - mentions SoundStream and BigVGAN but lacks direct comparison evidence for this specific token-based approach.
- Break condition: If the quantization process loses critical audio information, reconstruction quality degrades significantly.

## Foundational Learning

- Concept: Conditional probability modeling in sequence generation
  - Why needed here: The text-to-song task is formulated as modeling p(S|C,P) where generation depends on both music score and natural language prompts
  - Quick check question: How does conditioning on multiple inputs (score + prompt) differ from standard autoregressive generation?

- Concept: Contrastive learning for cross-modal alignment
  - Why needed here: Tri-tower framework uses contrastive loss to align text representations with corresponding vocal and accompaniment embeddings
  - Quick check question: What's the difference between standard contrastive learning and the tri-tower extension used here?

- Concept: Vector quantization for audio compression
  - Why needed here: SoundStream's residual vector quantizer extracts discrete acoustic tokens that serve as model targets
  - Quick check question: Why might discrete tokens be preferable to continuous representations for this generation task?

## Architecture Onboarding

- Component map: 
  - Stage 1: Music score → Multi-scale transformer → Acoustic tokens → Unit-based vocoder → Singing voice
  - Stage 2: Singing voice + Text prompt → Multi-scale transformer → Acoustic tokens → Unit-based vocoder → Accompaniment
  - Final stage: Mix vocal and accompaniment waveforms

- Critical path: Music score → Stage 1 → Vocal output → Stage 2 → Final song output

- Design tradeoffs:
  - Two-stage vs end-to-end: Simpler modeling but potential error accumulation
  - Token-based vs raw waveform: Computational efficiency vs potential quality loss
  - Contrastive pretraining vs direct training: Better controllability vs additional training complexity

- Failure signatures:
  - Stage 1 fails: Poor vocal quality, pitch errors, unnatural pronunciation
  - Stage 2 fails: Accompaniment doesn't match vocal melody, ignores text prompts
  - Reconstruction fails: Artifacts in final audio, poor timbre

- First 3 experiments:
  1. Test Stage 1 independently with held-out music scores to verify basic SVS quality
  2. Test Stage 2 with reference vocals and prompts to verify accompaniment generation
  3. Test end-to-end pipeline with simple prompts to verify basic functionality before complex controls

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the tri-tower contrastive pre-training framework compare to other cross-modal learning approaches like MUSICAL (Manco et al., 2022) or MULLAN (Huang et al., 2022a) in terms of effectiveness for text-to-song synthesis?
- Basis in paper: [explicit] The paper states that the tri-tower framework "outperforms other text encoders with the highest CLAP and REL score and the lowest KLD score" and that "including both vocal and accompaniment helps the model learn to ground more attribute-related song concepts."
- Why unresolved: While the paper provides some comparative results, it does not provide a comprehensive evaluation against all relevant baselines, leaving room for further exploration of the relative strengths and weaknesses of different approaches.
- What evidence would resolve it: A more extensive comparison study involving a wider range of cross-modal learning approaches, evaluated on the same dataset and metrics used in the paper, would provide a clearer understanding of the relative effectiveness of the tri-tower framework.

### Open Question 2
- Question: What are the specific challenges and limitations of using source separation techniques like Demucs (Défossez et al., 2019) for extracting vocal and accompaniment tracks from mixed audio in the context of text-to-song synthesis?
- Basis in paper: [explicit] The paper mentions that "the reliance on source separation imposes a great challenge to improving audio quality" and that "while the current source separation methods remain suboptimal, it is urgent to improve the quality of source separation."
- Why unresolved: The paper acknowledges the limitations of source separation but does not provide a detailed analysis of the specific challenges and potential solutions.
- What evidence would resolve it: A thorough investigation into the performance of different source separation techniques on the dataset used in the paper, along with an analysis of the errors and artifacts introduced by these methods, would shed light on the specific challenges and potential avenues for improvement.

### Open Question 3
- Question: How does the two-stage generation approach in Melodist compare to a fully end-to-end text-to-song synthesis model in terms of audio quality, temporal correspondence, and style consistency?
- Basis in paper: [explicit] The paper states that "it is hard to achieve end-to-end generation since the song contains much more information (pitch variation, timbre, emotion, instruments, etc.) than the music score" and that "neither serves as the suitable prior." However, it does not provide a direct comparison with a fully end-to-end model.
- Why unresolved: The paper presents a two-stage approach as a solution to the challenges of text-to-song synthesis, but it does not explore the potential benefits or drawbacks of a fully end-to-end approach.
- What evidence would resolve it: A comparative study between Melodist and a fully end-to-end text-to-song synthesis model, trained on the same dataset and evaluated on the same metrics, would provide insights into the trade-offs between the two approaches.

## Limitations

- Evaluation relies entirely on human MOS scores without comparison to objective metrics that could provide more granular quality assessment
- Chinese song dataset was crawled from unspecified sources without clear curation criteria, raising questions about dataset quality and representativeness
- Two-stage decomposition approach may accumulate errors between stages that aren't fully captured in the evaluation

## Confidence

*High Confidence:* The technical implementation details of the multi-scale transformer architecture and the general two-stage generation framework are well-specified and reproducible. The use of acoustic tokens via SoundStream and the unit-based vocoder approach is standard in the field.

*Medium Confidence:* The tri-tower contrastive pretraining mechanism is conceptually sound, but the specific implementation details (temperature parameters, projection dimensions, exact alignment strategies) are underspecified. The reported MOS score of 3.90 is plausible given the architecture but requires independent verification.

*Low Confidence:* The effectiveness of the two-stage approach compared to potential end-to-end alternatives is not rigorously established. The dataset construction process lacks transparency regarding source diversity and quality control. The generalization claims to other languages or musical styles are not substantiated.

## Next Checks

1. **Independent MOS replication**: Have independent evaluators rate a held-out test set of generated songs using the same 5-point scale to verify the reported MOS of 3.90 is reproducible and not subject to selection bias.

2. **Ablation study on pretraining**: Train Melodist variants with and without the tri-tower contrastive pretraining to quantify the actual contribution of this component to the final quality, rather than relying on qualitative descriptions.

3. **Cross-dataset generalization test**: Evaluate Melodist on an English-language singing dataset (such as those used in prior SVS work) to assess whether the model generalizes beyond the Chinese corpus it was trained on, or if performance degrades significantly.