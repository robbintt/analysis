---
ver: rpa2
title: 'Shrinking Your TimeStep: Towards Low-Latency Neuromorphic Object Recognition
  with Spiking Neural Network'
arxiv_id: '2401.01912'
source_url: https://arxiv.org/abs/2401.01912
tags: []
core_contribution: This paper proposes a novel approach to improve the performance
  of spiking neural networks (SNNs) at low latencies. The authors address the issue
  of high latency in existing SNNs by introducing a method called Shrinking SNN (SSNN),
  which divides the SNN into multiple stages with progressively shrinking timesteps.
---

# Shrinking Your TimeStep: Towards Low-Latency Neuromorphic Object Recognition with Spiking Neural Network

## Quick Facts
- arXiv ID: 2401.01912
- Source URL: https://arxiv.org/abs/2401.01912
- Reference count: 13
- Primary result: 6.55% to 21.41% accuracy improvement at very low latencies compared to baseline SNNs

## Executive Summary
This paper addresses the challenge of high inference latency in spiking neural networks (SNNs) for neuromorphic object recognition. The authors propose Shrinking SNN (SSNN), a novel framework that divides SNNs into multiple stages with progressively shrinking timesteps, reducing temporal redundancy. A temporal transformer is incorporated to preserve information during timestep reduction, while multiple early classifiers are added during training to mitigate gradient issues. Experiments on CIFAR10-DVS, N-Caltech101, and DVS-Gesture datasets demonstrate significant accuracy improvements at low latencies.

## Method Summary
The Shrinking SNN (SSNN) framework improves SNN performance at low latencies by dividing the network into stages with progressively shrinking timesteps, reducing temporal redundancy. A temporal transformer is used to smoothly transform the temporal scale and preserve information during timestep shrinkage. Multiple early classifiers are added during training to mitigate the mismatch between surrogate gradients and true gradients, as well as gradient vanishing/exploding issues. The approach is evaluated on neuromorphic datasets including CIFAR10-DVS, N-Caltech101, and DVS-Gesture.

## Key Results
- SSNN achieves 6.55% to 21.41% accuracy improvement over baseline SNNs at very low latencies
- Significant performance gains demonstrated on CIFAR10-DVS, N-Caltech101, and DVS-Gesture datasets
- Outperforms existing SNNs in low-latency scenarios

## Why This Works (Mechanism)
SSNN works by addressing two key challenges in SNNs: temporal redundancy and gradient issues. By dividing the network into stages with shrinking timesteps, it reduces unnecessary temporal computation while maintaining performance. The temporal transformer ensures smooth information preservation during timescale transformation, preventing loss of critical temporal features. Early classifiers help mitigate gradient problems by providing multiple optimization targets throughout the temporal evolution.

## Foundational Learning
- **Spiking Neural Networks (SNNs)**: Event-driven neural networks that process information through discrete spikes. Needed because they offer event-based processing suitable for neuromorphic data.
- **Temporal Transformers**: Attention-based mechanisms for processing sequential data. Required for smooth temporal scale transformation without information loss.
- **Surrogate Gradients**: Approximate gradients used to train SNNs with backpropagation. Critical for enabling gradient-based optimization despite non-differentiable spiking behavior.
- **Early Classification**: Training with multiple classification heads at different temporal stages. Helps mitigate gradient vanishing/exploding and provides robustness to latency variations.
- **Temporal Redundancy**: Unnecessary temporal information that doesn't contribute to the final decision. Reducing this improves efficiency and reduces latency.

## Architecture Onboarding

**Component Map**: Input -> Early Classifiers -> Temporal Transformer -> SSNN Stages -> Output

**Critical Path**: Input → Early Classifiers → Temporal Transformer → SSNN Stages → Final Classifier

**Design Tradeoffs**: 
- Smaller timesteps reduce latency but may lose temporal information
- More stages improve granularity but increase complexity
- Early classifiers improve training stability but add computational overhead

**Failure Signatures**:
- Performance degradation indicates information loss during timestep shrinkage
- Training instability suggests gradient mismatch or vanishing/exploding issues
- Baseline SNN performance below expected values indicates implementation issues

**First Experiments**:
1. Verify temporal transformer preserves information by analyzing intermediate representations
2. Test baseline SNN accuracy before implementing SSNN modifications
3. Evaluate sensitivity to early classifier coefficients (λ) and gradient norms

## Open Questions the Paper Calls Out
None

## Limitations
- Performance heavily depends on correct implementation of temporal transformer and early classifier modules
- Specific architectural details of VGG-9 and ResNet-18 variants used are not fully specified
- Substantial accuracy gains require careful tuning of all components

## Confidence
- Core approach validity: Medium
- Implementation reproducibility: Medium
- Performance claims: Medium

## Next Checks
1. Verify temporal transformer implementation correctly preserves information during timestep reduction
2. Test sensitivity of performance to early classifier coefficients (λ) and gradient norms during training
3. Compare baseline SNN implementation accuracy against published results before implementing SSNN modifications