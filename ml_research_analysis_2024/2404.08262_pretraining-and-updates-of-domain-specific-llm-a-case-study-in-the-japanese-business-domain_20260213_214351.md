---
ver: rpa2
title: 'Pretraining and Updates of Domain-Specific LLM: A Case Study in the Japanese
  Business Domain'
arxiv_id: '2404.08262'
source_url: https://arxiv.org/abs/2404.08262
tags:
- japanese
- llms
- language
- business
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first Japanese business domain-specific
  LLM and explores strategies for updating it with recent knowledge. The authors pretrain
  a 13B parameter Japanese LLM from scratch on a curated dataset of 220B tokens, with
  19.8% business domain-specific content.
---

# Pretraining and Updates of Domain-Specific LLM: A Case Study in the Japanese Business Domain

## Quick Facts
- arXiv ID: 2404.08262
- Source URL: https://arxiv.org/abs/2404.08262
- Authors: Kosuke Takahashi; Takahiro Omi; Kosuke Arima; Tatsuya Ishigaki
- Reference count: 12
- Primary result: First Japanese business domain-specific LLM achieving 0.90 accuracy on no-context QA

## Executive Summary
This paper presents the first Japanese business domain-specific LLM, a 13B parameter model pretrained from scratch on 220B tokens with 19.8% business domain content. The authors evaluate it on a newly created business QA benchmark across three settings and demonstrate that domain-specific pretraining improves accuracy on business questions without sacrificing general knowledge. They also explore updating the model with recent business documents, finding that mixing 10% older documents with new ones prevents catastrophic forgetting while maintaining high accuracy on recent business questions.

## Method Summary
The authors pretrain a 13B parameter Japanese LLM from scratch using Llama2 architecture on 220B tokens, with 19.8% business domain-specific content curated from publicly available web pages. They evaluate the model on a newly created Business Question Benchmark with 50 questions across three QA settings: no context, automatic retrieval, and manual retrieval. For updates, they continually pretrain the model with recent business documents (October-November 2023) using a mixture strategy controlled by hyperparameter r for non-latest document proportion.

## Key Results
- Pretrained model achieves 0.90 accuracy on no-context business QA, outperforming other Japanese LLMs
- Mixing 10% older documents with new ones prevents catastrophic forgetting during updates
- Model maintains general knowledge while excelling at domain-specific tasks
- Public release of both pretrained model and business QA benchmark

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific pretraining improves accuracy on domain questions without external context
- Mechanism: Curated Business Corpus provides specialized business knowledge that enhances the model's internal knowledge base
- Core assumption: Business domain knowledge is sufficiently distinct from general knowledge to benefit from dedicated training
- Evidence anchors:
  - [abstract]: "our pretrained model improves QA accuracy without losing general knowledge"
  - [section 2.1]: "The Curated Business Corpus was built by curating publicly available web pages published up to September 2023...identified relevant pages using predefined URLs and a list of cue words"
  - [corpus]: Strong - The paper explicitly describes the creation of a domain-specific corpus with clear selection criteria
- Break condition: If the domain knowledge overlap between general and business domains is high, the benefit would diminish

### Mechanism 2
- Claim: Mixing recent and older documents during updates prevents catastrophic forgetting
- Mechanism: The r hyperparameter controls the ratio of new vs. old data, maintaining knowledge balance
- Core assumption: Catastrophic forgetting can be mitigated by strategic data mixing rather than architectural solutions
- Evidence anchors:
  - [abstract]: "a proper mixture of the latest and older texts in the training data for the update is necessary"
  - [section 2.4]: "we continually pretrain our LLM with recent business documents with various ways of the mixture"
  - [corpus]: Moderate - The corpus contains both recent (October-November 2023) and older documents, but the specific mixing ratios are experimental
- Break condition: If the knowledge domains of old and new data are too divergent, mixing may not prevent forgetting

### Mechanism 3
- Claim: Language-specific pretraining improves performance on non-English tasks
- Mechanism: Filtering out non-Japanese content ensures the model focuses on Japanese linguistic patterns
- Core assumption: Multilingual pretraining dilutes language-specific performance, making monolingual training beneficial
- Evidence anchors:
  - [abstract]: "The development of Large Language Models (LLMs) in various languages has been advancing, but the combination of non-English languages with domain-specific contexts remains underexplored"
  - [section 2.2]: "Language identification is crucial for language-specific datasets...We used a two-method pipeline to identify non-Japanese documents"
  - [corpus]: Strong - The paper describes a rigorous filtering pipeline with multiple methods for language identification
- Break condition: If the language identification is imperfect, model performance could suffer from residual non-target language content

## Foundational Learning

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: The paper addresses this specific problem when updating the model with new business documents
  - Quick check question: What happens to a model's performance on old tasks when it's trained only on new data without any old data mixed in?

- Concept: Domain adaptation through pretraining
  - Why needed here: The core contribution involves pretraining a Japanese business-specific model from scratch
  - Quick check question: How does pretraining on domain-specific data differ from fine-tuning a general model on domain data?

- Concept: Evaluation metrics for question answering
  - Why needed here: The paper uses accuracy on a manually evaluated benchmark to assess model performance
  - Quick check question: Why might automatic metrics like BLEU or BERTScore be insufficient for evaluating factual correctness in business domain QA?

## Architecture Onboarding

- Component map: 13B parameter decoder-only transformer (Llama2 architecture), trained on 220B tokens with 19.8% business domain content, using AWS Trainium with 16 trn1.32xlarge instances
- Critical path: Data collection → Filtering (language ID, noise removal, deduplication) → Pretraining → Instruction tuning → Evaluation
- Design tradeoffs: Scratch pretraining vs. continual pretraining from multilingual base, domain-specific vs. general data ratios, AWS Trainium vs. other accelerators
- Failure signatures: Performance degradation on general tasks when focusing too heavily on domain data, catastrophic forgetting when updating with new data, evaluation bias from limited benchmark size
- First 3 experiments:
  1. Test language identification filtering by running a small subset through the pipeline and manually checking non-Japanese content
  2. Validate catastrophic forgetting by training a model on only new data and testing on old domain questions
  3. Test the mixing ratio by training models with r=0.0, r=0.1, r=0.3 and comparing validation losses on both old and new data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal ratio of recent to older texts for updating LLMs to prevent catastrophic forgetting while maintaining high accuracy on recent business questions?
- Basis in paper: [explicit] The paper conducts experiments on updating the LLM with various ratios of recent to older texts, finding that a 10% mixture of older documents with new ones prevents catastrophic forgetting while maintaining high accuracy.
- Why unresolved: While the paper identifies a promising ratio (10% older documents), it acknowledges that the optimal ratio may vary depending on the specific domain, the rate of change in the domain, and the characteristics of the LLM being updated. Further research is needed to establish general guidelines for different scenarios.
- What evidence would resolve it: Conducting experiments across multiple domains with varying rates of change and different LLM architectures to identify consistent patterns in optimal update ratios.

### Open Question 2
- Question: How does the performance of domain-specific LLMs trained from scratch compare to those continuously pretrained on existing multilingual models across different domains and languages?
- Basis in paper: [inferred] The paper compares its Japanese business domain-specific LLM (trained from scratch) with Japanese LLMs that underwent continual pretraining from multilingual models. While the scratch-trained model excels in no-context QA, continual pretraining models perform better in comprehension tasks requiring context. The paper suggests this might be due to the larger dataset used in continual pretraining.
- Why unresolved: The comparison is limited to Japanese business domain and a few Japanese LLMs. It's unclear whether the findings generalize to other domains and languages. Additionally, the paper only explores continual pretraining starting from Llama2, not other strong existing models.
- What evidence would resolve it: Conducting comprehensive experiments comparing scratch-trained and continually pretrained LLMs across multiple domains, languages, and starting models to determine when each approach is more effective.

### Open Question 3
- Question: What are the most effective evaluation metrics for domain-specific LLMs, particularly when assessing factual correctness and response appropriateness?
- Basis in paper: [explicit] The paper acknowledges the limitations of automatic metrics (e.g., BLEU, METEOR, BERTScore) in assessing factual correctness, which is crucial in the business domain. It opts for manual evaluation based on content faithfulness and response appropriateness, disregarding redundant parts of responses.
- Why unresolved: While manual evaluation addresses the shortcomings of automatic metrics, it is resource-intensive and may introduce subjectivity. The paper doesn't explore alternative approaches or combinations of metrics that could balance accuracy, efficiency, and objectivity in evaluating domain-specific LLMs.
- What evidence would resolve it: Developing and validating new evaluation metrics or frameworks that effectively capture factual correctness and response appropriateness in domain-specific contexts, potentially leveraging hybrid approaches combining automatic and human evaluation.

## Limitations
- Limited benchmark size with only 50 questions across three settings, raising concerns about statistical significance
- Heavy reliance on manual evaluation, which is resource-intensive and potentially subjective
- Substantial computational requirements (13B parameters, AWS Trainium infrastructure) limiting reproducibility
- Insufficient testing of the trade-off between domain specialization and general capability retention

## Confidence
- High Confidence - Pretraining methodology and corpus creation process are well-documented and follow established practices
- Medium Confidence - Evaluation results showing 0.90 accuracy on NoContext-QA are promising but limited by small benchmark size
- Low Confidence - Claims about practical utility in real business scenarios are not directly tested

## Next Checks
1. **Benchmark Expansion and Statistical Validation** - Expand the Business Question Benchmark to 200+ questions with stratified sampling across business subdomains. Perform statistical power analysis to determine minimum sample size for detecting meaningful performance differences between models.

2. **Cross-Domain Generalization Testing** - Evaluate the pretrained and updated models on a comprehensive suite of general Japanese language tasks (beyond the lm-evaluation-harness) to quantify the domain-generalization trade-off. Include tasks like common sense reasoning, general knowledge QA, and creative writing to assess retained capabilities.

3. **Ablation Study on Data Mixing Ratios** - Conduct a systematic ablation study testing r values from 0.0 to 1.0 in 0.1 increments, measuring both performance retention on old tasks and acquisition on new tasks. Plot Pareto frontiers to identify optimal trade-offs and test whether more sophisticated curriculum learning approaches could outperform simple mixing.