---
ver: rpa2
title: 'RDBE: Reasoning Distillation-Based Evaluation Enhances Automatic Essay Scoring'
arxiv_id: '2407.13781'
source_url: https://arxiv.org/abs/2407.13781
tags:
- scoring
- essay
- reasoning
- language
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Reasoning Distillation-Based Evaluation (RDBE)
  for automatic essay scoring (AES), addressing the lack of interpretability in existing
  approaches. RDBE integrates reasoning distillation to explain model scores while
  improving performance.
---

# RDBE: Reasoning Distillation-Based Evaluation Enhances Automatic Essay Scoring

## Quick Facts
- arXiv ID: 2407.13781
- Source URL: https://arxiv.org/abs/2407.13781
- Reference count: 5
- Primary result: RDBE achieves state-of-the-art QWK scores on DREsS N ew dataset while providing interpretability through reasoning generation

## Executive Summary
This paper introduces Reasoning Distillation-Based Evaluation (RDBE) for automatic essay scoring (AES), addressing the lack of interpretability in existing approaches. RDBE integrates reasoning distillation to explain model scores while improving performance. The method uses a large language model (LLM) to generate reasoning for essay scores, then fine-tunes a smaller model to replicate this reasoning and scoring process. Experiments on the DREsS N ew dataset show RDBE outperforms zero-shot LLM generation and autoregressive baselines across all scoring rubrics (content, organization, language, and total score), achieving state-of-the-art QWK scores. The approach enhances interpretability and performance, with potential applications to other text evaluation tasks.

## Method Summary
RDBE is a two-step framework that first uses Llama-3-70B to generate reasoning for each essay score in the training data, then fine-tunes a LongT5-Base model on this augmented dataset where the target output includes both the reasoning and the score. The model is trained using cross-entropy loss with AdamW optimizer for 15 epochs at batch size 8. During inference, the model takes an essay and rubric as input and generates reasoning followed by the predicted score. This approach distills interpretability and scoring capability from a large LLM into a smaller, more deployable model.

## Key Results
- RDBE outperforms zero-shot LLM generation and autoregressive baselines across all scoring rubrics (content, organization, language, and total score)
- Achieves state-of-the-art QWK scores on DREsS N ew dataset
- Outperforms Llama-3-70B despite having only 0.03% of its parameters
- Enhances interpretability by generating explanations for assigned scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RDBE improves AES performance by forcing the model to generate reasoning during fine-tuning
- Mechanism: During fine-tuning, the model learns to generate reasoning and interpretation before outputting the score, which forces deeper understanding of the essay content
- Core assumption: Generating reasoning before scoring leads to better feature extraction and understanding
- Evidence anchors:
  - [abstract]: "This interpretive capability is acquired during training by leveraging generated reasoning from a large language model (LLM) to distill a small language model (SLM)"
  - [section]: "The output consists of the corresponding reasoning and interpretation generated by the LLM, followed by '—> ' and the score for that rubric"
- Break condition: If the reasoning generation becomes too difficult or noisy, it may actually degrade performance rather than improve it

### Mechanism 2
- Claim: RDBE achieves state-of-the-art performance with a much smaller model compared to LLMs
- Mechanism: By distilling reasoning capabilities from a large LLM into a smaller model, RDBE achieves comparable or better performance with fewer parameters
- Core assumption: Reasoning distillation effectively transfers knowledge from larger to smaller models
- Evidence anchors:
  - [abstract]: "RDBE outperforms both zero-shot LLM generation and generation from a baseline fine-tuned model"
  - [section]: "RDBE outperforms Llama-3-70B despite having only 0.03% of its parameters"
- Break condition: If the distilled reasoning quality degrades significantly, the performance advantage over LLMs may disappear

### Mechanism 3
- Claim: RDBE enhances interpretability of AES systems by generating explanations for scores
- Mechanism: The model is trained to generate reasoning that explains why a particular score was assigned, making the scoring process transparent
- Core assumption: Users benefit from understanding the reasoning behind automated scores
- Evidence anchors:
  - [abstract]: "RDBE integrates interpretability to elucidate the rationale behind model scores"
  - [section]: "Your role is to provide insights into strengths and weaknesses related to the score by analyzing only both the quality of presence (if present) or absence of the aspects directly mentioned in the rubric's explanation"
- Break condition: If the generated reasoning is consistently poor or irrelevant, users may lose trust in the system

## Foundational Learning

- Concept: Quadratic Weighted Kappa (QWK) metric
  - Why needed here: QWK is the official evaluation metric for the DREsS N ew dataset and measures agreement between model predictions and human scores
  - Quick check question: What does a QWK score of 0.730 for the total score indicate about RDBE's performance?

- Concept: Encoder-decoder transformer architecture
  - Why needed here: The LongT5 model used in RDBE is an encoder-decoder architecture that can handle long sequences and generate text
  - Quick check question: How does an encoder-decoder model differ from an encoder-only model in terms of output capabilities?

- Concept: Knowledge distillation
  - Why needed here: RDBE uses reasoning distillation to transfer capabilities from a large LLM to a smaller SLM
  - Quick check question: In what way does reasoning distillation differ from traditional knowledge distillation in machine learning?

## Architecture Onboarding

- Component map: LLM (Llama-3-70B) -> Reasoning generation -> LongT5 fine-tuning -> Essay + rubric -> Reasoning + score

- Critical path: Essay text + rubric → LongT5 → reasoning → score
  The reasoning generation is the distinguishing feature that differentiates RDBE from standard AES

- Design tradeoffs:
  - Performance vs. interpretability: Generating reasoning improves interpretability but may slightly impact scoring speed
  - Model size vs. capability: Smaller models are more deployable but may have lower absolute performance ceiling
  - Synthetic data quality: The quality of LLM-generated reasoning directly impacts downstream performance

- Failure signatures:
  - If reasoning quality is poor but scores are accurate, the model may be gaming the task without true understanding
  - If scores are poor but reasoning is good, the reasoning generation may be decoupled from scoring decisions
  - If both are poor, the fine-tuning process or data quality may be the issue

- First 3 experiments:
  1. Test LongT5 baseline without reasoning generation on DREsS N ew to establish baseline performance
  2. Generate reasoning using Llama-3-70B on a small subset of DREsS N ew data to validate the reasoning quality
  3. Fine-tune LongT5 on the full reasoning-enhanced dataset and evaluate QWK scores across all rubrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of reasoning generated by Llama-3-70B compare to that of GPT-4 for the reasoning distillation process?
- Basis in paper: [explicit] The authors acknowledge that their limited budget prevented them from using more accurate models like GPT-4 for data synthesis, which could have generated higher-quality data and improved the performance of small language models in interpretation and scoring.
- Why unresolved: The paper only uses Llama-3-70B for generating reasoning due to budget constraints, so there is no direct comparison with GPT-4's reasoning quality.
- What evidence would resolve it: Conducting experiments using GPT-4 to generate reasoning and comparing the resulting performance of RDBE with that using Llama-3-70B-generated reasoning would provide a direct comparison.

### Open Question 2
- Question: Can RDBE be effectively applied to other text evaluation tasks beyond essay scoring, such as evaluating scientific papers or creative writing?
- Basis in paper: [explicit] The authors suggest that RDBE's principles can be adapted for other tasks involving the evaluation of long-form generated text by customizing tasks and scoring rubrics.
- Why unresolved: The paper only demonstrates RDBE's effectiveness on the DREsS New dataset for essay scoring, so its applicability to other text evaluation tasks remains untested.
- What evidence would resolve it: Applying RDBE to different text evaluation tasks, such as evaluating scientific papers or creative writing, and comparing its performance with existing methods would demonstrate its generalizability.

### Open Question 3
- Question: What is the impact of different reasoning distillation techniques on the performance of RDBE?
- Basis in paper: [inferred] The paper introduces reasoning distillation as a key component of RDBE but does not explore alternative distillation techniques or their impact on performance.
- Why unresolved: The paper focuses on a specific reasoning distillation approach without comparing it to other potential techniques.
- What evidence would resolve it: Experimenting with different reasoning distillation techniques, such as self-training or co-training, and comparing their impact on RDBE's performance would provide insights into the effectiveness of various approaches.

## Limitations

- The quality and consistency of LLM-generated reasoning data is uncertain, with no quantitative measures of reasoning quality provided
- Limited comparison with other contemporary AES approaches beyond the autoregressive baseline
- Results confined to a single dataset (DREsS N ew), raising questions about generalizability across different essay scoring tasks or languages

## Confidence

**High Confidence**: The mechanism that generating reasoning during fine-tuning can improve AES performance has strong empirical support through the QWK score improvements across all four rubrics.

**Medium Confidence**: The claim about RDBE achieving state-of-the-art performance with a much smaller model is supported by the 0.03% parameter comparison to Llama-3-70B, but lacks direct comparison with other contemporary AES approaches.

**Low Confidence**: The scalability claim for other text evaluation tasks is entirely speculative, with no empirical evidence provided beyond the essay scoring domain.

## Next Checks

1. **Reasoning Quality Assessment**: Conduct human evaluation of a sample of the LLM-generated reasoning to measure relevance, coherence, and alignment with rubric criteria.

2. **Cross-Dataset Generalization Test**: Evaluate RDBE on at least one additional AES dataset (e.g., ASAP or Kaggle essay scoring datasets) to verify whether the performance improvements generalize beyond DREsS N ew.

3. **Ablation Study on Reasoning Components**: Perform experiments removing either the reasoning generation step or the reasoning components from the target output during fine-tuning to isolate the contribution of reasoning distillation versus standard knowledge distillation.