---
ver: rpa2
title: 'Stitching Gaps: Fusing Situated Perceptual Knowledge with Vision Transformers
  for High-Level Image Classification'
arxiv_id: '2402.19339'
source_url: https://arxiv.org/abs/2402.19339
tags:
- image
- images
- embeddings
- knowledge
- relative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of detecting abstract concepts
  (e.g., comfort, danger, freedom) in images, where traditional deep learning approaches
  often underperform. The authors propose a neuro-symbolic method that fuses situated
  perceptual knowledge from a knowledge graph with vision transformer embeddings.
---

# Stitching Gaps: Fusing Situated Perceptual Knowledge with Vision Transformers for High-Level Image Classification

## Quick Facts
- arXiv ID: 2402.19339
- Source URL: https://arxiv.org/abs/2402.19339
- Reference count: 40
- Primary result: Hybrid KGE-ViT methods achieve 0.33 Macro F1 on ARTstract dataset for abstract concept detection

## Executive Summary
This work addresses the challenge of detecting abstract concepts (e.g., comfort, danger, freedom) in images, where traditional deep learning approaches often underperform. The authors propose a neuro-symbolic method that fuses situated perceptual knowledge from a knowledge graph with vision transformer embeddings. They automatically extract perceptual semantic units (objects, emotions, actions, etc.) from images, integrate them into the ARTstract Knowledge Graph (AKG), and compute KG embeddings. These are then fused with ViT embeddings using both absolute and relative representation techniques, including hybrid concatenation and Hadamard product methods. The proposed hybrid KGE-ViT approaches outperform existing state-of-the-art methods, achieving a Macro F1 score of 0.33 on the ARTstract dataset.

## Method Summary
The method extracts perceptual semantic units (PSUs) from images using pre-trained models for objects, emotions, actions, colors, art styles, and captions. These PSUs are aligned to ConceptNet and used to construct the ARTstract Knowledge Graph (AKG) using the SituAnnotate ontology. TransE is used to compute KG embeddings, with AC-related triples removed to prevent data leakage. Vision Transformer (ViT) embeddings are extracted from the same images. Both KGE and ViT embeddings are transformed using relative representation with 700 anchors (100 per class). The relative KGE and ViT embeddings are fused using concatenation and Hadamard product methods, then classified using an MLP with cross-entropy loss (lr=0.001, 50 epochs).

## Key Results
- Hybrid KGE-ViT methods surpass state-of-the-art approaches with 0.33 Macro F1 on ARTstract dataset
- Relative representation significantly improves KGE-based abstract concept classification
- KGE embeddings capture more abstract and semantic scene elements compared to ViT's pixel-level visual features
- Fusion of relative KGE and relative ViT embeddings achieves the highest F1 score in the task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Relative representation improves KGE performance by aligning image representations with semantic clusters of abstract concepts
- Mechanism: The relative representation method transforms each image embedding by comparing it to a subset of anchor samples from the training data, creating representations that emphasize semantic similarity to known AC clusters
- Core assumption: Images sharing the same abstract concept label will have semantically similar representations when compared to AC-specific anchor samples
- Evidence anchors:
  - [abstract] "The adoption of the relative representation method significantly bolsters KGE-based AC image classification"
  - [section] "Our results suggest that the relative representation method improves KGE-based models by providing more meaningful cluster-level semantic information"
  - [corpus] Weak - no direct corpus evidence for relative representation on KGE embeddings specifically
- Break condition: If anchor selection does not capture the semantic structure of ACs, or if ACs are not well-represented by cluster-level semantics

### Mechanism 2
- Claim: Hybrid KGE-ViT embeddings achieve superior performance by combining semantic knowledge with pixel-level visual features
- Mechanism: The Hadamard product and concatenation of relative KGE and relative ViT embeddings creates a representation that leverages both abstract semantic understanding (from KGE) and detailed visual features (from ViT)
- Core assumption: Abstract concepts require both high-level semantic context and low-level visual features for accurate classification
- Evidence anchors:
  - [abstract] "our hybrid KGE-ViT methods surpass state-of-the-art approaches"
  - [section] "the combination of Relative KGE and Relative ViT embeddings showcase the highest F1 score attained in this task"
  - [corpus] Weak - no direct corpus evidence for KGE-ViT hybrid approaches
- Break condition: If one modality dominates the hybrid representation, or if the modalities are incompatible in latent space

### Mechanism 3
- Claim: KGE embeddings capture high-level semantic relationships that ViT embeddings miss, particularly for culturally contextual concepts
- Mechanism: The ARTstract-KG encodes perceptual semantics and commonsense knowledge from cultural images, creating embeddings that represent abstract concepts through their relationships rather than visual appearance
- Core assumption: Abstract concepts like "freedom" or "comfort" are better represented through their semantic relationships and cultural contexts than through visual features alone
- Evidence anchors:
  - [abstract] "KGE embeddings represent more abstract and semantic scene elements"
  - [section] "KGE embeddings appear to encapsulate more 'high-level' semantic features compared to ViT embeddings"
  - [section] "KGE successfully associates the test image with semantically relevant training instances, whereas ViT fails to encode similarity"
- Break condition: If the knowledge graph lacks sufficient coverage of the abstract concepts, or if cultural biases in the KG dominate the representations

## Foundational Learning

- Concept: Knowledge Graph Embeddings (KGE)
  - Why needed here: KGEs transform the structured knowledge from ARTstract-KG into continuous vector representations that can be combined with visual features
  - Quick check question: What is the key difference between TransE and other KGE methods like DistMult or ComplEx?

- Concept: Relative Representation
  - Why needed here: Relative representation transforms absolute embeddings into a form that emphasizes semantic similarity to anchor samples, which is crucial for AC classification
  - Quick check question: How does the choice of anchor samples affect the quality of relative representations?

- Concept: Hybrid Fusion Methods
  - Why needed here: Different fusion methods (concatenation vs Hadamard product) have different properties for combining KGE and ViT embeddings
  - Quick check question: When would concatenation be preferred over Hadamard product for combining embeddings?

## Architecture Onboarding

- Component map: Image → PS extraction → KG construction → KGE computation → ViT features → relative representations → fusion → classification
- Critical path: Image → Perceptual Semantic Unit Extractors → ARTstract-KG → TransE KGE → ViT features → Relative representation → Hybrid fusion → MLP classifier
- Design tradeoffs:
  - Using relative vs absolute representations: relative improves KGE but may degrade ViT
  - Fusion method choice: concatenation preserves all information but may include noise; Hadamard product emphasizes agreement but may lose complementary information
  - Anchor selection strategy: random selection vs AC-balanced selection
- Failure signatures:
  - KGE performance worse than random: likely KG construction or KGE computation issues
  - ViT performance drops with relative representation: anchor selection may be introducing noise
  - Hybrid performance no better than single modality: fusion method may not be capturing complementarity
- First 3 experiments:
  1. Test KGE-only classification with different KGE methods (TransE vs DistMult vs ComplEx) to establish baseline
  2. Test relative representation impact by varying anchor selection strategies (random vs AC-balanced)
  3. Compare fusion methods (concatenation vs Hadamard product) on a subset of the data to understand complementarity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed method scale with larger knowledge graphs or more complex image datasets?
- Basis in paper: [explicit] The authors note that the ARTstract-KG contains over 1.9 million triples derived from 14,000 images, but the scalability of their method to larger knowledge graphs or more complex datasets is not explored
- Why unresolved: The paper focuses on the ARTstract dataset and does not provide experiments or analysis on larger or more complex datasets
- What evidence would resolve it: Experiments demonstrating the performance of the method on larger knowledge graphs and more complex image datasets, along with an analysis of computational resources and scalability

### Open Question 2
- Question: What are the specific contributions of each perceptual semantic unit (PS) to the overall performance of the model?
- Basis in paper: [inferred] The paper mentions that the method extracts various PS units (actions, emotions, objects, etc.) but does not provide a detailed analysis of the individual contributions of each PS unit to the model's performance
- Why unresolved: The paper does not include ablation studies or experiments that isolate the impact of each PS unit on the model's performance
- What evidence would resolve it: Ablation studies that systematically remove or modify individual PS units and measure their impact on the model's performance, along with an analysis of which PS units are most critical for different abstract concepts

### Open Question 3
- Question: How does the proposed method handle images from diverse cultural contexts beyond Euro-Western-centric perspectives?
- Basis in paper: [explicit] The authors acknowledge that the ARTstract dataset is mainly composed of Euro-Western-centric perspectives, which may introduce biases
- Why unresolved: The paper does not explore how the method performs on images from diverse cultural contexts or discuss strategies for mitigating cultural biases
- What evidence would resolve it: Experiments evaluating the method's performance on images from diverse cultural contexts, along with an analysis of cultural biases and strategies for improving the method's generalizability across cultures

## Limitations
- Limited direct evidence for relative representation effectiveness specifically on KGE embeddings
- Knowledge graph coverage uncertainty - no evaluation of ARTstract-KG completeness for the 7 abstract concepts
- Fusion method validation gap - no comparison of different fusion strategies beyond the two tested methods

## Confidence
- **High**: KGE-ViT hybrid approaches outperform single-modality methods on ARTstract dataset
- **Medium**: Relative representation improves semantic clustering for AC classification
- **Medium**: KGE embeddings capture more abstract semantic features than ViT embeddings

## Next Checks
1. Conduct ablation study on relative representation with varying anchor selection strategies (random vs AC-balanced) to isolate its contribution
2. Test alternative KGE methods (DistMult, ComplEx) and fusion strategies to verify TransE + Hadamard product optimality
3. Evaluate ARTstract-KG coverage by measuring missing edges for AC-related concepts and their impact on embedding quality