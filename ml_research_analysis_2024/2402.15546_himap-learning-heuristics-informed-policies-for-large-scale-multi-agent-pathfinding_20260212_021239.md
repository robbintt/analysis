---
ver: rpa2
title: 'HiMAP: Learning Heuristics-Informed Policies for Large-Scale Multi-Agent Pathfinding'
arxiv_id: '2402.15546'
source_url: https://arxiv.org/abs/2402.15546
tags:
- agents
- multi-agent
- pathfinding
- himap
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HiMAP introduces a novel approach to large-scale multi-agent pathfinding
  (MAPF) using imitation learning with heuristic guidance. The method trains a neural
  network to imitate expert collision-free paths generated by a heuristic solver (EECBS)
  on small-scale instances.
---

# HiMAP: Learning Heuristics-Informed Policies for Large-Scale Multi-Agent Pathfinding

## Quick Facts
- arXiv ID: 2402.15546
- Source URL: https://arxiv.org/abs/2402.15546
- Authors: Huijie Tang; Federico Berto; Zihan Ma; Chuanbo Hua; Kyuree Ahn; Jinkyoo Park
- Reference count: 21
- One-line primary result: HiMAP achieves success rates up to 0.8 for 512 agents on 40√ó40 maps and 256 agents on 80√ó80 maps

## Executive Summary
HiMAP introduces a novel approach to large-scale multi-agent pathfinding (MAPF) using imitation learning with heuristic guidance. The method trains a neural network to imitate expert collision-free paths generated by a heuristic solver (EECBS) on small-scale instances. During inference, HiMAP employs several techniques: preventing re-visitation using history tracking, adapting softmax temperature for exploration, treating completed agents as obstacles, and conflict-free planning with action masking and correction. Experiments on 40√ó40 and 80√ó80 maps with 30% obstacle density show competitive performance among imitation-learning-only MAPF approaches while maintaining simple training and implementation.

## Method Summary
HiMAP uses imitation learning to train a neural network (PRIMAL-like architecture) on expert collision-free paths generated by EECBS solver on small-scale instances. The model learns to predict actions (up, down, left, right, stay) based on agent observations consisting of a 9√ó9 FOV plus goal vector. During inference, several techniques improve performance: preventing re-visitation using history tracking, adapting softmax temperature for exploration, treating completed agents as obstacles, and conflict-free planning with action masking and correction. The approach is decentralized, with each agent making decisions based on local observations, enabling scalability to hundreds of agents on large maps.

## Key Results
- Success rates up to 0.8 for 512 agents on 40√ó40 maps with 30% obstacle density
- Success rates up to 0.8 for 256 agents on 80√ó80 maps with 30% obstacle density
- Demonstrates competitive performance among imitation-learning-only MAPF approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Imitation learning with heuristic guidance enables HiMAP to solve large-scale MAPF by training on expert collision-free paths from EECBS solver
- Mechanism: HiMAP uses a neural network trained to imitate expert actions (up, down, left, right, stay) derived from EECBS-generated paths on small-scale instances. During inference, the model uses the learned policy to predict actions based on agent observations.
- Core assumption: The expert paths from EECBS on small instances are representative enough to generalize to larger instances during inference
- Evidence anchors:
  - [abstract] "We train on small-scale instances using a heuristic policy as a teacher that maps each single agent observation information to an action probability distribution"
  - [section] "We use EECBS [8] to generate a series of expert collision-free paths on randomly generated environments"
- Break condition: Performance degrades significantly if the small-scale training data lacks diversity or doesn't capture the complexity of larger-scale scenarios

### Mechanism 2
- Claim: Inference techniques (history tracking, softmax temperature adaptation, completed agent treatment, conflict-free planning) improve HiMAP's performance on large-scale instances
- Mechanism: During pathfinding, HiMAP implements several strategies: preventing re-visitation using history tracking, adapting softmax temperature for exploration, treating completed agents as obstacles, and conflict-free planning with action masking and correction
- Core assumption: These inference techniques effectively address specific failure modes that emerge during large-scale pathfinding
- Evidence anchors:
  - [section] "During pathfinding, we adopt several strategies to improve performance such as Preventing Re-Visit, Treating Completed Agents as Obstacles and Softmax Temperature Adaptation"
  - [section] "We implement a two-stage technique to correct invalid actions to enable conflict-free planning"
- Break condition: If the inference techniques introduce computational overhead that outweighs their benefits, or if they create new failure modes

### Mechanism 3
- Claim: Decentralized approach with partial observability enables scalable MAPF solution
- Mechanism: Each agent makes decisions based on its own 9√ó9 FOV observation plus a vector to its goal, processed through a neural network. This decentralized decision-making allows parallel computation and scalability.
- Core assumption: Local observations are sufficient for agents to make globally consistent decisions when combined with the learned policy
- Evidence anchors:
  - [section] "Each agent ùëñ at location ùë£ùë° ùëñ has a four-channel observation ùëúùë° ùëñ in a 9 √ó 9 Field of View (FOV) centered around itself, plus a 3 √ó 1 vector pointing from its own position to its goal"
  - [section] "During pathfinding, we adopt several inference techniques to improve performance"
- Break condition: When local observations are insufficient to avoid complex deadlocks or when coordination failures cascade across agents

## Foundational Learning

- Concept: Imitation Learning
  - Why needed here: HiMAP uses imitation learning to train a neural network to mimic expert collision-free paths generated by a heuristic solver (EECBS)
  - Quick check question: What is the primary difference between imitation learning and reinforcement learning in the context of MAPF?

- Concept: Multi-Agent Pathfinding (MAPF)
  - Why needed here: HiMAP is specifically designed to solve MAPF problems, finding collision-free paths for multiple agents in shared environments
  - Quick check question: What are the key challenges that make MAPF particularly difficult for traditional algorithms?

- Concept: Decentralized vs Centralized Planning
  - Why needed here: HiMAP uses a decentralized approach where each agent makes decisions based on local observations, enabling scalability
  - Quick check question: How does decentralized decision-making contribute to scalability in large-scale MAPF?

## Architecture Onboarding

- Component map: Neural network (PRIMAL architecture without value and blocking heads) ‚Üí Observation processing (4-channel 9√ó9 FOV + goal vector) ‚Üí Inference techniques (history tracking, temperature adaptation, completed agent treatment, conflict-free planning) ‚Üí Action selection ‚Üí Environment update

- Critical path: Observation ‚Üí Neural network inference ‚Üí Inference technique application ‚Üí Action selection ‚Üí State update ‚Üí Next observation

- Design tradeoffs: HiMAP trades optimal pathfinding for scalability and simplicity, using imitation learning rather than reinforcement learning to avoid lengthy training and unstable convergence

- Failure signatures: Oscillating agents around dead ends (history tracking failure), agents getting stuck in local minima (temperature adaptation failure), conflicts persisting despite correction (conflict-free planning failure)

- First 3 experiments:
  1. Train HiMAP on small-scale instances (e.g., 4-16 agents on 10√ó10 maps) and verify it can reproduce expert paths
  2. Test inference techniques individually on medium-scale instances (e.g., 32 agents on 40√ó40 maps) to measure their individual impact
  3. Scale up to large instances (e.g., 256 agents on 80√ó80 maps) and measure success rate with all inference techniques enabled

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of HiMAP scale with larger map sizes and higher agent densities beyond the tested 40√ó40 and 80√ó80 configurations?
- Basis in paper: [inferred] The paper tests success rates on 40√ó40 and 80√ó80 maps with 30% obstacle density, but does not explore larger scales or different densities.
- Why unresolved: The experiments are limited to specific map sizes and obstacle densities, leaving scalability questions unanswered for more complex scenarios.
- What evidence would resolve it: Testing HiMAP on maps larger than 80√ó80 with varying obstacle densities (e.g., 40%, 50%) and reporting success rates would provide concrete data on scalability limits.

### Open Question 2
- Question: How do the four inference techniques contribute individually to HiMAP's overall performance, and what is their relative importance?
- Basis in paper: [explicit] The paper mentions "Baseline+History", "Baseline+TCAO", and "Baseline" ablations, but doesn't provide detailed ablation studies showing individual contributions of each technique.
- Why unresolved: While the paper shows combined effects of inference techniques, it doesn't isolate the impact of each technique through comprehensive ablation studies.
- What evidence would resolve it: Conducting experiments with each inference technique disabled individually and reporting success rate changes would quantify each technique's contribution.

### Open Question 3
- Question: How does HiMAP's performance compare with state-of-the-art RL-based approaches when trained and evaluated under identical conditions?
- Basis in paper: [inferred] The paper acknowledges that "at the current stage, HiMAP cannot outperform state-of-the-art learning-based techniques" but doesn't provide direct comparative experiments.
- Why unresolved: The paper lacks head-to-head comparisons with RL methods under controlled conditions, making it difficult to assess relative performance.
- What evidence would resolve it: Training and evaluating HiMAP and leading RL approaches (e.g., SACHA, SCRIMP) on the same datasets with identical evaluation metrics would provide definitive comparative results.

## Limitations

- HiMAP cannot outperform state-of-the-art learning-based techniques at the current stage
- The decentralized approach may struggle with complex coordination scenarios requiring global information
- Performance depends heavily on the quality and diversity of expert paths from EECBS solver

## Confidence

- High confidence: Imitation learning with heuristic guidance and inference techniques are directly supported by the abstract and method sections
- Medium confidence: Decentralized approach's effectiveness for scalability, based on performance metrics but with some architectural uncertainty
- Low confidence: Exact implementation details of observation space and conflict-free planning rules, which are not fully specified

## Next Checks

1. Verify the exact observation space representation by checking if the four channels are: (a) occupancy grid, (b) agent positions, (c) goal positions, and (d) path history, and confirm their specific encoding.

2. Implement the conflict-free planning technique by testing both vertex conflict resolution (through action masking) and swapping conflict resolution (through the two-stage correction) on simple test cases.

3. Conduct ablation studies to measure the individual contribution of each inference technique (history tracking, temperature adaptation, completed agent treatment) by testing them separately on medium-scale instances.