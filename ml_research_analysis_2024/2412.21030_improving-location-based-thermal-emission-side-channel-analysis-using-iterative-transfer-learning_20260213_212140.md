---
ver: rpa2
title: Improving Location-based Thermal Emission Side-Channel Analysis Using Iterative
  Transfer Learning
arxiv_id: '2412.21030'
source_url: https://arxiv.org/abs/2412.21030
tags:
- learning
- data
- power
- thermal
- used
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving side-channel attacks
  on AES-128 encryption using thermal emission data. The proposed method employs iterative
  transfer learning to enhance deep learning models, specifically multilayer perceptron
  (MLP) and convolutional neural network (CNN), for predicting encryption keys from
  thermal map images.
---

# Improving Location-based Thermal Emission Side-Channel Analysis Using Iterative Transfer Learning

## Quick Facts
- **arXiv ID**: 2412.21030
- **Source URL**: https://arxiv.org/abs/2412.21030
- **Reference count**: 15
- **Primary result**: Iterative transfer learning improves side-channel attack performance on AES-128, especially with limited data, reducing measurement-to-disclosure (MTD) from 54.7 to 53.8 for thermal maps.

## Executive Summary
This paper addresses the challenge of improving side-channel attacks on AES-128 encryption using thermal emission data. The proposed method employs iterative transfer learning to enhance deep learning models, specifically multilayer perceptron (MLP) and convolutional neural network (CNN), for predicting encryption keys from thermal map images. Traditional approaches train separate models for each byte of the key without considering inter-byte correlations. The iterative transfer learning technique first trains a model for one key byte and then uses this trained model as a pretrained model for subsequent bytes, leveraging parameter similarities across bytes. Experiments demonstrate that this approach significantly improves attack performance, especially when data is limited.

## Method Summary
The method applies iterative transfer learning to deep learning models (MLP and CNN) for side-channel analysis of AES-128 encryption. The approach starts by training a model on one key byte, then uses this trained model as a pretrained model for subsequent bytes. This leverages the assumption that parameters for attacking different bytes are similar. The input data consists of thermal map images (201x201 pixels) from AES-128 encryption simulation, with preprocessing involving Laplacian filtering and feature selection using standard deviation ranking. The performance is evaluated using measurement-to-disclosure (MTD) - the minimal number of measurements needed to successfully predict each key byte, averaged over 100 random permutations.

## Key Results
- Iterative transfer learning reduced MTD for MLP from 54.7 to 53.8 on thermal maps when data is limited
- For power consumption maps, CNN with iterative transfer learning achieved MTD of 56.0 compared to 204.0 without it
- The method significantly outperforms traditional approaches when training data is scarce
- Performance improvements were consistent across both MLP and CNN architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative transfer learning reduces the number of measurements needed to crack AES-128 by leveraging parameter similarities across bytes.
- Mechanism: The model first trains on one byte, then uses the trained model as a pretrained model for subsequent bytes. This iterative process transfers knowledge about the relationship between physical leakage and key byte values.
- Core assumption: The models' parameters for attacking different bytes are similar, allowing effective knowledge transfer.
- Evidence anchors:
  - [abstract]: "since the models' parameters for attacking different bytes may be similar, we can leverage transfer learning, meaning that we first train the model for one of the key bytes, then use the trained model as a pretrained model for the remaining bytes."
  - [section]: "While most of researches trained one model for one byte of the encryption key, which did not consider the relevance between each byte, this research invokes transfer learning."
- Break condition: If the parameter similarities across bytes are minimal or if the physical leakage patterns for different bytes are fundamentally different, transfer learning would provide no benefit.

### Mechanism 2
- Claim: Transfer learning improves performance especially when data is limited.
- Mechanism: By starting with a pretrained model, subsequent byte predictions require less data to achieve similar accuracy compared to training from scratch.
- Core assumption: The pretrained weights provide a better starting point than random initialization when data is scarce.
- Evidence anchors:
  - [abstract]: "Experimental results show that when using thermal or power consumption map images as input, and multilayer perceptron or convolutional neural network as the model, our method improves average performance, especially when the amount of data is insufficient."
  - [section]: "Fig. 5 shows the MTDs of MLP and CNN with or without ITL as the training data size varies. As the training data size decreases, the MTD increases, and MLP and CNN fail to crack all the bytes when the training data size is small. However, after applying ITL, the MTDs decrease for the same training data sizes."
- Break condition: If sufficient data is available for all bytes, the benefit of transfer learning diminishes and the overhead of transfer learning may not be justified.

### Mechanism 3
- Claim: The order of bytes in iterative transfer learning has minimal impact on effectiveness.
- Mechanism: Since all bytes share similar underlying patterns in their relationship to physical leakage, the specific starting byte doesn't significantly affect the overall transfer learning performance.
- Core assumption: The correlation between physical leakage and any key byte is similar enough that the transfer learning process is order-independent.
- Evidence anchors:
  - [section]: "Our preliminary experiments shows that the model performance converges within two iterations, and the effectiveness of the order of bytes is slight."
- Break condition: If certain bytes have fundamentally different leakage characteristics or if there are dependencies between byte predictions that make order matter, the effectiveness could vary significantly with byte order.

## Foundational Learning

- Concept: Side-channel attacks exploit physical information leakage during cryptographic operations.
  - Why needed here: The entire attack methodology relies on using thermal/power consumption maps as inputs to predict encryption keys, which is the core of side-channel analysis.
  - Quick check question: What physical measurements can be used in side-channel attacks to extract encryption keys?

- Concept: Transfer learning in deep learning involves using knowledge gained from one task to improve performance on a related task.
  - Why needed here: The paper applies transfer learning iteratively across the 16 bytes of an AES-128 key, treating each byte prediction as a related task.
  - Quick check question: How does transfer learning differ from training separate models for each task?

- Concept: Measurement-to-disclosure (MTD) as a performance metric for side-channel attacks.
  - Why needed here: MTD measures how many measurements are needed to successfully predict the key, which is the primary evaluation metric used to demonstrate the effectiveness of the proposed method.
  - Quick check question: Why is MTD a more practical metric than simple classification accuracy for side-channel attacks?

## Architecture Onboarding

- Component map: Data preprocessing (Laplacian filtering, feature selection) → Model training (MLP/CNN with iterative transfer learning) → Evaluation (MTD calculation over 100 permutations)
- Critical path: Feature selection → Model training for first byte → Iterative transfer learning for remaining bytes → MTD evaluation
- Design tradeoffs: Using all pixels vs. feature selection (dimensionality reduction vs. potential loss of information), using thermal vs. power consumption maps (different patterns but similar effectiveness with ITL)
- Failure signatures: High MTD values indicate poor model performance; failure to crack all bytes suggests insufficient data or inappropriate model architecture; inconsistent results across permutations suggest overfitting
- First 3 experiments:
  1. Train separate MLP models for each byte without transfer learning and measure MTD
  2. Apply iterative transfer learning starting with byte 0 and measure improvement in MTD
  3. Compare performance using thermal maps vs. power consumption maps with and without ITL to understand the impact of input data type

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the order of byte processing affect the performance of iterative transfer learning in side-channel attacks?
- Basis in paper: [explicit] The paper states, "Our preliminary experiments shows that the model performance converges within two iterations, and the effectiveness of the order of bytes is slight."
- Why unresolved: The paper indicates the order's effectiveness is slight but does not provide detailed analysis or specific results comparing different byte orders.
- What evidence would resolve it: Conducting experiments with different byte orders and comparing their performance metrics, such as MTD, would provide concrete evidence of the impact of byte order on iterative transfer learning effectiveness.

### Open Question 2
- Question: Can multitask learning be effectively applied to reduce training time in iterative transfer learning for side-channel attacks?
- Basis in paper: [explicit] The paper mentions, "One of the future research directions could be investigating the performance using different types of physical leakage information for the model input. We would also like to try using multitask learning to reduce the training time."
- Why unresolved: The paper suggests future exploration of multitask learning but does not present any experimental results or analysis of its potential benefits.
- What evidence would resolve it: Implementing multitask learning in the iterative transfer learning framework and comparing training times and attack performance with the current approach would demonstrate its effectiveness.

### Open Question 3
- Question: How do different types of physical leakage information, such as electromagnetic emissions, affect the performance of iterative transfer learning in side-channel attacks?
- Basis in paper: [explicit] The paper states, "One of the future research directions could be investigating the performance using different types of physical leakage information for the model input."
- Why unresolved: The paper focuses on thermal and power consumption maps but does not explore other types of physical leakage information like electromagnetic emissions.
- What evidence would resolve it: Conducting experiments using various types of physical leakage information as input and comparing their impact on the performance of iterative transfer learning would provide insights into their effectiveness.

## Limitations
- The approach relies on simulated data rather than real-world measurements, which may not capture actual hardware noise and variability
- Computational overhead of iterative transfer learning versus its benefits is not quantified
- Scalability to other cryptographic algorithms beyond AES-128 remains unexplored

## Confidence
- **High Confidence**: The core mechanism of iterative transfer learning improving performance with limited data is well-supported by the experimental results showing consistent MTD reductions across different model architectures and input types.
- **Medium Confidence**: The claim about minimal impact of byte ordering is supported by preliminary experiments but lacks comprehensive validation across all possible orderings and different datasets.
- **Low Confidence**: The generalizability of the approach to other side-channel scenarios (different hardware, different encryption algorithms) is largely speculative based on the current evidence.

## Next Checks
1. **Real Hardware Validation**: Test the iterative transfer learning approach on thermal emissions from actual AES-128 implementations on physical hardware to verify performance against simulated data results.

2. **Cross-Algorithm Generalization**: Apply the method to other cryptographic algorithms (e.g., RSA, ChaCha20) to evaluate whether the parameter similarity assumption holds across different encryption schemes.

3. **Computational Overhead Analysis**: Quantify the training time and computational resources required for iterative transfer learning compared to training separate models, particularly as the number of key bytes or model complexity increases.