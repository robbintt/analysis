---
ver: rpa2
title: Off-Policy Evaluation of Slate Bandit Policies via Optimizing Abstraction
arxiv_id: '2402.02171'
source_url: https://arxiv.org/abs/2402.02171
tags:
- slate
- lips
- abstraction
- variance
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses off-policy evaluation (OPE) for slate contextual
  bandits, where a policy selects multi-dimensional actions (slates) composed of multiple
  sub-actions. The main challenge is the high variance of existing estimators like
  Inverse Propensity Scoring (IPS) due to large action spaces, and the restrictive
  linearity assumption of PseudoInverse (PI) that leads to bias when violated.
---

# Off-Policy Evaluation of Slate Bandit Policies via Optimizing Abstraction

## Quick Facts
- arXiv ID: 2402.02171
- Source URL: https://arxiv.org/abs/2402.02171
- Authors: Haruka Kiyohara; Masahiro Nomura; Yuta Saito
- Reference count: 40
- Primary result: Introduces Latent IPS (LIPS) for off-policy evaluation of slate bandit policies, achieving substantial variance reduction compared to IPS without restrictive assumptions on reward functions

## Executive Summary
This paper addresses the challenge of off-policy evaluation (OPE) for slate contextual bandits, where policies select multi-dimensional actions composed of multiple sub-actions. The authors identify that standard estimators like Inverse Propensity Scoring (IPS) suffer from high variance due to large action spaces, while PseudoInverse (PI) imposes restrictive linearity assumptions that lead to bias when violated. To overcome these limitations, they propose Latent IPS (LIPS), a novel approach that defines importance weights in a low-dimensional slate abstraction space, optimizing these abstractions to minimize bias and variance in a data-driven manner.

The proposed method demonstrates substantial improvements over existing estimators through both theoretical analysis and empirical evaluation. LIPS achieves unbiased estimation given sufficient abstraction while providing significant variance reduction. Interestingly, the theoretical analysis suggests that intentionally using insufficient abstractions might minimize mean-squared-error (MSE) by achieving even greater variance reduction while remaining nearly unbiased. Empirical results on extreme classification datasets show that LIPS substantially outperforms existing estimators, particularly in scenarios with non-linear rewards and large slate spaces.

## Method Summary
The paper introduces Latent IPS (LIPS), a novel off-policy evaluation method for slate bandit policies that addresses the limitations of existing approaches. LIPS works by defining importance weights in a low-dimensional slate abstraction space rather than the full slate space. The key innovation is optimizing these slate abstractions to minimize the bias and variance of the LIPS estimator in a data-driven way. This optimization procedure allows LIPS to achieve substantial variance reduction compared to standard IPS while avoiding the restrictive linearity assumptions required by PseudoInverse methods. The approach is theoretically grounded, showing that LIPS can be unbiased given sufficient abstraction, and empirically validated on extreme classification datasets demonstrating significant performance improvements over existing estimators.

## Key Results
- LIPS achieves substantial variance reduction compared to standard IPS without imposing restrictive assumptions on reward function structure
- Theoretical analysis shows LIPS can be unbiased given sufficient abstraction while providing significant variance reduction
- Empirical evaluation on extreme classification datasets demonstrates LIPS substantially outperforms existing estimators, especially with non-linear rewards and large slate spaces
- Analysis suggests intentionally using insufficient abstractions might minimize MSE by achieving greater variance reduction while remaining nearly unbiased

## Why This Works (Mechanism)
LIPS works by optimizing slate abstractions to minimize the bias and variance of the off-policy evaluation estimator. By operating in a low-dimensional abstraction space rather than the full slate space, the method can define more stable importance weights that reduce variance compared to standard IPS. The data-driven optimization of these abstractions allows the method to adapt to the specific structure of the reward function without requiring restrictive assumptions like linearity. This approach effectively balances the trade-off between bias and variance, achieving unbiased estimation when sufficient abstraction is used while potentially minimizing MSE through controlled bias when using insufficient abstractions.

## Foundational Learning

**Slate Contextual Bandits**: Multi-dimensional action selection where a policy chooses slates composed of multiple sub-actions, commonly used in recommendation systems. Why needed: Provides the problem setting where LIPS operates and highlights the challenge of large action spaces.

**Off-Policy Evaluation (OPE)**: Estimating the performance of a policy using data collected from a different behavior policy. Why needed: Establishes the core problem LIPS addresses and explains why importance weighting is necessary.

**Importance Sampling**: A technique for estimating properties of a target distribution using samples from a different distribution. Why needed: Forms the basis of IPS and LIPS estimators, explaining how they use importance weights for OPE.

**Variance Reduction**: Techniques to decrease the variability of estimators to improve reliability. Why needed: Explains the primary motivation for LIPS and why operating in abstraction space helps.

**Bias-Variance Trade-off**: The fundamental trade-off in estimation where reducing variance often increases bias and vice versa. Why needed: Helps understand why intentionally using insufficient abstractions might minimize MSE.

**Abstraction Space Optimization**: The process of finding optimal low-dimensional representations for high-dimensional objects. Why needed: Explains the core innovation of LIPS in optimizing slate abstractions for better OPE.

## Architecture Onboarding

**Component Map**: Data -> Abstraction Optimization -> LIPS Estimator -> Performance Evaluation -> Optimized Slate Abstraction

**Critical Path**: The optimization of slate abstractions is the critical path, as it directly determines the quality of the LIPS estimator. This involves defining the abstraction space, optimizing the abstraction mapping, and computing importance weights in the abstract space.

**Design Tradeoffs**: The method trades computational complexity in the abstraction optimization step for improved estimation accuracy. Using higher-dimensional abstractions reduces bias but may increase variance, while lower-dimensional abstractions reduce variance but increase bias. The optimization procedure must balance these competing factors.

**Failure Signatures**: If the abstraction space is poorly chosen or optimization fails to find good abstractions, LIPS may suffer from high bias or variance similar to or worse than standard IPS. Computational intractability in the optimization procedure could limit scalability to very large slate spaces.

**3 First Experiments**:
1. Compare LIPS against standard IPS and PseudoInverse on synthetic data with known reward functions to validate theoretical properties
2. Test LIPS on a small recommendation dataset with varying slate sizes to understand scalability properties
3. Conduct ablation studies varying the dimensionality of the abstraction space to quantify the bias-variance trade-off

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions in the text provided. However, several implicit questions arise from the discussion of intentionally using insufficient abstractions to minimize MSE and the computational complexity of the optimization procedure.

## Limitations
- Computational complexity and scalability of the slate abstraction optimization procedure are not thoroughly analyzed
- Theoretical guarantees rely on assumptions about the abstraction space and existence of sufficient abstractions, with practical implications not fully explored
- Empirical evaluation is limited to specific synthetic and real-world datasets, leaving questions about performance across diverse recommendation scenarios

## Confidence
- **High**: The variance reduction claims compared to standard IPS are well-supported by theoretical analysis and empirical evidence
- **Medium**: The bias analysis and theoretical guarantees hold under stated assumptions, but practical limitations may arise
- **Medium**: The empirical results demonstrate improvements, but the scope of evaluation is somewhat limited

## Next Checks
1. Evaluate LIPS on larger-scale real-world recommendation datasets with millions of items to assess scalability
2. Conduct ablation studies varying the dimensionality of the abstraction space to understand trade-offs
3. Test the robustness of the optimization procedure when the behavior policy changes or when data is limited