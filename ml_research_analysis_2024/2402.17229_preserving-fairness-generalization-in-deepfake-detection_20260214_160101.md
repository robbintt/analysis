---
ver: rpa2
title: Preserving Fairness Generalization in Deepfake Detection
arxiv_id: '2402.17229'
source_url: https://arxiv.org/abs/2402.17229
tags:
- fairness
- detection
- loss
- features
- deepfake
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of fairness generalization in
  deepfake detection, where current methods perform well on intra-domain data but
  fail to maintain fairness when tested across different domains. The authors propose
  a novel framework that simultaneously considers features, loss, and optimization
  to preserve fairness during cross-domain detection.
---

# Preserving Fairness Generalization in Deepfake Detection

## Quick Facts
- arXiv ID: 2402.17229
- Source URL: https://arxiv.org/abs/2402.17229
- Reference count: 40
- One-line primary result: Novel framework improves fairness metrics (FFPR, FMEO, FDP, FOAE) while maintaining high AUC in cross-domain deepfake detection

## Executive Summary
This paper addresses the critical problem of fairness generalization in deepfake detection, where models perform well on intra-domain data but fail to maintain fairness across different domains. The authors propose a comprehensive framework that simultaneously tackles features, loss, and optimization to preserve fairness during cross-domain detection. By employing disentanglement learning to extract demographic and domain-agnostic forgery features, fusing them for fair learning across a flattened loss landscape, and using demographic distribution-aware margin loss, the method achieves significant improvements in fairness metrics while maintaining high detection accuracy.

## Method Summary
The proposed framework combines disentanglement learning, fair learning, and optimization to preserve fairness generalization in deepfake detection. It extracts demographic and domain-agnostic forgery features through a multi-encoder architecture with classification, contrastive, and reconstruction losses. These features are then fused using AdaIN and optimized with a bi-level fairness loss across a flattened loss landscape using sharpness-aware minimization. The method is evaluated on prominent deepfake datasets including FaceForensics++, DeepfakeDetection, Deepfake Detection Challenge, and Celeb-DF, demonstrating consistent improvements across different backbone architectures.

## Key Results
- Achieves significant improvements in fairness metrics (FFPR, FMEO, FDP, FOAE) while maintaining high detection accuracy (AUC)
- Outperforms state-of-the-art approaches particularly in cross-domain scenarios
- Demonstrates robustness across different backbone architectures (Xception, ResNet-50, EfficientNet-B3)
- Shows consistent performance across intersectional demographic groups

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Disentangling demographic features from forgery features reduces fairness bias in cross-domain detection.
- Mechanism: By separating demographic and domain-agnostic forgery features during training, the model learns representations that are less correlated with protected attributes, improving fairness generalization.
- Core assumption: Demographic and forgery features are entangled in the raw input, and separating them reduces bias without harming detection performance.
- Evidence anchors:
  - [abstract] "Our method employs disentanglement learning to extract demographic and domain-agnostic forgery features"
  - [section 3] "disentanglement could be an approach to enhance fairness by untangling the representations r(X)Y and r(X)D from r(X)"

### Mechanism 2
- Claim: Flattening the loss landscape improves fairness generalization by avoiding suboptimal minima.
- Mechanism: Using sharpness-aware minimization prevents the model from getting stuck in sharp minima that lead to poor cross-domain fairness.
- Core assumption: Fairness generalization is sensitive to the geometry of the loss landscape, and flatter minima correlate with better generalization.
- Evidence anchors:
  - [abstract] "fusing them to encourage fair learning across a flattened loss landscape"
  - [section 3] "Training these models results in sharp loss landscapes... Hence, it becomes essential to flatten the loss landscape to enhance fairness generalization."

### Mechanism 3
- Claim: Demographic distribution-aware margin loss improves minority subgroup generalization.
- Mechanism: By using margins that depend on subgroup sample size, the model is encouraged to learn better representations for minority groups, reducing bias.
- Core assumption: Minority subgroups have fewer training samples and benefit from larger margins to improve their representation quality.
- Evidence anchors:
  - [section 4.2] "We propose a demographic distribution-aware margin loss inspired by [47]... improving generalization for minority subgroups with small np by using larger margins"

## Foundational Learning

- Concept: Disentanglement learning
  - Why needed here: To separate demographic information from forgery features so that the model can learn fair representations without discarding forgery-relevant information.
  - Quick check question: What happens if you remove demographic features entirely instead of disentangling them?

- Concept: Sharpness-aware minimization
  - Why needed here: To find flatter minima in the loss landscape that lead to better fairness generalization across domains.
  - Quick check question: How does the loss landscape change when using sharpness-aware minimization versus standard SGD?

- Concept: Demographic distribution-aware margin loss
  - Why needed here: To address class imbalance within demographic subgroups and ensure minority groups are not under-represented in the learned features.
  - Quick check question: What would happen if you used standard cross-entropy loss instead of margin loss for demographic features?

## Architecture Onboarding

- Component map:
  Input -> Encoder -> Disentanglement learning -> Feature fusion (AdaIN) -> Fair classification -> Loss flattening -> Model update

- Critical path:
  Input → Encoder → Disentanglement learning → Feature fusion (AdaIN) → Fair classification → Loss flattening → Model update

- Design tradeoffs:
  - Using separate encoders for different feature types increases model complexity but improves disentanglement quality.
  - Adding contrastive loss improves generalization but requires careful positive/negative sampling.
  - Flattening the loss landscape improves fairness generalization but may slightly reduce detection accuracy.

- Failure signatures:
  - If demographic and forgery features are not well-separated, fairness metrics will not improve.
  - If the loss landscape is not sufficiently flattened, cross-domain fairness will degrade.
  - If minority subgroups are not properly handled, fairness disparities will persist.

- First 3 experiments:
  1. Train with disentanglement but without loss flattening to verify the importance of the flatness component.
  2. Replace demographic distribution-aware margin loss with standard cross-entropy to test its impact on minority subgroup performance.
  3. Remove feature fusion (AdaIN) to confirm that combining demographic and forgery features is necessary for fairness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do demographic and forgery features become entangled in deepfake detection, and can this entanglement be quantified?
- Basis in paper: [explicit] The paper discusses the entanglement of demographic and forgery features as a key issue, with Theorem 1 stating that if features are entangled, a perfect classifier for the target variable does not imply demographic parity.
- Why unresolved: While the paper identifies the entanglement problem, it doesn't provide a method to quantify or measure the degree of entanglement between demographic and forgery features.
- What evidence would resolve it: A proposed metric or framework to quantify the entanglement level between demographic and forgery features, along with empirical studies showing how this entanglement affects fairness in deepfake detection.

### Open Question 2
- Question: What is the impact of disentangling demographic and forgery features on the overall detection accuracy and fairness generalization?
- Basis in paper: [inferred] The paper introduces disentanglement learning to extract demographic and domain-agnostic forgery features, suggesting that this could improve fairness generalization. However, it doesn't provide a detailed analysis of how this disentanglement impacts detection accuracy.
- Why unresolved: While the paper shows that disentanglement can improve fairness, it doesn't explore the trade-off between detection accuracy and fairness, or how much disentanglement is optimal for maintaining both.
- What evidence would resolve it: Empirical studies comparing detection accuracy and fairness metrics across different levels of feature disentanglement, along with an analysis of the optimal balance between the two.

### Open Question 3
- Question: How does the choice of backbone architecture (e.g., Xception, ResNet-50, EfficientNet-B3) affect the fairness generalization capability of deepfake detection models?
- Basis in paper: [explicit] The paper mentions that their method was tested with different backbone architectures (Xception, ResNet-50, EfficientNet-B3) and showed consistent results, but doesn't provide a detailed comparison of how each backbone affects fairness generalization.
- Why unresolved: While the paper demonstrates that their method works with different backbones, it doesn't explore which backbone is most effective for fairness generalization or why certain backbones might be better suited for this task.
- What evidence would resolve it: A comparative study of fairness generalization across different backbone architectures, including an analysis of the architectural features that contribute to better fairness generalization.

## Limitations
- The effectiveness of disentanglement learning critically depends on the assumption that demographic and forgery features are sufficiently entangled in the raw input.
- The paper claims loss landscape flattening improves fairness generalization, but the relationship between sharpness and fairness is not fully established.
- The demographic distribution-aware margin loss assumes minority subgroups benefit from larger margins, but the optimal margin values are not derived theoretically.

## Confidence
- High confidence in the proposed framework's ability to improve fairness metrics (FFPR, FMEO, FDP, FOAE) while maintaining detection accuracy (AUC).
- Medium confidence in the disentanglement learning component's necessity and effectiveness.
- Medium confidence in the sharpness-aware minimization's contribution to fairness generalization.
- Medium confidence in the demographic distribution-aware margin loss's impact on minority subgroup performance.

## Next Checks
1. **Ablation Study on Disentanglement:** Train the model with and without the disentanglement module to quantify its specific contribution to fairness improvements.
2. **Loss Landscape Analysis:** Visualize and compare the loss landscapes with and without sharpness-aware minimization to verify the claimed flattening effect.
3. **Margin Loss Sensitivity:** Conduct experiments with different margin values (δ) to determine the optimal settings and validate the demographic distribution-aware design.