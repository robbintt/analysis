---
ver: rpa2
title: A Benchmarking Study of Kolmogorov-Arnold Networks on Tabular Data
arxiv_id: '2406.14529'
source_url: https://arxiv.org/abs/2406.14529
tags: []
core_contribution: This study benchmarks Kolmogorov-Arnold Networks (KANs) against
  Multi-Layer Perceptrons (MLPs) on real-world tabular datasets, addressing the lack
  of empirical evaluation on complex data. KANs leverage learnable activation functions
  on edges rather than fixed node activations, inspired by the Kolmogorov-Arnold theorem,
  aiming to improve interpretability and flexibility.
---

# A Benchmarking Study of Kolmogorov-Arnold Networks on Tabular Data

## Quick Facts
- arXiv ID: 2406.14529
- Source URL: https://arxiv.org/abs/2406.14529
- Reference count: 29
- KANs achieve superior or comparable accuracy and F1 scores vs MLPs on tabular data, especially on large datasets

## Executive Summary
This study benchmarks Kolmogorov-Arnold Networks (KANs) against Multi-Layer Perceptrons (MLPs) on ten real-world tabular datasets. KANs leverage learnable activation functions on edges rather than fixed node activations, inspired by the Kolmogorov-Arnold theorem, aiming to improve interpretability and flexibility. Experiments show KANs achieving superior or comparable accuracy and F1 scores, especially on large datasets, while requiring higher computational costs due to more complex activation functions. Training times are longer for KANs, and FLOPs are significantly higher than MLPs for similar parameter counts. These results confirm KANs as a viable alternative to MLPs for tabular data, with better handling of complex relationships but at greater computational expense.

## Method Summary
The authors conducted controlled experiments comparing KANs and MLPs on ten diverse tabular datasets. Both architectures were configured with comparable parameter counts and trained using AdamW optimizer for 10 epochs with learning rate decay. Performance was evaluated using accuracy, F1 score, precision, recall, FPR, FNR, and training time. FLOPs and parameter efficiency were also measured to quantify computational costs.

## Key Results
- KANs achieved superior or comparable accuracy and F1 scores compared to MLPs across all tested datasets
- Performance advantages were particularly pronounced on large datasets with numerous instances
- KANs required 2-3x more FLOPs than MLPs for comparable parameter counts, resulting in longer training times
- Computational cost increased significantly due to learnable 1D functions on edges replacing simple linear operations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: KANs improve performance on tabular data by replacing fixed activation functions with learnable edge-based functions, allowing greater flexibility in capturing nonlinear relationships.
- **Mechanism**: Traditional MLPs apply fixed nonlinear functions (e.g., ReLU, sigmoid) at each node, limiting the model's adaptability to the data's structure. KANs instead use learnable 1D functions on the edges, which can be optimized during training to better fit the underlying function being approximated. This design choice allows KANs to approximate more complex relationships with fewer layers or nodes.
- **Core assumption**: The learnable edge functions are sufficiently expressive and efficient to model the relationships in real-world tabular data without overfitting.
- **Evidence anchors**:
  - [abstract]: "KANs leverage learnable activation functions on edges rather than fixed node activations, inspired by the Kolmogorov-Arnold theorem, aiming to improve interpretability and flexibility."
  - [section]: "KANs innovate by incorporating learnable activation functions on edges (weights). This approach eliminates traditional linear weight matrices, replacing each weight parameter with a learnable 1D function that is typically parameterized as a spline."
- **Break condition**: If the computational cost of learning these edge functions outweighs the performance gains, or if the increased complexity leads to overfitting on smaller datasets.

### Mechanism 2
- **Claim**: KANs are especially effective on large datasets because their flexible edge functions can better model complex, high-dimensional interactions that are common in large tabular data.
- **Mechanism**: In large datasets, the number of instances increases the likelihood of encountering complex, non-linear relationships between features. KANs' learnable edge functions can adapt to these relationships more effectively than fixed activations, leading to improved accuracy. The paper notes KANs excel particularly in datasets with numerous instances.
- **Core assumption**: The dataset contains sufficiently complex interactions to benefit from the increased expressiveness of KANs' edge functions.
- **Evidence anchors**:
  - [abstract]: "KANs achieving superior or comparable accuracy and F1 scores, especially on large datasets, suggesting robust handling of complex data."
  - [section]: "This trend underscores KAN's effectiveness in handling various data complexities and task demands."
- **Break condition**: On small or simple datasets, the additional flexibility of KANs may not be needed, and MLPs may perform comparably with less computational overhead.

### Mechanism 3
- **Claim**: The higher computational cost of KANs is due to the more complex activation functions on edges, which require more floating-point operations (FLOPs) than traditional MLPs with the same number of parameters.
- **Mechanism**: Each edge in a KAN contains a learnable 1D function (e.g., a spline), which must be evaluated during both forward and backward passes. This increases the number of FLOPs per edge compared to a simple multiplication in MLPs. The paper shows that KANs perform a larger number of operations per parameter, explaining the increased training time and computational expense.
- **Core assumption**: The increased FLOPs directly translate to higher training times and resource usage, and this cost is acceptable given the performance gains.
- **Evidence anchors**:
  - [section]: "We highlighted how the improvements in the performance of KANs come at a higher computational cost, as expected, given the adoption of more complex activation functions."
  - [section]: "This indicates that KANs achieve higher performance at the cost of a higher number of operations, for a fixed budget of parameters."
- **Break condition**: If the computational budget is constrained, or if the performance improvement does not justify the increased cost, MLPs may be preferred.

## Foundational Learning

- **Concept**: Kolmogorov-Arnold Theorem (KAT)
  - Why needed here: KAT provides the theoretical foundation for KANs, stating that any multivariate continuous function can be represented as a composition of univariate functions and addition. Understanding KAT is essential to grasp why KANs use learnable functions on edges and how they can approximate complex relationships.
  - Quick check question: How does the Kolmogorov-Arnold theorem justify the structure of KANs compared to MLPs?

- **Concept**: Learnable vs. Fixed Activation Functions
  - Why needed here: KANs replace fixed activation functions (like ReLU in MLPs) with learnable 1D functions on edges. Understanding this distinction is crucial for appreciating KANs' flexibility and the trade-offs in computational cost and expressiveness.
  - Quick check question: What is the key architectural difference between KANs and MLPs regarding activation functions, and how does it impact model flexibility?

- **Concept**: Computational Complexity (FLOPs) and Parameter Efficiency
  - Why needed here: KANs achieve better performance but at higher computational cost. Understanding FLOPs and how KANs' learnable functions increase the number of operations is essential for evaluating when KANs are appropriate.
  - Quick check question: Why do KANs require more FLOPs than MLPs for the same number of parameters, and how does this impact training and deployment?

## Architecture Onboarding

- **Component map**: Input features → Learnable edge functions → Output predictions
- **Critical path**:
  1. Preprocess tabular data (handle missing values, encode categorical features)
  2. Configure KAN or MLP architecture with comparable parameter counts
  3. Train using AdamW optimizer for 10 epochs with learning rate decay
  4. Evaluate using accuracy, F1 score, precision, recall, FPR, FNR, and training time
  5. Compare FLOPs and parameter efficiency
- **Design tradeoffs**:
  - Flexibility vs. Computational Cost: KANs offer greater expressiveness but require more FLOPs and training time
  - Interpretability vs. Complexity: KANs are more interpretable due to learnable edge functions but are more complex to implement
  - Dataset Size: KANs excel on large datasets with complex interactions; MLPs may be preferable for smaller or simpler datasets
- **Failure signatures**:
  - Overfitting on small datasets due to increased model flexibility
  - Unacceptably high training times or resource usage
  - Minimal performance gains compared to MLPs on simple tasks
- **First 3 experiments**:
  1. Train a KAN and an MLP on a small tabular dataset (e.g., Breast Cancer) and compare accuracy, F1 score, and training time
  2. Scale up to a large dataset (e.g., Poker) and observe changes in performance and computational cost
  3. Measure FLOPs and parameter efficiency for both models across datasets to quantify the cost-benefit trade-off

## Open Questions the Paper Calls Out
- How does KAN performance scale with extremely large tabular datasets (e.g., millions of rows)?
- How do KANs perform on regression tasks compared to MLPs?
- Can KANs maintain their interpretability advantage when scaled to deeper architectures?

## Limitations
- Computational cost remains a significant barrier - KANs require 2-3x more FLOPs than MLPs for comparable parameter counts
- Limited hyperparameter exploration - only one KAN configuration per dataset was tested
- No ablation studies on spline parameterization to isolate which aspects drive performance gains
- Small number of datasets (10) and lack of diversity in tabular data types may limit generalizability

## Confidence
- **High**: KANs achieve superior or comparable accuracy/F1 scores vs MLPs on tabular data, especially on large datasets
- **Medium**: Performance gains justify computational costs in real-world applications
- **Medium**: KANs are a viable alternative to MLPs for tabular data

## Next Checks
1. Conduct extensive ablation studies varying spline complexity, depth, and width to isolate which architectural choices drive performance
2. Expand benchmark to include diverse tabular data types (time series, categorical-heavy, sparse data) and modern competitors (TabNet, GBDT, XGBoost)
3. Perform cost-benefit analysis comparing training/inference costs against performance gains for industry-relevant datasets