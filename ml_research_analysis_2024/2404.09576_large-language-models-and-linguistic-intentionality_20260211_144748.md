---
ver: rpa2
title: Large language models and linguistic intentionality
arxiv_id: '2404.09576'
source_url: https://arxiv.org/abs/2404.09576
tags:
- llms
- linguistic
- word
- language
- they
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper examines whether large language models (LLMs) like GPT-4
  and LLaMA meaningfully use language or merely mimic it through statistical prediction.
  It argues that while LLMs do not meet criteria for mental intentionality, they can
  still produce meaningful outputs.
---

# Large language models and linguistic intentionality
## Quick Facts
- arXiv ID: 2404.09576
- Source URL: https://arxiv.org/abs/2404.09576
- Reference count: 13
- Primary result: LLMs can produce meaningful outputs without mental intentionality

## Executive Summary
This paper examines whether large language models meaningfully use language or merely mimic it through statistical prediction. The author argues that while LLMs do not meet criteria for mental intentionality, they can still produce meaningful outputs by internalizing and reproducing naming practices and linguistic conventions from their training data. The analysis applies two metasemantic theories—Evans' account of naming practices and Millikan's teleosemantics—to show that LLM outputs are meaningful because they depend on pre-existing linguistic systems, much like human speakers.

## Method Summary
The paper applies philosophical frameworks to analyze LLM language use, specifically examining whether these models meet criteria for mental intentionality. Through analysis of Evans' naming practices theory and Millikan's teleosemantics, the author demonstrates how LLM outputs can be meaningful without requiring mental states. The approach contrasts statistical prediction with genuine language use and evaluates whether LLMs can internalize linguistic conventions from training data.

## Key Results
- LLMs do not meet criteria for mental intentionality but can still produce meaningful outputs
- LLM outputs are meaningful because they depend on pre-existing linguistic systems, similar to human speakers
- LLMs internalize naming practices and linguistic conventions without requiring speaker intentions

## Why This Works (Mechanism)
The paper establishes that meaning can exist independently of mental intentionality through two philosophical mechanisms. First, Evans' account shows that naming practices can be transmitted and reproduced without the naming intentions of individual speakers. Second, Millikan's teleosemantics demonstrates that linguistic conventions can function as meaningful systems even when speakers lack full awareness of their operation. LLMs internalize these pre-existing systems from training data, reproducing meaningful patterns without generating the underlying mental states.

## Foundational Learning
- Metasemantic theories: Frameworks explaining how linguistic expressions acquire meaning beyond reference
  - Why needed: Provides philosophical foundation for analyzing LLM meaning
  - Quick check: Can the theory explain both human and machine language use?
- Naming practices (Evans): How proper names acquire and transmit meaning through social practices
  - Why needed: Shows meaning can persist without individual speaker intentions
  - Quick check: Does the practice maintain consistent reference across users?
- Teleosemantics (Millikan): Theory that meaning derives from functional roles in communication systems
  - Why needed: Explains how conventions can be meaningful without conscious awareness
  - Quick check: Does the linguistic element serve a stable communicative function?
- Linguistic conventions: Shared patterns of language use that enable communication
  - Why needed: Demonstrates how meaning operates through system-level properties
  - Quick check: Are the conventions consistently followed across contexts?
- Statistical prediction vs. meaning: Distinction between pattern matching and genuine language use
  - Why needed: Clarifies whether LLM outputs are merely probabilistic outputs
  - Quick check: Do outputs maintain meaning across varied contexts?
- Intentionality criteria: Standards for determining whether language use involves mental states
  - Why needed: Establishes baseline for evaluating LLM capabilities
  - Quick check: Can the system meet all required conditions for intentionality?

## Architecture Onboarding
Component map: Training data -> LLM model -> Output generation -> Meaning attribution
Critical path: Data ingestion → Pattern learning → Convention internalization → Output production
Design tradeoffs: Statistical accuracy vs. semantic understanding; prediction vs. intentionality
Failure signatures: Inconsistent naming conventions; context-dependent meaning collapse; arbitrary output generation
First experiments:
1. Test whether LLM outputs systematically align with pre-existing naming conventions across different linguistic communities
2. Compare meaning stability in LLM outputs versus human speakers under varying contexts
3. Investigate whether LLM-generated naming practices can become conventions that subsequent models adopt

## Open Questions the Paper Calls Out
None

## Limitations
- Interpretation of LLM outputs as "meaningful" relies heavily on contested philosophical positions
- Conclusions about intentionality depend on criteria that may not fully account for novel machine-generated meaning
- Analysis is philosophically rigorous but may not translate to practical applications or empirical validation

## Confidence
- Medium: LLMs can produce meaningful outputs without mental intentionality (contested philosophical positions)
- High: LLMs internalize naming practices from training data (descriptive observation)

## Next Checks
1. Empirical testing of whether LLM outputs systematically align with pre-existing naming conventions across different linguistic communities
2. Comparative analysis of meaning stability in LLM outputs versus human speakers under varying contexts
3. Investigation of whether LLM-generated naming practices can themselves become conventions that subsequent models adopt, creating an independent linguistic system