---
ver: rpa2
title: 'MIRAI: Evaluating LLM Agents for Event Forecasting'
arxiv_id: '2407.01231'
source_url: https://arxiv.org/abs/2407.01231
tags:
- date
- code
- isocode
- list
- relations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MIRAI introduces a novel benchmark for evaluating large language
  model agents in forecasting international events. The benchmark provides an agentic
  environment with APIs for accessing a curated database of historical events and
  news articles.
---

# MIRAI: Evaluating LLM Agents for Event Forecasting

## Quick Facts
- arXiv ID: 2407.01231
- Source URL: https://arxiv.org/abs/2407.01231
- Reference count: 40
- Current LLM agents struggle with temporal forecasting tasks, with best GPT-4o achieving only 29.6 F1 score

## Executive Summary
MIRAI introduces a benchmark for evaluating large language model agents in forecasting international events. The benchmark provides an agentic environment with APIs for accessing a curated database of historical events and news articles. Experiments reveal that current LLM agents struggle significantly with temporal forecasting tasks, particularly for longer-term and fine-grained event predictions. The results highlight the need for improved temporal reasoning capabilities in LLM agents for complex international event prediction.

## Method Summary
The MIRAI benchmark evaluates LLM agents through an agentic environment that provides APIs for accessing historical events and news articles. Agents are tasked with forecasting international events across various temporal granularities. Performance is measured using F1 scores for predicting event relationships at different time horizons. The benchmark tests agents' abilities to use diverse tools and access historical data for making predictions about future events.

## Key Results
- Best GPT-4o agent achieved only 29.6 F1 score on second-level relation predictions
- Longer-term event forecasting proved particularly challenging for all tested agents
- Fine-grained event forecasting showed significant difficulty across all temporal granularities

## Why This Works (Mechanism)
The benchmark works by creating a controlled environment where LLM agents must use provided tools and historical data to make predictions about future events. The agentic setup forces models to demonstrate their reasoning capabilities in a structured forecasting task.

## Foundational Learning
- Temporal reasoning: Understanding event sequences and time-based relationships is crucial for accurate forecasting
- Tool integration: Agents must effectively combine multiple data sources and APIs to make informed predictions
- Historical context: Access to past events and news articles is essential for identifying patterns and trends

## Architecture Onboarding
Component Map: Historical DB -> Tool APIs -> LLM Agent -> Forecast Output
Critical Path: Data retrieval -> Analysis -> Prediction generation -> Validation
Design Tradeoffs: Comprehensive data access vs. computational efficiency
Failure Signatures: Over-reliance on recent data, temporal reasoning errors, pattern overfitting
First Experiments:
1. Test basic event relationship prediction with limited historical context
2. Evaluate tool usage diversity impact on forecasting accuracy
3. Measure performance degradation across increasing time horizons

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark focuses exclusively on international events, limiting generalizability
- Performance evaluation relies primarily on F1 scores, which may not capture full forecasting complexity
- Tests only limited set of LLM agents, primarily GPT-4o

## Confidence
- High Confidence: Current LLM agents struggle with temporal forecasting tasks
- Medium Confidence: Diverse tool-use and historical data access are critical for accurate forecasting
- Low Confidence: MIRAI comprehensively evaluates capabilities needed for international event forecasting

## Next Checks
1. Test MIRAI agents on forecasting tasks outside international events domain
2. Evaluate broader range of LLM agents including open-source models
3. Conduct field study comparing agent forecasts against actual future events over extended period