---
ver: rpa2
title: 'Graph-Skeleton: ~1% Nodes are Sufficient to Represent Billion-Scale Graph'
arxiv_id: '2402.09565'
source_url: https://arxiv.org/abs/2402.09565
tags: []
core_contribution: This paper proposes a novel method, Graph-Skeleton, to compress
  massive background nodes in web-scale graphs while preserving essential information
  for target node classification. The method fetches and condenses background nodes
  based on structural connectivity and feature correlation with target nodes, reducing
  graph size by 99% on a 0.24-billion-node dataset while maintaining comparable classification
  performance.
---

# Graph-Skeleton: ~1% Nodes are Sufficient to Represent Billion-Scale Graph

## Quick Facts
- arXiv ID: 2402.09565
- Source URL: https://arxiv.org/abs/2402.09565
- Authors: Linfeng Cao; Haoran Deng; Yang Yang; Chunping Wang; Lei Chen
- Reference count: 40
- Primary result: Compresses 0.24-billion-node graphs by 99% while maintaining comparable target node classification performance

## Executive Summary
This paper proposes Graph-Skeleton, a method to compress massive background nodes in web-scale graphs while preserving essential information for target node classification. The approach identifies and fetches background nodes based on two critical roles: enhancing structural connectivity between target nodes through bridging, and feature correlation with neighboring target nodes. Through extensive experiments on diverse web graph datasets, the method demonstrates that ~1% of nodes can effectively represent billion-scale graphs for target classification tasks.

## Method Summary
Graph-Skeleton compresses background nodes in large graphs by fetching essential nodes and condensing them into a compact skeleton graph. The method operates in two phases: (1) node fetching identifies bridging nodes that connect multiple target nodes within d1-hops and affiliation nodes that are top-k correlated with single target nodes within d2-hops, and (2) graph condensation applies three strategies - α (MSS-based condensation), β (target-only condensation with distance encoding), and γ (affiliation aggregation) - to create a compressed representation that preserves structural connectivity and feature correlation patterns essential for target node classification.

## Key Results
- Achieves 99% background node compression on a 0.24-billion-node dataset
- Maintains comparable classification performance across multiple datasets (DGraph, ogbn-arxiv, ogbn-mag, MAG240M, DBLP, IMDb)
- Demonstrates effectiveness on diverse web graph datasets with varying characteristics
- Shows robustness across different GNN architectures (SAGE, GAT, GIN, etc.)

## Why This Works (Mechanism)

### Mechanism 1: Bridging Background Nodes
- Claim: Background nodes enhance structural connectivity between target nodes through bridging
- Mechanism: Fetches background nodes that bridge multiple target nodes within limited hop distance (d1), preserving connectivity paths
- Core assumption: Shortest paths between target nodes through bridging background nodes contain essential structural information
- Evidence: Significant performance degradation when removing all background nodes; negligible impact of removing non-neighboring background nodes
- Break condition: Performance degrades significantly when bridging nodes are removed

### Mechanism 2: Feature Correlation
- Claim: Background nodes contribute through feature correlation with neighboring target nodes
- Mechanism: Fetches K highest correlation background nodes within d2-hop distance based on Pearson correlation coefficient
- Core assumption: Background nodes closer to target nodes have higher feature correlation and contribute more to classification
- Evidence: Background nodes closer to target nodes indeed have higher feature correlation
- Break condition: Performance degrades when correlation-based affiliation nodes are removed

### Mechanism 3: Structural Condensation
- Claim: Condensing background nodes with similar structural patterns reduces redundancy while preserving essential information
- Mechanism: Background nodes sharing identical multiple structure-sets (MSS) are condensed into synthetic nodes using linear message passing equivalence
- Core assumption: Background nodes with same MSS share equivalent structural information for linear message passing operations
- Evidence: Proposition 3.5 shows LMPP equivalence allows safe condensation of nodes with identical MSS
- Break condition: Performance degrades if condensation removes structurally diverse nodes

## Foundational Learning

- Graph Neural Networks and message passing: Understanding how GNNs aggregate information from neighboring nodes is fundamental to the framework. Quick check: What happens to GNN performance when you remove edges between target and background nodes?

- Graph condensation and sparsification: The method builds on graph reduction techniques with novel constraints on preserving target node information. Quick check: How does our method differ from traditional graph coarsening approaches?

- Pearson correlation coefficient and feature similarity: Used to identify affiliation background nodes based on feature correlation with target nodes. Quick check: Why do background nodes closer to target nodes have higher feature correlation?

## Architecture Onboarding

- Component map: Node fetching (BFS-based bridging and correlation search) → Vanilla subgraph construction → Three-level condensation (α: MSS-based, β: target-only, γ: affiliation aggregation) → Skeleton graph output
- Critical path: Node fetching → Condensation → Model training on skeleton
- Design tradeoffs: Higher d1/d2 increases coverage but also background node count; condensation level trades information preservation for compression ratio
- Failure signatures: Performance drops when bridging nodes are lost, when correlation information is insufficient, or when condensation removes structurally important nodes
- First 3 experiments:
  1. Test baseline performance on original graph vs random background node removal
  2. Test node fetching with different d1, d2 settings on small dataset
  3. Test each condensation strategy independently on constructed vanilla subgraph

## Open Questions the Paper Calls Out

1. How does the Graph-Skeleton method preserve information for distant nodes in the condensed graph? The paper discusses condensation strategies but doesn't explain how they maintain information flow for distant nodes.

2. How does performance scale with increasing graph size and complexity for graphs with billions of nodes and edges? The paper shows effectiveness on 0.24-billion-node graphs but doesn't discuss scalability for larger graphs.

3. What is the impact of target node sparsity on compression performance and effectiveness? The paper's sensitivity analysis is limited to specific datasets without exploring broader implications of target node sparsity.

## Limitations

- Limited empirical validation scope: Claims 99% compression on one dataset without systematic hyperparameter sensitivity analysis
- Weak theoretical foundations: Proposition 3.5 provides only a sufficient condition for condensation equivalence without broader theoretical guarantees
- Narrow dataset scope: Focuses on web-scale graphs that may not generalize to other graph types
- Limited scalability discussion: Effectiveness on 0.24-billion-node graphs doesn't establish performance for significantly larger graphs

## Confidence

- High confidence: Mechanism showing bridging background nodes enhance structural connectivity between target nodes, supported by direct experimental evidence
- Medium confidence: Feature correlation mechanism, as Pearson correlation findings are empirical observations without theoretical justification
- Low confidence: Structural condensation mechanism, as Proposition 3.5 only establishes a sufficient condition without proving necessity or broader guarantees

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary d1, d2, and k parameters across multiple datasets to establish robustness bounds and identify breaking points where performance degrades significantly.

2. **Theoretical Generalization Test**: Extend Proposition 3.5 analysis to prove whether LMPP equivalence is both necessary and sufficient for safe condensation, or identify counterexamples where equivalent MSS nodes still carry distinct classification-relevant information.

3. **Cross-Domain Generalization**: Apply the Graph-Skeleton framework to non-web graph datasets (e.g., biological networks, social networks with different structural properties) to validate whether the 99% compression claim holds across diverse graph types beyond the current web-scale focus.