---
ver: rpa2
title: 'Cross-lingual Text Classification Transfer: The Case of Ukrainian'
arxiv_id: '2404.02043'
source_url: https://arxiv.org/abs/2404.02043
tags:
- ukrainian
- classification
- language
- text
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores cross-lingual text classification transfer
  for the Ukrainian language, which lacks sufficient labeled datasets for common NLP
  tasks like toxicity, formality, and natural language inference (NLI). The authors
  test four transfer approaches: backtranslation, LLM prompting, training corpus translation,
  and adapter training, using translated English datasets and semi-natural Ukrainian
  test sets.'
---

# Cross-lingual Text Classification Transfer: The Case of Ukrainian

## Quick Facts
- arXiv ID: 2404.02043
- Source URL: https://arxiv.org/abs/2404.02043
- Authors: Daryna Dementieva; Valeriia Khylenko; Georg Groh
- Reference count: 20
- Primary result: LLM prompting with Mistral achieves strong results for formality and NLI tasks in Ukrainian, while fine-tuning XLM-RoBERTa excels in toxicity classification

## Executive Summary
This paper explores cross-lingual text classification transfer for the Ukrainian language, which lacks sufficient labeled datasets for common NLP tasks like toxicity, formality, and natural language inference (NLI). The authors test four transfer approaches: backtranslation, LLM prompting, training corpus translation, and adapter training, using translated English datasets and semi-natural Ukrainian test sets. Mistral LLM prompting achieved strong results for formality and NLI, while XLM-RoBERTa fine-tuning excelled in toxicity classification. Adapter training performed best for formality. The study provides baseline models and datasets, demonstrating that LLM prompting can be a solid baseline for Ukrainian text classification, with fine-tuning needed for toxicity detection. However, results suggest further improvements are possible, especially with native Ukrainian data.

## Method Summary
The study tests four cross-lingual transfer approaches for Ukrainian text classification across three tasks: toxicity classification, formality classification, and natural language inference. English datasets (Jigsaw, GYAFC, SNLI) were translated to Ukrainian using Opus and NLLB systems, and semi-natural Ukrainian test sets were compiled from tweets, legal acts, and fiction. The four approaches tested were: backtranslation (translating Ukrainian input to English for English classifiers), LLM prompting (zero-shot classification via Mistral), training corpus translation (fine-tuning XLM-RoBERTa on translated datasets), and adapter training (adding language-specific adapters to XLM-RoBERTa). All methods were evaluated using accuracy, precision, recall, and F1 scores on both translated and semi-natural test sets.

## Key Results
- LLM prompting with Mistral achieved the strongest results for formality classification and natural language inference tasks
- XLM-RoBERTa fine-tuning performed best for toxicity classification when using Opus translation system
- Adapter training showed promise for formality classification, providing an efficient parameter-efficient approach
- All methods showed significant performance drops when evaluated on semi-natural Ukrainian test sets compared to translated test sets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Backtranslation avoids fine-tuning but introduces dependency on translation system accuracy
- Mechanism: Translates Ukrainian input to English, applies English classifier, translates results back
- Core assumption: Translation system preserves semantic content and toxic/formal/inferable elements
- Evidence anchors:
  - [abstract]: "Backtranslation; LLM Prompting; Training Corpus Translation; and Adapter Training"
  - [section 4]: "we chose Opus translation system for toxicity classification as it preserves better the toxic lexicon"
  - [corpus]: Weak - limited comparison of translation quality across tasks
- Break condition: Translation system fails to preserve critical linguistic features like toxic lexicon or formality markers

### Mechanism 2
- Claim: Adapter training efficiently adapts multilingual encoder to Ukrainian while preserving task-specific knowledge
- Mechanism: Freezes multilingual encoder, adds task-specific adapter layers, replaces English adapter with Ukrainian adapter
- Core assumption: Adapter layers can effectively capture language-specific features without full fine-tuning
- Evidence anchors:
  - [abstract]: "Adapter Training – applying them for Ukrainian"
  - [section 3]: "the most parameter-efficient approach involves employing language-specific Adapter layers"
  - [corpus]: Limited - no explicit performance comparison with full fine-tuning
- Break condition: Adapter layers cannot capture sufficient language-specific features for the task

### Mechanism 3
- Claim: LLM prompting leverages emerging cross-lingual abilities without fine-tuning or translation
- Mechanism: Designs task-specific prompts in Ukrainian, leverages LLM's multilingual training
- Core assumption: LLM has sufficient exposure to Ukrainian in pretraining to understand task requirements
- Evidence anchors:
  - [abstract]: "LLM Prompting" and "emerging abilities (Wei et al., 2022)"
  - [section 5]: "Mistral once again outperformed all baselines" for NLI task
  - [corpus]: Weak - limited exploration of different LLM architectures
- Break condition: LLM lacks sufficient Ukrainian exposure or prompt design fails to elicit correct responses

## Foundational Learning

- Concept: Cross-lingual transfer learning
  - Why needed here: Enables knowledge transfer from resource-rich English to resource-poor Ukrainian
  - Quick check question: What are the main approaches to cross-lingual transfer and when would you choose each?

- Concept: Adapter-based fine-tuning
  - Why needed here: Provides parameter-efficient way to adapt multilingual models to Ukrainian
  - Quick check question: How do adapter layers differ from full fine-tuning in terms of parameters and effectiveness?

- Concept: Prompt engineering for zero-shot learning
  - Why needed here: Enables task execution without labeled data through careful prompt design
  - Quick check question: What are the key principles of effective prompt engineering for text classification?

## Architecture Onboarding

- Component map: Translation systems (NLLB, Opus) → English classifiers → Multilingual encoder (XLM-RoBERTa) → Task-specific adapters → LLM (Mistral) for prompting
- Critical path: Data preparation → Translation system selection → Model selection → Prompt design/Adapter training → Evaluation on semi-natural test sets
- Design tradeoffs: Backtranslation (no fine-tuning, translation dependency) vs. Adapter training (efficient, requires labeled data) vs. LLM prompting (zero-shot, LLM dependency)
- Failure signatures: Translation quality issues → Poor classifier performance; Adapter training instability → Poor Ukrainian adaptation; LLM prompting failures → Insufficient cross-lingual abilities
- First 3 experiments:
  1. Compare translation quality of NLLB vs Opus on toxicity classification subset
  2. Test adapter training on formality classification with small labeled Ukrainian dataset
  3. Experiment with different prompt structures for LLM prompting on NLI task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of cross-lingual text classification transfer approaches compare when using native Ukrainian data versus translated English data?
- Basis in paper: [inferred] The authors note that "aside from formality classification, the leading results for all tasks still show potential for improvement" and "we strongly encourage further additional investigations using native Ukrainian data for these tasks."
- Why unresolved: The paper primarily uses translated English datasets and semi-natural test sets due to the lack of sufficient labeled Ukrainian datasets.
- What evidence would resolve it: Conducting experiments using native Ukrainian datasets for the three tasks (toxicity classification, formality classification, and NLI) and comparing the results with those obtained from translated English data.

### Open Question 2
- Question: What is the impact of using larger, more powerful language models (e.g., those with more parameters) on the performance of cross-lingual text classification transfer for Ukrainian?
- Basis in paper: [explicit] The authors state, "Given resource constraints, our experiments only incorporated base and distilled versions of the models. Despite these limitations, the approaches we explored yielded promising results. However, employing models with more parameters could yield even stronger outcomes."
- Why unresolved: The experiments were limited to base and distilled versions of models due to resource constraints.
- What evidence would resolve it: Performing the same experiments using larger, more powerful language models and comparing the results with those obtained using base and distilled versions.

### Open Question 3
- Question: How would cross-lingual transfer from languages closer to Ukrainian, such as Polish or Croatian, compare to transfer from English in terms of performance and efficiency?
- Basis in paper: [explicit] The authors mention, "However, if resources such as datasets and models are accessible for languages closer to Ukrainian, such as Polish or Croatian, conducting cross-lingual transfer from these languages could potentially yield even better results."
- Why unresolved: The paper focuses on cross-lingual transfer from English, considering it the most resource-rich language for the most general scenario.
- What evidence would resolve it: Conducting experiments using datasets and models from languages closer to Ukrainian, such as Polish or Croatian, and comparing the results with those obtained from English.

## Limitations
- Translation quality varies significantly across tasks, with different systems optimal for different classification tasks
- Semi-natural test sets may not fully represent native Ukrainian usage patterns, particularly for legal and formal contexts
- Adapter training lacks direct comparison with full fine-tuning approaches to quantify parameter efficiency gains
- LLM prompting success depends heavily on prompt engineering quality, limiting generalizability across different architectures

## Confidence
- **High Confidence**: Backtranslation introduces translation dependency that affects classifier performance, particularly for toxicity tasks where lexical preservation is critical
- **Medium Confidence**: LLM prompting with Mistral is effective for formality and NLI tasks in Ukrainian, but effectiveness may vary with different LLM architectures and prompt designs
- **Medium Confidence**: Adapter training shows potential for Ukrainian adaptation but requires more extensive validation against full fine-tuning approaches

## Next Checks
1. Test the same four transfer approaches on a small native Ukrainian labeled dataset (if available) to establish upper bounds and compare with transfer-based performance
2. Conduct ablation studies on prompt engineering for LLM prompting across different Ukrainian text domains to identify optimal prompt structures
3. Compare adapter training with full fine-tuning of XLM-RoBERTa on Ukrainian data to quantify parameter efficiency gains against performance trade-offs