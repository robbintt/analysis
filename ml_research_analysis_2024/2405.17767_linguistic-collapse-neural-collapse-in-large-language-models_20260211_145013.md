---
ver: rpa2
title: 'Linguistic Collapse: Neural Collapse in (Large) Language Models'
arxiv_id: '2405.17767'
source_url: https://arxiv.org/abs/2405.17767
tags:
- neural
- https
- collapse
- width
- depth
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the phenomenon of Neural Collapse (NC) in large
  language models (LLMs) by empirically investigating the impact of scaling architectures
  and training of causal language models (CLMs) on their progression towards NC. The
  authors train a suite of Transformer-based CLMs across a grid of model widths, depths,
  and training epochs on the TinyStories dataset to assess the degrees to which NC
  properties develop and how they relate to generalization performance.
---

# Linguistic Collapse: Neural Collapse in (Large) Language Models

## Quick Facts
- arXiv ID: 2405.17767
- Source URL: https://arxiv.org/abs/2405.17767
- Reference count: 40
- Key outcome: Neural collapse properties emerge with scale and regularization in language models, correlating with improved generalization performance

## Executive Summary
This paper investigates neural collapse (NC) in large language models (LLMs) by training a suite of Transformer-based causal language models (CLMs) across varying model widths, depths, and training epochs on the TinyStories dataset. The authors empirically demonstrate that NC properties—including within-class variability collapse, hyperspherical uniformity, and classifier agreement—emerge with scale and regularization, and are correlated with improved validation performance. Notably, the study reveals a relationship between NC and generalization independent of model scale, suggesting NC may be a fundamental factor of generalization in language modeling. These findings extend the concept of neural collapse from computer vision to the more complex domain of language modeling, highlighting its potential as a general phenomenon across different architectures and tasks.

## Method Summary
The authors conducted controlled experiments by training Transformer-based causal language models (CLMs) on the TinyStories dataset, systematically varying model width, depth, and training epochs. The study focused on quantifying the emergence of neural collapse properties—such as within-class variability collapse, hyperspherical uniformity, and classifier agreement—and their relationship to generalization performance. By scaling architectures and training conditions, the authors assessed how these properties develop and correlate with validation accuracy, providing empirical evidence for the presence of neural collapse in language models.

## Key Results
- Neural collapse properties (within-class variability collapse, hyperspherical uniformity, classifier agreement) emerge with scale and regularization in CLMs.
- These properties are correlated with improved validation performance, suggesting a link between NC and generalization.
- A relationship between NC and generalization exists independent of model scale, indicating NC may be a fundamental factor of generalization in language modeling.

## Why This Works (Mechanism)
The paper does not explicitly discuss the underlying mechanisms driving neural collapse in language models. However, the empirical findings suggest that scaling model architectures and training conditions promotes the emergence of NC properties, which in turn enhance generalization performance. This implies that NC may arise from the optimization dynamics and regularization effects inherent in training large-scale models on structured data.

## Foundational Learning
- **Neural Collapse (NC)**: A phenomenon where features from the same class converge to their mean, and class means become maximally separated. *Why needed*: Understanding NC is crucial for interpreting model behavior and generalization in deep learning.
- **Causal Language Models (CLMs)**: Models that predict the next token in a sequence, trained to capture temporal dependencies. *Why needed*: CLMs are the primary architecture studied in this work, extending NC to language modeling.
- **Transformer Architecture**: A neural network architecture based on self-attention mechanisms, widely used in modern LLMs. *Why needed*: Transformers are the backbone of the CLMs studied, enabling the emergence of NC properties.
- **Hyperspherical Uniformity**: A property where feature vectors are uniformly distributed on a hypersphere. *Why needed*: This NC property is quantified and correlated with generalization in the experiments.
- **Within-Class Variability Collapse**: A phenomenon where feature vectors from the same class become tightly clustered. *Why needed*: This NC property is a key indicator of model convergence and generalization.
- **Classifier Agreement**: A measure of consistency in predictions across different layers or components of a model. *Why needed*: High classifier agreement is associated with NC and improved validation performance.

## Architecture Onboarding
- **Component Map**: Input -> Embedding Layer -> Transformer Blocks -> Output Layer -> Predictions
- **Critical Path**: The embedding and transformer layers are critical for feature extraction and the emergence of NC properties.
- **Design Tradeoffs**: Scaling model width and depth enhances NC properties but may increase computational costs. Regularization techniques balance generalization and convergence.
- **Failure Signatures**: Lack of NC properties may indicate underfitting or poor optimization. Over-regularization could suppress NC emergence.
- **3 First Experiments**:
  1. Train a small CLM on TinyStories and measure NC properties at initialization.
  2. Scale the model width and depth, then retrain and compare NC property development.
  3. Apply regularization techniques and assess their impact on NC emergence and validation performance.

## Open Questions the Paper Calls Out
None

## Limitations
- The findings are based on experiments using the TinyStories dataset, which consists of simple, synthetic stories with a limited vocabulary, potentially limiting generalizability to more complex, real-world language datasets.
- The analysis focuses on a narrow set of model configurations and training conditions, which could limit the broader applicability of the observed neural collapse properties.
- The paper does not fully address potential confounding factors, such as the impact of specific hyperparameters or architectural variations, on the emergence of neural collapse.

## Confidence
- **High confidence**: The empirical observations of neural collapse properties (e.g., within-class variability collapse, hyperspherical uniformity) in scaled CLMs.
- **Medium confidence**: The correlation between neural collapse and improved generalization performance, given the limited scope of the dataset and models tested.
- **Low confidence**: The broader applicability of neural collapse as a general factor of generalization in LLMs, particularly in more complex and diverse language modeling tasks.

## Next Checks
1. Replicate the experiments on larger, more diverse, and real-world language datasets to assess the generalizability of neural collapse properties.
2. Investigate the impact of varying hyperparameters (e.g., learning rates, regularization techniques) and architectural modifications on the emergence and extent of neural collapse.
3. Conduct ablation studies to isolate the specific contributions of neural collapse to generalization performance, distinguishing it from other potential factors such as model scale or training dynamics.