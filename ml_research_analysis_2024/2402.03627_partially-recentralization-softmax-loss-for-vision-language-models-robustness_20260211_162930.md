---
ver: rpa2
title: Partially Recentralization Softmax Loss for Vision-Language Models Robustness
arxiv_id: '2402.03627'
source_url: https://arxiv.org/abs/2402.03627
tags:
- arxiv
- adversarial
- robustness
- language
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vulnerability of vision-language models
  (VLMs) to adversarial attacks. The proposed method modifies the loss function to
  restrict top-K softmax outputs during fine-tuning, thereby improving model robustness.
---

# Partially Recentralization Softmax Loss for Vision-Language Models Robustness

## Quick Facts
- arXiv ID: 2402.03627
- Source URL: https://arxiv.org/abs/2402.03627
- Authors: Hao Wang; Jinzhe Jiang; Xin Zhang; Chen Li
- Reference count: 40
- Primary result: PRSL improves robustness against BIM adversarial attacks while maintaining baseline performance (BERTScore ~0.935-0.945)

## Executive Summary
This paper addresses the vulnerability of vision-language models (VLMs) to adversarial attacks by proposing a novel loss function called Partially Recentralization Softmax Loss (PRSL). The method restricts top-K softmax outputs during fine-tuning to improve model robustness against adversarial perturbations. Experiments on ViT-GPT2 (image captioning) and Blip2-OPT (VQA) models demonstrate that PRSL significantly enhances robustness against BIM attacks while maintaining similar baseline performance levels. The results show that models fine-tuned with PRSL exhibit fewer score degradations under attack compared to standard cross-entropy trained models across multiple evaluation metrics.

## Method Summary
The PRSL method modifies the standard cross-entropy loss by adding a Euclidean distance penalty between the ground truth probability and the top-K softmax outputs during fine-tuning. The hyperparameter b controls the weight of this penalty, while k determines which top-K probabilities are restricted. The approach aims to reduce model confidence in incorrect tokens beyond the top-K, making the model less susceptible to adversarial perturbations. The method was evaluated on two VLM architectures (ViT-GPT2 for image captioning and Blip2-OPT for VQA) using BIM adversarial attacks and multiple evaluation metrics including BERTScore, CIDEr, ROUGE, BLEU, and METEOR.

## Key Results
- PRSL significantly improves robustness against BIM adversarial attacks on both image captioning and VQA tasks
- Models fine-tuned with PRSL maintain baseline performance levels (BERTScore ~0.935-0.945) on clean data
- Different K values show varying robustness improvements, with moderate values (K=6-20) generally performing best
- The hyperparameter b=1e-5 represents an optimal tradeoff across different tasks and evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Restricting top-K softmax outputs during fine-tuning improves adversarial robustness
- Mechanism: By penalizing the model for placing high probability mass on incorrect tokens beyond the top-K, the PRSL loss function reduces the model's confidence in adversarial perturbations, making it less likely to generate incorrect outputs
- Core assumption: The diversity of softmax outputs contributes to model vulnerability to adversarial attacks
- Evidence anchors:
  - [abstract] "by restricting top K softmax outputs during fine-tuning"
  - [section] "we propose a novel loss function to mitigate the vulnerability issue of models by limiting the top-2 to top-k softmax output during fine-tuning"
- Break condition: If the top-K restriction becomes too severe, it may degrade model performance on clean data or cause the model to ignore important contextual information

### Mechanism 2
- Claim: The Euclidean distance component in PRSL loss encourages more concentrated probability distributions
- Mechanism: By adding a distance penalty between the top-K probabilities and the ground truth probability, the model is encouraged to make its predictions more certain and less spread out, reducing the impact of adversarial noise
- Core assumption: More concentrated probability distributions are less susceptible to adversarial perturbations
- Evidence anchors:
  - [section] "Here the set ð¾ = {ð‘˜ð‘—|ð‘— âˆˆ ð½ ð‘Žð‘›ð‘‘ ð‘˜ð‘— > ð‘˜ð‘—+1 ð‘“ð‘œð‘Ÿ ð‘Žð‘™ð‘™ ð‘— âˆˆ ð½ , ð‘— â‰  ð‘  },where J is the full vocabulary of the model, s is the ground truth next token. So that K is the set of top-k probabilities except ground truth."
  - [section] "We use Euclidean Distance as d[âˆ™,âˆ™]"
- Break condition: If the distance penalty becomes too strong, it may cause the model to become overconfident in incorrect predictions or fail to learn diverse representations

### Mechanism 3
- Claim: The hyperparameter b controls the trade-off between standard cross-entropy and PRSL robustness
- Mechanism: By adjusting the weight b, the model can balance between maintaining baseline performance and achieving robustness against adversarial attacks
- Core assumption: There exists an optimal balance between robustness and performance that can be achieved through hyperparameter tuning
- Evidence anchors:
  - [section] "Here, both the b and k are the hyperparameters. The b is searching by the grid search from 10âˆ’1 to 10âˆ’10"
  - [section] "Fig.1 shows the results of b from 10âˆ’4 to 10âˆ’6, under the Bertscore(t5-base) evaluation"
- Break condition: If b is set too high, the model may become overly robust but perform poorly on clean data; if too low, the robustness benefits may be minimal

## Foundational Learning

- Concept: Adversarial attacks on vision-language models
  - Why needed here: Understanding how adversarial examples work in multimodal contexts is crucial for appreciating why PRSL is necessary and how it addresses the problem
  - Quick check question: What makes vision-language models particularly vulnerable to adversarial attacks compared to unimodal models?

- Concept: Softmax function and its role in classification
  - Why needed here: The PRSL method specifically modifies the softmax outputs, so understanding the standard softmax behavior and its limitations is essential
  - Quick check question: How does the standard softmax function contribute to model vulnerability in adversarial settings?

- Concept: Cross-entropy loss and its limitations
  - Why needed here: PRSL builds upon cross-entropy loss, so understanding its standard form and why it might be insufficient for robustness is important
  - Quick check question: Why might standard cross-entropy loss be insufficient for ensuring adversarial robustness?

## Architecture Onboarding

- Component map: Pre-trained VLM (ViT-GPT2/Blip2-OPT) -> PRSL loss function -> Adversarial attack generator (BIM) -> Evaluation metrics (BERTScore, CIDEr, ROUGE, BLEU, METEOR) -> Hyperparameter search framework

- Critical path: 1. Load pre-trained model, 2. Implement PRSL loss function, 3. Fine-tune model on target dataset, 4. Generate adversarial examples, 5. Evaluate robustness using multiple metrics, 6. Tune hyperparameters (b and k)

- Design tradeoffs:
  - Higher k values provide better robustness but may reduce clean data performance
  - Larger b values increase robustness but require more careful hyperparameter tuning
  - Different evaluation metrics may show conflicting results, requiring careful interpretation

- Failure signatures:
  - Model performance degradation on clean data (BERTScore drops significantly)
  - Inconsistent results across different evaluation metrics
  - Hyperparameter search failing to find optimal b and k values
  - Adversarial attacks becoming ineffective due to overly restrictive k values

- First 3 experiments:
  1. Implement basic PRSL loss function with k=2 and b=1e-5, compare against standard cross-entropy on clean data
  2. Test different k values (2, 6, 20, 100) while keeping b fixed to observe robustness trends
  3. Perform grid search for optimal b values while keeping k fixed to find the best robustness-performance tradeoff

## Open Questions the Paper Calls Out
- How does the trade-off between model accuracy and robustness manifest in vision-language models when using Partially Recentralization Softmax Loss (PRSL)?
- How does the choice of top-k values in PRSL affect the diversity of model outputs?
- Can the robustness improvements from PRSL be generalized across different types of adversarial attacks beyond BIM?

## Limitations
- The approach relies heavily on hyperparameter tuning (b and k values) which may not generalize well across different VLM architectures or tasks
- The current evaluation focuses primarily on BIM adversarial attacks, leaving uncertainty about performance against other attack types
- The improvement margins, while statistically meaningful, are relatively modest (0.5-1.5% in BERTScore) and may not justify the added complexity in all deployment scenarios

## Confidence

**High Confidence Claims:**
- PRSL improves model robustness against BIM adversarial attacks on image captioning and VQA tasks
- The method maintains baseline performance levels on clean data within the reported BERTScore range
- Different K values show varying degrees of robustness improvement, with moderate values (K=6-20) generally performing best

**Medium Confidence Claims:**
- The Euclidean distance component is essential for robustness improvement (limited ablation evidence)
- The hyperparameter b=1e-5 represents an optimal tradeoff across different tasks
- The robustness gains translate meaningfully to real-world deployment scenarios

**Low Confidence Claims:**
- PRSL will generalize to other VLM architectures beyond ViT-GPT2 and Blip2-OPT
- The method performs comparably against other adversarial attack methods
- The robustness improvements justify the computational overhead of hyperparameter tuning

## Next Checks

1. **Ablation Study Validation**: Implement and test variants of the PRSL loss function removing either the top-K restriction or the Euclidean distance component to quantify their individual contributions to robustness improvements.

2. **Cross-Attack Generalization**: Evaluate the fine-tuned models against alternative adversarial attack methods (e.g., FGSM, PGD) to assess whether the robustness gains are specific to BIM attacks or generalize across attack types.

3. **Computational Overhead Analysis**: Measure and compare training time, memory usage, and inference latency between standard cross-entropy fine-tuning and PRSL fine-tuning to quantify the practical deployment costs.