---
ver: rpa2
title: Iterative Reasoning Preference Optimization
arxiv_id: '2404.19733'
source_url: https://arxiv.org/abs/2404.19733
tags:
- training
- iteration
- reasoning
- arxiv
- iterative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of improving chain-of-thought
  (CoT) reasoning in large language models (LLMs) using iterative preference optimization.
  Existing iterative preference optimization methods have shown success on general
  instruction tuning tasks but often fail to improve reasoning capabilities.
---

# Iterative Reasoning Preference Optimization

## Quick Facts
- arXiv ID: 2404.19733
- Source URL: https://arxiv.org/abs/2404.19733
- Authors: Richard Yuanzhe Pang; Weizhe Yuan; Kyunghyun Cho; He He; Sainbayar Sukhbaatar; Jason Weston
- Reference count: 12
- Key outcome: GSM8K accuracy improved from 55.6% to 81.6% (88.7% with majority voting) using iterative preference optimization with modified DPO loss

## Executive Summary
This work addresses the challenge of improving chain-of-thought (CoT) reasoning in large language models (LLMs) using iterative preference optimization. Existing iterative preference optimization methods have shown success on general instruction tuning tasks but often fail to improve reasoning capabilities. The proposed approach, Iterative Reasoning Preference Optimization (Iterative RPO), trains LLMs by iteratively generating multiple CoT solutions and final answers, constructing preference pairs based on answer correctness, and optimizing using a modified DPO loss with an additional negative log-likelihood (NLL) term. Experiments on GSM8K, MATH, and ARC-Challenge demonstrate significant improvements in reasoning performance.

## Method Summary
Iterative RPO works by generating multiple CoT solutions for each prompt, computing binary rewards based on answer correctness, constructing preference pairs (winners from correct answers, losers from incorrect answers), and training using a modified DPO loss that includes an additional NLL term. The process is repeated iteratively using the same fixed set of prompts, with temperature scheduling adjusted across iterations (0.8 for iterations 1-2, 1.3 for iterations 3-4) to ensure sufficient incorrect generations in later iterations. The method does not require human-in-the-loop annotation or additional training data, making it a simple and efficient approach to enhance LLM reasoning capabilities.

## Key Results
- GSM8K accuracy increased from 55.6% to 81.6% after Iterative RPO training, with 88.7% accuracy using majority voting
- MATH performance improved from 12.5% to 20.8% accuracy
- ARC-Challenge accuracy increased from 77.8% to 86.7%
- Performance gains diminish across iterations (17.5%, 4.9%, 3.1%, 0.5%), indicating an upper limit on learning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The DPO+NLL loss combination ensures the model increases probability mass for correct reasoning steps while decreasing probability for incorrect ones, preventing reward hacking.
- **Mechanism:** The NLL term directly maximizes likelihood of correct reasoning paths, while DPO creates a contrastive signal between correct and incorrect reasoning paths. Together they prevent the model from inflating probabilities for incorrect reasoning chains.
- **Core assumption:** The model can learn to distinguish between correct and incorrect reasoning paths through pairwise comparisons and explicit likelihood maximization.
- **Evidence anchors:**
  - [abstract]: "train using a modified DPO loss (Rafailov et al., 2023) with an additional negative log-likelihood (NLL) term, which we find to be crucial"
  - [section 2]: "The loss corresponding to each preference pair is as follows: LDPO+NLL = LNLL(xi, cw i, yw i) + αLDPO(cw i, yw i, cl i, yl i|xi)"

### Mechanism 2
- **Claim:** Iterative generation of preference pairs using the current model creates a self-improving loop where the model gradually learns to generate better reasoning chains.
- **Mechanism:** Each iteration generates new data using the current model, which becomes more capable at producing correct reasoning chains. This creates an upward spiral of improvement as the model trains on progressively better data.
- **Core assumption:** The model's ability to generate correct reasoning chains improves monotonically with each iteration.
- **Evidence anchors:**
  - [abstract]: "show reasoning improves across repeated iterations of this scheme"
  - [section 2]: "We see an improvement from 55.6% of zero-shot performance on GSM8K to 81.6% after our Iterative RPO training"

### Mechanism 3
- **Claim:** Using the same fixed set of prompts across iterations allows the model to focus on improving reasoning quality rather than exploring new problem distributions.
- **Mechanism:** By keeping the prompt distribution fixed, the model can concentrate on learning better reasoning strategies for known problem types rather than adapting to new distributions.
- **Core assumption:** The fixed prompt set contains sufficient diversity to cover the reasoning patterns needed for improvement.
- **Evidence anchors:**
  - [section 2]: "on each iteration in Self-Rewarding a new set of prompts is created to explore the input distribution, but in our approach we use the same fixed set of prompts"

## Foundational Learning

- **Concept:** Chain-of-Thought (CoT) reasoning
  - Why needed here: The entire method relies on generating and evaluating multi-step reasoning processes rather than just final answers
  - Quick check question: What is the difference between standard prompting and Chain-of-Thought prompting in LLMs?

- **Concept:** Preference optimization and contrastive learning
  - Why needed here: The method uses pairwise comparisons between correct and incorrect reasoning chains to train the model
  - Quick check question: How does DPO differ from standard supervised learning in terms of training objectives?

- **Concept:** Iterative training loops and self-improvement
  - Why needed here: The method improves performance by repeatedly generating new training data and retraining the model
  - Quick check question: What are the key differences between iterative training and standard fine-tuning approaches?

## Architecture Onboarding

- **Component map:** Base LLM → Iterative RPO training loop (CoT generation → reward computation → preference pair construction → DPO+NLL training) → Improved model
- **Critical path:** Model initialization → N-step reasoning generation → Reward computation → Preference pair selection → Loss computation and parameter update
- **Design tradeoffs:** Fixed prompts vs. prompt exploration, NLL term inclusion vs. pure DPO, sampling temperature adjustments across iterations
- **Failure signatures:** Accuracy plateaus or decreases across iterations, reward computation becomes unreliable, preference pairs become too similar or too noisy
- **First 3 experiments:**
  1. Run zero-shot CoT on GSM8K to establish baseline (55.6% accuracy)
  2. Implement first iteration of Iterative RPO and compare to standard DPO (73.1% vs 61.8%)
  3. Test the impact of removing NLL term from the loss function to validate its importance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Iterative RPO scale with model size? Does larger model size consistently lead to better reasoning performance improvements through this method?
- Basis in paper: [inferred] The paper uses Llama-2-70B and compares to smaller models, but does not systematically study scaling effects across different model sizes.
- Why unresolved: The paper only tests on one model size (70B parameters) and does not explore whether the method's effectiveness varies with model scale.
- What evidence would resolve it: Systematic experiments comparing Iterative RPO performance across multiple model sizes (e.g., 7B, 13B, 34B, 70B) on the same reasoning tasks.

### Open Question 2
- Question: What is the optimal number of iterations for Iterative RPO before performance plateaus or degrades? Is there a theoretical explanation for why performance eventually saturates?
- Basis in paper: [explicit] "We see performance improves across each iteration... However, the gain decays across the iterations (17.5%, 4.9%, 3.1%, 0.5%), indicating an upper limit on learning across iterations"
- Why unresolved: The paper only tests up to 4 iterations and does not provide theoretical analysis of the convergence behavior or optimal stopping criteria.
- What evidence would resolve it: Experiments extending beyond 4 iterations and theoretical analysis of the optimization dynamics explaining the observed saturation.

### Open Question 3
- Question: How robust is Iterative RPO to different reward model qualities? Can it still improve reasoning when using imperfect or noisy reward models?
- Basis in paper: [explicit] "our experimental setup does not require a sophisticated reward model to judge the model generations, as we assume the training prompts have provided gold labels which we compare to"
- Why unresolved: The paper uses ground truth labels for reward computation, but doesn't test how the method performs with imperfect reward models that are known to be challenging for reasoning tasks.
- What evidence would resolve it: Experiments comparing performance when using different reward models ranging from perfect ground truth to noisy/verifier models.

## Limitations

- The method requires a fixed set of prompts that must contain sufficient diversity to enable meaningful improvement; narrow or biased prompt distributions may cause overfitting.
- Performance improvements plateau after 2-4 iterations, suggesting diminishing returns and potential saturation of the training signal.
- The binary reward function (correct/incorrect) may oversimplify complex reasoning quality, potentially missing important distinctions between different types of errors.

## Confidence

**High Confidence:**
- Iterative RPO achieves significantly higher accuracy than baseline approaches on GSM8K and MATH datasets
- The NLL term in the loss function is crucial for the method's success

**Medium Confidence:**
- The iterative self-improvement mechanism creates a positive feedback loop where each iteration generates better training data
- Using the same fixed prompt set across iterations is beneficial for focusing on reasoning quality

**Low Confidence:**
- The method's effectiveness on reasoning tasks beyond mathematical and scientific domains
- The long-term stability and generalization of improvements across different model architectures and sizes

## Next Checks

1. **Cross-domain generalization test:** Apply Iterative RPO to logical reasoning and commonsense reasoning datasets to evaluate performance outside mathematical domains.

2. **Ablation study on prompt diversity:** Test the method with varying levels of prompt diversity (fixed vs. expanding prompt sets) to quantify the impact on reasoning improvement and generalization.

3. **Long-term stability analysis:** Track model performance across 10+ iterations to identify potential degradation patterns, overfitting, or reward hacking behaviors that may emerge in extended training.