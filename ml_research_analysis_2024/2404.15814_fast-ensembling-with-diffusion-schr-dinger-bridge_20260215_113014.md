---
ver: rpa2
title: "Fast Ensembling with Diffusion Schr\xF6dinger Bridge"
arxiv_id: '2404.15814'
source_url: https://arxiv.org/abs/2404.15814
tags:
- ensemble
- diffusion
- network
- bridge
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Diffusion Bridge Network (DBN), a method to
  accelerate inference in deep ensemble models by constructing a diffusion bridge
  between the logit distributions of a single ensemble member and the target ensemble.
  Unlike prior methods that predict outputs of other ensemble members directly, DBN
  learns a stochastic path via a score network, enabling ensemble predictions with
  a single forward pass.
---

# Fast Ensembling with Diffusion Schrödinger Bridge

## Quick Facts
- **arXiv ID**: 2404.15814
- **Source URL**: https://arxiv.org/abs/2404.15814
- **Authors**: Hyunsu Kim; Jongmin Yoon; Juho Lee
- **Reference count**: 31
- **Primary result**: DBN achieves DE-3 performance with ~1.17× FLOPs of a single model on CIFAR-10/100 and TinyImageNet

## Executive Summary
Diffusion Bridge Network (DBN) is a method to accelerate inference in deep ensemble models by constructing a diffusion bridge between the logit distributions of a single ensemble member and the target ensemble. Unlike prior methods that predict outputs of other ensemble members directly, DBN learns a stochastic path via a score network, enabling ensemble predictions with a single forward pass. Experiments show that DBN achieves performance comparable to DE-3 with only ~1.17× the FLOPs of a single model, outperforming existing distillation and bridge methods.

## Method Summary
DBN constructs a conditional diffusion bridge in logit space between a source model (one ensemble member) and target ensemble. The method trains a score network to estimate the reverse diffusion process, using progressive distillation to reduce sampling cost. A lightweight score network based on residual connections and depthwise separable convolutions is trained to simulate the Schrödinger bridge that transports the output distribution of a single ensemble member to match the ensemble average. Multiple diffusion bridges can be combined for larger ensembles with minimal overhead.

## Key Results
- DBN achieves accuracy of ~92% on CIFAR-10, comparable to DE-3
- DBN requires only ~1.17× FLOPs of a single model versus ~3× for DE-3
- DBN outperforms Bridge Network and other distillation methods on ACC, NLL, and DEE metrics
- DBN scales efficiently with ensemble size, maintaining performance advantage over DE and BN

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DBN reduces ensemble inference cost by learning a stochastic path between the source model's logits and the ensemble's logits.
- Mechanism: DBN uses Schrödinger bridge theory to construct a conditional diffusion process that transports the output distribution of a single ensemble member to match the ensemble average. This is done via a score network that estimates gradients of the log density along the path.
- Core assumption: The logit distributions of ensemble members are sufficiently correlated and can be connected by a smooth stochastic process.
- Evidence anchors:
  - [abstract] "DBN learns to simulate an Stochastic Differential Equation (SDE) that connects the output distribution of a single ensemble member to the output distribution of the ensembled model"
  - [section 2.2] "SB is an optimal transport problem that seeks to find the forward and backward processes... [which] provides the optimal solution to an entropy-regularizing optimization problem that finds the optimal path between p0 and p1"
  - [corpus] Weak evidence; no direct citations linking diffusion bridges to ensemble inference.
- Break condition: If ensemble members are too functionally diverse, the logit distributions may be too far apart for a single bridge to approximate accurately.

### Mechanism 2
- Claim: DBN achieves faster inference than Bridge Network (BN) by eliminating the need for low-loss subspace learning.
- Mechanism: Instead of constructing low-loss subspaces between pairs of ensemble members (as BN does), DBN directly learns a bridge from one source model to the full ensemble, avoiding pairwise subspace optimization.
- Core assumption: A single source model's feature extractor can provide sufficient information to reconstruct the ensemble output via the diffusion bridge.
- Evidence anchors:
  - [abstract] "Unlike prior methods that predict outputs of other ensemble members directly, DBN learns a stochastic path via a score network"
  - [section 2.1] "BN... hinges on the mode connectivity... employs techniques such as Bezier curves between each pair of ensemble members"
  - [section 3.1] "We assume that a classifier... is decomposed into two parts, a feature extractor... and then a classifier... That is, fθ(x) = (cλ ◦ gψ)(x)"
- Break condition: If the feature extractor from the source model is insufficiently expressive, the diffusion bridge may fail to capture ensemble diversity.

### Mechanism 3
- Claim: Progressive distillation of diffusion steps reduces inference cost while preserving performance.
- Mechanism: Multiple steps of the reverse diffusion process are distilled into a single step via a recursive training scheme, reducing the number of forward passes required.
- Core assumption: The multi-step diffusion process can be accurately approximated by a single-step model without significant performance loss.
- Evidence anchors:
  - [abstract] "we design the score network of DSB to be lightweighted and incorporate diffusion step distillation to further minimize computational overhead"
  - [section 3.3] "the distillation techniques for diffusion models, which distill multiple steps of the reverse diffusion process to a single step, do not significantly harm the generation performance while accelerating the sampling speed"
  - [section 3.3] "Let T = {ti}N i=1 be the discretized time interval used for diffusion bridge... Then a distilled score network... is then trained with the loss"
- Break condition: If the single-step approximation loses too much precision, the ensemble prediction accuracy degrades.

## Foundational Learning

- **Concept**: Schrödinger Bridge theory and entropy-regularized optimal transport
  - Why needed here: DBN relies on the mathematical framework of Schrödinger bridges to find stochastic paths between probability distributions.
  - Quick check question: What is the key difference between a Schrödinger bridge and a simple linear interpolation between distributions?

- **Concept**: Score-based generative modeling and denoising diffusion probabilistic models (DDPM)
  - Why needed here: DBN uses score networks similar to those in DDPM to estimate gradients of log densities for the diffusion process.
  - Quick check question: How does a score network in DDPM differ from a standard neural network classifier?

- **Concept**: Knowledge distillation and ensemble distillation techniques
  - Why needed here: DBN builds on ensemble distillation ideas but replaces direct output prediction with diffusion-based transport.
  - Quick check question: What is the main limitation of traditional ensemble distillation that DBN aims to overcome?

## Architecture Onboarding

- **Component map**: Input -> Feature extractor (gψ) -> Score network (εϕ) -> Diffusion simulator -> Ensemble prediction

- **Critical path**:
  1. Forward pass through source model to get features h1
  2. Sample temperature T and compute annealed source logit
  3. Initialize diffusion at Z1 = z1/T
  4. Run single-step diffusion to get Z0
  5. Apply softmax to Z0 to get ensemble prediction

- **Design tradeoffs**:
  - Score network size vs. inference speed: smaller networks are faster but less expressive
  - Number of diffusion bridges vs. accuracy: more bridges improve accuracy but increase cost
  - Temperature distribution vs. bridge stability: too high temperature may destabilize training

- **Failure signatures**:
  - High NLL or poor calibration despite good accuracy
  - Performance plateaus when increasing ensemble size beyond a threshold
  - Training instability when temperature is too high or too low

- **First 3 experiments**:
  1. Train a single DBN with 3 ensembles and compare accuracy to DE-3
  2. Test how many ensembles a single DBN can approximate before performance degrades
  3. Compare inference speed (FLOPs) of DBN vs. DE and BN for varying ensemble sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DBN scale with ensemble size beyond 9 models, and what is the theoretical limit of a single DBN's capacity?
- Basis in paper: [explicit] The paper states that DBNs can approximate LN+1 ensemble models with L DBNs, but also notes that a single DBN has intrinsic capacity limits and combining multiple DBNs increases training time.
- Why unresolved: The experiments only test up to 9 ensemble models, leaving uncertainty about performance saturation or degradation with larger ensembles.
- What evidence would resolve it: Empirical results testing DBN performance on ensembles of 10+ models, including analysis of accuracy, DEE, and training/inference efficiency.

### Open Question 2
- Question: Can DBN be extended to non-classification tasks like regression or structured prediction, and how would the conditional diffusion bridge formulation adapt?
- Basis in paper: [inferred] The current formulation is specific to K-way classification logits, and the paper does not explore DBN's applicability to other domains or output spaces.
- Why unresolved: The paper focuses solely on image classification benchmarks, so generalizability to other tasks remains unexplored.
- What evidence would resolve it: Experiments applying DBN to regression, structured prediction, or generative modeling tasks, along with theoretical analysis of the diffusion bridge formulation for continuous or structured outputs.

### Open Question 3
- Question: What is the impact of different temperature distributions (ptemp) on DBN performance, and could alternative annealing strategies improve accuracy or stability?
- Basis in paper: [explicit] The paper uses a Beta(1,5) distribution scaled to [1,2.2] for annealing, but does not explore sensitivity to this choice or compare alternative distributions.
- Why unresolved: The annealing strategy is treated as a fixed hyperparameter, with no analysis of its effect on training dynamics or final performance.
- What evidence would resolve it: Ablation studies varying ptemp (e.g., uniform, exponential, learned distributions) and measuring resulting accuracy, NLL, and training stability.

## Limitations

- Single DBN has intrinsic capacity limits and may not scale to very large ensembles without combining multiple bridges
- Performance depends on the correlation between ensemble members' logit distributions, which may not hold for diverse architectures
- The method requires training a separate score network for each source model, increasing training complexity

## Confidence

- **Computational efficiency advantage**: High - FLOPs measurements are provided and consistent across experiments
- **Accuracy claims**: Medium - Limited to three datasets with no extensive ablation on temperature annealing
- **Theoretical foundation**: High - Clear connection to established Schrödinger bridge theory, though novel application to ensemble inference

## Next Checks

1. Test DBN performance on a larger ensemble size (e.g., 10-20 members) to identify the scaling limit of a single bridge
2. Conduct an ablation study on temperature annealing strategies to determine optimal settings for different ensemble sizes
3. Evaluate DBN on additional datasets with varying domain characteristics to assess generalizability beyond CIFAR and TinyImageNet