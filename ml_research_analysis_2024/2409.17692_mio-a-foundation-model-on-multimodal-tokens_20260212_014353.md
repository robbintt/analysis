---
ver: rpa2
title: 'MIO: A Foundation Model on Multimodal Tokens'
arxiv_id: '2409.17692'
source_url: https://arxiv.org/abs/2409.17692
tags:
- arxiv
- image
- multimodal
- speech
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MIO, a foundation model that unifies multimodal
  understanding and generation across text, image, video, and speech using discrete
  tokens in an end-to-end, autoregressive framework. The model addresses the limitation
  of existing multimodal systems by supporting any-to-any modality conversion and
  generation of interleaved multimodal sequences.
---

# MIO: A Foundation Model on Multimodal Tokens

## Quick Facts
- arXiv ID: 2409.17692
- Source URL: https://arxiv.org/abs/2409.17692
- Authors: Zekun Wang; King Zhu; Chunpu Xu; Wangchunshu Zhou; Jiaheng Liu; Yibo Zhang; Jiashuo Wang; Ning Shi; Siyu Li; Yizhi Li; Haoran Que; Zhaoxiang Zhang; Yuanxing Zhang; Ge Zhang; Ke Xu; Jie Fu; Wenhao Huang
- Reference count: 40
- Primary result: Foundation model achieving competitive performance on MME-Unify leaderboard with emergent capabilities in interleaved multimodal generation

## Executive Summary
MIO introduces a unified foundation model that bridges the gap between multimodal understanding and generation across text, image, video, and speech using discrete tokens. The model addresses the limitations of existing multimodal systems by supporting any-to-any modality conversion and generation of interleaved multimodal sequences in an end-to-end, autoregressive framework. Through a carefully designed four-stage training process and supervised fine-tuning on 16 diverse tasks, MIO demonstrates competitive performance against dual-modal and any-to-any baselines while showcasing emergent capabilities like chain-of-visual-thought reasoning and multimodal in-context learning.

## Method Summary
MIO employs discrete tokens as the unified representation across all modalities, enabling seamless multimodal understanding and generation. The model is trained through a four-stage process: alignment pre-training to establish cross-modal correspondences, interleaved pre-training to learn joint representations, speech-enhanced pre-training to incorporate audio data, and supervised fine-tuning on 16 diverse tasks. The autoregressive architecture allows for any-to-any modality conversion and generation of interleaved multimodal sequences. The model uses a mixture-of-experts (MoE) architecture with 16 experts per layer, totaling approximately 1.2B parameters.

## Key Results
- Achieves competitive performance on MME-Unify leaderboard, ranking highly among multimodal models
- Demonstrates emergent capabilities including chain-of-visual-thought reasoning and multimodal in-context learning
- Excels in low-resource modalities (speech and video) compared to baseline models
- Supports any-to-any modality conversion and generation of interleaved multimodal sequences

## Why This Works (Mechanism)
MIO's effectiveness stems from its unified discrete token representation that enables seamless modality conversion. The four-stage training process progressively builds capabilities from basic cross-modal alignment to sophisticated interleaved generation. The autoregressive framework allows the model to learn temporal dependencies across modalities naturally. The MoE architecture provides computational efficiency while maintaining model capacity for handling diverse multimodal tasks.

## Foundational Learning

**Discrete Tokenization**
- Why needed: Provides a unified representation space for all modalities
- Quick check: Verify that all modalities can be converted to/from discrete tokens without information loss

**Autoregressive Generation**
- Why needed: Enables natural sequential generation of multimodal content
- Quick check: Test generation quality on simple text-to-image and image-to-text tasks

**Cross-Modal Alignment**
- Why needed: Establishes relationships between different modality representations
- Quick check: Evaluate retrieval performance across modalities

**Mixture-of-Experts**
- Why needed: Balances computational efficiency with model capacity
- Quick check: Compare performance with and without MoE on resource-constrained tasks

## Architecture Onboarding

**Component Map**
Text Encoder -> Discrete Token Space -> Modality-Specific Decoders (Image, Video, Speech)

**Critical Path**
Input Modality -> Encoder → Discrete Token Representation → Decoder → Output Modality

**Design Tradeoffs**
The use of discrete tokens enables unified representation but may introduce quantization noise. The four-stage training process balances efficiency with comprehensive capability development. The MoE architecture reduces computation but adds routing complexity.

**Failure Signatures**
Poor cross-modal alignment may manifest as degraded retrieval performance. Tokenization errors could lead to generation artifacts. MoE routing failures might cause mode collapse in specific modalities.

**3 First Experiments**
1. Test text-to-image generation quality with simple prompts
2. Evaluate cross-modal retrieval performance (image→text, text→image)
3. Assess speech-to-text transcription accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- Performance claims lack detailed quantitative comparisons with specific benchmark numbers
- Emergent capabilities demonstrated qualitatively but not systematically evaluated
- Training data includes proprietary speech-text interleaved datasets not publicly available
- Limited ablation studies to confirm advantages in low-resource modality performance

## Confidence

**High Confidence:**
- Unified discrete token representation approach
- End-to-end autoregressive framework design

**Medium Confidence:**
- Competitive performance on MME-Unify leaderboard claims

**Low Confidence:**
- Emergent capabilities quantification and systematic evaluation

## Next Checks
1. Request and examine specific quantitative benchmark results from MME-Unify leaderboard to verify ranking and performance metrics
2. Conduct ablation studies comparing MIO's performance on low-resource modalities against other models when trained with equal data quantities
3. Design and implement systematic evaluation protocols for emergent capabilities (chain-of-visual-thought reasoning, multimodal in-context learning) to quantify these abilities beyond qualitative demonstrations