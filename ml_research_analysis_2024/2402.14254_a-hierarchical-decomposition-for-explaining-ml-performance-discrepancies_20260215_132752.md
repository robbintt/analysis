---
ver: rpa2
title: A hierarchical decomposition for explaining ML performance discrepancies
arxiv_id: '2402.14254'
source_url: https://arxiv.org/abs/2402.14254
tags: []
core_contribution: This paper introduces a hierarchical decomposition framework for
  explaining ML performance discrepancies across domains. The approach provides both
  aggregate and detailed variable-level decompositions without requiring causal knowledge.
---

# A hierarchical decomposition for explaining ML performance discrepancies

## Quick Facts
- arXiv ID: 2402.14254
- Source URL: https://arxiv.org/abs/2402.14254
- Reference count: 40
- Introduces hierarchical decomposition framework for explaining ML performance discrepancies across domains

## Executive Summary
This paper presents a hierarchical decomposition framework for understanding performance discrepancies in machine learning models across different domains. The approach provides both aggregate and detailed variable-level decompositions without requiring causal knowledge, making it broadly applicable. The method uses Shapley values based on variance explained by hypothesized partial distribution shifts to measure variable importance, and derives debiased machine learning estimators with confidence intervals for both aggregate and detailed terms.

The framework is validated through simulations showing coverage rates approaching nominal levels, and demonstrated on real-world case studies including hospital readmission and insurance coverage prediction models. The method offers more intuitive explanations compared to existing approaches and provides asymptotically valid confidence intervals, addressing a critical need in understanding when and why ML models fail across different populations or settings.

## Method Summary
The hierarchical decomposition framework introduces a novel approach to explaining ML performance discrepancies by decomposing performance differences into aggregate and detailed variable-level components. The method defines importance using Shapley values based on variance explained by hypothesized partial distribution shifts, avoiding the need for causal knowledge. Debiased machine learning estimators are derived for both aggregate and detailed terms, providing asymptotically valid confidence intervals. The framework allows for intuitive explanations of performance gaps by quantifying the contribution of different variables and their interactions to overall performance discrepancies across domains.

## Key Results
- Simulations demonstrate coverage rates approaching nominal levels for derived confidence intervals
- Real-world case studies show utility in understanding performance gaps for hospital readmission prediction models
- Real-world case studies show utility in understanding performance gaps for insurance coverage prediction models
- Method provides more intuitive explanations compared to existing approaches for performance discrepancy analysis

## Why This Works (Mechanism)

## Foundational Learning
1. **Shapley value decomposition** - A method for fairly distributing gains or costs among multiple players based on their marginal contributions
   - Why needed: Provides a principled way to attribute performance differences to individual variables and their interactions
   - Quick check: Verify that Shapley values sum to total performance difference

2. **Debiased machine learning** - A framework for estimating causal parameters while controlling for regularization bias
   - Why needed: Ensures valid statistical inference for importance measures
   - Quick check: Confirm double robustness properties of the estimator

3. **Hierarchical decomposition** - Breaking down total effects into multiple levels of aggregation
   - Why needed: Allows for both aggregate and detailed variable-level explanations
   - Quick check: Validate that higher-level aggregates equal sum of detailed components

## Architecture Onboarding
**Component map:** Data inputs -> Hypothesized shift specification -> Variance estimation -> Shapley value calculation -> Hierarchical decomposition -> Confidence interval generation

**Critical path:** The core computational path flows from hypothesized partial distribution shifts through variance estimation to Shapley value calculations, with debiased machine learning ensuring valid statistical inference throughout.

**Design tradeoffs:** The method trades computational complexity for interpretability and statistical validity, requiring multiple model fits but providing detailed, confidence-interval-wrapped explanations.

**Failure signatures:** Poor hypothesized shift specifications may lead to misleading decompositions; high-dimensional problems may face computational challenges; violations of asymptotic assumptions may affect confidence interval coverage.

**First experiments:**
1. Apply framework to synthetic data with known ground truth to verify decomposition accuracy
2. Test sensitivity to different hypothesized shift specifications on benchmark datasets
3. Evaluate computational scaling behavior with increasing feature dimensions

## Open Questions the Paper Calls Out
None

## Limitations
- Computational complexity may limit scalability to very high-dimensional problems
- Dependence on the quality of hypothesized partial distribution shifts
- Potential sensitivity to model specification choices

## Confidence
- **High confidence**: Theoretical foundations of hierarchical decomposition framework and Shapley values for importance measurement
- **Medium confidence**: Debiased machine learning estimators and their asymptotic properties
- **Medium confidence**: Simulation results showing coverage rates approaching nominal levels

## Next Checks
1. Test the framework's performance on synthetic datasets with known ground truth decompositions to verify accuracy across different types of distribution shifts
2. Evaluate computational efficiency and scalability on high-dimensional datasets with varying numbers of features
3. Conduct sensitivity analyses to assess how different hypothesized shift specifications affect decomposition results and confidence interval coverage